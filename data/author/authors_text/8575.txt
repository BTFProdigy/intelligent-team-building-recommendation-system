Proceedings of NAACL HLT 2007, Companion Volume, pages 137?140,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Analysis and System Combination of Phrase- and N -gram-based
Statistical Machine Translation Systems
Marta R. Costa-jussa`1, Josep M. Crego1, David Vilar2
Jose? A. R. Fonollosa1, Jose? B. Marin?o1 and Hermann Ney2
1TALP Research Center (UPC), Barcelona 08034, Spain
{mruiz,jmcrego,adrian,canton}@gps.tsc.upc.edu
2RWTH Aachen University, Aachen D-52056, Germany
{vilar,ney}@i6.informatik.rwth-aachen.de
Abstract
In the framework of the Tc-Star project,
we analyze and propose a combination of
two Statistical Machine Translation sys-
tems: a phrase-based and an N -gram-based
one. The exhaustive analysis includes a
comparison of the translation models in
terms of efficiency (number of translation
units used in the search and computational
time) and an examination of the errors in
each system?s output. Additionally, we
combine both systems, showing accuracy
improvements.
1 Introduction
Statistical machine translation (SMT) has evolved
from the initial word-based translation models to
more advanced models that take the context sur-
rounding the words into account. The so-called
phrase-based and N -gram-based models are two ex-
amples of these approaches (Zens and Ney, 2004;
Marin?o et al, 2006).
In current state-of-the-art SMT systems, the
phrase-based or the N -gram-based models are usu-
ally the main features in a log-linear framework, rem-
iniscent of the maximum entropy modeling approach.
Two basic issues differentiate the N -gram-based
system from the phrase-based one: the training data
is sequentially segmented into bilingual units; and
the probability of these units is estimated as a bilin-
gual N -gram language model. In the phrase-based
model, no monotonicity restriction is imposed on the
segmentation and the probabilities are normally es-
timated simply by relative frequencies.
This paper extends the analysis of both systems
performed in (Crego et al, 2005a) by additionally
performing a manual error analysis of both systems,
which were the ones used by UPC and RWTH in the
last Tc-Star evaluation.
Furthermore, we will propose a way to combine
both systems in order to improve the quality of trans-
lations.
Experiments combining several kinds of MT sys-
tems have been presented in (Matusov et al, 2006),
based only on the single best output of each system.
Recently, a more straightforward approach of both
systems has been performed in (Costa-jussa` et al,
2006) which simply selects, for each sentence, one of
the provided hypotheses.
This paper is organized as follows. In section 2,
we briefly describe the phrase and the N -gram-based
baseline systems. In the next section we present the
evaluation framework. In Section 4 we report a struc-
tural comparison performed for both systems and, af-
terwards, in Section 5, we analyze the errors of both
systems. Finally, in the last two sections we rescore
and combine both systems, and the obtained results
are discussed.
2 Baseline Systems
2.1 Phrase-based System
The basic idea of phrase-based translation is to seg-
ment the given source sentence into units (here called
phrases), then translate each phrase and finally com-
pose the target sentence from these phrase transla-
tions.
In order to train these phrase-based models, an
alignment between the source and target training
sentences is found by using the standard IBM mod-
els in both directions (source-to-target and target-
to-source) and combining the two obtained align-
ments. Given this alignment an extraction of con-
tiguous phrases is carried out, specifically we extract
all phrases that fulfill the following restrictions: all
source (target) words within the phrase are aligned
only to target (source) words within the phrase.
The probability of these phrases is normally esti-
mated by relative frequencies, normally in both di-
rections, which are then combined in a log-linear way.
137
2.2 N-gram-based System
In contrast with standard phrase-based approaches,
the N -gram translation model uses tuples as bilin-
gual units whose probabilities are estimated as an
N -gram language model (Marin?o et al, 2006). This
model approximates the joint probability between
the source and target languages by using N -grams.
Given a word alignment, tuples define a unique
and monotonic segmentation of each bilingual sen-
tence, building up a much smaller set of units
than with phrases and allowing N -gram estimation
to account for the history of the translation pro-
cess (Marin?o et al, 2006).
2.3 Feature functions
Both baseline systems are combined in a log-linear
way with several additional feature functions: a tar-
get language model, a forward and a backward lex-
icon model and a word bonus are common features
for both systems. The phrase-based system also in-
troduces a phrase bonus model.
3 Evaluation framework
The translation models presented so far were the ones
used by UPC and RWTH in the second evaluation
campaign of the Tc-Star project. The goal of this
project is to build a speech-to-speech translation sys-
tem that can deal with real life data.
The corpus consists of the official version of the
speeches held in the European Parliament Plenary
Sessions (EPPS), as available on the web page of the
European Parliament. Table 1 shows some statistics.
The following tools have been used for building
both systems: Word alignments were computed us-
ing GIZA++ (Och, 2003), language models were es-
timated using the SRILM toolkit (Stolcke, 2002), de-
coding was carried out by the free available MARIE
decoder (Crego et al, 2005b) and the optimization
was performed through an in-house implementation
of the simplex method (Nelder and Mead, 1965).
Spanish English
Train Sentences 1.2M
Words 32M 31M
Vocabulary 159K 111K
Dev Sentences 1 122 699
Words 26K 21K
Test Sentences 1 117 894
Words 26K 26K
Table 1: Statistics of the EPPS Corpora.
4 Structural comparison
Both approaches aim at improving accuracy by in-
cluding word context in the model. However, the
implementation of the models are quite different and
may produce variations in several aspects.
Table 2 shows the effect on decoding time intro-
duced through different settings of the beam size.
Additionally, the number of available translation
units is shown, corresponding to number of avail-
able phrases for the phrase-based system and 1gram,
2gram and 3gram entries for the N -gram-based sys-
tem. Results are computed on the development set.
Task Beam Time(s) Units
50 2,677
es?en 10 852 537k
5 311
50 2,689
en?es 10 903 594k
5 329
50 1,264
es?en 10 281 104k 288k 145k
5 138
50 1,508
en?es 10 302 118k 355k 178k
5 155
Table 2: Impact on efficiency of the beam size in PB
(top) and NB system (bottom).
As it can be seen, the number of translation units
is similar in both tasks for both systems (537k ?
537k for Spanish to English and 594k ? 651k for
English to Spanish) while the time consumed in de-
coding is clearly higher for the phrase-based system.
This can be explained by the fact that in the phrase-
based approach, the same translation can be hypoth-
esized following several segmentations of the input
sentence, as phrases appear (and are collected) from
multiple segmentations of the training sentence pairs.
In other words, the search graph seems to be over-
populated under the phrase-based approach.
Table 3 shows the effect on translation accuracy
regarding the size of the beam in the search. Results
are computed on the test set for the phrase-based
and N -gram-based systems.
Results of the N -gram-based system show that de-
creasing the beam size produces a clear reduction
of the accuracy results. The phrase-based system
shows that accuracy results remain very similar un-
der the different settings. The reason is found on
how translation models are used in the search. In
the phrase-based approach, every partial hypothesis
138
Task Beam BLEU NIST mWER
50 51.90 10.53 37.54
es?en 10 51.93 10.54 37.49
5 51.87 10.55 37.47
50 47.75 9.94 41.20
en?es 10 47.77 9.96 41.09
5 47.86 10.00 40.74
50 51.63 10.46 37.88
es?en 10 51.50 10.45 37.83
5 51.39 10.45 37.85
50 47.73 10.08 40.50
en?es 10 46.82 9.97 41.04
5 45.59 9.83 41.04
Table 3: Impact on accuracy of the beam size in PB
(top) and NB system (bottom).
is scored uncontextualized, hence, a single score is
used for a given partial hypothesis (phrase). In the
N -gram-based approach, the model is intrinsically
contextualized, which means that each partial hy-
pothesis (tuple) depends on the preceding sequence
of tuples. Thus, if a bad sequence of tuples (bad
scored) is composed of a good initial sequence (well
scored), it is placed on top of the first stacks (beam)
and may cause the pruning of the rest of hypotheses.
5 Error analysis
In order to better asses the quality and the differ-
ences between the two systems, a human error anal-
ysis was carried out. The guidelines for this error
analysis can be found in (Vilar et al, 2006). We
randomly selected 100 sentences, which were evalu-
ated by bilingual judges.
This analysis reveals that both systems produce
the same kind of errors in general. However some dif-
ferences were identified. For the English to Spanish
direction the greatest problem is the correct genera-
tion of the right tense for verbs, with around 20% of
all translation errors being of this kind. Reordering
also poses an important problem for both phrase and
N-gram-based systems, with 18% or 15% (respec-
tively) of the errors falling into this category. Miss-
ing words is also an important problem. However,
most of them (approximately two thirds for both sys-
tems) are filler words (i.e. words which do not con-
vey meaning), that is, the meaning of the sentence
is preserved. The most remarkable difference when
comparing both systems is that the N -gram based
system produces a relatively large amount of extra
words (approximately 10%), while for the phrase-
based system, this is only a minor problem (2% of
the errors). In contrast the phrase-based system has
more problems with incorrect translations, that is
words for which a human can find a correspondence
in the source text, but the translation is incorrect.
Similar conclusions can be drawn for the inverse di-
rection. The verb generating problem is not so acute
in this translation direction due to the much simpli-
fied morphology of English. An important problem
is the generation of the right preposition.
The N -gram based system seems to be able to pro-
duce more accurate translations (reflected by a lower
percentage of translation errors). However, it gener-
ates too many additional (and incorrect words) in
the process. The phrase-based system, in contrast,
counteracts this effect by producing a more direct
correspondence with the words present in the source
sentence at the cost of sometimes not being able to
find the exact translation.
6 System Rescoring and
Combination
Integration of both output translations in the search
procedure is a complex task. Translation units of
both models are quite different and generation his-
tories pose severe implementation difficulties. We
propose a method for combining the two systems at
the level of N -best lists.
Some features that are useful for SMT are too com-
plex for including them directly in the search pro-
cess. A clear example are the features that require
the entire target sentence to be evaluated, as this is
not compatible with the pruning and recombination
procedures that are necessary for keeping the target
sentence generation process manageable. A possible
solution for this problem is to apply sentence level
re-ranking by using N -best lists.
6.1 Rescoring Criteria
The aim of the rescoring procedure is to choose the
best translation candidate out of a given set of N
possible translations. In our approach this transla-
tion candidates are produced independently by both
of the systems and then combined by a simple con-
catenation1. In order for the hypothesis to have a
comparable set of scores, we perform an additional
?cross-rescoring? of the lists.
Given an N -best list of the phrase-based (N -gram-
based) system, we compute the cost of each target
sentence of this N -best list for the N -gram-based
(phrase-based) system. However this computation
is not possible in all cases. Table 4 shows the per-
centage of target sentences that the N -gram-based
1With removal of duplicates.
139
(phrase-based) system is able to produce given an N -
best list of target sentences computed by the phrase-
based (N -gram-based) system. This percentage is
calculated on the development set.
The vocabulary of phrases is bigger than the vo-
cabulary of tuples, due to the fact that phrases are
extracted from multiple segmentations of the train-
ing sentence pairs. Hence, the number of sentences
reproduced by the N -gram-based system is smaller
than the number of sentences reproduced by the
phrase-based system. Whenever a sentence can not
be reproduced by a given system, the cost of the
worst sentence in the N -best list is assigned to it.
Task N -best % NB % PB
es?en 1000 37.5 57.5
en?es 1000 37.2 48.6
Table 4: Sentences (%) produced by each system.
6.2 Results
Table 5 shows results of the rescoring and system
combination experiments on the test set. The first
two rows include results of systems non-rescored and
PB (NB) rescored by NB (PB). The third row corre-
sponds to the system combination. Here, PB (NB)
rescored by NB (PB) are simply merged and ranked
by rescored score.
System N -best BLEU NIST mWER
Spanish-to-English
PB 1 51.90 10.54 37.50
PB 1000 52.55 10.61 37.12
NB 1 51.63 10.46 37.88
NB 1000 52.25 10.55 37.43
PB+NB 2 51.77 10.49 37.68
PB+NB 2000 52.31 10.56 37.32
English-to-Spanish
PB 1 47.75 9.94 41.2
PB 1000 48.46 10.13 39.98
NB 1 47.73 10.09 40.50
NB 1000 48.33 10.15 40.13
PB+NB 2 48.26 10.05 40.61
PB+NB 2000 48.54 10.16 40.00
Table 5: Rescoring and system combination results.
7 Discussion
The structural comparison has shown on the one
hand that the N -gram-based system outperforms
the phrase-based in terms of search time efficiency
by avoiding the overpopulation problem presented
in the phrase-based approach. On the other hand
the phrase-based system shows a better performance
when decoding under a highly constrained search.
A detailed error analysis has also been carried out
in order to better determine the differences in per-
formance of both systems. The N -gram based sys-
tem produced more accurate translations, but also a
larger amount of extra (incorrect) words when com-
pare to the phrase-based translation system.
In section 6 we have presented a system combina-
tion method using a rescoring feature for each SMT
system, i.e. the N -gram-based feature for the phrase-
based system and vice-versa. For both systems, con-
sidering the feature of the opposite system leads to
an improvement of BLEU score.
References
M.R. Costa-jussa`, J.M. Crego, A. de Gispert,
P. Lambert, M. Khalilov J.A.R. Fonollosa, J.B.
Marin?o, and R. Banchs. 2006. Talp phrase-based
statistical machine translation and talp system
combination the iwslt 2006. IWSLT06.
J. M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.
Fonollosa. 2005a. N-gram-based versus phrase-
based statistical machine translation. IWSLT05,
October.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005b.
An Ngram-based statistical machine translation
decoder. ICSLP05, April.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gis-
pert, P. Lambert, J.A.R. Fonollosa, and M.R.
Costa-jussa`. 2006. N-gram based machine trans-
lation. Computational Linguistics, 32(4):527?549.
E. Matusov, N. Ueffing, and H. Ney. 2006. Com-
puting consensus translation from multiple ma-
chine translation systems using enhanced hypothe-
ses alignment. EACL06, pages 33?40.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language
modeling toolkit. Proc. of the 7th Int. Conf. on
Spoken Language Processing, ICSLP?02, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and
Hermann Ney. 2006. Error Analysis of Machine
Translation Output. In LREC06, pages 697?702,
Genoa, Italy, May.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In HLT04, pages 257?264, Boston, MA, May.
140
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 41?48,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Augmenting a Small Parallel Text with Morpho-syntactic Language
Resources for Serbian-English Statistical Machine Translation
Maja Popovic?, David Vilar, Hermann Ney
Lehrstuhl fu?r Informatik VI
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{popovic,vilar,ney}@informatik.rwth-aachen.de
Slobodan Jovic?ic?, Zoran ?Saric?
Faculty of Electrical Engineering
University of Belgrade
Serbia and Montenegro
jovicic@etf.bg.ac.yu
Abstract
In this work, we examine the quality of
several statistical machine translation sys-
tems constructed on a small amount of
parallel Serbian-English text. The main
bilingual parallel corpus consists of about
3k sentences and 20k running words from
an unrestricted domain. The translation
systems are built on the full corpus as well
as on a reduced corpus containing only
200 parallel sentences. A small set of
about 350 short phrases from the web is
used as additional bilingual knowledge. In
addition, we investigate the use of mono-
lingual morpho-syntactic knowledge i.e.
base forms and POS tags.
1 Introduction and Related Work
The goal of statistical machine translation (SMT) is
to translate a source language sequence f1, . . . , fJ
into a target language sequence e1, . . . , eI by max-
imising the conditional probability Pr(eI1|fJ1 ). This
probability can be factorised into the translation
model probability P (fJ1 |eI1) which describes the
correspondence between the words in the source and
the target sequence, and the language model proba-
bility P (eJ1 ) which describes well-formedness of the
produced target sequence. These two probabilities
can be modelled independently of each other. For
detailed descriptions of SMT models see for exam-
ple (Brown et al, 1993; Och and Ney, 2003).
Translation probabilities are learnt from a bilin-
gual parallel text corpus and language model proba-
bilities are learnt from a monolingual text in the tar-
get language. Usually, the performance of a trans-
lation system strongly depends on the size of the
available training corpus. However, acquisition of
a large high-quality bilingual parallel text for the de-
sired domain and language pair requires lot of time
and effort, and, for many language pairs, is even not
possible. Besides, small corpora have certain advan-
tages - the acquisition does not require too much
effort and also manual creation and correction are
possible. Therefore there is an increasing number of
publications dealing with limited amounts of bilin-
gual data (Al-Onaizan et al, 2000; Nie?en and Ney,
2004).
For the Serbian language, as a rather minor and
not widely studied language, there are not many
language resources available, especially not parallel
texts. On the other side, investigations on this lan-
guage may be quite useful since the majority of prin-
ciples can be extended to the wider group of Slavic
languages (e.g. Czech, Polish, Russian, etc.).
In this work, we exploit small Serbian-English
parallel texts as a bilingual knowledge source for
statistical machine translation. In addition, we in-
vestigate the possibilities for improving the trans-
lation quality using morpho-syntactic information
in the source language. Some preliminary transla-
tion results on this language pair have been reported
in (Popovic? et al, 2004; Popovic? and Ney, 2004),
but no systematic investigation has been done so far.
This work presents several translation systems cre-
ated with different amounts and types of training
data and gives a detailed description of the language
resources used.
41
2 Language Resources
2.1 Language Characteristics
Serbian, as a Slavic language, has a very rich inflec-
tional morphology for all open word classes. There
are six distinct cases affecting not only common
nouns but also proper nouns as well as pronouns,
adjectives and some numbers. Some nouns and ad-
jectives have two distinct plural forms depending on
the number (if it is larger than four or not). There
are also three genders for the nouns, pronouns, ad-
jectives and some numbers leading to differences be-
tween the cases and also between the verb participles
for past tense and passive voice.
As for verbs, person and many tenses are ex-
pressed by the suffix, and the subject pronoun (e.g.
I, we, it) is often omitted (similarly as in Spanish and
Italian). In addition, negation of three quite impor-
tant verbs, ?biti? (to be, auxiliary verb for past tense,
conditional and passive voice), ?imati? (to have) and
?hteti? (to want, auxiliary verb for the future tense),
is done by adding the negative particle to the verb as
a prefix.
As for syntax, Serbian has a quite free word or-
der, and there are no articles, neither indefinite nor
definite.
All these characteristics indicate that morpho-
syntactic knowledge might be very useful for sta-
tistical machine translation involving Serbian lan-
guage, especially when only scarce amounts of par-
allel text are available.
2.2 Parallel Corpora
Finding high-quality bilingual or multilingual paral-
lel corpora involving Serbian language is a difficult
task. For example, there are several web-sites with
the news in both Serbian and English (some of them
in other languages as well), but these texts are only
comparable and not parallel at all. To our knowl-
edge, the only currently available Serbian-English
parallel text suitable for statistical machine trans-
lation is a manually created electronic version of
the Assimil language course which has been used
for some preliminary experiments in (Popovic? et al,
2004; Popovic? and Ney, 2004). We have used this
corpus for systematical investigations described in
this work.
2.2.1 Assimil Language Course
The electronic form of Assimil language course
contains about 3k sentences and 25k running words
of various types of conversations and descriptions as
well as a few short newspaper articles. Detailed cor-
pus statistics can be seen in Table 1. Since the do-
main of the corpus is basically not restricted, the vo-
cabulary size is relatively large. Due to the rich mor-
phology, the vocabulary for Serbian is almost two
times larger than for English. The average sentence
length for Serbian is about 8.5 words per sentence,
and for English about 9.5. This difference is mainly
caused by the lack of articles and omission of some
subject pronouns in Serbian .
The development and test set (500 sentences) are
randomly extracted from the original corpus and the
rest is used for training (referred to as 2.6k).
In order to investigate the scenario with extremely
scarce training material, a reduced training corpus
(referred to as 200) has been created by random ex-
traction of 200 sentences from the original training
corpus.
The morpho-syntactic annotation of the En-
glish part of the corpus has been done by the con-
straint grammar parser ENGCG for morphological
and syntactic analysis of English language. For each
word, this tool provides its base form and sequence
of morpho-syntactic tags.
For the Serbian corpus, to our knowlegde there
is no available tool for automatic annotation of this
language. Therefore, the base forms have been in-
troduced manually and the POS tags have been pro-
vided partly manually and partly automatically us-
ing a statistical maximum-entropy based POS tagger
similar to the one described in (Ratnaparkhi, 1996).
First, the 200 sentences of the reduced training cor-
pus have been annotated completely manually. Then
the first 500 sentences of the rest of the training cor-
pus have been tagged automatically and the errors
have been manually corrected. Afterwards, the POS
tagger has been trained on the extended corpus (700
sentences), the next 500 sentences of the rest are an-
notated, and the procedure has been repeated until
the annotation has been finished for the complete
corpus.
42
Table 1: Statistics of the Serbian-English Assimil corpus
Serbian English
Training: original base forms original no article
full corpus Sentences 2632 2632
(2.6k) Running Words + Punct. 22227 24808 23308
Average Sentence Length 8.4 9.5 8.8
Vocabulary Size 4546 2605 2645 2642
Singletons 2728 1253 1211
reduced corpus Sentences 200 200
(200) Running Words + Punct. 1666 1878 1761
Average Sentence Length 8.3 10.4 8.8
Vocabulary Size 778 596 603 600
Singletons 618 417 395
Dev+Test Sentences 500 500
Running Words + Punct. 4161 4657 4362
Average Sentence Length 8.3 9.3 8.7
Vocabulary Size 1457 1030 1055 1052
Running OOVs - 2.6k 12.1% 5.2% 4.8%
Running OOVs - 200 34.5% 27.6% 21.4%
OOVs - 2.6k 32.7% 19.5% 19.7%
OOVs - 200 76.2% 66.0% 66.8%
External Test Sentences 22 22
Running Words + Punct. 395 446 412
Average Sentence Length 18.0 20.3 18.7
Vocabulary Size 213 176 202 199
Running OOVs - 2.6k 44.3% 35.4% 32.1% 34.7%
Running OOVs - 200 53.7% 44.6% 43.7% 47.3 %
OOVs - 2.6k 61.5% 45.4% 44.0% 44.7%
OOVs - 200 74.6% 63.1% 63.9% 64.8%
Table 2: Statistics of the Serbian-English short phrases
Serbian English
Phrases original base forms original no article
Entries 351 351 351 351
Running Words + Punct. 617 617 730 700
Average Entry Length 1.8 1.8 2.1 2.0
Vocabulary Size 335 303 315 312
Singletons 239 209 209 208
New Running 2.6k 20.6% 14.4% 11.8% 11.8%
Words 200 50.6% 41.3% 36.7% 37.8%
New Vocabulary 2.6k 30.1% 22.1% 21.6% 21.2%
Words 200 70.7% 63.0% 63.2% 63.1%
43
2.2.2 Short Phrases
The short phrases used as an additional bilingual
knowledge source in our experiments have been col-
lected from the web and contain about 350 standard
words and short expressions with an average entry
length of 1.8 words for Serbian and 2 words for En-
glish. Table 2 shows that about 30% of words from
the phrase vocabulary are not present in the origi-
nal Serbian corpus and about 70% of those words
are not contained in the reduced corpus. For the
English language those numbers are smaller, about
20% for the original corpus and 60% for the reduced
one. These percentages are indicating that this par-
allel text, although very scarce, might be an useful
additional training material.
The phrases have also been morpho-syntactically
annotated in the same way as the main corpus.
2.2.3 External Test
In addition to the standard development and test
set described in Section 2.2.1, we also tested our
translation systems on a short external parallel text
collected from the BBC News web-site contain-
ing 22 sentences about relations between USA and
Ukraine after the revolution. As can be seen in Ta-
ble 1, this text contains very large portion of out-
of-vocabulary words (almost two thirds of Serbian
words and almost half of English words are not seen
in the training corpus), and has an average sentence
length about two times larger than the training cor-
pus.
3 Transformations in the Source Language
Standard SMT systems usually regard only full
forms of the words, so that translation of full forms
which have not been seen in the training corpus is
not possible even if the base form has been seen.
Since the inflectional morphology of the Serbian
language is very rich, as described in Section 2.1, we
investigate the use of the base forms instead of the
full forms to overcome this problem for the transla-
tion into English. We propose two types of trans-
formations of the Serbian corpus: conversion of the
full forms into the base forms and additional treat-
ment of the verbs.
For the other translation direction, we propose re-
moving the articles in the English part of the corpus
as the Serbian language does not have any.
3.1 Transformations of the Serbian Text
3.1.1 Base Forms
Serbian full forms of the words usually contain
information which is not relevant for translation into
English. Therefore, we propose conversion of all
Serbian words in their base forms. Although for
some other inflected languages like German and
Spanish this method did not yield any translation
improvement, we still considered it as promising be-
cause the number of Serbian inflections is consider-
ably higher than in the other two languages. Table 1
shows that this transformation significantly reduces
the Serbian vocabulary size so that it becomes com-
parable to the English one.
3.1.2 Treatment of Verbs
Inflections of Serbian verbs might contain rel-
evant information about the person, which is es-
pecially important when the pronoun is omitted.
Therefore, we apply an additional treatment of the
verbs. Whereas all other word classes are still re-
placed only by their base forms, for each verb a part
of the POS tag referring to the person is taken and
the verb is converted into a sequence of this tag and
its base form. For the three verbs described in Sec-
tion 2.1, the separation of the negative particle is also
applied: each negative full form is transformed into
the sequence of the POS tag, negative particle and
base form. The detailed statistics of this corpus is
not reported since there are no significant changes,
only the number of running words and average sen-
tence length increase thus becoming closer to the
values of the English corpus.
3.2 Transformations of the English Text
3.2.1 Removing Articles
Since the articles are one of the most frequent
word classes in English, but on the other side there
are no arcticles at all in Serbian, we propose remov-
ing the articles from the English corpus for trans-
lation into Serbian. Each English word which has
been detected as an article by means of its POS tag
has been removed from the corpus. In Table 1, it
can be seen that this method significantly reduces
the number of running words and the average sen-
tence length of the English corpus thus becoming
comparable to the values of the Serbian corpus.
44
4 Translation Experiments and Results
4.1 Experimental Settings
In order to systematically investigate the impact of
the bilingual training corpus size and the effects
of the morpho-syntactic information on the trans-
lation quality, the translation systems were trained
on the full training corpus (2.6k) and on the re-
duced training corpus (200), both with and with-
out short phrases. The translation is performed in
both directions, i.e. from Serbian to English and
other way round. For the Serbian to English trans-
lation systems, three versions of the Serbian corpus
have been used: original (baseline), base forms only
(sr base) and base forms with additional treatment
of the verbs (sr base+v-pos). For the translation into
Serbian, the systems were trained on two versions of
the English corpus: original (baseline) and without
articles (en no-article).
The baseline translation system is the Alignment
Templates system with scaling factors (Och and
Ney, 2002). Word alignments are produced using
GIZA++ toolkit without symmetrisation (Och and
Ney, 2003). Preprocessing of the source data has
been done before the training of the system, there-
fore modifications of the training and search pro-
cedure were not necessary for the translation of the
transformed source language corpora.
Although the development set has been used to
optimise the scaling factors, results obtained for this
set do not differ from those for the test set. There-
fore only the joint error rates (Development+Test)
are reported.
As for the external test set, results for this text are
reported only for the full corpus systems, since for
the reduced corpus the error rates are higher but the
effects of using phrases and morpho-syntactic infor-
mation are basically the same.
4.2 Translation Results
The evaluation metrics used in our experiments
are WER (Word Error Rate), PER (Position-
independent word Error Rate) and BLEU (BiLin-
gual Evaluation Understudy) (Papineni et al, 2002).
Since BLEU is an accuracy measure, we use 1-
BLEU as an error measure.
4.2.1 Translation from Serbian into English
Error rates for the translation from Serbian into
English are shown in Table 3 and some examples
are shown in Table 6. It can be seen that there is a
significant decrease in all error rates when the full
forms are replaced with their base forms. Since the
redundant information contained in the inflection is
removed, the system can better capture the relevant
information and is capable of producing correct or
approximatively correct translations even for unseen
full forms of the words (marked by ?UNKNOWN ?
in the baseline result example). The treatment of the
verbs yields some additional improvements.
From the first translation example in Table 6 it can
be seen how the problem of some out-of-vocabulary
words can be overcomed with the use of the base
forms. The second and third example are showing
the advantages of the verb treatment, the third one
illustrates the effect of separating the negative parti-
cle.
Reduction of the training corpus to only 200 sen-
tences (about 8% of the original corpus) leads to a
loss of error rates of about 45% relative. However,
the degradation is not higher than 35% if phrases and
morpho-syntactic information are available in addi-
tion to the reduced corpus.
The use of the phrases can improve the transla-
tion quality to some extent, especially for the sys-
tems with the reduced training corpus, but these im-
provements are less remarkable than those obtained
by replacing words with the base forms.
The best system with the complete corpus as well
as the best one with the reduced corpus use the
phrases and the transformed Serbian corpus where
the verb treatment has been applied.
4.2.2 Translation from English into Serbian
Table 4 shows results for the translation from En-
glish into Serbian. As expected, all error rates are
higher than for the other translation direction. Trans-
lation into the morphologically richer language al-
ways has poorer quality because it is difficult to find
the correct inflection.
The performance of the reduced corpus is de-
graded for about 40% relative for the baseline sys-
tem and for about 30% when the phrases are used
and the transformation of the English corpus has
been applied.
45
Table 3: Translation error rates [%] for Serbian?English
Serbian ? English Development+Test
Training Corpus Method WER PER 1-BLEU
2.6k baseline 45.6 39.6 70.0
2.6k sr base 43.5 38.2 68.9
2.6k sr base+v-pos 42.5 35.3 66.2
2.6k+phrases baseline 46.0 39.6 69.5
2.6k+phrases sr base 44.6 39.1 70.2
2.6k+phrases sr base+v-pos 42.1 35.3 66.0
200 baseline 66.5 61.1 91.6
200 sr base 63.2 58.2 90.3
200 sr base+v-pos 63.3 56.2 88.5
200+phrases baseline 65.2 59.5 90.2
200+phrases sr base 62.3 56.9 87.7
200+phrases sr base+v-pos 61.3 53.2 86.2
Table 4: Translation error rates [%] for English?Serbian
English ? Serbian Development+Test
Training Corpus Method WER PER 1-BLEU
2.6k baseline 53.1 46.9 78.6
2.6k en no-article 52.6 47.2 79.4
2.6k+phrases baseline 52.5 46.5 76.6
2.6k+phrases en no-article 52.3 47.0 79.6
200 baseline 73.6 68.0 93.0
200 en no-article 71.5 66.5 93.4
200+phrases baseline 71.7 66.7 92.3
200+phrases en no-article 67.9 62.9 92.1
Table 5: Translation error rates [%] for the external test
Serbian ? English External Test
Training Corpus Method WER PER 1-BLEU
2.6k baseline 72.2 64.8 92.2
2.6k sr base 66.8 61.4 86.9
2.6k sr base+v-pos 67.5 61.4 88.3
2.6k+phrases baseline 71.3 63.9 91.9
2.6k+phrases sr base 67.0 61.2 88.4
2.6k+phrases sr base+v-pos 69.7 61.2 89.8
English ? Serbian
2.6k baseline 85.3 77.0 96.4
2.6k en no-article 77.5 69.9 95.8
2.6k+phrases baseline 84.1 74.9 95.2
2.6k+phrases en no-article 77.7 70.1 94.8
46
The importance of the phrases seems to be larger
for this translation direction. Removing the English
articles does not have the significant role for the
translation systems with full corpus, but for the re-
duced corpus it has basically the same effect as the
use of phrases. The best system with the reduced
corpus has been built with the use of phrases and
removal of the articles.
Table 7 shows some examples of the translation
into Serbian with and without English articles. Al-
though these effects are not directly obvious, it can
be seen that removing of the redundant information
enables better learning of the relevant information
so that system is better capable of producing seman-
tically correct output. The first example illustrates
an syntactically incorrect output with the wrong in-
flection of the verb (?c?itam? means ?I read?). The
output of the system without articles is still not com-
pletely correct, but the semantic is completely pre-
served. The second example illustrates an output
produced by the baseline system which is neither
syntactically nor semantically correct (?you have I
drink?). The output of the new system still has an
error in the verb, informal form of ?you? instead of
the formal one, but nevertheless both the syntax and
semantics are correct.
4.2.3 Translation of the External Text
Translation results for the external test can be
seen in Table 5. As expected, the high number of
out-of-vocabulary words results in very high error
rates. Certain improvement is achieved with the
phrases, but the most significant improvements are
yielded by the use of Serbian base forms and re-
moval of English articles. Verb treatment in this case
does not outperform the base forms system, prob-
ably because there are not so many different verb
forms as in the other corpus, and only a small num-
ber of pronouns is missing.
5 Conclusions
In this work, we have examined the possibilities
for building a statistical machine translation system
with a small bilingual Serbian-English parallel text.
Our experiments showed that the translation results
for this language pair are comparable with results for
other language pairs, especially if the small size of
the corpus, unrestricted domain and rich inflectional
morphology of Serbian language are taken into ac-
count. With the baseline system, we obtained about
45% WER for translation into English and about
53% for translation into Serbian.
We have systematically investigated the impact of
the corpus size on translation quality, as well as the
importance of additional bilingual knowledge in the
form of short phrases. In addition, we have shown
that morpho-syntactic information is a valuable lan-
guage resource for translation of this language pair.
Depending on the availability of resources and
tools, we plan to examine parallel texts with other
languages, and also to do further investigations on
this language pair. We believe that more refined use
of the morpho-syntactic information can yield better
results (for example the hierarchical lexicon model
proposed in (Nie?en and Ney, 2001)). We also be-
lieve that the use of the conventional dictionaries
could improve the Serbian-English translation.
Acknowledgement
This work was partly funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistical Methods for Written Language Transla-
tion? (Ne572/5).
References
Y. Al-Onaizan, U. Germann, U. Hermjakob, K. Knight,
P. Koehn, D. Marcu, and K. Yamada. 2000. Translat-
ing with scarce resources. In National Conference on
Artificial Intelligence (AAAI).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
Sonja Nie?en and Hermann Ney. 2001. Toward hi-
erarchical models for statistical machine translation
of inflected languages. In 39th Annual Meeting of
the Assoc. for Computational Linguistics - joint with
EACL 2001: Proc. Workshop on Data-Driven Ma-
chine Translation, pages 47?54, Toulouse, France,
July.
Sonja Nie?en and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181?204, June.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
47
Table 6: Examples of Serbian?English translations with and without transformations
to je suvishe skupo . ? to biti suvishe skup . ? to SG3 biti suvishe skup .
base forms verb treatment
? Sr ? En (baseline) ? Sr? ? En ? Sr? ? En
it is it is it is
too UNKNOWN skupo . too expensive . too expensive .
on ne igra . ? on ne igrati . ? on ne SG3 igrati .
base forms verb treatment
? Sr ? En (baseline) ? Sr? ? En ? Sr? ? En
he he does not . he do not play . he does not play .
da , ali nemam ? da , ali nemati ? da , ali SG1 ne imati
mnogo vremena . base forms mnogo vreme . verb treatment mnogo vreme .
? Sr ? En (baseline) ? Sr? ? En ? Sr? ? En
yes , but I have yes , but not yes , but I have not got
much time . much time . much time .
Table 7: Examples of English?Serbian translations with and without transformations
you should not ? you should not
read in bed . remove articles read in bed .
? En ? Sr (baseline) ? En? ? Sr reference translation:
treba ne ne bi trebalo ne bi trebalo
c?itam u krevet . c?itate u krevet . da c?itate u krevetu .
have a drink . ? have drink .
remove articles
? En ? Sr (baseline) ? En? ? Sr reference translation:
imate pijem . uzmi nes?to za pic?e . uzmite nes?to za pic?e .
machine translation. In Proc. 40th Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. 40th Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
M. Popovic? and H. Ney. 2004. Towards the use of word
stems and suffixes for statistical machine translation.
In Proc. 4th Int. Conf. on Language Resources and
Evaluation (LREC), pages 1585?1588, Lisbon, Portu-
gal, May.
M. Popovic?, S. Jovic?ic?, and Z. ?Saric?. 2004. Statistical
machine translation of Serbian-English. In Proc. of
Int. Workshop on Speech and Computer (SPECOM),
pages 410?414, St. Petersburg, Russia, September.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
pages 133?142, Sommerset, NJ.
48
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 167?174,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Novel Reordering Approaches in Phrase-Based Statistical Machine
Translation
Stephan Kanthak, David Vilar, Evgeny Matusov, Richard Zens, and Hermann Ney
The authors are with the Lehrstuhl fu?r Informatik VI,
Computer Science Department, RWTH Aachen University,
D-52056 Aachen, Germany.
E-mail: {kanthak,vilar,matusov,zens,ney}@informatik.rwth-aachen.de.
Abstract
This paper presents novel approaches to
reordering in phrase-based statistical ma-
chine translation. We perform consistent
reordering of source sentences in train-
ing and estimate a statistical translation
model. Using this model, we follow a
phrase-based monotonic machine transla-
tion approach, for which we develop an ef-
ficient and flexible reordering framework
that allows to easily introduce different re-
ordering constraints. In translation, we
apply source sentence reordering on word
level and use a reordering automaton as in-
put. We show how to compute reordering
automata on-demand using IBM or ITG
constraints, and also introduce two new
types of reordering constraints. We further
add weights to the reordering automata.
We present detailed experimental results
and show that reordering significantly im-
proves translation quality.
1 Introduction
Reordering is of crucial importance for machine
translation. Already (Knight et al, 1998) use full un-
weighted permutations on the level of source words
in their early weighted finite-state transducer ap-
proach which implemented single-word based trans-
lation using conditional probabilities. In a refine-
ment with additional phrase-based models, (Kumar
et al, 2003) define a probability distribution over
all possible permutations of source sentence phrases
and prune the resulting automaton to reduce com-
plexity.
A second category of finite-state translation ap-
proaches uses joint instead of conditional probabili-
ties. Many joint probability approaches originate in
speech-to-speech translation as they are the natural
choice in combination with speech recognition mod-
els. The automated transducer inference techniques
OMEGA (Vilar, 2000) and GIATI (Casacuberta et
al., 2004) work on phrase level, but ignore the re-
ordering problem from the view of the model. With-
out reordering both in training and during search,
sentences can only be translated properly into a lan-
guage with similar word order. In (Bangalore et al,
2000) weighted reordering has been applied to tar-
get sentences since defining a permutation model on
the source side is impractical in combination with
speech recognition. In order to reduce the computa-
tional complexity, this approach considers only a set
of plausible reorderings seen on training data.
Most other phrase-based statistical approaches
like the Alignment Template system of Bender
et al (2004) rely on (local) reorderings which are
implicitly memorized with each pair of source and
target phrases in training. Additional reorderings on
phrase level are fully integrated into the decoding
process, which increases the complexity of the sys-
tem and makes it hard to modify. Zens et al (2003)
reviewed two types of reordering constraints for this
type of translation systems.
In our work we follow a phrase-based transla-
tion approach, applying source sentence reordering
on word level. We compute a reordering graph on-
demand and take it as input for monotonic trans-
lation. This approach is modular and allows easy
introduction of different reordering constraints and
probabilistic dependencies. We will show that it per-
forms at least as well as the best statistical machine
translation system at the IWSLT Evaluation.
167
In the next section we briefly review the basic
theory of our translation system based on weighted
finite-state transducers (WFST). In Sec. 3 we in-
troduce new methods for reordering and alignment
monotonization in training. To compare differ-
ent reordering constraints used in the translation
search process we develop an on-demand com-
putable framework for permutation models in Sec. 4.
In the same section we also define and analyze un-
restricted and restricted permutations with some of
them being first published in this paper. We con-
clude the paper by presenting and discussing a rich
set of experimental results.
2 Machine Translation using WFSTs
Let fJ1 and eIi be two sentences from a source and
target language. Assume that we have word level
alignments A of all sentence pairs from a bilingual
training corpus. We denote with e?J1 the segmenta-
tion of a target sentence eI1 into J phrases such that
fJ1 and e?J1 can be aligned to form bilingual tuples
(fj , e?j). If alignments are only functions of target
words A? : {1, . . . , I} ? {1, . . . , J}, the bilingual
tuples (fj , e?j) can be inferred with e. g. the GIATI
method of (Casacuberta et al, 2004), or with our
novel monotonization technique (see Sec. 3). Each
source word will be mapped to a target phrase of one
or more words or an ?empty? phrase ?. In particular,
the source words which will remain non-aligned due
to the alignment functionality restriction are paired
with the empty phrase.
We can then formulate the problem of finding the
best translation e?I1 of a source sentence fJ1 :
e?I1 = argmax
eI1
Pr(fJ1 , e
I
1)
= argmax
e?J1
?
A?A
Pr(fJ1 , e?
J
1 , A)
?= argmax
e?J1
max
A?A
Pr(A) ? Pr(fJ1 , e?
J
1 |A)
?= argmax
e?J1
max
A?A
?
fj :j=1...J
Pr(fj , e?j |f
j?1
1 , e?
j?1
1 , A)
= argmax
e?J1
max
A?A
?
fj :j=1...J
p(fj , e?j |f
j?1
j?m, e?
j?1
j?m, A)
In other words: if we assume a uniform distri-
bution for Pr(A), the translation problem can be
mapped to the problem of estimating an m-gram lan-
guage model over a learned set of bilingual tuples
(fj , e?j). Mapping the bilingual language model to a
WFST T is canonical and it has been shown in (Kan-
thak et al, 2004) that the search problem can then be
rewritten using finite-state terminology:
e?I1 = project-output(best(fJ1 ? T )) .
This implementation of the problem as WFSTs may
be used to efficiently solve the search problem in
machine translation.
3 Reordering in Training
When the alignment function A? is not monotonic,
target language phrases e? can become very long.
For example in a completely non-monotonic align-
ment all target words are paired with the last aligned
source word, whereas all other source words form
tuples with the empty phrase. Therefore, for lan-
guage pairs with big differences in word order, prob-
ability estimates may be poor.
This problem can be solved by reordering either
source or target training sentences such that align-
ments become monotonic for all sentences. We
suggest the following consistent source sentence re-
ordering and alignment monotonization approach in
which we compute optimal, minimum-cost align-
ments.
First, we estimate a cost matrix C for each sen-
tence pair (fJ1 , eI1). The elements of this matrix cij
are the local costs of aligning a source word fj to a
target word ei. Following (Matusov et al, 2004), we
compute these local costs by interpolating state oc-
cupation probabilities from the source-to-target and
target-to-source training of the HMM and IBM-4
models as trained by the GIZA++ toolkit (Och et al,
2003). For a given alignment A ? I ? J , we define
the costs of this alignment c(A) as the sum of the
local costs of all aligned word pairs:
c(A) =
?
(i,j)?A
cij (1)
The goal is to find an alignment with the minimum
costs which fulfills certain constraints.
3.1 Source Sentence Reordering
To reorder a source sentence, we require the
alignment to be a function of source words A1:
{1, . . . , J} ? {1, . . . , I}, easily computed from the
cost matrix C as:
A1(j) = argmini cij (2)
168
We do not allow for non-aligned source words. A1
naturally defines a new order of the source words fJ1
which we denote by f?J1 . By computing this permu-
tation for each pair of sentences in training and ap-
plying it to each source sentence, we create a corpus
of reordered sentences.
3.2 Alignment Monotonization
In order to create a ?sentence? of bilingual tuples
(f?J1 , e?
J
1 ) we required alignments between reordered
source and target words to be a function of target
words A2 : {1, . . . , I} ? {1, . . . , J}. This align-
ment can be computed in analogy to Eq. 2 as:
A2(i) = argminj c?ij (3)
where c?ij are the elements of the new cost matrix
C? which corresponds to the reordered source sen-
tence. We can optionally re-estimate this matrix by
repeating EM training of state occupation probabili-
ties with GIZA++ using the reordered source corpus
and the original target corpus. Alternatively, we can
get the cost matrix C? by reordering the columns of
the cost matrix C according to the permutation given
by alignment A1.
In alignment A2 some target words that were pre-
viously unaligned in A1 (like ?the? in Fig. 1) may
now still violate the alignment monotonicity. The
monotonicity of this alignment can not be guaran-
teed for all words if re-estimation of the cost matri-
ces had been performed using GIZA++.
The general GIATI technique (Casacuberta et al,
2004) is applicable and can be used to monotonize
the alignment A2. However, in our experiments
the following method performs better. We make
use of the cost matrix representation and compute
a monotonic minimum-cost alignment with a dy-
namic programming algorithm similar to the Lev-
enshtein string edit distance algorithm. As costs of
each ?edit? operation we consider the local align-
ment costs. The resulting alignment A3 represents
a minimum-cost monotonic ?path? through the cost
matrix. To make A3 a function of target words we
do not consider the source words non-aligned in A2
and also forbid ?deletions? (?many-to-one? source
word alignments) in the DP search.
An example of such consistent reordering and
monotonization is given in Fig. 1. Here, we re-
order the German source sentence based on the ini-
tial alignment A1, then compute the function of tar-
get words A2, and monotonize this alignment to A3
the very beginning of May would suit me .
the very beginning of May would suit me .
sehr gut Anfang Mai w?rde passen mir .
sehr gut Anfang Mai w?rde passen mir .
the very beginning of May would suit me .
mir sehrw?rde gut Anfang Mai passen .
.Mai|of_May w?rde|would passen|suit mir|me |.sehr|the_very gut|$ Anfang|beginning
A
A
A1
2
3
Figure 1: Example of alignment, source sentence re-
ordering, monotonization, and construction of bilin-
gual tuples.
with the dynamic programming algorithm. Fig. 1
also shows the resulting bilingual tuples (f?j , e?j).
4 Reordering in Search
When searching the best translation e?J1 for a given
source sentence fJ1 , we permute the source sentence
as described in (Knight et al, 1998):
e?I1 = project-output(best(permute(fJ1 ) ? T ))
Permuting an input sequence of J symbols re-
sults in J ! possible permutations and representing
the permutations as a finite-state automaton requires
at least 2J states. Therefore, we opt for computing
the permutation automaton on-demand while apply-
ing beam pruning in the search.
4.1 Lazy Permutation Automata
For on-demand computation of an automaton in the
flavor described in (Kanthak et al, 2004) it is suffi-
cient to specify a state description and an algorithm
that calculates all outgoing arcs of a state from the
state description. In our case, each state represents
a permutation of a subset of the source words fJ1 ,
which are already translated.
This can be described by a bit vector bJ1 (Zens
et al, 2002). Each bit of the state bit vector corre-
sponds to an arc of the linear input automaton and is
set to one if the arc has been used on any path from
the initial to the current state. The bit vectors of two
states connected by an arc differ only in a single bit.
Note that bit vectors elegantly solve the problem of
recombining paths in the automaton as states with
169
the same bit vectors can be merged. As a result, a
fully minimized permutation automaton has only a
single initial and final state.
Even with on-demand computation, complexity
using full permutations is unmanagable for long sen-
tences. We further reduce complexity by addition-
ally constraining permutations. Refer to Figure 2 for
visualizations of the permutation constraints which
we describe in the following.
4.2 IBM Constraints
The IBM reordering constraints are well-known in
the field of machine translation and were first de-
scribed in (Berger et al, 1996). The idea behind
these constraints is to deviate from monotonic trans-
lation by postponing translations of a limited num-
ber of words. More specifically, at each state we
can translate any of the first l yet uncovered word
positions. The implementation using a bit vector is
straightforward. For consistency, we associate win-
dow size with the parameter l for all constraints pre-
sented here.
4.3 Inverse IBM Constraints
The original IBM constraints are useful for a large
number of language pairs where the ability to skip
some words reflects the differences in word order
between the two languages. For some other pairs,
it is beneficial to translate some words at the end of
the sentence first and to translate the rest of the sen-
tence nearly monotonically. Following this idea we
can define the inverse IBM constraints. Let j be the
first uncovered position. We can choose any posi-
tion for translation, unless l ? 1 words on positions
j? > j have been translated. If this is the case we
must translate the word in position j. The inverse
IBM constraints can also be expressed by
invIBM(x) = transpose(IBM(transpose(x))) .
As the transpose operation can not be computed
on-demand, our specialized implementation uses bit
vectors bJ1 similar to the IBM constraints.
4.4 Local Constraints
For some language pairs, e.g. Italian ? English,
words are moved only a few words to the left or
right. The IBM constraints provide too many alter-
native permutations to chose from as each word can
be moved to the end of the sentence. A solution that
allows only for local permutations and therefore has
a)
0000 10001 11002 11103 11114
b)
0000
10001
01002 1100
2
10103
1
0110
3
11103
110141
01114
1111
4
13
2
10114 2
c)
0000
10001
01002
0010
3
0001
4 10014
1010
3 11002
1
1
1
1101
2
11113
11102 4
43
d)
0000
10001
01002 1100
2
10103
1
11103
11014 1111
43
2
Figure 2: Permutations of a) positions j = 1, 2, 3, 4
of a source sentence f1f2f3f4 using a window size
of 2 for b) IBM constraints, c) inverse IBM con-
straints and d) local constraints.
very low complexity is given by the following per-
mutation rule: the next word for translation comes
from the window of l positions1 counting from the
first yet uncovered position. Note, that the local con-
straints define a true subset of the permutations de-
fined by the IBM constraints.
4.5 ITG Constraints
Another type of reordering can be obtained using In-
version Transduction Grammars (ITG) (Wu, 1997).
These constraints are inspired by bilingual bracket-
ing. They proved to be quite useful for machine
translation, e.g. see (Bender et al, 2004). Here,
we interpret the input sentence as a sequence of seg-
ments. In the beginning, each word is a segment of
its own. Longer segments are constructed by recur-
sively combining two adjacent segments. At each
1both covered and uncovered
170
Chinese English Japanese English Italian English
train sentences 20 000 20 000 66107
words 182 904 160 523 209 012 160 427 410 275 427 402
singletons 3 525 2 948 4 108 2 956 6 386 3 974
vocabulary 7 643 6 982 9 277 6 932 15 983 10 971
dev sentences 506 506 500
words 3 515 3 595 4 374 3 595 3 155 3 253
sentence length (avg/max) 6.95 / 24 7.01 / 29 8.64 / 30 7.01 / 29 5.79 / 24 6.51 / 25
test sentences 500 500 506
words 3 794 ? 4 370 ? 2 931 3 595
sentence length (avg/max) 7.59 / 62 7.16 / 71 8.74 / 75 7.16 / 71 6.31 / 27 6.84 / 28
Table 1: Statistics of the Basic Travel Expression (BTEC) corpora.
combination step, we either keep the two segments
in monotonic order or invert the order. This pro-
cess continues until only one segment for the whole
sentence remains. The on-demand computation is
implemented in spirit of Earley parsing.
We can modify the original ITG constraints to
further limit the number of reorderings by forbid-
ding segment inversions which violate IBM con-
straints with a certain window size. Thus, the re-
sulting reordering graph contains the intersection of
the reorderings with IBM and the original ITG con-
straints.
4.6 Weighted Permutations
So far, we have discussed how to generate the per-
mutation graphs under different constraints, but per-
mutations were equally probable. Especially for the
case of nearly monotonic translation it is make sense
to restrict the degree of non-monotonicity that we
allow when translating a sentence. We propose a
simple approach which gives a higher probability
to the monotone transitions and penalizes the non-
monotonic ones.
A state description bJ1 , for which the following
condition holds:
Mon(j) : bj? = ?(j
? ? j) ? 1 ? j? ? J
represents the monotonic path up to the word fj . At
each state we assign the probability ? to that out-
going arc where the target state description fullfills
Mon(j+1) and distribute the remaining probability
mass 1? ? uniformly among the remaining arcs. In
case there is no such arc, all outgoing arcs get the
same uniform probability. This weighting scheme
clearly depends on the state description and the out-
going arcs only and can be computed on-demand.
5 Experimental Results
5.1 Corpus Statistics
The translation experiments were carried out on the
Basic Travel Expression Corpus (BTEC), a multilin-
gual speech corpus which contains tourism-related
sentences usually found in travel phrase books.
We tested our system on the so called Chinese-to-
English (CE) and Japanese-to-English (JE) Supplied
Tasks, the corpora which were provided during the
International Workshop on Spoken Language Trans-
lation (IWSLT 2004) (Akiba et al, 2004). In ad-
dition, we performed experiments on the Italian-to-
English (IE) task, for which a larger corpus was
kindly provided to us by ITC/IRST. The corpus
statistics for the three BTEC corpora are given in
Tab. 1. The development corpus for the Italian-to-
English translation had only one reference transla-
tion of each Italian sentence. A set of 506 source
sentences and 16 reference translations is used as
a development corpus for Chinese-to-English and
Japanese-to-English and as a test corpus for Italian-
to-English tasks. The 500 sentence Chinese and
Japanese test sets of the IWSLT 2004 evaluation
campaign were translated and automatically scored
against 16 reference translations after the end of the
campaign using the IWSLT evaluation server.
5.2 Evaluation Criteria
For the automatic evaluation, we used the crite-
ria from the IWSLT evaluation campaign (Akiba et
al., 2004), namely word error rate (WER), position-
independent word error rate (PER), and the BLEU
and NIST scores (Papineni et al, 2002; Doddington,
2002). The two scores measure accuracy, i. e. larger
scores are better. The error rates and scores were
computed with respect to multiple reference transla-
171
 40
 42
 44
 46
 48
 50
 52
 54
 56
 58
 60
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
Figure 3: Word error rate [%] as a function of the reordering window size for different reordering constraints:
Japanese-to-English (left) and Chinese-to-English (right) translation.
tions, when they were available. To indicate this, we
will label the error rate acronyms with an m. Both
training and evaluation were performed using cor-
pora and references in lowercase and without punc-
tuation marks.
5.3 Experiments
We used reordering and alignment monotonization
in training as described in Sec. 3. To estimate the
matrices of local alignment costs for the sentence
pairs in the training corpus we used the state occupa-
tion probabilities of GIZA++ IBM-4 model training
and interpolated the probabilities of source-to-target
and target-to-source training directions. After that
we estimated a smoothed 4-gram language model on
the level of bilingual tuples fj , e?j and represented it
as a finite-state transducer.
When translating, we applied moderate beam
pruning to the search automaton only when using re-
ordering constraints with window sizes larger than 3.
For very large window sizes we also varied the prun-
ing thresholds depending on the length of the input
sentence. Pruning allowed for fast translations and
reasonable memory consumption without a signifi-
cant negative impact on performance.
In our first experiments, we tested the four re-
ordering constraints with various window sizes. We
aimed at improving the translation results on the de-
velopment corpora and compared the results with
two baselines: reordering only the source training
sentences and translation of the unreordered test sen-
tences; and the GIATI technique for creating bilin-
gual tuples (fj , e?j) without reordering of the source
sentences, neither in training nor during translation.
5.3.1 Highly Non-Monotonic Translation (JE)
Fig. 3 (left) shows word error rate on the
Japanese-to-English task as a function of the win-
dow size for different reordering constraints. For
each of the constraints, good results are achieved
using a window size of 9 and larger. This can be
attributed to the Japanese word order which is very
different from English and often follows a subject-
object-verb structure. For small window sizes, ITG
or IBM constraints are better suited for this task, for
larger window sizes, inverse IBM constraints per-
form best. The local constraints perform worst and
require very large window sizes to capture the main
word order differences between Japanese and En-
glish. However, their computational complexity is
low; for instance, a system with local constraints
and window size of 9 is as fast (25 words per sec-
ond) as the same system with IBM constraints and
window size of 5. Using window sizes larger than
10 is computationally expensive and does not sig-
nificantly improve the translation quality under any
of the constraints.
Tab. 2 presents the overall improvements in trans-
lation quality when using the best setting: inverse
IBM constraints, window size 9. The baseline with-
out reordering in training and testing failed com-
pletely for this task, producing empty translations
for 37 % of the sentences2. Most of the original
alignments in training were non-monotonic which
resulted in mapping of almost all Japanese words to
? when using only the GIATI monotonization tech-
nique. Thus, the proposed reordering methods are of
crucial importance for this task.
2Hence a NIST score of 0 due to the brevity penalty.
172
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
BTEC Japanese-to-English (JE) dev
none 59.7 58.8 13.0 0.00
in training 57.8 39.4 14.7 3.27
+ 9-inv-ibm 40.3 32.1 45.1 8.59
+ rescoring* 39.1 30.9 53.2 9.93
BTEC Chinese-to-English (CE) dev
none 55.2 52.1 24.9 1.34
in training 54.0 42.3 23.0 4.18
+ 7-inv-ibm 47.1 39.4 34.5 6.53
+ rescoring* 48.3 40.7 39.1 8.11
Table 2: Translation results with optimal reorder-
ing constraints and window sizes for the BTEC
Japanese-to-English and Chinese-to-English devel-
opment corpora. *Optimized for the NIST score.
mWER mPER BLEU NIST
[%] [%] [%]
BTEC Japanese-to-English (JE) test
AT 41.9 33.8 45.3 9.49
WFST 42.1 35.6 47.3 9.50
BTEC Chinese-to-English (CE) test
AT 45.6 39.0 40.9 8.55
WFST 46.4 38.8 40.8 8.73
Table 3: Comparison of the IWSLT-2004 automatic
evaluation results for the described system (WFST)
with those of the best submitted system (AT).
Further improvements were obtained with a
rescoring procedure. For rescoring, we produced
a k-best list of translation hypotheses and used the
word penalty and deletion model features, the IBM
Model 1 lexicon score, and target language n-gram
models of the order up to 9. The scaling factors for
all features were optimized on the development cor-
pus for the NIST score, as described in (Bender et
al., 2004).
5.3.2 Moderately Non-Mon. Translation (CE)
Word order in Chinese and English is usually sim-
ilar. However, a few word reorderings over quite
large distances may be necessary. This is especially
true in case of questions, in which question words
like ?where? and ?when? are placed at the end of
a sentence in Chinese. The BTEC corpora contain
many sentences with questions.
The inverse IBM constraints are designed to per-
form this type of reordering (see Sec. 4.3). As shown
in Fig. 3, the system performs well under these con-
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
none 25.6 22.0 62.1 10.46
in training 28.0 22.3 58.1 10.32
+ 4-local 26.3 20.3 62.2 10.81
+ weights 25.3 20.3 62.6 10.79
+ 3-ibm 27.2 20.5 61.4 10.76
+ weights 25.2 20.3 62.9 10.80
+ rescoring* 22.2 19.0 69.2 10.47
Table 4: Translation results with optimal reordering
constraints and window sizes for the test corpus of
the BTEC IE task. *Optimized for WER.
straints already with relatively small window sizes.
Increasing the window size beyond 4 for these con-
straints only marginally improves the translation er-
ror measures for both short (under 8 words) and long
sentences. Thus, a suitable language-pair-specific
choice of reordering constraints can avoid the huge
computational complexity required for permutations
of long sentences.
Tab. 2 includes error measures for the best setup
with inverse IBM constraints with window size of 7,
as well as additional improvements obtained by a k-
best list rescoring.
The best settings for reordering constraints and
model scaling factors on the development corpora
were then used to produce translations of the IWSLT
Japanese and Chinese test corpora. These trans-
lations were evaluated against multiple references
which were unknown to the authors. Our system
(denoted with WFST, see Tab. 3) produced results
competitive with the results of the best system at the
evaluation campaign (denoted with AT (Bender et
al., 2004)) and, according to some of the error mea-
sures, even outperformed this system.
5.3.3 Almost Monotonic Translation (IE)
The word order in the Italian language does not
differ much from the English. Therefore, the abso-
lute translation error rates are quite low and translat-
ing without reordering in training and search already
results in a relatively good performance. This is re-
flected in Tab. 4. However, even for this language
pair it is possible to improve translation quality by
performing reordering both in training and during
translation. The best performance on the develop-
ment corpus is obtained when we constrain the re-
odering with relatively small window sizes of 3 to 4
and use either IBM or local reordering constraints.
173
On the test corpus, as shown in Tab. 4, all error mea-
sures can be improved with these settings.
Especially for languages with similar word order
it is important to use weighted reorderings (Sec. 4.6)
in order to prefer the original word order. Introduc-
tion of reordering weights for this task results in no-
table improvement of most error measures using ei-
ther the IBM or local constraints. The optimal prob-
ability ? for the unreordered path was determined
on the development corpus as 0.5 for both of these
constraints. The results on the test corpus using this
setting are also given in Tab. 4.
6 Conclusion
In this paper, we described a reordering framework
which performs source sentence reordering on word
level. We suggested to use optimal alignment func-
tions for monotonization and improvement of trans-
lation model training. This allowed us to translate
monotonically taking a reordering graph as input.
We then described known and novel reordering con-
straints and their efficient finite-state implementa-
tions in which the reordering graph is computed on-
demand. We also utilized weighted permutations.
We showed that our monotonic phrase-based trans-
lation approach effectively makes use of the reorder-
ing framework to produce quality translations even
from languages with significantly different word or-
der. On the Japanese-to-English and Chinese-to-
English IWSLT tasks, our system performed at least
as well as the best machine translation system.
Acknowledgement
This work was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5) and by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,
and J. Tsujii. 2004. Overview of the IWSLT04 Evalu-
ation Campaign. Proc. Int. Workshop on Spoken Lan-
guage Translation, pp. 1?12, Kyoto, Japan.
S. Bangalore and G. Riccardi. 2000. Stochastic Finite-
State Models for Spoken Language Machine Transla-
tion. Proc. Workshop on Embedded Machine Transla-
tion Systems, pp. 52?59.
O. Bender, R. Zens, E. Matusov, and H. Ney. 2004.
Alignment Templates: the RWTH SMT System. Proc.
Int. Workshop on Spoken Language Translation, pp.
79?84, Kyoto, Japan.
A. L. Berger, P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language Translation Apparatus and Method
of Using Context-based Translation Models. United
States Patent 5510981.
F. Casacuberta and E. Vidal. 2004. Machine Transla-
tion with Inferred Stochastic Finite-State Transducers.
Computational Linguistics, vol. 30(2):205-225.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality Using n-gram Co-Occurrence
Statistics. Proc. Human Language Technology Conf.,
San Diego, CA.
S. Kanthak and H. Ney. 2004. FSA: an Efficient and
Flexible C++ Toolkit for Finite State Automata using
On-demand Computation. Proc. 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pp. 510?517, Barcelona, Spain.
K. Knight and Y. Al-Onaizan. 1998. Translation with
Finite-State Devices. Lecture Notes in Artificial Intel-
ligence, Springer-Verlag, vol. 1529, pp. 421?437.
S. Kumar and W. Byrne. 2003. A Weighted Finite State
Transducer Implementation of the Alignment Template
Model for Statistical Machine Translation. Proc. Hu-
man Language Technology Conf. NAACL, pp. 142?
149, Edmonton, Canada.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric Word
Alignments for Statistical Machine Translation. Proc.
20th Int. Conf. on Computational Linguistics, pp. 219?
225, Geneva, Switzerland.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, vol. 29, number 1, pp. 19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Machine
Translation. Proc. 40th Annual Meeting of the Associ-
ation for Computational Linguistics, Philadelphia, PA,
pp. 311?318.
J. M. Vilar, 2000. Improve the Learning of Sub-
sequential Transducers by Using Alignments and Dic-
tionaries. Lecture Notes in Artificial Intelligence,
Springer-Verlag, vol. 1891, pp. 298?312.
D. Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
R. Zens, F. J. Och and H. Ney. 2002. Phrase-Based Sta-
tistical Machine Translation. In: M. Jarke, J. Koehler,
G. Lakemeyer (Eds.): KI - Conference on AI, KI 2002,
Vol. LNAI 2479, pp. 18-32, Springer Verlag.
R. Zens and H. Ney. 2003. A Comparative Study on
Reordering Constraints in Statistical Machine Trans-
lation. Proc. Annual Meeting of the Association
for Computational Linguistics, pp. 144?151, Sapporo,
Japan.
174
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 17?24, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Preprocessing and Normalization
for Automatic Evaluation of Machine Translation
Gregor Leusch and Nicola Ueffing and David Vilar and Hermann Ney
Lehrstuhl fu?r Informatik VI
RWTH Aachen University
D-52056 Aachen, Germany,
{leusch,ueffing,vilar,ney}@i6.informatik.rwth-aachen.de
Abstract
Evaluation measures for machine trans-
lation depend on several common meth-
ods, such as preprocessing, tokenization,
handling of sentence boundaries, and the
choice of a reference length. In this
paper, we describe and review some
new approaches to them and compare
these to state-of-the-art methods. We
experimentally look into their impact on
four established evaluation measures. For
this purpose, we study the correlation
between automatic and human evaluation
scores on three MT evaluation corpora.
These experiments confirm that the to-
kenization method, the reference length
selection scheme, and the use of sentence
boundaries we introduce will increase the
correlation between automatic and human
evaluation scores. We find that ignoring
case information and normalizing evalu-
ator scores has a positive effect on the
sentence level correlation as well.
1 Introduction
Machine translation (MT), as any other natural lan-
guage processing (NLP) research subject, depends
on the evaluation of its results. Unfortunately,
human evaluation of MT system output is a time
consuming and expensive task. This is why auto-
matic evaluation is preferred to human evaluation in
the research community.
Over the last years, a manifold of automatic evalu-
ation measures has been proposed and studied. This
underlines the importance, but also the complexity
of finding a suitable evaluation measure for MT.
We will give a short overview of some measures in
section 2 of this paper.
Although most of these measures share similar
ideas and foundation, we observe that researchers
tend to approach problems common to several
measures differently from each other. A noteworthy
example here is the determination of a translation
reference length.
In section 3, we will have a look onto structural
similarities and differences among several measures,
focussing on common steps. We will show that
decisions taken about them can be as important to
the outcome of an evaluation, as the choice of the
evaluation measure itself.
To this end, we will study the performance
of each error measure and setting by comparison
with human evaluation on three different evaluation
tasks in section 4. These experiments will show
that sophisticated tokenization as well as adding
sentence boundaries and a good choice for the
reference lengths will improve correlation between
automatic and human evaluation significantly. Case
normalization and evaluator normalization are help-
ful only when evaluating on sentence level; system
level evaluation is not affected by these methods.
After a discussion of these results in section 5, we
will conclude this paper in section 6.
2 Automatic evaluation measures
The majority of MT evaluation approaches are based
on the distance or similarity of MT candidate output
to a set of reference translations, i.e. to sentences
which are known to be correct. The lower this
distance is, or the higher the similarity, the better the
17
candidate translations are considered to be, and thus
the better the MT system.
2.1 Evaluation measures studied
Out of the vast amount of measures, we will focus
on the following measures that are widely used in
research and in evaluation campaigns: WER, PER,
BLEU, and NIST.
Let a test set consist of k = 1, . . . ,K candidate
sentences Ek generated by an MT system. For
each candidate sentence Ek, we have a set of r =
1, . . . , Rk reference sentences E?r,k. Let Ik denote
the length, and I?k the reference length for each
sentence Ek. We will explain in section 3.3 how the
reference length is calculated.
With this, we write the total candidate length over
the corpus as I? :=
?
k Ik, and the total reference
length as I?? :=
?
k I
?
k .
Let nem1 ,k denote the count of the m-gram e
m
1
within the candidate sentence Ek; similarly let
n?em1 ,r,k denote the same count within the reference
sentence E?r,k. The total m-gram count over the
corpus is then n?m :=
?
k
?
em1 ?Ek
nem1 ,k.
2.1.1 WER
The word error rate is defined as the Levenshtein
distance dL(Ek, E?r,k) between a candidate sentence
Ek and a reference sentence E?r,k, divided by the
reference length I?k for normalization.
For a whole candidate corpus with multiple
references, we define the WER to be:
WER :=
1
I??
?
k
min
r
dL
(
Ek, E?r,k
)
Note that the WER of a single sentence can be
calculated as the WER for a corpus of size K = 1.
2.1.2 PER
The position independent error rate (Tillmann et
al., 1997) ignores the ordering of the words within
a sentence. Independent of the word position, the
minimum number of deletions, insertions, and
substitutions to transform the candidate sentence
into the reference sentence is calculated. Using
the counts ne,r, n?e,r,k of a word e in the candidate
sentence Ek, and the reference sentence E?r,k, we
can calculate this distance as
dPER
(
Ek, E?r,k
)
:=
1
2
(?
?Ik?I?k
?
?+
?
e
?
?ne,k?n?e,r,k
?
?
)
This distance is then normalized into an error rate,
the PER, as described in section 2.1.1.
A promising approach is to compare bigram or
arbitrary m-gram count vectors instead of unigram
count vectors only. This will take into account the
ordering of the words within a sentence implicitly,
although not as strong as the WER does.
2.1.3 BLEU
BLEU (Papineni et al, 2001) is a precision
measure based on m-gram count vectors. The
precision is modified such that multiple references
are combined into a single m-gram count vector,
n?e,k := maxr n?e,r,k. Multiple occurrences of an
m-gram in the candidate sentence are counted as
correct only up to the maximum occurrence count
within the reference sentences. Typically, m =
1, . . . , 4.
To avoid a bias towards short candidate sentences
consisting of ?safe guesses? only, sentences shorter
than the reference length will be penalized with a
brevity penalty.
BLEU := lpBLEU ? gm
m
{
1
sm+n?m
?
(
sm+
?
k
?
em1 ?Ek
min
(
nem1 ,k , n?em1 ,k
))
}
with the geometric mean gm and a brevity penalty
lpBLEU := min
(
1 , exp
(
1 ?
I??
I?
))
In the original BLEU definition, the smoothing
term sm is zero. To allow for sentence-wise
evaluation, Lin and Och (2004) define the BLEU-S
measure with s1 := 1 and sm>1 := 0. We have
adopted this technique for this study.
2.1.4 NIST
The NIST score (Doddington, 2002) extends
the BLEU score by taking information weights of
the m-grams into account. The NIST information
weight is defined as
Info(em1 ) := ?
(
log2 ??nem1 ? log2
??nem?11
)
with ??nem1 :=
?
k,r
n?en1 ,k,r.
Note that the weight of a phrase occurring
in many references sentence for a candidate is
considered to be lower than the weight of a phrase
occurring only once!
18
The NIST score is the sum over all information
counts of the co-occurring m-grams, summed up
separately for each m = 1, . . . , 5 and normalized
by the total m-gram count.
NIST := lpNIST ?
?
m
(
1
n?m
?
?
k
?
em1 ?Ek
min
(
nem1 ,k , n?em1 ,k
)
? Info(em1 )
)
As in BLEU, there is a brevity penalty to avoid a
bias towards short candidates:
lpNIST := exp
(
? ? log22 min
(
1 ,
I?
I??
))
where ? := ? log2 2
log22 3
Due to the information weights, the value of the
NIST score depends highly on the selection of the
reference corpus. This must be taken into account
when comparing NIST scores of different evaluation
campaigns.
2.2 Other measures
Lin and Och (2004) introduce a family of three
measures named ROUGE. ROUGE-S is a skip-
bigram F-measure. ROUGE-L and ROUGE-W are
measures based on the length of the longest common
subsequence of the sentences. ROUGE-S has a
structure similar to the bigram PER presented here.
We expect ROUGE-L and ROUGE-W to have similar
properties to WER.
In (Leusch et al, 2003), we have described
INVWER, a word error rate enhanced by block
transposition edit operations. As structure and
scores of INVWER are similar to WER, we have
omitted INVWER experiments in this paper.
3 Preprocessing and normalization
Although the general idea is clear, there are still
several details to be specified when implementing
and using an automatic evaluation measure. We are
going to investigate the following problems:
The first detail we have to state more precisely is
the term ?word? in the above formulae. A common
approach for western languages is to consider spaces
as separators of words. The role of punctuation
marks in tokenization is arguable though. A
punctuation mark can separate words, it can be part
of a word, and it can be a word of its own. Equally
it can be irrelevant at all for evaluation.
On the same lines it is to be specified whether
we consider words to be equal if they differ only
with respect to upper and lower case. For the
IWSLT evaluation, (Paul et al, 2004) give an
introduction to how the handling of punctuation
and case information may affect automatic MT
evaluation.
Also, a method to calculate the ?reference
length? must specified if there are multiple reference
sentences of different length.
Since we want to compare automatic evaluation
with human evaluation, we have to clarify some
questions about assessing human evaluation as well:
Large evaluation tasks are usually distributed to
several human evaluators. To smooth evaluation
noise, it is common practice to have each candidate
sentence evaluated by at least two human judges in-
dependently. Therefore there are several evaluation
scores for each candidate sentence. We require a
single score for each system, though. Consequently,
we have to specify how to combine the evaluator
scores into sentence scores and then the sentence
scores into a system score.
Different definitions of this will have a significant
impact on automatic and human evaluation scores.
3.1 Tokenization and punctuation
The importance of punctuation as well as the
strictness of punctuation rules depends on the
language. In most western languages, correct
punctuation can vastly improve the legibility of
texts. Marks like full stop or comma separate words.
Other marks like apostrophes and hyphens can be
used to join words, forming new words by this. For
example, the spelling ?There?s? is a contraction of
?There is?.
Similar phenomena can be found in other lan-
guages, although the set of critical characters may
vary. Even when evaluating English translations, the
candidate sentences may contain source language
parts like proper names which should thus be treated
according to the source language.
From the viewpoint of an automatic evaluation
measure, we have to decide which units we would
consider to be words of their own.
We have studied four tokenization methods. The
simplest method is keeping the original sentences,
and considering only spaces as word separators.
Moreover, we can consider all punctuation marks to
separate words but remove them completely then.
The mteval tool (Papineni, 2002) improves this
19
Table 1: Tokenization methods studied
? Original candidate
Powell said: "We?d not be
alone; that?s for sure."
? Remove punctuation
Powell said We d not be alone
that s for sure
? Tokenization of punctuation (mteval)
Powell said : " We?d not be
alone ; that?s for sure . "
? Tokenization and treatment of abbreviations
and contractions
Powell said : " we would not be
alone ; that is for sure . "
scheme by keeping all punctuation marks as separate
words except for decimal points and hyphens
joining composita. We have extended this scheme
by implementing a treatment of common English
contractions. Table 1 illustrates these methods.
3.2 Case sensitivity
In western languages, maintaining correct upper
and lower case can improve the readability of a
text. Unfortunately, though the case of a word
depends on the word class, classification is not
always unambiguous. What is more, the first word
in a sentence is always written in upper case. This
lowers the significance of case information in MT
evaluation, as even a valid reordering of words
between candidate and reference sentence may lead
to conflicting cases. Consequently, we investigated
if and how case information can be exploited for
automatic evaluation.
3.3 Reference length
Each automatic evaluation measure we have taken
into account depends on the calculation of a refer-
ence length: WER, PER, and ROUGE are normalized
by it, whereas NIST or BLEU incorporate it for
the determination of the brevity penalty. In MT
evaluation practise, there are multiple reference
sentences for each candidate sentence, with different
lengths each. It is thus not intuitively clear what the
?reference length? is.
A simple choice here is the average length of the
reference sentences. Though this is modus operandi
for NIST, it is problematic with brevity penalty or F-
measure based scores, as even candidate sentences
that are identical to a shorter-than-average reference
sentence ? which we would intuitively consider to be
?optimal? ? will then receive a sub-optimal score.
BLEU incorporates a different method for the
determination of the reference length in its default
implementation: Reference length here is the
reference sentence length which is closest to the
candidate length. If there is more than one the
shortest of them is chosen.
For measures based on the comparison of single
sentences such as WER, PER, and ROUGE, at least
two more methods deserve consideration:
? The average length of the sentences with the
lowest absolute distance or highest similarity
to the candidate sentence. We call this method
?average nearest-sentence length?.
? The length of the sentence with the lowest
relative error rate or the highest relative
similarity. We call this method ?best length?.
Note that when using this method, not the
minimum absolute distance is used for the
error rate, but the distance that leads to
minimum relative error.
Other strategies studied by us, e.g. minimum
length of the reference sentences, did not show
any theoretical or experimental advantage over the
methods mentioned here. Thus we will not discuss
them in this paper.
3.4 Sentence boundaries
The position of a word within a sentence can be quite
significant for the correctness of the sentence.
WER, INVWER, and ROUGE-L take into account
the ordering explicitly. This is not the case with n-
PER, BLEU, or NIST, although the positions of inner
words are regarded implicitly by m-gram overlap.
To model the position of words at the initial or the
end of a sentence, one can enclose the sentence with
artificial sentence boundary words. Although this
is a common approach in language modelling, it
has to our knowledge not yet been applied to MT
evaluation.
3.5 Evaluator normalization
For human evaluation, it has to be specified how to
handle evaluator bias, and how to combine sentence
scores into system scores.
Regarding evaluator bias, even accurate evalua-
tion guidelines will not prevent a measurable dis-
crepancy between the scores assigned by different
human evaluators.
The 2003 TIDES/MT evaluation may serve as
an example here: Since the candidate sentences of
20
54321
0.0
0.2
0.4
0.6
0.8
1.0
Rel
ativ
e as
sess
men
t co
unt
Evaluator
Figure 1: Distribution of adequacy assessments for
each human evaluator. TIDES CE corpus.
the participating systems were randomly distributed
among ten human evaluators, one would expect the
assessed scores to be independent of the evaluator.
Figure 1 indicates that this is indeed not the case,
as the evaluators can clearly be distinguished by the
amount of good and bad marks they assessed.
(0, 1) evaluator normalization overcomes this
bias: For each human evaluator the average sentence
score given by him or her and its variance are
calculated. These assignments are then normalized
to (0, 1) expectation and standard deviation (Dod-
dington, 2003), separately for each evaluator.
Evaluator normalization should be unnecessary
for system evaluation, as the evaluator biases
tend to cancel out over the large amount of
candidate sentences if the alignment of evaluators
and systems is random enough. Moreover, with
(0, 1) normalization the calculated system scores are
relative, not absolute scores. As such they can only
be compared with scores out of the same evaluation.
Whereas the assessments by the human evaluators
are given on the sentence level, our interest may
lie on the evaluation of whole candidate systems.
Depending on the number of assessments per
candidate sentence, different combination methods
for the sentence scores can be considered for this,
e.g. mean or median. As our data consisted only
of two or three human assessments per sentence, we
have only applied the mean in our experiments.
It has to be defined how a system score is
calculated from the sentence scores. All of the
automatic evaluation measures implicitly weight the
candidate sentences by their length. Consequently,
we applied for the human evaluation scores a
weighting by length on sentence level as well.
Table 2: Corpus statistics
TIDES CE TIDES AE BTEC CE
Source language Chinese Arabic Chinese
Target language English English English
Sentences 919 663 500
Running words 25784 17763 3632
Punctuation marks 3760 2698 610
Ref. translations 4 4 16
Avg. ref. length 28.1 26.8 7.3
Candidate systems 7 6 11
4 Experimental results
To assess the impact of the mentioned preprocessing
steps, we calculated scores for several automatic
evaluation measures with varying preprocessing,
reference length calculation, etc. on three eval-
uation test sets from international MT evaluation
campaigns. We then compared these automatic eval-
uation results with human evaluation of adequacy
and fluency by determining a correlation coefficient
between human and automatic evaluation. We
chose Pearson?s r for this. Although all evaluation
measures were calculated using length weighting,
we did not do any weighting when calculating the
sentence level correlation.
Regarding the m-gram PER, we had studied m-
gram lengths of up to 8 both separately and in com-
bination with shorter m-gram lengths in previous
experiments. However, an m-gram length of greater
than 4 did not show noteworthy correlation. For this,
we will leave out these results in this paper.
For the sake of clarity, we will also leave
out measures that behave very similarly to akin
measures e.g. INVWER and WER, 2-PER and 1-
PER, or BLEU and BLEU-S.
Since WER and PER are error measures, whereas
BLEU and NIST are similarity measures, the
correlation coefficients with human evaluation will
have opposite signs. For convenience, we will look
at the absolute coefficients only.
4.1 Corpora
From the 2003 TIDES evaluation campaign we
included both the Chinese-English and the Arabic-
English test corpus in our experiments. Both were
provided with adequacy and fluency scores between
1 and 5 for seven and six candidate sets respectively.
As we wanted to perform experiments on a corpus
with a larger amount of MT systems, we also
included the IWSLT BTEC 2004 Chinese-English
21
evaluation (Akiba et al, 2004). We restricted our
experiments to the eleven MT systems that had been
trained on a common training corpus.
Corpus statistics can be found in table 2.
4.2 Experimental baseline
In our first experiment we studied the correlation
of the different evaluation measures with human
evaluation at ?baseline? conditions. These included
no sentence boundaries, but tokenization with
treatment of abbreviations, see table 1. For
sentence evaluation, conditions included evaluator
normalization. Case information was removed. We
used these settings in the other experiments, too, if
not stated otherwise.
Figure 2 shows the correlation between automatic
and human scores. On the TIDES corpora the
system level correlation is particularly high, at a
moderate sentence level correlation. We assume
the latter is due to the poor sentence inter-annotator
agreement on these corpora, which is then smoothed
out on system level. On the BTEC corpus
a high sentence level correlation accompanies a
significantly lower system level correlation. Note
that due to the much lower number of samples on
the system level (e.g. 5 vs. 5500), small changes
in the sentence level correlation are more likely to
be significant than such changes on system level.
We have verified these effects by inspecting the rank
correlation on both levels, as well as by experiments
on other corpora. Although these experiments
support our findings, we have omitted results here
WERPER BLEUSNIST l AdequacyFluency
TIDESCE TIDESAE BTECCE0.0
0.2
0.4
0.6
l l l l
l l l l
l
l
l
l
TIDESCE TIDESAE BTECCE0.0
0.20
.40.6
0.81
.0
l l l l
l l l l l
l
l
l
Figure 2: Pearson correlation coefficient between
automatic and human evaluation. Bars indicate
correlation with adequacy, circles with fluency
score.
Left: sentence, right: system level correlation.
W WERP PER B BLEUS ll no normalizationnormalization
0.0
0.2
0.4
0.6
l l l l l
l l l l l l l
TIDESCE TIDESAE
W P B W P B
0.00
.20.4
0.60
.81.0 l l l l l l l l l l l l
TIDESCE TIDESAE
W P B W P B
Figure 3: Effect of evaluator normalization.
Left: sentence, right: system level correlation.
W WERP PER B BLEUS ll use caseignore case
0.0
0.2
0.4
0.6
ll ll ll
ll ll l
l l
l
l
l l
l
TIDESCE TIDESAE BTECCE
W P B W P B W P B
0.00
.20.4
0.60
.81.0 l
l
ll
ll
ll ll ll ll
ll
ll
TIDESCE TIDESAE BTECCE
W P B W P B W P B
Figure 4: Effect of case normalization.
Left: sentence, right: system level correlation.
for the sake of clarity.
4.3 Evaluator normalization
We studied the effect of (0, 1)-normalization of
scores assigned by human evaluators. The NIST
measure showed a behavior very similar to that of
the other measures and is thus left out in the graph.
The correlation of all automatic measures both with
fluency and with adequacy increases significantly
at sentence level (figure 3). We do not notice a
positive effect on system level, which confirms the
assumption stated in section 3.5.
4.4 Tokenization and case normalization
The impact of case information was analyzed in our
next experiment. Figure 4 (again without the NIST
measure as it shows a similar behavior to the other
measures) indicates that it is advisable to disregard
case information when looking into adequacy on
sentence level. Surprisingly, this also holds for
22
W WERB BLEUS l AdequacyFluency ll llkeepremove tokenizetok+treat.
0.0
0.2
0.4
0.6
llll llll lll
l llll
TIDESCE TIDESAE
W B W B
0.00
.20.4
0.60
.81.0 llll llll llll llll
TIDESCE TIDESAE
W B W B
Figure 5: Effect of different tokenization steps.
Left: sentence, right: system level correlation.
fluency. We do no find a clear tendency on whether
or not to regard case information at system level.
Figure 5 indicates that the way of handling
punctuation we proposed does pay off when eval-
uating adequacy. For fluency our results were
contradictory: A slight decrease on the Arabic-
English corpus is accompanied by a slight decay on
the Chinese-English corpus. We did not investigate
the BTEC corpus here as most systems sticked to the
tokenization guidelines for this evaluation.
4.5 Reference length
The dependency of evaluation measures on the
selection of reference lengths is rarely covered in
the literature. However, as we can see in figure 6,
our experiments indicate a significant impact. The
selected three methods here are the default for
WER/PER, NIST, and BLEU, respectively. For the
distance based evaluation measures, represented by
W WERB BLEUS N NIST ll laveragenearest best
0.0
0.2
0.4
0.6
lllll
 
ll
 
lllll
 
ll
 
lllll
 
l
l
 TIDESCE TIDESAE BTECCE
W B N W B N W B N
0.00
.20.
40.6
0.81
.0
lllll
 
ll
 
lllll
 
ll
 
lllll
 
l
l
 TIDESCE TIDESAE BTECCE
W B N W B N W B N
Figure 6: Effect of different reference lengths.
Left: sentence, right: system level correlation.
P 2PERB BLEUS N NIST ll llnoneinitial endboth
0.0
0.2
0.4
0.6
llllllllllll lll
lllllllll
TIDESCE TIDESAE
P B N P B N
0.00
.20.4
0.60
.81.0 llllllllllll llllllllllll
TIDESCE TIDESAE
P B N P B N
Figure 7: Effect of sentence boundaries.
Left: sentence, right: system level correlation.
WER here, taking the length of the sentence leading
to the best score leads to the best correlation with
both fluency and adequacy. Taking the average
length instead seems to be the worst choice.
For brevity penalty based measures, the effect is
not as clear: On both TIDES corpora there is no
significant difference in correlation between using
the average length and the nearest length. On
the BTEC corpus, choosing the nearest sentence
length leads to a significantly higher correlation than
choosing the average length. We assume this is due
to the high number of reference sentences on this
corpus.
4.6 Sentence boundaries
As sentence boundaries will only influence m-gram
count vector based measures, we have restricted
our experiments to bigram PER, BLEU-S, and NIST
here. Including sentence boundaries (figure 7)
has a positive effect on correlation with fluency
and adequacy for both bigram PER and BLEU-S.
Sentence initials seem to be more important than
sentence ends here. For the NIST measure, we do
not find any significant effect.
5 Discussion
In a perfect MT world, any dependency of an
evaluation on case information or tokenization
should be inexistent, as MT systems already have
to deal with both in the translation process, and
could be designed to produce output according to
evaluation campaign guidelines. Once all translation
systems stick to the same specifications, no further
preprocessing steps should be necessary.
In practice there will be some systems that step
23
out of line. If we then choose strict rules regarding
case information and punctuation, automatic error
measures will penalize these systems rather hard,
whereas penalty is rather low if we choose lax ones.
In this situation case information will have a
large effect on the correlation between automatic
and human evaluation, depending on whether the
involved candidate systems will have a good or a bad
human evaluation. It is vital to keep this in mind
when drawing conclusions here regarding system
evaluation, despite the obvious importance of case
information in natural languages.
These considerations also hold for the treatment
of punctuation marks, as a special care should be
unnecessary if all systems sticked to tokenization
specifications. In practise, MT systems differ
in the way they generate and handle punctuation
marks. Therefore, appropriate preprocessing steps
are advisable.
Our experiments suggest that sentence boundaries
increase correlation between automatic scores and
adequacy both on sentence and on system level.
For fluency, the improvement is less significant, and
mainly depends on the sentence initials.
For length penalty based measures, we have found
that choosing the nearest sentence length yields the
highest correlation with human evaluation. For
distance based measures instead, it seems advisable
to choose the sentence that leads to the best relative
score as the one that determines the reference length.
6 Conclusion
We have described several MT evaluation measures.
We have pointed out common preprocessing steps
and auxiliary methods which have not been studied
in detail so far in spite of their importance for
the MT evaluation process. Particularly, we have
introduced a novel method for determining the
reference length of an evaluation candidate sentence,
and a simple method to incorporate sentence
boundary information to m-gram based evaluation
measures.
We then have performed several experiments
on these methods on three evaluation corpora.
The results indicate that both our new reference
length algorithm and the use of sentence boundaries
improve the correlation of the studied automatic
evaluation measures with human evaluation. Fur-
thermore, we have learned that case information
should be removed when performing automatic
sentence evaluation. On sentence level, evaluator
normalization can improve the correlation between
automatic and human evaluation.
Acknowledgements
This work was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5) and by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,
and J. Tsujii. 2004. Overview of the IWSLT04
evaluation campaign. In Proc. IWSLT, pp. 1?12,
Kyoto, Japan, September.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
G. Doddington. 2003. NIST MT Evaluation Workshop.
Personal communication, July.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications to
machine translation evaluation. In Proc. MT Summit
IX, pp. 240?247, New Orleans, LA, September.
C. Y. Lin and F. J. Och. 2004. Orange: a method for
evaluation automatic evaluation metrics for machine
translation. In Proc. COLING 2004, pp. 501?507,
Geneva, Switzerland, August.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, September.
K. A. Papineni. 2002. The NIST mteval scor-
ing software. http://www.itl.nist.gov/iad/
894.01/tests/mt/resources/scoring.htm.
M. Paul, H. Nakaiwa, and M. Federico. 2004. Towards
innovative evaluation methodologies for speech trans-
lation. In Working Notes of the NTCIR-4 Meeting,
volume 2, pp. 17?21.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated DP based search for
statistical translation. In European Conf. on Speech
Communication and Technology, pp. 2667?2670,
Rhodes, Greece, September.
24
Proceedings of the Second Workshop on Statistical Machine Translation, pages 33?39,
Prague, June 2007. c?2007 Association for Computational Linguistics
Can We Translate Letters?
David Vilar, Jan-T. Peter and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
D-52056 Aachen, Germany
{vilar,peter,ney}@cs.rwth-aachen.de
Abstract
Current statistical machine translation sys-
tems handle the translation process as the
transformation of a string of symbols into
another string of symbols. Normally the
symbols dealt with are the words in differ-
ent languages, sometimes with some addi-
tional information included, like morpho-
logical data. In this work we try to push
the approach to the limit, working not on the
level of words, but treating both the source
and target sentences as a string of letters.
We try to find out if a nearly unmodified
state-of-the-art translation system is able to
cope with the problem and whether it is ca-
pable to further generalize translation rules,
for example at the level of word suffixes and
translation of unseen words. Experiments
are carried out for the translation of Catalan
to Spanish.
1 Introduction
Most current statistical machine translation systems
handle the translation process as a ?blind? transfor-
mation of a sequence of symbols, which represent
the words in a source language, to another sequence
of symbols, which represent words in a target lan-
guage. This approach allows for a relative simplic-
ity of the models, but also has drawbacks, as re-
lated word forms, like different verb tenses or plural-
singular word pairs, are treated as completely differ-
ent entities.
Some efforts have been made e.g. to integrate
more information about the words in the form of Part
Of Speech tags (Popovic? and Ney, 2005), using addi-
tional information about stems and suffixes (Popovic?
and Ney, 2004) or to reduce the morphological vari-
ability of the words (de Gispert, 2006). State of the
art decoders provide the ability of handling different
word forms directly in what has been called factored
translation models (Shen et al, 2006).
In this work, we try to go a step further and treat
the words (and thus whole sentences) as sequences
of letters, which have to be translated into a new se-
quence of letters. We try to find out if the trans-
lation models can generalize and generate correct
words out of the stream of letters. For this approach
to work we need to translate between two related
languages, in which a correspondence between the
structure of the words can be found.
For this experiment we chose a Catalan-Spanish
corpus. Catalan is a romance language spoken in the
north-east of Spain and Andorra and is considered
by some authors as a transitional language between
the Iberian Romance languages (e.g. Spanish) and
Gallo-Romance languages (e.g. French). A common
origin and geographic proximity result in a similar-
ity between Spanish and Catalan, albeit with enough
differences to be considered different languages. In
particular, the sentence structure is quite similar in
both languages and many times a nearly monotoni-
cal word to word correspondence between sentences
can be found. An example of Catalan and Spanish
sentences is given in Figure 1.
The structure of the paper is as follows: In Sec-
tion 2 we review the statistical approach to machine
translation and consider how the usual techniques
can be adapted to the letter translation task. In Sec-
33
Catalan Perque` a mi m?agradaria estar-hi dues, una o dues setmanes, me?s o menys, depenent del
preu i cada hotel.
Spanish Porque a m?? me gustar??a quedarme dos, una o dos semanas, ma?s o menos, dependiendo del
precio y cada hotel.
English Because I would like to be there two, one or two weeks, more or less, depending on the
price of each hotel.
Catalan Si baixa aqu?? tenim una guia de la ciutat que li podem facilitar en la que surt informacio?
sobre els llocs me?s interessants de la ciutat.
Spanish Si baja aqu?? tenemos una gu??a de la ciudad que le podemos facilitar en la que sale infor-
macio?n sobre los sitios ma?s interesantes de la ciudad.
English If you come down here we have a guide book of the city that you can use, in there is
information about the most interesting places in the city.
Figure 1: Example Spanish and Catalan sentences (the English translation is provided for clarity).
tion 3 we present the results of the letter-based trans-
lation and show how to use it for improving transla-
tion quality. Although the interest of this work is
more academical, in Section 4 we discuss possible
practical applications for this approach. The paper
concludes in Section 5.
2 From Words To Letters
In the standard approach to statistical machine trans-
lation we are given a sentence (sequence of words)
fJ1 = f1 . . . fJ in a source language which is to be
translated into a sentence e?I1 = e?1 . . . e?I in a target
language. Bayes decision rule states that we should
choose the sentence which maximizes the posterior
probability
e?I1 = argmax
eI1
p(eI1|fJ1 ) , (1)
where the argmax operator denotes the search pro-
cess. In the original work (Brown et al, 1993) the
posterior probability p(eI1|fJ1 ) is decomposed fol-
lowing a noisy-channel approach, but current state-
of-the-art systems model the translation probabil-
ity directly using a log-linear model(Och and Ney,
2002):
p(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
e?I1
exp
(
?M
m=1 ?mhm(e?I1, fJ1 )
) ,
(2)
with hm different models, ?m scaling factors and
the denominator a normalization factor that can be
ignored in the maximization process. The ?m are
usually chosen by optimizing a performance mea-
sure over a development corpus using a numerical
optimization algorithm like the downhill simplex al-
gorithm (Press et al, 2002).
The most widely used models in the log lin-
ear combination are phrase-based models in source-
to-target and target-to-source directions, ibm1-like
scores computed at phrase level, also in source-to-
target and target-to-source directions, a target lan-
guage model and different penalties, like phrase
penalty and word penalty.
This same approach can be directly adapted to the
letter-based translation framework. In this case we
are given a sequence of letters FJ1 corresponding
to a source (word) string fJ1 , which is to be trans-
lated into a sequence of letters EI1 corresponding to
a string eI1 in a target language. Note that in this case
whitespaces are also part of the vocabulary and have
to be generated as any other letter. It is also impor-
tant to remark that, without any further restrictions,
the word sequences eI1 corresponding to a generated
letter sequence EI1 are not even composed of actual
words.
2.1 Details of the Letter-Based System
The vocabulary of the letter-based translation sys-
tem is some orders of magnitude smaller than the
vocabulary of a full word-based translation system,
at least for European languages. A typical vocabu-
lary size for a letter-based system would be around
70, considering upper- and lowercase letter, digits,
34
whitespace and punctuation marks, while the vocab-
ulary size of a word-based system like the ones used
in current evaluation campaigns is in the range of
tens or hundreds of thousands words. In a normal
situation there are no unknowns when carrying out
the actual translation of a given test corpus. The sit-
uation can be very different if we consider languages
like Chinese or Japanese.
This small vocabulary size allows us to deal with
a larger context in the models used. For the phrase-
based models we extract all phrases that can be used
when translating a given test corpus, without any
restriction on the length of the source or the tar-
get part1. For the language model we were able to
use a high-order n-gram model. In fact in our ex-
periments a 16-gram letter-based language model is
used, while state-of-the-art translation systems nor-
mally use 3 or 4-grams (word-based).
In order to better try to generate ?actual words?
in the letter-based system, a new model was added
in the log-linear combination, namely the count of
words generated that have been seen in the training
corpus, normalized with the length of the input sen-
tence. Note however that this models enters as an ad-
ditional feature function in the model and it does not
constitute a restriction of the generalization capabil-
ities the model can have in creating ?new words?.
Somehow surprisingly, an additional word language
model did not help.
While the vocabulary size is reduced, the average
sentence length increases, as we consider each let-
ter to be a unit by itself. This has a negative impact
in the running time of the actual implementation of
the algorithms, specially for the alignment process.
In order to alleviate this, the alignment process was
split into two passes. In the first part, a word align-
ment was computed (using the GIZA++ toolkit (Och
and Ney, 2003)). Then the training sentences were
split according to this alignment (in a similar way to
the standard phrase extraction algorithm), so that the
length of the source and target part is around thirty
letters. Then, a letter-based alignment is computed.
2.2 Efficiency Issues
Somewhat counter-intuitively, the reduced vocabu-
lary size does not necessarily imply a reduced mem-
1For the word-based system this is also the case.
ory footprint, at least not without a dedicated pro-
gram optimization. As in a sensible implementa-
tions of nearly all natural language processing tools,
the words are mapped to integers and handled as
such. A typical implementation of a phrase table is
then a prefix-tree, which is accessed through these
word indices. In the case of the letter-based transla-
tion, the phrases extracted are much larger than the
word-based ones, in terms of elements. Thus the to-
tal size of the phrase table increases.
The size of the search graph is also larger for
the letter-based system. In most current systems
the generation algorithm is a beam search algorithm
with a ?source synchronous? search organization.
As the length of the source sentence is dramatically
increased when considering letters instead of words,
the total size of the search graph is also increased, as
is the running time of the translation process.
The memory usage for the letter system can ac-
tually be optimized, in the sense that the letters can
act as ?indices? themselves for addressing the phrase
table and the auxiliary mapping structure is not nec-
essary any more. Furthermore the characters can be
stored in only one byte, which provides a signifi-
cant memory gain over the word based system where
normally four bytes are used for storing the indices.
These gains however are not expected to counteract
the other issues presented in this section.
3 Experimental Results
The corpus used for our experiment was built in the
framework of the LC-STAR project (Conejero et al,
2003). It consists of spontaneous dialogues in Span-
ish, Catalan and English2 in the tourism and travel-
ling domain. The test corpus (and an additional de-
velopment corpus for parameter optimization) was
randomly extracted, the rest of the sentences were
used as training data. Statistics for the corpus can
be seen in Table 1. Details of the translation system
used can be found in (Mauser et al, 2006).
The results of the word-based and letter-based
approaches can be seen in Table 2 (rows with la-
bel ?Full Corpus?). The high BLEU scores (up to
nearly 80%) denote that the quality of the trans-
lation is quite good for both systems. The word-
2The English part of the corpus was not used in our experi-
ments.
35
Spanish Catalan
Training Sentences 40 574
Running Words 482 290 485 514
Vocabulary 14 327 12 772
Singletons 6 743 5 930
Test Sentences 972
Running Words 12 771 12 973
OOVs [%] 1.4 1.3
Table 1: Corpus Statistics
based system outperforms the letter-based one, as
expected, but the letter-based system also achieves
quite a good translation quality. Example transla-
tions for both systems can be found in Figure 2. It
can be observed that most of the words generated
by the letter based system are correct words, and in
many cases the ?false? words that the system gen-
erates are very close to actual words (e.g. ?elos? in-
stead of ?los? in the second example of Figure 2).
We also investigated the generalization capabili-
ties of both systems under scarce training data con-
ditions. It was expected that the greater flexibility
of the letter-based system would provide an advan-
tage of the approach when compared to the word-
based approach. We randomly selected subsets of
the training corpus of different sizes ranging from
1 000 sentences to 40 000 (i.e. the full corpus) and
computed the translation quality on the same test
corpus as before. Contrary to our hopes, however,
the difference in BLEU score between the word-
based and the letter-based system remained fairly
constant, as can be seen in Figure 3, and Table 2
for representative training corpus sizes.
Nevertheless, the second example in Figure 2 pro-
vides an interesting insight into one of the possi-
ble practical applications of this approach. In the
example translation of the word-based system, the
word ?centreamericans? was not known to the sys-
tem (and has been explicitly marked as unknown in
Figure 2). The letter-based system, however, was
able to correctly learn the translation from ?centre-?
to ?centro-? and that the ending ?-ans? in Catalan
is often translated as ?-anos? in Spanish, and thus
a correct translation has been found. We thus chose
to combine both systems, the word-based system do-
ing most of the translation work, but using the letter-
based system for the translation of unknown words.
The results of this combined approach can be found
in Table 2 under the label ?Combined System?. The
combination of both approaches leads to a 0.5% in-
crease in BLEU using the full corpus as training ma-
terial. This increase is not very big, but is it over a
quite strong baseline and the percentage of out-of-
vocabulary words in this corpus is around 1% of the
total words (see Table 1). When the corpus size is
reduced, the gain in BLEU score becomes more im-
portant, and for the small corpus size of 1 000 sen-
tences the gain is 2.5% BLEU. Table 2 and Figure 3
show more details.
4 Practical Applications
The approach described in this paper is mainly of
academical interest. We have shown that letter-
based translation is in principle possible between
similar languages, in our case between Catalan and
Spanish, but can be applied to other closely related
language pairs like Spanish and Portuguese or Ger-
man and Dutch. The approach can be interesting for
languages where very few parallel training data is
available.
The idea of translating unknown words in a letter-
based fashion can also have applications to state-of-
the-art translation systems. Nowadays most auto-
matic translation projects and evaluations deal with
translation from Chinese or Arabic to English. For
these language pairs the translation of named en-
tities poses an additional problem, as many times
they were not previously seen in the training data
and they are actually one of the most informative
words in the texts. The ?translation? of these enti-
ties is in most cases actually a (more or less pho-
netic) transliteration, see for example (Al-Onaizan
and Knight, 2002). Using the proposed approach for
the translation of these words can provide a tighter
integration in the translation process and hopefully
increase the translation performance, in the same
way as it helps for the case of the Catalan-Spanish
translation for unseen words.
Somewhat related to this problem, we can find an
additional application in the field of speech recog-
nition. The task of grapheme-to-phoneme conver-
sion aims at increasing the vocabulary an ASR sys-
tem can recognize, without the need for additional
36
BLEU WER PER
Word-Based System Full Corpus 78.9 11.4 10.6
10k 74.0 13.9 13.2
1k 60.0 21.3 20.1
Letter-Based System Full Corpus 72.9 14.7 13.5
10k 69.8 16.5 15.1
1k 55.8 24.3 22.8
Combined System Full Corpus 79.4 11.2 10.4
10k 75.2 13.4 12.6
1k 62.5 20.2 19.0
Table 2: Translation results for selected corpus sizes. All measures are percentages.
Source (Cat) Be?, en principi seria per a les vacances de Setmana Santa que so?n les segu?ents que tenim
ara, entrant a juliol.
Word-Based Bueno, en principio ser??a para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando en julio.
Letter-Based Bueno, en principio ser??a para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando bamos en julio .
Reference Bueno, en principio ser??a para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando julio.
Source (Cat) Jo li recomanaria per exemple que intente?s apropar-se a algun pa??s ve?? tambe? com poden ser
els pa??sos centreamericans, una mica me?s al nord Panama?.
Word-Based Yo le recomendar??a por ejemplo que intentase acercarse a algu?n pa??s vecino tambie?n como
pueden ser los pa??ses UNKNOWN centreamericans, un poco ma?s al norte Panama?.
Letter-Based Yo le recomendar??a por ejemplo que intentaseo acercarse a algu?n pa??s ve?? tambie?n como
pueden ser elos pa??ses centroamericanos, un poco ma?s al norte Panama?.
Combined Yo le recomendar??a por ejemplo que intentase acercarse a algu?n pa??s vecino tambie?n como
pueden ser los pa??ses centroamericanos, un poco ma?s al norte Panama?.
Reference Yo le recomendar??a por ejemplo que intentase acercarse a algu?n pa??s vecino tambie?n como
pueden ser los pa??ses centroamericanos, un poco ma?s al norte Panama?.
Figure 2: Example translations of the different approaches. For the word-based system an unknown word
has been explicitly marked.
37
 50
 55
 60
 65
 70
 75
 80
 0  5000  10000  15000  20000  25000  30000  35000  40000
Word-Based
Letter-Based
Combined
Figure 3: Translation quality depending of the corpus size.
acoustic data. The problem can be formulated as a
translation from graphemes (?letters?) to a sequence
of graphones (?pronunciations?), see for example
(Bisani and Ney, 2002). The proposed letter-based
approach can also be adapted to this task.
Lastly, a combination of both, word-based and
letter-based models, working in parallel and perhaps
taking into account additional information like base
forms, can be helpful when translating from or into
rich inflexional languages, like for example Spanish.
5 Conclusions
We have investigated the possibility of building a
letter-based system for translation between related
languages. The performance of the approach is quite
acceptable, although, as expected, the quality of the
word-based approach is superior. The combination
of both techniques, however, allows the system to
translate words not seen in the training corpus and
thus increase the translation quality. The gain is spe-
cially important when the training material is scarce.
While the experiments carried out in this work are
more interesting from an academical point of view,
several practical applications has been discussed and
will be the object of future work.
Acknowledgements
This work was partly funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (NE 572/5-3).
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 workshop on Computational ap-
proaches to semitic languages, pages 1?13, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Max Bisani and Hermann Ney. 2002. Investigations
on joint-multigram models for grapheme-to-phoneme
conversion. In Proceedings of the 7th International
Conference on Spoken Language Processing, vol-
ume 1, pages 105?108, Denver, CO, September.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
38
mation. Computational Linguistics, 19(2):263?311,
June.
D. Conejero, J. Gimnez, V. Arranz, A. Bonafonte, N. Pas-
cual, N. Castell, and A. Moreno. 2003. Lexica and
corpora for speech-to-speech translation: A trilingual
approach. In European Conf. on Speech Commu-
nication and Technology, pages 1593?1596, Geneva,
Switzerland, September.
Adria` de Gispert. 2006. Introducing Linguistic Knowl-
edge into Statistical Machine Translation. Ph.D. the-
sis, Universitat Polite`cnica de Catalunya, Barcelona,
October.
Arne Mauser, Richard Zens, Evgeny Matusov, Sas?a
Hasan, and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of the International Workshop on
Spoken Language Translation, pages 103?110, Kyoto,
Japan.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 295?302, Philadelphia, PA, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Maja Popovic? and Hermann Ney. 2004. Towards the
Use of Word Stems and Suffixes for Statistical Ma-
chine Translation. In 4th International Conference on
Language Resources and Evaluation (LREC), pages
1585?1588, Lisbon, Portugal, May.
Maja Popovic? and Hermann Ney. 2005. Exploiting
Phrasal Lexica and Additional Morpho-syntactic Lan-
guage Resources for Statistical Machine Translation
with Scarce Training Data. In 10th Annual Conference
of the European Association for Machine Translation
(EAMT), pages 212?218, Budapest, Hungary, May.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge,
UK.
Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello
Federico. 2006. The JHU Workshop 2006 IWSLT
System. In Proc. of the International Workshop on
Spoken Language Translation, pages 59?63, Kyoto,
Japan.
39
Proceedings of the Second Workshop on Statistical Machine Translation, pages 96?103,
Prague, June 2007. c?2007 Association for Computational Linguistics
Human Evaluation of Machine Translation Through Binary System
Comparisons
David Vilar, Gregor Leusch
and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
D-52056 Aachen, Germany
{vilar,leusch,ney}@cs.rwth-aachen.de
Rafael E. Banchs
D. of Signal Theory and Communications
Universitat Polite`cnica de Catalunya
08034 Barcelona, Spain
rbanchs@gps.tsc.upc.edu
Abstract
We introduce a novel evaluation scheme for
the human evaluation of different machine
translation systems. Our method is based
on direct comparison of two sentences at a
time by human judges. These binary judg-
ments are then used to decide between all
possible rankings of the systems. The ad-
vantages of this new method are the lower
dependency on extensive evaluation guide-
lines, and a tighter focus on a typical eval-
uation task, namely the ranking of systems.
Furthermore we argue that machine transla-
tion evaluations should be regarded as sta-
tistical processes, both for human and au-
tomatic evaluation. We show how confi-
dence ranges for state-of-the-art evaluation
measures such as WER and TER can be
computed accurately and efficiently without
having to resort to Monte Carlo estimates.
We give an example of our new evaluation
scheme, as well as a comparison with classi-
cal automatic and human evaluation on data
from a recent international evaluation cam-
paign.
1 Introduction
Evaluation of machine translation (MT) output is a
difficult and still open problem. As in other natu-
ral language processing tasks, automatic measures
which try to asses the quality of the translation
can be computed. The most widely known are the
Word Error Rate (WER), the Position independent
word Error Rate (PER), the NIST score (Dodding-
ton, 2002) and, especially in recent years, the BLEU
score (Papineni et al, 2002) and the Translation Er-
ror Rate (TER) (Snover et al, 2005). All of the-
ses measures compare the system output with one
or more gold standard references and produce a nu-
merical value (score or error rate) which measures
the similarity between the machine translation and a
human produced one. Once such reference transla-
tions are available, the evaluation can be carried out
in a quick, efficient and reproducible manner.
However, automatic measures also have big dis-
advantages; (Callison-Burch et al, 2006) describes
some of them. A major problem is that a given sen-
tence in one language can have several correct trans-
lations in another language and thus, the measure of
similarity with one or even a small amount of ref-
erence translations will never be flexible enough to
truly reflect the wide range of correct possibilities of
a translation. 1 This holds in particular for long sen-
tences and wide- or open-domain tasks like the ones
dealt with in current MT projects and evaluations.
If the actual quality of a translation in terms of
usefulness for human users is to be evaluated, human
evaluation needs to be carried out. This is however
a costly and very time-consuming process. In this
work we present a novel approach to human evalu-
ation that simplifies the task for human judges. In-
stead of having to assign numerical scores to each
sentence to be evaluated, as is done in current evalu-
ation procedures, human judges choose the best one
out of two candidate translations. We show how this
method can be used to rank an arbitrary number of
systems and present a detailed analysis of the statis-
tical significance of the method.
1Compare this with speech recognition, where apart from
orthographic variance there is only one correct reference.
96
2 State-of-the-art
The standard procedure for carrying out a human
evaluation of machine translation output is based on
the manual scoring of each sentence with two nu-
merical values between 1 and 5. The first one mea-
sures the fluency of the sentence, that is its readabil-
ity and understandability. This is a monolingual fea-
ture which does not take the source sentence into
account. The second one reflects the adequacy, that
is whether the translated sentence is a correct trans-
lation of the original sentence in the sense that the
meaning is transferred. Since humans will be the
end users of the generated output,2 it can be ex-
pected that these human-produced measures will re-
flect the usability and appropriateness of MT output
better than any automatic measure.
This kind of human evaluation has however addi-
tional problems. It is much more time consuming
than the automatic evaluation, and because it is sub-
jective, results are not reproducible, even from the
same group of evaluators. Furthermore, there can
be biases among the human judges. Large amounts
of sentences must therefore be evaluated and proce-
dures like evaluation normalization must be carried
out before significant conclusions from the evalua-
tion can be drawn. Another important drawback,
which is also one of the causes of the aforemen-
tioned problems, is that it is very difficult to define
the meaning of the numerical scores precisely. Even
if human judges have explicit evaluation guidelines
at hand, they still find it difficult to assign a numeri-
cal value which represents the quality of the transla-
tion for many sentences (Koehn and Monz, 2006).
In this paper we present an alternative to this eval-
uation scheme. Our method starts from the obser-
vation that normally the final objective of a human
evaluation is to find a ?ranking? of different systems,
and the absolute score for each system is not relevant
(and it can even not be comparable between differ-
ent evaluations). We focus on a method that aims to
simplify the task of the judges and allows to rank the
systems according to their translation quality.
3 Binary System Comparisons
The main idea of our method relies in the fact
that a human evaluator, when presented two differ-
ent translations of the same sentence, can normally
choose the best one out of them in a more or less
2With the exception of cross-language information retrieval
and similar tasks.
definite way. In social sciences, a similar method
has been proposed by (Thurstone, 1927).
3.1 Comparison of Two Systems
For the comparison of two MT systems, a set of
translated sentence pairs is selected. Each of these
pairs consists of the translations of a particular
source sentence from the two systems. The human
judge is then asked to select the ?best? translation of
these two, or to mark the translations to be equally
good. We are aware that the definition of ?best? here
is fuzzy. In our experiments, we made a point of not
giving the evaluators explicit guidelines on how to
decide between both translations. As a consequence,
the judges were not to make a distinction between
fluency and adequacy of the translation. This has a
two-fold purpose: on the one hand it simplifies the
decision procedure for the judges, as in most of the
cases the decision is quite natural and they do not
need to think explicitly in terms of fluency and ade-
quacy. On the other hand, one should keep in mind
that the final goal of an MT system is its usefulness
for a human user, which is why we do not want to
impose artificial constraints on the evaluation proce-
dure. If only certain quality aspects of the systems
are relevant for the ranking, for example if we want
to focus on the fluency of the translations, explicit
guidelines can be given to the judges. If the evalua-
tors are bilingual they can use the original sentences
to judge whether the information was preserved in
the translation.
After our experiment, the human judges provided
feedback on the evaluation process. We learned
that the evaluators normally selected the translation
which preserved most of the information from the
original sentence. Thus, we expect to have a slight
preference for adequacy over fluency in this evalu-
ation process. Note however that adequacy and flu-
ency have shown a high correlation3 in previous ex-
periments. This can be explained by noting that a
low fluency renders the text incomprehensible and
thus the adequacy score will also be low.
The difference in the amount of selected sen-
tences of each system is an indicator of the differ-
ence in quality between the systems. Statistics can
be carried out in order to decide whether this differ-
ence is statistically significant; we will describe this
in more detail in Section 3.4.
3At least for ?sensible? translation systems. Academic
counter-examples could easily be constructed.
97
3.2 Evaluation of Multiple Systems
We can generalize our method to find a ranking of
several systems as follows: In this setting, we have
a set of n systems. Furthermore, we have defined an
order relationship ?is better than? between pairs of
these systems. Our goal now is to find an ordering
of the systems, such that each system is better than
its predecessor. In other words, this is just a sorting
problem ? as widely known in computer science.
Several efficient sorting algorithms can be found
in the literature. Generally, the efficiency of sort-
ing algorithms is measured in terms of the number
of comparisons carried out. State-of-the-art sort-
ing algorithms have a worst-case running time of
O(n log n), where n is the number of elements to
sort. In our case, because such binary comparisons
are very time consuming, we want to minimize the
absolute number of comparisons needed. This mini-
mization should be carried out in the strict sense, not
just in an asymptotic manner.
(Knuth, 1973) discusses this issue in detail. It is
relatively straightforward to show that, in the worst
case, the minimum number of comparisons to be
carried out to sort n elements is at least dlog n!e
(for which n log n is an approximation). It is not
always possible to reach this minimum, however, as
was proven e.g. for the case n = 12 in (Wells, 1971)
and for n = 13 in (Peczarski, 2002). (Ford Jr and
Johnson, 1959) propose an algorithm called merge
insertion which comes very close to the theoretical
limit. This algorithm is sketched in Figure 1. There
are also algorithms with a better asymptotic runtime
(Bui and Thanh, 1985), but they only take effect for
values of n too large for our purposes (e.g., more
than 100). Thus, using the algorithm from Figure 1
we can obtain the ordering of the systems with a
(nearly) optimal number of comparisons.
3.3 Further Considerations
In Section 3.1 we described how to carry out the
comparison between two systems when there is only
one human judge carrying out this comparison. The
comparison of systems is a very time consuming
task. Therefore it is hardly possible for one judge
to carry out the evaluation on a whole test corpus.
Usually, subsets of these test corpora are selected
for human evaluations instead. In order to obtain
a better coverage of the test corpus, but also to try
to alleviate the possible bias of a single evaluator, it
is advantageous to have several evaluators carrying
out the comparison between two systems. However,
there are two points that must be considered.
The first one is the selection of sentences each hu-
man judge should evaluate. Assume that we have al-
ready decided the amount of sentences m each eval-
uator has to work with (in our case m = 100). One
possibility is that all human judges evaluate the same
set of sentences, which presumably will cancel pos-
sible biases of the evaluators. A second possibility is
to give each judge a disjunct set of sentences. In this
way we benefit from a higher coverage of the corpus,
but do not have an explicit bias compensation.
In our experiments, we decided for a middle
course: Each evaluator receives a randomly selected
set of sentences. There are no restrictions on the se-
lection process. This implicitly produces some over-
lap while at the same time allowing for a larger set
of sentences to be evaluated. To maintain the same
conditions for each comparison, we also decided
that each human judge should evaluate the same set
of sentences for each system pair.
The other point to consider is how the evaluation
results of each of the human judges should be com-
bined into a decision for the whole system. One
possibility would be to take only a ?majority vote?
among the evaluators to decide which system is the
best. By doing this, however, possible quantitative
information on the quality difference of the systems
is not taken into account. Consequently, the output is
strongly influenced by statistical fluctuations of the
data and/or of the selected set of sentences to eval-
uate. Thus, in order to combine the evaluations we
just summed over all decisions to get a total count of
sentences for each system.
3.4 Statistical Significance
The evaluation of MT systems by evaluating trans-
lations of test sentences ? be it automatic evaluation
or human evaluation ? must always be regarded as
a statistical process: Whereas the outcome, or score
R, of an evaluation is considered to hold for ?all?
possible sentences from a given domain, a test cor-
pus naturally consists of only a sample from all these
sentences. Consequently, R depends on that sam-
ple of test sentences. Furthermore, both a human
evaluation score and an automatic evaluation score
for a hypothesis sentence are by itself noisy: Hu-
man evaluation is subjective, and as such is subject
to ?human noise?, as described in Section 2. Each
automatic score, on the other hand, depends heavily
on the ambiguous selection of reference translations.
Accordingly, evaluation scores underly a probability
98
1. Make pairwise comparisons of bn/2c disjoint pairs of elements. (If n is odd, leave one element out).
2. Sort the bn/2c larger elements found in step 1, recursively by merge insertion.
3. Name the bn/2c elements found in step 2 a1, a2, . . . , abn/2c and the rest b1, b2, . . . , bdn/2e, such that
a1 ? a2 ? ? ? ? ? abn/2c and bi ? ai for 1 ? i ? bn/2c. Call b1 and the a?s the ?main chain?.
4. Insert the remaining b?s into the main chain, using binary insertion, in the following order (ignore the
bj such that j > dn/2e): b3, b2; b5, b4; b11, . . . , b6; . . . ; btk , . . . , btk?1+1; . . . with tk =
2k+1+(?1)k
3 .
Figure 1: The merge insertion algorithm as presented in (Knuth, 1973).
distribution, and each evaluation result we achieve
must be considered as a sample from that distribu-
tion. Consequently, both human and automatic eval-
uation results must undergo statistical analysis be-
fore conclusions can be drawn from them.
A typical application of MT evaluation ? for ex-
ample in the method described in this paper ? is to
decide whether a given MT system X , represented
by a set of translated sentences, is significantly better
than another system Y with respect to a given eval-
uation measure. This outcome is traditionally called
the alternative hypothesis. The opposite outcome,
namely that the two systems are equal, is known
as the null hypothesis. We say that certain values
of RX , RY confirm the alternative hypothesis if the
null hypothesis can be rejected with a given level
of certainty, e.g. 95%. In the case of comparing
two MT systems, the null hypothesis would be ?both
systems are equal with regard to the evaluation mea-
sure; that is, both evaluation scoresR, R? come from
the same distribution R0?.
As R is randomly distributed, it has an expecta-
tion E[R] and a standard error se[R]. Assuming a
normal distribution for R, we can reject the null hy-
pothesis with a confidence of 95% if the sampled
score R is more than 1.96 times the standard error
away from the null hypothesis expectation:
R significant ? |E[R0] ? R| > 1.96 se[R0] (1)
The question we have to solve is: How can we es-
timate E[R0] and se[R0]? The first step is that we
consider R and R0 to share the same standard error
se[R0] = se[R]. This value can then be estimated
from the test data. In a second step, we give an es-
timate for E[R0], either inherent in the evaluation
measure (see below), or from the estimate for the
comparison system R?.
A universal estimation method is the bootstrap
estimate: The core idea is to create replications of
R by random sampling from the data set (Bisani
and Ney, 2004). Bootstrapping is generally possi-
ble for all evaluation measures. With a high number
of replicates, se[R] and E[R0] can be estimated with
satisfactory precision.
For a certain class of evaluation measures, these
parameters can be estimated more accurately and ef-
ficiently from the evaluation data without resorting
to Monte Carlo estimates. This is the class of er-
rors based on the arithmetic mean over a sentence-
wise score: In our binary comparison experiments,
each judge was given hypothesis translations ei,X ,
ei,Y . She could then judge ei,X to be better than,
equal to, or worse than ei,Y . All these judgments
were counted over the systems. We define a sentence
score ri,X,Y for this evaluation method as follows:
ri,X,Y :=
?
??
??
+1 ei,X is better than ei,Y
0 ei,X is equal to ei,Y
?1 ei,X is worse than ei,Y
. (2)
Then, the total evaluation score for a binary com-
parison of systems X and Y is
RX,Y :=
1
m
m?
i=1
ri,X,Y , (3)
with m the number of evaluated sentences.
For this case, namelyR being an arithmetic mean,
(Efron and Tibshirani, 1993) gives an explicit for-
mula for the estimated standard error of the score
RX,Y . To simplify the notation, we will use R in-
stead of RX,Y from now on, and ri instead of ri,X,Y .
se[R] =
1
m ? 1
?
?
?
?
m?
i=1
(ri ? R)2 . (4)
With x denoting the number of sentences where
ri = 1, and y denoting the number of sentences
99
where ri = ?1,
R =
x ? y
m
(5)
and with basic algebra
se[R] =
1
m ? 1
?
x + y ?
(x ? y)2
m
. (6)
Moreover, we can explicitly give an estimate for
E[R0]: The null hypothesis is that both systems are
?equally good?. Then, we should expect as many
sentences where X is better than Y as vice versa,
i.e. x = y. Thus, E[R0] = 0.
Using Equation 4, we calculate se[R] and thus a
significance range for adequacy and fluency judg-
ments. When comparing two systems X and Y ,
we assume for the null hypothesis that se[R0] =
se[RX ] and E[R0] = E[RY ] (or vice versa).
A very useful (and to our knowledge new) result
for MT evaluation is that se[R] can also be explic-
itly estimated for weighted means ? such as WER,
PER, and TER. These measures are defined as fol-
lows: Let di, i = 1, . . . ,m denote the number of ?er-
rors? (edit operations) of the translation candidate ei
with regard to a reference translation with length li.
Then, the total error rate will be computed as
R :=
1
L
m?
i=1
di (7)
where
L :=
m?
i=1
li (8)
As a result, each sentence ei affects the overall score
with weight li ? the effect of leaving out a sen-
tence with length 40 is four times higher than that
of leaving out one with length 10. Consequently,
these weights must be considered when estimating
the standard error of R:
se[R] =
?
?
?
? 1
(m ? 1)(L ? 1)
m?
i=1
(
di
li
? R
)2
? li
(9)
With this Equation, Monte-Carlo-estimates are no
longer necessary for examining the significance of
WER, PER, TER, etc. Unfortunately, we do not ex-
pect such a short explicit formula to exist for the
standard BLEU score. Still, a confidence range
for BLEU can be estimated by bootstrapping (Och,
2003; Zhang and Vogel, 2004).
Spanish English
Train Sentences 1.2M
Words 32M 31M
Vocabulary 159K 111K
Singletons 63K 46K
Test Sentences 1 117
Words 26K
OOV Words 72
Table 1: Statistics of the EPPS Corpus.
4 Evaluation Setup
The evaluation procedure was carried out on the data
generated in the second evaluation campaign of the
TC-STAR project4. The goal of this project is to
build a speech-to-speech translation system that can
deal with real life data. Three translation directions
are dealt with in the project: Spanish to English, En-
glish to Spanish and Chinese to English. For the sys-
tem comparison we concentrated only in the English
to Spanish direction.
The corpus for the Spanish?English language pair
consists of the official version of the speeches held in
the European Parliament Plenary Sessions (EPPS),
as available on the web page of the European Parlia-
ment. A more detailed description of the EPPS data
can be found in (Vilar et al, 2005). Table 1 shows
the statistics of the corpus.
A total of 9 different MT systems participated in
this condition in the evaluation campaign that took
place in February 2006. We selected five representa-
tive systems for our study. Henceforth we shall refer
to these systems as System A through System E. We
restricted the number of systems in order to keep the
evaluation effort manageable for a first experimental
setup to test the feasibility of our method. The rank-
ing of 5 systems can be carried out with as few as 7
comparisons, but the ranking of 9 systems requires
19 comparisons.
5 Evaluation Results
Seven human bilingual evaluators (6 native speakers
and one near-native speaker of Spanish) carried out
the evaluation. 100 sentences were randomly cho-
sen and assigned to each of the evaluators for every
system comparison, as discussed in Section 3.3. The
results can be seen in Table 2 and Figure 2. Counts
4http://www.tc-star.org/
100
0 10 20 30 40 50 60 70
0
10
20
30
40
50
60
70
l
l
l
l
ll
l
# "First system better"
# "S
eco
nd s
yste
m b
ette
r" l
B?AD?CA?CE?AE?BB?DD?A
(a) Each judge.
0 100 200 300 400
0
100
200
300
400
# "First system better"
# "S
eco
nd s
yste
m b
ette
r"
lB?AD?C
A?C E?A
E?B
B?D D?A
(b) All judges.
Figure 2: Results of the binary comparisons. Number of times the winning system was really judged ?better?
vs. number of times it was judged ?worse?. Results in hatched area can not reject null hypothesis, i.e. would
be considered insignificant.
missing to 100 and 700 respectively denote ?same
quality? decisions.
As can be seen from the results, in most of the
cases the judges clearly favor one of the systems.
The most notable exception is found when compar-
ing systems A and C, where a difference of only 3
sentences is clearly not enough to decide between
the two. Thus, the two bottom positions in the final
ranking could be swapped.
Figure 2(a) shows the outcome for the binary
comparisons separately for each judge, together with
an analysis of the statistical significance of the re-
sults. As can be seen, the number of samples (100)
would have been too low to show significant re-
sults in many experiments (data points in the hatched
area). In some cases, the evaluator even judged bet-
ter the system which was scored to be worse by the
majority of the other evaluators (data points above
the bisector). As Figure 2(b) shows, ?the only thing
better than data is more data?: When we summarize
R over all judges, we see a significant difference
(with a confidence of 95%) at all comparisons but
two (A vs. C, and E vs. B). It is interesting to note
that exactly these two pairs do not show a significant
difference when using a majority vote strategy.
Table 3 shows also the standard evaluation met-
rics. Three BLEU scores are given in this table, the
one computed on the whole corpus, the one com-
puted on the set used for standard adequacy and flu-
ency computations and the ones on the set we se-
lected for this task5. It can be seen that the BLEU
scores are consistent across all data subsets. In this
case the ranking according to this automatic measure
matches exactly the ranking found by our method.
When comparing with the adequacy and fluency
scores, however, the ranking of the systems changes
considerably: B D E C A. However, the difference
between the three top systems is quite small. This
can be seen in Figure 3, which shows some auto-
matic and human scores for the five systems in our
experiments, along with the estimated 95% confi-
dence range. The bigger difference is found when
comparing the bottom systems, namely System A
and System C. While our method produces nearly
no difference the adequacy and fluency scores indi-
cate System C as clearly superior to System A. It is
worth noting that the both groups use quite different
translation approaches (statistical vs. rule-based).
5Regretfully these two last sets were not the same. This is
due to the fact that the ?AF Test Set? was further used for eval-
uating Text-to-Speech systems, and thus a targeted subset of
sentences was selected.
101
Sys E1 E2 E3 E4 E5 E6 E7
?
A 29 19 38 17 32 29 41 205
B 40 59 48 53 63 64 45 372
C 32 22 29 23 32 34 42 214
D 39 61 59 50 64 58 46 377
A 32 31 31 31 47 38 40 250
C 37 29 32 22 39 45 43 247
A 36 28 17 28 34 37 31 211
E 41 47 44 43 53 45 58 331
B 26 29 18 24 43 36 33 209
E 34 33 28 27 32 29 43 226
B 34 28 30 31 40 41 48 252
D 23 17 23 17 24 28 38 170
A 36 14 27 9 31 30 34 181
D 34 50 40 50 57 61 57 349
Final ranking (best?worst): E B D A C
Table 2: Result of the binary system comparison.
Numbers of sentences for which each system was
judged better by each evaluator (E1-E7).
Subset: Whole A+F Binary
Sys BLEU BLEU A F BLEU
A 36.3 36.2 2.93 2.46 36.3
B 49.4 49.3 3.74 3.58 49.2
C 36.3 36.2 3.53 3.31 36.1
D 48.2 46.8 3.68 3.48 47.7
E 49.8 49.6 3.67 3.46 49.4
Table 3: BLEU scores and Adequacy and Fluency
scores for the different systems and subsets of the
whole test set. BLEU values in %, Adequacy (A)
and Fluency (F) from 1 (worst) to 5 (best).
6 Discussion
In this section we will review the main drawbacks of
the human evaluation listed in Section 2 and analyze
how our approach deals with them. The first one
was the use of explicit numerical scores, which are
difficult to define exactly. Our system was mainly
designed for the elimination of this issue.
Our evaluation continues to be time consuming.
Even more, the number of individual comparisons
needed is in the order of log(n!), in contrast with the
standard adequacy-fluency evaluation which needs
2n individual evaluations (two evaluations per sys-
tem, one for fluency, another one for adequacy). For
n in the range of 1 up to 20 (a realistic number of
systems for current evaluation campaigns) these two
quantities are comparable. And actually each of our
CA
DB
E
CA
DB
E
CA
DB
E
CA
DB
E
ll
ll
l
ll
ll
l
ll
ll
l
ll
l l
lFluency
Adequacy
1?WER
BLEU
0.3 0.4 0.5 0.6 0.7
          worse <?  normalized score  ?> better
Me
asu
re &
 Sys
tem
Figure 3: Normalized evaluation scores. Higher
scores are better. Solid lines show the 95% con-
fidence range. Automatic scores calculated on the
whole test set, human scores on the A+F subset.
evaluations should be simpler than the standard ad-
equacy and fluency ones. Therefore the time needed
for both evaluation procedures is probably similar.
Reproducibility of the evaluation is also an impor-
tant concern. We computed the number of ?errors?
in the evaluation process, i.e. the number of sen-
tences evaluated by two or more evaluators where
the evaluators? judgement was different. Only in
10% of the cases the evaluation was contradictory,
in the sense that one evaluator chose one sentence as
better than the other, while the other evaluator chose
the other one. In 30% of the cases, however, one
evaluator estimated both sentences to be of the same
quality while the other judged one sentence as supe-
rior to the other one. As comparison, for the fluency-
adequacy judgement nearly one third of the com-
mon evaluations have a difference in score greater or
equal than two (where the maximum would be four),
and another third a score difference of one point6.
With respect to biases, we feel that it is almost im-
possible to eliminate them if humans are involved. If
one of the judges prefers one kind of structure, there
will a bias for a system producing such output, in-
dependently of the evaluation procedure. However,
the suppression of explicit numerical scores elimi-
nates an additional bias of evaluators. It has been
observed that human judges often give scores within
6Note however that possible evaluator biases can have a
great influence in these statistics.
102
a certain range (e.g. in the mid-range or only ex-
treme values), which constitute an additional diffi-
culty when carrying out the evaluation (Leusch et
al., 2005). Our method suppresses this kind of bias.
Another advantage of our method is the possibil-
ity of assessing improvements within one system.
With one evaluation we can decide if some modi-
fications actually improve performance. This eval-
uation even gives us a confidence interval to weight
the significance of an improvement. Carrying out
a full adequacy-fluency analysis would require a lot
more effort, without giving more useful results.
7 Conclusion
We presented a novel human evaluation technique
that simplifies the task of the evaluators. Our method
relies on two basic observations. The first one is that
in most evaluations the final goal is to find a ranking
of different systems, the absolute scores are usually
not so relevant. Especially when considering human
evaluation, the scores are not even comparable be-
tween two evaluation campaigns. The second one
is the fact that a human judge can normally choose
the best one out of two translations, and this is a
much easier process than the assessment of numeri-
cal scores whose definition is not at all clear. Taking
this into consideration we suggested a method that
aims at finding a ranking of different MT systems
based on the comparison of pairs of translation can-
didates for a set of sentences to be evaluated.
A detailed analysis of the statistical significance
of the method is presented and also applied to some
wide-spread automatic measures. The evaluation
methodology was applied for the ranking of 5 sys-
tems that participated in the second evaluation cam-
paign of the TC-STAR project and comparison with
standard evaluation measures was performed.
8 Acknowledgements
We would like to thank the human judges who par-
ticipated in the evaluation. This work has been
funded by the integrated project TC-STAR? Tech-
nology and Corpora for Speech-to-Speech Transla-
tion ? (IST-2002-FP6-506738).
References
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluationx.
IEEE ICASSP, pages 409?412, Montreal, Canada,
May.
T. Bui and M. Thanh. 1985. Significant improvements to
the Ford-Johnson algorithm for sorting. BIT Numeri-
cal Mathematics, 25(1):70?75.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. Proceeding of the 11th Conference of the Eu-
ropean Chapter of the ACL: EACL 2006, pages 249?
256, Trento, Italy, Apr.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. Proc. ARPA Workshop on Human Language
Technology.
B. Efron and R. J. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York and
London.
L. Ford Jr and S. Johnson. 1959. A Tournament Problem.
The American Mathematical Monthly, 66(5):387?389.
D. E. Knuth. 1973. The Art of Computer Programming,
volume 3. Addison-Wesley, 1st edition. Sorting and
Searching.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. Proceedings of the Workshop on Statisti-
cal Machine Translation, pages 102?121, New York
City, Jun.
G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005.
Preprocessing and normalization for automatic evalu-
ation of machine translation. 43rd ACL: Proc. Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization, pages 17?24, Ann Ar-
bor, Michigan, Jun.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. Proc. of the 41st ACL, pages
160?167, Sapporo, Japan, Jul.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. Proc. of the 40th ACL, pages 311?318,
Philadelphia, PA, Jul.
M. Peczarski. 2002. Sorting 13 elements requires 34
comparisons. LNCS, 2461/2002:785?794, Sep.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci-
ulla, and R. Weischedel. 2005. A study of translation
error rate with targeted human annotation. Technical
Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-
2005-58, University of Maryland, College Park, MD.
L. Thurstone. 1927. The method of paired comparisons
for social values. Journal of Abnormal and Social Psy-
chology, 21:384?400.
D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.
2005. Statistical Machine Translation of European
Parliamentary Speeches. Proceedings of MT Summit
X, pages 259?266, Phuket, Thailand, Sep.
M. Wells. 1971. Elements of combinatorial computing.
Pergamon Press.
Y. Zhang and S. Vogel. 2004. Measuring confidence
intervals for the machine translation evaluation met-
rics. Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation, pages 4?6, Baltimore, MD.
103
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 66?69,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The RWTH Machine Translation System for WMT 2009
Maja Popovic?, David Vilar, Daniel Stein, Evgeny Matusov and Hermann Ney
RWTH Aachen University
Aachen, Germany
Abstract
RWTH participated in the shared transla-
tion task of the Fourth Workshop of Sta-
tistical Machine Translation (WMT 2009)
with the German-English, French-English
and Spanish-English pair in each transla-
tion direction. The submissions were gen-
erated using a phrase-based and a hierar-
chical statistical machine translation sys-
tems with appropriate morpho-syntactic
enhancements. POS-based reorderings of
the source language for the phrase-based
systems and splitting of German com-
pounds for both systems were applied. For
some tasks, a system combination was
used to generate a final hypothesis. An ad-
ditional English hypothesis was produced
by combining all three final systems for
translation into English.
1 Introduction
For the WMT 2009 shared task, RWTH submit-
ted translations for the German-English, French-
English and Spanish-English language pair in both
directions. A phrase-based translation system en-
hanced with appropriate morpho-syntactic trans-
formations was used for all translation direc-
tions. Local POS-based word reorderings were ap-
plied for the Spanish-English and French-English
pair, and long range reorderings for the German-
English pair. For this language pair splitting
of German compounds was also applied. Spe-
cial efforts were made for the French-English and
German-English translation, where a hierarchi-
cal system was also used and the final submis-
sions are the result of a system combination. For
translation into English, an additional hypothesis
was produced as a result of combination of the
final German-to-English, French-to-English and
Spanish-to-English systems.
2 Translation models
2.1 Phrase-based model
We used a standard phrase-based system similar to
the one described in (Zens et al, 2002). The pairs
of source and corresponding target phrases are ex-
tracted from the word-aligned bilingual training
corpus. Phrases are defined as non-empty contigu-
ous sequences of words. The phrase translation
probabilities are estimated using relative frequen-
cies. In order to obtain a more symmetric model,
the phrase-based model is used in both directions.
2.2 Hierarchical model
The hierarchical phrase-based approach can be
considered as an extension of the standard phrase-
based model. In this model we allow the phrases
to have ?gaps?, i.e. we allow non-contiguous parts
of the source sentence to be translated into pos-
sibly non-contiguous parts of the target sentence.
The model can be formalized as a synchronous
context-free grammar (Chiang, 2007). The model
also included some additional heuristics which
have shown to be helpful for improving translation
quality, as proposed in (Vilar et al, 2008).
The first step in the hierarchical phrase extrac-
tion is the same as for the phrased-based model.
Having a set of initial phrases, we search for
phrases which contain other smaller sub-phrases
and produce a new phrase with gaps. In our sys-
tem, we restricted the number of non-terminals for
each hierarchical phrase to a maximum of two,
which were also not allowed to be adjacent. The
scores of the phrases are again computed as rela-
tive frequencies.
2.3 Common models
For both translation models, phrase-based and hi-
erarchical, additional common models were used:
word-based lexicon model, phrase penalty, word
penalty and target language model.
66
The target language model was a standard n-
gram language model trained by the SRI language
modeling toolkit (Stolcke, 2002). The smooth-
ing technique we apply was the modified Kneser-
Ney discounting with interpolation. In our case we
used a 4-gram language model.
3 Morpho-syntactic transformations
3.1 POS-based word reorderings
For the phrase-based systems, the local and
long range POS-based reordering rules described
in (Popovic? and Ney, 2006) were applied on the
training and test corpora as a preprocessing step.
Local reorderings were used for the Spanish-
English and French-English language pairs in or-
der to handle differences between the positions of
nouns and adjectives in the two languages. Adjec-
tives in Spanish and French, as in most Romanic
languages, are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, for these language pairs local
reorderings of nouns and adjective groups in the
source language were applied. The following se-
quences of words are considered to be an adjective
group: a single adjective, two or more consecutive
adjectives, a sequence of adjectives and coordinate
conjunctions, as well as an adjective along with its
corresponding adverb. If the source language is
Spanish or French, each noun is moved behind the
corresponding adjective group. If the source lan-
guage is English, each adjective group is moved
behind the corresponding noun.
Long range reorderings were applied on the
verb groups for the German-English language pair.
Verbs in the German language can often be placed
at the end of a clause. This is mostly the case
with infinitives and past participles, but there are
many cases when other verb forms also occur at
the clause end. For the translation from German
into English, following verb types were moved to-
wards the beginning of a clause: infinitives, infini-
tives+zu, finite verbs, past participles and negative
particles. For the translation from English to Ger-
man, infinitives and past participles were moved
to the end of a clause, where punctuation marks,
subordinate conjunctions and finite verbs are con-
sidered as the beginning of the next clause.
3.2 German compound words
For the translation from German into English, Ger-
man compounds were split using the frequency-
based method described in (Koehn and Knight,
2003). For the other translation direction, the En-
glish text was first translated into the modified
German language with split compounds. The gen-
erated output was then postprocessed, i.e. the
components were merged using the method de-
scribed in (Popovic? et al, 2006): a list of com-
pounds and a list of components are extracted from
the original German training corpus. If the word
in the generated output is in the component list,
check if this word merged with the next word is in
the compound list. If it is, merge the two words.
4 System combination
For system combination we used the approach de-
scribed in (Matusov et al, 2006). The method is
based on the generation of a consensus transla-
tion out of the output of different translation sys-
tems. The core of the method consists in building
a confusion network for each sentence by align-
ing and combining the (single-best) translation hy-
pothesis from one MT system with the translations
produced by the other MT systems (and the other
translations from the same system, if n-best lists
are used in combination). For each sentence, each
MT system is selected once as ?primary? system,
and the other hypotheses are aligned to this hy-
pothesis. The resulting confusion networks are
combined into a signle word graph, which is then
weighted with system-specific factors, similar to
the approach of (Rosti et al, 2007), and a trigram
LM trained on the MT hypotheses. The translation
with the best total score within this word graph is
selected as consensus translation. The scaling fac-
tors of these models are optimized using the Con-
dor toolkit (Berghen and Bersini, 2005) to achieve
optimal BLEU score on the dev set.
5 Experimental results
5.1 Experimental settings
For all translation directions, we used the provided
EuroParl and News parallel corpora to train the
translation models and the News monolingual cor-
pora to train the language models. All systems
were optimised for the BLEU score on the develop-
ment data (the ?dev-a? part of the 2008 evaluation
data). The other part of the 2008 evaluation set
(?dev-b?) is used as a blind test set. The results re-
ported in the next section will be referring to this
test set. For the tasks including a system combi-
nation, the parameters for the system combination
67
were also trained on the ?dev-b? set. The reported
evaluation metrics are the BLEU score and two
syntax-oriented metrics which have shown a high
correlation with human evaluations: the PBLEU
score (BLEU calculated on POS sequences) and
the POS-F-score PF (similar to the BLEU score but
based on the F-measure instead of precision and
on arithmetic mean instead of geometric mean).
The POS tags used for reorderings and for syn-
tactic evaluation metrics for the English and the
German corpora were generated using the statisti-
cal n-gram-based TnT-tagger (Brants, 2000). The
Spanish corpora are annotated using the FreeLing
analyser (Carreras et al, 2004), and the French
texts using the TreeTagger1.
5.2 Translation results
Table 1 presents the results for the German-
English language pair. For translation from Ger-
man into English, results for the phrase-based sys-
tem with and without verb reordering and com-
pound splitting are shown. The hierarchical sys-
tem was trained with split German compounds.
The final submission was produced by combining
those five systems. The improvement obtained by
system combination on the unseen test data 2009
is similar, i.e. from the systems with BLEU scores
of 17.0%, 17.2%, 17.5%, 17.6% and 17.7% to the
final system with 18.5%.
German?English BLEU PBLEU PF
phrase-based 17.8 31.6 39.7
+reorder verbs 18.2 32.6 40.3
+split compounds 18.0 31.9 40.0
+reord+split 18.4 33.1 40.7
hierarchical+split 18.5 33.5 40.1
system combination 19.2 33.8 40.9
English?German BLEU PBLEU PF
phrase-based 13.6 31.6 39.7
+reorder verbs 13.7 32.4 40.2
+split compounds 13.7 32.3 40.1
+reord+split 13.7 32.3 40.1
system combination 14.0 32.7 40.3
Table 1: Translation results [%] for the German-
English language pair, News2008 dev-b.
The other translation direction is more difficult
and improvements from morpho-syntactic trans-
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
formations are smaller. No hierarchical system
was trained for this translation direction. The com-
bination of the four phrase-based systems leads
to further improvements (on the unseen test set
as well: contrastive hypotheses have the BLEU
scores in the range from 12.7% to 13.0%, and the
final BLEU score is 13.2%).
The results for the French-English language
pair are shown in Table 2. For the French-to-
English system, we submitted the result of the
combination of three systems: a phrase-based with
and without local reorderings and a hierarchical
system. For the unseen test set, the BLEU score of
the system combination output is 24.4%, whereas
the contrastive hypotheses have 23.2%, 23.4% and
24.1%. For the other translation direction we did
not use the system combination, the submission is
produced by the phrase-based system with local
adjective reorderings.
French?English BLEU PBLEU PF
phrase-based 20.9 37.1 43.2
+reorder adjectives 21.3 38.2 43.6
hierarchical 20.3 36.7 42.6
system combination 21.7 38.5 43.8
English?French BLEU PBLEU PF
phrase-based 20.2 39.5 45.9
+reorder adjectives 20.7 40.6 46.4
Table 2: Translation results [%] for the French-
English language pair, News2008 dev-b.
Table 3 presents the results for the Spanish-
English language pair. As in the English-to-
French translation, the phrase-based system with
adjective reorderings is used to produce the sub-
mitted hypothesis for both translation directions.
Spanish?English BLEU PBLEU PF
phrase-based 22.1 38.5 44.1
+reorder adjectives 22.5 39.2 44.6
English?Spanish BLEU PBLEU PF
phrase-based 20.6 29.3 35.7
+reorder adjectives 21.1 29.7 35.9
Table 3: Translation results [%] for the Spanish-
English language pair, News2008 dev-b.
68
The result of the additional experiment, i.e. for
the multisource translation int English is presented
in Table 4. The English hypothesis is produced by
the combination of the three best systems for each
language pair, and it can be seen that the transla-
tion performance increases in all measures. This
suggests that each language pair poses different
difficulties for the translation task, and the com-
bination of all three can improve performance.
F+S+G?English BLEU PBLEU PF
system combination 25.1 41.0 46.4
Table 4: Multisource translation results [%]:
the English hypothesis is obtained as result of
a system combination of all language pairs,
News2008 dev-b.
6 Conclusions
The RWTH system submitted to the WMT 2009
shared translation task used a phrase-based sys-
tem and a hierarchical system with appropriate
morpho-syntactic extensions, i.e. POS based word
reorderings and splitting of German compounds
were used. System combination produced gains
in BLEU score over phrasal-system baselines in
the German-to-English, English-to-German and
French-to-English tasks.
Acknowledgments
This work was realised as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
Powell?s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP),
pages 224?231, Seattle, WA.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 239?242, Lisbon, Portugal,
May.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, (33):201?228.
Philipp Koehn and Kevin Knight. 2003. Empiri-
cal methods for compound splitting. In Proceed-
ings 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 347?354, Budapest, Hungary, April.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems Using Enhanced
Hypotheses Alignment. In Proceedings of EACL
2006 (11th Conference of the European Chapter
of the Association for Computational Linguistics),
pages 33?40, Trento, Italy, April.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Trans-
lation. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC), pages 1278?1283, Genoa, Italy, May.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of german compound
words. In Proceedings of the 5th International Con-
ference on Natural Language Processing (FinTAL),
pages 616?624, Turku, Finland, August. Lecture
Notes in Computer Science, Springer Verlag.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining Outputs from Multiple Ma-
chine Translation Systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190?197, October.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
25th German Conference on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artifi-
cial Intelligence (LNAI), pages 18?32, Aachen, Ger-
many, September. Springer Verlag.
69
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 174?179,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Simple and Effective Approach for Consistent Training of Hierarchical
Phrase-based Translation Models
Stephan Peitz
1
and David Vilar
2
and Hermann Ney
1
1
Lehrstuhl f?ur Informatik 6
Computer Science Department
2
Pixformance GmbH
RWTH Aachen University D-10587 Berlin, Germany
D-52056 Aachen, Germany david.vilar@gmail.com
{peitz,ney}@cs.rwth-aachen.de
Abstract
In this paper, we present a simple ap-
proach for consistent training of hierarchi-
cal phrase-based translation models. In
order to consistently train a translation
model, we perform hierarchical phrase-
based decoding on training data to find
derivations between the source and tar-
get sentences. This is done by syn-
chronous parsing the given sentence pairs.
After extracting k-best derivations, we
reestimate the translation model proba-
bilities based on collected rule counts.
We show the effectiveness of our proce-
dure on the IWSLT German?English and
English?French translation tasks. Our
results show improvements of up to 1.6
points BLEU.
1 Introduction
In state of the art statistical machine translation
systems, the translation model is estimated by fol-
lowing heuristic: Given bilingual training data,
a word alignment is trained with tools such as
GIZA
++
(Och and Ney, 2003) or fast align (Dyer
et al., 2013). Then, all valid translation pairs are
extracted and the translation probabilities are com-
puted as relative frequencies (Koehn et al., 2003).
However, this extraction method causes several
problems. First, this approach does not consider,
whether a translation pair is extracted from a likely
alignment or not. Further, during the extraction
process, models employed in decoding are not
considered.
For phrase-based translation, a successful ap-
proach addressing these issues is presented in
(Wuebker et al., 2010). By applying a phrase-
based decoder on the source sentences of the train-
ing data and constraining the translations to the
corresponding target sentences, k-best segmenta-
tions are produced. Then, the phrases used for
these segmentations are extracted and counted.
Based on the counts, the translation model prob-
abilities are recomputed. To avoid over-fitting,
leave-one-out is applied.
However, for hierarchical phrase-based transla-
tion an equivalent approach is still missing.
In this paper, we present a simple and effec-
tive approach for consistent reestimation of the
translation model probabilities in a hierarchical
phrase-based translation setup. Using a heuristi-
cally extracted translation model as starting point,
the training data are parsed bilingually. From the
resulting hypergraphs, we extract k-best deriva-
tions and the rules applied in each derivation. This
is done with a top-down k-best parsing algorithm.
Finally, the translation model probabilities are re-
computed based on the counts of the extracted
rules. In our procedure, we employ leave-one-out
to avoid over-fitting. Further, we consider all mod-
els which are used in translation to ensure a con-
sistent training.
Experimental results are presented on the
German?English and English?French IWSLT
shared machine translation task (Cettolo et al.,
2013). We are able to gain improvements of up to
1.6% BLEU absolute and 1.4% TER over a com-
petitive baseline. On all tasks and test sets, the
improvements are statistically significant with at
least 99% confidence.
The paper is structured as follow. First, we re-
vise the state of the art hierarchical phrase-based
extraction and translation process. In Section 3,
we propose our training procedure. Finally, ex-
perimental results are given in Section 4 and we
conclude with Section 5.
2 Hierarchical Phrase-based Translation
In hierarchical phrase-based translation (Chiang,
2005), discontinuous phrases with ?gaps? are
allowed. The translation model is formalized
as a synchronous context-free grammar (SCFG)
174
and consists of bilingual rules, which are based
on bilingual standard phrases and discontinuous
phrases. Each bilingual rule rewrites a generic
non-terminal X into a pair of strings
?
f and e?
with both terminals and non-terminals in both lan-
guages
X ? ?
?
f, e??. (1)
In a standard hierarchical phrase-based translation
setup, obtaining these rules is based on a heuristic
extraction from automatically word-aligned bilin-
gual training data. Just like in the phrase-based
approach, all bilingual rules of a sentence pair
are extracted given an alignment. The standard
phrases are stored as lexical rules in the rule set.
In addition, whenever a phrase contains a sub-
phrase, this sub-phrase is replaced by a generic
non-terminal X . With these hierarchical phrases
we can define the hierarchical rules in the SCFG.
The rule probabilities which are in general defined
as relative frequencies are computed based on the
joint counts C(X ? ?
?
f, e??) of a bilingual rule
X ? ?
?
f, e??
p
H
(
?
f |e?) =
C(X ? ?
?
f, e??)
?
?
f
?
C(X ? ?
?
f
?
, e??)
. (2)
The translation probabilities are computed in
source-to-target as well as in target-to-source di-
rection. In the translation processes, these proba-
bilities are integrated in the log-linear combination
among other models such as a language model,
word lexicon models, word and phrase penalty and
binary features marking hierarchical phrases, glue
rule and rules with non-terminals at the bound-
aries.
The translation process of hierarchical phrase-
based approach can be considered as parsing prob-
lem. Given an input sentence in the source lan-
guage, this sentence is parsed using the source lan-
guage part of the SCFG. In this work, we perform
this step with a modified version of the CYK+ al-
gorithm (Chappelier and Rajman, 1998). The out-
put of this algorithm is a hypergraph, which rep-
resents all possible derivations of the input sen-
tence. A derivation represents an application of
rules from the grammar to generate the given in-
put sentence. Using the the associated target part
of the applied rule, for each derivation a transla-
tion can be constructed. In a second step, the lan-
guage model score is incorporated. Given the hy-
pergraph, this is done with the cube pruning algo-
rithm presented in (Chiang, 2007).
3 Translation Model Training
We propose following pipeline for consistent hi-
erarchical phrase-based training: First we train a
word alignment, from which the baseline trans-
lation model is extracted as described in the pre-
vious section. The log-linear parameter weights
are tuned with MERT (Och, 2003) on a develop-
ment set to produce the baseline system. Next,
we perform decoding on the training data. As the
translations are constrained to the given target sen-
tences, we name this step forced decoding in the
following. Details are given in the next subsection.
Given the counts C
FD
(X ? ?
?
f, e??) of the rules,
which have been applied in the forced decoding
step, the translation probabilities p
FD
(
?
f |e?) for the
translation model are recomputed:
p
FD
(
?
f |e?) =
C
FD
(X ? ?
?
f, e??)
?
?
f
?
C
FD
(X ? ?
?
f
?
, e??)
. (3)
Finally, using the translation model with the
reestimated probabilities, we retune the log-linear
parameter weights and obtain our final system.
3.1 Forced Decoding
In this section, we describe the forced decoding
for hierarchical phrase-based translation in detail.
Given a sentence pair of the training data, we
constrain the translation of the source sentence to
produce the corresponding target sentence. For
this constrained decoding process, the language
model score is constant as the translation is fixed.
Hence, the incorporation of the a language model
is not needed. This results in a simplification of
the decoding process as we do not have to employ
the cube pruning algorithm as described in the pre-
vious section. Consequently, forced decoding for
hierarchical phrase-based translation is equivalent
to synchronous parsing of the training data. Dyer
(2010) has described an approach to reduce the
average-case run-time of synchronous parsing by
splitting one bilingual parse into two successive
monolingual parses. We adopt this method and
first parse the source sentence and then the target
sentence with CYK+.
If the given sentence pair has been parsed suc-
cessfully, we employ a top-down k-best parsing
algorithm (Chiang and Huang, 2005) on the re-
sulting hypergraph to find the k-best derivations
between the given source and target sentence. In
this step, all models of the translation process are
175
included (except for the language model). Further,
leave-one-out is applied to counteract overfitting.
Note, that the model weights of the baseline sys-
tem are used to perform forced decoding.
Finally, we extract and count the rules which
have been applied in the derivations. These counts
are used to recompute the translation probabilities.
3.2 Recombination
In standard hierarchical phrase-based decoding,
partial derivations that are indistinguishable from
each other are recombined. In (Huck et al., 2013)
two schemes are presented. Either derivations that
produce identical translations or derivations with
identical language model context are recombined.
As in forced decoding the translation is fixed and
a language model is missing, both schemes are not
suitable.
However, a recombination scheme is necessary
to avoid derivations with the same application
of rules. Further, recombining such derivations
increases simultaneously the amounts of consid-
ered derivations during k-best parsing. Given two
derivations with the same set of applied rules, the
order of application of the rules may be different.
Thus, we propose following scheme for recom-
bining derivations in forced decoding: Derivations
that produce identical sets of applied rules are re-
combined. Figure 1 shows an example for k = 3.
Employing the proposed scheme, derivations d
1
and d
2
are recombined since both share the same
set of applied rules ({r
1
, r
3
, r
2
}).
d
1
: {r
1
, r
3
, r
2
}
d
2
: {r
3
, r
2
, r
1
}
d
3
: {r
4
, r
5
, r
1
, r
2
}
(a)
d
1
: {r
1
, r
3
, r
2
}
d
3
: {r
4
, r
5
, r
1
, r
2
}
d
4
: {r
6
, r
5
, r
2
, r
3
}
(b)
Figure 1: Example search space before (a) and af-
ter (b) applying recombination.
4 Experiments
4.1 Setup
The experiments were carried out on the IWSLT
2013 German?English shared translation task.
1
1
http://www.iwslt2013.org
German English English French
Sentences 4.32M 5.23M
Run. Words 108M 109M 133M 147M
Vocabulary 836K 792K 845K 888K
Table 1: Statistics for the bilingual training
data of the IWSLT 2013 German?English and
English?French task.
It is focusing the translation of TED talks. Bilin-
gual data statistics are given in Table 1. The base-
line system was trained on all available bilingual
data and used a 4-gram LM with modified Kneser-
Ney smoothing (Kneser and Ney, 1995; Chen and
Goodman, 1998), trained with the SRILM toolkit
(Stolcke, 2002). As additional data sources for the
LM we selected parts of the Shuffled News and
LDC English Gigaword corpora based on cross-
entropy difference (Moore and Lewis, 2010). In
all experiments, the hierarchical search was per-
formed as described in Section 2.
To confirm the efficacy of our approach, addi-
tional experiments were run on the IWSLT 2013
English?French task. Statistics are given in Ta-
ble 1.
The training pipeline was set up as described
in the previous section. Tuning of the log-linear
parameter weights was done with MERT on a pro-
vided development set. As optimization criterion
we used BLEU (Papineni et al., 2001).
Forced decoding was performed on the TED
talks portion of the training data (?140K sen-
tences). In both tasks, around 5% of the sentences
could not be parsed. In this work, we just skipped
those sentences.
We report results in BLEU [%] and TER [%]
(Snover et al., 2006). All reported results are av-
erages over three independent MERT runs, and
we evaluated statistical significance with MultE-
val (Clark et al., 2011).
4.2 Results
Figure 2 shows the performance of setups us-
ing translation models with reestimated translation
probabilities. The setups vary in the k-best deriva-
tion size extracted in the forced decoding (fd) step.
Based on the performance on the development set,
we selected two setups with k = 500 using leave-
one-out (+l1o) and k = 750 without leave-one-
out (-l1o). Table 2 shows the final results for
the German?English task. Performing consistent
translation model training improves the translation
176
dev
*
eval11 test
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
baseline 33.1 46.8 35.7 44.1 30.5 49.7
forced decoding -l1o 33.2 46.3 36.3 43.4 31.2 48.8
forced decoding +l1o 33.6 46.2 36.6 43.0 31.8 48.3
Table 2: Results for the IWSLT 2013 German?English task. The development set used for MERT is
marked with an asterisk (*). Statistically significant improvements with at least 99% confidence over the
baseline are printed in boldface.
dev
*
eval11 test
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
baseline 28.1 55.7 37.5 42.7 31.7 49.5
forced decoding +l1o 28.8 55.0 39.1 41.6 32.4 49.0
Table 3: Results for the IWSLT 2013 English?French task. The development set used for MERT is
marked with an asterisk (*). Statistically significant improvements with at least 99% confidence over the
baseline are printed in boldface.
 31.5
 32
 32.5
 33
 33.5
 34
 1  10  100  1000  10000
B
L
E
U
[
%
]
k
dev fd +l1odev fd -l1odev baseline
Figure 2: BLEU scores on the IWSLT
German?English task of setups using trans-
lation models trained with different k-best
derivation sizes. Results are reported on dev with
(+l1o) and without leave-one-out (-l1o).
quality on all test sets significantly. We gain an
improvement of up to 0.7 points in BLEU and 0.9
points in TER. Applying leave-one-out results in
an additional improvement by up to 0.4 % BLEU
and 0.5 % TER. The results for English?French
are given in Table 3. We observe a similar im-
provement by up to 1.6 % BLEU and 1.1 % TER.
The improvements could be the effect of do-
main adaptation since we performed forced decod-
ing on the TED talks portion of the training data.
Thus, rules which were applied to decode the in-
domain data might get higher translation probabil-
ities.
Furthermore, employing leave-one-out seems to
avoid overfitting as the average source rule length
in training is reduced from 5.0 to 3.5 (k = 500).
5 Conclusion
We have presented a simple and effective approach
for consistent training of hierarchical phrase-based
translation models. By reducing hierarchical de-
coding on parallel training data to synchronous
parsing, we were able to reestimate the trans-
lation probabilities including all models applied
during the translation process. On the IWSLT
German?English and English?French tasks, the
final results show statistically significant improve-
ments of up to 1.6 points in BLEU and 1.4 points
in TER.
Our implementation was released as part of Jane
(Vilar et al., 2010; Vilar et al., 2012; Huck et al.,
2012; Freitag et al., 2014), the RWTH Aachen
University open source statistical machine trans-
lation toolkit.
2
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreements no 287658 and no 287755.
2
http://www.hltpr.rwth-aachen.de/jane/
177
References
Mauro Cettolo, Jan Nieheus, Sebastian St?uker, Luisa
Bentivogli, and Marcello Federico. 2013. Report on
the 10th iwslt evaluation campaign. In Proc. of the
International Workshop on Spoken Language Trans-
lation, Heidelberg, Germany, December.
J.-C. Chappelier and M. Rajman. 1998. A general-
ized CYK algorithm for parsing stochastic CFG. In
Proceedings of the First Workshop on Tabulation in
Parsing and Deduction, pages 133?137, April.
Stanley F. Chen and Joshuo Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.
David Chiang and Liang Huang. 2005. Better k-best
Parsing. In Proceedings of the 9th Internation Work-
shop on Parsing Technologies, pages 53?64, Octo-
ber.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 263?270,
Ann Arbor, Michigan, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Controlling
for optimizer instability. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages 176?181, Portland, Oregon,
June.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A Simple, Fast, and Effective Reparameter-
ization of IBM Model 2. In Proceedings of NAACL-
HLT, pages 644?648, Atlanta, Georgia, June.
Chris Dyer. 2010. Two monolingual parses are better
than one (synchronous parse). In In Proc. of HLT-
NAACL.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open source machine translation sys-
tem combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April. To appear.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Matthias Huck, David Vilar, Markus Freitag, and Her-
mann Ney. 2013. A performance study of cube
pruning for large-scale hierarchical machine transla-
tion. In Proceedings of the NAACL 7th Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 29?38, Atlanta, Georgia, USA, June.
Reinerd Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processingw, volume 1,
pages 181?184, May.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal Phrase-Based Translation. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 127?133, Edmonton, Alberta.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short
Papers), pages 220?224, Uppsala, Sweden, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, P.O. Box
218, Yorktown Heights, NY 10598, September.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
178
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475?484, Uppsala, Sweden, July.
179
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 262?270,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Jane: Open Source Hierarchical Translation, Extended with Reordering
and Lexicon Models
David Vilar, Daniel Stein, Matthias Huck and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
Aachen, Germany
{vilar,stein,huck,ney}@cs.rwth-aachen.de
Abstract
We present Jane, RWTH?s hierarchical
phrase-based translation system, which
has been open sourced for the scientific
community. This system has been in de-
velopment at RWTH for the last two years
and has been successfully applied in dif-
ferent machine translation evaluations. It
includes extensions to the hierarchical ap-
proach developed by RWTH as well as
other research institutions. In this paper
we give an overview of its main features.
We also introduce a novel reordering
model for the hierarchical phrase-based
approach which further enhances transla-
tion performance, and analyze the effect
some recent extended lexicon models have
on the performance of the system.
1 Introduction
We present a new open source toolkit for hi-
erarchical phrase-based translation, as described
in (Chiang, 2007). The hierarchical phrase model
is an extension of the standard phrase model,
where the phrases are allowed to have ?gaps?. In
this way, long-distance dependencies and reorder-
ings can be modelled in a consistent way. As in
nearly all current statistical approaches to machine
translation, this model is embedded in a log-linear
model combination.
RWTH has been developing this tool during
the last two years and it was used success-
fully in numerous machine translation evalua-
tions. It is developed in C++ with special at-
tention to clean code, extensibility and efficiency.
The toolkit is available under an open source
non-commercial license and downloadable from
http://www.hltpr.rwth-aachen.de/jane.
In this paper we give an overview of the main
features of the toolkit and introduce two new ex-
tensions to the hierarchical model. The first one
is an additional reordering model inspired by the
reordering widely used in phrase-based transla-
tion systems and the second one comprises two
extended lexicon models which further improve
translation performance.
2 Related Work
Jane implements many features presented in pre-
vious work developed both at RWTH and other
groups. As we go over the features of the system
we will provide the corresponding references.
Jane is not the first system of its kind, al-
though it provides some unique features. There
are other open source hierarchical decoders avail-
able. These include
? SAMT (Zollmann and Venugopal, 2006):
The original version is not maintained any
more and we had problems working on big
corpora. A new version which requires
Hadoop has just been released, however the
documentation is still missing.
? Joshua (Li et al, 2009): A decoder written
in Java by the John Hopkins University. This
project is the most similar to our own, how-
ever both were developed independently and
each one has some unique features. A brief
comparison between these two systems is in-
cluded in Section 5.1.
? Moses (Koehn et al, 2007): The de-facto
standard phrase-based translation decoder
has now been extended to support hierarchi-
cal translation. This is still in an experimental
branch, however.
3 Features
In this section we will only give a brief overview
of the features implemented in Jane. For de-
tailed explanation of previously published algo-
262
rithms and methods, we refer to the given litera-
ture.
3.1 Search Algorithms
The search for the best translation proceeds in two
steps. First, a monolingual parsing of the input
sentence is carried out using the CYK+ algorithm
(Chappelier and Rajman, 1998), a generalization
of the CYK algorithm which relaxes the require-
ment for the grammar to be in Chomsky normal
form. From the CYK+ chart we extract a hyper-
graph representing the parsing space.
In a second step the translations are generated,
computing the language model scores in an inte-
grated fashion. Both the cube pruning and cube
growing algorithms (Huang and Chiang, 2007) are
implemented. For the latter case, the extensions
concerning the language model heuristics similar
to (Vilar and Ney, 2009) have also been included.
3.2 Language Models
Jane supports four formats for n-gram language
models:
? The ARPA format for language models. We
use the SRI toolkit (Stolcke, 2002) to support
this format.
? The binary language model format supported
by the SRI toolkit. This format allows for a
more efficient language model storage, which
reduces loading times. In order to reduce
memory consumption, the language model
can be reloaded for every sentence, filtering
the n-grams that will be needed for scoring
the possible translations. This format is spe-
cially useful for this case.
? Randomized LMs as described in (Talbot and
Osborne, 2007), using the open source im-
plementation made available by the authors
of the paper. This approach uses a space ef-
ficient but approximate representation of the
set of n-grams in the language model. In
particular the probability for unseen n-grams
may be overestimated.
? An in-house, exact representation format
with on-demand loading of n-grams, using
the internal prefix-tree implementation which
is also used for phrase storage (see also Sec-
tion 3.9).
Several language models (also of mixed formats)
can be used during search. Their scores are com-
bined in the log-linear framework.
3.3 Syntactic Features
Soft syntactic features comparable to (Vilar et al,
2008) are implemented in the extraction step of
the toolkit. In search, they are considered as ad-
ditional feature functions of the translation rules.
The decoder is able to handle an arbitrary num-
ber of non-terminal symbols. The extraction has
been extended so that the extraction of SAMT-
rules is included (Zollmann and Venugopal, 2006)
but this approach is not fully supported (there
may be empty parses due to the extended num-
ber of non-terminals). We instead opted to sup-
port the generalization presented in (Venugopal et
al., 2009), where the information about the new
non-terminals is included as an additional feature
in the log-linear model.
In addition, dependency information in the
spirit of (Shen et al, 2008) is included. Jane fea-
tures models for string-to-dependency language
models and computes various scores based on the
well-formedness of the resulting dependency tree.
Jane supports the Stanford parsing format,1 but
can be easily extended to other parsers.
3.4 Additional Reordering Models
In the standard formulation of the hierarchical
phrase-based translation model two additional
rules are added:
S ? ?S?0X?1, S?0X?1?
S ? ?X?0, X?0?
(1)
This allows for a monotonic concatenation of
phrases, very much in the way monotonic phrase-
based translation is carried out.
It is a well-known fact that for phrase-based
translation, the use of additional reordering mod-
els is a key component, essential for achieving
good translation quality. In the hierarchical model,
the reordering is already integrated in the transla-
tion formalism, but there are still cases where the
required reorderings are not captured by the hier-
archical phrases alone.
The flexibility of the grammar formalism allows
us to add additional reordering models without the
need to explicitely modify the code for supporting
them. The most straightforward example would
1http://nlp.stanford.edu/software/lex-parser.shtml
263
be to include the ITG-Reorderings (Wu, 1997), by
adding following rule
S ? ?S?0S?1, S?1S?0? (2)
We can also model other reordering constraints.
As an example, phrase-level IBM reordering con-
straints with a window length of 1 can be included
substituting the rules in Equation (1) with follow-
ing rules
S ? ?M?0,M?0?
S ? ?M?0S?1,M?0S?1?
S ? ?B?0M?1,M?1B?0?
M ? ?X?0, X?0?
M ? ?M?0X?1,M?0X?1?
B ? ?X?0, X?0?
B ? ?B?0X?1, X?1B?0?
(3)
In these rules we have added two additional non-
terminals. The M non-terminal denotes a mono-
tonic block and the B non-terminal a back jump.
Actually both of them represent monotonic trans-
lations and the grammar could be simplified by
using only one of them. Separating them allows
for more flexibility, e.g. when restricting the jump
width, where we only have to restrict the maxi-
mum span width of the non-terminal B. These
rules can be generalized for other reordering con-
straints or window lengths.
Additionally distance-based costs can be com-
puted for these reorderings. To the best of our
knowledge, this is the first time such additional
reorderings have been applied to the hierarchical
phrase-based approach.
3.5 Extended Lexicon Models
We enriched Jane with the ability to score hy-
potheses with discriminative and trigger-based
lexicon models that use global source sentence
context and are capable of predicting context-
specific target words. This approach has recently
been shown to improve the translation results of
conventional phrase-based systems. In this sec-
tion, we briefly review the basic aspects of these
extended lexicon models. They are similar to
(Mauser et al, 2009), and we refer there for a more
detailed exposition on the training procedures and
results in conventional phrase-based decoding.
Note that the training for these models is not
distributed together with Jane.
3.5.1 Discriminative Word Lexicon
The first of the two lexicon models is denoted as
discriminative word lexicon (DWL) and acts as a
statistical classifier that decides whether a word
from the target vocabulary should be included in
a translation hypothesis. For that purpose, it con-
siders all the words from the source sentence, but
does not take any position information into ac-
count, i.e. it operates on sets, not on sequences or
even trees. The probability of a word being part
of the target sentence, given a set of source words,
are decomposed into binary features, one for each
source vocabulary entry. These binary features are
combined in a log-linear fashion with correspond-
ing feature weights. The discriminative word lex-
icon is trained independently for each target word
using the L-BFGS (Byrd et al, 1995) algorithm.
For regularization, Gaussian priors are utilized.
DWL model probabilities are computed as
p(e|f) =
?
e?VE
p(e?|f) ?
?
e?e
p(e+|f)
p(e?|f)
(4)
with VE being the target vocabulary, e the set of
target words in a sentence, and f the set of source
words, respectively. Here, the event e+ is used
when the target word e is included in the target
sentence and e? if not. As the left part of the prod-
uct in Equation (4) is constant given a source sen-
tence, it can be dropped, which enables us to score
partial hypotheses during search.
3.5.2 Triplet Lexicon
The second lexicon model we employ in Jane,
the triplet lexicon model, is in many aspects re-
lated to IBM model 1 (Brown et al, 1993), but
extends it with an additional word in the con-
ditioning part of the lexical probabilities. This
introduces a means for an improved representa-
tion of long-range dependencies in the data. Like
IBM model 1, the triplets are trained iteratively
with the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). Jane implements
the so-called inverse triplet model p(e|f, f ?).
The triplet lexicon model score t(?) of the ap-
plication of a rule X ? ??, ?? where (?, ?) is
a bilingual phrase pair that may contain symbols
from the non-terminal set is computed as
t(?, ?, fJ0 ) = (5)
?
?
e
log
?
?
2
J ? (J + 1)
?
j
?
j?>j
p(e|fj , fj?)
?
?
264
with e ranging over all terminal symbols in the tar-
get part ? of the rule. The second sum selects all
words from the source sentence fJ0 (including the
empty word that is denoted as f0 here). The third
sum incorporates the rest of the source sentence
right of the first triggering word. The order of
the triggers is not relevant because per definition
p(e|f, f ?) = p(e|f ?, f), i.e. the model is symmet-
ric. Non-terminals in ? have to be skipped when
the rule is scored.
In Jane, we also implemented scoring for a vari-
ant of the triplet lexicon model called the path-
constrained (or path-aligned) triplet model. The
characteristic of path-constrained triplets is that
the first trigger f is restricted to the aligned target
word e. The second trigger f ? is allowed to move
along the whole remaining source sentence. For
the training of the model, we use word alignment
information obtained by GIZA++ (Och and Ney,
2003). To be able to apply the model in search,
Jane has to be run with a phrase table that con-
tains word alignment for each phrase, too, with the
exception of phrases which are composed purely
of non-terminals. Jane?s phrase extraction can op-
tionally supply this information from the training
data.
(Hasan et al, 2008) and (Hasan and Ney, 2009)
employ similar techniques and provide some more
discussion on the path-aligned variant of the
model and other possible restrictions.
3.6 Forced Alignments
Jane has also preliminary support for forced align-
ments between a given source and target sentence.
Given a sentence in the source language and its
translation in the target language, we find the best
way the source sentence can be translated into
the given target sentence, using the available in-
ventory of phrases. This is needed for more ad-
vanced training approaches like the ones presented
in (Blunsom et al, 2008) or (Cmejrek et al, 2009).
As reported in these papers, due to the restrictions
in the phrase extraction process, not all sentences
in the training corpus can be aligned in this way.
3.7 Optimization Methods
Two method based on n-best for minimum error
rate training (MERT) of the parameters of the log-
linear model are included in Jane. The first one
is the procedure described in (Och, 2003), which
has become a standard in the machine translation
community. We use an in-house implementation
of the method.
The second one is the MIRA algorithm, first
applied for machine translation in (Chiang et al,
2009). This algorithm is more adequate when the
number of parameters to optimize is large.
If the Numerical Recipes library (Press et al,
2002) is available, an additional general purpose
optimization tool is also compiled. Using this
tool a single-best optimization procedure based on
the downhill simplex method (Nelder and Mead,
1965) is included. This method, however, can be
considered deprecated in favour of the above men-
tioned methods.
3.8 Parallelized operation
If the Sun Grid Engine2 is available, all operations
of Jane can be parallelized. For the extraction pro-
cess, the corpus is split into chunks (the granular-
ity being user-controlled) which are distributed in
the computer cluster. Count collection, marginal
computation and count normalization all happens
in an automatic and parallel manner.
For the translation process a batch job is started
on a number of computers. A server distributes the
sentences to translate to the computers that have
been made available to the translation job.
The optimization process also benefits from
the parallelized optimization. Additionally, for
the minimum error rate training methods, random
restarts may be performed on different computers
in a parallel fashion.
The same client-server infrastructure used for
parallel translation may also be reused for inter-
active systems. Although no code in this direction
is provided, one would only need to implement a
corresponding frontend which communicates with
the translation server (which may be located on an-
other machine).
3.9 Extensibility
One of the goals when implementing the toolkit
was to make it easy to extend it with new features.
For this, an abstract class was created which we
called secondary model. New models need only to
derive from this class and implement the abstract
methods for data reading and costs computation.
This allows for an encapsulation of the computa-
tions, which can be activated and deactivated on
demand. The models described in Sections 3.3
2http://www.sun.com/software/sge/
265
through 3.5 are implemented in this way. We thus
try to achieve loose coupling in the implementa-
tion.
In addition a flexible prefix tree implementation
with on-demand loading capabilities is included as
part of the code. This class has been used for im-
plementing on-demand loading of phrases in the
spirit of (Zens and Ney, 2007) and the on-demand
n-gram format described in Section 3.2, in addi-
tion to some intermediate steps in the phrase ex-
traction process. The code may also be reused in
other, independent projects.
3.10 Code
The main core of Jane has been implemented in
C++. Our guideline was to write code that was
correct, maintainable and efficient. We tried to
achieve correctness by means of unit tests inte-
grated in the source as well as regression tests. We
also defined a set of coding guidelines, which we
try to enforce in order to have readable and main-
tainable code. Examples include using descriptive
variable names, appending an underscore to pri-
vate members of classes or having each class name
start with an uppercase letter while variable names
start with lowercase letters.
The code is documented at great length using
the doxygen system,3 and the filling up of the
missing parts is an ongoing effort. Every tool
comes with an extensive help functionality, and
the main tools also have their own man pages.
As for efficiency we always try to speed up the
code and reduce memory consumption by imple-
menting better algorithms. We try to avoid ?dark
magic programming methods? and hard to follow
optimizations are only applied in critical parts of
the code. We try to document every such occur-
rence.
4 Experimental Results
In this section we will present some experimental
results obtained using Jane. We will pay special
attention to the performance of the new reordering
and lexicon models presented in this paper. We
will present results on three different large-scale
tasks and language pairs.
Additionally RWTH participated in this year?s
WMT evaluation, where Jane was one of the sub-
mitted systems. We refer to the system description
for supplementary experimental results.
3http://www.doxygen.org
dev test
System BLEU TER BLEU TER
Jane baseline 24.2 59.5 25.4 57.4
+ reordering 25.2 58.2 26.5 56.1
Table 1: Results for Europarl German-English
data. BLEU and TER results are in percentage.
4.1 Europarl Data
The first task is the Europarl as defined in the
Quaero project. The main part of the corpus in
this task consists of the Europarl corpus as used in
the WMT evaluation (Callison-Burch et al, 2009),
with some additional data collected in the scope of
the project.
We tried the reordering approach presented in
Section 3.4 on the German-English language pair.
The results are shown in Table 1. As can be seen
from these results, the additional reorderings ob-
tain nearly 1% improvement both in BLEU and
TER scores. Regrettably for this corpus the ex-
tended lexicon models did not bring any improve-
ments.
Table 2 shows the results for the French-English
language pair of the Europarl task. On this task
the extended lexicon models yield an improve-
ment over the baseline system of 0.9% in BLEU
and 0.9% in TER on the test set.
4.2 NIST Arabic-English
We also show results on the Arabic-English
NIST?08 task, using the NIST?06 set as develop-
ment set. It has been reported in other work that
the hierarchical system is not competitive with a
phrase-based system for this language pair (Birch
et al, 2009). We report the figures of our state-
of-the-art phrase-based system as comparison (de-
noted as PBT).
As can be seen from Table 3, the baseline
Jane system is in fact 0.6% worse in BLEU and
1.0% worse in TER than the baseline PBT sys-
tem. When we include the extended lexicon mod-
els we see that the difference in performance is re-
duced. For Jane the extended lexicon models give
an improvement of up to 1.9% in BLEU and 1.7%
in TER, respectively, bringing the system on par
with the PBT system extended with the same lex-
icon models, and obtaining an even slightly better
BLEU score.
266
dev test
BLEU TER BLEU TER
Baseline 30.0 52.6 31.1 50.0
DWL 30.4 52.2 31.4 49.6
Triplets 30.4 52.0 31.7 49.4
path-constrained Triplets 30.3 52.1 31.6 49.3
DWL + Triplets 30.7 52.0 32.0 49.1
DWL + path-constrained Triplets 30.8 51.7 31.6 49.3
Table 2: Results for the French-English task. BLEU and TER results are in percentage.
dev (MT?06) test (MT?08)
Jane PBT Jane PBT
BLEU TER BLEU TER BLEU TER BLEU TER
Baseline 43.2 50.8 44.1 49.4 44.1 50.1 44.7 49.1
DWL 45.3 48.7 45.1 48.4 45.6 48.4 45.6 48.4
Triplets 44.4 49.1 44.6 49.2 45.3 48.8 44.9 49.0
path-constrained Triplets 44.3 49.4 44.7 49.1 44.9 49.3 45.3 48.7
DWL + Triplets 45.0 48.9 45.1 48.5 45.3 48.6 45.5 48.5
DWL + path-constrained Triplets 45.2 48.8 45.1 48.6 46.0 48.5 45.8 48.3
Table 3: Results for the Arabic-English task. BLEU and TER results are in percentage.
5 Discussion
We feel that the hierarchical phrase-based transla-
tion approach still shares some shortcomings con-
cerning lexical selection with conventional phrase-
based translation. Bilingual lexical context be-
yond the phrase boundaries is barely taken into
account by the base model. In particular, if only
one generic non-terminal is used, the selection of
a sub-phrase that fills the gap of a hierarchical
phrase is not affected by the words composing the
phrase it is embedded in ? except for the language
model score. This shortcoming is one of the issues
syntactically motivated models try to address.
The extended lexicon models analyzed in this
work also try to address this issue. One can con-
sider that they complement the efforts that are be-
ing made on a deep structural level within the hi-
erarchical approach. Though they are trained on
surface forms only, without any syntactic informa-
tion, they still operate at a scope that exceeds the
capability of common feature sets of standard hi-
erarchical phrase-based SMT systems.
As the experiments in Section 4 show, the ef-
fect of these extended lexicon models is more im-
portant for the hierarchical phrase-based approach
than for the phrase-based approach. In our opinion
this is probably mainly due to the higher flexibil-
ity of the hierarchical system, both because of its
intrinsic nature and because of the higher number
of phrases extracted by the system. The scoring
of the phrases is still carried out by simple relative
frequencies, which seem to be insufficient. The
additional lexicon models seem to help in this re-
spect.
5.1 Short Comparison with Joshua
As mentioned in Section 2, Joshua is the most
similar decoder to our own. It was developed in
parallel at the Johns Hopkins University and it is
267
System words/sec
Joshua 11.6
Jane cube prune 15.9
Jane cube grow 60.3
Table 4: Speed comparison Jane vs. Joshua. We
measure the translated words per second.
currently used by a number of groups around the
world.
Jane was started separately and independently.
In their basic working mode, both systems imple-
ment parsing using a synchronous grammar and
include language model information. Each of the
projects then progressed independently, most of
the features described in Section 3 being only
available in Jane.
Efficiency is one of the points where we think
Jane outperforms Joshua. One of the reasons can
well be the fact that it is written in C++ while
Joshua is written in Java. In order to compare run-
ning times we converted a grammar extracted by
Jane to Joshua?s format and adapted the parame-
ters accordingly. To the best of our knowledge we
configured both decoders to perform the same task
(cube pruning, 300-best generation, same pruning
parameters). Except for some minor differences4
the results were equal.
We tried this setup on the IWSLT?08 Arabic to
English translation task. The speed results (mea-
sured in translated words per second) can be seen
in Table 4. Jane operating with cube prune is
nearly 50% faster than Joshua, at the same level
of translation performance. If we switch to cube
grow, the speed difference is even bigger, with
a speedup of nearly 4 times. However this usu-
ally comes with a penalty in BLEU score (nor-
mally under 0.5% BLEU in our experience). This
increased speed can be specially interesting for
applications like interactive machine translation
or online translation services, where the response
time is critical and sometimes even more impor-
tant than a small (and often hardly noticeable) loss
in translation quality.
Another important point concerning efficiency
is the startup time. Thanks to the binary format
described in Section 3.9, there is virtually no delay
4E.g. the OOVs seem to be handled in a slightly different
way, as the placement was sometimes different.
in the loading of the phrase table in Jane.5 In fact
Joshua?s long phrase table loading times were the
main reason the performance measures were done
on a small corpus like IWSLT instead of one of the
large tasks described in Section 4.
We want to make clear that we did not go into
great depth in the workings of Joshua, just stayed
at the basic level described in the manual. This
tool is used also for large-scale evaluations and
hence there certainly are settings for dealing with
these big tasks. Therefore this comparison has to
be taken with a grain of salt.
We also want to stress that we explicitly chose
to leave translation results out of this comparison.
Several different components have great impact
on translation quality, including phrase extraction,
minimum error training and additional parameter
settings of the decoder. As we pointed out we
do not have the expertise in Joshua to perform all
these tasks in an optimal way, and for that reason
we did not include such a comparison. However,
both JHU and RWTH participated in this year?s
WMT evaluation, where the systems, applied by
their respective authors, can be directly compared.
And in no way do we see Joshua and Jane as
?competing? systems. Having different systems
is always enriching, and particularly as system
combination shows great improvements in trans-
lation quality, having several alternative systems
can only be considered a positive situation.
6 Licensing
Jane is distributed under a custom open source
license. This includes free usage for non-
commercial purposes as long as any changes made
to the original software are published under the
terms of the same license. The exact formulation
is available at the download page for Jane.
7 Conclusion
With Jane, we release a state-of-the-art hi-
erarchical toolkit to the scientific community
and hope to provide a good starting point for
fellow researchers, allowing them to have a
solid system even if the research field is new
to them. It is available for download from
http://www.hltpr.rwth-aachen.de/jane. The
system in its current state is stable and efficient
enough to handle even large-scale tasks such as
5There is, however, still some delay when loading the lan-
guage model for some of the supported formats.
268
the WMT and NIST evaluations, while producing
highly competitive results.
Moreover, we presented additional reordering
and lexicon models that further enhance the per-
formance of the system.
And in case you are wondering, Jane is Just an
Acronym, Nothing Else. The name comes from
the character in the Ender?s Game series (Card,
1986).
Acknowledgments
Special thanks to the people who have contributed
code to Jane: Markus Freitag, Stephan Peitz, Car-
men Heger, Arne Mauser and Niklas Hoppe.
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation, and also partly based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. HR001-06-C-0023. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the DARPA.
References
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A Quantitative Analysis of Reordering Phe-
nomena. In Proc. of the Workshop on Statistical Ma-
chine Translation, pages 197?205, Athens, Greece,
March.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A Discriminative Latent Variable Model for Statis-
tical Machine Translation. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 200?208, Columbus, Ohio,
June.
Peter F. Brown, Stephan A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311, June.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. 1995. A Limited Memory Algorithm
for Bound Constrained Optimization. SIAM Journal
on Scientific Computing, 16(5):1190?1208.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. of the Workshop on Statistical Machine Trans-
lation, pages 1?28, Athens, Greece, March.
Orson Scott Card. 1986. Speaker for the Dead. Tor
Books.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochas-
tic CFG. In Proc. of the First Workshop on Tab-
ulation in Parsing and Deduction, pages 133?137,
Paris, France, April.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new Features for Statistical Machine Trans-
lation. In Proc. of the Human Language Technology
Conference / North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 218?226, Boulder, Colorado, June.
David Chiang. 2007. Hierarchical Phrase-based
Translation. Computational Linguistics, 33(2):201?
228, June.
Martin Cmejrek, Bowen Zhou, and Bing Xiang. 2009.
Enriching SCFG Rules Directly From Efficient
Bilingual Chart Parsing. In Proc. of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 136?143, Tokyo, Japan.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the Royal
Statistical Society Series B, 39(1):1?22.
Sas?a Hasan and Hermann Ney. 2009. Comparison of
Extended Lexicon Models in Search and Rescoring
for SMT. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), vol-
ume short papers, pages 17?20, Boulder, CO, USA,
June.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet Lexicon Mod-
els for Statistical Machine Translation. In Proc. of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP), pages 372?381.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
144?151, Prague, Czech Republic, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 177?180,
Prague, Czech Republic, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An Open Source Toolkit for Parsing-Based Machine
Translation. In Proc. of the Workshop on Statisti-
cal Machine Translation, pages 135?139, Athens,
Greece, March.
269
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conference on Empirical Methods
for Natural Language Processing (EMNLP), pages
210?218, Singapore, August.
John A. Nelder and Roger Mead. 1965. The Downhill
Simplex Method. Computer Journal, 7:308.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
Recipes in C++. Cambridge University Press, Cam-
bridge, UK.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio, June.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Interna-
tional Conference on Spoken Language Processing
(ICSLP), volume 3, pages 901?904, Denver, Col-
orado, September.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom Filter Language Models: Tera-scale LMs on
the Cheap. In Proc. of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 468?476, Prague, Czech
Republic, June.
Ashish Venugopal, Andreas Zollmann, N.A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
Softening Syntactic Constraints to Improve Statis-
tical Machine Translation. In Proc. of the Human
Language Technology Conference / North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 236?244, Boulder,
Colorado, June.
David Vilar and Hermann Ney. 2009. On LM Heuris-
tics for the Cube Growing Algorithm. In Proc. of
the Annual Conference of the European Association
for Machine Translation (EAMT), pages 242?249,
Barcelona, Spain, May.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing Soft Syntax Features and Heuristics for
Hierarchical Phrase Based Machine Translation.
In Proc. of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 190?197,
Waikiki, Hawaii, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2007. Efficient
Phrase-Table Representation for Machine Transla-
tion with Applications to Online MT and Speech
Translation. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 492?499, Rochester, New York, April.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax Augmented Machine Translation via Chart Pars-
ing. In Proc. of the Human Language Technology
Conference / North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 138?141, New York, June.
270
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65?70,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Evaluate with Confidence Estimation: Machine ranking of translation
outputs using grammatical features
Eleftherios Avramidis, Maja Popovic, David Vilar, Aljoscha Burchardt
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
name.surname@dfki.de
Abstract
We present a pilot study on an evaluation
method which is able to rank translation out-
puts with no reference translation, given only
their source sentence. The system employs a
statistical classifier trained upon existing hu-
man rankings, using several features derived
from analysis of both the source and the tar-
get sentences. Development experiments on
one language pair showed that the method
has considerably good correlation with human
ranking when using features obtained from a
PCFG parser.
1 Introduction
Automatic evaluation metrics for Machine Transla-
tion (MT) have mainly relied on analyzing both the
MT output against (one or more) reference transla-
tions. Though, several paradigms in Machine Trans-
lation Research pose the need to estimate the quality
through many translation outputs, when no reference
translation is given (n-best rescoring of SMT sys-
tems, system combination etc.). Such metrics have
been known as Confidence Estimation metrics and
quite a few projects have suggested solutions on this
direction. With our submission to the Shared Task,
we allow such a metric to be systematically com-
pared with the state-of-the-art reference-aware MT
metrics.
Our approach suggests building a Confidence Es-
timation metric using already existing human judg-
ments. This has been motivated by the existence
of human-annotated data containing comparisons of
the outputs of several systems, as a result of the
evaluation tasks run by the Workshops on Statistical
Machine Translation (WMT) (Callison-Burch et al,
2008; Callison-Burch et al, 2009; Callison-Burch
et al, 2010). This amount of data, which has been
freely available for further research, gives an op-
portunity for applying machine learning techniques
to model the human annotators? choices. Machine
Learning methods over previously released evalua-
tion data have been already used for tuning com-
plex statistical evaluation metrics (e.g. SVM-Rank
in Callison-Burch et al (2010)). Our proposition
is similar, but works without reference translations.
We develop a solution of applying machine learning
in order to build a statistical classifier that performs
similar to the human ranking: it is trained to rank
several MT outputs, given analysis of possible qual-
itative criteria on both the source and the target side
of every given sentence. As qualitative criteria, we
use statistical features indicating the quality and the
grammaticality of the output.
2 Automatic ranking method
2.1 From Confidence Estimation to ranking
Confidence estimation has been seen from the Nat-
ural Language Processing (NLP) perspective as a
problem of binary classification in order to assess
the correctness of a NLP system output. Previ-
ous work focusing on Machine Translation includes
statistical methods for estimating correctness scores
or correctness probabilities, following a rich search
over the spectrum of possible features (Blatz et al,
2004a; Ueffing and Ney, 2005; Specia et al, 2009;
Raybaud and Caroline Lavecchia, 2009; Rosti et al,
65
2007).
In this work we slightly transform the binary clas-
sification practice to fit the standard WMT human
evaluation process. As human annotators have pro-
vided their evaluation in the form of ranking of five
system outputs at a sentence level, we build our eval-
uation mechanism with similar functionality, aim-
ing to training from and evaluating against this data.
Evaluation scores and results can be then calculated
based on comparative analysis of the performance of
each system.
Whereas latest work, such as Specia et al (2010),
has focused on learning to assess segment perfor-
mance independently for each system output, our
contribution measures the performance by compar-
ing the system outputs with each other and con-
sequently ranking them. The exact method is de-
scribed below.
2.2 Internal pairwise decomposition
We build one classifier over all input sentences.
While the evaluation mechanism is trained and eval-
uated on a multi-class (ranking) basis as explained
above, the classifier is expected to work on a binary
level: we provide the features from the analysis of
the two system outputs and the source, and the clas-
sifier should decide if the first system output is better
than the second one or not.
In order to accomplish such training, the n sys-
tems? outputs for each sentence are broken down to
n ? (n ? 1) pairs, of all possible comparisons be-
tween two system outputs, in both directions (sim-
ilar to the calculation of the Spearman correlation).
For each pair, the classifier is trained with a class
value c, for the pairwise comparison of system out-
puts ti and tj with respective ranks ri and rj , deter-
mined as:
c(ri, rj) =
{
1 ri < rj
?1 ri > rj
At testing time, after the classifier has made all
the pairwise decisions, those need to be converted
back to ranks. System entries are ordered, according
to how many times each of them won in the pair-
wise comparison, leading to rank lists similar to the
ones provided by human annotators. Note that this
kind of decomposition allows for ties when there are
equal times of winnings.
2.3 Acquiring features
In order to obtain features indicating the quality of
the MT output, automatic NLP analysis tools are ap-
plied on both the source and the two target (MT-
generated) sentences of every pairwise comparison.
Features considered can be seen in the following cat-
egories, according to their origin:
? Sentence length: Number of words of source
and target sentences, source-length to target-
length ratio.
? Target language model: Language models
provide statistics concerning the correctness of
the words? sequence on the target language.
Such language model features include:
? the smoothed n-gram probability of the
entire target sentence for a language
model of order 5, along with
? uni-gram, bi-gram, tri-gram probabilities
and a
? count of unknown words
? Parsing: Processing features acquired from
PCFG parsing (Petrov et al, 2006) for both
source and target side include:
? parse log likelihood,
? number of n-best trees,
? confidence for the best parse,
? average confidence of all trees.
Ratios of the above target features to their re-
spective source features were included.
? Shallow grammatical match: The number of
occurences of particular node tags on both the
source and the target was counted on the PCFG
parses. In particular, NPs, VPs, PPs, NNs and
punctuation occurences were counted. Then
the ratio of the occurences of each tag in the
target sentence by its occurences on the source
sentence was also calculated.
2.4 Classifiers
The machine learning core of the system was built
supporting two classification approaches.
66
? Na?ve Bayes allows prediction of a binary
class, given the assumption that the features are
statistically independent.
p(C,F1, . . . , Fn) = p(C)
i=1?
n
p(Fi|C)
p(C) is estimated by relative frequencies of
the training pairwise examples, while p(Fi|C)
for our continuous features are estimated with
LOESS (locally weighted linear regression
similar to Cleveland (1979))
? k-nearest neighbour (knn) algorithm allows
classifying based on the closest training exam-
ples in the feature space.
3 Experiment
3.1 Experiment setup
A basic experiment was designed in order to deter-
mine the exact setup and the feature set of the metric
prior to the shared task submission. The classifiers
for the task were learnt using the German-English
testset of the WMT 2008 and 2010 (about 700 sen-
tences)1. For testing, the classifiers were used to per-
form ranking on a test set of 184 sentences which
had been kept apart from the 2010 data, with the cri-
terion that they do not contain contradictions among
human judgments.
In order to allow further comparison with other
evaluation metrics, we performed an extended ex-
periment: we trained the classifiers over the WMT
2008 and 2009 data and let them perform automatic
ranking on the full WMT 2010 test set, this time
without any restriction on human evaluation agree-
ment.
In both experiments, tokenization was performed
with the PUNKT tokenizer (Kiss et al, 2006; Gar-
rette and Klein, 2009), while n-gram features were
generated with the SRILM toolkit (Stolcke, 2002).
The language model was relatively big and had been
built upon all lowercased monolingual training sets
for the WMT 2011 Shared Task, interpolated on
the 2007 test set. As a PCFG parser, the Berkeley
Parser (Petrov and Klein, 2007) was preferred, due
1data acquired from http://www.statmt.org/wmt11
to the possibility of easily obtaining complex inter-
nal statistics, including n-best trees. Unfortunately,
the time required for parsing leads to significant de-
lays at the overall processing. The machine learn-
ing algorithms were implemented with the Orange
toolkit (Dem?ar et al, 2004).
3.2 Feature selection
Although the automatic NLP tools provided a lot of
features (section 2.3), the classification methods we
used (and particularly na?ve Bayes were the develop-
ment was focused on) would be expected to perform
better given a smaller group of statistically inde-
pendent features. Since exhaustive training/testing
of all possible feature subsets was not possible,
we performed feature selection based on the Reli-
eff method (Kononenko, 1994; Kira and Rendell,
1992). Automatic ranking was performed based on
the most promising feature subsets. The results are
examined below.
3.3 Results
The performance of the classifier is measured after
the classifier output has been converted back to rank
lists, similar to the WMT 2010 evaluation. We there-
fore calculated two types of rank coefficients: aver-
aged Kendall?s tau on a segment level, and Spear-
man?s rho on a system level, based on the percentage
that the each system?s translations performed better
than or equal to the translations of any other system.
The results for the various combinations of fea-
tures and classifiers are depicted on Table 1. Na?ve
Bayes provides the best score on the test set, with
? = 0.81 on a system level and ? = 0.26 on a
segment level, trained with features including the
number of the unknown words, the source-length
by target-length ratio, the VP count ratio and the
source-target ratio of the parsing log-likelihood. The
number of unknown words particularly appears to be
a strong indicator for the quality of the sentence. On
the first part of the table we can also observe that
language model features do not perform as well as
the features deriving from the processing informa-
tion delivered by the parser. On the second part of
the table we compare the use of various grammatical
combinations. The third part contains the correlation
obtained by various similar internal parsing-related
features.
67
features na?ve Bayes knn
rho tau rho tau
basic experiment
ngram 0.19 0.05 0.13 0.01
unk, len 0.67 0.20 0.73 0.24
unk, len, bigram 0.61 0.21 0.74 0.21
unk, len, ngram 0.63 0.19 0.59 0.21
unk, len, trigram 0.67 0.20 0.76 0.21
unk, len, logparse 0.75 0.21 0.74 0.25
unk, len, nparse, VP 0.67 0.24 0.61 0.20
unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24
unk, len, nparse, NP, confbestparse 0.78 0.23 0.74 0.23
unk, len, nparse, VP, confavg 0.75 0.21 0.78 0.23
unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24
unk, len, nparse, VP, logparse 0.81 0.26 0.75 0.23
extended experiment
unk, len, nparse, VP, logparse 0.60 0.23 0.28 0.02
Table 1: System-level Spearman?s rho and segment-level Kendall?s tau correlation coefficients achieved on automatic
ranking (average absolute value)
The correlation coefficients of the extended exper-
iment, allowing comparison with last year?s shared
task, are shown on the last line of the table. With
coefficients ? = 0.60 and ? = 0.23, our metric
performs relatively low compared to the other met-
rics of WMT10 (indicatively iBLEU: ? = 0.95,
? = 0.39 according to Callison-Burch et al (2010).
Though, it still has a position in the list, scoring bet-
ter than several other reference-aware metrics (e.g.
of ? = 0.47 and ? = 0.12 respectively) for the par-
ticular language pair.
4 Discussion
A concern on the use of Confidence Estimation for
MT evaluation has to do with the possibility of a
system ?tricking? such metrics. This would for ex-
ample be the case when a system offers a well-
formed candidate translation and gets a good score,
despite having no relation to the source sentence
in terms of meaning. We should note that we are
not capable of fully investigating this case based
on the current set of experiments, because all of
the systems in our data sets have shown acceptable
scores (11-25 BLEU and 0.58-0.78 TERp accord-
ing to Callison-Burch et al (2010)), when evaluated
against reference translations. Though, we would
assume that we partially address this problem by us-
ing ratios of source to target features (length, syn-
tactic constituents), which means that in order for a
sentence to trick the metric, it would need a com-
parable sentence length and a grammatical structure
that would allow it to achieve feature ratios similar
to the other systems? outputs. Previous work (Blatz
et al, 2004b; Ueffing and Ney, 2005) has used fea-
tures based on word alignment, such as IBM Mod-
els, which would be a meaningful addition from this
aspect.
Although k-nearest-neighbour is considered to be
a superior classifier, best results are obtained by
na?ve Bayes. This may have been due of the fact
that feature selection has led to small sets of uncor-
related features, where na?ve Bayes is known to per-
form well. K-nearest-neighbour and other complex
classification methods are expected to prove useful
when more complex feature sets are employed.
5 Conclusion and Further work
The experiments presented in this article indicate
that confidence metrics trained over human rankings
can be possibly used for several tasks of evaluation,
given particular conditions, where e.g. there is no
reference translation given. Features obtained from
68
a PCFG parser seem to be leading to better correla-
tions, given our basic test set. Although correlation
is not particularly high, compared to other reference-
aware metrics in WMT 10, there is clearly a poten-
tial for further improvement.
Nevertheless this is still a small-scale experiment,
given the restricted data size and the single transla-
tion direction. The performance of the system on
broader training and test sets will be evaluated in the
future. Feature selection is also subject to change
if other language pairs are introduced, while more
sophisticated machine learning algorithms, allowing
richer feature sets, may also lead to better results.
Acknowledgments
This work was done with the support of the
TaraXU? Project2, financed by TSB Technologie-
stiftung Berlin?Zukunftsfonds Berlin, co-financed
by the European Union?European fund for regional
development.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004a. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004b. Confidence estimation for
machine translation. In M. Rollins (Ed.), Mental Im-
agery. Yale University Press.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2http://taraxu.dfki.de
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
William S. Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American statistical association, 74(368):829?836.
Janez Dem?ar, Blaz Zupan, Gregor Leban, and Tomaz
Curk. 2004. Orange: From experimental machine
learning to interactive data mining. In Principles of
Data Mining and Knowledge Discovery, pages 537?
539.
Dan Garrette and Ewan Klein. 2009. An extensi-
ble toolkit for computational semantics. In Proceed-
ings of the Eighth International Conference on Com-
putational Semantics, IWCS-8 ?09, pages 116?127,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Kenji Kira and Larry A. Rendell. 1992. The feature se-
lection problem: traditional methods and a new algo-
rithm. In Proceedings of the tenth national conference
on Artificial intelligence, AAAI?92, pages 129?134.
AAAI Press.
Tibor Kiss, Jan Strunk, Ruhr universit?t Bochum, and
Ruhr universit?t Bochum. 2006. Unsupervised mul-
tilingual sentence boundary detection. In Proceedings
of IICS-04, Guadalajara, Mexico and Springer LNCS
3473.
Igor Kononenko. 1994. Estimating attributes: analy-
sis and extensions of relief. In Proceedings of the
European conference on machine learning on Ma-
chine Learning, pages 171?182, Secaucus, NJ, USA.
Springer-Verlag New York, Inc.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In In HLT-NAACL ?07.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In In ACL ?06, pages 433?
440.
Sylvain Raybaud and Kamel Smaili Caroline Lavecchia,
David Langlois. 2009. Word-and sentence-level con-
fidence measures for machine translation. In Euro-
pean Association of Machine Translation 2009.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of the North
American Chapter of the Association for Compu-
tational Linguistics Human Language Technologies,
pages 228?235.
69
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009. Improv-
ing the confidence of machine translation quality es-
timates. In Machine Translation Summit XII, Ottawa,
Canada.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estimation.
Machine Translation, 24:39?50, March.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP 2002, pages 901?904.
Nicola Ueffing and Hermann Ney. 2005. Word-level
confidence estimation for machine translation using
phrase-based translation models. Computational Lin-
guistics, pages 763?770.
70
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99?103,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Evaluation without references:
IBM1 scores as evaluation metrics
Maja Popovic?, David Vilar, Eleftherios Avramidis, Aljoscha Burchardt
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
name.surname@dfki.de
Abstract
Current metrics for evaluating machine trans-
lation quality have the huge drawback that
they require human-quality reference transla-
tions. We propose a truly automatic evalua-
tion metric based on IBM1 lexicon probabili-
ties which does not need any reference transla-
tions. Several variants of IBM1 scores are sys-
tematically explored in order to find the most
promising directions. Correlations between
the new metrics and human judgments are cal-
culated on the data of the third, fourth and fifth
shared tasks of the Statistical Machine Trans-
lation Workshop. Five different European lan-
guages are taken into account: English, Span-
ish, French, German and Czech. The results
show that the IBM1 scores are competitive
with the classic evaluation metrics, the most
promising being IBM1 scores calculated on
morphemes and POS-4grams.
1 Introduction
Currently used evaluation metrics such as BLEU (Pa-
pineni et al, 2002), METEOR (Banerjee and Lavie,
2005), etc. are based on the comparison between
human reference translations and the automatically
generated hypotheses in the target language to be
evaluated. While this scenario helps in the design
of machine translation systems, it has two major
drawbacks. The first one is the practical criticism
that using reference translations is inefficient and ex-
pensive: in real-life situations, the quality of ma-
chine translation must be evaluated without having
to pay humans for producing reference translations
first. The second criticism is methodological: in
using reference translation, the problem of evalu-
ating translation quality (e.g., completeness, order-
ing, domain fit, etc.) is transformed into a kind of
paraphrase evaluation in the target language, which
is a very difficult problem itself. In addition, the
set of selected references always represents only a
small subset of all good translations. To remedy
these drawbacks, we propose a truly automatic eval-
uation metric which is based on the IBM1 lexicon
scores (Brown et al, 1993).
The inclusion of IBM1 scores in translation sys-
tems has shown experimentally to improve transla-
tion quality (Och et al, 2003). They also have been
used for confidence estimation for machine transla-
tion (Blatz et al, 2003). To the best of our knowl-
edge, these scores have not yet been used as an eval-
uation metric.
We carry out a systematic comparison between
several variants of IBM1 scores. The Spearman?s
rank correlation coefficients on the document (sys-
tem) level between the IBM1 metrics and the hu-
man ranking are computed on the English, French,
Spanish, German and Czech texts generated by var-
ious translation systems in the framework of the
third (Callison-Burch et al, 2008), fourth (Callison-
Burch et al, 2009) and fifth (Callison-Burch et al,
2010) shared translation tasks.
2 IBM1 scores
The IBM1 model is a bag-of-word translation model
which gives the sum of all possible alignment proba-
bilities between the words in the source sentence and
the words in the target sentence. Brown et al (1993)
defined the IBM1 probability score for a translation
99
pair fJ1 and eI1 in the following way:
P (fJ1 |eI1) =
1
(I + 1)J
J
?
j=1
I
?
i=0
p(fj |ei) (1)
where fJ1 is the source language sentence of length
J and eI1 is the target language sentence of length I .
As it is a conditional probability distribution, we
investigated both directions as evaluation metrics. In
order to avoid frequent confusions about what is the
source and what the target language, we defined our
scores in the following way:
? source-to-hypothesis (sh) IBM1 score:
IBM1sh =
1
(H + 1)S
S
?
j=1
H
?
i=0
p(sj|hi) (2)
? hypothesis-to-source (hs) IBM1 score:
IBM1hs =
1
(S + 1)H
H
?
i=1
S
?
j=0
p(hi|sj) (3)
where sj are the words of the original source lan-
guage sentence, S is the length of this sentence, hi
are the words of the target language hypothesis, and
H is the length of this hypothesis.
In addition to the standard IBM1 scores calculated
on words, we also investigated:
? MIBM1 scores ? IBM1 scores of word mor-
phemes in each direction;
? PnIBM1 scores ? IBM1 scores of POS n-grams
in each direction.
A parallel bilingual corpus for the desired lan-
guage pair and a tool for training the IBM1 model
are required in order to obtain IBM1 probabilities
p(fj|ei). For the POS n-gram scores, appropriate
POS taggers for each of the languages are necessary.
The POS tags cannot be only basic but must have
all details (e.g. verb tenses, cases, number, gender,
etc.). For the morpheme scores, a tool for splitting
words into morphemes is necessary.
3 Experiments on WMT 2008, WMT 2009
and WMT 2010 test data
3.1 Experimental set-up
The IBM1 probabilities necessary for the IBM1
scores are learnt using the WMT 2010 News
Commentary bilingual corpora consisting of the
Spanish-English, French-English, German-English
and Czech-English parallel texts. Spanish, French,
German and English POS tags were produced using
the TreeTagger1, and the Czech texts are tagged us-
ing the COMPOST tagger (Spoustova? et al, 2009).
The morphemes for all languages are obtained us-
ing the Morfessor tool (Creutz and Lagus, 2005).
The tool is corpus-based and language-independent:
it takes a text as input and produces a segmenta-
tion of the word forms observed in the text. The
obtained results are not strictly linguistic, however
they often resemble a linguistic morpheme segmen-
tation. Once a morpheme segmentation has been
learnt from some text, it can be used for segment-
ing new texts. In our experiments, the splitting are
learnt from the training corpus used for the IBM1
lexicon probabilities. The obtained segmentation is
then used for splitting the corresponding source texts
and hypotheses. Detailed corpus statistics are shown
in Table 1.
Using the obtained IBM1 probabilities of words,
morphemes and POS n-grams, the scores de-
scribed in Section 2 are calculated for the
Spanish-English, French-English, German-English
and Czech-English translation outputs from each
translation direction. For each of the IBM1 scores,
the system level Spearman correlation coefficients ?
with the human ranking are calculated for each doc-
ument. In total, 32 correlation coefficients are ob-
tained for each score ? four English outputs from
the WMT 2010 task, four from the WMT 2009 and
eight from the WMT 2008 task, together with six-
teen outputs in other four target languages. The ob-
tained correlation results were then summarised into
the following three values:
? mean
a correlation coefficient averaged over all trans-
lation outputs;
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
100
Spanish English French English German English Czech English
sentences 97122 83967 100222 94693
running words 2661344 2338495 2395141 2042085 2475359 2398780 2061422 2249365
vocabulary:
words 69620 53527 56295 50082 107278 54270 125614 52081
morphemes 14178 13449 12004 12485 22211 13499 18789 12961
POS tags 69 44 33 44 54 44 611 44
POS-2grams 2459 1443 826 1443 1611 1454 27835 1457
POS-3grams 27350 20474 10409 19838 19928 20769 209481 20522
POS-4grams 135166 121182 62177 114555 114314 123550 637337 120646
Table 1: Statistics of the corpora for training IBM1 lexicon models.
? rank>
percentage of documents where the particular
score has better correlation than the other IBM1
scores;
? rank?
percentage of documents where the particular
score has better or equal correlation than the
other IBM1 scores.
3.2 Comparison of IBM1 scores
The first step towards deciding which IBM1 score
to submit to the WMT 2011 evaluation task was a
comparison of the average correlations i.e. mean
values. These values for each of the IBM1 scores
are presented in Table 2. The left column shows
average correlations of the source-hypothesis (sh)
scores, and the right one of the hypothesis-source
(hs) scores.
mean IBM1sh IBM1hs
words 0.066 0.308
morphemes 0.227 0.445
POS tags 0.006 0.337
POS-2grams 0.058 0.337
POS-3grams 0.172 0.376
POS-4grams 0.196 0.442
Table 2: Average correlations of source-hypothesis (left
column) and hypothesis-source (right column) IBM1
scores.
It can be seen that the morpheme, POS-3gram and
POS-4gram scores have the best correlations in both
directions. Apart from that, it can be observed that
all the hs scores have better correlations than sh
scores. Therefore, all the further experiments will
deal only with the hs scores, and the subscript hs is
omitted.
In the next step, all the hs scores are sorted ac-
cording to each of the three values described in
Section 3.1, i.e. average correlation mean, rank>
and rank?, and the results are shown in Table 3.
The most promising scores according to each of
the three values are morpheme score MIBM1, POS-
3gram score P3IBM1 and POS-4gram score P4IBM1.
3.2.1 Combined IBM1 scores
The last experiment was to combine the most
promising IBM1 scores in order to see if the correla-
tion with human rankings can be further improved.
In general, a combined IBM1 score is defined as
arithmetic mean of various individual IBM1hs scores
described in Section 2:
COMBIBM1 =
K
?
k=1
wk ? IBM1k (4)
The following combinations were investigated:
? P1234IBM1
combination of all POS n-gram scores;
? MP1234IBM1
combination of all POS n-gram scores and the
morpheme score;
? MP34IBM1
combination of the most promising individual
scores, i.e. POS-3gram, POS-4gram and mor-
pheme scores;
101
mean rank> rank?
0.445 morphemes 60.6 POS-4grams 71.3 POS-4grams
0.442 POS-4grams 54.4 morphemes 61.3 POS-3grams
0.376 POS-3grams 50.6 POS-3grams 56.3 morphemes
0.337 POS-2grams 39.4 POS tags 48.1 POS tags
0.337 POS tags 36.3 words 43.7 POS-2grams
0.308 words 35.6 POS-2grams 42.5 words
Table 3: IBM1hs scores sorted by average correlation (column 1), rank> value (column 2) and rank? value (column
3). The most promising scores are those calculated on morphemes (MIBM1), POS-3grams (P3IBM1) and POS-4grams
(P4IBM1).
? MP4IBM1
combination of the two most promising indi-
vidual scores, i.e. POS-4gram score and mor-
pheme score.
For each of the scores, two variants were investi-
gated, with and without (i.e. with uniform) weights
wk. The weigths were choosen proportionally to
the average correlation of each individual score. Ta-
ble 4 contains average correlations for all combined
scores, together with the weight values.
combined score mean
P1234IBM1 0.403
+weights (0.15, 0.15, 0.3, 0.4) 0.414
MP1234IBM1 0.466
+weights (0.2, 0.05, 0.05, 0.2, 0.5) 0.486
MP34IBM1 0.480
+weights (0.25, 0.25, 0.5) 0.498
MP4IBM1 0.494
+weights (0.4, 0.6) 0.496
Table 4: Average correlations of the investigated IBM1hs
combinations. The weight values are choosen accord-
ing to the average correlation of the particular individual
IBM1 score.
The POS n-gram combination alone does not yield
any improvement over the best individual scores.
Introduction of the morpheme score increases the
average correlation, especially when only the best
n-gram scores are chosen. Apart from that, intro-
ducing weights improves the average correlation for
each of the combined scores.
The final step in our experiments consists of rank-
ing the weighted combined scores. The rank> and
rank? values for these scores are presented in Ta-
ble 5. According to the rank> values, the MP4IBM1
score clearly outperforms all other scores. This
score also has the highest mean value together with
the MP34IBM1 score. As for rank? values, all
morpheme-POS scores have similar values signifi-
cantly outperforming the P1234IBM1 score.
combined score rank> rank?
P1234IBM1 25.0 36.4
MP1234IBM1 44.8 68.7
MP34IBM1 39.6 64.6
MP4IBM1 55.2 65.7
Table 5: rank> (column 1) and rank? (column 2) values
of the weighted IBM1hs combinations.
Following all these observations, we decided to
submit the MP4IBM1 score to the WMT 2011 evalu-
ation task.
4 Conclusions and outlook
The results presented in this article show that the
IBM1 scores have the potential to be used as replace-
ment of current evaluation metrics based on refer-
ence translations. Especially the scores abstracting
away from word surface particularities (i.e. vocabu-
lary, domain) based on morphemes, POS-3grams and
4grams show a high average correlation of about 0.5
(the average correlation of the BLEU score on the
same data is 0.566).
An important point for future optimisation is to
investigate effects of the selection of training data
for the IBM1 models (and its similarity to the train-
ing data of the involved statistical translation sys-
tems). Furthermore, investigation of how to assign
the weights for combining the corresponding indi-
102
vidual scores, as well as of the possible impact of
different morpheme splittings should be carried out.
Other direction for future work is combination with
other features (i.e. POS language models).
This method is currently being tested and fur-
ther developed in the framework of the TARAX ?U
project2. In this project, three industry and one re-
search partners develop a hybrid machine transla-
tion architecture that satisfies current industry needs,
which includes a number of large-scale evalua-
tion rounds involving various languages: English,
French, German, Czech, Spanish, Russian, Chinese
and Japanese. By the time of writing this article, the
first human evaluation round in TARAX ?U on a pilot
set of about 7000 sentences is running. The metrics
proposed in this paper will be tested on the TARAX ?U
data as soon as they are available. First results will
be reported in the presentation of this paper.
Acknowledgments
This work has been partly developed within the
TARAX ?U project financed by TSB Technologies-
tiftung Berlin ? Zukunftsfonds Berlin, co-financed
by the European Union ? European fund for regional
development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In Pro-
ceedings of the ACL 05 Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for MT and/or Summa-
rization, pages 65?72, Ann Arbor, MI, June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Final report, JHU/CLSP Summer
Workshop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
Meta-Evaluation of Machine Translation. In Proceed-
ings of the 3rd ACL 08 Workshop on Statistical Ma-
2http://taraxu.dfki.de/
chine Translation (WMT 08), pages 70?106, Colum-
bus, Ohio, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR
(WMT 10), pages 17?53, Uppsala, Sweden, July.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Technical Re-
port Report A81, Computer and Information Science,
Helsinki University of Technology, Helsinki, Finland,
March.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for statistical machine translation. Technical re-
port, Johns Hopkins University 2003 Summer Work-
shop on Language Engineering, Center for Language
and Speech Processing, Baltimore, MD, USA, August.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 02), pages 311?318, Philadel-
phia, PA, July.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 763?771,
Athens, Greece, March.
103
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 485?489,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
DFKI Hybrid Machine Translation System for WMT 2011
- On the Integration of SMT and RBMT
Jia Xu and Hans Uszkoreit and Casey Kennington and David Vilar and Xiaojun Zhang
DFKI GmbH, Language Technology Lab
Stuhlsatzenhausweg 3
D-66123 Saarbru?cken Germany
{Jia.Xu,uszkoreit,David.Vilar}@dfki.de, {bakuzen,xiaojun.zhang.iiken}@gmail.com
Abstract
We present the DFKI hybrid translation sys-
tem at the WMT workshop 2011. Three SMT
and two RBMT systems are combined at the
level of the final translation output. The trans-
lation results show that our hybrid system sig-
nificantly outperformed individual systems by
exploring strengths of both rule-based and sta-
tistical translations.
1 Introduction
Machine translation (MT), in particular the statisti-
cal approach to it, has undergone incremental im-
provements in recent years. While rule-based ma-
chine translation (RBMT) maintains competitive-
ness in human evaluations. Combining the advan-
tages of both approaches have been investigated by
many researchers such as (Eisele et al, 2008).
Nonetheless, significant improvements over statis-
tical approaches still remain to be shown. In this
paper, we present the DFKI hybrid system in the
WMT workshop 2011. Our system is different from
the system of the last year (Federmann et al, 2010),
which is based on the shallow phrase substitution.
In this work, two rule-based translation systems are
applied. In addition, three statistical machine trans-
lation systems are built, including a phrase-based,
a hierarchical phrase-based and a syntax-based sys-
tem. Instead of combining with rules or post-editing,
we perform system combination on the final transla-
tion hypotheses. We applied the CMU open toolkit
(Heafield and Lavie, 2010) among numerous com-
bination methods such as (Matusov, 2009), (Sim et
al., 2007) and (He et al, 2008). The final transla-
tion output outperforms each individual output sig-
nificantly.
2 Individual translation systems
2.1 Phrase-based system
We use the IBM model 1 and 4 (Brown et al, 1993)
and Hidden-Markov model (HMM) (Vogel et al,
1996) to train the word alignment using the mgiza
toolkit1. We applied the EMS in Moses (Koehn et
al., 2007) to build up the phrase-based translation
system. Features in the log-linear model include
translation models in two directions, a language
model, a distortion model and a sentence length
penalty. A dynamic programming beam search al-
gorithm is used to generate the translation hypoth-
esis with maximum probability. We applied a 5-
gram mixture language model with each sub-model
trained on one fifth of the monolingual corpus with
Kneser-Ney smoothing using SRILM toolkit (Stol-
cke, 2002). We did not perform any tuning, because
it hurts the evaluation performance in our experi-
ments.
2.2 Syntax-based system
To capture the syntactic structure, we also built a
tree-based system using the same configuration of
EMS in Moses (Koehn et al, 2007). Tree-based
models operate on so-called grammar rules, which
include variables in the mapping rules. To increase
the diversity of models in combination, the lan-
guage model in each individual translation system
is trained differently. For the tree-based system,
we applied a 4-gram language model with Kneser-
Ney smoothing using SRILM toolkit (Stolcke, 2002)
trained on the whole monolingual corpus. The
test2007 news part is applied to tune the feature
weights using mert, because the tuning on test2007
1http://geek.kyloo.net/software/doku.php/mgiza:overview
485
improves the translation performance more than the
tuning on test2008 in a small-scale experiment for
the tree-based system.
2.3 Hierarchical phrase-based system
For the hierarchical system, we used the open source
hierarchical phrased-based system Jane, developed
at RWTH and free for non-commercial use (Vi-
lar et al, 2010). This approach is an extension
of the phrase-based approach, where the phrases
are allowed to have gaps (Chiang, 2007). In this
way long-range dependencies and reorderings can
be modeled in a consistent statistical framework.
The system uses a fairly standard setup, trained
using the bilingual data provided by the organizers,
word aligned using the mgiza. Two 5-gram language
models were used during decoding: one trained on
the monolingual part of the bilingual training data,
and a larger one trained on the additional news data.
Decoding was carried out using the cube pruning al-
gorithm. The tuning is performed on test2008 with-
out further experiments.
2.4 Rule-based systems
We applied two rule-based translation systems, the
Lucy system (Lucy, 2011) and the Linguatec sys-
tem (Aleksic? and Thurmair, 2011). The Lucy sys-
tem is a recent offspring of METAL. The Linguatec
system is a modular system consisting of grammar,
lexicon and morphological analyzers based on logic
programming using slot grammar.
3 Hybrid translation
A hybrid approach combining rule-based and sta-
tistical machine translation is usually investigated
with an in-box integration, such as multi-way trans-
lation (Eisele et al, 2008), post-editing (Ueffing et
al., 2008) or noun phrase substitution (Federmann
et al, 2010). However, significant improvements
over state-of-the-art statistical machine translation
are still expected. In the meanwhile system combi-
nation methods for instance described in (Matusov,
2009), (Sim et al, 2007) and (He et al, 2008) are
mostly evaluated to combine statistical translation
systems, rule-based systems are not considered. In
this work, we integrate the rule-based and statistical
machine translation system on the level of the final
PBT Syntax
PBT-2010 18.32
Max80words 20.65 21.10
Max100words 20.78
+Compound 21.52 22.13
+Newparallel 21.77
Table 1: Translation performance BLEU[%] on
phrase/syntax-based system using various settings eval-
uated on test10.
translation hypothesis and treat the rule-based sys-
tem anonymously as an individual system. In this
way an black-box integration is allowed using the
current system combination techniques.
We applied the CMU open toolkit (Heafield
and Lavie, 2010) MEMT, a package by Kenneth
Heafield to combine the translation hypotheses. The
language model is trained on the target side of the
parallel training corpus using SRILM (Stolcke,
2002). We used only the Europarl part to train lan-
guage models for tuning and all target side of paral-
lel data to train language models for decoding. The
beam size is set to 80, and 300 nbest is considered.
4 Translation experiments
4.1 MT Setup
The parallel training corpus consists of 1.8
million German-English parallel sentences from
Europarl-v6 (Koehn, MT Summit 2005) and news-
commentary with 48 million tokenized German
words and 54 million tokenized English words re-
spectively. The monolingual training corpus con-
tains the target side of the parallel training cor-
pus and the additional monolingual language model
training data downloaded from (SMT, 2011). We
did not apply the large-scale Gigaword corpus, be-
cause it does not significantly reduce the perplexity
of our language model but raises the computational
requirement heavily.
4.2 Single systems
For each individual translation system, different
configurations are experimented to achieve a higher
translation quality. We take phrase- and syntax-
based translation system as examples. Table 1
presents official submission result on DE-EN by
486
PBT+Syntax 20.37
PBT+Syntax+HPBT 20.78
PBT+HPBT+Linguatec+Lucy 20.27
PBT+Syntax+HPBT+Linguatec+Lucy 20.81
Table 2: Translation performance BLEU[%] on test2011
using hybrid system tuned on test10 with various settings
(DE-EN).
DFKI in 2010. In 2010?s translation system only
Europarl parallel corpus was applied, and the trans-
lation output was evaluated as 18.32% in the BLEU
score. In 2011, we added the News Commentary
parallel corpus and trained the language model on all
monolingual data provided by (SMT, 2011) except
for Gigaword. As shown in Table 1, if we increase
the maximum sentence length of the training cor-
pus from 80 to 100, the BLEU score increases from
20.65% to 20.78%. In the error analysis, we found
that many OOVs come from the compound words
in German. Therefore, we applied the compound
splitting for both German and English by activating
the corrensponding settings in the EMS in Moses.
This leads to a further improvement of nearly 1%
in the BLEU score. As we add the new parallel
corpus provided on the homepage of SMT work-
shop in 2011 (SMT, 2011) to the corpus in 2010,
a slight improvement can be achieved. Within one
year, the score for the DFKI PBT system DE-EN has
improved by nearly 3.5% absolute and 20% relative
BLEU score points, as shown in Table 1.
In the phrase-based translation, the tuning was not
applied, because it improves the results on the held-
out data but hurts the results on the evaluation set.
In our observation, the decrease is in the range of
0.01% to 1% in the BLEU score. However tun-
ing does help for the Tree-based system. Therefore
we applied the test2007 to optimize the parameters,
which enhanced the BLEU score from 17.52% to
21.10%. The compound splitting also improves the
syntax system, with about 1% in the BLEU score.
We did not add the new parallel corpus into the train-
ing for syntax system due to its larger computational
requirement than that of the phrase-based system.
Test10 Test08 Test11
Hybrid-2010 17.43
PBT 21.77 20.70 20.40
Syntax 22.13 20.50 20.49
HPBT 19.21 18.26 17.06
Linguatec 16.59 16.07 15.97
Lucy 16.57 16.66 16.68
Hybrid-2011 23.88 21.13 21.25
Table 3: Translation performance BLEU[%] on three test
sets using different translation systems in 2011 submis-
sion (DE-EN).
Test10 Test11
Hybrid-2010 14.42
PBT 15.46 14.05
Linguatec 14.92 12.92
Lucy 13.77 13.0
Hybrid-2011 15.55 15.83
Table 4: Translation performance BLEU[%] on two test
sets using different translation systems in 2011 submis-
sion (EN-DE).
4.3 Hybrid system
We applied test10 as the held-out data to tune
the German-English and English-German transla-
tion systems. For experiments, we applied a small-
scaled 4-gram language model trained only on the
target side of the Europarl parallel training data. As
shown in Table 2, different combinations are per-
formed on the hypotheses generated from single sys-
tems. We first combined the PBT with syntax sys-
tem, then together with the HPBT system. The
translation result in the BLEU score performs best
when we combine all three statistical machine trans-
lation systems and two rule-based systems together.
4.4 Evaluation results
For the decoding during the WMT evaluation, we
applied a larger 4-gram language model trained on
the target side of all parallel training corpus. As
shown in Table 3, in last year?s evaluation the DFKI
hybrid translation result was evaluated as 17.34% in
the BLEU score. In 2011, among all the transla-
tion systems, the syntax system performs the best
on test10 and test11, while the PBT performs the
487
SRC Diese Verordnung wurde vom Gesundheitsministerium in diesem Jahr einigermassen gemildert - die Ku?hlschrankpflicht fiel weg.
REF It was mitigated by the Ministry of Health this year - the obligation to have a refrigerator has been removed.
PBT This regulation by the Ministry of Health in this year - somewhat mitigated the fridge duty fell away.
Syntax This regulation was somewhat mitigated by the Ministry of Health this year - the refrigerator duty fell away.
HPBT This regulation was by the Ministry of Health in reasonably Dokvadze this year - the Ku?hlschrankpflicht fell away.
Linguatec This ordinance was soothed to some extent by the brazilian ministry of health this year, the refrigerator duty was discontinued.
Lucy This regulation was quite moderated by the Department of Health, Education and Welfare this year - the refrigerator duty was omitted.
Hybrid This regulation was somewhat mitigated by the Ministry of Health this year - the fridge duty fell away.
SRC Die Deregulierung und Bakalas ehemalige Bergarbeiterwohnungen sind ein brisantes Thema.
REF Deregulation and Bakala ?s former mining flats are local hot topic.
PBT The deregulation and Bakalas former miners? homes are a sensitive issue.
Syntax The deregulation and Bakalas former miners? homes are a sensitive issue.
HPBT The deregulation and Bakalas former Bergarbeiterwohnungen are a hot topic.
Linguatec Former miner flats are an explosive topic the deregulation and Bakalas.
HPBT The deregulation and Bakalas former miner apartments are an explosive topic.
Hybrid The deregulation and Bakalas former miners? apartments are a sensitive issue.
Table 5: Examples of translation output by the different systems.
best on test08. The rule-based sytems, Linguatec
and Lucy are expected to have a higher score in the
human evaluation than in the automatic evaluation.
Furthermore, as we can see from Table 3, there is
still room to improve the Jane system, with better
modeling, configurations or even higher-order lan-
guage model. Using the hybrid system we success-
fully improved the translation result to 23.88% on
test10. The hybrid system outperforms the best sin-
gle system by 0.43% and 0.76% in the BLEU score
on the test08 and test11, respectively.
For the translation from English to German, the
translation result of last year?s submission was eval-
uated as 14.42% in the BLEU score, as shown in Ta-
ble 4. In this year, the phrase-based translation result
is 15.46% in the BLEU score. We only set up one
statistical translation system due to time limitation.
With the respect of the BLEU score, phrase-based
translation outperforms rule-based translations. Be-
tween rule-based translation systems, Linguatec per-
forms better on the test10 (14.92%) and Lucy per-
forms better on the test11 (13.0%). Combining three
translation hypotheses leads to a smaller improve-
ment (from 15.46% to 15.55%) on the test10 and a
greater improvement (from 14.05% to 15.83%) on
the test11 in the BLEU score over the single best
translation system. Comparing to last year?s trans-
lation output, the improvement is over one percent
absolutely (from 14.42% to 15.55%) in the BLEU
score on the test10.
4.5 Output examples
Table 5 shows two translation examples from the
MT output of the test2011. We list the source sen-
tence in German and its reference translation as
well as the translation results generated by different
translation systems. As can be seen from Table 5,
the translation quality of source sentences is greatly
improved using the hybrid system over the single in-
dividual systems. Translations of words and word
orderings are more appropriate by the hybrid sys-
tem.
5 Conclusion and future work
We presented the DFKI hybrid translation system
submitted in the WMT workshop 2011. The hy-
brid translation is performed on the final translation
output by individual systems, including a phrase-
based system, a syntax-based system, a hierarchical
phrase-based system and two rule-based systems.
Combining the results from statistical and rule-
based systems significantly improved the translation
performance over the single-best system, which is
shown by the automatic evaluation scores and the
output examples. Despite of the encouraging results,
there is still room to improve our system, such as the
tuning in the phrase-based translation and a better
language model in the combination.
488
References
Vera Aleksic? and Gregor Thurmair. 2011. Personal
translator at wmt2011 - a rule-based mt system with
hybrid components. In Proceedings of WMT work-
shop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Andreas Eisele, Christian Federmann, Hans Uszkoreit,
Herve? Saint-Amand, Martin Kay, Michael Jellinghaus,
Sabine Hunsicker, Teresa Herrmann, and Yu Chen.
2008. Hybrid architectures for multi-engine machine
translation. In Proceedings of Translating and the
Computer 30, pages ASLIB, ASLIB/IMI, London,
United Kingdom, November.
Christian Federmann, Andreas Eisele, Hans Uszkoreit,
Yu Chen, Sabine Hunsicker, and Jia Xu. 2010. Fur-
ther experiments with shallow hybrid mt systems. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 237?
248, Uppsala, Sweden. John Benjamins.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of EMNLP,
October.
Kenneth Heafield and Alon Lavie. 2010. Voting on n-
grams for machine translation system combination. In
Proc. Ninth Conference of the Association for Machine
Translation in the Americas, Denver, Colorado, Octo-
ber.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL.
Philipp Koehn. MT Summit 2005. Europarl: A parallel
corpus for statistical machine translation.
Lucy. 2011. Home page of software lucy and services.
http://www.lucysoftware.com.
Evgeny Matusov. 2009. Combining Natural Language
Processing Systems to Improve Machine Translation
of Speech. Ph.D. thesis, Department of Electrical
and Computer Engineering, Johns Hopkins University,
Baltimore, MD.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and
P. C. Woodland. 2007. Consensus network decoding
for statistical machine translation system combination.
In IN IEEE INT. CONF. ON ACOUSTICS, SPEECH,
AND SIGNAL PROCESSING.
SMT. 2011. Sixth workshop on statistical machine trans-
lation home page. http://www.statmt.org/wmt11/.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference On Spoken Language Processing, pages
901?904, Denver, Colorado, September.
Nicola Ueffing, Jens Stephan, Evgeny Matusov, Lo ic
Dugast, George F. Foster, Roland Kuhn, Jean Senel-
lart, and Jin Yang. 2008. Tighter integration of rule-
based and statistical mt in serial system combination.
In Proceedings of COLING 2008, pages 913?920.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Trans-
lation, Extended with Reordering and Lexicon Mod-
els. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
262?270, Uppsala, Sweden, July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copenhagen,
Denmark, August.
489
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 91?96,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lightly-Supervised Training for Hierarchical Phrase-Based Machine
Translation
Matthias Huck1 and David Vilar1,2 and Daniel Stein1 and Hermann Ney1
1 Human Language Technology and Pattern 2 DFKI GmbH
Recognition Group, RWTH Aachen University Berlin, Germany
<surname>@cs.rwth-aachen.de david.vilar@dfki.de
Abstract
In this paper we apply lightly-supervised
training to a hierarchical phrase-based statis-
tical machine translation system. We employ
bitexts that have been built by automatically
translating large amounts of monolingual data
as additional parallel training corpora. We ex-
plore different ways of using this additional
data to improve our system.
Our results show that integrating a second
translation model with only non-hierarchical
phrases extracted from the automatically gen-
erated bitexts is a reasonable approach. The
translation performance matches the result we
achieve with a joint extraction on all train-
ing bitexts while the system is kept smaller
due to a considerably lower overall number of
phrases.
1 Introduction
We investigate the impact of an employment of large
amounts of unsupervised parallel data as training
data for a statistical machine translation (SMT) sys-
tem. The unsupervised parallel data is created by au-
tomatically translating monolingual source language
corpora. This approach is called lightly-supervised
training in the literature and has been introduced by
Schwenk (2008). In contrast to Schwenk, we do not
apply lightly-supervised training to a conventional
phrase-based system (Och et al, 1999; Koehn et al,
2003) but to a hierarchical phrase-based translation
(HPBT) system.
In hierarchical phrase-based translation (Chiang,
2005) a weighted synchronous context-free gram-
mar is induced from parallel text, the search is
based on CYK+ parsing (Chappelier and Rajman,
1998) and typically carried out using the cube prun-
ing algorithm (Huang and Chiang, 2007). In addi-
tion to the contiguous lexical phrases as in standard
phrase-based translation, the hierarchical phrase-
based paradigm also allows for phrases with gaps
which are called hierarchical phrases. A generic
non-terminal symbol serves as a placeholder that
marks the gaps.
In this paper we study several different ways
of incorporating unsupervised training data into
a hierarchical system. The basic techniques we
employ are the use of multiple translation mod-
els and a distinction of the hierarchical and the
non-hierarchical (i.e. lexical) part of the transla-
tion model. We report experimental results on
the large-scale NIST Arabic-English translation task
and show that lightly-supervised training yields sig-
nificant gains over the baseline.
2 Related Work
Large-scale lightly-supervised training for SMT as
we define it in this paper has been first carried out
by Schwenk (2008). Schwenk translates a large
amount of monolingual French data with an initial
Moses (Koehn et al, 2007) baseline system into En-
glish. In Schwenk?s original work, an additional
bilingual dictionary is added to the baseline. With
lightly-supervised training, Schwenk achieves im-
provements of around one BLEU point over the
baseline. In a later work (Schwenk and Senellart,
2009) he applies the same method for translation
model adaptation on an Arabic-French task with
91
gains of up to 3.5 points BLEU. 1
Hierarchical phrase-based translation has been pi-
oneered by David Chiang (Chiang, 2005; Chiang,
2007) with his Hiero system. The hierarchical
paradigm has been implemented and extended by
several groups since, some have published their soft-
ware as open source (Li et al, 2009; Hoang et al,
2009; Vilar et al, 2010).
Combining multiple translation models has been
investigated for domain adaptation by Foster and
Kuhn (2007) and Koehn and Schroeder (2007) be-
fore. Heger et al (2010) exploit the distinction be-
tween hierarchical and lexical phrases in a similar
way as we do. They train phrase translation proba-
bilities with forced alignment using a conventional
phrase-based system (Wuebker et al, 2010) and em-
ploy them for the lexical phrases while the hierarchi-
cal phrases stay untouched.
3 Using the Unsupervised Data
The most straightforward way of trying to improve
the baseline with lightly-supervised training would
be to concatenate the human-generated parallel data
and the unsupervised data and to jointly extract
phrases from the unified parallel data (after having
trained word alignments for the unsupervised bitexts
as well). This method is simple and expected to
be effective usually. There may however be two
drawbacks: First, the reliability and the amount of
parallel sentences may differ between the human-
generated and the unsupervised part of the training
data. It might be desirable to run separate extrac-
tions on the two corpora in order to be able to dis-
tinguish and weight phrases (or rather their scores)
according to their origin during decoding. Second, if
we incorporate large amounts of additional unsuper-
vised data, the amount of phrases that are extracted
may become much larger. We would want to avoid
blowing up our phrase table sizes without an appro-
1Schwenk names the method lightly-supervised training be-
cause the topics that are covered in the monolingual source lan-
guage data that is being translated may potentially also be cov-
ered by parts of the language model training data of the system
which is used to translate them. This can be considered as a
form of light supervision. We loosely apply the term lightly-
supervised training if we mean the process of utilizing a ma-
chine translation system to produce additional bitexts that are
used as training data, but still refer to the automatically pro-
duced bilingual corpora as unsupervised data.
Arabic English
Sentences 2 514 413
Running words 54 324 372 55 348 390
Vocabulary 264 528 207 780
Singletons 115 171 91 390
Table 1: Data statistics for the preprocessed Arabic-
English parallel training corpus. In the corpus, numer-
ical quantities have been replaced by a special category
symbol.
dev (MT06) test (MT08)
Sentences 1 797 1 360
Running words 49 677 45 095
Vocabulary 9 274 9 387
OOV [%] 0.5 0.4
Table 2: Data statistics for the preprocessed Arabic part
of the dev and test corpora. In the corpus, numerical
quantities have been replaced by a special category sym-
bol.
priate effect on translation quality. This holds in par-
ticular in the case of hierarchical phrases. Phrase-
based machine translation systems are usually able
to correctly handle local context dependencies, but
often have problems in producing a fluent sentence
structure across long distances. It is thus an intuitive
supposition that using hierarchical phrases extracted
from unsupervised data in addition to the hierar-
chical phrases extracted from the presumably more
reliable human-generated bitexts does not increase
translation quality. We will compare a joint extrac-
tion to the usage of two separate translation mod-
els (either without separate weighting, with a binary
feature, or as a log-linear mixture). We will further
check if including hierarchical phrases from the un-
supervised data is beneficial or not.
4 Experiments
We use the open source Jane toolkit (Vilar et al,
2010) for our experiments, a hierarchical phrase-
based translation software written in C++.
4.1 Baseline System
The baseline system has been trained using a
human-generated parallel corpus of 2.5M Arabic-
English sentence pairs. Word alignments in both
92
directions were produced with GIZA++ and sym-
metrized according to the refined method that was
suggested by Och and Ney (2003).
The models integrated into our baseline system
are: phrase translation probabilities and lexical
translation probabilities for both translation direc-
tions, length penalties on word and phrase level,
three binary features marking hierarchical phrases,
glue rule, and rules with non-terminals at the bound-
aries, four simple additional count- and length-
based binary features, and a large 4-gram language
model with modified Kneser-Ney smoothing that
was trained with the SRILM toolkit (Stolcke, 2002).
We ran the cube pruning algorithm, the depth of
the hierarchical recursion was restricted to one by
using shallow rules as proposed by Iglesias et al
(2009).
The scaling factors of the log-linear model com-
bination have been optimized towards BLEU with
MERT (Och, 2003) on the MT06 NIST test corpus.
MT08 was employed as held-out test data. Detailed
statistics for the parallel training data are given in
Table 1, for the development and the test corpus in
Table 2.
4.2 Unsupervised Data
The unsupervised data that we integrate has been
created by automatic translations of parts of the
Arabic LDC Gigaword corpus (mostly from the
HYT collection) with a standard phrase-based sys-
tem (Koehn et al, 2003). We thus in fact conduct a
cross-system and cross-paradigm variant of lightly-
supervised training. Translating the monolingual
Arabic data has been performed by LIUM, Le Mans,
France. We thank Holger Schwenk for kindly pro-
viding the translations.
The score computed by the decoder for each
translation has been normalized with respect to the
sentence length and used to select the most reliable
sentence pairs. Word alignments for the unsuper-
vised data have been produced in the same way as
for the baseline bilingual training data. We report
the statistics of the unsupervised data in Table 3.
4.3 Translation Models
We extracted three different phrase tables, one from
the baseline human-generated parallel data only,
one from the unsupervised data only, and one joint
Arabic English
Sentences 4 743 763
Running words 121 478 207 134 227 697
Vocabulary 306 152 237 645
Singletons 130 981 102 251
Table 3: Data statistics for the Arabic-English unsuper-
vised training corpus after selection of the most reliable
sentence pairs. In the corpus, numerical quantities have
been replaced by a special category symbol.
phrase table from the concatenation of the baseline
data and the unsupervised data. We will denote the
different extractions as baseline, unsupervised, and
joint, respectively.
The conventional restrictions have been applied
for phrase extraction in all conditions, i.e. a maxi-
mum length of ten words on source and target side
for lexical phrases, a length limit of five (including
non-terminal symbols) on source side and ten on tar-
get side for hierarchical phrases, and at most two
non-terminals per rule which are not allowed to be
adjacent on the source side. To limit the number of
hierarchical phrases, a minimum count cutoff of one
and an extraction pruning threshold of 0.1 have been
applied to them. Note that we did not prune lexical
phrases.
Statistics on the phrase table sizes are presented
in Table 4.2 In total the joint extraction results in
almost three times as many phrases as the baseline
extraction. The extraction from the unsupervised
data exclusively results in more than two times as
many hierarchical phrases as from the baseline data.
The sum of the number of hierarchical phrases from
baseline and unsupervised extraction is very close
to the number of hierarchical phrases from the joint
extraction. If we discard the hierarchical phrases ex-
tracted from the unsupervised data and use the lex-
ical part of the unsupervised phrase table (27.3M
phrases) as a second translation model in addition to
the baseline phrase table (67.0M phrases), the over-
all number of phrases is increased by only 41% com-
pared to the baseline system.
2The phrase tables have been filtered towards the phrases
needed for the translation of a given collection of test corpora.
93
number of phrases
lexical hierarchical total
extraction from baseline data 19.8M 47.2M 67.0M
extraction from unsupervised data 27.3M 115.6M 142.9M
phrases present in both tables 15.0M 40.1M 55.1M
joint extraction baseline + unsupervised 32.1M 166.5M 198.6M
Table 4: Phrase table sizes. The phrase tables have been filtered towards a larger set of test corpora containing a total
of 2.3 million running words.
dev (MT06) test (MT08)
BLEU TER BLEU TER
[%] [%] [%] [%]
HPBT baseline 44.1 49.9 44.4?0.9 49.4?0.8
HPBT unsupervised only 45.3 48.8 45.2 49.1
joint extraction baseline + unsupervised 45.6 48.7 45.4?0.9 49.1?0.8
baseline hierarchical phrases + unsupervised lexical phrases 45.1 49.1 45.2 49.2
baseline hierarchical phrases + joint extraction lexical phrases 45.3 48.7 45.3 49.1
baseline + unsupervised lexical phrases 45.3 48.9 45.3 49.0
baseline + unsupervised lexical phrases (with binary feature) 45.3 48.8 45.4 49.0
baseline + unsupervised lexical phrases (separate scaling factors) 45.3 48.9 45.0 49.3
baseline + unsupervised full table 45.6 48.6 45.1 48.9
baseline + unsupervised full table (with binary feature) 45.5 48.6 45.2 48.8
baseline + unsupervised full table (separate scaling factors) 45.5 48.7 45.3 49.0
Table 5: Results for the NIST Arabic-English translation task (truecase). The 90% confidence interval is given for the
baseline system as well as for the system with joint phrase extraction. Results in bold are significantly better than the
baseline.
4.4 Experimental Results
The empirical evaluation of all our systems on the
two standard metrics BLEU (Papineni et al, 2002)
and TER (Snover et al, 2006) is presented in Ta-
ble 5. We have also checked the results for statistical
significance over the baseline. The confidence in-
tervals have been computed using bootstrapping for
BLEU and Cochran?s approximate ratio variance for
TER (Leusch and Ney, 2009).
When we combine the full baseline phrase ta-
ble with the unsupervised phrase table or the lexi-
cal part of it, we either use common scaling factors
for their source-to-target and target-to-source trans-
lation costs, or we use common scaling factors but
mark entries from the unsupervised table with a bi-
nary feature, or we optimize the four translation fea-
tures separately for each of the two tables as part of
the log-linear model combination.
Including the unsupervised data leads to a sub-
stantial gain on the unseen test set of up to +1.0%
BLEU absolute. The different ways of combining
the manually produced data with the unsupervised
have little impact on translation quality. This holds
specifically for the combination with only the lexical
phrases, which, when marked with a binary feature,
is able to obtain the same results as the full (joint
extraction) system but with much less phrases. We
compared the decoding speed of these two setups
and observed that the system with less phrases is
clearly faster (5.5 vs. 2.6 words per second, mea-
sured on MT08). The memory requirements of the
systems do not differ greatly as we are using a bi-
narized representation of the phrase table with on-
demand loading. All setups consume slightly less
than 16 gigabytes of RAM.
94
5 Conclusion
We presented several approaches of applying
lightly-supervised training to hierarchical phrase-
based machine translation. Using the additional au-
tomatically produced bitexts we have been able to
obtain considerable gains compared to the baseline
on the large-scale NIST Arabic-to-English transla-
tion task. We showed that a joint phrase extraction
from human-generated and automatically generated
parallel training data is not required to achieve sig-
nificant improvements. The same translation qual-
ity can be achieved by adding a second translation
model with only lexical phrases extracted from the
automatically created bitexts. The overall amount of
phrases can thus be kept much smaller.
Acknowledgments
The authors would like to thank Holger Schwenk
from LIUM, Le Mans, France, for making the au-
tomatic translations of the Arabic LDC Gigaword
corpus available. This work was partly supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-08-C-0110.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the DARPA.
References
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochastic
CFG. In Proc. of the First Workshop on Tabulation
in Parsing and Deduction, pages 133?137, April.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc. of
the 43rd Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 263?270, Ann Arbor,
MI, June.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201?228,
June.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proc. of the Second Work-
shop on Statistical Machine Translation, pages 128?
135, Prague, Czech Republic, June.
Carmen Heger, Joern Wuebker, David Vilar, and Her-
mann Ney. 2010. A Combination of Hierarchical Sys-
tems with Forced Alignments from Phrase-Based Sys-
tems. In Proc. of the Int. Workshop on Spoken Lan-
guage Translation (IWSLT), Paris, France, December.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchical,
and Syntax-Based Statistical Machine Translation. In
Proc. of the Int. Workshop on Spoken Language Trans-
lation (IWSLT), pages 152?159, Tokyo, Japan.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proc. of the Annual Meeting of the Assoc. for Com-
putational Linguistics (ACL), pages 144?151, Prague,
Czech Republic, June.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule Filtering by Pattern
for Efficient Hierarchical Translation. In Proc. of the
12th Conf. of the Europ. Chapter of the Assoc. for
Computational Linguistics (EACL), pages 380?388,
Athens, Greece, March.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proc. of the Second Workshop on Statistical
Machine Translation, pages 224?227, Prague, Czech
Republic, June.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc. of
the Human Language Technology Conf. / North Amer-
ican Chapter of the Assoc. for Computational Lin-
guistics (HLT-NAACL), pages 127?133, Edmonton,
Canada, May/June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of the Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 177?180, Prague, Czech Re-
public, June.
Gregor Leusch and Hermann Ney. 2009. Edit distances
with block movements and error rate confidence esti-
mates. Machine Translation, December.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An Open
Source Toolkit for Parsing-Based Machine Transla-
tion. In Proc. of the Workshop on Statistical Machine
Translation, pages 135?139, Athens, Greece, March.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In Proc. of the Joint SIGDAT Conf.
on Empirical Methods in Natural Language Process-
ing and Very Large Corpora (EMNLP99), pages 20?
28, University of Maryland, College Park, MD, June.
95
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of the An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of the 40th
Annual Meeting of the Assoc. for Computational Lin-
guistics (ACL), pages 311?318, Philadelphia, PA, July.
Holger Schwenk and Jean Senellart. 2009. Translation
Model Adaptation for an Arabic/French News Trans-
lation System by Lightly-Supervised Training. In MT
Summit XII, Ottawa, Ontario, Canada, August.
Holger Schwenk. 2008. Investigations on Large-Scale
Lightly-Supervised Training for Statistical Machine
Translation. In Proc. of the Int. Workshop on Spo-
ken Language Translation (IWSLT), pages 182?189,
Waikiki, Hawaii, October.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), pages 223?231, Cambridge,
MA, August.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 3,
Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Transla-
tion, Extended with Reordering and Lexicon Models.
In ACL 2010 Joint Fifth Workshop on Statistical Ma-
chine Translation and Metrics MATR, pages 262?270,
Uppsala, Sweden, July.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training Phrase Translation Models with Leaving-
One-Out. In Proc. of the Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 475?
484, Uppsala, Sweden, July.
96
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 382?387,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DFKI?s SMT System for WMT 2012
David Vilar
German Research Center for Artificial Intelligence (DFKI GmbH)
Language Technology Lab
Berlin, Germany
david.vilar@dfki.de
Abstract
We describe DFKI?s statistical based submis-
sion to the 2012 WMT evaluation. The sub-
mission is based on the freely available ma-
chine translation toolkit Jane, which supports
phrase-based and hierarchical phrase-based
translation models. Different setups have been
tested and combined using a sentence selec-
tion method.
1 Introduction
In this paper we present DFKI?s submission for
the 2012 MT shared task based on statistical ap-
proaches. We use a variety of phrase-based and hi-
erarchical phrase-based translation systems with dif-
ferent configurations and enhancements and com-
pare their performance. The output of the systems
are later combined using a sentence selection mech-
anism. Somewhat disappointingly the sentence se-
lection hardly improves over the best single system.
DFKI participated in the German to English and
English to German translation tasks. Technical
problems however hindered a more complete system
for this last translation direction.
This paper is organized as follows: Section 2 re-
ports on the different single systems that we built for
this shared task. Section 3 describes the sentence se-
lection mechanism used for combining the output of
the different systems. Section 4 concludes the paper.
2 Single Systems
For all our setups we used the Jane toolkit (Vi-
lar et al, 2010a), which in its current version sup-
ports both phrase-based and hierarchical phrase-
based translation models. In this Section we present
the different settings that we used for the task.
The bilingual training data used for training all
systems was the combination of the provided Eu-
roparl and News data. We also used two baseline 4-
gram language models trained on the same Europarl
training data and on the enhanced News Commen-
tary monolingual training data. The newstest2010
dataset was used for optimization of the systems.
2.1 Phrase-based System
The first system is a baseline phrase-based system
trained on the available bilingual training data. Word
alignments is trained using GIZA++ (Och and Ney,
2003), phrase extraction is performed with Jane us-
ing standard settings, i.e. maximum source phrase
length 6, maximum target phrase length 12, count
features, etc. Consult the Jane documentation for
more details. For reordering the standard distance-
based reordering model is computed. Scaling factors
are trained using MERT on n-best lists.
2.1.1 Verb reorderings
Following (Popovic? and Ney, 2006), for German
to English translation, we perform verb reordering
by first POS-tagging the source sentence and after-
wards applying hand-defined rules. This includes
rules for reordering verbs in subordinate clauses and
participles.
2.1.2 Moore LM
Moore and Lewis (2010) propose a method for
filtering large quantities of out-of-domain language-
model training data by comparing the cross-entropy
382
of an in-domain language model and an out-of-
domain language model trained on a random sam-
pling of the data. We followed this approach to filter
the news-crawl corpora provided the organizers. By
experimenting on the development set we decided
to use a 4-gram language model trained on 15M fil-
tered sentences (the original data comprising over
30M sentences).
2.2 Hierarchical System
We also trained a hierarchical system on the same
data as the phrase-based system, and also tried the
additional language model trained according to Sec-
tion 2.1.2, as well as the verb reorderings described
in Section 2.1.1.
2.2.1 Poor Man?s Syntax
Vilar et al (2010b) propose a ?syntax-based? ap-
proach similar to (Venugopal et al, 2009), but us-
ing automatic clustering methods instead of linguis-
tic parsing for defining the non-terminals used in the
resulting grammar. The main idea of the method is
to cluster the words (mimicking the concept of Part-
of-Speech tagging), performing a phrase extraction
pass using the word classes instead of the actual
words and performing another clustering on the
phrase level (corresponding to the linguistic classes
in a parse tree).
2.2.2 Lightly-Supervised Training
Huck et al (2011) propose to augment the mono-
lingual training data by translating available addi-
tional monolingual data with an existing translation
system. We adapt this approach by translating the
data selected according to Section 2.1.2 with the
phrase-based translation system described in Sec-
tion 2.1, and use this additional data to expand the
bilingual data available for training the hierarchical
phrase-based system.1
2.3 Experimental Results
Table 1 shows the results obtained for the German
to English translation direction on the newstest2011
dataset. The baseline phrase-based system obtains a
1The decision of which system to use to produce the addi-
tional training material follows mainly a practical reason. As
the hierarchical model is more costly to train and at decoding
time, we chose the phrase-based system as the generating sys-
tem.
BLEU score of 18.2%. The verb reorderings achieve
an improvement of 0.6% BLEU, and adding the ad-
ditional language model obtains an additional 1.6%
BLEU improvement.
The hierarchical system baseline achieves a bet-
ter BLEU score than the baseline PBT system, and
is comparable to the PBT system with additional re-
orderings. In fact, adding the verb reorderings to
the hierarchical system slightly degrades its perfor-
mance. This indicates that the hierarchical model is
able to reflect the verb reorderings necessary for this
translation direction. Adding the bigger language
model of Section 2.1.2 also obtains a nice improve-
ment of 1.4% BLEU for this system. On the other
hand and somewhat disappointingly, the lightly su-
pervised training and the poor man?s syntax ap-
proach are not able to improve translation quality.
For the English to German translation direction
we encountered some technical problems, and we
were not able to perform as many experiments as for
the opposite direction. The results are shown in Ta-
ble 2 and show similar trends as for the German to
English direction, except that the hierarchical sys-
tem in this case does not outperform the PBT base-
line.
3 Sentence Selection
In this section we will describe the system combi-
nation method based on sentence selection that we
used for combining the output of the systems de-
scribed in Section 2. This approach was tried suc-
cessfully in (Vilar et al, 2011).
We use a log-linear model for computing the
scores of the different translation hypotheses, gen-
erated by all the systems described in Section 2, i.e.
those listed in Tables 1 and 2. The model scaling
factors are computed using a standard MERT run
on the newstest2011 dataset, optimizing for BLEU.
This is comparable to the usual approach used for
rescoring n-best lists generated by a single system,
and has been used previously for sentence selection
purposes (see (Hildebrand and Vogel, 2008) which
uses a very similar approach to our own). Note that
no system dependent features like translation prob-
abilities were computed, as we wanted to keep the
system general.
We will list the features we compute for each of
383
System BLEU[%]
PBT Baseline 18.2
PBT + Reordering 18.8
PBT + Reordering + Moore LM 20.4
Hierarchical Baseline 18.7
Hierarchical + Moore LM 20.1
Hierarchical + Moore LM + Lightly Supervised 19.8
Poor Man?s Syntax 18.6
Hierarchical + Reordering 18.5
Table 1: Translation results for the different single systems, German to English.
System BLEU[%]
PBT Baseline 12.4
Hierarchical Baseline 11.6
Hierarchical + Moore LM 13.1
Poor Man?s Syntax 11.6
Table 2: Translation results for the different single systems, English to German
the systems. We have used features that try to focus
on characteristics that humans may use to evaluate a
system.
3.1 Cross System BLEU
BLEU was introduced in (Papineni et al, 2002)
and it has been shown to have a high correlation
with human judgement. In spite of its shortcom-
ings (Callison-Burch et al, 2006), it has been con-
sidered the standard automatic measure in the devel-
opment of SMT systems (with new measures being
added to it, but not substituting it, see for e.g. (Cer
et al, 2010)).
Of course, the main problem of using the BLEU
score as a feature for sentence selection in a real-
life scenario is that we do not have the references
available. We overcame this issue by generating
a custom set of references for each system, using
the other systems as gold translations. This is of
course inexact, but n-grams that appear on the out-
put of different systems can be expected to be more
probable to be correct, and BLEU calculated this
way gives us a measure of this agreement. This ap-
proach can be considered related to n-gram poste-
riors (Zens and Ney, 2006) or minimum Bayes risk
decoding (e.g. (Ehling et al, 2007)) in the context of
n-best rescoring, but applied without prior weight-
ing (unavailable directly) and more focused on the
evaluation interpretation.
We generated two features based on this idea.
The first one is computed at the system level, i.e. it
is the same for each sentence produced by a sys-
tem and serves as a kind of prior weight similar
to the one used in other system combination meth-
ods (e.g. (Matusov et al, 2008)). The other feature
was computed at the sentence level. For this we used
the smoothed version of BLEU proposed in (Lin and
Och, 2004), again using the output of the rest of
the systems as pseudo-reference. As optimization
on BLEU often tends to generate short translations,
we also include a word penalty feature.
3.2 Error Analysis Features
It is safe to assume that a human judge will try
to choose those translations which contain the least
amount of errors, both in terms of content and gram-
maticality. A classification of errors for machine
translation systems has been proposed in (Vilar et
al., 2006), and (Popovic? and Ney, 2011) presents
how to compute a subset of these error categories au-
tomatically. The basic idea is to extend the familiar
Word Error Rate (WER) and Position independent
384
word Error Rate (PER) measures on word and base-
form2 levels to identify the different kind of errors.
For our system we included following features:
Extra Word Errors (EXTer) Extra words in the
hypothesis not present in the references.
Inflection Errors (hINFer) Words with wrong in-
flection. Computed comparing word-level er-
rors and base-form-level errors.
Lexical Errors (hLEXer) Wrong lexical choices
in the hypothesis with respect to the references.
Reordering Errors (hRer) Wrong word order in
the hypothesis.
Missing Words (MISer) Words present in the ref-
erence that are missing in the hypothesis.
All these features are computed using the open
source Hjerson3 tool (Popovic?, 2011), which also
outputs the standard WER metric, which we added
as an additional feature.
As was the case in Section 3.1, for computing
these measures we do not have a reference available,
and thus we use the rest of the systems as pseudo-
references. This has the interesting effect that some
?errors? are actually beneficial for the performance
of the system. For example, it is known that sys-
tems optimised on the BLEU metric tend to produce
short hypotheses. In this sense, the extra words con-
sidered as errors by the EXTer measure may be ac-
tually beneficial for the overall performance of the
system.
3.3 IBM1 Scores
IBM1-like scores on the sentence level are known to
perform well for the rescoring of n-best lists from
a single system (see e.g. (Hasan et al, 2007)). Ad-
ditionally, they have been shown in (Popovic et al,
2011) to correlate well with human judgement for
evaluation purposes. We thus include them as addi-
tional features.
2Computed using the TreeTagger tool (http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/)
3The abbreviations for the features are taken over directly
from the output of the tool.
De-En En-De
Best System 20.4 13.1
Worst System 18.2 11.6
Sentence Selection 20.9 13.3
Table 3: Sentence selection results
3.4 Additional Language Model
We used a 5-gram language model trained on the
whole news-crawl corpus as an additional model for
rescoring. We used a different language model as the
one described in Section 2.1.2 as not to favor those
systems that already included it at decoding time.
3.5 Experimental Results
The sentence selection improved a little bit over the
best single system for German to English transla-
tion, but hardly so for English to German, as shown
in Table 3. For English to German this can be due to
the small amount of systems that were available for
the sentence selection system. Note also that these
results are measured on the same corpus the system
was trained on, so we expect the improvement on
unseen test data to be even smaller. Nevertheless the
sentence selection system constituted our final sub-
mission for the MT task.
4 Conclusions
For this year?s evaluation DFKI used a statistical
system based around the Jane machine translation
toolkit (Vilar et al, 2010a), working in its two
modalities: phrase-based and hierarchical phrase-
based models. Different enhancements were tried
in addition to the baseline configuration: POS-based
verb reordering, monolingual data selection, poor
man?s syntax and lightly supervised training, with
mixed results.
A sentence selection mechanism has later been
applied in order to combine the output of the dif-
ferent configurations. Although encouraging results
had been obtained in (Vilar et al, 2011), for this task
we found only a small improvement. This may be
due to the strong similarity of the systems, as they
are basically trained on the same data. In (Vilar et
al., 2011) the training data was varied across the sys-
tems, which may have produced a bigger variety in
385
the translation outputs that can be of advantage for
the selection mechanism. This is an issue that should
be explored in more detail for further work.
We also plan to do a comparison with system
combination approaches where new hypotheses can
be generated (instead of selecting one from a pre-
defined set), and study under which conditions each
approach is more suited than the other.
5 Acknowledgements
This work was done with the support of the
TaraXU? Project4, financed by TSB Technologies-
tiftung Berlin-Zukunftsfonds Berlin, co-financed by
the European Union-European fund for regional de-
velopment.
References
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of Bleu in
Machine Translation Research. In Proc. of the 11th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 249?256,
Trento, Italy, April.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 555?563,
Los Angeles, CA, USA.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007.
Minimum Bayes risk decoding for BLEU. In Annual
Meeting of the Assoc. for Computational Linguistics,
pages 101?104, Prague, Czech Republic, June.
Sas?a Hasan, Richard Zens, and Hermann Ney. 2007. Are
very large N-best lists useful for SMT? In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting, pages 57?60, Rochester, NY, April. Associa-
tion for Computational Linguistics.
A.S. Hildebrand and S. Vogel. 2008. Combination of
machine translation systems via hypothesis selection
from combined n-best lists. In MT at work: Proc. of
the Eighth Conference of the Association for Machine
Translation in the Americas, pages 254?261.
Matthias Huck, David Vilar, Daniel Stein, and Hermann
Ney. 2011. Lightly-Supervised Training for Hier-
archical Phrase-Based Machine Translation. In The
4http://taraxu.dfki.de
EMNLP 2011 Workshop on Unsupervised Learning in
NLP, Edinburgh, UK, July.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a Method for Evaluating Automatic Evaluation Met-
rics for Machine Translation. In Proc. of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Geneva, Switzerland.
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose B. Marino,
Matthias Paulik, Salim Roukos, Holger Schwenk,
and Hermann Ney. 2008. System combination for
machine translation of spoken and written language.
IEEE Transactions on Audio, Speech and Language
Processing, 16(7):1222?1237, September.
R.C. Moore and W. Lewis. 2010. Intelligent selection of
language model training data. In Proceedings of the
ACL 2010 Conference Short Papers, pages 220?224.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Maja Popovic? and Hermann Ney. 2006. POS-based word
reorderings for statistical machine translation. In In-
ternational Conference on Language Resources and
Evaluation, pages 1278?1283, Genoa, Italy, May.
Maja Popovic? and Hermann Ney. 2011. Towards Au-
tomatic Error Analysis of Machine Translation Out-
put. Computational Linguistics, 37(4):657?688, De-
cember.
Maja Popovic, David Vilar, Eleftherios Avramidis, and
Aljoscha Burchardt. 2011. Evaluation without ref-
erences: Ibm1 scores as evaluation metrics. In Proc.
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 99?103. Association for Computational
Linguistics, July.
Maja Popovic?. 2011. Hjerson: An Open Source Tool
for Automatic Error Classification of Machine Trans-
lation Output. The Prague Bulletin of Mathematical
Linguistics, pages 59?68.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
Softening Syntactic Constraints to Improve Statisti-
cal Machine Translation. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 236?244, Boulder,
Colorado, USA, June.
386
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Transla-
tion Output. In International Conference on Language
Resources and Evaluation, pages 697?702, Genoa,
Italy, May.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010a. Jane: Open Source Hierarchical Transla-
tion, Extended with Reordering and Lexicon Models.
In Proc. of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 262?270,
Uppsala, Sweden, July.
David Vilar, Daniel Stein, Stephan Peitz, and Hermann
Ney. 2010b. If I Only Had a Parser: Poor Man?s
Syntax for Hierarchical Machine Translation. In Inter-
national Workshop on Spoken Language Translation,
pages 345?352, Paris, France, December.
David Vilar, Eleftherios Avramidis, Maja Popovic?, and
Sabine Hunsicker. 2011. Dfki?s sc and mt submis-
sions to iwslt 2011. In International Workshop on Spo-
ken Language Translation, San Francisco, CA, USA,
December.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
387
Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 29?38,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
A Performance Study of Cube Pruning for Large-Scale Hierarchical
Machine Translation
Matthias Huck1 and David Vilar2 and Markus Freitag1 and Hermann Ney1
1 Human Language Technology and Pattern 2 DFKI GmbH
Recognition Group, RWTH Aachen University Alt-Moabit 91c
D-52056 Aachen, Germany D-10559 Berlin, Germany
<surname>@cs.rwth-aachen.de david.vilar@dfki.de
Abstract
In this paper, we empirically investigate the
impact of critical configuration parameters in
the popular cube pruning algorithm for decod-
ing in hierarchical statistical machine transla-
tion. Specifically, we study how the choice
of the k-best generation size affects trans-
lation quality and resource requirements in
hierarchical search. We furthermore exam-
ine the influence of two different granular-
ities of hypothesis recombination. Our ex-
periments are conducted on the large-scale
Chinese?English and Arabic?English NIST
translation tasks. Besides standard hierarchi-
cal grammars, we also explore search with re-
stricted recursion depth of hierarchical rules
based on shallow-1 grammars.
1 Introduction
Cube pruning (Chiang, 2007) is a widely used
search strategy in state-of-the-art hierarchical de-
coders. Some alternatives and extensions to the
classical algorithm as proposed by David Chiang
have been presented in the literature since, e.g. cube
growing (Huang and Chiang, 2007), lattice-based
hierarchical translation (Iglesias et al, 2009b; de
Gispert et al, 2010), and source cardinality syn-
chronous cube pruning (Vilar and Ney, 2012). Stan-
dard cube pruning remains the commonly adopted
decoding procedure in hierarchical machine transla-
tion research at the moment, though. The algorithm
has meanwhile been implemented in many publicly
available toolkits, as for example in Moses (Koehn
et al, 2007; Hoang et al, 2009), Joshua (Li et
al., 2009a), Jane (Vilar et al, 2010), cdec (Dyer et
al., 2010), Kriya (Sankaran et al, 2012), and Niu-
Trans (Xiao et al, 2012). While the plain hierar-
chical approach to machine translation (MT) is only
formally syntax-based, cube pruning can also be uti-
lized for decoding with syntactically or semantically
enhanced models, for instance those by Venugopal
et al (2009), Shen et al (2010), Xie et al (2011),
Almaghout et al (2012), Li et al (2012), Williams
and Koehn (2012), or Baker et al (2010).
Here, we look into the following key aspects of hi-
erarchical phrase-based translation with cube prun-
ing:
? Deep vs. shallow grammar.
? k-best generation size.
? Hypothesis recombination scheme.
We conduct a comparative study of all combinations
of these three factors in hierarchical decoding and
present detailed experimental analyses with respect
to translation quality and search efficiency. We fo-
cus on two tasks which are of particular interest to
the research community: the Chinese?English and
Arabic?English NIST OpenMT translation tasks.
The paper is structured as follows: We briefly out-
line some important related work in the following
section. We subsequently give a summary of the
grammars used in hierarchical phrase-based trans-
lation, including a presentation of the difference be-
tween a deep and a shallow-1 grammar (Section 3).
Essential aspects of hierarchical search with the
cube pruning algorithm are explained in Section 4.
We show how the k-best generation size is defined
(we apply the limit without counting recombined
29
candidates), and we present the two different hy-
pothesis recombination schemes (recombination T
and recombination LM). Our empirical investiga-
tions and findings constitute the major part of this
work: In Section 5, we first accurately describe our
setup, then conduct a number of comparative exper-
iments with varied parameters on the two translation
tasks, and finally analyze and discuss the results. We
conclude the paper in Section 6.
2 Related Work
Hierarchical phrase-based translation (HPBT) was
first proposed by Chiang (2005). Chiang also in-
troduced the cube pruning algorithm for hierarchical
search (Chiang, 2007). It is basically an adaptation
of one of the k-best parsing algorithms by Huang
and Chiang (2005). Good descriptions of the cube
pruning implementation in the Joshua decoder have
been provided by Li and Khudanpur (2008) and Li
et al (2009b). Xu and Koehn (2012) implemented
hierarchical search with the cube growing algorithm
in Moses and compared its performance to Moses?
cube pruning implementation. Heafield et al re-
cently developed techniques to speed up hierarchical
search by means of an improved language model in-
tegration (Heafield et al, 2011; Heafield et al, 2012;
Heafield et al, 2013).
3 Probabilistic SCFGs for HPBT
In hierarchical phrase-based translation, a proba-
bilistic synchronous context-free grammar (SCFG)
is induced from a bilingual text. In addition to con-
tinuous lexical phrases, hierarchical phrases with
usually up to two gaps are extracted from the word-
aligned parallel training data.
Deep grammar. The non-terminal set of a stan-
dard hierarchical grammar comprises two symbols
which are shared by source and target: the initial
symbol S and one generic non-terminal symbol X .
Extracted rules of a standard hierarchical grammar
are of the form X ? ??, ?,? ? where ??, ?? is a
bilingual phrase pair that may contain X , i.e. ? ?
({X } ? VF )+ and ? ? ({X } ? VE)+, where VF
and VE are the source and target vocabulary, respec-
tively. The ? relation denotes a one-to-one corre-
spondence between the non-terminals in ? and in ?.
A non-lexicalized initial rule and a special glue rule
complete the grammar. We denote standard hierar-
chical grammars as deep grammars here.
Shallow-1 grammar. Iglesias et al (2009a) pro-
pose a limitation of the recursion depth for hierar-
chical rules with shallow grammars. In a shallow-1
grammar, the generic non-terminal X of the stan-
dard hierarchical approach is replaced by two dis-
tinct non-terminals XH and XP . By changing the
left-hand sides of the rules, lexical phrases are al-
lowed to be derived from XP only, hierarchical
phrases from XH only. On all right-hand sides of
hierarchical rules, the X is replaced by XP . Gaps
within hierarchical phrases can thus be filled with
continuous lexical phrases only, not with hierarchi-
cal phrases. The initial and glue rules are adjusted
accordingly.
4 Hierarchical Search with Cube Pruning
Hierarchical search is typically carried out with a
parsing-based procedure. The parsing algorithm is
extended to handle translation candidates and to in-
corporate language model scores via cube pruning.
The cube pruning algorithm. Cube pruning op-
erates on a hypergraph which represents the whole
parsing space. This hypergraph is built employ-
ing a customized version of the CYK+ parsing al-
gorithm (Chappelier and Rajman, 1998). Given
the hypergraph, cube pruning expands at most k
derivations at each hypernode.1 The pseudocode
of the k-best generation step of the cube pruning
algorithm is shown in Figure 1. This function is
called in bottom-up topological order for all hy-
pernodes. A heap of active derivations A is main-
tained. A initially contains the first-best derivations
for each incoming hyperedge (line 1). Active deriva-
tions are processed in a loop (line 3) until a limit k
is reached or A is empty. If a candidate deriva-
tion d is recombinable, the RECOMBINE auxiliary
function recombines it and returns true; otherwise
(for non-recombinable candidates) RECOMBINE re-
turns false. Non-recombinable candidates are ap-
pended to the list D of k-best derivations (line 6).
This list will be sorted before the function terminates
1The hypergraph on which cube pruning operates can be
constructed based on other techniques, such as tree automata,
but CYK+ parsing is the dominant approach.
30
(line 8). The PUSHSUCC auxiliary function (line 7)
updates A with the next best derivations following d
along the hyperedge. PUSHSUCC determines the
cube order by processing adjacent derivations in a
specific sequence (of predecessor hypernodes along
the hyperedge and phrase translation options).2
k-best generation size. Candidate derivations are
generated by cube pruning best-first along the in-
coming hyperedges. A problem results from the lan-
guage model integration, though: As soon as lan-
guage model context is considered, monotonicity
properties of the derivation cost can no longer be
guaranteed. Thus, even for single-best translation,
k-best derivations are collected to a buffer in a beam
search manner and finally sorted according to their
cost. The k-best generation size is consequently a
crucial parameter to the cube pruning algorithm.
Hypothesis recombination. Partial hypotheses
with states that are indistinguishable from each other
are recombined during search. We define two no-
tions of when to consider two derivations as indis-
tinguishable, and thus when to recombine them:
Recombination T. The T recombination scheme
recombines derivations that produce identical
translations.
Recombination LM. The LM recombination
scheme recombines derivations with identical
language model context.
Recombination is conducted within the loop of
the k-best generation step of cube pruning. Re-
combined derivations do not increment the gener-
ation count; the k-best generation limit is thus ef-
fectively applied after recombination.3 In general,
more phrase translation candidates per hypernode
are being considered (and need to be rated with the
language model) in the recombination LM scheme
compared to the recombination T scheme. The more
partial hypotheses can be recombined, the more it-
erations of the inner code block of the k-best gen-
eration loop are possible. The same internal k-best
2See Vilar (2011) for the pseudocode of the PUSHSUCC
function and other details which are omitted here.
3Whether recombined derivations contribute to the genera-
tion count or not is a configuration decision (or implementa-
tion decision). Please note that some publicly available toolkits
count recombined derivations by default.
Input: a hypernode and the size k of the k-best list
Output: D, a list with the k-best derivations
1 let A? heap({(e,1|e|) | e ? incoming edges)})
2 let D ? [ ]
3 while |A| > 0 ? |D| < k do
4 d? pop(A)
5 if not RECOMBINE(D, d) then
6 D ? D ++ [d]
7 PUSHSUCC(d,A)
8 sort D
Figure 1: k-best generation with the cube pruning al-
gorithm.
generation size results in a larger search space for re-
combination LM. We will examine how the overall
number of loop iterations relates to the k-best gener-
ation limit. By measuring the number of derivations
as well as the number of recombination operations
on our test sets, we will be able to give an insight
into how large the fraction of recombinable candi-
dates is for different configurations.
5 Experiments
We conduct experiments which evaluate perfor-
mance in terms of both translation quality and
computational efficiency, i.e. translation speed and
memory consumption, for combinations of deep
or shallow-1 grammars with the two hypothesis
recombination schemes and an exhaustive range
of k-best generation size settings. Empirical re-
sults are presented on the Chinese?English and
Arabic?English 2008 NIST tasks (NIST, 2008).
5.1 Experimental Setup
We work with parallel training corpora of 3.0 M
Chinese?English sentence pairs (77.5 M Chinese /
81.0 M English running words after preprocessing)
and 2.5 M Arabic?English sentence pairs (54.3 M
Arabic / 55.3 M English running words after prepro-
cessing), respectively. Word alignments are created
by aligning the data in both directions with GIZA++
and symmetrizing the two trained alignments (Och
and Ney, 2003). When extracting phrases, we apply
several restrictions, in particular a maximum length
of ten on source and target side for lexical phrases,
a length limit of five on source and ten on target
side for hierarchical phrases (including non-terminal
symbols), and no more than two gaps per phrase.
31
Table 1: Data statistics for the test sets. Numbers have
been replaced by a special category symbol.
Chinese MT08 Arabic MT08
Sentences 1 357 1 360
Running words 34 463 45 095
Vocabulary 6 209 9 387
The decoder loads only the best translation options
per distinct source side with respect to the weighted
phrase-level model scores (100 for Chinese, 50 for
Arabic). The language models are 4-grams with
modified Kneser-Ney smoothing (Kneser and Ney,
1995; Chen and Goodman, 1998) which have been
trained with the SRILM toolkit (Stolcke, 2002).
During decoding, a maximum length constraint
of ten is applied to all non-terminals except the ini-
tial symbol S . Model weights are optimized with
MERT (Och, 2003) on 100-best lists. The op-
timized weights are obtained (separately for deep
and for shallow-1 grammars) with a k-best gen-
eration size of 1 000 for Chinese?English and of
500 for Arabic?English and kept for all setups.
We employ MT06 as development sets. Trans-
lation quality is measured in truecase with BLEU
(Papineni et al, 2002) on the MT08 test sets.
Data statistics for the preprocessed source sides of
both the Chinese?English MT08 test set and the
Arabic?English MT08 test set are given in Table 1.
Our translation experiments are conducted with
the open source translation toolkit Jane (Vilar et
al., 2010; Vilar et al, 2012). The core imple-
mentation of the toolkit is written in C++. We
compiled with GCC version 4.4.3 using its -O2
optimization flag. We employ the SRILM li-
braries to perform language model scoring in the
decoder. In binarized version, the language mod-
els have a size of 3.6G (Chinese?English) and 6.2G
(Arabic?English). Language models and phrase ta-
bles have been copied to the local hard disks of the
machines. In all experiments, the language model
is completely loaded beforehand. Loading time of
the language model and any other initialization steps
are not included in the measured translation time.
Phrase tables are in the Jane toolkit?s binarized for-
mat. The decoder initializes the prefix tree struc-
ture, required nodes get loaded from secondary stor-
age into main memory on demand, and the loaded
content is being cleared each time a new input sen-
tence is to be parsed. There is nearly no overhead
due to unused data in main memory. We do not
rely on memory mapping. Memory statistics are
with respect to virtual memory. The hardware was
equipped with RAM well beyond the requirements
of the tasks, and sufficient memory has been re-
served for the processes.
5.2 Experimental Results
Figures 2 and 3 depict how the Chinese?English
and Arabic?English setups behave in terms of
translation quality. The k-best generation size in
cube pruning is varied between 10 and 10 000.
The four graphs in each plot illustrate the results
with combinations of deep grammar and recombi-
nation scheme T, deep grammar and recombination
scheme LM, shallow grammar and recombination
scheme T, as well as shallow grammar and recom-
bination scheme LM. Figures 4 and 5 show the cor-
responding translation speed in words per second for
these settings. The maximum memory requirements
in gigabytes are given in Figures 6 and 7. In order
to visualize the trade-offs between translation qual-
ity and resource consumption somewhat better, we
plotted translation quality against time requirements
in Figures 8 and 9 and translation quality against
memory requirements in Figures 10 and 11. Transla-
tion quality and model score (averaged over all sen-
tences; higher is better) are nicely correlated for all
configurations, as can be concluded from Figures 12
through 15.
5.3 Discussion
Chinese?English. For Chinese?English trans-
lation, the system with deep grammar performs gen-
erally a bit better with respect to quality than the
shallow one, which accords with the findings of
other groups (de Gispert et al, 2010; Sankaran et
al., 2012). The LM recombination scheme yields
slightly better quality than the T scheme, and with
the shallow-1 grammar it outperforms the T scheme
at any given fixed amount of time or memory allo-
cation (Figures 8 and 10).
Shallow-1 translation is up to roughly 2.5 times
faster than translation with the deep grammar. How-
ever, the shallow-1 setups are considerably slowed
down at higher k-best sizes as well, while the ef-
fort pays off only very moderately. Overall, the
32
 23
 23.5
 24
 24.5
 25
 25.5
 10  100  1000  10000
BLE
U [%
]
k-best generation size
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 2: Chinese?English translation quality (truecase).
 42.5
 43
 43.5
 44
 44.5
 45
 10  100  1000  10000
BLE
U [%
]
k-best generation size
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 3: Arabic?English translation quality (truecase).
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10  100  1000  10000
wo
rds
 pe
r se
con
d
k-best generation size
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 4: Chinese?English translation speed.
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 10  100  1000  10000
wo
rds
 pe
r se
con
d
k-best generation size
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 5: Arabic?English translation speed.
 0
 8
 16
 24
 32
 40
 10  100  1000  10000
giga
byte
s
k-best generation size
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 6: Chinese?English memory requirements.
 0
 8
 16
 24
 32
 40
 10  100  1000  10000
giga
byte
s
k-best generation size
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 7: Arabic?English memory requirements.
33
 23
 23.5
 24
 24.5
 25
 25.5
 0.125 0.25  0.5  1  2  4  8  16  32
BLE
U [%
]
seconds per word
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 8: Trade-off between translation quality and speed
for Chinese?English.
 42.5
 43
 43.5
 44
 44.5
 45
 0.125 0.25  0.5  1  2  4  8  16  32
BLE
U [%
]
seconds per word
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 9: Trade-off between translation quality and speed
for Arabic?English.
 23
 23.5
 24
 24.5
 25
 25.5
 8  16  32  64
BLE
U [%
]
gigabytes
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 10: Trade-off between translation quality and mem-
ory requirements for Chinese?English.
 42.5
 43
 43.5
 44
 44.5
 45
 16  32  64  128
BLE
U [%
]
gigabytes
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 11: Trade-off between translation quality and mem-
ory requirements for Arabic?English.
shallow-1 grammar at a k-best size between 100 and
1 000 seems to offer a good compromise of quality
and efficiency. Deep translation with k = 2000 and
the LM recombination scheme promises high qual-
ity translation, but note the rapid memory consump-
tion increase beyond k = 1000 with the deep gram-
mar. At k ? 1 000, memory consumption is not an
issue in both deep and shallow systems, but transla-
tion speed starts to drop at k > 100 already.
Arabic?English. Shallow-1 translation produces
competitive quality for Arabic?English translation
(de Gispert et al, 2010; Huck et al, 2011). The
LM recombination scheme boosts the BLEU scores
slightly. The systems with deep grammar are slowed
down strongly with every increase of the k-best size.
Their memory consumption likewise inflates early.
We actually stopped running experiments with deep
grammars for Arabic?English at k = 7000 for the
T recombination scheme, and at k = 700 for the LM
recombination scheme because 124G of memory did
not suffice any more for higher k-best sizes. The
memory consumption of the shallow systems stays
nearly constant across a large range of the surveyed
k-best sizes, but Figure 11 reveals a plateau where
more resources do not improve translation quality.
Increasing k from 100 to 2 000 in the shallow setup
with LM recombination provides half a BLEU point,
but reduces speed by a factor of more than 10.
34
 23
 23.5
 24
 24.5
 25
 25.5
-8.7 -8.65 -8.6 -8.55 -8.5 -8.45 -8.4
BLE
U [%
]
average model score
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
Figure 12: Relation of translation quality and average
model score for Chinese?English (deep grammar).
 42.5
 43
 43.5
 44
 44.5
 45
-6.6 -6.5 -6.4 -6.3 -6.2 -6.1
BLE
U [%
]
average model score
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
Figure 13: Relation of translation quality and average
model score for Arabic?English (deep grammar).
 23
 23.5
 24
 24.5
 25
 25.5
-9.4 -9.35 -9.3 -9.25 -9.2 -9.15 -9.1
BLE
U [%
]
average model score
NIST Chinese-to-English (MT08)
shallow-1, recombination T
shallow-1, recombination LM
Figure 14: Relation of translation quality and average
model score for Chinese?English (shallow-1 grammar).
 42.5
 43
 43.5
 44
 44.5
 45
-12.1 -12 -11.9 -11.8 -11.7 -11.6
BLE
U [%
]
average model score
NIST Arabic-to-English (MT08)
shallow-1, recombination T
shallow-1, recombination LM
Figure 15: Relation of translation quality and average
model score for Arabic?English (shallow-1 grammar).
Actual amount of derivations. We measured the
amount of hypernodes (Table 2), the amount of actu-
ally generated derivations after recombination, and
the amount of generated candidate derivations in-
cluding recombined ones?or, equivalently, loop it-
erations in the algorithm from Figure 1?for se-
lected limits k (Tables 3 and 4). The ratio of the
average amount of derivations per hypernode after
and before recombination remains consistently at
low values for all recombination T setups. For the
setups with LM recombination scheme, this recom-
bination factor rises with larger k, i.e. the fraction
of recombinable candidates increases. The increase
is remarkably pronounced for Arabic?English with
deep grammar. The steep slope of the recombina-
tion factor may be interpreted as an indicator for un-
desired overgeneration of the deep grammar on the
Arabic?English task.
6 Conclusion
We systematically studied three key aspects of hier-
archical phrase-based translation with cube pruning:
Deep vs. shallow-1 grammars, the k-best generation
size, and the hypothesis recombination scheme. In
a series of empirical experiments, we revealed the
trade-offs between translation quality and resource
requirements to a more fine-grained degree than this
is typically done in the literature.
35
Table 2: Average amount of hypernodes per sentence and average length of the preprocessed input sentences on the
NIST Chinese?English (MT08) and Arabic?English (MT08) tasks.
Chinese?English Arabic?English
deep shallow-1 deep shallow-1
avg. #hypernodes per sentence 480.5 200.7 896.4 308.4
avg. source sentence length 25.4 33.2
Table 3: Detailed statistics about the actual amount of derivations on the NIST Chinese?English task (MT08).
deep
recombination T recombination LM
avg. #derivations avg. #derivations avg. #derivations avg. #derivations
per hypernode per hypernode per hypernode per hypernode
k (after recombination) (incl. recombined) factor (after recombination) (incl. recombined) factor
10 10.0 11.7 1.17 10.0 18.2 1.82
100 99.9 120.1 1.20 99.9 275.8 2.76
1000 950.1 1142.3 1.20 950.1 4246.9 4.47
10000 9429.8 11262.8 1.19 9418.1 72008.4 7.65
shallow-1
recombination T recombination LM
avg. #derivations avg. #derivations avg. #derivations avg. #derivations
per hypernode per hypernode per hypernode per hypernode
k (after recombination) (incl. recombined) factor (after recombination) (incl. recombined) factor
10 9.7 11.3 1.17 9.6 13.6 1.41
100 90.8 105.2 1.16 90.4 168.6 1.86
1000 707.3 811.3 1.15 697.4 2143.4 3.07
10000 6478.1 7170.4 1.11 6202.8 34165.6 5.51
Table 4: Detailed statistics about the actual amount of derivations on the NIST Arabic?English task (MT08).
deep
recombination T recombination LM
avg. #derivations avg. #derivations avg. #derivations avg. #derivations
per hypernode per hypernode per hypernode per hypernode
k (after recombination) (incl. recombined) factor (after recombination) (incl. recombined) factor
10 10.0 18.3 1.83 10.0 71.5 7.15
100 98.0 177.4 1.81 98.0 1726.0 17.62
500 482.1 849.0 1.76 482.1 14622.1 30.33
1000 961.8 1675.0 1.74 ? ? ?
shallow-1
recombination T recombination LM
avg. #derivations avg. #derivations avg. #derivations avg. #derivations
per hypernode per hypernode per hypernode per hypernode
k (after recombination) (incl. recombined) factor (after recombination) (incl. recombined) factor
10 9.6 12.1 1.26 9.6 16.6 1.73
100 80.9 105.2 1.30 80.2 193.8 2.42
1000 690.1 902.1 1.31 672.1 2413.0 3.59
10000 5638.6 7149.5 1.27 5275.1 31283.6 5.93
36
Acknowledgments
This work was partly achieved as part of the Quaero
Programme, funded by OSEO, French State agency
for innovation. This material is also partly based
upon work supported by the DARPA BOLT project
under Contract No. HR0011-12-C-0015. Any opin-
ions, findings and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the
DARPA. The research leading to these results has
received funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no 287658.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2012. Ex-
tending CCG-based Syntactic Constraints in Hierar-
chical Phrase-Based SMT. In Proc. of the Annual
Conf. of the European Assoc. for Machine Translation
(EAMT), pages 193?200, Trento, Italy, May.
Kathryn Baker, Michael Bloodgood, Chris Callison-
Burch, Bonnie Dorr, Nathaniel Filardo, Lori
Levin, Scott Miller, and Christine Piatko. 2010.
Semantically-Informed Syntactic Machine Transla-
tion: A Tree-Grafting Approach. In Proc. of the Conf.
of the Assoc. for Machine Translation in the Americas
(AMTA), Denver, CO, USA, October/November.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochastic
CFG. In Proc. of the First Workshop on Tabulation in
Parsing and Deduction, pages 133?137, Paris, France,
April.
Stanley F. Chen and Joshua Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report TR-10-98, Computer
Science Group, Harvard University, Cambridge, MA,
USA, August.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc. of
the Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 263?270, Ann Arbor, MI,
USA, June.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hierar-
chical Phrase-Based Translation with Weighted Finite-
State Transducers and Shallow-n Grammars. Compu-
tational Linguistics, 36(3):505?533.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning framework for
finite-state and context-free translation models. In
Proc. of the ACL 2010 System Demonstrations, pages
7?12, Uppsala, Sweden, July.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left Language
Model State for Syntactic Machine Translation. In
Proc. of the Int. Workshop on Spoken Language Trans-
lation (IWSLT), pages 183?190, San Francisco, CA,
USA, December.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language Model Rest Costs and Space-Efficient Stor-
age. In Proc. of the 2012 Joint Conf. on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, EMNLP-
CoNLL ?12, pages 1169?1178, Jeju Island, Korea,
July.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2013.
Grouping Language Model Boundary Words to Speed
K-Best Extraction from Hypergraphs. In Proc. of the
Human Language Technology Conf. / North American
Chapter of the Assoc. for Computational Linguistics
(HLT-NAACL), Atlanta, GA, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchical,
and Syntax-Based Statistical Machine Translation. In
Proc. of the Int. Workshop on Spoken Language Trans-
lation (IWSLT), pages 152?159, Tokyo, Japan, Decem-
ber.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proc. of the 9th Int. Workshop on Parsing
Technologies, pages 53?64, October.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proc. of the Annual Meeting of the Assoc. for Com-
putational Linguistics (ACL), pages 144?151, Prague,
Czech Republic, June.
Matthias Huck, David Vilar, Daniel Stein, and Hermann
Ney. 2011. Advancements in Arabic-to-English Hier-
archical Machine Translation. In 15th Annual Confer-
ence of the European Association for Machine Trans-
lation, pages 273?280, Leuven, Belgium, May.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Rule Filtering by Pat-
tern for Efficient Hierarchical Translation. In Proc. of
the 12th Conf. of the Europ. Chapter of the Assoc. for
Computational Linguistics (EACL), pages 380?388,
Athens, Greece, March.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Hierarchical Phrase-
Based Translation with Weighted Finite State Trans-
37
ducers. In Proc. of the Human Language Technology
Conf. / North American Chapter of the Assoc. for Com-
putational Linguistics (HLT-NAACL), pages 433?441,
Boulder, CO, USA, June.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modeling. In
Proc. of the International Conf. on Acoustics, Speech,
and Signal Processing, volume 1, pages 181?184, De-
troit, MI, USA, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proc. of the Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 177?180, Prague, Czech Republic, June.
Zhifei Li and Sanjeev Khudanpur. 2008. A Scalable
Decoder for Parsing-Based Machine Translation with
Equivalent Language Model State Maintenance. In
Proceedings of the Second Workshop on Syntax and
Structure in Statistical Translation, SSST ?08, pages
10?18, Columbus, OH, USA, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009a. Joshua: An Open
Source Toolkit for Parsing-Based Machine Transla-
tion. In Proc. of the Workshop on Statistical Machine
Translation (WMT), pages 135?139, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur, and
Wren Thornton. 2009b. Decoding in Joshua: Open
Source, Parsing-Based Machine Translation. The
Prague Bulletin of Mathematical Linguistics, (91):47?
56, January.
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van
Genabith. 2012. Using Syntactic Head Information in
Hierarchical Phrase-Based Translation. In Proc. of the
Workshop on Statistical Machine Translation (WMT),
pages 232?242, Montre?al, Canada, June.
NIST. 2008. Open Machine Translation 2008 Evalua-
tion. http://www.itl.nist.gov/iad/mig/
tests/mt/2008/.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of the An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of the Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, USA, July.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya - An end-to-end Hierarchical Phrase-
based MT System. The Prague Bulletin of Mathemat-
ical Linguistics, (97):83?98, April.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-Dependency Statistical Machine Translation.
Computational Linguistics, 36(4):649?671, Decem-
ber.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 3,
Denver, CO, USA, September.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
Softening Syntactic Constraints to Improve Statisti-
cal Machine Translation. In Proc. of the Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics (HLT-
NAACL), pages 236?244, Boulder, CO, USA, June.
David Vilar and Hermann Ney. 2012. Cardinality
pruning and language model heuristics for hierarchi-
cal phrase-based translation. Machine Translation,
26(3):217?254, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Transla-
tion, Extended with Reordering and Lexicon Models.
In Proc. of the Workshop on Statistical Machine Trans-
lation (WMT), pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2012. Jane: an advanced freely available hierar-
chical machine translation toolkit. Machine Transla-
tion, 26(3):197?216, September.
David Vilar. 2011. Investigations on Hierarchi-
cal Phrase-Based Machine Translation. Ph.D. the-
sis, RWTH Aachen University, Aachen, Germany,
November.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proc. of the Workshop on Statistical Machine Transla-
tion (WMT), pages 388?394, Montre?al, Canada, June.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li. 2012.
NiuTrans: An Open Source Toolkit for Phrase-based
and Syntax-based Machine Translation. In Proc. of
the ACL 2012 System Demonstrations, pages 19?24,
Jeju, Republic of Korea, July.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A Novel
Dependency-to-String Model for Statistical Machine
Translation. In Proc. of the Conf. on Empirical Meth-
ods for Natural Language Processing (EMNLP), pages
216?226, Edinburgh, Scotland, UK, July.
Wenduan Xu and Philipp Koehn. 2012. Extending Hiero
Decoding in Moses with Cube Growing. The Prague
Bulletin of Mathematical Linguistics, (98):133?142,
October.
38
