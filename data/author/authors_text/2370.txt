Fine Grained Classification of Named Entities 
 
Michael Fleischman and Eduard Hovy 
USC Information Science Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
U.S.A. 
{fleisch, hovy} @ISI.edu 
 
Abstract 
 
While Named Entity extraction is useful in 
many natural language applications, the 
coarse categories that most NE extractors 
work with prove insufficient for complex 
applications such as Question Answering and 
Ontology generation.  We examine one 
coarse category of named entities, persons, 
and describe a method for automatically 
classifying person instances into eight finer-
grained subcategories.  We present a 
supervised learning method that considers the 
local context surrounding the entity as well as 
more global semantic information derived 
from topic signatures and WordNet.  We 
reinforce this method with an algorithm that 
takes advantage of the presence of entities in 
multiple contexts. 
 
1. Introduction 
 
There has been much interest in the recent past 
concerning automated categorization of named 
entities in text.  Recent advances have made some 
systems (such as BBN?s IdentiFinder (Bikel, 
1999)) very successful when classifying named 
entities into broad categories, such as person, 
organization, and location.  While the accurate 
classification of general named entities is useful in 
many areas of natural language research, more fine-
grained categorizations would be of particular 
value in areas such as Question Answering, 
information retrieval, and the automated 
construction of ontologies.   
The research presented here focuses on the 
subcategorization of person names, which extends 
research on the subcategorization of location names 
(Fleischman, 2001).  While locations can often be 
classified based solely on the words that surround 
the instance, person names are often more 
challenging because classification relies on much 
deeper semantic intuitions gained from the 
surrounding text.  Further, unlike the case with 
location names, exhaustive lists of person names by 
category do not exist and cannot be relied upon for 
training and test set generation.  Finally, the domain 
of person names presents a challenge because the 
same individual (e.g., ?Ronald Reagan?) is often 
represented differently at different points in the 
same text (e.g., ?Mr. Reagan?, ?Reagan?, etc.). 
The subcategorization of person names is not a 
trivial task for humans either, as the examples 
below illustrate.  Here, names of persons have been 
encrypted using a simple substitution cipher.  The 
names are of only three subtypes: politician, 
businessperson, and entertainer, yet prove 
remarkably difficult to classify based upon the 
context of the sentence. 
 
1.  Unfortunately, Mocpm_____ and his immediate 
family did not cooperate in the making of the film . 
2. "The idea that they'd introduce Npn Fuasm______ 
into that is amazing ,"he said. 
3. "It's dangerous to be right when government is 
wrong ," Lrsyomh______ told reporters 
 
1. Mocpm = Nixon: politician 
2. Npn Fuasm = Bob Dylan: entertainer 
3. Lrsyomh = Keating: businessperson 
 
In this work we examine how different features 
and learning algorithms can be employed to 
automatically subcategorize person names in text.  
In doing this we address how to inject semantic 
information into the feature space, how to 
automatically generate training sets for use with 
supervised learning algorithms, and how to handle 
orthographic inconsistencies between instances of 
the same person. 
 
2. Data Set Generation 
 
A large corpus of person instances was 
compiled from a TREC9 database consisting of 
articles from the Associated Press and the Wall 
Street Journal.  Data was word tokenized, stemmed 
 using the Porter stemming algorithm (Porter, 1980), 
part of speech tagged using Brill?s tagger (Brill, 
1994), and named entity tagged using BBN?s 
IdentiFinder (Bikel, 1999).  Person instances were 
classified into one of eight categories: athlete, 
politician/government, clergy, businessperson, 
entertainer/artist, lawyer, doctor/scientist, and 
police.  These eight categories were chosen because 
of their high frequency in the corpus and also 
because of their usefulness in applications such as 
Question Answering.  A training set of roughly 
25,000 person instances was then created using a 
partially automated classification system. 
In generating the training data automatically we 
first attempted to use the simple tagging method 
described for location names in (Fleischman, 
2001).  This method involved collecting lists of 
instances of each category from the Internet and 
using those lists to classify person names found by 
IdentiFinder.  Although robust with location names, 
this method proved inadequate with persons (in a 
sample of 300, over 25% of the instances were 
found to be incorrect).  This was due to the fact that 
the same name will often refer to multiple 
individuals (e.g., ?Paul Simon? refers to a 
politician, an entertainer, and Belgian scientist). 
In order to avoid this problem we implemented 
a simple bootstrapping procedure in which a seed 
data set of 100 instances of each of the eight 
categories was hand tagged and used to generate a 
decision list classifier using the C4.5 algorithm 
(Quinlan, 1993) with the word frequency and topic 
signature features described below.  This simple 
classifier was then run over a large corpus and 
classifications with a confidence score above a 
90% threshold were collected.  These confident 
instances were then compared to the lists collected 
from the Internet, and, only if there was agreement 
between the two sources, were the instances 
included in the final training set.  This procedure 
produced a large training set with very few 
misclassified instances (over 99% of the instances 
in a sample of 300 were found to be correct).  A 
validation set of 1000 instances from this set was 
then hand tagged to assure proper classification. 
A consequence of using this method for data 
generation is that the training set created is not a 
random sample of person instances in the real 
world.  Rather, the training set is highly skewed, 
including only those instances that are both easy 
enough to classify using a simple classifier and 
common enough to be included in lists found on 
the Internet.  To examine the generalizability of 
classifiers trained on such data, a held out data set 
of 1300 instances, also from the AP and WSJ, was 
collected and hand tagged.   
 
3. Features 
 
3.1 Word Frequency Features 
 
Each instance in the text is paired with a set of 
features that represents how often the words 
surrounding the target instance occur with a 
specific sub-categorization in the training set.  For 
example, in example sentence 2 in the introduction, 
the word ?introduce? occurs immediately before 
the person instance.  The feature set describing this 
instance would thus include eight different features; 
each denoting the frequency with which 
?introduce? occurred in the training set 
immediately preceding an instance of a politician, a 
businessperson, an entertainer, etc.  The feature set 
includes these eight different frequencies for 10 
distinct word positions (totaling 80 features per 
instance).  The positions used include the three 
individual words before the occurrence of the 
instance, the three individual words after the 
instance, the two-word bigrams immediately before 
and after the instance, and the three-word trigrams 
immediately before and after the instance (see 
Figure 1). 
 
# Position N-gram Category Freq. 
1 previous unigram ?introduce? politician 3 
2 previous unigram ?introduce? entertainer 43 
3 following bigram ?into that? politician 2 
4 following bigram ?into that? business 0 
Figure 1.  Subset of word frequency features for instance in 
example 2, above.  Shows the frequency with which an n-gram 
appears in the training data in a specific position relative to 
instances of a specific category. 
 
These word frequency features provide 
information similar to the binary word features that 
are often used in text categorization (Yang, 1997) 
with only a fraction of the dimensionality.  Such 
reduced dimensionality feature sets can be 
preferable when classifying very small texts 
(Fleischman, in preparation).  
 
3.2 Topic Signature Features 
 
Inspection of the data made clear the need for 
semantic information during classification.  We 
therefore created features that use topic signatures 
 for each of the person subcategories.  A topic 
signature, as described in (Lin and Hovy, 2000), is 
a list of terms that can be used to signal the 
membership of a text in the relevant topic or 
category.  Each term in a text is given a topic 
signature score that indicates its ability to signal 
that the text is in a relevant category (the higher the 
score, the more that term is indicative of that 
category).  The topic signatures are automatically 
generated for each specific term by computing the 
likelihood ratio (?-score) between two hypotheses 
(Dunning, 1993).  The first hypothesis (h1) is that 
the probability (p1) that the text is in the relevant 
category, given a specific term, is equivalent to the 
probability (p2) that the text is in the relevant 
category, given any other term (h1: p1=p2).  The 
second hypothesis (h2) is that these two 
probabilities are not equivalent, and that p1 is much 
greater than p2 (h2: p1>>p2).  The calculation of 
this likelihood ratio [-2logL(h1)/L(h2)] for each 
feature and for each category gives a list of all the 
terms in a document set with scores indicating how 
much the presence of that term in a specific 
document indicates that the document is in a 
specific category.  
 
Politician Entertainer 
Word ?-score Word ?-score 
campaign 3457.049 Star 3283.872 
republican 1969.707 Actor 2478.675 
budget 140.292 Budget 17.312 
bigot 2.577 Sexist 3.874 
Figure 2.  Subset of topic signatures generated from training set 
for two categories. 
 
In creating topic signature features for the 
subcategorization of persons, we created a database 
of topic signatures generated from the training set 
(see Figure 2).1  Each sentence from the training set 
was treated as a unique document, and the 
classification of the instance contained in that 
sentence was treated as the relevant topic.  We 
implemented the algorithm described in (Lin and 
Hovy, 2000) with the addition of a cutoff, such that 
the topic signatures for a term are only included if 
the p1/p2 for that term is greater than the mean 
p1/p2 over all terms.  This modification was made 
to ensure the assumption that p1 is much greater 
than p2.  A weighted sum was then computed for 
each of the eight person subcategories according to 
the formula below: 
                                                 
1 To avoid noise, we used only those sentences in which 
each person instance was of the same category. 
 
Topic Sig ScoreType= ?N [ ?-score of wordn,Type 
          /(distance from instance)2] 
 
where N is the number of words in the sentence, 
?-score of wordn,Type is the topic signature score of 
word n for topic Type, and distance from instance 
is the number of words away from the instance 
that word n is.  These topic signature scores are 
calculated for each of the eight subcategories.   
These eight topic signature features convey 
semantic information about the overall context 
in which each instance exists.  The topic 
signature scores are weighted according to the 
inverse square of their distance under the (not 
always true) assumption that the farther away a 
word is from an instance, the less information it 
bears on classification.  This weighting is 
particularly important when instances of 
different categories occur in the same sentence 
(e.g., ??of those donating to Bush?s campaign 
was actor Arnold Schwarzenegger??). 
 
3.3 WordNet Features 
 
A natural limitation of the topic signature 
features is their inability to give weight to 
related and synonymous terms that do not 
appear in the training data.  To address this 
limitation, we took advantage of the online 
resource WordNet (Fellbaum, 1998).  The 
WordNet hypernym tree was expanded for each 
word surrounding the instance and each word in 
the tree was given a score based on the topic 
signature database generated from the training 
data.  The scores were then weighted by the 
inverse of their height in the tree and then 
summed together, similarly to the procedure in 
(Resnik, 1993).  These sums are computed for 
each word surrounding the instance, and are 
summed according to the weighting process 
described above.  This produces a distinct 
WordNet feature for each of the eight classes 
and is described by the equation below: 
 
WordNet Score Type=  
?N[?M ?-score of wordm,Type/(depth of wordm in WordNet)] 
/(distance from instance) 2 
 
where the variables are as above and M is the 
number of words in the WordNet hypernym 
tree.  These WordNet features supplement the 
coverage of the topic signatures generated from 
the training data by including synonyms that 
 may not have existed in that data set.  Further, 
the features include information gained from the 
hypernyms themselves (e.g., the hypernym of 
?Congress? is ?legislature?).  These final 
hypernym scores are weighted by the inverse of 
their height in the tree to reduce the effect of 
concepts that may be too general (e.g., at the top 
of the hypernym tree for ?Congress? is 
?group?).  In order to avoid noise due to 
inappropriate word senses, we only used data 
from senses that matched the part of speech.  
These eight WordNet features add to the above 
features for a total of 96 features. 
 
4. Methods 
 
4.1 Experiment 1: Held out data 
 
 
To examine the generalizability of classifiers 
trained on the automatically generated data, a C4.5 
decision tree classifier (Quinlan, 1993) was trained 
and tested on the held out test set described above.   
Initial results revealed that, due to differing 
contexts, instances of the same name in a single 
text would often be classified into different 
subcategories.  To deal with this problem, we 
augmented the classifier with another program, 
MemRun, which standardizes the subcategorization 
of instances based on their most frequent 
classification.  Developed and tested in 
(Fleischman, 2001), MemRun is based upon the 
hypothesis that by looking at all the classifications 
an instance has received throughout the test set, an 
?average? sub-categorization can be computed that 
offers a better guess than a low confidence 
individual classification. 
MemRun operates in two rounds.  In the first 
round, each instance of the test set is evaluated 
using the decision tree, and a classification 
hypothesis is generated.  If the confidence level of 
this hypothesis is above a certain threshold 
(THRESH 1), then the hypothesis is entered into 
the temporary database (see Figure 3) along with 
the degree of confidence of that hypothesis, and the 
number of times that hypothesis has been received. 
Because subsequent occurrences of person 
instances frequently differ orthographically from 
their initial occurrence (e.g., ?George Bush? 
followed by ?Bush?) a simple algorithm was 
devised for surface reference disambiguation.  The 
algorithm keeps a record of initial full name usages 
of all person instances in a text.  When partial 
references to the instance are later encountered in 
the text, as determined by simple regular 
expression matching, they are entered into the 
MemRun database as further occurrences of the 
original instance.  This record of full name 
references is cleared after a text is examined to 
avoid possible instance confusions (e.g., ?George 
W. Bush? and ?George Bush Sr.?).  This simple 
algorithm operates on the assumption that partial 
references to individuals with the same last name in 
the same text will not occur due to human authors? 
desire to avoid any possible confusion.2  When all 
of the instances in the data set are examined, the 
round is complete.  
In MemRun?s second round, the data set is 
reexamined, and hypothesis classifications are 
again produced.  If the confidence of one of these 
hypotheses is below a second threshold (THRESH 
2), then the hypothesis is ignored and the database 
value is used.3  In this experiment, the entries in the 
database are compared and the most frequent entry 
(i.e., the max classification based on confidence 
level multiplied by the increment) is returned.  
When all instances have been again examined, the 
round is complete.   
 
Figure 3. MemRun database for Decision Tree classifier 
 
4.2 Experiment 2: Learning Algorithms 
 
Having examined the generalizability when 
using automatically generated training data, we turn 
to the question of appropriate learning algorithms 
for the task.  We chose to examine five different 
learning algorithms.  Along with C4.5, we 
examined a feed-forward neural network with 50 
hidden units, a k-Nearest Neighbors 
implementation (k=1) (Witten & Frank, 1999), a 
Support Vector Machine implementation using a 
linear kernel (Witten & Frank, 1999), and a na?ve 
Bayes classifier using discretized attributes and 
                                                 
2 This algorithm does not address definite descriptions and 
pronominal references because they are not classified by 
IdentiFinder as people names, and thus are not marked for 
fine-grained classification in the test set. 
3 The ability of the algorithm to ignore the database?s 
suggestion in the second round allows instances with the 
same name (e.g., ?Paul Simon?) to receive different 
classifications in different contexts. 
Instance Class Confidence Occur 
George Bush Politician 97.5% 4 
 Business 83.4% 1 
Dana Carvey Entertainer 92.4% 7 
 Politician 72.1% 2 
 with feature subset selection (Kohavi & 
Sommerfield, 1996).  For each classifier, 
comparisons were based on results from the 
validation set (~1000 instances) described above. 
 
4.3 Experiment 3: Feature sets 
 
To examine the effectiveness of the individual 
types of features, a C4.5 decision tree classifier 
(Quinlan, 1993) was trained on the 25,000 instance 
data set described above using all possible 
combinations of the three feature sets.  The 
performance was ascertained on the validation set 
described above. 
 
5. Results 
 
5.1 Experiment 1: Held out data 
 
47.3
70.4
83.5
57
60.9
70.1
40
50
60
70
80
% 
Co
rre
ct
Validation Held out
Baseline No MemRun MemRun
 
Figure 4.  Results of classifier on validation set and held 
out data.  Results compare baseline of always choosing 
most probable class with C4.5 classifier both with and 
without MemRun.   
 
The results of the classifier on both the 
validation set and the held out test set can be seen 
in Figure 4.  The results are presented for a 
classifier trained using the C4.5 algorithm both 
with and without MemRun (THRESH1=85, 
THRESH2=98).  Also shown is the baseline score 
for each test set computed by always choosing the 
most frequent classification (Politician for both).   
It is clear from the figure that the classifiers for 
both test sets and for both conditions performed 
better than baseline.  Also clear is that the MemRun 
algorithm significantly improves performance on 
both the validation and held out test sets.   
Figure 4 further shows a large discrepancy 
between the performance of the classifier on the 
two data sets.  Expectedly, the validation set is 
classified more easily both with and without 
MemRun.  The size of the discrepancy is a function 
of how different the distribution of the training set 
is from the true distribution of person instances in 
the world.  While this discrepancy is undeniable, it 
is interesting to note how well the classifier 
generalizes given the very biased sample upon 
which it was trained. 
 
5.2 Experiment 2: Learning Algorithms 
 
47.3
57.7
64
68.1 69.5
70.4
40
45
50
55
60
65
70
75
%
 C
or
re
ct
Baseline k-NN Na?ve
Bayes
SVM Neural Net C4.5
 Figure 5. Comparison of different learning algorithms on 
a validation set.  Learners include: k-Nearest Neighbors, 
Na?ve Bayes, support vector machine, neural network, and 
C4.5 decision tree. 
 
Figure 5 shows the results of comparing 
different machine learning strategies.  It is clear 
from the figure that all the algorithms perform 
better than the baseline score, while the C4.5 
algorithm performs the best.  This is not 
surprising as decision trees combine powerful 
aspects of non-linear separation and feature 
selection.   
Interestingly, however, there is no clear 
relationship between performance and the 
theoretical foundations of the classifier.    
Although the two top performers (decision tree 
and the neural network) are both non-linear 
classifiers, the linear SVM outperforms the non-
linear k-Nearest Neighbors.  This must, 
however, be taken with a grain of salt, as little 
was done to optimize either the k-NN or SVM 
implementation. 
Another interesting finding in recent work 
is an apparent relationship between classifier 
type and performance on held out data.  While 
the non-parametric learners, i.e. C4.5 and k-NN, 
are fairly robust to generalization, the 
parametric learners, i.e. Na?ve Bayes and SVM, 
perform significantly worse on the new 
distribution.  In future work, we intend to 
examine further this possible relationship. 
 
 5.3 Experiment 3: Feature sets 
 
The results of the feature set experiment can 
be seen in figure 6.  Results are shown for the 
validation set using all combinations of the 
three feature sets.  A baseline measure of 
always classifying the most frequent category 
(Politician) is also displayed.  
It is clear that each of the single feature sets 
(frequency features, topic signature features, 
and WordNet features) is sufficient to 
outperform the baseline.  Interestingly, topic 
signature features outperform WordNet 
features, even though they are similar in form.  
This suggests that the WordNet features are 
noisy and may contain too much generality.  It 
may be more appropriate to use a cutoff, such 
that only the concepts two levels above the term 
are examined.  Another source of noise comes 
from words with multiple senses.  Although our 
method uses only word senses of the 
appropriate part of speech, WordNet still often 
provides many different possible senses.  
 
47.3
59.4
63.6
67.1
60.7
68 68.1
70.4
40
45
50
55
60
65
70
75
%
 C
or
re
ct
Baseline Freq WN Sig Freq & WN Freq & Sig Sig & WN All
 Figure 6. Results of using different combinations of 
feature sets.  Results shown on validation set using C4.5 
classifier without MemRun. 
 
Also of interest is the effect of combining 
any two feature sets.  While using topic 
signatures and either word frequencies or 
WordNet features improves performance by a 
small amount, combining frequency and 
WordNet scores results in performance worse 
than WordNet alne.  This suggests over fitting 
of the training data and may be due to the noise 
in the WordNet features. 
It is clear, however, that the combination of 
all three features provides considerable 
improvement in performance over any of the 
individual features.  In future work we will 
examine how ensemble learning (Hastie, 2001) 
might be used to capitalize further on these 
qualitatively different feature sets. 
 
6. Related Work 
 
While much research has gone into the coarse 
categorization of named entities, we are not aware 
of much previous work using learning algorithms 
to perform more fine-grained classification.   
Wacholder et al (1997) use hand-written rules 
and knowledge bases to classify proper names into 
broad categories.  They employ an aggregation 
method similar to MemRun, but do not use 
multiple thresholds to increase accuracy. 
MacDonald (1993) also uses hand-written rules 
for coarse named entity categorization.  However, 
where Wacholder et al use evidence internal to the 
entity name, MacDonald employs local context to 
aid in classification.  Such hand-written heuristic 
rules resemble those we automatically generate. 
Bechet et al (2000) use a decision tree 
algorithm to classify unknown proper names into 
the categories: first name, last name, country, town, 
and organization.  This is still a much coarser 
distinction than that focused on in this research.  
Further, Bechet et al focused only on those proper 
names embedded in complex noun phrases (NPs), 
using only elements in the NP as its feature set.   
 
7. Conclusions 
 
The results of these experiments, though 
preliminary, are very promising.  Our research 
makes clear that positive results are possible 
with relatively simple statistical techniques.  
This research has shown that training data 
construction is critical.  The failure of our 
automatic data generation algorithm to produce 
a good sample of training data is evident in the 
large disparity between performances on 
validation and held out test sets.  There are at 
least two reasons for the algorithm?s poor 
sampling. 
First, by using only high confidence guesses 
from the seed trained classifier, the training data 
may have a disproportionate number of 
instances that are easy to classify.  This is 
evident in the number of partial names that are 
present in the held out test set versus the 
training set.  Partial names, such as ?Simon? 
instead of ?Paul Simon,? usually occur with 
weaker evidence for classification than full 
 names.  In the training set only 45.1% of the 
instances are partial names, whereas in the more 
realistic distribution of the held out set, 58.4% 
are partial names. 
The second reason for the poor sampling 
stems from the use of lists of person names.  
Because the training set is derived from 
individuals in these lists, the coverage of 
individuals included in the training set is 
inherently limited.  For example, in the 
businessperson category, lists of individuals 
were taken from such resources as Forbes? 
annual ranking of the nation?s wealthiest 
people, under the assumption that wealthy 
people are often in the news.  However, the list 
fails to mention the countless vice presidents 
and analysts that frequent the pages of the Wall 
Street Journal.  This failure to include such 
lower level businesspersons means that a large 
space of the classification domain is not 
covered by the training set, which in turn leads 
to poor results on the held out test set. 
The results of these experiments suggest 
that better fine-grained classification of named 
entities will require not only more sophisticated 
feature selection, but also a better data 
generation procedure.  In future work, we will 
investigate more sophisticated bootstrapping 
methods, as (Collins & Singer, 1999) as well as 
co-training and co-testing (Muslea et al, 2000).   
In future work we will also examine 
adapting the hierarchical decision list algorithm 
from (Yarowsky, 2000) to our task.  Treating 
fine-grained classification of named entities as a 
word sense disambiguation problem (where 
categories are treated as different senses of a 
generic ?person name?) allows these methods to 
be directly applicable.  The algorithm is 
particularly relevant in that it provides an 
intuitive way to take advantage of the 
similarities of certain categories (e.g., Athlete 
and Entertainer). 
Of more theoretical concern are the 
problems of miscellaneous classifications that 
do not fit easily into any category, as well as, 
instances that may fit into more than one 
category (e.g., Ronald Reagan can be either a 
Politician or an Entertainer).  We plan to 
address these issues as well as problems that 
may arise with extending this system for use 
with other classes, such as organizations.   
 
8. References 
 
Bechet, F., Nasr, A., Genet, F.  2000.  Tagging unknown proper 
names using decision trees.  Proc. of ACL, Hong Kong. 
 
Bikel, D., Schwartz, R., Weischedel, R.  1999.  An algorithm 
that learns what?s in a name.  Machine Learning: Special 
Issue on NL Learning, 34, 1-3. 
 
Brill E. 1994. Some advances in rule based part of speech 
tagging.  Proc. of AAAI, Los Angeles. 
 
Collins, M. and Singer, Y. 1999. Unsupervised models for 
named entity classification. Proc. of the Joint SIGDAT 
Conference on Empirical Methods in Natural Language 
Processing and Very Large Corpora.  
 
Dunning, T., 1993. Accurate methods for statistics of surprise 
and coincidence. Computational Linguistics, 19:61--74. 
 
Fellbaum, C. (ed.), 1998. An electronic lexical database. 
Cambridge, MA: MIT Press.  
 
Fleischman, M. 2001.  Automated Subcategorization of Named 
Entities.  Proc. of the ACL Student Workshop.  
 
Hastie, T., Tibshirani, R., and J. Friedman. 2001.  The 
Elements of Statistical Learning. Springer. 
 
Jurafsky, D. and Martin, J..  2000.  Speech and Language 
Processing.  Upper Saddle River, NJ: Prentice Hall. 
 
Kohavi,, R., Sommerfield, D., and Dougherty, J, 1996.  
Data mining using MLC++ : A machine learning library 
in C ++. Tools with Artificial Intelligence, pp. 234-245. 
 
Lin, C.-Y. and E.H. Hovy, 2000. The Automated 
Acquisition of Topic Signatures for Text 
Summarization. Proc. of the COLING Conference. 
Strasbourg, France.  
 
MacDonald D.D., 1993.  Internal and external evidence in the 
identification and semantic categorization of proper names. 
In B.Boguraev and J. Pustejovsky, eds., Corpus Processing 
for Lexical Acquisition, pp. 61-76, Cambridge: MIT Press. 
 
Muslea, I., Minton, S., Knoblock, C.  Selective sampling 
with redundant views.  Proc. of the 15th National 
Conference on Artificial Intelligence, AAAI-2000.  
 
Porter, M. F. 1980.  An algorithm for suffix stripping. 
Program, 14 (no. 3), 130-137. 
 
Quinlin, J.R., 1993.  C4.5: Programs for Machine Learning.  
San Mateo, CA: Morgan Kaufmann Publishers. 
 
Resnik, P. 1993. Selection and Information. PhD thesis, 
University of Pennsylvania. 
 
Wacholder, N., Ravin, Y., Choi, M. 1997.  Disambiguation of 
Proper Names in Text.  Proc. of the Fifth Conference on 
Applied Natural Language Processing, Washington, D.C. 
Witten, I. & Frank, E. 1999.  Data Mining: Practical 
Machine Learning Tools and Techniques with JAVA 
implementations.  Morgan Kaufmann, October. 
Yarowsky D. 2000. Hierarchical Decision List for Word 
Sense Disambiguation. Computers and the Humanities. 
34: 179-186. 
Yang, Y., Pedersen, J.O., 1997.  A Comparative Study on 
Feature Selection in Text Categorization, Proc. of the 
14th International Conference on Machine Learning 
ICML97, pp. 412-420 
 
A Maximum Entropy Approach to FrameNet Tagging 
 
Michael Fleischman and Eduard Hovy 
USC Information Science Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{fleisch, hovy }@ISI.edu 
  
Abstract 
The development of FrameNet, a large 
database of semantically annotated sentences, 
has primed research into statistical methods 
for semantic tagging.  We advance previous 
work by adopting a Maximum Entropy 
approach and by using Viterbi search to find 
the highest probability tag sequence for a 
given sentence.  Further we examine the use 
of syntactic pattern based re-ranking to further 
increase performance.  We analyze our 
strategy using both extracted and human 
generated syntactic features.  Experiments 
indicate 85.7% accuracy using human 
annotations on a held out test set. 
1 Introduction 
The ability to develop automatic methods for semantic 
classification has been hampered by the lack of large 
semantically annotated corpora.  Recent work in the 
development of FrameNet, a large database of 
semantically annotated sentences, has laid the 
foundation for the use of statistical approaches to 
automatic semantic classification.   
The FrameNet project seeks to annotate a large 
subset of the British National Corpus with semantic 
information.  Annotations are based on Frame 
Semantics (Fillmore, 1976), in which frames are defined 
as schematic representations of situations involving 
various Frame Elements such as participants, props, and 
other conceptual roles.   
In each FrameNet sentence, a single target 
predicate is identified and all of its relevant Frame 
Elements are tagged with their element-type (e.g., 
Agent, Judge), their syntactic Phrase Type (e.g., NP, 
PP), and their Grammatical Function (e.g., External 
Argument, Object Argument).  Figure 1 shows an 
example of an annotated sentence and its appropriate 
semantic frame. 
 
To our knowledge, Gildea and Jurafsky (2000) is 
the only work that uses FrameNet to build a statistical 
semantic classifier.  They split the problem into two 
distinct sub-tasks: Frame Element identification and 
Frame Element classification.  In the identification 
phase, they use syntactic information extracted from a 
parse tree to learn the boundaries of Frame Elements in 
sentences.  The work presented here, focuses only on 
the second phase: classification. 
Gildea and Jurafsky (2000) describe a system that 
uses completely syntactic features to classify the Frame 
Elements in a sentence.  They extract features from a 
parse tree and model the conditional probability of a 
semantic role given those features.  They report an 
accuracy of 76.9% on a held out test set. 
 
 
 
 
 
 
She  clapped  her hands  in inspiration. 
Frame:        Body-Movement 
Frame Elements:   
Agent     Body Part Cause 
     -NP             -NP   -PP 
     -Ext.            -Obj.   -Comp. 
 
Figure 1.  Frame for lemma ?clap? shown with three core Frame 
Elements and a sentence annotated with element type, phrase 
type, and grammatical function. 
 
We extend Gildea and Jurafsky (2000)?s initial 
effort in three ways.  First, we adopt a Maximum 
Entropy (ME) framework to better learn the feature 
weights associated with the classification model.  
Second, we recast the classification task as a tagging 
problem in which an n-gram model of Frame Elements 
is applied to find the most probable tag sequence (as 
opposed to the most probable individual tags).  Finally, 
we implement a re-ranking system that takes advantage 
of the sentence-level syntactic patterns of each 
sequence.  We analyze our results using syntactic 
features extracted from a parse tree generated by Collins 
parser (Collins, 1997) and compare those to models 
built using features extracted from FrameNet?s human 
annotations.   
2 Method 
2.1 
2.2 
                                                          
Training (32,251 sentences), development (3,491 
sentences), and held out test sets (3,398 sentences) were 
generated from the June 2002 FrameNet release 
following the divisions used in Gildea and Jurafsky 
(2000) 1 .  Because human-annotated syntactic 
information could only be obtained for a subset of their 
data, the training, development, and test sets used here 
are approximately 10% smaller than those used in 
Gildea and Jurafsky (2000).2  There are on average 2.2 
Frame Elements per sentence, falling into one of 126 
unique classes.   
Maximum Entropy 
ME models implement the intuition that the best model 
will be the one that is consistent with all the evidence, 
but otherwise, is as uniform as possible.  (Berger et al, 
1996).  Following recent successes using it for many 
NLP tasks (Och and Ney, 2002; Koeling, 2000), we use 
ME to implement a Frame Element classifier. 
We use the YASMET ME package (Och, 
2002) to train an approximation of the model below: 
 
P(r| pt, voice, position, target, gf, h) 
 
Here r indicates the element type, pt the phrase type, gf 
the grammatical function, h the head word, and target 
the target predicate.  Due to data sparsity issues, we do 
not calculate this model directly, but rather, model 
various feature combinations as described in Gildea and 
Jurafsky (2000).   
The classifier was trained, using only features that 
had a frequency in training of one or more, and until 
performance on the development set ceased to improve.  
Feature weights were smoothed using a Bayesian 
method, such that weight limits are Gaussian distributed 
with mean 0 and standard deviation 1.  
Tagging 
Frame Elements do not occur in isolation, but rather, 
depend very much on what other Elements occur in a 
sentence.  For example, if a Frame Element is tagged as 
an Agent it is highly unlikely that the next Element will 
also be an Agent.  We exploit this dependency by 
treating the Frame Element classification task as a 
tagging problem. 
The YASMET MEtagger was used to apply an n-
gram tag model to the classification task (Bender et al, 
2003).  The feature set for the training data was 
2.3 
3 Results 
                                                          
1 Divisions given by Dan Gildea via personal communication. 
2  Gildea and Jurafsky (2000) use 36995 training, 4000 
development, and 3865 test sentences.  They do not report 
results using hand annotated syntactic information. 
augmented to include information about the tags of the 
previous one and two Frame Elements in the sentence: 
 
P(r| pt, voice, position, target, gf, h, r -1,r -1+r -2) 
 
Viterbi search was then used to find the most probable 
tag sequence through all possible sequences. 
Pattern Features 
A great deal of information useful for classification can 
be found in the syntactic patterns associated with each 
sequence of Frame Elements.  A typical syntactic 
pattern is exhibited by the sentence ?Alexandra bent her 
head.?  Here ?Alexandra? is an external argument Noun 
Phrase, ?bent? is the target, and ?her head? is an object 
argument Noun Phrase.  In the training data, a syntactic 
pattern of NP-ext, target, NP-obj, given the predicate 
bend, was associated 100% of the time with the Frame 
Element pattern: ?Agent target BodyPart?, thus, 
providing powerful evidence as to the classification of 
those Frame Elements.   
We exploit these sentence-level patterns by 
implementing a re-ranking system that chooses among 
the n-best tagger outputs.  The re-ranker was trained on 
a development corpus, which was first tagged using the 
MEtagger described above.  For each sentence in the 
development corpus, the 10 best tag sequences are 
output by the classifier and described by three 
probabilities: 3  1) the sequence?s probability given by 
the ME classifier (ME); 2) the conditional probability of 
that sequence given the syntactic pattern and the target 
predicate (pat+target); 3) a back off conditional 
probability of the tag sequence given just the syntactic 
pattern (pat).  A ME model is then used to combine the 
log of these probabilities to give a model of the form: 
 
P(tag-seq| ME, pat+target, pat) 
 
Figure 2 shows the performance of the base ME model, 
the base model within a tagging framework, and the 
base model within a tagging framework plus the re-
ranker.  Results are shown for data sets trained and 
tested using human annotated syntactic features and 
trained and tested using automatically extracted 
syntactic features.  In both cases the training and test 
sets are identical.   
For both the extracted and human conditions, 
adopting a tagging framework improves results by over 
1%.  However, while the syntactic pattern based re-
ranker increases performance using human annotations 
by nearly 2%, the effect when using automatically 
extracted information is only 0.5%.  This is reasonable 
3  Using n-best lists of 50 and 100 showed no significant 
difference in performance. 
considering that the re-ranker?s effectiveness is 
correlated with the level of noise in the syntactic 
patterns upon which it is based. 
The difference in performance between the models 
under both human and extracted conditions was 
relatively consistent: averaging 8.7% with a standard 
deviation of 0.7. 
As a further analysis, we have examined the 
performance of our base ME model on the same test set 
as that used in Gildea and Jurafsky (2000).  Using only 
extracted information, we achieve an accuracy of 
74.9%, two percent lower than their reported results.  
This result is not unreasonable, however, because, due 
to limited time, very little effort was spent tuning the 
parameters of the model.  
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  Performance of models on held out test data.  ME refers 
to results of the base Maximum Entropy model, Tagger to a 
combined ME and Viterbi search model, Re-Rank to the Tagger 
augmented with a re-ranker.  Extracted refers to models trained 
using features extracted from parse trees, Human to models using 
features from FrameNet?s human annotations. 
4 Conclusion 
It is clear that using a tagging framework and syntactic 
patterns improves performance of the semantic classifier 
when features are extracted from either automatically 
generated parse trees or human annotations.  The most 
striking result of these experiments, however, is the 
dramatic decrease in performance associated with using 
features extracted from a parse tree.   
This decrease in performance can be traced to at 
least two aspects of the automatic extraction process: 
noisy parser output and limited grammatical 
information.   
To compensate for noisy parser output, our current 
work is focusing on two strategies.  First, we are 
looking at using shallower but more reliable methods 
for syntactic feature generation, such as part of speech 
tagging and text chunking, to either replace or augment 
the syntactic parser.  Second, we are using ontological 
information, such as word classes and synonyms, in the 
hopes that semantic information may supplement the 
noisy syntactic information. 
The models trained on features extracted from parse 
trees do not have access to rich grammatical 
information.  Following Gildea and Jurafsky (2000), 
automatic extraction of grammatical information here is 
limited to the governing category of a Noun Phrase.  
The FrameNet annotations, however, are much richer 
and include information about complements, modifiers, 
etc.  We are looking at ways to include such information 
either by using alternative parsers (Hermjakob, 1997) or 
as a post processing task (Blaheta and Charniak, 2000).  
In future work, we will extend the strategies 
outlined here to incorporate Frame Element 
identification into our model.  By treating semantic 
classification as a single tagging problem, we hope to 
create a unified, practical, and high performance system 
for Frame Element tagging. 
76.375.8
74
85.7
83.8
82.6
68
70
72
74
76
78
80
82
84
86
88
ME Tagger Re-Rank
% 
Co
rre
ct
Extracted Human
Acknowledgments 
The authors would like to thank Dan Gildea who 
generously allowed us access to his data files and Oliver 
Bender for making the MEtagger software publicly 
available.  Finally, we thank Franz Och whose help and 
expertise were invaluable. 
References 
O. Bender, K. Macherey, F. J. Och, and H. Ney. 2003. 
Comparison of Alignment Templates and Maximum 
Entropy Models for Natural Language Processing. EACL-
2003.  Budapest, Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 1996. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, vol. 22, no. 1. 
D. Blaheta and E. Charniak. 2000. Assigning Function Tags to 
Parsed Text, In Proc. of the 1st NAACL, Seattle, WA. 
M. Collins. 1997. Three generative, lexicalized models for 
statistical parsing.  In Proc. of the 35th Annual Meeting of 
the ACL. 
C. Fillmore 1976.  Frame semantics and the nature of 
language. In Annals of the New York Academy of Sciences: 
Conference on the Origin and Development of Language 
and Speech, Volume 280 (pp. 20-32).  
D. Gildea and D. Jurafsky. 2000.  Automatic Labeling of 
Semantic Roles, ACL-2000, Hong Kong.  
U. Hermjakob, 1997. Learning Parse and Translation 
Decisions from Examples with Rich Context. Ph.D. 
Dissertation, University of Texas at Austin, Austin, TX. 
R. Koeling. 2000. Chunking with maximum entropy models. 
CoNLL-2000. Lisbon, Portugal. 
F.J. Och, H. Ney. 2002. Discriminative Training and 
Maximum Entropy Models for Statistical Machine 
Translation. ACL-2002. Philadelphia, PA. 
F.J. Och. 2002. Yet another maxent toolkit: YASMET. www-
i6.informatik.rwth-aachen.de/Colleagues/och/. 
Offline Strategies for Online Question Answering: 
Answering Questions Before They Are Asked 
 
Michael Fleischman, Eduard Hovy,  
Abdessamad Echihabi 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{fleisch, hovy, echihabi} @ISI.edu 
  
Abstract 
Recent work in Question Answering has 
focused on web-based systems that 
extract answers using simple lexico-
syntactic patterns.  We present an 
alternative strategy in which patterns are 
used to extract highly precise relational 
information offline, creating a data 
repository that is used to efficiently 
answer questions.  We evaluate our 
strategy on a challenging subset of 
questions, i.e. ?Who is ?? questions, 
against a state of the art web-based 
Question Answering system.  Results 
indicate that the extracted relations 
answer 25% more questions correctly and 
do so three orders of magnitude faster 
than the state of the art system. 
1 Introduction 
Many of the recent advances in Question 
Answering have followed from the insight that 
systems can benefit by exploiting the redundancy 
of information in large corpora.  Brill et al (2001) 
describe using the vast amount of data available on 
the World Wide Web to achieve impressive 
performance with relatively simple techniques.  
While the Web is a powerful resource, its 
usefulness in Question Answering is not without 
limits.   
The Web, while nearly infinite in content, is 
not a complete repository of useful information.  
Most newspaper texts, for example, do not remain 
accessible on the Web for more than a few weeks.  
Further, while Information Retrieval techniques are 
relatively successful at managing the vast quantity 
of text available on the Web, the exactness 
required of Question Answering systems makes 
them too slow and impractical for ordinary users. 
In order to combat these inadequacies, we 
propose a strategy in which information is 
extracted automatically from electronic texts 
offline, and stored for quick and easy access.  We 
borrow techniques from Text Mining in order to 
extract semantic relations (e.g., concept-instance 
relations) between lexical items.  We enhance 
these techniques by increasing the yield and 
precision of the relations that we extract.   
Our strategy is to collect a large sample of 
newspaper text (15GB) and use multiple part of 
speech patterns to extract the semantic relations.  
We then filter out the noise from these extracted 
relations using a machine-learned classifier.  This 
process generates a high precision repository of 
information that can be accessed quickly and 
easily. 
We test the feasibility of this strategy on one 
semantic relation and a challenging subset of 
questions, i.e., ?Who is ?? questions, in which 
either a concept is presented and an instance is 
requested (e.g., ?Who is the mayor of Boston??), 
or an instance is presented and a concept is 
requested (e.g., ?Who is Jennifer Capriati??).  By 
choosing this subset of questions we are able to 
focus only on answers given by concept-instance 
relationships.  While this paper examines only this 
type of relation, the techniques we propose are 
easily extensible to other question types. 
Evaluations are conducted using a set of ?Who 
is ?? questions collected over the period of a few 
months from the commercial question-based 
search engine www.askJeeves.com.  We extract 
approximately 2,000,000 concept-instance 
relations from newspaper text using syntactic 
patterns and machine-learned filters (e.g., 
?president Bill Clinton? and ?Bill Clinton, 
president of the USA,?).  We then compare 
answers based on these relations to answers given 
by TextMap (Hermjakob et al, 2002), a state of the 
art web-based question answering system.  Finally, 
we discuss the results of this evaluation and the 
implications and limitations of our strategy. 
3.1 
2 
3 
3.2 
Related Work 
A great deal of work has examined the problem of 
extracting semantic relations from unstructured 
text.  Hearst (1992) examined extracting hyponym 
data by taking advantage of lexical patterns in text.  
Using patterns involving the phrase ?such as?, she 
reports finding only 46 relations in 20M of New 
York Times text.  Berland and Charniak (1999) 
extract ?part-of? relations between lexical items in 
text, achieving only 55% accuracy with their 
method.  Finally, Mann (2002) describes a method 
for extracting instances from text that takes 
advantage of part of speech patterns involving 
proper nouns.  Mann reports extracting 200,000 
concept-instance pairs from 1GB of Associated 
Press text, only 60% of which were found to be 
legitimate descriptions.   
These studies indicate two distinct problems 
associated with using patterns to extract semantic 
information from text.  First, the patterns yield 
only a small amount of the information that may be 
present in a text (the Recall problem).  Second, 
only a small fraction of the information that the 
patterns yield is reliable (the Precision problem).   
Relation Extraction 
Our approach follows closely from Mann (2002).  
However, we extend this work by directly 
addressing the two problems stated above.  In 
order to address the Recall problem, we extend the 
list of patterns used for extraction to take 
advantage of appositions.  Further, following 
Banko and Brill (2001), we increase our yield by 
increasing the amount of data used by an order of 
magnitude over previously published work.  
Finally, in order to address the Precision problem, 
we use machine learning techniques to filter the 
output of the part of speech patterns, thus purifying 
the extracted instances. 
Data Collection and Preprocessing 
Approximately 15GB of newspaper text was 
collected from: the TREC 9 corpus (~3.5GB), the 
TREC 2002 corpus (~3.5GB), Yahoo! News 
(.5GB), the AP newswire (~2GB), the Los Angeles 
Times (~.5GB), the New York Times (~2GB), 
Reuters (~.8GB), the Wall Street Journal 
(~1.2GB), and various online news websites 
(~.7GB).  The text was cleaned of HTML (when 
necessary), word and sentence segmented, and part 
of speech tagged using Brill?s tagger (Brill, 1994). 
Extraction Patterns 
Part of speech patterns were generated to take 
advantage of two syntactic constructions that often 
indicate concept-instance relationships: common 
noun/proper noun constructions (CN/PN) and 
appositions (APOS).  Mann (2002) notes that 
concept-instance relationships are often expressed 
by a syntactic pattern in which a proper noun 
follows immediately after a common noun.  Such 
patterns (e.g. ?president George Bush?) are very 
productive and occur 40 times more often than 
patterns employed by Hearst (1992).  Table 1 
shows the regular expression used to extract such 
patterns along with examples of extracted patterns. 
 
${NNP}*${VBG}*${JJ}*${NN}+${NNP}+ 
trainer/NN Victor/NNP Valle/NNP  
ABC/NN spokesman/NN Tom/NNP Mackin/NNP 
official/NN Radio/NNP Vilnius/NNP  
German/NNP expert/NN Rriedhart/NNP 
Dumez/NN Investment/NNP 
Table 1.  The regular expression used to extract CN/PN 
patterns (common noun followed by proper noun).  
Examples of extracted text are presented below.  Text in 
bold indicates that the example is judged illegitimate.  
 
${NNP}+\s*,\/,\s*${DT}*${JJ}*${NN}+(?:of\/IN)* 
          \s*${NNP}*${NN}*${IN}*${DT}*${NNP}* 
          ${NN}*${IN}*${NN}*${NNP}*,\/,  
Stevens/NNP  ,/, president/NN of/IN the/DT firm/NN  ,/, 
Elliott/NNP Hirst/NNP  ,/, md/NN of/IN Oldham/NNP Signs/NNP  ,/, 
George/NNP McPeck/NNP,/, an/DT engineer/NN from/IN Peru/NN,/, 
Marc/NNP Jonson/NNP,/, police/NN chief/NN of/IN Chamblee/NN ,/, 
David/NNP Werner/NNP ,/, a/DT real/JJ estate/NN investor/NN ,/, 
Table 2.  The regular expression used to extract APOS 
patterns (syntactic appositions).  Examples of extracted 
text are presented below.  Text in bold indicates that the 
example is judged illegitimate.  
In addition to the CN/PN pattern of Mann 
(2002), we extracted syntactic appositions (APOS).  
This pattern detects phrases such as ?Bill Gates, 
chairman of Microsoft,?.  Table 2 shows the 
regular expression used to extract appositions and 
examples of extracted patterns.  These regular 
expressions are not meant to be exhaustive of all 
possible varieties of patterns construed as CN/PN 
or APOS.  They are ?quick and dirty? 
implementations meant to extract a large 
proportion of the patterns in a text, acknowledging 
that some bad examples may leak through. 
3.3 Filtering 
The concept-instance pairs extracted using the 
above patterns are very noisy.  In samples of 
approximately 5000 pairs, 79% of the APOS 
extracted relations were legitimate, and only 45% 
of the CN/PN extracted relations were legitimate.  
This noise is primarily due to overgeneralization of 
the patterns (e.g., ?Berlin Wall, the end of the Cold 
War,?) and to errors in the part of speech tagger 
(e.g., ?Winnebago/CN Industries/PN?).  Further, 
some extracted relations were considered either 
incomplete (e.g., ?political commentator Mr. 
Bruce?) or too general (e.g., ?meeting site Bourbon 
Street?) to be useful.  For the purposes of learning 
a filter, these patterns were treated as illegitimate. 
In order to filter out these noisy concept-
instance pairs, 5000 outputs from each pattern 
were hand tagged as either legitimate or 
illegitimate, and used to train a binary classifier.  
The annotated examples were split into a training 
set (4000 examples), a validation set (500 
examples); and a held out test set (500 examples).  
The WEKA machine learning package (Witten and 
Frank, 1999) was used to test the performance of 
various learning and meta-learning algorithms, 
including Na?ve Bayes, Decision Tree, Decision 
List, Support Vector Machines, Boosting, and 
Bagging.   
Table 4 shows the list of features used to 
describe each concept-instance pair for training the 
CN/PN filter.  Features are split between those that 
deal with the entire pattern, only the concept, only 
the instance, and the pattern?s overall orthography.  
The most powerful of these features examines an 
Ontology in order to exploit semantic information 
about the concept?s head.  This semantic 
information is found by examining the super-
concept relations of the concept head in the 
110,000 node Omega Ontology (Hovy et al, in 
prep.).   
 
 
Feature 
Type  
Pattern  
Features 
Binary ${JJ}+${NN}+${NNP}+ 
Binary ${NNP}+${JJ}+${NN}+${NNP}+ 
Binary ${NNP}+${NN}+${NNP}+ 
Binary ${NNP}+${VBG}+${JJ}+${NN}+${NNP}+ 
Binary ${NNP}+${VBG}+${NN}+${NNP}+ 
Binary ${NN}+${NNP}+ 
Binary ${VBG}+${JJ}+${NN}+${NNP}+ 
Binary ${VBG}+${NN}+${NNP}+ 
  Concept Features 
Binary Concept head ends in "er" 
Binary Concept head ends in "or" 
Binary Concept head ends in "ess" 
Binary Concept head ends in "ist" 
Binary Concept head ends in "man" 
Binary Concept head ends in "person" 
Binary Concept head ends in "ant" 
Binary Concept head ends in "ial" 
Binary Concept head ends in "ate" 
Binary Concept head ends in "ary" 
Binary Concept head ends in "iot" 
Binary Concept head ends in "ing" 
Binary Concept head is-a occupation 
Binary Concept head is-a person 
Binary Concept head is-a organization 
Binary Concept head is-a company 
Binary Concept includes digits 
Binary Concept has non-word 
Binary Concept head in general list 
Integer Frequency of concept head in CN/PN 
Integer Frequency of concept head in APOS 
  Instance Features 
Integer Number of lexical items in instance 
Binary Instance contains honorific 
Binary Instance contains common name 
Binary Instance ends in honorific 
Binary Instance ends in common name 
Binary Instance ends in determiner 
  Case Features 
Integer Instance: # of lexical items all Caps 
Integer Instance: # of lexical items start w/ Caps 
Binary Instance: All lexical items start w/ Caps 
Binary Instance: All lexical items all Caps 
Integer Concept: # of lexical items all Caps 
Integer Concept: # of lexical items start w/ Caps 
Binary Concept: All lexical items start w/ Caps 
Binary Concept: All lexical items all Caps 
Integer Total # of lexical items all Caps 
Integer Total # of lexical items start w/ Caps 
Table 4.  Features used to train CN/PN pattern filter.  
Pattern features address aspects of the entire pattern, 
Concept features look only at the concept, Instance 
features examine elements of the instance, and Case 
features deal only with the orthography of the lexical 
items. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  Performance of machine learning algorithms 
on a validation set of 500 examples extracted using the 
CN/PN pattern.  Algorithms are compared to a baseline 
in which only concepts that inherit from ?Human? or 
?Occupation? in Omega pass through the filter.   
4 
4.1 
                                                          
Extraction Results 
Machine Learning Results 
Figure 1 shows the performance of different 
machine learning algorithms, trained on 4000 
extracted CN/PN concept-instance pairs, and tested 
on a validation set of 500.  Na?ve Bayes, Support 
Vector Machine, Decision List and Decision Tree 
algorithms were all evaluated and the Decision 
Tree algorithm (which scored highest of all the 
algorithms) was further tested with Boosting and 
Bagging meta-learning techniques.  The algorithms 
are compared to a baseline filter that accepts 
concept-instance pairs if and only if the concept 
head is a descendent of either the concept 
?Human? or the concept ?Occupation? in Omega.  
It is clear from the figure that the Decision Tree 
algorithm plus Bagging gives the highest precision 
and overall F-score.  All subsequent experiments 
are run using this technique.1 
Since high precision is the most important 
criterion for the filter, we also examine the 
performance of the classifier as it is applied with a 
threshold.  Thus, a probability cutoff is set such 
that only positive classifications that exceed this 
cutoff are actually classified as legitimate.  Figure 
2 shows a plot of the precision/recall tradeoff as 
this threshold is changed.  As the threshold is 
raised, precision increases while recall decreases.  
Based on this graph we choose to set the threshold 
at 0.9.  
Learning Algorithm Performance
0.5
0.6
0.7
0.8
0.9
1
Baseline Na?ve Bayes SVM Decision
List
Decision
Tree
DT +
Boosting
DT +
Bagging
Recall Precision F-Score
4.2 
                                                          
1 Precision and Recall here refer only to the output of the 
extraction patterns.  Thus, 100% recall indicates that all 
legitimate concept-instance pairs that were extracted using the 
patterns, were classified as legitimate by the filter.  It does not 
indicate that all concept-instance information in the text was 
extracted.  Precision is to be understood similarly. 
Applying the Decision Tree algorithm with 
Bagging, using the pre-determined threshold, to the 
held out test set of 500 examples extracted with the 
CN/PN pattern yields a precision of .95 and a 
recall of .718.  Under these same conditions, but 
applied to a held out test set of 500 examples 
extracted with the APOS pattern, the filter has a 
precision of .95 and a recall of .92.   
 
 
 
 
 
 
 
 
Precision vs. Recall
as a Function of Threshold
0.955
96
0.965
97
0.975
98
0.985
99
0.995
0.4 0.5 0.6 0.7 0.8 0.9
Recall
P
re
ci
si
o
n
 0.
 0.
 0.
 0.
 
Figure 2.  Plot of precision and recall on a 500 example 
validation set as a threshold cutoff for positive 
classification is changed.  As the threshold is increased, 
precision increases while recall decreases.  At the 0.9 
threshold value, precision/recall on the validation set is 
0.98/0.7, on a held out test set it is 0.95/0.72.   
Final Extraction Results 
The CN/PN and APOS filters were used to extract 
concept-instance pairs from unstructured text.  The 
approximately 15GB of newspaper text (described 
above) was passed through the regular expression 
patterns and filtered through their appropriate 
learned classifier.  The output of this process is 
approximately 2,000,000 concept-instance pairs.  
Approximately 930,000 of these are unique pairs, 
comprised of nearly 500,000 unique instances 2 , 
paired with over 450,000 unique concepts3 (e.g., 
2 Uniqueness of instances is judged here solely on the basis of 
surface orthography.  Thus, ?Bill Clinton? and ?William 
Clinton? are considered two distinct instances.  The effects of 
collapsing such cases will be considered in future work. 
3 As with instances, concept uniqueness is judged solely on the 
basis of orthography.  Thus, ?Steven Spielberg? and ?J. Edgar 
Hoover? are both considered instances of the single concept 
Threshold=0.90
Threshold=0.80
?sultry screen actress?), which can be categorized 
based on nearly 100,000 unique complex concept 
heads (e.g., ?screen actress?) and about 14,000 
unique simple concept heads (e.g., ?actress?).  
Table 3 shows examples of this output. 
A sample of 100 concept-instance pairs was 
randomly selected from the 2,000,000 extracted 
pairs and hand annotated.  93% of these were 
judged legitimate concept-instance pairs. 
 
Concept head Concept Instance 
Producer Executive producer Av Westin 
Newspaper Military newspaper Red Star 
Expert Menopause expert Morris Notwlovitz 
Flutist Flutist James Galway 
Table 3.  Example of concept-instance repository.  
Table shows extracted relations indexed by concept 
head, complete concept, and instance. 
5 
                                                                                          
Question Answering Evaluation 
A large number of questions were collected over 
the period of a few months from 
www.askJeeves.com.  100 questions of the form 
?Who is x? were randomly selected from this set.  
The questions queried concept-instance relations 
through both instance centered queries (e.g., ?Who 
is Jennifer Capriati??) and concept centered 
queries (e.g., ?Who is the mayor of Boston??).  
Answers to these questions were then 
automatically generated both by look-up in the 
2,000,000 extracted concept-instance pairs and by 
TextMap, a state of the art web-based Question 
Answering system which ranked among the top 10 
systems in the TREC 11 Question Answering track 
(Hermjakob et al, 2002). 
Although both systems supply multiple 
possible answers for a question, evaluations were 
conducted on only one answer.4  For TextMap, this 
answer is just the output with highest confidence, 
i.e., the system?s first answer.  For the extracted 
instances, the answer was that concept-instance 
pair that appeared most frequently in the list of 
extracted examples.  If all pairs appear with equal 
frequency, a selection is made at random. 
Answers for both systems are then classified 
by hand into three categories based upon their 
 
?director.?  See Fleischman and Hovy (2002) for techniques 
useful in disambiguating such instances. 
4 Integration of multiple answers is an open research question 
and is not addressed in this work. 
information content. 5  Answers that unequivocally 
identify an instance?s celebrity (e.g., ?Jennifer 
Capriati is a tennis star?) are marked correct.  
Answers that provide some, but insufficient, 
evidence to identify the instance?s celebrity (e.g., 
?Jennifer Capriati is a defending champion?) are 
marked partially correct.  Answers that provide no 
information to identify the instance?s celebrity 
(e.g., ?Jennifer Capriati is a daughter?) are marked 
incorrect.6  Table 5 shows example answers and 
judgments for both systems. 
 
State of the Art  Extraction  
Answer Mark Answer Mark 
Who is Nadia 
Comaneci? 
U.S. 
citizen 
P Romanian 
Gymnast 
C 
Who is Lilian 
Thuram? 
News 
page 
I French 
defender 
P 
Who is the mayor 
of Wash., D.C.? 
Anthony 
Williams 
C no answer 
found 
I 
Table 5.  Example answers and judgments of a state of 
the art system and look-up method using extracted 
concept-instance pairs on questions collected online.  
Ratings were judged as either correct (C), partially 
correct (P), or incorrect (I). 
6 
                                                          
Question Answering Results 
Results of this comparison are presented in Figure 
3.  The simple look-up of extracted concept-
instance pairs generated 8% more partially correct 
answers and 25% more entirely correct answers 
than TextMap.  Also, 21% of the questions that 
TextMap answered incorrectly, were answered 
partially correctly using the extracted pairs; and 
36% of the questions that TextMap answered 
incorrectly, were answered entirely correctly using 
the extracted pairs.  This suggests that over half of 
the questions that TextMap got wrong could have 
benefited from information in the concept-instance 
pairs.  Finally, while the look-up of extracted pairs 
took approximately ten seconds for all 100 
questions, TextMap took approximately 9 hours.  
5  Evaluation of such ?definition questions? is an active 
research challenge and the subject of a recent TREC pilot 
study.  While the criteria presented here are not ideal, they are 
consistent, and sufficient for a system comparison. 
6  While TextMap is guaranteed to return some answer for 
every question posed, there is no guarantee that an answer will 
be found amongst the extracted concept-instance pairs.  When 
such a case arises, the look-up method?s answer is counted as 
incorrect. 
This difference represents a time speed up of three 
orders of magnitude. 
There are a number of reasons why the state of 
the art system performed poorly compared to the 
simple extraction method.  First, as mentioned 
above, the lack of newspaper text on the web 
means that TextMap did not have access to the 
same information-rich resources that the extraction 
method exploited.  Further, the simplicity of the 
extraction method makes it more resilient to the 
noise (such as parser error) that is introduced by 
the many modules employed by TextMap.  And 
finally, because it is designed to answer any type 
of question, not just ?Who is?? questions, 
TextMap is not as precise as the extraction 
technique.  This is due to both its lack of tailor 
made patterns for specific question types, as well 
as, its inability to filter those patterns with high 
precision. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
7 
                                                          
Figure 3.  Evaluation results for the state of the art 
system and look-up method using extracted concept-
instance pairs on 100 ?Who is ?? questions collected 
online.  Results are grouped by category: partially 
correct, entirely correct, and entirely incorrect. 
Discussion and Future Work 
The information repository approach to Question 
Answering offers possibilities of increased speed 
and accuracy for current systems.  By collecting 
information offline, on text not readily available to 
search engines, and storing it to be accessible 
quickly and easily, Question Answering systems 
will be able to operate more efficiently and more 
effectively.   
In order to achieve real-time, accurate 
Question Answering, repositories of data much 
larger than that described here must be generated.  
We imagine huge data warehouses where each 
repository contains relations, such as birthplace-of, 
location-of, creator-of, etc.  These repositories 
would be automatically filled by a system that 
continuously watches various online news sources, 
scouring them for useful information.   
Such a system would have a large library of 
extraction patterns for many different types of 
relations.  These patterns could be manually 
generated, such as the ones described here, or 
learned from text, as described in Ravichandran 
and Hovy (2002).  Each pattern would have a 
machine-learned filter in order to insure high 
precision output relations.  These relations would 
then be stored in repositories that could be quickly 
and easily searched to answer user queries. 7   
In this way, we envision a system similar to 
(Lin et al, 2002).  However, instead of relying on 
costly structured databases and pain stakingly 
generated wrappers, repositories are automatically 
filled with information from many different 
patterns.  Access to these repositories does not 
require wrapper generation, because all 
information is stored in easily accessible natural 
language text.  The key here is the use of learned 
filters which insure that the information in the 
repository is clean and reliable. 
Performance on a Question 
Answering Task
10
15
20
25
30
35
40
45
50
Partial Correct Incorrect
%
 C
or
re
ct
State of the Art System Extraction System
Such a system is not meant to be complete by 
itself, however.  Many aspects of Question 
Answering remain to be addressed.  For example, 
question classification is necessary in order to 
determine which repositories (i.e., which relations) 
are associated with which questions.   
Further, many question types require post 
processing.  Even for ?Who is ?? questions 
multiple answers need to be integrated before final 
output is presented.  An interesting corollary to 
using this offline strategy is that each extracted 
instance has with it a frequency distribution of 
associated concepts (e.g., for ?Bill Clinton?: 105 
?US president?; 52 ?candidate?; 4 ?nominee?).  
This distribution can be used in conjunction with 
time/stamp information to formulate mini 
biographies as answers to ?Who is ?? questions. 
We believe that generating and maintaining 
information repositories will advance many aspects 
of Natural Language Processing.  Their uses in 
7 An important addition to this system would be the inclusion 
of time/date stamp and data source information.  For, while 
?George Bush? is ?president? today, he will not be forever. 
data driven Question Answering are clear.  In 
addition, concept-instance pairs could be useful in 
disambiguating references in text, which is a 
challenge in Machine Translation and Text 
Summarization.   
In order to facilitate further research, we have 
made the extracted pairs described here publicly 
available at www.isi.edu/~fleisch/instances.txt.gz.  
In order to maximize the utility of these pairs, we 
are integrating them into an Ontology, where they 
can be more efficiently stored, cross-correlated, 
and shared.   
Acknowledgments 
 
The authors would like to thank Miruna Ticrea for 
her valuable help with training the classifier.  We 
would also like to thank Andrew Philpot for his work 
on integrating instances into the Omega Ontology, 
and Daniel Marcu whose comments and ideas were 
invaluable. 
References 
 
Michelle Banko, Eric Brill. 2001. Scaling to Very Very 
Large Corpora for Natural Language Disambiguation.  
Proceedings of the Association for Computational 
Linguistics, Toulouse, France.  
 
Matthew Berland and Eugene Charniak. 1999. Finding 
Parts in Very Large Corpora.  Proceedings of the 37th 
Annual Meeting of the Association for Computational 
Linguistics. College Park, Maryland. 
 
Eric Brill. 1994. Some advances in rule based part of speech 
tagging.  Proc. of AAAI.  Seattle, Washington. 
 
Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais, 
and Andrew Ng. 2001. Data-Intensive Question 
Answering. Proceedings of the 2001 Text REtrieval 
Conference (TREC 2001), Gaithersburg, MD. 
 
Michael Fleischman and Eduard Hovy. 2002.  Fine 
Grained Classification of Named Entities.  19th 
International Conference on Computational 
Linguistics (COLING). Taipei, Taiwan. 
 
Ulf Hermjakob, Abdessamad Echihabi, and Daniel 
Marcu. 2002.  Natural Language Based 
Reformulation Resource and Web Exploitation for 
Question Answering.  In Proceedings of the TREC-
2002 Conference, NIST. Gaithersburg, MD. 
 
Marti Hearst. 1992. Automatic Acquisition of 
Hyponyms from Large Text Corpora.  Proceedings of 
the Fourteenth International Conference on 
Computational Linguistics, Nantes, France. 
 
Jimmy Lin, Aaron Fernandes, Boris Katz, Gregory 
Marton, and Stefanie Tellex. 2002. Extracting 
Answers from the Web Using Data Annotation and 
Data Mining Techniques.  Proceedings of the 2002 
Text REtrieval Conference (TREC 2002) 
Gaithersburg, MD.  
 
Gideon S. Mann. 2002.  Fine-Grained Proper Noun 
Ontologies for Question Answering.  SemaNet'02: 
Building and Using Semantic Networks, Taipei, 
Taiwan. 
 
Deepak Ravichandran and Eduard Hovy. 2002.  
Learning surface text patterns for a Question 
Answering system.  Proceedings of the 40th ACL 
conference. Philadelphia, PA.  
 
I. Witten and E. Frank. 1999.  Data Mining: Practical 
Machine Learning Tools and Techniques with JAVA 
implementations.  Morgan Kaufmann, San Francisco, 
CA. 
 
 
 
 
 
 
Maximum Entropy Models for FrameNet Classification  
 
Michael Fleischman, Namhee Kwon and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{fleisch, nkwon, hovy }@ISI.edu 
  
Abstract 
The development of FrameNet, a large 
database of semantically annotated sen-
tences, has primed research into statistical 
methods for semantic tagging.  We ad-
vance previous work by adopting a 
Maximum Entropy approach and by using 
previous tag information to find the high-
est probability tag sequence for a given 
sentence.  Further we examine the use of 
sentence level syntactic pattern features to 
increase performance.  We analyze our 
strategy on both human annotated and 
automatically identified frame elements, 
and compare performance to previous 
work on identical test data.  Experiments 
indicate a statistically significant im-
provement (p<0.01) of over 6%. 
1 Introduction 
Recent work in the development of FrameNet, a 
large database of semantically annotated sentences, 
has laid the foundation for statistical approaches to 
the task of automatic semantic classification.   
The FrameNet project seeks to annotate a large 
subset of the British National Corpus with seman-
tic information.  Annotations are based on Frame 
Semantics (Fillmore, 1976), in which frames are 
defined as schematic representations of situations 
involving various frame elements such as partici-
pants, props, and other conceptual roles.   
In each FrameNet sentence, a single target 
predicate is identified and all of its relevant frame 
elements are tagged with their semantic role (e.g., 
Agent, Judge), their syntactic phrase type (e.g., 
NP, PP), and their grammatical function (e.g., ex-
ternal argument, object argument).  Figure 1 shows 
an example of an annotated sentence and its appro-
priate semantic frame.   
 
 
 
 
 
 
 
          She  clapped  her hands  in inspiration. 
Frame:        Body-Movement 
 
Frame Elements:   
Agent     Body Part Cause 
       -NP            -NP  -PP 
       -Ext             -Obj -Comp 
 
Figure 1.  Frame for lemma ?clap? shown with three 
core frame elements and a sentence annotated with ele-
ment type, phrase type, and grammatical function. 
 
As of its first release in June 2002, FrameNet 
has made available 49,000 annotated sentences.  
The release contains 99,000 annotated frame ele-
ments for 1462 distinct lexical predicates (927 
verbs, 339 nouns, and 175 adjectives). 
While considerable in scale, the FrameNet da-
tabase does not yet approach the magnitude of re-
sources available for other NLP tasks.  Each target 
predicate, for example, has on average only 30 sen-
tences tagged.  This data sparsity makes the task of 
learning a semantic classifier formidable, and in-
creases the importance of the modeling framework 
that is employed. 
2 Related Work 
To our knowledge, Gildea and Jurafsky (2002) 
is the only work to use FrameNet to build a statis-
tically based semantic classifier.  They split the 
problem into two distinct sub-tasks: frame element 
identification and frame element classification.  In 
the identification phase, syntactic information is 
extracted from a parse tree to learn the boundaries 
of the frame elements in a sentence.  In the classi-
fication phase, similar syntactic information is 
used to classify those elements into their semantic 
roles.   
In both phases Gildea and Jurafsky (2002) 
build a model of the conditional probabilities of the 
classification given a vector of syntactic features.  
The full conditional probability is decomposed into 
simpler conditional probabilities that are then in-
terpolated to make the classification.  Their best 
performance on held out test data is achieved using 
a linear interpolation model: 
where r is the class to be predicted, x is the vector 
of syntactic features, xi is a subset of those fea-
tures, ?i is the weight given to that subset condi-
tional probability (as determined using the EM 
algorithm), and m is the total number of subsets 
used.  Using this method, they report a test set ac-
curacy of 78.5% on classifying semantic roles and 
precision/recall scores of .726/.631 on frame ele-
ment identification.  
We extend Gildea and Jurafsky (2002)?s initial 
effort in three ways.  First, we adopt a maximum 
entropy (ME) framework in order to learn a more 
accurate classification model.  Second, we include 
features that look at previous tags and use previous 
tag information to find the highest probability se-
mantic role sequence for a given sentence.  Finally, 
we examine sentence-level patterns that exploit 
more global information in order to classify frame 
elements.  We compare the results of our classifier 
to that of Gildea and Jurafsky (2002) on matched 
test sets of both human annotated and automati-
cally identified frame elements.  
3 
                                                          
Semantic Role Classification 
Training (36,993 sentences / 75,548 frame ele-
ments), development (4,000 sentences / 8,167 
frame elements), and held out test sets (3,865 sen-
tences / 7,899 frame elements) were obtained in 
order to exactly match those used in Gildea and 
Jurafsky (2002)1 .  In the experiments presented 
below, features are extracted for each frame ele-
ment in a sentence and used to classify that ele-
ment into one of 120 semantic role categories.  The 
boundaries of each frame element are given based 
on the human annotations in FrameNet.  In Section 
4, experiments are performed using automatically 
identified frame elements. 
3.1 
                                                          
1 Data sets (including parse trees) were obtained from Dan 
Gildea via personal communication. 
Features 
For each frame element, features are extracted 
from the surface text of the sentence and from an 
automatically generated syntactic parse tree 
(Collins, 1997).  The features used are described 
below: 
 
)|()|(
0
i
m
i
i xrpxrp ?
=
= ? ? Target predicate (tar): Although there may 
be many predicates in a sentence with associ-
ated frame elements, classification operates on 
only one target predicate at a time.  The target 
predicate is the only feature that is not ex-
tracted from the sentence itself and must be 
given by the user.  Note that the frame which 
the target predicate instantiates is not given, 
leaving any word sense ambiguities to be han-
dled implicitly by the classifier.2 
? Phrase type (pt):  The syntactic phrase type of 
the frame element (e.g. NP, PP) is extracted 
from the parse tree of the sentence by finding 
the constituent in the tree whose boundaries 
match the human annotated boundaries of the 
element.  In cases where there exists no con-
stituent that perfectly matches the element, the 
constituent is chosen which matches the largest 
text span of the element and has the same left-
most boundary.  
? Syntactic head (head): The syntactic heads of 
the frame elements are extracted from the 
frame element?s matching constituent (as de-
scribed above) using a heuristic method de-
scribed by Michael Collins. 3   This method 
extracts the syntactic heads of constituents; 
thus, for example, the second frame element in 
Figure 1 has head ?hands,? while the third 
frame element has head ?in.? 
? Logical Function (lf): A simplification of the 
grammatical function annotation (see section 
1) is extracted from the parse tree.  Unlike the 
2 Because of the interaction of head word features with the 
target predicate, we suspect that ambiguous lexical items do 
not account for much error.  This question, however, will be 
addressed explicitly in future work. 
3 http://www.ai.mit.edu/people/mcollins/papers/heads 
Table 1.  Feature sets used in ME frame element classifier.  Shows individual feature sets, example feature 
function from that set, and total number of feature functions in the set.  Examples taken from frame element 
?in inspiration,? shown in Figure 1. 
 Number Feature Set Example function Number of Functions 
in Feature Set 
0 f(r, tar) f(CAUSE, ?clap?)=1 6,518 
1 f(r, tar, pt) f(CAUSE, ?clap?, PP)=1 12,030 
2 f(r, tar, pt, lf) f(CAUSE, ?clap?, PP, other)=1 14,615 
3 f(r, pt, pos, voice) f(CAUSE, NP, ?clap?, active)=1 1,215 
4 f(r, pt, pos, voice ,tar) f(CAUSE, PP, after, active, ?clap?)=1 15,602 
5 f(r ,head) f(CAUSE, ?in?)=1 18,504 
6 f(r, head, tar) f(CAUSE, ?in?, ?clap?)=1 38,223 
7 f(r, head, tar, pt) f(CAUSE, ?in?, ?clap?, PP)=1 39,740 
8 f(r, order, syn) f(CAUSE, 2, 
[NP-Ext,Target,NP-Obj,PP-other])=1 
13,228 
9 f(r, tar, order, syn) f(CAUSE, ?clap?, 2, 
[NP-Ext,Target,NP-Obj,PP-other])=1 
40,580 
10 f(r,r_-1) f(CAUSE, BODYPART)=1 1,158 
11 f(r,r_-1,r_-2) f(CAUSE, BODYPART, AGENT)=1 2,030 
Total Number of Features:  203,443 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
full grammatical function, the lf can have only 
one of three values: external argument, object 
argument, other.  A node is considered an ex-
ternal argument if it is an ancestor of an S 
node, an object argument if it is an ancestor of 
a VP node, and other for all other cases.  This 
feature is only applied to frame elements 
whose phrase type is NP.  
? Position (pos): The position of the frame ele-
ment relative to the target (before, after) is ex-
tracted based on the surface text of the 
sentence. 
? Voice (voice): The voice of the sentence (ac-
tive, passive) is determined using a simple 
regular expression passed over the surface text 
of the sentence. 
? Order (order): The position of the frame ele-
ment relative to the other frame elements in the 
sentence.  For example, in the sentence from 
Figure 1, the element ?She? has order=0, while 
?in inspiration? has order=2. 
? Syntactic pattern (pat): The sentence level 
syntactic pattern of the sentence is generated 
by looking at the phrase types and logical 
functions of each frame element in the sen-
tence.  For example, in the sentence: ?Alexan-
dra bent her head;? ?Alexandra? is an external 
argument Noun Phrase, ?bent? is a target 
predicate, and ?her head? is an object argu-
ment Noun Phrase.  Thus, the syntactic pattern 
associated with the sentence is [NP-ext, target, 
NP-obj].   
These syntactic patterns can be highly in-
formative for classification.  For example, in 
the training data, a syntactic pattern of [NP-
ext, target, NP-obj] given the predicate bend 
was associated 100% of the time with the 
Frame Element pattern: ?AGENT TARGET 
BODYPART.? 
? Previous role (r_n): Frame elements do not 
occur in isolation, but rather, depend very 
much on the other elements in a sentence.  
This dependency can be exploited in classifica-
tion by using the semantic roles of previously 
classified frame elements as features in the 
classification of a current element.  This strat-
egy takes advantage of the fact that, for exam-
ple, if a frame element is tagged as an AGENT 
it is highly unlikely that the next element will 
also be an AGENT. 
The previous role feature indicates the 
classification that the n-previous frame ele-
ment received.  During training, this informa-
tion is provided by simply looking at the true 
classes of the frame element occurring n posi-
tions before the target element.  During testing, 
hypothesized classes of the n elements are used 
and Viterbi search is performed to find the 
most probable tag sequence for a sentence. 
3.2 Maximum Entropy 
ME models implement the intuition that the best 
model will be the one that is consistent with the set 
of constrains imposed by the evidence, but other-
wise is as uniform as possible (Berger et al, 1996).  
We model the probability of a semantic role r 
given a vector of features x according to the ME 
formulation below: 
3.3 
3.4 
Experiments 
We present three experiments in which different 
feature sets are used to train the ME classifier.  The 
first experiment uses only those feature combina-
tions described in Gildea and Jurafsky (2002) (fea-
ture sets 0-7 from Table 1).  The second 
experiment uses a super set of the first and incor-
porates the syntactic pattern features described 
above (feature sets 0-9).  The final experiment uses 
the previous tags and implements Viterbi search to 
find the best tag sequence (feature sets 0-11). 
?
=
=
n
i
i
x
xrfZxrp
0
i )],(exp[1)|( ?  
Here Zx is a normalization constant, fi(r,x) is a fea-
ture function which maps each role and vector 
element (or combination of elements) to a binary 
value, n is the total number of feature functions, 
and ?i is the weight for a given feature function.  
The final classification is just the role with highest 
probability given its feature vector and the model. 
We further investigate the effect of varying two 
aspects of classifier training: the standard deviation 
of the Gaussian priors used for smoothing, and the 
number of sentences used for training.  To examine 
the effect of optimizing the standard deviation, a 
range of values was chosen and a classifier was 
trained using each value until performance on a 
development set ceased to improve.   
The feature functions that we employ can be 
divided into feature sets based upon the types and 
combinations of features on which they operate.  
Table 1 lists the feature sets that we use, as well as 
the number of individual feature functions they 
contain.  The feature combinations were chosen 
based both on previous work and trial and error.  In 
future work we will examine more principled fea-
ture selection techniques. 
To examine the effect of training set size on 
performance, five data sets were generated from 
the original set with 36, 367, 3674, 7349, and 
24496 sentences, respectively.  These data sets 
were created by going through the original set and 
selecting every thousandth, hundredth, tenth, fifth, 
and every second and third sentence, respectively.  
It is important to note that the feature functions 
described here are not equivalent to the subset 
conditional distributions that are used in the Gildea 
and Jurafsky model.  ME models are log-linear 
models in which feature functions map specific 
instances of syntactic features and classes to binary 
values (e.g., if a training element has head=?in? 
and role=CAUSE, then, for that element, the feature 
function f(CAUSE, ?in?) will equal 1).  Thus, ME is 
not here being used as another way to find weights 
for an interpolated model.  Rather, the ME ap-
proach provides an overarching framework in 
which the full distribution of semantic roles given 
syntactic features can be modeled. 
 
 
 
 
 
 
 
 
 
 
 We train the ME models using the GIS algo-
rithm (Darroch and Ratcliff, 1972) as implemented 
in the YASMET ME package (Och, 2002).  We 
use the YASMET MEtagger (Bender et al, 2003) 
to perform the Viterbi search.  The classifier was 
trained until performance on the development set 
ceased to improve.  Feature weights were 
smoothed using Gaussian priors with mean 0 
(Chen and Rosenfeld, 1999).  The standard devia-
tion of this distribution was optimized on the de-
velopment set for each experiment. 
 
Classifier Performance on Test Set
81.7
83.6
84.7
78.5
76
78
80
82
84
86
G&J Exp 1 Exp 2 Exp 3
%
 C
or
re
ct
Figure 2.  Performance of models on test data using 
hand annotated frame element boundaries.  G&J refers 
to the results of Gildea and Jurafsky (2002).  Exp 1 in-
corporates feature sets 0-7 from Table 1; Exp 2 feature 
sets 0-9; Exp 3 features 0-11.  
Results  
Figure 2 shows the results of our experiments 
alongside those of (Gildea and Jurafsky, 2002) on 
identical held out test sets.  The difference in per-
formance between each classifier is statistically 
significant at (p<0.01) (Mitchell, 1997), with the 
exception of Exp 2 and Exp 3, whose difference is 
statistically significant at (p<0.05).   
 
Table 2.  Effect of different smoothing parameter (std. 
dev.) values on classification performance. 
Std. Dev. % Correct 
1 79.9 
2 82.1 
4 81.9 
 
Table 2 shows the effect of varying the stan-
dard deviation of the Gaussian priors used for 
smoothing in Experiment 1.  The difference in per-
formance between the classifiers trained using 
standard deviation 1 and 2 is statistically signifi-
cant at (p<0.01). 
 
10%
20%
30%
40%
50%
60%
70%
80%
90%
10 100 1000 10000 100000
# Sentences in Training
%
 C
or
re
ct
 
Figure 3.  Effect of training set size on semantic role 
classification. 
 
Figure 3 shows the change in performance as a 
function of training set size.  Classifiers were 
trained using the full set of features described for 
Experiment 3. 
Table 3 shows the confusion matrix for a subset 
of semantic roles.  Five roles were chosen for pres-
entation based upon their high contribution to clas-
sifier error.  Confusion between these five account 
for 27% of all errors made amongst the 120 possi-
ble roles.  The tenth role, other, represents the sum 
of the remaining 115 roles.  Table 4 presents ex-
ample errors for five of the most confused roles.   
3.5 Discussion 
It is clear that the ME models improve perform-
ance on frame element classification.  There are a 
number of reasons for this improvement. 
First, for this task the log-linear model employed 
in the ME framework is better than the linear 
interpolation model used by Gildea and Jurafsky.  
One possible reason for this is that semantic role 
classification benefits from the ME model?s bias 
for more uniform probability distributions that sat-
isfy the constraints placed on the model by the 
training data.   
Another reason for improved performance comes 
from ME?s simpler design.  Instead of having to 
worry about finding proper backoff strategies 
amongst distributions of features subsets, ME al-
lows one to include many features in a single 
model and automatically adjusts the weights of 
these features appropriately. 
 
Table 3.  Confusion matrix for five roles which contrib-
ute most to overall system error. Columns refer to ac-
tual role.  Rows refer to the model?s hypothesis.  Other 
refers to combination of all other roles. 
  Area Spkr Goal Msg Path Other Prec. 
Area 98  6  18 16 0.710 
Spkr  373  23  41 0.853 
Goal 11  431  28 50 0.828 
Msg  18 1 315  33 0.858 
Path 32  36  415 41 0.791 
Other 15 21 26 24 33 5784 0.979 
Recall 0.628 0.905 0.862 0.87 0.84 0.969   
 
Also, because the ME models find weights for 
many thousands of features, they have many more 
degrees of freedom than the linear interpolated 
models of Gildea and Jurafsky.  Although many 
degrees of freedom can lead to overfitting of the 
training data, the smoothing procedure employed 
in our experiments helps to counteract this prob-
lem.  As evidenced in Table 2, by optimizing the 
standard deviation used in smoothing the ME 
models are able to show significant increases in 
performance on held out test data. 
Finally, by including in our model sentence-
level pattern features and information about previ-
ous classes, global information can be exploited for 
improved classification.  The accuracy gained by 
including such global information confirms the 
intuition that the semantic role of an element is 
much related to the entire sentence of which it is a 
part. 
Having discussed the advantages of the models 
presented here, it is interesting to look at the errors 
that the system makes.  It is clear from the confu-
sion matrix in Table 3 that a great deal of the sys-
tem error comes from relatively few semantic 
roles.4  Table 4 offers some insight into why these 
errors occur.  For example, the confusions exem-
plified in 1 and 2 are both due to the fact that the 
particular phrases employed can be used in multi-
ple roles (including the roles hypothesized by the 
system).  Thus, while ?across the counter? may be 
considered a goal when one is talking about a per-
son and their head, the same phrase would be con-
sidered a path if one were talking about a mouse 
who is running.   
 
Table 4.  Example errors for five of the most often con-
fused semantic roles 
 Actual Proposed Example Sentence 
1 Goal Path The barman craned his head 
across the counter. 
2 Area Path Mr. Glass began hallucinating, 
throwing books around the 
classroom. 
3 Message Speaker Debate lasted until 20 Septem-
ber, opposition being voiced 
by a number of Italian and 
Spanish prelates. 
4 Addressee Speaker Furious staff claim they were 
even called in from holiday to 
be grilled by a specialist secu-
rity firm 
5 Reason Evaluee We cannot but admire the 
efficiency with which she 
took control of her own life. 
 
Examples 3 and 4, while showing phrases with 
similar confusions, stand out as being errors caused 
by an inability to deal with passive sentences.  
Such errors are not unexpected; for, even though 
the voice of the sentence is an explicit feature, the 
system suffers from the paucity of passive sen-
tences in the data (approximately 5%). 
Finally, example 5 shows an error that is based 
on the difficult nature of the decision itself (i.e., it 
is unclear whether ?the efficiency? is the reason for 
admiration, or what is being admired).  Often 
times, phrases are assigned semantic roles that are 
not obvious even to human evaluators.  In such 
cases it is difficult to determine what information 
might be useful for the system. 
Having looked at the types of errors that are 
common for the system, it becomes interesting to 
examine what strategy may be best to overcome 
such errors.  Aside from new features, one solution 
is obvious: more data.  The curve in Figure 2 
shows that there is still a great deal of performance 
to be gained by training the current ME models on 
more data.  The slope of the curve indicates that 
we are far from a plateau, and that even constant 
increases in the amount of available training data 
may push classifier performance above 90% accu-
racy.   
Having demonstrated the effectiveness of the 
ME approach on frame element classification 
given hand annotated frame element boundaries, 
we next examine the value of the approach given 
automatically identified boundaries. 
                                                          
4 
4.1 
Frame Element Identification 
Gildea and Jurafsky equate the task of locating 
frame element boundaries to one of identifying 
frame elements amongst the parse tree constituents 
of a given sentence.  Because not all frame element 
boundaries exactly match constituent boundaries, 
this approach can perform no better than 86.9% 
(i.e. the number of elements that match constitu-
ents (6864) divided by the total number of ele-
ments (7899)) on the test set.   
Features 
Frame element identification is a binary classifica-
tion problem in which each constituent in a parse 
tree is described by a feature vector and, based on 
that vector, tagged as either a frame element or not.  
In generating feature vectors we use a subset of the 
features described for role tagging as well as an 
additional path feature. 
 
 
Figure 4.  Generation of path features used in frame 
element tagging.  The path from the constituent ?in in-
spiration? to the target predicate ?clapped? is repre-
sented as the string PP?VP?VBD.  
 
Gildea and Jurafsky introduce the path feature 
in order to capture the structural relationship be-
tween a constituent and the target predicate.  The  4 44% of all error is due to confusion between only nine roles. 
Table 5.  Results of frame element identification.  G&J represents results reported in (Gildea and Jurafsky, 2002), 
ME results for the experiments reported here.  The second column shows precision, recall, and F-scores for the task 
of frame element identification, the third column for the combined task of identification and classification.   
FE ID only FE ID + FE Classification Method 
Precision Recall F-Score Precision Recall F-Score 
G&J Boundary id + baseline role labeler .726 .631 .675 .67 .468 .551 
ME Boundary id + ME role labeler .736 .679 .706 .6 .554 .576 
 
path of a constituent is represented by the nodes 
through which one passes while traveling up the 
tree from the constituent and then down through 
the governing category to the target.  Figure 4 
shows an example of this feature for a frame ele-
ment from the sentence presented in Figure 1. 
4.2 
4.3 
Experiments 
We use the ME formulation described in Section 
3.2 to build a binary classifier.  The classifier fea-
tures follow closely those used in Gildea and Juraf-
sky.  We model the data using the feature sets: f(fe, 
path), f(fe, path, tar), and f(fe, head, tar), where fe 
represents the binary classification of the constitu-
ent.  While this experiment only uses three feature 
sets, the heterogeneity of the path feature is so 
great that the classifier itself uses 1,119,331 unique 
binary features. 
With the constituents having been labeled, we 
apply the ME frame element classifier described 
above.  Results are presented using the classifier of 
Experiment 1, described in section 3.3. We then 
investigate the effect of varying the number of 
constituents used for training on identification per-
formance.  Five data sets of approximately 100,000 
10,000, 1,000, and 100 constituents were generated 
from the original set by random selection and used 
to train ME models as described above. 
Results 
Table 5 compares the results of Gildea and Juraf-
sky (2002) and the ME frame element identifier on 
both the task of frame element identification alone, 
and the combined task of frame element identifica-
tion and classification.  In order to be counted cor-
rect on the combined task, the constituent must 
have been correctly identified as a frame element, 
and then must have been correctly classified into 
one of the 120 semantic categories.   
Recall is calculated based on the total number 
of frame elements in the test set, not on the total 
number of elements that have matching parse con-
stituents.  Thus, the upper limit is 86.9%, not 
100%.  Precision is calculated as the number of 
correct positive classifications divided by the num-
ber of total positive classifications.  
The difference in the F-scores on the identifica-
tion task alone and on the combined task are statis-
tically significant at the (p<0.01) level 5 .  The 
accuracy of the ME semantic classifier on the 
automatically identified frame elements is 81.5%, 
not a statistically significant difference from its 
performance on hand labeled elements, but a statis-
tically significant difference from the classifier of 
Gildea and Jurafsky (2002) (p<0.01). 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
100 1000 10000 100000 1000000
# Constituents in Training
F-
Sc
or
e
 
Figure 5.  Effect of training set size on frame element 
boundary identification. 
 
Figure 5 shows the results of varying the train-
ing set size on identification performance.  For 
each data set, thresholds were chosen to maximize 
F-Score. 
4.4 
                                                          
Discussion 
It is clear from the results above that the perform-
ance of the ME model for frame element classifica-
tion is robust to the use of automatically identified 
frame element boundaries.  Further, the ME 
5 G&J?s results for the combined task were generated with a 
threshold applied to the FE classifier (Dan Gildea, personal 
communication).  This is why their precision/recall scores are 
dissimilar to their accuracy scores, as reported in section 3.  
Because the ME classifier does not employ a threshold, com-
parisons must be based on F-score. 
framework yields better results on the frame ele-
ment identification task than the simple linear in-
terpolation model of Gildea and Jurafsky.  This 
result is not surprising given the discussion in Sec-
tion 3.   
What is striking, however, is the drastic overall 
reduction in performance on the combined 
identification and classification task.  The 
bottleneck here is the identification of frame 
element boundaries.  Unlike with classification 
though, Figure 5 indicates that a plateau in the 
learning curve has been reached, and thus, more 
data will not yield as dramatic an improvement for 
the given feature set and model.   
5 Conclusion 
The results reported here show that ME models 
provide higher performance on frame element clas-
sification tasks, given both human and automati-
cally identified frame element boundaries, than the 
linear interpolation models examined in previous 
work.  We attribute this increase to the benefits of 
the ME framework itself, the incorporation of sen-
tence-level syntactic patterns into our feature set, 
and the use of previous tag information to find the 
most probable sequence of roles for a sentence.   
But perhaps most striking in our results are the 
effects of varying training set size on the perform-
ance of the classification and identification models.  
While for classification, the learning curve appears 
to be still increasing with training set size, the 
learning curve for identification appears to have 
already begun to plateau.  This suggests that while 
classification will continue to improve as the Fra-
meNet database gets larger, increased performance 
on identification will rely on the development of 
more sophisticated models. 
In future work, we intend to apply the lessons 
learned here to the problem of frame element iden-
tification.  Gildea and Jurafsky have shown that 
improvements in identification can be had by more 
closely integrating the task with classification (they 
report an F-Score of .719 using an integrated 
model).  We are currently exploring a ME ap-
proach which integrates these two tasks under a 
tagging framework.  Initial results show that sig-
nificant improvements can be had using techniques 
similar to those described above. 
Acknowledgments 
The authors would like to thank Dan Gildea who 
generously allowed us access to his data files and 
Oliver Bender for making the MEtagger software 
available.  Finally, we thank Franz Och whose help 
and expertise was invaluable. 
References 
O. Bender, K. Macherey, F. J. Och, and H. Ney. 
2003. Comparison of Alignment Templates and 
Maximum Entropy Models for Natural Lan-
guage Processing. Proc. of EACL-2003.  Buda-
pest, Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 
1996. A Maximum Entropy Approach to Natu-
ral Language Processing. Computational Lin-
guistics, vol. 22, no. 1. 
S. F. Chen and R. Rosenfeld. 1999. A Gaussian 
prior for smoothing maximum entropy models. 
Technical Report CMUCS -99-108, Carnegie 
Mellon University 
M. Collins. 1997. Three generative, lexicalized 
models for statistical parsing.  Proc. of the 35th 
Annual Meeting of the ACL.  pages 16-23, Ma-
drid, Spain. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
iterative scaling for log-linear models. Annals 
of Mathematical Statistics, 43:1470-1480. 
C. Fillmore 1976.  Frame semantics and the nature 
of language. Annals of the New York Academy 
of Sciences: Conference on the Origin and De-
velopment of Language and Speech, Volume 
280 (pp. 20-32).  
D. Gildea and D. Jurafsky. 2002.  Automatic La-
beling of Semantic Roles, Computational Lin-
guistics, 28(3) 245-288 14.  
T. Mitchell. 1997.  Machine Learning.  McGraw-
Hill International Editions, New York, NY. 
Pages 143-145. 
F.J. Och. 2002. Yet another maxent toolkit: 
YASMET. www-i6.informatik.rwth-
aachen.de/Colleagues/och/. 
 
 
FrameNet-based Semantic Parsing using Maximum Entropy Models 
Namhee Kwon 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
nkwon@isi.edu 
Michael Fleischman 
Messachusetts Institute of 
Technology,  
77 Massachusetts Ave 
Cambridge, MA 02139 
mbf@mit.edu 
Eduard Hovy 
Information Sciences Institute 
University of Southern California
4676 Admiralty Way 
Marina del Rey, CA 90292 
hovy@isi.edu 
 
Abstract 
As part of its description of lexico-semantic 
predicate frames or conceptual structures, the 
FrameNet project defines a set of semantic 
roles specific to the core predicate of a 
sentence.  Recently, researchers have tried to 
automatically produce semantic interpretations 
of sentences using this information.  Building 
on prior work, we describe a new method to 
perform such interpretations.  We define 
sentence segmentation first and show how 
Maximum Entropy re-ranking helps achieve a 
level of 76.2% F-score (answer among top-
five candidates) or 61.5% (correct answer). 
1 Introduction 
To produce a semantic analysis has long been a 
goal of Computational Linguistics.  To do so, 
however, requires a representation of the semantics 
of each predicate.  Since each predicate may have a 
particular collection of semantic roles (agent, 
theme, etc.) the first priority is to build a collection 
of predicate senses with their associated role 
frames.  This task is being performed in the 
FrameNet project based on frame semantics 
(Fillmore, 1976). 
Each frame contains a principal lexical item as 
the target predicate and associated frame-specific 
roles, such as offender and buyer, called frame 
elements.  FrameNet I contains 1,462 distinct 
predicates (927 verbs, 339 nouns, 175 adjectives) 
in 49,000 annotated sentences with 99,000 
annotated frame elements.  Given these, it would 
be interesting to attempt an automatic sentence 
interpretation. 
We build semantic parsing based on FrameNet, 
treating it as a classification problem.  We split the 
problem into three parts: sentence segmentation, 
frame element identification for each segment, and 
semantic role tagging for each frame element.  In 
this paper, we provide a pipeline framework of 
these three phases, followed by a step of re-ranking 
from n-best lists of every phase for the final output.  
All classification and re-ranking are performed by 
Maximum Entropy. 
The top-five final outputs provide an F-score of 
76.2% for the correct frame element identification 
and semantic role tagging.  The performance of the 
single best output is 61.5% F-score. 
The rest of the paper is organized as follows: we 
review related work in Section 2, explain 
Maximum Entropy in Section 3, describe the 
detailed method in Section 4, show the re-ranking 
process in Section 5, and conclude in Section 6.   
2 Related Work 
The first work using FrameNet for semantic 
parsing was done by Gildea and Jurafsky (G & J, 
2002) using conditional probabilistic models.  
They divide the problem into two sub-tasks: frame 
element identification and frame element 
classification.  Frame element identification 
identifies the frame element boundaries in a 
sentence, and frame element classification 
classifies each frame element into its appropriate 
semantic role.  The basic assumption is that the 
frame element (FE) boundaries match the parse 
constituents, and both identification and 
classification are then done for each constituent1. 
In addition to the separate two phase model of 
frame element identification and role classification, 
they provide an integrated model that exhibits 
improved performance.  They define a frame 
element group (FEG) as a set of frame element 
roles present in a particular sentence.  By 
integrating FE identification with role labeling, 
allowing FEG priors and role labeling decision to 
affect the determination of next FE identification, 
they accomplish F-score of 71.9% for FE 
identification and 62.8% for both of FE 
identification and role labeling.  However, since 
this integrated approach has an exponential 
complexity in the number of constituents, they 
apply a pruning scheme of using only the top m 
                                                     
1 The final output performance measurement is limited 
to the number of parse constituents matching the 
frame element boundaries. 
hypotheses on the role for each constituent (m = 
10). 
Fleischman et al(FKH, 2003) extend G & J?s 
work and achieve better performance in role 
classification for correct frame element boundaries.  
Their work improves accuracy from 78.5% to 
84.7%.  The main reasons for improvement are 
first the use of Maximum Entropy and second the 
use of sentence-wide features such as Syntactic 
patterns and previously identified frame element 
roles.  It is not surprising that there is a 
dependency between each constituent?s role in a 
sentence and sentence level features reflecting this 
dependency improve the performance. 
In this paper, we extend our previous work 
(KFH) by adopting sentence level features even for 
frame element identification. 
3 Maximum Entropy 
ME models implement the intuition that the best 
model is the one that is consistent with the set of 
constraints imposed by the evidence, but otherwise 
is as uniform as possible (Berger et al 1996).  We 
model the probability of a class c given a vector of 
features x according to the ME formulation below: 
        )],(exp[1)|(
0
xcfZxcp ii
n
ix
?=?=  
Here xZ  is normalization constant, ),( xcfi  is a 
feature function which maps each class and vector 
element to a binary value, n is the total number of 
feature functions, and i?  is a weight for the 
feature function.  The final classification is just the 
class with the highest probability given its feature 
vector and the model.   
It is important to note that the feature functions 
described here are not equivalent to the subset 
conditional distributions that are used in G & J?s 
model.  ME models are log-linear models in which 
feature functions map specific instances of features 
and classes to binary values.  Thus, ME is not here 
being used as another way to find weights for an 
interpolated model.  Rather, the ME approach 
provides an overarching framework in which the 
full distribution of classes (semantic roles) given 
features can be modeled. 
4 Model 
We define the problem into three subsequent 
processes (see Figure 1): 1) sentence segmentation 
2) frame element identification, and 3) semantic 
role tagging for the identified frame elements.  In 
order to use sentence-wide features for the FE 
identification, a sentence should have a single non-
overlapping constituent sequence instead of all the 
independent constituents.  Sentence segmentation 
is applied before FE identification for this purpose.  
For each segment the classification into FE or not 
is performed in the FE identification phase, and 
from the FE-tagged constituents the semantic role 
classification is applied in the role tagging phase. 
He got up, bent briefly over her hand.
(He) (got up) (bent) (briefly) (over her hand)
FE NO T FE FE
(He) (briefly) (over her hand)
Agent Manner Path
Input sentence
1) Sentence Segmentation: 
choose the highest constituents 
while separating target word 
2) Frame Element Boundary Identification:
apply ME classification to classify each segment 
into classes of FE (frame element), T (target), NO (none)
Extract the identified FEs:
choose segments that are identified as FEs
3) Semantic Role Tagging:
apply ME classification to classify each FE
Into classes of 120 semantic roles
Output role: Agent (He), Manner (briefly), Path (over her hand) 
for the target ?bent?
Fig. 1. The sequence of steps on a sample sentence. 
4.1 Sentence Segmentation 
The advantages of applying sentence 
segmentation before FE identification are 
considered in two ways.  First we can utilize 
sentence-wide features, and second the number of 
constituents as FE candidates is reduced, which 
reduces the convergence time in training. 
We segment a sentence with parse constituents2.  
During training, we split a sentence into true frame 
elements and the remainder.  After choosing frame 
elements as segments, we choose the highest level 
constituents in parse tree for other parts, and then 
make a complete sentence composed of a sequence 
of constituent segments.  During testing, we need 
to consider all combinations of various level 
constituents.  We know the given target word 
should be a separate segment because a target word 
is not a part of other FEs.  Since most frame 
elements tend to be among the higher levels of a 
parse tree, we decide to use the highest 
constituents while separating the target word.  
Figure 2 shows an example of the segmentation for 
                                                     
2 We use Michael Collins?s parser :  
http://www.cis.upenn.edu/~mcollins/ 
an actual sentence in FrameNet with the target 
word ?bent?.  
He got up bent briefly over her hand
PRP VBD RP
PRT
VBD RP IN PRP$ NN
VP
VP
VP
ADVP
NP
PP
NP
S
Fig. 2. A sample sentence segmentation: ?bent? is 
a target predicate in a sentence and the shaded 
constituent represents each segment. 
However, this segmentation for testing reduces 
the FE coverage of constituents, which means our 
FE classification performance is limited.  Table 1 
shows the FE coverage and the number of 
constituents for our development set.  The FE 
coverage of individual constituents (86.36%) 
means the accuracy of the parser.  This limitation 
and will be discussed in detail in Section 4.4. 
Method Number of constituents 
FE coverage 
(%) 
Individual 
constituents  115,380 86.36 
Sentence 
segmentation 29,688 77.25 
Table 1.  The number of constituents and FE 
coverage for development set. 
4.2 Frame Element Identification 
Frame element identification is executed for the 
sequence of segments. For the example sentence in 
Figure 2, ?(He) (got up) (bent) (briefly) (over her 
hand)?, there are five segments and each segment 
has its own feature vector.  Maximum Entropy 
classification into the classes of FE, Target, or 
None is conducted for each.  Since the target 
predicate is given we don?t need to classify a target 
word into a class, but we do not exclude it from the 
segments because we want to get benefit of using 
previous segment?s features. 
The initial features are adopted from G & J and 
FKH, and most features are common to both of 
frame element identification and semantic role 
classification.  The features are: 
? Target predicate (target): The target 
predicate, the principal word in a sentence, is 
the feature that is provided by the user.  
Although there can be many predicates in a 
sentence, only one predicate is defined at a 
time. 
? Target identification (tar): The target 
identification is a binary value, indicating 
whether the given constituent is a target or not.  
Because we have a target word in a sequence 
of segments, we provide this information 
explicitly. 
? Constituent path (path): From the syntactic 
parse tree of a sentence, we extract the path 
from each constituent to the target predicate.   
The path is represented by the nodes through 
which one passes while traveling up the tree 
from the constituent and then down through 
the governing category to the target word.  For 
example, ?over her hand? in a sentence of 
Figure 2 has a path PP?VP?VBD. 
? Phrase Type (pt): The syntactic phrase type 
(e.g., NP, PP) of each constituent is also 
extracted from the parse tree. 
? Syntactic Head (head): The syntactic head of 
each constituent is obtained based on Michael 
Collins?s heuristic method3.  When the head is 
a proper noun, ?proper-noun? substitutes for 
the real head.  The decision if the head is 
proper noun is done by the part of speech tag 
in a parse tree.  
? Logical Function (lf): The logical functions of 
constituents in a sentence are simplified into 
three values: external argument, object 
argument, other.  We follow the links in the 
parse tree from the constituent to the ancestors 
until we meet either S or VP.  If the S is found 
first, we assign external argument to the 
constituent, and if the VP is found, we assign 
object argument. Otherwise, other is assigned.  
Generally, a grammatical function of external 
argument is a subject, and that of object 
argument is an object.  This feature is applied 
only to constituents whose phrase type is NP.  
? Position (pos): The position indicates whether 
a constituent appears before or after the target 
predicate and whether the constituent has the 
same parent as the target predicate or not. 
? Voice (voice): The voice of a sentence (active, 
passive) is determined by a simple regular 
expression over the surface form of the 
sentence. 
? Previous class (c_n): The class information of 
the nth-previous constituent (target, frame 
element, or none) is used to exploit the 
dependency between constituents.  During 
training, this information is provided by simply 
                                                     
3 http://www.ai.mit.edu/people/mcollins/papers/heads 
looking at the true classes of the frame element 
occurring n-positions before the current 
element.  During testing, hypothesized classes 
of the n elements are used and Viterbi search is 
performed to find the most probable tag 
sequence for a sentence. 
The combination of these features is used in ME 
classification as feature sets.  The feature sets are 
optimized by previous work and trial and error 
experiments.  Table 2 shows the lists of feature sets 
for ?briefly? in a sentence of ?He got up, bent 
briefly over her hand?.  These feature sets contain 
the previous or next constituent?s features, for 
example, pt_-1 represents the previous 
constituent?s phrase type and lf_1 represents the 
next constituent?s logical function. 
Feature Set Example Functions 
f(c, target) f(c, ?bent?) = 1 
f(c, target, pt) f(c, ?bent?,ADVP) = 1 
f(c, target, pt, lf) f(c, ?bent?,ADVP,other) = 1 
f(c, pt, pos, voice) f(c, ADVP,after_yes,active) = 1 
f(c, pt, lf) f(c, ADVP,other) = 1 
f(c, pt_-1, lf_-1) f(c, VBD_-1, other_-1) = 1 
f(c, pt_1, lf_1) f(c, PP_1, other_1) = 1 
f(c, pt_-1, pos_-1,voice) f(c, VBD_-1,t_-1,active) = 1 
f(c, pt_1, pos_1, voice) f(c, PP_1,after_yes_1, active) = 1 
f(c, head) f(c, ?briefly?) = 1 
f(c, head, target) f(c, ?briefly?, ?bent?) = 1 
f(c, path) f(c, ADVP?VP?VBD) = 1 
f(c, path_-1) f(c, VBD_-1) = 1 
f(c, path_1) f(c, PP?VP?VBD_1) = 1 
f(c, tar) f(c, 0) = 1 
f(c, c_-1) f(c, ?target?_-1) = 1 
f(c, c_-1,c_-2) f(c, ?target?_-1,?NO FE?_-2) = 1 
Table 2. Feature sets used in ME frame element 
identification.  Example functions of ?briefly? 
from the sample sentence in Fig.2 are shown. 
4.3 Semantic Role Classification 
The semantic role classification is executed only 
for the constituents that are classified into FEs in 
the previous FE identification phase.  Maximum 
Entropy classification is performed to classify each 
FE into classes of semantic roles. 
Most features from the frame element 
identification in Section 4.2 are still used, and two 
additional features are applied.  The feature sets 
are in Table 3. 
? Order (order): The relative position of a 
frame element in a sentence is given.  For 
example, in the sentence from Figure 2, there 
are three frame elements, and the element 
?He? has order 0, while ?over her hand? has 
order 2. 
? Syntactic pattern (pat): The sentence level 
syntactic pattern is generated from the parse 
tree by looking at the phrase type and logical 
functions of each frame element in a sentence.  
For example, in the sentence from Figure 2, 
?He? is an external argument Noun Phrase, 
?bent? is a target predicate, and ?over her 
hand? is an external argument Prepositional 
Phrase.  Thus, the syntactic pattern associated 
with the sentence is [NP-ext, target, PP-ext]. 
Feature Sets 
f(c, target) f(r, head) 
f(r, target, pt) f(r, head, target) 
f(r, target, pt, lf) f(r, head, target, pt) 
f(r, pt, pos, voice) f(r, order, syn) 
f(r, pt, pos, voice, target) f(r,target, order, syn) 
f(r, r_-1) f(r,r_-1,r_-2) 
Table 3. Feature sets used in ME semantic role 
classification. 
4.4 Experiments and Results 
Since FrameNet II was published during our 
research, we continued using FrameNet I (120 
semantic role categories).  We can, therefore, 
compare our results with previous research by 
matching exactly the same data as used in G & J 
and FKH.  We thank Dan Gildea for providing the 
following data set: training (36,993 sentences / 
75,548 frame elements), development (4,000 
sentences / 8,167 frame elements), and held our 
test sets (3,865 sentences / 7,899 frame elements). 
We train the ME models using the GIS 
algorithm (Darroch and Ratcliff, 1972) as 
implemented in the YASMET ME package (Och, 
2002).  For testing, we use the YASMET 
MEtagger (Bender et al 2003) to perform the 
Viterbi search for choosing the most probable tag 
sequence for a sentence using the probabilities 
from training.  Feature weights are smoothed using 
Gaussian priors with mean 0 (Chen and Rosenfeld, 
1999).  The standard deviation of this distribution 
and the number of GIS iterations for training are 
optimized on development set for each experiment.  
Table 4 shows the performance for test set. The 
evaluation is done for individual frame elements. 
To segment a sentence before FE identification 
or role tagging improves the overall performance 
(from 57.6% to 60.0% in Table 4).  Since the 
segmentation reduces the FE coverage of segments, 
we conduct the experiment with the manually 
chosen segmentation to see how much the 
segmentation helps the performance.  Here, we 
extract segments from the parse tree constituents, 
so the FE coverage is 86% for test set, which 
maches the parsing accuracy.  Table 5 shows the 
performance of the frame element identification for 
test set:  F-score is 77.2% that is much better than 
71.7% of our automatic segmentation. 
FE identification FE identification & Role tagging Method 
Prec Rec F Prec Rec F 
G & J 
separated 
model 
73.6 63.1 67.5 67.0 46.8 55.1 
FKH 
ME model 73.6 67.9 70.6 60.0 55.4 57.6 
Our model 
(segmentation 
+ ME 
classification) 
75.5 68.2 71.7 62.9 56.8 60.0 
Table 4. Performance comparison for test set. 
Precision Recall F-score 
82.1 72.9 77.2 
Table 5.  Result of frame element identification on 
manual segmentation of test set 
5 n-best Lists and Re-ranking 
As stated, the sentence segmentation improves 
the performance by using sentence-wide features, 
but it drops the FE coverage of constituents.  In 
order to determine a good segmentation for a 
sentence that does not reduce the FE coverage, we 
perform another experiment by using re-ranking.  
We obtain all possible segmentations for a given 
sentence, and conduct frame element identification 
and semantic role classification for all 
segmentations.  During both phases, we get n-best 
lists with Viterbi search, and finally choose the 
best output with re-ranking method.  Figure 3 
shows the overall framework of this task.  
5.1 Maximum Entropy Re-ranking 
We model the probability of output r given 
candidates? feature sets {x1 .. xt} where t is the total 
number of candidates and xj is a feature set of the 
jth candidate according to the following ME 
formulation: 
]})..{,(exp[1})..{|(
0
1 1?
=
=
n
i
tiixt
xxrfZxxrp ?
 
where Zx is a normalization factor, fi(r,{x1..xt}) is a 
feature function which maps each output and all 
candidates? feature sets to a binary value, n is the 
total number of feature functions, and ?i is the 
weight for a given feature function.  The weight ?i 
is associated with only each feature function while 
the weight in the ME classifier is associated with 
all possible classes as well as feature functions.  
The final decision is r having the highest 
probability of p(r|{x1..xt}) from t number of 
candidates. 
As a feature set for each candidate, we use the 
ME classification probability that is calculated 
during Viterbi search.  These probabilities are 
conditional probabilities given feature sets and 
these feature sets depend on the previous output, 
for example, semantic role tagging is done for the 
identified FEs in the previous phase.  For this 
reason, the product of these conditional 
probabilities is used as a feature set. 
    )|(*)|(*)|()|( ferpsegfepssegpsrp =  
where s is a given sentence, seg is a segmentation, 
fe is a frame element identification, and r is the 
final semantic role tagging.  p(fe|seg) and p(r|fe) 
are produced from the ME classification but 
p(seg|s) is computed by a heuristic method and a 
development set optimization experiment.  The 
adopted p(seg|s) is composed of p(each segment?s 
part of speech tag | target?s part of speech tag), 
p(the number of total segments in a sentence | total 
number of words in a sentence), and the average of 
each segment?s p(head word of FE | target). 
Two additional feature sets other than p(r|s) are 
applied to get slight improvement for re-ranking 
performance, which are average of p(parse tree 
depth of FE | target) and average of p(head word 
of FE | target). 
5.2 Experiments and Results 
We apply ME re-ranking in YASMET-ME 
package.  We train re-ranking model with 
development set after obtaining candidate lists for 
the set.  For a simple cross validation, the 
development set is divided into a sub-training set 
(3,200 sentences) and a sub-development set (800 
sentences) by selecting every fifth sentence.  
Training for re-ranking is executed with the sub- 
training set and optimization is done with the sub-
development set.  The final test is applied to test 
set. 
The possible number of segmentations is different 
depending on sentences, but the average number of 
segmentation lists is 15.24 for the development  set.  
For these segmentations, we compute 10-best5 lists 
for the FE identification and 10-best lists for the 
semantic role classification. 
                                                     
4  To reduce the number of different segmenations 
while not dropping the FE coverage, the segmentations 
having too many segments for a long sentence are 
excluded. 
5 The experiment showed 10-best lists outperformed 
other n-best lists where n is less than 10.  The bigger 
number was not tested because of  huge number of lists. 
 He craned over the balcony again but finally he seemed to sigh.
1. (He) (craned) (over) (the) (balcony) (again) (but) (finally) (he) (seemed) (to) (sigh).
?
6. (He) (craned) (over) (the balcony) (again) (but) (finally) (he) (seemed) (to sigh).
7. (He) (craned) (over) (the balcony) (again) (but) (finally) (he) (seemed to sigh).
?
11. (He) (craned) (over the balcony) (again) (but) (finally) (he) (seemed to sigh).
12. (He) (craned) (over the balcony) (again) (but) (finally he seemed to sigh).
Input sentence
Sentence Segmentation: segment a sentence into all possible combinations of constituents of a                                        
parse tree while separating target word (In this example, target word is ?craned?.)
Frame Element Identification:  apply ME classification to all segmentations and get n-best output 
classifying each segment into FE (frame element), T (target), or NO (none), then extract segments that are 
identified as frame elements
(1)
(2)
(4)
1.1 (He)
?
6.1 (He) (the balcony)
?
11.1 (He) (over the balcony)
?
12.1 (He) (over the balcony)
12.2 (He) 
12.3 (He) (over the balcony) (again)
..
(3)
Semantic Role Classification: apply ME classification into 120 semantic roles and get n-best output for each
1.1.1 Agent (He)
?.
6.1.1 Agent (He), BodyPart (the balcony)
?.
12.1.1 Agent (He), Goal (over the balcony)
12.1.2 Agent (He), Path (over the balcony)
12.1.3 Self-mover (He), Goal (over the balcony)
?.
Re-ranking : apply ME re-ranking and choose the best one from long lists
Final Output Agent (He), Path (over the balcony)
 
Fig. 3.  The framework of the re-ranking method with an actual system output.  (1) contains different number 
of segmentations depending on each sentence, (2) has mn number of lists when we obtain m possible 
segmentations in (1) and we get n-best FE identifications, (3) has mnn number of lists when we get n-best 
role classifications given mn lists (4) shows finally chosen output. 
Table 6 shows the performance of re-ranking.  
To evaluate the performance of top-n, the best 
tagging output for a sentence is chosen among n-
lists and the performance is computed for that list.  
The top-5 lists show two interesting points: one is 
that precision is very high, and the other is that F-
score including role tagging is not much different 
from F-score of only FE identification.  In other 
words, there are a few (not 120) confusing roles for 
a given frame element, and we have many frame 
elements that are not identified even in n-best lists. 
FE identification FE identification  & Role tagging Re-rank 
Prec Rec F Prec Rec F 
Top-1 77.4 66.0 71.2 66.7 57.0 61.5 
Top-2 81.8 69.2 75.0 75.6 64.0 69.4 
Top-5 86.8 72.4 78.0 83.7 69.9 76.2 
Table 6. Re-ranking performance for test set 
To improve our re-ranker, more features 
regarding these problems should be added, and a 
more principled method to obtain the probability of 
segmenations, p(seg) in Sectioin 5.1, needs to be 
investigated. 
Table 7 compares the final output with G & J?s 
best result.  Our model is slightly worse than their 
integrated model, but it supports much further 
experimentation in segmentation and re-ranking. 
FE identification FE indetification & Role tagging Method 
Prec Rec F Prec Rec F 
G & J 
integrated 
model 
74.0 70.1 72.0 64.6 61.2 62.9 
Our 
model w/ 
re-ranking 
77.4 66.0 71.2 66.7 57.0 61.5 
Table 7. The final output for test set.  
6 Conclusion 
We describe a pipeline framework to analyze 
sentences into frame elements and semantic roles 
based on the FrameNet corpus.  The process 
includes four steps: sentence segmentation, FE 
identification, role classification, and final re-
ranking of the n-best outputs. 
In future work, we will investigate ways to 
reduce the gap between the five-best output 
performance and the single best output.  More 
features should be extracted to improve re-ranking 
accuracy.  Although the segmentation improves the 
performance, since the final output is dominated by 
the initial segmentation, we will explore a smart 
segmentation method, possibly one not even 
limited to constituents. 
In addition to the provided syntactic features, we 
will apply semantic features using ontology.  
Finally, the challenge is to apply this type of work 
to new predicates, ones not yet treated in 
FrameNet.  We are searching for methods to 
achieve this. 
References  
O. Bender, K. Macherey, F.J. Och, and H. Ney. 
2003. Comparison of Alignment Templates and 
Maximum Entropy Models for Natural Language 
Processing. Proc. of EACL-2003. Budapest, 
Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 
1996. A Maximum Entropy Approach to Natural 
Language Proc. of Computational Linguistics, 
vol. 22, no. 1. 
S.F. Chen and R. Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMUCS-99-108, Carnegie 
Mellon University. 
M. Collins. 1997. Three Generative, Lexicalized 
Models for Statistical Parsing. Proc. of the 35th 
Annual Meeting of the ACL. pages 16-23, 
Madrid, Spain. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-Linear Models.  Annals 
of Mathematical Statistics, 43:1470-1480. 
C.Fillmore 1976. Frame Semantics and the Nature 
of Language.  Annals of the New York Academy 
of Science Conference on the Origin and 
Development of Language and Speech, Volume 
280 (pp. 20-32). 
M. Fleischman, N. Kwon, and E. Hovy. 2003. 
Maximum Entropy Models for FrameNet 
Classification. Proc. of Empirical Methods in 
Natural Language Processing conference 
(EMNLP) 2003. Sapporo, Japan.  
D. Gildea and D. Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3) 245-288 14. 
K. Hacioglu, W. Ward. 2003. Target word 
detection and semantic role chunking using 
support vector machines. Proc. of HLT-NAACL 
2003, Edmonton, Canada. 
F.J. Och. 2002. Yet Another Maxent Toolkit: 
YASMET www-i6.informatik.rwth-aachen.de/ 
Colleagues/och/. 
S. Pradhan, K. Haciolgu, W. Ward, J. Martin, D. 
Jurafsky. 2003. Semantic Role Parsing: Adding 
Semantic Structure to Unstructured Text. Proc of 
of the International Conference on Data Mining 
(ICDM-2003), Melbourne, FL  
C. Thompson, R. Levy, and C. Manning. 2003. A 
Generative Model for FrameNet Semantic Role 
Labeling. Proc. of the Fourteenth European 
Conference on Machine Learning, Croatia 
Proceedings of NAACL HLT 2007, Companion Volume, pages 37?40,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Situated Models of Meaning for Sports Video Retrieval  
Michael Fleischman 
MIT Media Lab 
mbf@mit.edu 
Deb Roy 
MIT Media Lab 
dkroy@media.mit.edu 
 
Abstract 
Situated models of meaning ground words in the 
non-linguistic context, or situation, to which they 
refer.  Applying such models to sports video re-
trieval requires learning appropriate representa-
tions for complex events.  We propose a method 
that uses data mining to discover temporal pat-
terns in video, and pair these patterns with associ-
ated closed captioning text.  This paired corpus is 
used to train a situated model of meaning that sig-
nificantly improves video retrieval performance. 
1 Introduction 
Recent advances in digital broadcasting and re-
cording allow fans access to an unprecedented 
amount of sports video.  The growing need to 
manage and search large video collections presents 
a challenge to traditional information retrieval (IR) 
technologies.  Such methods cannot be directly 
applied to video data, even when closed caption 
transcripts are available; for, unlike text docu-
ments, the occurrence of a query term in a video is 
often not enough to assume the video?s relevance 
to that query.  For example, when searching 
through video of baseball games, returning all clips 
in which the phrase ?home run? occurs, results 
primarily in video of events where a home run 
does not actually occur.  This follows from the fact 
that in sports, as in life, people often talk not about 
what is currently happening, but rather, they talk 
about what did, might, or will happen in the future.   
Traditional IR techniques cannot address such 
problems because they model the meaning of a 
query term strictly by that term?s relationship to 
other terms.  To build systems that successfully 
search video, IR techniques should be extended to 
exploit not just linguistic information but also ele-
ments of the non-linguistic context, or situation, 
that surrounds language use.  This paper presents a 
method for video event retrieval from broadcast 
sports that achieves this by learning a situated 
model of meaning from an unlabeled video corpus. 
The framework for the current model is derived 
from previous work on computational models of 
verb learning (Fleischman & Roy, 2005).  In this 
earlier work, meaning is defined by a probabilistic 
mapping between words and representations of the 
non-linguistic events to which those words refer.  
In applying this framework to events in video, we 
follow recent work on video surveillance in which 
complex events are represented as temporal rela-
tions between lower level sub-events (Hongen et 
al., 2004).  While in the surveillance domain, hand 
crafted event representations have been used suc-
cessfully, the greater variability of content in 
broadcast sports demands an automatic method for 
designing event representations.   
The primary focus of this paper is to present a 
method for mining such representations from large 
video corpora, and to describe how these represen-
tations can be mapped to natural language.  We 
focus on a pilot dataset of broadcast baseball 
games.  Pilot video retrieval tests show that using a 
situated model significantly improves perform-
ances over traditional language modeling methods. 
2 Situated Models of Meaning 
Building situated models of meaning operates in 
three phases (see Figure 1): first, raw video data is 
abstracted into multiple streams of discrete fea-
tures.  Temporal data mining techniques are then 
applied to these feature streams to discover hierar-
chical temporal patterns.  These temporal patterns 
form the event representations that are then 
mapped to words from the closed caption stream. 
2.1 Feature Extraction  
The first step in representing events in video is to 
abstract the very high dimensional raw video data 
into more semantically meaningful streams of in-
formation.  Ideally, these streams would corre-
spond to basic events that occur in sports video 
(e.g., hitting, throwing, catching, kicking, etc.). 
Due to the limitations of computer vision tech-
niques, extracting such ideal features is often in-
feasible.  However, by exploiting the ?language of
37
Figure 1.  Video processing pipeline for learning situated models of meaning. 
 
film? that is used to produce sports video, informa-
tive features can be extracted that are also easy to 
compute.  Thus, although we cannot easily identify 
a player hitting the ball, we can easily detect fea-
tures that correlate with hitting: e.g., when a scene 
focusing on the pitching mound immediately 
jumps to one zooming in on the field (Figure 1).  
While such correlations are not perfect, pilot tests 
show that baseball events can be classified using 
such features (Fleischman et. al., in prep). 
Importantly, this is the only phase of our frame-
work that is domain specific; i.e., it is the only as-
pect of the framework designed specifically for use 
with baseball data.  Although many feature types 
can be extracted, we focus on only two feature 
types: visual context, and camera motion. 
 
Visual Context 
 
Visual context features encode general properties 
of the visual scene in a video segment.  The first 
step in extracting such features is to split the raw 
video into ?shots? based on changes in the visual 
scene due to editing (e.g., jumping from a close up 
of the pitcher to a wide angle of the field).  Shot 
detection is a well studied problem in multimedia 
research; in this work, we use the method of 
Tardini et al (2005) because of its speed and 
proven performance on sports video.   
After a game is segmented into shots, each shot 
is categorized into one of three categories: pitch-
ing-scene, field-scene, or other.  Categorization is 
based on image features (e.g., color histograms, 
edge detection, motion analysis) extracted from an 
individual key frame chosen from that shot.  A de-
cision tree is trained (with bagging and boosting) 
using the WEKA machine learning toolkit that 
achieves over 97% accuracy on a held out dataset.  
Camera Motion 
 
Whereas visual context features provide informa-
tion about the global situation that is being ob-
served, camera motion features afford more precise 
information about the actions occurring in the 
video.  The intuition here is that the camera is a 
stand in for a viewer?s focus of attention.  As ac-
tion in the video takes place, the camera moves to 
follow it, mirroring the action itself, and providing 
an informative feature for event representation.   
Detecting camera motion (i.e., pan/tilt/zoom) is a 
well-studied problem in video analysis.  We use 
the system of (Bouthemy et al, 1999) which com-
putes the pan, tilt, and zoom motions using the pa-
rameters of a two-dimensional affine model fit to 
every pair of sequential frames in a video segment.  
The output of this system is then clustered into 
characteristic camera motions (e.g. zooming in fast 
while panning slightly left) using a 1st order Hid-
den Markov Model  with 15 states, implemented 
using the Graphical Modeling Toolkit (GMTK).   
2.2 Temporal Pattern Mining 
In this step, temporal patterns are mined from the 
features abstracted from the raw video data.  As 
described above, ideal semantic features (such as 
hitting and catching) cannot be extracted easily 
from video. We hypothesize that finding temporal 
patterns between scene and camera motion features 
can produce representations that are highly corre-
lated with sports events.  Importantly, such tempo-
ral patterns are not strictly sequential, but rather, 
are composed of features that can occur in complex 
and varied temporal relations to each other.  For 
example, Figure 1 shows the representation for a 
fly ball event that is composed of: a camera pan-
38
ning up followed by a camera pan down, occurring 
during a field scene, and before a pitching scene. 
Following previous work in video content classi-
fication (Fleischman et al, 2006), we use tech-
niques from temporal data mining to discover 
event patterns from feature streams.  The algorithm 
we use is fully unsupervised. It processes feature 
streams by examining the relations that occur be-
tween individual features within a moving time 
window.  Following Allen (1984), any two features 
that occur within this window must be in one of 
seven temporal relations with each other (e.g. be-
fore, during, etc.).  The algorithm keeps track of 
how often each of these relations is observed, and 
after the entire video corpus is analyzed, uses chi-
square analyses to determine which relations are 
significant.  The algorithm iterates through the 
data, and relations between individual features that 
are found significant in one iteration (e.g. 
[BEFORE, camera panning up, camera panning 
down]), are themselves treated as individual fea-
tures in the next.  This allows the system to build 
up higher-order nested relations in each iteration 
(e.g. [DURING, [BEFORE, camera panning up, 
camera panning down], field scene]]).  The tempo-
ral patterns found significant in this way are then 
used as the event representations that are then 
mapped to words. 
2.3 Linguistic Mapping 
The last step in building a situated model of mean-
ing is to map words onto the representations of 
events mined from the raw video.  We equate the 
learning of this mapping to the problem of estimat-
ing the conditional probability distribution of a 
word given a video event representation.  Similar 
to work in image retrieval (Barnard et al, 2003), 
we cast the problem in terms of Machine Transla-
tion: given a paired corpus of words and a set of 
video event representations to which they refer, we 
make the IBM Model 1 assumption and use the 
expectation-maximization method to estimate the 
parameters (Brown et al, 1993):   
?
=
+
=
m
j
ajm jvideowordpl
C
videowordp
1
)|()1()|(
     (1) 
This paired corpus is created from a corpus of 
raw video by first abstracting each video into the 
feature streams described above.  For every shot 
classified as a pitching scene, a new instance is 
created in the paired corpus corresponding to an 
event that starts at the beginning of that shot and 
ends exactly four shots after.  This definition of an 
event follows from the fact that most events in 
baseball must start with a pitch and usually do not 
last longer than four shots (Gong et al, 2004).   
For each of these events in the paired corpus, a 
representation of the video is generated by match-
ing all patterns (and the nested sub-patterns) found 
from temporal mining to the feature streams of the 
event.  These video representations are then paired 
with all the words from the closed captioning that 
occur during that event (plus/minus 10 seconds).   
3 Experiments 
Work on video IR in the news domain often fo-
cuses on indexing video data using a set of image 
classifiers that categorize shots into pre-determined 
concepts (e.g. flag, outdoors, George Bush, etc.).  
Text queries must then be translated (sometimes 
manually) in terms of these concepts (Worring & 
Snoek, 2006).  Our work focuses on a more auto-
mated approach that is closer to traditional IR tech-
niques.  Our framework extends the language 
modeling approach of Ponte and Croft (1998) by 
incorporating a situated model of meaning.   
In Ponte and Croft (1998), documents relevant to 
a query are ranked based on the probability that 
each document generated each query term.  We 
follow this approach for video events, making the 
assumption that the relevance of an event to a 
query depends both on the words associated with 
the event (i.e. what was said while the event oc-
curred), as well as the situational context modeled 
by the video event representations: 
? ??=
query
word
videowordpcaptionwordpeventqueryp )1()|()|()|( ??  (2) 
The p(word|caption) is estimated using the lan-
guage modeling technique described in Ponte and 
Croft (1998).  The p(word|video) is estimated as in 
equation 1 above.  ? is used to weight the models.  
 
Data 
 
The system has been evaluated on a pilot set of 6 
broadcast baseball games totaling about 15 hours 
and 1200 distinct events.  The data represents 
video of 9 different teams, at 4 different stadiums, 
broadcast on 4 different stations.  Highlights (i.e., 
events which terminate with the player either out 
or safe) were hand annotated, and categorized ac-
cording to the type of the event (e.g., strikeout vs. 
homerun), the location of the event (e.g., right field 
vs. infield), and the nature of the event (e.g., fly 
ball vs. line drive).  Each of these categories was 
39
used to automatically select query terms to be used 
in testing.  Similar to Berger & Lafferty (1999), the 
probability distribution of terms given a category is 
estimated using a normalized log-likelihood ratio 
(Moore, 2004), and query terms are sampled ran-
domly from this distribution.  This gives us a set of 
queries for each annotated category (e.g., strikeout: 
?miss, chasing?; flyball: ?fly, streak?).  Although 
much noisier than human produced queries, this 
procedure generates a large amount of test queries 
for which relevant results can easily be determined 
(e.g., if a returned event for the query ?fly, streak? 
is of the flyball category, it is marked relevant). 
 Experiments are reported using 6-fold cross 
validation during which five games are used to 
train the situated model while the sixth is held out 
for testing.  Because data is sparse, the situation 
model is trained only on the hand annotated high-
light events.  However, retrieval is always tested 
using both highlight and non-highlight events.  
Figure 2.  Effect of situated model on video IR. 
 
 Results 
 
Figure 2 shows results for 520 automatically gen-
erated queries of one to four words in length.  
Mean average precision (MAP), a common metric 
that combines elements of precision, recall, and 
ranking, is used to measure the relevance of the top 
five results returned for each query.  We show re-
sults for the system using only linguistic informa-
tion (i.e. ?=1), only non-linguistic information (i.e. 
?=0), and both information together (i.e. ?=0.5).   
The poor performance of the system using only 
non-linguistic information is expected given the 
limited training data and the simple features used 
to represent events.  Interestingly, using only lin-
guistic information produces similarly poor per-
formance.  This is a direct result of announcers? 
tendency to discuss topics not currently occurring 
in the video.  By combining text and video analy-
ses, though, the system performs significantly bet-
ter (p<0.01) by determining when the observed 
language actually refers to the situation at hand.  
4 Conclusion 
We have presented a framework for video retrieval 
that significantly out-performs traditional IR meth-
ods applied to closed caption text. Our new ap-
proach incorporates the visual content of baseball 
video using automatically learned event represen-
tations to model the situated meaning of words. 
Results indicate that integration of situational con-
text dramatically improves performance over tradi-
tional methods alone.  In future work we will 
examine the effects of applying situated models of 
meaning to other tasks (e.g., machine translation).   
References 
Allen, J.F. (1984). A General Model of Action and Time. Arti-
ficial Intelligence. 23(2). 
Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei, D, 
and Jordan, M. (2003), "Matching Words and Pictures," 
Journal of Machine Learning Research, Vol 3. 
Berger, A.  and Lafferty, J. (1999). Information Retrieval as 
Statistical Translation. In Proceedings of SIGIR-99. 
Bouthemy, P., Gelgon, M., Ganansia, F. (1999). A unified 
approach to shot change detection and camera motion char-
acterization. IEEE Trans. on Circuits and Systems for Video 
Technology, 9(7):1030-1044. 
Brown, P., Della Pietra, S., Della Pietra, V. Mercer, R. (1993). 
The mathematics of machine translation: Parameter estima-
tion. Computational Linguistics, 19(10). 
Fleischman, M. and Roy, D. (2005). Intentional Context in 
Situated Language Learning.  In Proc. of 9th Conference on 
Comp. Natural Language Learning. 
Fleischman, M., DeCamp, P. Roy, D.  (2006). Mining Tempo-
ral Patterns of Movement for Video Content Classification.  
The 8th ACM SIGMM International Workshop on Multi-
media Information Retrieval. 
Fleischman, M., Roy, B., Roy, D. (in prep.). Automated Fea-
ture Engineering inBaseball Highlight Classification. 
Gong, Y., Han, M., Hua, W., Xu, W.  (2004). Maximum en-
tropy model-based baseball highlight detection and classifi-
cation.  Computer Vision and Image Understanding. 96(2). 
Hongen, S., Nevatia, R. Bremond, F. (2004). Video-based 
event recognition: activity representation and probabilistic 
recognition methods.  Computer Vision and Image Under-
standing. 96(2). pp: 129 - 162  
Moore, Robert C. (2004). Improving IBM Word Alignment 
Model 1. in Proc. of 42nd ACL.  
Ponte, J.M., and Croft, W.B. (1998). A Language Modeling 
Approach to Information Retrieval. In Proc. of SIGIR?98.  
Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005). Shot 
Detection and Motion Analysis for Automatic MPEG-7 
Annotation of Sports Videos.  In 13th International Confer-
ence on Image Analysis and Processing. 
Worring, M., Snoek, C.. (2006). Semantic Indexing and Re-
trieval of Video.  Tutorial at ACM Multimedia 
40
Proceedings of ACL-08: HLT, pages 121?129,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Grounded Language Modeling for 
Automatic Speech Recognition of Sports Video  
Michael Fleischman 
Massachusetts Institute of Technology 
Media Laboratory 
mbf@mit.edu 
Deb Roy 
Massachusetts Institute of Technology 
Media Laboratory 
dkroy@media.mit.edu 
 
 
Abstract 
Grounded language models represent the rela-
tionship between words and the non-linguistic 
context in which they are said.  This paper de-
scribes how they are learned from large cor-
pora of unlabeled video, and are applied to the 
task of automatic speech recognition of sports 
video.  Results show that grounded language 
models improve perplexity and word error 
rate over text based language models, and fur-
ther, support video information retrieval better 
than human generated speech transcriptions. 
1 Introduction 
Recognizing speech in broadcast video is a neces-
sary precursor to many multimodal applications 
such as video search and summarization (Snoek 
and Worring, 2005;).  Although performance is 
often reasonable in controlled environments (such 
as studio news rooms), automatic speech recogni-
tion (ASR) systems have significant difficulty in 
noisier settings (such as those found in live sports 
broadcasts) (Wactlar et al, 1996).  While many 
researches have examined how to compensate for 
such noise using acoustic techniques, few have 
attempted to leverage information in the visual 
stream to improve speech recognition performance 
(for an exception see Murkherjee and Roy, 2003).   
In many types of video, however, visual context 
can provide valuable clues as to what has been 
said.  For example, in video of Major League 
Baseball games, the likelihood of the phrase ?home 
run? increases dramatically when a home run has 
actually been hit.  This paper describes a method 
for incorporating such visual information in an 
ASR system for sports video.  The method is based 
on the use of grounded language models to repre-
sent the relationship between words and the non-
linguistic context to which they refer (Fleischman 
and Roy, 2007).   
Grounded language models are based on re-
search from cognitive science on grounded models 
of meaning. (for a review see Roy, 2005, and Roy 
and Reiter, 2005).  In such models, the meaning of 
a word is defined by its relationship to representa-
tions of the language users? environment.  Thus, 
for a robot operating in a laboratory setting, words 
for colors and shapes may be grounded in the out-
puts of its computer vision system (Roy & Pent-
land, 2002); while for a simulated agent operating 
in a virtual world, words for actions and events 
may be mapped to representations of the agent?s 
plans or goals (Fleischman & Roy, 2005).   
This paper extends previous work on grounded 
models of meaning by learning a grounded lan-
guage model from naturalistic data collected from 
broadcast video of Major League Baseball games.  
A large corpus of unlabeled sports videos is col-
lected and paired with closed captioning transcrip-
tions of the announcers? speech. 1  This corpus is 
used to train the grounded language model, which 
like traditional language models encode the prior 
probability of words for an ASR system.  Unlike 
traditional language models, however, grounded 
language models represent the probability of a 
word conditioned not only on the previous word(s), 
but also on features of the non-linguistic context in 
which the word was uttered.   
Our approach to learning grounded language 
models operates in two phases.  In the first phase, 
events that occur in the video are represented using 
hierarchical temporal pattern automatically mined  
                                                          
1
 Closed captioning refers to human transcriptions of speech 
embedded in the video stream primarily for the hearing im-
paired.  Closed captioning is reasonably accurate (although not 
perfect) and available on some, but not all, video broadcasts. 
121
 Figure 1.  Representing events in video.  a) Events are represented by first abstracting the raw video into visual con-
text, camera motion, and audio context features.  b) Temporal data mining is then used to discover hierarchical tem-
poral patterns in the parallel streams of features.  c) Temporal patterns found significant in each iteration are stored 
in a codebook that is used to represent high level events in video. 
 
from low level features.  In the second phase, a 
conditional probability distribution is estimated 
that describes the probability that a word was ut-
tered given such event representations. In the fol-
lowing sections we describe these two aspects of 
our approach and evaluate the performance of our 
grounded language model on a speech recognition 
task using video highlights from Major League 
Baseball games.  Results indicate improved per-
formance using three metrics: perplexity, word 
error rate, and precision on an information retrieval 
task. 
2 Representing Events in Sports Video 
Recent work in video surveillance has demon-
strated the benefit of representing complex events 
as temporal relations between lower level sub-
events (Hongen et al, 2004).  Thus, to represent 
events in the sports domain, we would ideally first 
represent the basic sub events that occur in sports 
video (e.g., hitting, throwing, catching, running, 
etc.) and then build up complex events (such as 
home run) as a set of temporal relations between 
these basic events.  Unfortunately, due to the limi-
tations of computer vision techniques, reliably 
identifying such basic events in video is not feasi-
ble.  However, sports video does have characteris-
tics that can be exploited to effectively represent 
complex events. 
Like much broadcast video, sports video is 
highly produced, exploiting many different camera 
angles and a human director who selects which 
camera is most appropriate given what is happen-
ing on the field.  The styles that different directors 
employ are extremely consistent within a sport and 
make up a ?language of film? which the machine 
can take advantage of in order to represent the 
events taking place in the video. 
Thus, even though it is not easy to automati-
cally identify a player hitting a ball in video, it is 
easy to detect features that correlate with hitting, 
e.g., when a scene focusing on the pitching mound 
immediately jumps to one zooming in on the field 
(see Figure 1).  Although these correlations are not 
perfect, experiments have shown that baseball 
events can be classified using such features 
(Fleischman et al, 2007).   
We exploit the language of film to represent 
events in sports video in two phases.  First, low 
level features that correlate with basic events in 
sports are extracted from the video stream.  Then, 
temporal data mining is used to find patterns 
within this low level event stream.   
2.1 Feature Extraction 
We extract three types of features: visual con-
text features, camera motion features, and audio 
context features.   
122
Visual Context Features 
Visual context features encode general proper-
ties of the visual scene in a video segment.  Super-
vised classifiers are trained to identify these 
features, which are relatively simple to classify in 
comparison to high level events (like home runs) 
that require more training data and achieve lower 
accuracy.  The first step in classifying visual con-
text features is to segment the video into shots (or 
scenes) based on changes in the visual scene due to 
editing (e.g. jumping from a close up to a wide 
shot of the field).  Shot detection and segmentation 
is a well studied problem; in this work we use the 
method of Tardini et al (2005).   
After the video is segmented into shots, indi-
vidual frames (called key frames) are selected and 
represented as a vector of low level features that 
describe the key frame?s color distribution, en-
tropy, etc. (see Fleischman and Roy, 2007 for the 
full list of low level features used).  The WEKA 
machine learning package is used to train a boosted 
decision tree to classify these frames into one of 
three categories: pitching-scene, field-scene, other 
(Witten and Frank, 2005).  Those shots whose key 
frames are classified as field-scenes are then sub-
categorized (using boosted decision trees) into one 
of the following categories: infield, outfield, wall, 
base, running, and misc.  Performance of these 
classification tasks is approximately 96% and 90% 
accuracy respectively. 
Camera Motion Features 
In addition to visual context features, we also 
examine the camera motion that occurs within a 
video.  Unlike visual context features, which pro-
vide information about the global situation that is 
being observed, camera motion features represent 
more precise information about the actions occur-
ring in a video.  The intuition here is that the cam-
era is a stand in for a viewer?s focus of attention.  
As actions occur in a video, the camera moves to 
follow it; this camera motion thus mirrors the ac-
tions themselves, providing informative features 
for event representation.   
Like shot boundary detection, detecting the mo-
tion of the camera in a video (i.e., the amount it 
pans left to right, tilts up and down, and zooms in 
and out) is a well-studied problem.  We use the 
system of Bouthemy et al (1999) which computes 
the camera motion using the parameters of a two-
dimensional affine model to fit every pair of se-
quential frames in a video.  A 15 state 1st order 
Hidden Markov Model, implemented with the 
Graphical Modeling Toolkit,2 then converts the 
output of the Bouthemy system into a stream of 
clustered characteristic camera motions (e.g. state 
12 clusters together motions of zooming in fast 
while panning slightly left). 
Audio Context 
The audio stream of a video can also provide use-
ful information for representing non-linguistic con-
text.  We use boosted decision trees to classify 
audio into segments of speech, excited_speech, 
cheering, and music.  Classification operates on a 
sequence of overlapping 30 ms frames extracted 
from the audio stream. For each frame, a feature 
vector is computed using, MFCCs (often used in 
speaker identification and speech detection tasks), 
as well as energy, the number of zero crossings, 
spectral entropy, and relative power between dif-
ferent frequency bands.  The classifier is applied to 
each frame, producing a sequence of class labels. 
These labels are then smoothed using a dynamic 
programming cost minimization algorithm (similar 
to those used in Hidden Markov Models).  Per-
formance of this system achieves between 78% 
and 94% accuracy.   
2.2 Temporal Pattern Mining 
Given a set of low level features that correlate with 
the basic events in sports, we can now focus on 
building up representations of complex events.  
Unlike previous work (Hongen et al, 2005) in 
which representations of the temporal relations 
between low level events are built up by hand, we 
employ temporal data mining techniques to auto-
matically discover such relations from a large cor-
pus of unannotated video. 
As described above, ideal basic events (such as 
hitting and catching) cannot be identified easily in 
sports video. By finding temporal patterns between 
audio, visual and camera motion features, how-
ever, we can produce representations that are 
highly correlated with sports events.  Importantly, 
such temporal patterns are not strictly sequential, 
but rather, are composed of features that can occur 
                                                          
2
 http://ssli.ee.washington.edu/~bilmes/gmtk/ 
123
in complex and varied temporal relations to each 
other.   
To find such patterns automatically, we follow 
previous work in video content classification in 
which temporal data mining techniques are used to 
discover event patterns within streams of lower 
level features.  The algorithm we use is fully unsu-
pervised and proceeds by examining the relations 
that occur between features in multiple streams 
within a moving time window.  Any two features 
that occur within this window must be in one of 
seven temporal relations with each other (e.g. be-
fore, during, etc.) (Allen, 1984).  The algorithm 
keeps track of how often each of these relations is 
observed, and after the entire video corpus is ana-
lyzed, uses chi-square analyses to determine which 
relations are significant.  The algorithm iterates 
through the data, and relations between individual 
features that are found significant in one iteration 
(e.g. [OVERLAP, field-scene, cheer]), are them-
selves treated as individual features in the next.  
This allows the system to build up higher-order 
nested relations in each iteration (e.g. [BEFORE, 
[OVERLAP, field-scene, cheer], field scene]]).   
The temporal patterns found significant in this 
way make up a codebook which can then be used 
as a basis for representing a video.  The term code-
book is often used in image analysis to describe a 
set of features (stored in the codebook) that are 
used to encode raw data (images or video).  Such 
codebooks are used to represent raw video using 
features that are more easily processed by the 
computer.  
Our framework follows a similar approach in 
which raw video is encoded (using a codebook of 
temporal patterns) as follows.  First, the raw video 
is abstracted into the visual context, camera mo-
tion, and audio context feature streams (as de-
scribed in Section 2.1).  These feature streams are 
then scanned, looking for any temporal patterns 
(and nested sub-patterns) that match those found in 
the codebook.  For each pattern, the duration for 
which it occurs in the feature streams is treated as 
the value of an element in the vector representation 
for that video.   
Thus, a video is represented as an n length vec-
tor, where n is the total number of temporal pat-
terns in the codebook.  The value of each element 
of this vector is the duration for which the pattern 
associated with that element was observed in the 
video.  So, if a pattern was not observed in a video 
at all, it would have a value of 0, while if it was 
observed for the entire length of the video, it would 
have a value equal to the number of frames present 
in that video.   
Given this method for representing the non-
linguistic context of a video, we can now examine 
how to model the relationship between such con-
text and the words used to describe it.  
3 Linguistic Mapping 
Modeling the relationship between words and non-
linguistic context assumes that the speech uttered 
in a video refers consistently (although not exclu-
sively) to the events being represented by the tem-
poral pattern features.  We model this relationship, 
much like traditional language models, using con-
ditional probability distributions.  Unlike tradi-
tional language models, however, our grounded 
language models condition the probability of a 
word not only on the word(s) uttered before it, but 
also on the temporal pattern features that describe 
the non-linguistic context in which it was uttered.  
We estimate these conditional distributions using a 
framework similar that used for training acoustic 
models in ASR and translation models in Machine 
Translation (MT). 
We generate a training corpus of utterances 
paired with representations of the non-linguistic 
context in which they were uttered.  The first step 
in generating this corpus is to generate the low 
level features described in Section 2.1 for each 
video in our training set.  We then segment each 
video into a set of independent events based on the 
visual context features we have extracted.  We fol-
low previous work in sports video processing 
(Gong et al, 2004) and define an event in a base-
ball video as any sequence of shots starting with a 
pitching-scene and continuing for four subsequent 
shots.  This definition follows from the fact that the 
vast majority of events in baseball start with a 
pitch and do not last longer than four shots.  For 
each of these events in our corpus, a temporal pat-
tern feature vector is generated as described in sec-
tion 2.2.  These events are then paired with all the 
words from the closed captioning transcription that 
occur during each event (plus or minus 10 sec-
onds).  Because these transcriptions are not neces-
sarily time synched with the audio, we use the 
method described in Hauptmann and Witbrock 
124
(1998) to align the closed captioning to the an-
nouncers? speech.   
Previous work has examined applying models 
often used in MT to the paired corpus described 
above (Fleischman and Roy, 2006).  Recent work 
in automatic image annotation (Barnard et al, 
2003; Blei and Jordan, 2003) and natural language 
processing (Steyvers et al, 2004), however, have 
demonstrated the advantages of using hierarchical 
Bayesian models for related tasks.  In this work we 
follow closely the Author-Topic (AT) model (Stey-
vers et al, 2004) which is a generalization of La-
tent Dirichlet Allocation (LDA) (Blei et al, 2005).3   
LDA is a technique that was developed to 
model the distribution of topics discussed in a large 
corpus of documents.  The model assumes that 
every document is made up of a mixture of topics, 
and that each word in a document is generated 
from a probability distribution associated with one 
of those topics.  The AT model generalizes LDA, 
saying that the mixture of topics is not dependent 
on the document itself, but rather on the authors 
who wrote it.  According to this model, for each 
word (or phrase) in a document, an author is cho-
sen uniformly from the set of the authors of the 
document.  Then, a topic is chosen from a distribu-
tion of topics associated with that particular author.  
Finally, the word is generated from the distribution 
associated with that chosen topic.  We can express 
the probability of the words in a document (W) 
given its authors (A) as: 
? ??
? ? ?
=
Wm Ax Tzd
xzpzmp
A
AWp )|()|(1)|(  (1) 
where T is the set of latent topics that are induced 
given a large set of training data.   
We use the AT model to estimate our grounded 
language model by making an analogy between 
documents and events in video.  In our framework, 
the words in a document correspond to the words 
in the closed captioning transcript associated with 
an event.  The authors of a document correspond to 
the temporal patterns representing the non- 
 
linguistic context of that event.  We modify the AT 
model slightly, such that, instead of selecting from 
                                                          
3
 In the discussion that follows, we describe a method for es-
timating unigram grounded language models.  Estimating 
bigram and trigram models can be done by processing on 
word pairs or triples, and performing normalization on the 
resulting conditional distributions. 
a uniform distribution (as is done with authors of 
documents), we select patterns from a multinomial 
distribution based upon the duration of the pattern.  
The intuition here is that patterns that occur for a 
longer duration are more salient and thus, should 
be given greater weight in the generative process.  
We can now rewrite (1) to give the probability of 
words during an event (W) given the vector of ob-
served temporal patterns (P) as: 
???
? ? ?
=
Wm Px Tz
xpxzpzmpPWp )()|()|()|(  (2) 
In the experiments described below we follow 
Steyver et al, (2004) and train our AT model using 
Gibbs sampling, a Markov Chain Monte Carlo 
technique for obtaining parameter estimates.  We 
run the sampler on a single chain for 200 iterations.  
We set the number of topics to 15, and normalize 
the pattern durations first by individual pattern 
across all events, and then for all patterns within an 
event.  The resulting parameter estimates are 
smoothed using a simple add N smoothing tech-
nique, where N=1 for the word by topic counts and 
N=.01 for the pattern by topic counts.   
4 Evaluation 
In order to evaluate our grounded language model-
ing approach, a parallel data set of 99 Major 
League Baseball games with corresponding closed 
captioning transcripts was recorded from live tele-
vision.  These games represent data totaling ap-
proximately 275 hours and 20,000 distinct events 
from 25 teams in 23 stadiums, broadcast on five 
different television stations.  From this set, six 
games were held out for testing (15 hours, 1200 
events, nine teams, four stations).  From this test 
set, baseball highlights (i.e., events which termi-
nate with the player either out or safe) were hand 
annotated for use in evaluation, and manually tran-
scribed in order to get clean text transcriptions for 
gold standard comparisons.  Of the 1200 events in 
the test set, 237 were highlights with a total word 
count of 12,626 (vocabulary of 1800 words). 
The remaining 93 unlabeled games are used to 
train unigram, bigram, and trigram grounded lan-
guage models.  Only unigrams, bigrams, and tri-
grams that are not proper names, appear greater 
than three times, and are not composed only of 
stop words were used.  These grounded language 
models are then combined in a backoff strategy 
125
with traditional unigram, bigram, and trigram lan-
guage models generated from a combination of the 
closed captioning transcripts of all training games 
and data from the switchboard corpus (see below).  
This backoff is necessary to account for the words 
not included in the grounded language model itself 
(i.e. stop words, proper names, low frequency 
words).  The traditional text-only language models 
(which are also used below as baseline compari-
sons) are generated with the SRI language model-
ing toolkit (Stolcke, 2002) using Chen and 
Goodman's modified Kneser-Ney discounting and 
interpolation (Chen and Goodman, 1998).  The 
backoff strategy we employ here is very simple: if 
the ngram appears in the GLM then it is used, oth-
erwise the traditional LM is used.  In future work 
we will examine more complex backoff strategies 
(Hsu, in review). 
We evaluate our grounded language modeling 
approach using 3 metrics: perplexity, word error 
rate, and precision on an information retrieval task. 
4.1 Perplexity 
Perplexity is an information theoretic measure of 
how well a model predicts a held out test set.  We 
use perplexity to compare our grounded language 
model to two baseline language models: a lan-
guage model generated from the switchboard cor-
pus, a commonly used corpus of spontaneous 
speech in the telephony domain (3.65M words; 27k 
vocab); and a language model that interpolates 
(with equal weight given to both) between the 
switchboard model and a language model trained 
only on the baseball-domain closed captioning 
(1.65M words; 17k vocab).  The results of calculat-
ing perplexity on the test set highlights for these 
three models is presented in Table 1 (lower is bet-
ter). 
Not surprisingly, the switchboard language 
model performs far worse than both the interpo-
lated text baseline and the grounded language 
model.  This is due to the large discrepancy be-
tween both the style and vocabulary of language 
about sports compared to the domain of telephony 
sampled by the switchboard corpus.  Of more in-
terest is the decrease in perplexity seen when using 
the grounded language model compared to the in-
terpolated model.  Note that these two language 
models are generated using the same speech tran-
scriptions, i.e. the closed captioning from the train-
ing games and the switchboard corpus.  However, 
whereas the baseline model remains the same for 
each of the 237 test highlights, the grounded lan-
guage model generates different word distributions 
for each highlight depending on the event features 
extracted from the highlight video. 
  
 Switchboard Interpolated 
(Switch+CC) 
Grounded 
ppl 1404 145.27 83.88 
 
Table 1.  Perplexity measures for three different lan-
guage models on a held out test set of baseball high-
lights (12,626 words).  We compare the grounded 
language model to two text based language models: one 
trained on the switchboard corpus alone; and interpo-
lated with one trained on closed captioning transcrip-
tions of baseball video.  
4.2 Word Accuracy and Error Rate 
Word error rate (WER) is a normalized measure of 
the number of word insertions, substitutions, and 
deletions required to transform the output tran-
scription of an ASR system to a human generated 
gold standard transcription of the same utterance.  
Word accuracy is simply the number of words in 
the gold standard that they system correctly recog-
nized.  Unlike perplexity which only evaluates the 
performance of language models, examining word 
accuracy and error rate requires running an entire 
ASR system, i.e. both the language and acoustic 
models.   
We use the Sphinx system to train baseball specific 
acoustic models using parallel acoustic/text data 
automatically mined from our training set.  Follow-
ing Jang and Hauptman (1999), we use an off the 
shelf acoustic model (the hub4 model) to generate 
an extremely noisy speech transcript of each game 
in our training set, and use dynamic programming 
to align these noisy outputs to the closed caption-
ing stream for those same games.  Given these two 
transcriptions, we then generate a paired acous-
tic/text corpus by sampling the audio at the time 
codes where the ASR transcription matches the 
closed captioning transcription.   
For example, if the ASR output contains the 
term sequence ?? and farther home run for David 
forty says?? and the closed captioning contains 
the sequence ??another home run for David 
Ortiz?,? the matched phrase ?home run for 
David? is assumed a correct transcription for the 
audio at the time codes given by the ASR system.  
Only looking at sequences of three words or more,  
126
76.6
80.3
89.6
70
75
80
85
90
95
switchboard interpolated grounded
W
o
rd
 
Er
ro
r 
R
at
e
 
(W
ER
)
31.3
25.4
15.1
0
5
10
15
20
25
30
35
switchboard interpolated grounded
W
o
rd
 
A
c
cu
ra
c
y 
(%
)
 
Figure 3.  Word accuracy and error rates for ASR sys-
tems using a grounded language model, a text based 
language model trained on the switchboard corpus, and 
the switchboard model interpolated with a text based 
model trained on baseball closed captions. 
 
we extract approximately 18 hours of clean paired 
data from our 275 hour training corpus.  A con-
tinuous acoustic model with 8 gaussians and 6000 
ties states is trained on this data using the Sphinx 
speech recognizer.4 
Figure 3 shows the WERs and accuracy for 
three ASR systems run using the Sphinx decoder 
with the acoustic model described above and either 
the grounded language model or the two baseline 
models described in section 4.1.  Note that per-
formance for all of these systems is very poor due 
to limited acoustic data and the large amount of 
background crowd noise present in sports video 
(and particularly in sports highlights).  Even with 
this noise, however, results indicate that the word 
accuracy and error rates when using the grounded 
language model is significantly better than both the 
switchboard model (absolute WER reduction of 
13%; absolute accuracy increase of 15.2%) and the 
switchboard interpolated with the baseball specific 
text based language model (absolute WER reduc-
tion of 3.7%; absolute accuracy increase of 5.9%).   
                                                          
4
 http://cmusphinx.sourceforge.net/html/cmusphinx.php 
Drawing conclusions about the usefulness of 
grounded language models using word accuracy or 
error rate alone is difficult.  As it is defined, these 
measures penalizes a system that mistakes ?a? for 
?uh? as much as one that mistakes ?run? for ?rum.?  
When using ASR to support multimedia applica-
tions (such as search), though, such substitutions 
are not of equal importance.  Further, while visual 
information may be useful for distinguishing the 
latter error, it is unlikely to assist with the former.  
Thus, in the next section we examine an extrinsic 
evaluation in which grounded language models are 
judged not directly on their effect on word accu-
racy or error rate, but based on their ability to sup-
port video information retrieval.  
4.3 Precision of Information Retrieval  
One of the most commonly used applications of 
ASR for video is to support information retrieval 
(IR).  Such video IR systems often use speech tran-
scriptions to index segments of video in much the 
same way that words are used to index text docu-
ments (Wactlar et al, 1996).  For example, in the 
domain of baseball, if a video IR system were is-
sued the query ?home run,? it would typically re-
turn a set of video clips by searching its database 
for events in which someone uttered the phrase 
?home run.?  Because such systems rely on ASR 
output to search video, the performance of a video 
IR system gives an indirect evaluation of the 
ASR?s quality.  Further, unlike the case with word 
accuracy or error rate, such evaluations highlight a 
systems ability to recognize the more relevant con-
tent words without being distracted by the more 
common stop words. 
Our metric for evaluation is the precision with 
which baseball highlights are returned in a video 
IR system.  We examine three systems: one that 
uses ASR with the grounded language model, a 
baseline system that uses ASR with the text only 
interpolated language model, and finally a system 
that uses human produced closed caption transcrip-
tions to index events. 
For each system, all 1200 events from the test 
set (not just the highlights) are indexed.  Queries 
are generated artificially using a method similar to 
Berger and Lafferty (1999) and used in Fleischman 
and Roy (2007).  First, each highlight is labeled 
with the event?s type (e.g. fly ball), the event?s lo-
cation (e.g. left field) and the event?s result (e.g. 
double play): 13 labels total.  Log likelihood ratios 
127
are then used to find the phrases (unigram, trigram, 
and bigram) most indicative of each label (e.g. ?fly 
ball? for category fly ball).  For each label, the 
three most indicative phrases are issued as queries 
to the system, which ranks its results using the lan-
guage modeling approach of Ponte and Croft 
(1998).  Precision is measured on how many of the 
top five returned events are of the correct category.   
Figure 4 shows the precision of the video IR 
systems based on ASR with the grounded language 
model, ASR with the text-only interpolated lan-
guage model, and closed captioning transcriptions.  
As with our previous evaluations, the IR results 
show that the system using ASR with the grounded 
language model performed better than the one us-
ing ASR with the text-only language model (5.1% 
absolute improvement).  More notably, though, 
Figure 4 shows that the system using the grounded 
language model performed better than the system 
using the hand generated closed captioning tran-
scriptions (4.6% absolute improvement).  Although 
this is somewhat counterintuitive given that hand 
transcriptions are typically considered gold stan-
dards, these results follow from a limitation of us-
ing text-based methods to index video.  
Unlike the case with text documents, the occur-
rence of a query term in a video is often not 
enough to assume the video?s relevance to that 
query.  For example, when searching through 
video of baseball games, returning all clips in 
which the phrase ?home run? occurs, results pri-
marily in video of events where a home run does 
not actually occur.  This follows from the fact that 
in sports, as in life, people often talk not about 
what is currently happening, but rather, they talk 
about what did, might, or will happen in the future.   
By taking into account non-linguistic context 
during speech recognition, the grounded language 
model system indirectly circumvents some of these 
false positive results.  This follows from the fact 
that an effect of using the grounded language 
model is that when an announcer utters a phrase 
(e.g., ?fly ball?), the system is more likely to rec-
ognize that phrase correctly if the event it refers to 
is actually occurring (e.g. if someone actually hit a 
fly ball).  Because the grounded language model 
system is biased to recognize phrases that describe 
what is currently happening, it returns fewer false 
positives and gets higher precision.  
0.26
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
0.35
ASR-LM CC ASR-GLM
Pr
ec
is
io
n
 
o
f T
o
p 
5
 
Figure 4. Precision of top five results of a video IR sys-
tem based on speech transcriptions.  Three different 
transcriptions are compared: ASR-LM uses ASR with a 
text-only interpolated language model (trained on base-
ball closed captioning and the switchboard corpus); 
ASR-GLM uses ASR with a grounded language model; 
CC uses human generated closed captioning transcrip-
tions (i.e., no ASR). 
5 Conclusions 
We have described a method for improving speech 
recognition in video.  The method uses grounded 
language modeling, an extension of tradition lan-
guage modeling in which the probability of a word 
is conditioned not only on the previous word(s) but 
also on the non-linguistic context in which the 
word is uttered.  Context is represented using hier-
archical temporal patterns of low level features 
which are mined automatically from a large unla-
beled video corpus.  Hierarchical Bayesian models 
are then used to map these representations to 
words.  Initial results show grounded language 
models improve performance on measures of per-
plexity, word accuracy and error rate, and preci-
sion on an information retrieval task. 
In future work, we will examine the ability of 
grounded language models to improve perform-
ance for other natural language tasks that exploit 
text based language models, such as Machine 
Translation.  Also, we are examining extending 
this approach to other sports domains such as 
American football.  In theory, however, our ap-
proach is applicable to any domain in which there 
is discussion of the here-and-now (e.g., cooking 
shows, etc.).  In future work, we will examine the 
strengths and limitations of grounded language 
modeling in these domains. 
128
References 
Allen, J.F. (1984). A General Model of Action and 
Time. Artificial Intelligence. 23(2). 
Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei, 
D, and Jordan, M. (2003), Matching Words and 
Pictures, Journal of Machine Learning Research, 
Vol 3. 
Berger, A. and Lafferty, J. (1999). Information 
Retrieval as Statistical Translation. In Proceed-
ings of SIGIR-99. 
Blei, D. and Jordan, M. (2003). Modeling annotated 
data. Proceedings of the 26th International Confer-
ence on Research and Development in Information 
Retrieval, ACM Press, 127?134.  
Blei, D. Ng, A., and Jordan, M (2003). ?Latent Dirichlet 
allocation.? Journal of Machine Learning Research 
3:993?1022.  
Bouthemy, P., Gelgon, M., Ganansia, F. (1999). A uni-
fied approach to shot change detection and cam-
era motion characterization. IEEE Trans. on 
Circuits and Systems for Video Technology, 
9(7). 
Chen, S. F. and Goodman, J., (1998). An Empirical 
Study of Smoothing Techniques for Language Mod-
eling, Tech. Report TR-10-98, Computer Science 
Group, Harvard U., Cambridge, MA. 
Fleischman M, Roy, D. (2007).  Situated Models of 
Meaning for Sports Video Retrieval.  HLT/NAACL. 
Rochester, NY. 
Fleischman, M. and Roy, D. (2007). Unsupervised Con-
tent-Based Indexing of Sports Video Retrieval.  9th 
ACM Workshop on Multimedia Information Retrieval 
(MIR). Augsburg, Germany. 
Fleischman, M. B. and Roy, D.  (2005)  Why Verbs are 
Harder to Learn than Nouns: Initial Insights from a 
Computational Model of Intention Recognition in 
Situated Word Learning.  27th Annual Meeting of the 
Cognitive Science Society, Stresa, Italy. 
Fleischman, M., DeCamp, P. Roy, D.  (2006). Mining 
Temporal Patterns of Movement for Video Content 
Classification.  ACM Workshop on Multimedia In-
formation Retrieval. 
Fleischman, M., Roy, B., and Roy, D. (2007).  Tempo-
ral Feature Induction for Sports Highlight Classifica-
tion.  In Proceedings of ACM Multimedia.  
Augsburg, Germany. 
Gong, Y., Han, M., Hua, W., Xu, W.  (2004). Maximum 
entropy model-based baseball highlight detection and 
classification.  Computer Vision and Image Un-
derstanding. 96(2). 
Hauptmann, A. , Witbrock, M., (1998) Story Segmenta-
tion and Detection of Commercials in Broadcast 
News Video, Advances in Digital Libraries. 
Hongen, S., Nevatia, R. Bremond, F. (2004). 
Video-based event recognition: activity repre-
sentation and probabilistic recognition methods.  
Computer Vision and Image Understanding. 
96(2). 
Hsu , Bo-June (Paul). (in review). Generalized Linear 
Interpolation of Language Models. 
Jang, P., Hauptmann, A. (1999).  Learning to Recognize 
Speech by Watching Television.  IEEE Intelligent 
Systems Magazine, 14(5), pp. 51-58.  
Mukherjee, N. and Roy, D.. (2003). A Visual Context-
Aware Multimodal System for Spoken Language 
Processing. Proc. Eurospeech, 4 pages.  
Ponte, J.M., and Croft, W.B. (1998). A Language Mod-
eling Approach to Information Retrieval. In Proc. of 
SIGIR?98.  
Roy, D. (2005). . Grounding Words in Perception and 
Action: Insights from Computational Models.  TICS. 
Roy, D. and Pentland, A. (2002).  Learning Words from 
Sights and Sounds: A Computational Model. Cogni-
tive Science, 26(1). 
Roy. D. and Reiter, E. (2005). . Connecting Language to 
the World. Artificial Intelligence, 167(1-2), 1-12. 
Snoek, C.G.M. and Worring, M.. (2005).  Multimodal 
video indexing: A review of the state-of-the-art. 
Multimedia Tools and Applications, 25(1):5-35. 
Steyvers, M., Smyth, P., Rosen-Zvi, M., & Griffiths, T. 
(2004). Probabilistic Author-Topic Models for In-
formation Discovery. The Tenth ACM SIGKDD In-
ternational Conference on Knowledge Discovery and 
Data Mining. Seattle, Washington. 
Stolcke, A., (2002). SRILM - An Extensible Language 
Modeling Toolkit, in Proc. Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.  
Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005). 
Shot Detection and Motion Analysis for Automatic 
MPEG-7 Annotation of Sports Videos.  In 13th In-
ternational Conference on Image Analysis and Proc-
essing. 
Wactlar, H., Witbrock, M., Hauptmann, A., (1996 ). 
Informedia: News-on-Demand Experiments in 
Speech Recognition.  ARPA Speech Recognition 
Workshop, Arden House, Harriman, NY.  
Witten, I. and Frank, E. (2005).  Data Mining: Practical 
machine learning tools and techniques.  2nd Edition, 
Morgan Kaufmann. San Francisco, CA. 
 
129
Multi-Document Person Name Resolution 
Michael Ben Fleischman 
Massachusetts Institute of Technology
77 Massachusetts Ave. 
Cambridge, MA 02139 
mbf@mit.edu 
Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292 
hovy@isi.edu 
 
Abstract 
Multi-document person name resolution fo-
cuses on the problem of determining if two 
instances with the same name and from dif-
ferent documents refer to the same individ-
ual.  We present a two-step approach in 
which a Maximum Entropy model is 
trained to give the probability that two 
names refer to the same individual.  We 
then apply a modified agglomerative clus-
tering technique to partition the instances 
according to their referents.   
1 Intro 
Artists and philosophers have long noted that mul-
tiple distinct entities are often referred to by one 
and the same name (Cohen and Cohen, 1998; Mar-
tinich, 2000).  Recently, this referential ambiguity 
of names has become of increasing concern to 
computational linguists, as well.  As the Internet 
increases in size and coverage, it becomes less and 
less likely that a single name will refer to the same 
individual on two different web sites.  This poses a 
great challenge to information retrieval (IR) and 
question-answering (QA) applications, which often 
rely on little data when responding to user queries.   
Another area in which referential ambiguity is 
problematic involves the automatic population of 
ontologies with instances.  For such tasks, concept-
instance pairs (such as Paul Simon/pop star) are 
extracted from the web, cleaned of noise, and then 
inserted into an already existing ontology.  The 
process of insertion requires that concept-instance 
pairs that have the same referent be merged to-
gether (e.g. Paul Simon/pop star and Paul Simon 
/singer).  Further, instances of the same name, but 
with different referents, must be distinguished (e.g. 
Paul Simon/pop star and Paul Simon /politician).   
We propose a two-step approach: first, we train a 
maximum entropy model to generate the probabil-
ity that any two concept-instance pairs refer to one 
and the same referent.  Then, a modified agglom-
erative clustering technique is used to merge the 
most likely instances together, forming clusters that 
correspond to individual referents.   
2 Related Work 
While there has been a great deal of work on 
coreference resolution within a single document, 
little work has focused on the challenges associated 
with resolving the reference of identical person 
names across multiple documents.   
Mann and Yarowsky (2003) are amongst the few 
who have examined this problem.  They treat it as a 
clustering task, in which, a combination of features 
(such as, a weighted bag of words and biographic 
information extracted from the text) are given to an 
agglomerative clustering algorithm, which outputs 
two clusters representing the two referents of the 
query name. 
Mann and Yarowsky (2003) report results on two 
types of evaluations: using hand-annotated web-
pages returned from truly ambiguous searches, they 
report precision/recall scores of 0.88/0.73; using 
?psuedonames?1 they report an accuracy of 86.4%. 
                                                          
1 Borrowing from techniques in word sense disambigua-
tion, they create a test set of 28 ?pseudonames? by ran-
While Mann and Yarowsky (2003) describe a 
number of useful features for multi-document per-
son name resolution, their technique is limited by 
only allowing a set number of referent clusters.  
Further, as discussed below, their use of artificial 
test data makes it difficult to determine how well it 
generalize to real world problems. 
Bagga and Baldwin (1998) also present an ex-
amination of multi-document person name resolu-
tion.  They first perform within-document 
coreference resolution to form coreference chains 
for each entity in each document.  They then use 
the text surrounding each reference chain to create 
summaries about each entity in each document.  
These summaries are then converted to a bag of 
words feature vector and are clustered using the 
standard vector space model often employed in IR. 
They evaluated their system on 11 entities named 
John Smith taken from a set of 173 New York 
Times articles.  Using an evaluation metric similar 
to a weighted sum of precision and recall they get 
an F-measure of 0.846.   
Although their technique allows for the discovery 
of a variable number of referents, its use of 
simplistic bag of words clustering is an inherently 
limiting aspect of their methodology.  Further, that 
they only evaluate their system, on a single person 
name begs the question of how well such a tech-
nique would fair on a more real-world challenge. 
3 Maximum Entropy Model  
3.1 Data 
Fleischman et al (2003) describe a dataset of con-
cept-instance pairs extracted automatically from a 
very large corpus of newspaper articles.  The data-
set (referred to here as the ACL dataset) contains 
approximately 2 million pairs (of which 93% are 
legitimate) in which the concept is represented by a 
complex noun phrase (e.g. president of the United 
                                                                                            
domly selecting two names from a hand crafted list of 8 
individuals (e.g., Haifa Al-Faisal and Tom Cruise) and 
treat the pair as one name with two referents.   
States) and the instance by a name (e.g. William 
Jefferson Clinton).2   
A set of 2675 legitimate concept-instance pairs 
was randomly selected from the ACL dataset de-
scribed above; each of these was then matched 
with another concept-instance pair that had an 
identical instance name, but a different concept 
name.  This set of matched pairs was hand tagged 
by a human annotator to reflect whether or not the 
identical instance names actually referred to the 
same individual.  The set was then randomly split 
into a training set of 1875 matched pairs (84% re-
ferring to the same individual), a development set 
of 400 matched pairs (85.5% referring to the same 
individual), and a test set of 400 matched pairs 
(83.5% referring to the same individual). 
3.2 Features 
In designing a binary classifier to determine 
whether two concept-instance pairs refer to the 
same individual, we formulate a number of differ-
ent features used to describe each matched pair.  
These features are summarized in Table 1, and de-
scribed in more detail below. 
Name Features 
We use a number of methods meant to express in-
formation available from the orthography of the 
instance name itself.  The first of these features 
(Name-Common) seeks to estimate the commonal-
ity of the instance name.  With this features we 
hope to capture the intuition that more common 
names (such as John Smith) will be more likely to 
refer to different individuals than more uncommon 
names (such as Yasir Arafat).  We calculate this 
feature by splitting the instance name into first, 
middle (if necessary) and last sub-names.  We then 
use a table of name frequencies downloaded from 
the US census website to give each sub-name a 
score; these scores are then multiplied together for 
a final value.   
The second name statistic feature estimates how 
famous the instance name is.  With this features we 
                                                          
2 Although the dataset includes multiple types of named 
entities, we focus here only on person names. 
hope to capture the intuition that names of very 
famous people (such as Michael Jackson) are less 
likely to refer to different individuals than less fa-
mous, yet equally common, names (such as John 
Smith).  We calculate this feature in two ways: 
first, we use the frequency of the instance name as 
it appears in the ACL dataset to give a representa-
tion of how often the name appears in newspaper 
text (Name-Fame); second, we use the number of 
hits reported on google.com for a query consisting 
of the quoted instance name itself (Web-Fame).  
These fame features are used both as is and scaled 
by the commonality feature described above. 
Web Features 
Aside from the fame features described above, we 
use a number of other features derived from web 
search results.  The first of which, called WebInter-
section, is simply the number of hits returned for a 
query using the instance name and the heads of 
each concept noun phrase in the match pair; i.e., 
(name + head1 +head2).   
The second, called WebDifference, is the abso-
lute value of the difference between the hits re-
turned from a query on the instance name and just 
the head of concept 1 vs. the instance name and 
just the head of concept 2; i.e., abs ((name + head1) 
-(name +head2)).   
The third, called WebRatio, is the ratio between 
the WebIntersection score and the sum of the hits 
returned when querying the instance name and just 
the head of concept 1 and the instance name and 
just the head of concept 2; i.e., (name + head1 
+head2) / ((name + head1) +(name +head2)).   
Overlap Features 
In order to capture some aspects of the contex-
tual cues to referent disambiguation, we include 
features representing the similarity between the 
sentential contexts from which each concept-
instance pair was extracted.  The similarity metric 
that we use is a simple word overlap score based 
on the number of words that are shared amongst 
both sentences.  We include scores in which each 
non-stop-word is treated equally (Sentence-Count), 
as well as, in which each non-stop-word is 
weighted according to its term frequency in a large 
corpus (Sentence-TF).  We further include two 
similar features that only examine the overlap in 
the concepts (Concept-Count and Concept-TF).  
 
Name Features 
Feature Name Description 
Name-Common  frequency of name in census data 
Name-Fame frequency of name in ACL dataset 
Web-Fame # of hits from name query 
Web Features 
Web Intersection query(name + head1 +head2) 
Web Difference 
abs( query(name + head1)  
+ query(name +head2)) 
Web Ratio 
query(name + head1 +head2)  
/ ( qry(name + head1) +qry(name +head2))
Overlap Features 
Sentence-Count
# of words common to  
context of both instances 
Sentence-TF 
as above but weighted  
by term frequency 
Concept-Count 
# of words common to  
concept of both instances 
Concept-TF 
as above but weighted  
by term frequency 
Semantic Features 
JCN sem. dist. of Jiang and Conrath 
HSO sem. dist. of Hirst and St. Onge 
LCH sem. dist. of Leacock and Chodrow 
Lin sem. dist. of Lin 
Res sem. dist. of Resnik 
Estimated Statistics 
F1 p(i1=i2 | i1?A, i2?B) 
F2 p(i1?A, i2?B | i1=i2) 
F3 p(i1?A | i2?B) + p(i2?B | i1?A) 
F4 p(i1?A, i2?B) / (p(i1?A) + p(I2?B)) 
Table 1.  Features used in Max. Ent. model split accord-
ing to feature type. 
Semantic Features 
Another important clue in determining the corefer-
ence of instances is the semantic relatedness of the 
concepts with which they are associated.  In order 
to capture this, we employ five metrics described in 
the literature that use the WordNet ontology to de-
termine a semantic distance between two lexical 
items (Budanitsky and Hirst. 2001).  We use the 
implementation described in Pedersen (2004) to 
create features corresponding to the scores on the 
following metrics shown in Table 1. Due to prob-
lems associated with word sense ambiguity, we 
take the maximum score amongst all possible com-
binations of senses for the heads of the concepts in 
the matched pair.  The final output to the model is 
a single similarity measure for each of the eight 
metrics described in Pedersen (2004). 
Estimated Statistics Features 
In developing features useful for referent disambigua-
tion, it is clear that the concept information to which we 
have access is very useful.  For example, given that we 
see John Edwards /politician and John Edwards 
/lawyer, our knowledge that politicians are often lawyers 
is very useful in judging referential identity.3  In order to 
exploit this information, we leverage the strong correla-
tion between orthographic identity of instance names 
and their referential identity.   
As described above, approximately 84% of those 
matched pairs that had identical instance names 
referred to the same referent.  In a separate exami-
nation, we found, not surprisingly, that nearly 
100% of pairs that were matched to instances with 
different names (such as Bill Clinton vs. George 
Clinton) referred to different referents.   
We take advantage of this strong correlation in 
developing features by first making the (admittedly 
wrong) assumption that orthographic identity is 
equivalent to referential identity, and then using 
that assumption to calculate a number of statistics 
over the large ACL dataset.  We postulate that the 
noise introduced by our assumption will be offset 
by the large size of the dataset, yielding a number 
of highly informative features. 
The statistics we calculate are as follows: 
  
P1:  The probability that instance 1 and in-
stance 2 have the same referent given that in-
stance 1 is paired with concept A and instance 2 
with concept B; i.e., p(i1=i2 | i1?A, i2?B) 
P2:  The probability that instance 1 is paired 
with concept A and instance 2 with concept B 
given that instance 1 and instance 2 have the 
same referent; i.e., p(i1?A, i2?B | i1=i2) 
P3:  The probability that instance 1 is paired 
with concept A given that instance 2 is paired 
with concept B plus the probability that instance 
                                                          
3 It should be noted that this feature is attempting to en-
code knowledge about what concepts occur together in 
the real world, which is different than, what is being 
encoded in the semantic features, described above.   
2 is paired with concept B given that instance 1 
is paired with concept A; i.e., p(i1?A | i2?B) 
+ p(i2?B | i1?A) 
P4:  The probability that instance 1 is paired 
with concept A and instance 2 is paired with 
concept B divided by the probability that in-
stance 1 is paired with concept A plus the prob-
ability that instance 2 is paired with concept B; 
i.e., p(i1?A, i2?B) / (p(i1?A) + p(i2?B)) 
 
 
 
 
 
 
 
Figure 1.  Results of Max. Ent. classifier on held out test 
data compared to baseline (i.e., always same referent). 
Aside from the noise introduced by the assump-
tion described above, another problem with these 
features arises when the derived probabilities are 
based on very low frequency counts.  Thus, when 
adding these features to the model, we bin each 
feature according to the number of counts that the 
score was based on.   
3.3 Model 
Maximum Entropy (Max. Ent.) models implement 
the intuition that the best model will be the one that 
is consistent with the set of constrains imposed by 
the evidence, but otherwise is as uniform as possi-
ble (Berger et al, 1996).  We model the probability 
of two instances having the same referent (r=[1,0]) 
given a vector of features x according to the Max. 
Ent. formulation below: 
?
=
=
n
i
i
x
xrfZxrp
0
i )],(exp[1)|( ?  
Here Zx is a normalization constant, fi(r,x) is a 
feature function over values of r and vector ele-
ments, n is the total number of feature functions, 
and ?i is the weight for a given feature function.  
The final output of the model is the probability 
83.50%
90.75%
78%
80%
82%
84%
86%
88%
90%
92%
Baseline Max Ent
%
 C
or
re
ct
given a feature vector that r=1; i.e., the probability 
that the referents are the same. 
We train the Max. Ent. model using the 
YASMET Max. Ent. package (Och, 2002).  Feature 
weights are smoothed using Gaussian priors with 
mean 0.  The standard deviation of this distribution 
is optimized on the development set, as is the num-
ber of training iterations and the probability thresh-
old used to make the hard classifications reported 
in the following experiment. 
3.4 Experimental Results  
Results for the classifier on the held out test set are 
reported in Figure 1.  Baseline here represents al-
ways choosing the most common classification 
(i.e., instance referents are the same).  Figure 2 
represents the learning curve associated with this 
task.  Figure 3 shows the effect on performance of 
incrementally adding the best feature set (as deter-
mined by greedily trying each one) to the model.   
 
 
 
 
 
 
 
Figure 2.  Learning curve of Max. Ent. model.   
3.5 Discussion 
It is clear from the results that this model outper-
forms the baseline for this task (p>0.01) (p<0.01) 
(Mitchell, 1997).  Interestingly, although the num-
ber of labeled examples that were used to train the 
system was by no means extravagant, it appears 
from the learning curve that increasing the size of 
the training set will not have a large effect on clas-
sifier performance.  Also of interest, Figure 3 
shows that the greedy feature selection technique 
found that the most powerful features for this task 
are the estimated statistic features and the web fea-
tures.  While the benefit of such large corpora fea-
tures is not surprising, the relative lack of power 
from the semantic and overlap features (which ex-
ploit ontolological and contextual information) was 
surprising. 4  In future work, we will examine how 
more sophisticated similarity metrics and larger 
windows of context (e.g., the whole document) 
might improve performance. 
4 Clustering 
 
 
 
 
 
 
 
 
Figure 3.  Results of Max. Ent. classifier on held out 
data using different subsets of feature types.  Feature 
types are greedily added one at a time, starting with Es-
timated Statistics and ending with Semantic Features.   
Having generated a model to predict the probability 
that two concept-instance pairs with the same name 
refer to the same individual, we are faced with the 
problem of using such a model to partition all of 
our concept-instance pairs according to the indi-
viduals to which they actually refer.  Although, 
ideally, we should be able to simply apply the 
model to all possible pairs, in reality, such a meth-
odology may lead to a contradiction.   
For example, given that the model predicts in-
stance A is identical to instance B, and in addition, 
that instance B is identical to C, because of the 
transitivity of the identity relation, we must assume 
that A is identical to C.  However, if the model 
predicts that A is not identical to C, (which can and 
does occur) we must assume the model is wrong in 
at least one of its three predictions. 
                                                          
4 Note that for these tests, the model parameters are not 
optimized for each run; thus, the performance is slightly 
worse than in Figure 1. 
11%
12%
13%
14%
15%
16%
10 100 1000
# of Training Examples
%
 E
rr
o
r
88.3%
83.5%
87.0%
89.3% 89.0%
88.3%
80%
81%
82%
83%
84%
85%
86%
87%
88%
89%
90%
baseline stat. +web +name +over +sem   
(all feats)
%
 C
or
re
ct
Following Ng and Cardie (2002), we address this 
problem by clustering each set of concept-instance 
pairs with identical names, using a form of group-
average agglomerative clustering, in which the 
similarity score between instances is just the prob-
ability output by the model.  Because standard ag-
glomerative clustering algorithms are O(n3) if 
cosign similarity metrics are not used (Manning 
and Schutze, 2001), we adapt the method to our 
framework.  Our algorithm operates as follows5: 
 
On input D={concept-instance pairs of same name}, 
build a fully connected graph G with vertex set D: 
 
1) Label each edge (d,d?) in G with a score correspond-
ing to the probability of identity predicted by the 
Max. Ent. model 
2) While the edge with max score in G > threshold: 
a. Merge the two nodes connected by the edge with 
the max score.   
b. For each node in the graph  
a. Merge the two edges connecting it to the 
newly merged node  
b. Assign the new edge a score equal to the avg. 
of the two old edge scores. 
 
The final output of this algorithm is a new graph 
in which each node represents a single referent as-
sociated with a set of concept-instance pairs.  This 
algorithm provides an efficient way, O(n2), to com-
pose the pair-wise information given by the model.  
Further, because the only free parameter is a merg-
ing threshold (which can be determined through 
cross-validation) the algorithm is free to choose a 
different number of referents for each instance 
name it is tested on.  This is critical for the task 
because each instance name can have any number 
of referents associated with it. 
4.1 Test Data 
In order to test clustering, we randomly selected a 
set of 31 instance names from the ACL dataset, 11 
of which referred to multiple individuals and 20 of 
which had only a single referent6.  Each concept-
                                                          
5 This algorithm was developed with Hal Daume (tech-
nical report, in prep.). 
6 In an examination of 113 different randomly selected 
instance names from the ACL dataset we found that 32 
instance pair with that instance name was then ex-
tracted and hand annotated such that each individ-
ual referent was given a unique identifying code.   
We chose not to test on artificially generated test 
examples (such as the pseudo-names described in 
Mann and Yarowsky, 2003) because of our reli-
ance on name orthography in feature generation 
(see section 3.2).  Further, such pseudo-names ig-
nore the fact that names often correlate with other 
features (such as occupation or birthplace), and that 
they do not guarantee clean test data (i.e., the two 
names chosen for artificial identity may themselves 
each refer to multiple individuals). 
4.2 Experimental Design 
In examining the results of the clustering, we chose 
to use a simple clustering accuracy as our perform-
ance metric.  According to this technique, we 
match the output of our system to a gold standard 
clustering (defined by the hand annotations de-
scribed above).7   
We compare our algorithm on the 31 sets of con-
cept-instance pairs described above against two 
baseline systems.  The first (baseline1) is simply a 
single clustering of all pairs into one cluster; i.e., 
all instances have the same referent.  The second 
(baseline2) is a simple greedy clustering algorithm 
that sequentially adds elements to the previous 
cluster whose last-added element is most similar 
(and exceeds some threshold set by cross valida-
tion).  
4.3 Results 
In examining performance, we present a weighted 
average over these 31 instance sets, based on the 
number of nodes (i.e., concept-instance pairs) in 
each set of instances (total nodes = 1256).  Cross-
validation is used to set the threshold for both the 
baseline2 and modified agglomerative algorithm.  
                                                                                            
appeared only once in the dataset, 53 appeared more 
than once but always referred to the same referent, and 
28 had multiple referents. 
7 While this is a relatively simple measure, we believe 
that, if anything, it is overly conservative, and thus, 
valid for the comparisons that we are making.   
These results are presented in Table 2.  Figure 4 
examines performance as a function of the number 
of referents within each of the 31 instance sets.   
4.4 Discussion 
 
 
 
 
 
 
 
 
 
 
 
Figure 4.  Plot of performance for modified agglomera-
tive clustering and Baseline system as a function of the 
number of referents in the test set.   
 
While the algorithm we present clearly outper-
forms the baseline2 method over all 31 instance 
sets (p<0.01), we can see that it only marginally 
outperforms our most simple baseline1 method 
(p<0.10) (Mitchell, 1997).  This is due to the fact 
that for each of the 20 instance sets that only have a 
single referent, the baseline achieves a perfect 
score, while the modified agglomerative method 
only achieves a score of 96.4%.  Given this aspect 
of the baseline, and the distribution of the data, the 
fact that our algorithm outperforms the baseline at 
all speaks to its usefulness for this task.   
A better sense of the usefulness of this algorithm, 
however, can be seen by looking at its performance 
only on instance sets with multiple referents.  As 
seen in Table 3, on multiple referent instance sets, 
modified agglomerative clustering outperforms 
both the baseline1 and baseline2 methods by a sta-
tistically significant margin (p<0.01) (Mitchell, 
1997). 
5 Conclusion 
The problem of cross-document person name dis-
ambiguation is of growing concern in many areas 
of natural language processing.  We have presented 
a two-step methodology for the disambiguation of 
automatically extracted concept-instance pairs.  
Our approach first applies a Maximum Entropy 
model to all concept-instance pairs that share the 
same instance name.  The output probabilities of 
this model are then inputted to a modified agglom-
erative clustering algorithm that partitions the pairs 
according to the individuals to which they refer.  
This algorithm not only allows for a dynamically 
set number of referents, but also, outperforms two 
baseline methods. 
A clear example of the success of this algorithm 
can be seen in the output of the system for the in-
stance set for Michael Jackson (Appendix A, Table 
2).  Here, a name that refers to many individuals is 
fairly well partitioned into appropriate clusters.  
With the instance set for Sonny Bono (Appendix A, 
Table 1), however, we can see why this task is so 
challenging.  Here, although, Sonny Bono only re-
fers to one individual, the system finds (like many 
of the rest of us) that the likelihood of a singer also 
being a politician is so low that the name must re-
fer to two different people.  While this assumption 
is often true (as is the case with Paul Simon), we 
would have hoped that information from our web 
and fame features would have overridden the sys-
tem?s bias in this circumstance. 
In future work we will examine how other fea-
tures may be useful in attacking such hard cases.  
Also, we will examine how this technique can be 
applied more generally to problems that exist be-
tween non-identical, but similar names (e.g. Bill 
Clinton vs. William Jefferson Clinton).   
Acknowledgements 
The authors would like to thank Regina Barzilay for her 
helpful comments and advice, and Hal Daume for his 
useful insights into and discussion of the problem.  We 
also thank Deb Roy for his continued support 
. 
References  
A. Bagga, and B. Baldwin. 1998.  Entity-Based Cross-
Document Coreferencing Using the Vector Space 
Model. . COLING-ACL'98. 
40%
50%
60%
70%
80%
90%
100%
1 2-3 4-5 >6
# of Referents
%
 C
or
re
ct
Baseline Graph Clustering
A. Berger, S. Della Pietra and V. Della Pietra, 1996. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, vol. 22, no. 1. 
A. Budanitsky and G. Hirst . 2001. Semantic distance in 
WordNet: An experimental, application-oriented 
evaluation of five measures. In Workshop on Word-
Net and Other Lexical Resources,  NAACL. 
J. Cohen and E. Cohen.  1998.  The Big Lebowski. Film. 
Columbia TriStar Pictures. 
M. Fleischman, E. Hovy, and A. Echihabi. 2003.  Off-
line Strategies for Online Question Answering: An-
swering Questions Before They Are Asked.  ACL, 
Sapporo, Japan. 
G. S. Mann, David Yarowsky, 2003 Unsupervised Per-
sonal Name Disambiguation, CoNLL, Edmonton, 
Canada. 
C. Manning and H. Schutze.  2001 Foundations of Sta-
tistical Natural Language Processing. MIT Press, 
Cambridge, Ma.  
A.P. Martinich, ed. 2000. The Philosophy of Language, 
Oxford University Press, Oxford, UK. 
T. Mitchell. 1997.  Machine Learning.  McGraw-Hill 
International Editions, New York, NY. Pgs. 143-145. 
V. Ng and C. Cardie Improving Machine Learning Ap-
proaches to Coreference Resolution. ACL, 2002.  
F.J. Och. 2002. Yet another maxent toolkit: YASMET. 
www-i6.informatik.rwth-aachen.de/Colleagues/och/. 
T. Pedersen. 2004. WordNet::Similarity v0.07 . 
http://www.d.umn.edu/~tpederse/similarity.html. 
Appendix A. Sample Cluster Output. 
 
Cluster 1  
time pop star Cluster 1 (cont) 
the singer former rock star 
onetime singer Cluster 2 
former singer Lawmaker 
pop singer crooning lawmaker 
former entertainer mayoral candidate 
former pop star republican politician 
former pop singer congressman 
entertainer Cluster 3 
onetime beau A freshman republican 
Singer  
Table 1.  Output clusters for the name Sonny Bono. 
 
 
 
 
Cluster 1  
platinum recording artist Cluster 2 (cont) 
cbs records artist rocker 
artist american pop superstar 
Cluster 2 visiting idol 
singer idol 
pop idol pop music superstar 
day pop superstar package entertainer 
international pop star another entertainer 
starring singer american pop singer 
american singer Cluster 3 
rock superstar local talk radio personality 
suing pop superstar kabc radio talk show host 
pop superstar los angeles radio personality 
enigmatic pop superstar veteran kabc radio talk show host 
featuring pop star ubiquitous radio commentator 
embattled pop star radio broadcaster 
controversial pop star broadcaster 
including singer Cluster 4 
featuring singer author 
even singer british beer guru 
signing pop performer beer expert 
pop singer Cluster 5 
surrounding entertainer mannequin collector 
joining entertainer Cluster 6 
including entertainer kfor commander 
singing superstar the commander of kfor 
including superstar commander of kfor 
american superstar british commander 
superstar Cluster 7 
ailing superstar the nato commander of    the kosovo liberation force 
reuter pop superstar Cluster 8 
reclusive pop superstar Designer 
quiet pop superstar Cluster 9 
embattled pop superstar deputy secretary    of transportation 
alleging pop superstar deputy secretary of the    department of transportation 
music superstar Cluster 10 
the us pop star historian 
rock star Cluster 11 
pop star education department spokesman 
entertainer company spokesman 
pop recording star dow corning spokesman 
newlywed pop star Cluster 12 
fellow pop star judge 
the singer Cluster 13 
superstar singer receiver 
setting singer career browns receiver 
rock singer trading receiver 
surrounding pop singer ravens receiver 
suing pop singer baltimore wide receiver 
reuter pop singer agent wide receiver 
prague pop singer wide receiver 
pop singer Cluster 14 
rock sensation baylor offensive tackle 
music sensation Cluster 15 
pop sensation beer writer 
Table 2.  Output clusters for the name Michael Jackson 
SENSEVAL Automatic Labeling of Semantic Roles using Maximum Entropy 
Models 
Namhee Kwon  
Information Science Institute 
University of Southern California
4676 Admiralty Way 
Marina del Rey, CA 90292 
nkwon@isi.edu 
Michael Fleischman 
Messachusetts Institute of 
Technology 
77 Massachusetts Ave 
Cambridge, MA 02139 
mbf@mit.edu 
Eduard Hovy 
Information Science Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
hovy@isi.edu 
 
Abstract 
As a task in SensEval-3, Automatic Labeling 
of Semantic Roles is to identify frame 
elements within a sentence and tag them with 
appropriate semantic roles given a sentence, a 
target word and its frame.  We apply 
Maximum Entropy classification with feature 
sets of syntactic patterns from parse trees and 
officially attain 80.2% precision and 65.4% 
recall.  When the frame element boundaries 
are given, the system performs 86.7% 
precision and 85.8% recall. 
1 Introduction 
The Automatic Labeling of Semantic Roles track 
in SensEval-3 focuses on identifying frame 
elements in sentences and tagging them with their 
appropriate semantic roles based on FrameNet1. 
For this task, we extend our previous work 
(Fleischman et el., 2003) by adding a sentence 
segmentation step and by adopting a few additional 
feature vectors for Maximum Entropy Model.  
Following the task definition, we assume the frame 
and the lexical unit of target word are known 
although we have assumed only the target word is 
known in the previous work. 
2 Model 
We separate the problem of FrameNet tagging 
into three subsequent processes: 1) sentence 
segmentation 2) frame element identification, and 
3) semantic role tagging.  We assume the frame 
element (FE) boundaries match the parse 
constituents, so we segment a sentence based on 
parse constituents.  We consider steps 2) and 3) as 
classification problems.  In frame element 
identification, we use a binary classifier to 
determine if each parse constituent is a FE or not, 
while, in semantic role tagging, we classify each 
                                                     
1 http://www.icsi.berkeley.edu/~framenet 
identified FE into its appropriate semantic role.2  
Figure 1 shows the sequence of steps. 
He fastened the panel from an old radio to the headboard wi th 
sticky tape and tied the driving wheel to Pete 's cardboard box 
wi th st ring
(He) (fastened the panel from an old radio to the headboard 
wi th sticky tape) (and) (t ied) (the driving wheel) ( to Pete 's 
cardboard box) (wi th s tring)
(He) (the driving wheel) (to Pete 's cardboard box) (wi th  string)
Agen t I tem Goal Connecto r
Input sentence
1) Sentence Segmentation: choose the highest 
consti tuen ts while separating targe t word 
2) Frame Element Identification: apply ME 
classification to classify each segment in t o classes of 
FE (frame element), T (target), NO (none) then ext ract 
iden ti fied f rame elements
3) Semantic Role Tagging: apply ME classification to 
classify each FE Into classes of various semantic roles
Output role: Agent (He), I tem ( the driving wheel), 
Goal ( to Pete?s cardboard box), Connector (wi th string)
Fig. 1. The sequence of steps on a sample sentence 
having a target word ?tied?. 
We train the ME models using the GIS 
algorithm (Darroch and Ratcliff, 1972) as 
implemented in the YASMET ME package (Och, 
2002).  We use the YASMET MEtagger (Bender et 
al. 2003) to perform the Viterbi search for 
choosing the most probable tag sequence for a 
sentence using the probabilities computed during 
training. Feature weights are smoothed using 
Gaussian priors with mean 0 (Chen and Rosenfeld, 
1999). 
2.1 Sentence Segmentation 
We segment a sentence into a sequence of non-
overlapping constituents instead of all individual 
constituents. There are a number of advantages to 
applying sentence segmentation before FE 
                                                     
2 We are currently ignoring null instantiations.  
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
boundary identification.  First, it allows us to 
utilize sentence-wide features for FE identification.  
The sentence-wide features, containing dependent 
information between frame element such as the 
previously identified class or the syntactic pattern, 
have previously been shown to be powerful 
features for role classification (Fleischman et al, 
2003).  Further, it allows us to reduce the number 
of candidate constituents for FE, which reduces the 
convergence time in training.  
The constituents are derived from a syntactic 
parse tree3.  Although we need to consider all 
combinations of various level constituents in a 
parse tree, we know the given target word should 
be a separate segment because a target word is not 
a part of other FEs.4  Since most frame elements 
tend to be in higher levels of the parse tree, we 
decide to use the highest constituents (the parse 
constituents having the maximum number of 
words) while separating the target word.  Figure 2 
shows an example of the segmentation for an 
actual sentence in FrameNet with the target word 
?tied?. 
  
He tied the to Pete box
PRP VBD DT TO NP NN
VP
VP
VP
NP
NP
PP
NP
S
?s
CC
and
PP
fastened the panel
from an old radio  
to the headboard  
with sticky tape
?.
VBG
driving wheel
NN
POSNNP
NN
cardboard
IN
with
NP
NN
string
Fig. 2. A sample sentence segmentation: ?tied? is 
the target predicate, and the shaded constituent 
represents each segment. 
However, this segmentation reduces the FE 
coverage of constituents (the number of 
constituents matching frame elements).  In Table 1, 
?individual constituents? means a list of all 
constituents, and ?Sentence segmentation? means a 
sequence of non-overlapping constituents that are 
taken in our work.  We can regard 85.8% as the 
accuracy of the parser. 
                                                     
3 We use Charniak parser :  
   http://www.cs.brown.edu/people/ec/#software 
4 Although 17% of constituents are both a target and a 
frame element, there is no case that a target is a part of a 
frame element. 
Method Number of constituents 
FE coverage 
(%) 
Individual 
constituents  342,245 85.8 
Sentence 
segmentation 66,401 79.5 
Table 1.  FE coverage for the test set. 
2.2 Frame Element Identification 
Frame element identification is executed for 
segments to classify into the classes on FE, Target, 
or None.  When a constituent is both a target and a 
frame element, we set it as a frame element when 
training because we are interested in identifying 
frame elements not a target. 
The initial features are adopted from (Gildea and 
Juraksky 2002) and (Fleischman, Kwon, and Hovy 
2003), and a few additional features are also used.  
The features are: 
? Target predicate (target): The target is the 
principal lexical item in a sentence.  
? Target lexical name (lexunit): The formal 
lexical name of target predicate is the string of 
the original form of target word and 
grammatical type.  For example, when the 
target is ?tied?, the lexical name is ?tie.v?.   
? Target type (ltype): The target type is a part 
of lexunit representing verb, noun, or adjective. 
(e.g. ?v? for a lexunit ?tie.v?) 
? Frame name (frame): The semantic frame is 
defined in FrameNet with corresponding target. 
? Constituent path (path): From the syntactic 
parse tree of a sentence, we extract the path 
from each constituent to the target predicate.   
The path is represented by the nodes through 
which one passes while traveling up the tree 
from the constituent and then down through 
the governing category to the target word.  For 
example, ?the driving wheel? in the sentence 
of Figure 2 has the path, NP?VP?VBD. 
? Partial path (ppath): The partial path is a 
variation of path, and it produces the same path 
as above if the constituent is under the same 
?S? as target word, if not, it gives ?nopath?.  
? Syntactic Head (head): The syntactic head of 
each constituent is obtained based on Michael 
Collins?s heuristic method5.  When the head is 
a proper noun, ?proper-noun? substitutes for 
the real head.  The decision as to whether the 
head is a proper noun is made based on the 
part of speech tags used in the parse tree.  
                                                     
5 http://www.ai.mit.edu/people/mcollins/papers/heads 
? Phrase Type (pt): The syntactic phrase type 
(e.g., NP, PP) of each constituent is also 
extracted from the parse tree.  It is not the 
same as the manually defined PT in FrameNet. 
? Logical Function (lf): The logical functions of 
constituents in a sentence are simplified into 
three values: external argument, object 
argument, other. When the constituent?s 
phrase type is NP, we follow the links in the 
parse tree from the constituent to the ancestors 
until we meet either S or VP.  If the S is found 
first, we assign external argument to the 
constituent, and if the VP is found, we assign 
object argument. Otherwise, other is assigned.   
? Position (pos): The position indicates whether 
a constituent appears before or after the target 
predicate. 
? Voice (voice): The voice of a sentence (active, 
passive) is determined by a simple regular 
expression over the surface form of the 
sentence. 
? Previous class (c_n): The class information of 
the nth-previous constituent (Target, FE, or 
None) is used to exploit the dependency 
between constituents.  During training, this 
information is provided by simply looking at 
the true class of the constituent occurring n-
positions before the target element.  During 
testing, the hypothesized classes are used for 
Viterbi search. 
Feature Set Example Functions 
f(c, lexunit) f(c, tie.v) = 1 
f(c, pt, pos, voice) f(c, NP,after,active) = 1 
f(c, pt, lf) f(c, ADVP,obj) = 1 
f(c, pt_-1, lf_-1) f(c, VBD_-1, other_-1) = 1 
f(c, pt_1, lf_1) f(c, PP_1, other_1) = 1 
f(c, head) f(c, wheel) = 1 
f(c, head, frame) f(c, wheel, Attaching) = 1 
f(c, path) f(c, NP?VP?VBD) = 1 
f(c, path_-1) f(c, VBD_-1) = 1 
f(c, path_1) f(c, PP?VP?VBD_1) = 1 
f(c, target) f(c, tied) = 1 
f(c, ppath) f(c, NP?VP?VBD) = 1 
f(c, ppath, pos) f(c,NP?VP?VBD, after) = 1 
f(c, ppath_-1, pos_-1) f(c, VBD_-1,after) = 1 
f(c ,ltype,  ppath) f(c, v, NP?VP?VBD) = 1 
f(c ,ltype,  path) f(c, v, NP?VP?VBD) = 1 
f(c ,ltype,  path_-1) f(c, v,VBD_-1) = 1 
f(c  frame) f(c, Attaching) = 1 
f(c, frame, c_-1) f(c, Attaching, T_-1) = 1 
f(c,frame, c_-2,c_-1) f(c, Attaching,NO_-2,T_-1)=1 
Table 2. Feature sets used in ME frame element 
identification.  Example functions of ?the driving 
wheel? from the sample sentence in Fig.2. 
The combinations of these features that are used 
in the ME model are shown in Table 2.  These 
feature sets contain the previous or next 
constituent?s features, for example, pt_-1 
represents the previous constituent?s phrase type 
and lf_1 represents the next constituent?s logical 
function.  
2.3 Semantic Role Classification 
Semantic role classification is executed only for 
the constituents that are classified into FEs in the 
previous FE identification phase by employing 
Maximum Entropy classification.   
In addition to the features in Section 2.2, two 
more features are applied.   
? Order (order): The relative position of a 
frame element in a sentence is given.  For 
example, the sentence from Figure 2 has four 
frame elements, where the element ?He? has 
order 0, while ?with string? has order 3. 
? Syntactic pattern (pat): The sentence level 
syntactic pattern is generated from the parse 
tree by considering the phrase type and logical 
functions of each frame element in the 
sentence.  In the example sentence in Figure 2, 
?He? is an external argument Noun Phrase, 
?tied? is a target predicate, and ?the driving 
wheel? is an object argument Noun Phrase.  
Thus, the syntactic pattern associated with the 
sentence is [NP-ext, target, NP-obj, PP-other, 
PP-other]. 
Table 3 shows the list of feature sets used for the 
ME role classification. 
Feature Set 
f(r, lexunit) f(r, pt, lf) 
f(r, target) f(r, pt_-1, lf_-1) 
f(r, pt, pos, voice) f(r, pt_1, lf_1) 
f(r, head) f(r, order, syn) 
f(r, head, lexunit) f(r, lexunit, order, syn) 
f(r, head, frame) f(r, pt, pos, voice, lexunit) 
f(r, frame, r_-1) f(r, frame, r_-2,r_-1) 
f(r, frame,r_-3, r_-2,r_-1) 
Table 3. Feature sets used in role classification. 
3 Results 
SensEval-3 provides the following data set: 
training set (24,558 sentences/ 51,323 frame 
elements/ 40 frames), and test set (8,002 sentences/ 
16,279 frame elements/ 40 frames). We submit two 
sets to SensEval-3, one (test A) is the output of all 
above processes (identifying frame elements and 
tagging them given a sentence), and the other (test 
B) is to tag semantic roles given frame elements.  
For test B, we attempt the role classification for all 
frame elements including frame elements not 
matching the parse tree constituents.  Although 
there are frame elements that have two different 
semantic roles, we ignore those cases and assign 
one semantic role per frame element.  This 
explains why test B shows 99% attempted frame 
elements.  The attempted number for test A is the 
number of frame elements identified by our system.  
Table 4 shows the official scores for these tests. 
Test Prec. Overlap Recall Attempted 
Test A 80.2 78.4 65.4 81.5 
Test B 86.7 86.6 85.8 99.0 
Table 4. Final SensEval-3 scores for the test set. 
In the official evaluation, the precision and recall 
are calculated by counting correct roles that 
overlap even in only one word with the reference 
set.  Overlap score shows how much of an actual 
FE is identified as an FE not penalizing wrongly 
identified part.  Since this evaluation is so lenient, 
we perform another evaluation to check exact 
matches. 
FE boundary 
Identification 
FE boundary 
Identification & 
Role labeling Method 
Prec Rec Prec Rec 
Test A 80.3 66.1 71.1 58.5 
Test B 100.0 99.0 86.7 85.8 
Table 5.  Exact match scores for the test set. 
4 Discussion and Conclusion 
Due to time limit, we?ve not done many 
experiments with different feature sets or 
thresholds in ME classification.  We expect that 
recall will increase with lower thresholds 
especially in lenient evaluation and the final 
performance will increase by optimizing 
parameters. 
References  
O. Bender, K. Macherey, F.J. Och, and H. Ney. 
2003.      Comparison of Alignment Templates 
and Maximum Entropy Models for Natural 
Language Processing. Proc. of EACL-2003. 
Budapest, Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 
1996. A Maximum Entropy Approach to Natural 
Language Proc. of Computational Linguistics, 
vol. 22, no. 1. 
E. Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. Proc. of NAACL-2000, Seattle, USA. 
S.F. Chen and R. Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMUCS-99-108, Carnegie 
Mellon University. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-Linear Models.  Annals 
of Mathematical Statistics, 43:1470-1480. 
C.Fillmore 1976. Frame Semantics and the Nature 
of Language.  Annals of the New York Academy 
of Science Conference on the Origin and 
Development of Language and Speech, Volume 
280 (pp. 20-32). 
M. Fleischman, N. Kwon, and E. Hovy. 2003. 
Maximum Entropy Models for FrameNet 
Classification. Proc. of Empirical Methods in 
Natural Language Processing conference 
(EMNLP), 2003. Sapporo, Japan.  
D. Gildea and D. Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3) 245-288 14. 
F.J. Och. 2002. Yet Another Maxent Toolkit: 
YASMET www-i6.informatik.rwth-aachen.de/ 
Colleagues/och/. 
C. Thompson, R. Levy, and C. Manning. 2003. A 
Generative Model for FrameNet Semantic Role 
Labeling. Proc. of the Fourteenth European 
Conference on Machine Learning, 2003. Croatia. 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 104?111, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
 Intentional Context in  
Situated Natural Language Learning  
 
Michael Fleischman and Deb Roy 
Cognitive Machines  
The Media Laboratory  
Massachusetts Institute of Technology 
mbf@mit.edu, dkroy@media.mit.edu 
 
Abstract 
Natural language interfaces designed for 
situationally embedded domains (e.g. 
cars, videogames) must incorporate 
knowledge about the users? context to 
address the many ambiguities of situated 
language use. We introduce a model of 
situated language acquisition that operates 
in two phases.  First, intentional context is 
represented and inferred from user actions 
using probabilistic context free grammars.  
Then, utterances are mapped onto this 
representation in a noisy channel 
framework.  The acquisition model is 
trained on unconstrained speech collected 
from subjects playing an interactive game, 
and tested on an understanding task. 
1 Introduction 
As information technologies move off of our 
desktops and into the world, the need for Natural 
Language Processing (NLP) systems that exploit 
information about the environment becomes 
increasingly apparent.  Whether in physical 
environments (for cars and cell phones) or in 
virtual ones (for videogames and training 
simulators), applications are beginning to demand 
language interfaces that can understand 
unconstrained speech about constrained domains.  
Unlike most text-based NLP research, which 
focuses on open-domain problems, work we refer 
to as situated NLP focuses on improving language 
processing by exploiting domain-specific 
information about the non-linguistic situational 
context of users? interactions.  For applications 
where agents interact in shared environments, such 
information is critical for successful 
communication. 
Previous work in situated NLP has focused on 
methods for grounding the meaning of words in 
physical and virtual environments.  The motivation 
for this work comes from the inability of text-
based NLP technologies to offer viable models of 
semantics for human computer interaction in 
shared environments.  For example, imagine a 
situation in which a human user is interacting with 
a robotic arm around a table of different colored 
objects.  If the human were to issue the command 
?give me the blue one,? both the manually-coded 
(Lenat, 1995; Fellbaum, 1998) and statistical 
models (Manning and Schutze, 2000) of meaning 
employed in text-based NLP are inadequate; for, in 
both models, the meaning of a word is based only 
on its relations to other words.  However, in order 
for the robot to successfully ?give me the blue 
one,? it must be able to link the meaning of the 
words in the utterance to its perception of the 
environment (Roy, Hsiao, & Mavridis, 2004).  
Thus, recent work on grounding meaning has 
focused on how words and utterances map onto 
physical descriptions of the environment: either in 
the form of perceptual representations (Roy, in 
press, Siskind, 2001, Regier, 1996) or control 
schemas (Bailey, 1997 Narayanan, 1999).1 
While such physical descriptions are useful 
representations for some classes of words (e.g., 
colors, shapes, physical movements), they are 
insufficient for more abstract language, such as 
that which denotes intentional action.  This 
insufficiency stems from the fact that intentional 
actions (i.e. actions performed with the purpose of 
achieving a goal) are highly ambiguous when 
described only in terms of their physically 
observable characteristics.  For example, imagine a 
situation in which one person moves a cup towards 
another person and utters the unknown word 
                                                 
1 Note that Narayanan?s work moves away from purely 
physical to metaphorical levels of description.  
104
 ?blicket.?  Now, based only on the physical 
description of this action, one might come to think 
of ?blicket? as meaning anything from ?give cup?, 
to ?offer drink?, to ?ask for change.?  This 
ambiguity stems from the lack of contextual 
information that strictly perceptual descriptions of 
action provide.  
This research presents a methodology for 
modeling the intentional context of utterances and 
describes how such representations can be used in 
a language learning task.  We decompose language 
learning into two phases: intention recognition and 
linguistic mapping.  In the first phase, we model 
intentional action using a probabilistic context free 
grammar.  We use this model to parse sequences of 
observed physical actions, thereby inferring a 
hierarchical tree representation of a user?s 
intentions.  In the second phase, we use a noisy 
channel model to learn a mapping between 
utterances and nodes in that tree representation.  
We present pilot situated language acquisition 
experiments using a dataset of paired spontaneous 
speech and action collected from human subjects 
interacting in a shared virtual environment.  We 
evaluate the acquired model on a situated language 
understanding task. 
2 Intention Recognition 
The ability to infer the purpose of others? actions 
has been proposed in the psychological literature 
as essential for language learning in children 
(Tommasello, 2003, Regier, 2003).  In order to 
understand how such intention recognition might 
be modeled in a computational framework, it is 
useful to specify the types of ambiguities that make 
intentional actions difficult to model.  Using as an 
example the situation involving the cup described 
above, we propose that this interaction 
demonstrates two distinct types of ambiguity.  The 
first type, which we refer to as a vertical ambiguity 
describes the ambiguity between the ?move cup? 
vs. ?offer drink? meanings of ?blicket.?  Here the 
ambiguity is based on the level of description that 
the speaker intended to convey.  Thus, while both 
meanings are correct (i.e., both meanings 
accurately describe the action), only one 
corresponds to the word ?blicket.?   
The second type of ambiguity, referred to as 
horizontal ambiguity describes the ambiguity 
between the ?offer drink? vs. ?ask for change? 
interpretations of ?blicket.?  Here there is an 
ambiguity based on what actually is the intention 
behind the physical action.  Thus, it is the case that 
only one of these meaning corresponds to ?blicket? 
and the other meaning is not an accurate 
description of the intended action. 
Figure 1 shows a graphical representation of 
these ambiguities.  Here the leaf nodes represent a 
basic physical description of the action, while the 
root nodes represent the highest-level actions for 
which the leaf actions were performed2.  Such a 
tree representation is useful in that it shows both 
the horizontal ambiguity that exists between the 
nodes labeled ?ask for change? and ?offer drink,? 
as well as the vertical ambiguity that exits between 
the nodes labeled ?offer drink? and ?move cup.? 
 
Figure 1:  Graphical representation of vertical and 
horizontal ambiguities for actions. 
 
In order to exploit the intuitive value of such a 
tree representation, we model intention recognition 
using probabilistic context free grammars 
(PCFG)3.  We develop a small set of production 
rules in which the left hand side represents a higher 
order intentional action (e.g., ?offer drink?), and 
the right hand side represents a sequence of lower 
level actions that accomplish it (e.g. ?grasp cup?, 
?move cup?, ?release cup?).  Each individual 
action (i.e. letter in the alphabet of the PCFG) is 
further modeled as a simple semantic frame that 
contains roles for an agent, an object, an action, 
and multiple optional modifier roles (see inset 
figure 1).  While in this initial work productions 
are created by hand (a task made feasible by the 
                                                 
2 In other words, high-level actions (e.g. ?be polite) are 
preformed by means of the performance of lower-level 
actions (e.g. ?offer drink?). 
3 The idea of a ?grammar of behavior? has a rich history 
in the cognitive sciences dating back at least to Miller et 
al., 1960 
O ffe r  D r in k
M o v e  C u p
o b s e r v e d  a c t io n
A c tio n :   O f fe r  
A g e n t:  P e r s o n 1
P a t ie n t:  P e r s o n 2
O b je c t:   D r in k
A s k  fo r  C h a n g e
B e  P o l i te
L i f t  C u p S l id e  C u p
? B lic k e t?
105
 constrained nature of situated domains) learning 
such rules automatically is discussed in section 4.2. 
Just as in the plan recognition work of Pynadath, 
(1999), we cast the problem of intention 
recognition as a probabilistic parsing problem in 
which sequences of physical actions are used to 
infer an abstract tree representation.  Resolving 
horizontal ambiguities thus becomes equivalent to 
determining which parse tree is most likely given a 
sequence of events.  Further, resolving vertical 
ambiguities corresponds to determining which 
level node in the inferred tree is the correct level of 
description that the speaker had in mind.   
3 Linguistic Mapping 
Given a model of intention recognition, the 
problem for a language learner becomes one of 
mapping spoken utterances onto appropriate 
constituents of their inferred intentional 
representations.  Given the intention representation 
above, this is equivalent to mapping all of the 
words in an utterance to the role fillers of the 
appropriate semantic frame in the induced 
intention tree.  To model this mapping procedure, 
we employ a noisy channel model in which the 
probability of inferring the correct meaning given 
an utterance is approximated by the (channel) 
probability of generating that utterance given that 
meaning, times the (source) prior probability of the 
meaning itself (see Equation 1). 
 
?)|( utterancemeaningp               (1) 
         )1()()|( ?? ?? meaningpmeaningutterancep  
 
Here utterance refers to some linguistic unit 
(usually a sentence) and meaning refers to some 
node in the tree (represented as a semantic frame) 
inferred during intention recognition4.  We can use 
the probability associated with the inferred tree (as 
given by the PCFG parser) as the source 
probability.  Further, we can learn the channel 
probabilities in an unsupervised manner using a 
variant of the EM algorithm similar to machine 
translation (Brown et al, 1993), and statistical 
language understanding (Epstein, 1996).   
4 Pilot Experiments 
4.1 Data Collection 
                                                 
4 ? refers to a weighting coefficient. 
In order to avoid the many physical and perceptual 
problems that complicate work with robots and 
sensor-grounded data, this work focuses on 
language learning in virtual environments.  We 
focus on multiplayer videogames , which support 
rich types of social interactions.  The complexities 
of these environments highlight the problems of 
ambiguous speech described above, and 
distinguish this work from projects characterized 
by more simplified worlds and linguistic 
interactions, such as SHRDLU (Winograd, 1972).  
Further, the proliferation of both commercial and 
military applications (e.g., Rickel et al, 2002) 
involving such virtual worlds suggests that they 
will continue to become an increasingly important 
area for natural language research in the future.   
 
 
Figure 2: Screen shot of Neverwinter Nights game used 
in experimentation. 
 
In order to test our model, we developed a virtual 
environment based on the multi-user videogame 
Neverwinter Nights.5  The game, shown in Figure 
2, provides useful tools for generating modules in 
which players can interact.  The game was 
instrumented such that all players? speech/text 
language and actions are recorded during game 
play.  For data collection, a game was designed in 
which a single player must navigate their way 
through a cavernous world, collecting specific 
objects, in order to escape.  Subjects were paired 
such that one, the novice, would control the virtual 
character, while the other, the expert, guided her 
through the world.  While the expert could say 
anything in order to tell the novice where to go and 
what to do, the novice was instructed not to speak, 
but only to follow the commands of the expert.  
 
                                                 
5 http://nwn.bioware.com/ 
106
 RightClickDoor RightClickFloor RightClickFloor RightClickFloor LeftClickDoor
?ok go into the room? ?go over to that door? ?now open the door?
Expert?s utterances:
Novice?s actions:
RightClickDoor RightClickFloor RightClickFloor RightClickFloor LeftClickDoor
MoveThruRoomOpenDoor OpenDoor
FindAxe PickUpAxe
GetAxe
Intention Recognition
Action:  Get
Agent:   Player
Object:  Axe
GetAxe -> GoToAxe TakeAxe
FindAxe -> Open Move Open
OpenDoor -> ClickDoor
Behavior Grammar
Action:  Open
Agent:   Player
Object:  Door
RightClickDoor RightClickFloor RightClickFloor RightClickFloor LeftClickDoor
MoveThruRoomOpenDoor OpenDoor
FindAxe PickUpAxe
GetAxe
Linguistic Mapping
?now open the door?
P(words|roles)
Figure 3.  Experimental methodology: a) subjects? speech and action sequences are recorded; b) an intentional tree is 
inferred over the sequence of observed actions using a PCFG parser; c) the linguistic mapping algorithm examines 
the mappings between the utterance and all possible nodes to learn the best mapping of words given semantic roles. 
 
The purpose behind these restrictions was to elicit 
free and spontaneous speech that is only 
constrained by the nature of the task.  This 
environment seeks to emulate the type of speech 
that a real situated language system might 
encounter: i.e., natural in its characteristics, but 
limited in its domain of discourse.  
The subjects in the data collection were 
university graduate and undergraduate students.  
Subjects (8 male, 4 female) were staggered such 
that the novice in one trial became the expert in the 
next.  Each pair played the game at least five times, 
and for each of those trials, all speech from the 
expert and all actions from the novice were 
recorded.  Table 1 shows examples of utterances 
recorded from game play, the observed actions 
associated with them, and the actions? inferred 
semantic frame. 
 
Utterance Action Frame 
ok this time you are 
gonna get the axe first 
MOVE  
ROOM1 
act: GET  
obj: AXE 
through the red archway 
on your right 
MOVE  
ROOM2 
act: MOVE 
goal: ARCH 
manner: THRU 
now open that door CLICK_ON 
LEVER 
act: OPEN  
obj: DOOR 
ok now take the axe CLICK_ON 
CHEST 
act: TAKE 
obj: AXE 
source: CHEST 
Table 1: Representative test utterances collected from 
subjects with associated game actions and frames 
 
Data collection produces two parallel streams of 
information: the sequence of actions taken by the 
novice and the audio stream produced by the 
expert (figure 3a).  The audio streams are 
automatically segmented into utterances using a 
speech endpoint detector, which are then 
transcribed by a human annotator.  Each action in 
the sequence is then automatically parsed, and each 
node in the tree is replaced with a semantic frame 
(figure 3b).6  The data streams are then fed into the 
linguistic mapping algorithms as a parallel corpus 
of the expert?s transcribed utterances and the 
inferred semantic roles associated with the 
novice?s actions (figure 3c). 
4.2 Algorithms 
Intention Recognition 
 
As described in section 2, we represent the task 
model associated with the game as a set of 
production rules in which the right hand side 
consists of an intended action (e.g. ?find key?) and 
the left hand side consists of a sequence of sub-
actions that are sufficient to complete that action 
(e.g. ?go through door, open chest, pick_up key?).  
By applying probabilities to the rules, intention 
recognition can be treated as a probabilistic context 
free parsing problem, following Pynadath, 1999.  
For these initial experiments we have hand-
annotated the training data in order to generate the 
grammar used for intention recognition, estimating 
their maximum likelihood probabilities over the 
training set.  In future work, we intend to examine 
how such grammars can be learned in conjunction 
with the language itself; extending research on 
learning task models (Nicolescu and Mataric, 
2003) and work on learning PCFGs (Klein and 
Manning, 2004) with our own work on 
unsupervised language learning. 
Given the PCFG, we use a probabilistic Earley 
parser (Stolcke, 1994), modified slightly to output 
                                                 
6 We use 65 different frames, comprised of 35 unique 
role fillers. 
107
 partial trees (with probabilities) as each action is 
observed.  Figure 4 shows a time slice of an 
inferred intention tree after a player mouse clicked 
on a lever in the game.  Note that both the vertical 
and horizontal ambiguities that exist for this action 
in the game parallel the ambiguities shown in 
Figure 1.  As described above, each node in the 
tree is represented as a semantic frame (see figure 
4 insets), whose roles are aligned to the words in 
the utterances during the linguistic mapping phase. 
 
Linguistic Mapping 
 
The problem of learning a mapping between 
linguistic labels and nodes in an inferred 
intentional tree is recast as one of learning the 
channel probabilities in Equation 1.  Each node in 
a tree is treated as a simple semantic frame and the 
role fillers in these frames, along with the words in 
the utterances, are treated as a parallel corpus.  
This corpus is used as input to a standard 
Expectation Maximization algorithm that estimates 
the probabilities of generating a word given the 
occurrence of a role filler.  We follow IBM Model 
1 (Brown et al, 1993) and assume that each word 
in an utterance is generated by exactly one role in 
the parallel frame  
Using standard EM to learn the role to word 
mapping is only sufficient if one knows to which 
level in the tree the utterance should be mapped.  
However, because of the vertical ambiguity 
inherent in intentional actions, we do not know in 
advance which is the correct utterance-to-level 
mapping.  To account for this, we extend the 
standard EM algorithm as follows (see figure 3c): 
1) set uniform likelihoods for all utterance-to-
level mappings  
2) for each mapping, run standard EM 
3) merge output distributions of EM (weighting 
each by its mapping likelihood) 
4) use merged distribution to recalculate 
likelihoods of all utterance-to-level mappings 
5) goto step 2 
4.3 Experiments  
Methodologies for evaluating language acquisition 
tasks are not standardized.  Given our model, there 
exists the possibility of employing intrinsic 
measures of success, such as word alignment 
accuracy.  However, we choose to measure the 
success of learning by examining the related (and 
more natural) task of language understanding.   
For each subject pair, the linguistic mapping 
algorithms are trained on the first four trials of 
game play and tested on the final trial.  (This gives 
on average 130 utterances of training data and 30 
utterances of testing data per pair.)  For each 
utterance in the test data, we calculate the 
likelihood that it was generated by each frame seen 
in testing.  We select the maximum likelihood 
frame as the system?s hypothesized meaning for 
the test utterance, and examine both how often the 
maximum likelihood estimate exactly matches the 
true frame (frame accuracy), and how many of the 
role fillers within the estimated frame match the 
role fillers of the true frame (role accuracy).7   
 
Figure 4: Inferred intention tree (with semantic 
frames) from human subject game play. 
 
For each subject, the algorithm?s parameters are 
optimized using data from all other subjects. We 
assume correct knowledge of the temporal 
alignment between utterances and actions.  In 
future work, we will relax this assumption to 
explore the effects of not knowing which actions 
correspond to which utterances in time. 
To examine the performance of the model, three 
experiments are presented.  Experiment 1 
examines the basic performance of the algorithms 
on the language understanding task described 
above given uniform priors.  The system is tested 
under two conditions: 1) using the extended EM 
algorithm given an unknown utterance-to-level 
alignment, and 2) using the standard EM algorithm 
given the correct utterance-to-level alignment.   
Experiment 2 tests the benefit of incorporating 
intentional context directly into language 
understanding.  This is done by using the parse 
probability of each hypothesized intention as the 
                                                 
7 See Fleischman and Roy (2005) for experiments 
detailing performance on specific word categories. 
F in d  K e y  E x it  L e v e l
G o  T h r o u g h  D o o r
O p e n  D o o r
P u l l  L e v e r T u r n  K n o b
c l ic k _ o n  l e v e r
A c t io n :  M o v e  
A g e n t:  P la y e r
O b je c t:  D o o r
M a n n e r : T h ro u g h
A c tio n :  G e t  
A g e n t:  P la y e r
O b je c t:  K e y
S o u r c e : C h e s t
A c t io n :  E x it
A g e n t:  
P la y e r
O b je c t:  L e v e l
A c t io n :  O p e n
A g e n t:  
P la y e r
O b je c t:  D o o r
108
 source probability in Equation 1.  Thus, given an 
utterance to understand, we cycle through all 
possible actions in the grammar, parse each one as 
if it were observed, and use the probability 
generated by the parser as its prior probability.  By 
changing the weighting coefficient (?) between the 
source and channel probabilities, we show the 
range of performances of the system from using no 
context at all (?=1) to using only context itself 
(?=0) in understanding.   
 
 
 
 
 
 
 
 
 
 
 
 
Figure 5: Comparison of models trained with utterance-
to-level alignment both known and unknown.  
Performance is on a language understanding task 
(baseline equivalent to choosing most frequent frame) 
 
Experiment 3 studies to what extent inferred tree 
structures are necessary when modeling language 
acquisition.  Although, in section 1, we have 
presented intuitive reasons why such structures are 
required, one might argue that inferring trees over 
sequences of observed actions might not actually 
improve understanding performance when 
compared to a model trained only on the observed 
actions themselves.  This hypothesis is tested by 
comparing a model trained given the correct 
utterance-to-level alignment (described in 
experiment 1) with a model in which each 
utterance is aligned to the leaf node (i.e. observed 
action) below the correct level of alignment.  For 
example, in figure 4, this would correspond to 
mapping the utterance ?go through the door?, not 
to ?GO THROUGH DOOR?, but rather to 
?CLICK_ON LEVER.?   
4.4 Results 
Experiment 1: We present the average performance 
over all subject pairs, trained with the correct 
utterance-to-level alignment both known and 
unknown, and compare it to a baseline of choosing 
the most frequent frame from the training data.  
Figure 5 shows the percentage of maximum 
likelihood frames chosen by the system that 
exactly match the intended frame (frame 
accuracy), as well as, the percentage of roles from 
the maximum likelihood frame that overlap with 
roles in the intended frame (role accuracy). 
As expected, the understanding performance 
goes down for both frames and roles when the 
correct utterance-to-level alignment is unknown.  
Interestingly, while the frame performance 
declines by 14.3%, the performance on roles only 
declines 6.4%.  This difference is due primarily to 
the fact that, while the mapping from words to 
action role fillers is hindered by the need to 
examine all alignments, the mapping from words 
to object role fillers remains relatively robust.  This 
is due to the fact that while each level of intention 
carries a different action term, often the objects 
described at different levels remain the same.  For 
example, in figure 4, the action fillers ?TAKE?, 
?MOVE?, ?OPEN?, and ?PULL? occur only once 
along the path.  However, the object filler 
?DOOR? occurs multiple times.  Thus, the chance 
that the role filler ?DOOR? correctly maps to the 
word ?door? is relatively high compared to the role 
filler ?OPEN? mapping to the word ?open.?8  
 
 
 
 
 
 
 
 
 
 
Figure 6: Frame accuracy as a function of ? value (Eq. 
1) trained on unknown utterance-to-level alignments. 
 
Experiment 2: Figure 6 shows the average frame 
accuracy of the system trained without knowing 
the correct utterance-to-level alignment, as a 
function of varying the ? values from Equation 1.  
The graph shows that including intentional context 
does improve system performance when it is not 
given too much weight (i.e., at relatively high 
alpha values).  This suggests that the benefit of 
intentional context is somewhat outweighed by the 
power of the learned role to word mappings.  
                                                 
8 This asymmetry for learning words about actions vs. 
objects is well known in psychology (Gleitman, 1990) 
and is addressed directly in Fleischman and Roy, 2005. 
2 5 %
2 7 %
2 9 %
3 1 %
3 3 %
3 5 %
3 7 %
3 9 %
4 1 %
4 3 %
4 5 %
4 7 %
4 9 %
0 . 2 0 . 4 0 . 6 0 . 8 1
?
Fr
am
e 
A
cc
ur
ac
y
0 %
1 0 %
2 0 %
3 0 %
4 0 %
5 0 %
6 0 %
7 0 %
8 0 %
9 0 %
f r a m e  a c c u r a c y r o le  a c c u r a c y
b a s e l in e u n k n o w n  le v e l k n o w n  le v e l
109
 Looking closer, we find a strong negative 
correlation (r=-0.81) between the understanding 
performance using only channel probabilities (?=1) 
and the improvement obtained by including the 
intentional context.  In other words, the better one 
does without context, the less context improves 
performance.  Thus, we expect that in noisier 
environments (such as when speech recognition is 
employed) where channel probabilities are less 
reliable, employing intentional context will be 
even more advantageous. 
 
Experiment 3: Figure 7 shows the average 
performance on both frame and role accuracy for 
systems trained without using the inferred tree 
structure (on leaf nodes only) and on the full tree 
structure (given the correct utterance-to-level 
alignment).  Baselines are calculated by choosing 
the most frequent frame from training.9 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
frame accuracy role accuracy
baseline (observed) observed baseline (inferred) inferred
 
Figure 7: Comparison of models trained on inferred 
intentional tree vs. directly on observed actions  
 
It is clear from the figure that understanding 
performance is higher when the intentional tree is 
used in training.  This is a direct result of the fact 
that speakers often speak about high-level 
intentions with words that do not directly refer to 
the observed actions.  For example, after opening a 
door, experts often say: ?go through the door,? for 
which the observed action is a simple movement 
(e.g., ?MOVE ROOMx?).  Also, by referring to 
high-level intentions, experts can describe 
sequences of actions that are not immediately 
referred to.  For example, an expert might say: ?get 
the key? to describe a sequence of actions that 
begins with ?CLICK_ON CHEST.?  Thus, the 
result of not learning over a parsed hierarchical 
                                                 
9 Note that baselines are different for the two conditions, 
because there are a differing number of frames used in 
the leaf node only condition.   
representation of intentions is increased noise, and 
subsequently, poorer understanding performance.  
5 Discussion 
The results from these experiments, although 
preliminary, indicate that this model of language 
acquisition performs well above baseline on a 
language understanding task.  This is particularly 
encouraging given the unconstrained nature of the 
speech on which it was trained.  Thus, even free 
and spontaneous speech can be handled when 
modeling a constrained domain of discourse.10   
In addition to performing well given difficult 
data, the experiments demonstrate the advantages 
of using an inferred intentional representation both 
as a contextual aid to understanding and as a 
representational scaffolding for language learning.  
More important than these preliminary results, 
however, is the general lesson that this work 
suggests about the importance of knowledge 
representations for situated language acquisition.   
As discussed in section 2, learning language 
about intentional action requires dealing with two 
distinct types of ambiguity.  These difficulties 
cannot be handled by merely increasing the 
amount of data used, or switching to a more 
sophisticated learning algorithm.  Rather, dealing 
with language use for situated applications requires 
building appropriate knowledge representations 
that are powerful enough for unconstrained 
language, yet scalable enough for practical 
applications.  The work presented here is an initial 
demonstration of how the semantics of 
unconstrained speech can be modeled by focusing 
on constrained domains.   
As for scalability, it is our contention that for 
situated NLP, it is not a question of being able to 
scale up a single model to handle open-domain 
speech. The complexity of situated communication 
requires the use of domain-specific knowledge for 
modeling language use in different contexts.  Thus, 
with situated NLP systems, it is less productive to 
focus on how to scale up single models to operate 
beyond their original domains.  Rather, as more 
individual applications are tackled (e.g. cars, 
                                                 
10 Notably, situated applications for which natural 
language interfaces are required typically have limited 
domains (e.g., talking to one?s car doesn?t require open-
domain language processing). 
110
 phones, videogames, etc.) the interesting question 
becomes one of how agents can learn to switch 
between different models of language as they 
interact in different domains of discourse. 
6 Conclusion 
We have introduced a model of language 
acquisition that explicitly incorporates intentional 
contexts in both learning and understanding.  We 
have described pilot experiments on paired 
language and action data in order to demonstrate 
both the model?s feasibility as well as the efficacy 
of using intentional context in understanding.  
Although we have demonstrated a first step toward 
an advanced model of language acquisition, there 
is a great deal that has not been addressed.  First, 
what is perhaps most obviously missing is any 
mention of syntax in the language learning process 
and its role in bootstrapping for language 
acquisition.  Future work will focus on moving 
beyond the IBM Model 1 assumptions, to develop 
more syntactically-structured models.   
Further, although the virtual environment used in 
this research bears similarity to situated 
applications that demand NL interfaces, it is not 
known exactly how well the model will perform 
?in the real world.?  Future work will examine 
installing models in real world applications. In 
parallel investigations, we will explore our method 
as a cognitive model of human language learning. 
Finally, as was mentioned previously, the task 
model for this domain was hand annotated and, 
while the constrained nature of the domain 
simplified this process, further work is required to 
learn such models jointly with language.   
In summary, we have presented first steps 
toward tackling problems of ambiguity inherent in 
grounding the semantics of situated language. We 
believe this work will lead to practical applications 
for situated NLP, and provide new tools for 
modeling human cognitive structures and 
processes underlying situated language use 
(Fleischman and Roy, 2005). 
Acknowledgments 
Peter Gorniak developed the software to capture 
data from the videogame used in our experiments. 
References 
D. Bailey, J Feldman, S. Narayanan., & G. Lakoff.. 
Embodied lexical development. 19th Cognitive 
Science Society Meeting. Mahwah, NJ, 1997. 
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & 
R. L. Mercer. ?The Mathematics of Sta-tistical 
Machine Translation: Parameter Estimation,? 
Computational Linguistics 19(2). 1993. 
M Epstein  Statistical Source Channel Models for 
Natural Language Understanding Ph. D. thesis, 
New York University, September, 1996  
C. Fellbaum WordNet: An On-line Lexical Database 
and Some of its Applications. MIT Press, 1998. 
M. Fleischman and D.K. Roy.  Why Verbs are 
Harder to Learn than Nouns: Initial Insights from a 
Computational Model of Intention Recognition in 
Situated Word Learning.  CogSci. Italy, 2005. 
L. Gleitman. "The structural sources of verb 
meanings." Language Acquisition, 1(1), 1990. 
D. Klein and C. Manning, "Corpus-Based Induction 
of Syntactic Structure: Models of Dependency and 
Constituency", Proc. of the 42nd ACL, 2004 [ 
D. B Lenat,. CYC: A Large-Scale Investment in 
Knowledge Infrastructure". Comm. of ACM, 1995. 
C. Manning, H. Schutze,. Foundations of Statistical 
Natural Language Processing. MIT Press, 2001. 
G. A. Miller, E. Galanter, and K. Pribram 1960. Plans 
and the Structure of Behavior. New York: Halt. 
S. Narayanan.. Moving right along: A computational 
model of metaphoric reasoning about events. In 
Proc. of AAAI. Orlando, FL, 1999. 
M. Nicolescu, M. Mataric?, Natural Methods for 
Robot Task Learning: Instructive Demonstration, 
Generalization and Practice, AGENTS, Australia, 2003.  
D. Pynadath, 1999. Probabilistic Grammars for Plan 
Recognition. Ph.D. Thesis, University of Michigan.  
T. Regier. The human semantic potential. MIT Press, 
Cambridge, MA, 1996. 
T. Regier. Emergent constraints on word-learning: A 
computational review. TICS, 7, 263-268, 2003.  
J. Rickel, S. Marsella, J. Gratch, R. Hill, D. Traum 
and W. Swartout, "Towards a New Generation of 
Virtual Humans for Interactive Experiences," in 
IEEE Intelligent Systems July/August 2002. 
D.Roy, K. Hsiao, and N. Mavridis. Mental imagery 
for a conversational robot. IEEE Trans. on 
Systems, Man, and Cybernetics, 34(3) 2004. 
D. Roy. (in press). Grounding Language in the 
World: Schema Theory Meets Semiotics. AI. 
J.  Siskind. Grounding the Lexical Semantics of 
Verbs in Visual Perception using Force Dynamics 
and Event Logic. JAIR, 2001. 
A. Stolcke. Bayesian Learning of Probabilistic 
Language Models. Ph.d., UC Berkeley, 1994. 
M. Tomasello. Constructing a Language: A Usage-
Based Theory of Language Acquisition. Harvard 
University Press, 2003. 
T. Winograd. Understanding Natural Language. 
Academic Press, 1972. 
111
