Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 18?25,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LEILA: Learning to Extract Information by Linguistic Analysis
Fabian M. Suchanek
Max-Planck-Institute
for Computer Science
Saarbru?cken/Germany
suchanek@mpii.mpg.de
Georgiana Ifrim
Max-Planck-Institute
for Computer Science
Saarbru?cken/Germany
ifrim@mpii.mpg.de
Gerhard Weikum
Max-Planck-Institute
for Computer Science
Saarbru?cken/Germany
weikum@mpii.mpg.de
Abstract
One of the challenging tasks in the con-
text of the Semantic Web is to automati-
cally extract instances of binary relations
from Web documents ? for example all
pairs of a person and the corresponding
birthdate. In this paper, we present LEILA,
a system that can extract instances of ar-
bitrary given binary relations from natu-
ral language Web documents ? without
human interaction. Different from previ-
ous approaches, LEILA uses a deep syn-
tactic analysis. This results in consistent
improvements over comparable systems
(such as e.g. Snowball or TextToOnto).
1 Introduction
1.1 Motivation
Search engines, question answering systems and
classification systems alike can greatly profit from
formalized world knowledge. Unfortunately, man-
ually compiled collections of world knowledge
(such as e.g. WordNet (Fellbaum, 1998)) often
suffer from low coverage, high assembling costs
and fast aging. In contrast, the World Wide Web
provides an enormous source of knowledge, as-
sembled by millions of people, updated constantly
and available for free. Since the Web data con-
sists mostly of natural language documents, a first
step toward exploiting this data would be to ex-
tract instances of given target relations. For exam-
ple, one might be interested in extracting all pairs
of a person and her birthdate (the birthdate-
relation), pairs of a company and the city of its
headquarters (the headquarters-relation) or
pairs of an entity and the concept it belongs to (the
instanceOf-relation). The task is, given a set
of Web documents and given a target relation, ex-
tracting pairs of entities that are in the target rela-
tion. In this paper, we propose a novel method for
this task, which works on natural language Web
documents and does not require human interac-
tion. Different from previous approaches, our ap-
proach involves a deep linguistic analysis, which
helps it to achieve a superior performance.
1.2 Related Work
There are numerous Information Extraction (IE)
approaches, which differ in various features:
? Arity of the target relation: Some systems are
designed to extract unary relations, i.e. sets of
entities (Finn and Kushmerick, 2004; Califf and
Mooney, 1997). In this paper we focus on the
more general binary relations.
? Type of the target relation: Some systems
are restricted to learning a single relation,
mostly the instanceOf-relation (Cimiano
and Vo?lker, 2005b; Buitelaar et al, 2004).
In this paper, we are interested in extracting
arbitrary relations (including instanceOf).
Other systems are designed to discover new
binary relations (Maedche and Staab, 2000).
However, in our scenario, the target relation is
given in advance.
? Human interaction: There are systems that re-
quire human intervention during the IE process
(Riloff, 1996). Our work aims at a completely
automated system.
? Type of corpora: There exist systems that can
extract information efficiently from formatted
data, such as HTML-tables or structured text
(Graupmann, 2004; Freitag and Kushmerick,
2000). However, since a large part of the Web
consists of natural language text, we consider in
this paper only systems that accept also unstruc-
tured corpora.
? Initialization: As initial input, some systems
require a hand-tagged corpus (J. Iria, 2005;
Soderland et al, 1995), other systems require
text patterns (Yangarber et al, 2000) or tem-
plates (Xu and Krieger, 2003) and again oth-
ers require seed tuples (Agichtein and Gravano,
2000; Ruiz-Casado et al, 2005; Mann and
Yarowsky, 2005) or tables of target concepts
(Cimiano and Vo?lker, 2005a). Since hand-
18
labeled data and manual text patterns require
huge human effort, we consider only systems
that use seed pairs or tables of concepts.
Furthermore, there exist systems that use the
whole Web as a corpus (Etzioni et al, 2004) or that
validate their output by the Web (Cimiano et al,
2005). In order to study different extraction tech-
niques in a controlled environment, however, we
restrict ourselves to systems that work on a closed
corpus for this paper.
One school of extraction techniques concen-
trates on detecting the boundary of interesting en-
tities in the text, (Califf and Mooney, 1997; Finn
and Kushmerick, 2004; Yangarber et al, 2002).
This usually goes along with the restriction to
unary target relations. Other approaches make
use of the context in which an entity appears
(Cimiano and Vo?lker, 2005a; Buitelaar and Ra-
maka, 2005). This school is mostly restricted to
the instanceOf-relation. The only group that
can learn arbitrary binary relations is the group
of pattern matching systems (Etzioni et al, 2004;
Agichtein and Gravano, 2000; Ravichandran and
Hovy, 2002; Brin, 1999; Soderland, 1999; Xu et
al., 2002; Ruiz-Casado et al, 2005; Mann and
Yarowsky, 2005). Surprisingly, none of these sys-
tems uses a deep linguistic analysis of the cor-
pus. Consequently, most of them are extremely
volatile to small variations in the patterns. For ex-
ample, the simple subordinate clause in the fol-
lowing example (taken from (Ravichandran and
Hovy, 2002)) can already prevent a surface pat-
tern matcher from discovering a relation between
?London? and the ?river Thames?: ?London, which has
one of the busiest airports in the world, lies on the banks
of the river Thames.?
1.3 Contribution
This paper presents LEILA (Learning to Extract
Information by Linguistic Analysis), a system that
can extract instances of an arbitrary given binary
relation from natural language Web documents
without human intervention. LEILA uses a deep
analysis for natural-language sentences as well as
other advanced NLP methods like anaphora reso-
lution, and combines them with machine learning
techniques for robust and high-yield information
extraction. Our experimental studies on a variety
of corpora demonstrate that LEILA achieves very
good results in terms of precision and recall and
outperforms the prior state-of-the-art methods.
1.4 Link Grammars
There exist different approaches for parsing nat-
ural language sentences. They range from sim-
ple part-of-speech tagging to context-free gram-
mars and more advanced techniques such as Lex-
ical Functional Grammars, Head-Driven Phrase
Structure Grammars or stochastic approaches. For
our implementation, we chose the Link Grammar
Parser (Sleator and Temperley, 1993). It is based
on a context-free grammar and hence it is simpler
to handle than the advanced parsing techniques.
At the same time, it provides a much deeper se-
mantic structure than the standard context-free
parsers. Figure 1 shows a simplified example of
a linguistic structure produced by the link parser
(a linkage).
A linkage is a connected planar undirected
graph, the nodes of which are the words of the sen-
tence. The edges are called links. They are labeled
with connectors. For example, the connector subj
in Figure 1 marks the link between the subject and
the verb of the sentence. The linkage must ful-
fill certain linguistic constraints, which are given
by a link grammar. The link grammar specifies
which word may be linked by which connector to
preceding and following words. Furthermore, the
parser assigns part-of-speech tags, i.e. symbols
identifying the grammatical function of a word in
a sentence. In the example in Figure 1, the let-
ter ?n? following the word ?composers? indentifies
?composers? as a noun.
Chopin was.v     great  among the composers.n of   his  time.n
subj compl mod
prepObj
mod
prepObj
detdet
Figure 1: A simple linkage
Figure 2 shows how the Link Parser copes with a
more complex example. The relationship between
the subject ?London? and the verb ?lies? is not dis-
rupted by the subordinate clause:
London, which has one of the busiest airports, lies on the banks of the river Thames. 
subj
mod subj obj prep
prepObj
det
sup mod
prepObj
det mod
prepObj
det grp
Figure 2: A complex linkage
We say that a linkage expresses a relation r, if
the underlying sentence implies that a pair of enti-
ties is in r. Note that the deep grammatical anal-
ysis of the sentence would allow us to define the
meaning of the sentence in a theoretically well-
founded way (Montague, 1974). For this paper,
however, we limit ourselves to an intuitive under-
standing of the notion of meaning.
We define a pattern as a linkage in which two
19
words have been replaced by placeholders. Figure
3 shows a pattern derived from the linkage in Fig-
ure 1 by replacing ?Chopin? and ?composers? by the
placeholders ?X? and ?Y?.
    X       was.v       great  among the       Y        of  his    time.n
subj compl mod
prepObj
mod
prepObj
detdet
Figure 3: A pattern
We call the (unique) shortest path from one
placeholder to the other the bridge, marked in bold
in the figure. The bridge does not include the
placeholders. Two bridges are regarded as equiva-
lent, if they have the same sequence of nodes and
edges, although nouns and adjectives are allowed
to differ. For example, the bridge in Figure 3 and
the bridge in Figure 4 (in bold) are regarded as
equivalent, because they are identical except for
a substitution of ?great? by ?mediocre?. A pattern
matches a linkage, if an equivalent bridge occurs
in the linkage. For example, the pattern in Figure
3 matches the linkage in Figure 4.
Mozart was.v clearly mediocre  among the composers.n.
subj
compl
mod
prepObj
detmod
Figure 4: A matching linkage
If a pattern matches a linkage, we say that the
pattern produces the pair of words that the link-
age contains in the position of the placeholders.
In Figure 4, the pair ?Mozart? / ?composers? is pro-
duced by the pattern in Figure 3.
2 System Description
2.1 Document Pre-Processing
LEILA accepts HTML documents as input. To
allow the system to handle date and number ex-
pressions, we normalize these constructions by
regular expression matching in combination with
a set of functions. For example, the expression
?November 23rd to 24th 1998? becomes ?1998-11-23
to 1998-11-24? and the expression ?0.8107 acre-feet?
becomes ?1000 cubic-meters?. Then, we split the
original HTML-document into two files: The first
file contains the proper sentences with the HTML-
tags removed. The second file contains the non-
grammatical parts, such as lists, expressions us-
ing parentheses and other constructions that can-
not be handled by the Link Parser. For example,
the character sequence ?Chopin (born 1810) was a
great composer? is split into the sentence ?Chopin
was a great composer? and the non-grammatical in-
formation ?Chopin (born 1810)?. The grammatical
file is parsed by the Link Parser.
The parsing allows for a restricted named entity
recognition, because the parser links noun groups
like ?United States of America? by designated con-
nectors. Furthermore, the parsing allows us to do
anaphora resolution. We use a conservative ap-
proach, which simply replaces a third person pro-
noun by the subject of the preceding sentence.
For our goal, it is essential to normalize nouns
to their singular form. This task is non-trivial,
because there are numerous words with irregular
plural forms and there exist even word forms that
can be either the singular form of one word or the
plural form of another. By collecting these excep-
tions systematically from WordNet, we were able
to stem most of them correctly with our Plural-to-
Singular Stemmer (PlingStemmer1). For the non-
grammatical files, we provide a pseudo-parsing,
which links each two adjacent items by an artifi-
cial connector. As a result, the uniform output of
the preprocessing is a sequence of linkages, which
constitutes the input for the core algorithm.
2.2 Core Algorithm
As a definition of the target relation, our algorithm
requires a function (given by a Java method) that
decides into which of the following categories a
pair of words falls:
? The pair can be an example for the target re-
lation. For instance, for the birthdate-
relation, the examples can be given by a list of
persons with their birth dates.
? The pair can be a counterexample. For the
birthdate-relation, the counterexamples can
be deduced from the examples (e.g. if ?Chopin?
/ ?1810? is an example, then ?Chopin? / ?2000?
must be a counterexample).
? The pair can be a candidate. For birthdate,
the candidates would be all pairs of a proper
name and a date that are not an example or a
counterexample.
? The pair can be none of the above.
The core algorithm proceeds in three phases:
1. In the Discovery Phase, it seeks linkages in
which an example pair appears. It replaces the
two words by placeholders, thus producing a
pattern. These patterns are collected as positive
patterns. Then, the algorithm runs through the
sentences again and finds all linkages that match
1available at http://www.mpii.mpg.de/ ?suchanek
20
a positive pattern, but produce a counterexam-
ple. The corresponding patterns are collected as
negative patterns2.
2. In the Training Phase, statistical learning is ap-
plied to learn the concept of positive patterns.
The result of this process is a classifier for pat-
terns.
3. In the Testing Phase, the algorithm considers
again all sentences in the corpus. For each link-
age, it generates all possible patterns by replac-
ing two words by placeholders. If the two words
form a candidate and the pattern is classified as
positive, the produced pair is proposed as a new
element of the target relation (an output pair).
In principle, the core algorithm does not depend on
a specific grammar or a specific parser. It can work
on any type of grammatical structures, as long as
some kind of pattern can be defined on them. It is
also possible to run the Discovery Phase and the
Testing Phase on different corpora.
2.3 Learning Model
The central task of the Discovery Phase is deter-
mining patterns that express the target relation.
These patterns are generalized in the Training
Phase. In the Testing Phase, the patterns are used
to produce the output pairs. Since the linguistic
meaning of the patterns is not apparent to the sys-
tem, the Discovery Phase relies on the following
hypothesis: Whenever an example pair appears
in a sentence, the linkage and the corresponding
pattern express the target relation. This hypoth-
esis may fail if a sentence contains an example
pair merely by chance, i.e. without expressing the
target relation. Analogously, a pattern that does
express the target relation may occasionally pro-
duce counterexamples. We call these patterns false
samples. Virtually any learning algorithm can deal
with a limited number of false samples.
To show that our approach does not depend
on a specific learning algorithm, we implemented
two classifiers for LEILA: One is an adaptive k-
Nearest-Neighbor-classifier (kNN) and the other
one uses a Support Vector Machine (SVM). These
classifiers, the feature selection and the statistical
model are explained in detail in (Suchanek et al,
2006). Here, we just note that the classifiers yield
a real valued label for a test pattern. This value
can be interpreted as the confidence of the classifi-
cation. Thus, it is possible to rank the output pairs
of LEILA by their confidence.
2Note that different patterns can match the same linkage.
3 Experiments
3.1 Setup
We ran LEILA on different corpora with increasing
heterogeneity:
? Wikicomposers: The set of all Wikipedia arti-
cles about composers (872 HTML documents).
We use it to see how LEILA performs on a docu-
ment collection with a strong structural and the-
matic homogeneity.
? Wikigeography: The set of all Wikipedia
pages about the geography of countries (313
HTML documents).
? Wikigeneral: A set of random Wikipedia arti-
cles (78141 HTML documents). We chose it to
assess LEILA?s performance on structurally ho-
mogenous, but thematically random documents.
? Googlecomposers: This set contains one doc-
ument for each baroque, classical, and roman-
tic composer in Wikipedia?s list of composers,
as delivered by a Google ?I?m feeling lucky?
search for the composer?s name (492 HTML
documents). We use it to see how LEILA per-
forms on a corpus with a high structural hetero-
geneity. Since the querying was done automat-
ically, the downloaded pages include spurious
advertisements as well as pages with no proper
sentences at all.
We tested LEILA on different target relations with
increasing complexity:
? birthdate: This relation holds between a person
and his birth date (e.g. ?Chopin? / ?1810?). It is
easy to learn, because it is bound to strong sur-
face clues (the first element is always a name,
the second is always a date).
? synonymy: This relation holds between two
names that refer to the same entity (e.g.
?UN?/?United Nations?). The relation is more so-
phisticated, since there are no surface clues.
? instanceOf: This relation is even more sophis-
ticated, because the sentences often express it
only implicitly.
We compared LEILA to different competitors. We
only considered competitors that, like LEILA, ex-
tract the information from a corpus without using
other Internet sources. We wanted to avoid run-
ning the competitors on our own corpora or on our
own target relations, because we could not be sure
to achieve a fair tuning of the competitors. Hence
we ran LEILA on the corpora and the target rela-
tions that our competitors have been tested on by
their authors. We compare the results of LEILA
with the results reported by the authors. Our com-
petitors, together with their respective corpora and
relations, are:
21
? TextToOnto3: A state-of-the-art representative
for non-deep pattern matching. The system pro-
vides a component for the instanceOf rela-
tion and takes arbitrary HTML documents as in-
put. For completeness, we also consider its suc-
cessor Text2Onto (Cimiano and Vo?lker, 2005a),
although it contains only default methods in its
current state of development.
? Snowball (Agichtein and Gravano, 2000):
A recent representative of the slot-extraction
paradigm. In the original paper, Snowball has
been tested on the headquarters relation.
This relation holds between a company and the
city of its headquarters. Snowball was trained
on a collection of some thousand documents
and then applied to a test collection. For copy-
right reasons, we only had access to the test col-
lection (150 text documents).
? (Cimiano and Vo?lker, 2005b) present a new sys-
tem that uses context to assign a concept to
an entity. We will refer to this system as the
CV-system. The approach is restricted to the
instanceOf-relation, but it can classify in-
stances even if the corpus does not contain ex-
plicit definitions. In the original paper, the sys-
tem was tested on a collection of 1880 files from
the Lonely Planet Internet site4.
For the evaluation, the output pairs of the sys-
tem have to be compared to a table of ideal pairs.
One option would be to take the ideal pairs from a
pre-compiled data base. The problem is that these
ideal pairs may differ from the facts expressed in
the documents. Furthermore, these ideal pairs do
not allow to measure how much of the document
content the system actually extracted. This is why
we chose to extract the ideal pairs manually from
the documents. In our methodology, the ideal pairs
comprise all pairs that a human would understand
to be elements of the target relation. This involves
full anaphora resolution, the solving of reference
ambiguities, and the choice of truly defining con-
cepts. For example, we accept Chopin as instance
of composer but not as instance of member,
even if the text says that he was a member of some
club. Of course, we expect neither the competi-
tors nor LEILA to achieve the results in the ideal
table. However, this methodology is the only fair
way of manual extraction, as it is guaranteed to
be system-independent. If O denotes the multi-
set of the output pairs and I denotes the multi-set
of the ideal pairs, then precision, recall, and their
3http://www.sourceforge.net/projects/texttoonto
4http://www.lonelyplanet.com/
harmonic mean F1 can be computed as
recall = |O ? I||I| precision =
|O ? I|
|O|
F1 = 2 ? recall ? precisionrecall + precision .
To ensure a fair comparison of LEILA to Snow-
ball, we use the same evaluation as employed in
the original Snowball paper (Agichtein and Gra-
vano, 2000), the Ideal Metric. The Ideal Metric
assumes the target relation to be right-unique (i.e.
a many-to-one relation). Hence the set of ideal
pairs is right-unique. The set of output pairs can
be made right-unique by selecting the pair with the
highest confidence for each first component. Du-
plicates are removed from the ideal pairs and also
from the output pairs. All output pairs that have
a first component that is not in the ideal set are
removed.
There is one special case for the CV-system,
which uses the Ideal Metric for the non-right-
unique instanceOf relation. To allow for a fair
comparison, we used the Relaxed Ideal Metric,
which does not make the ideal pairs right-unique.
The calculation of recall is relaxed as follows:
recall = |O ? I||{x|?y : (x, y) ? I}|
Due to the effort, we could extract the ideal pairs
only for a sub-corpus. To ensure significance in
spite of this, we compute confidence intervals for
our estimates: We interpret the sequence of out-
put pairs as a repetition of a Bernoulli-experiment,
where the output pair can be either correct (i.e.
contained in the ideal pairs) or not. The parameter
of this Bernoulli-distribution is the precision. We
estimate the precision by drawing a sample (i.e.
by extracting all ideal pairs in the sub-corpus). By
assuming that the output pairs are identically in-
dependently distributed, we can calculate a confi-
dence interval for our estimation. We report confi-
dence intervals for precision and recall for a con-
fidence level of ? = 95%. We measure precision
at different levels of recall and report the values
for the best F1 value. We used approximate string
matching techniques to account for different writ-
ings of the same entity. For example, we count
the output pair ?Chopin? / ?composer? as correct,
even if the ideal pairs contain ?Frederic Chopin? /
?composer?. To ensure that LEILA does not just
reproduce the example pairs, we list the percent-
age of examples among the output pairs. During
our evaluation, we found that the Link Grammar
parser does not finish parsing on roughly 1% of
the files for unknown reasons.
22
Table 1: Results with different relations
Corpus Relation System #D #O #C #I Precision Recall F1 %E
Wikicomposers birthdate LEILA(SVM) 87 95 70 101 73.68%? 8.86% 69.31%? 9.00% 71.43% 4.29%
Wikicomposers birthdate LEILA(kNN) 87 90 70 101 78.89%? 8.43% 70.30%? 8.91% 74.35% 4.23%
Wikigeography synonymy LEILA(SVM) 81 92 74 164 80.43%? 8.11% 45.12%? 7.62% 57.81% 5.41%
Wikigeography synonymy LEILA(kNN) 81 143 105 164 73.43%? 7.24% 64.02%? 7.35% 68.40% 4.76%
Wikicomposers instanceOf LEILA(SVM) 87 685 408 1127 59.56%? 3.68% 36.20%? 2.81% 45.03% 6.62%
Wikicomposers instanceOf LEILA(kNN) 87 790 463 1127 58.61%? 3.43% 41.08%? 2.87% 48.30% 7.34%
Wikigeneral instanceOf LEILA(SVM) 287 921 304 912 33.01%? 3.04% 33.33%? 3.06% 33.17% 3.62%
Googlecomposers instanceOf LEILA(SVM) 100 787 210 1334 26.68%? 3.09% 15.74%? 1.95% 19.80% 4.76%
Googlecomposers instanceOf LEILA(kNN) 100 840 237 1334 28.21%? 3.04% 17.77%? 2.05% 21.80% 8.44%
Googlec.+Wikic. instanceOf LEILA(SVM) 100 563 203 1334 36.06%? 3.97% 15.22%? 1.93% 21.40% 5.42%
Googlec.+Wikic. instanceOf LEILA(kNN) 100 826 246 1334 29.78%? 3.12% 18.44%? 2.08% 22.78% 7.72%
#O ? number of output pairs #D ? number of documents in the hand-processed sub-corpus
#C ? number of correct output pairs %E ? proportion of example pairs among the correct output pairs
#I ? number of ideal pairs Recall and Precision with confidence interval at ? = 95%
3.2 Results
3.2.1 Results on different relations
Table 1 summarizes our experimental results
with LEILA on different relations. For the birth-
date relation, we used Edward Morykwas? list of
famous birthdays5 as examples. As counterexam-
ples, we chose all pairs of a person that was in the
examples and an incorrect birthdate. All pairs of
a proper name and a date are candidates. We ran
LEILA on the Wikicomposer corpus. LEILA per-
formed quite well on this task. The patterns found
were of the form ?X was born in Y ? and ?X (Y )?.
For the synonymy relation we used all pairs
of proper names that share the same synset in
WordNet as examples (e.g. ?UN?/?United Na-
tions?). As counterexamples, we chose all pairs of
nouns that are not synonymous in WordNet (e.g.
?rabbit?/?composer?). All pairs of proper names are
candidates. We ran LEILA on the Wikigeography
corpus, because this set is particularly rich in syn-
onyms. LEILA performed reasonably well. The
patterns found include ?X was known as Y ? as well
as several non-grammatical constructions such as
?X (formerly Y )?.
For the instanceOf relation, it is difficult to se-
lect example pairs, because if an entity belongs
to a concept, it also belongs to all super-concepts.
However, admitting each pair of an entity and one
of its super-concepts as an example would result in
far too many false positives. The problem is to de-
termine for each entity the (super-)concept that is
most likely to be used in a natural language defini-
tion of that entity. Psychological evidence (Rosch
et al, 1976) suggests that humans prefer a certain
layer of concepts in the taxonomy to classify en-
tities. The set of these concepts is called the Ba-
sic Level. Heuristically, we found that the low-
est super-concept in WordNet that is not a com-
pound word is a good approximation of the ba-
5http://www.famousbirthdates.com
sic level concept for a given entity. We used all
pairs of a proper name and the corresponding ba-
sic level concept of WordNet as examples. We
could not use pairs of proper names and incorrect
super-concepts as counterexamples, because our
corpus Wikipedia knows more meanings of proper
names than WordNet. Therefore, we used all pairs
of a common noun and an incorrect super-concept
from WordNet as counterexamples. All pairs of
a proper name and a WordNet concept are candi-
dates.
We ran LEILA on the Wikicomposers corpus.
The performance on this task was acceptable, but
not impressive. However, the chances to obtain a
high recall and a high precision were significantly
decreased by our tough evaluation policy: The
ideal pairs include tuples deduced by resolving
syntactic and semantic ambiguities and anaphoras.
Furthermore, our evaluation policy demands that
non-defining concepts like member not be cho-
sen as instance concepts. In fact, a high propor-
tion of the incorrect assignments were friend,
member, successor and predecessor, de-
creasing the precision of LEILA. Thus, compared
to the gold standard of humans, the performance
of LEILA can be considered reasonably good. The
patterns found include the Hearst patterns (Hearst,
1992) ?Y such as X?, but also more complex pat-
terns like ?X was known as a Y ?, ?X [. . . ] as Y ?, ?X
[. . . ] can be regarded as Y ? and ?X is unusual among
Y ?. Some of these patterns could not have been
found by primitive regular expression matching.
To test whether thematic heterogeneity influ-
ences LEILA, we ran it on the Wikigeneral corpus.
Finally, to try the limits of our system, we ran it on
the Googlecomposers corpus. As shown in Table
1, the performance of LEILA dropped in these in-
creasingly challenging tasks, but LEILA could still
produce useful results. We can improve the results
on the Googlecomposers corpus by adding the Wi-
kicomposers corpus for training.
23
The different learning methods (kNN and SVM)
performed similarly for all relations. Of course, in
each of the cases, it is possible to achieve a higher
precision at the price of a lower recall. The run-
time of the system splits into parsing (? 40s for
each document, e.g. 3:45h for Wikigeography)
and the core algorithm (2-15min for each corpus,
5h for the huge Wikigeneral).
3.2.2 Results with different competitors
Table 2 shows the results for comparing LEILA
against various competitors (with LEILA in bold-
face). We compared LEILA to TextToOnto and
Text2Onto for the instanceOf relation on the
Wikicomposers corpus. TextToOnto requires an
ontology as source of possible concepts. We gave
it the WordNet ontology, so that it had the same
preconditions as LEILA. Text2Onto does not re-
quire any input. Text2Onto seems to have a preci-
sion comparable to ours, although the small num-
ber of found pairs does not allow a significant con-
clusion. Both systems have drastically lower recall
than LEILA.
For Snowball, we only had access to the test
corpus. Hence we trained LEILA on a small por-
tion (3%) of the test documents and tested on
the remaining ones. Since the original 5 seed
pairs that Snowball used did not appear in the col-
lection at our disposal, we chose 5 other pairs
as examples. We used no counterexamples and
hence omitted the Training Phase of our algorithm.
LEILA quickly finds the pattern ?Y -based X?. This
led to very high precision and good recall, com-
pared to Snowball ? even though Snowball was
trained on a much larger training collection.
The CV-system differs from LEILA, because its
ideal pairs are a table, in which each entity is as-
signed to its most likely concept according to a hu-
man understanding of the text ? independently of
whether there are explicit definitions for the entity
in the text or not. We conducted two experiments:
First, we used the document set used in Cimiano
and Vo?lker?s original paper (Cimiano and Vo?lker,
2005a), the Lonely Planet corpus. To ensure a
fair comparison, we trained LEILA separately on
the Wikicomposers corpus, so that LEILA cannot
have example pairs in its output. For the evalu-
ation, we calculated precision and recall with re-
spect to an ideal table provided by the authors.
Since the CV-system uses a different ontology, we
allowed a distance of 4 edges in the WordNet hi-
erarchy to count as a match (for both systems).
Since the explicit definitions that our system relies
on were sparse in the corpus, LEILA performed
worse than the competitor. In a second experi-
ment, we had the CV-system run on the Wikicom-
posers corpus. As the CV-system requires a set
of target concepts, we gave it the set of all con-
cepts in our ideal pairs. Furthermore, the sys-
tem requires an ontology on these concepts. We
gave it the WordNet ontology, pruned to the tar-
get concepts with their super-concepts. We evalu-
ated by the Relaxed Ideal Metric, again allowing
a distance of 4 edges in the WordNet hierarchy to
count as a match (for both systems). This time,
our competitor performed worse. This is because
our ideal table is constructed from the definitions
in the text, which our competitor is not designed
to follow. These experiments only serve to show
the different philosophies in the definition of the
ideal pairs for the CV-system and LEILA. The CV-
system does not depend on explicit definitions, but
it is restricted to the instanceOf-relation.
4 Conclusion and Outlook
We addressed the problem of automatically ex-
tracting instances of arbitrary binary relations
from natural language text. The key novelty of our
approach is to apply a deep syntactic analysis to
this problem. We have implemented our approach
and showed that our system LEILA outperforms
existing competitors.
Our current implementation leaves room for fu-
ture work. For example, the linkages allow for
more sophisticated ways of resolving anaphoras
or matching patterns. LEILA could learn nu-
merous interesting relations (e.g. country /
president or isAuthorOf) and build up an
ontology from the results with high confidence.
LEILA could acquire and exploit new corpora on
its own (e.g., it could read newspapers) and it
could use its knowledge to acquire and structure
its new knowledge more efficiently. We plan to
exploit these possibilities in our future work.
4.1 Acknowledgements
We would like to thank Eugene Agichtein for his
caring support with Snowball. Furthermore, Jo-
hanna Vo?lker and Philipp Cimiano deserve our
sincere thanks for their unreserved assistance with
their system.
References
[Agichtein and Gravano2000] E. Agichtein and L. Gravano.
2000. Snowball: extracting relations from large plain-text
collections. In ACM 2000, pages 85?94, Texas, USA.
[Brin1999] Sergey Brin. 1999. Extracting patterns and rela-
tions from the world wide web. In Selected papers from
the Int. Workshop on the WWW and Databases, pages
172?183, London, UK. Springer-Verlag.
[Buitelaar and Ramaka2005] P. Buitelaar and S. Ramaka.
2005. Unsupervised ontology-based semantic tagging
24
Table 2: Results with different competitors
Corpus M Relation System #D #O #C #I Prec Rec F1
Snowball corp. S headquarters LEILA(SVM) 54 92 82 165 89.13%? 6.36% 49.70%? 7.63% 63.81%
Snowball corp. S headquarters LEILA(kNN) 54 91 82 165 90.11%? 6.13% 49.70%? 7.63% 64.06%
Snowball corp. S headquarters Snowball 54 144 49 165 34.03%? 7.74% 29.70%? 6.97% 31.72%
Snowball corp. I headquarters LEILA(SVM) 54 50 48 126 96.00%? 5.43% 38.10%? 8.48% 54.55%
Snowball corp. I headquarters LEILA(kNN) 54 49 48 126 97.96%? 3.96% 38.10%? 8.48% 54.86%
Snowball corp. I headquarters Snowball 54 64 31 126 48.44%?12.24% 24.60%? 7.52% 32.63%
Wikicomposers S instanceOf LEILA(SVM) 87 685 408 1127 59.56%? 3.68% 36.20%? 2.81% 45.03%
Wikicomposers S instanceOf LEILA(kNN) 87 790 463 1127 58.61%? 3.43% 41.08%? 2.87% 48.30%
Wikicomposers S instanceOf Text2Onto 87 36 18 1127 50.00% 1.60%? 0.73% 3.10%
Wikicomposers S instanceOf TextToOnto 87 121 47 1127 38.84%? 8.68% 4.17%? 1.17% 7.53%
Wikicomposers R instanceOf LEILA(SVM) 87 336 257 744 76.49%? 4.53% 34.54%? 3.42% 47.59%
Wikicomposers R instanceOf LEILA(kNN) 87 367 276 744 75.20%? 4.42% 37.10%? 3.47% 49.68%
Wikicomposers R instanceOf CV-system 87 134 30 744 22.39% 4.03%? 1.41% 6.83%
Lonely Planet R instanceOf LEILA(SVM) ? 159 42 289 26.42%? 6.85% 14.53%? 4.06% 18.75%
Lonely Planet R instanceOf LEILA(kNN) ? 168 44 289 26.19%? 6.65% 15.22%? 4.14% 19.26%
Lonely Planet R instanceOf CV-system ? 289 92 289 31.83%? 5.37% 31.83%? 5.37% 31.83%
M ? Metric (S: Standard, I: Ideal Metric, R: Relaxed Ideal Metric). Other abbreviations as in Table 1
for knowledge markup. In W. Buntine, A. Hotho, and
Stephan Bloehdorn, editors, Workshop on Learning in Web
Search at the ICML 2005.
[Buitelaar et al2004] P. Buitelaar, D. Olejnik, and M. Sin-
tek. 2004. A protege plug-in for ontology extraction from
text based on linguistic analysis. In ESWS 2004, Herak-
lion, Greece.
[Califf and Mooney1997] M. Califf and R. Mooney. 1997.
Relational learning of pattern-match rules for informa-
tion extraction. ACL-97 Workshop in Natural Language
Learning, pages 9?15.
[Cimiano and Vo?lker2005a] P. Cimiano and J. Vo?lker.
2005a. Text2onto - a framework for ontology learn-
ing and data-driven change discovery. In A. Montoyo,
R. Munozand, and E. Metais, editors, Proc. of the 10th Int.
Conf. on Applications of Natural Language to Information
Systems, pages 227?238, Alicante, Spain.
[Cimiano and Vo?lker2005b] P. Cimiano and J. Vo?lker.
2005b. Towards large-scale, open-domain and ontology-
based named entity classification. In Int. Conf. on Recent
Advances in NLP 2005, pages 166?172.
[Cimiano et al2005] P. Cimiano, G. Ladwig, and S. Staab.
2005. Gimme the context: Contextdriven automatic se-
mantic annotation with cpankow. In Allan Ellis and Tat-
suya Hagino, editors, WWW 2005, Chiba, Japan.
[Etzioni et al2004] O. Etzioni, M. Cafarella, D. Downey,
S. Kok, A. Popescu, T. Shaked, S. Soderland, D. S. Weld,
and A. Yates. 2004. Web-scale information extraction
in knowitall (preliminary results). In WWW 2004, pages
100?110.
[Fellbaum1998] C. Fellbaum. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
[Finn and Kushmerick2004] A. Finn and N. Kushmerick.
2004. Multi-level boundary classification for information
extraction. In ECML 2004, pages 111?122.
[Freitag and Kushmerick2000] D. Freitag and N. Kushmer-
ick. 2000. Boosted wrapper induction. In American Nat.
Conf. on AI 2000.
[Graupmann2004] Jens Graupmann. 2004. Concept-based
search on semi-structured data exploiting mined semantic
relations. In EDBT Workshops, pages 34?43.
[Hearst1992] A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In ICCL 1992, Nantes,
France.
[J. Iria2005] F. Ciravegna J. Iria. 2005. Relation extraction
for mining the semantic web.
[Maedche and Staab2000] A. Maedche and S. Staab. 2000.
Discovering conceptual relations from text. In W. Horn,
editor, ECAI 2000, pages 85?94, Berlin, Germany.
[Mann and Yarowsky2005] Gideon Mann and David
Yarowsky. 2005. Multi-field information extraction and
cross-document fusion. In ACL 2005.
[Montague1974] R. Montague. 1974. Universal grammar.
In Formal Philosophy. Selected Papers of Richard Mon-
tague. Yale University Press.
[Ravichandran and Hovy2002] D. Ravichandran and
E. Hovy. 2002. Learning surface text patterns for a
question answering system. In ACL 2002, Philadelphia,
USA.
[Riloff1996] E. Riloff. 1996. Automatically generating ex-
traction patterns from untagged text. Annual Conf. on AI
1996, pages 1044?1049.
[Rosch et al1976] E. Rosch, C.B. Mervis, W.D. Gray, D.M.
Johnson, and P. Boyes-Bream. 1976. Basic objects in
natural categories. Cognitive Psychology, pages 382?439.
[Ruiz-Casado et al2005] Maria Ruiz-Casado, Enrique Al-
fonseca, and Pablo Castells. 2005. Automatic extraction
of semantic relationships for wordnet by means of pattern
learning from wikipedia. In NLDB 2006, pages 67?79.
[Sleator and Temperley1993] D. Sleator and D. Temperley.
1993. Parsing english with a link grammar. 3rd Int. Work-
shop on Parsing Technologies.
[Soderland et al1995] S. Soderland, D. Fisher, J. Aseltine,
and W. Lehnert. 1995. Crystal: Inducing a conceptual
dictionary. IJCAI 1995, pages 1314?1319.
[Soderland1999] S. Soderland. 1999. Learning information
extraction rules for semi-structured and free text. Machine
Learning, pages 233?272.
[Suchanek et al2006] Fabian M. Suchanek, Georgiana
Ifrim, and Gerhard Weikum. 2006. Combining Linguistic
and Statistical Analysis to Extract Relations from Web
Documents. In SIGKDD 2006.
[Xu and Krieger2003] F. Xu and H. U. Krieger. 2003. In-
tegrating shallow and deep nlp for information extraction.
In RANLP 2003, Borovets, Bulgaria.
[Xu et al2002] F. Xu, D. Kurz, J. Piskorski, and
S. Schmeier. 2002. Term extraction and mining
term relations from free-text documents in the financial
domain. In Int. Conf. on Business Information Systems
2002, Poznan, Poland.
[Yangarber et al2000] R. Yangarber, R. Grishman,
P. Tapanainen, and S. Huttunen. 2000. Automatic
acquisition of domain knowledge for information extrac-
tion. In ICCL 2000, pages 940?946, Morristown, NJ,
USA. Association for Computational Linguistics.
[Yangarber et al2002] R. Yangarber, W. Lin, and R. Grish-
man. 2002. Unsupervised learning of generalized names.
In ICCL 2002, pages 1?7, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
25
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 913?921,
Beijing, August 2010
The Bag-of-Opinions Method for Review Rating Prediction from Sparse
Text Patterns
Lizhen Qu
Max-Planck Institute
for Informatics
lqu@mpii.mpg.de
Georgiana Ifrim
Bioinformatics Research
Centre
ifrim@birc.au.dk
Gerhard Weikum
Max-Planck Institute
for Informatics
weikum@mpii.mpg.de
Abstract
The problem addressed in this paper is to
predict a user?s numeric rating in a prod-
uct review from the text of the review. Un-
igram and n-gram representations of text
are common choices in opinion mining.
However, unigrams cannot capture impor-
tant expressions like ?could have been bet-
ter?, which are essential for prediction
models of ratings. N-grams of words, on
the other hand, capture such phrases, but
typically occur too sparsely in the train-
ing set and thus fail to yield robust pre-
dictors. This paper overcomes the limita-
tions of these two models, by introducing
a novel kind of bag-of-opinions represen-
tation, where an opinion, within a review,
consists of three components: a root word,
a set of modifier words from the same sen-
tence, and one or more negation words.
Each opinion is assigned a numeric score
which is learned, by ridge regression,
from a large, domain-independent cor-
pus of reviews. For the actual test case
of a domain-dependent review, the re-
view?s rating is predicted by aggregat-
ing the scores of all opinions in the re-
view and combining it with a domain-
dependent unigram model. The paper
presents a constrained ridge regression al-
gorithm for learning opinion scores. Ex-
periments show that the bag-of-opinions
method outperforms prior state-of-the-art
techniques for review rating prediction.
1 Introduction
1.1 Motivation
Opinion mining and sentiment analysis has be-
come a hot research area (Pang and Lee, 2008).
There is ample work on analyzing the sentiments
of online-review communities where users com-
ment on products (movies, books, consumer elec-
tronics, etc.), implicitly expressing their opinion
polarities (positive, negative, neutral), and also
provide numeric ratings of products (Titov and
McDonald, 2008b; Lerman et al, 2009; Hu and
Liu, 2004; Titov and McDonald, 2008a; Pang
and Lee, 2005; Popescu and Etzioni, 2005a). Al-
though ratings are more informative than polari-
ties, most prior work focused on classifying text
fragments (phrases, sentences, entire reviews) by
polarity. However, a product receiving mostly 5-
star reviews exhibits better customer purchase be-
havior compared to a product with mostly 4-star
reviews. In this paper we address the learning and
prediction of numerical ratings from review texts,
and we model this as a metric regression problem
over an appropriately defined feature space.
Formally, the input is a set of rated documents
(i.e., reviews), {xi, yi}Ni=1, where xi is a sequence
of word-level unigrams (w1, ..., wl) and yi ? R is
a rating. The goal is to learn a function f(x) that
maps the word vector x into a numerical rating y?,
which indicates both the polarity and strength of
the opinions expressed in a document.
Numerical review rating prediction is harder
than classifying by polarity. Consider the follow-
ing example from Amazon book reviews:
The organization of the book is hard to follow
and the chapter titles are not very helpful, so go-
ing back and trying to find information is quite
913
difficult.
We note that there are many subjective words
(hard, helpful, difficult) modified by opinion mod-
ifiers such as (very, quite) and negation words like
(not). For rating prediction, considering opin-
ion modifiers is crucial; very helpful is a much
stronger sentiment than helpful. Negation words
also need attention. As pointed out by Liu and
Seneff (2009) we cannot simply reverse the polar-
ity. For example, if we assign a higher positive
score to very helpful than to helpful, simply re-
versing the sign of the scores would incorrectly
suggest that not helpful is less negative than not
very helpful.
The widely used unigram (bag-of-words)
model (Pang and Lee, 2005; Snyder and Barzilay,
2007; Goldberg and Zhu, 2006; Ganu et al, 2009)
cannot properly capture phrase patterns. Con-
sider the following example: not so helpful vs.
not so bad. In a unigram-based regression model
each unigram gets a weight indicating its polarity
and strength. High positive/negative weights are
strongly positive/negative clues. It is reasonable
to assign a positive weight to helpful and a nega-
tive weight to bad. The fundamental problem of
unigrams arises when assigning a weight to not.
If not had a strongly negative weight, the posi-
tive weight of helpful would be strongly reduced
while the negative weight of bad would be ampli-
fied (by combining weights). This clearly fails to
capture the true intentions of the opinion phrases.
The same problem holds for so, which is an inten-
sifier that should keep the same sign as the word
it modifies. We refer to this limitation of the uni-
gram model as polarity incoherence.
A promising way of overcoming this weakness
is to include n-grams, generalizing the bag-of-
words model into a bag-of-phrases model (Bac-
cianella et al, 2009; Pang and Lee, 2008). How-
ever, regression models over the feature space
of all n-grams (for either fixed maximal n or
variable-length phrases) are computationally ex-
pensive in their training phase. Moreover and
most importantly for our setting, including n-
grams in the model results in a very high dimen-
sional feature space: many features will then oc-
cur only very rarely in the training data. There-
fore, it is difficult if not impossible to reliably
learn n-gram weights from limited-size training
sets. We refer to this problem as the n-gram spar-
sity bottleneck. In our experiments we inves-
tigate the effect of using bigrams and variable-
length ngrams for improving review rating predic-
tion.
1.2 Contribution
To overcome the above limitations of unigram and
n-gram features, we have developed a novel kind
of bag-of-opinions model, which exploits domain-
independent corpora of opinions (e.g., all Amazon
reviews), but is finally applied for learning predic-
tors on domain-specific reviews (e.g., movies as
rated in IMDB or Rottentomatoes). A document
is represented as a bag of opinions each of which
has three components: a root word, a set of modi-
fier words and one or more negation words. In the
phrase not very helpful, the opinion root is help-
ful, one (of potentially many) opinion modifier(s)
is very, and a negation word is not. We enforce po-
larity coherence by the design of a learnable func-
tion that assigns a score to an opinion.
Our approach generalizes the cumulative linear
offset model (CLO) presented in (Liu and Seneff,
2009). The CLO model makes several restrictive
assumptions, most notably, that all opinion scores
within one document are the same as the overall
document rating. This assumption does not hold
in practice, not even in reviews with extremely
positive/negative ratings. For example, in a 5-
star Amazon review the phrases most impressive
book and it helps explain should receive different
scores. Otherwise, the later transfer step to dif-
ferent domains would yield poor predictions. Due
to this restriction, CLO works well on particular
types of reviews that have pro/con entries listing
characteristic major opinions about the object un-
der review. For settings with individual reviews
whose texts do not exhibit any specific structure,
the CLO model faces its limitations.
In our bag-of-opinions method, we address the
learning of opinion scores as a constrained ridge
regression problem. We consider the opinion
scores in a given review to be drawn from an
unknown probability distribution (so they do not
have to be the same within a document). We es-
timate the review rating based on a set of statis-
914
tics (e.g., expectation, variance, etc.) derived from
the scores of opinions in a document. Thus, our
method has a sound statistical foundation and can
be applied to arbitrary reviews with mixed opin-
ion polarities and strengths. We avoid the n-gram
sparsity problem by the limited-size structured
feature space of (root,modifiers,negators) opin-
ions.
We treat domain-independent and domain-
dependent opinions differently in our system. In
the first step we learn a bag-of-opinions model on
a large dataset of online reviews to obtain scores
for domain-independent opinions. Since the po-
larity of opinions is not bound to a topic, one
can learn opinion scores from a pooled corpus
of reviews for various categories, e.g., movies,
books, etc., and then use these scored opinions
for predicting the ratings of reviews belonging
to a particular category. In order to also capture
domain-dependent information (possibly comple-
mentary to the opinion lexicon used for learn-
ing domain-independent opinions), we combine
the bag-of-opinions model with an unigram model
trained on the domain-dependent corpus. Since
domain-dependent training is typically limited,
we model it using unigram models rather than
bag-of-opinions. By combining the two models,
even if an opinion does not occur in the domain-
dependent training set but it occurs in a test re-
view, we can still accurately predict the review rat-
ing based on the globally learned opinion score. In
some sense our combined learning scheme is sim-
ilar to smoothing in standard learning techniques,
where the estimate based on a limited training
set is smoothed using a large background corpus
(Zhai and Lafferty, 2004).
In summary, the contributions of this paper are
the following:
1. We introduce the bag-of-opinions model, for
capturing the influence of n-grams, but in a
structured way with root words, modifiers,
and negators, to avoid the explosion of the
feature space caused by explicit n-gram mod-
els.
2. We develop a constrained ridge regression
method for learning scores of opinions from
domain-independent corpora of rated re-
views.
3. For transferring the regression model to
newly given domain-dependent applications,
we derive a set of statistics over opinion
scores in documents and use these as fea-
tures, together with standard unigrams, for
predicting the rating of a review.
4. Our experiments with Amazon reviews from
different categories (books, movies, music)
show that the bag-of-opinions method out-
performs prior state-of-the-art techniques.
2 Bag-of-Opinions Model
In this section we first introduce the bag-of-
opinions model, followed by the method for
learning (domain-independent) model parameters.
Then we show how we annotate opinions and how
we adapt the model to domain-dependent data.
2.1 Model Representation
We model each document as a bag-of-opinions
{opk}Kk=1, where the number of opinionsK varies
among documents. Each opinion opk consists
of an opinion root wr, r ? SR, a set of opin-
ion modifiers {wm}Mm=1, m ? SM and a set of
negation words {wz}Zz=1, z ? SZ , where the sets
SR, SM , SZ are component index sets of opinion
roots, opinion modifiers and negation words re-
spectively. The union of these sets forms a global
component index set S ? Nd, where d is the di-
mension of the index space. The opinion root de-
termines the prior polarity of the opinion. Modi-
fiers intensify or weaken the strength of the prior
polarity. Negation words strongly reduce or re-
verse the prior polarity. For each opinion, the
set of negation words consists of at most a nega-
tion valence shifter like not (Kennedy and Inkpen,
2006) and its intensifiers like capitalization of the
valence shifter. Each opinion component is asso-
ciated with a score. We assemble the scores of
opinion elements into an opinion-score by using
a score function. For example, in the opinion not
very helpful, the opinion root helpful determines
the prior polarity positive say with a score 0.9, the
modifier very intensifies the polarity say with a
915
score 0.5. The prior polarity is further strongly re-
duced by the negation word not with e.g., a score
-1.2. Then we sum up the scores to get a score of
0.2 for the opinion not very helpful.
Formally, we define the function score(op) as
a linear function of opinion components, which
takes the form
score(op) = sign(r)?rxr
+
M?
m=1
sign(r)?mxm
+
Z?
z=1
sign(r)?zxz (1)
where {xz, xm, xr} are binary variables denoting
the presence or absence of negation words, modi-
fiers and opinion root. {?z, ?m, ?r} are weights of
each opinion elements. sign(r) : wr ? {?1, 1}
is the opinion polarity function of the opinion root
wr. It assigns a value 1/-1 if an opinion root is
positive/negative. Due to the semantics of opin-
ion elements, we have constraints that ?r ? 0
and ?z ? 0. The sign of ?m is determined in the
learning phase, since we have no prior knowledge
whether it intensifies or weakens the prior polar-
ity.
Since a document is modeled as a bag-of-
opinions, we can simply consider the expec-
tation of opinion scores as the document rat-
ing. If we assume the scores are uniformly dis-
tributed, the prediction function is then f(x) =
1
K
?K
k=1 score(opk) which assigns the average of
opinion scores to the document x.
2.2 Learning Regression Parameters
We assume that we can identify the opinion roots
and negation words from a subjectivity lexicon. In
this work we use MPQA (Wilson et al, 2005). In
addition, the lexicon provides the prior polarity of
the opinion roots. In the training phase, we are
given a set of documents with ratings {xi, yi}Ni=1,
and our goal is to find an optimal function f?
whose predictions {y?i}Ni=1 are as close as possi-
bile to the original ratings {yi}Ni=1. Formally, we
aim to minimize the following loss function:
L = 12N
N?
i=1
(f(xi)? yi)2 (2)
where f(xi) is modeled as the average score of
opinions in review xi.
First, we rewrite score(op) as the dot
product ??,p? between a weight vector
? = [?z,?m, ?r] and a feature vector
p = [sign(r)xz, sign(r)xm, sign(r)xr].
In order to normalize the vectors, we
rewrite the weight and feature vectors in
the d dimensional vector space of all root
words, modifiers and negation words. Then
? = [..,?z, 0, ..,?m, 0, .., ?r, 0..] ? Rd and p =
[sign(r)xz, 0, .., sign(r)xm, 0, .., sign(r)xr, ...] ?
Rd. The function f(xi) can then be written as
the dot product ??,vi?, where vi = 1Ki
?Ki
k=1 pk,
with Ki the number of opinions in review xi.
By using this feature representation, the learning
problem is equivalent to:
min
?
L(?) = 12N
N?
i=1
(??,vi?+ ?0 ? yi)2
s.t.
?z ? 0 z ? SZ
?r ? 0 r ? SR (3)
where ? ? Rd, ? = [?z,?m,?r]. ?0 is the inter-
cept of the regression function, which is estimated
as the mean of the ratings in the training set. We
define a new variable y?i = yi ? ?0.
In order to avoid overfitting, we add an l2 norm
regularizer to the loss function with the parameter
? > 0.
LR(?) = 12N
N?
i=1
(??,vi? ? y?i)2 +
?
2 ? ? ?
2
2
s.t.
?z ? 0 z ? SZ
?r ? 0 r ? SR (4)
We solve the above optimization problem by Al-
gorithm 1 using coordinate descent. The proce-
dure starts with ?0 = 0, ?0 ? Rd. Then it up-
dates iteratively every coordinate of the vector ?
until convergence. Algorithm 1 updates every co-
ordinate ?j , j ? {1, 2, ..., d} of ? by solving the
following one-variable sub-problem:
minlj??j?cjLR(?1, ..., ?j , ..., ?d)
916
where lj and cj denote the lower and upper
bounds of ?j . If j ? SZ , lj = ?? and cj = 0.
If j ? SR, lj = 0 and cj = ?. Otherwise both
bounds are infinity.
According to (Luo and Tseng, 1992), the solu-
tion of this one-variable sub-problem is
??j = max{lj ,min{cj , gj}}
where
gj =
1
N
?N
i=1 vij(y?i ?
?
l 6=j ?lvl)
1
N
?N
i=1 v2ij + ?
Here gj is the close form solution of standard
ridge regression at coordinate j (for details see
(Friedman et al, 2008)). We prove the conver-
gence of Algorithm 1, by the following theorem
using techniques in (Luo and Tseng, 1992).
Theorem 1 A sequence of ? generated by Algo-
rithm 1 globally converges to an optimal solution
?? ? ?? of problem (4), where ?? is the set of
optimal solutions.
Proof: Luo and Tseng (1992) show that coordi-
nate descent for constrained quadratic functions
in the following form converges to one of its global
optimal solutions.
min? h(?) = ??,Q??/2 + ?q,??
s.t. ET? ? b
where Q is a d?d symmetric positive-definite ma-
trix, E is a d? d matrix having no zero column, q
is a d-vector and b is a d-vector.
We rewrite LR in matrix form as
1
2N (y? ?V?)
T (y? ?V?) + ?2?
T?
= 12N (V?)
T (V?) + ?2?
T? ? 12N ((V?)
T y?
? 12N y?
T (V?)) + 12N y?
T y?
= ??,Q??/2 + ?q,??+ constant
where
Q = BTB,B =
[ ?
1
NV?
?Id?d
]
,q = ?1N (V
T y?)
where Id?d is the identity matrix. Because ? >
0, all columns of B are linearly independent. As
Q = BTB and symmetric, Q is positive definite.
We define E as a d ? d diagonal matrix with
all entries on the main diagonal equal to 1 except
eii = ?1, i ? SZ and b is a d-vector with all
entries equal to ?? except bi = 0, for i ? SZ or
i ? SR.
Because the almost cyclic rule is applied to
generate the sequence {?t}, the algorithm con-
verges to a solution ?? ? ??.
Algorithm 1 Constrained Ridge Regression
1: Input: ? and {vn, y?n}Nn=1
2: Output: optimal ?
3: repeat
4: for j = 1, ..., d do
5: gj =
1
N
PN
i=1 vij(y?i?
P
l 6=j ?lvl)
1
N
PN
i=1 v2ij+?6:
??j =
?
?
?
0, if j ? SR and gj < 0
0, if j ? SZ and gj > 0
gj , else
7: end for
8: until Convergence condition is satisfied
2.3 Annotating Opinions
The MPQA lexicon contains separate lexicons for
subjectivity clues, intensifiers and valence shifters
(Wilson et al, 2005), which are used for identify-
ing opinion roots, modifiers and negation words.
Opinion roots are identified as the positive and
negative subjectivity clues in the subjectivity lex-
icon. In the same manner, intensifiers and va-
lence shifters of the type {negation, shiftneg} are
mapped to modifiers and negation words. Other
modifier candidates are adverbs, conjunctions and
modal verbs around opinion roots. We consider
non-words modifiers as well, e.g., punctuations,
capitalization and repetition of opinion roots. If
the opinion root is a noun, adjectives are also in-
cluded into modifier sets.
The automatic opinion annotation starts with
locating the continous subjectivity clue sequence.
Once we find such a sequence and at least one
of the subjectivity clue is positive or negative, we
search to the left up to 4 words for negation words
and modifier candidates, and stop if encountering
another opinion root. Similarly, we search to the
917
right up to 3 unigrams for modifiers and stop if
we find negation words or any other opinion roots.
The prior polarity of the subjectivity sequence is
determined by the polarity of the last subjectivity
clue with either positive or negative polarity in the
sequence. The other subjectivity clues in the same
sequence are treated as modifiers.
2.4 Adaptation to Domain-Dependent Data
The adaptation of the learned (domain-
independent) opinion scores to the target
domain and the integration of domain-dependent
unigrams is done in a second ridge-regression
task. Note that this is a simpler problem than
typical domain-adaptation, since we already know
from the sentiment lexicon which are the domain-
independent features. Additionally, its relatively
easy to obtain a large mixed-domain corpus for
reliable estimation of domain-independent opin-
ion scores (e.g., use all Amazon product reviews).
Furthermore, we need a domain-adaptation step
since domain-dependent and domain-independent
data have generally different rating distributions.
The differences are mainly reflected in the
intercept of the regression function (estimated
as the mean of the ratings). This means that
we need to scale the positive/negative mean of
the opinion scores differently before using it
for prediction on domain-dependent reviews.
Moreover, other statistics further characterize the
opinion score distribution. We use the variance
of opinion scores to capture the reliability of
the mean, multiplied by the negative sign of the
mean to show how much it strengthens/weakens
the estimation of the mean. The mean score of
the dominant polarity (major exp) is also used
to reduce the influence of outliers. Because
positive and negative means should be scaled
differently, we represent positive and negative
values of the mean and major exp as 4 different
features. Together with variance, they are the 5
statistics of the opinion score distribution. The
second learning step on opinion score statistics
and domain-dependent unigrams as features,
re-weights the importance of domain-independent
and domain-dependent information according to
the target domain bias.
3 Experimental Setup
We performed experiments on three target do-
mains of Amazon reviews: books, movies
(DVDs), and music (CDs). For each domain,
we use ca. 8000 Amazon reviews for evalua-
tion; an additional set of ca. 4000 reviews are
withheld for parameter tuning (regularization pa-
rameter, etc.). For learning weights for domain-
independent opinions, we use a mixed-domain
corpus of ca. 350,000 reviews from Amazon
(electronics, books, dvds, etc.); this data is dis-
joint from the test sets and contains no reviews
from the music domain. In order to learn un-
biased scores, we select about the same number
of positive and negative reviews (where reviews
with more/less than 3 stars are regarded as posi-
tive/negative). The regularization parameters used
for this corpus are tuned on withheld data with ca.
6000 thematically mixed reviews.1.
We compare our method, subsequently referred
to as CRR-BoO (Constrained Ridge Regression
for Bag-of-Opinions), to a number of alternative
state-of-the-art methods. These competitors are
varied along two dimensions: 1) feature space,
and 2) training set. Along the first dimension,
we consider a) unigrams coined uni, b) unigrams
and bigrams together, coined uni+bi, c) variable-
length n-grams coined n-gram, d) the opinion
model by (Liu and Seneff, 2009) coined CLO (cu-
mulative linear offset model). As learning pro-
cedure, we use ridge regression for a), b), and
d), and bounded cyclic regression, coined BCR,
for c). Along the second - orthogonal - di-
mension, we consider 3 different training sets:
i) domain-dependent training set coined DD, ii)
the large mixed-domain training set coined MD,
iii) domain-dependent training set and the large
mixed-domain training set coined DD+MD. For
the DD+MD training set, we apply our two stage
approach for CRR-BoO and CLO, i.e., we use
the mixed-domain corpus for learning the opinion
scores in the first stage, and integrate unigrams
from DD in a second domain-adaptation stage.
We train the remaining feature models directly on
the combination of the whole mixed-domain cor-
1All datasets are available from
http://www.mpi-inf.mpg.de/?lqu
918
feature models uni uni+bi n-gram CLO CRR-BoO
DD
book 1.004 0.961 0.997 1.469 0.942
dvd 1.062 1.018 1.054 1.554 0.946
music 0.686 0.672 0.683 0.870 0.638
MD
book 1.696 1.446 1.643 1.714 1.427
dvd 1.919 1.703 1.858 1.890 1.565
music 2.395 2.160 2.340 2.301 1.731
DD+MD
book 1.649 1.403 1.611 1.032 0.884
dvd 1.592 1.389 1.533 1.086 0.928
music 1.471 1.281 1.398 0.698 0.627
Table 1: Mean squared error for rating prediction methods on Amazon reviews.
pus and the training part of DD.
The CLO model is adapted as follows. Since
bags-of-opinions generalize CLO, adjectives and
adverbs are mapped to opinion roots and modi-
fiers, respectively; negation words are treated the
same as CLO. Subsequently we use our regression
technique. As Amazon reviews do not contain pro
and con entries, we learn from the entire review.
For BCR, we adapt the variable-length n-grams
method of (Ifrim et al, 2008) to elastic-net-
regression (Friedman et al, 2008) in order to ob-
tain a fast regularized regression algorithm for
variable-length n-grams. We search for signifi-
cant n-grams by incremental expansion in back-
ward direction (e.g., expand bad to not bad). BCR
pursues a dense solution for unigrams and a sparse
solution for n-grams. Further details on the BCR
learning algorithm will be found on a subsequent
technical report.
As for the regression techniques, we show
only results with ridge regression (for all fea-
ture and training options except BCR). It outper-
formed -support vector regression (SVR) of lib-
svm (Chang and Lin, 2001), lasso (Tibshirani,
1996), and elastic net (Zou and Hastie, 2005) in
our experiments.
4 Results and Discussion
Table 1 shows the mean square error (MSE) from
each of the three domain-specific test sets. The er-
ror is defined as MSE = 1N
?N
i=1(f(xi) ? yi)2.
The right most two columns of the table show re-
sults for the full-fledge two-stage learning for our
method and CLO, with domain-dependent weight
learning and the domain adaptation step. The
other models are trained directly on the given
training sets. For the DD and DD+MD train-
ing sets, we use five-fold cross-validation on the
domain-specific sets. For the MD training set, we
take the domain-specific test sets as hold-out data
for evaluation.
Table 1 clearly shows that our CRR-BoO
method outperforms all alternative methods by a
significant margin. Most noteworthy is the mu-
sic domain, which is not covered by the mixed-
domain corpus. As expected, unigrams only per-
form poorly, and adding bigrams leads only to
marginal improvements. BCR pursues a dense
solution for unigrams and a sparse solution for
variable-length n-grams, but due to the sparsity
of occurence of long n-grams, it filters out many
interesting-but-infrequent ngrams and therefore
performs worse than the dense solution of the
uni+bi model. The CLO method of (Liu and Sen-
eff, 2009) shows unexpectedly poor performance.
Its main limitation is the assumption that opinion
scores are identical within one document. This
does not hold in documents with mixed opinion
polarities. It also results in conflicts for opinion
components that occur in both positive and nega-
tive documents. In contrast, CRR-BoO naturally
captures the mixture of opinions as a bag of pos-
itive/negative scores. We only require that the
mean of opinion scores equals the overall docu-
ment rating.
The right most column of Table 1 shows that
our method can be improved by learning opinion
scores from the large mixed-domain corpus. How-
919
opinion score
good 0.18
recommend 1.64
most difficult -1.66
but it gets very good! 2.37
would highly recommend 2.73
would not recommend -1.93
Table 2: Example opinions learned from the Ama-
zon mixed-domain corpus.
ever, the high error rates of the models learned di-
rectly on the MD corpus show that direct training
on the mixed-domain data can introduce a signifi-
cant amount of noise into the prediction models.
Although the noise can be reduced by learning
from MD and DD together, the performance is
still worse than when learning directly from the
domain-dependent corpora. Additionally, when
the domain is not covered by the mixed-domain
corpus (e.g., music), the results are even worse.
Thus, the two stages of our method (learning
domain-independent opinion scores plus domain-
adaptation) are decisive for a good performance,
and the sentiment-lexicon-based BoO model leads
to robust learning of domain-independent opinion
scores.
Another useful property of BoO is its high in-
terpretability. Table 2 shows example opinion
scores learned from the mixed-domain corpus.
We observe that the scores corelate well with our
intuitive interpretation of opinions.
Our CRR-BoO method is highly scalable.
Excluding the preprocessing steps (same for
all methods), the learning of opinion compo-
nent weights from the ca. 350,000 domain-
independent reviews takes only 11 seconds.
5 Related Work
Rating prediction is modeled as an ordinal re-
gression problem in (Pang and Lee, 2005; Gold-
berg and Zhu, 2006; Snyder and Barzilay, 2007).
They simply use the bag-of-words model with re-
gression algorithms, but as seen previously this
cannot capture the expressive power of phrases.
The resulting models are not highly interpretable.
Baccianella et al (2009) restrict the n-grams to
the ones having certain POS patterns. However,
the long n-grams matching the patterns still suffer
from sparsity. The same seems to hold for sparse
n-gram models (BCR in this paper) in the spirit
of Ifrim et al (2008). Although sparse n-gram
models can explore arbitrarily large n-gram fea-
ture spaces, they can be of little help if the n-grams
of interests occur sparsely in the datasets.
Since our approach can be regarded as learning
a domain-independent sentiment lexicon, it is re-
lated to the area of automatically building domain-
independent sentiment lexicons (Esuli and Sebas-
tiani, 2006; Godbole et al, 2007; Kim and Hovy,
2004). However, this prior work focused mainly
on the opinion polarity of opinion words, neglect-
ing the opinion strength. Recently, the lexicon
based approaches were extended to learn domain-
dependent lexicons (Kanayama and Nasukawa,
2006; Qiu et al, 2009), but these approaches
also neglect the aspect of opinion strength. Our
method requires only the prior polarity of opinion
roots and can thus be used on top of those meth-
ods for learning the scores of domain-dependent
opinion components. The methods proposed in
(Hu and Liu, 2004; Popescu and Etzioni, 2005b)
can also be categorized into the lexicon based
framework because their procedure starts with a
set of seed words whose polarities are propagated
to other opinion bearing words.
6 Conclusion and Future Work
In this paper we show that the bag-of-opinions
(BoO) representation is better suited for captur-
ing the expressive power of n-grams while at the
same time overcoming their sparsity bottleneck.
Although in this paper we use the BoO represen-
tation to model domain-independent opinions, we
believe the same framework can be extended to
domain-dependent opinions and other NLP appli-
cations which can benefit from modelling n-grams
(given that the n-grams are decomposable in some
way). Moreover, the learned model can be re-
garded as a domain-independent opinion lexicon
with each entry in the lexicon having an associated
score indicating its polarity and strength. This in
turn has potential applications in sentiment sum-
marization, opinionated information retrieval and
opinion extraction.
920
References
Baccianella, S., A. Esuli, and F. Sebastiani. 2009.
Multi-facet rating of product reviews. In ECIR.
Springer.
Chang, C.C. and C.J. Lin, 2001. LIBSVM: a
library for support vector machines. Soft-
ware available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Esuli, A. and F. Sebastiani. 2006. Sentiwordnet: A
publicly available lexical resource for opinion min-
ing. In LREC, pages 417?422.
Friedman, J., T. Hastie, and R. Tibshirani. 2008.
Regularization paths for generalized linear models
via coordinate descent. Technical report, Techni-
cal Report, Available at http://www-stat. stanford.
edu/jhf/ftp/glmnet. pdf.
Ganu, G., N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In 12th International Workshop on the
Web and Databases.
Godbole, Namrata, Manjunath Srinivasaiah, and
Steven Skiena. 2007. Large-scale sentiment anal-
ysis for news and blogs. In ICWSM.
Goldberg, A. B. and X.J. Zhu. 2006. See-
ing stars when there aren?t many stars: Graph-
based semi-supervised learning for sentiment cat-
egorization. In HLT-NAACL 2006 Workshop on
Textgraphs: Graph-based Algorithms for Natural
Language Processing.
Hu, M.Q. and B. Liu. 2004. Mining and summarizing
customer reviews. In CIKM, pages 168?177. ACM
New York,USA.
Ifrim, G., G. Bakir, and G. Weikum. 2008. Fast logis-
tic regression for text categorization with variable-
length n-grams. In KDD, pages 354?362, New
York,USA. ACM.
Kanayama, H. and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented senti-
ment analysis. In EMNLP, pages 355?363.
Kennedy, A. and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110?
125.
Kim, S.M. and E. Hovy. 2004. Determining the senti-
ment of opinions. In COLING, pages 1367?1373.
Lerman, K., S. Blair-Goldensohn, and R. McDonald.
2009. Sentiment summarization: Evaluating and
learning user preferences. In EACL, pages 514?522.
ACL.
Liu, J.J. and S. Seneff. 2009. Review Sentiment
Scoring via a Parse-and-Paraphrase Paradigm. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
161?169. ACL.
Luo, Z.Q. and Q. Tseng. 1992. On the convergence of
the coordinate descent method for convex differen-
tiable minimization. Journal of Optimization The-
ory and Applications, 72(1):7?35.
Pang, B. and L. Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In ACL, page 124. ACL.
Pang, B. and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Popescu, A.M. and O. Etzioni. 2005a. Extracting
product features and opinions from reviews. In
HLT/EMNLP, volume 5, pages 339?346. Springer.
Popescu, A.M. and O. Etzioni. 2005b. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP, volume 5, pages 339?
346. Springer.
Qiu, G., B. Liu, J.J. Bu, and C. Chen. 2009. Ex-
panding Domain Sentiment Lexicon through Dou-
ble Propagation. In IJCAI.
Snyder, B. and R. Barzilay. 2007. Multiple as-
pect ranking using the good grief algorithm. In
NAACL/HLT, pages 300?307.
Tibshirani, R. 1996. Regression shrinkage and selec-
tion via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Titov, I. and R. McDonald. 2008a. A joint model of
text and aspect ratings for sentiment summarization.
In HLT/ACL, pages 308?316.
Titov, I. and R. McDonald. 2008b. Modeling online
reviews with multi-grain topic models. In WWW,
pages 111?120. ACM.
Wilson, T., J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In HLT/ACL, pages 347?354.
Zhai, C. X. and J. Lafferty. 2004. A study of smooth-
ing methods for language models applied to infor-
mation retrieval. ACM Transactions on Information
Systems, 22(2):179?214.
Zou, H. and T. Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of
the Royal Statistical Society Series B(Statistical
Methodology), 67(2):301?320.
921
