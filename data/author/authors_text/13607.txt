Proceedings of NAACL-HLT 2013, pages 556?562,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Embracing Ambiguity: A Comparison of Annotation Methodologies for
Crowdsourcing Word Sense Labels
David Jurgens
Department of Computer Science
University of California, Los Angeles
jurgens@cs.ucla.edu
Abstract
Word sense disambiguation aims to identify
which meaning of a word is present in a given
usage. Gathering word sense annotations is a
laborious and difficult task. Several methods
have been proposed to gather sense annota-
tions using large numbers of untrained anno-
tators, with mixed results. We propose three
new annotation methodologies for gathering
word senses where untrained annotators are
allowed to use multiple labels and weight the
senses. Our findings show that given the ap-
propriate annotation task, untrained workers
can obtain at least as high agreement as anno-
tators in a controlled setting, and in aggregate
generate equally as good of a sense labeling.
1 Introduction
Word sense annotation is regarded as one of the most
difficult annotation tasks (Artstein and Poesio, 2008)
and building manually-annotated corpora with high-
quality sense labels is often a time- and resource-
consuming task. As a result, nearly all sense-tagged
corpora in wide-spread use are created using trained
annotators (Hovy et al, 2006; Passonneau et al,
2010), which results in a knowledge acquisition bot-
tleneck for training systems that require sense labels
(Gale et al, 1992). In other NLP areas, this bot-
tleneck has been addressed through gathering anno-
tations using many untrained workers on platforms
such as Amazon Mechanical Turk (MTurk), a task
commonly referred to as crowdsourcing. Recently,
several works have proposed gathering sense anno-
tations using crowdsourcing (Snow et al, 2008; Bie-
mann and Nygaard, 2010; Passonneau et al, 2012b;
Rumshisky et al, 2012). However, these meth-
ods produce sense labels that are different from the
commonly used sense inventories such as WordNet
(Fellbaum, 1998) or OntoNotes (Hovy et al, 2006).
Furthermore, while Passonneau et al (2012b) did
use WordNet sense labels, they found the quality
was well below that of trained experts.
We revisit the task of crowdsourcing word sense
annotations, focusing on two key aspects: (1) the
annotation methodology itself, and (2) the restric-
tion to single sense assignment. First, the choice in
sense inventory plays an important role in gathering
high-quality annotations; fine-grained inventories
such as WordNet often contain several related senses
for polysemous words, which untrained annotators
find difficult to correctly apply in a given context
(Chugur et al, 2002; McCarthy, 2006; Palmer et
al., 2007; Rumshisky and Batiukova, 2008; Brown
et al, 2010). However, many agreement studies
have restricted annotators to using a single sense,
which can significantly lower inter-annotator agree-
ment (IAA) in the presence of ambiguous or poly-
semous usages; indeed, multiple studies have shown
that when allowed, annotators readily assign multi-
ple senses to a single usage (Ve?ronis, 1998; Mur-
ray and Green, 2004; Erk et al, 2009; Passonneau
et al, 2012b). Therefore, we focus on annotation
methodologies that enable workers to use as many
labels as they feel appropriate, asking the question:
if allowed to make labeling ambiguity explicit, will
annotators agree? Furthermore, we adopt the goal
of Erk et al (2009), which enabled annotators to
weight each sense by its applicability to the given
context, thereby quantifying the ambiguity.
556
This paper provides the following contributions.
First, we demonstrate that the choice in annotation
setup can significantly improve IAA and that the la-
bels of untrained workers follow consistent patterns
that enable creating high quality labeling from their
aggregate. Second, we find that the sense labeling
from crowdsourcing matches performance with an-
notators in a controlled setting.
2 Related Work
Given the potential utility of a sense-labeled corpus,
multiple studies have examined how to efficiently
gather high quality sense annotations. Snow et al
(2008) had MTurk workers, referred to as Turkers,
disambiguate uses of ?president.? While they re-
ported extremely high IAA (0.952), their analysis
was only performed on a single word.
Biemann and Nygaard (2010) and Biemann
(2012) construct a sense-labeled corpus by concur-
rently constructing the sense inventory itself. Turk-
ers used a lexical substitution task to identify valid
substitutions of a target word. The contexts for the
resulting substitutions were clustered based on their
word overlap and the resulting clusters were labeled
as senses. Biemann and Nygaard (2010) showed that
the number of sense definitions for a word in their
inventory was correlated with the number in Word-
Net, often with their inventory having fewer senses
by combining related meanings and omitting rare
meanings.
Hong and Baker (2011) evaluated multiple anno-
tation strategies for gathering FrameNet sense anno-
tations, ultimately yielding high (>90%) accuracy
for most terms after filtering. They highlight am-
biguous and polysemous usages as a notable source
of errors, which the present work directly addresses.
In the most related work, Passonneau et al
(2012b) had Turkers annotate contexts using one or
more senses, with the requirement that a worker la-
bels all contexts. While they found that agreement
between all workers was low, their annotations could
be combined using the GLAD model (Whitehill et
al., 2000) to obtain good performance, though not
as good as trained annotators.
3 Annotation Methodologies
We consider three methodologies for gathering
sense labels: (1) the methodology of Erk et al
(2009) for gathering weighted labels, (2) a multi-
stage strategy that uses both binary and Likert rat-
ings, and (3) MaxDiff, a paired choice format.
Likert Ratings Likert rating scales provide the
most direct way of gathering weighted sense labels;
Turkers are presented with all senses of a word and
then asked to rate each on a numeric scale. We adopt
the annotation guidelines of Erk et al (2009) which
used a five-point scale, ranging from 1 to 5, indicat-
ing the sense does not apply or that it matches the
contextual usage exactly, respectively.
Select and Rate Recent efforts in crowdsourc-
ing have proposed multi-stage processes for accom-
plishing complex tasks, where efforts by one group
of workers are used to create new subtasks for other
workers to complete (Bernstein et al, 2010; Kittur
et al, 2011; Kulkarni et al, 2012). We propose a
two-stage strategy that aims to reduce the complex-
ity of the annotation task, referred to as Select and
Rate (S+R). First, Turkers are presented with all the
senses and asked to make a binary choice of which
senses apply. Second, a Likert rating task is created
for only those senses whose selection frequency is
above a threshold, thereby concentrating worker fo-
cus on a potentially smaller set of senses.
Our motivation for S+R is two-fold. First, the
sense definitions of certain words may be unclear
or misinterpreted by a minority of the Turkers, who
then systematically rate inapplicable senses as appli-
cable. The Select task can potentially remove such
noise and therefore improve both IAA and rating
quality in the subsequent Rate task. Second, while
the present study analyzes words with 4?8 senses,
we are ultimately interested in annotating highly
polysemous words with tens of senses, which could
present a significant cognitive burden for an annota-
tor to rate concurrently. Here, the Select stage can
potentially reduce the number of senses presented,
leading to less cognitive burden in the Rate stage.
Furthermore, as a pragmatic benefit, removing in-
applicable senses reduces the visual space required
for displaying the questions on the MTurk platform,
which can improve annotation throughput.
MaxDiff MaxDiff is an alternative to scale-based
ratings in which Turkers are presented with a only
subset of all of a word?s senses and then asked to se-
lect (1) the sense option that best matches the mean-
557
add.v ask.v win.v argument.n interest.n paper.n different.a important.a
Erk et al (2009) IAA 0.470 0.354 0.072 0.497 0.320 0.403 0.212 0.466
MTurk Likert IAA 0.336 0.212 0.129 0.250 0.209 0.522 0.030 0.240
MTurk Select 0.309 0.127 0.179 0.192 0.164 0.449 0.024 0.111
MTurk Rate 0.204 0.076 0.026 0.005 0.081 0.108 0.005 0.116
MTurk MaxDiff 0.493 0.353 0.295 - 0.349 0.391 0.220 0.511
Likert Mode 0.500 0.369 0.083 0.445 0.388 0.518 0.124 0.516
S+R Median 0.473 0.394 0.149 0.497 0.390 0.497 0.103 0.416
MTurk MaxDiff 0.508 0.412 0.184 - 0.408 0.496 0.115 0.501
Sampled Baseline 0.238 0.178 0.042 0.254 0.162 0.205 0.100 0.221
Random Baseline 0.239 0.186 0.045 0.249 0.269 0.200 0.110 0.269
Table 1: IAA per word (top) and IAA between aggregate labelings and the GWS annotators (bottom)
ing in the example context and (2) the sense option
that least matches (Louviere, 1991). In our setting,
we presented three options at a time for words with
fewer than seven senses, and four options for those
with seven senses. For a single context, multiple
subsets of the senses are presented and then their rel-
ative ranking is used to produce the numeric rating.
The final applicability ratings were produced using
a modification of the counting procedure of Orme
(2009). First, all sense ratings are computed as the
number of times the sense was rated best minus the
number of times rated least. Second, all negatively-
rated senses are assigned score of 1, and all posi-
tively ratings are normalized to be (1, 5].
4 Experiments
For measuring the difference in methodologies, we
propose three experiments based on different anal-
yses of comparing Turker and non-Turker annota-
tions on the same dataset, the latter of which we re-
fer to as the reference labeling. First, we measure
the ability of the Turkers individually by evaluat-
ing their IAA with the reference labeling. Second,
many studies using crowdsourcing combine the re-
sults into a single answer, thereby leveraging the
wisdom of the crowds (Surowiecki, 2005) to smooth
over inconsistencies in the data. Therefore, in the
second experiment, we evaluate different methods
of combining Turker responses into a single sense
labeling, referred to as an aggregate labeling, and
comparing that with the reference labeling. Third,
we measure the replicability of the Turker annota-
tions (Kilgarriff, 1999) using a sampling methodol-
ogy. Two equally-sized sets of Turker annotations
are created by randomly sampling without replace-
ment from the full set of annotations for each item.
IAA is calculated between the aggregate labelings
computed from each set. This sampling is repeated
50 times and we report the mean IAA as a measure
of the expected degree of replicability when anno-
tating using different groups of Turkers.
For the reference sense labeling, we use a subset
of the GWS dataset of Erk et al (2009), where three
annotators rated 50 instances each for eight words.
For clarity, we refer to these individuals as the GWS
annotators. Given a word usage in a sentence, GWS
annotators rated the applicability of all WordNet 3.0
senses using the same Likert scale as described in
Section 3. Contexts were drawn evenly from the
SemCor (Miller et al, 1993) and SENSEVAL-3 lex-
ical substitution (Mihalcea et al, 2004) corpora.
GWS annotators were apt to use multiple senses,
with nearly all instances having multiple labels.
For each annotation task, Turkers were presented
with an identical set of annotation guidelines, fol-
lowed by methodology-specific instructions.1 To in-
crease the familiarity with the task, four instances
were shown per task, with all instances using the
same target word. Unlike Passonneau et al (2012b),
we did not require a Turker to annotate all contexts
for a single word; however many Turkers did com-
plete the majority of instances. Both the Likert, Se-
lect, and Rate tasks used ten Turkers each. Senses
were passed from Select to Rate if they received at
1Full guidelines are available at http://cs.ucla.
edu/?jurgens/sense-annotation/
558
least three votes. For MaxDiff, we gathered at least
3n annotations per context where n is the number of
senses of the target word, ensuring that each sense
appeared at least once. Due to resource limitations,
we omitted the evaluation of argument.n for MaxD-
iff. Following the recommendation of Kosinski et al
(2012), Turkers were paid $0.05USD for each Lik-
ert, Select, and Rate task. For MaxDiff, due to their
shorter nature and comparably high volume, Turkers
were paid $0.03USD per task.
To ensure fluency in English as well as reduce the
potential for low-quality results, we prefaced each
task with a simple test question that asked the Turker
to pick out a definition of the target word from a list
of four options. The incorrect options were selected
so that they would be nonsensical for anyone famil-
iar with the target word. Additionally, we rejected
all Turker responses where more than one option
was missing a rating. In the case of missing ratings,
we infer a rating of 1. Approximately 20-30% of the
submissions were rejected by these criteria, under-
scoring the importance of filtering.
For measuring IAA, we selected Krippendorff?s
? (Krippendorff, 1980; Artstein and Poesio, 2008),
which is an agreement coefficient that handles miss-
ing data, as well as different levels of measurement,
e.g., nominal data (Select and MaxDiff) and interval
data (Likert and Rate).2 Krippendorff?s ? adjusts for
chance, ranging between [?1, 1] for nominal data
and (?1, 1] for interval data, where 1 indicates per-
fect agreement and -1 indicates systematic disagree-
ment; random labels would have an expected ? of
zero. We treat each sense and instance combination
as a separate item to rate.
5 Results
The results of the first experiment appear in the top
of Table 1. Two important aspects emerge. First, the
word itself plays a significant role in IAA. Though
Erk et al (2009) reported a pair-wise IAA of the
GWS annotators between 0.466 and 0.506 using
Spearman?s ?, the IAA varies considerably between
words for both Turkers and GWS annotators when
measured using Krippendorff?s ?.
Second, the choice of annotation methodology
2We note that although the ratings are technically given on
an ordinal scale (ranks), we use the interval scale to allow com-
parison with rational ratings from the aggregate solutions.
significantly impacts IAA. While both the Likert and
S+R tasks have lower IAA than the GWS annota-
tors do, the MaxDiff annotators achieve higher IAA
for almost all words. We hypothesize that compar-
ing senses for applicability is an easier task for the
untrained worker, rather than having to construct a
mental scale of what constitutes the applicability of
each sense. Surprisingly, the binary Select task has
a lower IAA than the more complex the Likert task.
An analysis of the duration of median task comple-
tion times for the Likert and Select tasks showed lit-
tle difference (with the exception of paper.n, which
was on average 50 second faster for Likert ratings),
suggesting that both tasks are equally as cognitively
demanding. In addition, the Rate task has the lowest
IAA, despite its similarity to the Likert task. An in-
spection of the annotations shows that the full rating
scale was used, so the low value is not due to Turk-
ers always using the same rating, which would yield
an IAA near chance.
In the second experiment, we created a aggregate
sense labeling and compared its IAA with the GWS
annotators, shown in Table 1 (bottom). For scale-
based ratings, we considered three arithmetic oper-
ations for selecting the final rating: mode, median,
and mean. We found that the mode yielded the high-
est average IAA for the Likert ratings and median for
S+R; however, the differences in IAA using each op-
eration were often small. We compare the IAA with
GWS annotators against two baselines: one gener-
ated by sampling from the GWS annotators? rating
distribution, and a second generated by uniformly
sampling in [1, 5]. By comparison, the aggregate la-
belings have a much larger IAA than the baselines,
which is often at least as high as the IAA amongst
the GWS annotators themselves, indicating that the
Turkers in aggregate are capable of producing equiv-
alent ratings. Of the three annotation methodolo-
gies, MaxDiff provides the highest IAA both within
its annotators and with its aggregate key. Surpris-
ingly, neither the Likert or S+R aggregate labeling
appears better than the other.
Based on the second experiment, we measured
the average IAA across all words between the ag-
gregate Likert and MaxDiff solutions, which was
0.472. However, this IAA is significantly affected by
the annotations for win.v and different.a, which had
the lowest IAA among Turkers (Table 1) and there-
559
Corpus Sense Inventory IAA Measurement
SensEval-1
(Kilgarriff and Rosenzweig, 2000)
HECTOR 0.950 Replicability experiment
(Kilgarriff, 1999)
OntoNotes (Hovy et al, 2006) OntoNotes ? 0.90? Pair-wise agreement
SALSA (Burchardt et al, 2006) FrameNet 0.86 Percentage agreement
SensEval-2 Lexical Sample
(Kilgarriff, 2002)
WordNet 1.7 0.853, 0.710, 0.673? Adjudicated Agreement
GWS with MaxDiff Replicability WordNet 3.0 0.815 Krippendorff?s ?
SemCor (Fellbaum et al, 1998) WordNet 1.6 0.786, 0.57? Percentage agreement
SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement
MASC (Passonneau et al, 2012a) WordNet 3.1 -0.02 to 0.88/ Krippendorff?s ?
with MASI (Passonneau et al, 2006)
MASC, single phase reported
in Passonneau et al (2010)
WordNet 3.1 0.515 Krippendorff?s ?
GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff?s ?
GWS with Erk et al (2009) annotators WordNet 3.0 0.349 Krippendorff?s ?
? Not all words achieved this agreement.
? Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagree-
ment, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a case
of agreement. However, the unadjudicated agreement for the dataset was 67.3 measured using pair-wise agreement. A
re-annotation by Palmer et al (2004) produced a similar pair-wise agreement of 71.0.
? Tou et al (1999) perform a re-annotation test of the same data using student annotators, finding substantially lower agreement
 Excludes agreement for argument.n, which was not annotated
/ IAA ranges for 37 words; no corpus-wide IAA is provided.
Table 2: IAA for sense-annotated corpora
fore produce noisy aggregate solutions. When win.v
and different.a are excluded, the agreement between
aggregate Likert and MaxDiff solutions is 0.649.
While this IAA is still moderate, it suggests that
Turkers can still produce similar annotations even
when using different annotation methodologies.
For the third experiment, replicability is reported
as the average IAA between the sampled aggregate
labelings for all annotated words. Table 2 shows this
IAA for Likert and MaxDiff methodologies in com-
parison to other sense annotation studies. Krippen-
dorff (2004) recommends that an ? of 0.8 is nec-
essary to claim high-quality agreement, which is
achieved by the MaxDiff methodology. In contrast,
the average IAA between sampled Likert ratings is
significantly lower, though the methodology does
achieve an ? of 0.812 for paper.n. However, when
the two words with the lowest IAA, win.v and differ-
ent.a, are excluded, the average ? increases to 0.880
for MaxDiff and 0.649 for Likert. Overall, these re-
sults suggest that MaxDiff can generate highly repli-
cable annotations with agreement on par with that of
other high-quality sense-labeled corpora. Further-
more, the Likert methodology may in aggregate still
produce moderately replicable annotations in some
cases.
6 Conclusion and Future Work
Word sense disambiguation is a difficult task, both
for humans and algorithms, with an important bot-
tleneck in acquiring large sense annotated corpora.
As a potential solution, we proposed three annota-
tion methodologies for crowdsourcing sense labels.
Importantly, we relax the single sense assignment
restriction in order to let annotators explicitly note
ambiguity through weighted sense ratings. Our find-
ings reveal that moderate IAA can be obtained using
MaxDiff ratings, with IAA surpassing that of anno-
tators in a controlled setting. Furthermore, our find-
ings showed marked differences in rating difficulty
per word, even in the weighted rating setting. In
future work, we will investigate what factors influ-
ence annotation difficulty in order to improve IAA
to what is considered expert levels, drawing from
existing work analyzing difficulty in the single label
setting (Murray and Green, 2004; Passonneau et al,
2009; Cinkova? et al, 2012).
560
References
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555?596.
Michael S. Bernstein, Ggreg Little, Robert C. Miller,
Bjo?n Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceedings
of UIST, pages 313?322. ACM.
Chris Biemann and Valerie Nygaard. 2010. Crowdsourc-
ing WordNet. In The 5th International Conference of
the Global WordNet Association (GWC-2010).
Chris Biemann. 2012. Turk Bootstrap Word Sense In-
ventory 2.0: A Large-Scale Resource for Lexical Sub-
stitution. In Proceedings of LREC.
Susan Windisch Brown, Travis Rood, and Martha Palmer.
2010. Number or nuance: Which factors restrict reli-
able word sense annotation? In Proceedings of LREC.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Recent
Successes and Future Directions, pages 32?39. ACL.
Silvie Cinkova?, Martin Holub, and Vincent Kr??. 2012.
Managing uncertainty in semantic tagging. In Pro-
ceedings of EACL, pages 840?850. ACL.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word usages.
In Proceedings of ACL, pages 10?18. ACL.
Christiane Fellbaum, Jaochim Grabowski, and Shari Lan-
des. 1998. Performance and confidence in a seman-
tic annotation task. WordNet: An electronic lexical
database, pages 217?237.
Christine Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. A method for disambiguating word
senses in a large corpus. Computers and the Humani-
ties, 26(5):415?439.
J. Hong and C.F. Baker. 2011. How Good is the Crowd
at ?real? WSD? In Proceedings of the Fifth Linguistic
Annotation Workshop (LAW V), pages 30?37. ACL.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of NAACL, pages
57?60. ACL.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and
results for english senseval. Computers and the Hu-
manities, 34(1):15?48.
Adam Kilgarriff. 1999. 95% replicability for manual
word sense tagging. In Proceedings of EACL, pages
277?278. ACL.
Adam Kilgarriff. 2002. English lexical sample task de-
scription. In Senseval-2: Proceedings of the 2nd In-
ternational Workshop on Evaluating Word Sense Dis-
ambiguation Systems.
A. Kittur, B. Smus, S. Khamkar, and R.E. Kraut. 2011.
Crowdforge: Crowdsourcing complex work. In Pro-
ceedings of UIST, pages 43?52. ACM.
M. Kosinski, Y. Bachrach, G. Kasneci, J. Van-Gael, and
T. Graepel. 2012. Crowd IQ: Measuring the intelli-
gence of crowdsourcing platforms. In ACM Web Sci-
ences. ACM.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage, Beverly Hills, CA.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology. Sage, Thousand Oaks,
CA, second edition.
A. Kulkarni, M. Can, and B. Hartmann. 2012. Collabo-
ratively crowdsourcing workflows with turkomatic. In
Proceedings of CSCW, pages 1003?1012. ACM.
J. J. Louviere. 1991. Best-Worst Scaling: A Model for
the Largest Difference Judgments. Technical report,
University of Alberta. Working Paper.
Diana McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. In Proceedings of the
ACL Workshop on Making Sense of Sense: Bringing
Psycholinguistics and Computational Linguistics To-
gether, pages 17?24.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text, pages 25?28. ACL.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of HLT, pages 303?308. ACL.
G. Craig Murray and Rebecca Green. 2004. Lexical
knowledge and human disagreement on a WSD task.
Computer Speech & Language, 18(3):209?222.
Bryan Orme. 2009. MaxDiff Analysis: Simple Count-
ing, Individual-Level Logit, and HB. Sawtooth Soft-
ware.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the Second Work-
shop on Scalable Natural Language Understanding
Systems. ACL.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
Natural Language Engineering, 13(02):137?163.
561
Rebecca Passonneau, Nizar Habash, and Owen Rambow.
2006. Inter-annotator agreement on a multilingual
semantic annotation task. In Proceedings of LREC,
pages 1951?1956.
Rebecca J. Passonneau, Ansaf Salleb-Aouissi, and Nancy
Ide. 2009. Making sense of word sense variation. In
Proceedings of the NAACL HLT Workshop on Seman-
tic Evaluations: Recent Achievements and Future Di-
rections.
Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense anno-
tation of polysemous words by multiple annotators. In
Proceedings of LREC.
Rebecca J Passonneau, Collin Baker, Christiane Fell-
baum, and Nancy Ide. 2012a. The MASC word sense
sentence corpus. In Proceedings of LREC.
Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-
Aouissi, and Nancy Ide. 2012b. Multiplicity and
word sense: evaluating and learning from multiply la-
beled word sense annotations. Language Resources
and Evaluation, 46(2):209?252.
Anna Rumshisky and Olga Batiukova. 2008. Polysemy
in verbs: systematic relations between senses and their
effect on annotation. In Proceedings of the Workshop
on Human Judgements in Computational Linguistics,
pages 33?41. ACL.
Anna Rumshisky, Nick Botchan, Sophie Kushkuley, and
James Pustejovsky. 2012. Word Sense Inventories by
Non-experts. In Procoeedings of LREC.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fastbut is it good?: Eval-
uating non-expert annotations for natural language
tasks. In Proceedings of EMNLP, pages 254?263.
ACL.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 41?43.
James Surowiecki. 2005. The wisdom of crowds. An-
chor.
Ng Hwee Tou, Chung Yong Lim, and Shou King Foo.
1999. A Case Study on Inter-Annotator Agreement
for Word Sense Disambiguation. In Proceedings of
the ACL SIGLEX Workshop on Standardizing Lexical
Resources.
Jean Ve?ronis. 1998. A study of polysemy judgments and
inter-annotator agreement. In Program and advanced
papers of the Senseval workshop, pages 2?4.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2000. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. In Proceedings of
NIPS.
562
Proceedings of the ACL 2010 System Demonstrations, pages 30?35,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
The S-Space Package: An Open Source Package for Word Space Models
David Jurgens
University of California, Los Angeles,
4732 Boelter Hall
Los Angeles, CA 90095
jurgens@cs.ucla.edu
Keith Stevens
University of California, Los Angeles,
4732 Boelter Hall
Los Angeles, CA 90095
kstevens@cs.ucla.edu
Abstract
We present the S-Space Package, an open
source framework for developing and eval-
uating word space algorithms. The pack-
age implements well-known word space
algorithms, such as LSA, and provides a
comprehensive set of matrix utilities and
data structures for extending new or ex-
isting models. The package also includes
word space benchmarks for evaluation.
Both algorithms and libraries are designed
for high concurrency and scalability. We
demonstrate the efficiency of the reference
implementations and also provide their re-
sults on six benchmarks.
1 Introduction
Word similarity is an essential part of understand-
ing natural language. Similarity enables meaning-
ful comparisons, entailments, and is a bridge to
building and extending rich ontologies for evaluat-
ing word semantics. Word space algorithms have
been proposed as an automated approach for de-
veloping meaningfully comparable semantic rep-
resentations based on word distributions in text.
Many of the well known algorithms, such as
Latent Semantic Analysis (Landauer and Dumais,
1997) and Hyperspace Analogue to Language
(Burgess and Lund, 1997), have been shown to
approximate human judgements of word similar-
ity in addition to providing computational mod-
els for other psychological and linguistic phenom-
ena. More recent approaches have extended this
approach to model phenomena such as child lan-
guage acquisition (Baroni et al, 2007) or seman-
tic priming (Jones et al, 2006). In addition, these
models have provided insight in fields outside of
linguistics, such as information retrieval, natu-
ral language processing and cognitive psychology.
For a recent survey of word space approaches and
applications, see (Turney and Pantel, 2010).
The parallel development of word space models
in different fields has often resulted in duplicated
work. The pace of development presents a need
for a reliable method for accurate comparisons be-
tween new and existing approaches. Furthermore,
given the frequent similarity of approaches, we
argue that the research community would greatly
benefit from a common library and evaluation util-
ities for word spaces. Therefore, we introduce the
S-Space Package, an open source framework with
four main contributions:
1. reference implementations of frequently
cited algorithms
2. a comprehensive, highly concurrent library of
tools for building new models
3. an evaluation framework for testing mod-
els on standard benchmarks, e.g. the TOEFL
Synonym Test (Landauer et al, 1998)
4. a standardized interface for interacting with
all word space models, which facilitates word
space based applications.
The package is written in Java and defines a
standardized Java interface for word space algo-
rithms. While other word space frameworks ex-
ist, e.g. (Widdows and Ferraro, 2008), the focus
of this framework is to ease the development of
new algorithms and the comparison against exist-
ing models. Compared to existing frameworks,
the S-Space Package supports a much wider vari-
ety of algorithms and provides significantly more
reusable developer utilities for word spaces, such
as tokenizing and filtering, sparse vectors and
matrices, specialized data structures, and seam-
less integration with external programs for di-
mensionality reduction and clustering. We hope
that the release of this framework will greatly fa-
cilitate other researchers in their efforts to de-
velop and validate new word space models. The
toolkit is available at http://code.google.com/
p/airhead-research/, which includes a wiki
30
containing detailed information on the algorithms,
code documentation and mailing list archives.
2 Word Space Models
Word space models are based on the contextual
distribution in which a word occurs. This ap-
proach has a long history in linguistics, starting
with Firth (1957) and Harris (1968), the latter
of whom defined this approach as the Distribu-
tional Hypothesis: for two words, their similarity
in meaning is predicted by the similarity of the
distributions of their co-occurring words. Later
models have expanded the notion of co-occurrence
but retain the premise that distributional similarity
can be used to extract meaningful relationships be-
tween words.
Word space algorithms consist of the same core
algorithmic steps: word features are extracted
from a corpus and the distribution of these features
is used as a basis for semantic similarity. Figure 1
illustrates the shared algorithmic structure of all
the approaches, which is divided into four compo-
nents: corpus processing, context selection, fea-
ture extraction and global vector space operations.
Corpus processing normalizes the input to cre-
ate a more uniform set of features on which the al-
gorithm can work. Corpus processing techniques
frequently include stemming and filtering of stop
words or low-frequency words. For web-gathered
corpora, these steps also include removal of non
linguistic tokens, such as html markup, or restrict-
ing documents to a single language.
Context selection determines which tokens in a
document may be considered for features. Com-
mon approaches use a lexical distance, syntac-
tic relation, or document co-occurrence to define
the context. The various decisions for selecting
the context accounts for many differences between
otherwise similar approaches.
Feature extraction determines the dimensions of
the vector space by selecting which tokens in the
context will count as features. Features are com-
monly word co-occurrences, but more advanced
models may perform a statistical analysis to se-
lect only those features that best distinguish word
meanings. Other models approximate the full set
of features to enable better scalability.
Global vector space operations are applied to
the entire space once the initial word features have
been computed. Common operations include al-
tering feature weights and dimensionality reduc-
Document-Based Models
LSA (Landauer and Dumais, 1997)
ESA (Gabrilovich and Markovitch, 2007)
Vector Space Model (Salton et al, 1975)
Co-occurrence Models
HAL (Burgess and Lund, 1997)
COALS (Rohde et al, 2009)
Approximation Models
Random Indexing (Sahlgren et al, 2008)
Reflective Random Indexing (Cohen et al, 2009)
TRI (Jurgens and Stevens, 2009)
BEAGLE (Jones et al, 2006)
Incremental Semantic Analysis (Baroni et al, 2007)
Word Sense Induction Models
Purandare and Pedersen (Purandare and Pedersen, 2004)
HERMIT (Jurgens and Stevens, 2010)
Table 1: Algorithms in the S-Space Package
tion. These operations are designed to improve
word similarity by changing the feature space it-
self.
3 The S-Space Framework
The S-Space framework is designed to be extensi-
ble, simple to use, and scalable. We achieve these
goals through the use of Java interfaces, reusable
word space related data structures, and support for
multi-threading. Each word space algorithm is de-
signed to run as a stand alone program and also to
be used as a library class.
3.1 Reference Algorithms
The package provides reference implementations
for twelve word space algorithms, which are listed
in Table 1. Each algorithm is implemented in its
own Java package, and all commonalities have
been factored out into reusable library classes.
The algorithms implement the same Java interface,
which provides a consistent abstraction of the four
processing stages.
We divide the algorithms into four categories
based on their structural similarity: document-
based, co-occurrence, approximation, and Word
Sense Induction (WSI) models. Document-based
models divide a corpus into discrete documents
and construct the vector space from word fre-
quencies in the documents. The documents are
defined independently of the words that appear
in them. Co-occurrence models build the vector
space using the distribution of co-occurring words
in a context, which is typically defined as a re-
gion around a word or paths rooted in a parse
tree. The third category of models approximate
31
Corpus Processing Context Selection Feature Extraction Global Operations
Vector Space
Token Filtering
Stemming
Bigramming
Dimensionality Reduction
Feature Selection
Matrix Transforms
Lexical Distance
In Same Document
Syntactic Link
Word Co-occurence
Joint Probabilitiy
Approximation
Corpus
Figure 1: A high-level depiction of common algorithmic steps that convert a corpus into a word space
co-occurrence data rather than model it explic-
itly in order to achieve better scalability for larger
data sets. WSI models also use co-occurrence but
also attempt to discover distinct word senses while
building the vector space. For example, these al-
gorithms might represent ?earth? with two vectors
based on its meanings ?planet? and ?dirt.?
3.2 Data Structures and Utilities
The S-Space Package provides efficient imple-
mentations for matrices, vectors, and specialized
data structures such as multi-maps and tries. Im-
plementations are modeled after the java.util li-
brary and offer concurrent implementations when
multi-threading is required. In addition, the li-
braries provide support for converting between
multiple matrix formats, enabling interaction with
external matrix-based programs. The package also
provides support for parsing different corpora for-
mats, such as XML or email threads.
3.3 Global Operation Utilities
Many algorithms incorporate dimensionality re-
duction to smooth their feature data, e.g. (Lan-
dauer and Dumais, 1997; Rohde et al, 2009),
or to improve efficiency, e.g. (Sahlgren et al,
2008; Jones et al, 2006). The S-Space Pack-
age supports two common techniques: the Sin-
gular Value Decomposition (SVD) and random-
ized projections. All matrix data structures are de-
signed to seamlessly integrate with six SVD im-
plementations for maximum portability, including
SVDLIBJ1 , a Java port of SVDLIBC2, a scalable
sparse SVD library. The package also provides
a comprehensive library for randomized projec-
tions, which project high-dimensional feature data
into a lower dimensional space. The library sup-
ports both integer-based projections (Kanerva et
al., 2000) and Gaussian-based (Jones et al, 2006).
The package supports common matrix trans-
formations that have been applied to word
spaces: point wise mutual information (Dekang,
1http://bender.unibe.ch/svn/codemap/Archive/svdlibj/
2http://tedlab.mit.edu/?dr/SVDLIBC/
1998), term frequency-inverse document fre-
quency (Salton and Buckley, 1988), and log en-
tropy (Landauer and Dumais, 1997).
3.4 Measurements
The choice of similarity function for the vector
space is the least standardized across approaches.
Typically the function is empirically chosen based
on a performance benchmark and different func-
tions have been shown to provide application spe-
cific benefits (Weeds et al, 2004). To facili-
tate exploration of the similarity function param-
eter space, the S-Space Package provides sup-
port for multiple similarity functions: cosine sim-
ilarity, Euclidean distance, KL divergence, Jac-
card Index, Pearson product-moment correlation,
Spearman?s rank correlation, and Lin Similarity
(Dekang, 1998)
3.5 Clustering
Clustering serves as a tool for building and refin-
ing word spaces. WSI algorithms, e.g. (Puran-
dare and Pedersen, 2004), use clustering to dis-
cover the different meanings of a word in a cor-
pus. The S-Space Package provides bindings for
using the CLUTO clustering package3. In addi-
tion, the package provides Java implementations
of Hierarchical Agglomerative Clustering, Spec-
tral Clustering (Kannan et al, 2004), and the Gap
Statistic (Tibshirani et al, 2000).
4 Benchmarks
Word space benchmarks assess the semantic con-
tent of the space through analyzing the geomet-
ric properties of the space itself. Currently used
benchmarks assess the semantics by inspecting the
representational similarity of word pairs. Two
types of benchmarks are commonly used: word
choice tests and association tests. The S-Space
Package supports six tests, and has an easily ex-
tensible model for adding new tests.
3http://glaros.dtc.umn.edu/gkhome/views/cluto
32
Word Choice Word Association
Algorithm Corpus TOEFL ESL RDWP R-G WordSim353 Deese
BEAGLE TASA 46.03 35.56 46.99 0.431 0.342 0.235
COALS TASA 65.33 60.42 93.02 0.572 0.478 0.388
HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318
HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042
ISA TASA 41.33 18.75 33.72 0.245 0.150 0.286
LSA TASA 56.00a 50.00 45.83 0.652 0.519 0.349
LSA Wiki 60.76 54.17 59.20 0.681 0.614 0.206
P&P TASA 34.67 20.83 31.39 0.088 -0.036 0.216
RI TASA 42.67 27.08 34.88 0.224 0.201 0.211
RI Wiki 68.35 31.25 40.80 0.226 0.315 0.090
RI + Perm.b TASA 52.00 33.33 31.39 0.137 0.260 0.268
RRI TASA 36.00 22.92 34.88 0.088 0.138 0.109
VSM TASA 61.33 52.08 84.88 0.496 0.396 0.200
a Landauer et al (1997) report a score of 64.4 for this test, while Rohde et al (2009) report a score of 53.4.
b + Perm indicates that permutations were used with Random Indexing, as described in (Sahlgren et al, 2008)
Table 2: A comparison of the implemented algorithms on common evaluation benchmarks
4.1 Word Choice
Word choice tests provide a target word and a list
of options, one of which has the desired relation to
the target. Word space models solve these tests by
selecting the option whose representation is most
similar. Three word choice benchmarks that mea-
sure synonymy are supported.
The first test is the widely-reported Test of En-
glish as a Foreign Language (TOEFL) synonym
test from (Landauer et al, 1998), which consists
of 80 multiple-choice questions with four options.
The second test comes from the English as a Sec-
ond Language (ESL) exam and consists of 50
question with four choices (Turney, 2001). The
third consists of 200 questions from the Canadian
Reader?s Digest Word Power (RDWP) (Jarmasz
and Szpakowicz, 2003), which unlike the previ-
ous two tests, allows the target and options to be
multi-word phrases.
4.2 Word Association
Word association tests measure the semantic re-
latedness of two words by comparing word space
similarity with human judgements. Frequently,
these tests measure synonymy; however, other
types of word relations such as antonymy (?hot?
and ?cold?) or functional relatedness (?doctor?
and ?hospital?) are also possible. The S-Space
Package supports three association tests.
The first test uses data gathered by Rubenstein
and Goodneough (1965). To measure word simi-
larity, word similarity scores of 51 human review-
ers were gathered a set of 65 noun pairs, scored on
a scale of 0 to 4. The ratings are then correlated
with word space similarity scores.
Finkelstein et al (2002) test for relatedness. 353
word pairs were rated by either 13 or 16 subjects
on a 0 to 10 scale for how related the words are.
This test is notably more challenging for word
space models because human ratings are not tied
to a specific semantic relation.
The third benchmark considers the antonym as-
sociation. Deese (1964) introduced 39 antonym
pairs that Greffenstette (1992) used to assess
whether a word space modeled the antonymy rela-
tionship. We quantify this relationship by measur-
ing the similarity rank of each word in an antonym
pair, w1, w2, i.e. w2 is the kth most-similar word
to w1 in the vector space. The antonym score is
calculated as 2rankw1 (w2)+rankw2 (w1) . The score
ranges from [0, 1], where 1 indicates that the most
similar neighbors in the space are antonyms. We
report the mean score for all 39 antonyms.
5 Algorithm Analysis
The content of a word space is fundamentally
dependent upon the corpus used to construct it.
Moreover, algorithms which use operations such
as the SVD have a limit to the corpora sizes they
33
 0
 5000
 10000
 15000
 20000
 25000
 100000  200000  300000  400000  500000  600000
63.5M 125M 173M 228M 267M 296M
Se
co
nd
s
Number of documents
Tokens in Documents (in millions)
LSA
VSM
COALS
BEAGLE
HAL
RI
Figure 2: Processing time across different corpus
sizes for a word space with the 100,000 most fre-
quent words
 0
 100
 200
 300
 400
 500
 600
 700
 800
2 3 4 5 6 7 8
Pe
rc
en
ta
ge
 im
pr
ov
em
en
t
Number of threads
RRI
BEAGLE
COALS
LSA
HAL
RI
VSM
Figure 3: Run time improvement as a factor of in-
creasing the number of threads
can process. We therefore highlight the differ-
ences in performance using two corpora. TASA
is a collection of 44,486 topical essays introduced
in (Landauer and Dumais, 1997). The second cor-
pus is built from a Nov. 11, 2009 Wikipedia snap-
shot, and filtered to contain only articles with more
than 1000 words. The resulting corpus consists of
387,082 documents and 917 million tokens.
Table 2 reports the scores of reference algo-
rithms on the six benchmarks using cosine simi-
larity. The variation in scoring illustrates that dif-
ferent algorithms are more effective at capturing
certain semantic relations. We note that scores are
likely to change for different parameter configura-
tions of the same algorithm, e.g. token filtering or
changing the number of dimensions.
As a second analysis, we report the efficiency
of reference implementations by varying the cor-
pus size and number of threads. Figure 2 reports
the total amount of time each algorithm needs for
processing increasingly larger segments of a web-
gathered corpus when using 8 threads. In all cases,
only the top 100,000 words were counted as fea-
tures. Figure 3 reports run time improvements due
to multi-threading on the TASA corpus.
Algorithm efficiency is determined by three fac-
tors: contention on global statistics, contention on
disk I/O, and memory limitations. Multi-threading
benefits increase proportionally to the amount of
work done per context. Memory limitations ac-
count for the largest efficiency constraint, espe-
cially as the corpus size and number of features
grow. Several algorithms lack data points for
larger corpora and show a sharp increase in run-
ning time in Figure 2, reflecting the point at which
the models no longer fit into 8GB of memory.
6 Future Work and Conclusion
We have described a framework for developing
and evaluating word space algorithms. Many well
known algorithms are already provided as part of
the framework as reference implementations for
researches in distributional semantics. We have
shown that the provided algorithms and libraries
scale appropriately. Last, we motivate further re-
search by illustrating the significant performance
differences of the algorithms on six benchmarks.
Future work will be focused on providing sup-
port for syntactic features, including dependency
parsing as described by (Pado? and Lapata, 2007),
reference implementations of algorithms that use
this information, non-linear dimensionality reduc-
tion techniques, and more advanced clustering al-
gorithms.
References
Marco Baroni, Alessandro Lenci, and Luca Onnis.
2007. Isa meets lara: A fully incremental word
space model for cognitively plausible simulations of
semantic learning. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguis-
tics.
Curt Burgess and Kevin Lund. 1997. Modeling pars-
ing constraints with high-dimensional context space.
Language and Cognitive Processes, 12:177210.
Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-
dows. 2009. Reflective random indexing and indi-
rect inference: A scalable method for discovery of
implicit connections. Journal of Biomedical Infor-
matics, 43.
J. Deese. 1964. The associative structure of some com-
mon english adjectives. Journal of Verbal Learning
and Verbal Behavior, 3(5):347?357.
34
Lin Dekang. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the Joint An-
nual Meeting of the Association for Computational
Linguistics and International Conference on Com-
putational Linguistics, pages 768?774.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Z. S.
Rivlin, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions of Information Systems, 20(1):116?
131.
J. R. Firth, 1957. A synopsis of linguistic theory 1930-
1955. Oxford: Philological Society. Reprinted in
F. R. Palmer (Ed.), (1968). Selected papers of J. R.
Firth 1952-1959, London: Longman.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI?07: Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, pages 1606?1611.
Gregory Grefenstette. 1992. Finding semantic similar-
ity in raw text: The Deese antonyms. In Working
notes of the AAAI Fall Symposium on Probabilis-
tic Approaches to Natural Language, pages 61?65.
AAAI Press.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Conference on
Recent Advances in Natural Language Processing,
pages 212?219.
Michael N. Jones, Walter Kintsch, and Doughlas J. K.
Mewhort. 2006. High-dimensional semantic space
accounts of priming. Journal of Memory and Lan-
guage, 55:534?552.
David Jurgens and Keith Stevens. 2009. Event detec-
tion in blogs using temporal random indexing. In
Proceedings of RANLP 2009: Events in Emerging
Text Types Workshop.
David Jurgens and Keith Stevens. 2010. HERMIT:
Flexible Clustering for the SemEval-2 WSI Task. In
Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2010). Association
of Computational Linguistics.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Ran-
dom indexing of text samples for latent semantic
analysis. In L. R. Gleitman and A. K. Josh, editors,
Proceedings of the 22nd Annual Conference of the
Cognitive Science Society, page 1036.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2004. On clusterings: Good, bad and spectral. Jour-
nal of the ACM, 51(3):497?515.
Thomas K. Landauer and Susan T. Dumais. 1997. A
solution to Plato?s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104:211?240.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. In-
troduction to Latent Semantic Analysis. Discourse
Processes, (25):259?284.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-Based Construction of Seman-
tic Space Models. Computational Linguistics,
33(2):161?199.
Amruta Purandare and Ted Pedersen. 2004. Word
sense discrimination by clustering contexts in vector
and similarity spaces. In HLT-NAACL 2004 Work-
shop: Eighth Conference on Computational Natu-
ral Language Learning (CoNLL-2004), pages 41?
48. Association for Computational Linguistics.
Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2009. An improved model of
semantic similarity based on lexical co-occurrence.
Cognitive Science. sumitted.
H. Rubenstein and J. B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8:627?633.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of the 30th Annual Meeting of the Cog-
nitive Science Society (CogSci?08).
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing & Management, 24:513?523.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, 18(11):613?620.
Robert Tibshirani, Guenther Walther, and Trevor
Hastie. 2000. Estimating the number of clusters in a
dataset via the gap statistic. Journal Royal Statistics
Society B, 63:411?423.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491?502.
Julie Weeds, David Weir, and Diana McCarty. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics
COLING?04, pages 1015?1021.
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic vectors: a scalable open source package and
online technology management application. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08).
35
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1341?1351,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Align, Disambiguate and Walk: A Unified Approach for
Measuring Semantic Similarity
Mohammad Taher Pilehvar, David Jurgens and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{pilehvar,jurgens,navigli}@di.uniroma1.it
Abstract
Semantic similarity is an essential com-
ponent of many Natural Language Pro-
cessing applications. However, prior meth-
ods for computing semantic similarity of-
ten operate at different levels, e.g., sin-
gle words or entire documents, which re-
quires adapting the method for each data
type. We present a unified approach to se-
mantic similarity that operates at multiple
levels, all the way from comparing word
senses to comparing text documents. Our
method leverages a common probabilistic
representation over word senses in order to
compare different types of linguistic data.
This unified representation shows state-of-
the-art performance on three tasks: seman-
tic textual similarity, word similarity, and
word sense coarsening.
1 Introduction
Semantic similarity is a core technique for many
topics in Natural Language Processing such as
Textual Entailment (Berant et al, 2012), Seman-
tic Role Labeling (Fu?rstenau and Lapata, 2012),
and Question Answering (Surdeanu et al, 2011).
For example, textual similarity enables relevant
documents to be identified for information re-
trieval (Hliaoutakis et al, 2006), while identifying
similar words enables tasks such as paraphrasing
(Glickman and Dagan, 2003), lexical substitution
(McCarthy and Navigli, 2009), lexical simplifica-
tion (Biran et al, 2011), and Web search result
clustering (Di Marco and Navigli, 2013).
Approaches to semantic similarity have often
operated at separate levels: methods for word sim-
ilarity are rarely applied to documents or even sin-
gle sentences (Budanitsky and Hirst, 2006; Radin-
sky et al, 2011; Halawi et al, 2012), while
document-based similarity methods require more
linguistic features, which often makes them in-
applicable at the word or microtext level (Salton
et al, 1975; Maguitman et al, 2005; Elsayed et
al., 2008; Turney and Pantel, 2010). Despite the
potential advantages, few approaches to semantic
similarity operate at the sense level due to the chal-
lenge in sense-tagging text (Navigli, 2009); for ex-
ample, none of the top four systems in the recent
SemEval-2012 task on textual similarity compared
semantic representations that incorporated sense
information (Agirre et al, 2012).
We propose a unified approach to semantic sim-
ilarity across multiple representation levels from
senses to documents, which offers two signifi-
cant advantages. First, the method is applicable
independently of the input type, which enables
meaningful similarity comparisons across differ-
ent scales of text or lexical levels. Second, by op-
erating at the sense level, a unified approach is able
to identify the semantic similarities that exist in-
dependently of the text?s lexical forms and any se-
mantic ambiguity therein. For example, consider
the sentences:
t1. A manager fired the worker.
t2. An employee was terminated from work by
his boss.
A surface-based approach would label the sen-
tences as dissimilar due to the minimal lexical
overlap. However, a sense-based representation
enables detection of the similarity between the
meanings of the words, e.g., fire and terminate.
Indeed, an accurate, sense-based representation is
essential for cases where different words are used
to convey the same meaning.
The contributions of this paper are threefold.
First, we propose a new unified representation of
the meaning of an arbitrarily-sized piece of text,
referred to as a lexical item, using a sense-based
probability distribution. Second, we propose a
novel alignment-based method for word sense dis-
1341
ambiguation during semantic comparison. Third,
we demonstrate that this single representation can
achieve state-of-the-art performance on three sim-
ilarity tasks, each operating at a different lexical
level: (1) surpassing the highest scores on the
SemEval-2012 task on textual similarity (Agirre
et al, 2012) that compares sentences, (2) achiev-
ing a near-perfect performance on the TOEFL syn-
onym selection task proposed by Landauer and
Dumais (1997), which measures word pair sim-
ilarity, and also obtaining state-of-the-art perfor-
mance in terms of the correlation with human
judgments on the RG-65 dataset (Rubenstein and
Goodenough, 1965), and finally (3) surpassing the
performance of Snow et al (2007) in a sense-
coarsening task that measures sense similarity.
2 A Unified Semantic Representation
We propose a representation of any lexical item as
a distribution over a set of word senses, referred
to as the item?s semantic signature. We begin
with a formal description of the representation at
the sense level (Section 2.1). Following this, we
describe our alignment-based disambiguation al-
gorithm which enables us to produce sense-based
semantic signatures for those lexical items (e.g.,
words or sentences) which are not sense annotated
(Section 2.2). Finally, we propose three methods
for comparing these signatures (Section 2.3). As
our sense inventory, we use WordNet 3.0 (Fell-
baum, 1998).
2.1 Semantic Signatures
The WordNet ontology provides a rich net-
work structure of semantic relatedness, connect-
ing senses directly with their hypernyms, and pro-
viding information on semantically similar senses
by virtue of their nearby locality in the network.
Given a particular node (sense) in the network, re-
peated random walks beginning at that node will
produce a frequency distribution over the nodes
in the graph visited during the walk. To ex-
tend beyond a single sense, the random walk may
be initialized and restarted from a set of senses
(seed nodes), rather than just one; this multi-seed
walk produces a multinomial distribution over all
the senses in WordNet with higher probability as-
signed to senses that are frequently visited from
the seeds. Prior work has demonstrated that multi-
nomials generated from random walks over Word-
Net can be successfully applied to linguistic tasks
such as word similarity (Hughes and Ramage,
2007; Agirre et al, 2009), paraphrase recogni-
tion, textual entailment (Ramage et al, 2009),
and pseudoword generation (Pilehvar and Navigli,
2013).
Formally, we define the semantic signature of
a lexical item as the multinomial distribution gen-
erated from the random walks over WordNet 3.0
where the set of seed nodes is the set of senses
present in the item. This representation encom-
passes both when the item is itself a single sense
and when the item is a sense-tagged sentence.
To construct each semantic signature, we use
the iterative method for calculating topic-sensitive
PageRank (Haveliwala, 2002). Let M be the ad-
jacency matrix for the WordNet network, where
edges connect senses according to the rela-
tions defined in WordNet (e.g., hypernymy and
meronymy). We further enrich M by connecting
a sense with all the other senses that appear in its
disambiguated gloss.1 Let ~v(0) denote the prob-
ability distribution for the starting location of the
random walker in the network. Given the set of
senses S in a lexical item, the probability mass
of ~v(0) is uniformly distributed across the senses
si ? S, with the mass for all sj /? S set to zero.
The PageRank may then be computed using:
~v (t) = (1? ?)M~v (t?1) + ? ~v (0) (1)
where at each iteration the random walker may
jump to any node si ? S with probability ?/|S|.
We follow standard convention and set ? to 0.15.
We repeat the operation in Eq. 1 for 30 itera-
tions, which is sufficient for the distribution to
converge. The resulting probability vector ~v(t) is
the semantic signature of the lexical item, as it
has aggregated its senses? similarities over the en-
tire graph. For our semantic signatures we used
the UKB2 off-the-shelf implementation of topic-
sensitive PageRank.
2.2 Alignment-Based Disambiguation
Commonly, semantic comparisons are between
word pairs or sentence pairs that do not have their
lexical content sense-annotated, despite the poten-
tial utility of sense annotation in making seman-
tic comparisons. However, traditional forms of
word sense disambiguation are difficult for short
texts and single words because little or no con-
textual information is present to perform the dis-
ambiguation task. Therefore, we propose a novel
1http://wordnet.princeton.edu
2http://ixa2.si.ehu.es/ukb/
1342
Figure 1: (a) Example alignments of the first sense of term manager (in sentence t1) to the two first
senses of the word types in sentence t2, along with the similarity of the two senses? semantic signatures;
(b) Alignments which maximize the similarities across words in t1 and t2 (the source side of an alignment
is taken as the disambiguated sense of its corresponding word).
alignment-based sense disambiguation that lever-
ages the content of the paired item in order to dis-
ambiguate each element. Leveraging the paired
item enables our approach to disambiguate where
traditional sense disambiguation methods can not
due to insufficient context.
We view sense disambiguation as an alignment
problem. Given two arbitrarily ordered texts, we
seek the semantic alignment that maximizes the
similarity of the senses of the context words in
both texts. To find this maximum we use an align-
ment procedure which, for each word type wi in
item T1, assigns wi to the sense that has the max-
imal similarity to any sense of the word types in
the compared text T2. Algorithm 1 formalizes the
alignment process, which produces a sense dis-
ambiguated representation as a result. Senses are
compared in terms of their semantic signatures,
denoted as function R. We consider multiple def-
initions ofR, defined later in Section 2.3.
As a part of the disambiguation procedure, we
leverage the one sense per discourse heuristic of
Yarowsky (1995); given all the word types in two
compared lexical items, each type is assigned a
single sense, even if it is used multiple times. Ad-
ditionally, if the same word type appears in both
sentences, both will always be mapped to the same
sense. Although such a sense assignment is poten-
tially incorrect, assigning both types to the same
sense results in a representation that does no worse
than a surface-level comparison.
We illustrate the alignment-based disambigua-
tion procedure using the two example sentences t1
and t2 given in Section 1. Figure 1(a) illustrates
example alignments of the first sense of manager
to the first two senses of the word types in sentence
t2 along with the similarity of the two senses?
Algorithm 1 Alignment-based Sense Disambiguation
Input: T1 and T2, the sets of word types being compared
Output: P , the set of disambiguated senses for T1
1: P ? ?
2: for each token ti ? T1
3: max sim? 0
4: best si? null
5: for each token tj ? T2
6: for each si ? Senses(ti), sj ? Senses(tj)
7: sim?R(si, sj)
8: if sim > max sim then
9: max sim = sim
10: best si = si
11: P ? P ? {best si}
12: return P
semantic signatures. For the senses of manager,
sense manager1n obtains the maximal similarity
value to boss1n among all the possible pairings of
the senses for the word types in sentence t2, and as
a result is selected as the sense labeling for man-
ager in sentence t1.3 Figure 1(b) shows the final,
maximally-similar sense alignment of the word
types in t1 and t2. The resulting alignment pro-
duces the following sets of senses:
Pt1 = {manager1n, fire4v, worker1n}
Pt2 = {employee1n, terminate4v, work3n, boss2n}
where Px denotes the corresponding set of senses
of sentence x.
2.3 Semantic Signature Similarity
Cosine Similarity. In order to compare seman-
tic signatures, we adopt the Cosine similarity mea-
sure as a baseline method. The measure is com-
puted by treating each multinomial as a vector and
then calculating the normalized dot product of the
two signatures? vectors.
3We follow Navigli (2009) and denote with wip the i-th
sense of w in WordNet with part of speech p.
1343
However, a semantic signature is, in essence,
a weighted ranking of the importance of Word-
Net senses for each lexical item. Given that the
WordNet graph has a non-uniform structure, and
also given that different lexical items may be of
different sizes, the magnitudes of the probabilities
obtained may differ significantly between the two
multinomial distributions. Therefore, for com-
puting the similarity of two signatures, we also
consider two nonparametric methods that use the
ranking of the senses, rather than their probability
values, in the multinomial.
Weighted Overlap. Our first measure provides
a nonparametric similarity by comparing the simi-
larity of the rankings for intersection of the senses
in both semantic signatures. However, we addi-
tionally weight the similarity such that differences
in the highest ranks are penalized more than differ-
ences in lower ranks. We refer to this measure as
the Weighted Overlap. Let S denote the intersec-
tion of all senses with non-zero probability in both
signatures and rji denote the rank of sense si ? S
in signature j, where rank 1 denotes the highest
rank. The sum of the two ranks r1i and r2i for a
sense is then inverted, which (1) weights higher
ranks more and (2) when summed, provides the
maximal value when a sense has the same rank in
both signatures. The unnormalized weighted over-
lap is then calculated as?|S|i=1(r1i + r2i )?1. Then,
to bound the similarity value in [0, 1], we normal-
ize the sum by its maximum value, ?|S|i=1(2i)?1,
which occurs when each sense has the same rank
in both signatures.
Top-k Jaccard. Our second measure uses the
ranking to identify the top-k senses in a signa-
ture, which are treated as the best representatives
of the conceptual associates. We hypothesize that
a specific rank ordering may be attributed to small
differences in the multinomial probabilities, which
can lower rank-based similarities when one of the
compared orderings is perturbed due to slightly
different probability values. Therefore, we con-
sider the top-k senses as an unordered set, with
equal importance in the signature. To compare two
signatures, we compute the Jaccard Index of the
two signatures? sets:
RJac(Uk, Vk) =
|Uk ? Vk|
|Uk ? Vk|
(2)
whereUk denotes the set of k senses with the high-
est probability in the semantic signature U .
Dataset MSRvid MSRpar SMTeuroparl OnWN SMTnews
Training 750 750 734 - -
Test 750 750 459 750 399
Table 1: Statistics of the provided datasets for the
SemEval-2012 Semantic Textual Similarity task.
3 Experiment 1: Textual Similarity
Measuring semantic similarity of textual items has
applications in a wide variety of NLP tasks. As
our benchmark, we selected the recent SemEval-
2012 task on Semantic Textual Similarity (STS),
which was concerned with measuring the seman-
tic similarity of sentence pairs. The task received
considerable interest by facilitating a meaningful
comparison between approaches.
3.1 Experimental Setup
Data. We follow the experimental setup used in
the STS task (Agirre et al, 2012), which provided
five test sets, two of which had accompanying
training data sets for tuning system performance.
Each sentence pair in the datasets was given a
score from 0 to 5 (low to high similarity) by hu-
man judges, with a high inter-annotator agreement
of around 0.90 when measured using the Pearson
correlation coefficient. Table 1 lists the number of
sentence pairs in training and test portions of each
dataset.
Comparison Systems. The top-ranking partic-
ipating systems in the SemEval-2012 task were
generally supervised systems utilizing a variety of
lexical resources and similarity measurement tech-
niques. We compare our results against the top
three systems of the 88 submissions: TLsim and
TLsyn, the two systems of S?aric? et al (2012), and
the UKP2 system (Ba?r et al, 2012). UKP2 utilizes
extensive resources among which are a Distribu-
tional Thesaurus computed on 10M dependency-
parsed English sentences. In addition, the sys-
tem utilizes techniques such as Explicit Semantic
Analysis (Gabrilovich and Markovitch, 2007) and
makes use of resources such as Wiktionary and
Wikipedia, a lexical substitution system based on
supervised word sense disambiguation (Biemann,
2013), and a statistical machine translation sys-
tem. The TLsim system uses the New York Times
Annotated Corpus, Wikipedia, and Google Book
Ngrams. The TLsyn system also uses Google
Book Ngrams, as well as dependency parsing and
named entity recognition.
1344
Ranking System Overall Dataset-specificALL ALLnrm Mean ALL ALLnrm Mean Mpar Mvid SMTe OnWN SMTn
1 1 1 ADW 0.866 0.871 0.711 0.694 0.887 0.555 0.706 0.604
2 3 2 UKP2 0.824 0.858 0.677 0.683 0.873 0.528 0.664 0.493
3 4 6 TLsyn 0.814 0.857 0.660 0.698 0.862 0.361 0.704 0.468
4 2 3 TLsim 0.813 0.864 0.675 0.734 0.880 0.477 0.679 0.398
Table 2: Performance of our system (ADW) and the 3 top-ranking participating systems (out of 88) in
the SemEval-2012 Semantic Textual Similarity task. Rightmost columns report the corresponding Pear-
son correlation r for individual datasets, i.e., MSRpar (Mpar), MSRvid (Mvid), SMTeuroparl (SMTe),
OnWN (OnWN) and SMTnews (SMTn). We also provide scores according to the three official evalua-
tion metrics (i.e., ALL, ALLnrm, and Mean). Rankings are also presented based on the three metrics.
System Configuration. Here we describe the
configuration of our approach, which we have
called Align, Disambiguate and Walk (ADW). The
STS task uses human similarity judgments on an
ordinal scale from 0 to 5. Therefore, in ADW we
adopted a similar approach to generating similar-
ity values to that adopted by other participating
systems, whereby a supervised system is trained
to combine multiple similarity judgments to pro-
duce a final rating consistent with the human an-
notators. We utilized the WEKA toolkit (Hall et
al., 2009) to train a Gaussian Processes regression
model for each of the three training sets (cf. Table
1). The features discussed hereafter were consid-
ered in our regression model.
Main features. We used the scores calculated
using all three of our semantic signature compar-
ison methods as individual features. Although the
Jaccard comparison is parameterized, we avoided
tuning and instead used four features for distinct
values of k: 250, 500, 1000, and 2500.
String-based features. Additionally, because
the texts often contain named entities which are
not present in WordNet, we incorporated the sim-
ilarity values produced by four string-based mea-
sures, which were used by other teams in the STS
task: (1) longest common substring which takes
into account the length of the longest overlap-
ping contiguous sequence of characters (substring)
across two strings (Gusfield, 1997), (2) longest
common subsequence which, instead, finds the
longest overlapping subsequence of two strings
(Allison and Dix, 1986), (3) Greedy String Tiling
which allows reordering in strings (Wise, 1993),
and (4) the character/word n-gram similarity pro-
posed by Barro?n-Ceden?o et al (2010).
We followed S?aric? et al (2012) and used the
models trained on the SMTeuroparl and MSRpar
datasets for testing on the SMTnews and OnWN
test sets, respectively.
3.2 STS Results
Three evaluation metrics are provided by the or-
ganizers of the SemEval-2012 STS task, all of
which are based on Pearson correlation r of human
judgments with system outputs: (1) the correla-
tion value for the concatenation of all five datasets
(ALL), (2) a correlation value obtained on a con-
catenation of the outputs, separately normalized
by least square (ALLnrm), and (3) the weighted
average of Pearson correlations across datasets
(Mean). Table 2 shows the scores obtained by
ADW for the three evaluation metrics, as well as
the Pearson correlation values obtained on each
of the five test sets (rightmost columns). We also
show the results obtained by the three top-ranking
participating systems (i.e., UKP2, TLsim, and TL-
syn). The leftmost three columns show the system
rankings according to the three metrics.
As can be seen from Table 2, our system (ADW)
outperforms all the 88 participating systems ac-
cording to all the evaluation metrics. Our sys-
tem shows a statistically significant improvement
on the SMTnews dataset, with an increase in the
Pearson correlation of over 0.10. MSRpar (MPar)
is the only dataset in which TLsim (S?aric? et al,
2012) achieves a higher correlation with human
judgments. Named entity features used by the TL-
sim system could be the reason for its better per-
formance on the MSRpar dataset, which contains
a large number of named entities.
3.3 Similarity Measure Analysis
To gain more insight into the impact of our
alignment-based disambiguation approach, we
carried out a 10-fold cross-validation on the three
training datasets (cf. Table 1) using the systems
described hereafter.
ADW-MF. To build this system, we utilized our
main features only; i.e., we did not make use of
additional string-based features.
1345
DW. Similarly to ADW-MF, this system utilized
the main features only. In DW, however, we re-
placed our alignment-based disambiguation phase
with a random walk-based WSD system that dis-
ambiguated the sentences separately, without per-
forming any alignment. As our WSD system,
we used UKB, a state-of-the-art knowledge-based
WSD system that is based on the same topic-
sensitive PageRank algorithm used by our ap-
proach. UKB initializes the algorithm from all
senses of the words in the context of a word to
be disambiguated. It then picks the most relevant
sense of the word according to the resulting prob-
ability vector. As the lexical knowledge base of
UKB, we used the same semantic network as that
utilized by our approach for calculating semantic
signatures.
Table 3 lists the performance values of the two
above-mentioned systems on the three training
sets in terms of Pearson correlation. In addition,
we present in the table correlation scores for four
other similarity measures reported by Ba?r et al
(2012):
? Pairwise Word Similarity that comprises of
a set of WordNet-based similarity measures
proposed by Resnik (1995), Jiang and Con-
rath (1997), and Lin (1998b). The aggre-
gation strategy proposed by Corley and Mi-
halcea (2005) has been utilized for extend-
ing these word-to-word similarity measures
for calculating text-to-text similarities.
? Explicit Semantic Analysis (Gabrilovich
and Markovitch, 2007) where the high-
dimensional vectors are obtained on Word-
Net, Wikipedia and Wiktionary.
? Distributional Thesaurus where a similarity
score is computed similarly to that of Lin
(1998a) using a distributional thesaurus ob-
tained from a 10M dependency-parsed sen-
tences of English newswire.
? Character n-grams which were also used as
one of our additional features.
As can be seen from Table 3, our alignment-
based disambiguation approach (ADW-MF) is
better suited to the task than a conventional WSD
approach (DW). Another interesting point is the
high scores achieved by the Character n-grams
Similarity measure DatasetMpar Mvid SMTe
DW 0.448 0.820 0.660
ADW-MF 0.485 0.842 0.721
Explicit Semantic Analysis 0.427 0.781 0.619
Pairwise Word Similarity 0.564 0.835 0.527
Distributional Thesaurus 0.494 0.481 0.365
Character n-grams 0.658 0.771 0.554
Table 3: Performance of our main-feature sys-
tem with conventional WSD (DW) and with the
alignment-based disambiguation approach (ADW-
MF) vs. four other similarity measures, using 10-
fold cross validation on the training datasets MSR-
par (Mpar), MSRvid (Mvid), and SMTeuroparl
(SMTe).
measure. This confirms that string-based meth-
ods are strong baselines for semantic textual sim-
ilarity. Except for the MSRpar (Mpar) dataset,
our system (ADW-MF) outperforms all other sim-
ilarity measures. The scores obtained by Explicit
Semantic Analysis and Distributional Thesaurus
are not competitive on any dataset. On the other
hand, Pairwise Word Similarity achieves a high
performance on MSRpar and MSRvid datasets,
but performs surprisingly low on the SMTeuroparl
dataset.
4 Experiment 2: Word Similarity
We now proceed from the sentence level to the
word level. Word similarity has been a key prob-
lem for lexical semantics, with significant efforts
being made by approaches in distributional se-
mantics to accurately identify synonymous words
(Turney and Pantel, 2010). Different evaluation
methods exist in the literature for evaluating the
performance of a word-level semantic similarity
measure; we adopted two well-established bench-
marks: synonym recognition and correlating word
similarity judgments with those from human an-
notators.
For synonym recognition, we used the TOEFL
dataset created by Landauer and Dumais (1997).
The dataset consists of 80 multiple-choice syn-
onym questions from the TOEFL test; a word is
paired with four options, one of which is a valid
synonym. Test takers with English as a second
language averaged 64.5% correct. Despite multi-
ple approaches, only recently has the test been an-
swered perfectly (Bullinaria and Levy, 2012), un-
derscoring the challenge of synonym recognition.
1346
Approach Accuracy
PPMIC (Bullinaria and Levy, 2007) 85.00%
GLSA (Matveeva et al, 2005) 86.25%
LSA (Rapp, 2003) 92.50%
ADWJac 93.75?2.5%
ADWWO 95.00%
ADWCos 96.25%
PR (Turney et al, 2003) 97.50%
PCCP (Bullinaria and Levy, 2012) 100.00%
Table 4: Accuracy on the 80-question TOEFL
Synonym test. ADWJac, ADWWO, and ADWCos
correspond to results with the Jaccard, Weighted
Overlap and Cosine signature comparison mea-
sures, respectively.
For the similarity judgment evaluation, we
used as benchmark the RG-65 dataset created by
Rubenstein and Goodenough (1965). The dataset
contains 65 word pairs judged by 51 human sub-
jects on a scale of 0 to 4 according to their seman-
tic similarity. Ideally, a measure?s similarity judg-
ments are expected to be highly correlated with
those of humans. To be consistent with the previ-
ous literature (Hughes and Ramage, 2007; Agirre
et al, 2009), we used Spearman?s rank correlation
in our experiment.
4.1 Experimental Setup
Our alignment-based sense disambiguation trans-
forms the task of comparing individual words
into that of calculating the similarity of the best-
matching sense pair across the two words. As
there is no training data we do not optimize the k
value for computing signature similarity with the
Jaccard index; instead, we report, for the synonym
recognition and the similarity judgment evalua-
tions, the respective range of accuracies and the
average correlation obtained upon using five val-
ues of k randomly selected in the range [50, 2500]:
678, 1412, 1692, 2358, 2387.
4.2 Word Similarity Results: TOEFL dataset
Table 4 lists the accuracy performance of the sys-
tem in comparison to the existing state of the
art on the TOEFL test. ADWWO, ADWCos,
and ADWJac correspond to our approach when
Weighted Overlap, Cosine, and Jaccard signa-
ture comparison measures are used, respectively.
Despite not being tuned for the task, our model
achieves near-perfect performance, answering all
but three questions correctly with the Cosine mea-
sure. Among the top-performing approaches, only
Word Synonym choices (correct in bold)
fanciful familiar apparent? imaginative? logical
verbal oral? overt fitting verbose?
resolved settled? forgotten? publicized examined
percentage volume sample proportion profit??
figure list solve? divide? express
highlight alter? imitate accentuate? restore
Table 5: Questions answered incorrectly by our
approach. Symbols ? and ? correspond to the
choices of our approach with the Weighted Over-
lap and Cosine signature comparisons respec-
tively. We do not include the mistakes made when
the Jaccard measure was used as they vary with
the k value.
that of Rapp (2003) uses word senses, an approach
that is outperformed by our method.
The errors produced by our system were largely
the result of sense locality in the WordNet net-
work. Table 5 highlights the incorrect responses.
The synonym mistakes reveal cases where senses
of the two words are close in WordNet, indicating
some relatedness. For example, percentage may
be interpreted colloquially as monetary value (e.g.,
?give me my percentage?) and elicits the synonym
of profit in the economic domain, which ADW in-
correctly selects as a synonym.
4.3 Word Similarity Results: RG-65 dataset
Table 6 shows the Spearman?s ? rank correlation
coefficients with human judgments on the RG-65
dataset. As can be seen from the Table, our ap-
proach with the Weighted Overlap signature com-
parison improves over the similar approach of
Hughes and Ramage (2007) which, however, does
not involve the disambiguation step and considers
a word as a whole unit as represented by the set of
its senses.
5 Experiment 3: Sense Similarity
WordNet is known to be a fine-grained sense in-
ventory with many related word senses (Palmer et
al., 2007). Accordingly, multiple approaches have
attempted to identify highly similar senses in or-
der to produce a coarse-grained sense inventory.
We adopt this task as a way of evaluating our sim-
ilarity measure at the sense level.
5.1 Coarse-graining Background
Earlier work on reducing the polysemy of sense
inventories has considered WordNet-based sense
relatedness measures (Mihalcea and Moldovan,
2001) and corpus-based vector representations of
1347
Approach Correlation
ADWCos 0.825
Agirre et al (2009) 0.830
Hughes and Ramage (2007) 0.838
Zesch et al (2008) 0.840
ADWJac 0.841
ADWWO 0.868
Table 6: Spearman?s ? correlation coefficients
with human judgments on the RG-65 dataset.
ADWJac, ADWWO, and ADWCos correspond to
results with the Jaccard, Weighted Overlap and
Cosine signature comparison measures respec-
tively.
word senses (Agirre and Lopez, 2003; McCarthy,
2006). Navigli (2006) proposed an automatic ap-
proach for mapping WordNet senses to the coarse-
grained sense distinctions of the Oxford Dictio-
nary of English (ODE). The approach leverages
semantic similarities in gloss definitions and the
hierarchical relations between senses in the ODE
to cluster WordNet senses. As current state of
the art, Snow et al (2007) developed a super-
vised SVM classifier that utilized, as its features,
several earlier sense relatedness techniques such
as those implemented in the WordNet::Similarity
package (Pedersen et al, 2004). The classifier
also made use of resources such as topic signatures
data (Agirre and de Lacalle, 2004), the WordNet
domain dataset (Magnini and Cavaglia`, 2000), and
the mappings of WordNet senses to ODE senses
produced by Navigli (2006).
5.2 Experimental Setup
We benchmark the accuracy of our similarity mea-
sure in grouping word senses against those of Nav-
igli (2006) and Snow et al (2007) on two datasets
of manually-labeled sense groupings of WordNet
senses: (1) sense groupings provided as a part of
the Senseval-2 English Lexical Sample WSD task
(Kilgarriff, 2001) which includes nouns, verbs and
adjectives; (2) sense groupings included in the
OntoNotes project4 (Hovy et al, 2006) for nouns
and verbs. Following the evaluation methodology
of Snow et al (2007), we combine the Senseval-2
and OntoNotes datasets into a third dataset.
Snow et al (2007) considered sense grouping as
a binary classification task whereby for each word
every possible pairing of senses has to be classified
4Sense groupings belong to a pre-version 1.0: http://
cemantix.org/download/sense/ontonotes-sense-groups.tar.gz
Onto SE-2 Onto + SE-2
Method Noun Verb Noun Verb Adj Noun Verb
RCos 0.406 0.522 0.450 0.465 0.484 0.441 0.485
RWO 0.421 0.544 0.483 0.482 0.531 0.470 0.503
RJac 0.418 0.531 0.478 0.473 0.501 0.465 0.493SVM 0.370 0.455 NA NA 0.473 0.423 0.432
ODE 0.218 0.396 NA NA 0.371 0.331 0.288
Table 7: F-score sense merging evaluation on
three hand-labeled datasets: OntoNotes (Onto),
Senseval-2 (SE-2), and combined (Onto+SE-2).
Results are reported for all three of our signature
comparison measures and also for two previous
works (last two rows).
as either merged or not-merged. We constructed
a simple threshold-based classifier to perform the
same binary classification. To this end, we cal-
culated the semantic similarity of each sense pair
and then used a threshold value t to classify the
pair as merged if similarity ? t and not-merged
otherwise. We sampled out 10% of the dataset for
tuning the value of t, thus adapting our classifier
to the fine granularity of the dataset. We used the
same held-out instances to perform a tuning of the
k value used for Jaccard index, over the same val-
ues of k as in Experiment 1 (cf. Section 3).
5.3 Sense Merging Results
For a binary classification task, we can directly
calculate precision, recall and F-score by con-
structing a contingency table. We show in Ta-
ble 7 the F-score performance of our classifier as
obtained by an averaged 10-fold cross-validation.
Results are presented for all three of the mea-
sures of semantic signature comparison and for
the three datasets: OntoNotes, Senseval-2, and
the two combined. In addition, we show in Ta-
ble 7 the F-score results provided by Snow et al
(2007) for their SVM-based system and for the
mapping-based approach of Navigli (2006), de-
noted by ODE.
Table 7 shows that our methodology yields im-
provements over previous work on both datasets
and for all parts of speech, irrespective of
the semantic signature comparison method used.
Among the three methods, Weighted Overlap
achieves the best performance, which demon-
strates that our transformation of semantic signa-
tures into ordered lists of concepts and calculating
similarity by rank comparison has been helpful.
1348
6 Related Work
Due to the wide applicability of semantic similar-
ity, significant efforts have been made at different
lexical levels. Early work on document-level sim-
ilarity was driven by information retrieval. Vector
space methods provided initial successes (Salton
et al, 1975), but often suffer from data spar-
sity when using small documents, or when doc-
uments use different word types, as in the case
of paraphrases. Later efforts such as LSI (Deer-
wester et al, 1990), PLSA (Hofmann, 2001) and
Topic Models (Blei et al, 2003; Steyvers and Grif-
fiths, 2007) overcame these sparsity issues using
dimensionality reduction techniques or modeling
the document using latent variables. However,
such methods were still most suitable for compar-
ing longer texts. Complementary approaches have
been developed specifically for comparing shorter
texts, such as those used in the SemEval-2012
STS task (Agirre et al, 2012). Most similar to
our approach are the methods of Islam and Inkpen
(2008) and Corley and Mihalcea (2005), who per-
formed a word-to-word similarity alignment; how-
ever, they did not operate at the sense level. Ram-
age et al (2009) used a similar semantic represen-
tation of short texts from random walks on Word-
Net, which was applied to paraphrase recognition
and textual entailment. However, unlike our ap-
proach, their method does not perform sense dis-
ambiguation prior to building the representation
and therefore potentially suffers from ambiguity.
A significant amount of effort has also been put
into measuring similarity at the word level, fre-
quently by approaches that use distributional se-
mantics (Turney and Pantel, 2010). These meth-
ods use contextual features to represent semantics
at the word level, whereas our approach represents
word semantics at the sense level. Most similar to
our approach are those of Agirre et al (2009) and
Hughes and Ramage (2007), which represent word
meaning as the multinomials produced from ran-
dom walks on the WordNet graph. However, un-
like our approach, neither of these disambiguates
the two words being compared, which potentially
conflates the meanings and lowers the similarity
judgment.
Measures of sense relatedness have frequently
leveraged the structural properties of WordNet
(e.g., path lengths) to compare senses. Budanit-
sky and Hirst (2006) provided a survey of such
WordNet-based measures. The main drawback
with these approaches lies in the WordNet struc-
ture itself, where frequently two semantically sim-
ilar senses are distant in the WordNet hierar-
chy. Possible solutions include relying on wider-
coverage networks such as WikiNet (Nastase and
Strube, 2013) or multilingual ones such as Babel-
Net (Navigli and Ponzetto, 2012b). Fewer works
have focused on measuring the similarity ? as op-
posed to relatedness ? between senses. The topic
signatures method of Agirre and Lopez (2003)
represents each sense as a vector over corpus-
derived features in order to build comparable sense
representations. However, topic signatures often
produce lower quality representations due to spar-
sity in the local structure of WordNet, especially
for rare senses. In contrast, the random walk
used in our approach provides a denser, and thus
more comparable, representation for all WordNet
senses.
7 Conclusions
This paper presents a unified approach for comput-
ing semantic similarity at multiple lexical levels,
from word senses to texts. Our method leverages
a common probabilistic representation at the sense
level for all types of linguistic data. We demon-
strate that our semantic representation achieves
state-of-the-art performance in three experiments
using semantic similarity at different lexical levels
(i.e., sense, word, and text), surpassing the per-
formance of previous similarity measures that are
often specifically targeted for each level.
In future work, we plan to explore the impact of
the sense inventory-based network used in our se-
mantic signatures. Specifically, we plan to investi-
gate higher coverage inventories such as BabelNet
(Navigli and Ponzetto, 2012a), which will handle
texts with named entities and rare senses that are
not in WordNet, and will also enable cross-lingual
semantic similarity. Second, we plan to evaluate
our method on larger units of text and formalize
comparison methods between different lexical lev-
els.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We would like to thank Sameer S. Pradhan
for providing us with an earlier version of the
OntoNotes dataset.
1349
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly
available topic signatures for all WordNet nominal senses.
In Proceedings of LREC, pages 1123?1126, Lisbon, Por-
tugal.
Eneko Agirre and Oier Lopez. 2003. Clustering WordNet
word senses. In Proceedings of RANLP, pages 121?130,
Borovets, Bulgaria.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-
ova, Marius Pas?ca, and Aitor Soroa. 2009. A study
on similarity and relatedness using distributional and
WordNet-based approaches. In Proceedings of NAACL,
pages 19?27, Boulder, Colorado.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-
Agirre. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proceedings of SemEval-2012, pages
385?393, Montreal, Canada.
Lloyd Allison and Trevor I. Dix. 1986. A bit-string longest-
common-subsequence algorithm. Information Processing
Letters, 23(6):305?310.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual similar-
ity by combining multiple content similarity measures. In
Proceedings of SemEval-2012, pages 435?440, Montreal,
Canada.
Alberto Barro?n-Ceden?o, Paolo Rosso, Eneko Agirre, and
Gorka Labaka. 2010. Plagiarism detection across distant
language pairs. In Proceedings of COLING, pages 37?45,
Beijing, China.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012.
Learning entailment relations by global graph structure
optimization. Computational Linguistics, 38(1):73?111.
Chris Biemann. 2013. Creating a system for lexical sub-
stitutions from scratch using crowdsourcing. Language
Resources and Evaluation, 47(1):97?122.
Or Biran, Samuel Brody, and Noe?mie Elhadad. 2011.
Putting it simply: a context-aware approach to lexical sim-
plification. In Proceedings of ACL, pages 496?501, Port-
land, Oregon.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent Dirichlet Allocation. The Journal of Machine
Learning Research, 3:993?1022.
Alexander Budanitsky and Graeme Hirst. 2006. Evaluat-
ing WordNet-based measures of Lexical Semantic Relat-
edness. Computational Linguistics, 32(1):13?47.
John A. Bullinaria and Joseph. P. Levy. 2007. Extracting
semantic representations from word co-occurrence statis-
tics: A computational study. Behavior Research Methods,
(3):510.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting
semantic representations from word co-occurrence statis-
tics: stop-lists, stemming, and SVD. Behavior Research
Methods, 44:890?907.
Courtney Corley and Rada Mihalcea. 2005. Measuring the
semantic similarity of texts. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment, pages 13?18, Ann Arbor, Michigan.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,
George W. Furnas, and Richard A. Harshman. 1990. In-
dexing by Latent Semantic Analysis. Journal of American
Society for Information Science, 41(6):391?407.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and diversifying Web search results with graph-based
Word Sense Induction. Computational Linguistics, 39(3).
Tamer Elsayed, Jimmy Lin, and Douglas W. Oard. 2008.
Pairwise document similarity in large collections with
MapReduce. In Proceedings of ACL-HLT, pages 265?
268, Columbus, Ohio.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Hagen Fu?rstenau and Mirella Lapata. 2012. Semi-supervised
Semantic Role Labeling via structural alignment. Compu-
tational Linguistics, 38(1):135?171.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of IJCAI, pages 1606?
1611, Hyderabad, India.
Oren Glickman and Ido Dagan. 2003. Acquiring lexi-
cal paraphrases from a single corpus. In Proceedings of
RANLP, pages 81?90, Borovets, Bulgaria.
Dan Gusfield. 1997. Algorithms on strings, trees, and se-
quences: computer science and computational biology.
Cambridge University Press.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda
Koren. 2012. Large-scale learning of word relatedness
with constraints. In Proceedings of KDD, pages 1406?
1414, Beijing, China.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The WEKA data mining software: an update. ACM
SIGKDD Explorations Newsletter, 11(1):10?18.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank. In
Proceedings of WWW, pages 517?526, Hawaii, USA.
Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis,
Euripides GM Petrakis, and Evangelos Milios. 2006.
Information retrieval by semantic similarity. Interna-
tional Journal on Semantic Web and Information Systems,
2(3):55?73.
Thomas Hofmann. 2001. Unsupervised Learning by Prob-
abilistic Latent Semantic Analysis. Machine Learning,
42(1):177?196.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The
90% solution. In Proceedings of NAACL, pages 57?60,
NY, USA.
Thad Hughes and Daniel Ramage. 2007. Lexical semantic
relatedness with random graph walks. In Proceedings of
EMNLP-CoNLL, pages 581?589, Prague, Czech Repub-
lic.
Aminul Islam and Diana Inkpen. 2008. Semantic text sim-
ilarity using corpus-based word similarity and string sim-
ilarity. ACM Transactions on Knowledge Discovery from
Data, 2(2):10:1?10:25.
Jay J. Jiang and David W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy. In
Proceedings of ROCLING X, pages 19?30, Taiwan.
1350
Adam Kilgarriff. 2001. English lexical sample task descrip-
tion. In Proceedings of Senseval, pages 17?20, Toulouse,
France.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to Plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review; Psychological Review, 104(2):211.
Dekang Lin. 1998a. Automatic retrieval and clustering of
similar words. In Proceedings of COLING, pages 768?
774, Montreal, Quebec, Canada.
Dekang Lin. 1998b. An information-theoretic definition of
similarity. In Proceedings of ICML, pages 296?304, San
Francisco, CA.
Bernardo Magnini and Gabriela Cavaglia`. 2000. Integrat-
ing subject field codes into WordNet. In Proceedings of
LREC, pages 1413?1418, Athens, Greece.
Ana G. Maguitman, Filippo Menczer, Heather Roinestad, and
Alessandro Vespignani. 2005. Algorithmic detection of
semantic similarity. In Proceedings of WWW, pages 107?
116, Chiba, Japan.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christiaan Royer. 2005. Terms representation with gener-
alized latent semantic analysis. In Proceedings of RANLP,
Borovets, Bulgaria.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Evalu-
ation, 43(2):139?159.
Diana McCarthy. 2006. Relating WordNet senses for word
sense disambiguation. In Proceedings of the Workshop on
Making Sense of Sense at EACL-06, pages 17?24, Trento,
Italy.
Rada Mihalcea and Dan Moldovan. 2001. Automatic gen-
eration of a coarse grained WordNet. In Proceedings
of NAACL Workshop on WordNet and Other Lexical Re-
sources, Pittsburgh, USA.
Vivi Nastase and Michael Strube. 2013. Transforming
Wikipedia into a large scale multilingual concept network.
Artificial Intelligence, 194:62?85.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belNet: The automatic construction, evaluation and appli-
cation of a wide-coverage multilingual semantic network.
Artificial Intelligence, 193:217?250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b. Babel-
Relate! a joint multilingual approach to computing seman-
tic relatedness. In Proceedings of AAAI, pages 108?114,
Toronto, Canada.
Roberto Navigli. 2006. Meaningful clustering of senses
helps boost Word Sense Disambiguation performance. In
Proceedings of COLING-ACL, pages 105?112, Sydney,
Australia.
Roberto Navigli. 2009. Word Sense Disambiguation: A sur-
vey. ACM Computing Surveys, 41(2):1?69.
Martha Palmer, Hoa Dang, and Christiane Fellbaum. 2007.
Making fine-grained and coarse-grained sense distinc-
tions, both manually and automatically. Natural Lan-
guage Engineering, 13(2):137?163.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity - measuring the relatedness of
concepts. In Proceedings of AAAI, pages 144?152, San
Jose, CA.
Mohammad Taher Pilehvar and Roberto Navigli. 2013.
Paving the way to a large-scale pseudosense-annotated
dataset. In Proceedings of NAACL-HLT, pages 1100?
1109, Atlanta, USA.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and
Shaul Markovitch. 2011. A word at a time: comput-
ing word relatedness using temporal semantic analysis. In
Proceedings of WWW, pages 337?346, Hyderabad, India.
Daniel Ramage, Anna N. Rafferty, and Christopher D. Man-
ning. 2009. Random walks for text semantic similarity. In
Proceedings of the 2009 Workshop on Graph-based Meth-
ods for Natural Language Processing, pages 23?31, Sun-
tec, Singapore.
Reinhard Rapp. 2003. Word sense discovery based on sense
descriptor dissimilarity. In Proceedings of the Ninth Ma-
chine Translation Summit, pages 315?322, New Orleans,
LA.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings of
IJCAI, pages 448?453, Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Gerard Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communications of
the ACM, 18(11):613?620.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and Andrew Y.
Ng. 2007. Learning to merge word senses. In EMNLP-
CoNLL, pages 1005?1014, Prague, Czech Republic.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424?440.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-factoid
questions from Web collections. Computational Linguis-
tics, 37(2):351?383.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal of
Artificial Intelligence Research, 37:141?188.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and
Victor Shnayder. 2003. Combining independent modules
to solve multiple-choice synonym and analogy problems.
In Proceedings of RANLP, pages 482?489, Borovets, Bul-
garia.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder, and
Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems for
measuring semantic text similarity. In Proceedings of
SemEval-2012, pages 441?448, Montreal, Canada.
Michael J. Wise. 1993. String similarity via greedy string
tiling and running Karp-Rabin matching. In Department
of Computer Science Technical Report, Sydney.
David Yarowsky. 1995. Unsupervised Word Sense Disam-
biguation rivaling supervised methods. In Proceedings of
ACL, pages 189?196, Cambridge, Massachusetts.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych. 2008.
Using Wiktionary for computing semantic relatedness. In
Proceedings of AAAI, pages 861?866, Chicago, Illinois.
1351
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1294?1304,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Validating and Extending Semantic Knowledge Bases
using Video Games with a Purpose
Daniele Vannella, David Jurgens, Daniele Scarfini, Domenico Toscani and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
surname@di.uniroma1.it
Abstract
Large-scale knowledge bases are impor-
tant assets in NLP. Frequently, such re-
sources are constructed through automatic
mergers of complementary resources, such
as WordNet and Wikipedia. However,
manually validating these resources is pro-
hibitively expensive, even when using
methods such as crowdsourcing. We pro-
pose a cost-effective method of validat-
ing and extending knowledge bases using
video games with a purpose. Two video
games were created to validate concept-
concept and concept-image relations. In
experiments comparing with crowdsourc-
ing, we show that video game-based vali-
dation consistently leads to higher-quality
annotations, even when players are not
compensated.
1 Introduction
Large-scale knowledge bases are an essential
component of many approaches in Natural Lan-
guage Processing (NLP). Semantic knowledge
bases such as WordNet (Fellbaum, 1998), YAGO
(Suchanek et al, 2007), and BabelNet (Navigli
and Ponzetto, 2010) provide ontological struc-
ture that enables a wide range of tasks, such as
measuring semantic relatedness (Budanitsky and
Hirst, 2006) and similarity (Pilehvar et al, 2013),
paraphrasing (Kauchak and Barzilay, 2006), and
word sense disambiguation (Navigli and Ponzetto,
2012; Moro et al, 2014). Furthermore, such
knowledge bases are essential for building unsu-
pervised algorithms when training data is sparse
or unavailable. However, constructing and updat-
ing semantic knowledge bases is often limited by
the significant time and human resources required.
Recent approaches have attempted to build or
extend these knowledge bases automatically. For
example, Snow et al (2006) and Navigli (2005)
extend WordNet using distributional or structural
features to identify novel semantic connections
between concepts. The recent advent of large
semi-structured resources has enabled the creation
of new semantic knowledge bases (Medelyan et
al., 2009; Hovy et al, 2013) through automati-
cally merging WordNet and Wikipedia (Suchanek
et al, 2007; Navigli and Ponzetto, 2010; Nie-
mann and Gurevych, 2011). While these auto-
matic approaches offer the scale needed for open-
domain applications, the automatic processes of-
ten introduce errors, which can prove detrimental
to downstream applications. To overcome issues
from fully-automatic construction methods, sev-
eral works have proposed validating or extending
knowledge bases using crowdsourcing (Biemann
and Nygaard, 2010; Eom et al, 2012; Sarasua et
al., 2012). However, these methods, too, are lim-
ited by the resources required for acquiring large
numbers of responses.
In this paper, we propose validating and extend-
ing semantic knowledge bases using video games
with a purpose. Here, the annotation tasks are
transformed into elements of a video game where
players accomplish their jobs by virtue of playing
the game, rather than by performing a more tradi-
tional annotation task. While prior efforts in NLP
have incorporated games for performing annota-
tion and validation (Siorpaes and Hepp, 2008b;
Herda?gdelen and Baroni, 2012; Poesio et al,
2013), these games have largely been text-based,
adding game-like features such as high-scores on
top of an existing annotation task. In contrast,
we introduce two video games with graphical 2D
gameplay that is similar to what game players are
familiar with. The fun nature of the games pro-
vides an intrinsic motivation for players to keep
playing, which can increase the quality of their
work and lower the cost per annotation.
Our work provides the following three contribu-
tions. First, we demonstrate effective video game-
based methods for both validating and extending
1294
semantic networks, using two games that operate
on complementary sources of information: seman-
tic relations and sense-image mappings. In con-
trast to previous work, the annotation quality is
determined in a fully automatic way. Second, we
demonstrate that converting games with a purpose
into more traditional video games creates an in-
creased player incentive such that players annotate
for free, thereby significantly lowering annotation
costs below that of crowdsourcing. Third, for both
games, we show that games produce better quality
annotations than crowdsourcing.
2 Related Work
Multiple works have proposed linguistic
annotation-based games with a purpose for
tasks such as anaphora resolution (Hladk?a et
al., 2009; Poesio et al, 2013), paraphrasing
(Chklovski and Gil, 2005), term associations
(Artignan et al, 2009; Lafourcade and Joubert,
2010), query expansion (Simko et al, 2011), and
word sense disambiguation (Chklovski and Mi-
halcea, 2002; Seemakurty et al, 2010; Venhuizen
et al, 2013). Notably, all of these linguistic games
focus on users interacting with text, in contrast
to other highly successful games with a purpose
in other domains, such as Foldit (Cooper et al,
2010), in which players fold protein sequences,
and the ESP game (von Ahn and Dabbish, 2004),
where players label images with words.
Most similar to our work are games that create
or validate common sense knowledge. Two games
with a purpose have incorporated video game-
like mechanics for annotation. First, Herda?gdelen
and Baroni (2012) validate automatically acquired
common sense relations using a slot machine
game where players must identify valid relations
and arguments from randomly aligned data within
a time limit. Although the validation is embedded
in a game-like setting, players are limited to one
action (pulling the lever) unlike our games, which
feature a variety of actions and rich gameplay ex-
perience to keep players interested longer. Sec-
ond, Kuo et al (2009) describe a pet-raising game
where players must answer common sense ques-
tions in order to obtain pet food. While their game
is among the most video game-like, the annotation
task is a chore the player must perform in order to
return to the game, rather than an integrated, fun
part of the game?s objectives, which potentially
decreases motivation for answering correctly.
Several works have proposed adapting existing
word-based board game designs to create or val-
idate common sense knowledge. von Ahn et al
(2006) generate common sense facts by using a
game similar to Taboo
TM
, where one player must
list facts about a computer-selected lemma and a
second player must guess the original lemma hav-
ing seen only the facts. Similarly, Vickrey et al
(2008) gather free associations to a target word
with the constraint, similar to Taboo
TM
, where
players cannot enter a small set of banned words.
Vickrey et al (2008) also present two games simi-
lar to the Scattergories
TM
, where players are given
a category and then must list things in that cate-
gory. The two variants differ in the constraints im-
posed on the players, such as beginning all items
with a specific letter. For all three games, two
players play the same game under time limits and
then are rewarded if their answers match.
Last, three two-player games have focused
on validating and extending knowledge bases.
Rzeniewicz and Szyma?nski (2013) extend Word-
Net with common-sense knowledge using a 20
Questions-like game. In a rapid-play style game,
OntoPronto attempts to classify Wikipedia pages
as either categories or individuals (Siorpaes and
Hepp, 2008a). SpotTheLink uses a similar rapid
question format to have players align the DBpedia
and PROTON ontologies by agreeing on the dis-
tinctions between classes (Thaler et al, 2011).
Unlike dynamic gaming elements common in
our video games, the above games are all focused
on interacting with textual items. Another major
limitation is their need for always having two play-
ers, which requires them to sustain enough inter-
est to always maintain an active pool of players.
While the computer can potentially act as a second
player, such a simulated player is often limited to
using preexisting knowledge or responses, which
makes it difficult to validate new types of entities
or create novel answers. In contrast, we drop this
requirement thanks to a new strategy for assign-
ing confidence scores to the annotations based on
negative associations.
3 Video Game with a Purpose Design
To create video games, our development process
focused on a common design philosophy and a
common data set.
3.1 Design Objectives
Three design objectives were used to develop the
video games. First, the annotation task should be
a central and natural action with familiar video
game mechanics. That is, the annotation should
1295
be supplied by common actions such as collecting
items, puzzles, or destroying objects, rather than
through extrinsic tasks that players must complete
in order to return to the game. This design has
the benefits of (1) growing the annotator pool with
video games players, and (2) potentially increas-
ing annotator enjoyment.
Second, the game should be playable by a single
player, with reinforcement for correct game play
coming from gold standard examples.
1
We note
that gold standard examples may come from both
true positive and true negative items.
Third, the game design should be sufficiently
general to annotate a variety of linguistic phenom-
ena, such that only the game data need be changed
to accomplish a different annotation task. While
some complex linguistic annotation tasks such as
preposition attachment may be difficult to inte-
grate directly into gameplay, many simpler but still
necessary annotation tasks such as word and im-
age associations can be easily modeled with tradi-
tional video game mechanics.
3.2 Annotation Setup
Tasks We focused on two annotation tasks: (1)
validating associations between two concepts, and
(2) validating associations between a concept and
an image. For each task we developed a video
game with a purpose that integrates the task within
the game, as illustrated in Sections 4 and 5.
Knowledge base As the reference knowledge
base, we chose BabelNet
2
(Navigli and Ponzetto,
2010), a large-scale multilingual semantic ontol-
ogy created by automatically merging WordNet
with other collaboratively-constructed resources
such as Wikipedia and OmegaWiki. BabelNet
data offers two necessary features for generat-
ing the games? datasets. First, by connecting
WordNet synsets to Wikipedia pages, most synsets
are associated with a set of pictures; while often
noisy, these pictures sometimes illustrate the tar-
get concept and are an ideal case for validation.
Second, BabelNet contains the semantic relations
from both WordNet and hyperlinks in Wikipedia;
these relations are again an ideal case of valida-
tion, as not all hyperlinks connect semantically-
related pages in Wikipedia. Last, we stress that
while our games use BabelNet data, they could
easily validate or extend other knowledge bases
such as YAGO (Suchanek et al, 2007) as well.
1
This design is in contrast to two-player games where mu-
tual agreement reinforces correct behavior.
2
http://babelnet.org
Data We created a common set of concepts, C,
used in both games, containing sixty synsets se-
lected from all BabelNet synsets with at least fifty
associated images. Using the same set of synsets,
separate datasets were created for the two valida-
tion tasks. In each dataset, a concept c ? C is
associated with two sets: a set V
c
containing items
to validate, and a setN
c
with examples of true neg-
ative items (i.e., items where the relation to c does
not hold). We use the notation V and N when re-
ferring to the to-validate and true negative sets for
all concepts in a dataset, respectively.
For the concept-concept dataset, V
c
is the union
of V
B
c
, which contains the lemmas of all synsets
incident to c in BabelNet, and V
n
c
, which con-
tains novel lemmas derived from statistical asso-
ciations. Specifically, novel lemmas were selected
by computing the ?
2
statistic for co-occurrences
between the lemmas of c and all other part of
speech-tagged lemmas in Wikipedia. The 30 lem-
mas with the highest ?
2
are included in V
c
. To
enable concept-to-concept annotations, we disam-
biguate novel lemmas using a simple heuristic
based on link co-occurrence count (Navigli and
Ponzetto, 2012). Each set V
c
contains 77.6 lem-
mas on average.
For the concept-image data, V
c
is the union of
V
B
c
, which contains all images associated with c in
BabelNet, and V
n
c
, which contains web-gathered
images using a lemma of c as the query. Web-
gathered images were retrieved using Yahoo! Boss
image search and the first result set (35 images)
was added to V
c
. Each set V
c
contains 77.0 images
on average.
For both datasets, each negative set N
c
is con-
structed as ?
c
?
?C\{c}
V
B
c
?
, i.e., from the items re-
lated in BabelNet to all other concepts in C. By
constructingN
c
directly from the knowledge base,
play actions may be validated based on recogni-
tion of true negatives, removing the heavy burden
for ever manually creating a gold standard test set.
Annotation Aggregation In each game, an item
is annotated when players make a binary choice as
to whether the item?s relation is true (e.g., whether
an image is related to a concept). To produce a
final annotation, a rating of p ? n is computed,
where p and n denote the number of times players
have marked the item?s relation as true or false, re-
spectively. Items with a positive rating after aggre-
gating are marked as true examples of the relation
and false otherwise.
1296
(a) The passphrase shown at the start (b) Main gameplay screen with a close-up of a player?s interaction with two humans
Figure 1: Screenshots of the key elements of Infection
4 Game 1: Infection
The first game, Infection, validates the concept-
concept relation dataset.
Design Infection is designed as a top-down
shooter game in the style of Commando. Infection
features the classic game premise that a virus has
partially infected humanity, turning people into
zombies. The player?s responsibility is to stop
zombies from reaching the city and rescue humans
that are fleeing to the city. Both zombies and hu-
mans appear at the top of the screen, advance to
the bottom and, upon reaching it, enter the city.
In the game, some humans are infected, but
have not yet become zombies; these infected hu-
mans must be stopped before reaching the city.
Because infected and uninfected humans look
identical, the player uses a passphrase call-and-
response mechanism to distinguish between the
two. Each level features a randomly-chosen
passphrase that the player?s character shouts. Un-
infected humans are expected to respond with a
word or phrase related to the passphrase; in con-
trast, infected humans have become confused due
to the infection and will say something completely
unrelated in an attempt to sneak past. When an in-
fected human reaches the city, the city?s total in-
fection level increases; should the infection level
increase beyond a certain threshold, the player
fails the stage and must replay it to advance the
game. Furthermore, if any time after ten humans
have been seen, the player has killed more than
80% of the uninfected humans, the player?s gun is
taken by the survivors and she loses the stage.
Figure 1a shows instructions for the passphrase
?medicine.? In the corresponding gameplay,
shown in the close up of Figure 1b, a hu-
man shouts a valid response, ?radiology? for the
level?s passphrase, while the nearby infected hu-
man shouts an incorrect response ?longitude.?
Gameplay is divided into eight stages, each with
increasing difficulty. Each stage has a goal of
saving a specific number of uninfected humans.
Infection incorporates common game mechanics,
such as unlockable weapons, power-ups that re-
store health, and achievements. Scoring is based
on both the number of zombies killed and the per-
centage of uninfected humans saved, motivating
players to kill infected humans in order to increase
their score. Importantly, Infection also includes a
leaderboard where players compete for top posi-
tions based on their total scores.
Annotation Each human is assigned a response
selected uniformly from V or N . Humans with
responses from N are treated as infected. Players
annotate by selecting which humans are infected:
Allowing a human with a response from V to enter
the city is treated as a positive annotation; killing
that human is treated as a negative annotation.
The design of Infection enables annotating mul-
tiple types of conceptual relations such as syn-
onymy or antonymy by changing only the descrip-
tion of the passphrase and how uninfected humans
are expected to respond.
Quality Enforcement Mechanisms Infection in-
cludes two game mechanics to limit adversarial
players from creating many low quality annota-
tions. Specifically, the game prevents players
from both (1) allowing all humans to live, via the
city infection level and (2) killing all humans, via
survivors taking the player?s gun; these actions
would both generate many false positives and false
negatives, respectively. These mechanics ensure
the game naturally produces better quality anno-
tations; in contrast, common crowdsourcing plat-
forms do not support analogous mechanics for en-
forcing this type of correctness at annotation time.
5 Game 2: The Knowledge Towers
The second game, The Knowledge Towers (TKT),
validates the concept-image dataset.
Design TKT is designed as a single-player role
playing game (RPG) where the player explores a
1297
(a) An example tower?s concept (b) Image selection screen (c) Gameplay
Figure 2: Screenshots of the key elements of The Knowledge Towers.
series of towers to unlock long-forgotten knowl-
edge. At the start of each tower, a target con-
cept is shown, e.g., the tower of ?tango,? along
with a description of the concept (Figure 2a). The
player must then recover the knowledge of the tar-
get concept by acquiring pictures of it. Pictures are
obtained through defeating monsters and opening
treasure chests, such as those shown in Figure 2c.
However, players must distinguish pictures of the
tower?s concept from unrelated pictures. When an
image is picked up, the player may keep or discard
it, as shown in Figure 2b. A player?s inventory is
limited to eight pictures to encourage them to se-
lect the most relevant pictures only.
Once the player has collected enough pictures,
the door to the boss room is unlocked and the
player may enter to defeat the boss and complete
the tower. Pictures may also be deposited in spe-
cial reward chests that grant experience bonuses if
the deposited pictures are from V . Gathering un-
related pictures has adverse effects on the player.
If the player finishes the level with a majority of
unrelated pictures, the player?s journey is unsuc-
cessful and she must replay the tower.
TKT includes RPG game elements commonly
found in game series such as Diablo and the Leg-
end of Zelda: players begin with a specific charac-
ter class that has class-specific skills, such as War-
rior or Thief, but will unlock the ability to play as
other classes by successfully completing the tow-
ers. Last, TKT includes a leaderboard where play-
ers can compete for positions; a player?s score is
based on increasing her character?s abilities and
her accuracy at discarding images from N .
Annotation Players annotate by deciding which
images to keep in their inventory. Images receive
positive rating annotations from: (1) depositing
the image in a reward chest, and (2) ending the
level with the image still in the inventory. Con-
versely, images receive a negative rating when a
player (1) views the image but intentionally avoids
picking it up or (2) drops the image from her in-
ventory.
TKT is designed to assist in the validation and
extension of automatically-created image libraries
that link to semantic concepts, such as ImageNet
(Deng et al, 2009) and that of Torralba et al
(2008). However, its general design allows for
other types of annotations, such as image labeling,
by changing the tower?s instructions and pictures.
Quality Enforcement Mechanisms Similar to
Infection, TKT includes analogous mechanisms
for limiting adversarial player annotations. Play-
ers who collect no images are prevented from en-
tering the boss room, limiting their ability to gen-
erate false negative annotations. Similarly, players
who collect all images are likely to have half of
their images from N and therefore fail the tower?s
quality-check after defeating the boss.
6 Experiments
Two experiments were performed with Infection
and TKT: (1) an evaluation of players? ability to
play accurately and to validate semantic relations
and image associations and (2) a comprehensive
cost comparison. Each experiment compared (a)
free and financially-incentivized versions of each
game, (b) crowdsourcing, and (c) a non-video
game with a purpose.
6.1 Experimental Setup
Gold Standard Data To compare the quality of
annotation from games and crowdsourcing, a gold
standard annotation was produced for a 10% sam-
ple of each dataset (cf. Section 3.2). Two annota-
tors independently rated the items and, in cases of
disagreement, a third expert annotator adjudicated.
Unlike in the game setting, annotators were free to
consult additional resources such as Wikipedia.
To measure inter-annotator agreement (IAA) on
the gold standard annotations, we calculated Krip-
1298
pendorff?s ? (Krippendorff, 2004; Artstein and
Poesio, 2008); ? ranges between [-1,1] where 1
indicates complete agreement, -1 indicates sys-
tematic disagreement, and values near 0 indicate
agreement at chance levels. Gold standard an-
notators had high agreement, 0.774, for concept-
concept relations. However, image-concept agree-
ment was only moderate, 0.549. A further analy-
sis revealed differences in the annotators? thresh-
olds for determining association, with one anno-
tator permitting more abstract relations. However,
the adjudication process resolved these disputes,
resulting in substantial agreement by all annota-
tors on the final gold annotations.
Incentives At the start of each game, players were
shown brief descriptions of the game and a de-
scription of a contest where the top-ranked players
would win either (1) monetary prizes in the form
of gift cards, or (2) a mention and thanks in this
paper. We refer to these as the paid and free ver-
sions of the game, respectively. In the paid setting,
the five top-ranking players were offered gift cards
valued at 25, 15, 15, 10, and 10 USD, starting from
first place (a total of 75 USD per game). To in-
crease competition among players and to perform
a fairer time comparison with crowdsourcing, the
contest period was limited to two weeks.
6.2 Comparison Methods
To compare with the video games, items were
annotated using two additional methods: crowd-
sourcing and a non-video game with a purpose.
Crowdsourcing Setup Crowdsourcing was per-
formed using the CrowdFlower platform. Anno-
tation tasks were designed to closely match each
game?s annotation process. A task begins with a
description of a target synset and its textual def-
inition; following, ten annotation questions are
shown. Separate tasks were used for validat-
ing concept-concept and concept-image relations.
Each tasks? questions were shown as a binary
choice of whether the item is related to the task?s
concept. Workers were paid 0.05 USD per task.
Each question was answered by three workers.
Following common practices for guarding
against adversarial workers (Mason and Suri,
2012), the tasks for concept c include quality
check questions using items from N
c
. Workers
who rate too many relations from N
c
as valid are
removed by CrowdFlower and prevented from par-
ticipating further. One of the ten questions in a
task used an item fromN
c
, resulting in a task mix-
ture of 90% annotation questions and 10% quality-
check questions. However, we note that both of
our video games use data that is 50% annotation,
50% quality-check. While the crowdsourcing task
could be adjusted to use an increased number of
quality-check options, such a design is uncommon
and artificially inflates the cost of the crowdsourc-
ing comparison beyond what would be expected.
Therefore, although the crowdsourcing and game-
based annotation tasks differ slightly, we chose to
use the common setup in order to create a fair cost-
comparison between the two.
Non-video Game with a Purpose To measure
the impact of the video game itself on the anno-
tation process, we developed a non-video game
with a purpose, referred to as SuchGame. Players
perform a single action in SuchGame: after be-
ing shown a concept c and its textual definition, a
player answers whether an item is related to the
concept. Items are drawn equally from V
c
and N
c
,
with players scoring a point each time they select
that an item from N is not related. A round of
gameplay contains ten questions. After the round
ends, players see their score for that round and the
current leaderboard. Two versions of SuchGame
were released, one for each dataset. SuchGame
was promoted with same free recognition incen-
tive as Infection and TKT.
6.3 Game Release
Both video games were released to multiple on-
line forums, social media sites, and Facebook
groups. SuchGame was released to separate Face-
book groups promoting free webgames and groups
for indie games. For each release, we estimated
an upper-bound of the audience sizes using avail-
able statistics such as Facebook group sites, web-
site analytics, and view counts. The free and paid
versions had sizes of 21,546 and 14,842 people,
respectively; SuchGame had an upper bound of
569,131 people. Notices promoting the game were
separated so that audiences saw promotions for
one of either the paid or free incentive version.
Games were also released in such a way as to pre-
serve the anonymity of the study, which limited
our ability to advertise to public venues where the
anonymity might be compromised.
7 Results and Discussion
7.1 Gameplay Analysis
In this section we analyze the games in terms of
participation and player?s ability to correctly play.
Players completed over 1388 games during the
1299
 0 50 100
 150 200 250
 300 350 400
 450
Numb
er of I
tems
Player
CorrectIncorrect
(a) Infection (free)
 0 50
 100 150
 200 250
 300 350
 400
Numb
er of I
tems
Player
CorrectIncorrect
(b) Infection (paid)
 0 100
 200 300
 400 500
 600 700
Numb
er of I
tems
Player
CorrectIncorrect
(c) TKT (free)
 0 200
 400 600
 800 1000
 1200 1400
 1600
Numb
er of I
tems
Player
CorrectIncorrect
(d) TKT (paid)
Figure 3: Accuracy of the top-40 players in rejecting true negative items during gameplay.
G.S. Agreement
# Players # Anno. N -Acc. Krip.?s ? True Pos. True Neg. All Cost per Ann.
TKT free 100 3005 97.0 0.333 82.5 82.5 82.5 $0.000
TKT paid 97 3318 95.4 0.304 69.0 92.1 74.0 $0.023
Crowdflower 290 13854 - 0.478 59.5 93.7 66.2 $0.008
Infection free 89 3150 71.0 0.445 67.8 68.4 68.1 $0.000
Infection paid 163 3355 65.9 0.330 69.1 54.8 61.1 $0.022
Crowdflower 1097 13764 - 0.167 16.9 96.4 59.6 $0.008
Table 1: Annotation statistics from all sources. N -Accuracy denotes accuracy at rejecting items fromN ;
G.S. Agreement denotes percentage agreement of the aggregated annotations with the gold standard.
study period. The paid and free versions of TKT
had similar numbers of players, while the paid ver-
sion of Infection attracted nearly twice the play-
ers compared to the free version, shown in Ta-
ble 1, Column 1. However, both versions created
approximately the same number of annotations,
shown in Column 2. Surprisingly, SuchGame re-
ceived little attention, with only a few players
completing a full round of game play. We believe
this emphasizes the strength of video game-based
annotation; adding incentives and game-like fea-
tures to an annotation task will not necessarily in-
crease its appeal. Given SuchGame?s minimal in-
terest, we omit it from further analysis.
Second, the type of incentive did not change the
percentage of items from N that players correctly
reject, shown for all players as N -accuracy in Ta-
ble 1 Column 3 and per-player in Figure 3. How-
ever, players were much more accurate at reject-
ing items from N in TKT than in Infection. We
attribute this difference to the nature of the items
and the format of the games. The images used
by TKT provide concrete examples of a concept,
which can be easily compared with the game?s cur-
rent concept; in addition, TKT allows players to
inspect items as long as a player prefers. In con-
trast, concept-concept associations require more
background knowledge to determine if a relation
exists; furthermore, Infection gives players limited
time to decide (due to board length) and also con-
tains cognitive distractors (zombies). Neverthe-
less, player accuracy remains high for both games
(Table 1, Col. 3) indicating the games represent a
viable medium for making annotation decisions.
Last, the distribution of player annotation fre-
quencies (Figure 3) suggests that the leaderboard
and incentives motivated players. Especially in the
paid condition, a clear group appears in the top
five positions, which were advertised as receiving
prizes. The close proximity of players in the paid
positions is a result of continued competition as
players jostled for higher-paying prizes.
7.2 Annotation Quality
This section assesses the annotation quality of
both games and of CrowdFlower in terms of (1)
the IAA of the participants, measured using Krip-
pendorff?s ?, and (2) the percentage agreement of
the resulting annotations with the gold standard.
Players in both free and paid games had similar
IAA, though the free version is consistently higher
(Table 1, Col. 4).
3
For images, crowdsourcing
workers have a higher IAA than game players;
however, this increased agreement is due to ad-
versarial workers consistently selecting the same,
incorrect answer. In contrast, both video games
contain mechanisms for limiting such behavior.
The strength of both crowdsourcing and games
with a purpose comes from aggregating multiple
annotations of a single item; i.e., while IAA may
3
In conversations with players after the contest ended,
several mentioned that being aware their play was contribut-
ing to research motivated them to play more accurately.
1300
Lemma Abbreviated Definition Most-selected Items
atom
The smallest possible
particle of a chemical
element
spectrum, nonparticulate radiation, molecule, hydrogen, electron
? ? ?
chord
A combination of three
or more notes
voicing, triad, tonality,? strum, note, harmony
?
color
An attribute from re-
flected or emitted light
orange, brown,? video, sadness, RGB, pigment
? ? ? ?
fire
The state of combustion
in which inflammable
material burns
sprinkler, machine gun, chemical reduction, volcano, organic chemistry
? ? ?
religion
The expression of
man?s belief in and
reverence for a super-
human power
polytheistic,? monotheistic, Jainism, Christianity,? Freedom of religion
? ? ?
Table 2: Examples of the most-selected words and images from the free version of both games. Bolded
words and images with a dashed border denote items not in BabelNet. Only the items marked with a ?
were rated as valid in the aggregated CrowdFlower annotations.
be low, the majority annotation of an item may be
correct. Therefore, in Table 1, we calculate the
percentage agreement of the aggregated annota-
tions with the gold standard annotations for ap-
proving valid relations (true positives; Col. 5), re-
jecting invalid relations (true negatives; Col. 6),
and for both combined (Col. 7). On average, both
video games in all settings produce more accurate
annotations than crowdsourcing. Indeed, despite
having lower IAA for images, the free version of
TKT provides an absolute 16.3% improvement in
gold standard agreement over crowdsourcing.
Examining the difference in annotation quality
for true positives and negatives, we see a strong
bias with crowdsourcing towards rejecting all
items. This bias leads to annotations with few false
positives, but as Column 5 shows, crowdflower
workers consistently performed much worse than
game players at identifying valid relations, pro-
ducing many false negative annotations. Indeed,
for concept-concept relations, workers identified
only 16.9% of the valid relations.
In contrast to crowdsourcing, both games were
effective at identifying valid relations. Table
2 shows examples of the most frequently cho-
sen items from V for the free versions of both
games. For both games, players were equally
likely to select novel items, suggesting the games
can serve a useful purpose of adding these miss-
ing relations in automatically constructed knowl-
edge bases. Highlighting one example, the five
most selected concept-concept relations for chord
were all novel; BabelNet included many relations
to highly-specific concepts (e.g., ?Circle of fifths?)
but did not include relations to more commonly-
associated concepts, like note and harmony.
7.3 Cost Analysis
This section provides a cost-comparison between
the video games and crowdsourcing. The free
versions of both games proved highly success-
ful, yielding high-quality annotations at no direct
cost. Both free and paid conditions produced sim-
ilar volumes of annotations, suggesting that play-
ers do not need financial incentives provided that
the games are fun to play. It could be argued that
the recognition incentive was motivating players
in the free condition and thus some incentive was
required. However, player behavior indicates oth-
erwise: After the contest period ended, no players
in the free setting registered for being acknowl-
edged by name, which strongly suggests the in-
centive was not contributing to their motivation for
playing. Furthermore, a minority of players con-
tinued to play even after the contest period ended,
suggesting that enjoyment was a driving factor.
1301
Last, while crowdsourcing has seen different qual-
ity and volume from workers in paid and unpaid
settings (Rogstadius et al, 2011), in contrast, our
games produced approximately-equivalent results
from players in both settings.
Crowdsourcing was slightly more cost-effective
than both games in the paid condition, as shown
in Table 1, Column 8. However, three additional
factors need to be considered. First, both games
intentionally uniformly sample between V and N
to increase player engagement,
4
which generates a
larger number of annotations for items in N than
are produced by crowdsourcing. When annota-
tions on items in N are included for both games
and crowdsourcing, the costs per annotation drop
to comparable levels: $0.007 for CrowdFlower
tasks, $0.008 for TKT, and $0.011 for Infection.
Second, for both annotation tasks, crowdsourc-
ing produced lower quality annotations, especially
for valid relations. Based on agreement with the
gold standard (Table 1, Col. 5), the estimated cost
for crowdsourcing a correct true positive annota-
tion increases to $0.014 for a concept-image and
a $0.048 for concepts-concept annotation. In con-
trast, the cost when using video games increases
only to $0.033 for concept-image and $0.031 for
concept-concept. These cost increases suggest
that crowdsourcing is not always cheaper with re-
spect to quality.
Third, we note that both video games in the paid
setting incur a fixed cost (for the prizes) and there-
fore additional games played can only further de-
crease the cost per annotation. Indeed, the present
study divided the audience pool into two separate
groups which effectively halved the potential num-
ber of annotations per game. Assuming combining
the audiences would produce the same number of
annotations, both our games? costs per annotation
drop to $0.012.
Last, video games can potentially come with
indirect costs due to software development and
maintenance. Indeed, Poesio et al (2013) report
spending 60,000? in developing their Phrase De-
tectives game with a purpose over a two-year pe-
riod. In contrast, both games here were developed
as a part of student projects using open source soft-
ware and assets and thus incurred no cost; fur-
thermore, games were created in a few months,
rather than years. Given that few online games
attain significant sustained interest, we argue that
4
Earlier versions that used mostly items from V proved
less engaging due to players frequently performing the same
action, e.g., saving most humans or collecting most pictures.
our lightweight model is preferable for producing
video games with a purpose. While using students
is not always possible, the development process
is fast enough to sufficiently reduce costs below
those reported for Phrase Detectives.
8 Conclusion
Two video games have been presented for vali-
dating and extending knowledge bases. The first
game, Infection, validates concept-concept rela-
tions, and the second, The Knowledge Towers,
validates image-concept relations. In experiments
involving online players, we demonstrate three
contributions. First, games were released in two
conditions whereby players either saw financial
incentives for playing or a personal satisfaction
incentive where they were thanked by us. We
demonstrated that both conditions produced nearly
identical numbers of annotations and, moreover,
that players were disinterested in the satisfaction
incentive, suggesting they played out of interest
in the game itself. Furthermore, we demonstrated
the effectiveness of a novel design for games with
a purpose which does not require two players for
validation and instead reinforces behavior only
using true negative items that required no man-
ual annotation. Second, in a comparison with
crowdsourcing, we demonstrate that video game-
based annotations consistently generated higher-
quality annotations. Last, we demonstrate that
video game-based annotation can be more cost-
effective than crowdsourcing or annotation tasks
with game-like features: The significant number
of annotations generated by the satisfaction incen-
tive condition shows that a fun game can generate
high-quality annotations at virtually no cost. All
annotated resources, demos of the games, and a
live version of the top-ranking items for each con-
cept are currently available online.
5
In the future we will apply our video games
to the validation of more data, such as the new
Wikipedia bitaxonomy (Flati et al, 2014).
Acknowledgments
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
We thank Francesco Cecconi for his support
with the websites and the many video game play-
ers without whose enjoyment this work would not
be possible.
5
http://lcl.uniroma1.it/games/
1302
References
Guillaume Artignan, Mountaz Hasco?et, and Math-
ieu Lafourcade. 2009. Multiscale visual analysis
of lexical networks. In Proceedings of the Inter-
national Conference on Information Visualisation,
pages 685?690.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Chris Biemann and Valerie Nygaard. 2010. Crowd-
sourcing wordnet. In Proceedings of the 5th Global
WordNet conference.
Alexander Budanitsky and Graeme Hirst. 2006.
Evaluating WordNet-based measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13?47.
Timothy Chklovski and Yolanda Gil. 2005. Improv-
ing the design of intelligent acquisition interfaces for
collecting world knowledge from web contributors.
In Proceedings of the International Conference on
Knowledge Capture, pages 35?42. ACM.
Tim Chklovski and Rada Mihalcea. 2002. Building a
Sense Tagged Corpus with Open Mind Word Expert.
In Proceedings of ACL 2002 Workshop on WSD: Re-
cent Successes and Future Directions, Philadelphia,
PA, USA.
Seth Cooper, Firas Khatib, Adrien Treuille, Janos
Barbero, Jeehyung Lee, Michael Beenen, Andrew
Leaver-Fay, David Baker, Zoran Popovi?c, and Foldit
players. 2010. Predicting protein structures with a
multiplayer online game. Nature, 466(7307):756?
760.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. 2009. ImageNet: A large-scale
hierarchical image database. In Proceedings of the
Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 248?255.
Soojeong Eom, Markus Dickinson, and Graham Katz.
2012. Using semi-experts to derive judgments on
word sense alignment: a pilot study. In Proceed-
ings of the Conference on Language Resources and
Evaluation (LREC), pages 605?611.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Tiziano Flati, Daniele Vannella, Tommaso Pasini, and
Roberto Navigli. 2014. Two is bigger (and better)
than one: the Wikipedia Bitaxonomy Project. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Bal-
timore, Maryland.
Amac? Herda?gdelen and Marco Baroni. 2012. Boot-
strapping a game with a purpose for common sense
collection. ACM Transactions on Intelligent Sys-
tems and Technology, 3(4):1?24.
Barbora Hladk?a, Ji?r?? M??rovsk`y, and Pavel Schlesinger.
2009. Play the language: Play coreference. In
Proceedings of the Joint Conference of the Asso-
ciation for Computational Linguistics and Inter-
national Joint Conference of the Asian Federation
of Natural Language Processing (ACL-IJCNLP),
pages 209?212. Association for Computational Lin-
guistics.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of the
Conference of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 455?462.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to Its Methodology. Sage, Thousand
Oaks, CA, second edition.
Yen-ling Kuo, Jong-Chuan Lee, Kai-yang Chiang, Rex
Wang, Edward Shen, Cheng-wei Chan, and Jane
Yung-jen Hsu. 2009. Community-based game de-
sign: experiments on social games for common-
sense data collection. In Proceedings of the ACM
SIGKDD Workshop on Human Computation, pages
15?22.
Mathieu Lafourcade and Alain Joubert. 2010. Com-
puting trees of named word usages from a crowd-
sourced lexical network. In Proceedings of the In-
ternational Multiconference on Computer Science
and Information Technology (IMCSIT), pages 439?
446, Wisla, Poland.
Winter Mason and Siddharth Suri. 2012. Conducting
behavioral research on amazons mechanical turk.
Behavior Research Methods, 44(1):1?23.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716?754.
Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: A Unified Approach. Transactions of
the Association for Computational Linguistics.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual se-
mantic network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Uppsala, Sweden, pages 216?225.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Joining forces pays off: Multilingual Joint Word
Sense Disambiguation. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1399?
1410, Jeju, Korea.
1303
Roberto Navigli. 2005. Semi-automatic extension of
large-scale linguistic knowledge bases. In Proceed-
ings of the 18th Internationa Florida AI Research
Symposium Conference, Clearwater Beach, Florida,
15?17 May 2005, pages 548?553.
Elisabeth Niemann and Iryna Gurevych. 2011. The
people?s web meets linguistic knowledge: Auto-
matic sense alignment of Wikipedia and WordNet.
In Proceedings of the International Conference on
Computational Semantics (IWCS), pages 205?214.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: a Unified Approach for Measuring Semantic
Similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1341?1351, Sofia, Bulgaria.
Massimo Poesio, Jon Chamberlain, Udo Kruschwitz,
Livio Robaldo, and Luca Ducceschi. 2013. Phrase
detectives: Utilizing collective intelligence for
internet-scale language resource creation. ACM
Transactions on Interactive Intelligent Systems,
3(1):3:1?3:44, April.
Jakob Rogstadius, Vassilis Kostakos, Aniket Kittur,
Boris Smus, Jim Laredo, and Maja Vukovic. 2011.
An assessment of intrinsic and extrinsic motivation
on task performance in crowdsourcing markets. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Jacek Rzeniewicz and Julian Szyma?nski. 2013.
Bringing Common Sense to WordNet with a Word
Game. In Computational Collective Intelligence.
Technologies and Applications, volume 8083 of Lec-
ture Notes in Computer Science, pages 296?305.
Springer.
Cristina Sarasua, Elena Simperl, and Natalya F Noy.
2012. CrowdMap: Crowdsourcing ontology align-
ment with microtasks. In Proceedings of the Inter-
national Semantic Web Conference (ISWC), pages
525?541.
Nitin Seemakurty, Jonathan Chu, Luis Von Ahn, and
Anthony Tomasic. 2010. Word sense disambigua-
tion via human computation. In Proceedings of the
ACM SIGKDD Workshop on Human Computation,
pages 60?63. ACM.
Jakub Simko, Michal Tvarozek, and Maria Bielikova.
2011. Little search game: term network acquisition
via a human computation game. In Proceedings of
the ACM conference on Hypertext and Hypermedia,
pages 57?62.
Katharina Siorpaes and Martin Hepp. 2008a. Games
with a purpose for the semantic web. IEEE Intelli-
gent Systems, 23(3):50?60.
Katharina Siorpaes and Martin Hepp. 2008b. On-
togame: Weaving the semantic web by online
games. In Sean Bechhofer, Manfred Hauswirth, Jrg
Hoffmann, and Manolis Koubarakis, editors, The
Semantic Web: Research and Applications, volume
5021 of Lecture Notes in Computer Science, pages
751?766. Springer Berlin Heidelberg.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogeneous
evidence. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics (COLING-ACL), Sydney, Aus-
tralia, pages 801?808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. unifying WordNet and Wikipedia. In Proceed-
ings of the 16th World Wide Web Conference, Banff,
Canada, 8?12 May 2007, pages 697?706.
Stefan Thaler, Elena Paslaru Bontas Simperl, and
Katharina Siorpaes. 2011. SpotTheLink: A Game
for Ontology Alignment. In Proceedings of the
6th Conference on Professional Knowledge Man-
agement: From Knowledge to Action, pages 246?
253.
Antonio Torralba, Robert Fergus, and William T Free-
man. 2008. 80 million tiny images: A large data
set for nonparametric object and scene recognition.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 30(11):1958?1970.
Noortje J. Venhuizen, Valerio Basile, Kilian Evang, and
Johan Bos. 2013. Gamification for word sense la-
beling. In Proceedings of the International Confer-
ence on Computational Semantics (IWCS).
David Vickrey, Aaron Bronzan, William Choi, Aman
Kumar, Jason Turner-Maier, Arthur Wang, and
Daphne Koller. 2008. Online word games for se-
mantic data collection. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 533?542.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
Conference on Human Factors in Computing Sys-
tems (CHI), pages 319?326.
Luis von Ahn, Mihir Kedia, and Manuel Blum. 2006.
Verbosity: a game for collecting common-sense
facts. In Proceedings of the Conference on Human
Factors in Computing Systems (CHI), pages 75?78.
1304
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 359?362,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
HERMIT: Flexible Clustering for the SemEval-2 WSI Task
David Jurgens
University of California, Los Angeles
Los Angeles, California, USA
jurgens@cs.ucla.edu
Keith Stevens
University of California, Los Angeles
Los Angeles, California, USA
kstevens@cs.ucla.edu
Abstract
A single word may have multiple un-
specified meanings in a corpus. Word
sense induction aims to discover these dif-
ferent meanings through word use, and
knowledge-lean algorithms attempt this
without using external lexical resources.
We propose a new method for identify-
ing the different senses that uses a flexi-
ble clustering strategy to automatically de-
termine the number of senses, rather than
predefining it. We demonstrate the effec-
tiveness using the SemEval-2 WSI task,
achieving competitive scores on both the
V-Measure and Recall metrics, depending
on the parameter configuration.
1 Introduction
The Word Sense Induction task of SemEval 2010
compares several sense induction and discrimina-
tion systems that are trained over a common cor-
pus. Systems are provided with an unlabeled train-
ing corpus consisting of 879,807 contexts for 100
polysemous words, with 50 nouns and 50 verbs.
Each context consists of several sentences that use
a single sense of a target word, where at least one
sentence contains the word. Systems must use the
training corpus to induce sense representations for
the many word senses and then use those represen-
tations to produce sense labels for the same 100
words in unseen contexts from a testing corpus.
We perform this task by utilizing a distribu-
tional word space formed using dimensionality
reduction and a hybrid clustering method. Our
model is highly scalable; the dimensionality of the
word space is reduced immediately through a pro-
cess based on random projections. In addition, an
online part of our clustering algorithm maintains
only a centroid that describes an induced word
sense, instead of all observed contexts, which lets
the model scale to much larger corpora than those
used in the SemEval-2 WSI task.
2 The Word Sense Induction Model
We perform word sense induction by modeling
individual contexts in a high dimensional word
space. Word senses are induced by finding con-
texts which are similar and therefore likely to use
the same sense of the target word. We use a hybrid
clustering method to group similar contexts.
2.1 Modeling Context
For a word, each of its contexts are represented by
the words with which it co-occurs. We approx-
imate this high dimensional co-occurrence space
with the Random Indexing (RI) word space model
(Kanerva et al, 2000). RI represents the occur-
rence of a word with an index vector, rather than
a set of dimensions. An index vector is a fixed,
sparse vector that is orthogonal to all other words?
index vectors with a high probability; the total
number of dimensions in the model is fixed at a
small value, e.g. 5,000. Orthogonality is obtained
by setting a small percentage of the vector?s values
to ?1 and setting the rest to 0.
A context is represented by summing the index
vectors corresponding to the n words occurring to
the left and right of the polysemous word. Each
occurrence of the polysemous word in the entire
corpus is treated as a separate context. Contexts
are represented by a compact first-order occur-
rence vector; using index vectors to represent the
occurrences avoids the computational overhead of
other dimensional reduction techniques such as
the SVD.
2.2 Identifying Related Contexts
Clustering separates similar context vectors into
dissimilar clusters that represent the distinct
senses of a word. We use an efficient hybrid of
online K-Means and Hierarchical Agglomerative
359
Clustering (HAC) with a threshold. The thresh-
old allows for the final number of clusters to be
determined by data similarity instead of having to
specify the number of clusters.
The set of context vectors for a word are clus-
tered using K-Means, which assigns a context to
the most similar cluster centroid. If the near-
est centroid has a similarity less than the cluster
threshold and there are not K clusters, the context
forms a new cluster. We define the similarity be-
tween contexts vectors as the cosine similarity.
Once the corpus has been processed, clusters
are repeatedly merged using HAC with the aver-
age link criteria, following (Pedersen and Bruce,
1997). Average link clustering defines cluster sim-
ilarity as the mean cosine similarity of the pair-
wise similarity of all data points from each clus-
ter. Cluster merging stops when the two most sim-
ilar clusters have a similarity less than the clus-
ter threshold. Reaching a similarity lower than the
cluster threshold signifies that each cluster repre-
sents a distinct word sense.
2.3 Applying Sense Labels
Before training and evaluating our model, all
occurrences of the 100 polysemous words were
stemmed in the corpora. Stemming was required
due to a polysemous word being used in multiple
lexical forms, e.g. plural, in the corpora. By stem-
ming, we avoid the need to combine contexts for
each of the distinct word forms during clustering.
After training our WSI model on the training
corpus, we process the test corpus and label the
context for each polysemous word with an induced
sense. Each test context is labeled with the name
of the cluster whose centroid has the highest co-
sine similarity to the context vector. We represent
the test contexts in the same method used for train-
ing; index vectors are re-used from training.
3 Evaluation and Results
The WSI task evaluated the submitted solutions
with two methods of experimentation: an unsuper-
vised method and a supervised method. The unsu-
pervised method is measured according to the V-
Measure and the F-Score. The supervised method
is measured using recall.
3.1 Scoring
The first measure used is the V-Measure (Rosen-
berg and Hirschberg, 2007), which compares the
clusters of target contexts to word classes. This
measure rates the homogeneity and completeness
of a clustering solution. Solutions that have word
clusters formed from one word class are homoge-
neous; completeness measures the degree to which
a word class is composed of target contexts allo-
cated to a single cluster.
The second measure, the F-Score, is an ex-
tension from information retrieval and provides a
contrasting evaluation metric by using a different
interpretation of homogeneity and completeness.
For the F-Score, the precision and recall of all pos-
sible context pairs are measured, where a word
class has the expected context pairs and a provided
solution contains some word pairs that are correct
and others that are unexpected. The F-Score tends
to discount smaller clusters and clusters that can-
not be assigned to a word class (Manandhar et al,
2010).
3.2 Parameter Tuning
Previous WSI evaluations provided a test corpus,
a set of golden sense labels, and a scoring mecha-
nism, which allowed models to do parameter tun-
ing prior to providing a set of sense labels. The
SemEval 2010 task provided a trial corpus that
contains contexts for four verbs that are not in the
evaluation corpus, which can be used for train-
ing and testing. The trial corpus also came with a
set of golden sense assignments. No golden stan-
dard was provided for the training or test corpora,
which limited any parameter tuning.
HERMIT exposes three parameters: cluster
threshold, the maximum number of clusters and
the window size for a context. An initial anal-
ysis from the trial data showed that the window
size most affected the scores; small window sizes
resulted in higher V-Measure scores, while larger
window sizes maximized the F-Score. Because
contexts are represented using only first-order fea-
tures, a smaller window size should have less over-
lap, which potentially results in a higher number
of clusters. We opted to maximize the V-Measure
score by using a window size of ?1.
Due to the limited number of training instances,
our precursory analysis with the trial data did not
show significant differences for the remaining two
parameters; we arbitrarily selected a clustering
threshold of .15 and a maximum of 15 clusters per
word without any parameter tuning.
After the release of the testing key, we per-
360
formed a post-hoc analysis to evaluate the effects
of parameter tuning on the scores. We include two
alternative parameter configurations that were op-
timized for the F-Score (HERMIT-F) and the su-
pervised evaluations (HERMIT-S). The HERMIT-
F variation used a threshold of 0.85 and a win-
dow size of ?10 words. The HERMIT-S variation
used a threshold of 0.85 and a window size of ?1
words. We did not vary the maximum number of
clusters, which was set at 15.
For each evaluation, we provide the scores of
seven systems: the three HERMIT configurations,
the highest and lowest scoring submitted systems,
the Most Frequent Sense (MFS) baseline, and a
Random baseline provided by the evaluation team.
We provide the scores for each experiment when
evaluating all words, nouns, and verbs. We also
include the system?s rank relative to all submitted
systems and the average number of senses gen-
erated for each system; our alternative HERMIT
configurations are given no rank.
3.3 Unsupervised Evaluation
System All Nouns Verbs Rank Senses
HERMIT-S 16.2 16.7 15.3 10.83
HERMIT 16.1 16.7 15.6 1 10.78
Random 4.4 4.6 4.1 18 4.00
HERMIT-F 0.015 0.008 0.025 1.54
MFS 0.0 0.0 0.0 27 1.00
LOW 0.0 0.0 0.1 28 1.01
Table 1: V-Measure for the unsupervised evalua-
tion
System All Nouns Verbs Rank Senses
MFS 63.4 57.0 72.7 1 1.00
HIGH 63.3 57.0 72.4 2 1.02
HERMIT-F 62.1 56.7 69.9 1.54
Random 31.9 30.4 34.1 25 4.00
HERMIT 26.7 30.1 24.4 27 10.78
HERMIT-S 26.5 23.9 30.3 10.83
LOW 16.1 15.8 16.4 28 9.71
Table 2: F-Scores for the unsupervised evaluation
The unsupervised evaluation considers a golden
sense labeling to be word classes and a set of in-
duced word senses as clusters of target contexts
(Manandhar et al, 2010). Tables 1 and 2 display
the results for the unsupervised evaluation when
measured according to the V-Measure and the F-
Score, respectively. Our system provides the best
V-Measure of all submitted systems for this eval-
uation. This is in part due to the average number
of senses our system generated (10.78), which fa-
vors more homogenous clusters. Conversely, this
configuration does poorly when measured by F-
Score, which tends to favor systems that generate
fewer senses per word.
When configured for the F-Score, HERMIT-
F performs well; this configuration would have
ranked third for the F-Score if it had been submit-
ted. However, its performance is also due to the
relatively few senses per word it generates, 1.54.
The inverse performance of both optimized con-
figurations is reflective of the contrasting nature of
the two performance measures.
3.4 Supervised Evaluation
System All Noun Verb Rank
HIGH 62.44 59.43 66.82 1
MFS 58.67 53.22 66.620 15
HERMIT-S 58.48 54.18 64.78
HERMIT 58.34 53.56 65.30 17
Random 57.25 51.45 65.69 19
HERMIT-F 56.44 53.00 61.46
LOW 18.72 1.55 43.76 28
Table 3: Supervised recall for the 80/20 split
System All Noun Verb Rank
HIGH 61.96 58.62 66.82 1
MFS 58.25 52.45 67.11 12
HERMIT 57.27 52.53 64.16 18
HERMIT-S 57.10 52.76 63.46
Random 56.52 50.21 65.73 20
HERMIT-F 56.18 52.26 61.88
LOW 18.91 1.52 44.23 28
Table 4: Supervised recall for the 60/40 split
The supervised evaluation simulates a super-
vised Word Sense Disambiguation (WSD) task.
The induced sense labels for the test corpus are
split such that the first set is used for mapping in-
duced senses to golden senses and the remaining
sense labels are treated as sense labels provided
by a WSD system, which allows for evaluation.
Five splits are done at random to avoid any biases
created due to the separation of the mapping cor-
pus and the evaluation corpus; the resulting score
for this task is the average recall over the five di-
visions. Two sets of splits were used for evalua-
tion: one with 80% of the senses as the mapping
portion and 20% as the evaluation portion and one
with 60% as the mapping portion corpus and 40%
for evaluation.
The results for the 80/20 split and 60/40 split
are displayed in tables 3 and 4, respectively. In
both supervised evaluations, our submitted system
361
 0
 4
 8
 12
Cl
us
te
rs Clusters
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
 2  4  6  8 10 12 14
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
F-
Sc
or
e
V-
M
ea
su
re
Window Size
F-Score
V-Measure
Figure 1: A comparison for F-Score and V-
Measure for different window sizes. Scores are an
average using thresholds of 0.15, 0.55 and 0.75.
does moderately well. In both cases it outperforms
the Random baseline and does almost as well as
the MFS baseline. The submitted system outper-
forms the Random baseline and approaches the
MFS baseline for the 80/20 split. The HERMIT-S
version, which is optimized for this task, provides
similar results.
4 Discussion
The HERMIT system is easily configured to
achieve close to state of the art performance for
either evaluation measure on the unsupervised
benchmark. This reconfigurability allows the al-
gorithm to be tuned for producing a few coarse
senses of a word, or many finer-grained senses.
We further investigated the performance with
respect to the window size parameter on both mea-
sures. Since each score can be effectively opti-
mized individually, we considered whether both
scores could be maximized concurrently. Figure
1 presents the impact of the window size on both
measures using an average of three threshold pa-
rameter configurations.
The analysis of both measures indicates that
reasonable performance can be obtained from us-
ing a slightly larger context window. For ex-
ample, a window size of 4 has an average F-
Score of 52.4 and V-Measure of 7.1. Although
this configuration produces scores lower than the
optimized versions, its performance would have
ranked 12th according to V-Measure and 15th for
F-Score. These scores are consistent with the me-
dian performance of the submitted systems and of-
fer a middle ground should a HERMIT user want
a compromise between many fine-grained word
senses and a few coarse-grained word senses.
5 Conclusion
We have shown that our model is a highly flexi-
ble and tunable Word Sense Induction model. De-
pending on the task, it can be optimized to gen-
erate a set of word senses that range from be-
ing broad and representative to highly refined.
Furthermore, we demonstrated a balanced perfor-
mance setting for both measures for when param-
eter tuning is not possible. The model we sub-
mitted and presented is only one possible config-
uration available, and in the future we will be ex-
ploring the effect of other context features, such
as syntactic structure in the form of word ordering
(Sahlgren et al, 2008) or dependency parse trees,
(Pado? and Lapata, 2007), and other clustering al-
gorithms. Last, this model is provided as part of
the S-Space Package (Jurgens and Stevens, 2010),
an open source toolkit for word space algorithms.
References
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Deonstrations.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In L. R. Gleitman and A. K. Josh,
editors, Proceedings of the 22nd Annual Conference
of the Cognitive Science Society, page 1036.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
Task 14: Word Sense Induction & Disambiguation.
In Proceedings of SemEval-2.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-Based Construction of Seman-
tic Space Models. Computational Linguistics,
33(2):161?199.
Ted Pedersen and Rebecca Bruce. 1997. Distinguish-
ing word senses in untagged text. In Proceedings
of the Second Conference on Empirical Methods in
Natural Language Processing, pages 197?207.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL). ACL.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode or-
der in word space. In Proceedings of the 30th
Annual Meeting of the Cognitive Science Society
(CogSci?08).
362
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 189?198,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
An Evaluation of Graded Sense Disambiguation using Word Sense Induction
David Jurgens1,2
1HRL Laboratories, LLC
Malibu, California, USA
2Department of Computer Science
University of California, Los Angeles
jurgens@cs.ucla.edu
Abstract
Word Sense Disambiguation aims to label the
sense of a word that best applies in a given
context. Graded word sense disambiguation
relaxes the single label assumption, allow-
ing for multiple sense labels with varying de-
grees of applicability. Training multi-label
classifiers for such a task requires substan-
tial amounts of annotated data, which is cur-
rently not available. We consider an alter-
nate method of annotating graded senses using
Word Sense Induction, which automatically
learns the senses and their features from cor-
pus properties. Our work proposes three ob-
jective to evaluate performance on the graded
sense annotation task, and two new methods
for mapping between sense inventories using
parallel graded sense annotations. We demon-
strate that sense induction offers significant
promise for accurate graded sense annotation.
1 Introduction
Word Sense Disambiguation (WSD) aims to identify
the sense of a word in a given context, using a pre-
defined sense inventory containing the word?s differ-
ent meanings (Navigli, 2009). Traditionally, WSD
approaches have assumed that each occurrence of
a word is best labeled with a single sense. How-
ever, human annotators often disagree about which
sense is present (Passonneau et al, 2010), espe-
cially in cases where some of the possible senses
are closely related (Chugur et al, 2002; McCarthy,
2006; Palmer et al, 2007).
Recently, Erk et al (2009) have shown that in
cases of sense ambiguity, a graded notion of sense
labeling may be most appropriate and help reduce
the ambiguity. Specifically, within a given context,
multiple senses of a word may be salient to the
reader, with different levels of applicability. For ex-
ample, in the sentence
? The athlete won the gold metal due to her hard
work and dedication.
multiple senses could be considered applicable for
?won? according to theWordNet 3.0 sense inventory
(Fellbaum, 1998):
1. win (be the winner in a contest or competition; be victo-
rious)
2. acquire, win, gain (win something through one?s efforts)
3. gain, advance, win, pull ahead, make headway, get ahead,
gain ground (obtain advantages, such as points, etc.)
4. succeed, win, come through, bring home the bacon, de-
liver the goods (attain success or reach a desired goal)
In this context, many annotators would agree that the
athlete has both won an object (the gold metal itself)
and won a competition (signified by the gold medal).
Although contexts can be constructed to elicit only
one of these senses, in the example above, a graded
annotation best matches human perception.
Graded word sense (GWS) annotation offers sig-
nificant advantages for sense annotation with a fine-
grained sense inventory. However, creating a suf-
ficiently large annotated corpus for training super-
vised GWS disambiguation models presents a sig-
nificant challenge, i.e., the laborious task of gath-
ering annotations for all combinations of a word?s
senses, along with variation in those senses appli-
cabilities. To our knowledge, Erk et al (2009) have
provided the only data set with GWS annotations for
11 terms.
189
Therefore, we consider the use of Word Sense In-
duction (WSI) for GWS annotation. WSI removes
the need for substantial training data by automati-
cally deriving a word?s senses and associated sense
features through examining its contextual uses. Fur-
thermore, the data-driven sense discovery defines
senses as they are present in the corpus, which may
identify usages not present in traditional sense in-
ventories (Lau et al, 2012). Last, many WSI models
represent senses loosely as abstractions over usages,
which potentially may transfer well to expressing
GWS annotations as a blend of their sense usages.
In this paper, we consider the performance ofWSI
models on a GWS task. The contributions of this
paper are as follows. First, in Sec. 2, we motivate
three GWS annotation objectives and propose corre-
sponding measures that provide fine-grained analy-
sis of the capabilities of different WSI models. Sec-
ond, in Sec. 4, we propose two new sense mapping
procedures for converting an induced sense inven-
tory to a reference sense inventory when GWS an-
notations are present, and demonstrate significant
performance improvement using these procedures
on GWS annotation. Last, in Sec. 5, we demon-
strate a complete evaluation framework using three
graph-based WSI models as examples, generating
several insights for how to better evaluate GWS dis-
ambiguation systems.
2 Evaluating GWS Annotations
Graded word sense annotation conveys multiple lev-
els of information, both in which senses are present
and their relative levels of applicability; and so, no
single evaluation measure alone is appropriate for
assessing GWS annotation capability. Therefore, we
propose three objectives for the evaluating the sense
labeling: (1) Detection of which senses are present,
(2) Ranking senses according to applicability, and
(3) Perception of the graded presence of each sense.
We separate the three objectives as a way to evaluate
how well different techniques perform on each as-
pect individually, which may encourage future work
in ensemble WSD methods that use combinations of
the techniques. Figure 1 illustrates each evaluation
on example annotations. We note that Erk and Mc-
Carthy (2009) have also proposed an alternate set of
evaluation measures for GWS annotations. Where
applicable, we describe and compare their measures
to ours for the three objectives.
In the following definitions, let SiG refer to the set
of senses {s1, . . . , sn} present in context i according
to the gold standard, and similarly, let SiL refer to
the set of senses for context i as labeled by a WSD
system using the same sense inventory. Let peri(sj)
refer to the perceived numeric applicability rating of
sense sj in context i.
Detection measures the ability to accurately iden-
tify which senses are applicable in a given context,
independent of their applicability. While the most
basic of the evaluations, systems that are highly ac-
curate at multi-sense detection could be used for rec-
ognizing ambiguous contexts where multiple senses
are applicable or for evaluating the granularity of
sense ontologies by testing for correlations between
senses in a multi-sense labeling. Detection is mea-
sured using the Jaccard Index between SiG and SiL
for a given context i: S
i
G?SiL
SiG?SiL
Ranking measures the ability to order the senses
present in context i according to their applicabil-
ity but independent of their quantitative applicabil-
ity scores. Even though multiple senses are present,
a context may have a clear primary senses. By pro-
viding a ranking in agreement with human judge-
ments, systems create a primary sense label for each
context. When the induced senses are mapped to a
sense inventory, selecting the primary sense is analo-
gous to non-graded WSD where a context is labeled
with its most applicable sense.
To compare sense rankings, we use Goodman and
Kruskal?s ?, which is related to Kendall?s ? rank cor-
relation. When the data has many tied ranks, ? is
preferable to both Kendall?s ? as well as Spearman?s
? rank correlation (Siegel and Castellan Jr., 1988),
the latter of which is used by Erk and McCarthy
(2009) for evaluating sense rankings. The use of ?
was motivated by our observation that in the GWS
dataset (described later in Section 5.1), roughly 65%
of the instances contained at least one tied ranking
between senses.
To compute ?, we examine all pair-wise combi-
nations of senses (si, sj) of the target word. Let
rG(si) and rL(si) denote the ranks of sense si in
the gold standard and provided annotations. In the
event that a ranking does not include senses, all
of the inapplicable senses are assigned a tied rank
190
Instance Gold Standard Annotation
The athlete won the gold metal due to her
hard work and dedication.
win.v.1: 0.6, win.v.2: 0.4
(not applicable: win.v.3, win.v.4)
Test Annotation Detection Ranking Perception
win.v.1: 0.7, win.v.2: 0.3 1.0 1.0 0.983
win.v.1: 1.0 0.5 1.0 0.832
win.v.2: 1.0 0.5 0.333 0.554
win.v.3: 0.5, win.v.1: 0.3, win.v.4: 0.2 0.25 -0.2 0.405
Figure 1: Example annotations of the same context compared with the gold standard according to Detection,
Ranking, and Perception.
lower than the least applicable sense; i.e., for m
applicable senses, all inapplicable senses have rank
m+1. A pair of senses, (si, sj) is said to be con-
cordant if rG(si) < rG(sj) and rL(si) < rL(sj) or
rG(si) > rG(sj) and rL(si) > rL(sj), and discor-
dant otherwise. ? is defined as c?dc+d where c is the
number of concordant pairs and d is the number of
discordant.
Perception measures the ability to equal human
judgements on the levels of applicability for each
sense in a context. Unlike ranking, this evaluation
quantifies the difference in sense applicability. As a
potential application, these differences can be used
to quantify the contextual ambiguity. For example,
the relative applicability differences can be used to
distinguish between ambiguous contexts where mul-
tiple highly-applicable senses exist and unambigu-
ous contexts where a single main sense exists but
other senses are still minimally applicable.
To quantify Perception, we compare sense label-
ings using the cosine similarity. Each labeling is rep-
resented as a vector with a separate component for
each sense, whose value is the applicability of that
sense. The Perception for two annotations of con-
text j is then calculated as
?
i perj(sGi )? perj(sLi )
?
?
i perj(sGi )2 ?
?
?
i perj(sLi )2
.
Note that because all sense perceptibilities are non-
negative, the cosine similarity is bounded to [0, 1].
Erk and McCarthy (2009) propose an alternate
measure for comparing the applicability values us-
ing the Jensen-Shannon divergence. The sense an-
notations are normalized to probability distributions,
denotedG and L, and the divergence is computed as:
JSD(G||L) = 12DKL(G||M) +
1
2DKL(L||M)
where M is the average of the distributions G and L
and DKL denotes the Kullback-Leibler divergence.
While both approaches are similar in intent, we find
that the cosine similarity better matches the expected
difference in Perception for cases where two anno-
tations use different numbers of senses. For exam-
ple, the fourth test annotation in Fig. 1 has a JSS1
of 0.593, despite its significant differences in order-
ing and the omission of a sense. Indeed, in cases
where the set of senses in a test annotation is com-
pletely disjoint from the set of gold standard senses,
the JSS will be positive due to comparing the two
distributions against their average; In contrast, the
cosine similarity in such cases will be zero, which
we argue better matches the expectation that such an
annotation does not meet the Perception objective.
3 WSI Models
For evaluation we adapt three recent graph-based
WSI methods for the task of graded-sense annota-
tion: Navigli and Crisafulli (2010), referred to as
Squares, Jurgens (2011), referred to as Link, and
UoY (Korkontzelos and Manandhar, 2010). At an
abstract level, these methods operate in two stages.
First, a graph is built, using either words or word
pairs as vertices, and edges are added denoting some
form of association between the vertices. Second,
senses are derived by clustering or partitioning the
graph. We selected these methods based on their su-
perior performance on recent benchmarks and also
1The JSD is a distance measure in [0, 1], which we convert
to a similarity JSS = 1? JSD for easier comparison.
191
for their significant differences in approach. Follow-
ing, we briefly summarize each method to highlight
its key parameters and then describe its adaptation
to GWS annotation.
Squares Navigli and Crisafulli (2010) propose a
method that builds a separate graph for each term
for sense induction. First, a large corpus is used to
identify associated terms using the Dice coefficient:
For two terms w1, w2, Dice(w1, w2) = 2c(w1,w2)c(w1)+c(w2)
where c(w) is the frequency of occurrence. Next,
for a given term w the initial graph, G, is con-
structed by adding edges to every term w2 where
Dice(w,w2) ? ?, and then the step is repeated for
the neighbors of each term w2 that was added.
Once the initial graph is constructed, edges are
pruned to separate the graph into components. Nav-
igli and Crisafulli (2010) found improved perfor-
mance on their target application using a pruning
method based on the number of squares (closed
paths of length 4) in which an edge participates. Let
s denote the number of squares that an edge e par-
ticipates in and p denote the number of squares that
would be possible from the set of neighbors of e.
Edges with sp < ? are removed. The remaining con-
nected components in G denote the senses of w.
Sense disambiguation on a context of w is per-
formed by computing the intersection of the con-
text?s terms with the terms in each of the connected
components. As originally specified, the component
with the largest overlap is labeled as the sense of w.
We adapt this to graded senses by returning all inter-
secting components with applicability proportional
to their overlap. Furthermore, for efficiency, we use
only noun, verb, and adjective lemmas in the graphs.
Link Jurgens (2011) use an all-words method
where a single graph is built in order to derive the
senses of all words in it. Here, the graph?s clus-
ters do not correspond to a specific word?s senses
but rather to contextual features that can be used to
disambiguate any of the words in the cluster.
In its original specification, the graph is built with
edges between co-occurring words and edge weights
corresponding to co-occurrence frequency. Edges
below a specified threshold ? are removed, and then
link community detection (Ahn et al, 2010) is ap-
plied to discover sense-disambiguating word com-
munities, which are overlapping cluster of vertices
in the graph, rather than hard partitions. Once the set
of communities is produced, communities with three
or fewer vertices are removed, under the assumption
that these communities contain too few features to
reliably disambiguate.
Senses are disambiguated by finding the commu-
nity with the largest overlap score, computed as the
weighted Jaccard Index. For a context with the set
of features Fi and a community with features Fj , the
overlap is measured as |Fj | ? |Fi?Fj ||Fi?Fj | .
We adapt this algorithm in three ways. First,
rather than use co-occurrence frequency to weight
edges between terms, we weight edges accord to
their statistical association with the G-test (Dunning,
1993). The G-test weighting helps remove edges
whose large edge weights are due to high corpus fre-
quency but provide no disambiguating information,
and the weighting also allows the ? parameter to
be more consistently set across corpora of different
sizes. Second, while Jurgens (2011) used only nouns
as vertices in the graph, we include both verbs and
adjectives due to needing to identify senses for both.
Third, for graded senses, we disambiguate a context
by reporting all overlapping communities, weighted
by their overlap score.
UoY Korkontzelos and Manandhar (2010) pro-
pose a WSI model that builds a graph for each term
for disambiguation. The graph is built in four stages,
with four main tuning parameters, summarized next.
First, using a reference corpus, all contexts of the
target word w are selected to build a list of co-
occurring noun lemmas, retaining all those with fre-
quency above P1. Second, the Log-Likelihood ratio
(Dunning, 1993) is computed between all selected
nouns and w, retaining only those with an associa-
tion above P2. Third, all remaining nouns are used
to create all
(n
2
)
noun pairs. Next, each term and
pair is mapped to the set of contexts in the reference
corpus in which it is present. A pair (wi, wj) is re-
tained only if its set of contexts is dissimilar to the
sets of contexts of both its member terms, using the
Dice coefficient to measure the similarity of the sets.
Pairs with a Dice coefficient above P4 with either of
its constituent terms are removed. Last, edges are
added between nouns and noun pairs according to
their conditional probabilities of occurring with each
other. Edges with a conditional probability less than
192
P3 are not included.
Once the graph has been constructed, the Chi-
nese Whispers graph partitioning algorithm (Bie-
mann, 2006) is used to identify word senses. Each
graph partition is assigned a separate sense of w.
Next, each partition is mapped to the set of contexts
in the reference corpus in which at least one of its
vertices occurs. Partitions whose context sets are a
strict subset of another are merged with the subsum-
ing partition.
Word sense disambiguation occurs by counting
the number of overlapping vertices for each parti-
tion and selecting the partition with the highest over-
lap as the sense of w. We extend this to graded an-
notation by selecting all partitions with at least one
vertex present and set the applicability equal to the
degree of overlap.
4 Evaluation Across Sense Inventories
Directly comparing GWS annotations from the in-
duced and gold standard sense inventories requires
first creating a mapping from the induced senses to
the gold standard inventory. Agirre et al (2006) pro-
pose a sense-mapping procedure, which was used in
the previous two SemEval WSI Tasks (Agirre and
Soroa, 2007; Manandhar et al, 2010). We consider
this procedure and two extensions of it to support
learning a mapping from graded sense annotations.
The procedure of Agirre et al (2006) uses three
corpora: (1) a base corpus from which the senses
are derived, (2) a mapping corpus annotated with
both gold standard senses, denoted gs, and induced
senses, denoted is, and (3) a test corpus annotated
with is senses that will be converted to gs senses.
Once the senses are induced from the base cor-
pus, the mapping corpus is annotated with is senses
and a matrix M is built where cell i, j initially con-
tains the counts of each time gsj and isi were used
to label the same instance. The rows of this matrix
are then normalized such that each cell now repre-
sents p(gsj|isi). The final mapping selects the most
probable gs sense for each is sense.
To label the test corpus, each instance that is
labeled with isi is relabeled with the gs sense
with the highest conditional probability given isi.
When a context c is annotated by a set of labels
L = {isi, . . . , isj}, the final sense labeling con-
tains the set of all gs to which the is senses were
mapped, weighted by their mapping frequencies:
perc(gsj) = 1|L|
?
isi?L ?(isi, gsj) where ? returns
1 if isi is mapped to gsj and 0 otherwise.
The original algorithm of Agirre et al (2006) does
not consider the role of applicability in evaluating
whether an is sense should be mapped to a gs sense;
is senses with different levels of applicability in the
same context are treated equivalently in updating
M . Therefore, as a first extension, referred to as
Graded, we revise the update rule for constructing
M where for the set of contexts C labeled by both
isi and gsj ,Mi,j =
?
c?C perc(isi)?perc(gsj). As
in (Agirre et al, 2006), M is normalized and each is
sense is mapped to its most probable gs sense.
To label the test corpus using theGraded method,
the applicability of the is sense is also included.
For a context c is annotated with senses L =
{isi, . . . , isj}, the final sense labeling contains the
set of all gs senses to which the is senses were
mapped, weighted by their mapping frequencies:
perc(gsj) =
?
isi?L [?(isi, gsj)? perc(isi)] . The
applicabilities are then normalized to sum to 1.
The prior two methods restrict an is sense to map-
ping to only a single gs sense. However, an is sense
may potentially correspond to multiple gs senses,
each with different levels of applicability. There-
fore, we consider a second extension, referred to as
Distribution, that uses the same matrix construc-
tion as the Graded procedure, but rather than map-
ping each is to a single sense, maps it to a distribu-
tion over all gs senses for which it was co-annotated,
which is the normalized row vector in M for an is
sense. Labeling in the test corpus is then done by
summing the distributions of the is senses annotated
in the context and normalizing to create a probability
distribution over the union of their gs senses.
5 Experiments
We adapt the supervised WSD setting used in prior
SemEval WSI Tasks (Agirre and Soroa, 2007; Man-
andhar et al, 2010) to evaluation the models accord-
ing to the three proposed objectives. In the super-
vised setting, WSI systems provide GWS annotation
of their induced senses for the test corpus, which
is already labeled with the gold-standard GWS an-
notations. Then, a portion of the test corpus with
gold standard annotations is used to build a mapping
from induced senses to the reference sense inven-
193
Term PoS # senses Avg. # Senses
per Instance
add verb 6 4.18
ask verb 7 5.98
win verb 4 3.98
argument noun 7 5.18
interest noun 7 5.12
paper noun 7 5.54
different adj. 5 4.98
important adj. 5 4.82
Table 1: The terms from the GWS dataset (Erk et
al., 2009) used in this evaluation
tory using one of the three algorithms described in
Section 4. The remaining, held-out test corpus in-
stances have their induced senses converted to the
gold standard sense inventory and the sense label-
ings are evaluated for the three objectives from Sec-
tion 2. In our experiments we divide the reference
corpus into five evenly-sized segments and then use
four segments (80% of the test corpus) for construct-
ing the mapping and then evaluate the converted
GWS annotations of the remaining segment.
5.1 Graded Annotation Data
The gold standard GWS annotations are derived
from a subset of the GWS data provided by Erk et
al. (2009). Here, three annotators rated the applica-
bility of all WordNet 3.0 senses of a word in a single
sentence context. Ratings were done using a 5-point
ordinal ranking according to the judgements from 1
? this sense is not applicable to 5 ? this usage exactly
reflects this sense. Annotators used a wide-range of
responses, leading to many applicable senses per in-
stance. We selected the subset of the GWS dataset
where each term has 50 annotated contexts, which
were distributed evenly between SemCor (Miller et
al., 1993) and the SENSEVAL-3 lexical substitution
corpus (Mihalcea et al, 2004). Table 1 summarizes
the target terms in this context.
To prepare the data for evaluation, we constructed
the gold standard GWS annotations using the mean
applicability ratings of all three annotators for each
context. Senses that received a mean rating of 1 (not
applicable) were not listed in gold standard labeling
for that instance. All remaining responses were nor-
malized to sum to 1.
5.2 Model Configuration
For consistency, all three WSI models were trained
using the same reference corpus. We used a 2009
snapshot of Wikipedia,2 which was PoS tagged and
lemmatized using the TreeTagger (Schmid, 1994).
All of target terms occurred over 12,000 times. The
G-test between terms was computed using a three-
sentence sliding window within each article in the
corpus. The Dice coefficient was calculated using a
single sentence as context.
For all three models, we performed a limited grid
search to find the best performing system param-
eters, within reasonable computational limits. We
summarize the parameters and models, selecting the
configuration with the highest average Perception
score. For all models, the applicability ratings for
each instance are normalized to sum to 1.
Model Parameter Range Selected
Squares ?={0.008, 0.009, . . . , 0.092} 0.037?={0.25, 0.30, . . . , 0.50, 0.55} 0.55
Link ?={400, 500, . . . , 900, 1000} 500
UoY
P1={10, 20} 20
P2={10, 20, 30} 20
P3={0.2, 0.3, 0.4} 0.3
P4={0.4, 0.6, 0.8} 0.4
5.3 Baselines
Prior WSI evaluations have used the Most Frequent
Sense (MFS) labeling a strong baseline in the super-
vised WSD task. For the GWS setting, we consider
five other baselines that select one, some, or all of
the sense of the target word, with different ordering
strategies. In the six baselines, each instance is la-
beled as follows:
MFS: the most frequent sense of the word
RS: a single, randomly-selected sense
ASF: all senses, ranked in order of frequency starting
with the most frequent
ASR: all senses, randomly ranked
ASE: all senses, ranked equally
RSM: a random number of senses, ranked arbitrarily
To establish applicability values from a ranking of n
senses, we set applicability to the ith ranked sense of
(n?i)+1
?n
k=1 k
, where rank 1 is the highest ranked sense.
2http://wacky.sslmit.unibo.it/
194
Agirre et al (2006) Mapping Graded Mapping Distribution Mapping
Model D R P D R P D R P Recall
Squares 0.192 -0.024 0.382 0.198 0.555 0.504 0.879 0.562 0.925 0.560
Link 0.282 0.081 0.454 0.335 0.436 0.528 0.854 0.503 0.907 0.800
UoY 0.238 0.116 0.445 0.244 0.486 0.528 0.848 0.528 0.907 0.940
Table 2: Average performance of the three WSI models according to Detection, Ranking, and Percetion
Baseline Detection Ranking Perception
MFS 0.204 0.334 0.469
RS 0.167 -0.036 0.363
ASF 0.846 0.218 0.830
ASR 0.846 0.006 0.776
ASE 0.846 0.000 0.862
RSM 0.546 0.005 0.632
Table 3: Average performance of the six baselines
5.4 Results and Discussion
Each WSI model was trained and then used to la-
bel the sense of each target term in the GWS corpus.
The three sense-mapping procedures were then ap-
plied to the induced sense labels on the held-out in-
stances to perform a comparison in the graded sense
annotations. Table 2 reports the performance for the
three evaluation measures for each model and map-
ping configuration on all instances where the sense
mapping is defined. The sense mapping is unde-
fined when (1) a WSI model cannot match an in-
stance?s features to any of its senses therefore leaves
the instance unannotated or (2) when an instance is
labeled with an is sense not seen in the training data.
Therefore, we report the additional statistic, Recall,
that indicates the percentage of instances that were
both labeled by the WSI model and mapped to gs
senses. Table 3 summarizes the baselines? perfor-
mance.
The results show three main trends. First, intro-
ducing applicability into the sense mapping process
noticeably improves performance. For almost all
models and scores, using the Graded Mapping im-
proves performance a small amount. However, the
largest increase comes from using the Distribution
mapping where induced senses are represented as
distributions over the gold standard senses.
Second, performance was well ahead of the base-
lines across the three evaluations, when consider-
ing the models? best performances. The Squares
and Link models were able to outperform the base-
lines that list all senses on the Detection objec-
tive, which the UoY model only improves slightly
from this baseline. For the Ranking objective, all
models substantially outperform the best baseline,
MFS; and similarly, for the Perception objective,
all models outperform the best performing baseline,
ASE. Overall, these performance suggest that in-
duce senses can be successfully used to produce
quality GWS annotations.
Third, the WSI models themselves show signif-
icant differences in their recall and multi-labeling
frequencies. The Squares model is only able to la-
bel approximately 56% of the GWS instances due to
sparseness in its sense representation. Indeed, only
12 of its 237 annotated instances received more than
one sense label, revealing that the model?s perfor-
mance is mostly based on correctly identifying the
primary sense in a context and not on identifying
the less applicable senses. The UoY model shows a
similar trend, with most instances being assigned a
median of 2 senses. However, its sense representa-
tion is sufficiently dense to have the highest recall of
any of the models. In contrast to the other two mod-
els, the Link model varies significantly in the num-
ber of induced senses assigned: ?argument,? ?ask,?
?different,? and ?win? were assigned over 60 senses
on average to each of their instances, with ?differ-
ent? having an average of 238, while the remaining
terms were assigned under two senses on average.
Furthermore, the results also revealed two unex-
pected findings. First, the ASE baseline performed
unexpectedly high in Perception, despite its assign-
ment of uniform applicability to all senses. We hy-
pothesize this is due to the majority of instances in
the GWS dataset being labeled with most of a word?s
senses, as indicated by Table 1, which results in their
195
perceptibilities becoming normalized to small val-
ues. Because the ASE solution has applicability rat-
ings for all senses, normalization brings the ratings
close to those of the gold standard solution, and fur-
thermore, the difference in score between applicable
and inapplicable senses become too small to signifi-
cantly affect the resulting cosine similarity. As an al-
ternate model, we reevaluated the baselines against
the gold standard using the Jensen-Shannon diver-
gence as proposed by Erk and McCarthy (2009).
Again, ASE is still the highest performing baseline
on Perception. The high performance for both eval-
uation measures suggests that an alternate measure
may be better suited for quantifying the difference
in solutions? GWS applicabilities.
Second, performance was higher on the Percep-
tion task than on Ranking, the former of which was
anticipated being more difficult. We attribute the
lower Ranking performance to two factors. First,
the GWS data contains main tied rank senses; how-
ever, ties in sense ranks after the mapping process
are relatively rare, which reduces ?. Second, in-
stances in the GWS often have senses within close
applicability ranges. When scoring an induced an-
notation that swaps the applicability, the Perception
is less affected by the small change in applicability
magnitude, whereas Ranking is more affected due to
the change in ordering.
6 Conclusion and Future Work
GWS annotations offer great potential for reli-
ably annotating using fine-grained sense invento-
ries, where word instance may elicit several concur-
rent meanings. Given the expense of creating an-
notated training corpora with sufficient examples of
the graded senses, WSI offers significant promise for
learning senses automatically while needing only a
small amount GWS annotated data to learn the sense
mapping for a WSD task.
In this paper, we have carried out an initial study
on the performance of WSI systems on a GWS an-
notation task. Our primary contribution is an end-
to-end framework for mapping and evaluating in-
duced GWS data. We first proposed three objectives
for graded sense annotation along with correspond-
ing evaluation measures that reliably convey the ef-
fectiveness given the nature of GWS annotations.
Second, we proposed two new mapping procedures
that use graded sense applicability for converting in-
duced senses into a reference sense inventory. Using
three graph-based WSI models, we demonstrated
that incorporating graded sense applicability into the
sense mapping significantly improves GWS perfor-
mance over the commonly used method of Agirre et
al. (2006). Furthermore, our study demonstrated the
potential of WSI systems, showing that all the mod-
els were able to outperform all six of the proposed
baseline on the Ranking and Perception objectives.
Our findings raise several avenues for future
work. First, our study only considered three graph-
based WSI models; future work is needed to as-
sess the capabilities other WSI approaches, such as
vector-based or Bayesian. We are also interested in
comparing the performance of the Link model with
other recently developed all-words WSI approaches
such as Van de Cruys and Apidianaki (2011).
Second, the proposed evaluation relies on a su-
pervised mapping to the gold standard sense inven-
tory, which has potential to lose information and in-
correctly map new senses not in the gold standard.
While unsupervised clustering evaluations such as
the V-measure (Rosenberg and Hirschberg, 2007)
and paired Fscore (Artiles et al, 2009) are capable
of evaluating without such a mapping, future work
is needed to test extrinsic soft clustering evaluations
such as BCubed (Amigo? et al, 2009) or develop
analogous techniques that take into account graded
class membership used in GWS annotations.
Last, we note that our setup normalized the GWS
ratings into probability distribution, which is stan-
dard in the SemEval evaluation setup. However, this
normalization incorrectly transforms GWS annota-
tions where no predominant sense was rated at the
highest value, e.g., an annotation of only two senses
rated as 3 on a scale of 1 to 5. While these percepti-
bilities may be left unnormalized, it is not clear how
to compare the induced GWS annotations with such
mid-interval values, or when the rating scale of the
WSI system is potentially unbounded. Future work
is needed both in GWS evaluation and in quantify-
ing applicability along a range in GWS-based WSI
systems to address this issue.
All models and data will be released as a part of
the S-Space Package (Jurgens and Stevens, 2010).3
3https://github.com/fozziethebeat/S-Space
196
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations, pages 7?12.
ACL, June.
Eneko Agirre, David Mart??nez, Oier o? de Lacalle, and
Aitor Soroa. 2006. Evaluating and optimizing the pa-
rameters of an unsupervised graph-based WSD algo-
rithm. In Proceedings of TextGraphs: the First Work-
shop on Graph Based Methods for Natural Language
Processing, pages 89?96. Association for Computa-
tional Linguistics.
Yong-Yeol Ahn, James P. Bagrow, and Sune Lehmann.
2010. Link communities reveal multiscale complexity
in networks. Nature, (466):761?764, August.
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4):461?486.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of EMNLP, pages 534?542. Association
for Computational Linguistics.
Chris Biemann. 2006. Chinese whispers: an efficient
graph clustering algorithm and its application to natu-
ral language processing problems. In Proceedings of
the First Workshop on Graph Based Methods for Nat-
ural Language Processing, pages 73?80. Association
for Computational Linguistics.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the ACL-02 workshop on
Word sense disambiguation: recent successes and fu-
ture directions - Volume 8, WSD ?02, pages 32?39,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1, pages 440?449.Association for
Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1, pages 10?18.Association
for Computational Linguistics.
Christine Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations. Association for Computational Lin-
guistics.
David Jurgens. 2011. Word sense induction by com-
munity detection. In Proceedings of Sixth ACL Work-
shop on Graph-based Methods for Natural Language
Processing (TextGraphs-6). Association for Computa-
tional Linguistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
Uoy: Graphs of unambiguous vertices for word sense
induction and disambiguation. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 355?358. Association for Computational Lin-
guistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for computational Linguistics (EACL 2012).
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-
2010 task 14: Word sense induction & disambigua-
tion. In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68. Association for
Computational Linguistics.
Diana McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. Making Sense of Sense:
Bringing Psycholinguistics and Computational Lin-
guistics Together, page 17.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The senseval-3 english lexical sample task.
In Senseval-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text, pages 25?28. Barcelona, Spain, Association for
Computational Linguistics.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance.
In Proceedings of the workshop on Human Language
Technology, pages 303?308. Association for Compu-
tational Linguistics.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing
word senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 116?
126. Association for Computational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
197
sense distinctions, both manually and automatically.
Natural Language Engineering, 13(02):137?163.
Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense anno-
tation of polysemous words by multiple annotators. In
Proceedings of Seventh International Conference on
Language Resources and Evaluation (LREC-7).
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL). ACL, June.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing.
Sidney Siegel and N. John Castellan Jr. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, second edition.
Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent Semantic Word Sense Induction and Disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies (ACL/HLT), pages 1476?
1485.
198
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 356?364,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 2: Measuring Degrees of Relational Similarity
David A. Jurgens
Department of Computer Science
University of California, Los Angeles
jurgens@cs.ucla.edu
Saif M. Mohammad
Emerging Technologies
National Research Council Canada
saif.mohammad@nrc-cnrc.gc.ca
Peter D. Turney
Emerging Technologies
National Research Council Canada
peter.turney@nrc-cnrc.gc.ca
Keith J. Holyoak
Department of Psychology
University of California, Los Angeles
holyoak@lifesci.ucla.edu
Abstract
Up to now, work on semantic relations has fo-
cused on relation classification: recognizing
whether a given instance (a word pair such as
virus:flu) belongs to a specific relation class
(such as CAUSE:EFFECT). However, instances
of a single relation class may still have signif-
icant variability in how characteristic they are
of that class. We present a new SemEval task
based on identifying the degree of prototypi-
cality for instances within a given class. As
a part of the task, we have assembled the first
dataset of graded relational similarity ratings
across 79 relation categories. Three teams
submitted six systems, which were evaluated
using two methods.
1 Introduction
Relational similarity measures the degree of corre-
spondence between two relations, where instance
pairs that have high relational similarity are said to
be analogous, i.e., to express the same relation (Tur-
ney, 2006). However, a class of analogous relations
may still have significant variability in the degree of
relational similarity of its members. Consider the
four word pairs dog:bark, cat:meow, floor:squeak,
and car:honk. We could say that these four X:Y
pairs are all instances of the semantic relation EN-
TITY:SOUND; that is, X is an entity that character-
istically makes the sound Y . Within a class of anal-
ogous pairs, certain pairs are more characteristic of
the relation. For example, many would agree that
dog:bark and cat:meow are better prototypes of the
ENTITY:SOUND relation than floor:squeak. Our task
requires automatic systems to quantify the degree of
prototypicality of a target pair by measuring the re-
lational similarity between it and pairs that are given
as defining examples of a particular relation.
So far, most work in semantic relations has fo-
cused on differences between relation categories for
classifying new relation instances. Past SemEval
tasks that use relations have focused largely on dis-
crete classification (Girju et al, 2007; Hendrickx et
al., 2010) and paraphrasing the relations connecting
noun compounds with a verb (Butnariu et al, 2010),
which is also a form of discrete classification due to
the lack of continuous degrees. However, there is
some loss of information in any discrete classifica-
tion of semantic relations. Furthermore, while some
discrete classifiers provide a degree of confidence or
probability for a relation classification, there is no
a priori reason that such values would correspond
to human prototypicality judgments. Our proposed
task is distinct from these past tasks in that we fo-
cus on measuring the degree of relational similarity.1
A graded measure of the degree of relational simi-
larity would tell us that dog:bark is more similar to
cat:meow than to floor:squeak. The discrete classifi-
cation ENTITY:SOUND drops this information.
Systems that are successful at identifying degrees
of relation similarity can have a significant impact
where an application must choose between multi-
ple instances of the same relation. We illustrate
this with two examples. First, consider a rela-
tional search task (Cafarella et al, 2006). A user
of a relational search engine might give the query,
1Task details and data are available at
https://sites.google.com/site/semeval2012task2/ .
356
Subcategory Relation name Relation schema Paradigms Responses
8(e) AGENT:GOAL ?Y is the goal of X? pilgrim:shrine patient:health
assassin:death runner:finish
climber:peak astronaut:space
5(e) OBJECT:TYPICAL ACTION ?an X will typically Y ? glass:break ice:melt
soldier:fight lion:roar
juggernaut:crush knife:stab
4(h) DEFECTIVE ?an X is is a defect in Y ? fallacy:logic pimple:skin
astigmatism:sight ignorance:learning
limp:walk tumor:body
Table 1: Examples of the three manually selected paradigms and the corresponding pairs generated by Turkers.
?List all things that are part of a car.? SemEval-
2007 Task 4 proposed that a relational search engine
would use semantic relation classification to answer
queries like this one. For this query, a classifier that
was trained with the relation PART:WHOLE would be
used. However, a system for measuring degrees of
relational similarity would be better suited to rela-
tional search than a discrete classifier, because the
relational search engine could then rank the output
list in order of applicability. For the same query, the
search engine could rank each item X in descending
order of the degree of relational similarity between
X:car and a training set of prototypical examples of
the relation PART:WHOLE. This would be analogous
to how standard search engines rank documents or
web pages in descending order of relevance to the
user?s query.
As a second example, consider the role of rela-
tional similarity in analogical transfer. When faced
with a new situation, we look for an analogous sit-
uation in our past experience, and we use analogi-
cal inference to transfer information from the past
experience (the source domain) to the new situation
(the target domain) (Gentner, 1983; Holyoak, 2012).
Analogy is based on relational similarity (Gentner,
1983; Turney, 2008). The degree of relational sim-
ilarity in an analogy is indicative of the likelihood
that transferred knowledge will be applicable in the
target domain. For example, past experience tells us
that a dog barks to send a signal to other creatures. If
we transfer this knowledge to a new experience with
a cat meowing, we can predict that the cat is sending
a signal, and we can act appropriately with that pre-
diction. If we transfer this knowledge to a new expe-
rience with a floor squeaking, we might predict that
the floor is sending a signal, which might lead us to
act inappropriately. If we have a choice among sev-
eral source analogies, usually the source pair with
the highest degree of relational similarity to the tar-
get pair will prove to be the most useful analogy in
the target domain, providing practical benefits be-
yond discrete relational classification.
2 Task Description
Here, we describe our task and the two-level hierar-
chy of semantic relation classes used for the task.
2.1 Objective
Our task is to rate word pairs by the degree to
which they are prototypical members of a given re-
lation class. The relation class is specified by a
few paradigmatic (highly prototypical) examples of
word pairs that belong to the class and also by a
schematic representation of the relation class. The
task requires comparing a word pair to the paradig-
matic examples and/or the schematic representation.
For example, suppose the relation class is REVERSE.
We may specify this class by the paradigmatic ex-
amples attack:defend, buy:sell, love:hate, and the
schematic representation ?X is the reverse act of
Y ? or ?X may be undone by Y .? Given a pair
such as repair:break, we compare this pair to the
paradigmatic examples and/or the schematic repre-
sentation, in order to estimate its degree of prototyp-
icality. The challenges are (1) to infer the relation
from the paradigmatic examples and identify what
relational or featural attributes best characterize that
relation, and (2) to identify the relation of the given
pair and rate how similar it is to that shared by the
paradigmatic examples.
357
2.2 Relation Categories
Researchers in psychology and linguistics have con-
sidered many different categorizations of semantic
relations. The particular relation categorization is
often driven by both the type of data and the in-
tended application. Nastase and Szpakowicz (2003)
propose a two-level hierarchy for noun-modifier re-
lations, which has been widely used (Nakov and
Hearst, 2008; Nastase et al, 2006; Turney and
Littman, 2005; Turney, 2005). Others have used
classifications based on the requirements for a spe-
cific task, such as Information Extraction (Pantel
and Pennacchiotti, 2006) or biomedical applications
(Stephens et al, 2001).
We adopt the relation classification scheme of Be-
jar et al (1991), which includes ten high-level cat-
egories (e.g., CAUSE-PURPOSE and SPACE-TIME).
Each category has between five and ten more re-
fined subcategories (e.g., CAUSE-PURPOSE includes
CAUSE:EFFECT and ACTION:GOAL), for a total of
79 distinct subcategories. Although these cate-
gories do not reflect all possible semantic rela-
tions, they greatly expand the coverage of rela-
tion types from those used in past relation-based
SemEval tasks (Girju et al, 2007; Hendrickx et
al., 2010), which used only seven and nine re-
lation types, respectively. Furthermore, the clas-
sification includes many of the fundamental rela-
tions, e.g., TAXONOMIC and PART:WHOLE, while
also including relations between a variety of parts
of speech and less common relations, such as REF-
ERENCE (e.g., SIGN:SIGNIFICANT) and NONAT-
TRIBUTE (e.g., AGENT:ATYPICAL ACTION). Using
such a large relation class inventory enables evalu-
ating the generality of an approach, while still mea-
suring performance on commonly used relations.
3 Task Data
We constructed a new data set for the task, in which
word pairs are manually classified into relation cat-
egories. Word pairs within a category are manually
distinguished according to how well they represent
the category; that is, the degree to which they are
relationally similar to paradigmatic members of the
given semantic relation class. Paradigmatic mem-
bers of a class were taken from examples provided
by Bejar et al (1991). Due to the large number of
Question 1: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
What relation best describes these X:Y word pairs?
(1) ?X worships/reveres Y ?
(2) ?X seeks/desires/aims for Y ?
(3) ?X harms/destroys Y ?
(4) ?X uses/exploits/employs Y ?
Question 2: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
These X:Y pairs share a relation, ?X R Y ?. Give four ad-
ditional word pairs that illustrate the same relation, in the
same order (X on the left, Y on the right). Please do not
use phrases composed of two or more words in your ex-
amples (e.g., ?racing car?). Please do not use names of
people, places, or things in your examples (e.g., ?Europe?,
?Kleenex?).
(1) :
(2) :
(3) :
(4) :
Figure 1: An example of the two questions for Phase 1.
annotations needed, we used Amazon Mechanical
Turk (MTurk),2 which is a popular choice in com-
putational linguistics for gathering large numbers of
human responses to linguistic questions (Snow et al,
2008; Mohammad and Turney, 2010). We refer to
the MTurk workers as Turkers.
The data set was built in two phases. In the first
phase, Turkers were given three paradigmatic exam-
ples of a subcategory and asked to create new pairs
that instantiate the same relation as the paradigms.
In the second phase, people were asked to distin-
guish the new pairs from the first phase according to
the degree to which they are good representatives of
the given subcategory.
Phase 1 In the first phase, we built upon the
paradigmatic examples of Bejar et al (1991), who
provided one to ten examples for each subcategory.
From these examples, we manually selected three
instances to use as seeds for generating new exam-
ples, adding examples when a subcategory had less
than three. The examples were selected to be bal-
anced across topic domains so as not to bias the
Turkers. For each subcategory, we manually created
a schematic representation of the relation for the ex-
amples. Table 1 gives three examples.
2https://www.mturk.com/
358
To gather new examples of each subcategory,
a two-part questionnaire was presented to Turk-
ers (see Figure 1). In the first part, Turkers were
shown the three paradigm word pairs for a sub-
category along with a list of four relation descrip-
tions (schematic representations of possible rela-
tions). One of the four schematic representations
accurately described the three paradigm pairs and
the other three schematics were distractors (con-
founding descriptions). Turkers were asked to se-
lect which of the four schematic representations best
matched the paradigms. The first part of the ques-
tionnaire serves as quality control by ensuring that
the Turker is capable of recognizing the relation. An
incorrect answer to the question is used to recog-
nize and eliminate confused or negligent responses,
which were approximately 7% of the responses.
In the second part of the Phase 1 questionnaire,
Turkers were shown the three prototypes again and
asked to generate four word pairs that expressed the
same relation. Turkers were directed to be mindful
of the order of the words in each pair, as reversed
orderings can have very different degrees of proto-
typicality in the case of directional relations.
The Turkers provided a total of 3160 additional
examples for the 79 subcategories, 2905 of which
were unique. We applied minor manual correction
to remove spelling errors, which reduced the total
number of examples to 2823. A median of 38 exam-
ples were found per subcategory with a maximum of
40 and minimum of 23. We note that Phase 1 gathers
both high and low quality examples of the relation,
which were all included to capture different degrees
of prototypicality.
We included an additional 395 pairs by randomly
sampling five instances of each subcategory and
creating a new pair from the reversed arguments,
i.e., adding pair Y :X to the subcategory contain-
ing X:Y . Adding reversals was inspired by an ob-
servation during Phase 1 that reversed pairs would
occasionally be added by the Turkers themselves.
We were curious to see what impact reversals would
have on Turker responses and on the output of au-
tomatic systems. Reversals should reveal order sen-
sitivity with a strongly directional relation, such as
PART:WHOLE, but also perhaps there is order sensi-
tivity with more symmetric relations, such as SYN-
ONYMY. Phase 1 produced a total of 3218 pairs.
Question 1: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
What relation best describes these X:Y word pairs?
(1) ?X worships/reveres Y ?
(2) ?X seeks/desires/aims for Y ?
(3) ?X harms/destroys Y ?
(4) ?X uses/exploits/employs Y ?
Question 2: Consider the following word pairs: pil-
grim:shrine, hunter:quarry, assassin:victim, climber:peak.
These X:Y pairs share a relation, ?X R Y ?. Now consider
the following word pairs:
(1) pig:mud
(2) politician:votes
(3) dog:bone
(4) bird:worm
Which of the above numbered word pairs is the MOST illus-
trative example of the same relation ?X R Y ??
Which of the above numbered word pairs is the LEAST illus-
trative example of the same relation ?X R Y ??
Note: In some cases, a word pair might be in reverse order.
For example, tree:forest is in reverse order for the relation
?X is made from a collection of Y ?. The correct order would
be forest:tree; a forest is made from a collection of trees.
You should treat reversed pairs as BAD examples of the given
relation.
Figure 2: An example of the two questions for Phase 2.
Phase 2 In the second phase, the response pairs
from Phase 1 were ranked according to their pro-
totypicality. We opted to create a ranking using
MaxDiff questions (Louviere, 1991). MaxDiff is a
choice procedure consisting of a question about a
target concept and four or five alternatives. A partic-
ipant must choose both the best and worse answers
from the given alternatives.
MaxDiff is a strong alternative to creating a rank-
ing from standard rating scales, such as the Likert
scale, because it avoids scale biases. Furthermore
MaxDiff is more efficient than other choice proce-
dures such as pairwise comparison, because it does
not require comparing all pairs.
Like Phase 1, Phase 2 was performed using a two-
part questionnaire. The first question was identical
to that of Phase 1: four examples of the same re-
lation subcategory generated in Phase 1 were pre-
sented and the Turker was asked to select the cor-
rect relation from a list of four options. This first
question served as a quality control measure for en-
suring the Turker could properly identify the rela-
tion in question and it also served as a hint, guiding
359
the Turker toward the intended understanding of the
shared relation underlying the three paradigms. In
the second part, the Turker selects the most and least
illustrative example of that relation from among
the four examples of pairs generated by Turkers in
Phase 1.
We aimed for five Turker responses for each
MaxDiff question but averaged 4.73 responses for
each MaxDiff question in a subcategory, with a
minimum of 3.45 responses per MaxDiff question.
Turkers answered a total of 48,846 questions over a
period of five months, of which 6,536 (13%) were
rejected due to a missing answer or an incorrect re-
sponse to the first question.
3.1 Measuring Prototypicality
The MaxDiff responses were converted into the
prototypicality scores using a counting procedure
(Orme, 2009). For each word pair, the prototyp-
icality is scored as the percentage of times it is
chosen as most illustrative minus the percentage of
times it is chosen as least illustrative (see Figure 2).
While methods such as hierarchical Bayes models
can be used to compute a numerical rank from the
responses, we found the counting method to produce
very reasonable results.
3.2 Data Sets
The 79 subcategories were divided into training
and testing segments. Ten subcategories were pro-
vided as training with both the Turkers? MaxDiff
responses and the computed prototypicality ratings.
The ten training subcategories were randomly se-
lected. The remaining 69 subcategories were used
for testing. All data sets are now released on the task
website under the Creative Commons 3.0 license.3
Participants were given the list of all pairs gath-
ered in Phase 1 and the Phase 2 responses for the 10
training subcategories. Phase 2 responses for the 69
test categories were not made available. Participants
also had access to the set of questionnaire materials
provided to the Turkers, the full list of paradigmatic
examples provided by Bejar et al (1991), and the
confounding schema relations from the initial ques-
tions in Phase 1 and Phase 2, which might serve as
negative training examples.
3http://creativecommons.org/licenses/by/3.0/
4 Evaluation
Systems are given examples of pairs from a single
category and asked to provide numeric ratings of the
degree of relational similarity for each pair relative
to the relation expressed in that category.
4.1 Scoring
Spearman?s rank correlation coefficient, ?, and a
MaxDiff score were used to evaluate the systems.
For Spearman?s ?, the prototypicality rating of each
pair is used to build a ranking of all pairs in a sub-
category. Spearman?s ? is then computed between
the pair rankings of a system and the gold standard
ranking. This evaluation abstracts away from com-
paring the numeric values so that only their relative
ordering in prototypicality is measured.
In the second scoring procedure, we measure the
accuracy of a system at answering the same set of
MaxDiff questions as answered by the Turkers in
Phase 2 (see Figure 2). Given the four word pairs,
the system selects the pair with the lowest numeri-
cal rating as least illustrative and the pair with the
highest numerical rating as most illustrative. Ties
in prototypicality are broken arbitrarily. Accuracy is
measured as the percentage of questions answered
correctly. An answer is considered correct when it
agrees with the majority of the Turkers. In some
cases, two answers may be considered correct. For
example, when five Turkers answer a given MaxD-
iff question, two Turkers might choose one pair as
the most illustrative and two other Turkers might
choose another pair as the most illustrative. In this
case, both pairs would count as correct choices for
the most illustrative pair.
4.2 Baselines
We consider two baselines for evaluation: Random
and PMI. The Random baseline rates each pair in a
subcategory randomly. The expected Spearman cor-
relation for Random ratings is zero. The expected
MaxDiff score for Random ratings would be 25%
(because there are four word pairs to choose from
in Phase 2) if there were always a unique majority,
but it is actually about 31%, due to cases where two
pairs both get two votes from the Turkers.
Given a MaxDiff question, a Turker might select
the pair whose words are most strongly associated
360
Team Members System Description
Beneme?rita
Universidad
Auto?noma de
Puebla (Me?xico)
(BUAP)
Mireya T. Vidal,
Darnes V. Ayala,
Jose A.R. Ortiz,
Azucena M.
Rendon,
David Pinto, and
Saul L. Silverio
BUAP Each pair is represented as a vector over multiple features: lexical,
intervening words, WordNet relations between the pair, and syntactic
features such as part of speech and morphology. Prototypicality is
based on cosine similarity with the class?s pairs.
University of Texas at
Dallas (UTD)
Bryan Rink and
Sanda Harabagiu
NB Unsupervised learning identifies intervening patterns between all word
pairs. Each pattern is then ranked according to its subcategory
specificity by learning a generative model from patterns to word pairs.
Prototypicality ratings are based on confidence that the highest scoring
pattern found for a pair belongs to the subcategory.
SVM Intervening patterns are found using the same method as UTD-NB.
Word pairs are then represented as feature vectors of matching
patterns. An SVM classifier is trained using a subcategory?s pairs as
positive training data and all other pairs as negative. Prototypicality
ratings are based on SVM confidence of class inclusion.
University of
Minnesota, Duluth
(Duluth)
Ted Pedersen V0 WordNet is used to build the set of concepts connected by WordNet
relations to the pairs? words. Prototypicality is estimated using the
vector similarity of the concatenated glosses.
V1 Same procedure as V0, with one further expansion to related concepts.
V2 Same procedure as V0, with two further expansions to related concepts.
Table 2: Descriptions of the participating teams and systems.
as the most illustrative and the least associated as
the least illustrative. Therefore, we propose a sec-
ond baseline where pairs are rated according to their
Pointwise Mutual Information (PMI) (Church and
Hanks, 1990), which measures the statistical asso-
ciation between two words. For this baseline, the
prototypicality rating given to a word pair is simply
the PMI score for the pair. For two terms x and y,
PMI(x, y) is defined as log2
(
p(x,y)
p(x)p(y)
)
where p(?)
denotes the probability of a term or pair of terms.
The PMI score was calculated using the method of
Turney (2001) on a corpus of approximately 50 bil-
lion tokens, indexed by the Wumpus search engine.4
To calculate p(x, y), we recorded all co-occurrences
of both terms within a ten-word window.
5 Systems
Three teams submitted six systems for evaluation.
Table 2 summarizes the teams and systems. Two
teams (BUAP and UTD) based their approaches on
discovering relation-specific patterns for each cat-
egory, while the third team (Duluth) used vector
space comparisons of the glosses related to the pairs.
4http://www.wumpus-search.org/
No single system was able to achieve superior per-
formance on all subcategories. Table 3 reports the
averages across all subcategories for Spearman?s ?
and MaxDiff accuracy. Five systems were able to
perform above the Random baseline, while only one
system, UTD-NB, consistently performed above the
PMI baseline.
However, the average performance masks supe-
rior performance on individual subcategories. Ta-
ble 3 also reports the number of subcategories in
which a system obtained a statistically significant
Spearman?s ? with the gold standard ranking. De-
spite the low average performance, most models
were able to obtain significant correlation in multi-
ple subcategories. Furthermore, the significant cor-
relations for different systems were not always ob-
tained in the same subcategories. Across all subcat-
egories, 43 had a significant correlation at p < 0.05
and 27 at p < 0.01. The broad coverage of signifi-
cantly correlated subcategories spanned by the com-
bination of all systems and the PMI baseline sug-
gests that high performance on this task may be pos-
sible, but that adapting to each of the specific rela-
tion types may be very beneficial.
361
Team System Spearman?s ? # of Subcategories MaxDiff
p < 0.05 p < 0.01
BUAP BUAP 0.014 2 0 31.7
UTD NB 0.229 22 16 39.4
SVM 0.116 11 5 34.7
Duluth V0 0.050 9 3 32.4
V1 0.039 10 4 31.5
V2 0.038 7 3 31.1
Baselines Random 0.018 4 0 31.2
PMI 0.112 15 7 33.9
Table 3: Average Spearman?s ? and MaxDiff scores for all system across all 69 test subcategories. Columns 4 and 5
denote the number of subcategories with a Spearman?s ? that is statistically significant at the noted level of confidence.
Relation Class Random PMI BUAP UTD-NB UTD-SVM Duluth-V0 Duluth-V1 Duluth-V2
Class-Inclusion 0.057 0.221 0.064 0.233 0.093 0.045 0.178 0.168
Part-Whole 0.012 0.144 0.066 0.252 0.142 -0.061 -0.084 -0.054
Similar 0.026 0.094 -0.036 0.214 0.131 0.183 0.208 0.198
Contrast -0.049 0.032 0.000 0.206 0.162 0.142 0.120 0.051
Attribute 0.037 -0.032 -0.095 0.158 0.052 0.044 -0.003 0.008
Non-Attribute -0.070 0.191 0.009 0.098 0.094 0.079 0.066 0.074
Case Relations 0.090 0.168 -0.037 0.241 0.187 -0.011 -0.068 -0.115
Cause-Purpose -0.011 0.130 0.114 0.183 0.060 0.021 0.022 0.042
Space-Time 0.013 0.084 0.035 0.375 0.139 0.055 -0.004 0.040
Reference 0.142 0.125 -0.001 0.346 0.082 0.028 0.074 0.067
Table 4: Average Spearman?s ? correlation with the Turker rankings in each of the high-level relation categories, with
the highest average correlation for each subcategory shown in bold.
6 Discussion
Sensitivity to Pair Association The PMI base-
line performed much better than anticipated, outper-
forming all systems but UTD-NB on many of the
subcategories, despite treating all relations as direc-
tionless. Performance was highest in subcategories
where the X:Y pair might reasonably be expected
to occur together, e.g., FUNCTIONAL or CONTRA-
DICTORY. However, PMI benefits from the design
of our task, which focuses on rating pairs within a
given subcategory. In a different task that mixed
pairs from a variety of subcategories, PMI would
perform poorly, because it would assign high scores
to pairs of strongly associated words, regardless of
whether they belong to a given subcategory.
Difficulty of Specific Subcategories Performance
across the high-level categories was highly varied
between approaches. The category-level summary
shown in Table 4 reveals high-level trends in diffi-
culty across all submitted systems. The submitted
systems performed best for subcategories under the
Similar category, while the systems performed worst
for Non-Attribute subcategories.
As a further possibility of explaining performance
differences between subcategories, we considered
the hypothesis that the difficulty of a subcategory is
inversely proportional to the range of prototypicality
scores, i.e., subcategories with restricted ranges are
more difficult. However, we found that the difficulty
was uncorrelated with both the size of the interval
spanned by prototypicality scores and the standard
deviation of the scores.
Sensitivity to Argument Reversal The direction-
ality of a relation can significantly impact the rated
prototypicality of a pair whose arguments have been
reversed. As an approximate measure of the ef-
fect on prototypicality when a pairs? arguments are
reversed, we calculated the expected drop in rank
362
Spearman?s ?
Team System No Reversals With Reversals
BUAP BUAP -0.003 0.014
UTD NB 0.190 0.229
SVM 0.104 0.116
Duluth V0 0.062 0.050
V1 0.040 0.039
V2 0.046 0.038
Baselines Random 0.004 0.018
PMI 0.143 0.112
Table 5: Average pair ranking correlation for all subcate-
gories when reversed pairs are included and excluded.
between a pair and its reversed form. Based on
the Turker rankings, the SEQUENCE (e.g., preg-
nancy:birth) and FUNCTIONAL (e.g., weapon:knife)
subcategories exhibited the strongest sensitivity to
argument reversal, while ATTRIBUTE SIMILARITY
(e.g., rake:fork) and CONTRARY (e.g., happy:sad)
exhibited the least.
The inclusion of reversed pairs potentially adds
a small amount of noise to the relation identifica-
tion process for subcategories with directional rela-
tions. Two teams, BUAP and UTD, accounted for
relation directionality, while Duluth did not, which
resulted in the Duluth systems ranking reversed pairs
the same. Therefore, we conducted a post-hoc anal-
ysis of the impact of reversals by removing the re-
versed pairs from the computed prototypicality rank-
ings. Table 5 reports the resulting Spearman?s ?.
With Spearman?s ?, we can easily evaluate the im-
pact of the reversals, because we can delete a re-
versed pair without affecting anything else. For the
MaxDiff questions, if there is one reversal in a group
of four choices, then we need to delete the whole
MaxDiff question. Therefore we do not include the
MaxDiff score in Table 5.
Removing reversals decreased performance in the
three systems that were sensitive to pair order-
ing (BUAP, UTD-NB, and UTD-SVM), while only
marginally increasing performance in the three sys-
tems that ignored the ordering. The performance de-
crease in systems that use ordering suggests that the
reversed pairs are easily identified and ranked ap-
propriately low. As a further estimate of the models?
ability to correctly order reversals, we compared the
difference in a reversal?s rank for both a system?s
Team System RMSE
BUAP BUAP 256.07
UT Dallas NB 257.15
SVM 209.95
Baseline Random 227.25
Table 6: RMSE in estimating the difference in rank be-
tween a pair and its reversal in the gold standard.
ranking and the ranking computed from Turker Re-
sponses. Table 6 reports the Root Mean Squared
Error (RMSE) in ranking difference for the three
systems that took argument order into account. Al-
though not the best performing system, Table 6 indi-
cates that the UTD-SVM system was most able to
appropriately weight reversals? prototypicality. In
contrast, the UTD-NB system often had many pairs
tied for the lowest rank, which either resulted in pair
and its reversal being tied or having a much smaller
rank difference, thereby increasing its RMSE.
7 Conclusions
We have introduced a new task focused on rating the
degrees of prototypicality for word pairs sharing the
same relation. Participants first identify the relation
shared between example pairs and then rate the de-
gree to which each pair expresses that relation. As
a part of the task, we constructed a dataset of proto-
typicality ratings for 3218 word pairs in 79 different
relation categories.
Participating systems used combinations of
corpus-based, syntactic, and WordNet features, with
varying degrees of success. The task also included a
competitive baseline, PMI, which surpassed all but
one system. Several models obtained moderate per-
formance in select relation subcategories, but no one
approach succeeded in general, which introduces
much opportunity for future improvement. We also
hope that both the example pairs and their prototyp-
icality ratings will be a valuable data set for future
research in Linguistics as well as Cognitive Psychol-
ogy. All data sets for this task have been made pub-
licly available on the task website.
Acknowledgements
This research was supported by ONR grant
N000140810186.
363
References
Isaac I. Bejar, Roger Chaffin, and Susan E. Embretson.
1991. Cognitive and Psychometric Analysis of Ana-
logical Problem Solving. Springer-Verlag.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Dair-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval-2010), pages
39?44. Association for Computational Linguistics.
Michael J. Cafarella, Michele Banko, and Oren Etzioni.
2006. Relational web search. In WWW Conference.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Dedre Gentner. 1983. Structure-mapping: A theoretical
framework for analogy. Cognitive science, 7(2):155?
170.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proceedings of the
4th International Workshop on Semantic Evaluation
(SemEval-2007).
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O? Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. SemEval-2010 task 8: Multi-way classi-
fication of semantic relations between pairs of nom-
inals. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval-2010), pages
33?38. Association for Computational Linguistics.
Keith J. Holyoak. 2012. Analogy and relational reason-
ing. In Oxford handbook of thinking and reasoning,
pages 234?259. Oxford University Press.
Jordan J. Louviere. 1991. Best-worst scaling: A model
for the largest difference judgments. Working Paper.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
mechanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL-HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 26?34. Association for Com-
putational Linguistics.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proceedings of ACL, volume 8.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301. ACL Press Tilburg,, The Nether-
lands.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and wordnet-
based features. In Proceedings of AAAI, volume 21,
page 781.
Bryan Orme. 2009. Maxdiff analysis: Simple counting,
individual-level logit, and hb.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 113?120. Associa-
tion for Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
M. Stephens, M. Palakal, S. Mukhopadhyay, R. Raje,
J. Mostafa, et al 2001. Detecting gene relations from
medline abstracts. In Pacific Symposium on Biocom-
puting, volume 6, pages 483?495. Citeseer.
Peter D. Turney and Michael L Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1?3):251?278.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML-2001), pages 491?502.
Peter D. Turney. 2005. Measuring semantic similarity
by latent relational analysis. In Proceedings of IJCAI,
pages 1136?1141.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter D. Turney. 2008. The latent relation mapping en-
gine: Algorithm and experiments. Journal of Artificial
Intelligence Research, 33(1):615?655.
364
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 222?231, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 12: Multilingual Word Sense Disambiguation
Roberto Navigli, David Jurgens and Daniele Vannella
Dipartimento di Informatica
Sapienza Universita` di Roma
Viale Regina Elena, 295 ? 00161 Roma Italy
{navigli,jurgens,vannella}@di.uniroma1.it
Abstract
This paper presents the SemEval-2013 task on
multilingual Word Sense Disambiguation. We
describe our experience in producing a mul-
tilingual sense-annotated corpus for the task.
The corpus is tagged with BabelNet 1.1.1,
a freely-available multilingual encyclopedic
dictionary and, as a byproduct, WordNet 3.0
and the Wikipedia sense inventory. We present
and analyze the results of participating sys-
tems, and discuss future directions.
1 Introduction
Word Sense Disambiguation (WSD), the task of au-
tomatically assigning predefined meanings to words
occurring in context, is a fundamental task in com-
putational lexical semantics (Navigli, 2009; Navigli,
2012). Several Senseval and SemEval tasks have
been organized in the past to study the performance
and limits of disambiguation systems and, even
more importantly, disambiguation settings. While
an ad-hoc sense inventory was originally chosen for
the first Senseval edition (Kilgarriff, 1998; Kilgarriff
and Palmer, 2000), later tasks (Edmonds and Cot-
ton, 2001; Snyder and Palmer, 2004; Mihalcea et
al., 2004) focused on WordNet (Miller et al, 1990;
Fellbaum, 1998) as a sense inventory. In 2007 the
issue of the fine sense granularity of WordNet was
addressed in two different SemEval disambiguation
tasks, leading to the beneficial creation of coarser-
grained sense inventories from WordNet itself (Nav-
igli et al, 2007) and from OntoNotes (Pradhan et al,
2007).
In recent years, with the exponential growth of
the Web and, consequently, the increase of non-
English speaking surfers, we have witnessed an up-
surge of interest in multilinguality. SemEval-2010
tasks on cross-lingual Word Sense Disambiguation
(Lefever and Hoste, 2010) and cross-lingual lexi-
cal substitution (Mihalcea et al, 2010) were orga-
nized. While these tasks addressed the multilin-
gual aspect of sense-level text understanding, they
departed from the traditional WSD paradigm, i.e.,
the automatic assignment of senses from an existing
inventory, and instead focused on lexical substitu-
tion (McCarthy and Navigli, 2009). The main factor
hampering traditional WSD from going multilingual
was the lack of a freely-available large-scale multi-
lingual dictionary.
The recent availability of huge collaboratively-
built repositories of knowledge such as Wikipedia
has enabled the automated creation of large-scale
lexical knowledge resources (Hovy et al, 2013).
Over the past few years, a wide-coverage multi-
lingual ?encyclopedic? dictionary, called BabelNet,
has been developed (Navigli and Ponzetto, 2012a).
BabelNet1 brings together WordNet and Wikipedia
and provides a multilingual sense inventory that cur-
rently covers 6 languages. We therefore decided to
put the BabelNet 1.1.1 sense inventory to the test
and organize a traditional Word Sense Disambigua-
tion task on a given English test set translated into 4
other languages (namely, French, German, Spanish
and Italian). Not only does BabelNet enable mul-
tilinguality, but it also provides coverage for both
lexicographic (e.g., apple as fruit) and encyclopedic
1http://babelnet.org
222
meanings (e.g., Apple Inc. as company). In this pa-
per we describe our task and disambiguation dataset
and report on the system results.
2 Task Setup
The task required participating systems to annotate
nouns in a test corpus with the most appropriate
sense from the BabelNet sense inventory or, alter-
natively, from two main subsets of it, namely the
WordNet or Wikipedia sense inventories. In contrast
to previous all-words WSD tasks we did not focus
on the other three open classes (i.e., verbs, adjec-
tives and adverbs) since BabelNet does not currently
provide non-English coverage for them.
2.1 Test Corpus
The test set consisted of 13 articles obtained from
the datasets available from the 2010, 2011 and 2012
editions of the workshop on Statistical Machine
Translation (WSMT).2 The articles cover different
domains, ranging from sports to financial news.
The same article was available in 4 different lan-
guages (English, French, German and Spanish). In
order to cover Italian, an Italian native speaker man-
ually translated each article from English into Ital-
ian, with the support of an English mother tongue
advisor. In Table 1 we show for each language the
number of words of running text, together with the
number of multiword expressions and named enti-
ties annotated, from the 13 articles.
2.2 Sense Inventories
2.2.1 BabelNet inventory
To semantically annotate all the single- and multi-
word expressions, as well as the named entities, oc-
curring in our test corpus we used BabelNet 1.1.1
(Navigli and Ponzetto, 2012a). BabelNet is a mul-
tilingual ?encyclopedic dictionary? and a semantic
network currently covering 6 languages, namely:
English, Catalan, French, German, Italian and Span-
ish. BabelNet is obtained as a result of a novel inte-
gration and enrichment methodology. This resource
is created by linking the largest multilingual Web en-
cyclopedia ? i.e., Wikipedia ? to the most popular
computational lexicon ? i.e., WordNet 3.0. The inte-
gration is performed via an automatic mapping and
2http://www.statmt.org/wmt12/
by filling in lexical gaps in resource-poor languages
with the aid of Machine Translation (MT).
Its lexicon includes lemmas which denote both
lexicographic meanings (e.g., balloon) and ency-
clopedic ones (e.g., Montgolfier brothers). The
basic meaning unit in BabelNet is the Babel
synset, modeled after the WordNet synset (Miller
et al, 1990; Fellbaum, 1998). A Babel synset
is a set of synonyms which express a concept
in different languages. For instance, { Globus
aerosta`ticCA, BalloonEN, Ae?rostationFR, BallonDE,
Pallone aerostaticoIT, . . . , Globo aerosta?ticoES } is
the Babel synset for the balloon aerostat, where the
language of each synonym is provided as a subscript
label. Thanks to their multilingual nature, we were
able to use Babel synsets as interlingual concept tags
for nouns occurring within text written in any of the
covered languages.
2.2.2 WordNet and Wikipedia inventories
Since BabelNet 1.1.1 is a superset of the Word-
Net 3.0 and Wikipedia sense inventories,3 once text
is annotated with Babel synsets, it turns out to
be annotated also according to either WordNet or
Wikipedia, or both. In fact, in order to induce the
WordNet annotations, one can restrict to those lex-
ical items annotated with Babel synsets which con-
tain WordNet senses for the target lemma; similarly,
for Wikipedia, we restrict to those items tagged with
Babel synsets which contain Wikipedia pages for the
target lemma.
2.3 BabelNet sense inventory validation
Because BabelNet is an automatic integration of
WordNet and Wikipedia, the resulting Babel synsets
may contain WordNet and Wikipedia entries about
different meanings of the same lemma. The under-
lying cause is a wrong mapping between the two
original resources. For instance, in BabelNet 1.1
the WordNet synset { arsenic, As, atomic number
33 } was mapped to the Wikipedia page AS (RO-
MAN COIN), and therefore the same Babel synset
mixed the two meanings.
In order to avoid an inconsistent semantic tag-
ging of text, we decided to manually check all the
mappings in BabelNet 1.1 between Wikipedia pages
3For version 1.1.1 we used the English Wikipedia database
dump from October 1, 2012.
223
Language Instances Single- Multiword Named Mean senses Mean senses
words expressions Entities per instance per lemma
BabelNet
English 1931 1604 127 200 1.02 1.09
French 1656 1389 89 176 1.05 1.15
German 1467 1267 21 176 1.00 1.05
Italian 1706 1454 211 41 1.22 1.27
Spanish 1481 1103 129 249 1.15 1.19
Wikipedia
English 1242 945 102 195 1.15 1.16
French 1039 790 72 175 1.18 1.14
German 1156 957 21 176 1.07 1.08
Italian 1977 869 85 41 1.20 1.18
Spanish 1103 758 107 248 1.11 1.10
WordNet
English 1644 1502 85 57 1.01 1.10
Table 1: Statistics for the sense annotations of the test set.
and WordNet senses involving lemmas in our En-
glish test set for the task. Overall, we identified 8306
synsets for 978 lemmas to be manually checked. We
recruited 8 annotators in our research group and as-
signed each lemma to two annotators. Each anno-
tator was instructed to check each Babel synset and
determine whether any of the following three opera-
tions was needed:
? Delete a mapping and separate the WordNet
sense from the Wikipedia page (like in the ar-
senic vs. AS (ROMAN COIN) example above);
? Add a mapping between a WordNet sense and a
Wikipedia page (formerly available as two sep-
arate Babel synsets);
? Merge two Babel synsets which express the
same concept.
After disagreement adjudication carried out by
the first author, the number of delete, add and merge
operations was 493, 203 and 43, respectively, for a
total of 739 operations (i.e., 8.8% of synsets cor-
rected). As a result of our validation of BabelNet
1.1, we obtained version 1.1.1, which is currently
available online.
2.4 Sense Annotation
To ensure high quality annotations, the annotation
process was completed in three phases. Because
BabelNet is a superset of both the WordNet and
Wikipedia sense inventories, all annotators used the
BabelNet 1.1.1 sense inventory for their respective
language. These BabelNet annotations were then
projected into WordNet and Wikipedia senses. An-
notation was performed by one native speaker each
for English, French, German and Spanish and, for
Italian, by two native speakers who annotated dif-
ferent subsets of the corpus.
In the first phase, each annotator was instructed
to inspect each instance to check that (1) the lemma
was tagged with the correct part of speech, (2) lem-
mas were correctly annotated as named entity or
multiword expressions, and (3) the meaning of the
instance?s lemma had an associated sense in Ba-
belNet. Based on these criteria, annotators removed
dozens of instances from the original data.
In the second phase, each instance in the En-
glish dataset was annotated using BabelNet senses.
To reduce the time required for annotation in the
other languages, the sense annotations for the En-
glish dataset were then projected onto the other four
224
Language Projected Valid Invalid
instances projections projections
French 1016 791 225
German 592 373 219
Italian 1029 774 255
Spanish 911 669 242
Table 2: Statistics when using the English sense an-
notations to project the correct sense of a lemma in
another language of the sentence-aligned test data.
languages using the sense translation API of Babel-
Net (Navigli and Ponzetto, 2012d). The projection
operated as follows, using the aligned sentences in
the English and non-English texts. For an instance
in the non-English text, all of the senses for that in-
stance?s lemma were compared with the sense an-
notations in the English sentence. If any of that
lemma?s senses was used in the English sentence,
then that sense was selected for the non-English
instance. The matching procedure operates at the
sentence-aligned level because the instances them-
selves are not aligned; i.e., different languages have
different numbers of instances per sentence, which
are potentially ordered differently due to language-
specific construction. Ultimately, this projection la-
beled approximately 50-70% of the instances in the
other four languages. Given the projected senses,
the annotators for the other four languages were then
asked to (1) correct the projected sense labels and
(2) annotate those still without senses.4 These anno-
tations were recorded in text in a stand-off file; no
further annotation tools were used.
The resulting sense projection proved highly use-
ful for selecting the correct sense. Table 2 shows
the number of corrections made by the annotators
to the projected senses, who changed only 22-37%
of the labels. While simple, the projection method
offers significant potential for generating good qual-
ity sense-annotated data from sentence-aligned mul-
tilingual text.
In the third phase, an independent annotator re-
viewed the labels for the high-frequency lemmas for
4During the second phase, annotators were also allowed
to add and remove instances that were missed during the first
phase, which resulted in small number of changes.
all languages to check for systematic errors and dis-
cuss possible changes to the labeling. This review
resulted in only a small number of changes to less
than 5% of the total instances, except for German
which had a slightly higher percentage of changes.
Table 1 summarizes the sense annotation statis-
tics for the test set. Annotators were allowed to use
multiple senses in the case of ambiguity, but en-
couraged to use a single sense whenever possible.
In rare cases, a lemma was annotated with senses
from a different lemma. For example, WordNet does
not contain a sense for ?card? that corresponds to
the penalty card meaning (as used in sports such
as football). In contrast, BabelNet has a sense for
?penalty card? from Wikipedia which, however, is
not mapped to the lemma ?card?. In such cases,
we add both the closest meaning from the original
lemma (e.g., the rectangual piece of paper sense in
WordNet) and the most suitable sense that may have
a different lemma form (e.g., PENALTY CARD).
Previous annotation studies have shown that,
when a fine-grained sense inventory is used, annota-
tors will often label ambiguous instances with multi-
ple senses if allowed (Erk and McCarthy, 2009; Jur-
gens and Klapaftis, 2013). Since BabelNet is a com-
bination of a fine-grained inventory (WordNet) and
contains additional senses from Wikipedia, we ana-
lyzed the average number of BabelNet sense anno-
tations per instance, shown in column six of Table 1.
Surprisingly, Table 1 suggests that the rate of mul-
tiple sense annotation varies significantly between
languages.
BabelNet may combine multiple Wikipedia pages
into a single BabelNet synset. As a result, when
Wikipedia is used as a sense inventory, instances are
annotated with all of the Wikipedia pages associated
with each BabelNet synset. Indeed, Table 1 shows a
markedly increased multi-sense annotation rate for
three languages when using Wikipedia.
As a second analysis, we considered the observed
level of polysemy for each of the unique lemmas.
The last column of Table 1 shows the average num-
ber of different senses seen for each lemma across
the test sets. In all languages, often only a single
sense of a lemma was used. Because the test set is
constructed based on topical documents, infrequent
lemmas mostly occurred within a single document
where they were used with a consistent interpreta-
225
tion. However, we note that in the case of lem-
mas that were only seen with a single sense, this
sense does not always correspond to the most fre-
quent sense as seen in SemCor.
3 Evaluation
Task 12 uses the standard definitions of precision
and recall for WSD evaluation (see, e.g., (Navigli,
2009)). Precision measures the percentage of the
sense assignments provided by the system that are
identical to the gold standard; Recall measures the
percentage of instances that are correctly labeled by
the system. When a system provides sense labels
for all instances, precision and recall are equivalent.
Systems using BabelNet and WordNet senses are
compared against the Most Frequent Sense (MFS)
baseline obtained by using the WordNet most fre-
quent sense. For the Wikipedia sense inventory, we
constructed a pseudo-MFS baseline by selecting (1)
the Wikipedia page associated with the highest rank-
ing WordNet sense, as ranked by SemCor frequency,
or (2) when no synset for a lemma was associ-
ated with a WordNet sense, the first Wikipedia page
sorted using BabelNet?s ordering criteria, i.e., lexi-
cographic sorting. We note that, in the second case,
this procedure frequently selected the page with the
same name as the lemma itself. For instance, the
first sense of Dragon Ball is the cartoon with title
DRAGON BALL, followed by two films (DRAGON
BALL (1990 FILM) and DRAGON BALL EVOLU-
TION).
Systems were scored separately for each sense in-
ventory. We note that because the instances in each
test set are filtered to include only those that can
be labeled with the respective inventory, both the
Wikipedia and WordNet test sets are subsets of the
instances in the BabelNet test set.
4 Participating Systems
Three teams submitted a total of seven systems for
the task, with at least one participant attempting
all of the sense inventory and language combina-
tions. Six systems participated in the WSD task
with BabelNet senses; two teams submitted four sys-
tems using WordNet senses; and one team submitted
three systems for Wikipedia-based senses. Notably,
all systems used graph-based approaches for sense
disambiguation, either using WordNet or BabelNet?s
synset graphs. We summarize the teams? systems as
follows.
DAEBAK! DAEBAK! submitted one system
called PD (Peripheral Diversity) based on BabelNet
path indices from the BabelNet synset graph. Us-
ing a ?5 sentence window around the target word,
a graph is constructed for all senses of co-occurring
lemmas following the procedure proposed by Nav-
igli and Lapata (2010). The final sense is selected
based on measuring connectivity to the synsets of
neighboring lemmas. The MFS is used as a backoff
strategy when no appropriate sense can be picked
out.
GETALP GETALP submitted three systems, two
for BabelNet and one for WordNet, all based on
the ant-colony algorithm of (Schwab et al, 2012),
which uses the sense inventory network structure
to identify paths connecting synsets of the target
lemma to the synsets of other lemmas in context.
The algorithm requires setting several parameters
for the weighting of the structure of the context-
based graph, which vary across the three systems.
The BN1 system optimizes its parameters from the
trial data, while the BN2 and WN1 systems are
completely unsupervised and optimize their param-
eters directly from the structure of the BabelNet and
WordNet graphs.
UMCC-DLSI UMCC-DLSI submitted three
systems based on the ISR-WN resource (Gutie?rrez
et al, 2011), which enriches the WordNet se-
mantic network using edges from multiple lexical
resources, such as WordNet Domains and the
eXtended WordNet. WSD was then performed
using the ISR-WN network in combination with
the algorithm of Gutie?rrez (2012), which is an
extension of the Personalized PageRank algorithm
for WSD (Agirre and Soroa, 2009) which includes
senses frequency. The algorithm requires initial-
izing the PageRank algorithm with a set of seed
synsets (vertices) in the network; this initialization
represents the key variation among UMCC?s three
approaches. The RUN-1 system performs WSD
using all noun instances from the sentence context.
In contrast, the RUN-2 works at the discourse level
and initializes the PageRank using the synsets of all
226
Team System English French German Italian Spanish
DAEBAK! PD 0.604 0.538 0.591 0.613 0.600
GETALP BN-1 0.263 0.261 0.404 0.324 -
GETALP BN-2 0.266 0.257 0.400 0.324 0.371
UMCC-DLSI RUN-1 0.677 0.605 0.618 0.657 0.705
UMCC-DLSI RUN-2 0.685 0.605 0.621 0.658 0.710
UMCC-DLSI RUN-3 0.680 - - - -
MFS 0.665 0.453 0.674 0.575 0.645
Table 3: System performance, reported as F1, for all five languages in the test set when using BabelNet
senses. Top performing systems are marked in bold.
nouns in the document. Finally, the RUN-3 system
initializes using all words in the sentence.
5 Results and Discussion
All teams submitted at least one system using the
BabelNet inventory, shown in Table 3. The UMCC-
DLSI systems were consistently able to outperform
the MFS baseline (a notoriously hard-to-beat heuris-
tic) in all languages except German. Additionally,
the DAEBAK! system outperformed the MFS base-
line on French and Italian. The UMCC-DLSI RUN-
2 system performed the best for all languages. No-
tably, this system leverages the single-sense per dis-
course heuristic (Yarowsky, 1995), which uses the
same sense label for all occurrences of a lemma in a
document.
UMCC-DLSI submitted the only three sys-
tems to use Wikipedia-based senses. Table 4 shows
their performance. Of the three sense inventories,
Wikipedia had the most competitive MFS baseline,
scoring at least 0.694 on all languages. Notably,
the Wikipedia-based system has the lowest recall of
all systems. Despite having superior precision to the
MFS baseline, the low recall brought the resulting
F1 measure below the MFS.
Two teams submitted four total systems for Word-
Net, shown in Table 5. The UMCC-DLSI RUN-2
system was again the top-performing system, under-
scoring the benefit of using discourse information in
selecting senses. The other two UMCC-DLSI sys-
tems also surpassed the MFS baseline. Though still
performing worse than the MFS baseline, when us-
ing the WordNet sense graph, the GETALP system
sees a noticeable improvement of 0.14 over its per-
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  5  10  15  20  25  30  35  40  45
WS
D F
1
Number of senses for the instance
DAEBAK! PDGETALP BN-2 UMCC-DLSI Run-2
Figure 1: F1 measure according to the degree of
instance polysemy, reported when at least ten in-
stances have the specified polysemy.
formance on English data when using the WordNet
sense graph.
The disambiguation task encompasses multiple
types of entities. Therefore, we partitioned the Ba-
belNet test data according to the type of instance be-
ing disambiguated; Table 6 highlights the results per
instance type, averaged across all languages.5 Both
multiword expressions and named entities are less
polysemous, resulting in a substantially higher MFS
baseline that no system was able to outperform on
the two classes. However, for instances made of a
single term, both of the UMCC-DLSI systems were
able to outperform the MFS baseline.
BabelNet adds many Wikipedia senses to the ex-
isting WordNet senses, which increases the poly-
5We omit the UMCC-DLSI Run-3 system from analysis, as
it participated in only a single language.
227
English French German Italian Spanish
Team System Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
UMCC-DLSI RUN-1 0.619 0.484 0.543 0.817 0.480 0.605 0.758 0.460 0.572 0.785 0.458 0.578 0.773 0.493 0.602
UMCC-DLSI RUN-2 0.620 0.487 0.546 0.815 0.478 0.603 0.769 0.467 0.581 0.787 0.463 0.583 0.778 0.502 0.610
UMCC-DLSI RUN-3 0.622 0.489 0.548 - - - - - - - - - - - -
MFS 0.860 0.753 0.803 0.698 0.691 0.694 0.836 0.827 0.831 0.833 0.813 0.823 0.830 0.819 0.824
Table 4: The F1 measure for each system across all five languages in the test set when using Wikipedia-based
senses.
Team System Precision Recall F1
GETALP WN-1 0.406 0.406 0.406
UMCC-DLSI RUN-1 0.639 0.635 0.637
UMCC-DLSI RUN-2 0.649 0.645 0.647
UMCC-DLSI RUN-3 0.642 0.639 0.640
MFS 0.630 0.630 0.630
Table 5: System performance when using WordNet senses. Top performing systems are marked in bold.
Team System Single term Multiword expression Named Entity
DAEBAK! PD 0.502 0.801 0.910
GETALP BN-1 0.232 0.724 0.677
GETALP BN-2 0.235 0.740 0.656
UMCC-DLSI RUN-1 0.582 0.806 0.865
UMCC-DLSI RUN-2 0.584 0.809 0.864
MFS 0.511 0.853 0.920
Table 6: System F1 per instance type, averaged across all submitted languages, with the highest system
scores in bold.
English French German Italian Spanish
Team System Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
DAEBAK PD 0.769 0.364 0.494 0.747 0.387 0.510 0.762 0.307 0.438 0.778 0.425 0.550 0.778 0.450 0.570
GETALP BN-2 0.793 0.111 0.195 0.623 0.130 0.215 0.679 0.124 0.210 0.647 0.141 0.231 0.688 0.177 0.282
UMCC-DLSI RUN-1 0.787 0.421 0.549 0.754 0.441 0.557 0.741 0.330 0.457 0.796 0.461 0.584 0.830 0.525 0.643
UMCC-DLSI RUN-2 0.791 0.419 0.548 0.760 0.436 0.554 0.746 0.332 0.460 0.799 0.453 0.578 0.837 0.530 0.649
Table 7: System performance when the system?s annotations are restricted to only those senses that it also
uses in the aligned sentences of at least two other languages.
semy of most instances. As a further analysis, we
consider the relationship between the polysemy of
an instance?s target and system performance. In-
stances were grouped according to the number of
BabelNet senses that their lemma had; following,
systems were scored on each grouping. Figure 1
shows the performance of the best system from each
team on each polysemy-based instance grouping,
with a general trend of performance decay as the
number of senses increases. Indeed, all systems?
performances are negatively correlated with the de-
gree of polysemy, ranging from -0.401 (UMCC-
DLSI RUN-1) to -0.654 (GETALP BN-1) when
measured using Pearson?s correlation. All systems?
228
correlations are significant at p < 0.05.
Last, we note that all systems operated by sense-
annotating each language individually without tak-
ing advantage of either the multilingual structure of
BabelNet or the sentence alignment of the test data.
For example, the sense projection method used to
create the initial set of multilingual annotations on
our test data (cf. Table 2) suggests that the sense
translation API could be used as a reliable source for
estimating the correctness of an annotation; specifi-
cally, given the sense annotations for each language,
the translation API could be used to test whether the
sense is also present in the aligned sentence in the
other languages.
Therefore, we performed a post-hoc analysis of
the benefit of multilingual sense alignment using the
results of the four systems that submitted for all lan-
guages in BabelNet. For each language, we filter
the sense annotations such that an annotation for an
instance is retained only if the system assigned the
same sense to some word in the aligned sentence
from at least two other languages.
Table 7 shows the resulting performance for the
four systems. As expected, the systems exhibit sig-
nificantly lower recall due to omitting all language-
specific instances. However, the resulting precision
is significantly higher than the original performance,
shown in Table 3. Additionally, we analyzed the set
of instances reported for each system and confirmed
that the improvement is not due to selecting only
monosemous lemmas. Despite the GETALP system
having the lower performance of the four systems
when all instances are considered, the system ob-
tains the highest precision for the English dataset.
Furthermore, the UMCC-DLSI systems still obtain
moderate recall, while enjoying 0.106-0.155 abso-
lute improvements in precision across all languages.
While the resulting F1 is lower due to a loss of recall,
we view this result as a solid starting point for other
methods to sense-tag the remaining instances. Over-
all, these results corroborate previous studies sug-
gesting that highly precise sense annotations can be
obtained by leveraging multiple languages (Navigli
and Ponzetto, 2012b; Navigli and Ponzetto, 2012c).
6 Conclusion and Future Directions
Following recent SemEval efforts with word senses
in multilingual settings, we have introduced a new
task on multilingual WSD that uses the recently
released BabelNet 1.1.1 sense inventory. Using a
data set of 13 articles in five languages, all nomi-
nal instances were annotated with BabelNet senses.
Because BabelNet is a superset of WordNet and
Wikipedia, the task also facilitates analysis in those
sense inventories.
Three teams submitted seven systems, with all
systems leveraging the graph-based structure of
WordNet and BabelNet. Several systems were able
to outperform the competitive MFS baseline, except
in the case of Wikipedia, but current performance
leaves significant room for future improvement. In
addition, we believe that future research could lever-
age sense parallelism available in sentence-aligned
multilingual corpora, together with enriched infor-
mation available in future versions of BabelNet. All
of the resources for this task, including the newest
1.1.1 version of BabelNet, were released on the task
website.6
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
A large group of people assisted with SemEval-
2013 Task 12, and without whose help this
task would not have been possible. In particular,
we would like to thank Philipp Cimiano, Maud
Erhmann, Sascha Hinte, Jesu?s Roque Campan?a
Go?mez, and Andreas Soos for their assistance
in sense annotation; our fellow LCL team mem-
bers: Moreno De Vincenzi, Stefano Faralli, Tiziano
Flati, Marc Franco Salvador, Andrea Moro, Silvia
Necs?ulescu, and Taher Pilehvar for their invaluable
assistance in creating BabelNet 1.1.1, preparing and
validating sense annotations, and sense-tagging the
Italian corpus; last, we thank Jim McManus for his
help in producing the Italian test data.
6http://www.cs.york.ac.uk/semeval-2013/
task12/
229
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of EACL, Athens, Greece, pages 33?41.
Philip Edmonds and Scott Cotton. 2001. Senseval-2:
Overview. In Proceedings of The Second International
Workshop on Evaluating Word Sense Disambiguation
Systems, pages 1?6, Toulouse, France.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of Empirical Meth-
ods in Natural Language Processing, pages 440?449,
Singapore.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Yoan Gutie?rrez, Antonio Ferna?ndez Orqu??n, Sonia
Va?zquez, and Andre?s Montoyo. 2011. Enriching the
integration of semantic resources based on wordnet.
Procesamiento del Lenguaje Natural, 47:249?257.
Yoan Gutie?rrez. 2012. Ana?lisis sema?ntico multidimen-
sional aplicado a la desambiguacio?n del lenguaje nat-
ural. Ph.D. thesis, Universidad de Alicante.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-structured
content and artificial intelligence: The story so far. Ar-
tificial Intelligence, 194:2?27.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation.
Adam Kilgarriff and Martha Palmer. 2000. Introduction
to the special issue on senseval. Computers and the
Humanities, 34(1-2):1?13.
Adam Kilgarriff. 1998. Senseval: An exercise in eval-
uating word sense disambiguation programs. In Pro-
ceedings of the First International Conference on Lan-
guage Resources and Evaluation, pages 1255?1258,
Granada, Spain.
Els Lefever and Veronique Hoste. 2010. Semeval-2010
task 3: Cross-lingual word sense disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 15?20, Uppsala, Sweden.
Association for Computational Linguistics.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of the 3rd International Work-
shop on the Evaluation of Systems for the Semantic
Analysis of Text (SENSEVAL-3) at ACL-04, Barcelona,
Spain, 25?26 July 2004, pages 25?28.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
Semeval-2010 task 2: Cross-lingual lexical substitu-
tion. In Proceedings of the 5th international workshop
on semantic evaluation, pages 9?14, Uppsala, Sweden.
Association for Computational Linguistics.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet: an
online lexical database. International Journal of Lexi-
cography, 3(4):235?244.
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study on graph connectivity for unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 32(4):678?
692.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b.
BabelRelate! a joint multilingual approach to com-
puting semantic relatedness. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence (AAAI), Toronto, Ontario, Canada.
Roberto Navigli and Simone Paolo Ponzetto. 2012c.
Joining forces pays off: Multilingual Joint Word Sense
Disambiguation. In Proceedings of EMNLP-CoNLL,
pages 1399?1410, Jeju Island, Korea.
Roberto Navigli and Simone Paolo Ponzetto. 2012d.
Multilingual WSD with just a few lines of code: the
BabelNet API. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2012), Jeju, Korea.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 Task 07: Coarse-
grained English all-words task. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-2007), Prague, Czech Republic, pages 30?
35.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Roberto Navigli. 2012. A quick tour of Word Sense
Disambiguation, Induction and related approaches. In
Proceedings of the 38th Conference on Current Trends
in Theory and Practice of Computer Science (SOF-
SEM), pages 115?129.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the 4th International Workshop on Semantic
Evaluations (SemEval-2007), Prague, Czech Repub-
lic, pages 87?92.
Didier Schwab, Je?ro?me Goulian, Andon Tchechmedjiev,
and Herve? Blanchon. 2012. Ant colony algorithm for
230
the unsupervised word sense disambiguation of texts:
Comparison and evaluation. In Proceedings of the
24th International Conference on Computational Lin-
guistics (COLING), pages 8?15, Mumbai, India.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Proceedings of ACL 2004
SENSEVAL-3 Workshop, pages 41?43, Barcelona,
Spain.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA, USA.
231
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 290?299, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 13:
Word Sense Induction for Graded and Non-Graded Senses
David Jurgens
Dipartimento di Informatica
Sapienza Universita` di Roma
jurgens@di.uniroma1.it
Ioannis Klapaftis
Search Technology Center Europe
Microsoft
ioannisk@microsoft.com
Abstract
Most work on word sense disambiguation has
assumed that word usages are best labeled
with a single sense. However, contextual am-
biguity or fine-grained senses can potentially
enable multiple sense interpretations of a us-
age. We present a new SemEval task for evalu-
ating Word Sense Induction and Disambigua-
tion systems in a setting where instances may
be labeled with multiple senses, weighted by
their applicability. Four teams submitted nine
systems, which were evaluated in two settings.
1 Introduction
Word Sense Disambiguation (WSD) attempts to
identify which of a word?s meanings applies in a
given context. A long-standing task, WSD is fun-
damental to many NLP applications (Navigli, 2009).
Typically, each usage of a word is treated as express-
ing only a single sense. However, contextual ambi-
guity as well as the relatedness of certain meanings
can potentially elicit multiple sense interpretations.
Recent work has shown that annotators find multi-
ple applicable senses in a given target word context
when using fine-grained sense inventories such as
WordNet (Ve?ronis, 1998; Murray and Green, 2004;
Erk et al, 2009; Passonneau et al, 2012b; Jurgens,
2013; Navigli et al, 2013). Such contexts would be
better annotated with multiple sense labels, weight-
ing each sense according to its applicability (Erk et
al., 2009; Jurgens, 2013), in effect allowing ambigu-
ity or multiple interpretations to be explicitly mod-
eled. Accordingly, the first goal of this task is to
evaluate WSD systems in a setting where instances
may be labeled with one or more senses, weighted
by their applicability.
WSD methods are ultimately defined and poten-
tially restricted by their choice in sense inventory;
for example, a sense inventory may have insufficient
sense-annotated data to build WSD systems for spe-
cific types of text (e.g., social media), or the inven-
tory may lack domain-specific senses. Word Sense
Induction (WSI) has been proposed as a method for
overcoming such limitations by learning the senses
automatically from text. In essence, a WSI algo-
rithm acts as a lexicographer by grouping word us-
ages according to their shared meaning. The sec-
ond goal of this task is to assess the performance of
WSI algorithms when they are able to model multi-
ple meanings of a usage with graded senses.
Task 12 focuses on disambiguating senses for 50
target lemmas: 20 nouns, 20 verbs, and 10 adjectives
(Sec. 2). Since the Task evaluates only unsupervised
systems, no training data was provided; however, to
enable more comparison, Unsupervised WSD sys-
tems were also allowed to participate. Participat-
ing systems were evaluated in two settings (Sec. 3),
depending on whether they used induced senses or
WordNet 3.1 senses for their annotations. The re-
sults (Sec. 5) demonstrate a substantial improvement
over the competitive most frequent sense baseline.
2 Task Description
This task required participating systems to annotate
instances of nouns, verb, and adjectives using Word-
Net 3.1 (Fellbaum, 1998), which was selected due
to its fine-grained senses. Participants could label
each instance with one or more senses, weighting
290
We all are relieved to lay aside our fight-or-flight reflexes and to commemorate our births from out of the dark
centers of the women, to feel the complexity of our love and frustration with each other, to stretch our cognition to
encompass the thoughts of every entity we know.
dark%3:00:01:: ? devoid of or deficient in light or brightness; shadowed or black
dark%3:00:00:: ? secret
I ask because my practice has always been to allow about five minutes grace, then remove it.
ask%2:32:02:: ? direct or put; seek an answer to
ask%2:32:04:: ? address a question to and expect an answer from
Table 1: Example instances with multiple senses due to intended double meanings (top) or contextual am-
biguity (bottom). Senses are specified using their WordNet 3.1 sense keys.
each by their applicability. Table 1 highlights two
example contexts where multiple senses apply. The
first example shows a case of an intentional dou-
ble meaning that evokes both the physical aspect of
dark.a as being devoid of light and the causal re-
sult of being secret. In contrast, the second example
shows a case of multiple interpretations from ambi-
guity; a different preceding context could generate
the alternate interpretations ?I ask [you] because?
(sense ask%2:32:04::) or ?I ask [the question]
because? (sense ask%2:32:02::).
2.1 Data
Three datasets were provided with the task. The trial
dataset provided weighted word sense annotations
using the data gathered by Erk et al (2009). The
trial dataset consisted of 50 contexts for eight words,
where each context was labeled with WordNet 3.0
sense ratings from three untrained lexicographers.
Due to the unsupervised nature of the task, partic-
ipants were not provided with sense-labeled training
data. However, WSI systems were provided with the
ukWaC corpus (Baroni et al, 2009) to use in induc-
ing senses. Previous SemEval WSI tasks had pro-
vided participants with corpora specific to the task?s
target terms; in contrast, this task opted to use a large
corpus to enable WSI methods that require corpus-
wide statistics, e.g., statistical associations.
Test data was drawn from the Open American
National Corpus (Ide and Suderman, 2004, OANC)
across a variety of genres and from both the spoken
and written portions of the corpus, summarized in
Table 2. All contexts were manually inspected to en-
sure that the lemma being disambiguated was of the
correct part of speech and had an interpretation that
matched at least one WordNet 3.1 sense. This filter-
ing also removed instances that were in a colloca-
tion, or had an idiomatic meaning. Ultimately, 4664
contexts were used as test data, with a minimum of
22 and a maximum of 100 contexts per word.
2.2 Sense Annotation
Recent work proposes to gather sense annotations
using crowdsourcing in order to reduce the time
and cost of acquiring sense-annotated corpora (Bie-
mann and Nygaard, 2010; Passonneau et al, 2012b;
Rumshisky et al, 2012; Jurgens, 2013). There-
fore, we initially annotated the Task?s data using the
method of Jurgens (2013), where workers on Ama-
zon Mechanical Turk (AMT) rated all senses of a
word on a Likert scale from one to five, indicat-
ing the sense does not apply at all or completely
applies, respectively. Twenty annotators were as-
signed per instance, with their ratings combined by
selecting the most frequent rating. However, we
found that while the annotators achieved moderate
inter-annotator agreement (IAA), the resulting an-
notations were not of high enough quality to use in
the Task?s evaluations. Specifically, for some senses
and contexts, AMT annotators required more infor-
mation about sense distinctions than was feasible to
integrate into the AMT setting, which led to consis-
tent but incorrect sense assignments.
Therefore, the test data was annotated by the two
authors, with the first author annotating all instances
and the second author annotating a 10% sample of
each lemma?s instances in order to calculate IAA.
IAA was calculated using Krippendorff?s ? (Krip-
pendorff, 1980; Artstein and Poesio, 2008), which is
an agreement measurement that adjusts for chance,
291
Spoken Written
Genre Face-to-face Telephone Fiction Journal Letters Non-fiction Technical Travel Guides All
Instances 52 699 127 2403 103 477 611 192 4664
Tokens 1742 30,700 3438 69,479 2238 11,780 17,337 4490 141,204
Mean senses/inst. 1.17 1.08 1.15 1.13 1.31 1.10 1.11 1.11 1.12
Table 2: Test data used in Task 12, divided according to source type
ranging in (?1, 1] for interval data, where 1 indi-
cates perfect agreement and -1 indicates systematic
disagreement; two random annotations have an ex-
pected ? of zero. We treat each sense and instance
combination as a separate item to rate. The total IAA
for the dataset was 0.504, and on individual words,
ranged from 0.903 for number.n to 0.00 for win.v.
While this IAA is less than the 0.8 recommended by
Krippendorff (2004), it is consistent with the IAA
distribution for the sense annotations of MASC on
other parts of the OANC corpus: Passonneau et al
(2012a) reports an ? of 0.88 to -0.02 with the MASI
statistic (Passonneau et al, 2006).
Table 2 summarizes the annotation statistics for
the Task?s data. The annotation process resulted in
far fewer senses per instance in the trial data, which
we attribute to using trained annotators. An analysis
across the corpora genres showed that the multiple-
sense annotation rates were similar. Due to the vari-
ety of contextual sources, all lemmas were observed
with at least two distinct senses.
3 Evaluation
We adopt a two-part evaluation setting used in pre-
vious SemEval WSI and WSD tasks (Agirre and
Soroa, 2007; Manandhar et al, 2010). The first eval-
uation uses a traditional WSD task that directly com-
pares WordNet sense labels. For WSI systems, their
induced sense labels are converted to WordNet 3.1
labels via a mapping procedure. The second evalu-
ation performs a direct comparison of the two sense
inventories using clustering comparisons.
3.1 WSD Task
In the first evaluation, we adopt a WSD task with
three objectives: (1) detecting which senses are ap-
plicable, (2) ranking senses by their applicability,
and (3) measuring agreement in applicability rat-
ings with human annotators. Each objectives uses
a specific measurement: (1) the Jaccard Index, (2)
positionally-weighted Kendall?s ? similarity, and
(3) a weighted variant of Normalized Discounted
Cumulative Gain, respectively. Each measure is
bounded in [0, 1], where 1 indicates complete agree-
ment with the gold standard. We generalize the tra-
ditional definition of WSD Recall such that it mea-
sures the average score for each measure across all
instances, including those not labeled by the system.
Systems are ultimately scored using the F1 measure
between each objective?s measure and Recall.
3.1.1 Transforming Induced Sense Labels
In the WSD setting, induced sense labels may be
transformed into a reference inventory (e.g., Word-
Net 3.1) using a sense mapping procedure. We fol-
low the 80/20 setup of Manandhar et al (2010),
where the corpus is randomly divided into five par-
titions, four of which are used to learn the sense
mapping; the sense labels for the held-out partition
are then converted and compared with the gold stan-
dard. This process is repeated so that each partition
is tested once. For learning the sense mapping func-
tion, we use the distribution mapping technique of
Jurgens (2012), which takes into account the sense
applicability weights in both labelings.
3.1.2 Jaccard Index
Given two sets of sense labels for an instance,
X and Y , the Jaccard Index is used to measure the
agreement: |X?Y ||X?Y | . The Jaccard Index is maximized
when X and Y use identical labels, and is mini-
mized when the sets of sense labels are disjoint.
3.1.3 Positionally-Weighted Kendall?s ?
Rank correlations have been proposed for evalu-
ating a system?s ability to order senses by applicabil-
ity; in previous work, both Erk and McCarthy (2009)
and Jurgens (2012) propose rank correlation coeffi-
cients that assume all positions in the ranking are
equally important. However, in the case of graded
292
sense evaluation, often only a few senses are appli-
cable, with the applicability ratings of the remain-
ing senses being relatively inconsequential. There-
fore, we consider an alternate rank scoring based on
Kumar and Vassilvitskii (2010), which weights the
penalty of reordering the lower positions less than
the penalty of reordering the first ranks.
Kendall?s ? distance, K, is a measure of the
number of item position swaps required to make
two sequences identical. Kumar and Vassilvitskii
(2010) extend this distance definition using a vari-
able penalty function ? for the cost of swapping two
positions, which we denote K?. By using an appro-
priate ?, K? can be biased towards the correctness
of higher ranks by assigning a smaller ? to lower
ranks. Because K? is a distance measure, its value
range will be different depending on the number of
ranks used. Therefore, to convert the measure to a
similarity we normalize the distance to [0, 1] by di-
viding by the maximum K? distance and then sub-
tracting the distance from one. Given two rankings
x and y where x is the reference by which y is to be
measured, we may compute the normalized similar-
ity using
Ksim? = 1?
K?(x, y)
Kmax? (x)
. (1)
Equation 1 has its maximal value of one when rank-
ing y is identical to ranking x, and its minimal value
of zero when y is in the reverse order as x. We refer
to this value as the positionally-weighted Kendall?s
? similarity, Ksim? . As defined, K
sim
? does not ac-
count for ties. Therefore, we arbitrarily break ties in
a deterministic fashion for both rankings. Second,
we define ? to assign higher cost to the first ranks:
the cost to move an item into position i, ?i, is de-
fined as n?(i+1)n , where n is the number of senses.
3.1.4 Weighted NDCG
To compare the applicability ratings for sense an-
notations, we recast the annotation process in an In-
formation Retrieval setting: Given an example con-
text acting as a query over a word?s senses, the task
is to retrieve all applicable senses, ranking and scor-
ing them by their applicability. Moffat and Zobel
(2008) propose using Discounted Cumulative Gain
(DCG) as a method to compare a ranking against a
baseline. Given (1) a gold standard weighting of the
k senses applicable to a context, where wi denotes
the applicability for sense i in the gold standard, and
(2) a ranking of the k senses by some method, the
DCG may be calculated as
?k
i=1
2wi+1?1
log2(i+1)
. DCG is
commonly normalized to [0, 1] so that the value is
comparable when computed on rankings with dif-
ferent k and weight values. To normalize, the maxi-
mum value is calculated by first computing the DCG
on the ranking when the k items are sorted by their
weights, referred as the Ideal DCG (IDCG), and then
normalizing as NDCG = DCGIDCG .
The DCG only considers the weights assigned
in the gold standard, which potentially masks im-
portance differences in the weights assigned to the
senses. Therefore, we propose weighting the DCG
by the relative difference in the two weights. Given
an alternate weighting of the k items, denoted as w?i,
WDCG =
k?
i=1
min(wi,w?i)
max(wi,w?i)
(
2wi+1 ? 1
)
log2(i)
. (2)
The key impact in Equation 2 comes from weight-
ing an item?s contribution to the score by its rela-
tive deviation in absolute weight. A set of weights
that achieves an equivalent ranking may have a low
WDCG if the weights are significantly higher or
lower than the reference. Equation 2 may be nor-
malized in the same way as the DCG. We refer to
this final normalized measure as the Weighted Nor-
malized Discounted Cumulative Gain (WNDCG).
3.2 Sense Cluster Comparisons
Sense induction can be viewed as an unsupervised
clustering task where usages of a word are grouped
into clusters, each representing uses of the same
meaning. In previous SemEval tasks on sense in-
duction, instances were labeled with a single sense,
which yields a partition over the instances into dis-
joint sets. The proposed partition can then be com-
pared with a gold-standard partition using many ex-
isting clustering comparison methods, such as the
V-Measure (Rosenberg and Hirschberg, 2007) or
paired FScore (Artiles et al, 2009). Such cluster
comparison methods measure the degree of similar-
ity between the sense boundaries created by lexicog-
raphers and those created by WSI methods.
In the present task, instances are potentially la-
beled both with multiple senses and with weights
293
reflecting the applicability. This type of sense label-
ing produces a fuzzy clustering: An instance may
belong to one or more sense clusters with its clus-
ter membership relative to its weight for that sense.
Formally, we refer to (1) a solution where the sets
of instances overlap as a cover and (2) a solution
where the sets overlap and instances may have par-
tial memberships in a set as fuzzy cover.
We propose two new fuzzy measures for com-
paring fuzzy sense assignments: Fuzzy B-Cubed
and Fuzzy Normalized Mutual Information. The
two measures provide complementary information.
B-Cubed summarizes the performance per instance
and therefore provides an estimate of how well a sys-
tem would perform on a new corpus with a similar
sense distribution. In contrast, Fuzzy NMI is mea-
sured based on the clusters rather than the instances,
thereby providing a performance analysis that is in-
dependent of the corpus sense distribution.
3.2.1 Fuzzy B-Cubed
Bagga and Baldwin (1998) proposed a clustering
evaluation known as B-Cubed, which compares two
partitions on a per-item basis. Amigo? et al (2009)
later extended the definition of B-Cubed to compare
overlapping clusters (i.e., covers). We generalize B-
Cubed further to handle the case of fuzzy covers.
B-Cubed is based on precision and recall, which es-
timate the fit between two clusterings, X and Y at
the item level. For an item i, precision reflects how
many items sharing a cluster with i inX appear in its
cluster in Y ; conversely, recall measures how many
items sharing a cluster in Y with i also appear in its
cluster in X . The final B-Cubed value is the har-
monic mean of the two scores.
To generalize B-Cubed to fuzzy covers, we adopt
the formalization of Amigo? et al (2009), who define
item-based precision and recall functions, P and R,
in terms of a correctness function, C ? {0, 1}. For
notational brevity, let avg be a function that returns
the mean value of a series, and ?x(i) denote the set
of clusters in clusteringX of which item i is a mem-
ber. B-Cubed precision and recall may therefore cal-
culated over all n items:
B-Cubed Precision = avg
i
[ avg
j 6=i???y(i)
P (i, j)] (3)
B-Cubed Recall = avg
i
[ avg
j 6=i???x(i)
R(i, j)]. (4)
When comparing partitions, P and R are defined as
1 if two items cluster labels are identical. To gen-
eralize B-Cubed for fuzzy covers, we redefine P
and R to account for differences in the partial clus-
ter membership of items. Let `X(i) denote the set
of clusters of which i is a member, and wk(i) de-
note the membership weight of item i in cluster k in
X . We therefore define C with respect to X of two
items as
C(i, j,X) =
?
k?`X(i)?`X(j)
1?|wk(i)?wk(j)|. (5)
Equation 5 is maximized when i and j have
identical membership weights in the clusters of
which they are members. Importantly, Equation
5 generalizes to the correctness operations both
when comparing partitions and covers, as defined
by Amigo? et al (2009). Item-based Precision
and Recall are then defined using Equation 5 as
P (i, j,X) = Min(C(i,j,X),C(i,j,Y ))C(i,j,X) and R(i, j,X) =
Min(C(i,j,X),C(i,j,Y ))
C(i,j,Y ) , respectively. These fuzzy gen-
eralizations are used in Equations 3 and 4.
3.2.2 Fuzzy Normalized Mutual Information
Mutual information measures the dependence be-
tween two random variables. In the context of
clustering evaluation, mutual information treats the
sense labels as random variables and measures the
level of agreement in which instances are labeled
with the same senses (Danon et al, 2005). For-
mally, mutual information is defined as I(X;Y ) =
H(X)?(H(X|Y ) whereH(X) denotes the entropy
of the random variable X that represents a parti-
tion, i.e., the sets of instances assigned to each sense.
Typically, mutual information is normalized to [0, 1]
in order to facilitate comparisons between multiple
clustering solutions on the same scale (Luo et al,
2009), with Max(H(X), H(Y )) being the recom-
mended normalizing factor (Vinh et al, 2010).
In its original formulation Mutual information
is defined only to compare non-overlapping cluster
partitions. Therefore, we propose a new definition of
mutual information between fuzzy covers using ex-
tension of Lancichinetti et al (2009) for calculating
the normalized mutual information between covers.
In the case of partitions, a clustering is represented
as a discrete random variable whose states denote
the probability of being assigned to each cluster. In
294
the fuzzy cover setting, each item may be assigned
to multiple clusters and no longer has a binary as-
signment to a cluster, but takes on a value in [0, 1].
Therefore, each cluster Xi can be represented sepa-
rately as a continuous random variable, with the en-
tire fuzzy cover denoted as the variableX1...k, where
the ith entry of X is the continuous random vari-
able for cluster i. However, by modeling clusters us-
ing continuous domain, differential entropy must be
used for the continuous variables; importantly, dif-
ferential entropy does not obey the same properties
as discrete entropy and may be negative.
To avoid calculating entropy in the continuous do-
main, we therefore propose an alternative method of
computing mutual information based on discretiz-
ing the continuous values of Xi in the fuzzy set-
ting. For the continuous random variable Xi, we
discretize the value by dividing up probability mass
into discrete bins. That is, the support of Xi is parti-
tioned into disjoint ranges, each of which represents
a discrete outcome of Xi. As a result, Xi becomes a
categorical distribution over a set of weights ranges
{w1, . . . , wn} that denote the strength of member-
ship in the fuzzy set. With respect to sense annota-
tion, this discretization process is analogous to hav-
ing an annotator rate the applicability of a sense for
an instance using a Likert scale instead of using a
rational number within a fixed bound.
Discretizing the continuous cluster membership
ratings into bins allows us to avoid the problematic
interpretation of entropy in the continuous domain
while still expanding the definition of mutual infor-
mation from a binary cluster membership to one of
degrees. Using the definition of Xi and Yj as a cate-
gorical variables over discrete ratings, we may then
estimate the entropy and joint entropy as follows.
H(Xi) =
n?
i=1
p(wi)log2p(wi) (6)
where p(wi) is the probability of an instance being
labeled with rating wi Similarly, we may define the
joint entropy of two fuzzy clusters as
H(Xk, Yl) =
n?
i=1
m?
j=1
p(wi, wj)log2p(wi, wj) (7)
where p(wi, wj) is the probability of an instance be-
ing labeled with rating wi in cluster Xk and wj in
cluster Yl, and m denotes the number of bins for Yl.
The conditional entropy between two clusters may
then be calculated as
H(Xk|Yl) = H(Xk, Yl)?H(Yl).
Together, Equations 6 and 7 may be used to define
I(X,Y ) as in the original definition. We then nor-
malize using the method of McDaid et al (2011).
Based on the limited range of fuzzy memberships
in [0, 1], we selected uniformly distributed bins in
[0, 1] at 0.1 intervals when discretizing the member-
ship weights for sense labelings.
3.3 Baselines
Task 12 included multiple baselines based on mod-
eling different types of WSI and WSD systems.
Due to space constraints, we include only the four
most descriptive here: (1) Semcor MFS which la-
bels each instance with the most frequent sense of
that lemma in SemCor, (2) Semcor Ranked Senses
baseline, which labels each instance with all of the
target lemma?s senses, ranked according to their fre-
quency in SemCor, using weights n?i+1n , where n is
the number of senses and i is the rank, (3) 1c1inst
which labels each instance with its own induced
sense and (4) All-instances, One sense which la-
bels all instances with the same induced sense. The
first two baselines directly use WordNet 3.1 senses,
while the last two use induced senses.
4 Participating Systems
Four teams submitted nine systems, seven of which
used induced sense inventories. AI-KU submitted
three WSI systems based on a lexical substitution
method; a language model is built from the target
word?s contexts in the test data and the ukWaC cor-
pus and then Fastsubs (Yuret, 2012) is used to iden-
tify lexical substitutes for the target. Together, the
contexts of the target and substitutes are used to
build a distributional model using the S-CODE al-
gorithm (Maron et al, 2010). The resulting contex-
tual distributions are then clustered using K-means
to identify word senses. The University of Mel-
bourne (Unimelb) team submitted two WSI systems
based on the approach of Lau et al (2012). Their
systems use a Hierarchical Dirichlet Process (Teh
et al, 2006) to automatically infer the number of
senses from contextual and positional features. Un-
295
WSD F1 Cluster Comparison
Team System Jac. Ind. Ksim? WNDCG Fuzzy NMI Fuzzy B-Cubed #Cl #S
AI-KU Base 0.197 0.620 0.387 0.065 0.390 7.76 6.61
AI-KU add1000 0.197 0.606 0.215 0.035 0.320 7.76 6.61
AI-KU remove5-add1000 0.244 0.642 0.332 0.039 0.451 3.12 5.33
Unimelb 5p 0.218 0.614 0.365 0.056 0.459 2.37 5.97
Unimelb 50k 0.213 0.620 0.371 0.060 0.483 2.48 6.08
UoS #WN Senses 0.192 0.596 0.315 0.047 0.201 8.08 6.77
UoS top-3 0.232 0.625 0.374 0.045 0.448 3.00 5.44
La Sapienza system-1 0.149 0.507 0.311 - - - 8.69
La Sapienza system-2 0.149 0.510 0.383 - - - 8.67
All-instances, One sense 0.192 0.609 0.288 0.0 0.623 1.00 6.62
1c1inst 0.0 0.0 0.0 0.071 0.0 1.00 0.0
Semcor MFS 0.455 0.465 0.339 - - - 1.00
Semcor Ranked Senses 0.149 0.559 0.489 - - - 8.66
Table 3: Performance on the five evaluation measures for all system and selected baselines. Top system
performances are marked in bold.
like other teams, the Unimelb systems were trained
on a Wikipedia corpus instead of the ukWaC cor-
pus. The University of Sussex (UoS) team submit-
ted two WSI systems that use dependency-parsed
features from the corpus, which are then clustered
into senses using the MaxMax algorithm (Hope and
Keller, 2013); the resulting fine-grained clusters are
then combined based on their degree of separabil-
ity. The La Sapienza team submitted two Unsu-
pervised WSD systems based applying Personal-
ized Page Rank (Agirre and Soroa, 2009) over a
WordNet-based network to compare the similarity of
each sense with the similarity of the context, ranking
each sense according to its similarity.
5 Results and Discussion
Table 3 shows the main results for all instances. Ad-
ditionally, we report the number of induced clusters
used to label each sense as #Cl and the number of
resulting WordNet 3.1 senses for each sense with
#S. As in previous WSD tasks, the MFS baseline
was quite competitive, outperforming all systems on
detecting which senses were applicable, measured
using the Jaccard Index. However, most systems
were able to outperform the MFS baseline on rank-
ing senses and quantifying their applicability.
Previous cluster comparison evaluations often
faced issues with the measures being biased either
towards the 1c1inst baseline or labeling all instances
with the same sense. However, Table 3 shows that
Team System F1 NMI B-Cubed
AI-KU Base 0.641 0.045 0.351
AI-KU add1000 0.601 0.023 0.288
AI-KU remove5-add1000 0.628 0.026 0.421
Unimelb 5p 0.596 0.035 0.421
Unimelb 50k 0.605 0.039 0.441
UoS #WN Senses 0.574 0.031 0.180
UoS top-3 0.600 0.028 0.414
La Sapienza System-1 0.204 - -
La Sapienza System-2 0.217 - -
All-instances, One sense 0.569 0.0 0.570
1c1inst 0.0 0.018 0.0
Semcor MFS 0.477 0.0 0.570
Table 4: System performance in the single-sense set-
ting. Top system performances are marked in bold.
systems are capable of performing well in both the
Fuzzy NMI and Fuzzy B-Cubed measures, thereby
avoiding the extreme performance of either baseline.
An analysis of the systems? results showed that
many systems labeled instances with a high num-
ber of senses, which could have been influenced by
the trial data having significantly more instances la-
beled with multiple senses than the test data. There-
fore, we performed a second analysis that parti-
tioned the test set into two sets: those labeled with
a single sense and those with multiple senses. For
single-sense set, we modified the test setting to have
systems also label instances with a single sense:
(1) the sense mapping function for WSI systems
(Sec. 3.1.1) was modified so that after the mapping,
296
WSD F1 Cluster Comparison
Team System Jac. Ind. Ksim? WNDCG Fuzzy NMI Fuzzy B-Cubed
AI-KU Base 0.394 0.617 0.317 0.029 0.078
AI-KU add1000 0.394 0.620 0.214 0.014 0.061
AI-KU remove5-add1000 0.434 0.585 0.290 0.004 0.116
Unimelb 5p 0.436 0.585 0.286 0.019 0.130
Unimelb 5000k 0.414 0.602 0.298 0.021 0.134
UoS #WN Senses 0.367 0.627 0.313 0.036 0.037
UoS top-3 0.421 0.574 0.302 0.006 0.113
La Sapienza system-1 0.263 0.660 0.447 - -
La Sapienza system-2 0.412 0.694 0.536 - -
All-instances, One sense 0.387 0.635 0.254 0.0 0.130
1c1inst 0.0 0.0 0.0 0.300 0.0
Semcor MFS 0.283 0.373 0.197
Semcor Ranked Senses 0.263 0.593 0.395
Table 5: System performance on all instances labeled with multiple senses. Top system performances are
marked in bold.
only the highest-weighted WordNet 3.1 sense was
used, and (2) the La Sapienza system output was
modified to retain only the highest weighted sense.
In this single-sense setting, systems were evaluated
using the standard WSD Precision and Recall mea-
sures; we report the F1 measure of Precision and Re-
call. The remaining subset of instances annotated
with multiple senses were evaluated separately.
Table 4 shows the systems? performance on
single-sense instances, revealing substantially in-
creased performance and improvement over the
MFS baseline for WSI systems. Notably, the per-
formance of the best sense-remapped WSI systems
surpasses the performance of many supervised WSD
systems in previous WSD evaluations (Kilgarriff,
2002; Mihalcea et al, 2004; Pradhan et al, 2007;
Agirre et al, 2010). This performance suggests that
WSI systems using graded labels provide a way to
leverage huge amounts of unannotated corpus data
for finding sense-related features in order to train
semi-supervised WSD systems.
Table 5 shows the performance on the subset of
instances that were annotated with multiple senses.
We note that in this setting, the mapping proce-
dure transforms the All-Instances One Sense base-
line into the average applicability rating for each
sense in the test corpus. Notably, the La Sapienza
systems sees a significant performance increase in
this setting; their systems label each instance with
all of the lemma?s senses, which significantly de-
grades performance in the most common case where
only a single sense applies. However, when multi-
ple senses are known to be present, their method for
quantifying sense applicability appears closest to the
gold standard judgments. Furthermore, the majority
of WSI systems are able to surpass all four baselines
on identifying which senses are present and quanti-
fying their applicability.
6 Conclusion
We have introduced a new evaluation setting for
WSI and WSD systems where systems are measured
by their ability to detect and weight multiple appli-
cable senses for a single context. Four teams submit-
ted nine systems, annotating a total of 4664 contexts
for 50 words from the OANC. Many systems were
able to surpass the competitive MFS baseline. Fur-
thermore, when WSI systems were trained to pro-
duce only a single sense label, the performance of
resulting semi-supervised WSD systems surpassed
that of many supervised systems in previous WSD
evaluations. Future work may assess the impact of
graded sense annotations in a task-based setting. All
materials have been released on the task website.1
Acknowledgments
We thank Rebecca Passonneau for her feedback and
suggestions for target lemmas used in this task.
1
http://www.cs.york.ac.uk/semeval-2013/task13/
297
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
2: Evaluating word sense induction and discrimination
systems. In Proceedings of the Fourth International
Workshop on Semantic Evaluations, pages 7?12. ACL.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of EACL, pages 33?41. ACL.
Eneko Agirre, Oier Lo?pez De Lacalle, Christine Fell-
baum, Andrea Marchetti, Antonio Toral, and Piek
Vossen. 2010. SemEval-2010 task 17: All-words
word sense disambiguation on specific domains. In
Proceedings of SemEval-2010. ACL.
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4):461?486.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of EMNLP, pages 534?542. Association
for Computational Linguistics.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC, pages 563?
566.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Chris Biemann and Valerie Nygaard. 2010. Crowdsourc-
ing wordnet. In The 5th International Conference of
the Global WordNet Association (GWC-2010).
Leon Danon, Albert D??az-Guilera, Jordi Duch, and Alex
Arenas. 2005. Comparing community structure iden-
tification. Journal of Statistical Mechanics: Theory
and Experiment, 2005(09):P09008.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
440?449. ACL.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 10?18. ACL.
Christine Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
David Hope and Bill Keller. 2013. MaxMax: A Graph-
Based Soft Clustering Algorithm Applied to Word
Sense Induction. In Proceedings of CICLing, pages
368?381.
Nancy Ide and Keith Suderman. 2004. The american
national corpus first release. In Proceedings of the
Fourth Language Resources and Evaluation Confer-
ence, pages 1681?1684.
David Jurgens. 2012. An Evaluation of Graded Sense
Disambiguation using Word Sense Induction. In Pro-
ceedings of *SEM, the First Joint Conference on Lexi-
cal and Computational Semantics. ACL.
David Jurgens. 2013. Embracing Ambiguity: A Com-
parison of Annotation Methodologies for Crowdsourc-
ing Word Sense Labels. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL). ACL.
Adam Kilgarriff. 2002. English lexical sample
task description. In Proceedings of ACL-SIGLEX
SENSEVAL-2 Workshop.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage, Beverly Hills, CA.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology. Sage, Thousand Oaks,
CA, second edition.
Ravi Kumar and Sergei Vassilvitskii. 2010. General-
ized distances between rankings. In Proceedings of
the 19th International Conference on World Wide Web
(WWW), pages 571?580. ACM.
Andrea Lancichinetti, Santo Fortunato, and Ja?nos
Kerte?sz. 2009. Detecting the overlapping and hierar-
chical community structure in complex networks. New
Journal of Physics, 11(3):033015.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for computational Linguistics (EACL 2012).
Ping Luo, Hui Xiong, Guoxing Zhan, Junjie Wu, and
Zhongzhi Shi. 2009. Information-theoretic distance
measures for clustering validation: Generalization and
normalization. IEEE Transactions on Knowledge and
Data Engineering, 21(9):1249?1262.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
task 14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68. ACL.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-of-
speech induction. In Proceedings of Advances in Neu-
ral Information Processing Systems (NIPS).
298
Aaron F. McDaid, Derek Greene, and Neil Hurley. 2011.
Normalized mutual information to evaluate overlap-
ping community finding algorithms. arXiv:1110.2515.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text, pages 25?28. ACL.
Alistair Moffat and Justin Zobel. 2008. Rank-biased
precision for measurement of retrieval effectiveness.
ACM Transactions on Information Systems (TOIS),
27(1):2.
G. Craig Murray and Rebecca Green. 2004. Lexical
knowledge and human disagreement on a wsd task.
Computer Speech & Language, 18(3):209?222.
Roberto Navigli, David Jurgens, and Daniele Vanilla.
2013. Semeval-2013 task 12: Multilingual word sense
disambiguation. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation.
Roberto Navigli. 2009. Word Sense Disambiguation: A
Survey. ACM Computing Surveys, 41(2):1?69.
Rebecca Passonneau, Nizar Habash, and Owen Rambow.
2006. Inter-annotator agreement on a multilingual se-
mantic annotation task. In Proceedings of the Fifth
International Conference on Language Resources and
Evaluation (LREC), pages 1951?1956.
Rebecca J Passonneau, Collin Baker, Christiane Fell-
baum, and Nancy Ide. 2012a. The MASC word sense
sentence corpus. In Proceedings of LREC.
Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-
Aouissi, and Nancy Ide. 2012b. Multiplicity and
word sense: evaluating and learning from multiply la-
beled word sense annotations. Language Resources
and Evaluation, pages 1?34.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 task 17: English
lexical sample, SRL, and all-words. In Proceedings of
the 4th International Workshop on Semantic Evalua-
tions. ACL.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL). ACL.
Anna Rumshisky, Nick Botchan, Sophie Kushkuley, and
James Pustejovsky. 2012. Word sense inventories by
non-experts. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC).
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Jean Ve?ronis. 1998. A study of polysemy judgments and
inter-annotator agreement. In Program and advanced
papers of the Senseval workshop.
Nguyen Xuan Vinh, Julien Epps, and James Bailey.
2010. Information theoretic measures for clusterings
comparison: Variants, properties, normalization and
correction for chance. The Journal of Machine Learn-
ing Research, 11:2837?2854.
Deniz Yuret. 2012. FASTSUBS: An Efcient Admissible
Algorithm for Finding the Most Likely Lexical Substi-
tutes Using a Statistical Language Model. Computing
Research Repository (CoRR).
299
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17?26,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 3: Cross-Level Semantic Similarity
David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
{jurgens,pilehvar,navigli}@di.uniroma1.it
Abstract
This paper introduces a new SemEval
task on Cross-Level Semantic Similarity
(CLSS), which measures the degree to
which the meaning of a larger linguistic
item, such as a paragraph, is captured by
a smaller item, such as a sentence. High-
quality data sets were constructed for four
comparison types using multi-stage an-
notation procedures with a graded scale
of similarity. Nineteen teams submitted
38 systems. Most systems surpassed the
baseline performance, with several attain-
ing high performance for multiple com-
parison types. Further, our results show
that comparisons of semantic representa-
tion increase performance beyond what is
possible with text alone.
1 Introduction
Given two linguistic items, semantic similarity
measures the degree to which the two items have
the same meaning. Semantic similarity is an es-
sential component of many applications in Nat-
ural Language Processing (NLP), and similarity
measurements between all types of text as well
as between word senses lend themselves to a va-
riety of NLP tasks such as information retrieval
(Hliaoutakis et al., 2006) or paraphrasing (Glick-
man and Dagan, 2003).
Semantic similarity evaluations have largely fo-
cused on comparing similar types of lexical items.
Most recently, tasks in SemEval (Agirre et al.,
2012) and *SEM (Agirre et al., 2013) have intro-
duced benchmarks for measuring Semantic Tex-
tual Similarity (STS) between similar-sized sen-
tences and phrases. Other data sets such as that
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
of Rubenstein and Goodenough (1965) measure
similarity between word pairs, while the data sets
of Navigli (2006) and Kilgarriff (2001) offer a bi-
nary similar-dissimilar distinction between senses.
Notably, all of these evaluations have focused on
comparisons between a single type, in contrast to
application-based evaluations such as summariza-
tion and compositionality which incorporate tex-
tual items of different sizes, e.g., measuring the
quality of a paragraph?s sentence summarization.
Task 3 introduces a new evaluation where sim-
ilarity is measured between items of different
types: paragraphs, sentences, phrases, words and
senses. Given an item of the lexically-larger type,
a system measures the degree to which the mean-
ing of the larger item is captured in the smaller
type, e.g., comparing a paragraph to a sentence.
We refer to this task as Cross-Level Semantic Sim-
ilarity (CLSS). A major motivation of this task
is to produce semantic similarity systems that are
able to compare all types of text, thereby free-
ing downstream NLP applications from needing to
consider the type of text being compared. Task 3
enables assessing the extent to which the mean-
ing of the sentence ?do u know where i can watch
free older movies online without download?? is
captured in the phrase ?streaming vintage movies
for free?, or how similar is ?circumscribe? to the
phrase ?beating around the bush.? Furthermore,
by incorporating comparisons of a variety of item
sizes, Task 3 unifies in a single task multiple ob-
jectives from different areas of NLP such as para-
phrasing, summarization, and compositionality.
Because CLSS generalizes STS to items of dif-
ferent types, successful CLSS systems can directly
be applied to all STS-based applications. Fur-
thermore, CLSS systems can be used in other
similarity-based applications such as text simpli-
fication (Specia et al., 2012), keyphrase iden-
tification (Kim et al., 2010), lexical substitu-
tion (McCarthy and Navigli, 2009), summariza-
17
tion (Sp?arck Jones, 2007), gloss-to-sense mapping
(Pilehvar and Navigli, 2014b), and modeling the
semantics of multi-word expressions (Marelli et
al., 2014) or polysemous words (Pilehvar and Nav-
igli, 2014a).
Task 3 was designed with three main objectives.
First, the task should include multiple types of
comparison in order to assess each type?s difficulty
and whether specialized resources are needed for
each. Second, the task should incorporate text
from multiple domains and writing styles to en-
sure that system performance is robust across text
types. Third, the similarity methods should be able
to operate at the sense level, thereby potentially
uniting text- and sense-based similarity methods
within a single framework.
2 Task Description
2.1 Objective
Task 3 is intended to serve as an initial task for
evaluating the capabilities of systems at measuring
all types of semantic similarity, independently of
the size of the text. To accomplish this objective,
systems were presented with items from four com-
parison types: (1) paragraph to sentence, (2) sen-
tence to phrase, (3) phrase to word, and (4) word to
sense. Given a pair of items, a system must assess
the degree to which the meaning of the larger item
is captured in the smaller item. WordNet 3.0 was
chosen as the sense inventory (Fellbaum, 1998).
2.2 Rating Scale
Following previous SemEval tasks (Agirre et al.,
2012; Jurgens et al., 2012), Task 3 recognizes that
two items? similarity may fall within a range of
similarity values, rather than having a binary no-
tion of similar or dissimilar. Initially a six-point
(0?5) scale similar to that used in the STS tasks
was considered (Agirre et al., 2012); however, an-
notators found difficulty in deciding between the
lower-similarity options. After multiple revisions
and feedback from a group of initial annotators,
we developed a five-point Likert scale for rating a
pair?s similarity, shown in Table 1.
1
The scale was designed to systematically order
a broad range of semantic relations: synonymy,
similarity, relatedness, topical association, and un-
relatedness. Because items are of different sizes,
the highest rating is defined as very similar rather
1
Annotation materials along with all training and test
data are available on the task website http://alt.qcri.
org/semeval2014/task3/.
than identical to allow for some small loss in the
overall meaning. Furthermore, although the scale
is designed as a Likert scale, annotators were given
flexibility when rating items to use values between
the defined points in the scale, indicating a blend
of two relations. Table 2 provides examples of
pairs for each scale rating for all four comparison
type.
3 Task Data
Though several data sets exist for STS and com-
paring words and senses, no standard data set ex-
ists for CLSS. Therefore, we created a pilot data
set designed to test the capabilities of systems in a
variety of settings. The task data for all compar-
isons but word-to-sense was created using a three-
phase process. First, items of all sizes were se-
lected from publicly-available data sets. Second,
the selected items were used to produce a second
item of the next-smaller level (e.g., a sentence in-
spires a phrase). Third, the pairs of items were
annotated for their similarity. Because of the ex-
pertise required for working with word senses, the
word-to-sense data set was constructed by the or-
ganizers using a separate but similar process. In
the training and test data, each comparison type
had 500 annotated examples, for a total of 2000
pairs each for training and test. We first describe
the corpora used by Task 3 followed by the anno-
tation process. We then describe the construction
of the word-to-sense data set.
3.1 Corpora
Test and training data were constructed by draw-
ing from multiple publicly-available corpora and
then manually generating a paired item for com-
parison. To achieve our second objective for the
task, the data sets used to create item pairs in-
cluded texts from specific domains, social media,
and text with idiomatic or slang language. Table
3 summarizes the corpora and their distribution
across the test and training sets for each compari-
son type, with a high-level description of the genre
of the data. We briefly describe the corpora next.
The WikiNews, Reuters 21578, and Microsoft
Research (MSR) Paraphrase corpora are all drawn
from newswire text, with WikiNews being au-
thored by volunteer writers and the latter two cor-
pora written by professionals. Travel Guides was
drawn from the Berlitz travel guides data in the
Open American National Corpus (Ide and Suder-
man, 2004) and includes very verbose sentences
18
4 ? Very
Similar
The two items have very similar meanings and the most important ideas, concepts, or actions in the larger
text are represented in the smaller text. Some less important information may be missing, but the smaller
text is a very good summary of the larger text.
3 ? Somewhat
Similar
The two items share many of the same important ideas, concepts, or actions, but include slightly different
details. The smaller text may use similar but not identical concepts (e.g., car vs. vehicle), or may omit a
few of the more important ideas present in the larger text.
2 ? Somewhat
related but not
similar
The two items have dissimilar meaning, but share concepts, ideas, and actions that are related. The smaller
text may use related but not necessarily similar concepts (window vs. house) but should still share some
overlapping concepts, ideas, or actions with the larger text.
1 ? Slightly
related
The two items describe dissimilar concepts, ideas and actions, but may share some small details or domain
in common and might be likely to be found together in a longer document on the same topic.
0 ? Unrelated The two items do not mean the same thing and are not on the same topic.
Table 1: The five-point Likert scale used to rate the similarity of item pairs. See Table 2 for examples.
with many named entities. Wikipedia Science
was drawn from articles tagged with the cate-
gory Science on Wikipedia. Food reviews were
drawn from the SNAP Amazon Fine Food Re-
views data set (McAuley and Leskovec, 2013)
and are customer-authored reviews for a variety of
food items. Fables were taken from a collection of
Aesop?s Fables. The Yahoo! Answers corpus was
derived from the Yahoo! Answers data set, which
is a collection of questions and answers from the
Community Question Answering (CQA) site; the
data set is notable for having the highest degree of
ungrammaticality in our test set. SMT Europarl
is a collection of texts from the English-language
proceedings of the European parliament (Koehn,
2005); Europarl data was also used in the PPDB
corpus (Ganitkevitch et al., 2013), from which
phrases were extracted. Wikipedia was used to
generate two phrase data sets from (1) extracting
the definitional portion of an article?s initial sen-
tence, e.g., ?An [article name] is a [definition],?
and (2) captions for an article?s images. Web
queries were gathered from online sources of real-
world queries. Last, the first and second authors
generated slang and idiomatic phrases based on
expressions contained in Wiktionary.
For all comparison types, the test data included
one genre that was not seen in the training data
in order to test the generalizability of the systems
on data from a novel domain. In addition, we
included a new type of challenge genre with Fa-
bles; unlike other domains, the sentences paired
with the fable paragraphs were potentially seman-
tic interpretations of the intent of the fable, i.e.,
the moral of the story. These interpretations often
have little textual overlap with the fable itself and
require a deeper interpretation of the paragraph?s
meaning in order to make the correct similarity
judgment.
Prior to the annotation process, all content was
filtered to ensure its size and format matched the
desired text type. By average, a paragraph in our
dataset consists of 3.8 sentences. Typos and gram-
matical mistakes in the community-produced con-
tent were left unchanged.
3.2 Annotation Process
A two-phase process was used to produce the test
and training data sets for all but word-to-sense.
Phase 1 generates the item pairs from source texts
and Phase 2 rates the pairs? similarity.
Phase 1 In this phase, annotators were shown the
larger text of a comparison type and then asked
to produce the smaller text of the pair at a spec-
ified similarity; for example an annotator may be
shown a paragraph and asked to write a sentence
that is a ?3? rating. Annotators were instructed to
leave the smaller text blank if they had difficulty
understanding the larger text.
The requested similarity ratings were balanced
to create a uniform distribution of similarity val-
ues. Annotators were asked only to generate rat-
ings of 1?4; pairs with a ?0? rating were automat-
ically created by pairing the larger item with ran-
dom selections of text of the appropriate size from
the same corpus. The intent of Phase 1 is to pro-
duce varied item pairs with an expected uniform
distribution of similarity values along the rating
scale.
Four annotators participated in Phase 1 and
were paid a bulk rate of e110 for completing the
work. In addition to the four annotators, the first
two organizers also assisted in Phase 1: Both com-
pleted items from the SCIENTIFIC genre and the
first organizer produced 994 pairs, including all
19
PARAGRAPH TO SENTENCE
Paragraph: Teenagers take aerial shots of their neigh-
bourhood using digital cameras sitting in old bottles which
are launched via kites - a common toy for children liv-
ing in the favelas. They then use GPS-enabled smart-
phones to take pictures of specific danger points - such as
rubbish heaps, which can become a breeding ground for
mosquitoes carrying dengue fever.
Rating Sentence
4 Students use their GPS-enabled cellphones to
take birdview photographs of a land in order
to find specific danger points such as rubbish
heaps.
3 Teenagers are enthusiastic about taking aerial
photograph in order to study their neighbour-
hood.
2 Aerial photography is a great way to identify
terrestrial features that aren?t visible from the
ground level, such as lake contours or river
paths.
1 During the early days of digital SLRs, Canon
was pretty much the undisputed leader in
CMOS image sensor technology.
0 Syrian President Bashar al-Assad tells the US
it will ?pay the price? if it strikes against Syria.
SENTENCE TO PHRASE
Sentence: Schumacher was undoubtedly one of the very
greatest racing drivers there has ever been, a man who was
routinely, on every lap, able to dance on a limit accessible
to almost no-one else.
Rating Phrase
4 the unparalleled greatness of Schumacher?s
driving abilities
3 driving abilities
2 formula one racing
1 north-south highway
0 orthodontic insurance
PHRASE TO WORD
Phrase: loss of air pressure in a tire
Rating Word
4 flat-tire
3 deflation
2 wheel
1 parking
0 butterfly
WORD TO SENSE
Word: automobile
n
Rating Sense
4 car
1
n
(a motor vehicle with four wheels; usually
propelled by an internal combustion engine)
3 vehicle
1
n
(a conveyance that transports people
or objects)
2 bike
1
n
(a motor vehicle with two wheels and a
strong frame)
1 highway
1
n
(a major road for any form of motor
transport)
0 pen
1
n
(a writing implement with a point from
which ink flows)
Table 2: Example pairs and their ratings.
those for the METAPHORIC genre, and those that
the other annotators left blank.
Phase 2 Here, the item pairs produced in Phase
1 were rated for their similarity according to the
scale described in Section 2.2. An initial pilot
study showed that crowdsourcing was only mod-
erately effective for producing these ratings with
high agreement. Furthermore, the texts used in
Task 3 came from a variety of genres, such as
scientific domains, which some workers had dif-
ficulty understanding. While we note that crowd-
sourcing has been used in prior STS tasks for
generating similarity scores (Agirre et al., 2012;
Agirre et al., 2013), both tasks? efforts encoun-
tered lower worker score correlations on some por-
tions of the dataset (Diab, 2013), suggesting that
crowdsourcing may not be reliable for judging the
similarity of certain types of text. See Section 3.5
for additional details.
Therefore, to ensure high quality, the first two
organizers rated all items independently. Because
the sentence-to-phrase and phrase-to-word com-
parisons contain slang and idiomatic language, a
third American English mother tongue annotator
was added for those data sets. The third annotator
was compensated e250 for their assistance.
Annotators were allowed to make finer-grained
distinctions in similarity using multiples of 0.25.
For all items, when any two annotators disagreed
by one or more scale points, we performed an
adjudication to determine the item?s rating in the
gold standard. The adjudication process revealed
that nearly all disagreements were due to annota-
tor mistakes, e.g., where one annotator had over-
looked a part of the text or had misunderstood the
text?s meaning. The final similarity rating for an
unadjudicated item was the average of its ratings.
3.3 Word-to-Sense
Word-to-sense comparison items were generated
in three phases. To increase the diversity and
challenge of the data set, the word-to-sense was
created for four types of words: (1) a word and
its intended meaning are in WordNet, (2) a word
was not in the WordNet vocabulary, e.g., the verb
?zombify,? (3) the word is in WordNet, but has a
novel meaning that is not in WordNet, e.g., the ad-
jective ?red? referring to Communist, and (4) a set
of challenge words where one of the word?s senses
and a second sense are directly connected by an
edge in the WordNet network, but the two senses
are not always highly similar.
20
Paragraph-to-Sentence Sentence-to-Phrase Phrase-to-Word
Corpus Genre Train Test Train Test Train Test
WikiNews Newswire 15.0 10.0 9.2 6.0
Reuters 21578 Newswire 20.2 15.0 5.0
Travel Guides Travel 15.2 10.0 15.0 9.8
Wikipedia Science Scientific ? 25.6 ? 14.8
Food Reviews Review 19.6 20.0
Fables Metaphoric 9.0 5.2
Yahoo! Answers CQA 21.0 14.2 17.6 17.4
SMT Europarl Newswire 35.4 14.4
MSR Paraphrase Newswire 10.0 10.0 8.8 6.0
Idioms Idiomatic 12.8 12.6 20.0 20.0
Slang Slang ? 15.0 ? 25.0
PPDB Newswire 10.0 10.0
Wikipedia Glosses Lexicographic 28.2 17.0
Wikipedia Image Captions Descriptive 23.0 17.0
Web Search Queries Search 5.0 5.0
Table 3: Percentages of the training and test data per source corpus.
In Phase 1, to select the first type of word,
lemmas in WordNet were ranked by frequency
in Wikipedia; the ranking was divided into ten
equally-sized groups, with words sampled evenly
from groups in order to control for word frequency
in the task data. For the second type, words not
present in WordNet were drawn from two sources:
examining words in Wikipedia, which we refer
to as out-of-vocabulary (OOV), and slang words.
For the third type, to identify words with a novel
sense, we examined Wiktionary entries and chose
novel, salient senses that were distinct from those
in WordNet. We refer to words with a novel mean-
ing as out-of-sense (OOS). Words of the fourth
type were chosen by hand. The part-of-speech dis-
tributions for all four types of items were balanced
as 50% noun, 25% verb, 25% adjective.
In Phase 2, each word was associated with a
particular WordNet sense for its intended mean-
ing, or the closest available sense in WordNet
for OOV or OOS items. To select a comparison
sense, we adopted a neighborhood search proce-
dure: All synsets connected by at most three edges
in the WordNet semantic network were shown.
Given a word and its neighborhood, the corre-
sponding sense for the item pair was selected by
matching the sense with an intended similarity for
the pair, much like how text items were gener-
ated in Phase 1. The reason behind using this
neighborhood-based selection process was to min-
imize the potential bias of consistently selecting
lower-similarity items from those further away in
the WordNet semantic network.
In Phase 3, given all word-sense pairs, annota-
tors were shown the definitions associated with the
intended meaning of the word and of the sense.
Definitions were drawn from WordNet or from
Wiktionary, if the word was OOV or OOS. An-
notators had access to the WordNet structure for
the compared sense in order to take into account
its parents and siblings.
3.4 Trial Data
The trial data set was created using a separate
process. Source text was drawn from WikiNews;
we selected the text for the larger item of each
level and then generated the text or sense of the
smaller. A total of 156 items were produced.
After, four fluent annotators independently rated
all items. Inter-annotator agreement rates varied
in 0.734?0.882, using Krippendorff?s ? (Krippen-
dorff, 2004) on the interval scale.
3.5 Data Set Discussion
The resulting annotation process produced a high-
quality data set. First, Table 4 shows the inter-
annotator agreement (IAA) statistics for each
comparison type on both the full and unadjudi-
cated portions of the data set. IAA was measured
using Krippendorff?s ? for interval data. Because
the disagreements that led to lower ? in the full
data were resolved via adjudication, the quality of
the full data set is expected to be on par with that
of the unadjudicated data. The annotation quality
for Task 3 was further improved by manually ad-
judicating all significant disagreements.
In contrast, the data sets of current STS tasks
aggregated data from annotators with moderate
correlation with each other (Diab, 2013); STS-
2012 (Agirre et al., 2012) saw inter-annotator
Pearson correlations of 0.530?0.874 per data set
and STS-2013 (Agirre et al., 2013) had average
21
Training Test
Data All Unadj. All Unadj.
Para.-to-Sent. 0.856 0.916 0.904 0.971
Sent.-to-Phr. 0.773 0.913 0.766 0.980
Phr.-to-Word 0.735 0.895 0.730 0.988
Word-to-Sense 0.681 0.895 0.655 0.952
Table 4: IAA rates for the task data.
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Sco
ring
 sc
ale
Paragraph-to-Sentence
TrainingTest
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Sentence-to-Phrase
TrainingTest
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Sco
ring
 sc
ale
Phrase-to-Word
TrainingTest
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
100 200 300 400 500
Word-to-Sense
TrainingTest
Figure 1: Similarity ratings distributions.
inter-annotator correlations of 0.377?0.832. How-
ever, we note that Pearson correlation and Krip-
pendorff?s ? are not directly comparable (Artstein
and Poesio, 2008), as annotators? scores may be
correlated, but completely disagree.
Second, the two-phase construction process
produced values that were evenly distributed
across the rating scale, shown in Figure 1 as the
distribution of the values for all data sets. How-
ever, we note that this creation procedure was very
resource intensive and, therefore, semi-automated
or crowdsourcing-based approaches for produc-
ing high-quality data will be needed to expand
the size of the data in future CLSS-based eval-
uations. Nevertheless, as a pilot task, the man-
ual effort was essential for ensuring a rigorously-
constructed data set for the initial evaluation.
4 Evaluation
Participation The ultimate goal of Task 3 is to
produce systems that can measure similarity for
multiple types of items. Therefore, we strongly
encouraged participating teams to submit systems
that were capable of generating similarity judg-
ments for multiple comparison types. However,
to further the analysis, participants were also per-
mitted to submit systems specialized to a single
domain. Teams were allowed at most three system
submissions, regardless of the number of compar-
ison types supported.
Scoring Systems were required to provide sim-
ilarity values for all items within a comparison
type. Following prior STS evaluations, systems
were scored for each comparison type using Pear-
son correlation. Additionally, we include a second
score using Spearman?s rank correlation, which is
only affected by differences in the ranking of items
by similarity, rather than differences in the similar-
ity values. Pearson correlation was chosen as the
official evaluation metric since the goal of the task
is to produce similar scores. However, Spearman?s
rank correlation provides an important metric for
assessing systems whose scores do not match hu-
man scores but whose rankings might, e.g., string-
similarity measures. Ultimately, a global ranking
was produced by ordering systems by the sum of
their Pearson correlation values for each of the
four comparison levels.
Baselines The official baseline system was
based on the Longest Common Substring (LCS),
normalized by the length of items using the
method of Clough and Stevenson (2011). Given
a pair, the similarity is reported as the normalized
length of the LCS. In the case of word-to-sense,
the LCS for a word-sense pair is measured be-
tween the sense?s definition in WordNet and the
definitions of each sense of the pair?s word, report-
ing the maximal LCS. Because OOV and slang
words are not in WordNet, the baseline reports the
average similarity value of non-OOV items. Base-
line scores were made public after the evaluation
period ended.
Because LCS is a simple procedure, a second
baseline based on Greedy String Tiling (GST)
(Wise, 1996) was added after the evaluation pe-
riod concluded. Unlike LCS, GST better handles
the transpositions of tokens across the two texts
and can still report high similarity when encoun-
tering reordered text. The minimum match length
for GST was set to 6.
5 Results
Nineteen teams submitted 38 systems. Of those
systems, 34 produced values for paragraph-to-
sentence and sentence-to-phrase comparisons, 22
for phrase-to-word, and 20 for word-to-sense.
Two teams submitted revised scores for their sys-
tems after the deadline but before the test set had
22
Team System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank Spearman Rank
Meerkat Mafia pairingWords? 0.794 0.704 0.457 0.389
SimCompass run1 0.811 0.742 0.415 0.356 1 1
ECNU run1 0.834 0.771 0.315 0.269 2 2
UNAL-NLP run2 0.837 0.738 0.274 0.256 3 6
SemantiKLUE run1 0.817 0.754 0.215 0.314 4 4
UNAL-NLP run1 0.817 0.739 0.252 0.249 5 7
UNIBA run2 0.784 0.734 0.255 0.180 6 8
RTM-DCU run1? 0.845 0.750 0.305
UNIBA run1 0.769 0.729 0.229 0.165 7 10
UNIBA run3 0.769 0.729 0.229 0.165 8 11
BUAP run1 0.805 0.714 0.162 0.201 9 13
BUAP run2 0.805 0.714 0.142 0.194 10 9
Meerkat Mafia pairingWords 0.794 0.704 -0.044 0.389 11 12
HULTECH run1 0.693 0.665 0.254 0.150 12 16
GST Baseline 0.728 0.662 0.146 0.185
HULTECH run3 0.669 0.671 0.232 0.137 13 15
RTM-DCU run2? 0.785 0.698 0.221
RTM-DCU run3 0.780 0.677 0.208 14 17
HULTECH run2 0.667 0.633 0.180 0.169 15 14
RTM-DCU run1 0.786 0.666 0.171 16 18
RTM-DCU run3? 0.786 0.663 0.171
Meerkat Mafia SuperSaiyan 0.834 0.777 17 19
Meerkat Mafia Hulk2 0.826 0.705 18 20
RTM-DCU run2 0.747 0.588 0.164 19 22
FBK-TR run3 0.759 0.702 20 23
FBK-TR run1 0.751 0.685 21 24
FBK-TR run2 0.770 0.648 22 25
Duluth Duluth2 0.501 0.450 0.241 0.219 23 21
AI-KU run1 0.732 0.680 24 26
LCS Baseline 0.527 0.562 0.165 0.109
UNAL-NLP run3 0.708 0.620 25 27
AI-KU run2 0.698 0.617 26 28
TCDSCSS run2 0.607 0.552 27 29
JU-Evora run1 0.536 0.442 0.090 0.091 28 31
TCDSCSS run1 0.575 0.541 29 30
Duluth Duluth1 0.458 0.440 0.075 0.076 30 5
Duluth Duluth3 0.455 0.426 0.075 0.079 31 3
OPI run1 0.433 0.213 0.152 32 36
SSMT run1 0.789 33 34
DIT run1 0.785 34 32
DIT run2 0.784 35 33
UMCC DLSI SelSim run1 0.760 36 35
UMCC DLSI SelSim run2 0.698 37 37
UMCC DLSI Prob run1 0.023 38 38
Table 5: Task results. Systems marked with a ? were submitted after the deadline but are positioned
where they would have ranked.
been released. These systems were scored and
noted in the results but were not included in the
official ranking.
Table 5 shows the performance of the participat-
ing systems across all the four comparison types in
terms of Pearson correlation. The two right-most
columns show system rankings by Pearson (Offi-
cial Rank) and Spearman?s ranks correlation.
The SimCompass system attained first place,
partially due to its superior performance on
phrase-to-word comparisons, providing an im-
provement of 0.10 over the second-best sys-
tem. The late-submitted version of the Meerkat
Mafia pairingWords? system corrected a bug in
the phrase-to-word comparison, which ultimately
would have attained first place due to large per-
formance improvements over SimCompass on
phrase-to-word and word-to-sense. ENCU and
UNAL-NLP systems rank respectively second and
third while the former being always in top-4 and
the latter being among the top-7 systems across the
four comparison types. Most systems were able
to surpass the naive LCS baseline; however, the
more sophisticated GST baseline (which accounts
for text transposition) outperforms two-thirds of
the systems. Importantly, both baselines perform
23
poorly on smaller text, highlighting the impor-
tance of performing a semantic comparison, as op-
posed to a string-based one.
Within the individual comparison types, spe-
cialized systems performed well for the larger
text sizes. In the paragraph-to-sentence type, the
run1 system of UNAL-NLP provides the best of-
ficial result, with the late RTM-DCU run1? sys-
tem surpassing its performance slightly. Meerkat
Mafia provides the best performance in sentence-
to-phrase with its SuperSaiyan system and the
best performances in phrase-to-word and word-to-
sense with its late pairingWords? system.
Comparison-Type Analysis Performance
across the comparison types varied considerably,
with systems performing best on comparisons
between longer textual items. As a general trend,
both the baselines? and systems? performances
tend to decrease with the size of lexical items
in the comparison types. A main contributing
factor to this is the reliance on textual similarity
measures (such as the baselines), which perform
well when two items? may share content. How-
ever, as the items? content becomes smaller, e.g.,
a word or phrase, the textual similarity does not
necessarily provide a meaningful indication of
the semantic similarity between the two. This
performance discrepancy suggests that, in order
to perform well, CLSS systems must rely on
comparisons between semantic representations
rather than textual representations. The two
top-performing systems on these smaller levels,
Meerkat Mafia and SimCompass, used additional
resources beyond WordNet to expand a word or
sense to its definition or to represent words with
distributional representations.
Per-genre results and discussions Task 3 in-
cludes multiple genres within the data set for each
comparison type. Figure 2 shows the correlation
of each system for each of these genres, with sys-
tems ordered left to right according to their official
ranking in Table 5. An interesting observation is
that a system?s official rank does not always match
the rank from aggregating its correlations for each
genre individually. This difference suggests that
some systems provided good similarity judgments
on individual genres, but their range of similarity
values was not consistent between genres leading
to lower overall Pearson correlation. For instance,
in the phrase-to-word comparison type, the ag-
gregated per-genre performance of Duluth-1 and
Duluth-3 are among the best whereas their over-
all Pearson performance puts these systems among
the worst-performing ones in the comparison type.
Among the genres, CQA, SLANG, and ID-
IOMATIC prove to be the more difficult for sys-
tems to interpret and judge. These genres in-
cluded misspelled, colloquial, or slang language
which required converting the text into semantic
form in order to meaningfully compare it. Fur-
thermore, as expected, the METAPHORIC genre
was the most difficult, with no system perform-
ing well; we view the METAPHORIC genre as an
open challenge for future systems to address when
interpreting larger text. On the other hand, SCI-
ENTIFIC, TRAVEL, and NEWSWIRE tend to be
the easiest genres for paragraph-to-sentence and
sentence-to-phrase. All three genres tend to in-
clude many named entities or highly-specific lan-
guage, which are likely to be more preserved in the
more-similar paired items. Similarly, DESCRIP-
TIVE and SEARCH genres were easiest in phrase-
to-word, which also often featured specific words
that were preserved in highly-similar pairs. In
the case of word-to-sense, REGULAR proves to be
the least difficult genre. Interestingly, in word-
to-sense, most systems attained moderate perfor-
mance for comparisons with words not in Word-
Net (i.e., OOV) but had poor performance for
slang words, which were also OOV. This differ-
ence suggests that systems could be improved with
additional semantic resources for slang.
Spearman Rank Analysis Although the goal of
Task 3 is to have systems produce similarity judg-
ments, some applications may benefit from simply
having a ranking of pairs, e.g., ranking summa-
rizations by goodness. The Spearman rank corre-
lation measures the ability of systems to perform
such a ranking. Surprisingly, with the Spearman-
based ranking, the Duluth1 and Duluth3 systems
attain the third and fifth ranks ? despite being
among the lowest ranked with Pearson. Both sys-
tems were unsupervised and produced similarity
values that did not correlate well with those of
humans. However, their Spearman ranks demon-
strate the systems ability to correctly identify rela-
tive similarity and suggests that such unsupervised
systems could improve their Pearson correlation
by using the training data to tune the range of sim-
ilarity values to match those of humans.
24
 0
 1
 2
 3
 4
 5
UNAL-NLP-2
ECNU-1
Meerkat_Mafia-SS
Meerkat_Mafia-H
SemantiKLUE-1
UNAL-NLP-1
SimCompass-1
BUAP-1
BUAP-2
Meerkat_Mafia-PW
SSMT-1
RTM-DCU-1
DIT-1DIT-2UNIBA-2
RTM-DCU-3
FBK-TR-2
UNIBA-1
UNIBA-3
FBK-TR-3
FBK-TR-1
RTM-DCU-2
AI-KU-1
UNAL-NLP-3
AI-KU-2
HULTECH-1
HULTECH-3
HULTECH-2
TCDSCSS-2
TCDSCSS-1
JU-Evora-1
Duluth-2
Duluth-1
Duluth-3
(a)
 Pa
rag
rap
h-t
o-S
en
ten
ce
  
Co
rre
lat
ion
s
pe
r g
en
re
CQAReview
TravelNewswire
ScientificMetaphoric
 0
 1
 2
 3
 4
 5
Meerkat_Mafia-SS
ECNU-1
UMCC_DLSI_SelSim-1
SemantiKLUE-1
SimCompass-1
UNAL-NLP-1
UNAL-NLP-2
UNIBA-2
UNIBA-1
UNIBA-3
BUAP-1
BUAP-2
Meerkat_Mafia-H
Meerkat_Mafia-PW
FBK-TR-3
UMCC_DLSI_SelSim-2
FBK-TR-1
AI-KU-1
RTM-DCU-3
HULTECH-3
RTM-DCU-1
HULTECH-1
FBK-TR-2
HULTECH-2
UNAL-NLP-3
AI-KU-2
RTM-DCU-2
TCDSCSS-2
TCDSCSS-1
Duluth-2
JU-Evora-1
Duluth-1
OPI-1
Duluth-3
(b)
 Se
nte
nc
e-t
o-P
hra
se
    
Co
rre
lat
ion
s
pe
r g
en
re
ScientificCQA
IdiomaticSlang
TravelNewswire
 0
 1
 2
 3
SimCompass-1
ECNU-1
UNAL-NLP-2
UNIBA-2
HULTECH-1
UNAL-NLP-1
Duluth-2
HULTECH-3
UNIBA-1
UNIBA-3
SemantiKLUE-1
OPI-1
RTM-DCU-3
HULTECH-2
RTM-DCU-1
RTM-DCU-2
BUAP-1
BUAP-2
JU-Evora-1
Duluth-1
Duluth-3
Meerkat_Mafia-PW
(c)
 Ph
ras
e-t
o-W
ord
    
  
Co
rre
lat
ion
s
pe
r g
en
re
SlangNewswire
IdiomaticDescriptive
LexicographicSearch
 0
 1
 2
Meerkat_Mafia-PW
SimCompass-1
SemantiKLUE-1
ECNU-1
UNAL-NLP-2
UNAL-NLP-1
Duluth-2
BUAP-1
BUAP-2
UNIBA-2
HULTECH-2
UNIBA-1
UNIBA-3
OPI-1
HULTECH-1
HULTECH-3
JU-Evora-1
Duluth-3
Duluth-1
UMCC_DLSI_Prob-1
(d)
 W
ord
-to
-Se
ns
e
Co
rre
lat
ion
s
pe
r g
en
re
OOSOOV
regularregular-challenge
Slang
Figure 2: A stacked histogram for each system, showing its Pearson correlations for genre-specific por-
tions of the gold-standard data, which may also be negative.
6 Conclusion
This paper introduces a new similarity task, Cross-
Level Semantic Similarity, for measuring the se-
mantic similarity of lexical items of different
sizes. Using a multi-phase annotation proce-
dure, we have produced a high-quality data set of
4000 items comprising of various genres, evenly-
split between training and test with four types of
comparison: paragraph-to-sentence, sentence-to-
phrase, phrase-to-word, and word-to-sense. Nine-
teen teams submitted 38 systems, with most teams
surpassing the baseline system and several sys-
tems achieving high performance in multiple types
of comparison. However, a clear performance
trend emerged where systems perform well only
when the text itself is similar, rather than its under-
lying meaning. Nevertheless, the results of Task 3
are highly encouraging and point to clear future
objectives for developing CLSS systems that op-
erate on more semantic representations rather than
text. In future work on CLSS evaluation, we first
intend to develop scalable annotation methods to
increase the data sets. Second, we plan to add new
evaluations where systems are tested according to
their performance in an application related to each
comparison-type, such as measuring the quality of
a paraphrase or summary.
Acknowledgments
We would like to thank Tiziano Flati, Marc Franco Salvador,
Maud Erhmann, and Andrea Moro for their help in preparing
the trial data; Gaby Ford, Chelsea Smith, and Eve Atkinson
for their help in generating the training and test data; and
Amy Templin for her help in generating and rating the train-
ing and test data.
The authors gratefully acknowledge the
support of the ERC Starting Grant Multi-
JEDI No. 259234.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-
Agirre. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval-2012), pages
385?393, Montr?eal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared Task:
Semantic textual similarity, including a pilot on typed-
similarity. In Proceedings of the Second Joint Confer-
25
ence on Lexical and Computational Semantics (*SEM),
Atlanta, Georgia.
Ron Artstein and Massimo Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational Lin-
guistics, 34(4):555?596.
Paul Clough and Mark Stevenson. 2011. Developing a cor-
pus of plagiarised short answers. Language Resources
and Evaluation, 45(1):5?24.
Mona Diab. 2013. Semantic textual similarity: past present
and future. In Joint Symposium on Semantic Process-
ing. Keynote address. http://jssp2013.fbk.eu/
sites/jssp2013.fbk.eu/files/Mona.pdf.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-
Burch. 2013. PPDB: The paraphrase database. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-HLT),
pages 758?764, Atlanta, Georgia.
Oren Glickman and Ido Dagan. 2003. Acquiring lexical
paraphrases from a single corpus. In Proceedings of the
International Conference on Recent Advances in Natural
Language Processing (RANLP), pages 81?90, Borovets,
Bulgaria.
Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis,
Euripides GM Petrakis, and Evangelos Milios. 2006.
Information retrieval by semantic similarity. Interna-
tional Journal on Semantic Web and Information Systems,
2(3):55?73.
Nancy Ide and K. Suderman. 2004. The American Na-
tional Corpus First Release. In Proceedings of the 4
th
Language Resources and Evaluation Conference (LREC),
pages 1681?1684, Lisbon, Portugal.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. SemEval-2012 Task 2: Measuring
Degrees of Relational Similarity. In Proceedings of
the 6th International Workshop on Semantic Evaluation
(SemEval-2012), pages 356?364, Montr?eal, Canada.
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In The Proceedings of the Second International
Workshop on Evaluating Word Sense Disambiguation Sys-
tems (SENSEVAL-2), pages 17?20, Toulouse, France.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010. SemEval-2010 Task 5: Automatic
Keyphrase Extraction from Scientific Articles. In Pro-
ceedings of the 5th International Workshop on Semantic
Evaluation (SemEval-2010), pages 21?26, Los Angeles,
California.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of Machine
Translation Summit X, pages 79?86, Phuket, Thailand.
Klaus Krippendorff. 2004. Content Analysis: An Introduc-
tion to Its Methodology. Sage, Thousand Oaks, CA, sec-
ond edition.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Ben-
tivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014.
SemEval-2014 Task 1: Evaluation of compositional dis-
tributional semantic models on full sentences through se-
mantic relatedness and textual entailment. In Proceedings
of the 8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Julian John McAuley and Jure Leskovec. 2013. From ama-
teurs to connoisseurs: modeling the evolution of user ex-
pertise through online reviews. In Proceedings of the 22nd
International Conference on World Wide Web (WWW),
pages 897?908, Rio de Janeiro, Brazil.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Evalu-
ation, 43(2):139?159.
Roberto Navigli. 2006. Meaningful clustering of senses
helps boost Word Sense Disambiguation performance. In
Proceedings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics (COLING-
ACL), pages 105?112, Sydney, Australia.
Mohammad Taher Pilehvar and Roberto Navigli. 2014a. A
large-scale pseudoword-based evaluation framework for
state-of-the-art Word Sense Disambiguation. Computa-
tional Linguistics, 40(4).
Mohammad Taher Pilehvar and Roberto Navigli. 2014b.
A robust approach to aligning heterogeneous lexical re-
sources. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics, pages 468?
478, Baltimore, USA.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Karen Sp?arck Jones. 2007. Automatic summarising: The
state of the art. Information Processing and Management,
43(6):1449?1481.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.
2012. SemEval-2012 Task 1: English Lexical Simplifica-
tion. In Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval-2012), pages 347?355.
Michael J. Wise. 1996. YAP3: Improved detection of simi-
larities in computer program and other texts. In Proceed-
ings of the twenty-seventh SIGCSE technical symposium
on Computer science education, SIGCSE ?96, pages 130?
134, Philadelphia, Pennsylvania, USA.
26
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 1?6,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Capturing Nonlinear Structure in Word Spaces through Dimensionality
Reduction
David Jurgens
University of California, Los Angeles,
4732 Boelter Hall
Los Angeles, CA 90095
jurgens@cs.ucla.edu
Keith Stevens
University of California, Los Angeles,
4732 Boelter Hall
Los Angeles, CA 90095
kstevens@cs.ucla.edu
Abstract
Dimensionality reduction has been shown
to improve processing and information ex-
traction from high dimensional data. Word
space algorithms typically employ lin-
ear reduction techniques that assume the
space is Euclidean. We investigate the ef-
fects of extracting nonlinear structure in
the word space using Locality Preserv-
ing Projections, a reduction algorithm that
performs manifold learning. We apply
this reduction to two common word space
models and show improved performance
over the original models on benchmarks.
1 Introduction
Vector space models of semantics frequently em-
ploy some form of dimensionality reduction for
improvement in representations or computational
overhead. Many of the dimensionality reduc-
tion algorithms assume that the unreduced word
space is linear. However, word similarities have
been shown to exhibit many non-metric proper-
ties: asymmetry, e.g North Korea is more sim-
ilar to Red China than Red China is to North
Korea, and non-transitivity, e.g. Cuba is similar
the former USSR, Jamaica is similar to Cuba,
but Jamaica is not similar to the USSR (Tversky,
1977). We hypothesize that a non-linear word
space model might more accurately preserve these
non-metric relationships.
To test our hypothesis, we capture the non-
linear structure with dimensionality reduction by
using Locality Preserving Projection (LPP) (He
and Niyogi, 2003), an efficient, linear approxi-
mation of Eigenmaps (Belkin and Niyogi, 2002).
With this reduction, the word space vectors are as-
sumed to exist on a nonlinear manifold that LPP
learns in order to project the vectors into a Eu-
clidean space. We measure the effects of us-
ing LPP on two basic word space models: the
Vector Space Model and a Word Co-occurrence
model. We begin with a brief overview of these
word spaces and common dimensionality reduc-
tion techniques. We then formally introduce LPP.
Following, we use two experiments to demonstrate
LPP?s capacity to accurately dimensionally reduce
word spaces.
2 Word Spaces and Reductions
We consider two common word space models
that have been used with dimensionality reduc-
tion. The first is the Vector Space Model (VSM)
(Salton et al, 1975). Words are represented as
vectors where each dimension corresponds to a
document in the corpus and the dimension?s value
is the number of times the word occurred in the
document. We label the second model the Word
Co-occurrence (WC) model: each dimension cor-
respond to a unique word, with the dimension?s
value indicating the number of times that dimen-
sion?s word co-occurred.
Dimensionality reduction has been applied to
both models for three kinds of benefits: to im-
prove computational efficiency, to capture higher
order relationships between words, and to reduce
noise by smoothing or eliminating noisy features.
We consider three of the most popular reduction
techniques and the general word space models to
which they have been applied: linear projections,
feature elimination and random approximations.
The most frequently applied linear projection
technique is the Singular Value Decomposition
(SVD). The SVD factors a matrix A, which rep-
resents a word space, into three matrices U?V ?
such that ? is a diagonal matrix containing the
singular values of A, ordered descending based on
their effect on the variance in the values of A. The
original matrix can be approximated by using only
the top k singular values, setting all others to 0.
The approximation matrix, A? = Uk?kV ?k , is the
least squares best-fit rank-k approximation of A.
1
The SVD has been used with great success on
both models. Latent Semantic Analysis (LSA)
(Landauer et al, 1998) extends the (VSM) by de-
composing the space using the SVD and mak-
ing the word space the left singular vectors, Uk.
WC models have also utilized the SVD to improve
performance (Schu?tze, 1992; Bullinaria and Levy,
2007; Baroni and Lenci, 2008).
Feature elimination reduces the dimensional-
ity by removing those with low information con-
tent. This approach has been successfully applied
to WC models such as HAL (Lund and Burgess,
1996) by dropping those with low entropy. This
technique effectively removes the feature dimen-
sions of high frequency words, which provide lit-
tle discriminatory content.
Randomized projections have also been suc-
cessfully applied to VSM models, e.g. (Kanerva
et al, 2000) and WC models, e.g. (Sahlgren et al,
2008). This reduction statistically approximates
the original space in a much lower dimensional
space. The projection does not take into account
the structure of data, which provides only a com-
putational benefit from fewer dimensions, unlike
the previous two reductions.
3 Locality Preserving Projection
For a set of vectors, x1, x2, . . . , xn ? Rm, LPP
preserves the distance in the k-dimensional space,
where k ? m, by solving the following minimiza-
tion problem,
min
w
?
ij
(w?xi ?w?xj)2Sij (1)
where w is a transformation vector that projects x
into the lower dimensional space, and S is a ma-
trix that represents the local structure of the origi-
nal space. Minimizing this equation is equivalent
to finding the transformation vector that best pre-
serves the local distances in the original space ac-
cording to S. LPP assumes that the data points xi
exist on a manifold. This is in contrast to the SVD,
which assumes that the space is Euclidean and per-
forms a global, rather than local, minimization. In
treating the space as a manifold, LPP is able to dis-
cover some of the nonlinear structure of the data
from its local structure.
To solve the minimization problem in Equation
1, LPP uses a linear approximation of the Lapla-
cian Eigenmaps procedure (Belkin and Niyogi,
2002) as follows:
1. Let X be a matrix where xi is the ith row vec-
tor. Construct an adjacency matrix, S, which
represents the local structure of the original
vector space, by making an edge between
points xi and xj if xj is locally proximate to
xi. Two variations are available for determin-
ing proximity: either the k-nearest neighbors,
or all the data points with similarity > ?.
2. Weight the edges in S proportional to the
closeness of the data points. Four main op-
tions are available: a Gaussian kernel, a poly-
nomial kernel, cosine similarity, or binary.
3. Construct the diagonal matrix D where entry
Dii =
?
j Sij . Let L = D ? S. Then solve
the generalized eigenvector problem:
XLX?w = ?XDX?w. (2)
He and Niyogi (2003) show that solving this
problem is equivalent to solving Equation 1.
4. Let Wk = [w1, . . . ,wk] denote the matrix of
transformation vectors, sorted in descending
order according to their eigenvalues ?. The
original space is projected into k dimensions
by W?k X ? Xk.
For many applications of LPP, such as doc-
ument clustering (He et al, 2004), the original
data matrix X is transformed by first perform-
ing Principle Component Analysis and discarding
the smallest principle components, which requires
computing the full SVD. However, for large data
sets such as those frequently used in word space
algorithms, performing the full SVD is computa-
tionally infeasible.
To overcome this limitation, Cai et al (2007a)
show how Spectral Regression may be used as
an alternative for solving the same minimization
equation through an iterative process. The princi-
ple idea is that Equation 2 may be recast as
Sy = ?Dy (3)
where y = X?w, which ensures y will be an
eigenvector with the same eigenvalue for the prob-
lem in Equation 2. Finding the transformation
matrix Wk, used in step 4, is done in two steps.
First, Equation 3 is solved to produce eigenvectors
[y0, . . . ,yk], sorted in decreasing order according
to their eigenvalues ?. Second, the set of trans-
formation vectors composing Wk, [w1, . . . ,wk],
is found by a least-squares regression:
wj = argmin
w
n
?
i=1
(w?xi ? yji )2 + ?||w||2 (4)
2
where yji denotes the value of the jth dimension
of yi. The ? parameter penalizes solutions pro-
portionally to their magnitude, which Cai et al
(2007b) note ensures the stability of w as an ap-
proximate eigenproblem solution.
4 Experiments
Two experiments measures the effects of nonlin-
ear dimensionality reduction for word spaces. For
both, we apply LPP to two basic word space mod-
els, the VSM and WC. In the first experiment,
we measure the word spaces? abilities to model
semantic relations, as determined by priming ex-
periments. In the second experiment, we evaluate
the representation capabilities of the LPP-reduced
models on standard word space benchmarks.
4.1 Setup
For the VSM-based word space, we consider three
different weighting schemes: no weighting, TF-
IDF and the log-entropy (LE) used in (Landauer
et al, 1998). For the WC-based word space, we
use a 5 word sliding window. Due to the large pa-
rameter space for LPP models, we performed only
a limited configuration search. An initial analysis
using the 20 nearest neighbors and cosine simi-
larity did not show significant performance differ-
ences when the number of dimensions was varied
between 50 and 1000. We therefore selected 300
dimensions for all tests. Further work is needed to
identify the impact of different parameters. Stop
words were removed only for the WC+LPP model.
We compare the LPP-based spaces to three mod-
els: VSM, HAL, and LSA.
Two corpora are used to train the models in both
experiments. The first corpus, TASA, is a collec-
tion of 44,486 essays that are representative of the
reading a student might see upon entering college,
introduced by (Landauer et al, 1998). The cor-
pus consists of 98,420 unique words; no filtering
is done when processing this corpus. The second
corpus, WIKI, is a 387,082 article subset of a De-
cember 2009 Wikipedia snapshot consisting of all
the articles with more than 1,000 tokens. The cor-
pus is filtered to retain the top 100,000 most fre-
quent tokens in addition to all the tokens used in
each experiment?s data set.
4.2 Experiment 1
Semantic priming measures word association
based on human responses to a provided cue.
Priming studies have been used to evaluate word
spaces by equating vector similarity with an in-
creased priming response. We use data from two
types of priming experiments to measure whether
LPP models better correlate with human perfor-
mance than non-LPP word spaces.
Normed Priming Nelson et al (1998) collected
free association responses to 5,019 prime words.
An average of 149 participants responded to each
prime with the first word that came to mind.
Based on this dataset, we introduce a new
benchmark that correlates word space similarity
with the associative strength of semantic priming
pairs. We use three measures for modeling prime-
target strength, which were inspired by Steyvers
et al (2004). Let Wab be the percentage of partici-
pants who responded to prime a with target b. The
three measures of associative strength are
S1ab = Wab
S2ab = Wab +Wba
S3ab = S2ab +
?
c S2acS2cb
These measure three different levels of semantic
relatedness between words a and b. S1ab measures
the relationship from a to b, which is frequently
asymmetric due to ordering, e.g. ?orange? pro-
duces ?juice? more frequently than ?juice? pro-
duces ?orange.? S2ab measures the symmetric asso-
ciation between a and b; Steyvers et al (2004) note
that this may better model the associative strength
by including weaker associates that may have been
a suitable second response. S3ab further increases
the association by including the indirect associa-
tions between a and b from all cued primes.
For each measure, we rank a prime?s targets
according to their strength and then compute the
Spearman rank correlation with the prime-target
similarities in the word space. The rank compari-
son measures how well word space similarity cor-
responds to the priming association. We report the
average rank correlation of associational strengths
over all primes.
Priming Effect The priming study by Hodgson
(1991), which evaluated how different semantic
relationships affected the strength of priming, pro-
vides the data for our second priming test. Six re-
lationships were examined in the study: antonymy,
synonymy, conceptual association (sleep and bed),
categorical coordinates (mist and rain), phrasal as-
sociates (pony and express), and super- and sub-
ordinates. Each relationship contained an average
3
Antonymy Conceptual Coordinates
Algorithm Rb U E R U E R U E
VSM+LPP+LE 0.103 0.018 0.085 0.197 0.050 0.147 0.071 0.027 0.044
VSM+LPP+TF-IDF 0.348 0.321 0.027 0.408 0.414 -0.005 0.323 0.294 0.029
VSM+LPP 0.247 0.122 0.124 0.312 0.120 0.193 0.230 0.111 0.119
VSM+LPPa 0.298 0.070 0.228 0.284 0.033 0.252 0.321 0.037 0.284
WC+LPP 0.255 0.071 0.185 0.413 0.110 0.303 0.431 0.134 0.298
HAL 0.813 0.716 0.096 0.845 0.814 0.031 0.861 0.809 0.052
HALa 0.915 0.879 0.037 0.867 0.846 0.021 0.913 0.861 0.052
LSA 0.235 0.023 0.213 0.392 0.028 0.364 0.199 0.014 0.185
LSAa 0.287 0.061 0.226 0.362 0.041 0.321 0.316 0.037 0.278
VSM 0.051 0.011 0.040 0.111 0.012 0.099 0.032 0.008 0.024
Phrasal Ordinates Synonymy
Algorithm R U E R U E R U E
VSM+LPP+LE 0.147 0.039 0.108 0.225 0.032 0.193 0.081 0.027 0.053
VSM+LPP+TF-IDF 0.438 0.425 0.013 0.277 0.290 -0.013 0.344 0.328 0.017
VSM+LPP 0.234 0.107 0.127 0.273 0.115 0.158 0.237 0.157 0.080
VSM+LPPa 0.202 0.031 0.171 0.270 0.032 0.238 0.299 0.069 0.230
WC+LPP 0.274 0.087 0.186 0.324 0.076 0.248 0.345 0.111 0.233
HAL 0.805 0.776 0.029 0.825 0.789 0.036 0.757 0.681 0.076
HALa 0.866 0.856 0.010 0.881 0.857 0.024 0.898 0.879 0.019
LSA 0.280 0.021 0.258 0.258 0.018 0.240 0.197 0.019 0.178
LSAa 0.269 0.030 0.238 0.326 0.032 0.294 0.327 0.052 0.275
VSM 0.104 0.013 0.091 0.061 0.008 0.053 0.052 0.009 0.043
a Processed using the WIKI corpus
b R are related primes, U are unrelated primes, E is the priming effect
Table 1: Experiment 1 priming results for the six relation categories from Hodgson (1991)
Word Choice Word Association
Algorithm Corpus TOEFL ESL RDWP F. et al R.&G. Deese
VSM+LPP+le TASA 24.000 50.000 45.313 0.296 0.092 0.034
VSM+LPP+tf-idf TASA 22.667 25.000 37.209 0.023 0.086 0.001
VSM+LPP TASA 41.333 54.167 39.063 0.219 0.136 0.045
VSM+LPP Wiki 33.898 48.780 43.434 0.530 0.503 0.108
WC+LPP TASA 46.032 40.000 45.783 0.423 0.414 0.126
HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318
HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042
LSA TASA 56.000 50.000 55.814 0.516 0.651 0.349
LSA Wiki 60.759 54.167 59.200 0.614 0.681 0.206
VSM TASA 61.333 52.083 84.884 0.396 0.496 0.200
Table 2: Results from Experiment 2 on six word space benchmarks
of 23 word pairs. Hodgson?s results showed that
priming effects were exhibited by the prime-target
pairs in all six categories.
We use the same methodology as Pado? and La-
pata (2007) for this data set; the prime-target (Re-
lated Primes) cosine similarity is compared with
the average cosine similarity between the prime
and all other targets (Unrelated Primes) within the
semantic category. The priming effect is the dif-
ference between the two similarity values.
4.3 Experiment 2
We use six standard word space benchmarks to
test our hypothesis that LPP can accurately capture
general semantic knowledge and association based
relations. The benchmarks come in two forms:
word association and word choice tests.
Word choice tests provide a target word and a
list of options, one of which has the desired rela-
tion to the target. To answer these questions, we
select the option with the highest cosine similar-
ity with the target. Three word choice synonymy
benchmarks are used: the Test of English as a For-
eign Language (TOEFL) test set from (Landauer
et al, 1998), the English as a Second Language
(ESL) test set from (Turney, 2001), and the Cana-
dian Reader?s Digest Word Power (RDWP) from
(Jarmasz and Szpakowicz, 2003).
4
Algorithm Corpus S1 S2 S3
VSM+LPP+LE TASA 0.457 0.413 0.255
VSM+LPP+TF-IDF TASA 0.464 0.390 0.207
VSM+LPP TASA 0.457 0.427 0.275
VSM+LPP Wiki 0.472 0.440 0.333
WC+LPP TASA 0.469 0.437 0.315
HAL TASA 0.485 0.434 0.310
HAL Wiki 0.462 0.406 0.266
LSA TASA 0.494 0.481 0.414
LSA Wiki 0.489 0.472 0.398
VSM TASA 0.484 0.460 0.407
Table 3: Experiment 1 results for normed priming.
Word association tests measure the semantic re-
latedness of two words by comparing their simi-
larity in the word space with human judgements.
These tests are more precise than word choice tests
because they take into account the specific value
of the word similarity. Three word association
benchmarks are used: the word similarity data set
of Rubenstein and Goodenough (1965), the word-
relatedness data set of Finkelstein et al (2002),
and the antonymy data set of Deese (1964), which
measures the degree to which high similarity cap-
tures the antonymy relationship. The Finkelstein
et al test is notable in that the human judges were
free to score based on any word relationship.
5 Results and Discussion
The LPP-based models show mixed performance
in comparison to existing models on normed prim-
ing tasks, shown in Table 3. Adding LPP to
the VSM decreased performance; however, when
WIKI was used instead of TASA, the VSM+LPP
model increased .15 on all correlations, whereas
LSA?s performance decreased. This suggests that
LPP needs more data than LSA to properly model
the word space manifold. WC+LPP performs
comparably to HAL, which indicates that LPP
is effective in retaining the original WC space?s
structure in significantly fewer dimensions.
For the categorical priming tests shown in Ta-
ble 1, LPP-based models show competitive results.
VSM+LPP with the WIKI corpus performs much
better than other VSM+LPP configurations. Un-
like in the previous priming experiment, adding
LPP to the base models resulted in a significant
performance improvement. We also note that both
HAL models and the VSM+LPP+TF-IDF model
have high similarity ratings for unrelated primes.
We posit that these models? feature weighting re-
sults in poor differentiation between words in the
same semantic category, which causes their de-
creased performance.
For experiment 2, LPP-based spaces showed
mixed results on word choice benchmarks, while
showing notable improvement on the more pre-
cise word association benchmarks. Table 2 lists
the results. Notably, LPP-based spaces performed
well on the ESL synonym benchmark but poorly
on the TOEFL synonym benchmark, even when
the larger WIKI corpus was used. This suggests
that LPP was not effective in retaining the re-
lationship between certain classes of synonyms.
Given that performance did not improve with the
WIKI corpus, further analysis is needed to iden-
tify whether a different representation of the local
structure would improve results or if the poor per-
formance is due to another factor. While LSA and
VSM model performed best on all benchmarks,
LPP-based spaces performed competitively on the
word association tests. In all but two tests, the
WC+LPP model outperformed HAL.
The results from both experiments indicate that
LPP is capable of accurately representing distri-
butional information in a much lower dimensional
space. However, in many cases, applications using
the SVD-reduced representations performed bet-
ter. In addition, application of standard weight-
ing schemes worsened LPP-models? performance,
which suggests that the local neighborhood is ad-
versely distorted. Nevertheless, we view these re-
sults as a promising starting point for further eval-
uation of nonlinear dimensionality reduction.
6 Conclusions and Future Work
We have shown that LPP is an effective dimen-
sionality reduction technique for word space algo-
rithms. In several benchmarks, LPP provided a
significant benefit to the base models and in a few
cases outperformed the SVD. However, it does not
perform consistently better than existing models.
Future work will focus on four themes: identifying
optimal LPP parameter configurations; improving
LPP with weighting; measuring LPP?s capacity to
capture higher order co-occurrence relationships,
as was shown for the SVD (Lemaire et al, 2006);
and investigating whether more computationally
expensive nonlinear reduction algorithms such as
ISOMAP (Tenenbaum et al, 2000) are better for
word space algorithms. We plan to release imple-
mentations of the LPP-based models as a part of
the S-Space Package (Jurgens and Stevens, 2010).
5
References
Marco Baroni and Alessandro Lenci. 2008. Con-
cepts and properties in word spaces. From context to
meaning: Distributional models of the lexicon in lin-
guistics and cognitive science (Special issue of the
Italian Journal of Linguistics), 1(20):55?88.
Mikhail Belkin and Partha Niyogi. 2002. Laplacian
Eigenmaps and Spectral Techniques for Embedding
and Clustering. In Advances in Neural Information
Processing Systems, number 14.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: a computational study. Behav-
ior Research Methods, 39:510?526.
Deng Cai, Xiaofei He, and Jiawei Han. 2007a. Spec-
tral regression for efficient regularized subspace
learning. In IEEE International Conference on
Computer Vision (ICCV?07).
Deng Cai, Xiaofei He, Wei Vivian Zhang, , and Jiawei
Han. 2007b. Regularized Locality Preserving In-
dexing via Spectral Regression. In Proceedings of
the 2007 ACM International Conference on Infor-
mation and Knowledge Management (CIKM?07).
James Deese. 1964. The associative structure of
some common english adjectives. Journal of Verbal
Learning and Verbal Behavior, 3(5):347?357.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Woflman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions of Information
Systems, 20(1):116?131.
Xiaofei He and Partha Niyogi. 2003. Locality preserv-
ing projections. In Advances in Neural Information
Processing Systems 16 (NIPS).
Xiaofei He, Deng Cai, Haifeng Liu, and Wei-Ying
Ma. 2004. Locality preserving indexing for doc-
ument representation. In SIGIR ?04: Proceedings
of the 27th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 96?103.
James M. Hodgson. 1991. Informational constraints
on pre-lexical priming. Language and Cognitive
Processes, 6:169?205.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Conference on
Recent Advances in Natural Language Processing,
pages 212?219.
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In L. R. Gleitman and A. K. Josh,
editors, Proceedings of the 22nd Annual Conference
of the Cognitive Science Society, page 1036.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to Latent Semantic Analy-
sis. Discourse Processes, (25):259?284.
Beno??t Lemaire, , and Guy Henhie?re. 2006. Effects
of High-Order Co-occurrences on Word Semantic
Similarities. Current Psychology Letters, 1(18).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occrrence. Behavoir Research Methods, Instru-
ments & Computers, 28(2):203?208.
Douglas L. Nelson, Cathy L. McEvoy, and
Thomas A. Schreiber. 1998. The Uni-
versity of South Florida word association,
rhyme, and word fragment norms. http:
//www.usf.edu/FreeAssociation/.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-Based Construction of Seman-
tic Space Models. Computational Linguistics,
33(2):161?199.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8:627?633.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode or-
der in word space. In Proceedings of the 30th
Annual Meeting of the Cognitive Science Society
(CogSci?08).
Gerard Salton, A. Wong, and C. S. Yang. 1975. A
vector space model for automatic indexing. Com-
munications of the ACM, 18(11):613?620.
Hinrich Schu?tze. 1992. Dimensions of meaning.
In Proceedings of Supercomputing ?92, pages 787?
796.
Mark Steyvers, Richard M. Shiffrin, and Douglas L.
Nelson, 2004. Word association spaces for predict-
ing semantic similarity effects in episodic memory.
American Psychological Assocation.
Joshua B. Tenenbaum, Vin de Silva, and John C.
Langford. 2000. A global geometric framework
for nonlinear dimensionality reduction. Science,
290(5500):2319?2323.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491?502.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84:327?352.
6
Proceedings of the TextGraphs-6 Workshop, pages 24?28,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
Word Sense Induction by Community Detection
David Jurgens1,2
1HRL Laboratories, LLC 2Department of Computer Science
Malibu, California, USA University of California, Los Angeles
jurgens@cs.ucla.edu
Abstract
Word Sense Induction (WSI) is an unsu-
pervised approach for learning the multiple
senses of a word. Graph-based approaches to
WSI frequently represent word co-occurrence
as a graph and use the statistical properties
of the graph to identify the senses. We rein-
terpret graph-based WSI as community detec-
tion, a well studied problem in network sci-
ence. The relations in the co-occurrence graph
give rise to word communities, which distin-
guish senses. Our results show competitive
performance on the SemEval-2010 WSI Task.
1 Introduction
Many words have several distinct meanings. For ex-
ample, ?law? may refer to legislation, a rule, or po-
lice depending on the context. Word Sense Induc-
tion (WSI) discovers the different senses of a word,
such as ?law,? by examining its contextual uses. By
deriving the senses of a word directly from a corpus,
WSI is able to identify specialized, topical meanings
in domains such as medicine or law, which prede-
fined sense inventories may not include.
aWe consider graph-based approaches to WSI,
which typically construct a graph from word occur-
rences or collocations. The core problem is how to
identify sense-specific information within the graph
in order to perform sense induction. Current ap-
proaches have used clustering (Dorow and Wid-
dows, 2003; Klapaftis and Manandhar, 2008) or
statistical graph models (Klapaftis and Manandhar,
2010) to identify sense-specific subgraphs.
We reinterpret the challenge of identifying sense-
specific information in a co-occurrence graph as one
of community detection, where a community is de-
fined as a group of connected nodes that are more
connected to each other than to the rest of the graph
(Fortunato, 2010). Within the co-occurrence graph,
we hypothesize that communities identify sense-
specific contexts for each of the terms. Community
detection identifies groups of contextual cues that
constrain each of the words in a community to a sin-
gle sense.
To test our hypothesis, we require a community
detection algorithm with two key properties: (1) a
word may belong to multiple, overlapping commu-
nities, which is necessary for discovering multiple
senses, and (2) the community detection may be hi-
erarchically tuned, which corresponds to sense gran-
ularity. Therefore, we adapt a recent, state of the art
approach, Link Clustering (Ahn et al, 2010). Our
initial study suggests that community detection of-
fers competitive performance and sense quality.
2 Word Sense Induction
A co-occurrence graph is fundamental to our ap-
proach; terms are represented as nodes and an
edge between two nodes indicates the terms? co-
occurrence, with a weight proportional to frequency.
While prior work has focused on clustering the
nodes to induce senses, using Link Clustering (Ahn
et al, 2010), we cluster the edges, which is equiv-
alent to grouping the word collocations to iden-
tify sense-specific contexts. We summarize our ap-
proach as four steps: (1) selecting the contextual
cues, (2) building a co-occurrence graph, (3) per-
forming community detection on the graph, and (4)
sense labeling new contexts using the discovered
communities.
Context Refinement Representing the co-
occurrence graph for all terms in a context is
24
prohibitively expensive. Moreover, often only a
subset of the terms in a context constrain the sense
of an ambiguous word. Therefore, we refine a
word?s context to include only a subset of the terms
present. Following previous work (Ve?ronis, 2004),
we select only nouns in the context.
Early experiments indicated that including infre-
quent terms in the co-occurrence graph yielded poor
performance, which we attribute to having too few
connecting edges to identify meaningful community
structure. Therefore, we include only those nouns
occurring in the most frequent 5000 tokens, which
are likely to be representative the largest communi-
ties in which a term takes part. Last, we include all
the nouns and verbs used in the SemEval 2010 WSI
Task (Manandhar et al, 2010), which are used in
our evaluation. The selected context terms are then
stemmed using the Porter stemmer.
Building the Co-occurrence Graph The graph is
iteratively constructed by adding edges between the
terms from a context. For each pair-wise combi-
nation of terms, an edge is added and its weight
is increased by 1. This step effectively embeds a
clique if it did not exist before, connecting all of
the context?s words within the graph. Once all con-
texts have been seen, the graph is then pruned to re-
move all edges with weight below a threshold ? =
25. This step removes edges form infrequent collo-
cations, which may not contribute sufficient graph
structure for community detection, and as a practi-
cal consideration, greatly speeds up the community
detection process. However, we note that parameter
was largely unoptimized and future work may see a
benefit from accounting for edge weight.
Community Detection Within the co-occurrence
graph, communities may have partial overlap. For
example, Figure 1 illustrates a part of the local graph
for ?mouse.? Two clear senses emerge from the
neighbors: one for the input device and another for
the animal. However, the terms that correspond
to one sense also co-occur with terms correspond-
ing to the other sense, e.g., ?information,? which
hinders finding communities directly from discon-
nected components in the local neighborhood. Find-
ing sense-specific communities requires recognizing
that the co-occurring terms may be shared by mul-
tiple communities. Therefore, to identify communi-
mouse
access
data
information
people
software
computer
line
cell
lines
rolefunction
movement
control
research
analysis
screen
point
development
study
blood
expression
protein
proteins
stem
gene
windows
text full
webdisplay
application
differentiation
growth
studies
science
results
button
user
type
types
device
size
surface
culture
shape
window
medline
skin
factor
adult
genes
tissue
bone
tissues
Figure 1: A portion of the local co-occurrence graph
for ?mouse? from the SemEval-2010 Task 14 corpus
ties we selected the approach of Ahn et al (2010),
summarized next, which performs well for overlap-
ping community structure.
First, the edges are clustered using an unweighted
similarity function based on the neighbors of two
edges, ei,j and ei,k: sim(ei,j , ei,k) = nj?nknj?nk , where
ni denotes the node i and its neighbors. This simi-
larity reflects the percentage of terms that co-occur
in common with the term for nodes j and k, inde-
pendent of the terms that co-occur with the shared
term for i. For example, in Figure 1, the similarity
for the edges connecting ?mouse? with ?user? and
?software,? 25 , measures the overlap in the neighbors
of ?user? and ?software? independent of the neigh-
bors for ?mouse,? such as ?cell? and ?size.?
Using this similarity function, the edges are ag-
glomeratively clustered into a dendrogram. We use
the single-link criteria which iteratively merges the
two clusters connected by the edge pair with the
highest similarity. The dendrogram may then be cut
at different levels to reveal different cluster granu-
larities; cuts near the bottom of the dendrogram cre-
ate a larger number of small groups of collocations,
whereas cuts near the top create fewer, larger groups
of collocations. To select the specific partitioning
of the dendrogram into clusters, we select the solu-
tion that maximizes the partition density, which Ahn
et al (2010) define as D = 2M
?
c mc
mc?(nc?1)
(nc?2)(nc?1) ,
where M is the number of edges in the graph, c de-
25
notes a specific cluster, and nc and mc are the num-
ber of nodes and edges in cluster c, respectively.
The final set of communities is derived from these
partitions: a node is a member of each community in
which one of its edges occurs. Last, we remove all
communities of size 3 and below, which we interpret
as having too few semantic constraints to reliably
disambiguate each of its terms.
Sense Induction from Communities Each term
in a community is treated as having a specific sense,
with one sense per community. To label a contextual
usage, we identify the community that best maps to
the context. For a given context, made of the set of
words W , we score each community i, consisting of
words C , using the Jaccard index weighted by com-
munity size: score(Ci,W ) = |Ci| ? |Ci?W ||Ci?W | . This
similarity function favors mapping contexts to larger
communities, which we interpret as having more se-
mantic constraints. The final sense labeling consists
of the scores for all overlapping communities.
3 Evaluation
We use the SemEval-2 Task 14 evaluation (Manand-
har et al, 2010) to measure the quality of induced
senses. We summarize the evaluation as follows.
Systems are provided with an unlabeled training cor-
pus consisting of 879,807 multi-sentence contexts
for 100 polysemous words, comprised of 50 nouns
and 50 verbs. Systems induce sense representations
for target words from the training corpus and then
use those representations to label the senses of the
target words in unseen contexts from a test corpus.
We use the entire multi-sentence context for build-
ing the co-occurrence graph.
The induced sense labeling is scored using two
unsupervised and one supervised methods. The un-
supervised scores consists of two contrasting mea-
sures: the paired FScore (Artiles et al, 2009) and
the V-Measure (Rosenberg and Hirschberg, 2007).
Briefly, the V-Measure rates the homogeneity and
completeness of a clustering solution. Solutions that
have word clusters formed from one gold-standard
sense are homogeneous; completeness measures the
degree to which a gold-standard sense?s instances
are assigned to a single cluster. The paired FScore
reflects the overlap of the solution and the gold stan-
dard in cluster assignments for all pair-wise combi-
FScore V-Meas. S80/20 S60/40
SPD 61.1 (3) 3.6 (18) 57.64 (18) 57.64 (16)
SV 56.16 (9) 8.7 (6) 57.90 (18) 57.36 (17)
SF 63.4 (1) 0 (26) 56.18 (21) 56.20 (21)
BestF 63.3 (1) 0 (26) 58.69 (14) 58.24 (13)
BestV 26.7 (25) 16.2 (1) 58.34 (16) 57.27 (17)
BestS 49.8 (15) 15.7 (2) 62.44 (1) 61.96 (1)
MFS 63.4 0 58.67 58.95
Table 1: Performance results on the SemEval-2010
WSI Task, with rank shown in parentheses. Refer-
ence scores of the best submitted systems are shown
in the bottom.
 0
 30
 60
 90
 120
 0
 75
 150
 225
 300
 375
M
em
be
rs
hi
ps
Co
m
m
un
ity
 S
ize
Avg. Memberships
Avg. Size
0.00
0.20
0.40
0.60
0.80
1.00
50K
100K
150K
200K
250K
300K
350K
400K
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
F-
Sc
or
e
V-
M
ea
su
re
Merge Steps Prior to Cutting the Dendrogram (in thousands)
F-Score
V-Measure
Figure 2: V-Measure and paired FScore results
for different partitionings of the dendrogram. The
dashed vertical line indicates SPD
nation of instances. The supervised evaluation trans-
forms the induced sense clusters of a portion of the
corpus into a word sense classifier, which is then
tested on the remaining corpus. An 80/20 train-test
split, S80/20, and 60/40 split, S60/40, are both used.
Results As a first measure of the quality of the in-
duced senses, we evaluated both the solution that
maximized the partition density, referred to as SPD,
and an additional 5,000 solutions, evenly distributed
among the possible dendrogram partitionings. Fig-
ure 2 shows the score distribution for V-Measure and
paired FScore. Table 1 lists the scores and rank for
SPD and the solutions that optimize the V-Measure,
SV , and FScore, SF , among the 26 participating
Task-14 systems. For comparison, we include the
highest performing systems on each measure and the
Most Frequent Sense (MFS) baseline.
26
Discussion Optimizing the partition density re-
sults in high performance only for the FScore; how-
ever, optimizing for the V-Measure yields competi-
tive performance on both measures. The behavior is
encouraging as most approaches submitted to Task
14 favor only one measure.
Figure 2 indicates a relationship between the V-
Measure and community memberships. Therefore,
using SV , we calculated the Pearson correlation be-
tween a term?s scores and the number of community
memberships within a single solution. The corre-
lation with the paired FScore, r = -0.167, was not
statistically significant at p < .05, while correlation
with the V-Measure, r = 0.417 is significant with
p < 1.6e-5. This suggests that at a specific com-
munity granularity, additional communities enable
the WSI mapping process to make better sense dis-
tinctions between contexts. However, we note that
V-Measure begins to drop as the average commu-
nity membership increases in solutions after SV , as
shown in Figure 2. We suspect that as the agglomer-
ative merge process continues, communities repre-
senting different senses become merged, leading to
a loss of purity.
The lower performance of SPD and the impact of
community memberships raises the important ques-
tion of how to best select the communities. While
co-occurrence graphs have been shown to exhibit
small-world network patterns (Ve?ronis, 2004), op-
timizing for the general criterion of partition density
that has performed well on such networks does not
result in communities that map well to sense-specific
contexts. We believe that this behavior is due to
impact of the sense inventory; selecting a commu-
nity solution purely based on the graph?s structure
may not capture the correct sense distinctions, ei-
ther having communities with too few members to
distinguish between senses or too many members,
which conflates senses. However, a promising fu-
ture direction is to examine whether the there exist
features of the graph structure that would allow for
recognizing the specific community solutions that
correspond directly to different sense granularities
without the need for an external evaluation metric.
4 Related Work
We highlight those related works with connections
to community detection. Ve?ronis (2004) demon-
strated that word co-occurrence graphs follow a
small-world network pattern. In his scheme, word
senses are discovered by iteratively deleting the
more connected portions of the subgraph to reveal
the different senses? network structure. Our work
capitalizes on this intuition of discovering sense-
related subgraphs, but leverages formalized methods
for community detection to identify them.
Dorow and Widdows (2003) identify sense-
related subgraphs in a similar method to commu-
nity detection for local region of the co-occurrence
graph. They use a random walk approach to identify
regions of the graph that are sense-specific. Though
not identical, we note that the random walk model
has been successfully applied to community detec-
tion (Rosvall et al, 2009). Furthermore, Dorow and
Widdows (2003) performs graph clustering on a per-
word basis; in contrast, the proposed approach iden-
tifies communities for the entire graph, effectively
performing an all-word WSI.
Klapaftis and Manandhar (2010) capture hierar-
chical relations between collocations using a Hi-
erarchical Random Graph model where nodes are
collocations and edges indicate their co-occurrence,
which improved performance over non-hierarchical
models. Our community detection approach also
captures the hierarchical structure of the collocation
graph, but uses a much simpler graphical representa-
tion that for n terms requires O(n) nodes and O(n2)
edges, compared to O(n2) nodes and O(n3) edges
for the above approach, which allows it to build the
collocation graph from a larger set of terms.
5 Conclusion
We have proposed a new graph-based method for
WSI based on finding sense-specific word commu-
nities within a co-occurrence graph, which are then
identify distinguish senses in new contexts. An
initial analysis using the SemEval-2010 WSI task
demonstrates competitive performance. Future re-
search will address two potential avenues: (1) the
impact of word frequency on community size and
memberships and (2) identifying both graph proper-
ties and semantic relations within hierarchical com-
munities that distinguish between sense granulari-
ties. Software for the WSI model and for Link Clus-
tering is available as a part of the S-Space Package
(Jurgens and Stevens, 2010).
27
References
Yong-Yeol Ahn, James P. Bagrow, and Sune Lehmann.
2010. Link communities reveal multiscale complexity
in networks. Nature, (466):761?764, August.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of EMNLP, pages 534?542. ACL.
Beate Dorow and Dominic Widdows. 2003. Discover-
ing corpus-specific word senses. In Proceedings of the
10th EACL, pages 79?82.
Santo Fortunato. 2010. Community detection in graphs.
Physics Reports, 486(3-5):75?174.
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
sense induction using graphs of collocations. In Pro-
ceeding of ECAI 2008, pages 298?302.
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Word
sense induction & disambiguation using hierarchical
random graphs. In Proceedings of EMNLP, pages
745?755. ACL.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-
2010 task 14: Word sense induction & disambigua-
tion. In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68. Association for
Computational Linguistics.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the Joint Con-
ference of EMNLP-CoNLL. ACL, June.
M. Rosvall, D. Axelsson, and C.T. Bergstrom. 2009.
The map equation. The European Physical Journal-
Special Topics, 178(1):13?23.
J. Ve?ronis. 2004. HyperLex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252.
28
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 113?123,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Measuring the Impact of Sense Similarity on Word Sense Induction
David Jurgens1,2
1HRL Laboratories, LLC
Malibu, California, USA
jurgens@cs.ucla.edu
Keith Stevens2
2University of California, Los Angeles
Los Angeles, California, USA
kstevens@cs.ucla.edu
Abstract
Word Sense Induction (WSI) is an unsuper-
vised learning approach to discovering the dif-
ferent senses of a word from its contextual
uses. A core challenge to WSI approaches
is distinguishing between related and possibly
similar senses of a word. Current WSI evalu-
ation techniques have yet to analyze the spe-
cific impact of similarity on accuracy. There-
fore, we present a new WSI evaluation that
quantifies the relationship between the relat-
edness of a word?s senses and the ability of a
WSI algorithm to distinguish between them.
Furthermore, we perform an analysis on sense
confusions in SemEval-2 WSI task according
to sense similarity. Both analyses for a rep-
resentative selection of clustering-based WSI
approaches reveals that performance is most
sensitive to the clustering algorithm and not
the lexical features used.
1 Introduction
Many words in a language have several distinct
meanings. For example, ?earth? may refer to the
planet Earth, dirt, or solid ground, depending on the
context. The goal of Word Sense Induction (WSI) is
to automatically discover the different senses by ex-
amining how a word is used. This unsupervised dis-
covery process produces a sense inventory where the
number of senses is corpus-driven and where senses
may reflect additional usages not present in a pre-
defined sense inventory, such as those for medicine
or law (Dorow and Widdows, 2003). Furthermore,
these discovered senses can be used to automati-
cally expand lexical resources such as WordNet or
FrameNet (Klapaftis and Manandhar, 2010).
Discovering the multiple senses is frequently
confounded by the relationships between a word?s
senses. While homonyms such as ?bass? or ?bank?
have unrelated senses, many polysemous words
have interrelated senses, with lexicographers of-
ten in disagreement for the number of fine-grained
senses (Palmer et al, 2007). For example, the most
frequent four senses for ?law? according to Word-
Net, shown in Table 1, are similar in several aspects
and could be ascribed interchangeably in some con-
texts. The difficulty of automatically distinguishing
two senses is proportional to their similarity because
of the increasing likelihood of the two senses shar-
ing similar contexts.
While the issue distinguishing between related
senses is a recognized issue for Word Sense Dis-
ambiguation (Chugur et al, 2002; McCarthy, 2006),
which uses supervised training to learn sense dis-
tinctions, measuring the impact of sense related-
ness on the harder problem of WSI remains unad-
dressed. The recent SemEval WSI tasks (Agirre and
Soroa, 2007; Manandhar and Klapaftis, 2009) have
provided a standard framework for evaluating WSI
systems, with a controlled training corpus designed
to limit sense ambiguity in the example contexts.
However, given the potential relatedness of a word?s
senses, we view it necessary to consider how WSI
methods perform relative to the degree of contextual
ambiguity. Our goal is therefore to quantify the sim-
ilarity at which a WSI approach is unable to distin-
guish between two senses, which reflects the sense
granularity at which the approach operates.
We propose two new evaluations. The first, de-
scribed in Section 4, uses a similarity-based pseudo-
word discrimination task to measure the discrimi-
nation capability for related senses along a graded
scale of similarity. As a second evaluation, in
113
1 the collection of rules imposed by authority
2 legal document setting forth rules governing a particular
kind of activity
3 a rule or body of rules of conduct inherent in human
nature and essential to or binding upon human society
4 a generalization that describes recurring facts or events
in nature
Table 1: Definitions for the top four senses of ?law?
according to WordNet
Section 5 we perform an error analysis using the
SemEval-2010 WSI task, examining sense confu-
sion relative to the sense similarities. For both evalu-
ations, we examine twenty different WSI clustering-
based models through combining five feature types
and four clustering algorithms. These models were
selected to be representative of a wide class of exist-
ing algorithms as a way of influence future algorith-
mic directions based on the current model?s perfor-
mance.
2 Clustering Contexts to Discover Senses
Frequently, WSI is treated as an unsupervised clus-
tering problem: The contexts in which a word ap-
pears are clustered in order to discover its senses
(Navigli, 2009). We selected four diverse cluster-
ing algorithms for evaluation based on three crite-
ria: (1) the ability to automatically determine the fi-
nal number of clusters given an upper bound or a
set of parameters, (2) an efficient run time, and (3)
high quality results in either WSI or other fields re-
lated to text analysis. The first criteria is essential
for WSI; the final number of senses must be derived
without supervision in order to reflect the true num-
ber of senses present in the corpus.
K-Means K-Means builds clusters based on the
similarity between two data points. Clusters grow
by assigning data points to the cluster with the most
similar centroid. After every data point is assigned,
each cluster?s centroid is recalculated to be the av-
erage of all the data points assigned to the cluster.
This process repeats until the centroids converge to
a fixed point. We choose initial seeds at random and
use the H2 criterion function (Zhao and Karypis,
2001). Although K-Means is efficient and widely
used, it requires the number of clusters to be spec-
ified a priori. Therefore, we follow the WSI model
of Pedersen and Kulkarni (2006) and use the Gap
Statistic (Tibshirani et al, 2000) to automatically de-
termine the number of clusters.
The Gap Statistic runs K-Means repeatedly with
different values of K , ranging from 1 to some sen-
sible maximum. The Gap Statistic first induces a
data model from the feature distributions of the ini-
tial dataset and then for each K , creates a set of arti-
ficial datasets by sampling from the derived model.
K is increased until the ?gap?, i.e. the distance be-
tween the objective function of the original dataset
and the average objective function of the artificial
datasets, is larger then the gap for the previous K
value. We calculate the gap using 10 artificial data
sets sampled from the model.
Spectral Clustering Spectral Clustering inter-
prets a dataset?s elements as vertices in graph with
edges based on their similarity (Ng et al, 2001).
Clusters are found by identifying the graph parti-
tion that produces the minimum conductance be-
tween every partition. This can be thought of as
trying to find small islands that are connected by as
few bridges as possible. We refer the reader to (von
Luxburg, 2007) for further technical details. To our
knowledge, only He et al (2010) have applied spec-
tral clustering to WSI, which was performed on a
Chinese dataset. However, the algorithm used by He
et al requires the number of clusters to be specified.
We instead use a hybrid spectral clustering algo-
rithm, first applied to information retrieval (Cheng
et al, 2006), that automatically selects the number
of clusters. This algorithm recursively partitions a
dataset in half by finding the cut that produces the
minimum conductance, which builds a tree of par-
titions. This split is done until either every data
point is in its own partition or a maximum number of
partitions is found. Partitions are then dynamically
merged, starting at leaf partitions, based on a cluster-
ing criteria. We use the relaxed correlation criteria
(Cheng et al, 2006), which tries to maximize both
inter cluster similarity and intra cluster dissimilarity.
The final cluttering generated is then the best tree-
respecting partition of the original data set.
Clustering By Committee Pantel and Lin (2002)
found that K-Means clustering folded all features
found in a cluster into the centroid, many of which
are not useful for identifying the desired word sense.
114
To overcome this, they proposed a novel cluster-
ing algorithm for WSI, Clustering by Committee
(CBC), which includes only the most distinguishing
features for a cluster into the centroid.
For each context, an initial set of ?committees?
is formed by clustering the most similar contexts to
each context, with the resulting committees ranked
to prefer larger, highly similar clusters. The final
set of committees (sense clusters) are selected by re-
cursively identifying the highest ranking committees
that are dissimilar to each other and then repeating
the process for any contexts not similar to existing
committees. In essence, CBC aims to find the clus-
ters that are similar to the largest set of contexts,
while keeping clusters dissimilar from each other.
CBC?s recursion ensures that contexts dissimilar to
the large committees are still grouped into their own
smaller committees, which enables the discovery of
infrequent senses with distinct contexts. We use a
hard sense assignment for each context, i.e., a con-
text is labeled with only one sense according to the
most similar cluster.
Streaming K-Means As WSI moves into induc-
ing senses from Web-scale amounts of data, exist-
ing clustering algorithms that keep all contexts in
memory become impractical. Jurgens and Stevens
(2010a) proposed an on-line hybrid clustering so-
lution using on-line K-Means and Hierarchical Ag-
glomerative Clustering, which automatically de-
cided the number of clusters without retaining all
the contexts. To the best of our knowledge, theirs
is the only work using an on-line approach. We
extend this work by applying a more theoretically
sound online K-Means algorithm, called Streaming
K-Means (Braverman et al, 2011), to WSI. We use
Streaming K-Means to conduct a direct algorithmic
comparison with K-Means in the hopes that online
approaches can be made just as effective as off-line
approaches.
Streaming K-Means processes each data point
only once, thus reducing the memory overhead dra-
matically. Instead of recording each data point, it
immediately assigns each data point to a cluster and
maintains K?C clusters. C varies as the algorithm
runs, initially being set to 0. When assigning a data
point, it is only assigned to an existing cluster when
their similar is above some threshold, otherwise the
data point becomes the centroid of a new cluster.
Once C reaches a threshold, based on an estimate of
the number of data points, or the overall K-Means
clustering cost reaches some limit, the centroids are
treated as new data points and re-clustered, with the
goal of merging some centroids. We follow (Jur-
gens and Stevens, 2010a) and cluster the final cen-
troids with Hierarchical Agglomerative Clustering,
with the average link criteria as suggested by (Ped-
ersen and Bruce, 1997).
3 Modeling Context
For each clustering algorithm, we consider five con-
text models that represent the types of lexical fea-
tures used by the majority of WSI approaches.
Co-Occurrence Contexts formed from word co-
occurrence are the most common in WSI algorithms.
For each occurrence of a word, those words within
a certain range are counted as features. Prior work
has used a variety of context sizes, e.g. words in
the same sentence (Bordag, 2006), in nearby lexi-
cal positions (Gauch and Futrelle, 1993), or within a
paragraph-sized context window (Pedersen, 2010).
We consider two co-occurrence context models:
a 5-word and a 25-word window. We note that in
co-occurrence-based word space algorithms, smaller
context sizes have shown to better capture paradag-
matic similarity, while larger sizes capture semantic
associativity (Peirsman et al, 2008; Utsumi, 2010).
Dependency-Relations Dependency parsing cre-
ates a syntax tree where words are directly linked
according to their relation. These links refine co-
occurrence based contexts by utilizing syntactic in-
dications of how words are related. Dependency
parsed features have proven highly effective for
word representations in many NLP applications,
e.g., (Pado? and Lapata, 2007; Baroni et al, 2010).
We follow Pantel and Lin (2002) and Dorow and
Widdows (2003) using the sentence as contexts and
all words with a dependency path of length 3 or less,
with the last word and its relation as a feature. We
note that recently Kern et al (2010) achieved good
WSI performance with only a small, manually-tuned
subset of all relations as context.
Word Ordering Word ordering can provide a
mild form of syntactic information (Jones et al,
2006; Sahlgren et al, 2008). While other syntac-
115
tic features may provide significantly more informa-
tion, word ordering is efficient to compute and pro-
vides an alternative source of syntactic information
for knowledge-lean systems or for languages where
NLP tools are not readily available.
Because we treat word ordering as a syntactic fea-
ture, we limit the context to words occurring in the
same sentence. A feature is the combination of a
co-occurring word and its relative position, i.e. the
same word in different positions is treated as two
separate features.
Parts of Speech Part of speech tagging can pro-
vide a preliminary coarse-grained sense disambigua-
tion of a word?s contextual features, where a word
may have as many senses as it does parts of speech.
For example, consider an occurrence of ?house? in
the context of ?address? as a noun and verb: ?I went
to his house address,? and ?I heard the legislator ad-
dress the house.? Labeling ?address? with its part
of speech provides for more semantic information
on its meaning, which further constrains the sense
of ?house.? Prior work (Pedersen and Bruce, 1997)
has suggested that this information can improve per-
formance, but to our knowledge, the impact of POS
features has not been evaluated in isolation.
Each context is formed from the containing sen-
tence; a feature is a combination of each word and its
part of speech, e.g., ?board-NOUN? is distinct from
?board-VERB.?
4 WSI Performance on Related Senses
The proposed methodology measures the ability of a
WSI approach to distinguish between related senses.
However, generating a large corpus with manu-
ally labeled sense assignments and sense similarity
judgements is prohibitively expensive. Therefore,
we employ a pseudo-word discrimination task where
a base word and a second word, its confounder,
are replaced throughout the corpus with a pseudo-
word. The objective is then to determine which of
the words was originally present given the context
of an occurrence of the pseudo-word. Due to not
requiring manual annotation, this type of task was
initially proposed as a substitute for word sense dis-
ambiguation (Schu?tze, 1992; Gale et al, 1992) and
for selectional preferences (Clark and Weir, 2002).
Following the suggestions of Chambers and Ju-
festival laws
offices 0.13660 interests 0.18289
play 0.13751 politics 0.20440
convention 0.20296 governments 0.29125
tournament 0.29007 regulations 0.40761
concerts 0.48348 legislation 0.56112
Table 2: Example confounders for ?festival? and
?laws? and their similarities
rafsky (2010) on designing pseudo-words, pseudo-
words were created from words with the same part
of speech and equal frequency in the training cor-
pus. We selected nouns occurring more than 5,000
times in a 2009 Wikipedia snapshot and then drew
5,000 contexts for each. The snapshot was tagged
with the Stanford Part of Speech Tagger (Toutanova
et al, 2003) and parsed with the Malt Parser (Nivre
et al, 2006).
To evaluate the impact of sense similarity, pseudo-
words were created from word pairs with a broad
range of lexical similarities. We selected lexical
similarity as an approximation of sense similarity
in order to model the hypothesis that similar senses
may appear in similar contexts. Similarity scores
were calculated using cosine similarity on contex-
tual distributions built from a sliding ?2 word win-
dow over the Wikipedia corpus. Table 2 highlights
several example confounders and their similarities
with the base term. In total, we generated 5000 term-
confounder pairs from 98 base terms, with a mean of
51 confounders per term.
All clustering parameters were chosen using the
default values provided in the original papers. K-
means and Streaming K-Means were both set with
a maximum of 15 clusters, with the final number of
clusters being determined by the data itself.
4.1 Evaluation
The pseudo-word?s senses are induced from a train-
ing segment using each feature and clustering com-
bination. Given that both words making up the
pseudo-word may be polysemous, more than two
senses may be induced. Each sense cluster is la-
beled according to which of the original words was
present in the majority of its contexts. For testing,
each instance of the pseudo-word in a previously
unseen context is assigned the label of the cluster
116
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(a) 5-Word Co-Occurrence
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(b) 25-Word Co-Occurrence
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(c) Dependency Relations
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(d) Word Order
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(e) Parts of Speech (f)
Figure 1: Pseudo-word discrimination performance
to which it is most similar. We perform five-fold
cross-validation, using 4,000 contexts for training
and 1,000 contexts for testing. Discrimination ac-
curacy is reported as the average of all five runs.
Since an equal number of contexts are used for each
term, the base line accuracy of a most frequent sense
model is 50% for each pseudo-word.
4.2 Results and Discussion
Figure 1 shows the discrimination accuracy relative
to the similarity of a base pair and confounder, for
each feature and clustering algorithm combination.
Similarity values were binned at the 0.01 level with
a mean of 39.0 scores per bin (median=11). Be-
cause most word pairs are not related, the distri-
bution of similarity values is biased towards lower
values. Therefore, we omit similarity ranges above
0.5, as too few confounders occurred in that range to
draw reliable conclusions. The standard error (not
shown) is < 1 for all measurements.
The general trends suggests that the clustering al-
gorithm impacts the sense discriminatory ability far
more than the lexical feature choice. Furthermore,
sense similarity affects most clustering algorithms,
with most systems seeing a noticeable performance
drop when pseudo-word similarity is increased just
beyond 0. Performance at high similarity becomes
more variable for all algorithms and features.
For each clustering algorithm, we see dramati-
cally different trends. Streaming K-Means performs
well with co-occurrence based features and it does
poorly when either contexts have too many features,
as in the 25 Window Co-Occurrence feature space,
or the feature space overall is too sparse, as in the
Parts of Speech and Ordering feature spaces.
K-Means with the gap statistic converges to the
most frequent sense baseline for nearly every con-
founder pair. We note that this behavior significantly
differs from that seen in (Pedersen and Kulkarni,
2006), which clustered second-order co-occurrence
vectors rather than the first-order features that we
use. Our analysis showed that the H2 criterion was
responsible for this behavior. A subsequent analy-
sis revealed that K-Means still converged to MFS
for the E1, E2, I1, and I2 criterion functions (Zhao
and Karypis, 2001) as well as when the number of
artificial datasets was increased up to 100. How-
ever, additional tests using the same features on the
SemEval-1 WSI task did not converge to MFS. Fur-
ther investigation is needed to identify the cause of
convergence and what types of data are appropriate
117
the Gap Statistic.
Clustering by Committee performs well on most
models, but significantly worse on dependency re-
lation features. A subsequent analysis showed that
CBC generates significantly more clusters than all
other models. For the POS, 5 word window, and 25
window Co-Occurrence feature spaces, CBC gener-
ated between 205 and 247 clusters on average, per
word. With the order feature space, CBC generated
1087 clusters per word. However, when paired with
dependency relation features, the number of clusters
drops to only 78 per word.
Spectral Clustering is most affected by sense
similarity, performing competitively for unrelated
senses but dropping significantly when words be-
come even slightly similar. This performance drop
is seen across all features. Performance is therefore
low, with the exception of dependency relations.
Overall, these results suggest that sense related-
ness is a important factor in WSI performance and
its impact should be considered in future WSI eval-
uations. A potential next step is to vary the pro-
portion of contexts from the confounder. The cur-
rent method intentionally uses a uniform distribu-
tion to avoid potential bias; however, word sense dis-
tributions are rarely equal, and a varied distribution
would more closely model real world distributions.
Similarly, the current method tested only two senses,
whereas an n-way disambiguation between multiple
confounders should also provide further insight into
a WSI approach?s discriminatory abilities.
5 Sense Confusion in SemEval-2 Task 14
As a second experiment, we analyze incorrect sense
assignments on SemEval-2 Task 14 (Manandhar et
al., 2010) to measure whether sense-relatedness bi-
ases which sense was incorrectly selected. For WSI
systems, a similarity bias would indicate that similar
senses are more likely to be incorrectly identified as
a single sense.
We summarize Task 14 as follows. Systems are
provided with an unlabeled training corpus con-
sisting of 879,807 multi-sentence contexts for 100
polysemous words, comprised of 50 nouns and 50
verbs. Systems induce sense representations for tar-
get words from the training corpus and then use
those representations to label the senses of the tar-
get words in unseen contexts from a test corpus.
The induced senses are then evaluated against the
gold standard labels OntoNotes (Hovy et al, 2006)
senses labels for the test corpus. For our evaluation,
we use both the two contrasting unsupervised mea-
sures, the paired FScore (Artiles et al, 2009) and the
V-Measure (Rosenberg and Hirschberg, 2007), and
a supervised measure. For each metric, we use the
evaluation framework provided by the organizers of
SemEval-2 Task 14.1
The V-Measure rates the homogeneity and com-
pleteness of a clustering solution. Solutions that
have word clusters formed from one gold-standard
sense are homogeneous; completeness measures the
degree to which a gold-standard sense?s instances
are assigned to a single cluster. The paired FScore
measures two types of overlap of a solution and the
gold standard in cluster assignments for all in pair-
wise combination of instances. This score tends
to penalize solutions with many small clusters and
highly heterogeneous clusters (Manandhar and Kla-
paftis, 2009).
The supervised evaluation measures the recall
when building a Word Sense Disambiguation classi-
fier from the induced senses. The WSI system labels
the entire corpus, which is then divided into train-
ing and test portions. The sense labels in the train-
ing portion are used to construct a mapping from in-
duced senses to the gold standard OntoNotes labels.
This mapping is then evaluated for the induced la-
bels in the test. We report the scores for the 80%
training and 20% testing scenario.
5.1 Evaluation
We expect that if sense similarity is a factor in sense
confusion, the probability of confusion will increase
with sense similarity. Therefore, we measure the
probability of labeling an instance with the incorrect
OntoNotes sense relative to the sense similarity with
the gold standard sense.
In order to calculate the incorrect assignments,
the induced senses must be mapped to OntoNotes
senses. Each induced sense, si, is mapped to the
OntoNotes sense that occurs most frequently among
the instances in the test corpus that are assigned in-
duced sense si. We note that this labeling process
is only an approximate solution to assigning gold
standard labels to induced senses. A more robust
1
http://www.cs.york.ac.uk/semeval2010_WSI/
118
 0
 50
 100
 150
 200
 250
 300
 350
 0.05  0.1  0.15  0.2  0.25  0.3
Fr
eq
ue
nc
y 
of
 S
en
se
 C
on
fu
sio
n
Ontonote Sense Similarity
Actual (avg)
Baseline (avg)
Actual (max)
Baseline (max)
(a) Streaming K-means
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0.05  0.1  0.15  0.2  0.25  0.3
Fr
eq
ue
nc
y 
of
 S
en
se
 C
on
fu
sio
n
Ontonote Sense Similarity
Actual (avg)
Baseline (avg)
Actual (max)
Baseline (max)
(b) CBC
 0
 50
 100
 150
 200
 250
 300
 350
 0.05  0.1  0.15  0.2  0.25  0.3
Fr
eq
ue
nc
y 
of
 S
en
se
 C
on
fu
sio
n
Ontonote Sense Similarity
Actual (avg)
Baseline (avg)
Actual (max)
Baseline (max)
(c) Spectral Clustering
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0.05  0.1  0.15  0.2  0.25  0.3
Fr
eq
ue
nc
y 
of
 S
en
se
 C
on
fu
sio
n
Ontonote Sense Similarity
Actual (avg)
Baseline (avg)
Actual (max)
Baseline (max)
(d) K-means
Figure 2: The error frequency distributions for confusing the correct sense with another sense of the given
similarity when using a 5-word co-occurrence window as context. Dashed lines indicate the null models.
labeling could take into account the distribution of
gold standard senses labels in the corpus from which
the senses are induced; however, such labels are not
available in the Task 14 training corpus.
For each incorrect sense assignment, we mea-
sure the similarity of the confused sense to the
correct sense. To our knowledge, no work has
been done on calculating sense similarity within the
OntoNotes sense hierarchy.2 Therefore, we approxi-
mate OntoNotes sense similarity by using sense sim-
ilarity in the WordNet ontology, on which has many
similarity measures have been defined. Following
Budanitsky and Hirst (2006), we estimate the Word-
Net sense similarity using the method proposed by
Jiang and Conrath (1997).
Each OntoNotes sense si is mapped to a set of
WordNet 3.0 senses Si = {wn1, . . . , wnn} using
2We suspect that this is in part because a word?s OntoNotes
senses have been designed to minimize sense confusion.
the sense mapping provided by the CoNLL shared
task.3 The sense similarity for two OntoNotes
senses is computed using one of two methods:
sim = 1|S1||S2|
?
wni?S1,wnj?S2
JCN(wni, wnj),
(1)
or
sim = argmax
wni?S1,wnj?S2
JCN(wni, wnj), (2)
where JCN indicates the Jiang-Conrath similar-
ity of two WordNet senses, calculated using Word-
Net::Similarity (Pedersen et al, 2004). Eq. 1 com-
putes similarity as the average similarity of all pair-
wise WordNet sense combinations, while Eq. 2 uses
the highest similarity. The resulting OntoNote sense
similarities range from 0 to 1, with 1 being maxi-
mally similar. We excluded 10 words from the test
3
http://conll.bbn.com/index.php/data.html
119
Context Feature Clustering V-Measure F-Score Recall # Clusters Purity GoF p-Value
5-Word Co-Occurrence
Streaming 6.7 55.5 54.8 4.74 0.103 p < 2.07e-37
Spectral 10.8 39.2 54.3 8.41 0.194 p < 1.11e-25
CBC 23.9 8.2 39.5 39.7 0.665 p < 0.916
K-Means 2.5 61.8 55.6 1.68 0.020 p < 1.20e-37
25-Word Co-Occurrence
Streaming 2.6 61.7 55.5 1.7 0.020 p < 1.20e-37
Spectral 5.0 48.6 55.9 3.3 0.083 p < 4.36e-32
CBC 21.3 11.6 45.0 32.2 0.561 p < 0.011
K-Means 2.5 61.8 55.6 1.68 0.020 p < 1.20e-37
Dependency Relations
Streaming 3.0 61.5 55.6 1.9 0.022 p < 7.33e-38
Spectral 8.5 46.8 55.3 5.9 0.134 p < 5.45e-14
CBC 12.9 31.3 52.4 11.4 0.259 p < 4.07e-12
K-Means 2.5 61.8 55.6 1.6 0.020 p < 1.20e-37
Word Order
Streaming 10.8 43.1 54.2 10.8 0.300 p < 4.46e-24
Spectral 12.2 32.4 53.7 10.0 0.26 p < 3.27e-20
CBC 27.2 11.8 30.3 54.9 0.857 p < 0.999
K-Means 2.5 61.8 55.6 1.6 0.020 p < 1.20e-37
Parts of Speech
Streaming 6.6 53.0 54.5 4.7 0.117 p < 1.06e-39
Spectral 10.9 39.4 53.7 8.3 0.201 p < 2.38e-13
CBC 23.8 08.0 40.1 39.7 0.678 p < 1.04e-2
K-Means 2.5 61.8 55.6 1.6 0.020 p < 1.20e-37
SemEval-2 Most Frequent Sense 0.0 63.4 58.6 1.0 0.0 p < 4.244e-23
Best SemEval-2 FScore 0.0 63.3 58.6 1.0 0.0 p < 2.893e-23
Best SemEval-2 VMeasure 16.2 26.7 58.3 10.7 0.367 p < 1.956e-14
Best SemEval-2 Supervised Recall 15.7 49.7 62.4 11.5 0.187 p < 8.910e-19
Table 3: Unsupervised and Supervised scores on the SemEval-2010 WSI Task for each feature and clustering
models, with reference scores for the top performing systems for each evaluation shown below.
set that did not have mappings from OntoNotes to
WordNet 3.0 senses, and additional 23 words that
only had two senses, which prevented testing for
a similarity bias. The remaining 67 words yielded
4,097 test instances for evaluation.
Each instance of the test corpus was tested for
sense confusion, recording the similarity of the in-
correctly assigned sense and the gold standard sense.
The resulting incorrect assignments are transformed
into an error distribution according by accumulating
error counts into similarity bins where each bin has a
range of 0.02. We analyze the WSI systems defined
in section 4 as well as the results of three systems
that participated in Task 14 and scored the highest
on the paired FScore, V-measure, or Supervised Re-
call evaluations.
To quantify the impact, we compare each system?s
error distribution against a null model over the set of
incorrect test instances missed by that system. In
the null model, the incorrect sense for each instance
is selected with uniform probability from the avail-
able senses. This behavior produces a distribution
with no similarity bias. The cumulative error dis-
tribution for the null model is not uniform due to
multiple sense pairings having the same similarity.4
To quantify the difference between a system?s error
distribution and corresponding null model, we cal-
culate the G-test as a measure of Goodness of Fit
(GoF). The resulting p-values reflect the probability
of observing the system?s error distribution if there
was no bias from sense-similarity.
5.2 Results and Discussion
We compare the error analysis against the evalua-
tion measures of Task 14. Table 3 displays the eval-
4Verb senses often have a JCN similarity of 0 due to hav-
ing no shared parent within the WordNet verb sense hierarchy,
which results in high frequency distribution around 0.
120
uation measures. We also report the average num-
ber of clusters per word, the cluster purity, and the
p-value when using Eq. 2 to measure sense similar-
ity. Figure 2 visualizes the error distributions for the
four clustering algorithms on 5-word co-occurrence
features. The distributions in Figure 2 are represen-
tative of those of the other context models, which we
omit due to space. Each plot reflects the frequency
at which a sense with the specified similarity was
confused for the correct sense.
The low p-values in Table 3 indicate a significant
deviation from the null model. Examining the shape
of the error distribution in Figure 2 reveals a no-
ticeable skew towards higher similarity when an in-
correct sense assignment is made. This distribution
skew is also consistent for both similarity measures.
Comparing the Task 14 results in Table 3 to the
sense confusion trends in in Figure 2 highlights an
interesting pattern among the various models: as the
number of induced sense clusters increases, the er-
ror distribution better approximates the null model.
Specifically, the GoF for all models was well corre-
lated with cluster purity (?=0.66), and the number of
clusters (?=0.76). CBC generated the highest num-
ber of clusters and has a sense confusion distribution
that closely matches the null model, indicating that
it is less affected by sense similarity. In compari-
son, all of the Streaming K-Means models, which
have the fewest clusters, differ noticeably from the
null model. Spectral Clustering, which also gener-
ates fewer clusters than CBC, has an observed con-
fusion rate that differs from the baseline. K-Means
again reduces to the MFS baseline.
When comparing along the feature sets, we see
that on average Word Order features generate the
highest V-Measure scores, highest purity, and high-
est p-values for Streaming K-Means and CBC. This
result correlates well with the average number of
features seen per context: Word Order contexts used
0.03% of the feature space while contexts in other
feature spaces used between 0.07% and 0.12% of
the feature space, suggesting that the SemEval mea-
sures are determined in part by feature space den-
sity. Similarly, 25-word co-occurrence features had
the highest percentage of features used per context,
0.12%, and generated the lowest V-Measure, purity
score, and p-value for 3 clustering models.
These scores support another known trend in the
SemEval-2 evaluation: the performance on the V-
Measure is proportional to the number of induced
sense clusters, while the paired FScore is inversely
proportional. But what is surprising is that models
which perform well against the V-Measure also ex-
hibit a smaller sense similarity bias, suggesting that
CBC and similar clustering methods are suitable for
situations where competing senses of a word have a
high degree of overlap.
As a final comparison, we also computed the
sense bias for the top 3 SemEval systems under each
measure. The best of these models are listed in Table
3. We did not find any consistent trends between the
V-Measure, purity, and p-value among these mod-
els. The top F-Scoring models all used either a first
or second order co-occurrence feature space similar
to ours (Kern et al, 2010; Pedersen, 2010), whereas
the top supervised score was achieved by a graph-
based system (Klapaftis and Manandhar, 2008).
6 Future Work and Conclusion
We presented a two evaluation for WSI approaches
and examined the performance of a wide range of
algorithms. The results raise a potential issue for
clustering-based WSI approaches: sense discrimi-
nation degrades notably as the sense relatedness in-
creases. We highlight three potential avenues for
future research. First, this methodology should be
applied to additional WSI models, such as graph-
based (Klapaftis and Manandhar, 2008; Navigli and
Crisafulli, 2010) and probabilistic models (Dinu and
Lapata, 2010; Elshamy et al, 2010). Second, we
plan to extend the analysis to different sense dis-
tributions, varying number of senses, and for hu-
man annotated sense similarity data. Third, this
evaluation makes the simplifying assumption of one
sense per instance; however, Erk et al (2009) note
that the relations between senses may cause a single
word instance to evoke multiple senses within the
same context. Therefore, a future experiment should
consider how WSI systems might address learning
senses given the presence of multiple, similar senses
for a single instance.
All models, associated data sets, testing frame-
work, and scores have been released as a part of the
open-source S-Space Package (Jurgens and Stevens,
2010b).5
5
http://code.google.com/p/airhead-research/
121
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations, pages 7?12.
ACL, June.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of EMNLP, pages 534?542. ACL.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Stefan Bordag. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. In Pro-
ceedings of the 11th EACL, pages 137?144.
Vladimir Braverman, Adam Meyerson, Rafail Ostrovsky,
Alan Roytman, Michael Shindler, and Brian Tagiku.
2011. Streaming k-means on Well-Clusterable Data.
In Proceedings of SODA 2011.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47, March.
Nathanael Chambers and Dan Jurafsky. 2010. Improving
the Use of Pseudo-Words for Evaluating Selectional
Preferences. In ACL 2010.
David Cheng, Ravi Kannan, Santosh Vempala, and Grant
Wang. 2006. A divide-and-merge methodology for
clustering. ACM Transactions on Database Systems
(TODS), 31(4):1499?1525.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the ACL-02 workshop on
Word sense disambiguation: recent successes and fu-
ture directions - Volume 8, WSD ?02, pages 32?39,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172. Association
for Computational Linguistics.
Beate Dorow and Dominic Widdows. 2003. Discover-
ing corpus-specific word senses. In Proceedings of the
10th EACL, pages 79?82.
Wesam Elshamy, Doina Caragea, and William H. Hsu.
2010. KSU KDD: Word sense induction by cluster-
ing in topic space. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 367?
370. Association for Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1, pages 10?18. Association
for Computational Linguistics.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Work on statistical methods for word
sense disambiguation. In AAAI Fall Symposium on
Probabilistic Approaches to Natural Language, pages
54?60.
Susan Gauch and Robert P. Futrelle. 1993. Experiments
in automatic word class and word sense identification
for information retrieval. In Proceedings of the 3rd
Annual Symposium on Document Analysis and Infor-
mation Retrieval, pages 425?434.
Zhengyan He, Yang Song, and Houfeng Wang. 2010.
Applying Spectral Clustering for Chinese Word Sense
Induction. In Proceedings of the CIPS-SIGHAN Joint
Conference on Chinese Language Processing.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
57?60. Association for Computational Linguistics.
Jay J. Jiang and David W. Conrath. 1997. Semantic Sim-
ilarity Based on Corpus Statistics and Lexical Taxon-
omy. In Proceedings of International Conference on
Research in Computational Linguistics,.
Michael N. Jones, Walter Kintsch, and Doughlas J. K.
Mewhort. 2006. High-dimensional semantic space ac-
counts of priming. Journal of Memory and Language,
55:534?552.
David Jurgens and Keith Stevens. 2010a. HERMIT: Us-
ing word ordering applied to the Sense Induction task
of SemEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluations. Association for
Computational Linguistics.
David Jurgens and Keith Stevens. 2010b. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations.
Roman Kern, Markus Muhr, and Michael Granitzer.
2010. KCDC: Word sense induction by using gram-
matical dependencies and sentence phrase structure.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 351?354. Association for
Computational Linguistics.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
sense induction using graphs of collocations. In Pro-
ceeding of the 2008 conference on ECAI 2008: 18th
European Conference on Artificial Intelligence, pages
298?302.
122
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Tax-
onomy learning using word sense induction. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, HLT ?10, pages
82?90, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Suresh Manandhar and Ioannis P. Klapaftis. 2009.
SemEval-2010 Task 14: Evaluation Setting for Word
Sense Induction & Disambiguation Systems. In
NAACL-HLT 2009 Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-
2010 task 14: Word sense induction & disambigua-
tion. In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68. Association for
Computational Linguistics.
Diana McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. Making Sense of Sense:
Bringing Psycholinguistics and Computational Lin-
guistics Together, page 17.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing
word senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 116?
126. Association for Computational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation: a
survey. ACM Computing Surveys, 41(2):1?69.
Andrew Ng, Michael Jordan, and Yair Weiss. 2001. On
spectral clustering: Analysis and an algorithm. In Ad-
vances in Neural Information Processing Systems 14:
Proceeding of the 2001 Conference, pages 849?856.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC, pages 2216?2219.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models. Com-
putational Linguistics, 33(2):161?199.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
Natural Language Engineering, 13(02):137?163.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM Conference
on Knowledge Discovery and Data Mining (KDD-02),
pages 613?619.
Ted Pedersen and Rebecca Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of the
Second Conference on Empirical Methods in Natural
Language Processing, pages 197?207, August.
Ted Pedersen and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In In Proceedings of the Demo Session of
HLT/NAACL, pages 276?279.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet:: Similarity: measuring the
relatedness of concepts. In Demonstration Papers at
HLT-NAACL 2004 on XX, pages 38?41. Association
for Computational Linguistics.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters Ap-
plied to the Sense Induction Task of SemEval-2. In
Proceedings of the SemEval 2010 Workshop : the
5th International Workshop on Semantic Evaluations,
pages 363?366, July.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts. 2008.
Size matters. Tight and loose context definitions in En-
glish word space models. In ESSLLI Workshop on Dis-
tributional Lexical Semantics, pages 34?41.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL). ACL, June.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order in
word space. In Proceedings of the 30th Annual Meet-
ing of the Cognitive Science Society (CogSci?08).
Hinrich Schu?tze, 1992. Context Space, pages 113?120.
AAAI Press, Menlo Park, CA.
Robert Tibshirani, Guenther Walther, and Trevor Hastie.
2000. Estimating the number of clusters in a dataset
via the gap statistic. Journal Royal Statistics Society
B, 63:411?423.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. pages 252?
259.
Akira Utsumi. 2010. Exploring the Relationship be-
tween Semantic Spaces and Semantic Relations. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC?2010),
pages 257?262.
Ulrike von Luxburg. 2007. A tutorial on spectral cluster-
ing. Statistics and Computing, 17(4):395?416.
Ying Zhao and George Karypis. 2001. Criterion func-
tions for document clustering: Experiments and analy-
sis. Technical Report UMN CS 01-040, University of
Minnesota.
123
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 51?61,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Twitter Users #CodeSwitch Hashtags! #MoltoImportante #wow #???
David Jurgens, Stefan Dimitrov, Derek Ruths
School of Computer Science
McGill University
Montreal, Canada
jurgens@cs.mcgill.ca, stefan.dimitrov@mail.mcgill.ca,
druths@networkdynamics.org
Abstract
When code switching, individuals incor-
porate elements of multiple languages into
the same utterance. While code switching
has been studied extensively in formal and
spoken contexts, its behavior and preva-
lence remains unexamined in many newer
forms of electronic communication. The
present study examines code switching in
Twitter, focusing on instances where an
author writes a post in one language and
then includes a hashtag in a second lan-
guage. In the first experiment, we per-
form a large scale analysis on the lan-
guages used in millions of posts to show
that authors readily incorporate hashtags
from other languages, and in a manual
analysis of a subset the hashtags, reveal
prolific code switching, with code switch-
ing occurring for some hashtags in over
twenty languages. In the second experi-
ment, French and English posts from three
bilingual cities are analyzed for their code
switching frequency and its content.
1 Introduction
Online platforms enable individuals from a wide
variety of linguistic backgrounds to communi-
cate. When individuals share multiple languages
in common, their communication will occasion-
ally include linguistic elements from multiple lan-
guages (Nilep, 2006), a practice commonly re-
ferred to as code switching. Typically, during code
switching, the text or speech in a language retains
its syntactic and morphological constraints for that
language, rather than having text from both lan-
guages conform to one of the language?s grammat-
ical rules. This requirement enables code switch-
ing to be separated from borrowing, where foreign
words are integrated into a native language?s lexi-
con and morphology (Gumperz, 1982; Poplack et
al., 1988; Sankoff et al., 1990).
While work on code switching began with con-
versational analyses, recent work has examined
the phenomena in electronic communication, find-
ing similar evidence of code switching (Climent
et al., 2003; Lee, 2007; Paolillo, 2011). How-
ever, these investigations into code switching have
largely examined interpersonal communication or
settings where the number of participants is lim-
ited. In contrast, social media platforms such as
Twitter offer individuals the ability to write a text
that is decoupled from direct conversation but may
be read widely.
Twitter enables users to post messages with spe-
cial markers known as hashtags, which can serve
as a side channel to comment on the post itself
(Davidov et al., 2010). As a result, multilingual
authors have embraced using hashtags from lan-
guages other than the language of their post. Con-
sider the following real examples:
? Eating an apple for lunch while everyone
around me eats cheeseburgers and fries.
#yoquiero
? Jetzt gibt?s was vern?unftiges zum es-
sen! #salad #turkey #lunch #healthy
#healthylifestyle #loveit
? Hasta ma?nana a todo mundo. Que tengan
linda noche. #MarketerosNocturnos #Mar-
ketingDigital #BlackVirs #SocialMedia
? 1% ????????????? ??????????????????-
???D+ ???? C ??? B+????A ????????????-
???????????? #???? #??????? #fail
Here, the first author posted in English with a
Spanish hashtag reflecting the author?s envious
disposition. In the second, the author comments
in German on sensible food, using multiple En-
glish hashtags to describe the meal and their atti-
tude. In the third and fourth, the authors comment
51
on sleep and school, respectively, and then each
use hashtags with similar meanings in both their
native language and English.
Hashtags provide authors with a communica-
tion medium that also has broader social utility
by embedding their post within global discussion
of other posts using the same hashtag (Letierce et
al., 2010) or by becoming a part of a virtual com-
munity (Gupta et al., 2010). These social motiva-
tions resemble those seen for why individuals may
code switch, such as to assimilate into a group or
make discussions easier (Urciuoli, 1995). Twit-
ter and other hashtag-supporting platforms such as
Instagram and Facebook offer a unique setting for
code switching hashtags for two reasons: (1) po-
tential readers are disconnected from the author,
who may not know of their language fluency, and
(2) text translation is built into the platform, which
enables readers to translate a post into their na-
tive language. As such, authors may be motivated
to include a hashtag of another language to in-
crease their potential audience size or to appear as
a member of a multilingual virtual community.
Despite the prevalence of non-English tweets,
which are approaching 50% of the total volume
(Liu et al., 2014), no study has examined the
prevalence of hashtag code switching. We pro-
pose an initial study of hashtag code switching in
Twitter focusing on three central questions: (1)
for which language pairs do authors write in the
first language and then incorporate a hashtag of
the second language, (2) when tweets include a
hashtag of a different language, which instances
signal code switching behavior, and (3) the degree
to which bilingual populations code switch hash-
tags. Here, we adopt a general definition of code
switching as instances where an individual estab-
lishes a linguistic context in one language and
then includes elements (such as words) from one
or more other languages different from the first.
Two experiments are performed to answer these
questions. In the first, we test general methods to
identify which languages adopt the same hashtags
and whether those shared hashtags are examples of
code switching. In the second, we focus on three
bilingual cities to examine hashtag code switching
behavior in French and English speakers.
Our study provides three main contributions.
First, we demonstrate that hashtag code switching
is widespread in Twitter. Second, we show that
Twitter as a platform includes multiple phenom-
ena that can be falsely interpreted as code switch-
ing and therefore must be accounted for in future
analyses. Third, in a study of French and English
tweets from three cities, we find that an increased
rate of bilinguality decreases the frequency of in-
cluding hashtags from another language but in-
creases the overall rate of code switching when
such hashtags are present. Furthermore, all data
for the experiments is made publicly available.
2 Related Work
Research on code switching is long standing, with
many theories proposed for the motivations be-
hind code switching and how the two languages
interact linguistically (Poplack and Sankoff, 1984;
Myers-Scotton, 1997; Auer, 1998). Most related
to the present work are those studies examining
code switching in online communications.
Climent et al. (2003) examined the use of Span-
ish and Catalan in newsgroups, finding it occurs
2.2% and 4.4% of the Catalan and Spanish con-
texts, respectively. Lee (2007) analyzed a cor-
pus of Cantonese and English emails and ICQ
instant messages and surveyed Hong Kong users
of each form of communication. She found that
the users preferred mixed-language communica-
tion, with no user indicating that they communi-
cated in only Cantonese. Furthermore, the shorter,
more informal ICQ messages were more likely to
be code switched (99.4%) than emails (41.3%).
Paolillo (2011) measured code switching
amongs English, Hindi, and Punjabi in both
IRC and Usenet forum posts, finding similar
to Climent et al. (2003) that the shorter, more
conversational IRC posts had higher rates of
code switching. Paolillo (2011) also note that
code switching rates differed between Hindi and
Punjabi speakers.
The present work differs significantly from
these three studies in two aspects. First, we as-
sess code switching across all language commu-
nities on Twitter, rather than examining individual
groups of bilingual speakers. Second, we focus
our analysis only on the code switching of a post?s
hashtag due to its unique role in microtext (Gupta
et al., 2010), which has yet to be examined in this
context.
3 Hashtag Use in Twitter
Hashtags provide general functionality on Twit-
ter and prior works have proposed that they serve
52
Name Description Examples
ANNOTATION Serves as an annotation about the author?s feelings or comments
on the content of a tweet.
#happy #fail #cute #joking
#YoloSwaggins
COMMUNITY A topical entity that links the tweet with an external community,
which is commonly topical but also includes ?team-like? groups
#music #friends #BecauseItIs-
TheCup #TeamEdward
NAMED
ENTITY
Refers to a specific entity that has a universally recognized
name.
#Glee #TeenChoiceAwards
#WorldCup2014
PLATFORM Refer to some feature or behavior specific to the Twitter plat-
form.
#followback #lasttweet #oomf
APPLICATION Generated by a third-party application, which automatically in-
cludes its hashtag in the message.
#AndroidGames #NowPlaying
#iPhone #Android
VOTING Created as a result of certain real-world phenomena asking in-
dividuals to tweet with specific hashtags as a way of voting.
#MtvHottest #iHeartAwards
ADVERTISING Promoting an item, good, or service, which can be sought out
by interested parties.
#forsale #porn
SPAM Used by adversarial parties to appear on trending lists and to
make spam accounts appear real.
#NanaLoveLingga #681team
#LORDJASONJEROME
Table 1: A taxonomy of hashtag according to their intended use.
a dual role as (1) bookmarking content with the
tag?s particular expression and (2) functioning as a
method for ad hoc community formation and dis-
cussion around a tag?s topic (Gupta et al., 2010;
Davidov et al., 2010; Yang et al., 2012). However,
the diverse user base of the Twitter platform has
given rise to additional roles for hashtags beyond
these two. For example, many popular hashtags
focus on promoting users to follow each other,
1
such as #followback and #openfollow. Similarly,
contests are run on Twitter, which have individu-
als vote by posting using a specific hashtag, e.g.,
#MtvHottest.
Given hashtags? flexible roles, some may be
used in multiple languages without being exam-
ples of code switching, such as the contest-based
or follower-promotion hashtags noted above.
Therefore, we first propose a taxonomy for clas-
sifying all types of hashtags according to their pri-
mary observed use in order to disentangle poten-
tial code switching behavior from Twitter-specific
behavior. To construct the taxonomy, two an-
notators independently reviewed several thousand
hashtags of different frequency to assess the dif-
ferences in how the tag was used in practice. Each
annotator then proposed their own taxonomy. The
final taxonomy was produced from a discussion of
differences, with both annotators initially propos-
ing highly similar taxonomies.
2
1
In Twitter, following denotes creating a directional social
relation from one account to another.
2
We note that a small number of hashtags did not fit this
taxonomy due to their idiosyncratic use. These hashtags were
typically single-letter hashtags used when spelling out words,
e.g., ?tonight is going to be #f #u #n,? or when the author has
mistakenly used punctuation, which is not included in Twit-
Table 1 shows the proposed taxonomy, contain-
ing eight broad types of hashtags. The first two
types of hashtags correspond to the main hash-
tag roles proposed in Yang et al. (2012). The
NAMED ENTITY tags also serve as method for
individuals to link their content with a specific
audience like the COMMUNITY type; however,
NAMED ENTITY tags were treated as a separate
group for the purposes of this study because the
entities typically have a common name which is
used in all languages and therefore would not be
translated; in contrast, COMMUNITY hashtags re-
fer to more general topics such as #soccer, which
may be translated, e.g., #futbol. Hashtags of the
five remaining types would likely not be observed
in instances of code switching, with such hash-
tags often being used for purposes other than inter-
personal communication.
4 Experiment 1: Popular Hashtags
Persistently popular hashtags reflect established
norms of communication on Twitter. We hypoth-
esize that these hashtags may be adopted by the
speakers of multiple languages for joining a global
discussion. Therefore, the first experiment ex-
amines the most-used hashtags over a five month
period to measure two aspects: (1) which lan-
guages adopt the hashtags of other languages and
(2) which hashtags used in multiple languages are
evidence of code switching.
ter?s definition of a hashtag, e.g., ?#I?mAwesome,? which has
the hashtag #I rather than the full expression.
53
4.1 Experimental Setup
Data Hashtag frequencies were calculated from
981M tweets spanning March 2014 to July 2014.
Frequencies were calculated over this five month
period in order to focus on widely-used hashtags,
rather than bursty hashtags that are popular only
for a short time, such as those studied in Huang
et al. (2010) and Lin et al. (2013). For each hash-
tag, up to 10K non-retweet posts containing that
hashtag were retained, randomly sampling from
the time period studied when more than 10K were
observed. To enable a more reliable estimate of
the language distribution, we restrict our analysis
to only those hashtags with more than 1000 posts,
for a total number of 19.4M posts for 4624 hash-
tags, with an average of 4204 posts per hashtag.
Language Identification The languages of
tweets were identified using a two-step procedure.
First, message content was filtered to remove con-
tent such as usernames, URLs, emoji, and hash-
tags. Tweets with fewer than three remaining to-
kens were excluded (e.g., a message with only
hashtags). Second, the remaining content was
processed using langid.py (Lui and Baldwin,
2012), a state of the art language identification
program that supports the diversity of languages
found on Twitter.
Determining the language of a hashtag in a gen-
eral setting for all languages is difficult due to the
presence of acronyms, abbreviations, and slang.
Therefore, we adopt a heuristic where a hashtag?s
language is set as the language used by the major-
ity of its tweets. To quantify the accuracy of this
heuristic, two annotators inspected the tweets of
200 hashtags to identify the language of the hash-
tag and for the majority of the tweets. This anal-
ysis showed that the heuristic correctly identifies
the hashtag?s language in 96.5% of the instances.
4.2 Hashtag Sharing by Languages
The adoption of a hashtag by a second language
was measured by calculating the frequency with
which tweets using a hashtag with language l
1
were labeled with language l
2
. The noisy nature
of microtext is known to make language identifi-
cation difficult (Bergsma et al., 2012; Goldszmidt
et al., 2013) and can create spurious instances
of second-language hashtag adoption. Therefore,
we impose a minimum frequency of hashtag use
where l
2
is only said to use a hashtag of l
1
if at
least 20 tweets using that hashtag were labeled
Hashtag # Langs. Primary
Lang.
Type
#lastfm 39 en APPLICATION
#WaliSupitKEPO 32 id SPAM
#RenggiTampan-
DanKece
32 id SPAM
#NP 32 en APPLICATION
#Np 32 en APPLICATION
#MTVHottest 31 en VOTING
#SidikLoveTini 30 id SPAM
#np 30 en APPLICATION
#GER 29 en NAMED ENTITY
#User Indonesia 29 id APPLICATION
#Soccer 29 en COMMUNITY
#RobotKepo 29 id APPLICATION
#KeePO 27 id APPLICATION
#NowPlaying 28 en APPLICATION
#Hot 28 en ADVERTISEMENT
Table 3: The hashtags associated with the most
number of languages having at least 20 tweets us-
ing that hashtag
with l
2
. To quantify the accuracy of our hashtag
adoption measure, two annotators inspected the
second-language tweets of 200 hashtags, sampled
from the data and representing 40 language pair
combinations; this analysis showed that with the
filtering the assertion that at least one author from
language l
1
used a hashtag of language l
2
was cor-
rect in 67% of the instances.
Table 2 shows the frequency with which au-
thors using the 15 most-commonly observed lan-
guages (shown as columns using their ISO 639-1
language codes) adopt a hashtag from another of
the most-common languages (shown as rows), re-
vealing widespread sharing of hashtags between
languages. English hashtags are the most fre-
quently used in other languages, likely due to it be-
ing the most common language in Twitter. How-
ever, other languages? hashtags are also adopted,
with Spanish, Japanese, and Indonesian being the
most common after English.
Despite the strong evidence of using of a sin-
gle hashtag in multiple languages, the results in
Table 2 should not be interpreted as evidence of
code switching. Table 3 shows the 15 hashtags
used in the most number of languages. The ma-
jority of these hashtags are generated by either
(1) Twitter-based applications that automatically
write a tweet in a user?s native language and then
append a fixed English-language hashtag or (2)
spam-like accounts that use the same hashtag and
include random text snippets in various languages,
neither of which signal code switching behavior.
Furthermore, given the noise introduced by lan-
guage misidentification and spam behavior on the
54
Language of tweet
de ru ko pt en it fr zh es ar th ja id nl tr
de 2 1 4 15 6 9 4 9 1 4 6 1
ru 3 3 3 25 7 5 8 7 2 1 7 7 7 1
ko 4 2 13 3 6 5 10 3 10 11 4 2
pt 14 3 64 45 40 13 63 2 4 3 15 10
en 1705 532 155 1235 1735 2183 1171 2482 362 176 742 1097 1101 342
it 5 2 1 10 29 15 4 22 5 3 6 3 1
fr 38 2 3 36 87 49 28 67 8 1 12 19 29 6
zh 3 4 2 2 12 1 2 4 1 11 1 1
es 67 17 3 321 435 264 206 105 29 5 32 66 66 31
ar 6 2 38 4 9 6 7 8 5 1 2
th 3 7 1 24 5 4 8 8 2 6 4 1
ja 17 18 11 11 123 17 24 132 45 2 2 14 12 4
id 84 2 6 25 131 88 58 14 92 6 5 11 52 17
nl 13 1 3 17 6 11 2 9 1 1
tr 17 1 3 28 9 7 7 13 3 1 22 9
Table 2: The frequency with which a hashtag is used by multiple languages. Columns denote the lan-
guage in which the tweet is written; rows denote the hashtag?s language; and cell values report the
number of hashtags where the column?s language has used the hashtag in at least 20 tweets. Diagonal
same-language values are omitted for clarity.
Twitter platform, we view the initial results in Ta-
ble 2 an overestimate of hashtag adoption by lan-
guages other than the hashtag?s source language.
A further inspection of language classification er-
rors revealed four common factors: (1) the lack of
accents on characters,
3
(2) the use of short words,
which appeared ambiguous to langid.py, (3)
the use of non-Latin characters for emoticons or
visual affect, and (4) proper names originating
from a language different from the tweet?s. Never-
theless, the observed trends do provide some guid-
ance as to which language pairs might share hash-
tags and also may code switch.
Among the hashtags in Table 3, two are legit-
imately used by authors in multiple languages:
#soccer and #GER, the latter corresponding to the
German soccer team. Both hashtags were popular
due to the World Cup, which occurred during the
time period studied. For both, authors included
these hashtags while taking part in a global con-
versation about the games and event. The hashtag
#soccer is a clear case of code switching, where
individuals are communicating their interests in
multiple languages, even when equivalent hash-
tags in the tweet?s language are actively being
used. Indeed, over half of the languages using
#football had at least one tweet containing both
#football and #futbol. The example of #GER high-
lights a boundary case of code switching. Here,
GER is an abbreviation for the country?s name,
making it a highly-recognized marker, rather than
3
In particular, the lack of character accents caused signif-
icant difficulties in distinguish between Spanish and Catalan.
an example of a language change that results in
code switching; however, the country has differ-
ent names depending on the language used (e.g.,
Deutschland), which does point to an active choice
on an author?s part when selecting a particular
name and its abbreviation.
4.3 Analysis by Hashtag Type
In a second analysis, we focus specifically on
hashtags classified as COMMUNITY and ANNO-
TATION, which are more associated with inten-
tional communication actions and therefore more
likely to be used in instances of code switching.
Performing such an analysis at scale would re-
quire automated methods for classifying hashtags
by their use, which is beyond the scope of this ini-
tial investigation. Therefore, we performed a man-
ual analysis of the 100 most-common, 100 least-
common, and 100 median-frequency hashtags in
our dataset to assess the distribution of hashtag
types and cases of code switching among the
COMMUNITY and ANNOTATION hashtags. Two
annotators labeled each hashtag, achieving 64.6%
agreement on the type annotations; disagreements
were largely due to mistaken assignments rather
that disputed classifications.
4
An adjudication step
resolved all disagreements. Additionally, eleven
hashtags were excluded from analysis due being
made of common words (e.g., #go, #be) which had
4
In particular, mistakes were more common when analyz-
ing hashtags used in languages outside the annotators? flu-
ency, which required a more careful assessment of why the
hashtag was being used.
55
 0
 10
 20
 30
 40
 50
advertisement
annotation
application
community
named entity
platform
spam voting
Fre
que
ncy
Lowest FrequencyMedian FrequencyHighest Frequency
Figure 1: Type distributions of the sets of 100
highest, median, and lowest frequency hashtags
used in our dataset
no meaningful interpretation for their use. Fol-
lowing, we describe the results of the analysis and
then highlight several types of hashtags.
Figure 1 shows the distribution of hashtag types
observed in the three samples. SPAM and AP-
PLICATION hashtags were most common among
highest frequency hashtags, whereas the low-
est frequency tags in the dataset were also ei-
ther SPAM or VOTING. Surprisingly, the me-
dian frequency hashtags had the majority of the
discussion-related hashtag types
Within the ANNOTATION and COMMUNITY
types, we selected thirteen hashtags each to man-
ually evaluate if code switching behavior was ob-
served. For each hashtag, two annotators reviewed
all associated tweets that were identified as using
a different language than that of the hashtag. An-
notators were instructed to consider the tweet an
instance of code switching only in cases where
(1) there was sufficient text to determine the mes-
sage?s actual language and (2) the message was an
act of communication (in contrast to spam-like or
nonsensical messages).
Code switching behavior was observed for
eleven of the ANNOTATION hashtags and twelve
of the COMMUNITY hashtags. Table 4 shows
those code switched hashtags and the languages
in which they were seen, highlighting the varying
frequency with which hashtags were used in multi-
ple languages. For example, the primarily Arabic
hashtag #Hadith was used in English and Dutch
tweets; similarity, all three Spanish hashtags were
used in English tweets.
Many hashtags are used primarily with lan-
guages that are associated with countries known
Hashtag Lang. Lang. of Code Switched Tweet
#Noticias es en
#Facts en id th fr es ru
#simple en id es fr ms tr tl sw zh ja ko
#bitch en ar cs de es fr id it ja ms nl pt ru
sv tl tr zh
#delicious en ca de es fr id it ja ko ms nl ru th
tr zh
#Design en ar de es fr ja kr pt th tl zh
#Felicidad es ca en
#SWAG en de es fr id it pl pt ru
#fresh en es fr id it ms nl sv
#BoludecesNO es en
#truth en ar bs bu es fr hi id ja it ms pa pt
ru tl zh
#Hadith ar nl en
#Quran ar fa ms id sw az it de en
#hadith ar fr en
#tech en de es nl ar el fr ro id it ja ms no
pl pt ru sq sv zh
#RemajaIndonesia jv ms
#class en ar tr es bg de fr pt he hr id it ja
lt lv ms nl ru sw tl uk zh
#animals en ar ca de es fr pt it ms ja mk pl pt
ro ru tl tr ur vi
#cine es ca de en fr ja pt ro ru
#sunday en es ar tr fr ca de el gl hu id it ja
ms ko pt nl nn no pl ro ru sl sv
th tl zh
#Energy en ru es de fr it pt tr
#change en ar nl es cs de el eu fr pt id it ja
ko jv lv ms nb no pl ro uk ru sv
ta th tl tr ur zh
#magic en nl fr ar ru ca cs de el it es hu id
ja jv ko lv ru ms nn pl pt ro sq
sv sw sl tl tr zh
Table 4: Code switched hashtags and the lan-
guages of the tweets in which they were seen
(ANNOTATION types top, COMMUNITY types bot-
tom).
to have bilingual speakers fluent in English. How-
ever, several hashtags were used in a variety of
diverse languages. For example, #truth was used
with languages such as Arabic, Bosnian, Bulgar-
ian Hindi, and Punjabi. The most widely code
switched hashtag was #magic. In English, the
hashtag is commonly used with content on magic
tricks; however, in other languages, the hashtag
often connotes surprise. For example, the Lat-
vian tweet ?Es izmekl?eju visu plauktu, nekur nav.
Mamma piejiet ne sekunde nepag?aja, kad vin?a
atrada. #magic? comments on having an item on
the shelf disappear when looking for it, only for it
to reappear like magic.
During annotation, we observed that authors
were highly productive in their code switching, us-
ing these hashtags to generate the types of emo-
tional and sarcastic messages typically seen in
same-language messages. For example, in the
Swedish tweet ?Bussen luktar spya och ?ol. #fresh?
the author is sarcastically commenting on a bus
56
that smells of vomit and beer.
4.4 Discussion
The process of annotating code switching for
hashtags revealed four notable trends in author be-
havior that occurred with multiple hashtags. First,
authors fluent in non-Latin writing systems will
often use Latin-transliterated hashtags, which are
then adopted by authors of Latin-based systems.
For example, the hashtag #aikatsu describes a col-
lectible card game and anime and is heavily used
by both Japanese and English authors. Similar-
ity, the transliterated hashtags #Hadith and #Quran
are commonly associated with Arabic-language
tweets, which rarely include an Arabic-script ver-
sion of those hashtags even when the tweets in-
clude other hashtags in Arabic.
Second, when two or more languages share the
same written form of a word (i.e., homographs),
the resulting hashtags become conflated and ap-
pear as false examples of code switching. For ex-
ample, #Real was widely used in both English and
Spanish, but with two meanings: the English us-
ages denoting something existent (i.e., not fake)
and the Spanish usages referring to Real Madrid
FC, a soccer club. The hashtag #cine also posed
a challenge due to abbreviation. While many
Spanish-language tweets include #cine (cinema),
tweets in other languages include #cinema and its
abbreviated form #cine, which matches the Span-
ish term, creating false evidence of code switch-
ing.
Third, multilingual individuals may adopt a
common hashtag for reasons other than code
switching, which we highlight with two examples.
The hashtag #1DWelcomeToBrazil is used in a
large number of English and Portuguese tweets.
This hashtag is associated with the travel arrival
of the English-speaking band One Direction to
Brazil. Similarly, the #100happydays hashtag was
spawned from a movement where individuals de-
scribe positive aspects of their day. These global
phenomena increases the difficulty of automati-
cally identifying code switching instances.
Fourth, spam accounts will occasionally latch
onto a hashtag and use it in a variety of languages.
For example, the popular hashtag #1000ADAY is
used to attract new followers, which resulted in
adult content services also using the hashtag to
post spam advertisements. Surprisingly, nearly
a third of tweets for this hashtag are in Russian
and feature fully-grammatical text that appears to
be randomly sampled from other sources, such as
lists of proverbs. After examining multiple ac-
counts, we speculate that these messages are actu-
ally bot accounts who need to generate sufficient
number of messages to avoid Twitter?s spam fil-
ters. Work on detecting fake accounts has largely
been done in English (Benevenuto et al., 2010;
Grier et al., 2010; Ghosh et al., 2012) and so may
benefit from detecting this cross-lingual hashtag
use in accounts.
5 Experiment 2: Bilingual Cities
The second experiment measures the prevalence
of hashtag code switching in tweets from three
cities with different populations of English and
French speakers: Montreal, Canada, Quebec City,
Canada and Paris, France. All three cities are
known to contain bilingual speaker as well, who
have been shown to actively code switch (Heller,
1992). To test for differences in the code switch-
ing behavior of populations, each city is analyzed
according to the degree to which Anglophone
and Francophone speakers incorporate hashtags
of other languages into their tweets and whether
translations of the code switched hashtags are used
in the original language.
5.1 Experimental Setup
Data Tweets were gathered for each city by us-
ing the method of Jurgens (2013) to identify Twit-
ter users with a home location within each city?s
greater metropolitan area. Tweets were then ex-
tracted for these users over a three year sample of
10% of Twitter. This process yielded 4.4M tweets
for Montreal, 203K for Quebec City, and 58.1M
for Paris. For efficiency, we restricted the Paris
dataset to 5M tweets, randomly sampled across the
time period.
Language Identification The language of a
tweet was identified using a similar process as in
Experiment 1. Because this setting restricts the
analysis to only English and French, a different
method was used to determine the language of a
hashtag. Given a tweet in language l
1
, the text of
a hashtag is tested to see if it wholly occurs within
the dictionary for l
1
; if not, a greedy tokenization
algorithm is run to attempt to split a hashtag into
constituent words that are in the dictionary of l
1
. If
either the dictionary-lookup and tokenization steps
57
French hashtags on English tweets English hashtags on French tweets
Quebec City Montreal Paris Quebec City Montreal Paris
imfc imfc comprendraquipourra lasttweet gohabsgo bbl
rilive charte sachezle bbl fail teaminsomniaque
relev seriea nian mtvhottest ind teamportugal
ceta bel hollande gohabsgo mtvhottest ps
preorderproblemonitunes brasil2014 federer not not findugame
derpatrash touspourgagner tropa fail soccer adp
villequebec 2ne1 guillaumeradio 100factsaboutme wow lasttweet
tufnations ma vousetespaspret herbyvip podcast follow
ta lavoixtva bel foodies ukraine teamom
rougeetor passionforezria retouraupensionnat electionsqc2014 int thebest
Table 5: The ten most frequent hashtags occurring in French and English tweets
 0
 2
 4
 6
 8
 10
 12
 14
      Montreal       Quebec City Paris
Perc
ent
age
 of 
twe
ets
Englist tweet with French hastagFrench tweet with English hastag
Figure 2: Percentages of tweets with any hashtag
that include a hashtag from the other language
succeed, the hashtag is said to be in l
1
. Other-
wise, the tests are repeated with the second lan-
guage l
2
. If the hashtag cannot be recognized in
l
1
or l
2
, it is assumed to be in the language of its
tweet. The aspell dictionaries were used to rec-
ognize words. Furthermore, after analyzing the er-
rors made due to missing words, dictionaries were
augmented to include common social media terms
in each language (e.g., ?selfie?). A manual anal-
ysis of 100 hashtags each for French and English
showed that this language assignment method was
correct for 91% of the instances.
5.2 Results
Francophone authors were much more likely to
use English hashtags than Anglophone authors
were for French hashtags. For tweets in each lo-
cale and language, Figure 2 shows the percentage
containing a hashtag in the other language relative
to the total number in that city using a hashtag in
either language. Notably, Paris has a higher rate of
using English hashtags than both Canadian cities.
We speculate that this difference is due to the high
rate of bilingualism in Montreal and Quebec City;
because authors are fully fluent in both languages,
should Francophone authors need to express them-
selves with an English hashtag, they may write the
entire tweet in English, rather than code switch-
ing. In contrast, Parisian authors are less likely to
be fully fluent in English (though functional) and
therefore express themselves primarily in French
with English hashtags as desired. An analogous
trend may be seen for French hashtags in the En-
glish tweets from Montreal, which has a higher
population of primarily Anglophone speakers who
might be less willing to communicate entirely in
French but will still use French hashtags to con-
nect their content with the dominant language used
in the city.
For each language and city, Table 5 shows
the ten most popular hashtags incorporated into
tweets of the other language. Examining the most
popular English tags in French tweets shows a
clear distinction in the two populations; French
Parisian tweets include more universal English
hashtags or those generated by applications, which
are not generally instances of code switching. In
contrast, the Canadian cities include more AN-
NOTATION type hashtags, including the sarcasm-
marking #not, which are more indicative of code
switching behavior.
An established linguistic convention within a
population can also motivate authors to prefer
one language?s expression over another (Myers-
Scotton, 1997). To test whether a high-frequency
concept was equally expressed in French and En-
glish or whether one language?s expression was
preferred, we created pairs of equivalent English
and French hashtags expressing the same con-
cept (e.g., #happy/#heureux) by translating the
50 most-popular English hashtags used in French
tweets. Then, the tweets for each city were an-
alyzed to identify which languages were used in
expressing each concept as a hashtag. The results
in Figure 3 reveal that for nearly half of the hash-
58
 0
 10
 20
 30
 40
 50
Montreal
Quebec City
Paris
Ha
sht
ag 
cou
nt
English onlyBoth Languages French Only
Figure 3: For 50 most-common concepts ex-
pressed in equivalent French and English transla-
tions, the frequency with which the hashtags for a
concept were seen in each language.
tags, equivalent French language versions are in
use; however, examining the relative frequencies
shows that in all cases, the English version is still
preferred, despite the presence of a large Franco-
phone population. For hashtags that were only
seen in English, many were of the COMMUNITY
type, e.g., #50factsaboutme, which may not have
an equivalent French-language version. However,
we observed that when both an English hashtag
and its French translation were attested, the use
of the English hashtag in French was most often
an instance of code switching. Hence, testing for
the presence hashtag translation pairs may serve as
a helpful heuristic for identifying hashtags whose
use signals code switching behavior.
6 Discussion
Typically, code switching is distinguished from
the related phenomena of borrowing by testing
whether the word is being fluently mixed into
the utterance instead of simply functioning as a
loan word (Poplack, 2001). Hashtags present a
unique challenge for distinguishing between the
two phenomena due their brief content and un-
structured usage: a hashtag may occur anywhere
in a tweet and its general content lacks grammat-
ical constraints. Examining the hashtags seen in
our study, we find evidence spanning both types
of uses. Common hashtags such as #win or #fail
are widely recognized outside of English and their
uses could easily be interpreted instances of bor-
rowing. However, the complexity of other hash-
tags gives the appearance that their uses go be-
yond that of borrowing, e.g., #goingbacktoschool
in ?Nadie dijo que ser??a f?acil, pero c?omo cuesta
estudiar despu?es de 4 a?nos de no tener nada
acad?emico cerquita #goingbacktoschool? where
the author is commenting on the difficulty of re-
turning for a degree. Still other posts include
multiple single-token hashtags from a second lan-
guage, e.g., the earlier example of ?Jetzt gibt?s was
vern?unftiges zum essen! #salad #turkey #lunch
#healthy #healthylifestyle #loveit.? Although indi-
vidually these hashtags may be widely recognized
and operate as interlingual markers, their com-
bined presence suggests an intentional language
shift on the part of the author that could be inter-
preted as code switching. Together, the examples
point to hashtag use by multiple languages as a
complex phenomena where shared hashtag enti-
ties exist on a graded scale from simple borrow-
ing to fully signaling code switching. Our study
is intended as a starting point for analyzing this
practice and all our data is made available to sup-
port future discussions on the roles these hashtags
play and how they facilitate communication both
within and across language communities.
7 Conclusion
The present work has provided an initial study of
code switching in Twitter focusing on instances
where an author produces a message in one lan-
guage and then includes a hashtag from a sec-
ond language. Our work provides three main con-
tributions. First, using state-of-the-art language
identification techniques, we show that hashtags
are widely shared across languages, though the
challenges of correctly classifying the language
of tweets limits our ability to quantify the exact
scale. Second, in a manual analysis of ANNOTA-
TION and COMMUNITY hashtags, we show that
authors readily code switch with these types of
hashtags, using them just as they would in single
language tweets (e.g., indicating sarcasm). Third,
in a case study of French and English tweets from
three Francophone cities with bilingual speakers,
we find that the cities with more bilingual speakers
tended to have fewer occurrences English hashtags
in French tweets, which we speculate is due to au-
thors being more likely to write such tweets en-
tirely in English, rather than code switch; however,
when English hashtags were observed in French
tweets from these more bilingual cities, they were
much more likely to be used in instances of code
switching. Data for all of the experiments is
59
available at http://www.networkdynamics.org/
datasets/.
Our work raises several avenues for future
work. First, we plan to examine how to improve
language identification in microtext in order to
gain a more accurate estimation of hashtag sharing
and code switching rates for languages. Second,
the Twitter platform enables measuring additional
factors that may influence an individual?s rate of
code switching; specifically, we plan to investigate
(1) a user?s historical tweets to estimate the degree
of bilinguality and (2) the impact of a user?s social
network with respect to homophily and language
use.
References
Peter Auer. 1998. Code-switching in conversation:
Language, interaction and identity. Routledge.
Fabr?cio Benevenuto, Gabriel Magno, Tiago Ro-
drigues, and Virg?lio Almeida. 2010. Detect-
ing spammers on twitter. In Collaboration, elec-
tronic messaging, anti-abuse and spam conference
(CEAS), volume 6, page 12.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74. Associ-
ation for Computational Linguistics.
S. Climent, J. Mor?e, A. Oliver, M. Salvatierra,
I. S`anchez, M. Taul?e, and L. Vallmanya. 2003.
Bilingual newsgroups in catalonia: A challenge for
machine translation. Journal of Computer-Mediated
Communication, 9(1).
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 107?116. Associa-
tion for Computational Linguistics.
Saptarshi Ghosh, Bimal Viswanath, Farshad Kooti,
Naveen Kumar Sharma, Gautam Korlam, Fabri-
cio Benevenuto, Niloy Ganguly, and Krishna Phani
Gummadi. 2012. Understanding and combating
link farming in the twitter social network. In Pro-
ceedings of the 21st international conference on
World Wide Web (WWW), pages 61?70. ACM.
Moises Goldszmidt, Marc Najork, and Stelios Papari-
zos. 2013. Boot-strapping language identifiers
for short colloquial postings. In Proceedings of
the European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery in
Databases (ECMLPKDD 2013). Springer Verlag,
September.
Chris Grier, Kurt Thomas, Vern Paxson, and Michael
Zhang. 2010. @ spam: the underground on 140
characters or less. In Proceedings of the 17th ACM
conference on Computer and communications secu-
rity (CCS), pages 27?37. ACM.
John Joseph Gumperz. 1982. Discourse strategies.
Cambridge University Press.
Manish Gupta, Rui Li, Zhijun Yin, and Jiawei Han.
2010. Survey on social tagging techniques. ACM
SIGKDD Explorations Newsletter, 12(1):58?72.
Monica Heller. 1992. The politics of codeswitch-
ing and language choice. Journal of Multilingual
& Multicultural Development, 13(1-2):123?142.
Jeff Huang, Katherine M Thornton, and Efthimis N
Efthimiadis. 2010. Conversational tagging in twit-
ter. In Proceedings of the 21st ACM conference on
Hypertext and hypermedia, pages 173?178. ACM.
David Jurgens. 2013. That?s what friends are for:
Inferring location in online social media platforms
based on social relationships. In Proceedings of the
7th International Conference on Weblogs and Social
Media (ICWSM). AAAI.
Carmen K. M. Lee. 2007. Linguistic features of email
and icq instant messaging in hong kong. In Brenda
Danet and Susan C. Herring, editors, The Multilin-
gual Internet: Language, Culture, and Communica-
tion Online. Oxford University Press.
Julie Letierce, Alexandre Passant, John Breslin, and
Stefan Decker. 2010. Understanding how twitter
is used to spread scientific messages. In WebSci10:
Extending the Frontiers of Society On-Line.
Yu-Ru Lin, Drew Margolin, Brian Keegan, Andrea
Baronchelli, and David Lazer. 2013. # bigbirds
never die: Understanding social dynamics of emer-
gent hashtags. In Seventh International Conference
on Weblogs and Social Media (ICWSM). AAAI.
Yabing Liu, Chloe Kliman-Silver, and Alan Mislove.
2014. The tweets they are a-changin?: Evolution
of twitter users and behavior. In Proceedings of the
8th International Conference on Weblogs and Social
Media (ICWSM). AAAI.
Marco Lui and Timothy Baldwin. 2012. langid. py:
An off-the-shelf language identification tool. In
Proceedings of the ACL 2012 System Demonstra-
tions, pages 25?30. Association for Computational
Linguistics.
Carol Myers-Scotton. 1997. Duelling Languages:
Grammatical Structure in Codeswitching. Claren-
don Press.
Chad Nilep. 2006. Code switching in sociocultural lin-
guistics. Colorado Research in Linguistics, 19(1):1?
22.
60
John C. Paolillo. 2011. Conversational codeswitch-
ing on usenet and internet relay chat. Lan-
guage@Internet, 8.
Shana Poplack and David Sankoff. 1984. Borrowing:
the synchrony of integration. Linguistics, 22(1):99?
136.
Shana Poplack, David Sankoff, and Christopher Miller.
1988. The social correlates and linguistic processes
of lexical borrowing and assimilation. Linguistics,
26(1):47?104.
Shana Poplack. 2001. Code-switching (linguistic). In
International Encyclopedia of the Social and Behav-
ioral Sciences, pages 2062?2065. Elsevier Science
Ltd., 2nd edition.
David Sankoff, Shana Poplack, and Swathi Vanniara-
jan. 1990. The case of the nonce loan in tamil. Lan-
guage variation and change, 2(01):71?101.
Bonnie Urciuoli. 1995. Language and borders. An-
nual Review of Anthropology, 24:pp. 525?546.
Lei Yang, Tao Sun, Ming Zhang, and Qiaozhu Mei.
2012. We know what@ you# tag: does the dual
role affect hashtag adoption? In Proceedings of the
21st international conference on World Wide Web
(WWW), pages 261?270. ACM.
61
