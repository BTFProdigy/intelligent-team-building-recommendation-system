Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 718?726,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Unsupervised Tokenization for Machine Translation
Tagyoung Chung and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Training a statistical machine translation
starts with tokenizing a parallel corpus.
Some languages such as Chinese do not in-
corporate spacing in their writing system,
which creates a challenge for tokenization.
Moreover, morphologically rich languages
such as Korean present an even bigger
challenge, since optimal token boundaries
for machine translation in these languages
are often unclear. Both rule-based solu-
tions and statistical solutions are currently
used. In this paper, we present unsuper-
vised methods to solve tokenization prob-
lem. Our methods incorporate informa-
tion available from parallel corpus to de-
termine a good tokenization for machine
translation.
1 Introduction
Tokenizing a parallel corpus is usually the first
step of training a statistical machine translation
system. With languages such as Chinese, which
has no spaces in its writing system, the main chal-
lenge is to segment sentences into appropriate to-
kens. With languages such as Korean and Hun-
garian, although the writing systems of both lan-
guages incorporate spaces between ?words?, the
granularity is too coarse compared with languages
such as English. A single word in these lan-
guages is composed of several morphemes, which
often correspond to separate words in English.
These languages also form compound nouns more
freely. Ideally, we want to find segmentations for
source and target languages that create a one-to-
one mapping of words. However, this is not al-
ways straightforward for two major reasons. First,
what the optimal tokenization for machine trans-
lation should be is not always clear. Zhang et al
(2008b) and Chang et al (2008) show that get-
ting the tokenization of one of the languages in
the corpus close to a gold standard does not nec-
essarily help with building better machine trans-
lation systems. Second, even statistical methods
require hand-annotated training data, which means
that in resource-poor languages, good tokenization
is hard to achieve.
In this paper, we explore unsupervised methods
for tokenization, with the goal of automatically
finding an appropriate tokenization for machine
translation. We compare methods that have ac-
cess to parallel corpora to methods that are trained
solely using data from the source language. Unsu-
pervised monolingual segmentation has been stud-
ied as a model of language acquisition (Goldwater
et al, 2006), and as model of learning morphol-
ogy in European languages (Goldsmith, 2001).
Unsupervised segmentation using bilingual data
has been attempted for finding new translation
pairs (Kikui and Yamamoto, 2002), and for finding
good segmentation for Chinese in machine trans-
lation using Gibbs sampling (Xu et al, 2008). In
this paper, further investigate the use of bilingual
information to find tokenizations tailored for ma-
chine translation. We find a benefit not only for
segmentation of languages with no space in the
writing system (such as Chinese), but also for the
smaller-scale tokenization problem of normaliz-
ing between languages that include more or less
information in a ?word? as defined by the writ-
ing system, using Korean-English for our exper-
iments. Here too, we find a benefit from using
bilingual information, with unsupervised segmen-
tation rivaling and in some cases surpassing su-
pervised segmentation. On the modeling side,
we use dynamic programming-based variational
Bayes, making Gibbs sampling unnecessary. We
also develop and compare various factors in the
model to control the length of the tokens learned,
and find a benefit from adjusting these parame-
ters directly to optimize the end-to-end translation
quality.
718
2 Tokenization
Tokenization is breaking down text into lexemes
? a unit of morphological analysis. For relatively
isolating languages such as English and Chinese, a
word generally equals a single token, which is usu-
ally a clearly identifiable unit. English, especially,
incorporates spaces between words in its writing
system, which makes tokenization in English usu-
ally trivial. The Chinese writing system does not
have spaces between words, but there is less am-
biguity where word boundaries lie in a given sen-
tence compared to more agglutinative languages.
In languages such as Hungarian, Japanese, and
Korean, what constitutes an optimal token bound-
ary is more ambiguous. While two tokens are usu-
ally considered two separate words in English, this
may be not be the case in agglutinative languages.
Although what is considered a single morpholog-
ical unit is different from language to language,
if someone were given a task to align words be-
tween two languages, it is desirable to have one-
to-one token mapping between two languages in
order to have the optimal problem space. For ma-
chine translation, one token should not necessarily
correspond to one morphological unit, but rather
should reflect the morphological units and writing
system of the other language involved in transla-
tion.
For example, consider a Korean ?word? meok-
eoss-da, which means ate. It is written as a sin-
gle word in Korean but consists of three mor-
phemes eat-past-indicative. If one uses morpho-
logical analysis as the basis for Korean tokeniza-
tion, meok-eoss-da would be split into three to-
kens, which is not desirable if we are translat-
ing Korean to English, since English does not
have these morphological counterparts. However,
a Hungarian word szekr?enyemben, which means in
my closet, consists of three morphemes closet-my-
inessive that are distinct words in English. In this
case, we do want our tokenizer to split this ?word?
into three morphemes szekr?eny em ben.
In this paper, we use segmentation and to-
kenization interchangeably as blanket terms to
cover the two different problems we have pre-
sented here. The problem of segmenting Chinese
sentences into words and the problem of segment-
ing Korean or Hungarian ?words? into tokens of
right granularity are different in their nature. How-
ever, our models presented in section 3 handle the
both problems.
3 Models
We present two different methods for unsuper-
vised tokenization. Both are essentially unigram
tokenization models. In the first method, we try
learning tokenization from word alignments with
a model that bears resemblance to Hidden Markov
models. We use IBMModel 1 (Brown et al, 1993)
for the word alignment model. The second model
is a relatively simpler monolingual tokenization
model based on counts of substrings which serves
as a baseline of unsupervised tokenization.
3.1 Learning tokenization from alignment
We use expectation maximization as our primary
tools in learning tokenization form parallel text.
Here, the observed data provided to the algorithm
are the tokenized English string e
n
1
and the unto-
kenized string of foreign characters c
m
1
. The un-
observed variables are both the word-level align-
ments between the two strings, and the tokeniza-
tion of the foreign string. We represent the tok-
enization with a string s
m
1
of binary variables, with
s
i
= 1 indicating that the ith character is the final
character in a word. The string of foreign words
f
?
1
can be thought of as the result of applying the
tokenization s to the character string c:
f = s ? c where ? =
m
?
i=1
s
i
We use IBM Model 1 as our word-level align-
ment model, following its assumptions that each
foreign word is generated independently from one
English word:
P (f |e) =
?
a
P (f ,a | e)
=
?
a
?
i
P (f
i
| e
a
i
)P (a)
=
?
i
?
j
P (f
i
| e
j
)P (a
i
= j)
and that all word-level alignments a are equally
likely: P (a) =
1
n
for all positions. While Model 1
has a simple EM update rule to compute posteri-
ors for the alignment variables a and from them
learn the lexical translation parameters P (f | e),
we cannot apply it directly here because f itself is
unknown, and ranges over an exponential number
of possibilities depending on the hidden segmenta-
tion s. This can be addressed by applying dynamic
programing over the sequence s. We compute the
719
posterior probability of a word beginning at posi-
tion i, ending at position j, and being generated by
English word k:
P (s
i...j
= (1, 0, . . . , 0, 1), a = k | e)
=
?(i)P (f | e
k
)P (a = k)?(j)
P (c | e)
where f = c
i
. . . c
j
is the word formed by con-
catenating characters i through j, and a is a vari-
able indicating which English position generated
f . Here ? and ? are defined as:
?(i) = P (c
i
1
, s
i
= 1 | e)
?(j) = P (c
m
j+1
, s
j
= 1 | e)
These quantities resemble forward and backward
probabilities of hidden Markov models, and can
be computed with similar dynamic programming
recursions:
?(i) =
L
?
?=1
?(i? ?)
?
a
P (a)P (c
i
i??
| e
a
)
?(j) =
L
?
?=1
?
a
P (a)P (c
j+?
j
| e
a
)?(j + ?)
where L is the maximum character length for a
word.
Then, we can calculate the expected counts of
individual word pairs being aligned (c
j
i
, e
k
) by ac-
cumulating these posteriors over the data:
ec(c
j
i
, e
k
) +=
?(i)P (a)P (c
j
i
| e
k
)?(j)
?(m)
The M step simply normalizes the counts:
?
P (f | e) =
ec(f, e)
?
e
ec(f, e)
Our model can be compared to a hiddenMarkov
model in the following way: a target word gen-
erates a source token which spans a zeroth order
Markov chain of characters in source sentence,
where a ?transition? represents a segmentation and
a ?emission? represents an alignment. The model
uses HMM-like dynamic programming to do in-
ference. For the convenience, we refer to this
model as the bilingual model in the rest of the
paper. Figure 1 illustrates our first model with
an small example. Under this model we are not
learning segmentation directly, but rather we are
learning alignments between two sentences. The
c
1
c
2
c
3
c
4
f
1
f
2
e
1
e
2
Figure 1: The figure shows a source sentence
f = f
1
, f
2
= s ? c
1
. . . c
4
where s = (0, 0, 1, 1)
and a target sentence e = e
1
, e
2
. There is a seg-
mentation between c
3
and c
4
; thus c
1
, c
2
, c
3
form
f
1
and c
3
forms f
2
. f
1
is generated by e
2
and f
2
is
generated by e
1
.
segmentation is by-product of learning the align-
ment. We can find the optimal segmentation of
a new source language sentence using the Viterbi
algorithm. Given two sentences e and f ,
a
?
= argmax
a
P (f ,a | e)
and segmentation s
?
implied by alignment a
?
is
the optimal segmentation of f found by this model.
3.2 Learning tokenization from substring
counts
The second tokenization model we propose is
much simpler. More sophisticated unsupervised
monolingual tokenization models using hierarchi-
cal Bayesian models (Goldwater et al, 2006)
and using the minimum description length prin-
ciple (Goldsmith, 2001; de Marcken, 1996) have
been studied. Our model is meant to serve as
a computationally efficient baseline for unsuper-
vised monolingual tokenization. Given a corpus
of only source language of unknown tokenization,
we want to find the optimal s given c ? s that
gives us the highest P (s | c). According to Bayes?
rule,
P (s | c) ? P (c | s)P (s)
Again, we assume that all P (s) are equally likely.
Let f = s?c = f
1
. . . f
?
, where f
i
is a word under
some possible segmentation s. We want to find the
s that maximizes P (f). We assume that
P (f) = P (f
1
)? . . .? P (f
?
)
To calculate P (f
i
), we count every possible
720
substring ? every possible segmentation of char-
acters ? from the sentences. We assume that
P (f
i
) =
count(f
i
)
?
k
count(f
k
)
We can compute these counts by making a sin-
gle pass through the corpus. As in the bilingual
model, we limit the maximum size of f for prac-
tical reasons and to prevent our model from learn-
ing unnecessarily long f . With P (f), given a se-
quence of characters c, we can calculate the most
likely segmentation using the Viterbi algorithm.
s
?
= argmax
s
P (f)
Our rationale for this model is that if a span of
characters f = c
i
. . . c
j
is an independent token, it
will occur often enough in different contexts that
such a span of characters will have higher prob-
ability than other spans of characters that are not
meaningful. For the rest of the paper, this model
will be referred to as the monolingual model.
3.3 Tokenizing new data
Since the monolingual tokenization only uses in-
formation from a monolingual corpus, tokenizing
new data is not a problem. However, with the
bilingual model, we are learning P (f | e). We are
relying on information available from e to get the
best tokenization for f. However, the parallel sen-
tences will not be available for new data we want
to translate. Therefore, for the new data, we have
to rely only on P (f) to tokenize any new data,
which can be obtained by calculating
P (f) =
?
e
P (f | e)P (e)
With P (f) from the bilingual model, we can run
the Viterbi algorithm in the same manner as mono-
lingual tokenization model for monolingual data.
We hypothesize that we can learn valuable infor-
mation on which token boundaries are preferable
in language f when creating a statistical machine
translation system that translates from language f
to language e.
4 Preventing overfitting
We introduce two more refinements to our word-
alignment induced tokenization model and mono-
lingual tokenization model. Since we are consid-
ering every possible token f that can be guessed
from our corpus, the data is very sparse. For the
bilingual model, we are also using the EM algo-
rithm to learn P (f | e), which means there is a
danger of the EM algorithm memorizing the train-
ing data and thereby overfitting. We put a Dirichlet
prior on our multinomial parameter for P (f | e)
to control this situation. For both models, we also
want a way to control the distribution of token
length after tokenization. We address this problem
by adding a length factor to our models.
4.1 Variational Bayes
Beal (2003) and Johnson (2007) describe vari-
ational Bayes for hidden Markov model in de-
tail, which can be directly applied to our bilingual
model. With this Bayesian extension, the emission
probability of our first model can be summarized
as follows:
?
e
| ? ? Dir(?),
f
i
| e
i
= e ? Multi(?
e
).
Johnson (2007) and Zhang et al (2008a) show
having small ? helps to control overfitting. Fol-
lowing this, we set our Dirichlet prior to be as
sparse as possible. It is set at ? = 10
?6
, the num-
ber we used as floor of our probability.
For the model incorporating the length factor,
which is described in the next section, we do not
place a prior on our transition probability, since
there are only two possible states, i.e. P (s = 1)
and P (s = 0). This distribution is not as sparse as
the emission probability.
Comparing variational Bayes to the traditional
EM algorithm, the E step stays the same but the
M step for calculating the emission probability
changes as follows:
?
P (f | e) =
exp(?(ec(f, e) + ?))
exp(?(
?
e
ec(f, e) + s?))
where ? is the digamma function, and s is the size
of the vocabulary from which f is drawn. Since
we do not accurately know s, we set s to be the
number of all possible tokens. As can be seen from
the equation, by setting ? to a small value, we are
discounting the expected count with help of the
digamma function. Thus, having lower ? leads to
a sparser solution.
4.2 Token length
We now add a parameter that can adjust the to-
kenizer?s preference for longer or shorter tokens.
721
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 1  2  3  4  5  6
ref
P(s)=0.55
lambda=3.16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 1  2  3  4  5  6
ref
P(s)=0.58
lambda=2.13
Figure 2: Distribution of token length for (from left to right) Chinese, and Korean. ?ref? is the empirical
distribution from supervised tokenization. Two length factors ? ?
1
and ?
2
are also shown. For ?
1
, the
parameter to geometric distribution P (s) is set to the value learned from our bilingual model. For ?
2
, ?
is set using the criterion described in the experiment section.
This parameter is beneficial because we want our
distribution of token length after tokenization to
resemble the real distribution of token length. This
parameter is also useful because we also want to
incorporate information on the number of tokens
in the other language in the parallel corpus. This is
based on the assumption that, if tokenization cre-
ates a one-to-one mapping, the number of tokens
in both languages should be roughly the same. We
can force the two languages to have about the same
number of tokens by adjusting this parameter. The
third reason is to further control overfitting. Our
observation is that certain morphemes are very
common, such that they will be always observed
attached to other morphemes. For example, in Ko-
rean, a noun attached with nominative case marker
is very common. Our model is likely to learn a
noun attached with the morpheme ? nominative
case marker ? rather than noun itself. This is not
desirable when the noun occurs with less common
morphemes; in these cases the morpheme will be
split off creating inconsistencies.
We have experimented with two different length
factors, each with one adjustable parameter:
?
1
(?) = P (s)(1? P (s))
??1
?
2
(?) = 2
??
?
The first, ?
1
, is the geometric distribution, where
l is length of a token and P (s) is probability of
segmentation between two characters. The second
length factor ?
2
was acquired through several ex-
periments and was found to work well. As can
been seen from Figure 2, the second factor dis-
counts longer tokens more heavily than the geo-
metric distribution. We can adjust the value of ?
and P (s) to increase or decrease number of tokens
after segmentation.
For our monolingual model, incorporating these
factors is straightforward. We assume that
P (f) ? P (f
1
)?(?
1
)? . . .? P (f
n
)?(?
n
)
where ?
i
is the length of f
i
. Then, we use the same
Viterbi algorithm to select the f
1
. . . f
n
that max-
imizes P (f), thereby selecting the optimal s ac-
cording to our monolingual model with a length
factor. We pick the value of ? and P (s) that
produces about the same number of tokens in the
source side as in the target side, thereby incorpo-
rating some information about the target language.
For our bilingual model, we modify our model
slightly to incorporate ?
1
, creating a hybrid
model. Now, our forward probability of forward-
backward algorithm is:
?(i) =
L
?
?=1
?(i? l)?
1
(?)
?
a
P (a)P (c
i
i??
| e
a
)
and the expected count of (c
j
i
, e
k
) is
ec(c
j
i
, e
k
) +=
?(i)P (a)P (c
j
i
| e
k
)?(j)?
1
(j ? i)
?(m)
For ?
1
, we can learn P (s) for the geometric dis-
tribution from the model itself:
1
P (s) =
1
m
m
?
i
?(i)?(i)
?(m)
1
The equation is for one sentence, but in practice, we sum
over all sentences in the training data to calculate P (s).
722
We can also fix P (s) instead of learning it through
EM. We incorporate ?
2
into the bilingual model
as follows: after learning P (f) from the bilingual
model, we pick the ? in the same manner as the
monolingual model and run the Viterbi algorithm.
After applying the length factor, what we have
is a log-linear model for tokenization, with two
feature functions with equal weights: the length
factor and P (f) learned from model.
5 Experiments
5.1 Data
We tested our tokenization methods on two differ-
ent language pairs: Chinese-English, and Korean-
English. For Chinese-English, we used FBIS
newswire data. The Korean-English parallel data
was collected from news websites and sentence-
aligned using two different tools described by
Moore (2002) and Melamed (1999). We used sub-
sets of each parallel corpus consisting of about 2M
words and 60K sentences on the English side. For
our development set and test set, Chinese-English
had about 1000 sentences each with 10 reference
translations taken from the NIST 2002 MT eval-
uation. For Korean-English, 2200 sentence pairs
were randomly sampled from the parallel corpus,
and held out from the training data. These were
divided in half and used for test set and develop-
ment set respectively. For all language pairs, very
minimal tokenization ? splitting off punctuation
? was done on the English side.
5.2 Experimental setup
We used Moses (Koehn et al, 2007) to train
machine translation systems. Default parameters
were used for all experiments except for the num-
ber of iterations for GIZA++ (Och and Ney, 2003).
GIZA++ was run until the perplexity on develop-
ment set stopped decreasing. For practical rea-
sons, the maximum size of a token was set at three
for Chinese, and four for Korean.
2
Minimum error
rate training (Och, 2003) was run on each system
afterwards and BLEU score (Papineni et al, 2002)
was calculated on the test sets.
For the monolingual model, we tested two ver-
sions with the length factor ?
1
, and ?
2
. We picked
? and P (s) so that the number of tokens on source
side (Chinese, and Korean) will be about the same
2
In the Korean writing system, one character is actually
one syllable block. We do not decompose syllable blocks
into individual consonants and vowels.
as the number of tokens in the target side (En-
glish).
For the bilingual model, as explained in the
model section, we are learning P (f | e), but only
P (f) is available for tokenizing any new data. We
compared two conditions: using only the source
data to tokenize the source language training data
according to P (f) (which is consistent with the
conditions at test time), and using both the source
and English data to tokenize the source language
training data (which might produce better tok-
enization by using more information). For the first
length factor ?
1
, we ran an experiment where the
model learns P (s) as described in the model sec-
tion, and we also had experiments where P (s)was
pre-set at 0.9, 0.7, 0.5, and 0.3 for comparison. We
also ran an experiment with the second length fac-
tor ?
2
where ? was picked as the same manner as
the monolingual model.
We varied tokenization of development set and
test set to match the training data for each ex-
periment. However, as we have implied in the
previous paragraph, in the one experiment where
P (f | e) was used to segment training data, di-
rectly incorporating information from target cor-
pus, tokenization for test and development set is
not exactly consistent with tokenization of train-
ing corpus. Since we assume only source corpus
is available at the test time, the test and the devel-
opment set was tokenized only using information
from P (f).
We also trained MT systems using supervised
tokenizations and tokenization requiring a mini-
mal effort for the each language pair. For Chinese-
English, the minimal effort tokenization is maxi-
mal tokenization where every Chinese character is
segmented. Since a number of Chinese tokeniz-
ers are available, we have tried four different to-
kenizations for the supervised tokenizations. The
first one is the LDC Chinese tokenizer available at
the LDC website
3
, which is compiled by Zhibiao
Wu. The second tokenizer is a maxent-based to-
kenizer described by Xue (2003). The third and
fourth tokenizations come from the CRF-based
Stanford Chinese segmenter described by Chang
et al (2008). The difference between third and
fourth tokenization comes from the different gold
standard, the third one is based on Beijing Uni-
versity?s segmentation (pku) and the fourth one is
based on Chinese Treebank (ctb). For Korean-
3
http://projects.ldc.upenn.edu/Chinese/LDC ch.htm
723
Chinese Korean
BLEU F-score BLEU
Supervised
Rule-based morphological analyzer 7.27
LDC segmenter 20.03 0.94
Xue?s segmenter 23.02 0.96
Stanford segmenter (pku) 21.69 0.96
Stanford segmenter (ctb) 22.45 1.00
Unsupervised
Splitting punctuation only 6.04
Maximal (Character-based MT) 20.32 0.75
Bilingual P (f | e) with ?
1
P (s) = learned 19.25 6.93
Bilingual P (f) with ?
1
P (s) = learned 20.04 0.80 7.06
Bilingual P (f) with ?
1
P (s) = 0.9 20.75 0.87 7.46
Bilingual P (f) with ?
1
P (s) = 0.7 20.59 0.81 7.31
Bilingual P (f) with ?
1
P (s) = 0.5 19.68 0.80 7.18
Bilingual P (f) with ?
1
P (s) = 0.3 20.02 0.79 7.38
Bilingual P (f) with ?
2
22.31 0.88 7.35
Monolingual P (f) with ?
1
20.93 0.83 6.76
Monolingual P (f) with ?
2
20.72 0.85 7.02
Table 1: BLEU score results for Chinese-English and Korean-English experiments and F-score of seg-
mentation compared against Chinese Treebank standard. The highest unsupervised score is highlighted.
English, the minimal effort tokenization splitting
off punctuation and otherwise respecting the spac-
ing in the Korean writing system. A Korean mor-
phological analysis tool
4
was used to create the su-
pervised tokenization.
For Chinese-English, since a gold standard for
Chinese segmentation is available, we ran an addi-
tional evaluation of tokenization from each meth-
ods we have tested. We tokenized the raw text
of Chinese Treebank (Xia et al, 2000) using all
of the methods (supervised/unsupervised) we have
described in this section except for the bilingual
tokenization using P (f | e) because the English
translation of the Chinese Treebank data was not
available. We compared the result against the gold
standard segmentation and calculated the F-score.
6 Results
Results from Chinese-English and Korean-English
experiments are presented in Table 1. Note that
nature of data and number of references are dif-
ferent for the two language pairs, and therefore
the BLEU scores are not comparable. For both
language pairs, our models perform equally well
as supervised baselines, or even better. We can
4
http://nlp.kookmin.ac.kr/HAM/eng/main-e.html
observe three things from the result. First, tok-
enization of training data using P (f | e) tested on
a test set tokenized with P (f) performed worse
than any other experiments. This affirms our be-
lief that consistency in tokenization is important
for machine translation, which was alsomentioned
by Chang et al (2008). Secondly, we are learning
valuable information by looking at the target lan-
guage. Compare the result of the bilingual model
with ?
2
as the length factor to the result of the
monolingual model with the same length factor.
The bilingual version consistently performed bet-
ter than the monolingual model in all language
pairs. This tells us we can learn better token
boundaries by using information from the target
language. Thirdly, our hypothesis on the need
for heavy discount for longer tokens is confirmed.
The value for P (s) learned by the model was 0.55,
and 0.58 for Chinese, and Korean respectively. For
both language pairs, this accurately reflects the
empirical distribution of token length, as can be
seen in Figure 2. However, experiments where
P (s) was directly optimized performed better, in-
dicating that this parameter should be optimized
within the context of a complete system. The sec-
ond length factor ?
2
, which discounts longer to-
kens even more heavily, generally performed bet-
724
English the two presidents will hold a joint press conference at the end of their summit talks .
Untokenized Korean ??????????????????????????????? .
Supervised ???? ??? ??? ???????? ?? ????? ????? ? ?? .
Bilingual P (f | e) with ?
1
??????? ?????????? ??????? ?????? ? .
Bilingual P (f) with ?
2
???? ??? ?????????? ??????? ????? ?? .
Monolingual P (f) with ?
1
??? ? ??? ?????????? ????????????? ? .
Monolingual P (f) with ?
2
???? ??? ?????????? ???????????? ?? .
Figure 3: Sample tokenization results for Korean-English data. The underscores are added to clearly
visualize where the breaks are.
ter than the first length factor when used in con-
junction with the bilingual model. Lastly, F-scores
of Chinese segmentations compared against the
gold standard shows higher segmentation accuracy
does not necessarily lead to higher BLEU score.
F-scores presented in Table 1 are not directly com-
parable for all different experiments because the
test data (Chinese Treebank) is used in training for
some of the supervised segmenters, but these num-
bers do show how close unsupervised segmenta-
tions are to the gold standard. It is interesting to
note that our highest unsupervised segmentation
result does make use of bilingual information.
Sample tokenization results for Korean-English
experiments are presented in Figure 3. We observe
that different configurations produce different tok-
enizations, and the bilingual model produced gen-
erally better tokenizations for translation com-
pared to the monolingual models or the super-
vised tokenizer. In this example, the tokenization
obtained from the supervised tokenizer, although
morphologically correct, is too fine-grained for the
purpose of translation to English. For example,
it correctly tokenized the attributive suffix ? -n
however, this is not desirable since English has no
such counterpart. Both variations of the monolin-
gual tokenization have errors such as incorrectly
not segmenting ??? gyeol-gwa-reul, which is
a compound of a noun and a case marker, into?
? ? gyeol-gwa reul as the bilingual model was
able to do.
6.1 Conclusion and future work
We have shown that unsupervised tokenization for
machine translation is feasible and can outperform
rule-based methods that rely on lexical analysis,
or supervised statistical segmentations. The ap-
proach can be applied both to morphological anal-
ysis of Korean and the segmentation of sentences
into words for Chinese, which may at first glace
appear to be quite different problems. We have
only shown how our methods can be applied to
one language of the pair, where one language is
generally isolating and the other is generally syn-
thetic. However, our methods could be extended
to tokenization for both languages by iterating be-
tween languages. We also used the most simple
word-alignment model, but more complex word
alignment models could be incorporated into our
bilingual model.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
References
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher Man-
ning. 2008. Optimizing Chinese word segmentation
for machine translation performance. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 224?232.
Carl de Marcken. 1996. Linguistic structure as compo-
sition and perturbation. In Meeting of the Associa-
tion for Computational Linguistics, pages 335?341.
Morgan Kaufmann Publishers.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the International Conference on Computational Lin-
guistics/Association for Computational Linguistics
(COLING/ACL-06), pages 673?680.
725
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In 2007 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 296?305, Prague, Czech Republic,
June. Association for Computational Linguistics.
Genichiro Kikui and Hirofumi Yamamoto. 2002.
Finding translation pairs from english-japanese un-
tokenized aligned corpora. In Proceedings of the
40th Annual Conference of the Association for
Computational Linguistics (ACL-02) workshop on
Speech-to-speech translation: algorithms and sys-
tems, pages 23?30. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-07), Demonstration Session, pages 177?
180.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25:107?130.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In AMTA ?02: Pro-
ceedings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, pages
135?144, London, UK. Springer-Verlag.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of the 41th Annual Conference of the Association for
Computational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Conference of the Association for
Computational Linguistics (ACL-02).
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Shizhe Huang, Tony
Kroch, and Mitch Marcus. 2000. Developing
Guidelines and Ensuring Consistency for Chinese
Text Annotation. In Proc. of the 2nd International
Conference on Language Resources and Evaluation
(LREC-2000), Athens, Greece.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 1017?1024, Manchester, UK, August.
Coling 2008 Organizing Committee.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. In International Journal of Com-
putational Linguistics and Chinese Language Pro-
cessing, volume 8, pages 29?48.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008a. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08),
pages 97?105, Columbus, Ohio.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008b. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 216?223.
726
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 636?645,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Effects of Empty Categories on Machine Translation
Tagyoung Chung and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We examine effects that empty categories have
on machine translation. Empty categories are
elements in parse trees that lack corresponding
overt surface forms (words) such as dropped
pronouns and markers for control construc-
tions. We start by training machine trans-
lation systems with manually inserted empty
elements. We find that inclusion of some
empty categories in training data improves the
translation result. We expand the experiment
by automatically inserting these elements into
a larger data set using various methods and
training on the modified corpus. We show that
even when automatic prediction of null ele-
ments is not highly accurate, it nevertheless
improves the end translation result.
1 Introduction
An empty category is an element in a parse tree
that does not have a corresponding surface word.
They include traces such as Wh-traces which indi-
cate movement operations in interrogative sentences
and dropped pronouns which indicate omission of
pronouns in places where pronouns are normally
expected. Many treebanks include empty nodes in
parse trees to represent non-local dependencies or
dropped elements. Examples of the former include
traces such as relative clause markers in the Penn
Treebank (Bies et al, 1995). An example of the lat-
ter include dropped pronouns in the Korean Tree-
bank (Han and Ryu, 2005) and the Chinese Tree-
bank (Xue and Xia, 2000).
In languages such as Chinese, Japanese, and Ko-
rean, pronouns are frequently or regularly dropped
when they are pragmatically inferable. These lan-
guages are called pro-drop languages. Dropped pro-
nouns are quite a common phenomenon in these lan-
guages. In the Chinese Treebank, they occur once
in every four sentences on average. In Korean the
Treebank, they are even more frequent, occurring
in almost every sentence on average. Translating
these pro-drop languages into languages such as En-
glish where pronouns are regularly retained could
be problematic because English pronouns have to be
generated from nothing.
There are several different strategies to counter
this problem. A special NULL word is typically
used when learning word alignment (Brown et al,
1993). Words that have non-existent counterparts
can be aligned to the NULL word. In phrase-based
translation, the phrase learning system may be able
to learn pronouns as a part of larger phrases. If the
learned phrases include pronouns on the target side
that are dropped from source side, the system may
be able to insert pronouns even when they are miss-
ing from the source language. This is an often ob-
served phenomenon in phrase-based translation sys-
tems. Explicit insertion of missing words can also
be included in syntax-based translation models (Ya-
mada and Knight, 2001). For the closely related
problem of inserting grammatical function particles
in English-to-Korean and English-to-Japanese ma-
chine translation, Hong et al (2009) and Isozaki et
al. (2010) employ preprocessing techniques to add
special symbols to the English source text.
In this paper, we examine a strategy of automat-
ically inserting two types of empty elements from
the Korean and Chinese treebanks as a preprocess-
636
Korean
*T* 0.47 trace of movement
(NP *pro*) 0.88 dropped subject or object
(WHNP *op*) 0.40 empty operator in relative
constructions
*?* 0.006 verb deletion, VP ellipsis,
and others
Chinese
(XP (-NONE- *T*)) 0.54 trace of A?-movement
(NP (-NONE- *)) 0.003 trace of A-movement
(NP (-NONE- *pro*)) 0.27 dropped subject or object
(NP (-NONE- *PRO*)) 0.31 control structures
(WHNP (-NONE- *OP*)) 0.53 empty operator in relative
constructions
(XP (-NONE- *RNR*)) 0.026 right node raising
(XP (-NONE- *?*)) 0 others
Table 1: List of empty categories in the Korean Treebank
(top) and the Chinese Treebank (bottom) and their per-
sentence frequencies in the training data of initial experi-
ments.
ing step. We first describe our experiments with data
that have been annotated with empty categories, fo-
cusing on zero pronouns and traces such as those
used in control constructions. We use these an-
notations to insert empty elements in a corpus and
train a machine translation system to see if they im-
prove translation results. Then, we illustrate differ-
ent methods we have devised to automatically insert
empty elements to corpus. Finally, we describe our
experiments with training machine translation sys-
tems with corpora that are automatically augmented
with empty elements. We conclude this paper by
discussing possible improvements to the different
methods we describe in this paper.
2 Initial experiments
2.1 Setup
We start by testing the plausibility of our idea
of preprocessing corpus to insert empty cate-
gories with ideal datasets. The Chinese Treebank
(LDC2005T01U01) is annotated with null elements
and a portion of the Chinese Treebank has been
translated into English (LDC2007T02). The Korean
Treebank version 1.0 (LDC2002T26) is also anno-
tated with null elements and includes an English
translation. We extract null elements along with
tree terminals (words) and train a simple phrase-
BLEU
Chi-Eng No null elements 19.31
w/ *pro* 19.68
w/ *PRO* 19.54
w/ *pro* and *PRO* 20.20
w/ all null elements 20.48
Kor-Eng No null elements 20.10
w/ *pro* 20.37
w/ all null elements 19.71
Table 2: BLEU score result of initial experiments.
Each experiment has different empty categories added in.
*PRO* stands for the empty category used to mark con-
trol structures and *pro* indicates dropped pronouns for
both Chinese and Korean.
based machine translation system. Both datasets
have about 5K sentences and 80% of the data was
used for training, 10% for development, and 10%
for testing.
We used Moses (Koehn et al, 2007) to train
machine translation systems. Default parameters
were used for all experiments. The same number
of GIZA++ (Och and Ney, 2003) iterations were
used for all experiments. Minimum error rate train-
ing (Och, 2003) was run on each system afterwards,
and the BLEU score (Papineni et al, 2002) was cal-
culated on the test sets.
There are several different empty categories in
the different treebanks. We have experimented with
leaving in and out different empty categories for dif-
ferent experiments to see their effect. We hypoth-
esized that nominal phrasal empty categories such
as dropped pronouns may be more useful than other
ones, since they are the ones that may be missing in
the source language (Chinese and Korean) but have
counterparts in the target (English). Table 1 summa-
rizes empty categories in Chinese and Korean tree-
bank and their frequencies in the training data.
2.2 Results
Table 2 summarizes our findings. It is clear that
not all elements improve translation results when in-
cluded in the training data. For the Chinese to En-
glish experiment, empty categories that mark con-
trol structures (*PRO*), which serve as the sub-
ject of a dependent clause, and dropped pronouns
(*pro*), which mark omission of pragmatically in-
637
word P (e | ?pro?) word P (e | ?PRO?)
the 0.18 to 0.45
i 0.13 NULL 0.10
it 0.08 the 0.02
to 0.08 of 0.02
they 0.05 as 0.02
Table 3: A lexical translation table from the Korean-
English translation system (left) and a lexical transla-
tion from the Chinese-English translation system (right).
For the Korean-English lexical translation table, the left
column is English words that are aligned to a dropped
pronoun (*pro*) and the right column is the conditional
probability of P (e | ?pro?). For the Chinese-English
lexical translation table, the left column is English words
that are aligned to a control construction marker (*PRO*)
and the right column is the conditional probability of
P (e | ?PRO?).
ferable pronouns, helped to improve translation re-
sults the most. For the Korean to English experi-
ment, the dropped pronoun is the only empty cate-
gory that seems to improve translation.
For the Korean to English experiment, we also
tried annotating whether the dropped pronouns are a
subject, an object, or a complement using informa-
tion from the Treebank?s function tags, since English
pronouns are inflected according to case. However,
this did not yield a very different result and in fact
was slightly worse. This is possibly due to data spar-
sity created when dropped pronouns are annotated.
Dropped pronouns in subject position were the over-
whelming majority (91%), and there were too few
dropped pronouns in object position to learn good
parameters.
2.3 Analysis
Table 3 and Table 4 give us a glimpse of why having
these empty categories may lead to better transla-
tion. Table 3 is the lexical translation table for the
dropped pronoun (*pro*) from the Korean to En-
glish experiment and the marker for control con-
structions (*PRO*) from the Chinese to English ex-
periment. For the dropped pronoun in the Korean
to English experiment, although there are errors,
the table largely reflects expected translations of a
dropped pronoun. It is possible that the system is in-
serting pronouns in right places that would be miss-
ing otherwise. For the control construction marker
in the Chinese to English experiment, the top trans-
lation for *PRO* is the English word to, which is ex-
pected since Chinese clauses that have control con-
struction markers often translate to English as to-
infinitives. However, as we discuss in the next para-
graph, the presence of control construction markers
may affect translation results in more subtle ways
when combined with phrase learning.
Table 4 shows how translations from the system
trained with null elements and the system trained
without null elements differ. The results are taken
from the test set and show extracts from larger sen-
tences. Chinese verbs that follow the empty node for
control constructions (*PRO*) are generally trans-
lated to English as a verb in to-infinitive form, a
gerund, or a nominalized verb. The translation re-
sults show that the system trained with this null el-
ement (*PRO*) translates verbs that follow the null
element largely in such a manner. However, it may
not be always closest to the reference. It is exempli-
fied by the translation of one phrase.
Experiments in this section showed that prepro-
cessing the corpus to include some empty elements
can improve translation results. We also identified
which empty categories maybe helpful for improv-
ing translation for different language pairs. In the
next section, we focus on how we add these ele-
ments automatically to a corpus that is not annotated
with empty elements for the purpose of preprocess-
ing corpus for machine translation.
3 Recovering empty nodes
There are a few previous works that have attempted
restore empty nodes for parse trees using the Penn
English Treebank. Johnson (2002) uses rather sim-
ple pattern matching to restore empty categories as
well as their co-indexed antecedents with surpris-
ingly good accuracy. Gabbard et al (2006) present
a more sophisticated algorithm that tries to recover
empty categories in several steps. In each step, one
or more empty categories are restored using pat-
terns or classifiers (five maximum-entropy and two
perceptron-based classifiers to be exact).
What we are trying to achieve has obvious simi-
larity to these previous works. However, there are
several differences. First, we deal with different
languages. Second, we are only trying to recover
638
Chinese English Reference System trained w/ nulls System trained w/o nulls
*PRO*?? implementing implementation implemented
*PRO*???? have gradually formed to gradually form gradually formed
*PRO*?????? attracting foreign investment attracting foreign investment attract foreign capital
Table 4: The first column is a Chinese word or a phrase that immediately follows empty node marker for Chinese
control constructions. The second column is the English reference translation. The third column is the translation
output from the system that is trained with the empty categories added in. The fourth column is the translation output
from the system trained without the empty categories added, which was given the test set without the empty categories.
Words or phrases and their translations presented in the table are part of larger sentences.
a couple of empty categories that would help ma-
chine translation. Third, we are not interested in re-
covering antecedents. The linguistic differences and
the empty categories we are interested in recovering
made the task much harder than it is for English. We
will discuss this in more detail later.
From this section on, we will discuss only
Chinese-English translation because Chinese
presents a much more interesting case, since we
need to recover two different empty categories that
are very similarly distributed. Data availability
was also a consideration since much larger datasets
(bilingual and monolingual) are available for
Chinese. The Korean Treebank has only about 5K
sentences, whereas the version of Chinese Treebank
we used includes 28K sentences.
The Chinese Treebank was used for all experi-
ments that are mentioned in the rest of this Section.
Roughly 90% of the data was used for the training
set, and the rest was used for the test set. As we have
discussed in Section 2, we are interested in recover-
ing dropped pronouns (*pro*) and control construc-
tion markers (*PRO*). We have tried three different
relatively simple methods so that recovering empty
elements would not require any special infrastruc-
ture.
3.1 Pattern matching
Johnson (2002) defines a pattern for empty node re-
covery to be a minimally connected tree fragment
containing an empty node and all nodes co-indexed
with it. Figure 1 shows an example of a pattern. We
extracted patterns according this definition, and it
became immediately clear that the same definition
that worked for English will not work for Chinese.
Table 5 shows the top five patterns that match con-
trol constructions (*PRO*) and dropped pronouns
(*pro*). The top pattern that matches *pro* and
*PRO* are both exactly the same, since the pat-
tern will be matched against parse trees where empty
nodes have been deleted.
When it became apparent that we cannot use the
same definition of patterns to successfully restore
empty categories, we added more context to the pat-
terns. Patterns needed more context for them to be
able to disambiguate between sites that need to be
inserted with *pro*s and sites that need to be in-
serted with *PRO*s. Instead of using minimal tree
fragments that matched empty categories, we in-
cluded the parent and siblings of the minimal tree
fragment in the pattern (pattern matching method
1). This way, we gained more context. However,
as can be seen in Table 5, there is still a lot of over-
lap between patterns for the two empty categories.
However, it is more apparent that at least we can
choose the pattern that will maximize matches for
one empty category and then discard that pattern for
the other empty category.
We also tried giving patterns even more context
by including terminals if preterminals are present in
the pattern (pattern matching method 2). In this way,
we are able have more context for patterns such as
(VP VV (IP ( NP (-NONE- *PRO*) ) VP)) by know-
ing what the verb that precedes the empty category
is. Instead of the original pattern, we would have
patterns such as (VP (VV??) ( IP ( NP (-NONE-
*PRO*)) VP)). We are able to gain more context be-
cause some verbs select for a control construction.
The Chinese verb ?? generally translates to En-
glish as to decide and is more often followed by
a control construction than by a dropped pronoun.
Whereas the pattern (VP (VV ??) ( IP ( NP (-
NONE- *PRO*)) VP)) occurred 154 times in the
training data, the pattern (VP (VV ??) (IP (NP
(-NONE- *pro*)) VP)) occurred only 8 times in the
639
IP
NP-SBJ
-NONE-
*pro*
VP
VV
??
NP-OBJ
PN
??
PU
?
? IP
VP
VV
??
NP-OBJ
PN
??
PU
?
(IP (NP-SBJ (-NONE- *pro*)) VP PU) (IP VP PU)
Figure 1: An example of a tree with an empty node (left), the tree stripped of an empty node (right), and a pattern that
matches the example. Sentences are parsed without empty nodes and if a tree fragment (IP VP PU) is encountered in
a parse tree, the empty node may be inserted according to the learned pattern (IP (NP-SBJ (-NONE- *pro*)) VP PU).
*PRO* *pro*
Count Pattern Count Pattern
12269 ( IP ( NP (-NONE- *PRO*) ) VP ) 10073 ( IP ( NP (-NONE- *pro*) ) VP )
102 ( IP PU ( NP (-NONE- *PRO*) ) VP PU ) 657 ( IP ( NP (-NONE- *pro*) ) VP PU )
14 ( IP ( NP (-NONE- *PRO*) ) VP PRN ) 415 ( IP ADVP ( NP (-NONE- *pro*) ) VP )
13 ( IP NP ( NP (-NONE- *PRO*) ) VP ) 322 ( IP NP ( NP (-NONE- *pro*) ) VP )
12 ( CP ( NP (-NONE- *PRO*) ) CP ) 164 ( IP PP PU ( NP (-NONE- *pro*) ) VP )
*PRO* *pro*
Count Pattern Count Pattern
2991 ( VP VV NP ( IP ( NP (-NONE- *PRO*) ) VP ) ) 1782 ( CP ( IP ( NP (-NONE- *pro*) ) VP ) DEC )
2955 ( VP VV ( IP ( NP (-NONE- *PRO*) ) VP ) ) 1007 ( VP VV ( IP ( NP (-NONE- *pro*) ) VP ) )
850 ( CP ( IP ( NP (-NONE- *PRO*) ) VP ) DEC ) 702 ( LCP ( IP ( NP (-NONE- *pro*) ) VP ) LC )
765 ( PP P ( IP ( NP (-NONE- *PRO*) ) VP ) ) 684 ( IP IP PU ( IP ( NP (-NONE- *pro*) ) VP ) PU )
654 ( LCP ( IP ( NP (-NONE- *PRO*) ) VP ) LC ) 654 ( TOP ( IP ( NP (-NONE- *pro*) ) VP PU ) )
Table 5: Top five minimally connected patterns that match *pro* and *PRO* (top). Patterns that match both *pro*
and *PRO* are shaded with the same color. The table on the bottom show more refined patterns that are given added
context by including the parent and siblings to minimally connected patterns. Many patterns still match both *pro*
and *PRO* but there is a lesser degree of overlap.
640
training data.
After the patterns are extracted, we performed
pruning similar to the pruning that was done by
Johnson (2002). The patterns that have less than
50% chance of matching are discarded. For exam-
ple, if (IP VP) occurs one hundred times in a tree-
bank that is stripped of empty nodes and if pattern
(IP (NP (-NONE- *PRO*)) VP) occurs less than
fifty times in the same treebank that is annotated
with empty nodes, it is discarded.1 We also found
that we can discard patterns that occur very rarely
(that occur only once) without losing much accu-
racy. In cases where there was an overlap between
two empty categories, the pattern was chosen for
either *pro* or *PRO*, whichever that maximized
the number of matchings and then discarded for the
other.
3.2 Conditional random field
We tried building a simple conditional random field
(Lafferty et al, 2001) to predict null elements. The
model examines each and every word boundary and
decides whether to leave it as it is, insert *pro*,
or insert *PRO*. The obvious disadvantage of this
method is that if there are two consecutive null el-
ements, it will miss at least one of them. Although
there were some cases like this in the treebank, they
were rare enough that we decided to ignore them.
We first tried using only differently sized local win-
dows of words as features (CRF model 1). We also
experimented with adding the part-of-speech tags of
words as features (CRF model 2). Finally, we exper-
imented with a variation where the model is given
each word and its part-of-speech tag and its imme-
diate parent node as features (CRF model 3).
We experimented with using different regulariza-
tions and different values for regularizations but it
did not make much difference in the final results.
The numbers we report later used L2 regularization.
3.3 Parsing
In this approach, we annotated nonterminal symbols
in the treebank to include information about empty
categories and then extracted a context free gram-
mar from the modified treebank. We parsed with
the modified grammar, and then deterministically re-
1See Johnson (2002) for more details.
*PRO* *pro*
Cycle Prec. Rec. F1 Prec Rec. F1
1 0.38 0.08 0.13 0.38 0.08 0.12
2 0.52 0.23 0.31 0.37 0.18 0.24
3 0.59 0.46 0.52 0.43 0.24 0.31
4 0.62 0.50 0.56 0.47 0.25 0.33
5 0.61 0.52 0.56 0.47 0.33 0.39
6 0.60 0.53 0.56 0.46 0.39 0.42
7 0.58 0.52 0.55 0.43 0.40 0.41
Table 6: Result using the grammars output by the Berke-
ley state-splitting grammar trainer to predict empty cate-
gories
covered the empty categories from the trees. Fig-
ure 2 illustrates how the trees were modified. For
every empty node, the most immediate ancestor of
the empty node that has more than one child was an-
notated with information about the empty node, and
the empty node was deleted. We annotated whether
the deleted empty node was *pro* or *PRO* and
where it was deleted. Adding where the child was
necessary because, even though most empty nodes
are the first child, there are many exceptions.
We first extracted a plain context free grammar af-
ter modifying the trees and used the modified gram-
mar to parse the test set and then tried to recover the
empty elements. This approach did not work well.
We then applied the latent annotation learning pro-
cedures of Petrov et al (2006)2 to refine the non-
terminals in the modified grammar. This has been
shown to help parsing in many different situations.
Although the state splitting procedure is designed to
maximize the likelihood of of the parse trees, rather
than specifically to predict the empty nodes, learning
a refined grammar over modified trees was also ef-
fective in helping to predict empty nodes. Table 6
shows the dramatic improvement after each split,
merge, and smoothing cycle. The gains leveled off
after the sixth iteration and the sixth order grammar
was used to run later experiments.
3.4 Results
Table 7 shows the results of our experiments. The
numbers are very low when compared to accuracy
reported in other works that were mentioned in the
beginning of this Section, which dealt with the Penn
English Treebank. Dropped pronouns are especially
2http://code.google.com/p/berkeleyparser/
641
IP
NP-SBJ
-NONE-
*pro*
VP
VV
??
NP-OBJ
PN
??
PU
?
? SPRO0IP
VP
VV
??
NP-OBJ
PN
??
PU
?
Figure 2: An example of tree modification
*PRO* *pro*
Prec. Rec. F1 Prec Rec. F1
Pattern 1 0.65 0.61 0.63 0.41 0.23 0.29
Pattern 2 0.67 0.58 0.62 0.46 0.24 0.31
CRF 1 0.66 0.31 0.43 0.53 0.24 0.33
CRF 2 0.68 0.46 0.55 0.58 0.35 0.44
CRF 3 0.63 0.47 0.54 0.54 0.36 0.43
Parsing 0.60 0.53 0.56 0.46 0.39 0.42
Table 7: Result of recovering empty nodes
hard to recover. However, we are dealing with a dif-
ferent language and different kinds of empty cate-
gories. Empty categories recovered this way may
still help translation. In the next section, we take the
best variation of the each method use it to add empty
categories to a training corpus and train machine
translation systems to see whether having empty cat-
egories can help improve translation in more realis-
tic situations.
3.5 Analysis
The results reveal many interesting aspects about re-
covering empty categories. The results suggest that
tree structures are important features for finding sites
where markers for control constructions (*PRO*)
have been deleted. The method utilizing patterns
that have more information about tree structure of
these sites performed better than other methods. The
fact that the method using parsing was better at pre-
dicting *PRO*s than the methods that used the con-
ditional random fields also corroborates this finding.
For predicting dropped pronouns, the method using
the CRFs did better than the others. This suggests
that rather than tree structure, local context of words
and part-of-speech tags maybe more important fea-
tures for predicting dropped pronouns. It may also
suggest that methods using robust machine learning
techniques are better outfitted for predicting dropped
pronouns.
It is interesting to note how effective the parser
was at predicting empty categories. The method us-
ing the parser requires the least amount of supervi-
sion. The method using CRFs requires feature de-
sign, and the method that uses patterns needs hu-
man decisions on what the patterns should be and
pruning criteria. There is also room for improve-
ment. The split-merge cycles learn grammars that
produce better parse trees rather than grammars that
predict empty categories more accurately. By modi-
fying this learning process, we may be able to learn
grammars that are better suited for predicting empty
categories.
4 Experiments
4.1 Setup
For Chinese-English, we used a subset of FBIS
newswire data consisting of about 2M words and
60K sentences on the English side. For our develop-
ment set and test set, we had about 1000 sentences
each with 10 reference translations taken from the
NIST 2002 MT evaluation. All Chinese data was
re-segmented with the CRF-based Stanford Chinese
segmenter (Chang et al, 2008) that is trained on
the segmentation of the Chinese Treebank for con-
sistency. The parser used in Section 3 was used to
parse the training data so that null elements could
be recovered from the trees. The same method for
recovering null elements was applied to the train-
642
BLEU BP *PRO* *pro*
Baseline 23.73 1.000
Pattern 23.99 0.998 0.62 0.31
CRF 24.69* 1.000 0.55 0.44
Parsing 23.99 1.000 0.56 0.42
Table 8: Final BLEU score result. The asterisk indicates
statistical significance at p < 0.05 with 1000 iterations
of paired bootstrap resampling. BP stands for the brevity
penalty in BLEU. F1 scores for recovering empty cate-
gories are repeated here for comparison.
ing, development, and test sets to insert empty nodes
for each experiment. The baseline system was also
trained using the raw data.
We used Moses (Koehn et al, 2007) to train
machine translation systems. Default parameters
were used for all experiments. The same number
of GIZA++ (Och and Ney, 2003) iterations were
used for all experiments. Minimum error rate train-
ing (Och, 2003) was run on each system afterwards
and the BLEU score (Papineni et al, 2002) was cal-
culated on the test set.
4.2 Results
Table 8 summarizes our results. Generally, all sys-
tems produced BLEU scores that are better than the
baseline, but the best BLEU score came from the
system that used the CRF for null element insertion.
The machine translation system that used training
data from the method that was overall the best in
predicting empty elements performed the best. The
improvement is 0.96 points in BLEU score, which
represents statistical significance at p < 0.002 based
on 1000 iterations of paired bootstrap resampling
(Koehn, 2004). Brevity penalties applied for cal-
culating BLEU scores are presented to demonstrate
that the baseline system is not penalized for produc-
ing shorter sentences compared other systems.3
The BLEU scores presented in Table 8 represent
the best variations of each method we have tried
for recovering empty elements. Although the dif-
ference was small, when the F1 score were same
for two variations of a method, it seemed that we
could get slightly better BLEU score with the varia-
tion that had higher recall for recovering empty ele-
3We thank an anonymous reviewer for tipping us to examine
the brevity penalty.
ments rather the variation with higher precision.
We tried a variation of the experiment where the
CRF method is used to recover *pro* and the pattern
matching is used to recover *PRO*, since these rep-
resent the best methods for recovering the respective
empty categories. However, it was not as successful
as we thought would be. The resulting BLEU score
from the experiment was 24.24, which is lower than
the one that used the CRF method to recover both
*pro* and *PRO*. The problem was we used a very
na?ve method of resolving conflict between two dif-
ferent methods. The CRF method identified 17463
sites in the training data where *pro* should be
added. Of these sites, the pattern matching method
guessed 2695 sites should be inserted with *PRO*
rather than *pro*, which represent more than 15%
of total sites that the CRF method decided to in-
sert *pro*. In the aforementioned experiment, wher-
ever there was a conflict, both *pro* and *PRO*
were inserted. This probably lead the experiment
to have worse result than using only the one best
method. This experiment suggest that more sophisti-
cated methods should be considered when resolving
conflicts created by using heterogeneous methods to
recover different empty categories.
Table 9 shows five example translations of source
sentences in the test set that have one of the empty
categories. Since empty categories have been auto-
matically inserted, they are not always in the cor-
rect places. The table includes the translation results
from the baseline system where the training and test
sets did not have empty categories and the transla-
tion results from the system (the one that used the
CRF) that is trained on an automatically augmented
corpus and given the automatically augmented test
set.
5 Conclusion
In this paper, we have showed that adding some
empty elements can help building machine transla-
tion systems. We showed that we can still benefit
from augmenting the training corpus with empty el-
ements even when empty element prediction is less
than what would be conventionally considered ro-
bust.
We have also shown that there is a lot of room for
improvement. More comprehensive and sophisti-
643
source ???? *PRO*????????
reference china plans to invest in the infrastructure
system trained w/ nulls china plans to invest in infrastructure
system trained w/o nulls china ?s investment in infrastructure
source ?? *PRO*????????????
reference good for consolidating the trade and shipping center of hong kong
system trained w/ nulls favorable to the consolidation of the trade and shipping center in hong kong
system trained w/o nulls hong kong will consolidate the trade and shipping center
source ?????? *PRO*??????
reference some large - sized enterprises to gradually go bankrupt
system trained w/ nulls some large enterprises to gradually becoming bankrupt
system trained w/o nulls some large enterprises gradually becoming bankrupt
source *pro*??????
reference it is not clear now
system trained w/ nulls it is also not clear
system trained w/o nulls he is not clear
source *pro*??????
reference it is not clear yet
system trained w/ nulls it is still not clear
system trained w/o nulls is still not clear
Table 9: Sample translations. The system trained without nulls is the baseline system where the training corpus and
test corpus did not have empty categories. The system trained with nulls is the system trained with the training corpus
and the test corpus that have been automatically augmented with empty categories. All examples are part of longer
sentences.
cated methods, perhaps resembling the work of Gab-
bard et al (2006) may be necessary for more accu-
rate recovery of empty elements. We can also con-
sider simpler methods where different algorithms
are used for recovering different empty elements, in
which case, we need to be careful about how recov-
ering different empty elements could interact with
each other as exemplified by our discussion of the
pattern matching algorithm in Section 3 and our ex-
periment presented in Section 4.2.
There are several other issues we may consider
when recovering empty categories that are miss-
ing in the target language. We only considered
empty categories that are present in treebanks. How-
ever, there might be some empty elements which are
not annotated but nevertheless helpful for improv-
ing machine translation. As always, preprocessing
the corpus to address a certain problem in machine
translation is less principled than tackling the prob-
lem head on by integrating it into the machine trans-
lation system itself. It may be beneficial to include
consideration for empty elements in the decoding
process, so that it can benefit from interacting with
other elements of the machine translation system.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work
was supported by NSF grants IIS-0546554 and IIS-
0910611.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank II
style. Penn Treebank Project, January.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher Man-
ning. 2008. Optimizing Chinese word segmentation
for machine translation performance. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 224?232.
Ryan Gabbard, Seth Kulick, and Mitchell Marcus. 2006.
Fully parsing the Penn Treebank. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 184?191, New York
644
City, USA, June. Association for Computational Lin-
guistics.
Na-Rae Han and Shijong Ryu. 2005. Guidelines for
Penn Korean Treebank version 2.0. Technical report,
IRCS, University of Pennsylvania.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang Rim.
2009. Bridging morpho-syntactic gap between source
and target sentences for English-Korean statistical ma-
chine translation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 233?236.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and Metrics, pages 244?251.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Confer-
ence of the Association for Computational Linguistics
(ACL-02), Philadelphia, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 388?395, Barcelona, Spain, July.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Ma-
chine Learning: Proceedings of the Eighteenth Inter-
national Conference (ICML 2001), Stanford, Califor-
nia.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Nianwen Xue and Fei Xia. 2000. The bracketing guide-
lines for the Penn Chinese Treebank. Technical Report
IRCS-00-08, IRCS, University of Pennsylvania.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Conference of the Association for Com-
putational Linguistics (ACL-01), Toulouse, France.
645
Sampling Tree Fragments from Forests
Tagyoung Chung?
University of Rochester
Licheng Fang??
University of Rochester
Daniel Gildea?
University of Rochester
Daniel S?tefankovic??
University of Rochester
We study the problem of sampling trees from forests, in the setting where probabilities for each
tree may be a function of arbitrarily large tree fragments. This setting extends recent work
for sampling to learn Tree Substitution Grammars to the case where the tree structure (TSG
derived tree) is not fixed. We develop a Markov chain Monte Carlo algorithm which corrects for
the bias introduced by unbalanced forests, and we present experiments using the algorithm to
learn Synchronous Context-Free Grammar rules for machine translation. In this application, the
forests being sampled represent the set of Hiero-style rules that are consistent with fixed input
word-level alignments. We demonstrate equivalent machine translation performance to standard
techniques but with much smaller grammars.
1. Introduction
Recent work on learning Tree Substitution Grammars (TSGs) has developed procedures
for sampling TSG rules from known derived trees (Cohn, Goldwater, and Blunsom
2009; Post and Gildea 2009). Here one samples binary variables at each node in the
tree, indicating whether the node is internal to a TSG rule or is a split point between
two rules. We consider the problem of learning TSGs in cases where the tree structure
is not known, but rather where possible tree structures are represented in a forest. For
example, we may wish to learn from text where treebank annotation is unavailable,
? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: chung@cs.rochester.edu.
?? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: lfang@cs.rochester.edu.
? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: gildea@cs.rochester.edu.
? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: stefanko@cs.rochester.edu.
Submission received: 26 October 2012; revised version received: 14 March 2013; accepted for publication:
4 May 2013.
doi:10.1162/COLI a 00170
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
but a forest of likely parses can be produced automatically. Another application on
which we focus our attention in this article arises in machine translation, where we
want to learn translation rules from a forest representing the phrase decompositions that
are consistent with an automatically derived word alignment. Both these applications
involve sampling TSG trees from forests, rather than from fixed derived trees.
Chappelier and Rajman (2000) present a widely used algorithm for sampling trees
from forests: One first computes an inside probability for each node bottom?up, and
then chooses an incoming hyperedge for each node top?down, sampling according to
each hyperedge?s inside probability. Johnson, Griffiths, and Goldwater (2007) use this
sampling algorithm in a Markov chain Monte Carlo framework for grammar learning.
We can combine the representations used in this algorithm and in the TSG learning
algorithm discussed earlier, maintaining two variables at each node of the forest, one
for the identity of the incoming hyperedge, and another representing whether the node
is internal to a TSG rule or is a split point. However, computing an inside probability
for each node, as in the first phase of the algorithm of Johnson, Griffiths, and Goldwater
(2007), becomes difficult because of the exponential number of TSG rules that can apply
at any node in the forest. Not only is the number of possible TSG rules that can apply
given a fixed tree structure exponentially large in the size of the tree, but the number of
possible tree structures under a node is also exponentially large. This problem is par-
ticularly acute during grammar learning, as opposed to sampling according to a fixed
grammar, because any tree fragment is a valid potential rule. Cohn and Blunsom (2010)
address the large number of valid unseen rules by decomposing the prior over TSG
rules into an equivalent probabilistic context-free grammar; however, this technique
only applies to certain priors. In general, algorithms that match all possible rules are
likely to be prohibitively slow, as well as unwieldy to implement. In this article, we
design a sampling algorithm that avoids explicitly computing inside probabilities for
each node in the forest.
In Section 2, we derive a general algorithm for sampling tree fragments from forests.
We avoid computing inside probabilities, as in the TSG sampling algorithms of Cohn,
Goldwater, and Blunsom (2009) and Post and Gildea (2009), but we must correct for
the bias introduced by the forest structure, a complication that does not arise when the
tree structure is fixed. In order to simplify the presentation of the algorithm, we first set
aside the complication of large, TSG-style rules, and describe an algorithm for sampling
trees from forests while avoiding computation of inside probabilities. This algorithm is
then generalized to learn the composed rules of TSG in Section 2.3.
As an application of our technique, we present machine translation experiments in
the remainder of the article. We learn Hiero-style Synchronous Context-Free Grammar
(SCFG) rules (Chiang 2007) from bilingual sentences for which a forest of possible
minimal SCFG rules has been constructed fromfixedword alignments. The construction
of this forest and its properties are described in Section 3. We make the assumption
that the alignments produced by a word-level model are correct in order to simplify
the computation necessary for rule learning. This approach seems safe given that the
pipeline of alignment followed by rule extraction has generally remained the state of
the art despite attempts to learn joint models of alignment and rule decomposition
(DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al. 2009; Blunsom and Cohn
2010a). We apply our sampling algorithm to learn the granularity of rule decomposition
in a Bayesian framework, comparing sampling algorithms in Section 4. The end-to-end
machine translation experiments of Section 5 show that our algorithm is able to achieve
performance equivalent to the standard technique of extracting all rules, but results in
a significantly smaller grammar.
204
Chung et al. Sampling Tree Fragments from Forests
2. Sampling Trees from Forests
As a motivating example, consider the small example forest of Figure 1. This forest
contains a total of five trees, one under the hyperedge labeled A, and four under the
hyperedge labeled B (the cross-product of the two options for deriving node 4 and the
two options for deriving node 5).
Let us suppose that we wish to sample trees from this forest according to a distribu-
tion Pt, and further suppose that this distribution is proportional to the product of the
weights of each tree?s hyperedges:
Pt(t) ?
?
h?t
w(h) (1)
To simplify the example, suppose that in Figure 1 each hyperedge has weight 1,
?h w(h) = 1
giving us a uniform distribution over trees:
?t Pt(t) = 15
A tree can be specified by attaching a variable zn to each node n in the forest
indicating which incoming hyperedge is to be used in the current tree. For example,
variable z1 can take values A and B in Figure 1, whereas variables z2 and z3 can only
take a single value. We use z to refer to the entire set of variables zn in a forest. Each
assignment to z specifies a unique tree, ?(z), which can be found by following the
incoming hyperedges specified by z from the goal node of each forest down to the
terminals.
A naive sampling strategy would be to resample each of these variables zn in
order, holding all others constant, as in standard Gibbs sampling. If we choose an
incoming hyperedge according to the probability Pt(?(z)) of the resulting tree, holding
all other variable assignments fixed, we see that, because Pt is uniform, we will choose
2 3 5
1
4
A
B
Figure 1
Example forest.
205
Computational Linguistics Volume 40, Number 1
with uniform probability of 1/m among the m incoming hyperedges at each node. In
particular, we will choose among the two incoming hyperedges at the root (node 1)
with equal probability, meaning that, over the long run, the sampler will spend half its
time in the state for the single tree corresponding to nodes 2 and 3, and only one eighth
of its time in each of the four other possible trees. Our naive algorithm has failed at its
goal of sampling among the five possible trees each with probability 1/5.
Thus, we cannot adopt the simple Gibbs sampling strategy, used for TSG induction
from fixed trees, of resampling one variable at a time according to the target distribution,
conditioned on all other variables. The intuitive reason for this, as illustrated by the
example, is the bias introduced by forests that are bushier (that is, havemore derivations
for each node) in some parts than in others. The algorithm derived in the remainder of
this section corrects for this bias, while avoiding the computation of inside probabilities
in the forest.
2.1 Choosing a Stationary Distribution
We will design our sampling algorithm by first choosing a distribution Pz over the set
of variables z defined earlier. We will show correctness of our algorithm by showing
that it is a Markov chain converging to Pz, and that Pz results in the desired distribution
Pt over trees.
The tree specified by an assignment to z will be denoted ?(z) (see Table 1). For a tree
t the vector containing the variables found at nodes in t will be denoted z[t]. This is a
subvector of z: for example, in Figure 1, if t chooses A at node 1 then z[t] = (z1, z2, z3),
and if t chooses B at node 1 then z[t] = (z1, z4, z5). We use z[?t] to denote the variables
not used in the tree t. The vectors z[t] and z[?t] differ according to t, but for any tree
t, the two vectors form a partition of z. There are many values of z that correspond to
the same tree, but each tree t corresponds to a unique subvector of variables z[t] and a
unique assignment to those specific variables?we will denote this unique assignment
of variables in z[t] by ?(t). (In terms of ? and ? one has for any z and t that ?(z) = t if
and only if z[t] = ?(t).)
Let Z be the random vector generated by our algorithm. As long as Pz(?(Z) = t) =
Pt(t) for all trees t, our algorithm will generate trees from the desired distribution.
Thus for any tree t the probability Pz(Z[t] = ?(t)) is fixed to Pt(t), but any distribu-
tion over the remaining variables not contained in t will still yield the desired dis-
tribution over trees. Thus, in designing the sampling algorithm, we may choose any
distribution Pz(Z[?t] | Z[t] = ?(t)). A simple and convenient choice is to make
Pz(Z[?t] | Z[t] = ?(t)) uniform. That is, each incoming hyperedge variable with m
alternatives assigns each hyperedge probability 1/m. (In fact, our algorithm can easily
Table 1
Notation
Pt desired distribution on trees
z vector of variables
Z random vector over z
?(z) tree corresponding to setting of z
Z[t] subset of random variables that occur in tree t
?(t) setting of variables in Z[t]
Z[?t] subset of random variables that do not occur in t
206
Chung et al. Sampling Tree Fragments from Forests
be adapted to other product distributions of Pz(Z[?t] | Z[t] = ?(t)).) This choice of
P(Z[?t] | Z[t] = ?(t)) determines a unique distribution for Pz:
Pz(Z = z) = Pt(?(z))Pz(Z[??(z)] = z[??(z)] | Z[?(z)] = z[?(z)])
= Pt(?(z))
?
v?z[??(z)]
1
deg(v)
(2)
where deg(v) is the number of possible values for v.
We proceed by designing a Gibbs sampler for this Pz. The sampler resamples vari-
ables from z one at a time, according to the joint probability Pz(z) for each alternative.
The set of possible values for zn at a node n having m incoming hyperedges consists of
the hyperedges ej, 1 ? j ? m. Let sj be the vector z with the value of zn changed to ej.
Note that the ?(sj)?s only differ at nodes below n.
Let z[in(n)] be the vector consisting of the variables at nodes below n (that is,
contained in subtrees rooted at n, or ?inside? n) in the forest, and let z[in(n)] be the
vector consisting of variables not under node n. Thus the vectors z[in(n)] and z[in(n)]
partition the complete vector z. We will use the notation z[t ? in(n)] to represent the
vector of variables from z that are both in a tree t and under a node n.
For Gibbs sampling, we need to compute the relative probabilities of sj?s. We now
consider the two terms of Equation (2) in this setting. Because of our requirement that Pz
correspond to the desired distribution Pt, the first term of Equation (2) can be computed,
up to a normalization constant, by evaluating our model over trees (Equation (1)).
The second term of Equation (2) is a product of uniform distributions that can be
decomposed into nodes below n and all other nodes:
Pz(Z[?t] = z[?t] | Z[t] = z[t]) =
?
v?z[?t? in(n)]
1
deg(v)
?
v?z[?t? in(n)]
1
deg(v)
(3)
where t = ?(z). Recall that ?(sj)?s only differ at vertices below n and hence
Pz(Z[?t] = z[?t] | Z[t] = z[t]) ?
?
v?z[?t? in(n)]
1
deg(v)
(4)
where we emphasize that ? refers to the relative probabilities of the sj?s, which corre-
spond to the options from which the Gibbs sampler chooses at a given step. We can
manipulate Equation (4) into a more computationally convenient form by multiplying
by the deg(v) term for each node v inside n. Because the terms for all nodes not included
in the current tree cancel each other out, we are left with:
Pz(Z[?t] = z[?t] | Z[t] = z[t]) ?
?
v?z[t? in(n)]
deg(v) (5)
Note that we only need to consider the nodes z[t] in the current tree, without needing
to examine the remainder of the forest at all.
207
Computational Linguistics Volume 40, Number 1
Substituting Equation (5) into Equation (2) gives a simple update rule for use in our
Gibbs sampler:
Pz(Z(i+1) = sj | Z(i) = z,n is updated) ? Pt(?(sj))
?
v?z[?(sj )?in(n)]
deg(v) (6)
To make a step of the Markov chain, we compute the right-hand side of Equation (6)
for every sj and then choose the next state of the chain Z(i+1) from the corresponding
distribution on the sj?s. The second term, which we refer to as the density factor, is
equal to the total number of trees in the forest under node n. This factor compensates
for the bias introduced by forests that are bushier in some places than in others, as in
the example of Figure 1. A related factor, defined on graphs rather than hypergraphs,
can be traced back as far as Knuth (1975), who wished to estimate the sum of values at
all nodes in a large tree by sampling a small number of the possible paths from the root
to the leaves. Knuth sampled paths uniformly and independently, rather than using a
continuously evolving Markov chain as in our Gibbs sampler.
2.2 Sampling Schedule
In a standard Gibbs sampler, updates are made iteratively to each variable z1, . . . , zN,
and this general strategy can be applied in our case. However, it may be wasteful to
continually update variables that are not used by the current tree and are unlikely to be
used by any tree. We propose an alternative sampling schedule consisting of sweeping
from the root of the current tree down to its leaves, resampling variables at each node
in the current tree as we go. If an update changes the structure of the current tree, the
sweep continues along the new tree structure. This strategy is shown in Algorithm 1,
where v(z, i) denotes the ith variable in a top?down ordering of the variables of the
current tree ?(z). The top?down ordering may be depth-first or breadth-first, among
other possibilities, as long as the variables at each node have lower indices than the
variables at the node?s descendants in the current tree.
To show that this sampling schedule will converge to the desired distribution over
trees, we will first show that Pz is a stationary distribution for the transition defined by
a single step of the sweep:
Lemma 1
For any setting of variables z, any top?down ordering v(z, i), and any i, updating
variable zv(z,i) according to Equation (6) is stationary with respect to the distribution
Pz defined by Equation (2).
Algorithm 1 Sampling algorithm
Require: A function v(z, i) returning the index of the ith variable of z in a top-down ordering of
the variables of the tree ?(z).
1: i ? 1
2: while i ? |Z[?(z)]| do  Until last node of current tree.
3: Resample zv(z,i) according to Equation (6)
4: i ? i + 1
5: end while
208
Chung et al. Sampling Tree Fragments from Forests
Proof
We will show that each step of the sweep is stationary for Pz by showing that it satisfies
detailed balance. Detailed balance is the condition that, on average, for each pair of
states z and z?, the number of transitions between the two states is the same in either
direction:
Pz(Z = z)P(Z(i+1) = z? | Z(i) = z) = Pz(Z = z?)P(Z(i+1) = z | Z(i) = z?) (7)
where P(Z(i+1) = z? | Z(i) = z) is the transition performed by one step of the sweep:
P(Z(i+1) = z? | Z(i) = z) =
{
Pz(Z=z? )
?
z?? Pz(Z=z
?? )I(z???v(z,i)=z?v(z,i) )
if z??v(z,i) = z?v(z,i)
0 otherwise
(8)
It is important to observe that, because the resampling step only changes the tree
structure below the ith node, the ith node in the new tree remains the same node. That
is, after making an update from z to z?, v(z, i) = v(z?, i), and, mathematically:
z??v(z,i) = z?v(z,i) ? v(z, i) = v(z?, i) ? z??v(z?,i) = z?v(z,i)
? z??v(z?,i) = z?v(z?,i)
Thus, the condition in Equation (8) is symmetric in z and z?, and we define the predicate
match(z, z?, i) to be equivalent to this condition. Substituting Equation (8) into the left-
hand side of Equation (7), we have:
Pz(Z = z)P(Z(i+1) = z? | Z(i) = z) =
{
Pz(Z=z)Pz(Z=z? )
?
z?? Pz(Z=z
?? )I(z???v(z,i)=z?v(z,i) )
if match(z, z?, i)
0 otherwise
(9)
By symmetry of the righthand side of Equation (9) in z and z?, we see that Equa-
tion (7) is satisfied. Because detailed balance implies stationarity, Pz is a stationary
distribution of P(Z(i+1) = z? | Z(i) = z). 
This lemma allows us to prove the correctness of our main algorithm:
Theorem 1
For any top?down sampling schedule v(z, i), and any desired distribution over trees
Pt that assigns non-zero probability to all trees in the forest, Algorithm 1 will converge
to Pt.
Proof
Because Pz is stationary for each step of the sweep, it is stationary for one entire sweep
from top to bottom.
To show that the Markov chain defined by an entire sweep is ergodic, we must show
that it is aperiodic and irreducible. It is aperiodic because the chain can stay in the same
configuration with non-zero probability by selecting the same setting for each variable
in the sweep. The chain is irreducible because any configuration can be reached in a
finite number of steps by sorting the variables in topological order bottom?up in the
forest, and then, for each variable, executing one sweep that selects a tree that includes
the desired variable with the desired setting.
209
Computational Linguistics Volume 40, Number 1
Because Pz is stationary for the chain defined by entire sweeps, and this chain is
ergodic, the chain will converge to Pz. Because Equation (2) guarantees that Pz(?(Z) =
t) = Pt(t), convergence to Pz implies convergence to Pt. 
2.3 Sampling Composed Rules
Our approach to sampling was motivated by a desire to learn TSG-style grammars,
where one grammar rule is the composition of a number of hyperedges in the forest.
We extend our sampling algorithm to handle this problem by using the same methods
that are used to learn a TSG from a single, fixed tree (Cohn, Goldwater, and Blunsom
2009; Post and Gildea 2009). We attach a binary variable to each node in the forest
indicating whether the node is a boundary between two TSG rules, or is internal to a
single TSG rule. Thus, the complete set of variables used by the sampler, z, now consists
of two variables at each node in the forest: one indicating the incoming hyperedge, and
one binary boundary variable. The proof of Section 2.1 carries through, with each new
binary variable v having deg(v) = 2 in Equation (2). As before, the current setting of z
partitions z into two sets of variables, those used in the current tree, z[?(z)], and those
outside the current tree, z[??(z)]. Given a fixed assignment to z, we can read off both
the current tree and its segmentation into TSG rules. We modify the tree probability of
Equation (1) to be a product over TSG rules r:
Pt(t) ?
?
r?t
w(r) (10)
in order to emphasize that grammar rules are no longer strictly equivalent to hyper-
edges in the forest. We modify the sampling algorithm of Algorithm 1 to make use of
this definition of Pt and to resample both variables at the current node. The incoming
hyperedge variable is resampled according to Equation (6), while the segmentation
variable is simply resampled according to Pt, as the update does not change the sets
z[?(z)] and z[??(z)].
The proof that the sampling algorithm converges to the correct distribution still
applies in the TSG setting, as it makes use of the partition of z into z[?(z)] and
z[??(z)], but does not depend on the functional form of the desired distribution over
trees Pt.
3. Phrase Decomposition Forest
In the remainder of this article, we will apply the algorithm developed in the previous
section to the problem of learning rules for machine translation in the context of a
Hiero-style, SCFG-based system. As in Hiero, our grammars will make use of a single
nonterminal X, and will contain rules with a mixture of nonterminals and terminals on
the right-hand side, with at most two nonterminal occurrences in the right-hand side
of a rule. In general, many overlapping rules of varying sizes are consistent with the
input word alignments, meaning that we must address a type of segmentation problem
in order to learn rules of the right granularity. Given the restriction to two right-hand
side nonterminals, the maximum number of rules that can be extracted from an input
sentence pair is O(n12) in the sentence length, because the left and right boundaries of
the left-hand side (l.h.s.) nonterminal and each of the two right-hand side nonterminals
can take O(n) positions in each of the two languages. This complexity leads us to
explore sampling algorithms, as dynamic programming approaches are likely to be
210
Chung et al. Sampling Tree Fragments from Forests
prohibitively slow. In this section, we show that the problem of learning rules can be
analyzed as a problem of identifying tree fragments of unknown size and shape in a
forest derived from the input word alignments for each sentence. These tree fragments
are similar to the tree fragments used in TSG learning. As in TSG learning, each rule
of the final grammar consists of some number of adjacent, minimal tree fragments:
one-level treebank expansions in the case of TSG learning and minimal SCFG rules,
defined subsequently, in the case of translation. The internal structure of TSG rules is
used during parsing to determine the final tree structure to output, and the internal
structure of machine translation rules will not be used at decoding time. This distinction
is irrelevant during learning. A more significant difference from TSG learning is that the
sets of minimal tree fragments in our SCFG application come not from a single, known
tree, but rather from a forest representing the set of bracketings consistent with the input
word alignments.
We now proceed to precisely define this phrase decomposition forest and discuss
some of its theoretical properties. The phrase decomposition forest is designed to extend
the phrase decomposition tree defined by Zhang, Gildea, and Chiang (2008) in order to
explicitly represent each possible minimal rule with a hyperedge.
A span [i, j] is a set of contiguous word indices {i, i + 1, . . . , j ? 1}. Given an aligned
Chinese?English sentence pair, a phrase n is a pair of spans n = ([i1, j1], [i2, j2]) such that
Chinese words in positions [i1, j1] are aligned only to English words in positions [i2, j2],
and vice versa. A phrase forest H = ?V,E? is a hypergraph made of a set of hypernodes
V and a set of hyperedges E. Each node n = ([i1, j1], [i2, j2]) ? V is a tight phrase as
defined by Koehn, Och, and Marcu (2003), namely, a phrase containing no unaligned
words at its boundaries. A phrase n = ([i1, j1], [i2, j2]) covers n? = ([i?1, j
?
1], [i
?
2, j
?
2]) if
i1 ? i?1 ? j?1 ? j1 ? i2 ? i?2 ? j?2 ? j2
Note that every phrase covers itself. It follows from the definition of phrases that if
i1 ? i?1 ? j?1 ? j1, then i2 ? i?2 ? j?2 ? j2. That means we can determine phrase coverage by
only looking at one language side of the phrases. We are going to use this property
to simplify the discussion of our proofs. We also define coverage between two sets of
phrases. Given two sets of phrases T and T?, we say T? covers T if for all t ? T, there
exists a t? ? T? such that t? covers t. We say that two phrases overlap if they intersect,
but neither covers the other.
If two phrases n = ([i1, j1], [i2, j2]) and n? = ([i?1, j
?
1], [i
?
2, j
?
2]) intersect, we can take the
union of the two phrases by taking the union of the source and target language spans,
respectively. That is, n1 ? n2 = ([i1, j1] ? [i?1, j?1], [i2, j2] ? [i?2, j?2]). An important property
of phrases is that if two phrases intersect, their union is also a phrase. For example,
given that have a date with her and with her today are both valid phrases in Figure 2, have
a date with her today must also be a valid phrase. Given a set T of phrases, we define
the union closure of the phrase set T, denoted
??(T), to be constructed by repeatedly
joining intersecting phrases until there are no intersecting phrases left.
Each edge in E, written as T ? n, is made of a set of non-intersecting tail nodes T ?
V, and a single head node n ? V that covers each tail node. Each edge is an SCFG rule
consistent with the word alignments. Each tail node corresponds to a right-hand-side
nonterminal in the SCFG rule, and any position included in n but not included in any tail
node corresponds to a right-hand-side terminal in the SCFG rule. For example, given the
aligned sentence pair of Figure 2, the edge {([3, 4], [5, 6]), ([5, 6], [3, 4])} ? ([2, 6], [1, 6]),
corresponds to a SCFG rule X ?? X1 ? X2, have a X2 with X1.
211
Computational Linguistics Volume 40, Number 1
?
I
??
have
?
a
?
date
?
with
??
her
today
Figure 2
Example word alignment, with boxes showing valid phrase pairs. In this example, all individual
alignment points are also valid phrase pairs.
For the rest of this section, we assume that there are no unaligned words. Unaligned
words can be temporarily removed from the alignment matrix before building the
phrase decomposition forest. After extracting the forest, they are put back into the
alignment matrix. For each derivation in the phrase decomposition forest, an unaligned
word appears in the SCFG rule whose left-hand side corresponds to the lowest forest
node that covers the unaligned word.
Definition 1
An edge T ? n is minimal if there does not exist another edge T? ? n such that T?
covers T.
A minimal edge is an SCFG rule that cannot be decomposed by factoring out some
part of its right-hand side as a separate rule. We define a phrase decomposition forest
to be made of all phrases from a sentence pair, connected by all minimal SCFG rules.
A phrase decomposition forest compactly represents all possible SCFG rules that are
consistent with word alignments. For the example word alignment shown in Figure 2,
the phrase decomposition forest is shown in Figure 3. Each boxed phrase in Figure 2
corresponds to a node in the forest of Figure 3, and hyperedges in Figure 3 represent
ways of building phrases out of shorter phrases.
A phrase decomposition forest has the important property that any SCFG rule
consistent with the word alignment corresponds to a contiguous fragment of some
complete tree found in the forest. For example, the highlighted tree fragment of the
forest in Figure 3 corresponds to the SCFG rule:
X ? ? X2 ? X1, have a X1 with X2
Thus any valid SCFG rule can be formed by selecting a set of adjacent hyperedges from
the forest and composing the minimal SCFG rules specified by each hyperedge. We
will apply the sampling algorithm developed in Section 2 to this problem of selecting
hyperedges from the forest.
212
Chung et al. Sampling Tree Fragments from Forests
Figure 3
A phrase decomposition forest extracted from the sentence pair ?????????, I have
a date with her today?. Each edge is a minimal SCFG rule, and the rules at the bottom level are
phrase pairs. Unaligned word ?a? shows up in the rule X ? X1X2,X1aX2 after unaligned
words are put back into the alignment matrix. The highlighted portion of the forest shows
an SCFG rule built by composing minimal rules.
The structure and size of phrase decomposition forests are constrained by the
following lemma:
Lemma 2
When there exists more than one minimal edge leading to the same head node n =
([i1, j1], [i2, j2]), each of these minimal edges is a binary split of phrase pair n, which
gives us either a straight or inverted binary SCFG rule with no terminals.
Proof
Suppose that there exist two minimal edges T1 ? n and T2 ? n leading to node n.
Consider the node set we get by taking the union closure of the tail nodes in T1
and T2:
??
(T1 ? T2) ? n
Figure 4 shows two cases of this construction. We show only the spans on the source
language side, which is enough to determine coverage properties. Let n have span [i, j]
on the source side. In the first case (left),
??(T1 ? T2) = {n}. We know
??(T1 ? T2) ? n
is also a valid edge because the unions of intersecting phrases are phrases, too. By the
definition of union closure,
??(T1 ? T2) covers both T1 and T2. Therefore T1 ? n and
213
Computational Linguistics Volume 40, Number 1
T2 ? n cannot both be minimal. In the second case (right),
??(T1 ? T2) = {n}. This
means that the phrases in T1 ? T2 overlap one another in a chain covering the entire span
of n. There must exist a phrase n1 = [i, k1] in T1 or T2 that begins at the left boundary
i of n. Without loss of generality, assume that n1 ? T1. There must exist another phrase
n2 = [k2, j2] ? T2 that overlaps with n such that k2 < k1 and j2 > k1. The span [k2, j] is
a valid phrase, because it consists of the union closure of all phrases that begin to the
right of k2:
??
{[i?, j?] | [i?, j?] ? T1 ? T2 ? i? ? k2} = {[k2, j]}
We also know that n1 ? n2 = [i, k2] is a valid phrase because the difference of two
overlapping phrases is also a valid phrase. Therefore k2 is a valid binary split point of n,
which means that either T2 is an edge formed by this binary split, or T2 is not minimal.
The span [k2, j]? n1 = [k1, j] is also a valid phrase formed by taking the difference of
two overlapping phrases, which makes k1 a valid binary split point for n. This makes T1
either an edge formed by the binary split at k1, or not a minimal phrase. Thus, whenever
we have two minimal edges, both consist of a binary split. 
Another interesting property of phrase decomposition forests relates to the length
of derivations. A derivation is a tree of minimal edges reaching from a given node all
the way down to the forest?s terminal nodes. The length of a derivation is the number
of minimal edges it contains.
Lemma 3
All derivations under a node in a phrase decomposition forest have the same length.
Proof
This is proved by induction. As the base case, all the nodes at the bottom of the
phrase decomposition forest have only one derivation of length 1. For the induction
step, we consider the two possible cases in Lemma 2. The case where a node n has
only a single edge underneath is trivial. It can have only one derivation length be-
cause the children under that single edge already do. For the case where there are
multiple valid binary splits for a node n at span (i, j), we assume the split points are
k1, . . . , ks. Because the intersection of two phrases is also a phrase, we know that spans
(i, k1), (k1, k2), . . . , (ks, j) are all valid phrases, and so is any concatenation of consecutive
phrases in these spans. Any derivation in this sub-forest structure leading from these
s + 1 spans to n has length s, which completes the proof under the assumption of the
induction. 
??(T1 ? T2)
T2
T1
??(T1 ? T2)
T2
i k1k2 j2 j
T1
Figure 4
Sketch of proof for Lemma 2. In the first case,
??(T1 ? T2) consists of more than one span, or
consists of one span that is strictly smaller than n. In the second case,
??(T1 ? T2) = {n}.
214
Chung et al. Sampling Tree Fragments from Forests
Because all the different derivations under the same node in a minimal phrase forest
contain the same number of minimal rules, we call that number the level of a node. The
fact that nodes can be grouped by levels forms the basis of our fast iterative sampling
algorithm as described in Section 5.3.
3.1 Constructing the Phrase Decomposition Forest
Given a word-aligned sentence pair, a phrase decomposition tree can be extracted with
a shift-reduce algorithm (Zhang, Gildea, and Chiang 2008). Whereas the algorithm of
Zhang, Gildea, and Chiang (2008) constructs a single tree which compactly represents
the set of possible phrase trees, we wish to represent the set of all trees as a forest.
We now describe a bottom?up parsing algorithm, shown in Algorithm 2, for building
this forest. The algorithm considers all spans (i, j) in order of increasing length. The
CYK-like loop over split points k (line 10) is only used for the case where a phrase can
be decomposed into two phrases, corresponding to a binary SCFG rule with no right-
hand side terminals. By Lemma 2, this is the only source of ambiguity in constructing
Algorithm 2 The CYK-like algorithm for building a phrase decomposition forest from
word-aligned sentence pair ?f, e?.
1: Extract all phrase pairs in the form of ([i1, j1], [i2, j2])
2: Build a forest node for each phrase pair, and let n(i, j) be the node corresponding to the phrase
pair whose source side is [i, j]
3: for s = 1 . . . |f | do
4: for i = 0 . . . |f | ? s do
5: j ? i + s
6: if n(i, j) exists then
7: continue
8: end if
9: split ? 0
10: for k = i + 1 . . . j ? 1 do
11: if both n(i, k) and n(k, j) exist then
12: add edge {n(i, k),n(k, j)} ? n(i, j)
13: split ? split + 1
14: end if
15: end for
16: if split = 0 then
17: T ? ?
18: l ? i
19: while l < j do
20: l? ? l + 1
21: for m ? j . . . l do
22: if n(l,m) exists then
23: T ? T ? n(l,m)
24: l? ? m
25: break
26: end if
27: end for
28: l ? l?
29: end while
30: add edge T ? n(i, j)
31: end if
32: end for
33: end for
215
Computational Linguistics Volume 40, Number 1
phrase decompositions. When no binary split is found (line 16), a single hyperedge is
made that connects the current span with all its maximal children. (A child is maximal
if it is not itself covered by another child.) This section can produce SCFG rules with
more than two right-hand side nonterminals, and it also produces any rules containing
both terminals and nonterminals in the right-hand side. Right-hand side nonterminals
correspond to previously constructed nodes n(l,m) in line 23, and right-hand side
terminals correspond to advancing a position in the string in line 20.
The running time of the algorithm is O(n3) in terms of the length of the Chinese
sentence f . The size of the resulting forests depends on the input alignments. The worst
case in terms of forest size is when the input consists of a monotonic, one-to-one word
alignment. In this situation, all (i, k, j) tuples correspond to valid hyperedges, and the
size of the output forest is O(n3). At the other extreme, when given a non-decomposable
permutation as an input alignment, the output forest consists of a single hyperedge.
In practice, given Chinese?English word alignments from GIZA++, we find that the
resulting forests are highly constrained, and the algorithm?s running time is negligible
in our overall system. In fact, we find it better to rebuild every forest from a word
alignment every time we re-sample a sentence, rather than storing the hypergraphs
across sampling iterations.
4. Comparison of Sampling Methods
To empirically verify the sampling methods presented in Section 2, we construct phrase
decomposition forests over which we try to learn composed translation rules. In this
section, we use a simple probability model for the tree probability Pt in order to study
the convergence behavior of our sampling algorithm. We will use a more sophisticated
probability model for our end-to-end machine translation experiments in Section 5.
For studying convergence, we desire a simpler model with a probability that can be
evaluated in closed form.
4.1 Model
We use a very basic generative model based on a Dirichlet process defined over
composed rules. The model is essentially the same as the TSG model used by Cohn,
Goldwater, and Blunsom (2009) and Post and Gildea (2009).
We define a single Dirichlet process over the entire set of rules. We draw the rule
distribution G from a Dirichlet process, and then rules from G.
G | ?,P0 ? Dir(?,P0)
r | G ? G
For the base distribution P0, we use a very simple uniform distribution where all rules
of the same size have equal probability:
P0(r) = Vf
?|rf |Ve
?|re|
where Vf is the vocabulary size of source language, and |rf | is the length of the source
side of the rule r. Integrating over G, we get a Chinese restaurant process for the
Dirichlet process. Customers in the Chinese restaurant analogy represent translation
rule instances in the machine translation setting, and tables represent rule types. The
216
Chung et al. Sampling Tree Fragments from Forests
Chinese restaurant has an infinite number of tables, and customers enter one by one and
choose a table to sit at. Let zi be the table chosen by ith customer. Then, the probability
of the customer choosing a table which is already occupied by customers who entered
the restaurant previously, or a new table, is given by following equations:
P(zi = t | z?i) =
{
nt
i?1+? 1 ? t ? T
?
i?1+? t = T + 1
where z?i is the current seating arrangement, t is the index of the table, nt is the number
of customers at the table t, and T is the total number of occupied tables in the restaurant.
In our model, a table t has a label indicating to which rule r the table is assigned. The
label of a table is drawn from the base distribution P0.
If we marginalize over tables labeled with the same rule, we get the following
probability of choosing r given the current analysis z?i of the data:
P(ri = r | z?i) =
nr + ?P0(r)
n + ? (11)
where nr is the number of times rule r has been observed in z?i, and n is total number
of rules observed in z?i.
4.2 Sampling Methods
We wish to sample from the set of possible decompositions into rules, including com-
posed rules, for each sentence in our training data. We follow the top?down sampling
schedule discussed in Section 2 and also implement tree-level rejection sampling as a
baseline.
Our rejection sampling baseline is a form of Metropolis-Hastings where a new tree
t is resampled from a simple proposal distribution Q(t), and then either accepted or
rejected according the Metropolis-Hastings acceptance rule, as shown in Algorithm 3.
As in Algorithm 1, we use v(z, i) to denote a top?down ordering of forest variables. As in
all our experiments, Pt is the current tree probability conditioned on the current trees for
all other sentences in our corpus, using Equation (11) as the rule probability in Equation
(10).
Our proposal distribution samples each variable with uniform probability working
top?down through the forest. The proposal distribution for an entire tree is thus:
Q(t) =
?
w?z[t]
1
deg(w)
This does not correspond to a uniform distribution over entire trees for the reasons dis-
cussed in Section 2. However, the Metropolis-Hastings acceptance probability corrects
for this, and thus the algorithm is guaranteed to converge to the correct distribution in
the long term. We will show that, because the proposal distribution does not re-use any
of the variable settings from the current tree, the rejection sampling algorithm converges
more slowly in practice than the more sophisticated alternative described in Section 2.2.
We now describe in more detail our implementation of the approach of Section 2.2.
We define two operations on a hypergraph node n, SAMPLECUT and SAMPLEEDGE, to
change the sampled tree from the hypergraph. SAMPLECUT(n) chooses whether n is a
217
Computational Linguistics Volume 40, Number 1
Algorithm 3 Metropolis-Hastings sampling algorithm.
Require: A function v(z, i) returning the index of the ith variable of z in a top-down ordering of
the variables of the tree ?(z).
1: i ? 1
2: while i < |Z[?(z)]| do
3: Sample znewv(z,i) according to uniform(z
new
v(z,i) )
4: i ? i + 1
5: end while
6: z ?
{
znew w/probmin
{
1, Pt (t(z
new ))Q(t(zold ))
Pt (t(zold ))Q(t(znew ))
}
zold otherwise
segmentation point or not, deciding if two rules should merge, while SAMPLEEDGE(n)
chooses a hyperedge under n, making an entire new subtree. Algorithm 4 shows our
implementation of Algorithm 1 in terms of tree operations and the sampling operations
SAMPLEEDGE(n) and SAMPLECUT(n).
4.3 Experiments
We used a Chinese?English parallel corpus available from the Linguistic Data Consor-
tium (LDC), composed of newswire text. The corpus consists of 41K sentence pairs,
which is 1M words on the English side. We constructed phrase decomposition forests
with this corpus and ran the top?down sampling algorithm and the rejection sampling
algorithm described in Section 4.2 for one hundred iterations.We used? = 100 for every
experiment. The likelihood of the current state was calculated for every iteration. Each
setting was repeated five times, and then we computed the average likelihood for each
iteration.
Figure 5 shows a comparison of the likelihoods found by rejection sampling and
top?down sampling. As expected, we found that the likelihood converged much more
quickly with top?down sampling. Figure 6 shows a comparison between two different
versions of top?down sampling: the first experiment was run with the density factor
described in Section 2, Equation (6), and the second one was run without the density
factor. The density factor has a much smaller effect on the convergence of our algorithm
than does the move from rejection sampling to top?down sampling, such that the dif-
ference between the two curves shown in Figure 6 is not visible at the scale of Figure 5.
(The first ten iterations are omitted in Figure 6 in order to highlight the difference.) The
small difference is likely due to the fact that our trees are relatively evenly balanced,
Algorithm 4 Top?down sampling algorithm.
1: queue.push(root)
2: while queue is not empty do
3: n = queue.pop()
4: SAMPLEEDGE(n)
5: SAMPLECUT(n)
6: for each child c of node n do
7: queue.push(c)
8: end for
9: end while
218
Chung et al. Sampling Tree Fragments from Forests
Figure 5
Likelihood graphs for rejection sampling and top?down sampling.
Figure 6
Likelihood graphs for top?down sampling with and without density factor. The first ten
iterations are omitted to highlight the difference.
such that the ratio of the density factor for two trees is not significant in comparison
to the ratio of their model probabilities. Nevertheless, we do find higher likelihood
states with the density factor than without it. This shows that, in addition to providing
a theoretical guarantee that our Markov chain converges to the desired distribution Pt
in the limit, the density factor also helps us find higher probability trees in practice.
219
Computational Linguistics Volume 40, Number 1
5. Application to Machine Translation
The results of the previous section demonstrate the performance of our algorithm in
terms of the probabilities of the model it is given, but do not constitute an end-to-end
application. In this section we demonstrate its use in a complete machine translation
system, using the SCFG rules found by the sampler in a Hiero-style MT decoder. We
discuss our approach and how it relates to previous work in machine translation in
Section 5.1 before specifying the precise probability model used for our experiments in
Section 5.2, discussing a technique to speed-up the model?s burn-in in Section 5.3, and
describing our experiments in Section 5.4.
5.1 Approach
A typical pipeline for training current statistical machine translation systems consists
of the following three steps: word alignment, rule extraction, and tuning of feature
weights. Word alignment is most often performed using the models of Brown et al.
(1993) and Vogel, Ney, and Tillmann (1996). Phrase extraction is performed differently
for phrase-based (Koehn, Och, and Marcu 2003), hierarchical (Chiang 2005), and syntax-
based (Galley et al. 2004) translation models, whereas tuning algorithms are generally
independent of the translation model (Och 2003; Chiang, Marton, and Resnik 2008;
Hopkins and May 2011).
Recently, a number of efforts have been made to combine the word alignment and
rule extraction steps into a joint model, with the hope both of avoiding some of the
errors of the word-level alignment, and of automatically learning the decomposition
of sentence pairs into rules (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al.
2009; Blunsom and Cohn 2010a; Neubig et al. 2011). This approach treats both word
alignment and rule decomposition as hidden variables in an EM-style algorithm. While
these efforts have been able to match the performance of systems based on two succes-
sive steps for word alignment and rule extraction, they have generally not improved
performance enough to become widely adopted. One possible reason for this is the
added complexity and in particular the increased computation time when compared
to the standard pipeline. The accuracy of word-level alignments from the standard
GIZA++ package has proved hard to beat, in particular when large amounts of training
data are available.
Given this state of affairs, the question arises whether static word alignments can
be used to guide rule learning in a model which treats the decomposition of a sentence
pair into rules as a hidden variable. Such an approach would favor rules which are
consistent with the other sentences in the data, and would contrast with the standard
practice inHiero-style systems of simply extracting all overlapping rules consistent with
static word alignments. Constraining the search over rule decomposition with word
alignments has the potential to significantly speed up training of rule decomposition
models, overcoming one of the barriers to their widespread use. Rule decomposition
models also have the benefit of producing much smaller grammars than are achieved
when extracting all possible rules. This is desirable given that the size of translation
grammars is one of the limiting computational factors in current systems, necessitating
elaborate strategies for rule filtering and indexing.
In this section, we apply our sampling algorithm to learn rules for the Hiero trans-
lation model of Chiang (2005). Hiero is based on SCFG, with a number of constraints on
the form that rules can take. The grammar has a single nonterminal, and each rule has
220
Chung et al. Sampling Tree Fragments from Forests
at most two right-hand side nonterminals. Most significantly, Hiero allows rules with
mixed terminals and nonterminals on the right-hand side. This has the great benefit
of allowing terminals to control re-ordering between languages, but also leads to very
large numbers of valid rules during the rule extraction process. We wish to see whether,
by adding a learned model of sentence decomposition to Hiero?s original method of
leveraging fixed word-level alignments, we can learn a small set of rules in a system that
is both efficient to train and efficient to decode. Our approach of beginning with fixed
word alignments is similar to that of Sankaran, Haffari, and Sarkar (2011), although
their sampling algorithm reanalyzes individual phrases extracted with Hiero heuristics
rather than entire sentences, and produces rules with no more than one nonterminal
on the right-hand side.
Most previous works on joint word alignment and rule extraction models were
evaluated indirectly by resorting to heuristic methods to extract rules from learned
word alignment or bracketing structures (DeNero, Bouchard-Cote, and Klein 2008;
Zhang et al. 2008; Blunsom et al. 2009; Levenberg, Dyer, and Blunsom 2012), and do not
directly learn the SCFG rules that are used during decoding. In this article, we work
with lexicalized translation rules with a mix of terminals and nonterminals, and we
use the rules found by our sampler directly for decoding. Because word alignments are
fixed in our model, any improvements we observe in translation quality indicate that
our model learns how SCFG rules interplay with each other, rather than fixing word
alignment errors.
The problem of rule decomposition is not only relevant to the Hiero model.
Translation models that make use of monolingual parsing, such as string-to-tree
(Galley et al. 2004), and tree-to-string (Liu, Liu, and Lin 2006), are all known to
benefit greatly from learning composed rules (Galley et al. 2006). In the particular
case of Hiero rule extraction, although there is no explicit rule composition step, the
extracted rules are in fact ?composed rules? in the sense of string-to-tree or tree-to-
string rule extraction, because they can be further decomposed into smaller SCFG rules
that are also consistent with word alignments. Although our experiments only include
the Hiero model, the method presented in this article is also applicable to string-to-
tree and tree-to-string models, because the phrase decomposition forest presented in
Section 3 can be extended to rule learning and extraction of other syntax-based MT
models.
5.2 Model
In this section, we describe a generative model based on the Pitman-Yor process (Pitman
and Yor 1997; Teh 2006) over derivation trees consisting of composed rules. Bayesian
methods have been applied to a number of segmentation tasks in natural language pro-
cessing, including word segmentation, TSG learning, and learning machine translation
rules, as a way of controlling the overfitting produced when Expectation Maximization
would tend to prefer longer segments. However, it is important to note that the Bayesian
priors in most cases control the size and number of the clusters, but do not explicitly
control the size of rules. In many cases, this type of Bayesian prior alone is not strong
enough to overcome the preference for longer, less generalizable rules. For example,
some previous work in word segmentation (Liang and Klein 2009; Naradowsky and
Toutanova 2011) adopts a ?length penalty? to remedy this situation. Because we have
the prior knowledge that longer rules are less likely to generalize and are therefore less
likely to be a good rule, we adopt a similar scheme to control the length of rules in
our model.
221
Computational Linguistics Volume 40, Number 1
In order to explicitly control the length of our rules, we generate a rule r in two
stages. First, we draw the length of a rule |r| =  from a probability distribution defined
over positive integers. We use a Poisson distribution:
P(; ?) = ?
e??
!
Because of the factorial in the denominator, the Poisson distribution decays quickly as
 becomes larger, which is ideal for selecting rule length because we want to encourage
learning of shorter rules and learn longer rules only when there is strong evidence for
them in the data.
A separate Pitman-Yor process is defined for the rules of each length . We draw
the rule distribution G from a Pitman-Yor process, and then rules of length  are drawn
from G.
G | ?, d,P0 ? PY(?, d,P0)
r | G ? G
The first two parameters, a concentration parameter ? and a discount parameter d,
control the shape of distribution G by controlling the size and the number of clusters.
The label of the cluster is decided by the base distribution P0. Because our alignment
is fixed, we do not need a complex base distribution that differentiates better aligned
phrases from others. We use a uniform distribution where each rule of the same size
has equal probability. Since the number of possible shorter rules is smaller than that of
longer rules, we need to reflect this fact and need to have larger uniform probability
for shorter rules and smaller uniform probability for longer rules. We reuse the Poisson
probability for the base distribution, essentially assuming that the number of possible
rules of length  is 1/P(; ?).
The Pitman-Yor process gives us the following probability of choosing r of size 
given the current analysis z?i of the data:
P(ri = r | , z?i) =
nr ? Trd + (Td + ?)P0(r)
n + ?
where nr is the number of times rule r has been observed in z?i, Tr is the number of
tables (in the Chinese restaurant metaphor) labeled r, and n is the total number of rules
of length  observed in z?i. Because we have drawn the length of the rule from a Poisson
distribution, the rule length probability is multiplied by this equation in order to obtain
the probability of the rule under our model.
Keeping track of table assignments during inference requires a lot of book-keeping.
In order to simplify the implementation, instead of explicitly keeping track of the
number of tables for each rule, we estimate the number of tables using the following
equations (Huang and Renals 2010):
Tr = ndr
T =
?
r:|r|=
ndr
222
Chung et al. Sampling Tree Fragments from Forests
In order to encourage learning rules with smaller parsing complexity and rules with
mixed terminals and nonterminals, which are useful for replicating re-orderings that
are seen in the data, we made use of the concept of scope (Hopkins and Langmead
2010) in our definition of rule length. The scope of a rule is defined as the number of
pairs of adjacent nonterminals in the source language right-hand side plus the number
of nonterminals at the beginning or end of the source language right-hand side. For
example,
X ? f1X1X2f2X3, X1e1X2X3e2
has scope 2 because X1 and X2 are adjacent in the source language and X3 is at the
end of the source language right-hand side. The target side of the rule is irrelevant.
The intuition behind this definition is that it measures the number of free indices into
the source language string required during parsing, under the assumption that the
terminals provide fixed anchor points into the string. Thus a rule of scope of k can be
parsed in O(nk). We define the length of a rule to be the number of terminals in the
source and the target side plus the scope of the rule. This is equivalent to counting the
total number of symbols in the rule, but only counting a nonterminal if it contributes
to parsing complexity. For example, the length of a rule that consists only of two
consecutive nonterminals would be 3, and the length of a rule that has two consecutive
nonterminals bounded by terminals on both sides would be 3 as well. This definition
of rule length encourages rules with mixed terminals and nonterminals over rules with
only nonterminals, which tend not to provide useful guidance to the translation process
during decoding.
5.3 Stratified Sampling
We follow the same Gibbs sampler introduced in Section 4.2. The SAMPLEEDGE oper-
ation in our Gibbs sampler can be a relatively expensive operation, because the entire
subtree under a node is being changed during sampling. We observe that in a phrase
decomposition forest, lexicalized rules, which are crucial to translation quality, appear
at the bottom level of the forest. This lexicalized information propagates up the forest
as rules get composed. It is reasonable to constrain initial sampling iterations to work
only on those bottom level nodes, and then gradually lift the constraint. This not only
makes the sampler much more efficient, but also gives it a chance to focus on getting
better estimates of the more important parameters, before starting to consider nodes
at higher levels, which correspond to rules of larger size. Fortunately, as mentioned in
Section 3, each node in a phrase decomposition forest already has a unique level, with
level 1 nodes corresponding to minimal phrase pairs. We design the sampler to use a
stratified sampling process (i.e., sampling level one nodes for K iterations, then level 1
and 2 nodes for K iterations, and so on). We emphasize that when we sample for level
2 nodes, level 1 nodes are also sampled, which means parameters for the smaller rules
are given more chance to mix, and thereby settle into a more stable distribution.
In our experiments, running the first 100 iterations of sampling with regular sam-
pling techniques took us about 18 hours. However, with stratified sampling, it took
only about 6 hours. We also compared translation quality as measured by decoding
with rules from the 100th sample, and by averaging over every 10th sample. Both
sampling methods gave us roughly the same translation quality as measured in BLEU.
We therefore used stratified sampling throughout our experiments.
223
Computational Linguistics Volume 40, Number 1
5.4 Experiments
We used a Chinese?English parallel corpus available from LDC,1 composed of
newswire text. The corpus consists of 41K sentence pairs, which is 1M words on the
English side. We used a 392-sentence development set with four references for parame-
ter tuning, and a 428-sentence test set with four references for testing.2 The development
set and the test set have sentences with less than 30 words. A trigram language model
was used for all experiments. BLEU (Papineni et al. 2002) was calculated for evaluation.
5.4.1 Baseline. For our baseline system, we extract Hiero translation rules using the
heuristic method (Chiang 2007), with the standard Hiero rule extraction constraints.
We use our in-house SCFG decoder for translation with both the Hiero baseline and our
sampled grammars. Our features for all experiments include differently normalized rule
counts and lexical weightings (Koehn, Och, and Marcu 2003) of each rule. Weights are
tuned using Pairwise Ranking Optimization (Hopkins and May 2011) using the baseline
grammar and development set, then used throughout the experiments.
Because our sampling procedure results in a smaller rule table, we also establish a
no-singleton baseline to compare our results to a simple heuristic method of reducing
rule table size. The no-singleton baseline discards rules that occur only once and that
have more than one word on the Chinese side during the Hiero rule extraction process,
before counting the rules and computing feature scores.
5.4.2 Experimental Settings.
Model parameters. For all experiments, we used d = 0.5 for the Pitman-Yor discount
parameter, except where we compared the Pitman-Yor process with Dirichlet process
(d = 0). Although we have a separate Pitman-Yor process for each rule length, we used
the same ? = 5 for all rule sizes in all experiments, including Dirichlet process experi-
ments. For rule length probability, a Poisson distribution where ? = 2 was used for all
experiments.
Sampling. The samples are initialized such that all nodes in a forest are set to be seg-
mented, and a random edge is chosen under each node. For all experiments, we ran the
sampler for 100 iterations and took the sample from the last iteration to compare with
the baseline. For stratified sampling, we increased the level we sample at every 10th
iteration. We also tried ?averaging? samples, where samples from every 10th iteration
are merged to a single grammar. For averaging samples, we took the samples from the
0th iteration (initialization) to the 70th iteration at every 10th iteration.3 We decided
on the 70th iteration (last iteration of level 7 sampling) as the last iteration because
we constrained the sampler not to sample nodes whose span covers more than seven
words (for SAMPLECUT only, SAMPLECUT always segments for these nodes), and the
likelihood becomes very stable at that point.
1 We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34,
LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92,
LDC2006E24). The language model is trained on the English side of entire data (1.65M sentences,
which is 39.3M words).
2 They are from newswire portion of NIST MT evaluation data from 2004, 2005, and 2006.
3 Not including initialization has negligible effect on translation quality.
224
Chung et al. Sampling Tree Fragments from Forests
Rule extraction. Because every tree fragment in the sampled derivation represents a
translation rule, we do not need to explicitly extract the rules; we merely need to collect
them and count them. However, derivations include purely nonlexical rules that do not
conform to the rule constraints of Hiero, and which are not useful for translation. To
get rid of this type of rule, we prune every rule that has a scope greater than 2. Whereas
Hiero does not allow two adjacent nonterminals in the source side, our pruning criterion
allows some rules of scope 2 that are not allowed by Hiero. For example, the following
rule (only source side shown) has scope 2 but is not allowed by Hiero:
X ? w1X1X2w2X3
In order to see if these rules have any positive or negative effects on translation, we
compare a rule set that strictly conforms to the Hiero constraints and a rule set that
includes all the rules of scope 2 or less.
5.4.3 Results. Table 2 summarizes our results. As a general test of our probability model,
we compare the result from initialization and the 100th sample. The translation perfor-
mance of the grammar from the 100th iteration of sampling is much higher than that
of the initialization state. This shows that states with higher probability in our Markov
chain generally do result in better translation, and that the sampling process is able to
learn valuable composed rules.
In order to determine whether the composed rules learned by our algorithm are
particularly valuable, we compare them to the standard baseline of extracting all rules.
The size of the grammar taken from the single sample (100th sample) is only about 9% of
the baseline but still produces translation results that are not far worse than the baseline.
A simple way to reduce the number of rules in the baseline grammar is to remove all
rules that occur only once in the training data and that contain more than a single word
on the Chinese side. This ?no-singleton? baseline still leaves us with more rules than
our algorithm, with translation results between those of the standard baseline and our
algorithm.
We also wish to investigate the trade-off between grammar size and translation
performance that is induced by including rules from multiple steps of the sampling
process. It is helpful for translation quality to include more than one analysis of each
sentence in the final grammar in order to increase coverage of new sentences. Averaging
samples also better approximates the long-term behavior of the Markov chain, whereas
taking a single sample involves an arbitrary random choice. When we average eight
Table 2
Comparisons of decoding results.
iteration model pruning #rules dev test time (s)
Baseline heuristic Hiero 3.59M 25.5 25.1 809
No-singleton heuristic Hiero 1.09M 24.7 24.2 638
Sampled 0th (init) Pitman-Yor scope < 3 212K 19.9 19.1 489
Sampled 100th Pitman-Yor scope < 3 313K 23.9 23.3 1,214
Sampled averaged (0 to 70) Pitman-Yor scope < 3 885K 26.2 24.5 1,488
Sampled averaged (0 to 70) Pitman-Yor Hiero 785K 25.6 25.1 532
Sampled averaged (0 to 70) Dirichlet scope < 3 774K 24.6 23.8 930
225
Computational Linguistics Volume 40, Number 1
different samples, we get a larger number of rules than from a single sample, but still
only a quarter as many rules as in the Hiero baseline. The translation results with eight
samples are comparable to the Hiero baseline (not significantly different according
to 1,000 iterations of paired bootstrap resampling [Koehn 2004]). Translation results
are better with the sampled grammar than with the no-singleton method of reducing
grammar size, while the sampled grammar was smaller than the no-singleton rule set.
Thus, averaging samples seems to produce a good trade-off between grammar size and
quality.
The filtering applied to the final rule set affects both the grammar size and de-
coding speed, because rules with different terminal/nonterminal patterns have vary-
ing decoding complexities. We experimented with two methods of filtering the final
grammar: retaining rules of scope no greater than three, and the more restrictive the
Hiero constraints. We do not see a consistent difference in translation quality between
these methods, but there is a large impact in terms of speed. The Hiero constraints
dramatically speeds decoding. The following is the full list of the Hiero constraints,
taken verbatim from Chiang (2007):
 If there are multiple initial phrase pairs containing the same set of
alignments, only the smallest is kept. That is, unaligned words are
not allowed at the edges of phrases.
 Initial phrases are limited to a length of 10 words on either side.
 Rules are limited to five nonterminals plus terminals on the French side.
 Rules can have at most two nonterminals, which simplifies the decoder
implementation. This also makes our grammar weakly equivalent to an
inversion transduction grammar (Wu 1997), although the conversion
would create a very large number of new nonterminal symbols.
 It is prohibited for nonterminals to be adjacent on the French side,
a major cause of spurious ambiguity.
 A rule must have at least one pair of aligned words, so that translation
decisions are always based on some lexical evidence.
Of these constraints, the differences between the Hiero constraints and scope filtering
are: First, the Hiero constraints limit the number of nonterminals in a rule to no more
than two. Second, the Hiero constraints do not allow two adjacent nonterminals in
the source side of a rule. As discussed previously, these two differences limit Hiero
grammar to be a subset of scope 2 grammar, whereas the scope-filtered grammar retains
all scope 2 rules. Among grammars with the Hiero constraint, smaller grammars are
generally faster. The relationship between the number of rules and the decoding time is
less than linear. This is because the decoder never considers rules containing sequences
of terminals not present in the source sentence. As the number of rules grows, we see
rules with larger numbers of terminals that in turn apply to fewer input sentences.
The sampled grammar has a more pronounced effect of reducing rule table size than
decoding speed. Our sampling method may be particularly valuable for very large data
sets where grammar size can become a limiting factor.
Finally, we wish to investigate whether the added power of the Pitman-Yor process
gives any benefit over the simpler Dirichlet process prior, using the same modeling
of word length in both cases. We find better translation quality with the Pitman-Yor
226
Chung et al. Sampling Tree Fragments from Forests
process, indicating that the additional strength of the Pitman-Yor process in suppressing
infrequent rules helps prevent overfitting.
6. Conclusion
We presented a hypergraph sampling algorithm that overcomes the difficulties inherent
in computing inside probabilities in applications where the segmentation of the tree into
rules is not known.
Given parallel text with word-level alignments, we use this algorithm to learn
sentence bracketing and SCFG rule composition. Our rule learning algorithm is based
on a compact structure that represents all possible SCFG rules extracted from word-
aligned sentences pairs, and works directly with highly lexicalized model parameters.
We show that by effectively controlling overfitting with a Bayesian model, and design-
ing algorithms that efficiently sample that parameter space, we are able to learn more
compact grammars with competitive translation quality. Based on the framework we
built in this work, it is possible to explore other rule learning possibilities that are known
to help translation quality, such as learning refined nonterminals.
Our general sampling algorithm is likely to be useful in settings beyond machine
translation. One interesting application would be unsupervised or partially supervised
learning of (monolingual) TSGs, given text where the tree structure is completely or
partially unknown, as in the approach of Blunsom and Cohn (2010b).
Acknowledgments
This work was partially funded by NSF
grant IIS-0910611.
References
Blunsom, P., T. Cohn, C. Dyer, and
M. Osborne. 2009. A Gibbs sampler for
phrasal synchronous grammar induction.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 2,
pages 782?790, Singapore.
Blunsom, Phil and Trevor Cohn. 2010a.
Inducing synchronous grammars with
slice sampling. In Proceedings of the Human
Language Technology: The 11th Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 238?241, Boulder, CO.
Blunsom, Phil and Trevor Cohn. 2010b.
Unsupervised induction of tree
substitution grammars for dependency
parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,204?1,213,
Cambridge, MA.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Chappelier, Jean-Ce?dric and Martin Rajman.
2000. Monte-Carlo sampling for NP-hard
maximization problems in the framework
of weighted parsing. In Natural Language
Processing (NLP 2000), pages 106?117,
Patras.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical
machine translation. In Proceedings of the
43rd Annual Conference of the Association
for Computational Linguistics (ACL-05),
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Chiang, David, Yuval Marton, and Philip
Resnik. 2008. Online large-margin training
of syntactic and structural translation
features. In Conference on Empirical
Methods in Natural Language Processing
(EMNLP-08), pages 224?233, Honolulu, HI.
Cohn, Trevor and Phil Blunsom. 2010.
Blocked inference in Bayesian tree
substitution grammars. In Proceedings of
the 48th Annual Meeting of the Association
for Computational Linguistics (ACL-10),
pages 225?230, Uppsala.
Cohn, Trevor, Sharon Goldwater, and
Phil Blunsom. 2009. Inducing compact
but accurate tree-substitution grammars.
In Proceedings of Human Language
227
Computational Linguistics Volume 40, Number 1
Technologies: The 2009 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 548?556, Boulder, CO.
DeNero, John, Alexandre Bouchard-Cote,
and Dan Klein. 2008. Sampling alignment
structure under a Bayesian translation
model. In Conference on Empirical
Methods in Natural Language Processing
(EMNLP-08), pages 314?323, Honolulu, HI.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe,
Wei Wang, and Ignacio Thayer. 2006.
Scalable inference and training of
context-rich syntactic translation models.
In Proceedings of the International Conference
on Computational Linguistics/Association
for Computational Linguistics (COLING/
ACL-06), pages 961?968, Sydney.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the
2004 Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL-04), pages 273?280,
Boston, MA.
Hopkins, Mark and Greg Langmead.
2010. SCFG decoding without
binarization. In Proceedings of the
2010 Conference on Empirical Methods
in Natural Language Processing,
pages 646?655, Cambridge, MA.
Hopkins, Mark and Jonathan May. 2011.
Tuning as ranking. In Proceedings of the
2011 Conference on Empirical Methods
in Natural Language Processing,
pages 1,352?1,362, Edinburgh.
Huang, Songfang and Steve Renals. 2010.
Power law discounting for n-gram
language models. In Proceedings of the
IEEE International Conference on Acoustic,
Speech, and Signal Processing (ICASSP?10),
pages 5,178?5,181, Dallas, TX.
Johnson, Mark, Thomas L. Griffiths, and
Sharon Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of the 2007
Meeting of the North American Chapter
of the Association for Computational
Linguistics (NAACL-07), pages 139?146,
Rochester, NY.
Knuth, D. E. 1975. Estimating the efficiency
of backtrack programs. Mathematics of
Computation, 29(129):121?136.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation.
In 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 388?395, Barcelona.
Koehn, Philipp, Franz Josef Och, and
Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings
of the 2003 Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL-03), pages 48?54,
Edmonton.
Levenberg, Abby, Chris Dyer, and Phil
Blunsom. 2012. A Bayesian model for
learning SCFGs with discontiguous rules.
In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 223?232,
Jeju Island.
Liang, Percy and Dan Klein. 2009. Online
EM for unsupervised models. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 611?619,
Boulder, CO.
Liu, Yang, Qun Liu, and Shouxun Lin.
2006. Tree-to-string alignment template
for statistical machine translation.
In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 609?616,
Sydney.
Naradowsky, Jason and Kristina Toutanova.
2011. Unsupervised bilingual morpheme
segmentation and alignment with
context-rich hidden semi-Markov models.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 895?904, Portland, OR.
Neubig, Graham, Taro Watanabe, Eiichiro
Sumita, Shinsuke Mori, and Tatsuya
Kawahara. 2011. An unsupervised model
for joint phrase alignment and extraction.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 632?641, Portland, OR.
Och, Franz Josef. 2003. Minimum error rate
training for statistical machine translation.
In Proceedings of the 41th Annual Conference
of the Association for Computational
Linguistics (ACL-03), pages 160?167,
Sapporo.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Conference of the Association
for Computational Linguistics (ACL-02),
pages 311?318, Philadelphia, PA.
228
Chung et al. Sampling Tree Fragments from Forests
Pitman, Jim and Marc Yor. 1997. The
two-parameter Poisson-Dirichlet
distribution derived from a stable
subordinator. Annals of Probability,
25(2):855?900.
Post, Matt and Daniel Gildea. 2009. Bayesian
learning of a tree substitution grammar.
In Proceedings of the Association for
Computational Linguistics (short paper),
pages 45?48, Singapore.
Sankaran, Baskaran, Gholamreza Haffari,
and Anoop Sarkar. 2011. Bayesian
extraction of minimal SCFG rules for
hierarchical phrase-based translation.
In Proceedings of the Sixth Workshop
on Statistical Machine Translation,
pages 533?541, Edinburgh.
Teh, Yee Whye. 2006. A hierarchical
Bayesian language model based on
Pitman-Yor processes. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 985?992, Sydney.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING-96), pages 836?841, Copenhagen.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Zhang, Hao, Daniel Gildea, and David
Chiang. 2008. Extracting synchronous
grammar rules from word-level
alignments in linear time. In Proceedings
of the 22nd International Conference on
Computational Linguistics (COLING-08),
pages 1,081?1,088, Manchester.
Zhang, Hao, Chris Quirk, Robert C. Moore,
and Daniel Gildea. 2008. Bayesian learning
of non-compositional phrases with
synchronous parsing. In Proceedings of
the 46th Annual Meeting of the Association
for Computational Linguistics (ACL-08),
pages 97?105, Columbus, OH.
229

2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 543?547,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Tuning as Linear Regression
Marzieh Bazrafshan, Tagyoung Chung and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We propose a tuning method for statistical ma-
chine translation, based on the pairwise rank-
ing approach. Hopkins and May (2011) pre-
sented a method that uses a binary classifier.
In this work, we use linear regression and
show that our approach is as effective as us-
ing a binary classifier and converges faster.
1 Introduction
Since its introduction, the minimum error rate train-
ing (MERT) (Och, 2003) method has been the most
popular method used for parameter tuning in ma-
chine translation. Although MERT has nice proper-
ties such as simplicity, effectiveness and speed, it is
known to not scale well for systems with large num-
bers of features. One alternative that has been used
for large numbers of features is the Margin Infused
Relaxed Algorithm (MIRA) (Chiang et al, 2008).
MIRA works well with a large number of features,
but the optimization problem is much more compli-
cated than MERT. MIRA also involves some modi-
fications to the decoder itself to produce hypotheses
with high scores against gold translations.
Hopkins and May (2011) introduced the method
of pairwise ranking optimization (PRO), which casts
the problem of tuning as a ranking problem be-
tween pairs of translation candidates. The problem
is solved by doing a binary classification between
?correctly ordered? and ?incorrectly ordered? pairs.
Hopkins and May (2011) use the maximum entropy
classifier MegaM (Daume? III, 2004) to do the binary
classification. Their method compares well to the
results of MERT, scales better for high dimensional
feature spaces, and is simpler than MIRA.
In this paper, we use the same idea for tuning, but,
instead of using a classifier, we use linear regression.
Linear regression is simpler than maximum entropy
based methods. The most complex computation that
it needs is a matrix inversion, whereas maximum en-
tropy based classifiers use iterative numerical opti-
mization methods.
We implemented a parameter tuning program
with linear regression and compared the results to
PRO?s results. The results of our experiments are
comparable to PRO, and in many cases (also on av-
erage) we get a better maximum BLEU score. We
also observed that on average, our method reaches
the maximum BLEU score in a smaller number of
iterations.
The contributions of this paper include: First, we
show that linear regression tuning is an effective
method for tuning, and it is comparable to tuning
with a binary maximum entropy classifier. Second,
we show linear regression is faster in terms of the
number of iterations it needs to reach the best re-
sults.
2 Tuning as Ranking
The parameter tuning problem in machine transla-
tion is finding the feature weights of a linear trans-
lation model that maximize the scores of the candi-
date translations measured against reference transla-
tions. Hopkins and May (2011) introduce a tuning
method based on ranking the candidate translation
pairs, where the goal is to learn how to rank pairs of
candidate translations using a gold scoring function.
543
PRO casts the tuning problem as the problem of
ranking pairs of sentences. This method iteratively
generates lists of ?k-best? candidate translations for
each sentence, and tunes the weight vector for those
candidates. MERT finds the weight vector that max-
imizes the score for the highest scored candidate
translations. In contrast, PRO finds the weight vec-
tor which classifies pairs of candidate translations
into ?correctly ordered? and ?incorrectly ordered,?
based on the gold scoring function. While MERT
only considers the highest scored candidate to tune
the weights, PRO uses the entire k-best list to learn
the ranking between the pairs, which can help pre-
vent overfitting.
Let g(e) be a scoring function that maps each
translation candidate e to a number (score) using a
set of reference translations. The most commonly
used gold scoring function in machine translation
is the BLEU score, which is calculated for the en-
tire corpus, rather than for individual sentences. To
use BLEU as our gold scoring function, we need to
modify it to make it decomposable for single sen-
tences. One way to do this is to use a variation of
BLEU called BLEU+1 (Lin and Och, 2004), which
is a smoothed version of the BLEU score.
We assume that our machine translation system
scores translations by using a scoring function which
is a linear combination of the features:
h(e) = wTx(e) (1)
where w is the weight vector and x is the feature vec-
tor. The goal of tuning as ranking is learning weights
such that for every two candidate translations e1 and
e2, the following inequality holds:
g(e1) > g(e2) ? h(e1) > h(e2) (2)
Using Equation 1, we can rewrite Equation 2:
g(e1) > g(e2) ? wT(x(e1) ? x(e2)) > 0 (3)
This problem can be viewed as a binary classifica-
tion problem for learning w, where each data point is
the difference vector between the feature vectors of
a pair of translation candidates, and the target of the
point is the sign of the difference between their gold
scores (BLEU+1). PRO uses the MegaM classifier
to solve this problem. MegaM is a binary maximum
entropy classifier which returns the weight vector
w as a linear classifier. Using this method, Hop-
kins and May (2011) tuned the weight vectors for
various translation systems. The results were close
to MERT?s and MIRA?s results in terms of BLEU
score, and the method was shown to scale well to
high dimensional feature spaces.
3 Linear Regression Tuning
In this paper, we use the same idea as PRO for tun-
ing, but instead of using a maximum entropy clas-
sifier, we use a simple linear regression to estimate
the vector w in Equation 3. We use the least squares
method to estimate the linear regression. For a ma-
trix of data points X, and a target vector g, the
weight vector can be calculated as:
w = (XTX)?1XTg (4)
Adding L2 regularization with parameter ? has the
following closed form solution:
w = (XTX + ?I)?1XTg (5)
Following the sampling method used in PRO, the
matrices X and vector g are prepared as follows:
For each sentence,
1. Generate a list containing the k best transla-
tions of the sentence, with each translation e
scored by the decoder using a function of the
form h(e) = wTx(e).
2. Use the uniform distribution to sample n ran-
dom pairs from the set of candidate transla-
tions.
3. Calculate the gold scores g for the candidates in
each pair using BLEU+1. Keep a pair of can-
didates as a potential pair if the difference be-
tween their g scores is bigger than a threshold
t.
4. From the potential pairs kept in the previous
step, keep the s pairs that have the highest dif-
ferences in g and discard the rest.
5. For each pair e1 and e2 kept in step 4, make two
data points (x(e1)? x(e2), g(e1)? g(e2)) and
(x(e2) ? x(e1), g(e2) ? g(e1)).
544
The rows of X consist of the inputs of the data points
created in step 5, i.e., the difference vectors x(e1)?
x(e2). Similarly, the corresponding rows in g are
the outputs of the data points, i.e., the gold score
differences g(e1) ? g(e2).
One important difference between the linear re-
gression method and PRO is that rather than using
the signs of the gold score differences and doing a
binary classification, we use the differences of the
gold scores directly, which allows us to use the in-
formation about the magnitude of the differences.
4 Experiments
4.1 Setup
We used a Chinese-English parallel corpus with the
English side parsed for our experiments. The cor-
pus consists of 250K sentence pairs, which is 6.3M
words on the English side. The corpus derives from
newswire texts available from LDC.1 We used a 392-
sentence development set with four references for
parameter tuning, and a 428-sentence test set with
four references for testing. They are drawn from the
newswire portion of NIST evaluations (2004, 2005,
2006). The development set and the test set only
had sentences with less than 30 words for decoding
speed.
We extracted a general SCFG (GHKM) grammar
using standard methods (Galley et al, 2004; Wang
et al, 2010) from the parallel corpus with a mod-
ification to preclude any unary rules (Chung et al,
2011). All rules over scope 3 are pruned (Hopkins
and Langmead, 2010). A set of nine standard fea-
tures was used for the experiments, which includes
globally normalized count of rules, lexical weight-
ing (Koehn et al, 2003), and length penalty. Our
in-house decoder was used for experiments with a
trigram language model. The decoder is capable
of both CNF parsing and Earley-style parsing with
cube-pruning (Chiang, 2007).
We implemented linear regression tuning using
1We randomly sampled our data from various differ-
ent sources (LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26,
LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2006E24, LDC2006E92, LDC2006E24) The language
model is trained on the English side of entire data (1.65M sen-
tences, which is 39.3M words.)
Average of max BLEU Max BLEU
dev test dev test
Regression 27.7 (0.91) 26.4 (0.82) 29.0 27.6
PRO 26.9 (1.05) 25.6 (0.84) 28.0 27.2
Table 1: Average of maximum BLEU scores of the ex-
periments and the maximum BLEU score from the ex-
periments. Numbers in the parentheses indicate standard
of deviations of maximum BLEU scores.
the method explained in Section 3. Following Hop-
kins and May (2011), we used the following param-
eters for the sampling task: For each sentence, the
decoder generates the 1500 best candidate transla-
tions (k = 1500), and the sampler samples 5000
pairs (n = 5000). Each pair is kept as a potential
data point if their BLEU+1 score difference is big-
ger than 0.05 (t = 0.05). Finally, for each sentence,
the sampler keeps the 50 pairs with the highest dif-
ference in BLEU+1 (s = 50) and generates two data
points for each pair.
4.2 Results
We ran eight experiments with random initial weight
vectors and ran each experiment for 25 iterations.
Similar to what PRO does, in each iteration, we lin-
early interpolate the weight vector learned by the re-
gression (w) with the weight vector of the previous
iteration (wt?1) using a factor of 0.1:
wt = 0.1 ? w + 0.9 ? wt?1 (6)
For the sake of comparison, we also implemented
PRO with exactly the same parameters, and ran it
with the same initial weight vectors.
For each initial weight vector, we selected the iter-
ation at which the BLEU score on the development
set is highest, and then decoded using this weight
vector on the test set. The results of our experi-
ments are presented in Table 1. In the first column,
we show the average over the eight initial weight
vectors of the BLEU score achieved, while in the
second column we show the results from the ini-
tial weight vector with the highest BLEU score on
the development set. Thus, while the second col-
umn corresponds to a tuning process where the sin-
gle best result is retained, the first column shows the
expected behavior of the procedure on a single ini-
tial weight vector. The linear regression method has
545
 12
 14
 16
 18
 20
 22
 24
 26
 28
 0  5  10  15  20  25
BL
EU
Iteration
reg-avg
pro-avg
Figure 1: Average of eight runs of regression and PRO.
higher BLEU scores on both development and test
data for both the average over initial weights and the
maximum over initial weights.
Figure 1 shows the average of the BLEU scores
on the development set of eight runs of the experi-
ments. We observe that on average, the linear regres-
sion experiments reach the maximum BLEU score
in a smaller number of iterations. On average, linear
regression reached the maximum BLEU score after
14 iterations and PRO reached the maximum BLEU
score after 20 iterations. One iteration took several
minutes for both of the algorithms. The largest por-
tion of this time is spent on decoding the develop-
ment set and reading in the k-best list. The sampling
phase, which includes performing linear regression
or running MegaM, takes a negligible amount of
time compared to the rest of the operations.
We experimented with adding L2 regularization
to linear regression. As expected, the experiments
with regularization produced lower variance among
the different experiments in terms of the BLEU
score, and the resulting set of the parameters had a
smaller norm. However, because of the small num-
ber of features used in our experiments, regulariza-
tion was not necessary to control overfitting.
5 Discussion
We applied the idea of tuning as ranking and modi-
fied it to use linear regression instead of binary clas-
sification. The results of our experiments show that
tuning as linear regression is as effective as PRO,
and on average it reaches a better BLEU score in a
fewer number of iterations.
In comparison with MERT, PRO and linear re-
gression are different in the sense that the latter two
approaches take into account rankings of the k-best
list, whereas MERT is only concerned with separat-
ing the top 1-best sentence from the rest of the k-
best list. PRO and linear regression are similar in
the sense that both are concerned with ranking the
k-best list. Their difference lies in the fact that PRO
only uses the information on the relative rankings
and uses binary classification to rank the points; on
the contrary, linear regression directly uses the infor-
mation on the magnitude of the differences. This dif-
ference between PRO and linear regression explains
why linear regression converges faster and also may
explain the fact that linear regression achieves a
somewhat higher BLEU score. In this sense, lin-
ear regression is also similar to MIRA since MIRA?s
loss function also uses the information on the magni-
tude of score difference. However, the optimization
problem for linear regression is simpler, does not re-
quire any changes to the decoder, and therefore the
familiar MERT framework can be kept.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work was
supported by NSF grant IIS-0910611.
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08).
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon. As-
sociation for Computational Linguistics.
Hal Daume? III. 2004. Notes on CG and LM-BFGS opti-
mization of logistic regression. August.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280, Boston.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
546
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of Coling 2004,
pages 501?507, Geneva, Switzerland, Aug 23?Aug
27. COLING.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36:247?277, June.
547
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 401?406,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Terminal-Aware Synchronous Binarization
Licheng Fang, Tagyoung Chung and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We present an SCFG binarization algorithm
that combines the strengths of early termi-
nal matching on the source language side and
early language model integration on the tar-
get language side. We also examine how dif-
ferent strategies of target-side terminal attach-
ment during binarization can significantly af-
fect translation quality.
1 Introduction
Synchronous context-free grammars (SCFG) are be-
hind most syntax-based machine translation mod-
els. Efficient machine translation decoding with an
SCFG requires converting the grammar into a bina-
rized form, either explicitly, as in synchronous bina-
rization (Zhang et al, 2006), where virtual nontermi-
nals are generated for binarization, or implicitly, as
in Earley parsing (Earley, 1970), where dotted items
are used.
Given a source-side binarized SCFG with termi-
nal set T and nonterminal set N , the time complex-
ity of decoding a sentence of length n with a m-gram
language model is (Venugopal et al, 2007):
O(n3(|N | ? |T |2(m?1))K)
where K is the maximum number of right-hand-side
nonterminals. SCFG binarization serves two impor-
tant goals:
? Parsing complexity for unbinarized SCFG
grows exponentially with the number of non-
terminals on the right-hand side of grammar
rules. Binarization ensures cubic time decod-
ing in terms of input sentence length.
? In machine translation, integrating language
model states as early as possible is essential to
reducing search errors. Synchronous binariza-
tion (Zhang et al, 2006) enables the decoder to
incorporate language model scores as soon as a
binarized rule is applied.
In this paper, we examine a CYK-like syn-
chronous binarization algorithm that integrates a
novel criterion in a unified semiring parsing frame-
work. The criterion we present has explicit consider-
ation of source-side terminals. In general, terminals
in a rule have a lower probability of being matched
given a sentence, and therefore have the effect of
?anchoring? a rule and limiting its possible applica-
tion points. Hopkins and Langmead (2010) formal-
ized this concept as the scope of a rule. A rule of
scope of k can be parsed in O(nk). The scope of a
rule can be calculated by counting the number of ad-
jacent nonterminal pairs and boundary nonterminals.
For example,
A? w1BCw2D
has scope two. Building on the concept of scope,
we define a cost function that estimates the expected
number of hyperedges to be built when a particular
binarization tree is applied to unseen data. This ef-
fectively puts hard-to-match derivations at the bot-
tom of the binarization tree, which enables the de-
coder to decide early on whether an unbinarized rule
can be built or not.
We also investigate a better way to handle target-
side terminals during binarization. In theory, differ-
ent strategies should produce equivalent translation
results. However, because decoding always involves
401
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 18000
1 2 3 4 5 6 7
N
um
be
r o
f r
ul
es
Number of right-hand-side nonterminals
Total
Binarizable
Monotonic
Figure 1: Rule Statistics
pruning, we show that different strategies do have a
significant effect in translation quality.
Other works investigating alternative binarization
methods mostly focus on the effect of nonterminal
sharing. Xiao et al (2009) also proposed a CYK-
like algorithm for synchronous binarization. Appar-
ently the lack of virtual nonterminal sharing in their
decoder caused heavy competition between virtual
nonterminals, and they created a cost function to
?diversify? binarization trees, which is equivalent to
minimizing nonterminal sharing.
DeNero et al (2009b) used a greedy method to
maximize virtual nonterminal sharing on the source
side during the -LM parsing phase. They show that
effective source-side binarization can improve the ef-
ficiency of parsing SCFG. However, their method
works only on the source side, and synchronous bina-
rization is put off to the +LM decoding phase (DeN-
ero et al, 2009a).
Although these ideas all lead to faster decoding
and reduced search errors, there can be conflicts in
the constraints each of them has on the form of rules
and accommodating all of them can be a challenge.
In this paper, we present a cubic time algorithm to
find the best binarization tree, given the conflicting
constraints.
2 The Binarization Algorithm
An SCFG rule is synchronously binarizable if when
simultaneously binarizing source and target sides,
virtual nonterminals created by binarizations always
have contiguous spans on both sides (Huang, 2007).
Algorithm 1 The CYK binarization algorithm.
CYK-BINARIZE(X ? ??, ??)
for i = 0 . . . |?| ? 1 do
T [i, i + 1]? cinit(i)
for s = 2 . . . |?| do
for i = 0 . . . |?|-1 do
j ? i + s
for k = i + 1 . . . j ? 1 do
t? T [i, k] + T [k, j] + c(?i, k, j?)
T [i, j]? min(T [i, j], t)
Even with the synchronous binarization constraint,
many possible binarizations exist. Analysis of our
Chinese-English parallel corpus has shown that the
majority of synchronously binarizable rules with ar-
ity smaller than 4 are monotonic, i.e., the target-side
nonterminal permutation is either strictly increasing
or decreasing (See Figure 1). For monotonic rules,
any source-side binarization is also a permissible
synchronous binarization.
The binarization problem can be formulated as a
semiring parsing (Goodman, 1999) problem. We
define a cost function that considers different bina-
rization criteria. A CYK-like algorithm can be used
to find the best binarization tree according to the
cost function. Consider an SCFG rule X ? ??, ??,
where ? and ? stand for the source side and the tar-
get side. Let B(?) be the set of all possible bina-
rization trees for ?. With the cost function c defined
over hyperedges in a binarization tree t, the optimal
binarization tree t? is
t? = argmin
t?B(?)
?
h?t
c(h)
where c(h) is the cost of a hyperedge h in t.
The optimization problem can be solved by Al-
gorithm 1. ?i, k, j? denotes a hyperedge h that con-
nects the spans (i, k) and (k, j) to the span (i, j).
cinit is the initialization for the cost function c. We
can recover the optimal source-side binarization tree
by augmenting the algorithm with back pointers.
Binarized rules are generated by iterating over the
nodes in the optimal binarization tree, while attach-
ing unaligned target-side terminals. At each tree
node, we generate a virtual nonterminal symbol by
concatenating the source span it dominates.
We define the cost function c(h) to be a
tuple of component cost functions: c(h) =
402
(c1(h), c2(h), ...). When two costs a and b are com-
pared, the components are compared piecewise, i.e.
c < c? ? c1 < c?1 ? (c1 = c?1 ? c2 < c?2) ? . . .
If the (min,+) operators on each component cost
satisfy the semiring properties, the cost tuple is also
a semiring. Next, we describe our cost functions and
how we handle target-side terminals.
2.1 Synchronous Binarization as a Cost
We use a binary cost b to indicate whether a binariza-
tion tree is a permissible synchronous binarization.
Given a hyperedge ?i, k, j?, we say k is a permissible
split of the span (i, j) if and only if the spans (i, k)
and (k, j) are both synchronously binarizable and
the span (i, j) covers a consecutive sequence of non-
terminals on the target side. A span is synchronously
binarizable if and only if the span is of length one,
or a permissible split of the span exists. The cost b
is defined as:
b(?i, k, j?) =
{
T if k is a permissible split of (i, j)
F otherwise
binit(i) = T
Under this configuration, the semiring operators
(min,+) defined for the cost b are (?,?). Using b as
the first cost function in the cost function tuple guar-
antees that we will find a tree that is a synchronously
binarized if one exists.
2.2 Early Source-Side Terminal Matching
When a rule is being applied while parsing a sen-
tence, terminals in the rule have less chance of be-
ing matched. We can exploit this fact by taking ter-
minals into account during binarization and placing
terminals lower in the binarization tree. Consider the
following SCFG rule:
VP ? PP?? JJ NN,propose a JJ NN PP
The synchronous binarization algorithm of Zhang et
al. (2006) binarizes the rule1 by finding the right-
most binarizable points on the source side:
1We follow Wu (1997) and use square brackets for straight
rules and pointed brackets for inverted rules. We also mark
brackets with indices to represent virtual nonterminals.
VP ? PP [?? [JJ NN]1]2,[[propose a JJ NN]1]2 PP
The source side of the first binarized rule ?[]1 ? JJ
NN, propose a JJ NN? contains a very frequent non-
terminal sequence ?JJ NN?. If one were to parse
with the binarized rule, and if the virtual nontermi-
nal []1 has been built, the parser needs to continue
following the binarization tree in order to determine
whether the original rule would be matched. Further-
more, having two consecutive nonterminals adds to
complexity since the parser needs to test each split
point.
The following binarization is equally valid but in-
tegrates terminals early:
VP ? PP [[?? JJ]1 NN]2,[[propose a JJ]1 NN]2 PP
Here, the first binarized rule ?[]1 ? ?? JJ, pro-
pose a JJ? anchors on a terminal and enables earlier
pruning of the original rule.
We formulate this intuition by asking the ques-
tion: given a source-side string ?, what binarization
tree, on average, builds the smallest number of hy-
peredges when the rule is applied? This is realized
by defining a cost function e which estimates the
probability of a hyperedge ?i, k, j? being built. We
use a simple model: assume each terminal or non-
terminal in ? is matched independently with a fixed
probability, then a hyperedge ?i, k, j? is derived if
and only if all symbols in the source span (i, j) are
matched. The cost e is thus defined as2
e(?i, k, j?) =
?
i?`<j
p(?`)
einit(i) = 0
For terminals, p(?`) can be estimated by counting
the source side of the training corpus. For nontermi-
nals, we simply assume p(?`) = 1.
With the hyperedge cost e, the cost of a binariza-
tion tree t is
?
h?t e(h), i.e., the expected number of
hyperedges to be built when a particular binarization
of a rule is applied to unseen data.3 The operators
2In this definition, k does not appear on the right-hand side
of the equation because all edges leading to the same span share
the same cost value.
3Although this cost function is defined as an expectation, it
does not form an expectation semiring (Eisner, 2001) because
403
for the cost e are the usual (min,+) operators on
real numbers.
2.3 Maximizing Nonterminal Sharing
During binarization, newly created virtual nontermi-
nals are named according to the symbols (terminals
and nonterminals) that they generate. For example, a
new virtual nonterminal covering two nonterminals
NP and VP is named NP+VP. To achieve maximum
virtual nonterminal sharing, we also define a cost
function n to count the number new nonterminals
generated by a binarization tree. We keep track of
all the nonterminals that have been generated when
binarizing a rule set. When the i?th rule is being
binarized, a nonterminal is considered new if it is
previously unseen in binarizing rules 1 to i?1. This
greedy approach is similar to that of DeNero et al
(2009b). The cost function is thus defined as:
n(?i, k, j?) =
{
1 if the VT for span (i, j) is new
0 otherwise
ninit(i) = 0
The semiring operators for this cost are also
(min,+) on real numbers.
2.4 Late Target-Side Terminal Attachment
Once the optimal source-side binarization tree is
found, we have a good deal of freedom to attach
target-side terminals to adjacent nonterminals, as
long as the bracketing of nonterminals is not vio-
lated. The following example is taken from Zhang
et al (2006):
ADJP ? RB?? PP? NN,RB responsible for the NN PP
With the source-side binarization fixed, we can pro-
duce distinct binarized rules by choosing different
ways of attaching target-side terminals:
ADJP ? [RB??]1 ? [PP?]3 NN ?2,[RB]1 ? resp. for the NN [PP]3 ?2
ADJP ? [RB??]1 ? [PP?]3 NN ?2,[RB]1 resp. for the ? NN [PP]3 ?2
The first binarization is generated by attaching the
target-side terminals as low as possible in a post-
it is defined as an expectation over input strings, instead of an
expectation over trees.
order traversal of the binarization tree. The conven-
tional wisdom is that early consideration of target-
side terminals promotes early language model score
integration (Huang et al, 2009). The second bina-
rization, on the contrary, attaches the target-side ter-
minals as high as possible in the binarization tree.
We argue that this late target-side terminal attach-
ment is in fact better for two reasons.
First, as in the example above, compare the fol-
lowing two rules resulting from early attachment of
target terminals and late attachment of target termi-
nals:
??2 ? []3 NN, resp. for the NN []3
??2 ? []3 NN, NN []3
The former has a much smaller chance of sharing
the same target side with other binarized rules be-
cause on the target side, many nonterminals will be
attached without any lexical evidence. We are more
likely to have a smaller set of rules with the latter
binarization.
Second, with the presence of pruning, dynamic
programming states that are generated by rules with
many target-side terminals are disadvantaged when
competing with others in the same bin because of
the language model score. As a result, these would
be discarded earlier, even if the original unbinarized
rule has a high probability. Consequently, we lose
the benefit of using larger rules, which have more
contextual information. We show in our experiment
that late target side terminal attachment significantly
outperforms early target side terminal attachment.
Although the problem can be alleviated by pre-
computing a language model score for the original
unbinarized rule and applying the heuristic to its bi-
narized rules, this still grants no benefit over late ter-
minal attachment. We show in our experiment that
late target-side terminal attachment significantly out-
performs early target side terminal attachment.
3 Experiments
3.1 Setup
We test our binarization algorithm on an Chinese-
English translation task. We extract a GHKM gram-
mar (Galley et al, 2004) from a parallel corpus with
the parsed English side with some modification so
404
-395
-390
-385
-380
-375
-370
-365
-360
-355
 10  100
M
od
el
 S
co
re
 (lo
g-p
rob
ab
ilit
y)
Seconds / Sentence (log scale)
(b,n)-early
(b,n)-late
(b,e,n)-early
(b,e,n)-late
Figure 2: Model Scores vs. Decoding Time
 17.5
 18
 18.5
 19
 19.5
 20
 20.5
 10  100
BL
EU
Seconds / Sentence (log scale)
(b,n)-early
(b,n)-late
(b,e,n)-early
(b,e,n)-late
Figure 3: BLEU Scores vs Decoding Time
as not to extract unary rules (Chung et al, 2011).
The corpus consists of 250K sentence pairs, which
is 6.3M words on the English side. A 392-sentence
test set was to evaluate different binarizations.
Decoding is performed by a general CYK SCFG
decoder developed in-house and a trigram language
model is used. The decoder runs the CYK algorithm
with cube-pruning (Chiang, 2007). In all our exper-
iments, we discard unbinarizable rules, which have
been shown by Zhang et al (2006) to have no signif-
icant effect on translation accuracy.
3.2 Results
We first discuss effects of maximizing nonterminal
sharing. Having nonterminal sharing maximization
as a part of the cost function for binarization did
yield slightly smaller grammars. However, we could
not discern any noticeable difference or trend in
terms of BLEU score, decoding speed, or model
score when comparing translation results that used
grammars that employed nonterminal sharing max-
imization and ones that did not. In the rest of this
section, all the results we discuss use nonterminal
sharing maximization as a part of the cost function.
We then compare the effects of early target-side
terminal attachment and late attachment. Figure 2
shows model scores of each decoder run with vary-
ing bin sizes, and Figure 3 shows BLEU scores
for corresponding runs of the experiments. (b,n)-
early is conventional synchronous binarization with
early target-side terminal attachment and nontermi-
nal sharing maximization, (b,n)-late is the same set-
ting with late target-side terminal attachment. The
tuples represent cost functions that are discussed in
Section 2. The figures clearly show that late attach-
ment of target-side terminals is better. Although
Figure 3 does not show perfect correlation with Fig-
ure 2, it exhibits the same trend. The same goes for
(b,e,n)-early and (b,e,n)-late.
Finally, we examine the effect of including the
source-side terminal-aware cost function, denoted
?e? in our cost tuples. Comparing (b,e,n)-late with
(b,n)-late, we see that terminal-aware binarization
gives better model scores and BLEU scores. The
trend is the same when one compares (b,e,n)-early
and (b,n)-early.
4 Conclusion
We examined binarizing synchronous context-free
grammars within a semiring parsing framework. We
proposed binarization methods that explicitly take
terminals into consideration. We have found that al-
though binarized rules are already scope 3, we can
still do better by putting infrequent derivations as
low as possible in a binarization tree to promote
early pruning. We have also found that attaching
target side terminals as late as possible promotes
smarter pruning of rules thereby improving model
score and translation quality at decoding time. Im-
provements we discuss in this paper result in better
search, and hence better translation.
Acknowledgments We thank Hao Zhang for use-
ful discussions and the anonymous reviewers for
their helpful comments. This work was supported
by NSF grants IIS-0546554 and IIS-0910611.
405
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon, June.
Association for Computational Linguistics.
J. DeNero, A. Pauls, and D. Klein. 2009a. Asynchronous
binarization for synchronous grammars. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 141?144. Association for Computational
Linguistics.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009b. Efficient parsing for transducer grammars. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 227?235, Boulder, Colorado, June. Association
for Computational Linguistics.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
J. Eisner. 2001. Expectation semirings: Flexible EM
for learning finite-state transducers. In Proceedings of
the ESSLLI workshop on finite-state methods in NLP.
Citeseer.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proceedings
of the NAACL/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation (SSST), pages 33?40,
Rochester, NY.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In NAACL07,
Rochester, NY, April.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?403.
T. Xiao, M. Li, D. Zhang, J. Zhu, and M. Zhou. 2009.
Better synchronous binarization for machine transla-
tion. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing: Vol-
ume 1-Volume 1, pages 362?370. Association for Com-
putational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (NAACL-06), pages 256?263, New
York, NY.
406
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 413?417,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Issues Concerning Decoding with Synchronous Context-free Grammar
Tagyoung Chung, Licheng Fang and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We discuss some of the practical issues that
arise from decoding with general synchronous
context-free grammars. We examine problems
caused by unary rules and we also examine
how virtual nonterminals resulting from bina-
rization can best be handled. We also inves-
tigate adding more flexibility to synchronous
context-free grammars by adding glue rules
and phrases.
1 Introduction
Synchronous context-free grammar (SCFG) is
widely used for machine translation. There are many
different ways to extract SCFGs from data. Hiero
(Chiang, 2005) represents a more restricted form of
SCFG, while GHKM (Galley et al, 2004) uses a gen-
eral form of SCFG.
In this paper, we discuss some of the practical is-
sues that arise from decoding general SCFGs that
are seldom discussed in the literature. We focus on
parsing grammars extracted using the method put
forth by Galley et al (2004), but the solutions to
these issues are applicable to other general forms of
SCFG with many nonterminals.
The GHKM grammar extraction method produces
a large number of unary rules. Unary rules are the
rules that have exactly one nonterminal and no ter-
minals on the source side. They may be problematic
for decoders since they may create cycles, which are
unary production chains that contain duplicated dy-
namic programming states. In later sections, we dis-
cuss why unary rules are problematic and investigate
two possible solutions.
GHKM grammars often have rules with many
right-hand-side nonterminals and require binariza-
tion to ensure O(n3) time parsing. However, bina-
rization creates a large number of virtual nontermi-
nals. We discuss the challenges of, and possible so-
lutions to, issues arising from having a large num-
ber of virtual nonterminals. We also compare bina-
rizing the grammar with filtering rules according to
scope, a concept introduced by Hopkins and Lang-
mead (2010). By explicitly considering the effect
of anchoring terminals on input sentences, scope-
3 rules encompass a much larger set of rules than
Chomsky normal form but they can still be parsed in
O(n3) time.
Unlike phrase-based machine translation, GHKM
grammars are less flexible in how they can seg-
ment sentence pairs into phrases because they are
restricted not only by alignments between words in
sentence pairs, but also by target-side parse trees. In
general, GHKM grammars suffer more from data
sparsity than phrasal rules. To alleviate this issue,
we discuss adding glue rules and phrases extracted
using methods commonly used in phrase-based ma-
chine translation.
2 Handling unary rules
Unary rules are common in GHKM grammars. We
observed that as many as 10% of the rules extracted
from a Chinese-English parallel corpus are unary.
Some unary rules are the result of alignment er-
rors, but other ones might be useful. For example,
Chinese lacks determiners, and English determiners
usually remain unaligned to any Chinese words. Ex-
tracted grammars include rules that reflect this fact:
NP ? NP, the NP
NP ? NP, a NP
413
However, unary rules can be problematic:
? Unary production cycles corrupt the translation
hypergraph generated by the decoder. A hyper-
graph containing a unary cycle cannot be topo-
logically sorted. Many algorithms for parame-
ter tuning and coarse-to-fine decoding, such as
the inside-outside algorithm and cube-pruning,
cannot be run in the presence of unary cycles.
? The existence of many unary rules of the form
?NP ? NP, the NP? quickly fills a pruning bin
with guesses of English words to insert without
any source-side lexical evidence.
The most obvious way of eliminating problem-
atic unary rules would be converting grammars into
Chomsky normal form. However, this may result
in bloated grammars. In this section, we present
two different ways to handle unary rules. The first
involves modifying the grammar extraction method,
and the second involves modifying the decoder.
2.1 Modifying grammar extraction
We can modify the grammar extraction method such
that it does not extract any unary rules. Galley et al
(2004) extracts rules by segmenting the target-side
parse parse tree based on frontier nodes. We modify
the definition of a frontier node in the following way.
We label frontier nodes in the English parse tree, and
examine the Chinese span each frontier node cov-
ers. If a frontier node covers the same span as the
frontier node that immediately dominates it, then the
dominated node is no longer considered a frontier.
This modification prevents unary rules from being
extracted.
Figure 1 shows an example of an English-Chinese
sentence pair with the English side automatically
parsed. Frontier nodes in the tree in the original
GHKM rule extraction method are marked with a
box. With the modification, only the top bold-
faced NP would be considered a frontier node. The
GHKM rule extraction results in the following rules:
NPB ????, the snowy egret
NP ? NPB, NPB
PP ? NP, with NP
NP ? PP, romance PP
With the change, only the following rule is extracted:
NP
NPB
NNP
romance
PP
IN
with
NP
NPB
DT
the
JJ
snowy
NN
egret
?? ? ? ?
Figure 1: A sentence fragment pair with erroneous align-
ment and tokenization
NP ????, romance with the snowy egret
We examine the effect of this modification has on
translation performance in Section 5.
2.2 Modifying the decoder
Modifying how grammars are extracted has an ob-
vious down side, i.e., the loss of generality. In the
previous example, the modification results in a bad
rule, which is the result of bad alignments. Before
the modification, the rule set includes a good rule:
NPB ????, the snowy egret
which can be applied at test time. Because of this,
one may still want to decode with all available unary
rules. We handle unary rules inside the decoder in
the following ways:
? Unary cycle detection
The na?ve way to detect unary cycles is back-
tracking on a unary chain to see if a newly gen-
erated item has been generated before. The run-
ning time of this is constrained only by the num-
ber of possible items in a chart span. In prac-
tice, however, this is often not a problem: if all
unary derivations have positive costs and a pri-
ority queue is used to expand unary derivations,
414
only the best K unary items will be generated,
where K is the pruning constant.
? Ban negative cost unary rules
When tuning feature weights, an optimizer may
try feature weights that may give negative costs
to unary productions. This causes unary deriva-
tions to go on forever. The solution is to set
a maximum length for unary chains, or to ban
negative unary productions outright.
3 Issues with binarization
3.1 Filtering and binarization
Synchronous binarization (Zhang et al, 2006) is
an effective method to reduce SCFG parsing com-
plexity and allow early language model integration.
However, it creates virtual nonterminals which re-
quire special attention at parsing time. Alternatively,
we can filter rules that have more than scope-3 to
parse in O(n3) time with unbinarized rules. This
requires Earley (Earley, 1970) style parsing, which
does implicit binarization at decoding time. Scope-
filtering may filter out unnecessarily long rules that
may never be applied, but it may also throw out
rules with useful contextual information. In addi-
tion, scope-filtering does not accommodate early lan-
guage model state integration. We compare the two
with an experiment. For the rest of the section, we
discuss issues created by virtual nonterminals.
3.2 Handling virtual nonterminals
One aspect of grammar binarization that is rarely
mentioned is how to assign probabilities to binarized
grammar rules. The na?ve solution is to assign prob-
ability one to any rule whose left-hand side is a vir-
tual nonterminal. This maintains the original model.
However, it is generally not fair to put chart items of
virtual nonterminals and those of regular nontermi-
nals in the same bin, because virtual items have arti-
ficially low costs. One possible solution is adding a
heuristic to push up the cost of virtual items for fair
comparison.
For our experiments, we use an outside estimate
as a heuristic for a virtual item. Consider the follow-
ing rule binarization (only the source side shown):
A ? BCD : ? log(p) ? V ? BC : 0A ? VD : ? log(p)
A ? BCD is the orginal rule and ? log(p) is the cost
of the rule. In decoding time, when a chart item is
generated from the binarized rule V ? BC, we add
? log(p) to its total cost as an optimistic estimate of
the cost to build the original unbinarized rule. The
heuristic is used only for pruning purposes, and it
does not change the real cost. The idea is similar
to A* parsing (Klein and Manning, 2003). One com-
plication is that a binarized rule can arise from multi-
ple different unbinarized rules. In this case, we pick
the lowest cost among the unbinarized rules as the
heuristic.
Another approach for handling virtual nontermi-
nals would be giving virtual items separate bins and
avoiding pruning them at all. This is usually not
practical for GHKM grammars, because of the large
number of nonterminals.
4 Adding flexibility
4.1 Glue rules
Because of data sparsity, an SCFG extracted from
data may fail to parse sentences at test time. For
example, consider the following rules:
NP ? JJ NN, JJ NN
JJ ? c1, e1
JJ ? c2, e2
NN ? c3, e3
This set of rules is able to parse the word sequence
c1 c3 and c2 c3 but not c1 c2 c3, if we have not seen
?NP ? JJ JJ NN? at training time. Because SCFGs
neither model adjunction, nor are they markovized,
with a small amount of data, such problems can oc-
cur. Therefore, we may opt to add glue rules as used
in Hiero (Chiang, 2005):
S ? C, C
S ? S C, S C
where S is the goal state and C is the glue nonter-
minal that can produce any nonterminals. We re-
fer to these glue rules as the monotonic glue rules.
We rely on GHKM rules for reordering when we use
the monotonic glue rules. However, we can also al-
low glue rules to reorder constituents. Wu (1997)
presents a better-constrained grammar designed to
only produce tail-recursive parses. See Table 1 for
the complete set of rules. We refer to these rules as
ABC glue rules. These rules always generate left-
415
S ? A A ? [A B] B ? ? B A ?
S ? B A ? [B B] B ? ? A A ?
S ? C A ? [C B] B ? ? C A ?
A ? [A C] B ? ? B C ?
A ? [B C] B ? ? A C ?
A ? [C C] B ? ? C C ?
Table 1: The ABC Grammar. We follow the convention
of Wu (1997) that square brackets stand for straight rules
and angle brackets stand for inverted rules.
heavy derivations, weeding out ambiguity and mak-
ing search more efficient. We learn probabilities of
ABC glue rules by using expectation maximization
(Dempster et al, 1977) to train a word-level Inver-
sion Transduction Grammar from data.
In our experiments, depending on the configura-
tion, the decoder failed to parse about 5% of sen-
tences without glue rules, which illustrates their ne-
cessity. Although it is reasonable to believe that re-
ordering should always have evidence in data, as
with GHKM rules, we may wish to reorder based
on evidence from the language model. In our ex-
periments, we compare the ABC glue rules with the
monotonic glue rules.
4.2 Adding phrases
GHKM grammars are more restricted than the
phrase extraction methods used in phrase-based
models, since, in GHKM grammar extraction,
phrase segmentation is constrained by parse trees.
This may be a good thing, but it suffers from loss
of flexibility, and it also cannot use non-constituent
phrases. We use the method of Koehn et al (2003)
to extract phrases, and, for each phrase, we add a
rule with the glue nonterminal as the left-hand side
and the phrase pair as the right-hand side. We exper-
iment to see whether adding phrases is beneficial.
There have been other efforts to extend GHKM
grammar to allow more flexible rule extraction. Gal-
ley et al (2006) introduce composed rules where
minimal GHKM rules are fused to form larger rules.
Zollmann and Venugopal (2006) introduce a model
that allows more generalized rules to be extracted.
BLEU
Baseline + monotonic glue rules 20.99
No-unary + monotonic glue rules 23.83
No-unary + ABC glue rules 23.94
No-unary (scope-filtered) + monotonic 23.99
No-unary (scope-filtered) + ABC glue rules 24.09
No-unary + ABC glue rules + phrases 23.43
Table 2: BLEU score results for Chinese-English with
different settings
5 Experiments
5.1 Setup
We extracted a GHKM grammar from a Chinese-
English parallel corpus with the English side parsed.
The corpus consists of 250K sentence pairs, which
is 6.3M words on the English side. Terminal-aware
synchronous binarization (Fang et al, 2011) was ap-
plied to all GHKM grammars that are not scope-
filtered. MERT (Och, 2003) was used to tune pa-
rameters. We used a 392-sentence development set
with four references for parameter tuning, and a 428-
sentence test set with four references for testing. Our
in-house decoder was used for experiments with a
trigram language model. The decoder is capable
of both CNF parsing and Earley-style parsing with
cube-pruning (Chiang, 2007).
For the experiment that incorporated phrases, the
phrase pairs were extracted from the same corpus
with the same set of alignments. We have limited
the maximum size of phrases to be four.
5.2 Results
Our result is summarized in Table 2. The baseline
GHKM grammar with monotonic glue rules yielded
a worse result than the no-unary grammar with the
same glue rules. The difference is statistically signif-
icant at p < 0.05 based on 1000 iterations of paired
bootstrap resampling (Koehn, 2004).
Compared to using monotonic glue rules, using
ABC glue rules brought slight improvements for
both the no-unary setting and the scope-filtered set-
ting, but the differences are not statistically signifi-
cant. In terms of decoding speed and memory usage,
using ABC glues and monotonic glue rules were vir-
tually identical. The fact that glue rules are seldom
used at decoding time may account for why there is
416
little difference in using monotonic glue rules and us-
ing ABC glue rules. Out of all the rules that were ap-
plied to decoding our test set, less than one percent
were glue rules, and among the glue rules, straight
glue rules outnumbered inverted ones by three to
one.
Compared with binarized no-unary rules, scope-
3 filtered no-unary rules retained 87% of the rules
but still managed to have slightly better BLEU score.
However, the score difference is not statistically sig-
nificant. Because the size of the grammar is smaller,
compared to using no-unary grammar, it used less
memory at decoding time. However, decoding speed
was somewhat slower. This is because the decoder
employs Early-style dotted rules to handle unbina-
rized rules, and in order to decode with scope-3
rules, the decoder needs to build dotted items, which
are not pruned until a rule is completely matched,
thus leading to slower decoding.
Adding phrases made the translation result
slightly worse. The difference is not statistically
significant. There are two possible explanations for
this. Since there were more features to tune, MERT
may have not done a good job. We believe the
more important reason is that once a phrase is used,
only glue rules can be used to continue the deriva-
tion, thereby losing the richer information offered
by GHKM grammar.
6 Conclusion
In this paper, we discussed several issues concerning
decoding with synchronous context-free grammars,
focusing on grammars resulting from the GHKM
extraction method. We discussed different ways to
handle cycles. We presented a modified grammar
extraction scheme that eliminates unary rules. We
also presented a way to decode with unary rules in
the grammar, and examined several different issues
resulting from binarizing SCFGs. We finally dis-
cussed adding flexibility to SCFGs by adding glue
rules and phrases.
Acknowledgments We would like to thank the
anonymous reviewers for their helpful comments.
This work was supported by NSF grants IIS-
0546554 and IIS-0910611.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?21.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
Licheng Fang, Tagyoung Chung, and Daniel Gildea.
2011. Terminal-aware synchronous binarization. In
Proceedings of the ACL 2011 Conference Short Pa-
pers, Portland, Oregon, June. Association for Compu-
tational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-04, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-
rich syntactic translation models. In Proceedings of
COLING/ACL-06, pages 961?968, July.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceedings
of NAACL-03.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL-03, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395, Barcelona, Spain, July.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL-
03.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?403.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of NAACL-06, pages 256?
263, New York, NY.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. Workshop on Statistical Machine Translation,
pages 138?141.
417
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 49?57,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Factors Affecting the Accuracy of Korean Parsing
Tagyoung Chung, Matt Post and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We investigate parsing accuracy on the Ko-
rean Treebank 2.0 with a number of different
grammars. Comparisons among these gram-
mars and to their English counterparts suggest
different aspects of Korean that contribute to
parsing difficulty. Our results indicate that the
coarseness of the Treebank?s nonterminal set
is a even greater problem than in the English
Treebank. We also find that Korean?s rela-
tively free word order does not impact parsing
results as much as one might expect, but in
fact the prevalence of zero pronouns accounts
for a large portion of the difference between
Korean and English parsing scores.
1 Introduction
Korean is a head-final, agglutinative, and mor-
phologically productive language. The language
presents multiple challenges for syntactic pars-
ing. Like some other head-final languages such
as German, Japanese, and Hindi, Korean exhibits
long-distance scrambling (Rambow and Lee, 1994;
Kallmeyer and Yoon, 2004). Compound nouns are
formed freely (Park et al, 2004), and verbs have
well over 400 paradigmatic endings (Martin, 1992).
Korean Treebank 2.0 (LDC2006T09) (Han and
Ryu, 2005) is a subset of a Korean newswire corpus
(LDC2000T45) annotated with morphological and
syntactic information. The corpus contains roughly
5K sentences, 132K words, and 14K unique mor-
phemes. The syntactic bracketing rules are mostly
the same as the previous version of the treebank
(Han et al, 2001) and the phrase structure annota-
tion schemes used are very similar to the ones used
in Penn English treebank. The Korean Treebank is
constructed over text that has been morphologically
analyzed; not only is the text tokenized into mor-
phemes, but all allomorphs are neutralized.
To our knowledge, there have been only a few pa-
pers focusing on syntactic parsing of Korean. Herm-
jakob (2000) implemented a shift-reduce parser for
Korean trained on very limited (1K sentences) data,
and Sarkar and Han (2002) used an earlier version
of the Treebank to train a lexicalized tree adjoining
grammar. In this paper, we conduct a range of ex-
periments using the Korean Treebank 2.0 (hereafter,
KTB) as our training data and provide analyses that
reveal insights into parsing morphologically rich lan-
guages like Korean. We try to provide comparisons
with English parsing using parsers trained on a simi-
lar amount of data wherever applicable.
2 Difficulties parsing Korean
There are several challenges in parsing Korean com-
pared to languages like English. At the root of many
of these challenges is the fact that it is highly in-
flected and morphologically productive. Effective
morphological segmentation is essential to learning
grammar rules that can generalize beyond the train-
ing data by limiting the number of out-of-vocabulary
words. Fortunately, there are good techniques for do-
ing so. The sentences in KTB have been segmented
into basic morphological units.
Second, Korean is a pro-drop language: subjects
and objects are dropped wherever they are pragmati-
cally inferable, which is often possible given its rich
morphology. Zero pronouns are a remarkably fre-
quent phenomenon in general (Han, 2006), occuring
49
an average of 1.8 times per sentence in the KTB.
The standard approach in parsing English is to ig-
nore NULL elements entirely by removing them (and
recursively removing unary parents of empty nodes
in a bottom-up fashion). This is less of a problem in
English because these empty nodes are mostly trace
elements that denote constituent movement. In the
KTB, these elements are removed altogether and a
crucial cue to grammatical inference is often lost.
Later we will show the profound effect this has on
parsing accuracy.
Third, word order in Korean is relatively free.
This is also partly due to the richer morphology,
since morphemes (rather than word order) are used
to denote semantic roles of phrases. Consider the
following example:
?? ???? ?? ??? .
John-NOM Mary-DAT book-ACC give-PAST .
In the example, any permutation of the first three
words produces a perfectly acceptable sentence.
This freedom of word order could potentially result
in a large number of rules, which could complicate
analysis with new ambiguities. However, formal
written Korean generally conforms to a canonical
word order (SOV).
3 Initial experiments
There has been some work on Korean morphologi-
cal analysis showing that common statistical meth-
ods such as maximum entropy modeling and condi-
tional random fields perform quite well (Lee et al,
2000; Sarkar and Han, 2002; Han and Palmer, 2004;
Lee and Rim, 2005). Most claim accuracy rate over
95%. In light of this, we focus on the parsing part of
the problem utilizing morphology analysis already
present in the data.
3.1 Setup
For our experiments we used all 5,010 sentences in
the Korean Treebank (KTB), which are already seg-
mented. Due to the small size of the corpus, we used
ten-fold cross validation for all of our experiments,
unless otherwise noted. Sentences were assigned to
folds in blocks of one (i.e., fold 1 contained sen-
tences 1, 11, 21, and so on.). Within each fold, 80%
of the data was assigned to training, 10% to devel-
opment, and 10% to testing. Each fold?s vocabulary
model F1 F1?40 types tokens
Korean 52.78 56.55 6.6K 194K
English (?02?03) 71.06 72.26 5.5K 96K
English (?02?04) 72.20 73.29 7.5K 147K
English (?02?21) 71.61 72.74 23K 950K
Table 1: Parser scores for Treebank PCFGs in Korean
and English. For English, we vary the size of the training
data to provide a better point of comparison against Ko-
rean. Types and tokens denote vocabulary sizes (which
for Korean is the mean over the folds).
was set to all words occurring more than once in its
training data, with a handful of count one tokens re-
placing unknown words based on properties of the
word?s surface form (all Korean words were placed
in a single bin, and English words were binned fol-
lowing the rules of Petrov et al (2006)). We report
scores on the development set.
We report parser accuracy scores using the stan-
dard F1 metric, which balances precision and recall
of the labeled constituents recovered by the parser:
2PR/(P + R). Throughout the paper, all evalua-
tion occurs against gold standard trees that contain
no NULL elements or nonterminal function tags or
annotations, which in some cases requires the re-
moval of those elements from parse trees output by
the parser.
3.2 Treebank grammars
We begin by presenting in Table 1 scores for the
standard Treebank grammar, obtained by reading a
standard context-free grammar from the trees in the
training data and setting rule probabilities to rela-
tive frequency (Charniak, 1996). For these initial
experiments, we follow standard practice in English
parsing and remove all (a) nonterminal function tags
and (b) NULL elements from the parse trees before
learning the grammar. For comparison purposes, we
present scores from parsing the Wall Street Journal
portion of the English Penn Treebank (PTB), using
both the standard training set and subsets of it cho-
sen to be similar in size to the KTB. All English
scores are tested on section 22.
There are two interesting results in this table.
First, Korean parsing accuracy is much lower than
English parsing accuracy, and second, the accuracy
difference does not appear to be due to a difference
in the size of the training data, since reducing the
50
size of the English training data did not affect accu-
racy scores very much.
Before attempting to explain this empirically, we
note that Rehbein and van Genabith (2007) demon-
strate that the F1 metric is biased towards parse trees
with a high ratio of nonterminals to terminals, be-
cause mistakes made by the parser have a smaller
effect on the overall evaluation score.1 They rec-
ommend that F1 not be used for comparing parsing
accuracy across different annotation schemes. The
nonterminal to terminal ratio in the KTB and PTB
are 0.40 and 0.45, respectively. It is a good idea to
keep this bias in mind, but we believe that this small
ratio difference is unlikely to account for the huge
gap in scores displayed in Table 1.
The gap in parsing accuracy is unsurprising in
light of the basic known difficulties parsing Korean,
summarized earlier in the paper. Here we observe a
number of features of the KTB that contribute to this
difficulty.
Sentence length On average, KTB sentences are
much longer than PTB sentences (23 words versus
48 words, respectively). Sentence-level F1 is in-
versely correlated with sentence length, and the rel-
atively larger drop in F1 score going from column 3
to 2 in Table 1 is partially accounted for by the fact
that column 3 represents 33% of the KTB sentences,
but 92% of the English sentences.
Flat annotation scheme The KTB makes rela-
tively frequent use of very flat and ambiguous rules.
For example, consider the extreme cases of rule am-
biguity in which the lefthand side nonterminal is
present three or more times on its righthand side.
There are only three instances of such ?triple+-
recursive? NPs among the?40K trees in the training
portion of the PTB, each occurring only once.
NP? NP NP NP , CC NP
NP? NP NP NP CC NP
NP? NP NP NP NP .
The KTB is an eighth of the size of this, but has
fifteen instances of such NPs (listed here with their
frequencies):
1We thank one of our anonymous reviewers for bringing this
to our attention.
NP? NP NP NP NP (6)
NP? NP NP NP NP NP (3)
NP? NP NP NP NP NP NP (2)
NP? NP NP NP NP NP NP NP (2)
NP? SLQ NP NP NP SRQ PAD (1)
NP? SLQ NP NP NP NP SRQ PAN (1)
Similar rules are common for other nonterminals as
well. Generally, flatter rules are easier to parse with
because they contribute to parse trees with fewer
nodes (and thus fewer independent decision points).
However, the presence of a single nonterminal on
both the left and righthand side of a rule means that
the annotation scheme is failing to capture distribu-
tional differences which must be present.
Nonterminal granularity This brings us to a final
point about the granularity of the nonterminals in the
KTB. After removing function tags, there are only
43 nonterminal symbols in the KTB (33 of them
preterminals), versus 72 English nonterminals (44
of them preterminals). Nonterminal granularity is
a well-studied problem in English parsing, and there
is a long, successful history of automatically refin-
ing English nonterminals to discover distributional
differences. In light of this success, we speculate
that the disparity in parsing performance might be
explained by this disparity in the number of nonter-
minals. In the next section, we provide evidence that
this is indeed the case.
4 Nonterminal granularity
There are many ways to refine the set of nontermi-
nals in a Treebank. A simple approach suggested
by Johnson (1998) is to simply annotate each node
with its parent?s label. The effect of this is to re-
fine the distribution of each nonterminal over se-
quences of children according to its position in the
sentence; for example, a VP beneath an SBAR node
will have a different distribution over children than a
VP beneath an S node. This simple technique alone
produces a large improvement in English Treebank
parsing. Klein and Manning (2003) expanded this
idea with a series of experiments wherein they manu-
ally refined nonterminals to different degrees, which
resulted in parsing accuracy rivaling that of bilexi-
calized parsing models of the time. More recently,
Petrov et al (2006) refined techniques originally
proposed by Matsuzaki et al (2005) and Prescher
51
SBJ subject with nominative case marker
OBJ complement with accusative case marker
COMP complement with adverbial postposition
ADV NP that function as adverbial phrase
VOC noun with vocative case maker
LV NP coupled with ?light? verb construction
Table 2: Function tags in the Korean treebank
model F1 F1?40
Korean
coarse 52.78 56.55
w/ function tags 56.18 60.21
English (small)
coarse 72.20 73.29
w/ function tags 70.50 71.78
English (standard)
coarse 71.61 72.74
w/ function tags 72.82 74.05
Table 3: Parser scores for Treebank PCFGs in Korean
and English with and without function tags. The small
English results were produced by training on ?02?04.
(2005) for automatically learning latent annotations,
resulting in state of the art parsing performance with
cubic-time parsing algorithms.
We begin this section by conducting some sim-
ple experiments with the existing function tags, and
then apply the latent annotation learning procedures
of Petrov et al (2006) to the KTB.
4.1 Function tags
The KTB has function tags that mark grammatical
functions of NP and S nodes (Han et al, 2001),
which we list all of them in Table 2. These function
tags are principally grammatical markers. As men-
tioned above, the parsing scores for both English
and Korean presented in Table 1 were produced with
grammars stripped of their function tags. This is
standard practice in English, where the existing tags
are known not to help very much. Table 3 presents
results of parsing with grammars with nonterminals
that retain these function tags (we include results
from Section 3 for comparison). Note that evalua-
tion is done against the unannotated gold standard
parse trees by removing the function tags after pars-
ing with them.
The results for Korean are quite pronounced:
we see a nearly seven-point improvement when re-
taining the existing tags. This very strongly sug-
gests that the KTB nonterminals are too coarse
when stripped of their function tags, and raises the
question of whether further improvement might be
gained from latent annotations.
The English scores allow us to make another point.
Retaining the provided function tags results in a
solid performance increase with the standard train-
ing corpus, but actually hurts performance when
training on the small dataset. Note clearly that this
does not suggest that parsing performance with the
grammar from the small English data could not be
improved with latent annotations (indeed, we will
show that they can), but only that the given annota-
tions do not help improve parsing accuracy. Taking
the Korean and English accuracy results from this ta-
ble together provides another piece of evidence that
the Korean nonterminal set is too coarse.
4.2 Latent annotations
We applied the latent annotation learning procedures
of Petrov et al2 to refine the nonterminals in the
KTB. The trainer learns refinements over the coarse
version of the KTB (with function tags removed). In
this experiment, rather than doing 10-fold cross vali-
dation, we split the KTB into training, development,
and test sets that roughly match the 80/10/10 splits
of the folds:
section file IDs
training 302000 to 316999
development 317000 to 317999
testing 320000 to 320999
This procedure results in grammars which can then
be used to parse new sentences. Table 4 displays the
parsing accuracy results for parsing with the gram-
mar (after smoothing) at the end of each split-merge-
smooth cycle.3 The scores in this table show that,
just as with the PTB, nonterminal refinement makes
a huge difference in parser performance.
Again with the caveat that direct comparison of
parsing scores across annotation schemes must be
taken loosely, we note that the KTB parsing accu-
racy is still about 10 points lower than the best ac-
2http://code.google.com/p/berkeleyparser/
3As described in Petrov et al (2006), to score a parse tree
produced with a refined grammar, we can either take the Viterbi
derivation or approximate a sum over derivations before project-
ing back to the coarse tree for scoring.
52
Viterbi max-sum
cycle F1 F1?40 F1 F1?40
1 56.93 61.11 61.04 64.23
2 63.82 67.94 66.31 68.90
3 69.86 72.83 72.85 75.63
4 74.36 77.15 77.18 78.18
5 78.07 80.09 79.93 82.04
6 78.91 81.55 80.85 82.75
Table 4: Parsing accuracy on Korean test data from the
grammars output by the Berkeley state-splitting grammar
trainer. For comparison, parsing all sentences of ?22 in
the PTB with the same trainer scored 89.58 (max-sum
parsing with five cycles) with the standard training corpus
and 85.21 when trained on ?2?4.
curacy scores produced in parsing the PTB which,
in our experiments, were 89.58 (using max-sum to
parse all sentences with the grammar obtained after
five cycles of training).
An obvious suspect for the difference in parsing
accuracy with latent grammars between English and
Korean is the difference in training set sizes. This
turns out not to be the case. We learned latent anno-
tations on sections 2?4 of the PTB and again tested
on section 22. The accuracy scores on the test set
peak at 85.21 (max-sum, all sentences, five cycles of
training). This is about five points lower than the En-
glish grammar trained on sections 2?21, but is still
over four points higher than the KTB results.
In the next section, we turn to one of the theoret-
ical difficulties with Korean parsing with which we
began the paper.
5 NULL elements
Both the PTB and KTB include many NULL ele-
ments. For English, these elements are traces de-
noting constituent movement. In the KTB, there
are many more kinds of NULL elements, in includ-
ing trace markers, zero pronouns, relative clause re-
duction, verb deletions, verb ellipsis, and other un-
known categories. Standard practice in English pars-
ing is to remove NULL elements in order to avoid
the complexity of parsing with ?-productions. How-
ever, another approach to parsing that avoids such
productions is to retain the NULL elements when
reading the grammar; at test time, the parser is given
sentences that contain markers denoting the empty
elements. To evaluate, we remove these elements
model F1 F1?40 tokens
English (standard training corpus)
coarse 71.61 72.74 950K
w/ function tags 72.82 74.05 950K
w/ NULLs 73.29 74.38 1,014K
Korean
w/ verb ellipses 52.85 56.52 3,200
w/ traces 55.88 59.42 3,868
w/ r.c. markers 56.74 59.87 3,794
w/ zero pronouns 57.56 61.17 4,101
latent (5) w/ NULLs 89.56 91.03 22,437
Table 5: Parser scores for Treebank PCFGs in English
and Korean with NULL elements. Tokens denotes the
number of words in the test data. The latent grammar
was trained for five iterations.
from the resulting parse trees output by the parser
and compare against the stripped-down gold stan-
dard used in previous sections, in order to provide
a fair point of comparison.
Parsing in this manner helps us to answer the ques-
tion of how much easier or more difficult parsing
would be if the NULL elements were present. In
this section, we present results from a variety of ex-
periments parsing will NULL tokens in this manner.
These results can be seen in Table 5. The first ob-
servation from this table is that in English, retaining
NULL elements makes a few points difference.
The first four rows of the KTB portion of Table 5
contains results with retaining different classes of
NULL elements, one at a time, according to the man-
ner described above. Restoring deleted pronouns
and relative clause markers has the largest effect,
suggesting that the absence of these optional ele-
ments removes key cues needed for parsing.
In order to provide a more complete picture of
the effect of empty elements, we train the Berkeley
latent annotation system on a version of the KTB
in which all empty elements are retained. The fi-
nal row of Table 5 contains the score obtained when
evaluating parse trees produced from parsing with
the grammar after the fifth iteration (after which per-
formance began to fall). With the empty elements,
we have achieved accuracy scores that are on par
with the best accuracy scores obtained parsing the
English Treebank.
53
6 Tree substitution grammars
We have shown that coarse labels and the prevalence
of NULL elements in Korean both contribute to pars-
ing difficulty. We now turn to grammar formalisms
that allow us to work with larger fragments of parse
trees than the height-one rules of standard context-
free grammars. Tree substitution grammars (TSGs)
have been shown to improve upon the standard En-
glish Treebank grammar (Bod, 2001) in parser ac-
curacy, and more recently, techniques for inferring
TSG subtrees in a Bayesian framework have enabled
learning more efficiently representable grammars,
permitting some interesting analysis (O?Donnell et
al., 2009; Cohn et al, 2009; Post and Gildea, 2009).
In this section, we try parsing the KTB with TSGs.
We experiment with different methods of learning
TSGs to see whether they can reveal any insights
into the difficulties parsing Korean.
6.1 Head rules
TSGs present some difficulties in learning and rep-
resentation, but a simple extraction heuristic called
a spinal grammar has been shown to be very use-
ful (Chiang, 2000; Sangati and Zuidema, 2009; Post
and Gildea, 2009). Spinal subtrees are extracted
from a parse tree by using a set of head rules to
maximally project each lexical item (a word or mor-
pheme). Each node in the parse tree having a differ-
ent head from its parent becomes the root of a new
subtree, which induces a spinal TSG derivation in
the parse tree (see Figure 1). A probabilistic gram-
mar is derived by taking counts from these trees,
smoothing them with counts of all depth-one rules
from the same training set, and setting rule probabil-
ities to relative frequency.
This heuristic requires a set of head rules, which
we present in Table 6. As an evaluation of our rules,
we list in Table 7 the accuracy results for parsing
with spinal grammars extracted using the head rules
we developed as well as with two head rule heuris-
tics (head-left and head-right). As a point of compar-
ison, we provide the same results for English, using
the standard Magerman/Collins head rules for En-
glish (Magerman, 1995; Collins, 1997). Function
tags were retained for Korean but not for English.
We observe a number of things from Table 7.
First, the relative performance of the head-left and
NT RC rule
S SFN second rightmost child
VV EFN rightmost XSV
VX EFN rightmost VJ or CO
ADJP EFN rightmost VJ
CV EFN rightmost VV
LV EFN rightmost VV
NP EFN rightmost CO
VJ EFN rightmost XSV or XSJ
VP EFN rightmost VX, XSV, or VV
? ? rightmost child
Table 6: Head rules for the Korean Treebank. NT is the
nonterminal whose head is being determined, RC identi-
fies the label of its rightmost child. The default is to take
the rightmost child as the head.
head-right spinal grammars between English and
Korean capture the linguistic fact that English is pre-
dominantly head-first and Korean is predominantly
head-final. In fact, head-finalness in Korean was so
strong that our head rules consist of only a handful
of exceptions to it. The default rule makes heads
of postpositions (case and information clitics) such
as dative case marker and topic marker. It is these
words that often have dependencies with words in
the rest of the sentence. The exceptions concern
predicates that occur in the sentence-final position.
As an example, predicates in Korean are composed
of several morphemes, the final one of which indi-
cates the mood of the sentence. However, this mor-
pheme often does not require any inflection to re-
flect long-distance agreement with the rest of the
sentence. Therefore, we choose the morpheme that
would be considered the root of the phrase, which
in Korean is the verbalization/adjectivization suf-
fix, verb, adjective, auxiliary predicate, and copula
(XSV, XSJ, VV, VJ, VX, CO). These items often in-
clude the information about valency of the predicate.
Second, in both languages, finer-grained specifi-
cation of head rules results in performance above
that of the heuristics (and in particular, the head-
left heuristic for English and head-right heuristic for
Korean). The relative improvements in the two lan-
guages are in line with each other: significant, but
not nearly as large as the difference between the
head-left and head-right heuristics.
Finally, we note that the test results together sug-
gest that parsing with spinal grammars may be a
54
(a) TOP
S
NP-SBJ
NPR
???
NNC
??
PAU
?
VP
NP-ADV
DAN
?
NNC
?
VP
VV
NNC
??
XSV
??
EPF
?
EFN
?
SFN
.
(b) S
NP-SBJ
NPR
???
NNC PAU
VP SFN
(c) S
NP-SBJ VP SFN
.
Figure 1: (a) A KTB parse tree; the bold nodes denote the top-level spinal subtree using our head selection rules. (b)
The top-level spinal subtree using the head-left and (c) head-right extraction heuristics. A gloss of the sentence is
Doctor Schwartz was fired afterward.
model F1 F1?40 size
Korean
spinal (head left) 59.49 63.33 49K
spinal (head right) 66.05 69.96 29K
spinal (head rules) 66.28 70.61 29K
English
spinal (head left) 77.92 78.94 158K
spinal (head right) 72.73 74.09 172K
spinal (head rules) 78.82 79.79 189K
Table 7: Spinal grammar scores on the KTB and on PTB
section 22.
good evaluation of a set of head selection rules.
6.2 Induced tree substitution grammars
Recent work in applying nonparametric machine
learning techniques to TSG induction has shown that
the resulting grammars improve upon standard En-
glish treebank grammars (O?Donnell et al, 2009;
Cohn et al, 2009; Post and Gildea, 2009). These
techniques use a Dirichlet Process prior over the sub-
tree rewrites of each nonterminal (Ferguson, 1973);
this defines a model of subtree generation that pro-
duces new subtrees in proportion to the number of
times they have previously been generated. Infer-
ence under this model takes a treebank and uses
Gibbs sampling to determine how to deconstruct a
parse tree into a single TSG derivation. In this sec-
tion, we apply these techniques to Korean.
This TSG induction requires one to specify a base
measure, which assigns probabilities to subtrees be-
ing generated for the first time in the model. One
base measure employed in previous work scored a
subtree by multiplying together the probabilities of
the height-one rules inside the subtree with a ge-
ometric distribution on the number of such rules.
Since Korean is considered to be a free word-order
language, we modified this base measure to treat the
children of a height-one rule as a multiset (instead of
a sequence). This has the effect of producing equiva-
lence classes among the sets of children of each non-
terminal, concentrating the mass on these classes in-
stead of spreading it across their different instantia-
tions.
To build the sampled grammars, we initialized the
samplers from the best spinal grammar derivations
and ran them for 100 iterations (once again, func-
tion tags were retained). We then took the state of
the training data at every tenth iteration, smoothed
together with the height-one rules from the standard
Treebank. The best score on the development data
for a sampled grammar was 68.93 (all sentences)
and 73.29 (sentences with forty or fewer words):
well above the standard Treebank scores from ear-
lier sections and above the spinal heuristics, but well
below the scores produced by the latent annotation
learning procedures (a result that is consistent with
English).
This performance increase reflects the results for
English demonstrated in the above works. We see a
large performance increase above the baseline Tree-
bank grammar, and a few points above the best
spinal grammar. One nice feature of these induced
TSGs is that the rules learned lend themselves to
analysis, which we turn to next.
6.3 Word order
In addition to the base measure mentioned above,
we also experimented with the standard base mea-
55
NP
NPR NNC
??
NNU NNX
?
Figure 2: Example of a long distance dependency learned
by TSG induction.
sure proposed by Cohn et al and Post & Gildea, that
treats the children of a nonterminal as a sequence.
The grammars produced sampling under a model
with this base measure were not substantively differ-
ent from those of the unordered base measure. A par-
tial explanation for this is that although Korean does
permit a significant amount of reordering relative to
English, the sentences in the KTB come from writ-
ten newswire text, where word order is more stan-
dardized. Korean sentences are characterized as hav-
ing a subject-object-verb (SOV) word order. There
is some flexibility; OSV, in particular, is common
in spoken Korean. In formal writing, though, SOV
word order is overwhelmingly preferred. We see this
reflected in the KTB, where SOV sentences are 63.5
times more numerous that OSV among sentences
that have explicitly marked both the subject and the
object. However, word order is not completely fixed
even in the formal writing. NP-ADV is most likely
to occur right before the VP it modifies, but can be
moved earlier. For example,
S? NP-SBJ NP-ADV VP
is 2.4 times more frequent than the alternative with
the order of the NPs reversed.
Furthermore, the notion of free(er) word order
does not apply to all constituents. An example is
nonterminals directly above preterminals. A Korean
verb may have up to seven affixes; however, they al-
ways agglutinate in a fixed order.
6.4 Long distance dependencies
The TSG inference procedure can be thought of
as discovering structural collocations in parse trees.
The model prefers subtrees that are common in the
data set and that comprise highly probable height-
one rules. The parsing accuracy of these grammars
is well below state of the art, but the grammars are
smaller, and the subtrees learned can help us analyze
the parse structure of the Treebank. One particular
class of subtree is one that includes multiple lexical
items with intervening nonterminals, which repre-
sent long distance dependencies that commonly co-
occur. In Korean, a certain class of nouns must ac-
company a particular class of measure word (a mor-
pheme) when counting the noun. In the example
shown in Figure 2, (NNC ??) (members of as-
sembly) is followed by NNU, which expands to in-
dicate ordinal, cardinal, and numeral nouns; NNU is
in turn followed by (NNX?), the politeness neutral
measure word for counting people.
7 Summary & future work
In this paper, we addressed several difficult aspects
of parsing Korean and showed that good parsing ac-
curacy for Korean can be achieved despite the small
size of the corpus.
Analysis of different parsing results from differ-
ent grammatical formalisms yielded a number of
useful observations. We found, for example, that the
set of nonterminals in the KTB is not differentiated
enough for accurate parsing; however, parsing accu-
racy improves substantially from latent annotations
and state-splitting techniques that have been devel-
oped with English as a testbed. We found that freer
word order may not be as important as might have
been thought from basic a priori linguistic knowl-
edge of Korean.
The prevalence of NULL elements in Korean is
perhaps the most interesting difficulty in develop-
ing good parsing approaches for Korean; this is
a key difference from English parsing that to our
knowledge is not addressed by any available tech-
niques. One potential approach is a special an-
notation of parents with deleted nodes in order to
avoid conflating rewrite distributions. For example,
S ? VP is the most common rule in the Korean
treebank after stripping away empty elements; how-
ever, this is a result of condensing the rule S? (NP-
SBJ *pro*) VP and S?VP, which presumably have
different distributions. Another approach would be
to attempt automatic recovery of empty elements as
a pre-processing step.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work
was supported by NSF grants IIS-0546554 and ITR-
0428020.
56
References
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Proc. ACL,
Toulouse, France, July.
Eugene Charniak. 1996. Tree-bank grammars. In Proc.
of the National Conference on Artificial Intelligence,
pages 1031?1036.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proc. ACL, Hong Kong.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proc. NAACL.
Michael Collins. 1997. Three penerative, lexicalised
models for statistical parsing. In Proc. ACL/EACL.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Mathemat-
ical Statistics, 1(2):209?230.
Chung-Hye Han and Martha Palmer. 2004. A mor-
phological tagger for Korean: Statistical tagging com-
bined with corpus-based morphological rule applica-
tion. Machine Translation, 18(4):275?297.
Na-Rae Han and Shijong Ryu. 2005. Guidelines for
Penn Korean Treebank version 2.0. Technical report,
IRCS, University of Pennsylvania.
Chung-hye Han, Na-Rae Han, and Eon-Suk Ko. 2001.
Bracketing guidelines for Penn Korean Treebank.
Technical report, IRCS, University of Pennsylvania.
Na-Rae Han. 2006. Korean zero pronouns: analysis and
resolution. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA, USA.
Ulf Hermjakob. 2000. Rapid parser development: a ma-
chine learning approach for Korean. In Proc. NAACL,
pages 118?123, May.
Mark Johnson. 1998. PCFGmodels of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and SinWon Yoon. 2004. Tree-local
MCTAG with shared nodes: Word order variation in
German and Korean. In Proc. TAG+7, Vancouver,
May.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proc. ACL.
Do-Gil Lee and Hae-Chang Rim. 2005. Probabilistic
models for Korean morphological analysis. In Com-
panion to the Proceedings of the International Joint
Conference on Natural Language Processing, pages
197?202.
Sang-zoo Lee, Jun-ichi Tsujii, and Hae-Chang Rim.
2000. Hidden markov model-based Korean part-of-
speech tagging considering high agglutinativity, word-
spacing, and lexical correlativity. In Proc. ACL.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ACL.
Samuel E. Martin. 1992. Reference Grammar of Korean:
A Complete Guide to the Grammar and History of the
Korean Language. Tuttle Publishing.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ACL, Ann Arbor, Michigan.
Timothy J. O?Donnell, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009. Fragment grammar: Exploring
reuse in hierarchical generative processes. Technical
report, MIT.
Seong-Bae Park, Jeong-Ho Chang, and Byoung-Tak
Zhang. 2004. Korean compound noun decomposition
using syllabic information only. In Computational
Linguistics and Intelligent Text Processing (CICLing),
pages 146?157.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING/ACL, Syd-
ney, Australia, July.
Matt Post and Daniel Gildea. 2009. Bayesian learning of
a tree substitution grammar. In Proc. ACL, Singapore,
Singapore, August.
Detlef Prescher. 2005. Inducing head-driven PCFGs
with latent heads: Refining a tree-bank grammar for
parsing. Machine Learning: ECML 2005, pages 292?
304.
Owen Rambow and Young-Suk Lee. 1994. Word order
variation and tree-adjoining grammar. Computational
Intelligence, 10:386?400.
Ines Rehbein and Josef van Genabith. 2007. Eval-
uating evaluation measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
(NODALIDA).
Federico Sangati and Willem Zuidema. 2009. Unsuper-
vised methods for head assignments. In Proc. EACL.
Anoop Sarkar and Chung-hye Han. 2002. Statistical
morphological tagging and parsing of Korean with an
LTAG grammar. In Proc. TAG+6.
57
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 468?479,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Direct Error Rate Minimization for Statistical Machine Translation
Tagyoung Chung?
University of Rochester
Rochester, NY 14627, USA
chung@cs.rochester.edu
Michel Galley
Microsoft Research
Redmond, WA 98052, USA
mgalley@microsoft.com
Abstract
Minimum error rate training is often the pre-
ferred method for optimizing parameters of
statistical machine translation systems. MERT
minimizes error rate by using a surrogate rep-
resentation of the search space, such as N -
best lists or hypergraphs, which only offer
an incomplete view of the search space. In
our work, we instead minimize error rate di-
rectly by integrating the decoder into the min-
imizer. This approach yields two benefits.
First, the function being optimized is the true
error rate. Second, it lets us optimize param-
eters of translations systems other than stan-
dard linear model features, such as distortion
limit. Since integrating the decoder into the
minimizer is often too slow to be practical, we
also exploit statistical significance tests to ac-
celerate the search by quickly discarding un-
promising models. Experiments with a phrase-
based system show that our approach is scal-
able, and that optimizing the parameters that
MERT cannot handle brings improvements to
translation results.
1 Introduction
Minimum error rate training (Och, 2003) is a com-
mon method for optimizing linear model parame-
ters, which is an important part of building good ma-
chine translation systems. MERT minimizes an arbi-
trary loss function, usually an evaluation metric such
as BLEU (Papineni et al, 2002) or TER (Snover
et al, 2006) from a surrogate representation of the
search space, such as the N -best candidate transla-
tions of a development set. Much of the recent work
? This research was conducted during the author?s intern-
ship at Microsoft Research.
on minimum error rate training focused on improv-
ing the method by Och (2003). Recent efforts ex-
tended MERT to work on lattices (Macherey et al,
2008) and hypergraphs (Kumar et al, 2009). Ran-
dom restarts and random walks (Moore and Quirk,
2008) are commonly used to combat the fact the
search space is highly non-convex, often with mul-
tiple minima.
Several problems still remain with MERT, three
of which are addressed by this work. First, the N -
best error surface explored by MERT is generally
not the same as the true error surface, which means
that the error rate at an optimum1 of the N -best er-
ror surface is not guaranteed to be any close to an
optimum of the true error surface. Second, most
SMT decoders make search errors, yet MERT ig-
nores the fact that the error surface of an error-prone
decoder differs from the one of an exact decoder
(Chang and Collins, 2011). MERT calculates an en-
velope from candidate translations and assumes all
translations on the envelope are reachable by the de-
coder, but these translations may become unreach-
able due to search errors. Third, MERT is only used
to tune linear model parameters, yet SMT systems
have many free decoder parameters?such as distor-
tion limit and beam size?that are not handled by
MERT. MERT does not provide a principled way to
set these parameters.
In order to overcome these issues, we explore the
application of direct search methods (Wright, 1995)
to SMT. To do this, we integrate the decoder and
the evaluation metric inside the objective function,
1The optimum found by MERT (Och, 2003) is generally not
globally optimal. An alternative that optimizes N -best lists ex-
actly is presented by Galley and Quirk (2011), and we do not
discuss it further here.
468
which takes source sentences and a set of weights as
inputs, and outputs the evaluation score (e.g., BLEU
score) computed on the decoded sentences. Since it
is impractical to calculate derivatives of this func-
tion, we use derivative-free optimization methods
such as the downhill simplex method (Nelder and
Mead, 1965) and Powell?s method (Powell, 1964),
which generally handle such difficult search condi-
tions relatively well. This approach confers several
benefits over MERT. First, the function being opti-
mized is the true error rate. Second, integrating the
decoder inside the objective function forces the op-
timizer to account for possible search errors. Third,
contrary to MERT, our approach does not require in-
put parameters to be those of a linear model, so our
approach can tune a broader range of features, in-
cluding non-linear and hidden-state parameters (e.g.,
distortion limit, beam size, and weight vector ap-
plied to future cost estimates).
In this paper, we make direct search reasonably
fast thanks to two speedup techniques. First, we
use a model selection acceleration technique called
racing (Moore and Lee, 1994) in conjunction with
randomization tests (Riezler and Maxwell, 2005) to
avoid decoding the entire development set at each
function evaluation. This approach discards the
current model whenever performance on the trans-
lated subset of the development data is deemed sig-
nificantly worse in comparison to the current best
model. Second, we store and re-use search graphs
across function evaluations, which eliminates some
of the redundancy of regenerating the same transla-
tions in different optimization steps.
Our experiments with a strong phrase-based trans-
lation system show that the direct search approach is
an effective alternative to MERT. The speed of direct
search is generally comparable to MERT, and trans-
lation accuracy is generally superior. The non-linear
and hidden-state features tuned in this work bring
gains on three language pairs, with improvements
ranging between 0.27 and 0.35 BLEU points.
2 Direct error rate minimization
Most current machine translation systems use a log-
linear model:
p(e|f) ? exp
(
?
i
?ihi(e, f)
)
where f is a source sentence, e is a target sentence,
hi is a feature function, and ?i is the weight of this
feature. Given a source sentence f , finding the best
target sentence e? according to the model is a search
problem, which is called decoding:
e? = argmax
e
exp
(
?
i
?ihi(e, f)
)
The target sentence e? is automatically evaluated
against a reference translation r using any metric
that is known to be relatively well correlated with
human judgment, such as BLEU or TER. Let us re-
fer to such error function as E
(
?
)
. Then, the process
of finding the best set of weights ?? according to an
error function E is another search:
?? = argmin
?
E
(
r; argmax
e
exp
(
?
i
?ihi(e, f)
)
)
The typical MERT process solves the problem in an
iterative fashion. At each step i, it produces N -best
lists by decoding with ??i, then uses these lists to
find ??i+1. Och (2003) presents an efficient multi-
directional line search algorithm, which is based on
the fact that the error count along each line is piece-
wise constant and thus easy to optimize exactly. The
process is repeated until a certain convergence crite-
rion is met, or until no new candidate sentences are
added to the pool. The left side of Figure 1 summa-
rizes this process.
Though simple and effective, there are several lim-
itations to this approach. The primary reason is that
it can only tune parameters that are part of the log-
linear model. Aside from having parameters from
the log-linear model, decoders generally have free
parameters ? that needs to be set manually, such
as beam size and distortion limit. These decoder-
related parameters have complex interactions with
linear model parameters, thus, ideally, we would
want to tune them jointly with decoder parameters
such as distortion limit.
Direct search addresses these problems by includ-
ing all feature parameters and all decoder-related pa-
rameters within the optimization framework. Fig-
ure 1 contrasts MERT with direct search. Rather
than optimizing candidate pools of translations, di-
rect search treats the decoder and the evaluation tool
469
decoder
candidate
pool
??
BLEU
optimization
input f
other params ?
model params ?
output e
repeat
decoder
??, ??
1-best BLEU
optimization
input f
model params ?
other params ?
Figure 1: Comparison of MERT (left) and direct search (right).
as a single function:
?(f, r;?,?) = E
(
r; argmax
e
exp
(
?
i
?ihi(e, f)
)
)
Then, it uses an optimization method to minimize
the function:
argmin
?,?
?(f, r;?,?)
This formulation solves the problem mentioned pre-
viously, since we jointly optimize ? and ?, thus
accounting for the dependencies between the two.
However, there are two problems to address with di-
rect error minimization. First, this approach requires
the entire development set to be re-decoded every
time the function is evaluated, which can be pro-
hibitively expensive. To address this problem, we
present several methods to speed up the search pro-
cess in Section 5. Second, since the gradient of stan-
dard evaluation metrics such as BLEU is not known
and since methods for estimating the gradient numer-
ically require too many function evaluations, we can-
not use common search methods that use derivatives
of a function. Therefore, we need robust derivative-
free optimization methods. We discuss such opti-
mization methods in Section 3.
3 Derivative-free optimization
As discussed in the previous sections, we need to
rely on derivative-free optimization methods for di-
rect search. We consider two such optimization
methods:
Powell?s method For each iteration, Powell?s
method tries to find a good direction along which the
function can be minimized. This direction is deter-
mined by searching along each standard base vector.
Then, a line search is performed along the direction
by using line search methods such as golden section
search or Fibonacci search. The process is repeated
until convergence. We implement the golden sec-
tion search as presented by Press et al (1992) in our
experiments. Although the golden section search is
only exact when the function is unimodal, we found
that it works quite well in practice. More details are
presented by Powell (1964).
Nelder-Mead method This approach sets up a
simplex on the search space, which is a polytope
with D + 1 vertices when there are D dimensions,
and successively moves the simplex to a lower point
to find a minimum of the function. The simplex is
moved using different actions, which are taken when
certain conditions are met. The basic idea behind
these actions is to replace the worst point in the sim-
plex with a new and better point, thereby moving the
simplex towards a minimum. This method has the
advantage of being able to deal with ?bumpy? func-
tions and depending on the configuration of the sim-
plex at the time, it is possible to escape some local
minima. This is often refer to as downhill simplex
method and more details are presented by Nelder
and Mead (1965).
4 Parameters
In this section, we discuss the parameters that we
optimize with direct search, in addition to standard
470
linear model parameters:
4.1 Distortion limit
Distortion limit is one of decoder parameters that
sets a limit on the number of words the decoder
is allowed to skip when deciding which source
phrase to translate in order to allow reordering. Fig-
ure 2 shows a translation example from English to
Japanese. Every word jumped over incurs a dis-
tortion cost, which is usually one of the transla-
tion model parameters, which thereby discourages
reordering of words unless language model supports
the reordering.
Since having a large distortion limit leads to
slower decoding, having the smallest possible dis-
tortion limit that still facilitates correct reordering
would be ideal. Not only this speeds up translation,
but this also leads to better translation quality by
minimizing search errors. Since a larger distortion
limit means there are more possible re-orderings of
translations, it is prone to more search errors. In fact,
there are evidences that tuning the distortion limit
is beneficial in improving quality of translation by
limiting search errors. Galley and Manning (2008)
conduct a line search along increments of distor-
tion limit and separately tune the translation model
parameters for each increment of distortion limit.
The result shows significant difference in translation
quality when distortion limit is tuned along with the
model parameters. Separately tuning model param-
eters for different distortion limit is necessary be-
cause model parameters are coupled with distortion
limit. A representative example: when distortion
limit is zero, the distortion penalty feature can have
any weight and not affect BLEU scores, but this is
not the case when distortion limit is larger than zero.
Tuning distortion limit in direct search in conjunc-
tion with related features such linear distortion elim-
inates the need for a line search for distortion limit.
4.2 Polynomial features
Most phrase-based decoders typically use a dis-
tortion penalty feature to discourage (or maybe
sometimes encourage) reordering. Whereas distor-
tion limit is a hard constraint?since the decoder
never considers jumps larger than the given limit?
distortion penalty is a soft constraint, since it penal-
izes reordering proportionally to the length of the
I did not see the book you borrowed
?? ???? ??? ?? ????
+5
-3
-3
Figure 2: Reordering in phrase-based translation. A min-
imum distortion limit of five is needed to correctly trans-
late this example. The source sentence is relatively sim-
ple but a relatively large distortion limit is needed to ac-
commodate the correct reordering due to typological dif-
ference between two languages.
jump. The total distortion penalty is calculated as
follows:
D(e, f) = ?d
?
j
|dj |pd
where ?d is the weight for distortion penalty feature,
and dj is the size of the jump needed to translate
the j-th phrase pair. For example, in Figure 2, the
total distortion penalty feature value is 11, which is
multiplied with ?d to get the total distortion cost of
translating the example sentence. Although pd is typ-
ically set to one (linear), one may consider polyno-
mial distortion penalty (Green et al, 2010). Green et
al. (2010) show that setting pd to a higher value than
one improves the translation quality, but uses a pre-
determined value for pd. Instead of manually setting
the value of pd, it can be given a value tuned with di-
rect search. Although we only discussed distortion
penalty here, it is straightforward to tune pi for each
feature hi(e, f)pi using direct error rate minimiza-
tion, where hi(e, f) is any linear model feature of
the decoder.
4.3 Future cost estimates
Since beam search involves pruning, it is crucial to
have good future cost estimation in order to min-
imize the number of search errors (Koehn et al,
2003). The concept of future cost estimation is re-
lated to heuristic functions in the A* search algo-
rithm. The total cost f(x) of a partial translation
hypothesis is estimated by combining g(x), which is
the actual current cost from the beginning of a sen-
tence to point x and h(x), which is the future cost
471
estimate from point x to the end of the sentence:
f(x) = g(x) + h(x)
In SMT decoding, the same feature weight vec-
tor is generally used when computing g(x) and h(x).
However, this may not be ideal since future cost esti-
mators use different heuristics depending on the fea-
tures. For example, the future cost estimator (Green
et al, 2010) for linear distortion always underesti-
mates completion cost, which is generally deemed
a good property. Unfortunately, some features have
estimators that tend to overestimate completion cost,
as it is the case with the language model. This prob-
lem is illustrated in Figure 3. The Figure shows
that the ratio between the estimated total cost and
the actual total cost converges to 1.0. However,
in earlier stages of translations, the estimated fu-
ture cost for language model is larger than it should
be, which leads to higher total estimated cost. In
the A* search parlance, we are using an inadmissi-
ble heuristic since the future cost is overestimated,
which leads to suboptimal search. This suggests that
separately tuning parameters that are involved in the
future cost estimation will lead to better pruning de-
cisions. This essentially doubles the number of lin-
ear model parameters, since for every feature used in
future cost estimation, we create a counterpart and
tune its weight independently.
4.4 Search parameters
In addition to the parameters listed above, we also
tune general decoder parameters that affect the
search quality: beam size and parameters controlling
histogram pruning and threshold pruning. While it
makes sense to set these parameters automatically
instead of manually, the methods we have presented
thus far are not particularly fit for this type of pa-
rameters. Indeed, if the sole goal is to maximize
translation quality (e.g., as measured by standard
BLEU), a larger beam size and less pruning is usu-
ally preferable. To address this problem, we opti-
mize these three parameters using a slightly different
objective function. When tuning any of these three
features, the goal of translation is to get the most ac-
curate translation given a pre-defined time limit, so
we change the objective to be a time-sensitive objec-
tive function. Much akin to brevity penalty in BLEU,
0 0.2 0.4 0.6 0.8 1
0
1
2
3
4
5
Figure 3: y axis is ratio between estimated total cost vs.
actual total cost of language model for thousands of trans-
lations. 1.0 means the estimated total cost and the actual
total cost are exactly the same, and anything higher than
1.0 means the future cost has been overestimated thereby
inflating the estimated total cost. The x-axis represents
how much translation has been completed. 0.1 means
10% of a sentence has been translated.
we define time penalty as:
TP
(
?
)
=
{
1.0 ti ? td
exp
(
1 ? titd
)
ti > td
where TP
(
?
)
is a time penalty that is multiplied
to BLEU, ti is the time it takes to translate devel-
opment set under current parameters, and td is the
desired time limit for translating the development
set. With this error metric, we still optimize for
the translation quality as long as the translation hap-
pens within desired time td. With the modified time-
sensitive BLEU score as error metric, direct search
may tune the parameters that have the speed and ac-
curacy trade-off that we want.2
5 Speeding up direct search
Optimizing the true error surface is generally more
computationally expensive than with any surrogate
error surface, since each function evaluation usually
requires decoding or re-decoding the entire devel-
opment set. Since SMT tuning sets used for error
2A disadvantage of using time in the definition of TP
`
?
?
is that it adds non-determinism that can make optimization un-
stable. Our solution is to replace time with pseudo-time, a de-
terministic substitute expressed as a linear combination of the
number of n-gram lookups and hypothesis expansions (these
two quantities correlate quite well with decoding time).
472
rate minimization often comprise one thousand sen-
tences or more, each function evaluation can take
minutes or more. However, this problem is some-
what mitigated by the fact that translating in batches
is highly parallelizable. Since MERT (Och, 2003) is
also easily parallelizable, we need to resort to other
speedup techniques to make direct search a practi-
cal alternative to MERT. We now present two tech-
niques that make optimization of the true error sur-
face more efficient.
5.1 A racing algorithm for speeding up SMT
model selection
Error rate minimization as presented in this paper
can be seen as a form of model selection, which
has been the focus of a lot of work in the learn-
ing literature. The most popular approaches to
model selection?such as minimizing cross valida-
tion error?tend to be very slow in practice; there-
fore, researchers have addressed the problem of ac-
celerating model selection using statistical tests.
Prior to considering the SMT case, we review one
of these methods in the case of leave-one-out cross
validation (LOOCV). Racing for model selection
(Maron and Moore, 1994; Moore and Lee, 1994)
works as follows: we are given a collection of Nm
models and Nd data points, and we must find the
model that minimizes the mean e?j = 1Nd
?
i ej(i),
where ej(i) is the classification error of model Mj
on the ith datapoint when trained on all datapoints
except the ith point. The models are evaluated con-
currently, and at any given step k ? [1, Nd], each
model Mj is associated with two pieces of informa-
tion: the current estimate of its mean error rate, and
the estimate of its variance. As evaluations progress,
we eliminate any model that is significantly worse
than any other model.3 We also note that the Rac-
ing technique first randomizes the order of the data
points to ensure that prefixes of the dataset are gen-
3The details of these statistical tests are not so important
here since we use different ones in the case of SMT, but we
briefly summarize them as follows: Maron and Moore (1994)
use a non-parametric method (Hoeffding bounds (Hoeffding,
1963)) for confidence estimation, and places confidence inter-
vals on the mean value of the random variable representing
ej(i). A model is discarded if its confidence interval no longer
overlaps with the confidence interval of the current best model.
Moore and Lee (1994) use a similar technique, but relies on
Bayesian statistics instead of Hoeffding bounds.
erally representative of the entire set.
In this work, we use Racing to speed up direct
search for SMT, but this requires two main adjust-
ments compared to the LOOCV case. First, our
models have real-valued parameters, so we cannot
exhaustively evaluate the set of all models since it is
infinite. Instead, we use direct search to select which
models compete against each other during Racing.
In the case of Powell?s method, all points of a grid
along the current search direction are evaluated in
parallel using Racing, before we turn to the next
line search. In the case of the downhill simplex op-
timizer and in the case of line searches other than
grid search (e.g., golden section search), the use of
Racing is more difficult because the function eval-
uations requested by these optimizers have depen-
dencies that generally prevent concurrent function
evaluations. Since functions in downhill simplex are
evaluated in sequence and not in parallel, our solu-
tion is to race the current model against our current
best model.4 When the evaluation of a model M is
interrupted because it is deemed significantly worse
than the current best model M? , the error rate of M
on the entire development set is extrapolated from
its relative performance on the decoded subset.5
The second main difference with the LOOCV
case is that we do not use confidence intervals to de-
termine which of two or models are best. In SMT, it
is common to use either bootstrap resampling (Efron
and Tibshirani, 1993; Och, 2003) or randomization
tests (Noreen, 1989). In this paper, we use the ran-
domization test for discarding unpromising models,
since this statistical test was shown to be less likely
to cause type-I errors6 than bootstrap methods (Rie-
zler and Maxwell, 2005). Since both kinds of statisti-
cal tests involve a time-consuming sampling step, it
4Since Racing only discards suboptimal models, the current
best model M? is one for which we have decoded the entire de-
velopment set. Once a new model M is evaluated, we perform
at step j a significance test to determine whether M ?s transla-
tion of sentences 1 . . . j is better or worse than M? translation
for the same range of sentences. If M is significantly worse, we
discard it. If M? is worse, we continue evaluating the perfor-
mance of M , since we need M ?s output for the full development
set if M eventually becomes the new best model.
5For example, if error rates of M? and M are respectively
10% and 11% on the subset decoded by both models and M? ?s
error on the entire set is 20%, M ?s extrapolated error is 22%.
6A type I error rejects a null hypothesis that is true.
473
is somewhat wasteful to perform a new test after the
decoding of each sentence, so we translate sentences
in small batches of K sentences before performing
each randomization test.7
We finally note that Racing no longer guarantees
that the error function observed by the optimizer is
the true error function. Racing causes some approx-
imations of the error function, but the degree of ap-
proximation is designed to be small in regions with
low error rates, and Racing ensures that the most
promising function evaluations in our progression to-
wards an optimum are unaffected. In contrast, the
approximation of the error function computed from
N -best lists or lattice does not share this property.8
To further speed up function evaluations in direct
search, we employ a method meant to deal with mod-
els that are nearly identical, a situation in which Rac-
ing usually does not help much. Indeed, when two
models produce very similar outputs, we often need
to run the race through every sentence of the devel-
opment set since none of the two models end up be-
ing significantly better. A solution to this problem
consists of discarding models that are nearly identi-
cal to other models, where similarity between mod-
els is solely measured from their outputs.9 To do
this, we resort again to a randomization test: Given
two models Ma and Mb, this test performs random
permutations between outputs of Ma and Mb, that is,
it determines for each sentence of index i whether or
not to permute the two model outputs, with proba-
bility p = 0.5. When Ma and Mb are very similar,
these permutations have little effect, even when we
repeat this sampling process many times. To cope
with this problem, we slightly modify the random-
7In our experiments, we set K = 50. Some other practi-
cal considerations: the significance level used for discarding
unpromising models is p ? .05. The randomization test is a
sampling-based technique, for which we must specify a sample
size R. In this paper, we use R = 5000.
8In the case of N -best MERT, it is not even guaranteed that
we find the true error rate of our current best model M while
searching the N -best error surface. In fact, if we take the pa-
rameters of our best model M and re-decode the development
set, we may get an error rate that is different from what was pre-
dicted from the N -best list. With direct search and Racing, no
such approximation affects our current best model.
9Measuring model similarity only based on parameter val-
ues is less effective, since features and other parameters are
sometimes redundant, and two models may behave similarly
while having fairly distinct parameter values.
ization test to discard one of the two nearly iden-
tical models. Specifically, we compute the gap?
measured in error rate?between the best random-
ized output and the worst randomized output. If this
gap is lower than a pre-defined threshold, we only
keep the best model.10 This adjustment to the sig-
nificance test makes direct search reasonably fast,
since Racing is effective during the initial steps of
search (when steps tend to be relatively big, and
when differences in error rate are pretty significant),
and our modification to randomization tests helps
while search converges towards an optimum using
increasingly smaller steps.
5.2 Lattice-based decoding
We use another technique to speed up direct search
by storing and re-using search graphs, which con-
sist of lattices in the case of phrase-based decod-
ing (Och et al, 1999) and hypergraphs in the case
of hierarchical decoding (Chiang, 2005). The suc-
cessive expansion of translation options in order to
construct the search graph is generally done from
scratch, but this can be wasteful when the same
sentences are translated multiple times, as it is the
case with direct search. Even when the parame-
ters of the decoder change across function evalua-
tions, some partial translation are likely to be con-
structed multiple times, and this is more likely to
happen when changes in parameters are relatively
small. To overcome this inefficiency, we memoize
hypotheses expansions made in all function evalu-
ations, which then allows us to reuse some edges
(or hyperedges) from previous iterations to construct
the current graph (or hypergraph). Since feature
values?including expensive features like language
model score?are stored into each edge, the speedup
is roughly proportional to the percentage of edges
we can reuse.
A more radical way of exploiting search graphs
of previous iterations is to use them as constraints in
a forced decoding approach. In this framework, the
decoder takes as input not only an input sentence,
but also a constraining search graph. During decod-
ing, it is forced to discard any translation hypothe-
10In the case where we compare our current best model and
a model that is currently being evaluated, we discard the latter.
In our experiments with BLEU, we discard if the gap is smaller
than 0.1 BLEU point.
474
decoder
constrained
decoder
??, ??
1-best BLEU
optimization
input f
other params ?
model params ?
lattice ?
repeat
Figure 4: Lattice-constrained decoding for direct search.
ses that violate the constraining search graph. This
makes the memoization method presented in the pre-
vious paragraph maximally efficient, since lattice-
constrained decoding has all linear model feature
values already pre-computed. While this approach
is similar in spirit to lattice-based MERT (Macherey
et al, 2008), there is a crucial difference. The opti-
mization steps in lattice MERT bypass the decoder,
but the lattice-based approach presented here does
not. The distinction is important when it comes to
tuning non-linear and hidden state parameters of the
decoder. For instance, the initial lattice may have
been constructed with a distortion limit of 4, while
the current model specifies a distortion limit of 2.
At that stage, optimization via lattice-constrained de-
coding instead of lattice-based MERT ensures that
we will never select a path of the input lattice that
corresponds to a distortion limit of more than 2. This
is important since the error rate must reflect the fact
that jumps of two or more words are not allowed.
Figure 4 shows how direct search with lattice-
constrained decoding is structured. Similarly to
MERT and as opposed to straight direct search, opti-
mization is repeated multiple times. Since each opti-
mization in the lattice-constrained case does not re-
quire recomputing any features, it usually turns into
very significant gains in terms of translation speed,
though it also causes a small loss of translation ac-
curacy in general. The overall approach depicted in
Figure 4 works as follows: a first set of lattices is
generated using an initial ?0 and ?0. We then run
direct search with a decoder constrained on this set
of lattices. After optimization has converged, the op-
Train MERT dev. Test
Korean-English 7.9M 1000 6000
Arabic-English 11.1M 1000 6000
Farsi-English 739K 1000 2000
Table 1: Size of bitexts in number of sentence pairs.
timal ?? and ?? are provided as input ?1 and ?1 to
start a new iteration of this process. Note that the
constraining lattices built at each iteration are always
merged with those of the previous ones, so constrain-
ing lattices grow over time. The two stopping crite-
ria are similar to MERT: if the norm of the difference
between the previous parameter vector?including
? and ??and the current vector falls below a pre-
defined tolerance value, we do not continue to the
next iteration. Alternatively, if a new pass of un-
constrained decoding generates lattices that are sub-
sumed by lattices constructed at previous iteration,
we stop and do not run the next optimization step.
6 Experiments
6.1 Setup
For our experiments, we use a phrase-based transla-
tion system similar to Moses (Koehn et al, 2007).
Our decoder uses many of the same features as
Moses, including four phrasal and lexicalized trans-
lation scores, phrase penalty, word penalty, a lan-
guage model score, linear distortion, and six lexical-
ized reordering scores. Unless specified otherwise,
the decoder?s stack size is 50, and the number of
translation options per input phrase is 25.
Table 1 summarizes the amount of training data
used to train translation systems from Korean, Ara-
bic, and Farsi into English. These data sets are
drawn from various sources, which include news,
web, and technical data, as well as United Nations
data in the case of Arabic. In order to get the sense
of how presented techniques generalize, we evalu-
ate our systems on a fairly broad domain. We use
development and test sets are a mix of news, web,
and technical data. All systems translate into En-
glish, for which we built a 5-gram language model
with cutoff counts 1, 1, 1, 2, 3 for unigrams to 5-
grams, using a corpus of roughly seven billion En-
glish words. This includes the target side of the par-
allel training data, plus a significant amount of data
gathered from the web.
475
# Minimizer Optimized parameters Arabic Korean Farsi
1 MERT with grid search lin, DL 29.12 (14.6) 23.30 (20.8) 32.16 (11.7)
2 Direct search (simplex) lin, DL 29.07 (1.2) 23.42 (4.4) 32.22 (1.3)
3 Direct search (Powell) lin, DL 29.20 (2.3) 23.39 (5.6) 32.28 (2.1)
4 Direct search (Powell) lin, extended, DL 29.39 (4.4) 23.61 (8.9) 32.51 (4.9)
5 Lattice-constrained (Powell) lin, extended, DL 29.27 (0.7) 23.43 (1.3) 32.42 (1.1)
6 Direct search (Powell) lin, extended, DL, search 29.31 (6.5) 23.46 (9.7) 32.62 (6.2)
Table 2: BLEU-4 scores (%) with one reference, translating into English; the numbers in parentheses are times in
hours to run parameter optimization end-to-end. ?Lin? refers to Moses linear model features; ?extended? refers to non-
linear and hidden state features (polynomial features, future cost); ?DL? refers to distortion limit; ?search? is the set of
parameters controlling search quality (parameters controlling beam size, histogram pruning, and threshold pruning).
Our baseline system is trained for each language
pair by running minimum error rate training (Och,
2003) on 1000 sentences. Each iteration of MERT
utilizes 19 random starting points, plus the points of
convergence at all previous iterations of MERT, and
a uniform weight vector. That is, the first iteration
of MERT uses 20 starting points, the second uses 21
points, etc. Since MERT is not able to directly opti-
mize search parameters such as distortion limit and
beam size, our baseline system uses grid search to
optimize them. To make this search more tractable,
we only perform the grid search for a single param-
eter: the distortion limit. For each language pair,
the grid search consists of repeating MERT for eight
distinct distortion limits ranging from 3 to 10. The
optimal distortion limits found for Korean, Arabic,
and Farsi, are 8, 5, and 6, respectively.11 To ensure
that the comparison with our approach is consistent,
this grid search is made on the MERT dev set itself.
The next subsection contrasts the different direct
search methods presented in this paper. Note that
all these experiments use the speedup techniques
based on statistical significance test presented in Sec-
tion 5. Indeed, we found that using these techniques
resulted in faster speeds without affecting the search
in any significant way. Models tuned with or without
significance tests often ended up identical.
6.2 Results
The main results are shown in Table 2, and are com-
puted using standard BLEU-4 (Papineni et al, 2002)
11We rerun MERT for each different distortion limit because
of the dependencies between this parameter and linear model
features, particularly linear distortion and lexicalized reordering
scores. A linear model that is effective with a distortion limit of
4 can be suboptimal for a limit of 8.
using one reference translation, and ignoring case.
Row 1 displays results of the MERT baseline, with
a distortion limit that was found optimal using a
grid search on the development set. Rows 2 and 3
show results of direct error rate minimization with
downhill simplex and Powell?s method, where di-
rect search optimizes both linear model parameters
and the distortion limit. We see here that the per-
formance of direct search is comparable and some-
times better than MERT, but the benefit of direct
search here is that it does not require an external grid
search to find an effective distortion limit (each di-
rect search is initialized with a distortion limit of 10).
Row 4 shows the performance of Powell?s method
using the extended parameter set (Section 4), which
includes model weights for future costs and polyno-
mial features. We lack space to present an exten-
sive analysis of the relative impact of the different
non-linear features and parameters discussed in this
paper, but we generally find that the following pa-
rameters work best: distortion limit, polynomial dis-
tortion penalty, and weight of future cost estimate of
the language model. The fact that Moses-style future
cost estimation for language models often overesti-
mates probably explains why the latter feature helps.
In the last row of Table 2, optimization is done
using the time-sensitive variant of BLEU presented
in Section 4.4, and the set of parameters tuned here
includes all the previous ones, in addition to beam
size, and the two parameters controlling histogram
and threshold pruning in beam search. Clearly, run-
ning direct search to directly optimize BLEU would
yield a very large beam size and would set pruning
parameters that are so permissive that they would al-
most completely disable pruning. The benefit of us-
ing the time-sensitive variant of BLEU is that direct
476
search is forced to find parameter weights that of-
fer a good balance between accuracy and speed. To
make our results in row 6 as comparable as possible
to row 4, we use the running time (on the develop-
ment set) of row 4 as a time constraint for the model
of row 6, which is to decode the entire development
set at least as fast. In other words, the system of
row 6 is optimized to be no slower than the system
of row 4, and is otherwise penalized due to the time
penalty. The effect of this is that translation speed at
tuning time is almost the same, and speed of systems
4 and 6 is roughly the same at test time. A com-
parison between rows 4 and 6 suggests that tuning
search parameters such as beam size and without af-
fecting time does not provide much gain in terms of
translation quality, but the method nevertheless has
one advantage: one can target a specific translation
speed without having to manually tune any param-
eter such as beam size, and without even having to
decide which parameter to manually tune.
Times to run optimizations end-to-end are re-
ported in parentheses in Table 2 and they take into
account the time to run the grid search in the case
of MERT. Times to decode test sets are not reported
here since they are roughly the same across all mod-
els. While translation accuracy with MERT and di-
rect search is roughly the same when the underly-
ing parameter set is the same, direct search wins in
running time when it comes to optimizing search pa-
rameters like distortion limit. Since each grid search
runs MERT eight times, MERT is generally faster
than direct search, but the difference of speed re-
mains reasonable if the number of tuned parameters
is the same, and direct search is rarely twice as slow.
We finally discuss the case of lattice-constrained
decoding, which is shown in row 5 of Table 2. This
method is not applicable when tuning parameters
that affect search thoroughness (row 6), such as
beam size. The reason is that lattice-constrained
decoding is a form of forced decoding that con-
siderably narrows the search space. Under a con-
strained decoding setting, it appears that a large
beam size seldom affects translation speed, but this
is misleading and largely due to constraints cre-
ated by the lattice. We thus evaluate the lattice-
constrained case without tuning ?search? features,
and find that direct search is significantly faster us-
ing lattice-constrained, with only a slight degrada-
tion of translation quality. Lattice constraints are
augmented 2-5 times before it converges.
7 Related work
The use of derivative-free optimization methods to
tune machine translation parameters has been tried
before. Bender et al (2004) used the Nelder-Mead
method to tune model parameters for a phrase-based
translation system. However, their way of making
direct search fast and practical is to set distortion
limit to zero, which results in poor translation qual-
ity for many language pairs. Zens et al (2007) also
use the Nelder-Mead method to tune parameters in a
log-linear model to maximize expected BLEU. Zhao
and Chen (2009) proposes changes to Nelder-Mead
method to better fit parameter tuning in their ma-
chine translation setting. They show the modifica-
tion brings better search of parameters over the regu-
lar Nelder-Mead method. Our work is related to the
search-based structured prediction (SEARN) model
of Daume? (2006), in the sense that direct search also
accounts for what happens during search (including
search errors) to try to find parameters that are not
only good for prediction, but for search as well.
8 Conclusion
This paper addressed the problem of minimizing er-
ror rate at a corpus level. We show that a technique
to directly minimize the true error rate, rather than
one estimated from a surrogate representation such
as an N -best list, is in fact feasible. We present two
techniques that make this minimization significantly
faster, to the point where this technique is a viable
alternative to MERT. In the case where free param-
eters of the decoder (such as distortion limit) also
need to be optimized, our technique is in fact much
faster. We also optimize non-linear and hidden state
features that cannot be tuned using MERT, which
yield improvements in translation accuracy. Experi-
ments on large test sets yield gains on three language
pairs, and our best configuration outperforms MERT
by 0.27 to 0.35 BLEU points using a baseline system
trained on large amounts of data.
Acknowledgments
We thank anonymous reviewers, Chris Quirk, Kristina
Toutanova, and Anthony Aue for valuable suggestions.
477
References
Oliver Bender, Richard Zens, Evgeny Matusov, and Her-
mann Ney. 2004. Alignment templates: the RWTH
SMT system. In Proc. of the International Workshop
on Spoken Language Translation, pages 79?84, Kyoto,
Japan.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through La-
grangian relaxation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 26?37, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270,
Ann Arbor, MI.
Hal Daume?, III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Angeles,
CA, USA.
B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 867?875, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Wassily Hoeffding. 1963. Probability inequalities for
sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13?30.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
ACL, Demonstration Session, pages 177?180.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum Bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 725?734,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
Oded Maron and Andrew W. Moore. 1994. Hoeffding
races: Accelerating model selection search for classi-
fication and function approximation. In Advances in
neural information processing systems 6, pages 59?66.
Morgan Kaufmann.
Andrew Moore and Mary Soon Lee. 1994. Efficient
algorithms for minimizing cross validation error. In
W. W. Cohen and H. Hirsh, editors, Proceedings of the
11th International Confonference on Machine Learn-
ing, pages 190?198. Morgan Kaufmann.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statistical
machine translation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 585?592, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
J. A. Nelder and R. Mead. 1965. A simplex method
for function minimization. Computer Journal, 7:308?
313.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience.
Franz Josef Och, Christoph Tillmann, Hermann Ney, and
Lehrstuhl Fiir Informatik. 1999. Improved alignment
models for statistical machine translation. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora,
pages 20?28.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Compu-
tational Linguistics (ACL-03).
478
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Compu-
tational Linguistics (ACL-02).
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. The Computer Journal, 7:155?
162.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical recipes
in C (2nd ed.): the art of scientific computing. Cam-
bridge University Press, New York, NY, USA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?64,
June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Pro-
ceedings of Association for Machine Translation in the
Americas, pages 223?231.
M H Wright. 1995. Direct search methods: Once
scorned, now respectable. Numerical Analysis,
344:191?208.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007. A
systematic comparison of training criteria for statisti-
cal machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 524?532.
Bing Zhao and Shengyuan Chen. 2009. A simplex
Armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, Compan-
ion Volume: Short Papers, pages 21?24, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
479
