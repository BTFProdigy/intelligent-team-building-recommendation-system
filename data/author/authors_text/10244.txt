Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 205?208,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
FastSum:
Fast and accurate query-based multi-document summarization
Frank Schilder and Ravikumar Kondadadi
Research & Development
Thomson Corp.
610 Opperman Drive, Eagan, MN 55123, USA
FirstName.LastName@Thomson.com
Abstract
We present a fast query-based multi-document
summarizer called FastSum based solely on
word-frequency features of clusters, docu-
ments and topics. Summary sentences are
ranked by a regression SVM. The summa-
rizer does not use any expensive NLP tech-
niques such as parsing, tagging of names or
even part of speech information. Still, the
achieved accuracy is comparable to the best
systems presented in recent academic com-
petitions (i.e., Document Understanding Con-
ference (DUC)). Because of a detailed fea-
ture analysis using Least Angle Regression
(LARS), FastSum can rely on a minimal set of
features leading to fast processing times: 1250
news documents in 60 seconds.
1 Introduction
In this paper, we propose a simple method for effec-
tively generating query-based multi-document sum-
maries without any complex processing steps. It
only involves sentence splitting, filtering candidate
sentences and computing the word frequencies in
the documents of a cluster, topic description and the
topic title. We use a machine learning technique
called regression SVM, as proposed by (Li et al,
2007). For the feature selection we use a new model
selection technique called Least Angle Regression
(LARS) (Efron et al, 2004).
Even though machine learning approaches dom-
inated the field of summarization systems in recent
DUC competitions, not much effort has been spent
in finding simple but effective features. Exceptions
are the SumBasic system that achieves reasonable
results with only one feature (i.e., word frequency
in document clusters) (Nenkova and Vanderwende,
2005). Our approach goes beyond SumBasic by
proposing an even more powerful feature that proves
to be the best predictor in all three recent DUC cor-
pora. In order to prove that our feature is more pre-
dictive than other features we provide a rigorous fea-
ture analysis by employing LARS.
Scalability is normally not considered when dif-
ferent summarization systems are compared. Pro-
cessing time of more than several seconds per sum-
mary should be considered unacceptable, in partic-
ular, if you bear in mind that using such a system
should help a user to process lots of data faster. Our
focus is on selecting the minimal set of features that
are computationally less expensive than other fea-
tures (i.e., full parse). Since FastSum can rely on
a minimal set of features determined by LARS, it
can process 1250 news documents in 60 seconds.1
A comparison test with the MEAD system2 showed
that FastSum is more than 4 times faster.
2 System description
We use a machine learning approach to rank all sen-
tences in the topic cluster for summarizability. We
use some features from Microsoft?s PYTHY system
(Toutonova et al, 2007), but added two new fea-
tures, which turned out to be better predictors.
First, the pre-processing module carries out tok-
enization and sentence splitting. We also created
a sentence simplification component which is based
14-way/2.0GHz PIII Xeon 4096Mb Memory
2http://www.summarization.com/mead/
205
on a few regular expressions to remove unimportant
components of a sentence (e.g., As a matter of fact,).
This processing step does not involve any syntac-
tic parsing though. For further processing, we ig-
nore all sentences that do not have at least two exact
word matches or at least three fuzzy matches with
the topic description.3
Features are mainly based on word frequencies of
words in the clusters, documents and topics. A clus-
ter contains 25 documents and is associated with a
topic. The topic contains a topic title and the topic
descriptions. The topic title is list of key words or
phrases describing the topic. The topic description
contains the actual query or queries (e.g., Describe
steps taken and worldwide reaction prior to intro-
duction of the Euro on January 1, 1999.).
The features we used can be divided into two sets;
word-based and sentence-based. Word-based fea-
tures are computed based on the probability of words
for the different containers (i.e., cluster, document,
topic title and description). At runtime, the different
probabilities of all words in a candidate sentence are
added up and normalized by length. Sentence-based
features include the length and position of the sen-
tence in the document. The starred features 1 and
4 are introduced by us, whereas the others can be
found in earlier literature.4
*1 Topic title frequency (1): ratio of number of
words ti in the sentence s that also appear in
the topic title T to the total number of words
t1..|s| in the sentence s:
?|s|
i=1
fT (ti)
|s| , where
fT =
{
1 : ti ? T
0 : otherwise
2 Topic description frequency (2): ratio of number
of words ti in the sentence s that also appear
in the topic description D to the total number
of words t1..|s| in the sentence s:
?|s|
i=1
fD(ti)
|s| ,
where fD =
{
1 : ti ? D
0 : otherwise
3 Content word frequency(3): the average content
word probability pc(ti) of all content words
3Fuzzy matches are defined by the OVERLAP similarity
(Bollegala et al, 2007) of at least 0.1.
4The numbers are used in the feature analysis, as in figure 2.
t1..|s| in a sentence s. The content word proba-
bility is defined as pc(ti) = nN , where n is the
number of times the word occurred in the clus-
ter and N is the total number of words in the
cluster:
?|s|
i=1
pc(ti)
|s|
*4 Document frequency (4): the average document
probability pd(ti) of all content words t1..|s| in
a sentence s. The document probability is de-
fined as pd(ti) = dD , where d is the number of
documents the word ti occurred in for a given
cluster and D is the total number of documents
in the cluster:
?|s|
i=1
pd(ti)
|s|
The remaining features are Headline frequency (5),
Sentence length (6), Sentence position (binary) (7),
and Sentence position (real) (8)
Eventually, each sentence is associated with a
score which is a linear combination of the above
mentioned feature values. We ignore all sentences
that do not have at least two exact word matches.5
In order to learn the feature weights, we trained a
SVM on the previous year?s data using the same fea-
ture set. We used a regression SVM. In regression,
the task is to estimate the functional dependence of
a dependent variable on a set of independent vari-
ables. In our case, the goal is to estimate the score
of a sentence based on the given feature set. In order
to get training data, we computed the word overlap
between the sentences from the document clusters
and the sentences in DUC model summaries. We
associated the word overlap score to the correspond-
ing sentence to generate the regression data. As a
last step, we use the pivoted QR decomposition to
handle redundancy. The basic idea is to avoid redun-
dancy by changing the relative importance of the rest
of the sentences based on the currently selected sen-
tence. The final summary is created from the ranked
sentence list after the redundancy removal step.
3 Results
We compared our system with the top performing
systems in the last two DUC competitions. With our
best performing features, we get ROUGE-2 (Lin,
2004) scores of 0.11 and 0.0925 on 2007 and 2006
5This threshold was derived experimentally with previous
data.
206
IIIT MS LIP6 IDA Peking FastSum Catalonia gen. Baseline
FastSum, 6 Top Systems and generic baseline for DUC 2007
ROU
GE?2
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
Figure 1: ROUGE-2 results including 95%-confidence
intervals for the top 6 systems, FastSum and the generic
baseline for DUC 2007
DUC data, respectively. These scores correspond
to rank 6th for DUC 2007 and the 2nd rank for
DUC 2006. Figure 1 shows a graphical compari-
son of our system with the top 6 systems in DUC
2007. According to an ANOVA test carried out by
the DUC organizers, these 6 systems are significant
better than the remaining 26 participating systems.
Note that our system is better than the PYTHY
system for 2006, if no sentence simplification was
carried out (DUC 2006: 0.089 (without simplifica-
tion); 0.096 (with simplification)). Sentence simpli-
fication is a computationally expensive process, be-
cause it requires a syntactic parse.
We evaluated the performance of the FastSum al-
gorithm using each of the features separately. Ta-
ble 1 shows the ROUGE score (recall) of the sum-
maries generated when we used each of the features
by themselves on 2006 and 2007 DUC data, trained
on the data from the respective previous year. Using
only the Document frequency feature by itself leads
to the second best system for DUC 2006 and to the
tenth best system for DUC 2007.
This first simple analysis of features indicates that
a more rigorous feature analysis would have bene-
fits for building simpler models. In addition, feature
selection could be guided by the complexity of the
features preferring those features that are computa-
tionally inexpensive.
Feature name 2007 2006
Title word frequency 0.096 0.0771
Topic word frequency 0.0996 0.0883
Content word frequency 0.1046 0.0839
Document frequency 0.1061 0.0903
Headline frequency 0.0938 0.0737
Sentence length 0.054 0.0438
Sentence position(binary) 0.0522 0.0484
Sentence position (real-valued) 0.0544 0.0458
Table 1: ROUGE-2 scores of individual features
We chose a so-called model selection algorithm
to find a minimal set of features. This problem can
be formulated as a shrinkage and selection method
for linear regression. The Least Angle Regres-
sion (LARS) (Efron et al, 2004) algorithm can be
used for computing the least absolute shrinkage and
selection operator (LASSO) (Tibshirani, 1996).At
each stage in LARS, the feature that is most corre-
lated with the response is added to the model. The
coefficient of the feature is set in the direction of the
sign of the feature?s correlation with the response.
We computed LARS on the DUC data sets from
the last three years. The graphical results for 2007
are shown in figure 2. In a LARS graph, features
are plotted on the x-axis and the corresponding co-
efficients are shown on y-axis. The value on the x-
axis is the ratio of norm of the coefficent vector to
the maximal norm with no constraint. The earlier a
feature appears on the x-axis, the better it is. Table
2 summarizes the best four features we determined
with LARS for the three available DUC data sets.
Year Top Features
2005 4 2 5 1
2006 4 3 2 1
2007 4 3 5 2
Table 2: The 4 top features for the DUC 2005, 2006 and
2007 data
Table 2 shows that feature 4, document frequency,
is consistently the most important feature for all
three data sets. Content word frequency (3), on the
other hand, comes in as second best feature for 2006
and 2007, but not for 2005. For the 2005 data, the
Topic description frequency is the second best fea-
ture. This observation is reflected by our single fea-
207
* * * * * * * * * ** *
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
2007
|beta|/max|beta|
Stand
ardize
d Coe
fficien
ts
* *
* * * * ** *
* * * * * * * ** *
*
* *
* * * * * ** *
* *
* * ** *
*** ** * * * ** ** * ** *
* * ** *
LASSO
108
23
4
Figure 2: Graphical output of LARS analysis:
Top features for 2007: 4 Document frequency, 3 Content word
frequency, 5 Headline frequency, 2 Topic description frequency
ture analysis for DUC 2006, as shown in table 1.
Similarly, Vanderwende et al (2006) report that they
gave the Topic description frequency a much higher
weight than the Content word frequency.
Consequently, we have shown that our new fea-
ture Document frequency is consistently the best
feature for all three past DUC corpora.
4 Conclusions
We proposed a fast query-based multi-document
summarizer called FastSum that produces state-of-
the-art summaries using a small set of predictors,
two of those are proposed by us: document fre-
quency and topic title frequency. A feature anal-
ysis using least angle regression (LARS) indicated
that the document frequency feature is the most use-
ful feature consistently for the last three DUC data
sets. Using document frequency alone can produce
competitive results for DUC 2006 and DUC 2007.
The two most useful feature that takes the topic de-
scription (i.e., the queries) into account is based on
the number of words in the topic description and the
topic title. Using a limited feature set of the 5 best
features generates summaries that are comparable to
the top systems of the DUC 2006 and 2007main task
and can be generated in real-time, since no compu-
tationally expensive features (e.g., parsing) are used.
From these findings, we draw the following con-
clusions. Since a feature set mainly based on word
frequencies can produce state-of-the-art summaries,
we need to analyze further the current set-up for the
query-based multi-document summarization task. In
particular, we need to ask the question whether the
selection of relevant documents for the DUC top-
ics is in any way biased. For DUC, the document
clusters for a topic containing relevant documents
were always pre-selected by the assessors in prepa-
ration for DUC. Our analysis suggests that simple
word frequency computations of these clusters and
the documents alone can produce reasonable sum-
maries. However, the human selecting the relevant
documents may have already influenced the way
summaries can automatically be generated. Our sys-
tem and systems such as SumBasic or SumFocus
may just exploit the fact that relevant articles pre-
screened by humans contain a high density of good
content words for summarization.6
References
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring Semantic Similarity between Words Using Web
Search Engines. In Proc. of 16th International World
Wide Web Conference (WWW 2007), pages 757?766,
Banff, Canada.
B. Efron, T. Hastie, I.M. Johnstone, and R. Tibshirani.
2004. Least angle regression. Annals of Statistics,
32(2):407?499.
S. Gupta, A. Nenkova, and D. Jurafsky. 2007. Measur-
ing Importance and Query Relevance in Topic-focused
Multi-document Summarization. In Proc. of the 45th
Annual Meeting of the Association for Computational
Linguistics, pages 193?196, Prague, Czech Republic.
S. Li, Y. Ouyang, W. Wang, and B. Sun. 2007. Multi-
document summarization using support vector regres-
sion. In Proceedings of DUC 2007, Rochester, USA.
C. Lin. 2004. Rouge: a package for automatic evaluation
of summaries. In Proceedings of the Workshop on Text
Summarization Branches Out (WAS 2004).
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. In MSR-TR-2005-101.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. J. Royal. Statist. Soc B., 58(1):267?288.
K. Toutonova, C. Brockett, J. Jagarlamudi, H. Suzuko,
and L. Vanderwende. 2007. The PYTHY Summa-
rization System: Microsoft Research at DUC2007. In
Proc. of DUC 2007, Rochester, USA.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Mi-
crosoft Research at DUC 2006: Task-focused summa-
rization with sentence simplification and lexical ex-
pansion. In Proc. of DUC 2006, New York, USA.
6Cf. Gupta et al (2007) who come to a similar conclusion
by comparing between word frequency and log-likelihood ratio.
208
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Surrogate Learning -
From Feature Independence to Semi-Supervised Classification
Sriharsha Veeramachaneni and Ravi Kumar Kondadadi
Thomson Reuters Research and Development
Eagan, MN 55123, USA
[harsha.veeramachaneni,ravikumar.kondadadi]@thomsonreuters.com
Abstract
We consider the task of learning a classi-
fier from the feature space X to the set of
classes Y = {0, 1}, when the features can
be partitioned into class-conditionally inde-
pendent feature sets X1 and X2. We show
that the class-conditional independence can be
used to represent the original learning task
in terms of 1) learning a classifier from X2
to X1 (in the sense of estimating the prob-
ability P (x1|x2))and 2) learning the class-
conditional distribution of the feature set X1.
This fact can be exploited for semi-supervised
learning because the former task can be ac-
complished purely from unlabeled samples.
We present experimental evaluation of the idea
in two real world applications.
1 Introduction
Semi-supervised learning is said to occur when the
learner exploits (a presumably large quantity of) un-
labeled data to supplement a relatively small labeled
sample, for accurate induction. The high cost of la-
beled data and the simultaneous plenitude of unla-
beled data in many application domains, has led to
considerable interest in semi-supervised learning in
recent years (Chapelle et al, 2006).
We show a somewhat surprising consequence of
class-conditional feature independence that leads
to a principled and easily implementable semi-
supervised learning algorithm. When the feature set
can be partitioned into two class-conditionally in-
dependent sets, we show that the original learning
problem can be reformulated in terms of the problem
of learning a first predictor from one of the partitions
to the other, plus a second predictor from the latter
partition to class label. That is, the latter partition
acts as a surrogate for the class variable. Assum-
ing that the second predictor can be learned from
a relatively small labeled sample this results in an
effective semi-supervised algorithm, since the first
predictor can be learned from only unlabeled sam-
ples.
In the next section we present the simple yet in-
teresting result on which our semi-supervised learn-
ing algorithm (which we call surrogate learning) is
based. We present examples to clarify the intuition
behind the approach and present a special case of
our approach that is used in the applications section.
We then examine related ideas in previous work and
situate our algorithm among previous approaches
to semi-supervised learning. We present empirical
evaluation on two real world applications where the
required assumptions of our algorithm are satisfied.
2 Surrogate Learning
We consider the problem of learning a classifier
from the feature space X to the set of classes Y =
{0, 1}. Let the features be partitioned into X =
X1 ? X2. The random feature vector x ? X will be
represented correspondingly as x = (x1,x2). Since
we restrict our consideration to a two-class problem,
the construction of the classifier involves the esti-
mation of the probability P (y = 0|x1,x2) at every
point (x1,x2) ? X .
We make the following assumptions on the joint
probabilities of the classes and features.
10
1. P (x1,x2|y) = P (x1|y)P (x2|y) for y ?
{0, 1}. That is, the feature sets x1 and x2
are class-conditionally independent for both
classes. Note that, when X1 and X2 are one-
dimensional, this condition is identical to the
Naive Bayes assumption, although in general
our assumption is weaker.
2. P (x1|x2) 6= 0, P (x1|y) 6= 0 and P (x1|y =
0) 6= P (x1|y = 1). These assumptions are
to avoid divide-by-zero problems in the alge-
bra below. If x1 is a discrete valued random
variable and not irrelevant for the classification
task, these conditions are often satisfied.
We can now show that P (y = 0|x1,x2) can
be written as a function of P (x1|x2) and P (x1|y).
When we consider the quantity P (y,x1|x2), we
may derive the following.
P (y,x1|x2) = P (x1|y,x2)P (y|x2)
? P (y,x1|x2) = P (x1|y)P (y|x2)
(from the independence assumption)
? P (y|x1,x2)P (x1|x2) = P (x1|y)P (y|x2)
? P (y|x1,x2)P (x1|x2)P (x1|y) = P (y|x2) (1)
Since P (y = 0|x2) + P (y = 1|x2) = 1, Equa-
tion 1 implies
P (y = 0|x1,x2)P (x1|x2)
P (x1|y = 0) +
P (y = 1|x1,x2)P (x1|x2)
P (x1|y = 1) = 1
? P (y = 0|x1,x2)P (x1|x2)P (x1|y = 0) +
(1? P (y = 0|x1,x2))P (x1|x2)
P (x1|y = 1) = 1(2)
Solving Equation 2 for P (y = 0|x1,x2), we ob-
tain
P (y = 0|x1,x2) =
P (x1|y = 0)
P (x1|x2) ?
P (x1|y = 1)? P (x1|x2)
P (x1|y = 1)? P (x1|y = 0)(3)
We have succeeded in writing P (y = 0|x1,x2) as
a function of P (x1|x2) and P (x1|y). Although this
result was previously observed in a different context
by Abney in (Abney, 2002), he does not use it to
derive a semi-supervised learning algorithm. This
result can lead to a significant simplification of the
learning task when a large amount of unlabeled data
is available. The semi-supervised learning algorithm
involves the following two steps.
1. From unlabeled data learn a predictor from the
feature space X2 to the space X1 to predict
P (x1|x2). There is no restriction on the learner
that can be used as long as it outputs posterior
class probability estimates.
2. Estimate the quantity P (x1|y) from a labeled
samples. In case x1 is finite valued, this can
be done by just counting. If X1 has low car-
dinality the estimation problem requires very
few labeled samples. For example, if x1 is
binary, then estimating P (x1|y) involves esti-
mating just two Bernoulli probabilities.
Thus, we can decouple the prediction problem into
two separate tasks, one of which involves predict-
ing x1 from the remaining features. In other words,
x1 serves as a surrogate for the class label. Fur-
thermore, for the two steps above there is no neces-
sity for complete samples. The labeled examples can
have the feature x2 missing.
At test time, an input sample (x1,x2) is classified
by computing P (x1|y) and P (x1|x2) from the pre-
dictors obtained from training, and plugging these
values into Equation 3. Note that these two quanti-
ties are computed for the actual value of x1 taken by
the input sample.
The following example illustrates surrogate learn-
ing.
??????????????
Example 1
Consider the following variation on a problem
from (Duda et al, 2000) of classifying fish on a con-
veryor belt as either salmon (y = 0) or sea bass
(y = 1). The features describing the fish are x1,
a binary feature describing whether the fish is light
(x1 = 0) or dark (x1 = 1), and x2 describes the
length of the fish which is real-valued. Assume (un-
realistically) that P (x2|y), the class-conditional dis-
tribution of x2, the length for salmon is Gaussian,
11
and for the sea bass is Laplacian as shown in Fig-
ure 1.
?4 ?2 0 2 40
0.5
x2
P(x2|y=0) P(x2|y=1)
Figure 1: Class-conditional probability distributions of
the feature x2.
Because of the class-conditional feature in-
dependence assumption, the joint distribution
P (x1,x2,y) = P (x2|y)P (x1,y) can now be
completely specified by fixing the joint probabil-
ity P (x1,y). Let P (x1 = 0,y = 0) = 0.3,
P (x1 = 0,y = 1) = 0.1, P (x1 = 1,y = 0) = 0.2,
and P (x1 = 1,y = 1) = 0.4. I.e., a salmon is more
likely to be light than dark and a sea bass is more
likely to be dark than light.
The full joint distribution is depicted in Figure 2.
Also shown in Figure 2 are the conditional distribu-
tions P (x1 = 0|x2) and P (y = 0|x1,x2).
Assume that we build a predictor to decide be-
tween x1 = light and x1 = dark from the length us-
ing a data set of unlabeled fish. On a random salmon,
this predictor will most likely decide that x1 = light
(because, for a salmon, x1 = light is more likely
than x1 = dark, and similarly for a sea bass the
predictor often decides that x1 = dark. Conse-
quently the predictor provides information about the
true class label y. This can also be seen in the sim-
ilarities between the curves P (y = 0|x1,x2) to the
curve P (x1|x2) in Figure 2.
Another way to interpret the example is to note
that if a predictor for P (x1|x2) were built on only
the salmons then P (x1 = light|x2) will be a con-
stant value (0.6). Similarly the value of P (x1 =
light|x2) for sea basses will also be a constant value
(0.2). That is, the value of P (x1 = light|x2) for
a sample is a good predictor of its class. However,
?4 ?2 0 2 4
0.5
x1 = 0
x1 = 1
x2
P(x1=1,y=0,x2) P(x1=1,y=1,x2)
P(x1=0,y=1,x2)P(x1=0,y=0,x2)
P(y=0|x1=1,x2)
P(y=0|x1=0,x2)
P(x1=0|,x2)
Figure 2: The joint distributions and the posterior distri-
butions of the class y and the surrogate class x1.
surrogate learning builds the predictor P (x1|x2) on
unlabeled data from both types of fish and there-
fore additionally requires P (x1|y) to estimate the
boundary between the classes.
2.1 A Special Case
The independence assumptions made in the setting
above may seem too strong to hold in real problems,
especially because the feature sets are required to
be class-conditionally independent for both classes.
We now specialize the setting of the classification
problem to the one realized in the applications we
present later.
We still wish to learn a classifier from X = X1 ?
X2 to the set of classes Y = {0, 1}. We make the
following slightly modified assumptions.
1. x1 is a binary random variable. That is, X1 =
{0, 1}.
2. P (x1,x2|y = 0) = P (x1|y = 0)P (x2|y =
0). We require that the feature x1 be class-
conditionally independent of the remaining fea-
tures only for the class y = 0.
3. P (x1 = 0,y = 1) = 0. This assumption says
that x1 is a ?100% recall? feature for y = 11.
Assumption 3 simplifies the learning task to the
estimation of the probability P (y = 0|x1 = 1,x2)
for every point x2 ? X2. We can proceed as before
1This assumption can be seen to trivially enforce the inde-
pendence of the features for class y = 1.
12
to obtain the expression in Equation 3.
P (y = 0|x1 = 1,x2)
= P (x1 = 1|y = 0)P (x1 = 1|x2) . . .
. . . P (x1 = 1|y = 1)? P (x1 = 1|x2)P (x1 = 1|y = 1)? P (x1 = 1|y = 0)
= P (x1 = 1|y = 0)P (x1 = 1|x2) ?
1? P (x1 = 1|x2)
1? P (x1 = 1|y = 0)
= P (x1 = 1|y = 0)P (x1 = 1|x2) ?
P (x1 = 0|x2)
P (x1 = 0|y = 0)
= P (x1 = 1|y = 0)P (x1 = 0|y = 0) ?
P (x1 = 0|x2)
(1? P (x1 = 0|x2))(4)
Equation 4 shows that P (y = 0|x1 = 1,x2)
is a monotonically increasing function of P (x1 =
0|x2). This means that after we build a predictor
from X2 to X1, we only need to establish the thresh-
old on P (x1 = 0|x2) to yield the optimum classi-
fication between y = 0 and y = 1. Therefore the
learning proceeds as follows.
1. From unlabeled data learn a predictor from the
feature space X2 to the binary space X1 to pre-
dict the quantity P (x1|x2).
2. Use labeled sample to establish the thresh-
old on P (x1 = 0|x2) to achieve the desired
precision-recall trade-off for the original clas-
sification problem.
Because of our assumptions, for a sample from
class y = 0 it is impossible to predict whether
x1 = 0 or x1 = 1 better than random by looking
at the x2 feature, whereas a sample from the posi-
tive class always has x1 = 1. Therefore the samples
with x1 = 0 serve to delineate the positive exam-
ples among the samples with x1 = 1. We therefore
call the samples that have x1 = 1 as the target sam-
ples and those that have x1 = 0 as the background
samples.
3 Related Work
Although the idea of using unlabeled data to im-
prove classifier accuracy has been around for several
decades (Nagy and Shelton, 1966), semi-supervised
learning has received much attention recently due
to impressive results in some domains. The com-
pilation of chapters edited by Chappelle et al is an
excellent introduction to the various approaches to
semi-supervised learning, and the related practical
and theoretical issues (Chapelle et al, 2006).
Similar to our setup, co-training assumes that the
features can be split into two class-conditionally
independent sets or ?views? (Blum and Mitchell,
1998). Also assumed is the sufficiency of either
view for accurate classification. The co-training al-
gorithm iteratively uses the unlabeled data classified
with high confidence by the classifier on one view,
to generate labeled data for learning the classifier on
the other.
The intuition underlying co-training is that the er-
rors caused by the classifier on one view are inde-
pendent of the other view, hence can be conceived
as uniform2 noise added to the training examples
for the other view. Consequently, the number of la-
bel errors in a region in the feature space is propor-
tional to the number of samples in the region. If the
former classifier is reasonably accurate, the propor-
tionally distributed errors are ?washed out? by the
correctly labeled examples for the latter classifier.
Seeger showed that co-training can also be viewed
as an instance of the Expectation-Maximization al-
gorithm (Seeger, 2000).
The main distinction of surrogate learning from
co-training is the learning of a predictor from one
view to the other, as opposed to learning predictors
from both views to the class label. We can there-
fore eliminate the requirement that both views be
sufficiently informative for reasonably accurate pre-
diction. Furthermore, unlike co-training, surrogate
learning has no iterative component.
Ando and Zhang propose an algorithm to regu-
larize the hypothesis space by simultaneously con-
sidering multiple classification tasks on the same
feature space (Ando and Zhang, 2005). They then
use their so-called structural learning algorithm for
semi-supervised learning of one classification task,
by the artificial construction of ?related? problems
on unlabeled data. This is done by creating prob-
lems of predicting observable features of the data
and learning the structural regularization parame-
ters from these ?auxiliary? problems and unlabeled
data. More recently in (Ando and Zhang, 2007) they
2Whether or not a label is erroneous is independent of the
feature values of the latter view.
13
showed that, with conditionally independent feature
sets predicting from one set to the other allows the
construction of a feature representation that leads
to an effective semi-supervised learning algorithm.
Our approach directly operates on the original fea-
ture space and can be viewed another justification
for the algorithm in (Ando and Zhang, 2005).
Multiple Instance Learning (MIL) is a learning
setting where training data is provided as positive
and negative bags of samples (Dietterich et al,
1997). A negative bag contains only negative ex-
amples whereas a positive bag contains at least one
positive example. Surrogate learning can be viewed
as artificially constructing a MIL problem, with the
targets acting as one positive bag and the back-
grounds acting as one negative bag (Section 2.1).
The class-conditional feature independence assump-
tion for class y = 0 translates to the identical and
independent distribution of the negative samples in
both bags.
4 Two Applications
We applied the surrogate learning algorithm to the
problems of record linkage and paraphrase genera-
tion. As we shall see, the applications satisfy the
assumptions in our second (100% recall) setting.
4.1 Record Linkage/ Entity Resolution
Record linkage is the process of identification and
merging of records of the same entity in different
databases or the unification of records in a single
database, and constitutes an important component of
data management. The reader is referred to (Win-
kler, 1995) for an overview of the record linkage
problem, strategies and systems. In natural language
processing record linkage problems arise during res-
olution of entities found in natural language text to
a gazetteer.
Our problem consisted of merging each of ?
20000 physician records, which we call the update
database, to the record of the same physician in
a master database of ? 106 records. The update
database has fields that are absent in the master
database and vice versa. The fields in common in-
clude the name (first, last and middle initial), sev-
eral address fields, phone, specialty, and the year-
of-graduation. Although the last name and year-
of-graduation are consistent when present, the ad-
dress, specialty and phone fields have several incon-
sistencies owing to different ways of writing the ad-
dress, new addresses, different terms for the same
specialty, missing fields, etc. However, the name
and year alone are insufficient for disambiguation.
We had access to ? 500 manually matched update
records for training and evaluation (about 40 of these
update records were labeled as unmatchable due to
insufficient information).
The general approach to record linkage involves
two steps: 1) blocking, where a small set of can-
didate records is retrieved from the master record
database, which contains the correct match with
high probability, and 2) matching, where the fields
of the update records are compared to those of the
candidates for scoring and selecting the match. We
performed blocking by querying the master record
database with the last name from the update record.
Matching was done by scoring a feature vector of
similarities over the various fields. The feature val-
ues were either binary (verifying the equality of a
particular field in the update and a master record) or
continuous (some kind of normalized string edit dis-
tance between fields like street address, first name
etc.).
The surrogate learning solution to our matching
problem was set up as follows. We designated the
binary feature of equality of year of graduation3 as
the ?100% recall? feature x1, and the remaining fea-
tures are relegated to x2. The required conditions
for surrogate learning are satisfied because 1) in our
data it is highly unlikely for two records with differ-
ent year- of-graduation to belong to the same physi-
cian and 2) if it is known that the update record
and a master record belong to two different physi-
cians, then knowing that they have the same (or dif-
ferent) year-of-graduation provides no information
about the other features. Therefore all the feature
vectors with the binary feature indicating equality
of year-of-graduation are targets and the remaining
are backgrounds.
First, we used feature vectors obtained from the
records in all blocks from all 20000 update records
to estimate the probability P (x1|x2). We used lo-
3We believe that the equality of the middle intial would have
worked just as well for x1.
14
Table 1: Precision and Recall for record linkage.
Training Precision Recall
proportion
Surrogate 0.96 0.95
Supervised 0.5 0.96 0.94
Supervised 0.2 0.96 0.91
gistic regression for this prediction task. For learn-
ing the logistic regression parameters, we discarded
the feature vectors for which x1 was missing and
performed mean imputation for the missing values
of other features. Second, the probability P (x1 =
1|y = 0) (the probability that two different ran-
domly chosen physicians have the same year of
graduation) was estimated straightforwardly from
the counts of the different years-of-graduation in the
master record database.
These estimates were used to assign the score
P (y = 1|x1 = 1,x2) to the records in a block (cf.
Equation 4). The score of 0 is assigned to feature
vectors which have x1 = 0. The only caveat is cal-
culating the score for feature vectors that had miss-
ing x1. For such records we assign the score P (y =
1|x2) = P (y = 1|x1 = 1,x2)P (x1 = 1|x2). We
have estimates for both quantities on the right hand
side. The highest scoring record in each block was
flagged as a match if it exceeded some appropriate
threshold.
We compared the results of the surrogate learn-
ing approach to a supervised logistic regression
based matcher which used a portion of the manual
matches for training and the remaining for testing.
Table 1 shows the match precision and recall for
both the surrogate learning and the supervised ap-
proaches. For the supervised algorithm, we show the
results for the case where half the manually matched
records were used for training and half for testing,
as well as for the case where a fifth of the records of
training and the remaining four-fifths for testing. In
the latter case, every record participated in exactly
one training fold but in four test folds.
The results indicate that the surrogate learner per-
forms better matching by exploiting the unlabeled
data than the supervised learner with insufficient
training data. The results although not dramatic are
still promising, considering that the surrogate learn-
ing approach used none of the manually matched
records.
4.2 Paraphrase Generation for Event
Extraction
Sentence classification is often a preprocessing step
for event or relation extraction from text. One of the
challenges posed by sentence classification is the di-
versity in the language for expressing the same event
or relationship. We present a surrogate learning ap-
proach to generating paraphrases for expressing the
merger-acquisition (MA) event between two organi-
zations in financial news. Our goal is to find para-
phrase sentences for the MA event from an unla-
beled corpus of news articles, that might eventually
be used to train a sentence classifier that discrimi-
nates between MA and non-MA sentences.
We assume that the unlabeled sentence corpus is
time-stamped and named entity tagged with orga-
nizations. We further assume that a MA sentence
must mention at least two organizations. Our ap-
proach to generate paraphrases is the following. We
first extract all the so-called source sentences from
the corpus that match a few high-precision seed pat-
terns. An example of a seed pattern used for the
MA event is ?<ORG1> acquired<ORG2>? (where
<ORG1> and <ORG2> are place holders for
strings that have been tagged as organizations). An
example of a source sentence that matches the seed
is ?It was announced yesterday that <ORG>Google
Inc.<ORG> acquired <ORG>Youtube <ORG>?.
The purpose of the seed patterns is to produce pairs
of participant organizations in an MA event with
high precision.
We then extract every sentence in the corpus that
contains at least two organizations, such that at least
one of them matches an organization in the source
sentences, and has a time-stamp within a two month
time window of the matching source sentence. Of
this set of sentences, all that contain two or more or-
ganizations from the same source sentence are des-
ignated as target sentences, and the rest are desig-
nated as background sentences.
We speculate that since an organization is unlikely
to have a MA relationship with two different orga-
nizations in the same time period the backgrounds
are unlikely to contain MA sentences, and more-
over the language of the non-MA target sentences is
15
Table 2: Patterns used as seeds and the number of source
sentences matching each seed.
Seed pattern # of sources
1 <ORG> acquired <ORG> 57
2 <ORG> bought <ORG> 70
3 offer for <ORG> 287
4 to buy <ORG> 396
5 merger with <ORG> 294
indistinguishable from that of the background sen-
tences. To relate the approach to surrogate learning,
we note that the binary ?organization-pair equality?
feature (both organizations in the current sentence
being the same as those in a source sentence) serves
as the ?100% recall? feature x1. Word unigram, bi-
gram and trigram features were used as x2. This
setup satisfies the required conditions for surrogate
learning because 1) if a sentence is about MA, the
organization pair mentioned in it must be the same
as that in a source sentence, (i.e., if only one of the
organizations match those in a source sentence, the
sentence is unlikely to be about MA) and 2) if an un-
labeled sentence is non-MA, then knowing whether
or not it shares an organization with a source does
not provide any information about the language in
the sentence.
If the original unlabeled corpus is sufficiently
large, we expect the target set to cover most of the
paraphrases for the MA event but may contain many
non-MA sentences as well. The task of generating
paraphrases involves filtering the target sentences
that are non-MA and flagging the rest of the tar-
gets as paraphrases. This is done by constructing a
classifier between the targets and backgrounds. The
feature set used for this task was a bag of word un-
igrams, bigrams and trigrams, generated from the
sentences and selected by ranking the n-grams by
the divergence of their distributions in the targets
and backgrounds. A support vector machine (SVM)
was used to learn to classify between the targets and
backgrounds and the sentences were ranked accord-
ing to the score assigned by the SVM (which is a
proxy for P (x1 = 1|x2)). We then thresholded the
score to obtain the paraphrases.
Our approach is similar in principle to the ?Snow-
ball? system proposed in (Agichtein and Gravano,
2000) for relation extraction. Similar to us, ?Snow-
ball? looks for known participants in a relationship in
an unlabeled corpus, and uses the newly discovered
contexts to extract more participant tuples. How-
ever, unlike surrogate learning, which can use a rich
set of features for ranking the targets, ?Snowball?
scores the newly extracted contexts according to a
single feature value which is confidence measure
based only on the number of known participant tu-
ples that are found in the context.
Example 2 below lists some sentences to illustrate
the surrogate learning approach. Note that the tar-
gets may contain both MA and non-MA sentences
but the backgrounds are unlikely to be MA.
??????????????
Example 2
Seed Pattern
?offer for <ORG>?
Source Sentences
1. <ORG>US Airways<ORG> said Wednesday it will
increase its offer for <ORG>Delta<ORG>.
Target Sentences (SVM score)
1.<ORG>US Airways<ORG> were to combine with a
standalone <ORG>Delta<ORG>. (1.0008563)
2.<ORG>US Airways<ORG> argued that the nearly
$10 billion acquisition of <ORG>Delta<ORG> would
result in an efficiently run carrier that could offer low
fares to fliers. (0.99958149)
3.<ORG>US Airways<ORG> is asking
<ORG>Delta<ORG>?s official creditors commit-
tee to support postponing that hearing. (-0.99914371)
Background Sentences (SVM score)
1. The cities have made various overtures to
<ORG>US Airways<ORG>, including a promise
from <ORG>America West Airlines<ORG> and the
former <ORG>US Airways<ORG>. (0.99957752)
2. <ORG>US Airways<ORG> shares rose 8 cents
to close at $53.35 on the <ORG>New York Stock
Exchange<ORG>. (-0.99906444)
??????????????
We tested our algorithm on an unlabeled corpus of
approximately 700000 financial news articles. We
experimented with the five seed patterns shown in
Table 2. We extracted a total of 870 source sentences
from the five seeds. The number of source sentences
matching each of the seeds is also shown in Table 2.
Note that the numbers add to more than 870 because
it is possible for a source sentence to match more
than one seed.
The participants that were extracted from sources
16
Table 3: Precision/Recall of surrogate learning on the
MA paraphrase problem for various thresholds. The
baseline of using all the targets as paraphrases for MA
has a precision of 66% and a recall of 100%.
Threshold Precision Recall
0.0 0.83 0.94
-0.2 0.82 0.95
-0.8 0.79 0.99
corresponded to approximately 12000 target sen-
tences and approximately 120000 background sen-
tences. For the purpose of evaluation, 500 randomly
selected sentences from the targets were manually
checked leading to 330 being tagged as MA and the
remaining 170 as non-MA. This corresponds to a
66% precision of the targets.
We then ranked the targets according to the score
assigned by the SVM trained to classify between the
targets and backgrounds, and selected all the targets
above a threshold as paraphrases for MA. Table 3
presents the precision and recall on the 500 manu-
ally tagged sentences as the threshold varies. The
results indicate that our approach provides an effec-
tive way to rank the target sentences according to
their likelihood of being about MA.
To evaluate the capability of the method to find
paraphrases, we conducted five separate experi-
ments using each pattern in Table 2 individually as a
seed and counting the number of obtained sentences
containing each of the other patterns (using a thresh-
old of 0.0). These numbers are shown in the differ-
ent columns of Table 4. Although new patterns are
obtained, their distribution only roughly resembles
the original distribution in the corpus. We attribute
this to the correlation in the language used to de-
scribe a MA event based on its type (merger vs. ac-
quisition, hostile takeover vs. seeking a buyer, etc.).
Finally we used the paraphrases, which were
found by surrogate learning, to augment the train-
ing data for a MA sentence classifier and evaluated
its accuracy. We first built a SVM classifier only
on a portion of the labeled targets and classified the
remaining. This approach yielded an accuracy of
76% on the test set (with two-fold cross validation).
We then added all the targets scored above a thresh-
old by surrogate learning as positive examples (4000
Table 4: Number of sentences found by surrogate learn-
ing matching each of the remaining seed patterns, when
only one of the patterns was used as a seed. Each column
is for one experiment with the corresponding pattern used
as the seed. For example, when only the first pattern was
used as the seed, we obtained 18 sentences that match the
fourth pattern.
Seeds 1 2 3 4 5
1 2 2 5 1
2 5 6 7 5
3 4 6 152 103
4 18 16 93 57
5 3 9 195 57
positive sentences in all were added), and all the
backgrounds that scored below a low threshold as
negative examples (27000 sentences), to the training
data and repeated the two-fold cross validation. The
classifier learned on the augmented training data im-
proved the accuracy on the test data to 86% .
We believe that better designed features (than
word n-grams) will provide paraphrases with higher
precision and recall of the MA sentences found by
surrogate learning. To apply our approach to a new
event extraction problem, the design step also in-
volves the selection of the x1 feature such that the
targets and backgrounds satisfy our assumptions.
5 Conclusions
We presented surrogate learning ? an easily imple-
mentable semi-supervised learning algorithm that
can be applied when the features satisfy the required
independence assumptions. We presented two appli-
cations, showed how the assumptions are satisfied,
and presented empirical evidence for the efficacy of
our algorithm. We have also applied surrogate learn-
ing to problems in information retrieval and docu-
ment zoning. We expect that surrogate learning is
sufficiently general to be applied in many NLP ap-
plications, if the features are carefully designed. We
briefly note that a surrogate learning method based
on regression and requiring only mean independence
instead of full statistical independence can be de-
rived using techniques similar to those in Section 2
? this modification is closely related to the problem
and solution presented in (Quadrianto et al, 2008).
17
References
S. Abney. 2002. Bootstrapping. In In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 360?367.
E. Agichtein and L. Gravano. 2000. Snowball: Extract-
ing Relations from Large Plain-Text Collections. In
Proceedings of the 5th ACM International Conference
on Digital Libraries (ACM DL), pages 85?94, June,
2-7.
R. K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and unla-
beled data. JMLR, 6:1817?1853.
R. K. Ando and T. Zhang. 2007. Two-view feature gen-
eration model for semi-supervised learning. In ICML,
pages 25?32.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT, pages 92?
100.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez.
1997. Solving the multiple instance problem with
axis-parallel rectangles. Artificial Intelligence, 89(1-
2):31?71.
R. O. Duda, P. E. Hart, and D. G. Stork. 2000. Pattern
Classification. Wiley-Interscience Publication.
G. Nagy and G. L. Shelton. 1966. Self-corrective charac-
ter recognition system. IEEE Trans. Information The-
ory, 12(2):215?222.
N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. Le.
2008. Estimating labels from label proportions. In
ICML ?08: Proceedings of the 25th international con-
ference on Machine learning, pages 776?783.
M. Seeger. 2000. Input-dependent regularization
of conditional density models. Technical re-
port, Institute for ANC, Edinburgh, UK. See
http://www.dai.ed.ac.uk/?seeger/papers.html.
W. E. Winkler. 1995. Matching and record linkage. In
Business Survey Methods, pages 355?384. Wiley.
18
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1406?1415,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Statistical NLG Framework for Aggregated Planning and Realization
Ravi Kondadadi?, Blake Howald and Frank Schilder
Thomson Reuters, Research & Development
610 Opperman Drive, Eagan, MN 55123
firstname.lastname@thomsonreuters.com
Abstract
We present a hybrid natural language gen-
eration (NLG) system that consolidates
macro and micro planning and surface re-
alization tasks into one statistical learn-
ing process. Our novel approach is based
on deriving a template bank automatically
from a corpus of texts from a target do-
main. First, we identify domain specific
entity tags and Discourse Representation
Structures on a per sentence basis. Each
sentence is then organized into semanti-
cally similar groups (representing a do-
main specific concept) by k-means cluster-
ing. After this semi-automatic processing
(human review of cluster assignments), a
number of corpus?level statistics are com-
piled and used as features by a ranking
SVM to develop model weights from a
training corpus. At generation time, a set
of input data, the collection of semanti-
cally organized templates, and the model
weights are used to select optimal tem-
plates. Our system is evaluated with au-
tomatic, non?expert crowdsourced and ex-
pert evaluation metrics. We also introduce
a novel automatic metric ? syntactic vari-
ability ? that represents linguistic variation
as a measure of unique template sequences
across a collection of automatically gener-
ated documents. The metrics for generated
weather and biography texts fall within ac-
ceptable ranges. In sum, we argue that our
statistical approach to NLG reduces the
need for complicated knowledge-based ar-
chitectures and readily adapts to different
domains with reduced development time.
?*Ravi Kondadadi is now affiliated with Nuance Commu-
nications, Inc.
1 Introduction
NLG is the process of generating natural-sounding
text from non-linguistic inputs. A typical NLG
system contains three main components: (1) Doc-
ument (Macro) Planning - deciding what content
should be realized in the output and how it should
be structured; (2) Sentence (Micro) planning -
generating a detailed sentence specification and
selecting appropriate referring expressions; and
(3) Surface Realization - generating the final text
after applying morphological modifications based
on syntactic rules (see e.g., Bateman and Zock
(2003), Reiter and Dale (2000) and McKeown
(1985)). However, document planning is arguably
one of the most crucial components of an NLG
system and is responsible for making the texts ex-
press the desired communicative goal in a coher-
ent structure. If the document planning stage fails,
the communicative goal of the generated text will
not be met even if the other two stages are perfect.
While most traditional systems simplify develop-
ment by using a pipelined approach where (1-3)
are executed in a sequence, this can result in er-
rors at one stage propagating to successive stages
(see e.g., Robin and McKeown (1996)). We pro-
pose a hybrid framework that combines (1-3) by
converting data to text in one single process.
Most NLG systems fall into two broad
categories: knowledge-based and statistical.
Knowledge-based systems heavily depend on hav-
ing domain expertise to come up with hand-
crafted rules at each stage of a pipeline. Although
knowledge-based systems can produce high qual-
ity text, they are (1) very expensive to build, in-
volving a lot of discussion with the end users of the
system for the document planning stage alone; (2)
have limited linguistic coverage, as it is time con-
suming to capture linguistic variation; and (3) one
has to start from scratch for each new domain be-
cause the developed components cannot be reused.
1406
Statistical systems, on the other hand, are fairly
inexpensive, more adaptable and rely on having
historical data for the given domain. Coverage is
likely to be high if more historical data is avail-
able. The main disadvantage with statistical sys-
tems is that they are more prone to errors and the
output text may not be coherent as there are less
constraints on the generated text.
Our framework is a hybrid of statistical and
template-based systems. Many knowledge-based
systems use templates to generate text. A tem-
plate structure contains ?gaps? that are filled to
generate the output. The idea is to create a lot
of templates from the historical data and select
the right template based on some constraints. To
the best of our knowledge, this is the first hy-
brid statistical-template-based system that com-
bines all three stages of NLG. Experiments with
different variants of our system (for biography and
weather subject matter domains) demonstrate that
our system generates reasonable texts.
Also, in addition to the standard metrics used
to evaluate NLG systems (e.g., BLEU, NIST, etc.),
we present a unique text evaluation metric called
syntactic variability to measure the linguistic vari-
ation of generated texts. This metric applies to the
document collection level and is based on com-
puting the number of unique template sequences
among all the generated texts. A higher number
indicates the texts are more variable and natural-
sounding whereas a lower number shows they are
more redundant. We argue that this metric is use-
ful for evaluating template-based systems and for
any type of text generation for domains where lin-
guistic variability is favored (e.g., the user is ex-
pected to go through more than one document in
the same session).
The main contributions of this paper are (1) A
statistical NLG system that combines document
and sentence planning and surface realization into
one single process; and (2) A new metric ? syntac-
tic variability ? is proposed to measure the syntac-
tic and morphological variability of the generated
texts. We believe this is the first work to propose
an automatic metric to measure linguistic variabil-
ity of generated texts in NLG.
Section 2 provides an overview of related work
on NLG. We present our main system in Section 3.
The system is evaluated and discussed in Section
4. Finally, we conclude in Section 5 and point out
future directions of research.
2 Background
Typically, knowledge-based NLG systems are im-
plemented by rules and, as mentioned above, have
a pipelined architecture for the document and
sentence planning stages and surface realization
(Hovy, 1993; Moore and Paris, 1993). However,
document planning is arguably the most impor-
tant task (Sripada et al, 2001). It follows that ap-
proaches to document planning are rule-based as
well and, concomitantly, are usually domain spe-
cific. For example, Bouayad-Agha, et al (2011)
proposed document planning based on an ontol-
ogy knowledge base to generate football sum-
maries. For rule?based systems, rules exist for
selecting content to grammatical choices to post-
processing (e.g., pronoun generation). These rules
are often tailored to a given system, with input
from multiple experts; consequently, there is a
high associated development cost (e.g., 12 person
months for the SUMTIME-METEO system (Belz,
2007)).
Statistical approaches can reduce extensive de-
velopment time by relying on corpus data to
?learn? rules for one or more components of an
NLG system (Langkilde and Knight, 1998). For
example, Duboue and McKeown (2003) proposed
a statistical approach to extract content selection
rules for biography descriptions. Further, statisti-
cal approaches should be more adaptable to differ-
ent domains than their rule-based equivalents (An-
geli et al, 2012). For example, Barzilay and Lap-
ata (2005) formulated content selection as a clas-
sification task to produce football summaries and
Kelly et al (2009) extended Barzilay and Lapata?s
approach for generating match reports for cricket.
The present work builds on Howald et al
(2013) where, in a given corpus, a combination of
domain specific named entity tagging and cluster-
ing sentences (based on semantic predicates) were
used to generate templates. However, while the
system consolidated both sentence planning and
surface realization with this approach (described
in more detail in Section 3), the document plan
was given via the input data and sequencing infor-
mation was present in training documents. For the
present research, we introduce a similar method
that leverages the distributions of document?level
features in the training corpus to incorporate a
statistical document planning component. Con-
sequently, we are able to create a streamlined
statistical NLG architecture that balances natural
1407
human?like variability with appropriate and accu-
rate information.
3 Methodology
In order to generate text for a given domain our
system runs input data through a statistical ranking
model to select a sequence of templates that best
fit the input data (E). In order to build the rank-
ing model, our system takes historical data (cor-
pus) for the domain through four components: (A)
preprocessing; (B) ?conceptual unit? creation; (C)
collecting statistics; and (D) ranking model build-
ing (summarized in Figure 1). In this section, we
describe each component in detail.
Figure 1: System Architecture.
3.1 Preprocessing
The first component processes the given corpus to
extract templates. We assume that each document
in the corpus is classified to a specific domain.
Preprocessing involves uncovering the underlying
semantic structure of the corpus and using this as
a foundation for template creation (Lu et al, 2009;
Lu and Ng, 2011; Konstas and Lapata, 2012).
We first split each document in the corpus into
sentences and create a shallow Discourse Repre-
sentation Structure (following Discourse Repre-
sentation Theory (Kamp and Reyle, 1993)) of each
sentence. The DRS consists of semantic predi-
cates and named entity tags. We use Boxer se-
mantic analyzer (Bos, 2008) to extract semantic
predicates such as EVENT or DATE. In parallel,
domain specific named entity tags are identified
and, in conjunction with the semantic predicates,
are used to create templates. We developed the
named-entity tagger for the weather domain our-
selves. To tag entities in the biography domain,
we used OpenCalais (www.opencalais.com). For
example, in the biography in (1), the conceptual
meaning (semantic predicates and domain-specific
entities) of sentences (a-b) are represented in (c-d).
The corresponding templates are showing in (e-f).
(1) Sentence
a. Mr. Mitsutaka Kambe has been serving as Managing Di-
rector of the 77 Bank, Ltd. since June 27, 2008.
b. He holds a Bachelor?s in finance from USC and a MBA
from UCLA.
Conceptual Meaning
c. SERVING | TITLE | PERSON | COMPANY | DATE
d. HOLDS | DEGREE | SUBJECT | INSTITUTION| EVENT
Templates
e. [person] has been serving as [title] of the [company]
since [date].
f. [person] holds a [degree] in [subject] from [institution]
and a [degree] from [institution].
The outputs of the preprocessing stage are the tem-
plate bank and predicate information for each tem-
plate in the corpus.1
3.2 Creating Conceptual Units
The next step is to create conceptual units for the
corpus by clustering templates. This is a semi-
automatic process where we use the predicate in-
formation for each template to compute similar-
ity between templates. We use k-means clustering
with k (equivalent to the number of semantic con-
cepts in the domain) set to an arbitrarily high value
(100) to over-generate (using the WEKA toolkit
(Witten and Frank, 2005)). This allows for easier
manual verification of the generated clusters and
we merge them if necessary. We assign a unique
identifier called a CuId (Conceptual Unit Identi-
fier) to each cluster, which represents a ?concep-
tual unit?. We associate each template in the cor-
pus to a corresponding CuId. For example, in (2),
using the templates in (1e-f), the identified named
entities are assigned to a clustered CuId (2a-b).
(2) Conceptual Units
a. {CuId : 000} ? [person] has been serving as [title] of the
[company] since [date].
b. {CuId : 001} ? [person] holds a [degree] in [subject]
from [institution] and a [degree] from [institution].
At this stage, we will have a set of conceptual
units with corresponding template collections (see
Howald et al (2013) for a further explanation of
Sections 3.1-3.2).
1A similar approach to the clustering of semantic content
is found in Duboue and McKeown (2003), where text with
stopwords removed were used as semantic input. Boxer pro-
vides a similar representation with the addition of domain
general tags. However, to contrast our work from Duboue
and McKeown, which focused on content selection, we are
focused on learning templates from the semantic representa-
tions for the complete generation system (covering content
selection, aggregation, sentence and document planning).
1408
3.3 Collecting Corpus Statistics
After identifying the different conceptual units and
the template bank, we collect a number of statistics
from the corpus:
? Frequency distribution of templates overall and per po-
sition
? Frequency distribution of CuIds overall and per posi-
tion
? Average number of entity tags by CuId as well as the
entity distribution by CuId
? Average number of entity tags by position as well as
the entity distribution by position
? Average number of words per CuId.
? Average number of words per CuId and position com-
bination.
? Average number of words per position
? Frequency distribution of the main verbs by position
? Frequency distribution of CuId sequences (bigrams and
trigrams only) overall and per position
? Frequency distribution of template sequences (bigrams
and trigrams only) overall and per position
? Frequency distribution of entity tag sequences overall
and per position
? The average, minimum, maximum number of CuIds
across all documents
As discussed in the next section, these statistics
are turned into features used for building a ranking
model in the next component.
3.4 Building a ranking model
The core component of our system is a statistical
model that ranks a set of templates for a given
position (sentence 1, sentence 2, ..., sentence n)
based on the input data. The input data in our
tasks was extracted from a training document; this
serves as a temporary surrogate to a database. The
task is to learn the ranks of all the templates from
all CuIds at each position.
To generate the training data, we first filter the
templates that have named entity tags not specified
in the input data. This will make sure the gener-
ated text does not have any unfilled entity tags. We
then rank templates according to the Levenshtein
edit distance (Levenshtein, 1966) from the tem-
plate corresponding to the current sentence in the
training document (using the top 10 ranked tem-
plates in training for ease of processing effort). We
experimented with other ranking schemes such as
entity-based similarity (similarity between entity
sequences in the templates) and a combination of
edit-distance based and entity-based similarities.
We obtained better results with edit distance. For
each template, we generate the following features
to build the ranking model. Most of the features
are based on the corpus statistics mentioned above.
? CuId given position: This is a binary feature where
the current CuId is either the same as the most frequent
CuId for the position (1) or not (0).
? Overlap of named entities: Number of common enti-
ties between current CuId and most likely CuId for the
position
? Prior template: Probability of the sequence of tem-
plates selected at the previous position and the current
template (iterated for the last three positions).
? Prior CuId: Probability of the sequence of the CuId
selected at the previous position and the current CuId
(iterated for the last three positions).
? Difference in number of words: Absolute difference
between number of words for current template and av-
erage number of words for the CuId
? Difference in number of words given position: Ab-
solute difference between number of words for cur-
rent template and average number of words for CuId
at given position
? Percentage of unused data: This feature represents
the portion of the unused input so far.
? Difference in number of named entities: Absolute
difference between the number of named entities in the
current template and the average number of named en-
tities for the current position
? Most frequent verb for the position: Binary valued
feature where the main verb of the template belongs to
the most frequent verb group given the position is either
the same (1) or not (0).
? Average number of words used: Ratio of number of
words in the generated text so far to the average number
of words.
? Average number of entities: Ratio of number of
named entities in the generated text so far to the av-
erage number of named entities.
? Most likely CuId given position and previous CuId:
Binary feature indicating if the current CuId is most
likely given the position and the previous CuId.
? Similarity between the most likely template in CuId
and current template: Edit distance between the cur-
rent template and the most likely template for the cur-
rent CuId.
? Similarity between the most likely template in CuId
given position and current template: Edit distance
between the current template and the most likely tem-
plate for the current CuId at the current position.
We used a linear kernel for a ranking SVM
(Joachims, 2002) (cost set to total queries) to learn
the weights associated with each feature for the
different domains.
3.5 Generation
At generation time, our system has a set of in-
put data, a semantically organized template bank
(collection of templates organized by CuId) and a
model from training on the documents for a given
domain. We first filter out those templates that
contain a named entity tag not present in the in-
put data. Then, we compute a score for each of the
remaining templates from the feature values and
the feature weights from the model. The template
with the highest overall score is selected and filled
with matching entity tags from the input data and
1409
appended to the generated text.
Before generating the next sentence, we track
those entities used in the initial sentence gener-
ation and decide to either remove those entities
from the input data or keep the entity for one or
more additional sentence generations. For exam-
ple, in the biography discourses, the name of the
person may occur only once in the input data, but
it may be useful for creating good texts to have
that person?s name available for subsequent gen-
erations. To illustrate in (3), if we remove James
Smithton from the input data after the initial gen-
eration, an irrelevant sentence (d) is generated as
the input data will only have one company after
the removal of James Smithton and the model will
only select a template with one company. If we
keep James Smithton, then the generations in (a-b)
are more cohesive.
(3) Use more than once
a. Mr. James Smithton was appointed CFO at Fordway
Internation in April.
b. Previously, Mr. Smithton was CFO of the Keyes
Development Group.
Use once and remove
c. Mr. James Smithton was appointed CFO at Fordway
Internation in April.
d. Keyes Development Group is a venture capital firm.
Deciding on what type of entities and how to
remove them is different for each domain. For ex-
ample, some entities are very unique to a text and
should not be made available for subsequent gen-
erations as doing so would lead to unwanted re-
dundancies (e.g., mentioning the name of current
company in a biography discourse more than once
as in (3)) and some entities are general and should
be retained. Our system possesses the ability to
monitor the data usage from historical data and we
can set parameters (based on the distribution of en-
tities) on the usage to ensure coherent generations
for a given domain.
Once the input data has been modified (i.e., an
entity have been removed, replaced or retained),
it serves as the new input data for the next sen-
tence generation. This process repeats until reach-
ing the minimum number of sentences for the do-
main (determined from the training corpus statis-
tic) and then continues until all of the remaining
input data is consumed (and not to exceed the pre-
determined maximum number of sentences, also
determined from the training corpus statistic).
4 Evaluation and Discussion
In this section, we first discuss the corpus data
used to train and generate texts. Then, the re-
sults of both automatic and human evaluations of
our system?s generations against the original and
baseline texts are considered as a means of de-
termining performance. For all experiments re-
ported in this section, the baseline system selects
the most frequent conceptual unit at the given po-
sition, chooses the most likely template for the
conceptual unit, and fills the template with input
data. The above process is repeated until the num-
ber of sentences is less than or equal to the average
number of sentences for the given domain.
4.1 Data
We ran our system on two different domains: cor-
porate officer and director biographies and off-
shore oil rig weather reports from the SUMTIME-
METEO corpus ((Reiter et al, 2005)). The biogra-
phy domain includes 1150 texts ranging from 3-17
sentences and the weather domain includes 1045
weather reports ranging from 1-6 sentences.2 We
used a training-test(generation) split of 70/30.
(4) provides generation comparisons for the
system ( DocSys), baseline ( DocBase) and orig-
inal ( DocOrig) randomly selected text snippets
from each domain. The variability of the gener-
ated texts ranges from a close similarity to slightly
shorter - not an uncommon (Belz and Reiter,
2006), but not necessarily detrimental, observation
for NLG systems (van Deemter et al, 2005).
(4) Weather DocOrig
a. Another weak cold front will move ne to Cornwall by later
Friday.
Weather DocSys
b. Another weak cold front will move ne to Cornwall during
Friday.
Weather DocBase
c. Another weak cold front from ne through the Cornwall will
remain slow moving.
Bio DocOrig
d. He previously served as Director of Sales Planning and
Manager of Loan Center.
Bio DocSys
e. He previously served as Director of Sales in Loan Center
of the Company.
Bio DocBase
2The SUMTIME-METEO project is a common bench
mark in NLG. However, we provide no comparison between
our system and SUMTIME-METEO as our system utilized the
generated forecasts from SUMTIME-METEO?s system as the
historical data. We cannot compare with other statistical gen-
eration systems like (Belz, 2007) as they only focussed on the
part of the forecasts the predicts wind characteristics whereas
our system generates the complete forecasts.
1410
f. He previously served as Director of Sales of the Company.
The DocSys and DocBase generations are
largely grammatical and coherent on the surface
with some variance, but there are graded semantic
variations (e.g., Director of Sales Planning vs. Di-
rector of Sales (4g-h) and move ne to Cornwall vs.
from ne through the Cornwall). Both automatic
and human evaluations are required in NLG to de-
termine the impact of these variances on the under-
standability of the texts in general (non-experts)
and as they are representative of particular subject
matter domains (experts). The following sections
discuss the evaluation results.
4.2 Automatic Metrics
We used BLEU?4 (Papineni et al, 2002), METEOR
(v.1.3) (Denkowski and Lavie, 2011) to evaluate
the texts at document level. Both BLEU?4 and
METEOR originate from machine translation re-
search. BLEU?4 measures the degree of 4-gram
overlap between documents. METEOR uses a un-
igram weighted f?score less a penalty based on
chunking dissimilarity. These metrics only eval-
uate the text on a document level but fail to iden-
tify ?syntactic repetitiveness? across documents in
a document collection. This is an important char-
acteristic of a document collection to avoid banal-
ity. To address this issue, we propose a new auto-
matic metric called syntactic variability. In order
to compute this metric, each document should be
represented as a sequence of templates by associ-
ating each sentence in the document with a tem-
plate in the template bank. Syntactic variability is
defined as the percentage of unique template se-
quences across all generated documents. It ranges
between 0 and 1. A higher value indicates that
more documents in the collection are linguistically
different from each other and a value closer to zero
shows that most of documents have the similar
language despite different input data.3
As indicated in Figure 2, the BLEU-4 scores are
low for all DocSys and DocBase generations (as
compared to DocOrig) for each domain. How-
ever, the METEOR scores, while low overall (rang-
ing from .201-.437) are noticeably increased over
BLEU-4 (which ranges from .199-.320).
Given the nature of each metric, the results in-
dicate that the generated and baseline texts have
3Of course, syntactic and semantic repetitiveness could be
captured by syntactic variability, but only if this is the nature
of the underlying historical data - financial texts tend to be
fairly repetitive.
Figure 2: Automatic Evaluations.
very different surface realizations compared to the
originals (low BLEU-4), but are still capturing the
content of the originals (higher METEOR). Both
BLEU?4 and METEOR measure the similarity of
the generated text to the original text, but fail to
penalize repetitiveness across texts, which is ad-
dressed by the syntactic variability metric. There
is no statistically significant difference between
DocSys and DocBase generations for METEOR
and BLEU?4.4 However, there is a statistically
significant difference in the syntactic variability
metric for both domains (weather - ?2=137.16,
d.f.=1, p<.0001; biography - ?2=96.641, d.f.=1,
p<.0001) - the variability of the DocSys gener-
ations is greater than the DocBase generations,
which shows that texts generated by our system
are more variable than the baseline texts.
The use of automatic metrics is a common eval-
uation method in NLG, but they must be recon-
ciled against non?expert and expert level evalua-
tions.
4.3 Non-Expert Human Evaluations
Two sets of crowdsourced human evaluation tasks
(run on CrowdFlower) were constructed to com-
pare against the automatic metrics: (1) an under-
standability evaluation of the entire text on a three-
point scale: Fluent = no grammatical or infor-
mative barriers; Understandable = some gram-
matical or informative barriers; Disfluent = sig-
nificant grammatical or informative barriers; and
(2) a sentence?level preference between sentence
pairs (e.g., ?Do you prefer Sentence A (from Do-
cOrig) or the corresponding Sentence B (from
DocBase/DocSys)?).
4BLEU?4: weather - ?2=1.418, d.f.=1, p=.230; biography
- ?2=0.311, d.f.=1, p=.354. METEOR: weather - ?2=1.016,
d.f.=1, p=.313; biography - ?2=0.851, d.f.=1, p=.354.
1411
Over 100 native English speakers contributed,
each one restricted to providing no more than
50 responses and only after they successfully an-
swered 4 ?gold data? questions correctly. We also
omitted those evaluators with a disproportionately
high response rate. No other data was collected on
the contributors (although geographic data (coun-
try, region, city) and IP addresses were available).
For the sentence?level preference task, the pair or-
derings were randomized to prevent click bias.
For the text?understandability task, 40 docu-
ments were chosen at random from the DocOrig
test set alng with the corresponding 40 Doc-
Sys and DocBase generations (240 documents to-
tal/120 for each domain). 8 judgments per doc-
ument were solicited from the crowd (1920 to-
tal judgments, 69.51 average agreement) and are
summarized in Figures 3 and 4 (biography and
weather respectively).
If the system is performing well and the rank-
ing model is actually contributing to increased
performance, the accepted trend should be that
the DocOrig texts are more fluent and preferred
compared to both the DocSys and DocBase sys-
tems. However, the differences between DocOrig
and DocSys will not be significant, the differences
between DocOrig and DocBase and DocSys and
DocBase will be significantly different.
Figure 3: Biography Text Evaluations.
Focusing on fluency ratings, it is expected that
the DocOrig generations will have the highest flu-
ency (as they are human generated). Further, if the
DocSys is performing well, it is expected that the
fluency rating will be less than the DocOrig and
higher than DocBase. Figure 3, which shows the
biography text evaluations, demonstrates this ac-
ceptable distribution of performances.
For the weather discourses, as evident from
Figure 4, the acceptable trend holds between the
DocSys and DocBase generations, and the Doc-
Sys generation fluency is actually slightly higher
than DocOrig. This is possibly because the Do-
cOrig texts are from a particular subject matter -
weather forecasts for offshore oil rigs in the U.K.
- which may be difficult for people in general to
understand. Nonetheless, the demonstrated trend
is favorable to our system.
In terms of significance, there are no statisti-
cally significant differences between the systems
for weather (DocOrig vs. DocSys - ?2=.347,
d.f.=1, p=.555; DocOrig vs. DocBase - ?2=.090,
d.f.=1, p=.764; DocSys vs. DocBase - ?2=.790,
d.f.=1, p=.373). While this is a good result for
comparing DocOrig and DocSys generations, it is
not for the other pairs. However, numerically, the
trend is in the right direction despite not being
able to demonstrate significance. For biography,
the trend fits nicely both numerically and in terms
of statistical significance (DocOrig vs. DocSys -
?2=5.094, d.f.=1, p=.024; DocOrig vs. DocBase -
?2=35.171, d.f.=1, p<.0001; DocSys vs. DocBase
- ?2=14.000, d.f.=1, p<.0001).
Figure 4: Weather Text Evaluations.
For the sentence preference task, equivalent
sentences across the 120 documents were chosen
at random (80 sentences from biography and 74
sentences from weather). 8 judgments per com-
parison were solicited from the crowd (3758 to-
tal judgments, 75.87 average agreement) and are
summarized in Figures 5 and 6 (biography and
weather, respectively).
Similar to the text?understandability task, an
acceptable performance pattern should include the
DocOrig texts being preferred to both DocSys and
DocBase generations and the DocSys generation
preferred to the DocBase. The closer the Doc-
Sys generation is to the DocOrig, the better Doc-
Sys is performing. The biography domain illus-
1412
Figure 5: Biography Sentence Evaluations.
trates this scenario (Figure 5) where the results are
similar to the text-understandability experiments.
In contrast, for weather domain, sentences from
DocBase system were preferred to our system?s
(Figure 6). We looked at the cases where the
preferences were in favor of DocBase. It appears
that because of high syntactic variability, our sys-
tem can produce quite complex sentences where as
the non-experts seem to prefer shorter and simpler
sentences because of the complexity of the text.
In terms of significance, there are no statisti-
cally significant differences between the systems
for weather (DocOrig vs. DocSys - ?2=6.48,
d.f.=1, p=.011; DocOrig vs. DocBase - ?2=.720,
d.f.=1, p=.396; DocSys vs. DocBase - ?2=.720,
d.f.=1, p=.396). The trend is different compared to
the fluency metric above in that the DocBase sys-
tem is outperforming the DocOrig generations to
an almost statistically significant difference - the
remaining comparisons follow the trend. We be-
lieve that this is for similar reasons stated above
- i.e., the generation may be a more digestible
version of a technical document. More problem-
atic is the results of the biography evaluations.
Here there is a statistically significant difference
between the DocSys and DocOrig and no sta-
tistically significant difference between the Doc-
Sys and DocBase generations (DocOrig vs. Doc-
Sys - ?2=76.880, d.f.=1, p<.0001; DocOrig vs.
DocBase - ?2=38.720, d.f.=1, p<.0001; DocSys
vs. DocBase - ?2=.720, d.f.=1, p=.396). Again,
this distribution of preferences is numerically sim-
ilar to the trend we would like to see, but the sta-
tistical significance indicates that there is some
ground to make up. Expert evaluations are po-
tentially informative for identifying specific short-
comings and how best to address them.
Figure 6: Weather Sentence Evaluations.
4.4 Expert Human Evaluations
We performed expert evaluations for the biogra-
phy domain only as we do not have access to
weather experts. The four biography reviewers are
journalists who write short biographies for news
archives.
For the biography domain, evaluations of the
texts were largely similar to the evaluations of
the non-expert crowd (76.22 average agreement
for the sentence?preference task and 72.95 for the
text?understandability task). For example, the dis-
fluent ratings were highest for the DocBase gen-
erations and lowest for the DocOrig generations.
Also, the fluent ratings were highest for the Do-
cOrig generations, and while the combined flu-
ent and understandable are higher for DocSys as
compared to DocBase, the DocBase generations
had a 10% higher fluent score (58.22%) as com-
pared to the DocSys fluent score (47.97%). Based
on notes from the reviewers, the succinctness of
the the DocBase generations are preferred in some
ways as they are in keeping with certain editorial
standards. This is further reflected in the sentence
preferences being 70% in favor of the DocBase
generations as compared to the DocSys genera-
tions (all other sentence comparisons were consis-
tent with the non-expert crowd).
These expert evaluations provide much needed
clarity to the NLG process. Overall, our system
is generating clearly acceptable texts. Further,
there are enough parameters inherent in the system
to tune to different domain expectations. This is
an encouraging result considering that no experts
were involved in the development of the system -
a key contrast to many other existing (especially
rule-based) NLG systems.
1413
5 Conclusions and Future Work
We have presented a hybrid (template-based and
statistical), single?staged NLG system that gen-
erates natural sounding texts and is domain?
adaptable. Our experiments with both ex-
perts and non?experts demonstrate that the
system-generated texts are comparable to human?
authored texts. The development time to adapt
our system to new domains is small compared to
other NLG systems; around a week to adapt the
system to weather and biography domains. Most
of the development time was spent on creating the
domain-specific entity taggers for the weather do-
main. The development time would be reduced to
hours if the historical data for a domain is readily
available with the corresponding input data.
The main limitation of our system is that it re-
quires significant historical data. Our system does
consolidate many traditional components (macro-
and micro-planning, lexical choice and aggrega-
tion),5 but the system cannot be applied to the do-
mains with no historical data. The quality and the
linguistic variability of the generated text is di-
rectly proportional to the amount of historical data
available.
We also presented a new automatic metric to
evaluate generated texts at document collection
level to identify boilerplate texts. This metric
computes ?syntactic repetitiveness? by counting
the number of unique template sequences across
the given document collection.
Future work will focus on extending our frame-
work by adding additional features to the model
that could improve the quality of the generated
text. For example, most NLG pipelines have a
separate component responsible for referring ex-
pression generation (Krahmer and van Deemter,
2012). While we address the associated concern
of data consumption in Section 3.5, we currently
do not have any features that would handle refer-
ring expression generation. We believe that this
is possible by identifying referring expressions in
templates and adding features to the model to give
higher scores to the templates having relevant re-
ferring expressions. We also would like to inves-
tigate using all the top-scored templates instead
of the highest-scoring template. This would help
achieve better syntactic-variability scores by pro-
ducing more natural-sounding texts.
5Lexical choice and aggregation are ?handled? insofar as
their existence in the historical data.
Acknowledgments
This research is made possible by Thomson
Reuters Global Resources (TRGR) with particu-
lar thanks to Peter Pircher, Jaclyn Sprtel and Ben
Hachey for significant support. Thank you also
to Khalid Al-Kofahi for encouragment, Leszek
Michalak and Andrew Lipstein for expert evalua-
tions and three anonymous reviewers for construc-
tive feedback.
References
Gabor Angeli, Percy Liang, and Dan Klein. 2012. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods for Natural Language
Processing (EMNLP 2010), pages 502?512.
Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the 2005 Conference on Empirical
Methods for Natural Language Processing (EMNLP
2005), pages 331?338.
John Bateman and Michael Zock. 2003. Natural
language generation. In R. Mitkov, editor, Oxford
Handbook of Computational Linguistics, Research
in Computational Semantics, pages 284?304. Ox-
ford University Press, Oxford.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of NLG systems. In
Proceedings of the European Association for Com-
putational Linguistics (EACL?06), pages 313?320.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of Human Language
Technologies 2007: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-HLT?07), pages
164?171.
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Nadjet Bouayad-Agha, Gerard Casamayor, and Leo
Wanner. 2011. Content selection from an ontology-
based knowledge base for the generation of foot-
ball summaries. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), pages 72?81.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation, pages 85?91.
1414
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of the
2003 Conference on Empirical Methods for Natural
Language Processing (EMNLP 2003), pages 2003?
2007.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341?385.
Blake Howald, Ravi Kondadadi, and Frank Schilder.
2013. Domain adaptable semantic clustering in
statistical nlg. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013), pages 143?154. Association for Com-
putational Linguistics, March.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Colin Kelly, Ann Copestake, and Nikiforos Karama-
nis. 2009. Investigating content selection for lan-
guage generation using machine learning. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation (ENLG), pages 130?137.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378.
Emiel Krahmer and Kees van Deemter. 2012. Com-
putational generation of referring expression: A sur-
vey. Computational Linguistics, 38(1):173?218.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?98),
pages 704?710.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceed-
ings of the 2011 Conference on Empirical Methods
for Natural Language Processing (EMNLP 2011),
pages 1611?1622.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-
ural language generation with tree conditional ran-
dom fields. In Proceedings of the 2009 Conference
on Empirical Methods for Natural Language Pro-
cessing (EMNLP 2009), pages 400?409.
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Johanna D. Moore and Cecile L. Paris. 1993. Planning
text for advisory dialogues: Capturing intentional
and rhetorical information. Computational Linguis-
tics, 19(4):651?694.
Kishore Papineni, Slim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), pages 311?318.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin
Yu. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Jacques Robin and Kathy McKeown. 1996. Exmpiri-
cally designing and evaluating a new revision-based
model for summary generation. Artificial Intelli-
gence, 85(1-2).
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and
Jin Yu. 2001. A two-stage model for content
determination. In Proceedings of the 8th Euro-
pean Workshop on Natural Language Generation
(ENLG), pages 1?8.
Kees van Deemter, Marie?t Theune, and Emiel Krahmer.
2005. Real vs. template-based natural language gen-
eration: a false opposition? Computational Linguis-
tics, 31(1):15?24.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.). Morgan Kaufmann, San Fran-
cisco, CA.
1415
Proceedings of the 14th European Workshop on Natural Language Generation, pages 178?182,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
GenNext: A Consolidated Domain Adaptable NLG System
Frank Schilder, Blake Howald and Ravi Kondadadi?
Thomson Reuters, Research & Development
610 Opperman Drive, Eagan, MN 55123
firstname.lastname@thomsonreuters.com
Abstract
We introduce GenNext, an NLG system
designed specifically to adapt quickly and
easily to different domains. Given a do-
main corpus of historical texts, GenNext
allows the user to generate a template bank
organized by semantic concept via derived
discourse representation structures in con-
junction with general and domain-specific
entity tags. Based on various features
collected from the training corpus, the
system statistically learns template rep-
resentations and document structure and
produces well?formed texts (as evaluated
by crowdsourced and expert evaluations).
In addition to domain adaptation, Gen-
Next?s hybrid approach significantly re-
duces complexity as compared to tradi-
tional NLG systems by relying on tem-
plates (consolidating micro-planning and
surface realization) and minimizing the
need for domain experts. In this descrip-
tion, we provide details of GenNext?s the-
oretical perspective, architecture and eval-
uations of output.
1 Introduction
NLG systems are typically tailored to very spe-
cific domains and tasks such as text summaries
from neonatal intensive care units (SUMTIME-
NEONATE (Portet et al, 2007)) or offshore oil
rig weather reports (SUMTIME-METEO (Reiter et
al., 2005)) and require significant investments in
development resources (e.g. people, time, etc.).
For example, for SUMTIME-METEO, 12 person
months were required for two of the system com-
ponents alone (Belz, 2007). Given the subject
matter of such systems, the investment is perfectly
?Ravi Kondadadi is now affiliated with Nuance Commu-
nications, Inc.
reasonable. However, if the domains to be gener-
ated are comparatively more general, such as fi-
nancial reports or biographies, then the scaling of
development costs becomes a concern in NLG.
NLG in the editorial process for companies and
institutions where content can vary must be do-
main adaptable. Spending a year or more of devel-
opment time to produce high quality market sum-
maries, for example, is not a viable solution if it is
necessary to start from scratch to produce other re-
ports. GenNext, a hybrid system that statistically
learns document and sentence template represen-
tations from existing historical data, is developed
to be consolidated and domain adaptable. In par-
ticular, GenNext reduces complexity by avoiding
the necessity of having a separate document plan-
ner, surface realizer, etc., and extensive expert in-
volvement at the outset of system development.
Section 2 describes the theoretical background,
architecture and implementation of GenNext. Sec-
tion 3 discusses the results of a non?expert and ex-
pert crowdsourced sentence preference evaluation
task. Section 4 concludes with several future ex-
periments for system improvement.
2 Architecture of GenNext
In general, NLG systems follow a prototypical ar-
chitecture where some input data from a given do-
main is sent to a ?document planner? which de-
cides content and structuring to create a document
plan. That document plan serves as an input to
a ?micro planner? where the content is converted
into a syntactic expression (with associated con-
siderations of aggregation and referring expres-
sion generation) and a text specification is created.
The text specification then goes through the final
stage of ?surface realization? where everything is
put together into an output text (McKeown, 1985;
Reiter and Dale, 2000; Bateman and Zock, 2003).
In contrast, the architecture of GenNext (sum-
marized in Figure 1) is driven by a domain-specific
178
Figure 1: GenNext System Architecture.
corpus text. There is often a structured database
underlying the domains of corpus text, the fields
of which are used for domain specific entity tag-
ging (in addition to domain general entity tagging
[e.g. DATE, LOCATION, etc.]). An overview of
the different stages, which are a combination of
statistical (e.g., Langkilde and Knight (1998)) and
template?based (e.g., van Deemter, et al (2005))
approaches, follows in (A-E).1
A: Semantic Representation - We take a do-
main specific training corpus and reduce each
sentence to a Discourse Representation Structure
(DRS) - formal semantic representations of sen-
tences (and texts) from Discourse Representation
Theory (Kamp and Reyle, 1993; Basile and Bos,
2011). Each DRS is a combination of domain gen-
eral named entities, predicates (content words) and
relational elements (function words). In parallel,
domain specific named entity tags are identified
and are used to create templates that syntactically
represent some conceptual meaning; for example,
the short biography in (1):
(1) Sentence
a. Mr. Mitsutaka Kambe has been serving as Managing
Director of the 77 Bank, Ltd. since June 27, 2008.
b. He holds a Bachelor?s in finance from USC and a MBA
from UCLA.
Conceptual Meaning
c. SERVING | MANAGING | DIRECTOR | PERSON | ...
d. HOLDS | BACHELOR | FINANCE | MBA | HOLD | ...
Once the semantic representations are created,
they are organized and identified by semantic con-
cept (?CuId?) (described in (B)). Our assumption
is that each cluster equates with a CuId repre-
sented by each individual sentence in the cluster
and is contrastive with other CuIds (for similar ap-
1For more detail see Howald, et al (2013) - semantic
clustering and micro-planning and Kondadadi, et al (2013) -
document planning.
proaches, see Barzilay and Lapata (2005), Angeli,
et al (2010) and Lu and Ng (2011)).
B: Creating Conceptual Units - To create the
CuIds (a semi-automatic process), we cluster the
sentences using k-means clustering with k set ar-
bitrarily high to over-generate (Witten and Frank,
2005). This facilitates manual verification of the
generated clusters to merge (rather than split) them
if necessary. We assign a unique CuId to each
cluster and associate each template in the corpus to
a corresponding CuId. For example, in (2), using
the sentences in (1a-b), the identified named en-
tities are assigned to a clustered CuId (2a-b) and
then each sentence in the training corpus is re-
duced to a template (2c-d).
(2) Content Mapping
a. {CuId : 000} ? Information: person: Mr. Mitsutaka
Kambe; title: Managing Director; company: 77 Bank,
Ltd.; date: June 27, 2008
b. {CuId : 001} ? Information: person: he; degree:
Bachelor?s, MBA; subject: finance; institution: USC;
UCLA
Templates
c. {CuId : 000}: [person] has been serving as [title] of the
[company] since [date].
d. {CuId : 001}: [person] holds a [degree] in [subject]
from [institution] and a [degree] from [institution].
At this stage, we will have a set of CuIds with cor-
responding template collections which represent
the entire ?micro-planning? aspect of our system.
C: Collecting Statistics - For the ?document plan-
ning? stage, we collect a number of statistics for
each domain, for example:
? Frequency distribution of CuIds by position
? Frequency distribution of templates by position
? Frequency distribution of entity sequence
? Average number of entities by CuId and position
These statistics, in addition to entity tags and tem-
plates, are used in building different features used
by the ranking model (D).
D: Building a Ranking Model - The core compo-
nent of our system is a statistical model that ranks
a set of templates for a given position (e.g. sen-
tence 1, sentence 2, ..., sentence n) based on the
input data (see also Konstas and Lapata (2012).
The learning task is to find the rank for all the tem-
plates from all CuIds at each position. To gener-
ate the training data, we first exclude the templates
that have named entities not specified in the input
data (ensuring completeness). We then rank tem-
plates according to the edit distance (Levenshtein,
179
1966) from the template corresponding to the cur-
rent sentence in the training document. For each
template, we build a ranking model with features,
for example:
? Prior template and CuId
? Difference in number of words given position
? Most likely CuId given position and previous CuId
? Template 1-3grams given position and CuId
We use a linear kernel for a ranking SVM
(Joachims, 2002) to learn the weights associated
with each feature. Each domain has its own model
that is used when generating texts (E).
E: Generation: At generation time, our system
has a set of input data, a semantically organized
template bank and a model from training on a
given domain of texts. For each sentence, we first
exclude those templates that contain a named en-
tity not present in the input data. Then we cal-
culate the feature values times the model weight
for each of the remaining templates. The tem-
plate with the highest score is selected, filled
with matching entities from the input data and ap-
pended to the generated text. Example generations
for each domain are included in (3).
(3) Financial
a. First quarter profit per share for Brown-Forman
Corporation expected to be $0.91 per share by analysts.
b. Brown-Forman Corporation July first quarter profits will
be below that previously estimated by Wall Street with
a range between $0.89 and $0.93 per share and a projected
mean per share of $0.91 per share.
c. The consensus recommendation is Hold.
Biography
d. Mr. Satomi Mitsuzaki has been serving as Managing
Director of Mizuho Bank since June 27, 2008.
e. He was previously Director of Regional Compliance of
Kyoto Branch.
f. He is a former Managing Executive Officer and Chief
Executive Officer of new Industrial Finance Business
Group in Mitsubishi Corporation.
Weather
g. Complex low from southern Norway will drift slowly NNE
to the Lofoten Islands by early tomorrow.
h. A ridge will persist to the west of British Isles for Saturday
with a series of weak fronts moving east across
the North Sea.
i. A front will move ENE across the northern North Sea
Saturday.
3 Evaluation and Discussion
We have tested GenNext on three domains: Corpo-
rate Officer and Director Biographies (1150 texts
ranging from 3-10 period ended sentences), Fi-
nancial Texts (Mutual Fund Performances [162
texts, 2-4 sentences] and Broker Recommenda-
tions [905 texts, 8-20 sentences]), and Offshore
Oil Rig Weather Reports (1054 texts, 2-6 sen-
tences) from SUMTIME-METEO (Reiter et al,
2005). The total number of templates for the finan-
cial domain is 1379 distributed across 38 different
semantic concepts; 2836 templates across 19 con-
cepts for biography; and 2749 templates across 9
concepts for weather texts.
We have conducted several evaluation experi-
ments comparing two versions of GenNext, one
applying the ranking model (rank) and one with
random selection of templates (non-rank) (both
systems use the same template bank, CuId as-
signment and filtering) and the original texts from
which the data was extracted (original).
We used a combination of automatic (e.g.
BLEU?4 (Papineni et al, 2002), METEOR
(Denkowski and Lavie, 2011)) and human metrics
(using crowdsourcing) to evaluate the output (see
generally, Belz and Reiter (2006). However, in the
interest of space, we will restrict the discussion to
a human judgment task on output preferences. We
found this evaluation task to be most informative
for system improvement. The task asks an evalu-
ator to provide a binary preference determination
(100 sentence pairs/domain): ?Do you prefer Sen-
tence A (from original) or the corresponding Sen-
tence B (from rank or non-rank)?. This task was
performed for each domain.2 We also engaged 3
experts from the financial and 4 from the biogra-
phy domains to perform the same preference task
(average agreement was 76.22) as well as provide
targeted feedback.
For the preference results, summarized in Fig-
ure 2, we would like to see no statistically signifi-
cant difference between GenNext-rank and orig-
inal, but statistically significant differences be-
tween GenNext-rank and GenNext-non-rank, and
original and GenNext-non-rank. If this is the case,
then GenNext-rank is producing texts similar to
the original texts, and is providing an observ-
able improvement over not including the model at
all (GenNext-non-rank). This is exactly what we
see for all domains.3 However, in general, there
2Over 100 native English speakers contributed, each one
restricted to providing no more than 50 responses and only
after they successfully answered 4 initial gold data questions
correctly and continued to answer periodic gold data ques-
tions. The pair orderings were randomized to prevent click
bias. 8 judgments per sentence pair was collected (2400 judg-
ments) and average agreement was 75.87.
3Original vs. GenNext-rank : financial - ?2=.29, p?.59;
biography - ?2=3.01, p?.047; weather - ?2=.95, p?.32.
Original vs. GenNext-non-rank : financial - ?2=16.71,
p?.0001; biography - ?2=45.43, p?.0001; weather -
180
Figure 2: Cross-Domain Non-Expert Preference Evaluations.
is a greater difference between the original and
GenNext-rank biographies compared to the finan-
cial and weather texts. We take it as a goal to ap-
proach, as close as possible, the preferences for
the original texts.
The original financial documents were machine
generated from a different existing system. As
such, it is not surprising to see similarity in perfor-
mance compared to GenNext-rank and potentially
explains why preferences for the originals is some-
what low (assuming a higher preference rating for
well-formed human texts). Further, the original
weather documents are highly technical and not
easily understood by the lay person, so, again, it is
not surprising to see similar performance. Biogra-
phies were human generated and easy to under-
stand for the average reader. Here, both GenNext-
rank and GenNext-non-rank have some ground to
make up. Insights from domain experts are poten-
tially helpful in this regard.
Expert evaluations provided similar results and
agreements compared to the non?expert crowd.
Most beneficial about the expert evaluations was
the discussion of integrating certain editorial stan-
dards into the system. For example, shorter texts
were preferred to longer texts in the financial do-
main, but not the biographies. Consequently, we
could adjust weights to favor shorter templates.
Also, in biographies, sentences with subordinated
elaborations were not preferred because these con-
tained subjective comments (e.g. a leader in in-
dustry, a well respected individual, etc.). Here,
?2=24.27, p?.0001. GenNext-rank vs. GenNext-non-rank
: financial - ?2=12.81, p?.0003; biography - ?2=25.19,
p?.0001; weather - ?2=16.19, p?.0001.
we could manually curate or could automatically
detect templates with subordinated clauses and re-
move them. These types of comments are useful
to adjust the system accordingly to end user ex-
pectations.
4 Conclusion and Future Work
We have presented our system GenNext which is
domain adaptable, given adequate historical data,
and has a significantly reduced complexity com-
pared to other NLG systems (see generally, Robin
and McKeown (1996)). To the latter point, devel-
opment time for semantically processing the cor-
pus, applying domain general and specific tags,
and building a model is accomplished in days and
weeks as opposed to months and years.
Future experimentation will focus on being able
to automatically extract templates for different do-
mains to create preset banks of templates in the
absence of adequate historical data. We are also
looking into different ways to increase the vari-
ability of output texts from selecting templates
within a range of top scores (rather than just the
highest score) to providing additional generated
information from input data analytics.
Acknowledgments
This research is made possible by Thomson
Reuters Global Resources (TRGR) with particu-
lar thanks to Peter Pircher, Jaclyn Sprtel and Ben
Hachey for significant support. Thank you also
to Khalid Al-Kofahi for encouragement, Leszek
Michalak and Andrew Lipstein for expert evalua-
tions and three anonymous reviewers for construc-
tive feedback.
181
References
Gabor Angeli, Percy Liang, and Dan Klein. 2012. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods for Natural Language
Processing (EMNLP 2010), pages 502?512.
Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the 2005 Conference on Empirical
Methods for Natural Language Processing (EMNLP
2005), pages 331?338.
Valerio Basile and Johan Bos. 2011. Towards generat-
ing text from discourse representation structures. In
Proceedings of the 13th European Workshop on Nat-
ural Language Generation (ENLG), pages 145?150.
John Bateman and Michael Zock. 2003. Natural
language generation. In R. Mitkov, editor, Oxford
Handbook of Computational Linguistics, Research
in Computational Semantics, pages 284?304. Ox-
ford University Press, Oxford.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of NLG systems. In
Proceedings of the European Association for Com-
putational Linguistics (EACL?06), pages 313?320.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of Human Language
Technologies 2007: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-HLT?07), pages
164?171.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation, pages 85?91.
Blake Howald, Ravi Kondadadi, and Frank Schilder.
2013. Domain adaptable semantic clustering in sta-
tistical NLG. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013), pages 143?154. Association for Com-
putational Linguistics, March.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Ravi Kondadadi, Blake Howald, and Frank Schilder.
2013. A statistical NLG framework for aggregated
planning and realization. In Proceedings of the An-
nual Conference for the Association of Computa-
tional Linguistics (ACL 2013). Association for Com-
putational Linguistics.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?98),
pages 704?710.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceed-
ings of the 2011 Conference on Empirical Methods
for Natural Language Processing (EMNLP 2011),
pages 1611?1622.
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Kishore Papineni, Slim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), pages 311?318.
Franois Portet, Ehud Reiter, Jim Hunter, and Somaya-
julu Sripada. 2007. Automatic generation of tex-
tual summaries from neonatal intensive care data. In
In Proccedings of the 11th Conference on Artificial
Intelligence in Medicine (AIME 07). LNCS, pages
227?236.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin
Yu. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Jacques Robin and Kathy McKeown. 1996. Empiri-
cally designing and evaluating a new revision-based
model for summary generation. Artificial Intelli-
gence, 85(1-2).
Kees van Deemter, Marie?t Theune, and Emiel Krahmer.
2005. Real vs. template-based natural language gen-
eration: a false opposition? Computational Linguis-
tics, 31(1):15?24.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.). Morgan Kaufmann, San Fran-
cisco, CA.
182
