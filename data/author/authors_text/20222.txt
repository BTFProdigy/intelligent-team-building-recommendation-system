Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1634?1642,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Fast and Accurate Misspelling Correction in Large Corpora
Octavian Popescu
Fondazione Bruno Kessler
Trento, Italy
popescu@fbk.eu
Ngoc Phuoc An Vo
University of Trento
Fondazione Bruno Kessler
Trento, Italy
ngoc@fbk.eu
Abstract
There are several NLP systems whose ac-
curacy depends crucially on finding mis-
spellings fast. However, the classical ap-
proach is based on a quadratic time algo-
rithm with 80% coverage. We present a
novel algorithm for misspelling detection,
which runs in constant time and improves
the coverage to more than 96%. We use
this algorithm together with a cross docu-
ment coreference system in order to find
proper name misspellings. The experi-
ments confirmed significant improvement
over the state of the art.
1 Introduction
The problem of finding the misspelled words in a
corpus is an important issue for many NLP sys-
tems which have to process large collections of
text documents, like news or tweets corpora, dig-
italized libraries etc. Any accurate systems, such
as the ones developed for cross document corefer-
ence, text similarity, semantic search or digital hu-
manities, should be able to handle the misspellings
in corpora. However, the issue is not easy and
the required processing time, memory or the de-
pendence on external resources grow fast with the
size of the analyzed corpus; consequently, most of
the existing algorithms are inefficient. In this pa-
per, we present a novel algorithm for misspelling
detection which overcomes the drawbacks of the
previous approaches and we show that this algo-
rithm is instrumental in improving the state of the
art of a cross document coreference system.
Many spelling errors in a corpus are acciden-
tal and usually just one or two letters in a word
are affected, like existnece vs. the dictionary form
existence. Such misspellings are rather a unique
phenomenon occurring randomly in a text. For an
automatic speller which has access to a dictionary,
finding and compiling a list of correct candidates
for the misspelled words like the one above is not
very difficult. However, not all misspellings are in
this category. To begin with, proper nouns, espe-
cially foreign proper names, are not present in the
dictionary and their misspelling may affect more
than one or two characters. Moreover, the mis-
spelling of proper names may not be random, for
example there might be different spellings of the
same Chinese or Russian name in English, the in-
correct ones occurring with some frequency. Also,
especially if the corpus contains documents writ-
ten by non native speakers, the number of char-
acters varying between the correct and the actual
written form may be more than two. In this case,
finding and compiling the list of correct candidates
is computationally challenging for traditional al-
gorithms, as the distance between the source string
and the words in the candidates list is high.
The Levenshtein distance has been used to com-
pile a list of correct form candidates for a mis-
spelled word. The Levenshtein distance between
two strings counts the number of changes needed
to transform one string into the other, where a
change is one of the basic edit operations: dele-
tion, insertion, substitution of a character and the
transposition of two characters. The Edit Dis-
tance algorithm, (ED) computes the similarity be-
tween two strings according to the Levenshtein
distance. Most of the random misspellings which
are produced by a native speaker are within one
or maximum two basic edit operations (Damerau,
1964). For this reason the ED algorithm is the
most common way to detect and correct the mis-
spellings. However, there is a major inconve-
nience associated with the use of ED, namely, ED
1634
runs in quadratic time considering the length of
the strings, O(n
2
). The computation time for more
than a few thousands pairs is up to several tens of
seconds, which is impracticably large for most of
large scale applications. By comparison, the num-
ber of proper names occurring in a medium sized
English news corpus is around 200, 000, which
means that there are some 200, 000, 000 pairs.
In order to cope with the need for a lower com-
putation time, on the basis of ED, a series of algo-
rithms have been developed that run in linear time
(Navaro 2001). Unfortunately, this improvement
is not enough for practical applications which in-
volve a large amount of data coming from large
corpora. The reason is two-fold: firstly, the linear
time is still too slow (Mihov and Schulz, 2004)
and secondly, the required memory depends both
on the strings? length and on the number of differ-
ent characters between the source string and the
correct word, and may well exceed several GBs.
Another solution is to index the corpus using struc-
tures like trie trees, or large finite state automata.
However, this solution may require large amounts
of memory and is inefficient when the number of
characters that differ between the source string and
the candidate words is more than two characters
(Boytsov, 2011).
We focus specifically on misspellings for which
there is no dictionary containing the correct form
and/or for which the Levenshtein distance to the
correct word may be higher than two characters.
For this purpose, we developed a novel approach
to misspelling correction based on a non indexing
algorithm, which we call the prime mapping algo-
rithm, PM. PM runs in constant time, O(1), with
insignificant memory consumption. The running
time of the PM algorithm does not depend either
on the strings? length or on the number of different
characters between the source string and the can-
didate word. It requires a static amount of mem-
ory, ranging from a few KBs to a maximum of a
few MBs, irrespective of the size of the corpus or
the number of pairs for which the misspelling rela-
tionship is tested. We run a series of experiments
using PM on various corpora in English and Ital-
ian. The results confirm that PM is practical for
large corpora. It successfully finds the candidate
words for misspellings even for large Levenshtein
distances, being more than 30 times faster than a
linear algorithm, and several hundred times faster
than ED. The running time difference is due to the
fact that PM maps the strings into numbers and
performs only one arithmetic operation in order to
decide whether the two strings may be in a mis-
spelling relationship. Instead of a quadratic num-
ber of characters comparisons, PM executes only
one arithmetic operation with integers.
We also report here the results obtained when
using PM inside a cross document coreference
system for proper nouns. Correcting a proper
name misspelling is actually a more complex task
than correcting a misspelled common word. Some
misspellings may not be random and in order to
cope with repetitive misspellings, as the ones re-
sulting from the transliteration of foreign names,
the PM is combined with a statistical learning al-
gorithm which estimates the probability of a cer-
tain type of misspelling considering the surround-
ing characters in the source string. Unlike with
common words, where a misspelling is obvious,
in the case of proper names, John vs. Jon for ex-
ample, it is unclear whether we are looking at two
different names or a misspelling. The string sim-
ilarity evidence is combined with contextual evi-
dence provided by a CDC system to disambiguate.
To evaluate the PM algorithm we use publicly
available misspelling annotated corpora contain-
ing documents created by both native and non-
native speakers. The PM within a CDC system for
proper names is evaluated using CRIPCO (Ben-
tivogli et al., 2008). The experiments confirm that
PM is a competitive algorithm and that the CDC
system gains in accuracy by using a module of
misspelling correction.
The rest of the paper is organized as follows. In
Section 2 we review the relevant literature. In Sec-
tion 3 we introduce the PM algorithm and com-
pare it against other algorithms. In Section 4 we
present the CDC system with misspelling correc-
tion for proper names. In Section 5 we present the
results obtained on English and Italian corpora.
2 Related Work
In a seminal paper (Damerau, 1964) introduced
the ED algorithm. The rationale for this algorithm
was the empirical observation that about 80% of
the misspelled words produced by native speakers
have distance 1 to the correct word. ED cannot be
extended to increase the accuracy, because for k =
2, k being the maximal admissible distance to the
correct word, the running time is too high. Most of
the techniques developed further use ED together
with indexing methods and/or parallel processing.
In (San Segundo et al., 2001) an M-best can-
1635
didate HMM recognizer for 10,000 Spanish city
names is built for speech documents. An N-gram
language model is incorporated to minimize the
search spaces. A 90% recognition rate is reported.
The model is not easily generalizable to the situ-
ation in which the names are unknown - as it is
the case with the personal proper names in a large
corpus. The N-gram model is memory demanding
and for 200,000 different names the dimension of
the requested memory is impracticably big.
The problem related to personal proper names
was discussed in (Allan and Raghavan, 2002).
However, the paper addresses only the problem of
clustering together the names which ?sound alike?
and no cross document coreference check was car-
ried out. The technique to find similar names
is based on a noisy channel model. The condi-
tional probabilities for each two names to be sim-
ilarly spelled are computed. The time complex-
ity is quadratic, which renders this technique un-
feasible for big data. In fact, the results are re-
ported for a 100 word set. A different approach
comes from considering search queries databases
(Bassil and Alwani, 2012). These techniques are
similar to the model based on the noisy channel,
as they compute the conditional probabilities of
misspellings based on their frequencies in similar
queries. Unfortunately, large numbers of queries
for proper names are not available. A similar tech-
nique, but using morphological features, was pre-
sented in (Veronis, 1988). The method can man-
age complex combinations of typographical and
phonographic errors.
It has been noted in many works dedicated to
error correction, see among others (Mihov and
Schulz, 2004), that the ED algorithm is imprac-
ticably slow when the number of pairs is large. A
solution is to build a large tries tree. While this
solution improves the searching time drastically,
the memory consumption may be large. Automata
indexing was used in (Oflazer, 1996). While the
memory consumption is much less than for the
tries tree approaches, it is still high. For Turk-
ish, the author reported 28,825 states and 118,352
transitions labeled with surface symbols. The re-
covery error rate is 80%. In (Boytsov, 2011) a
review of indexing methods is given. Testing on
5,000 strings for k=1,2,3 is reported and the paper
shows the problem the systems run into for bigger
values of k. In (Huld?en, 2009) a solution employ-
ing approximations via an A* strategy with finite
automata is presented. The method is much faster
for k bigger than the one presented in (Chodorow
and Leacock, 2000). However, the usage of A*
for proper names may be less accurate than the
one reported in the paper, because unlike the com-
mon words in a given language, the names may
have unpredictable forms, especially the foreign
names. The results reported show how the time
and memory vary for indexing methods according
to the length of the words for k=1,2,3.
A method that uses mapping from strings to
numbers is presented in (Reynaert, 2004). This
method uses sum of exponentials. The value of
the exponential was empirically found. However,
the mapping is only approximative. Our mapping
is precise and does not use exponential operations
which are time consuming.
The study in (Navarro, 2001) is focused on non
indexing approximate string search methods, in
particular on the simple ED distance. The non-
indexing methods may reach linear running time,
but it is not always the case that they are scalable
to big data. In (Nagata et al., 2006) a study on the
type of errors produced by non-native speakers of
English is carried out, but the long distance mis-
spellings are not considered.
3 Prime Mapping Misspeling Algorithm
The algorithms based on the Levenshtein dis-
tance use the dynamic programming technique to
build a table of character to character comparisons.
We present here a novel approach to misspelling
which does not build this table, skipping the need
to compare characters. In a nutshell, the prime
mapping algorithm, PM, replaces the characters
compare operations to a unique arithmetic oper-
ation.This can be done by associating to any letter
of the alphabet a unique prime number. For ex-
ample we can associate 2 to a, 3 to b, 5 to c ...
97 to z. Any string will be mapped into a unique
number which is the product of the prime numbers
corresponding to its letters. For example the name
abba is mapped to 2 ? 3 ? 3 ? 2 = 36. By computing
the ratio between any two words we can detect the
different letters with just one operation. For exam-
ple, the difference between abba and aba is 36/12
= 3, which corresponds uniquely to b because the
product/ratio of prime numbers is unique.
Unlike the ED algorithm, the prime mapping
does not find the number of edit operations needed
to transform one string into another. In fact, two
words that have just one letter in the mutual dif-
ference set may be quite distinct: all the strings
1636
aba, aab, baa differ by one letter when compared
with abba. In order to be in a misspelling relation-
ship, the two strings should also have a common
part, like prefix or middle, or suffix. The com-
plete Prime Mapping (PM) algorithm consists of
two successive steps: (1) find all the candidate
words that differ from the target word by at most
k characters and (2) check weather the target word
and the candidate word have a common part, suf-
fix, prefix or middle part. Both steps above are
executed in constant time, therefore they do not
depend either on the length of the strings or on k,
the maximal number of different characters. Nor-
mally, k = 3, because the probability of a mis-
spelled word having more than three distinct let-
ters is insignificant, but unlike in the case of ED,
the choice of k has no influence on the running
time. The first step takes an integer ratio and a
hash table key check, both being O(1). The sec-
ond step checks if the first k letters at the begin-
ning or at the end of the word are the same, and it
requires 2k character comparisons, which is also
an O(1) process, as k is fixed. The pseudo code
and detailed description of the PM algorithm are
given below.
Algorithm 1 Prime Mapping
Require: charList wordsList, primeList, k
Ensure: misspList
1: misspList? ?
2: foreach ? in charList: p(?)? p
i
, p
i
in primeList
3: foreach w in wordsList: p(w)?
?
p(?) , ? in w
4: primeKTable?
(
n
k
)
of prime arithmetics
5: for w in wordsList do
6: for w? in wordsList, w 6= w? do
7: r?
p(w)
p(w
?
)
8: if r in primeKTable then
9: if commonPart (w, w?) 6= ? then
10: misspList?misspList + (w, w?)
11: end if
12: end if
13: end for
14: end for
map letters to prime numbers. A helpful way
to assign primes to letters is according to their fre-
quency; on average, the numbers corresponding to
names are smaller and the operation gets less time.
compute a hash table with prime arithmetics
of K primes. In the hash table primeKTable we
record all the combinations that can result from di-
viding two products which have less than k primes:
1/p
i
, p
i
, p
i
/p
j
etc. If the ratio between two map-
pings is in the hash table, then the corresponding
words have all the letters in common, except for
at most k. The number of all the combination is
k letter difference #combination Memory
1 60 480B
2 435 8K
5 142,506 0.9MB
6 593, 775 3.8MB
10 30, 045, 015 180MB
Table 1: The PM algorithm memory needs
(
n
k
)
. The memory consumption for different val-
ues for k is given in Table 1. The figures compare
extremely favorably with the ones of ED based ap-
proaches (gigs magnitude) . (line 7-8)
find misspelling candidates by ratio. By com-
puting the ratio and by checking the hash table, we
found the pairs which use the same letters, except
for at most k. The procedure commonpart checks
whether the two strings also have a common part
by looking at the start and end k. If this is the case,
the pair is in a misspelling relationship.
Figure 1: PM vs. the fastest ED type algorithm
The PM is much faster than ED. The fastest
variant of ED, which does not compare strings
having length difference bigger than 1, theoret-
ically finds only 80% of the misspellings. In
practice, only around 60% of the misspellings are
found because of proper names and words mis-
spelled by non-native speakers. The PM algorithm
considers all possible pairs, finds more than 99%
of misspellings and is 35 times faster. To obtain
the same coverage, the ED algorithm must run for
more than 100 days. The time comparison for mil-
lions of pairs is plotted in Figure 1. The experi-
ments were performed on an i5, 2.8 GHz proces-
sor.
There is an immediate improvement we can
bring to the basic variant of PM. The figures re-
ported above are obtained by doing the whole set
of possible pairs. By taking into account the fact
that two words differing by k+1 letters cannot be
k similar, we can organize the number represent-
ing the names into an array which reduced drasti-
1637
cally the number of comparisons. For example, all
the words containing the letters x, y, z cannot be
k = 2 similar with the words not containing any of
these letters. By dividing the mapping of a word to
the primes associated with the letters of an k-gram,
we know if the words containing the k-gram can
be misspelling candidates with at most k differ-
ence, and there is no more need to carry out all the
ratios. We arrange the mappings of all words into
an array such that on the first indexes we have the
words containing the less frequent k + 1 gram, on
the next indexes we have the words containing the
second less frequent k+1 gram and do not contain
the first k+1 gram, on the next indexes the words
containing the third less frequent k + 1 gram and
do not contain the first two k+1 gram, etc. In this
way, even the most frequent k + 1 gram has only
a few words assigned and consequently the num-
ber of direct comparisons is reduced to the mini-
mum. The mapping corresponding to a k+1 gram
are ordered in this array according to the length of
the words. The number of trigrams is theoretically
large, the k + 1 power of the size of the alpha-
bet. However, the number of actually occurring
k-trigrams is only a small fraction of it. For exam-
ple, for k = 2, the number of trigrams is a few hun-
dred, out of the 2, 700 possible ones. PM2gram
runs in almost a quarter of the time needed by the
basic PM. For the same set of names we obtained
the results reported in Table 2. The last column
indicates how many times the algorithm is slower
than the PM in its basic form.
algorithm time coverage times slower
basicED 132 days 99% 310
ED1 14 days 80% 35
PM 9 hours 99% 1
PM2gram 2 hours 42min 96% 0.26
Table 2: ED variants versus MP
4 Correcting Proper Names Misspellings
In this section we focus on a class of words which
do not occur in a priorly given dictionary and for
which the misspelled variants may not be random.
Proper names are representative for this class. For
example, the same Russian name occurs in corpus
as Berezovski, Berezovsky or Berezovschi because
of inaccurate transliteration. By convention, we
consider the most frequent form as the canonical
one, and all the other forms as misspelled variants.
Many times, the difference between a canonical
form and a misspelled variant follows a pattern: a
Pattern Context Example
dj?dji ovic djiukanovic djukanovic
k?kh aler kaler khaler, taler thaler
ki?ky ovsk berezovski berezovsky
n?ng chan chan-hee chang-hee
dl?del abd abdelkarim abdlkrim
Table 3: Name misspellings patterns
particular group of letters substitutes another one
in the context created by the other characters in
the name. A misspelling pattern specifies the con-
text, as prefix or suffix of a string, where a particu-
lar group of characters is a misspelling of another.
See Table 3 for examples of such patterns.
Finding and learning such patterns, along with
their probability of indicating a true misspelling,
bring an important gain to CDC systems both in
running time and in alleviating the data-sparseness
problem. The CDC system computes the prob-
ability of coreference for two mentions t and t?
using a similarity metrics into a vectorial space,
where vectors are made out of contextual features
occurring with t and t? respectively (Grishman,
1994). However, the information extracted from
documents is often too sparse to decide on coref-
erence (Popescu, 2009). Coreference has a global
effect, as the CDC systems generally improve the
coverage creating new vectors by interpolating the
information resulting from the documents which
were coreferred (Hastie et al., 2005). This infor-
mation is used to find further coreferences that no
single pair of documents would allow. Thus, miss-
ing a coreference pair may result in losing the pos-
sibility of realizing further coreferences. However,
for two mentions matching a misspelling pattern
which is highly accurate, the threshold for contex-
tual evidence is lowered. Thus, correcting a mis-
spelling is not beneficial for a single mention only,
but for the accuracy of the whole.
The strategy we adopt for finding patterns is
to work in a bootstrapping manner, enlarging the
valid patterns list while maintaining a high accu-
racy of the coreference, over 90%. Initially, we
start with an empty base of patterns. Considering
only the very high precision threshold for coref-
erence, above 98% certainty, we obtain a set of
misspelling pairs. This set is used to extract pat-
terns of misspellings via a parameter estimation
found using the EM-algorithm. The pattern is con-
sidered valid only if it also has more than a given
number of occurrences. The recursion of the pre-
vious steps is carried out by lowering with an ?
the threshold for accuracy of coreference for pat-
1638
tern candidates. The details and the pseudo code
are given below.
Algorithm 2 Misspelling Pattern Extraction
Require: thCoref , ?, minO, thAcc
Require: thCDC
Ensure: pattList
1: pattList, candPattList? ?
2: while there is a pair (t, t?) to test for coreference do
3: if (t, t?) matches p, p in pattList then
4: prob? corefProb(p)
5: else
6: use PM algorithm on pair (t, t?)
7: prob? thCoref
8: end if
9: if pair (t, t?) coref with prob then
10: candPattList? candPattList + (t, t?)
11: end if
12: extractPatterns from candPattList
13: for cp in new extracted patterns do
14: if #cp>minO and corefProb(cp)>thAcc then
15: pattList? pattList + (t, t?)
16: end if
17: end for
18: if prob>thCDC then
19: corefer (t, t?)
20: end if
21: end while
22: thCoref ? thCoref - ?
23: goto line 2
1. Compile a list of misspelling candidates
For each source string, t, try to match t against the
list of patterns (initially empty). If there is a pat-
tern matching (t, t?) then their prior probability of
coreference is the probability associated with that
pattern (line 4).
2. CDC coreference evidence For each pair (t
,t?) in the canonical candidates list use the CDC
system to compute the probability of coreference
between t and t?. If the probability of coreference
of t and t? is higher than thCoref , the default
value is 98%, then consider t as a misspelling of t?
and put (t, t?) in a candidate pattern list (line 10).
3. Extract misspelling patterns Find patterns
in the candidate pattern list. Consider only pat-
terns with more than minO occurrences, whose
default value is 10, and which have the probability
of coreference higher than thAcc, whose default
value is 90% (line 15).
4. CDC and pattern evidence For each (t,t?)
pair matching a pattern and the CDC probabil-
ity of coreference more then thCDC, whose de-
fault value is 80%, then corefer t and t? (line
21). The fact that the pair (t,t?) matches a pattern
of misspelling is considered supporting evidence
for coreference and in this way it plays a direct
role in enhancing the system coverage. Decrease
thCoref by ?,whose default is value 0.5, and re-
peat the process of finding patterns (goto line 2).
To extract the pattern from a given list of pairs,
procedure extractPatterns at line 12 above, we
generate all the suffixes and prefixes of the strings.
We compute the probability that a group of char-
acters represents a spelling error, given a certain
suffix and/or prefix. We use the EM algorithm to
compute these probabilities. For a pair (P, S) of
a prefix and a suffix, the tuples (p(P)=p, p(S)=s,
pi) are the quantities to be estimated via EM, with
pi being the coreference probability. A corefer-
ence event is directly observable, without know-
ing, however, which prefix or suffix contribute to
the coreference. The EM equations are given be-
low, where X is the observed data; Z are the hid-
den variable, p and s respectively; ? the parame-
ters (p,s, pi); Q(?,?
(t)
) the expected log likelihood
at iteration t.
E? step ?
(t)
i
?
(t)
i
= E[z
i
|x
i
, ?
(t)
]
=
p(x
i
|z
i
,?
(t)
) p(z
i
=P |?
(t)
)
p(x
i
|?
(t)
)
=
pi
(t)
[p
(t)
]
x
i
[(1?p
(t)
]
(1?x
i
)
pi
(t)
[p
(t)
]
x
i
[(1?p
(t)
]
(
1?x
i
)+(1?pi
(t)
)[s
(t)
]
x
i
[(1?s
(t)
]
(1?x
i
)
(1)
M? step ?
(t+1)
?Q(?|?
t)
?pi
= 0 pi
(t+1)
=
?
i
?
(t)
i
n
?Q(?|?
t)
?p
= 0 p
(t+1)
=
?
i
?
(t)
i
x
i
?
i
?
(t)
i
?Q(?|?
t)
?s
= 0 s
(t+1)
=
?
i
(1??
(t)
i
)x
i
?
i
(1??
(t)
i
)
(2)
5 Experiments
We performed a set of experiments on different
corpora in order to evaluate: (1) the performances
of the PM algorithm for misspelling detection, (2)
the accuracy of proper name misspelling pattern
acquisition from large corpora, and (3) the im-
provements of a CDC system, employing a cor-
rection module for proper name misspellings.
In Section 5.1 the accuracy of the PM algorithm
is tested on various corpora containing annotated
misspellings of English words. In particular, we
were interested to see the results when the edit dis-
tance between the misspelled pair is bigger than 3,
because handling bigger values for k is crucial for
finding misspelling errors produced by non-native
speakers. The evaluation is directly relevant for
the correction of the spelling of foreign names.
1639
In Section 5.2 the proper name misspelling pat-
terns were extracted from two large news cor-
pora. One corpus is part of the English Gigawords,
LDC2009T13 (Parker et al., 2009) and the sec-
ond corpus is Adige500k in Italian (Magnini et al.,
2006). We use a Named Entity Recognizer which
has an accuracy above 90% for proper names. We
evaluated the accuracy of the patterns by random
sampling.
In Section 5.3 the accuracy of the CDC system
with the correction module for proper name mis-
spellings was tested against a gold standard.
5.1 PM Evaluation
We consider the following publicly available En-
glish corpora containing the annotation of the mis-
spelled words: Birkbeck, Aspell, Holbrook, Wiki-
pidia. Birkbeck is a heterogeneous collection of
documents, so in the experiments below we re-
fer to each document separately. In particular we
distinguish between misspellings of native speak-
ers vs. misspelling of non-native speakers. Fig-
ure 2 shows that there are two types of corpora.
For the first type, the misspellings found within
two characters are between 80% and 100% of
the whole number of misspellings. For the sec-
ond type, less than 50% of the misspellings are
within two characters.The second category is rep-
resented by the misspellings of non native speak-
ers. The misspellings are far from the correct
forms and they represent chunks of phonetically
similar phonemes, like boiz vs. boys. The situa-
tion of the foreign name misspellings is likely to
be similar to the misspellings found in the sec-
ond type of corpora. For those cases, handling
a k value bigger than 2 is crucial. Not only the
Figure 2: k = 1, 2
non-indexing methods, but also indexing ones are
rather inefficient for k values bigger than 2 for
large corpora. The PM algorithm does not have
this drawback, and we tested the coverage of the
errors we found for values of k ranging from 3 to
10. In Figure 3 we plot the distributions for the
Figure 3: Foreign Misspellings
corpora which are problematic for k=2. Values of
k are plotted on the OX axis, and the percentage of
the misspellings within the respective k on the OY
axis. The results showed PM is also able to find
the phonemically similar misspellings. We can see
that for k bigger than 9 the number of misspellings
is not significant.
The PM algorithm performed very well, being
able to find the misspellings even for large k val-
ues. There were 47, 837 words in Aspell, Holbrrok
and Wikipedia, and 30, 671 in Birkbeck, and PM
found all the misspelling pairs in a running time of
25 minutes. This is a very competitive time, even
for indexing methods. For k above 8 the access to
the hash table containing the prime combinations
was slower, but not significantly so.
5.2 Pattern Extraction Evaluation
We extracted the set of names using a NER from
the two corpora, LDC2009T13 and Adige500k.
The set of proper names is rather large in both cor-
pora - 160, 869 names from the English corpus and
185, 508 from the Italian corpus. Apparently, the
quasi-similar names, which are considered as mis-
spelled name candidates, is very high. In Figure
4 we plot this data. The English Cand and Italian
Cand are absolute values, while the English True
and Italian True represent percentages. For exam-
ple, a name of length 5 is likely to have around 23
misspelling candidates, but only 17% of them are
likely to be true misspellings, the rest being differ-
ent names.
Figure 4: Candidates vs. True Misspellings
1640
The numbers are estimated considering samples
having the size between 30 and 50, for each name
length. The percentages change rapidly with the
length of the string. For names with the length
bigger than 11, the probability that a misspelling
candidate is a true misspelling is more than 98%.
This fact suggests a strategy for pattern extrac-
tion: start from the higher name length towards the
lower length names. The patterns found by the al-
gorithm described in Section 4 have between 900
and 20 occurrences. There are 12 patterns having
more than 400 occurrences, 20 having between 20
and 50 occurrences, see Fig. 5.
Figure 5: Distribution of the patterns:
5.3 CDC and Misspelling correction
The CRIPCO corpus (Bentivogli et al., 2008)
is a gold standard for CDC in Italian, contain-
ing pieces of news extracted from Adige500k.
There are 107 names, the majority being Ital-
ian names. We scrambled the names to cre-
ate misspelling candidates. For example the
name leonardo was scrambled like teonardo,
lionaldo, loenarod etc. We considered the top 15
frequency letters and maximum 4 letters for each
scrambling. We randomly selected 70% of the
original CRIPCO making no modifications, and
called this corpus CRwCR. 30% of the original
documents were assigned to the invented pseudo-
names, and we called this corpus CRwSC (cor-
rect documents with scrambled names). From
Adige500k we randomly chose 20, 000 documents
and assigned them to the scrambled names as
well, calling this corpus NCRwSC. From these
pieces we created a new corpus: 70% of the initial
CRIPCO documents with the original names, 30%
of the CRIPCO documents with scrambled names
and 20, 000 documents with the same scrambled
names. For the names occurring in CRwCR, the
scrambled names are valid name misspellings in
the CRwSC corpus, and invalid in NCRwSC.
As expected, the PM algorithm found all the
Figure 6: Proper Names CRIPCO Evaluation
misspelling candidates and some others as well.
We let the threshold confidence of coreference to
vary from 90% to 98%. The number in Figure
6 refers to the precision and recall for the name
misspellings in the CRIPCO corpus created via
random scrambling. We were also interested to
see how the pattern finding procedure works, but
scrambling randomly produced too many contexts.
Therefore, we chose to modify the names in a non
random way, by replacing the final o to ino, ex.
paolo goes to paolino, and modifying one letter in
the word for half of the occurrences, ex. paorino.
The idea is that ino is a very common suffix for
names in Italian. The system was able to learn the
pseudo alternatives created in the context ino. The
noise introduced was relatively low, see Fig. 6.
6 Conclusion and Further Research
In this paper we described a system able to correct
misspellings, including proper name misspellings,
fast and accurately. The algorithm introduced,
PM, overcomes the time/memory limitations of
the approaches based on the edit distance.
The system is built on a novel string compare
algorithm which runs in constant time indepen-
dently of the length of the names or the number of
different letters allowed, with no auxiliary mem-
ory request. As such, the algorithm is much faster
than any other non-indexing algorithms. Because
it is independent of k, it can be used even for large
k, where even the indexing methods have limita-
tions. We also used an EM based technique to find
misspelling patterns. The results obtained are very
accurate.
The system makes a first selection of the docu-
ments, drastically reducing the human work load.
Another line of future research is to use the PM
algorithm in other NLP tasks, where finding the
pairs having some particular elements in common
is necessary: for example, comparing parsing trees
or dependency trees. We think that PM can be
used in other NLP tasks as well and we hope the
community can take advantage of it.
1641
References
James Allan and Hema Raghavan. 2002. Using Part-
of-Speech Patterns to Reduce Query Ambiguity. In
Proceedings of the 25th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 307?314. ACM.
Youssef Bassil and Mohammad Alwani. 2012. OCR
Post-Processing Error Correction Algorithm Using
Google?s Online Spelling Suggestion. Journal of
Emerging Trends in Computing and Information Sci-
ences, ISSN 2079-8407, Vol. 3, No. 1.
Luisa Bentivogli, Christian Girardi, and Emanuele Pi-
anta. 2008. Creating a Gold Standard for Person
Cross-Document Coreference Resolution in Italian
News. In The Workshop Programme, page 19.
Leonid Boytsov. 2011. Indexing Methods for Approx-
imate Dictionary Searching: Comparative Analysis.
Journal of Experimental Algorithmics (JEA), 16:1?
1.
Martin Chodorow and Claudia Leacock. 2000. An Un-
supervised Method for Detecting Grammatical Er-
rors. In Proceedings of the 1st North American
chapter of the Association for Computational Lin-
guistics conference, pages 140?147. Association for
Computational Linguistics.
Fred J Damerau. 1964. A Technique for Computer
Detection and Correction of Spelling Errors. Com-
munications of the ACM, 7(3):171?176.
Ralph Grishman. 1994. Whither Written Language
Evaluation? In Proceedings of the workshop on Hu-
man Language Technology, pages 120?125. Associ-
ation for Computational Linguistics.
Trevor Hastie, Robert Tibshirani, Jerome Friedman,
and James Franklin. 2005. The Elements of Statis-
tical Learning: Data Mining, Inference and Predic-
tion. The Mathematical Intelligencer, 27(2):83?85.
M?ans Huld?en. 2009. Fast Approximate String
Matching with Finite Automata. Procesamiento del
lenguaje natural, 43:57?64.
Bernardo Magnini, Emanuele Pianta, Christian Girardi,
Matteo Negri, Lorenza Romano, Manuela Speranza,
Valentina Bartalesi Lenzi, and Rachele Sprugnoli.
2006. I-CAB: The Italian Content Annotation Bank.
In Proceedings of LREC, pages 963?968.
Stoyan Mihov and Klaus U Schulz. 2004. Fast Ap-
proximate Search in Large Dictionaries. Computa-
tional Linguistics, 30(4):451?477.
Ryo Nagata, Koichiro Morihiro, Atsuo Kawai, and
Naoki Isu. 2006. A Feedback-Augmented Method
for Detecting Errors in The Writing of Learners of
English. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 241?248. Association for
Computational Linguistics.
Gonzalo Navarro. 2001. A Guided Tour to Approx-
imate String Matching. ACM computing surveys
(CSUR), 33(1):31?88.
Kemal Oflazer. 1996. Error-tolerant Finite-state
Recognition with Applications to Morphological
Analysis and Spelling Correction. Computational
Linguistics, 22(1):73?89.
Robert Parker, Linguistic Data Consortium, et al.
2009. English Gigaword Fourth Edition. Linguistic
Data Consortium.
Octavian Popescu. 2009. Person Cross Document
Coreference with Name Perplexity Estimates. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 997?1006. Association for Com-
putational Linguistics.
Martin Reynaert. 2004. Text Induced Spelling Cor-
rection. In Proceedings of the 20th international
conference on Computational Linguistics, page 834.
Association for Computational Linguistics.
Rub?en San Segundo, Javier Mac??as Guarasa, Javier
Ferreiros, P Martin, and Jos?e Manuel Pardo. 2001.
Detection of Recognition Errors and Out of the
Spelling Dictionary Names in a Spelled Name Rec-
ognizer for Spanish. In INTERSPEECH, pages
2553?2556.
Jean Veronis. 1988. Morphosyntactic Correction in
Natural Language Interfaces. In Proceedings of
the 12th conference on Computational linguistics-
Volume 2, pages 708?713. Association for Compu-
tational Linguistics.
1642
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 284?288,
Dublin, Ireland, August 23-24, 2014.
FBK-TR: Applying SVM with Multiple Linguistic Features for
Cross-Level Semantic Similarity
Ngoc Phuoc An Vo
Fondazione Bruno Kessler
University of Trento
Trento, Italy
ngoc@fbk.eu
Tommaso Caselli
TrentoRISE
Trento, Italy
t.caselli@trentorise.eu
Octavian Popescu
Fondazione Bruno Kessler
Trento, Italy
popescu@fbk.eu
Abstract
Recently, the task of measuring seman-
tic similarity between given texts has
drawn much attention from the Natural
Language Processing community. Espe-
cially, the task becomes more interesting
when it comes to measuring the seman-
tic similarity between different-sized texts,
e.g paragraph-sentence, sentence-phrase,
phrase-word, etc. In this paper, we, the
FBK-TR team, describe our system par-
ticipating in Task 3 "Cross-Level Seman-
tic Similarity", at SemEval 2014. We also
report the results obtained by our system,
compared to the baseline and other partic-
ipating systems in this task.
1 Introduction
Measuring semantic text similarity has become a
hot trend in NLP as it can be applied to other
tasks, e.g. Information Retrieval, Paraphrasing,
Machine Translation Evaluation, Text Summariza-
tion, Question and Answering, and others. Several
approaches proposed to measure the semantic sim-
ilarity between given texts. The first approach is
based on vector space models (VSMs) (Meadow,
1992). A VSM transforms given texts into "bag-
of-words" and presents them as vectors. Then, it
deploys different distance metrics to compute the
closeness between vectors, which will return as
the distance or similarity between given texts. The
next well-known approach is using text-alignment.
By assuming that two given texts are semantically
similar, they could be aligned on word or phrase
levels. The alignment quality can serve as a simi-
larity measure. "It typically pairs words from the
two texts by maximizing the summation of the
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
word similarity of the resulting pairs" (Mihalcea
et al., 2006). In contrast, the third approach uses
machine learning techniques to learn models built
from different lexical, semantic and syntactic fea-
tures and then give predictions on degree of simi-
larity between given texts (?ari
?
c et al., 2012).
At SemEval 2014, the Task 3 "Cross-Level Se-
mantic Similarity" (Jurgens et al., 2014) is to eval-
uate the semantic similarity across different sizes
of texts, in particular, a larger-sized text is com-
pared to a smaller-sized one. The task consists
of four types of semantic similarity comparison:
paragraph to sentence, sentence to phrase, phrase
to word, and word to sense. The degree of similar-
ity ranges from 0 (different meanings) to 4 (simi-
lar meanings). For evaluation, systems were eval-
uated, first, within comparison type and second,
across all comparison types. Two methods are
used to evaluate between system outputs and gold
standard (human annotation), which are Pearson
correlation and Spearman?s rank correlation (rho).
The FBK-TR team participated in this task with
three different runs. In this paper, we present a
clear and comprehensive description of our sys-
tem which obtained competitive results. Our main
approach is using machine learning technique to
learn models from different lexical and semantic
features from train corpora to make prediction on
the test corpora. We used support vector machine
(SVM) regression model to solve the task.
The remainder of the paper is organized as fol-
lows. Section 2 presents the system overview.
Sections 3, 4 and 5 describe the Semantic Word
Similarity, String Similarity and other features, re-
spectively. Section 6 discusses about SVM ap-
proach. Section 7 presents the experiment settings
for each subtask. Finally, Sections 8 and 9 present
the evaluation and conclusion.
284
Figure 1: System Overview.
2 System Overview
Our system was built on different linguistic fea-
tures as shown in Figure 1. By constructing a
pipeline system, each linguistic feature can be
used independently or together with others to mea-
sure the semantic similarity of given texts as well
as to evaluate the significance of each feature to
the accuracy of system?s predictions. On top of
this, the system is expandable and scalable for
adopting more useful features aiming for improv-
ing the accuracy.
3 Semantic Word Similarity Measures
At the lexical level, we built a simple, yet effec-
tive Semantic Word Similarity model consisting of
three components: WordNet similarity, Wikipedia
relatedness and Latent Semantic Analysis (LSA).
These components played important and compli-
mentary roles to each other.
3.1 Data Processing
We used the TreeTagger tool (Schmid, 1994) to
extract Part-of-Speech (POS) from each given
text, then tokenize and lemmatize it. On the basis
of the POS tags, we only picked lemmas of con-
tent words (Nouns and Verbs) from the given texts
and then paired them up regarding to similar POS
tags.
3.2 WordNet Similarity and Levenshtein
Distance
WordNet (Fellbaum, 1999) is a lexical database
for the English language in which words are
grouped into sets of synonyms (namely synsets,
each expressing a distinct concept) to provide
short, general definitions, and record the vari-
ous semantic relations between synsets. We used
Perdersen?s package WordNet:Similarity (Peder-
sen et al., 2004) to obtain similarity scores for
the lexical items covered in WordNet. Similarity
scores have been computed by means of the Lin
measure (Lin, 1998). The Lin measure is built on
Resnik?s measure of similarity (Resnik, 1995):
Sim
lin
=
2 ? IC(LCS)
IC(concept
1
) + IC(concept
2
)
(1)
where IC(LCS) is the information content (IC) of
the least common subsumer (LCS) of two con-
cepts.
To overcome the limit in coverage of WordNet,
we applied the Levenshtein distance (Levenshtein,
1966). The distance between two words is defined
by the minimum number of operations (insertions,
deletions and substitutions) needed to transform
one word into the other.
3.3 Wikipedia Relatedness
Wikipedia Miner (Milne and Witten, 2013) is a
Java-based package developed for extracting se-
mantic information from Wikipedia. Through our
experiments, we observed that Wikipedia related-
ness plays an important role for providing extra
information to measure the semantic similarity be-
tween words. We used the package Wikipedia
Miner from University of Waikato (New Zealand)
to extract additional relatedness scores between
words.
3.4 Latent Semantic Analysis (LSA)
We also took advantage from corpus-based ap-
proaches to measure the semantic similarity be-
tween words by using Latent Semantic Analysis
(LSA) technique (Landauer et al., 1998). LSA as-
sumes that similar and/or related words in terms
of meaning will occur in similar text contexts. In
general, a LSA matrix is built from a large cor-
pus. Rows in the matrix represent unique words
and columns represent paragraphs or documents.
The content of the matrix corresponds to the word
count per paragraph/document. Matrix size is then
reduced by means of Single Value Decomposition
(SVD) technique. Once the matrix has been ob-
tained, similarity and/or relatedness between the
words is computed by means of cosine values
(scaled between 0 and 1) for each word vector
in the matrix. Values close to 1 are assumed to
285
be very similar/related, otherwise dissimilar. We
trained our LSA model on the British National
Corpus (BNC)
1
and Wikipedia
2
corpora.
4 String Similarity Measures
The Longest Common Substring (LCS) is the
longest string in common between two or more
strings. Two given texts are considered similar if
they are overlapping/covering each other (e.g sen-
tence 1 covers a part of sentence 2, or otherwise).
We implemented a simple algorithm to extract the
LCS between two given texts. Then we divided the
LCS length by the product of normalized lengths
of two given texts and used it as a feature.
4.1 Analysis Before and After LCS
After extracting the LCS between two given texts,
we also considered the similarity for the parts be-
fore and after the LCS. The similarity between the
text portions before and after the LSC has been ob-
tained by means of the Lin measure and the Lev-
enshtein distance.
5 Other Features
To take into account other levels of analysis for se-
mantic similarity between texts, we extended our
features by means of topic modeling and Named
Entities.
5.1 Topic Modeling (Latent Dirichlet
Allocation - LDA)
Topic modeling is a generative model of docu-
ments which allows to discover topics embedded
in a document collection and their balance in each
document. If two given texts are expressing the
same topic, they should be considered highly sim-
ilar. We applied topic modeling, particularly, La-
tent Dirichlet allocation (LDA) (Blei et al., 2003)
to predict the topics expressed by given texts.
The MALLET topic model package (McCal-
lum, 2002) is a Java-based tool used for inferring
hidden "topics" in new document collections us-
ing trained models. We used Mallet topic model-
ing tool to build different models using BNC and
Wikipedia corpora.
We noticed that, in LDA, the number of top-
ics plays an important role to fine grained predic-
tions. Hence, we built different models for differ-
ent numbers of topics, from minimum 20 topics to
1
http://www.natcorp.ox.ac.uk
2
http://en.wikipedia.org/wiki/Wikipedia:Database_download
maximum 500 topics (20, 50, 100, 150, 200, 250,
300, 350, 400, 450 and 500). From the proportion
vectors (distribution of documents over topics) of
given texts, we applied three different measures to
compute the distance between each pair of texts,
which are Cosine similarity, Kullback-Leibler and
Jensen-Shannon divergences (Gella et al., 2013).
5.2 Named-Entity Recognition (NER)
NER aims at identifying and classifying entities
in a text with respect to a predefined set of cate-
gories such as person names, organizations, loca-
tions, time expressions, quantities, monetary val-
ues, percentages, etc. By exploring the training
set, we observed that there are lot of texts in this
task containing named entities. We deployed the
Stanford Named Entity Recognizer tool (Finkel et
al., 2005) to extract the similar and overlapping
named entities between two given texts. Then we
divided the number of similar/overlapping named
entities by the sum length of two given texts.
6 Support Vector Machines (SVMs)
Support vector machine (SVM) (Cortes and Vap-
nik, 1995) is a type of supervised learning ap-
proaches. We used the LibSVM package (Chang
and Lin, 2011) to learn models from the different
linguistic features described above. However, in
SVM the problem of finding optimal kernel pa-
rameters is critical and important for the learning
process. Hence, we used practical advice (Hsu et
al., 2003) for data scaling and a grid-search pro-
cess for finding the optimal parameters (C and
gamma) for building models. We trained the SVM
models in a regression framework.
7 Experiment Settings
For subtasks paragraph-to-sentence and sentence-
to-phrase, since the length between two units is
completely different, we decided, first to apply
topic model to identify if two given texts are ex-
pressing a same topic. Furthermore, named enti-
ties play an important role in these subtasks. How-
ever, as there are many named entities which are
not English words and cannot be identified by the
NER tool, we developed a program to detect and
identify common words occurring in both given
texts. Then we continued to extract other lexical
and semantic features to measure the similarity be-
tween the two texts.
286
Team Para2Sent Para2Sent
(Pearson) (Spearman)
UNAL-NLP, run2 (ranked 1st) 0.837 0.820
ECNU, run1(ranked 1st) 0.834 0.821
FBK-TR, run2 0.77 0.775
FBK-TR, run3 0.759 0.770
FBK-TR, run1 0.751 0.759
Baseline (LCS) 0.527 0.613
Table 1: Results for paragraph-to-sentence.
Team Sent2Phr Sent2Phr
(Pearson) (Spearman)
Meerkat_Mafia, 0.777 0.760
SuperSaiyan (ranked 1st)
FBK-TR, run3 0.702 0.695
FBK-TR, run1 0.685 0.681
FBK-TR, run2 0.648 0.642
Baseline (LCS) 0.562 0.626
Table 2: Results for sentence-to-phrase.
For the subtask word-to-sense, we used the Se-
mantic Word Similarity model which consists of
three components: WordNet similarity, Wikipedia
relatedness and LSA similarity (described in sec-
tion 3). For phrase-to-word, we extracted all
glosses of the given word, then computed the simi-
larity between the given phrase and each extracted
gloss. Finally, we selected the highest similarity
score for result.
8 Evaluations
As a result, we report our performance in the four
subtasks as follows.
8.1 Subtasks: Paragraph-to-Sentence and
Sentence-to-Phrase
The evaluation results using Pearson and Spear-
man correlations show the difference between our
system and best system in these two subtasks in
the Tables 1 and 2.
Team Para2Sent Sent2Phr Phr2Word Word2Sens Sum
SimCompass 0.811 0.742 0.415 0.356 2.324
(ranked 1st)
FBK-TR 0.759 0.702 0.305 0.155 1.95
Baseline 0.527 0.562 0.165 0.109 1.363
Table 3: Overall result using Pearson.
Team Para2Sent Sent2Phr Phr2Word Word2Sens Sum
SimCompass 0.801 0.728 0.424 0.344 2.297
(ranked 1st)
FBK-TR 0.770 0.695 0.298 0.150 1.913
Baseline 0.613 0.626 0.162 0.130 1.528
Table 4: Overall result using Spearman.
8.2 Subtasks: Phrase-to-Word and
Word-to-Sense
Even though we did not submit the results as
they looked very low, we report the scores for
the phrase-to-word and word-to-sense subtasks. In
the phrase-to-word subtask, we obtained a Pearson
score of 0.305 and Spearman value of 0.298. As
for the word-to-sense subtask, we scored 0.155 for
Pearson and 0.150 for Spearman.
Overall, with the submitted results for two sub-
tasks described in Section 8.1, our system?s runs
ranked 20th, 21st and 22nd among 38 participat-
ing systems. However, by taking into account the
un-submitted results for the two other subtasks,
our best run obtained 1.95 (Pearson correlation)
and 1.913 (Spearman correlation), which can be
ranked in the top 10 among 38 systems (figures
are reported in Table 3 and 4).
9 Conclusions and Future Work
In this paper, we describe our system participating
in the Task 3, at SemEval 2014. We present a com-
pact system using machine learning approach (par-
ticularly, SVMs) to learn models from a set of lex-
ical and semantic features to predict the degree of
similarity between different-sized texts. Although
we only submitted the results for two out of four
subtasks, we obtained competitive results among
the other participants. For future work, we are
planning to increase the number of topics in LDA,
as more fine-grained topics should allow predict-
ing better similarity scores. Finally, we will inves-
tigate more on the use of syntactic features.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology (TIST), 2(3):27.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
287
Vector Networks. Machine learning, 20(3):273?
297.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370.
Spandana Gella, Bahar Salehi, Marco Lui, Karl
Grieser, Paul Cook, and Timothy Baldwin. 2013.
Unimelb_nlp-core: Integrating predictions from
multiple domains and feature sets for estimating se-
mantic textual similarity. Atlanta, Georgia, USA,
page 207.
Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, et al.
2003. A Practical Guide to Support Vector Classifi-
cation.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014) August 23-24, 2014, Dublin,
Ireland.
Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse processes, 25(2-3):259?284.
Vladimir I Levenshtein. 1966. Binary Codes Capable
of Correcting Deletions, Insertions and Reversals.
In Soviet physics doklady, volume 10, page 707.
Dekang Lin. 1998. An Information-Theoretic Defini-
tion of Similarity. In ICML, volume 98, pages 296?
304.
Andrew Kachites McCallum. 2002. Mallet: A Ma-
chine Learning for Language Toolkit.
Charles T Meadow. 1992. Text Information Retrieval
Systems. Academic Press, Inc., Orlando, FL, USA.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. In AAAI, vol-
ume 6, pages 775?780.
David Milne and Ian H Witten. 2013. An Open-
Source Toolkit for Mining Wikipedia. Artificial In-
telligence, 194:222?239.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - Measuring the Re-
latedness of Concepts. In Demonstration Papers at
HLT-NAACL 2004, pages 38?41.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. arXiv
preprint cmp-lg/9511007.
Frane ?ari?c, Goran Glava?, Mladen Karan, Jan ?najder,
and Bojana Dalbelo Ba?i?c. 2012. Takelab: Sys-
tems for Measuring Semantic Text Similarity. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 441?448.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
international conference on new methods in lan-
guage processing, volume 12, pages 44?49. Manch-
ester, UK.
288
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 289?293,
Dublin, Ireland, August 23-24, 2014.
FBK-TR: SVM for Semantic Relatedness and Corpus Patterns for RTE
Ngoc Phuoc An Vo
Fondazione Bruno Kessler
University of Trento
Trento, Italy
ngoc@fbk.eu
Octavian Popescu
Fondazione Bruno Kessler
Trento, Italy
popescu@fbk.eu
Tommaso Caselli
TrentoRISE
Trento, Italy
t.caselli@trentorise.eu
Abstract
This paper reports the description and
scores of our system, FBK-TR, which
participated at the SemEval 2014 task
#1 "Evaluation of Compositional Distribu-
tional Semantic Models on Full Sentences
through Semantic Relatedness and Entail-
ment". The system consists of two parts:
one for computing semantic relatedness,
based on SVM, and the other for identi-
fying the entailment values on the basis
of both semantic relatedness scores and
entailment patterns based on verb-specific
semantic frames. The system ranked 11
th
on both tasks with competitive results.
1 Introduction
In the Natural Language Processing community,
meaning related tasks have gained an increasing
popularity. These tasks focus, in general, on a
couple of short pieces of text, like pair of sen-
tences, and the systems are required to infer a cer-
tain meaning relationship that exists between these
texts. Two of the most popular meaning related
tasks are the identification of Semantic Text Sim-
ilarity (STS) and Recognizing Textual Entailment
(RTE). The STS tasks require to identify the de-
gree of similarity (or relatedness) that exists be-
tween two text fragments (sentences, paragraphs,
. . . ), where similarity is a broad concept and its
value is normally obtained by averaging the opin-
ion of several annotators. The RTE task requires
the identification of a directional relation between
a pair of text fragments, namely a text (T) and a
hypothesis (H). The relation (T? H) holds when-
ever the truth of H follows from T.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
At SemEval 2014, the Task #1 "Evaluation
of Compositional Distributional Semantic Models
on Full Sentences through Semantic Relatedness
and Entailment" (Marelli et al., 2014a) primarily
aimed at evaluating Compositional Distributional
Semantic Models (CDSMs) of meaning over two
subtasks, namely semantic relatedness and tex-
tual entailment (ENTAILMENT, CONTRADIC-
TION and NEUTRAL), over pairs of sentences
(Marelli et al., 2014b). Concerning the relatedness
subtask, the system outputs are evaluated against
gold standard ratings in two ways, using Pearson
correlation and Spearman?s rank correlation (rho).
The Pearson correlation is used for evaluating and
ranking the participating systems. Similarly, for
the textual entailment subtask, system outputs are
evaluated against a gold standard rating with re-
spect to accuracy.
Our team, FBK-TR, participated in both sub-
tasks with five different runs. In this paper, we
present a comprehensive description of our system
which obtained competitive results in both tasks
and which is not based on CDSMs. Our approach
for the relatedness task is based on machine learn-
ing techniques to learn models from different lexi-
cal and semantic features from the train corpus and
then to make prediction on the test corpus. Par-
ticularly, we used support vector machine (SVM)
(Chang and Lin, 2011), regression model to solve
this subtask. On the other hand, the textual en-
tailment task uses a methodology mainly based on
corpus patterns automatically extracted from an-
notated text corpora.
The remainder of the paper is organized as
follows: Section 2 presents the SVM system
for semantic relatedness. Section 3 describes
the methodology used for extracting patterns and
computing the textual entailment values. Finally,
Section 4 discusses about the evaluations and Sec-
tion 5 presents conclusions and future work.
289
Figure 1: Schema of the system for computing entailment.
2 System Overview for Semantic
Relatedness Subtask
Concerning the Semantic Relatedness subtask our
SVM system is built on different linguistic fea-
tures, ranging from relatedness at the lexical level
(WordNet based measures, Wikipedia relatedness
and Latent Semantic Analysis), to sentence level,
including topic modeling based on Latent Dirich-
let allocation (LDA) and string similarity (Longest
Common Substring).
2.1 Lexical Features
At the lexical level, we built a simple, yet effective
Semantic Word Relatedness model, which con-
sists of 3 components: WordNet similarity (based
on the Lin measure as implemented in Pedersen
package WordNet:Similarity (Pedersen et
al., 2004), Wikipedia relatedness (as provided by
the Wikipedia Miner package (Milne and Witten,
2013)), and Latent Semantic Analysis (Landauer
et al., 1998), with a model trained on the British
National Corpus (BNC)
1
and Wikipedia. At this
level of analysis, we concentrated only on the
best matched (lemma) pairs of content words, i.e.
Noun-Noun, Verb-Verb, extracted from each sen-
tence pair. The content words have been automati-
cally extracted by means of part-of-speech tagging
(TreeTagger (Schmid, 1994)) and lemmatization.
For words which are not present in WordNet,
the relatedness score has been obtained by means
of the Levenshtein distance (Levenshtein, 1966).
1
http://www.natcorp.ox.ac.uk
2.2 Topic Modeling
We have applied topic modeling based on Latent
Dirichlet allocation (LDA) (Blei et al., 2003) as
implemented in the MALLET package (McCal-
lum, 2002). The topic model was developed us-
ing the BNC and Wikipedia (with the numbers
of topics varying from 20 to 500 topics). From
the proportion vectors (distribution of documents
over topics) of the given texts, we apply 3 differ-
ent measures (Cosine similarity, Kullback-Leibler
and Jensen-Shannon divergences) to compute the
distances between each pair of sentences.
2.3 String Similarity: Longest Common
Substring
As for the string level, two given sentences are
considered similar/related if they are overlap-
ping/covering each other (e.g sentence 1 covers
a part of sentence 2, or otherwise). Hence, we
considered the text overlapping between two
given texts as a feature for our system. The
extraction of the features at the string level was
computed in two steps: first, we obtained Longest
Common Substring between two given sentences.
After this, we also considered measuring the
similarity for the parts before and after the LCS
between two given texts, by means of the Lin
measure and the Levenshtein distance.
3 System Overview for RTE Subtask
The system for the identification of the entailment
values is illustrated in Figure 1. Entailment values
290
are computed starting from a baseline (only EN-
TAILMENT and NEUTRAL values) which relies
on the output (i.e. scores) of the semantic related-
ness system. After this step, two groups of entail-
ment patterns are applied whether the surface form
of a sentence pair is affirmative (i.e. absence of
negation words) or negative. Each type of pattern
provides in output an associated entailment value
which corresponds to the final value assigned by
the system.
The entailment patterns are based on verb-
specific semantic frames that include both syn-
tactic and semantic information. Hence, we have
explicit access to the information that individual
words have and to the process of combining them
in bigger units, namely phrases, which carry out
meanings. The patterns have two properties: i.)
the senses of the words inside the pattern are sta-
ble, they do not change whatever context is added
to the left, right or inside the phrase matching the
pattern, and ii.) the replacement of a word with an-
other word belonging to a certain class changes the
senses of the words. Patterns with these properties
are called Sense Discriminative Patterns (SDPs).
It has been noted (Popescu et al., 2011) that we can
associate to a phrase that is matched by an SDP a
set of phrases for which an entailment relationship
is decidable showing that there is a direct relation-
ship between SDPs and entailment .
SDP patterns have been obtained from large
parsed corpora. To maximize the accuracy of the
corpus we have chosen sentences containing at
maximum two finite verbs from BNC and Anno-
tated English Gigaword. We parsed this corpus
with the Stanford parser, discarding the sentences
from the Annotated English Gigaword which have
a different parsing. Each words is replaced with
their possible SUMO attributes (Niles and Pease,
2003). Only the following Stanford dependen-
cies are retained as valid [n, nsub]sbj, [d,i,p]obj,
prep, [x,c]comp. We considered only the most fre-
quent occurrences of such patterns for each verb.
To cluster into a single SDP pattern, all patterns
that are sense auto-determinative, we used the
OntoNotes (Hovy et al., 2006) and CPA (Hanks,
2008) lexica. Inside each cluster, we searched
for the most general hypernyms for each syntac-
tic slot such that there are no common patterns
between clusters (Popescu, 2013). However, the
patterns thus obtained are not sufficient enough
for the task. Some expressions may be the para-
phrasis a word in the context of an SDP. To ex-
tract this information, we considered all the pairs
in training that are in an ENTAILMENT relation-
ship, with a high relatedness score (4 to 5), and we
extracted the parts that are different for each gram-
matical slot. In this way, we compiled a list of
quasi synonym phrases that can be replaced inside
an SDP without affecting the replacement. This
is the only component that depends on the train-
ing corpus. Figure 2 describes the algorithm for
computing entailment on the basis of the SDPs.
The following subsections illustrate the identifi-
cation of entailment relation for affirmative sen-
tences and negated sentences.
Figure 2: Algorithm for computing entailment.
3.1 Entailment on Affirmative Sentences
Affirmative sentences use three types of entail-
ment patterns. The switch baseline and hyponym
patterns works in this way: If two sentences are
matched by the same SDP, and the difference be-
tween them is that the second one contains a hy-
pernym on the same syntactic position, then the
first one is entailed by the second (i.e. ENTAIL-
MENT). If the two SDPs are such that the dif-
ference between them is that the second contains
a word which is not synonym, hypernym or hy-
ponym on the same syntactic position, then there is
no entailment between the two phrases (i.e. NEU-
TRAL). The entailment direction is from the sen-
tence that contains the hyponym toward the other
291
sentence. The antonym patterns check if the two
SDPs are the same, with the only difference be-
ing in the verb of the second sentence being an
antonym of the verb in the first sentence (i.e.
CONTRADICTION).
3.2 Entailment on Negative Sentences
As for negated sentences, we distinguish between
existential negative phrases (i.e. there is no or
there are no) and factual negative ones (presence
of a negative polarity word). An assumption re-
lated to each SDP is that it entails the existence
of any of the component of the pattern which can
be expressed by means of dedicated phrases. A
SDP of the kind "[Human] beat [Animal]", en-
tails both phrases, namely there is a [Human] and
there is a [Animal]. We call this set of associ-
ated existential phrases, Existential Assumptions
(EAs). This type of existential entailment obtained
through the usage of SDP has a direct consequence
for handling the ENTAILMENT, CONTRADIC-
TION and NEUTRAL types of entailment when
one of the phrases is negated. If the first phrase
belongs to the EA of the second one, then the
first phrase is entailed by the second phrase; if the
first phrase is an existential negation of a phrase
belonging to the EA set of the second phrase,
meaning that it contains the string there is/are no,
then the first one is a contradiction of the second
phrase; if neither the first phrase, nor its negation
belong to the EA set of the second phrase, then the
two sentences are neutral with respect to the en-
tailment. The general rule described in 3.1 applies
to these types of phrases as well: replacing a word
on the same syntactic slot inside a phrase that is
matched by a SDP leads to a CONTRADICTION
type of entailment, if the replacement is a hyper-
nym of the original word. Similarly, the approach
can be applied to factual negative phrases. The
scope of negation is considered to be the extension
of the SDP and thus the negative set of EAs.
4 Evaluation and Ranking
Table 1 illustrates the results for Pearson and
Spearman correlations for the relatedness subtask
on the test set. Table 2 reports the Accuracy values
for the entailment subtask on the test set.
Concerning the relatedness results our systems
ranked 11
th
out of 17 participating systems. Best
score of our system is reported in Table 1. One
of the main reason for the relatively low results
Team Pearson Spearman
ECNU_run1 (ranked 1
st
) 0.82795 0.76892
FBK-TR_run3 0.70892 0.64430
Table 1: Results for semantic relatedness subtask.
Team Accuracy
Illinois-LH_run1 (ranked 1
st
) 84.575
FBK-TR_run3 75.401
?FBK-TR_baseline 64.080
?FBK-TR_new 85.082
Table 2: Results for entailment subtask.
of the systems for this subtask concerns the fact
that it is designed for a general-level of texts (i.e.
compositionality is not taken into account).
As for the entailment subtask, our system
ranked 11
th
out of 18 participating systems. The
submitted results of the system are illustrated in
Table 2 and are compared against the best system,
our baseline system (?FBK-TR_baseline) as de-
scribed in Figure 1, and a new version of the par-
ticipating system after fixing some bugs in the sub-
mitted version due to the processing of the parser?s
output (?FBK-TR_new). The new version of the
system scores in the top provides a new state of the
art result, with an improvement of 10 points with
respect to our submitted system.
5 Conclusion and Future Work
This paper reports the description of our system,
FBK-TR, which implements a general SVM se-
mantic relatedness system based on distributional
features (LSA, LDA), knowledge-based related
features (WordNet and Wikipedia) and string over-
lap (LCS). On top of that, we added structural in-
formation at both semantic and syntactic level by
using SDP patterns. The system reached compet-
itive results in both subtasks. By correcting some
bugs in the entailment scripts, we obtained an im-
provement over our submitted systems as well as
for the best ranking system. We plan to improve
and extend the relatedness system by means of
compositional methods. Finally, the entailment
system can be improved by taking into account
additional linguistic evidences, such as the alter-
nation between indefinite and definite determiners,
noun modifiers and semantically empty heads.
292
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology (TIST), 2(3):27.
Patrick Hanks. 2008. Mapping meaning onto use: a
Pattern Dictionary of English Verbs. In Proceedings
of the AACL 2008.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In Proceedings of
the human language technology conference of the
NAACL, Companion Volume: Short Papers, pages
57?60.
Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse processes, 25(2-3):259?284.
Vladimir I Levenshtein. 1966. Binary Codes Capa-
ble of Correcting Deletions, Insertions and Rever-
sals. In Soviet Physics Doklady, volume 10, page
707.
M Marelli, L Bentivogli, M Baroni, R Bernardi,
S Menini, and R Zamparelli. 2014a. Semeval-2014
Task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation, August 23-24, 2014, Dublin,
Ireland.
M Marelli, S Menini, M Baroni, L Bentivogli,
R Bernardi, and R Zamparelli. 2014b. A SICK
cure for the evaluation of compositional distribu-
tional semantic models. In Proceedings of LREC
2014, Reykjavik (Iceland): ELRA.
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit.
David Milne and Ian H Witten. 2013. An Open-
Source Toolkit for Mining Wikipedia. Artificial In-
telligence, 194:222?239.
Ian Niles and Adam Pease. 2003. Mapping Word-
Net to the SUMO Ontology. In Proceedings of the
IEEE International Knowledge Engineering Confer-
ence, pages 23?26.
Ted Pedersen, Patwardhan Siddharth, and Michelizzi
Jason. 2004. Wordnet::Similarity: Measuring the
Relatedness of Concepts. In Proceedings of the
HLT-NAACL 2004.
Octavian Popescu, Elena Cabrio, and Bernardo
Magnini. 2011. Textual Entailment Using Chain
Clarifying Relationships. In Proceedings of the IJ-
CAI Workshop Learning by Reasoning and its Appli-
cations in Intelligent Question-Answering.
Octavian Popescu. 2013. Learning Corpus Patterns
Using Finite State Automata. In Proceedings of the
ICSC 2013.
Helmut Schmid. 1994. Probabilistic Part-of-Sspeech
Tagging Using Decision Trees. In Proceedings of
international conference on new methods in lan-
guage processing, volume 12, pages 44?49. Manch-
ester, UK.
293
