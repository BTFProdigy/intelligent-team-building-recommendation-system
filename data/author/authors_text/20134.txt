Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63?72,
Dublin, Ireland, August 23-24, 2014.
SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing
Stephan Oepen
??
, Marco Kuhlmann
?
, Yusuke Miyao
?
, Daniel Zeman
?
,
Dan Flickinger
?
, Jan Haji
?
c
?
, Angelina Ivanova
?
, and Yi Zhang
?
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Link?ping University, Department of Computer and Information Science
?
National Institute of Informatics, Tokyo
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
?
Stanford University, Center for the Study of Language and Information
?
Nuance Communications Aachen GmbH
sdp-organizers@emmtee.net
Abstract
Task 8 at SemEval 2014 defines Broad-
Coverage Semantic Dependency Pars-
ing (SDP) as the problem of recovering
sentence-internal predicate?argument rela-
tionships for all content words, i.e. the se-
mantic structure constituting the relational
core of sentence meaning. In this task
description, we position the problem in
comparison to other sub-tasks in compu-
tational language analysis, introduce the se-
mantic dependency target representations
used, reflect on high-level commonalities
and differences between these representa-
tions, and summarize the task setup, partic-
ipating systems, and main results.
1 Background and Motivation
Syntactic dependency parsing has seen great ad-
vances in the past decade, in part owing to rela-
tively broad consensus on target representations,
and in part reflecting the successful execution of a
series of shared tasks at the annual Conference for
Natural Language Learning (CoNLL; Buchholz &
Marsi, 2006; Nivre et al., 2007; inter alios). From
this very active research area accurate and efficient
syntactic parsers have developed for a wide range
of natural languages. However, the predominant
data structure in dependency parsing to date are
trees, in the formal sense that every node in the de-
pendency graph is reachable from a distinguished
root node by exactly one directed path.
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and the
proceedings footer are added by the organizers: http://
creativecommons.org/licenses/by/4.0/.
Unfortunately, tree-oriented parsers are ill-suited
for producing meaning representations, i.e. mov-
ing from the analysis of grammatical structure to
sentence semantics. Even if syntactic parsing ar-
guably can be limited to tree structures, this is not
the case in semantic analysis, where a node will
often be the argument of multiple predicates (i.e.
have more than one incoming arc), and it will often
be desirable to leave nodes corresponding to se-
mantically vacuous word classes unattached (with
no incoming arcs).
Thus, Task 8 at SemEval 2014, Broad-Coverage
Semantic Dependency Parsing (SDP 2014),
1
seeks
to stimulate the dependency parsing community
to move towards more general graph processing,
to thus enable a more direct analysis of Who did
What to Whom? For English, there exist several
independent annotations of sentence meaning over
the venerable Wall Street Journal (WSJ) text of the
Penn Treebank (PTB; Marcus et al., 1993). These
resources constitute parallel semantic annotations
over the same common text, but to date they have
not been related to each other and, in fact, have
hardly been applied for training and testing of data-
driven parsers. In this task, we have used three
different such target representations for bi-lexical
semantic dependencies, as demonstrated in Figure 1
below for the WSJ sentence:
(1) A similar technique is almost impossible to apply to
other crops, such as cotton, soybeans, and rice.
Semantically, technique arguably is dependent on
the determiner (the quantificational locus), the mod-
ifier similar, and the predicate apply. Conversely,
the predicative copula, infinitival to, and the vac-
1
See http://alt.qcri.org/semeval2014/
task8/ for further technical details, information on how to
obtain the data, and official results.
63
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
A1 A2
(a) Partial semantic dependencies in PropBank and NomBank.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2 ARG3 ARG1
ARG2mwe _and_cARG1ARG1
BV
ARG1 implicit_conjARG1
(b) DELPH-IN Minimal Recursion Semantics?derived bi-lexical dependencies (DM).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice
top
ARG1
ARG2
ARG1
ARG2
ARG2
ARG1
ARG1 ARG1 ARG1ARG1
ARG1
ARG2
ARG1
ARG2
ARG1
ARG2
ARG1 ARG1 ARG1 ARG2
(c) Enju Predicate?Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
RSTR
PAT
EXT
PAT
ACT
RSTR
ADDR
ADDR
ADDR
ADDR
APPS.m
APPS.m
CONJ.m
CONJ.m CONJ.m
top
(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).
Figure 1: Sample semantic dependency graphs for Example (1).
uous preposition marking the deep object of ap-
ply can be argued to not have a semantic contri-
bution of their own. Besides calling for node re-
entrancies and partial connectivity, semantic depen-
dency graphs may also exhibit higher degrees of
non-projectivity than is typical of syntactic depen-
dency trees.
In addition to its relation to syntactic dependency
parsing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea & Jurafsky,
2002). In much previous work, however, target
representations typically draw on resources like
PropBank and NomBank (Palmer et al., 2005; Mey-
ers et al., 2004), which are limited to argument
identification and labeling for verbal and nominal
predicates. A plethora of semantic phenomena?
for example negation and other scopal embedding,
comparatives, possessives, various types of modi-
fication, and even conjunction?typically remain
unanalyzed in SRL. Thus, its target representations
are partial to a degree that can prohibit seman-
tic downstream processing, for example inference-
based techniques. In contrast, we require parsers
to identify all semantic dependencies, i.e. compute
a representation that integrates all content words in
one structure. Another difference to common inter-
pretations of SRL is that the SDP 2014 task defini-
tion does not encompass predicate disambiguation,
a design decision in part owed to our goal to focus
on parsing-oriented, i.e. structural, analysis, and in
part to lacking consensus on sense inventories for
all content words.
Finally, a third closely related area of much cur-
rent interest is often dubbed ?semantic parsing?,
which Kate and Wong (2010) define as ?the task of
mapping natural language sentences into complete
formal meaning representations which a computer
can execute for some domain-specific application.?
In contrast to most work in this tradition, our SDP
target representations aim to be task- and domain-
independent, though at least part of this general-
ity comes at the expense of ?completeness? in the
above sense; i.e. there are aspects of sentence mean-
ing that arguably remain implicit.
2 Target Representations
We use three distinct target representations for se-
mantic dependencies. As is evident in our run-
ning example (Figure 1), showing what are called
the DM, PAS, and PCEDT semantic dependencies,
there are contentful differences among these anno-
tations, and there is of course not one obvious (or
even objective) truth. In the following paragraphs,
64
we provide some background on the ?pedigree? and
linguistic characterization of these representations.
DM: DELPH-IN MRS-Derived Bi-Lexical De-
pendencies These semantic dependency graphs
originate in a manual re-annotation of Sections 00?
21 of the WSJ Corpus with syntactico-semantic
analyses derived from the LinGO English Re-
source Grammar (ERG; Flickinger, 2000). Among
other layers of linguistic annotation, this resource?
dubbed DeepBank by Flickinger et al. (2012)?
includes underspecified logical-form meaning rep-
resentations in the framework of Minimal Recur-
sion Semantics (MRS; Copestake et al., 2005).
Our DM target representations are derived through
a two-step ?lossy? conversion of MRSs, first to
variable-free Elementary Dependency Structures
(EDS; Oepen & L?nning, 2006), then to ?pure?
bi-lexical form?projecting some construction se-
mantics onto word-to-word dependencies (Ivanova
et al., 2012). In preparing our gold-standard
DM graphs from DeepBank, the same conversion
pipeline was used as in the system submission of
Miyao et al. (2014). For this target representa-
tion, top nodes designate the highest-scoping (non-
quantifier) predicate in the graph, e.g. the (scopal)
degree adverb almost in Figure 1.
2
PAS: Enju Predicate-Argument Structures
The Enju parsing system is an HPSG-based parser
for English.
3
The grammar and the disambigua-
tion model of this parser are derived from the Enju
HPSG treebank, which is automatically converted
from the phrase structure and predicate?argument
structure annotations of the PTB. The PAS data
set is extracted from the WSJ portion of the Enju
HPSG treebank. While the Enju treebank is an-
notated with full HPSG-style structures, only its
predicate?argument structures are converted into
the SDP data format for use in this task. Top
nodes in this representation denote semantic heads.
Again, the system description of Miyao et al. (2014)
provides more technical detail on the conversion.
PCEDT: Prague Tectogrammatical Bi-Lexical
Dependencies The Prague Czech-English De-
pendency Treebank (PCEDT; Haji
?
c et al., 2012)
4
is a set of parallel dependency trees over the WSJ
2
Note, however, that non-scopal adverbs act as mere in-
tersective modifiers, e.g. loudly is a predicate in DM, but the
main verb provides the top node in structures like Abrams
sang loudly.
3
See http://kmcs.nii.ac.jp/enju/.
4
See http://ufal.mff.cuni.cz/pcedt2.0/.
id form lemma pos top pred arg1 arg2
#20200002
1 Ms. Ms. NNP ? + _ _
2 Haag Haag NNP ? ? compound ARG1
3 plays play VBZ + + _ _
4 Elianti Elianti NNP ? ? _ ARG2
5 . . . ? ? _ _
Table 1: Tabular SDP data format (showing DM).
texts from the PTB, and their Czech translations.
Similarly to other treebanks in the Prague family,
there are two layers of syntactic annotation: an-
alytical (a-trees) and tectogrammatical (t-trees).
PCEDT bi-lexical dependencies in this task have
been extracted from the t-trees. The specifics of
the PCEDT representations are best observed in the
procedure that converts the original PCEDT data to
the SDP data format; see Miyao et al. (2014). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
3 Graph Representation
The SDP target representations can be character-
ized as labeled, directed graphs. Formally, a se-
mantic dependency graph for a sentence x =
x
1
, . . . , x
n
is a structure G = (V,E, `
V
, `
E
) where
V = {1, . . . , n} is a set of nodes (which are in
one-to-one correspondence with the tokens of the
sentence); E ? V ? V is a set of edges; and `
V
and `
E
are mappings that assign labels (from some
finite alphabet) to nodes and edges, respectively.
More specifically for this task, the label `
V
(i) of a
node i is a tuple consisting of four components: its
word form, lemma, part of speech, and a Boolean
flag indicating whether the corresponding token
represents a top predicate for the specific sentence.
The label `
E
(i? j) of an edge i? j is a seman-
tic relation that holds between i and j. The exact
definition of what constitutes a top node and what
semantic relations are available differs among our
three target representations, but note that top nodes
can have incoming edges.
All data provided for the task uses a column-
based file format (dubbed the SDP data format)
similar to the one of the 2009 CoNLL Shared Task
(Haji
?
c et al., 2009). As in that task, we assume gold-
standard sentence and token segmentation. For
ease of reference, each sentence is prefixed by a
line with just a unique identifier, using the scheme
2SSDDIII, with a constant leading 2, two-digit sec-
tion code, two-digit document code (within each
65
section), and three-digit item number (within each
document). For example, identifier 20200002 de-
notes the second sentence in the first file of PTB
Section 02, the classic Ms. Haag plays Elianti. The
annotation of this sentence is shown in Table 1.
With one exception, our fields (i.e. columns in
the tab-separated matrix) are a subset of the CoNLL
2009 inventory: (1) id, (2) form, (3) lemma, and
(4) pos characterize the current token, with token
identifiers starting from 1 within each sentence. Be-
sides the lemma and part-of-speech information, in
the closed track of our task, there is no explicit
analysis of syntax. Across the three target represen-
tations in the task, fields (1) and (2) are aligned and
uniform, i.e. all representations annotate exactly
the same text. On the other hand, fields (3) and (4)
are representation-specific, i.e. there are different
conventions for lemmatization, and part-of-speech
assignments can vary (but all representations use
the same PTB inventory of PoS tags).
The bi-lexical semantic dependency graph over
tokens is represented by two or more columns start-
ing with the obligatory, binary-valued fields (5)
top and (6) pred. A positive value in the top
column indicates that the node corresponding to
this token is a top node (see Section 2 below). The
pred column is a simplification of the correspond-
ing field in earlier tasks, indicating whether or not
this token represents a predicate, i.e. a node with
outgoing dependency edges. With these minor dif-
ferences to the CoNLL tradition, our file format can
represent general, directed graphs, with designated
top nodes. For example, there can be singleton
nodes not connected to other parts of the graph,
and in principle there can be multiple tops, or a
non-predicate top node.
To designate predicate?argument relations, there
are as many additional columns as there are pred-
icates in the graph (i.e. tokens marked + in the
pred column); these additional columns are called
(7) arg1, (8) arg2, etc. These colums contain
argument roles relative to the i-th predicate, i.e. a
non-empty value in column arg1 indicates that
the current token is an argument of the (linearly)
first predicate in the sentence. In this format, graph
reentrancies will lead to a token receiving argument
roles for multiple predicates (i.e. non-empty arg
i
values in the same row). All tokens of the same sen-
tence must always have all argument columns filled
in, even on non-predicate words; in other words,
all lines making up one block of tokens will have
the same number n of fields, but n can differ across
DM PAS PCEDT
(1) # labels 51 42 68
(2) % singletons 22.62 4.49 35.79
(3) # edge density 0.96 1.02 0.99
(4) %
g
trees 2.35 1.30 56.58
(5) %
g
projective 3.05 1.71 53.29
(6) %
g
fragmented 6.71 0.23 0.56
(7) %
n
reentrancies 27.35 29.40 9.27
(8) %
g
topless 0.28 0.02 0.00
(9) # top nodes 0.9972 0.9998 1.1237
(10) %
n
non-top roots 44.71 55.92 4.36
Table 2: Contrastive high-level graph statistics.
sentences, depending on the count of graph nodes.
4 Data Sets
All three target representations are annotations of
the same text, Sections 00?21 of the WSJ Cor-
pus. For this task, we have synchronized these
resources at the sentence and tokenization levels
and excluded from the SDP 2014 training and test-
ing data any sentences for which (a) one or more of
the treebanks lacked a gold-standard analysis; (b) a
one-to-one alignment of tokens could not be estab-
lished across all three representations; or (c) at least
one of the graphs was cyclic. Of the 43,746 sen-
tences in these 22 first sections of WSJ text, Deep-
Bank lacks analyses for close to 15%, and the Enju
Treebank has gaps for a little more than four per-
cent. Some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB
idiosyncrasies like ?G.m.b, H.?, ?S.p, A.?, and
?U.S., .?, and introducing a few new ones (Fares
et al., 2013). Finally, 232 of the graphs obtained
through the above conversions were cyclic. In total,
we were left with 34,004 sentences (or 745,543
tokens) as training data (Sections 00?20), and 1348
testing sentences (29,808 tokens), from Section 21.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 2
shows some high-level statistics of the graphs com-
prising the training data.
5
In terms of distinctions
5
These statistics are obtained using the ?official? SDP
toolkit. We refer to nodes that have neither incoming nor
outgoing edges and are not marked as top nodes as singletons;
these nodes are ignored in subsequent statistics, e.g. when
determining the proportion of edges per node (3) or the per-
centages of rooted trees (4) and fragmented graphs (6). The
notation ?%
n
? denotes (non-singleton) node percentages, and
?%
g
? percentages over all graphs. We consider a root node any
(non-singleton) node that has no incoming edges; reentrant
nodes have at least two incoming edges. Following Sagae and
Tsujii (2008), we consider a graph projective when there are
no crossing edges (in a left-to-right rendering of nodes) and no
roots are ?covered?, i.e. for any root j there is no edge i? k
66
Directed Undirected
DM PAS PCEDT DM PAS PCEDT
DM ? .6425 .2612 ? .6719 .5675
PAS .6688 ? .2963 .6993 ? .5490
PCEDT .2636 .2963 ? .5743 .5630 ?
Table 3: Pairwise F
1
similarities, including punctu-
ation (upper right diagonals) or not (lower left).
drawn in dependency labels (1), there are clear dif-
ferences between the representations, with PCEDT
appearing linguistically most fine-grained, and PAS
showing the smallest label inventory. Unattached
singleton nodes (2) in our setup correspond to
tokens analyzed as semantically vacuous, which
(as seen in Figure 1) include most punctuation
marks in PCEDT and DM, but not PAS. Further-
more, PCEDT (unlike the other two) analyzes some
high-frequency determiners as semantically vacu-
ous. Conversely, PAS on average has more edges
per (non-singleton) nodes than the other two (3),
which likely reflects its approach to the analysis of
functional words (see below).
Judging from both the percentage of actual trees
(4), the proportions of projective graphs (5), and the
proportions of reentrant nodes (7), PCEDT is much
more ?tree-oriented? than the other two, which at
least in part reflects its approach to the analysis
of modifiers and determiners (again, see below).
We view the small percentages of graphs without
at least one top node (8) and of graphs with at
least two non-singleton components that are not
interconnected (6) as tentative indicators of general
well-formedness. Intuitively, there should always
be a ?top? predicate, and the whole graph should
?hang together?. Only DM exhibits non-trivial (if
small) degrees of topless and fragmented graphs,
and these may indicate imperfections in the Deep-
Bank annotations or room for improvement in the
conversion from full MRSs to bi-lexical dependen-
cies, but possibly also exceptions to our intuitions
about semantic dependency graphs.
Finally, in Table 3 we seek to quantify pairwise
structural similarity between the three representa-
tions in terms of unlabeled dependency F
1
(dubbed
UF in Section 5 below). We provide four variants
of this metric, (a) taking into account the direc-
tionality of edges or not and (b) including edges
involving punctuation marks or not. On this view,
DM and PAS are structurally much closer to each
other than either of the two is to PCEDT, even more
such that i < j < k.
so when discarding punctuation. While relaxing
the comparison to ignore edge directionality also
increases similarity scores for this pair, the effect
is much more pronounced when comparing either
to PCEDT. This suggests that directionality of se-
mantic dependencies is a major source of diversion
between DM and PAS on the one hand, and PCEDT
on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntac-
tic and semantic dependency annotation schemes
according to the role that functional elements take.
In Figure 1 and the discussion of Table 2 above, we
already observed that PAS differs from the other
representations in integrating into the graph aux-
iliaries, the infinitival marker, the case-marking
preposition introducing the argument of apply (to),
and most punctuation marks;
6
while these (and
other functional elements, e.g. complementizers)
are analyzed as semantically vacuous in DM and
PCEDT, they function as predicates in PAS, though
do not always serve as ?local? top nodes (i.e. the se-
mantic head of the corresponding sub-graph): For
example, the infinitival marker in Figure 1 takes the
verb as its argument, but the ?upstairs? predicate
impossible links directly to the verb, rather than to
the infinitival marker as an intermediate.
At the same time, DM and PAS pattern alike
in their approach to modifiers, e.g. attributive ad-
jectives, adverbs, and prepositional phrases. Un-
like in PCEDT (or common syntactic dependency
schemes), these are analyzed as semantic predi-
cates and, thus, contribute to higher degrees of
node reentrancy and non-top (structural) roots.
Roughly the same holds for determiners, but here
our PCEDT projection of Prague tectogrammatical
trees onto bi-lexical dependencies leaves ?vanilla?
articles (like a and the) as singleton nodes.
The analysis of coordination is distinct in the
three representations, as also evident in Figure 1.
By design, DM opts for what is often called
the Mel?
?
cukian analysis of coordinate structures
(Mel?
?
cuk, 1988), with a chain of dependencies
rooted at the first conjunct (which is thus consid-
ered the head, ?standing in? for the structure at
large); in the DM approach, coordinating conjunc-
tions are not integrated with the graph but rather
contribute different types of dependencies. In PAS,
the final coordinating conjunction is the head of the
6
In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as
both predicates, arguments, and top nodes.
67
employee stock investment plans
compound compound compound
employee stock investment plans
ARG1
ARG1
ARG1
employee stock investment plans
ACT
PAT REG
Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .
structure and each coordinating conjunction (or in-
tervening punctuation mark that acts like one) is a
two-place predicate, taking left and right conjuncts
as its arguments. Conversely, in PCEDT the last
coordinating conjunction takes all conjuncts as its
arguments (in case there is no overt conjunction, a
punctuation mark is used instead); additional con-
junctions or punctuation marks are not connected
to the graph.
7
A linguistic difference between our representa-
tions that highlights variable granularities of anal-
ysis and, relatedly, diverging views on the scope
of the problem can be observed in Figure 2. Much
noun phrase?internal structure is not made explicit
in the PTB, and the Enju Treebank from which
our PAS representation derives predates the brack-
eting work of Vadas and Curran (2007). In the
four-way nominal compounding example of Fig-
ure 2, thus, PAS arrives at a strictly left-branching
tree, and there is no attempt at interpreting seman-
tic roles among the members of the compound ei-
ther; PCEDT, on the other hand, annotates both the
actual compound-internal bracketing and the as-
signment of roles, e.g. making stock the PAT(ient)
of investment. In this spirit, the PCEDT annota-
tions could be directly paraphrased along the lines
of plans by employees for investment in stocks. In
a middle position between the other two, DM dis-
ambiguates the bracketing but, by design, merely
assigns an underspecified, construction-specific de-
pendency type; its compound dependency, then,
is to be interpreted as the most general type of de-
pendency that can hold between the elements of
this construction (i.e. to a first approximation either
an argument role or a relation parallel to a prepo-
sition, as in the above paraphrase). The DM and
PCEDT annotations of this specific example hap-
pen to diverge in their bracketing decisions, where
the DM analysis corresponds to [...] investments
in stock for employees, i.e. grouping the concept
7
As detailed by Miyao et al. (2014), individual con-
juncts can be (and usually are) arguments of other predicates,
whereas the topmost conjunction only has incoming edges in
nested coordinate structures. Similarly, a ?shared? modifier of
the coordinate structure as a whole would take as its argument
the local top node of the coordination in DM or PAS (i.e. the
first conjunct or final conjunction, respectively), whereas it
would depend as an argument on all conjuncts in PCEDT.
employee stock (in contrast to ?common stock?).
Without context and expert knowledge, these de-
cisions are hard to call, and indeed there has been
much previous work seeking to identify and anno-
tate the relations that hold between members of a
nominal compound (see Nakov, 2013, for a recent
overview). To what degree the bracketing and role
disambiguation in this example are determined by
the linguistic signal (rather than by context and
world knowledge, say) can be debated, and thus the
observed differences among our representations in
this example relate to the classic contrast between
?sentence? (or ?conventional?) meaning, on the one
hand, and ?speaker? (or ?occasion?) meaning, on
the other hand (Quine, 1960; Grice, 1968). In
turn, we acknowledge different plausible points of
view about which level of semantic representation
should be the target representation for data-driven
parsing (i.e. structural analysis guided by the gram-
matical system), and which refinements like the
above could be construed as part of a subsequent
task of interpretation.
5 Task Setup
Training data for the task, providing all columns in
the file format sketched in Section 3 above, together
with a first version of the SDP toolkit?including
graph input, basic statistics, and scoring?were
released to candidate participants in early Decem-
ber 2013. In mid-January, a minor update to the
training data and optional syntactic ?companion?
analyses (see below) were provided, and in early
February the description and evaluation of a sim-
ple baseline system (using tree approximations and
the parser of Bohnet, 2010). Towards the end of
March, an input-only version of the test data was
released, with just columns (1) to (4) pre-filled; par-
ticipants then had one week to run their systems on
these inputs, fill in columns (5), (6), and upwards,
and submit their results (from up to two different
runs) for scoring. Upon completion of the testing
phase, we have shared the gold-standard test data,
official scores, and system results for all submis-
sions with participants and are currently preparing
all data for general release through the Linguistic
Data Consortium.
68
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05
Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42
Copenhagen-
80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01
Malm?
Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19
Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82
Link?ping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68
CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12
Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82
Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42
Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60
In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30
Table 4: Results of the closed (top) and open tracks (bottom). For each system, the second column (LF)
indicates the averaged LF score across all target representations), which was used to rank the systems.
Evaluation Systems participating in the task
were evaluated based on the accuracy with which
they can produce semantic dependency graphs for
previously unseen text, measured relative to the
gold-standard testing data. The key measures for
this evaluation were labeled and unlabeled preci-
sion and recall with respect to predicted dependen-
cies (predicate?role?argument triples) and labeled
and unlabeled exact match with respect to complete
graphs. In both contexts, identification of the top
node(s) of a graph was considered as the identifi-
cation of additional, ?virtual? dependencies from
an artificial root node (at position 0). Below we
abbreviate these metrics as (a) labeled precision,
recall, and F
1
: LP, LR, LF; (b) unlabeled precision,
recall, and F
1
: UP, UR, UF; and (c) labeled and
unlabeled exact match: LM, UM.
The ?official? ranking of participating systems, in
both the closed and the open tracks, is determined
based on the arithmetic mean of the labeled depen-
dency F
1
scores (i.e. the geometric mean of labeled
precision and labeled recall) on the three target rep-
resentations (DM, PAS, and PCEDT). Thus, to be
considered for the final ranking, a system had to
submit semantic dependencies for all three target
representations.
Closed vs. Open Tracks The task was sub-
divided into a closed track and an open track, where
systems in the closed track could only be trained
on the gold-standard semantic dependencies dis-
tributed for the task. Systems in the open track, on
the other hand, could use additional resources, such
as a syntactic parser, for example?provided that
they make sure to not use any tools or resources
that encompass knowledge of the gold-standard
syntactic or semantic analyses of the SDP 2014
test data, i.e. were directly or indirectly trained or
otherwise derived from WSJ Section 21.
This restriction implies that typical off-the-shelf
syntactic parsers had to be re-trained, as many data-
driven parsers for English include this section of
the PTB in their default training data. To simplify
participation in the open track, the organizers pre-
pared ready-to-use ?companion? syntactic analyses,
sentence- and token-aligned to the SDP data, in
two formats, viz. PTB-style phrase structure trees
obtained from the parser of Petrov et al. (2006) and
Stanford Basic syntactic dependencies (de Marn-
effe et al., 2006) produced by the parser of Bohnet
and Nivre (2012).
6 Submissions and Results
From 36 teams who had registered for the task,
test runs were submitted for nine systems. Each
team submitted one or two test runs per track. In
total, there were ten runs submitted to the closed
track and nine runs to the open track. Three teams
submitted to both the closed and the open track.
The main results are summarized and ranked in
Table 4. The ranking is based on the average LF
score across all three target representations, which
is given in the LF column. In cases where a team
submitted two runs to a track, only the highest-
ranked score is included in the table.
69
Team Track Approach Resources
Link?ping C extension of Eisner?s algorithm for DAGs, edge-factored
structured perceptron
?
Potsdam C & O graph-to-tree transformation, Mate companion
Priberam C & O model with second-order features, decoding with dual decom-
position, MIRA
companion
Turku O cascade of SVM classifiers (dependency recognition, label
classification, top recognition)
companion,
syntactic n-grams,
word2vec
Alpage C & O transition-based parsing for DAGs, logistic regression, struc-
tured perceptron
companion,
Brown clusters
Peking C transition-based parsing for DAGs, graph-to-tree transforma-
tion, parser ensemble
?
CMU O edge classification by logistic regression, edge-factored struc-
tured SVM
companion
Copenhagen-Malm? C graph-to-tree transformation, Mate ?
In-House O existing parsers developed by the organizers grammars
Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).
In the closed track, the average LF scores across
target representations range from 85.91 to 72.20.
Comparing the results for different target represen-
tations, the average LF scores across systems are
85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.
The scores for labeled exact match show a much
larger variation across both target representations
and systems.
8
In the open track, we see very similar trends.
The average LF scores across target representations
range from 86.27 to 75.89 and the corresponding
scores across systems are 88.64 for PAS, 84.95
for DM, and 67.52 for PCEDT. While these scores
are consistently higher than in the closed track,
the differences are small. In fact, for each of the
three teams that submitted to both tracks (Alpage,
Potsdam, and Priberam) improvements due to the
use of additional resources in the open track do not
exceed two points LF.
7 Overview of Approaches
Table 5 shows a summary of the systems that sub-
mitted final results. Most of the systems took
a strategy to use some algorithm to process (re-
stricted types of) graph structures, and apply ma-
chine learning like structured perceptrons. The
methods for processing graph structures are clas-
sified into three types. One is to transform graphs
into trees in the preprocessing stage, and apply con-
ventional dependency parsing systems (e.g. Mate;
Bohnet, 2010) to the converted trees. Some sys-
tems simply output the result of dependency pars-
ing (which means they inherently lose some depen-
8
Please see the task web page at the address indicated
above for full labeled and unlabeled scores.
dencies), while the others apply post-processing
to recover non-tree structures. The second strat-
egy is to use a parsing algorithm that can directly
generate graph structures (in the spirit of Sagae &
Tsujii, 2008; Titov et al., 2009). In many cases
such algorithms generate restricted types of graph
structures, but these restrictions appear feasible for
our target representations. The last approach is
more machine learning?oriented; they apply classi-
fiers or scoring methods (e.g. edge-factored scores),
and find the highest-scoring structures by some de-
coding method.
It is difficult to tell which approach is the best;
actually, the top three systems in the closed and
open tracks selected very different approaches. A
possible conclusion is that exploiting existing sys-
tems or techniques for dependency parsing was
successful; for example, Peking built an ensemble
of existing transition-based and graph-based depen-
dency parsers, and Priberam extended an existing
dependency parser. As we indicated in the task de-
scription, a novel feature of this task is that we have
to compute graph structures, and cannot assume
well-known properties like projectivity and lack of
reentrancies. However, many of the participants
found that our representations are mostly tree-like,
and this fact motivated them to apply methods that
have been well studied in the field of syntactic de-
pendency parsing.
Finally, we observe that three teams participated
in both the closed and open tracks, and all of them
reported that adding external resources improved
accuracy by a little more than one point. Systems
with (only) open submissions extensively use syn-
tactic features (e.g. dependency paths) from exter-
nal resources, and they are shown effective even
70
with simple machine learning models. Pre-existing,
tree-oriented dependency parsers are relatively ef-
fective, especially when combined with graph-to-
tree transformation. Comparing across our three
target representations, system scores show a ten-
dency PAS> DM> PCEDT, which can be taken as
a tentative indicator of relative levels of ?parsabil-
ity?. As suggested in Section 4, this variation most
likely correlates at least in part with diverging de-
sign decisions, e.g. the inclusion of relatively local
and deterministic dependencies involving function
words in PAS, or the decision to annotate contex-
tually determined speaker meaning (rather than
?mere? sentence meaning) in at least some construc-
tions in PCEDT.
8 Conclusions and Outlook
We have described the motivation, design, and out-
comes of the SDP 2014 task on semantic depen-
dency parsing, i.e. retrieving bi-lexical predicate?
argument relations between all content words
within an English sentence. We have converted to
a common format three existing annotations (DM,
PAS, and PCEDT) over the same text and have put
this to use for the first time in training and testing
data-driven semantic dependency parsers. Building
on strong community interest already to date and
our belief that graph-oriented dependency parsing
will further gain importance in the years to come,
we are preparing a similar (slightly modified) task
for SemEval 2015. Candidate modifications and
extensions will include cross-domain testing and
evaluation at the level of ?complete? predications
(in contrast to more lenient per-dependency F
1
used
this year). As optional new sub-tasks, we plan on
offering cross-linguistic variation and predicate (i.e.
semantic frame) disambiguation for at least some of
the target representations. To further probe the role
of syntax in the recovery of semantic dependency
relations, we will make available to participants
a wider selection of syntactic analyses, as well as
add a third (idealized) ?gold? track, where syntactic
dependencies are provided directly from available
syntactic annotations of the underlying treebanks.
Acknowledgements
We are grateful to ?eljko Agi
?
c and Bernd Bohnet
for consultation and assistance in preparing our
baseline and companion parses, to the Linguistic
Data Consortium (LDC) for support in distributing
the SDP data to participants, as well as to Emily M.
Bender and two anonymous reviewers for feedback
on this manuscript. Data preparation was supported
through access to the ABEL high-performance com-
puting facilities at the University of Oslo, and we
acknowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. Part of this
work has been supported by the infrastructural fund-
ing by the Ministry of Education, Youth and Sports
of the Czech Republic (CEP ID LM2010013).
References
Bohnet, B. (2010). Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (p. 89 ? 97). Beijing, China.
Bohnet, B., & Nivre, J. (2012). A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 1455 ? 1465). Jeju
Island, Korea.
Buchholz, S., & Marsi, E. (2006). CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of the 10th Conference on Natural Lan-
guage Learning (p. 149 ? 164). New York, NY,
USA.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4),
281 ? 332.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449 ? 454). Genoa, Italy.
Fares, M., Oepen, S., & Zhang, Y. (2013). Machine
learning for high-quality tokenization. Replicating
variable tokenization schemes. In Computational lin-
guistics and intelligent text processing (p. 231 ? 244).
Springer.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., & Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28,
71
245 ? 288.
Grice, H. P. (1968). Utterer?s meaning, sentence-
meaning, and word-meaning. Foundations of Lan-
guage, 4(3), 225 ? 242.
Haji?c, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart?, M. A., M?rquez, L., . . . Zhang, Y. (2009).
The CoNLL-2009 Shared Task. syntactic and seman-
tic dependencies in multiple languages. In Proceed-
ings of the 13th Conference on Natural Language
Learning (p. 1 ? 18). Boulder, CO, USA.
Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P., Bojar,
O., Cinkov?, S., . . . ?abokrtsk?, Z. (2012). An-
nouncing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(p. 3153 ? 3160). Istanbul, Turkey.
Ivanova, A., Oepen, S., ?vrelid, L., & Flickinger, D.
(2012). Who did what to whom? A contrastive study
of syntacto-semantic dependencies. In Proceedings
of the Sixth Linguistic Annotation Workshop (p. 2 ?
11). Jeju, Republic of Korea.
Kate, R. J., & Wong, Y. W. (2010). Semantic pars-
ing. The task, the state of the art and the future. In
Tutorial abstracts of the 20th Meeting of the Associ-
ation for Computational Linguistics (p. 6). Uppsala,
Sweden.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A.
(1993). Building a large annotated corpora of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19, 313 ? 330.
Mel?
?
cuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R.,
Zielinska, V., Young, B., & Grishman, R. (2004).
Annotating noun argument structure for NomBank.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (p. 803 ?
806). Lisbon, Portugal.
Miyao, Y., Oepen, S., & Zeman, D. (2014). In-house:
An ensemble of pre-existing off-the-shelf parsers. In
Proceedings of the 8th International Workshop on
Semantic Evaluation. Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291 ? 330.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson,
J., Riedel, S., & Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., & L?nning, J. T. (2006). Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71 ? 106.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Meeting of the Association for Computational
Linguistics (p. 433 ? 440). Sydney, Australia.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT press.
Sagae, K., & Tsujii, J. (2008). Shift-reduce depen-
dency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (p. 753 ? 760). Manchester, UK.
Titov, I., Henderson, J., Merlo, P., & Musillo, G.
(2009). Online graph planarisation for synchronous
parsing of semantic and syntactic dependencies. In
Proceedings of the 21st International Joint Confer-
ence on Artifical Intelligence (p. 1562 ? 1567).
Vadas, D., & Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computa-
tional Linguistics (p. 240 ? 247). Prague, Czech Re-
public.
72
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 6?11,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
Verbal Valency Frame Detection and Selection in Czech and English
Ond
?
rej Du?ek, Jan Haji
?
c and Zde
?
nka Ure?ov?
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?m
?
est? 25, 11800 Prague 1, Czech Republic
{odusek,hajic,uresova}@ufal.mff.cuni.cz
Abstract
We present a supervised learning method
for verbal valency frame detection and se-
lection, i.e., a specific kind of word sense
disambiguation for verbs based on subcat-
egorization information, which amounts
to detecting mentions of events in text.
We use the rich dependency annotation
present in the Prague Dependency Tree-
banks for Czech and English, taking ad-
vantage of several analysis tools (taggers,
parsers) developed on these datasets pre-
viously. The frame selection is based on
manually created lexicons accompanying
these treebanks, namely on PDT-Vallex for
Czech and EngVallex for English. The re-
sults show that verbal predicate detection
is easier for Czech, but in the subsequent
frame selection task, better results have
been achieved for English.
1 Introduction
Valency frames are a detailed semantic and syn-
tactic description of individual predicate senses.
1
As such, they represent different event types. We
present a system for automatic detection and se-
lection of verbal valency frames in Czech and En-
glish, which corresponds to detecting and disam-
biguating mentions of events in text. This is an im-
portant step toward event instance identification,
which should help greatly in linking the mentions
of a single event. We took advantage of the fact
that the Prague family of dependency treebanks
contains comparable valency frame annotation for
Czech and English (cf. Section 2). Thus the fea-
ture templates used in frame selection are the same
1
Valency can be observed for verbs, nouns, adjectives and
in certain theories, also for other parts of speech; however,
we focus on verbal valency only, as it is most common and
sufficiently described in theory and annotated in treebanks.
and the features initially considered differ only in
their instantiation (cf. Section 3).
While somewhat similar to the CoNLL 2009
Shared Task (Haji
?
c et al., 2009) in the predicate
detection part, our task differs from the semantic
role labeling task in that the whole frame has to
be detected, not only individual arguments, and is
therefore more difficult not only in terms of scor-
ing, but also in the selection part: several verbal
frames might share the same syntactic features,
making them virtually indistinguishable unless se-
mantics is taken into account, combined with a de-
tailed grammatical and morphological context.
2 Valency in the tectogrammatical
description
The annotation scheme of the Prague Dependency
Treebank (Bej
?
cek et al., 2012, PDT) and the
Prague Czech-English Dependency Treebank (Ha-
ji
?
c et al., 2012, PCEDT) is based on the formal
framework of the Functional Generative Descrip-
tion (Sgall, 1967; Sgall et al., 1986, FGD), de-
veloped within the Prague School of Linguistics.
The FGD is dependency-oriented with a ?strati-
ficational? (layered) approach to a systematic de-
scription of a language. The notion of valency in
the FGD is one of the core concepts operating on
the layer of linguistic meaning (tectogrammatical
layer, t-layer).
2.1 Valency frames
The FGD uses syntactic as well as semantic crite-
ria to identify verbal complements. It is assumed
that all semantic verbs ? and, potentially, nouns,
adjectives, and adverbs ? have subcategorization
requirements, which can be specified in the va-
lency frame.
Verbal valency modifications are specified
along two axes: The first axis concerns the (gen-
eral) opposition between inner participants (argu-
ments) and free modifications (adjuncts). This dis-
6
tinction is based on criteria relating to:
(a) the possibility of the same type of comple-
ment appearing multiple times with the same
verb (arguments cannot), and
(b) the possibility of the occurrence of the given
complements (in principle) with any verb
(typical for adjuncts).
The other axis relates to the distinction between
(semantically) obligatory and optional comple-
ments of the word, which again is based on cer-
tain operational criteria expressed as the dialogue
test (Panevov?, 1974). Five arguments are distin-
guished: Actor (ACT), Patient (PAT), Addressee
(ADDR), Origin (ORIG), and Effect (EFF). The
set of free modifications is much larger than that of
arguments; about 50 types of adjuncts are distin-
guished based on semantic criteria. Their set can
be divided into several subclasses: temporal (e.g.,
TWHEN, TSIN), local (e.g., LOC, DIR3), causal
(such as CAUS, CRIT), and other free modifica-
tions (e.g., MANN for general Manner, ACMP for
Accompaniment, EXT for Extent etc.).
All arguments (obligatory or optional) and
obligatory adjuncts are considered to be part of the
valency frame.
2.2 Tectogrammatical annotation
The PDT is a project for FGD-based manual an-
notation of Czech texts, started in 1996 at the In-
stitute of Formal and Applied Linguistics, Charles
University in Prague. It serves two main purposes:
1. to test and validate the FGD linguistic theory,
2. to apply and test machine learning methods for
part-of-speech and morphological tagging, de-
pendency parsing, semantic role labeling, coref-
erence resolution, discourse annotation, natural
language generation, machine translation and
other natural language processing tasks.
The language data in the PDT are non-abbreviated
articles from Czech newspapers and journals.
The PCEDT contains English sentences from
the Wall Street Journal section of the Penn Tree-
bank (Marcus et al., 1993, PTB-WSJ) and their
Czech translations, all annotated using the same
theoretical framework as the PDT.
The annotation of the PDT and the PCEDT is
very rich in linguistic information. Following the
stratificational approach of the FGD, the texts are
annotated at different but interlinked layers. There
are four such layers, two linear and two structured:
? the word layer (w-layer) ? tokenized but other-
wise unanalyzed original text,
? the morphological layer (m-layer) with parts-
of-speech, morphology and lemmatization,
? analytical layer (a-layer) ? surface dependency
syntax trees,
? tectogrammatical layer (t-layer) ? ?deep syn-
tax? trees according to the FGD theory.
While the PDT has all the layers annotated man-
ually, the PCEDT English annotation on the a-
layer has been created by automatic conversion
from the original Penn Treebank, including the
usual head assignment; morphology and the tec-
togrammatical layer are annotated manually, even
if not as richly as for Czech.
2
Valency is a core ingredient on the t-layer. Since
valency frames guide, i.a., the labeling of argu-
ments, valency lexicons with sense-distinguished
entries for both languages have been created to en-
sure consistent annotation.
2.3 Valency Lexicons for Czech and English
in the FGD Framework
PDT-Vallex (Haji
?
c et al., 2003; Ure?ov?, 2011) is a
valency lexicon of Czech verbs, nouns, and adjec-
tives, created in a bottom-up way during the an-
notation of the PDT. This approach made it pos-
sible to confront the pre-existing valency theory
with the real usage of the language.
Each entry in the lexicon contains a head-
word, according to which the valency frames are
grouped, indexed, and sorted. Each valency frame
includes the frame?s ?valency? (number of argu-
ments, or frame members) and the following in-
formation for each argument:
? its label (see Section 2.1),
? its (semantic) obligatoriness according to Pane-
vov? (1974)?s dialogue test,
? its required surface form (or several alternative
forms) typically using morphological, lexical
and syntactic constraints.
Most valency frames are further accompanied by a
note or an example which explains their meaning
and usage. The version of PDT-Vallex used here
contains 9,191 valency frames for 5,510 verbs.
EngVallex (Cinkov?, 2006) is a valency lex-
icon of English verbs based on the FGD frame-
work, created by an automatic conversion from
2
Attributes such as tense are annotated automatically, and
most advanced information such as topic and focus annota-
tion is not present.
7
PropBank frame files (Palmer et al., 2005) and by
subsequent manual refinement.
3
EngVallex was
used for the tectogrammatical annotation of the
English part of the PCEDT. Currently, it contains
7,699 valency frames for 4,337 verbs.
3 Automatic frame selection
Building on the modules for Czech and English
automatic tectogrammatical annotation used in the
TectoMT translation engine (?abokrtsk? et al.,
2008) and the CzEng 1.0 corpus (Bojar et al.,
2012),
4
we have implemented a system for au-
tomatic valency frame selection within the Treex
NLP Framework (Popel and ?abokrtsk?, 2010).
The frame selection system is based on logistic
regression from the LibLINEAR package (Fan et
al., 2008). We use separate classification models
for each verbal lemma showing multiple valency
frames in the training data. Due to identical anno-
tation schemata in both languages, our models use
nearly the same feature set,
5
consisting of:
? the surface word form of the lexical verb and all
its auxiliaries,
? their morphological attributes, such as part-of-
speech and grammatical categories,
? formemes ? compact symbolic morphosyn-
tactic labels (e.g., v:fin for a finite verb,
v:because+fin for a finite verb governed
by a subordinating conjunction, v:in+ger for
a gerund governed by a preposition),
6
? syntactic labels given by the dependency parser,
? all of the above properties found in the topolog-
ical and syntactic neighborhood of the verbal
node on the t-layer (parent, children, siblings,
nodes adjacent in the word order).
We experimented with various classifier settings
(regularization type and cost C, termination crite-
rion E) and feature selection techniques (these in-
volve adding a subset of features according to a
metric against the target class).
7
3
This process resulted in the interlinkage of both lexicons,
with additional links to VerbNet (Schuler, 2005) where avail-
able. Due to the refinement, the mapping is often not 1:1.
4
Note that annotation used in TectoMT and CzEng does
not contain all attributes found in corpora manually annotated
on the tectogrammatical layer. Valency frame IDs are an ex-
ample of an attribute that is missing from the automatic an-
notation of CzEng 1.0.
5
The only differences are due to the differences of part-
of-speech tagsets used.
6
See (Du?ek et al., 2012; Rosa et al., 2012) for a detailed
description of formemes.
7
The metrics used include the Anova F-score, minimum
4 Experiments
We evaluated the system described in Section 3
on PDT 2.5 for Czech and on the English part
of PCEDT 2.0 for English. From PCEDT 2.0,
whose division follows the PTB-WSJ, we used
Sections 02-21 as training data, Section 24 as
development data, and Section 23 as evaluation
data. Since the system is intended to be used in
a fully automatic annotation scenario, we use au-
tomatically parsed sentences with projected gold-
standard valency frames to train the classifiers.
The results of our system in the best setting
for both languages are given in Table 1.
8
The
unlabeled figures measure the ability of the sys-
tem to detect that a valency frame should be filled
for a given node. The labeled figures show the
overall system performance, including selecting
the correct frame. The frame selection accuracy
value shows only the percentage of frames se-
lected correctly, disregarding misplaced frames.
The accuracy for ambiguous verbs further disre-
gards frames of lemmas where only one frame is
possible. Here we include a comparison of our
trained classifier with a baseline that always se-
lects the most frequent frame seen in the training
data.
9
Our results using the classifier for both lan-
guages have been confirmed by pairwise bootstrap
resampling (Koehn, 2004) to be significantly bet-
ter than the baseline at 99% level.
We can see that the system is more successful
in Czech in determining whether a valency frame
should be filled for a given node. This is most
probably given by the fact that the most Czech
verbs are easily recognizable by their morphologi-
cal endings, whereas English verbs are more prone
to be misrepresented as nouns or adjectives.
The English system is better at selecting the cor-
rect valency frame. This is probably caused by
a more fine-grained word sense resolution in the
Czech valency lexicon, where more figurative uses
and idioms are included. For example, over 16%
Redundancy-Maximum Relevance (mRMR) (Peng et al.,
2005), ReliefF (Kononenko, 1994), mutual information (MI),
symmetric uncertainty (Witten and Frank, 2005, p. 291f.),
and an average of the ranks given by mRMR and MI.
8
The best setting for Czech uses L1-regularization and
10% best features according to Anova, with other parame-
ters tuned on the development set for each lemma. The best
setting for English uses L2-regularization with best feature
subsets tuned on the development set and fixed parameters
C = 0.1, E = 0.01.
9
All other parts of the system, up to the identification of
the frame to be filled in, are identical with the baseline.
8
Czech English
Unlabeled precision 99.09 96.03
Unlabeled recall 94.81 93.07
Unlabeled F-1 96.90 94.53
Labeled precision 78.38 81.58
Labeled recall 74.99 79.06
Labeled F-1 76.65 80.30
Frame selection accuracy 79.10 84.95
Ambiguous verbs
baseline 66.68 68.44
classifier 72.41 80.03
Table 1: Experimental results
of errors in the Czech evaluation data were caused
just by idioms or light verb constructions not be-
ing recognized by our system. In Czech, addi-
tional 15% of errors occurred for verbs where two
or more valency frames share the same number of
arguments and their labels, but these verb senses
are considered different (because they have differ-
ent meaning), compared to only 9% in English.
5 Related Work
As mentioned previously, the task of detecting and
selecting valency frames overlaps with semantic
role labeling (Haji
?
c et al., 2009). However, there
are substantial differences: we have focused only
on verbs (as opposed to all words with some se-
mantic relation marked in the data), and evaluated
on the exact frame assigned to the occurrence of
the verb in the treebank. On the other hand, we
are also evaluating predicate identification as in
Surdeanu et al. (2008), which Haji
?
c et al. (2009)
do not. Tagging and parsing have been automatic,
but not performed jointly with the frame selec-
tion task. This also explains that while the best
results reported for the CoNLL 2009 Shared task
(Bj?rkelund et al., 2009) are 85.41% labeled F-1
for Czech and 85.63% for English, they are not
comparable due to several reasons, the main be-
ing that SRL evaluates each argument separately,
while for a frame to be counted as correct in our
task, the whole frame (by means of the refer-
ence ID) must be selected correctly, which is sub-
stantially harder (if only for verbs). Moreover,
we have used the latest version of the PDT (the
PDT 2.5), and EngVallex-annotated verbs in the
PCEDT, while the English CoNLL 2009 Shared
Task is PropBank-based.
10
10
Please recall that EngVallex is a manually refined Prop-
Bank with different labeling scheme and generally m : n
Selecting valency frames is also very similar to
Word Sense Disambiguation (WSD), see e.g. (Ed-
monds and Cotton, 2001; Chen and Palmer, 2005).
The WSD however does not consider subcatego-
rization/valency information explicitly.
Previous works on the PDT include a rule-based
tool of Honetschl?ger (2003) and experiments by
Semeck? (2007) using machine learning. Both of
them, unlike our work, used gold-standard anno-
tation with just the frame ID removed.
6 Conclusions
We have presented a method of detecting mentions
of events in the form of verbal valency frame se-
lection for Czech and English. This method is
based on logistic regression with morphological
and syntactic features, trained on treebanks with
a comparable annotation scheme. We believe that
these results are first for this task on the granu-
larity of the lexicons (PDT-Vallex for Czech and
EngVallex for English), and they seem to be en-
couraging given that the most frequent verbs like
to be and to have have tens of possible frames,
heavily weighing down the resulting scores.
We plan to extend this work to use additional
features and lexical clustering, as well as to see
if the distinctions in the lexicons are justified, i.e.
if humans can effectively distinguish them in the
first place, similar to the work of Cinkov? et al.
(2012). A natural extension is to combine this
work with argument labeling to match or improve
on the ?perfect proposition? score of Surdeanu et
al. (2008) while still keeping the sense distinctions
on top of it. We could also compare this to other
languages for which similar valency lexicons ex-
ist, such as SALSA for German (Burchardt et al.,
2006) or Chinese PropBank (Xue, 2008).
Acknowledgments
This work was supported by the Grant No.
GPP406/13/03351P of the Grant Agency of the
Czech Republic, the project LH12093 of the Min-
istry of Education, Youth and Sports of the Czech
Republic and the Charles University SVV project
260 104. It has been using language resources
developed, stored, and distributed by the LIN-
DAT/CLARIN project of the Ministry of Edu-
cation, Youth and Sports of the Czech Republic
(project LM2010013).
mapping between PropBank and EngVallex frames.
9
References
E. Bej?cek, J. Panevov?, J. Popelka, P. Stra?n?k,
M. ?ev?c?kov?, J. ?t?ep?nek, and Z. ?abokrtsk?.
2012. Prague Dependency Treebank 2.5 ? a revis-
ited version of PDT 2.0. In Proceedings of COLING
2012: Technical Papers, Mumbai.
A. Bj?rkelund, L. Hafdell, and P. Nugues. 2009. Mul-
tilingual semantic role labeling. In Proceedings of
the Thirteenth Conference on Computational Nat-
ural Language Learning (CoNLL 2009): Shared
Task, pages 43?48, Boulder, Colorado, United
States, June.
O. Bojar, Z. ?abokrtsk?, O. Du?ek, P. Galu??c?kov?,
M. Majli?, D. Mare?cek, J. Mar??k, M. Nov?k,
M. Popel, and A. Tamchyna. 2012. The joy of paral-
lelism with CzEng 1.0. In LREC, page 3921?3928,
Istanbul.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pad?,
and M. Pinkal. 2006. The SALSA corpus: a
German corpus resource for lexical semantics. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC-2006).
J. Chen and M. Palmer. 2005. Towards robust high
performance word sense disambiguation of English
verbs using rich linguistic features. In Natural Lan-
guage Processing?IJCNLP 2005, pages 933?944.
Springer.
S. Cinkov?, M. Holub, and V. Kr??. 2012. Manag-
ing uncertainty in semantic tagging. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 840?850. Association for Computational Lin-
guistics.
S. Cinkov?. 2006. From PropBank to EngValLex:
adapting the PropBank-Lexicon to the valency the-
ory of the functional generative description. In
Proceedings of the fifth International conference on
Language Resources and Evaluation (LREC 2006),
Genova, Italy.
O. Du?ek, Z. ?abokrtsk?, M. Popel, M. Majli?,
M. Nov?k, and D. Mare
?
cek. 2012. Formemes
in English-Czech deep syntactic MT. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, page 267?274.
P. Edmonds and S. Cotton. 2001. Senseval-2:
Overview. In The Proceedings of the Second Inter-
national Workshop on Evaluating Word Sense Dis-
ambiguation Systems, SENSEVAL ?01, pages 1?5,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
R. E Fan, K. W Chang, C. J Hsieh, X. R Wang, and
C. J Lin. 2008. LIBLINEAR: a library for large lin-
ear classification. The Journal of Machine Learning
Research, 9:1871?1874.
J. Haji?c, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart?, L. M?rquez, A. Meyers, J. Nivre,
S. Pad?, J. ?t?ep?nek, P. Stra?n?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2009), June 4-5, Boulder, Colorado,
USA.
J. Haji?c, E. Haji?cov?, J. Panevov?, P. Sgall, O. Bo-
jar, S. Cinkov?, E. Fu?c?kov?, M. Mikulov?, P. Pajas,
J. Popelka, J. Semeck?, J. ?indlerov?, J. ?t?ep?nek,
J. Toman, Z. Ure?ov?, and Z. ?abokrtsk?. 2012.
Announcing Prague Czech-English Dependency
Treebank 2.0. In Proceedings of LREC, pages 3153?
3160, Istanbul.
J. Haji?c, J. Panevov?, Z. Ure?ov?, A. B?mov?,
V. Kol??rov?, and P. Pajas. 2003. PDT-VALLEX:
creating a large-coverage valency lexicon for tree-
bank annotation. In Proceedings of The Second
Workshop on Treebanks and Linguistic Theories,
volume 9, page 57?68.
V. Honetschl?ger. 2003. Using a Czech valency lexi-
con for annotation support. In Text, Speech and Di-
alogue, pages 120?125. Springer.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Empirical Methods
in Natural Language Processing, pages 388?395.
I. Kononenko. 1994. Estimating attributes: Analysis
and extensions of RELIEF. In Machine Learning:
ECML-94, page 171?182.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguis-
tics, 19(2):330.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
J. Panevov?. 1974. On verbal frames in functional
generative description. Prague Bulletin of Mathe-
matical Linguistics, 22:3?40.
H. Peng, F. Long, and C. Ding. 2005. Feature selec-
tion based on mutual information: criteria of max-
dependency, max-relevance, and min-redundancy.
IEEE Transactions on pattern analysis and machine
intelligence, page 1226?1238.
M. Popel and Z. ?abokrtsk?. 2010. TectoMT: modu-
lar NLP framework. Advances in Natural Language
Processing, pages 293?304.
R. Rosa, D. Mare
?
cek, and O. Du?ek. 2012. DEPFIX:
a system for automatic correction of Czech MT out-
puts. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, page 362?368. As-
sociation for Computational Linguistics.
10
K. K. Schuler. 2005. VerbNet: A Broad-Coverage,
Comprehensive Verb Lexicon. Ph.D. thesis, Univ. of
Pennsylvania.
J. Semeck?. 2007. Verb valency frames disambigua-
tion. The Prague Bulletin of Mathematical Linguis-
tics, (88):31?52.
P. Sgall, E. Haji?cov?, and J. Panevov?. 1986. The
meaning of the sentence in its semantic and prag-
matic aspects. D. Reidel, Dordrecht.
P. Sgall. 1967. Generativn? popis jazyka a c?esk? dekli-
nace. Academia, Praha.
M. Surdeanu, R. Johansson, A. Meyers, L. M?rquez,
and J. Nivre. 2008. The CoNLL 2008 shared
task on joint parsing of syntactic and semantic de-
pendencies. In CoNLL 2008: Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 159?177, Manchester, Eng-
land, August. Coling 2008 Organizing Committee.
Z. Ure?ov?. 2011. Valenc?n? slovn?k Pra?sk?ho z?vis-
lostn?ho korpusu (PDT-Vallex). Studies in Compu-
tational and Theoretical Linguistics. ?stav form?ln?
a aplikovan? lingvistiky, Praha, Czechia, ISBN 978-
80-904571-1-9, 375 pp.
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann Pub, 2nd edition.
N. Xue. 2008. Labeling Chinese predicates with se-
mantic roles. Computational linguistics, 34(2):225?
255.
Z. ?abokrtsk?, J. Pt?
?
cek, and P. Pajas. 2008. Tec-
toMT: highly modular MT system with tectogram-
matics used as transfer layer. In Proceedings of the
Third Workshop on Statistical Machine Translation,
page 167?170. Association for Computational Lin-
guistics.
11
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55?64,
Coling 2014, Dublin, Ireland, August 24 2014.
Comparing Czech and English AMRs
Zde
?
nka Ure
?
sov
?
a Jan Haji
?
c Ond
?
rej Bojar
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, 11800 Prague 1, Czech Republic
{uresova,hajic,bojar}@ufal.mff.cuni.cz
Abstract
This paper describes in detail the differences between Czech and English annotation us-
ing the Abstract Meaning Representation scheme, which stresses the use of ontologies (and
semantically-oriented verbal lexicons) and relations based on meaning or ontological content
rather than semantics or syntax. The basic ?slogan? of the AMR specification clearly states that
AMR is not an interlingua, yet it is expected that many relations as well as structures constructed
from these relations will be similar or even identical across languages. In our study, we have
investigated 100 sentences in English and their translations into Czech, annotated manually by
AMRs, with the goal to describe the differences and if possible, to classify them into two main
categories: those which are merely convention differences and thus can be unified by changing
such conventions in the AMR annotation guidelines, and those which are so deeply rooted in the
language structure that the level of abstraction which is inherent in the current AMR scheme does
not allow for such unification.
1 Introduction
In this paper, we follow on a previous first exploratory investigation of differences in AMR annotation
among different languages (Xue et al., 2014), which has classified the similarities and differences into
four categories: (a) no difference, (b) local difference only (such as multiword expressions vs. single
word terms), (c) reconcilable difference due to AMR conventions, and (d) deep differences which cannot
be unified in the AMR guidelines. In this paper, we would like to elaborate especially on the (b) and (c)
types, which have been only exemplified in the previous work. In this paper, we would like to not only
go deeper, but also present quantitative comparison on 100 parallel sentences, for all the aforementioned
categories and some of their subtypes.
We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building
also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study
(Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English
and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work.
2 Abstract Meaning Representation (AMR)
Syntactic treebanks in several languages (Marcus et al., 1993; Haji?c et al., 2003; Xue et al., 2005)
and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004),
TimeBank (Pustejovsky et al., 2003), FactBank (Saur?? and Pustejovsky, 2009), and the Penn Discourse
TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP
tasks. These annotated resources enabled substantial amounts of research in different areas of semantic
analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak,
2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the
PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013)
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0
55
and similar resources in other languages (Haji?c et al., 2009), and TimeBank has fueled much research in
the area of temporal analysis.
There have been efforts to create a unified representation which would cover at least a whole sentence,
or even a continuous text (Haji?c et al., 2003; Srikumar and Roth, 2013), and currently the Abstract
Meaning Representation represents an attempt to provide a common ground for truly semantic and fully
covering annotation representation.
An Abstract Meaning Representation is a rooted, directional and labeled graph that represents the
meaning of a sentence and it abstracts away from such syntactic notions as word category (verbs and
nouns), word order, morphological variation etc. Instead, it focuses on semantic relations between con-
cepts and makes heavy use of predicate-argument structures as defined in PropBank (for English). As
a result, the word order in the sentence is considered to be of little relevance to the meaning represen-
tation and is not necessarily maintained in the AMR. In addition, many function words (determiners,
prepositions) that do not contribute to meaning and are not explicitly represented in AMR, except for the
semantic relations they express. Readers are referred to Baranescu et al. (2013) for a complete descrip-
tion of AMR.
1
Figure 1: AMR annotation of the sentence ?This infatuation with city living truly baffles me.?
An example of an AMR-annotated sentence can be seen in Fig. 1. The predicate of the sentence
(baffle) becomes the root of the annotation graph, with a reference to the correct sense baffle-01 as
found in PropBank frame files for baffle; PropBank frame files play the role of an ontology of events.
Arguments of predicates, again as described in the PropBank frames, become the substitutes for roles of
the ?who did what to whom? interpretation - in the example sentence, infatuation - marked as ARG0 - is
the thing that baffles someone (the ARG1), i.e. me (the author of the text) in this case. This ?baffling? is
further modified by ?truly?, and marked simply as a modifier, the semantics of which is fully represented
by the word true itself. The agent (infatuation) has to be further restricted - it is the ?infatuation with
city living? which baffles the author - not just any infatuation. This is represented by the relation topic
assigned to the edge between infatuation and live-01 in the AMR graph, and the ?living? (sense
live-01) is further restricted by the location mentioned in the sentence, namely city. Finally,
the modifier this is kept in, since it is needed for reference to previous text, where the ?infatuation?
has been first mentioned.
While the graphical representation in Fig. 1 is simplified in that it does not show the AMR?s crucial
instance-of relations explicitly as edges in the AMR graph, Fig. 2 shows the native underlying
?bracketed? textual representation of the same tree, where the main nodes (i.e. those shown visibly in
Fig. 1) are mentions, and the labels baffle-01, true, live-01, city etc. represent links to
external ontologies. These links are currently represented only by these strings, or by links to PropBank
1
This paragraph as well as the two preceding ones are taken over (and slightly adapted) from the introductory sections of a
previous paper on this topic presented at LREC and co-authored by us (Xue et al., 2014); we share the same AMR formalism
and data.
56
files for events. In the future, these links will be wikified, i.e. for concepts described in an external
ontology, such as Wikipedia, they will be linked to it. The single- or two-letter ?indexes? are in fact the
labels (IDs) of the mentions, and they also serve for (co-)reference purposes; the slash (?/?) is a shortcut
for the instance-of relation.
(b / baffle-01
:ARG0 (i / infatuation
:topic (l / live-01
:location (c / city))
:mod (t / this))
:ARG1 (i2 / i)
:mod (t2 / true))
Figure 2: Textual form of the AMR annotation of the sentence ?This infatuation with city living truly
baffles me.?
In Czech, the event ontology has been approximated by the Czech valency dictionary, PDT-Vallex
(Haji?c et al., 2003), (Ure?sov?a, 2009), (Ure?sov?a and Pajas, 2009), (Ure?sov?a, 2006). No wikification of
non-event nodes has been attempted yet; this is a continuing work, as it is for English.
3 The Data
We have drawn on a blog on Virginia road construction, taken from the WB part of the Penn Treebank.
These sentences have already been annotated using AMRs, and also translated to Czech
2
and subse-
quently AMR-annotated. The English text has 1676 word and punctuation tokens (using the Penn Tree-
bank style tokenization), and its annotated AMR representation contains 1231 nodes (not counting the
instance-of nodes as separate nodes). The Czech version is a result of manually doubly-translated
English original, which has been mutually checked and then one (slightly corrected) translation has been
used for annotation. The Czech text has a total of 1563 tokens and its AMR representation contains 1215
nodes (again, not counting the instance-of nodes as separate nodes).
The data, once annotated, have been converted to a graph and in such a form presented to a linguist
familiar with the AMR style annotation, to study and extract statistics for this comparison study. Fig. 3
shows such a side-by-side graphs for English and Czech AMR for the example parallel sentences.
4 The Comparison
4.1 Quantitative Comparison
In the first pass, we have concentrated on marking and counting the following phenomena:
? structural identity: sentences with identical structure have been marked as being structurally the
same, even if some relation (edge) labels have been different
? structural differences: no. of structural differences have been noted in cases where one or more
(sub)parts of the AMR graph differ between the two languages
? local difference only: out of the above, certain differences have been marked as ?local only? - for
example, if a multiword expression annotated as several nodes in one language corresponds to a
single node in the other language
? relation differences: for each sentence, number of differences in relational labels has been counted
? reference differences: number of different references to an external ontology (or assumed differ-
ences in case no link to such an ontology was actually present in the annotation).
2
...and Chinese, but that was not used for this study.
57
Figure 3: AMR annotation of the sentence ?This infatuation with city living truly baffles me.? and its
translation to Czech (?Tohle/this poblouzn?en??/infatuation bydlen??m/living-INSTR v/in m?estsk?em/city-like
stylu/style m?e/me po?r?ad/still mate/baffles.?)
It is obvious that we could have observed also other types of differences, but at this point, we wanted
to have at least an idea how many differences exist in our approx. 1500-token sample. The resulting
figures are summarized in Table 1.
3
Same
structure
Different
substructures
Local difference
only
Relation
differences
Reference
differences
29 (sents) 193 (subgraphs) 92 (subgraphs) 331 (nodes) 37 (nodes)
of 100 of approx. 800
2
of 193 (all diffs) of 1215 Cz nodes of 1215 Cz nodes
29 % approx. 25 %
2
47.7 % 27.2 % 3.0 %
Table 1: Number and percentages of differences in the annotated data
The number of truly identically annotated sentences (including relation labeling) was only four, two
of which has been interjective ?sentences? at the beginning of the document (?Braaawk!?). On the other
hand, 18 additional sentences would be structurally identical (on top of the 29) if local differences were
disregarded, bringing the (unlabeled sentence identity) total to 47, or almost half of the data (47=29+18).
4.2 Analysis of Differences
The main goal of this study is to analyze differences in the annotation for the two languages, Czech
and English, and determine if a reconciliation of the annotation is possible or not (and for what reason
it is / it is not). Based on the above quantitative analysis, we have concentrated on relation labeling
differences due their high proportion, and on structural differences due to their heterogeneous nature.
The differences in reference annotation are small, but this is due to the lack of full referential annotation
(it has been done for events, but only assumed for other types of entities due to the lack of ontology,
or better to say due to the lack of ?wikification? annotation in both languages), rather than due to high
agreement. We will come back to this once wikification of the annotation is finished.
3
Structural differences are hard to quantify exactly, since the base is difficult to define; it is part of future work.
58
4.3 Differences in Relation Labeling
The differences in function labeling should be taken with a grain of salt. The crucial question is what
should count as a difference in relation labeling if the structure differs - should this be automatically
counted as a difference, or not at all? In the figures summarized in Table 1, we have taken a middle
ground: if the structural difference implied a change in labeling by itself, we have not counted that
difference in order not to ?penalize? the sentence annotation twice.
More detailed inspection of relation labeling differences, which appear to be relatively frequent at
more than 1/4th of all nodes in the annotation, revealed that the by far most frequent mismatch is caused
by different argument labeling for events.
4
While for most purely transitive verbs there is a complete
match, for most other there is a discrepancy due to the attempted semanticization of PDT-Vallex argu-
ment labels ADDR (addressee), EFF (effect) and ORIG (origin), while PropBank simply continues to
number arguments of corresponding verbs consecutively (for example, I thought there is.ARG1 ... vs.
Myslel/I-thought jsem, ?ze/that tam/there je.ARG3?EFF/is ...). The concept of ?shifting? in PDT-Vallex,
which compulsorily fills the first two arguments on syntactic grounds as ACT(ARG0) and PAT(ARG1)
is another source of differences. Furthermore, PropBank leaves out ARG0 e.g. for unaccusative verbs
(for example The window.ARG1 broke vs. Okno.ARG0?ACT/window se/itself rozbilo/broke.). Finally,
some differences are due to some arguments not being considered arguments at all in the other language,
in which case some other AMR label is used instead (for example, We could have spent 400M.ARG3 ...
elsewhere vs. ... mohli/could utratit/spend 400M.extent ... jinde/elsewhere).
These differences could possibly be consolidated (only) by carefully linking the two lexicons (with
AMR guidelines intact). This is in fact being performed in another project (Sindlerova et al., 2014), but
it is a daunting manual task, since the underlying theories behind PropBank and PDT-Vallex/EngVallex
differ. However, one has to ask if it does make sense to do so, because with enough parallel data available,
the mappings can be learned relatively easily: in most cases, no structural differences are involved and
there will be a simple one-to-one mapping between the labels (conditioned on the particular verb sense).
4.4 Structural Differences
Local differences can be safely ignored, since they will be in most cases resolved during the assumed
process of wikification, i.e., linking to an ontology concept. For example, the abbreviation VDOT (Vir-
ginia Department of Transportation), which has to be (and was) translated into Czech in an explanatory
way (otherwise the sentence would become not quite understandable, if only because of the real-world
context). Without wikification, it could not be linked as a whole, and thus a subgraph has been created
with the AMR-appropriate internal semantic relations in the translation (e.g. Virginia.location, etc.).
Certain differences, albeit ?localized? into a small subtree (or subgraph) corresponding to a single
node or another small subtree (subgraph), cannot be resolved by wikification of a different event ontology
(than PropBank or PDT-Vallex). For example, light verb constructions or even certain modal or aspectual
constructions could have a single verb equivalent resulting in two node vs. single node annotation: get
close vs. p?ribl???zit-se, make worse vs. zhor?sit, take position (for sb) vs. zast?avat-se or causing sprawl vs.
roztahuje-se.
Looking at the true structural differences, we have found that there are actually quite a few reasons for
them to appear in the annotation. We will describe them in more detail below.
Non-literal translation is the primary reason for such differences.
5
For example, destination vs.
kam/where-to lid?e/people jezd??/drive (Fig. 4), or job center vs. m??sto/place, kde/where pracuje/work
hodn?e/many lid??/people; these cases cannot be unified neither by changing the translation to a more literal
one (because it would be strongly misleading in the given context, despite the fact that literal translation
of both destination as well as job center does exist in Czech), neither by changing the guidelines, since
the level of abstraction of AMR does not call for a unification of such concepts. Sometimes, non-literal
4
The Czech PDT-Vallex argument labels have been mapped to PropBank labels as follows: ACT? ARG0, PAT? ARG1,
ADDR? ARG2, EFF? ARG3 and ORIG? ARG4.
5
This includes also cases of truly wrong translation, stemming of translator?s misunderstanding of the facts behind the
sentence. This has been found fairly often only after we studied the differences in depth, since a superficial reading and
standard translation revision procedure did not help.
59
translation is forced upon the translation because no word-for-word translation exists, such as in in the
aggregate, which has to be translated using an extra clause z/from celkov?eho/overall pohledu/view to/it
je/is tak/so, ?ze/that ... (Fig. 5).
Figure 4: AMR structural difference: destination vs. kam/where-to lid?e/people jezd??/drive
Phraseological differences and idioms form another large group of differences between the two lan-
guages. The possibility of changing the translation is even more remote than in the above case, even if we
had the chance: the provided translation is actually the correct and perfect one. The reason for different
annotation lies in the AMR scheme, which does not go that far to require ?unified? annotation in such
cases where the idiom or specific phrase cannot be linked to the external ontology as a single unit. For
example, English ?I don?t see any point? is translated as ?nem?a/not-have smysl/purpose?, and despite the
fact that have-purpose-91 is a specific event reference in English (and has been used in the anno-
tation), the verb ?see? still remains annotated as a separate event node, which is not the case in Czech,
since no ?seeing? is expressed in the sentence and it could hardly be asked for in the guidelines to be
inserted. Similarly, I commute back and forth has been translated simply as doj???zd??m/commute, which is
semantically perfectly equivalent but the back and forth has been kept in the English annotation, because
60
Figure 5: AMR structural difference: in the aggregate vs. z/from celkov?eho/overall pohledu/view to/it
je/is tak/so, ?ze/that ...
deleting it was (probably) considered loss of information. It is only the confrontation with the translation
to a different language when one realizes that with just a little more abstraction, the annotation could
have been structurally the same (by keeping only the commute node in in the English annotation).
6
Translation by interpretation is typically discouraged in translation school education, but sometimes
it is necessary to use it for smooth understanding of the translated text. Often, such interpretation results
in different AMR annotation. For example, Virginia centrist has been translated as st?redov?y/centrist
voli?c/voter [z/from Virginie/Virginia], because without the extra word voli?c, the literal translation of
centrist would not be understandable correctly in this context (Fig. 6). Similarly, a 55mph zone vs.
z?ona/zone s/with omezen??m/restriction na/to 55 mph (added word omezen??m/restriction), or traffic vs.
dopravn??/traffic z?acpa/jam.
Convention differences are inherent in many annotation schemes, and we have found them in AMR
6
One could perhaps also argue that adding the equivalent of back and forth to the Czech translation would unify the trans-
lation, too; however, adding its literal equivalent in Czech tam a zp?et would be considered superfluous and unnatural by Czech
speakers.
61
Figure 6: AMR structural difference: Virginia Centrist vs.
st?redov?y/centrist voli?c/voter [z/from Virginie/Virginia]
Figure 7: AMR convention difference:
auditor as a single node vs. person, who
audits
guidelines, too. Often, they were related to the use of ARG-of vs. keeping the nominalization as a
single node. For example, for auditor, translated quite literally as auditor into Czech, has been annotated
as ?a person, who audits? in English while in the Czech AMR structure, there is a single node (Fig. 7)
labeled as auditor (which undoubtedly will be correctly linked to some ontology entry after such link-
age/wikification is complete). These differences might be harder to consolidate, since such conventions
are very difficult to create proper guidelines for, especially across languages. No ontology (whether for
events or objects) will be complete either (to base the decisions on a particular ontology content).
5 Conclusions and Future Work
We have investigated differences in the annotation of parallel texts using the Abstract Meaning Rep-
resentation scheme, on approx. 1500 words of English-Czech corpus (100 sentences). We found and
counted the number of identities and four types of differences (structural, structural local, relational, and
referential), and exemplified them to see if a reconciliation (either by possibly changing the translation,
the guidelines, or the annotation itself) is possible.
This is a work in progress. Substantial amount of work remains. We will have to use larger data,
multiple annotation (interannotator agreement on English was relatively low and we expect to be the
case on Czech, too, once two annotators start annotating the same sentences), and we would also have
to actually suggest changes in the guidelines or their conventions, and to test them also on substantial
amounts of data.
The immediate extension of this work will cover wikification, i.e. the linking of all nodes in the AMR
representation of our dataset to some ontology: events are already covered, internally defined relations
are already annotated, too (such as named entity types, dates, quantities, etc.), but external links remain to
be added. We will not only use Wikipedia (as the term ?wikification? might suggest), but we will extend
this idea also to other sources, such as DBpedia or BabelNet, keeping all links in parallel if possible.
This should allow for deep comparison of the two languages also content-wise. We should then be able
to better answer the question of annotation unification which does depend on these links rather than on
the annotation guidelines themselves.
Parallel AMR-annotated data will be used at the JHU 2014 Summer Workshop, where technology for
AMR-based parsing, generation and possibly also MT will be developed, allowing also technological
insight into the AMR scheme across languages.
Acknowledgements
This work was supported by the grant GP13-03351P of the Grant Agency of the Czech Republic, projects
LH12093 and LM2010013 of the Ministry of Education, Youth and Sports of the Czech Republic, and
the EU FP7 project 610516 ?QTLeap?. It has been using language resources distributed by the LIN-
DAT/CLARIN
7
project of the Ministry of Education, Youth and Sports of the Czech Republic (project
LM2010013).
7
http://lindat.cz, resource used: http://hdl.handle.net/11858/00-097C-0000-0023-4338-F,
also at http://lindat.mff.cuni.cz/services/PDT-Vallex
62
References
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and
N. Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop, Sophia, Bulgaria.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richard Farkas, Filip Ginter, and Jan Haji?c. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. Transactions of the Association for Computational
Linguistics, 1:415?428.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter
of the Association for Computational Linguistics conference, pages 132?139. Association for Computational
Linguistics.
Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics,
28(3):245?288.
Jan Haji?c, Jarmila Panevov?a, Zde?nka Ure?sov?a, Alevtina B?emov?a, Veronika Kol?a?rov?a, and Petr Pajas. 2003. PDT-
VALLEX: Creating a Large-coverage Valency Lexicon for Treebank Annotation. In Proceedings of The Second
Workshop on Treebanks and Linguistic Theories, volume 9 of Mathematical Modeling in Physics, Engineering
and Cognitive Sciences, pages 57?68. V?axj?o University Press, November 14?15, 2003.
Jan Haji?c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Mart??, Llu??s M`arquez, Adam
Meyers, Joakim Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Lan-
guages. In Jan Haji?c, editor, Proceedings of the Thirteenth Conference on Computational Natural Language
Learning (CoNLL): Shared Task, pages 1?18, Boulder, CO, USA. Association for Computational Linguistics.
Jan Haji?c, Alena Bo?hmov?a, Eva Hajicov?a, and Barbora Hladk?a. 2003. The Prague Dependency Treebank: A
Three Level Annotation Scenario. In Anne Abeill?e, editor, Treebanks: Building and Using Annotated Corpora.
Kluwer Academic Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english: The penn treebank. Computational Linguistics, 19(2):313?330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank
Project: An Interim Report. In Proceedings of the NAACL/HLT Workshop on Frontiers in Corpus Annotation,
pages 24?31, Boston, Massachusetts.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL, pages 404?411.
Sameer S Pradhan, Wayne Ward, Kadri Hacioglu, James H Martin, and Daniel Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In HLT-NAACL, pages 233?240.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation (LREC 2008), Marrakech, Morocco.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew See, David Day, Lisa Ferro, Robert Gaizauskas, Marcia
Lazo, Andrea Setzer, and Beth Sundheim. 2003. The TimeBank Corpus. Corpus Linguistics, pages 647?656.
Roser Saur?? and James Pustejovsky. 2009. Factbank: a corpus annotated with event factuality. Language resources
and evaluation, 43(3):227?268.
Jana Sindlerova, Zdenka Uresova, and Eva Fucikova. 2014. Resources in Conflict: A Bilingual Valency Lexicon
vs. a Bilingual Treebank vs. a Linguistic Theory. In Nicoletta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evalua-
tion (LREC?14), pages 2490?2494, Reykjavik, Iceland, May 26-31. European Language Resources Association
(ELRA).
63
Vivek Srikumar and Dan Roth. 2013. Modeling semantic relations expressed by prepositions. Transactions of the
Association for Computational Linguistics, 1:231?242.
Zde?nka Ure?sov?a and Petr Pajas. 2009. Diatheses in the Czech Valency Lexicon PDT-Vallex. In Jana Levick?a
and Radovan Garab??k, editors, Slovko 2009, NLP, Corpus Linguistics, Corpus Based Grammar Research, pages
358?376, Bratislava, Slovakia. Jazykovedn?y ?ustav udov??ta
?
St?ura Slovenskej akad?emie vied, Slovensk?a akad?emia
vied.
Zde?nka Ure?sov?a, 2006. Verbal Valency in the Prague Dependency Treebank from the Annotator?s Viewpoint, pages
93?112. Veda, Bratislava, Bratislava, Slovensko.
Zde?nka Ure?sov?a. 2009. Building the PDT-VALLEX valency lexicon. In Catherine Smith
Michaela Mahlberg, Victorina Gonzalez-Diaz, editor, Proceedings of the Corpus Linguistics Conference),
http://ucrel.lancs.ac.uk/publications/cl2009/100 FullPaper.doc, July 20-23. University of Liverpool, UK.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In EMNLP, pages 88?94.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase Structure
Annotation of a Large Corpus. Natural Language Engineering, 11(2):207?238.
Nianwen Xue, Ondrej Bojar, Jan Hajic, Martha Palmer, Zdenka Uresova, and Xiuhong Zhang. 2014. Not an In-
terlingua, But Close: Comparison of English AMRs to Chinese and Czech. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno,
Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), pages 1765?1772, Reykjavik, Iceland, May 26-31. European Language
Resources Association (ELRA).
64
