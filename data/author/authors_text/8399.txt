The University of Ja?n Word Sense Disambiguation System
*
 
Manuel Garc?a-Vega 
Universidad de Ja?n 
Av. Madrid 35 
Ja?n, Spain, 23071 
mgarcia@ujaen.es 
Miguel A. Garc?a-Cumbreras 
Universidad de Ja?n 
Ja?n, Spain, 23071 
Av. Madrid 35, 23071 
magc@ujaen.es 
M. Teresa Mart?n-Valdivia 
Universidad de Ja?n 
Av. Madrid 35 
Ja?n, Spain, 23071 
maite@ujaen.es 
L. Alfonso Ure?a-L?pez 
Universidad de Ja?n 
Av. Madrid 35 
Ja?n, Spain, 23071 
laurena@ujaen.es 
                                                             
*
 This paper has been partially supported by the Spanish Government (MCYT) Project number TIC2003-07158-
C04-04 
 
 
Abstract 
This paper describes the architecture and re-
sults of the University of Ja?n system pre-
sented at the SENSEVAL-3 for the English-
lexical-sample and English-All-Words tasks. 
The system is based on a neural network ap-
proach. We have used the Learning Vector 
Quantization, which is a supervised learning 
algorithm based on the Kohonen neural model. 
1 Introduction 
Our system for SENSEVAL-3 uses a supervised 
learning algorithm for word sense disambiguation. 
The method suggested trains a neural network us-
ing the Learning Vector Quantization (LVQ) algo-
rithm, integrating several semantic relations of 
WordNet (Fellbaum, 1998) and SemCor corpus 
(Miller et al, 1993). The University of Ja?n system 
has been used in English-lexical-sample and Eng-
lish-All-Words tasks. 
2 Experimental Environment 
The presented disambiguator uses the Vector 
Space Model (VSM) as an information representa-
tion model. Each sense of a word is represented as 
a vector in an n-dimensional space where n is the 
number of words in all its contexts. 
The accuracy of the disambiguator depends es-
sentially on the word weights. We use the LVQ 
algorithm to adjust them. The input vector weights 
are calculated as shown by (Salton and McGill, 
1983) with the standard tf?idf, where the documents 
are the paragraphs. They are presented to the LVQ 
network and, after training, the output vectors 
(called prototype or codebook vectors) are ob-
tained, containing the adjusted weights for all 
senses of each word. 
Any word to disambiguate is represented with a 
vector in the same way. This representation must 
be compared with all the trained word sense vec-
tors by applying the cosine similarity rule: 
 
 
ik
ik
ik
sim
xw
xw
xw
?
?
),( =  [1] 
 
The sense corresponding to the vector of highest 
similarity is selected as the disambiguated sense. 
To train the neural network we have integrated 
semantic information from two linguistic re-
sources: SemCor corpus and WordNet lexical da-
tabase.  
2.1 SemCor  
Firstly, the SemCor (the Brown Corpus labeled 
with the WordNet senses) was fully used (the 
Brown-1, Brown-2 and Brown-v partitions). We 
used the paragraph as a contextual semantic unit 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
and each context was included in the training vec-
tor set. 
The SENSEVAL-3 English tasks have used the 
WordNet 1.7.1 sense inventory, but the SemCor is 
tagged with an earlier version of WordNet (spe-
cifically WordNet version 1.6).  
 
Figure 1. SemCor context for ?climb?. 
 
Therefore it was necessary to update the SemCor 
word senses. We have used the automatically 
mapped version of Semcor with the WordNet 1.7.1 
senses found in WordNet site
1
.  
Figure 2. WordNet artificial paragraph 
 
                                                             
1
 http://www.cogsci.princeton.edu/~wn/ 
Figure 1 shows the common format for the all 
the resource input paragraphs. For each word, the 
pos and sense are described, e.g. ?climb\2#1? is the 
verb ?climb? with sense 1. In addition, it has 158 
different words in its context and all of them are 
shown like the pair word-frequency. 
2.2 WordNet  
'Semantic relations from WordNet 1.7.1 were 
considered, in particular synonymy, antonymy, 
hyponymy, homonymy, hyperonymy, meronymy, 
and coordinate terms to generate artificial para-
graphs with words along each relation. 
For example, for a word with 7 senses, 7 artifi-
cial paragraphs with the synonyms of the 7 senses 
were added, 7 more with all its hyponyms, and so 
on. 
Figure 2 shows these artificial paragraphs for the 
?climb? verb. 
3 Learning Vector Quantization 
The LVQ algorithm (Kohonen, 1995) performs 
supervised learning, which uses a set of inputs with 
their correctly annotated outputs adjusting the 
model when an error is committed between the 
model outputs and the known outputs. 
The LVQ algorithm is a classification method 
based on neural competitive learning, which allows 
the definition of a group of categories on the input 
data space by reinforced learning, either positive 
(reward) or negative (punishment). In competitive 
learning, the output neurons compete to become 
active. Only a single output neuron is active at any 
one time. 
The general application of LVQ is to adjust the 
weights of labels to high dimensional input vec-
tors, which is technically done by representing the 
labels as regions of the data space, associated with 
adjustable prototype or codebook vectors. Thus, a 
codebook vector, w
k
, is associated for each class, 
k. This is particularly useful for pattern classifica-
tion problems. 
The learning algorithm is very simple. First, the 
learning rate and the codebook vectors are initial-
ised. Then, the following procedure is repeated for 
all the training input vectors until a stopping crite-
rion is satisfied:  
- Select a training input pattern, x, with class 
d, and present it to the network 
climb\2#1 158 a_hundred\5 1 ab-
sorb\2 1 advance\2 1 ... walk\2 1 
want\1 1 warn\2 1 warped\5 1 way\1 
2 west\3 1 whip\1 2 whir\1 1 
wraithlike\5 1 
climb\2#1 45 abruptly\4 1 absence\1 
1 ... stop\2 1 switch_off\2 1 
there\4 1 tube\1 1 two\5 1 unex-
pectedly\4 1 water\1 1 
... 
climb\2#2 33 adjust\2 1 almost\4 1 
arrange\2 1 ... procedure\1 1 re-
vetment\1 1 run\2 1 sky\1 1 
snatch\2 1 spread\2 1 stand\2 1 
truck\1 1 various\5 1 wait\2 1 
wing\1 1 
... 
climb\2#3 3 average\2 1 feel\2 1 
report\2 1 
climb\2#1 10 arise\2 1 come_up\2 1 
go\2 1 go_up\2 1 lift\2 1 locomote\2 
1 move\2 1 move_up\2 1 rise\2 1 
travel\2 1 
climb\2#1 3 climb_up\2 1 go_up\2 1 
mount\2 1 
climb\2#1 5 mountaineer\2 1 ramp\2 1 
ride\2 1 scale\2 1 twine\2 1 
climb\2#2 7 clamber\2 1 scramble\2 1 
shin\2 1 shinny\2 1 skin\2 1 sput-
ter\2 1 struggle\2 1 
... 
climb\2#5 2 go up\2 1 rise\2 1 
- Calculate the Euclidean distance between 
the input vector and each codebook vector 
||x-w
k
|| 
- Select the codebook vector, w
c
, that is 
closest to the input vector, x.  
 
 
{ }
kc
wxwx ?=?
k
min
 [2] 
 
This codebook vector is the winner neu-
ron and only this neuron updates its 
weights according the learning equation 
(equation 3). If the class of the input pat-
tern, x, matches the class of the winner 
codebook vector, w
c
 (the classification has 
been correct), then the codebook vector is 
moved closer to the pattern (reward), oth-
erwise it is moved further away. 
Let x(t) be a input vector at time t, and w
k
(t) the 
codebook vector for the class k at time t. The fol-
lowing equation defines the basic learning process 
for the LVQ algorithm. 
 
 [ ]  )()()()()1( tttstt
ccc
wxww ???+=+ ?  [3] 
 
Figure 3. Codebook vectors for ?climb? domain 
 
where s = 0, if k ? c; s = 1, if x(t) and w
c
(t) be-
long to the same class (c = d); and s = -1, if they do 
not (c ? d). ?(t) is the learning rate, and 0<?(t)<1 
is a monotically decreasing function of time. It is 
recommended that ?(t) should initially be rather 
small, say, smaller than 0.1 (Kohonen, 1995) and 
?(t) continues decreasing to a given threshold, u, 
very close to 0. 
The codebook vectors for the LVQ were initial-
ized to zero and every training vector was intro-
duced into the neural network, modifying the 
prototype vector weights depending on the correct-
ness in the winner election. 
All training vectors were introduced several 
times, updating the weights according to learning 
equation. ?(t) is a monotonically decreasing func-
tion and it represents the learning rate factor, be-
ginning with 0.1 and decreasing lineally: 
 
 ( ) ( ) ( )
P
tt
0
1
?
?? ?=+  [4] 
 
where P is the number of iterations performed in 
the training. The number of iterations has been 
fixed at 25 because at this point the network is 
stabilized. 
The LVQ must find the winner sense by calcu-
lating the Euclidean distances between the code-
book vectors and input vector. The shortest 
distance points to the winner and its weights must 
be updated. 
4 English Tasks 
The training corpus generated from SemCor and 
WordNet has been used to train the neural net-
works. All contexts of every word to disambiguate 
constitute a domain. Each domain represents a 
word and its senses. Figure 3 shows the codebook 
vectors generated after training process for ?climb? 
domain. 
We have generated one network per domain and 
after the training process, we have as many do-
mains as there are words to disambiguate adjusted. 
The network architecture per domain is shown in 
Figure 4. The number of input units is the number 
of different terms in all contexts of the given do-
main and the number of output units is the number 
of different senses. 
The disambiguator system has been used in Eng-
lish lexical sample and English all words tasks. 
For the English lexical sample task, we have 
used the available SENSEVAL-3 corpus to train 
the neural networks. We have also used the con-
texts generated using SemCor and WordNet for 
each word in SENSEVAL-3 corpus. For the Eng-
climb\2#1 1921 a\1#0 0.01883 
aarseth\1#0 0.03259 abelard\1#0 ... 
yorkshire\1#0 0.03950 young\3#0 
0.00380 zero\1#0 0.01449 
climb\2#2 235 act\1#0 -0.11558 
alone\4#0 -0.07754 ... windy\3#0 -
0.00922 worker\1#0 -0.02738 year\1#0 
-0.03715 zacchaeus\1#0 -0.02344 
climb\2#3 1148 abchasicus\1#0 
0.04127 able\3#0 -0.00945 ... 
young\3#0 -0.00275 zero\1#0 -0.00010
climb\2#4 258 age\1#0 -0.04180 air-
space\1#0 -0.02862 alone\4#0 -
0.01920 apple\1#0 -0.04242 ... 
world\1#0 -0.14184 year\1#0 -0.04113
young\3#0 -0.04831 zero\1#0 -0.06230
... 
lish all word task, we have only used the complete 
contexts of both SemCor and WordNet resources. 
The corpus has been tagged and lemmatized using 
the Tree-tagger (Schmid, 1994). 
Figure 4. The network architecture 
 
Once the training has finished, the testing be-
gins. The test is very simple. We establish the 
similarity between a given vector of the corpus 
evaluation with all the codebook vectors of its do-
main, and the highest similarity value corresponds 
to the disambiguated sense (winner sense). If it is 
not possible to  find a sense (it is impossible to ob-
tain  the cosine similarity value), we assign by de-
fault the most frequent sense (e.g. the first sense in 
WordNet). 
The official results achieved by the University of 
Ja?n system are presented in Table 1 for English 
lexical sample task, and in Table 2 for English all 
words. 
 
ELS Precision Recall Coverage 
Fine-grained 0.613 0.613 99.95% 
Coarse-grained 0.695 0.695 99.95% 
Table 1. Official results for ELS. 
 
EAW Precision Recall Coverage 
With U 0.590 0.590 100% 
Without U 0.601 0.588 97.795% 
Table 2. Official results for EAW. 
5 Conclusion 
This paper presents a new approach based on 
neural networks to disambiguate the word senses. 
We have used the LVQ algorithm to train a neural 
network to carry out the English lexical sample and 
English all words tasks. We have integrated two 
linguistic resources in the corpus provided by the 
organization: WordNet and SemCor.  
 
References 
Fellbaum, C. 1998. WordNet: An Electronic Lexi-
cal Database. The MIT Press 
Kohonen, T. 1995. Self-Organization and Associa-
tive Memory. 2nd Ed, Springer.Verlag, Berl?n. 
Kohonen, T., J. Hynninen, J. Kangas, J. Laak-
sonen, K. Torkkola. 1996. Technical Report, 
LVQ_PAK: The Learning Vector Quantization 
Program Package. Helsinki University of Tech-
nology, Laboratory of Computer and Information 
Science, FIN-02150 Espoo, Finland. 
Miller G., C. Leacock, T. Randee, R. Bunker. 
1993. A Semantic Concordance. Proc. of the 3rd 
DARPA Workshop on Human Language Tech-
nology. 
Salton, G. & McGill, M.J. 1983. Introduction to 
Modern Information Retrieval. McGraw-Hill, 
New York. 
Schmid, H., 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings 
of International Conference on New Methods in 
Language Processing. 
 
 
 
 
T
1 
T
2 
T
3 
T
M 
... 
Sense
1
 
Sense
1
 
Sense
N
 
... 
The R2D2 Team at SENSEVAL-3?
Sonia Va?zquez, Rafael Romero
Armando Sua?rez and Andre?s Montoyo
Dpto. de Lenguajes y Sistemas. Informa?ticos
Universidad de Alicante, Spain
{svazquez,romero}@dlsi.ua.es
{armando,montoyo}@dlsi.ua.es
Manuel Garc??a, M. Teresa Mart??n ?
M. ?Angel Garc??a and L. Alfonso Uren?a
Dpto. de Informa?tica
Universidad de Jae?n, Spain
{mgarcia,maite}@ujaen.es
{magc,laurena}@ujaen.es
Davide Buscaldi, Paolo Rosso ?
Antonio Molina, Ferra?n Pla? and Encarna Segarra
Dpto. de Sistemas Informa?ticos y Computacio?n
Univ. Polit. de Valencia, Spain
{dbuscaldi,prosso}@dsic.upv.es
{amolina,fpla,esegarra}@dsic.upv.es
Abstract
The R2D2 systems for the English All-Words and
Lexical Sample tasks at SENSEVAL-3 are based on
several supervised and unsupervised methods com-
bined by means of a voting procedure. Main goal
was to take advantage of training data when avail-
able, and getting maximum coverage with the help
of methods that not need such learning examples.
The results reported in this paper show that super-
vised and unsupervised methods working in par-
allel, and a simple sequence of preferences when
comparing the answers of such methods, is a feasi-
ble method. . .
The whole system is, in fact, a cascade of deci-
sions of what label to assign to a concrete instance
based on the agreement of pairs of systems, when
it is possible, or selecting the available answer from
one of them. In this way, supervised are preferred to
unsupervised methods, but these last ones are able
to tag such words that not have available training
data.
1 Introduction
Designing a system for Natural Language Process-
ing (NLP) requires a large knowledge on language
structure, morphology, syntax, semantics and prag-
matic nuances. All of these different linguistic
knowledge forms, however, have a common asso-
ciated problem, their many ambiguities, which are
difficult to resolve.
In this paper we concentrate on the resolution
of the lexical ambiguity that appears when a given
word in a context has several different meanings.
? This paper has been partially supported by the Spanish
Government (CICyT) under project number TIC-2003-7180
and the Valencia Government (OCyT) under project number
CTIDIB-2002-151
This specific task is commonly referred as Word
Sense Disambiguation (WSD). This is a difficult
problem that is receiving a great deal of attention
from the research community because its resolu-
tion can help other NLP applications as Machine
Translation (MT), Information Retrieval (IR), Text
Processing, Grammatical Analysis, Information Ex-
traction (IE), hypertext navigation and so on.
The R2D2 Team has participated in two tasks:
English all-words and lexical sample. We use sev-
eral different systems both supervised and unsuper-
vised. The supervised methods are based on Max-
imum Entropy (ME) (Lau et al, 1993; Berger et
al., 1996; Ratnaparkhi, 1998), neural network using
the Learning Vector Quantization algorithm (Koho-
nen, 1995) and Specialized Hidden Markov Mod-
els (Pla, 2000). The unsupervised methods are Rel-
evant Domains (RD) (Montoyo et al, 2003) and
the CIAOSENSO WSD system which is based on
Conceptual Density (Agirre and Rigau, 1995), fre-
quency of WordNet (Miller et al, 1993a) senses and
WordNet Domains (Magnini and Cavaglia, 2000).
In the following section we will show a more
complete description of the systems. Next, how
such methods were combined in two voting sys-
tems, and the results obtained in SENSEVAL-3. Fi-
nally, some conclusions will be presented.
2 Systems description
In this section the systems that have participated at
SENSEVAL-3 will be described.
2.1 Maximum Entropy
ME modeling provides a framework for integrating
information for classification from many heteroge-
neous information sources (Manning and Schu?tze,
1999). ME probability models have been success-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
fully applied to some NLP tasks, such as POS tag-
ging or sentence boundary detection (Ratnaparkhi,
1998). ME have been also applied to WSD (van
Halteren et al, 2001; Montoyo and Sua?rez, 2001;
Sua?rez and Palomar, 2002), and as meta-learner in
(Ilhan et al, 2001).
Our ME-based system has been shown competi-
tive (Ma`rquez et al, 2003) when compared to other
supervised systems such as Decision Lists, Support
Vector Machines, and AdaBoost. The features that
were defined to train the system are those described
in Figure 1.
? the target word itself
? lemmas of content-words at positions ?1, ?2, ?3
? words at positions ?1, ?2,
? words at positions ?1, ?2, ?3
? content-words at positions ?1, ?2, ?3
? POS-tags of words at positions ?1, ?2, ?3
? lemmas of collocations at positions (?2,?1),
(?1,+1), (+1,+2)
? collocations at positions (?2,?1), (?1,+1),
(+1,+2)
? lemmas of nouns at any position in context, occur-
ring at least m% times with a sense
? grammatical relation of the target word
? the word that the target word depends on
? the verb that the target word depends on
? the target word belongs to a multi-word, as identi-
fied by the parser
Figure 1: Features Used for the Supervised Learn-
ing of the ME system
Because the ME system needs annotated data
for the training, Semcor (Miller et al, 1993b) was
used for the English All-Words task, the system
was trained using Semcor (Miller et al, 1993b), and
parsed by Minipar (Lin, 1998). Only those words
that have 10 examples or more in Semcor were pro-
cessed in order to obtain a ME classifier.
For the Spanish Lexical Sample task, the train-
ing data from SENSEVAL-3 was the source of la-
beled examples. We did not use any parser, just the
lemmatization and POS-tagging information sup-
plied into the training data itself.
2.2 UPV-SHMM-AW
The upv-shmm-aw WSD system is a supervised ap-
proach based on Specialized Hidden Markov Mod-
els (SHMM).
Basically, a SHMM consists of changing the
topology of a Hidden Markov Model in order to get
a more accurate model which includes more infor-
mation. This is done by means of an initial step
previous to the learning process. It consists of the
redefinition of the input vocabulary and the output
tags. This redefinition is done by means of two pro-
cesses which transform the training set: the selec-
tion process chooses which input features (words,
lemmas, part-of-speech tags, ...) are relevant to the
task, and the specialization process redefines the
output tags by adding information from the input.
This specialization produces some changes in the
model topology, in order to allow the model to bet-
ter capture some contextual restrictions and to get a
more accurate model.
We used as training data the part of the Sem-
Cor corpus that is semantically annotated and su-
pervised for nouns, verbs, adjectives and adverbs,
and the test data set provided by SENSEVAL-2.
We used 10% of the training corpus as a develop-
ment data set in order to determine the best selection
and specialization criteria.
In the experiments, we used WordNet1.6 (Miller
et al, 1993a) as a dictionary that supplies all the
possible semantic senses for a given word. Our sys-
tem disambiguated all the polysemic lemmas, that
is, the coverage of our system was 100%. For un-
known words (words that did not appear in the train-
ing data set), we assigned the first sense in WordNet.
2.3 Relevant Domains
This is an unsupervised WSD method based on the
WordNet Domains lexical resource (Magnini and
Cavaglia, 2000). The underlaying working hypoth-
esis is that domain labels, such as ARCHITEC-
TURE, SPORT and MEDICINE provide a natural
way to establish semantic relations between word
senses, that can be used during the disambiguation
process. This resource has already been used on
Word Sense Disambiguation (Magnini and Strappa-
rava, 2000), but it has not made use of glosses infor-
mation. So our approach make use of a new lexical
resource obtained from glosses information named
Relevant Domains.
First step is to obtain the Relevant Domains re-
source from WordNet glosses. For this task is nec-
essary a previous part-of-speech tagging of Word-
Net glosses (each gloss has associated a domain la-
bel). So we extract all nouns, verbs, adjectives and
adverbs from glosses and assign them their associ-
ated domain label. With this information and using
the Association Ratio formula(w=word,D=domain
label), in (1), we obtain the Relevant Domains re-
source.
AR(w,D) = Pr(w|D)log2Pr(w|D)Pr(w) (1)
The final result is for each word, a set of domain
labels sorted by Association Ratio, for example,
for word plant? its Relevant Domains are: genetics
0.177515, ecology 0.050065, botany 0.038544 . . . .
Once obtained Relevant Domains the disam-
biguation process is carried out. We obtain from
the text source the context words that co-occur with
the word to be disambiguated (context could be
a sentence or a window of words). We obtain a
context vector from Relevant Domains and context
words (in case of repeated domain labels, they are
weighted). Furthermore we need a sense vector ob-
tained in the same way as context vector from words
of glosses of each word sense. We select the cor-
rect sense using the cosine measure between con-
text vector and sense vectors. So the selected sense
is that for which the cosine with the context vector
is closer to one.
2.4 LVQ-JA ?EN-ELS
The LVQ-JA ?EN-ELS system (Garc??a-Vega et al,
2003) is based on a supervised learning algorithm
for WSD. The method trains a neural network using
the Learning Vector Quantization (LVQ) algorithm
(Kohonen, 1995), integrating Semcor and several
semantic relations of WordNet.
The Vector Space Model (VSM) is used as an in-
formation representation model. Each sense of a
word is represented as a vector in an n-dimensional
space where n is the number of words in all its con-
texts.
We use the LVQ algorithm to adjust the word
weights. The input vector weights are calculated
as shown by (Salton and McGill, 1983) with the
standard (tf ? idf). They are presented to the LVQ
network and, after training, the output vectors are
obtained, containing the adjusted weights for all
senses of each word.
Any word to disambiguate is represented with a
vector in the same way. This representation must be
compared with all the trained sense vectors of the
word by applying the cosine similarity rule:
sim(wk, xi) = wk ? xi| wk | ? | xi | (2)
The sense corresponding to the vector of highest
similarity is selected as the disambiguated sense.
To train the neural network we have inte-
grated semantic information from two linguistic re-
sources: SemCor1.6 corpus and WordNet1.7.1 lex-
ical database. From Semcor1.6 we used the para-
graph as a contextual semantic unit and each con-
text was included in the training vector set. From
WordNet1.7.1 some semantic relations were consid-
ered, specifically, synonymy, antonymy, hyponymy,
homonymy, hyperonymy, meronymy, and coordi-
nate terms. This information was introduced to the
training set through the creation of artificial para-
graphs with the words of each relation. So, for a
word with 7 senses, 7 artificial paragraphs with the
synonyms of the 7 senses were added, 7 more with
all its hyponyms, and so on.
The learning algorithm is very simple. First, the
learning rate and the codebook vectors are initial-
ized. Then, the following procedure is repeated for
all the training input vectors until a stopping crite-
rion is satisfied:
- Select a training input pattern, x, with class d,
and present it to the network
- Calculate the Euclidean distance between the in-
put vector and each codebook vector || x? wk ||
- Select the codebook vector, wc, that is closest to
the input vector, x, like the winner sense.
- The winner neuron updates its weights accord-
ing the learning equation:
wc(t+ 1) = wc(t) + s ? ?(t) ? [x(t)? wc(t)] (3)
where s = 0, if k 6= c; s = 1, if x(t) and wc(t)
belong to the same class (c = d); and s = ?1, if
they do not (c 6= d). ?(t) is the learning rate, and
0 < ?(t) < 1 is a monotically decreasing func-
tion of time. It is recommended that ?(t) should
already initially be rather small, say, smaller than
0.1 (Kohonen, 1995) and ?(t) continues decreasing
to a given threshold, u, very close to 0.
2.5 CIAOSENSO
The CIAOSENSO WSD system is an unsupervised
system based on Conceptual Density, the frequency
of WordNet sense, and WordNet Domains. Concep-
tual Density is a measure of the correlation among
the sense of a given word and its context. The
noun sense disambiguation is performed by means
of a formula combining the Conceptual Density
with WordNet sense frequency (Rosso et al, 2003).
The context window used in both the English all-
words and lexical sample tasks is of 4 nouns. Ad-
ditional weights are assigned to those senses hav-
ing the same domain as the context nouns? senses.
Each weight is proportional to the frequency of such
senses, and is calculated as MDW (f, i) = 1/f ?1/i
where f is an integer representing the frequency
of the sense of the word to be disambiguated and
i gives the same information for the context word.
Example: If the word to be disambiguated is doc-
tor, the domains for senses 1 and 4 are, respec-
tively, Medicine and School. Therefore, if one of
the context words is university, the resulting weight
for doctor(4) and university(3) is 1/4 ? 1/3.
The sense disambiguation of an adjective is per-
formed only on the basis of the above weights.
Given one of its senses, we extract the synsets ob-
tained by the similar to, pertainym and attribute
relationships. For each of them, we calculate the
MDW with respect to the senses of the context
noun. The weight assigned to the adjective sense
is the average between these MDWs. The se-
lected sense is the one having the maximum average
weight.
The sense disambiguation of a verb is done nearly
in the same way, but taking into consideration only
the MDWs with the context words. In the all-words
task the context words are the noun before and af-
ter the verb, whereas in the lexical sample task the
context words are four (two before and two after the
verb), without regard to their morphological cate-
gory. This has been done in order to improve the
recall in the latter task, for which the test corpus is
made up mostly by verbs.
The sense disambiguation of adverbs (in both
tasks) is carried out in the same way of the disam-
biguation of verbs for the lexical sample task.
3 Tasks Processing
We have selected several combinations of such sys-
tems described before for two voting systems, one
for the Lexical-Sample task and the other for the
All-Words task.
3.1 English Lexical Sample Task
At the English Lexical Sample task we combined
the answers of four systems: Relevant Domains,
CIAOSENSO, LVQ-JA ?EN-ELS and Maximum En-
tropy.
The four methods worked in parallel and their
sets of answers were the input of a majority voting
procedure. This procedure selected those answers
with more systems agreements. In case of tie we
gave priority to supervised systems.
With this voting system we obtained around a
63% precision and a 52% recall.
3.2 English All Words Task
For this task we used a voting system combining
the results of Relevant Domains, Maximum En-
tropy, CIAOSENSO and UPV-SHMM-AW. So we
obtained the final results after 10 steps.
Step 1, we selected those answers with agree-
ment between ME and UPV-SHMM-AW (super-
vised systems).
Step 2, from no agreement in step 1 we selected
those answers with agreement between ME and Rel-
evant Domains.
Step 3, from no agreement in step 2 we selected
those answers with agreement between ME and
CIAOSENSO.
Step 4, from no agreement in step 3 we se-
lected those answers with agreement between
CIAOSENSO and UPV-SHMM-AW.
Step 5, from no agreement in step 4 we se-
lected those answers with agreement between UPV-
SHMM-AW and Relevant Domains.
Step 6, from no agreement in step 5 we selected
those answers with agreement between Relevant
Domains and CIAOSENSO.
Step 7, from no agreement in step 6 we selected
Maximum Entropy answers.
Step 8, from the remaining unlabeled instances
we selected UPV-SHMM-AW answers.
Step 9, from the remaining unlabeled instances
we selected Relevant Domains answers.
Step 10, from the remaining unlabeled instances
we selected CIAOSENSO answers.
Last step was labeling with the most frequent
sense in WordNet those instances that had been not
tagged by any system, but in view of the final results
only two instances had not answer and we didn?t
find them in WordNet.
With this voting system preference was given to
supervised systems over unsupervised systems.
We obtained around a 63% precision and a 63%
recall.
4 Conclusions
This paper presents the main characteristics of
the Maximum Entropy, LVQ-JAEN-ELS, UPV-
SHMM-AW, Relevant Domains and CIAOSENSO
systems within the framework of SENSEVAL-3 En-
glish Lexical Sample and All Words tasks. These
systems are combined with a voting technique ob-
taining a promising results for English All Words
and English Lexical Sample tasks.
References
Eneko Agirre and German Rigau. 1995. A pro-
posal for word sense disambiguation using Con-
ceptual Distance. In Proceedings of the Interna-
tional Conference ?Recent Advances in Natural
Language Processing? (RANLP95).
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Manuel Garc??a-Vega, Mar??a Teresa Mart??n-
Valdivia, and Luis Alfonso Uren?a. 2003.
Aprendizaje competitivo lvq para la desam-
biguacio?n le?xica. Revista de la Sociedad
Espaola para el Procesamiento del Lenguaje
Natural, 31:125?132.
H. Tolga Ilhan, Sepandar D. Kamvar, Dan Klein,
Christopher D. Manning, and Kristina Toutanova.
2001. Combining Heterogeneous Classifiers for
Word-Sense Disambiguation. In Judita Preiss
and David Yarowsky, editors, Proceedings of the
2nd International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-2),
pages 87?90, Toulouse, France, July. ACL-
SIGLEX.
T. Kohonen. 1995. Self-organization and associa-
tive memory. 2nd Ed. Springer Verlag, Berlin.
R. Lau, R. Rosenfeld, and S. Roukos. 1993.
Adaptative statistical language modeling using
the maximum entropy principle. In Proceedings
of the Human Language Technology Workshop,
ARPA.
Dekang Lin. 1998. Dependency-based evaluation
of minipar. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating Subject Field Codes into WordNet. In
M. Gavrilidou, G. Crayannis, S. Markantonatu,
S. Piperidis, and G. Stainhaouer, editors, Pro-
ceedings of LREC-2000, Second International
Conference on Language Resources and Evalu-
ation, pages 1413?1418, Athens, Greece.
Bernardo Magnini and C. Strapparava. 2000. Ex-
periments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Work-
shop on Word Senses and Multilinguality, Hong
Kong, China.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge,
Massachusetts.
Llu??s Ma`rquez, Fco. Javier Raya, John Car-
roll, Diana McCarthy, Eneko Agirre, David
Mart??nez, Carlo Strapparava, and Alfio
Gliozzo. 2003. Experiment A: several all-words
WSD systems for English. Technical Report
WP6.2, MEANING project (IST-2001-34460),
http://www.lsi.upc.es/?nlp/meaning/meaning.html.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1993a. Five Papers on WordNet. Special Issue of
the International journal of lexicography, 3(4).
George A. Miller, C. Leacock, R. Tengi, and
T. Bunker. 1993b. A Semantic Concordance. In
Proceedings of ARPA Workshop on Human Lan-
guage Technology, pages 303?308, Plainsboro,
New Jersey.
Andre?s Montoyo and Armando Sua?rez. 2001.
The University of Alicante word sense disam-
biguation system. In Judita Preiss and David
Yarowsky, editors, Proceedings of the 2nd In-
ternational Workshop on Evaluating Word Sense
Disambiguation Systems (SENSEVAL-2), pages
131?134, Toulouse, France, July. ACL-SIGLEX.
Andre?s Montoyo, Sonia Va?zquez, and German
Rigau. 2003. Me?todo de desambiguacio?n le?xica
basada en el recurso le?xico Dominios Rele-
vantes. Procesamiento del Lenguaje Natural, 30,
september.
F. Pla. 2000. Etiquetado Le?xico y Ana?lisis
Sinta?ctico Superficial basado en Modelos Es-
tad??sticos. Tesis doctoral, Departamento de Sis-
temas Informa?ticos y Computacio?n. Universidad
de Polite?cnica de Valencia, Septiembre.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
P. Rosso, F. Masulli, D. Buscaldi, F. Pla, and
A. Molina. 2003. Automatic noun disambigua-
tion. LNCS, Springer Verlag, 2588:273?276.
G. Salton and M.J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill,
New York.
Armando Sua?rez and Manuel Palomar. 2002.
A maximum entropy-based word sense disam-
biguation system. In Hsin-Hsi Chen and Chin-
Yew Lin, editors, Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 960?966, Taipei, Taiwan, August.
COLING 2002.
H. van Halteren, J. Zavrel, and W. Daelemans.
2001. Improving accuracy in wordclass tag-
ging through combination of machine learning
systems. Computational Linguistics, 27(2):199?
230.
BRUJA: Question Classification for Spanish.
Using Machine Translation and an English Classifier.
Miguel ?A. Garc??a Cumbreras
SINAI Group
Computer Sciences
University of Jae?n. Spain
magc@ujaen.es
L. Alfonso Uren?a Lo?pez
SINAI Group
Computer Sciences
University of Jae?n. Spain
laurena@ujaen.es
Fernando Mart??nez Santiago
SINAI Group
Computer Sciences
University of Jae?n .Spain
dofer@ujaen.es
Abstract
Question Classification is an important
task in Question Answering Systems. This
paper presents a Spanish Question Classi-
fier based on machine learning, automatic
online translators and different language
features. Our system works with Eng-
lish collections and bilingual questions
(English/Spanish). We have tested two
Spanish-English online translators to iden-
tify the lost of precision. We have made
experiments using lexical, syntactic and
semantic features to test which ones made
a better performance. The obtained results
show that our system makes good classifi-
cations, over a 80% in terms of accuracy
using the original English questions and
over a 65% using Spanish questions and
machine translation systems. Our conclu-
sion about the features is that a lexical,
syntactic and semantic features combina-
tion obtains the best result.
1 Introduction
A Question Answering (QA) system seeks and
shows the user an accurate and concise answer,
given a free-form question, and using a large text
data collection.
The use of Cross Language Information Re-
trieval Systems (CLIR) is growing, and also the
application of these ones into other general sys-
tems, such as Question Answering or Question
Classification.
A CLIR system is an Information Retrieval Sys-
tem that works with collections in several lan-
guages, and extract relevant documents or pas-
sages (Grefenstette, 1998).
We have proposed a Multilingual Question An-
swering System (BRUJA - in Spanish ?Busqueda
de Respuestas University of Jaen?) that works with
collections in several languages. Since there are
several languages, tasks such as obtaining rele-
vant documents and extracting the answer could
be accomplished in two ways: using NPL tools
and resources for each language or for a pivot lan-
guage only (English) and translating to the pivot
language the rest of the relevant information when
it is required. Because of the translation step, the
second approach is less accurate but more practi-
cal since we need only NPL resources for English.
The central question is the noise, because of the
translation process, is too high in order to use this
approach in spite of their practical advantages.
The first step of this system is a Question Clas-
sifier (QC). Given a query, a question classifica-
tion module obtains the class of such question.
This information is useful for the extraction of the
answer. For example, given the query ?Where
is Madrid?, the QA system expects a location
entity as answer type. The proposed QA mod-
ule works with questions in several languages,
translates them into English using different online
translators, and obtains the type of questions and
some features, such as the focus, the keywords
or the context. In this work we aim to find out
whether a multilingual QC module is possible by
using translation tools and English as pivot lan-
guage or not.
2 Question Classification
Question Classification is the task that, given a
question, classifies it in one of k semantic classes.
Some QC systems are based on regular expres-
sions and manual grammatical rules (Van Durme
et al, 2003).
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
39
Recent works in QC have studied different ma-
chine learning methods. (Zhang and Lee, 2003)
propose a QC system that uses Support Vec-
tor Machine (SVM) as the best machine learn-
ing algorithm. They compare the obtained results
with other algorithms, such as Nearest Neighbors,
Naive Bayes, Decision Tree or Sparse Network of
Winnows (SNoW).
(Li and Roth, 2002) propose a system based on
SNoW. They used five main classes and fifty fined
classes. Other systems have used SVM and modi-
fied kernels.
QC systems have some restrictions (Hacioglu
and Ward, 2003), such as:
? Traditional question classification uses a set
of rules, for instance ?questions that start
with Who ask about a person?. These are
manual rules that have to be revised to im-
prove the results.
? These rules are very weak, because when new
questions arise, the system has to be updated
to classify them.
Most of the QC systems use English as the main
language, and some of the best and standard re-
sources are developed for English.
It would be possible to build a question classi-
fier for every language based on machine learning,
using a good training corpus for each language,
but is something expensive to produce. For this
reason we have used Machine Translation Sys-
tems.
Machine Translation (MT) systems are very ap-
preciated in CLIR (McNamee et al, 2000). Last
years these systems have improved the results, but
there are not translators for each language pair and
the quality of the result depends on this pair.
The reason of using MT and not a Spanish clas-
sifier is simple: we have developed a multilingual
QA system that works in this moment with three
languages: English, Spanish and French. Because
it is too complex for us to work with resources into
these three languages and also to manage the in-
formation into three languages, our kernel system
works into English, and we use MT to translate
information when it is necessary.
We have developed a QC system that covers
three tasks:
? It uses machine learning algorithms. We have
tested methods based on Support Vector Ma-
chine, for instance SVMLight or LibSVM,
and TiMBL. TiMBL 1 is a program that
implements several Memory-Based Learning
techniques. It stores a representation of the
training set explicitly in memory, and classi-
fies new cases by extrapolation from the most
similar stored cases.
? To classify Spanish questions we have
checked two online machine translators. Our
proposal is to study how the translation can
affect in the final results, compared to the
original English results.
? Finally, we would obtain different results ap-
plying different levels of features (lexical,
syntactic and semantic). In the next section
well explain them and in results chapter we
will see these differences.
Our QC system has three independent modules,
so it will be easy to replace each one with other to
improve the final results. In Figure 1 we can see
them.
Figure 1: QC system Modules.
The first module translates the question into
other languages, Spanish in this case. We have
used two machine translation systema that work
well for the language pair Spanish-English: Epals
and Prompt. This module could work with other
machine translation systems and other languages
if there would be a good translator for the language
pair used.
The second module extracts some relevant fea-
tures (see next section) using the original or the
translated English questions. Some of these fea-
tures will be used by the machine learning module
(lexical, syntactic and semantic features) and the
1ILK Research Group, Tilburg University and CNTS Re-
search Group, University of Antwerp
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
40
others will be used later in the answers extraction
phase. Take into account that the second module
also extracts important features such as the context
of the question, the focus or the keywords that we
would use in next steps of the Question Answering
system.
The final module applies the machine learn-
ing algorithm and returns the question category
or class. In our first experiments we used Li-
brary for Support Vector Machines (LibSVM) and
Bayesian Logistic Regression (BBR), but for this
one we have used Tilburg Memory Based Learner
(TiMBL).
TiMBL (Daelemans et al, 2004) implements
several Memory-Based Learning techniques, clas-
sic k-NN classification kernel and several metrics.
It implements Stanfill modified value difference
metric (MVDM), Jeffrey Divergence and Class
voting in the k-NN kernel according to the dis-
tance of the nearest neighbors. It makes classifi-
cation using heuristic approximations, such as the
IGTREE decision tree algorithm and the TRIBL
and TRIBL2 hybrids. It also has optimizations for
fast classification.
2.1 Features in Question Classification
We have analyzed each question in order to extract
the following features:
? Lexical Features
? The two first words of the question
? All the words of the question in lower-
case
? The stemming words
? Bigrams of the question
? Each word with its position in the ques-
tion
? The interrogative pronoun of the ques-
tion
? The headwords of the nouns and verbs
? Syntactic Features
? The interrogative pronoun and the Part
of Speech (POS) of the rest of the words
? The headword (a word to which an in-
dependent meaning can be assigned) of
the first noun phrase
? All POS
? Chunks
? The first verb chunk
? The length of the question
? Semantic Features
? The question focus (a noun phrase that
is likely to be present in the answer)
? POS with the named entities recognized
? The type of the entity if the focus is one
of them
? Wordnet hypernyms for the nouns and
Wordnet synonyms for the verbs
We have used some English resources such as
the POS tagger TreeTagger (Schmid, 1994), Ling-
pipe 2 to make Named Entity Recognition, and the
Porter stemmer (Porter, 1980). We have also used
Wordnet to expand the queries.
3 Experiments and Results
3.1 Experimental Method
The experiments are made using some public
datasets available by USC (Hovy et al, 1999),
UIUC and TREC 3 as training and test collections.
These datasets have been labeled manually by
UIUC group by means of the following general
and detailed categories:
ABBR: abbreviation, expansion.
DESC: definition, description, manner, reason.
ENTY: animal, body, color, creation, cur-
rency, disease/medical, event, food, instrument,
language, letter, other, plant, product, religion,
sport, substance, symbol, technique, term, vehicle,
word.
HUM: description, group, individual, title.
LOC: city, country, mountain, other, state.
NUM: code, count, date, distance, money, or-
der, other, percent, period, speed, temperature,
size, weight.
For instance the question ?What does NATO
mean?? is an ABBR (abbreviation) category,
?What is a receptionist?? is a DESC (definition)
category or ?When did George Bush born?? is a
NUM (numeric) category.
The training data are a set of 5500 questions and
the test data are a set of 500 questions. All ques-
tions were labelled for the 10th conference Cross-
Language Evaluation Forum of Question Answer-
ing (CLEF-QA).
2LingPipe is a suite of Java tools designed to perform
linguistic analysis on natural language data, available in
http://www.alias-i.com/lingpipe
3http://trec.nist.gov
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
41
The same dataset has been used in other inves-
tigations, such as in (Li and Roth, 2002).
The distribution of these 5500 training ques-
tions, with respect to its interrogative pronoun or
the initial word is showed in Table 1.
Likewise, the distribution of categories of these
5500 training questions is showed in Table 2.
Table 1: Training questions distribution according
with its interrogative pronoun
Type Number
What 3242
Who 577
How 764
Where 273
When 131
Which 105
Why 103
Name 91
In 67
Define 4
Whom 4
Others 91
Table 2: Training questions distribution according
with its general category.
Category Number
ABBR 86
DESC 1162
ENTY 1251
HUM 1223
LOC 835
NUM 896
The distribution of the 500 test questions, with
respect to its interrogative pronoun or the initial
word, is showed in Table 3, and the distribution of
categories of these 500 test questions is showed in
Table 4.
Table 3: Test questions distribution according with
its interrogative pronoun.
Type Number
What 343
Who 47
How 35
Where 26
When 26
Which 6
Why 4
Name 2
In 5
Others 6
In our experiments we try to identify the general
category. Our proposal is to try a detailed classifi-
cation later.
Table 4: Test questions distribution according with
its general category.
Category Number
ABBR 9
DESC 138
ENTY 94
HUM 65
LOC 81
NUM 113
We have used the Accuracy as a general mea-
sure and the Precision of each category as a de-
tailed measure.
Accuracy = ]ofcorrectpredictions
]ofpredictions
(1)
precision(c) = ]ofcorrectpredictionsofthecategoryc
]ofpredictionsofthecategoryc
(2)
Other measure used is the F-score, defined
as the harmonic mean of precision and recall
(Van Rijsbergen, 1979). It is a commonly used
metric to summarize precision and recall in one
measure.
F ? score = 2 ? precision ? recall
precision + recall
(3)
3.2 Results
We have made some experiments changing the
machine translation systems:
? 5500 training questions and 500 test ques-
tions, all into English. This is the basic case.
? 5500 training questions into English and 500
test questions translated from Spanish using
the MT Epals.
? 5500 training questions into English and 500
test questions translated from Spanish using
the MT Prompt.
The MT resources are available in the following
URLs:
? Epals
http://www.epals.com
? Prompt
http://translation2.paralink.com
According to the lexical, syntactic and semantic
features we have made seven features sets. Our
proposal here is to check which ones increase the
final results. These features sets are the following:
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
42
1. Lexical Features: interrogative pronoun
(lex1)
2. Lexical and Syntactic Features: Two first
words of the question + All the words of the
question in lowercase + Stemming words +
Headwords (lexsyn2)
3. Lexical and Syntactic Features: previous four
+ Each word with its position in the ques-
tion + interrogative pronoun + The first verb
chunk (lexsyn3)
4. Semantic Features: The question focus +
POS with the named entities recognized +
The type of the entity if the focus is one of
them (sem4)
5. Syntactic Features: The interrogative pro-
noun and the Part of Speech (POS) of the
rest of the words + All POS + Chunks + The
length of the question (sin5)
6. All Lexical + all Syntactic + all Semantic
(lexsemsin6)
7. Lexical Features: Two first words of the
question + interrogative pronoun ; Syntactic
Features: + The headwords of the nouns and
verbs + The first verb chunk + the interrog-
ative pronoun + the Part of Speech (POS) of
the rest of the words + The length of the ques-
tion; Semantic Features: POS with the named
entities recognized (lexsemsin7)
We can see in the Table 5 the obtained results in
terms of global accuracy.
Table 5: Results in terms of Accuracy.
Features English original Epals Prompt
lex1 0,458 0,334 0,414
lexsyn2 0,706 0,656 0,632
lexsyn3 0,718 0,638 0,612
sem4 0,675456 0,59798 0,629555
sin5 0,608 0,438 0,518
lexsemsin6 0,839757 0,662626 0,722672
lexsemsin7 0,8 0,678 0,674
Note that the average loss of precision is around
17% if we use Epals, and around 12% if we use
Prompt.
(Li and Roth, 2002) obtain a better performance
for English, around a 92.5% in terms of accuracy.
The best results are obtained when we use a
combination of all lexical, syntactic and seman-
tic features. The main reason is that the classifier
works better when the number of features, which
can be different to each category, is increased.
For future work, it will be also necessary to
study the time consumption for each features set,
to decide which ones can be used.
Table 6 shows the results in terms of F-score.
Table 6: Results in terms of F-score.
Features English original Epals Prompt
lex1 0,476077 0,319793 0,441075
lexsyn2 0,708444 0,669692 0,6455
lexsyn3 0,721258 0,644813 0,614353
sem4 0,649405 0,593019 0,620068
sin5 0,576356 0,404038 0,48739
lexsemsin6 0,827789 0,664122 0,726667
lexsemsin7 0,795897 0,680039 0,68014
As an example in Table 7 we show detailed re-
sults for the best case, where the result for each
general category is showed.
Table 7: Detailed results for each category, using
the combination lexsemsin6 and the original Eng-
lish questions and the translated questions by us-
ing Prompt
Class English original Prompt
Precision F-score Precision F-score
ABBR 0.857 0.750 1 0.611
DESC 0.8442 0.906 0.695 0.806
ENTY 0.731 0.727 0.595 0.737
HUM 0.839 0.825 0.898 0.914
LOC 0.847 0.867 0.680 0.859
NUM 0.935 0.843 0.798 0.856
As we have seen there are no important differ-
ences between categories. In addition, this table
shows that the translation results are reliable since
for every category the lost of precision is similar
(about 15%).
There are some reasons for the lost of precision.
Some of them are the following:
1. Bad translation of synonym words. For in-
stance we can compare an English original
sentence: ?What are the animals that don?t
have backbones called??, and its Prompt
translation: ?How are they called the animals
that have no spines??. The word backbone
has been replaced with spine, so the IR sys-
tem cannot find the same lists of relevant doc-
uments.
2. Translation of Named Entities. For instance
we can compare an English original sentence:
?Who was Galileo??, and its Prompt transla-
tion: ?Who was Galilean??.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
43
3. General bad translations. For instance we can
compare an English original sentence: ?Who
discovered x-rays??, and its Prompt transla-
tion: ?Who discovered the beams the Xth??.
4 Conclusions
Multilingual Question Answering systems have
opened a new investigation task, where the ques-
tion classification is an important first phase to
know the type of answer and some relevant infor-
mation about this question.
Our option is to use some standards resources
for English and translate Spanish questions.
Of course we could develop a multilingual QC
system using good training corpus for every lan-
guage, but it is expensive to produce.
The use of machine translation systems is, then,
very important, so the study of different online
translators is a main task. In our case we have
applied them to translate questions from Spanish
into English.
We have made a complete investigation using
the two datasets of training and test questions that
have been used by other groups, all labelled man-
ually. Different parameters have been the test
file used (originally in English or translated from
Spanish with the MT Epals or Prompt), the ma-
chine learning algorithm, some TiMBL parame-
ters and the lexical, syntactic or semantic features.
The best results have been obtained using the
original English questions and a combination of
lexical, syntactic and semantic features. The best
MT has been Prompt.
We have some conclusions:
? Applying machine learning with a complete
set of training questions we obtain good re-
sults, over 0,8 in terms of accuracy.
? The use of machine translation systems de-
creases the results around 15%, but it will
be possible to increase the performance us-
ing other models based on machine learning
or a voting system for instance.
? A combination of all lexical, syntactic and se-
mantic features obtains the best results.
As future work we want to check the system
with other training and test datasets. We also want
to design a voting system using different QC mod-
els; models based on patterns (to detect the class
for some types of questions); models based on
rules (filtering non-redundancy types of questions.
For instance all questions with ?who? are related
to a person).
It could be also interested to test the combina-
tion between a better QC system, the current one
by Li and Roths for instance (Li and Roth, 2002),
and our machine translation method.
Finally, we want to study types of questions
with poor results in order to improve them apply-
ing other techniques, such as question expansion.
Acknowledgement This work has been sup-
ported by Spanish Government (MCYT) with
grant TIC2003-07158-C04-04.
References
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. Timbl: Tilburgmemory
based learner, version 5.1, reference guide. ilk tech-
nical report 04-02.
G. Grefenstette, editor. 1998. Cross-Language Infor-
mation Retrieval, volume 1. Kluwer academic pub-
lishers, Boston, USA.
K. Hacioglu and W. Ward. 2003. Question classifi-
cation with support vector machines and error cor-
recting codes. In Proceedings of Human Language
Technology conference (HLT-NAACL).
E. Hovy, L. Gerber, U. Hermjakob, C. Lin, and
D. Ravichandran. 1999. Towards sematics-based
answer pinpointing. In Proceedings of the DARPA
Human Language Technology conference (HLT).
X. Li and D. Roth. 2002. Learning question classifiers.
In In COLING?02.
P. McNamee, J. Mayfield, and C. Piatko. 2000. The
jhu/apl haircut system at trec- 8. In Proceedings of
the Eighth Text Retrieval Conference (TREC8).
M. F. Porter. 1980. An algorithm for suffix stripping.
In Program 14.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of International
Conference on New Methods in Language Process-
ing.
B. Van Durme, Y. Huang, A. Kupsc, and E. Nyberg.
2003. Towards light semantic processing for ques-
tion answering. In Proceedings of Human Language
Technology conference (HLT-NAACL).
C.J. Van Rijsbergen. 1979. Information retrieval.
D. Zhang and W. Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings
of the 26th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
44
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 78?82,
Prague, June 2007. c?2007 Association for Computational Linguistics
Combining Lexical-Syntactic Information with Machine Learning for
Recognizing Textual Entailment
Arturo Montejo-Ra?ez, Jose Manuel Perea, Fernando Mart??nez-Santiago,
Miguel A?ngel Garc??a-Cumbreras, Maite Mart??n-Valdivia, Alfonso Uren?a-Lo?pez
Dpto. de Informa?tica, Universidad de Jae?n
Campus de las Lagunillas s/n, 23071 - Jae?n
{amontejo, jmperea, dofer, magc, maite, laurena}@ujaen.es
Abstract
This document contains the description of
the experiments carried out by SINAI group.
We have developed an approach based on
several lexical and syntactic measures inte-
grated by means of different machine learn-
ing models. More precisely, we have eval-
uated three features based on lexical sim-
ilarity and 11 features based on syntactic
tree comparison. In spite of the relatively
straightforward approach we have obtained
more than 60% for accuracy. Since this
is our first participation we think we have
reached a good result.
1 Approach description
We fill face the textual entailment recognition us-
ing Machine Learning methods, i.e. identifying fea-
tures that characterize the relation between hypothe-
sis and associated text and generating a model using
existing entailment judgements that will allow us to
provide a new entailment judgement agains unseen
pairs text-hypothesis. This approach can be split into
the two processes shown in Figures 1 and 2.
In a more formal way, given a text t and an hy-
pothesis h we want to define a function e which takes
these two elements as arguments and returns and an-
swer to the entailment question:
e(t, h) =
{ Y ES if h is entailed by t
NO otherwise (1)
Now the question is to find that ideal function
Figure 1: Training processes
Figure 2: Classification processes
e(t, h). We will approximate this function using a
binary classifier:
e?(t, h) = bc(f,m) (2)
where
bc is a binary classifier
f is a set of features
m is the learned model for the classifier
Therefore, it only remains to select a binary clas-
sifier and a feature extraction method. We have per-
formed two experiments with different choices for
both decisions. These two experiments are detailed
below.
78
1.1 Lexical similarity
This experiment approaches the textual entailment
task being based on the extraction of a set of lexical
measures that show the existing similarity between
the hypothesis-text pairs. Our approach is similar
to (Ferrandez et al, 2007) but we make matching
between similar words too while (Ferrandez et al,
2007) apply exact matching (see below).
The first step previous to the calculation of the
different measures is to preprocess the pairs using
the English stopwords list. Next we have used the
GATE1 architecture to obtain the stems of tokens.
Once obtained stems, we have applied four different
measures or techniques:
? Simple Matching: this technique consists of
calculating the semantic distance between each
stem of the hypothesis and text. If this dis-
tance exceeds a threshold, both stems are con-
sidered similar and the similarity weight value
increases in one. The accumulated weight is
normalized dividing it by the number of ele-
ments of the hypothesis. In this experiment we
have considered the threshold 0.5. The values
of semantic distance measure range from 0 to
1. In order to calculate the semantic distance
between two tokens (stems), we have tried sev-
eral measures based on WordNet (Alexander
Budanitsky and Graeme Hirst, 2001). Lin?s
similarity measure (Lin, 1998) was shown to
be best overall measures. It uses the notion of
information content and the same elements as
Jiang and Conrath?s approach (Jiang and Con-
rath, 1997) but in a different fashion:
simL(c1, c2) = 2? log p(lso(c1, c2))log p(c1) + log p(c2)
where c1 and c2 are synsets, lso(c1,c2) is
the information content of their lowest super-
ordinate (most specific common subsumer) and
p(c) is the probability of encountering an in-
stance of a synset c in some specific corpus
(Resnik, 1995). The Simple Matching tech-
nique is defined in the following equation:
SIMmatching =
?
i?H similarity(i)
|H|
1http://gate.ac.uk/
where H is the set that contains the elements of
the hypothesis and similarity(i) is defined like:
similarity(i) =
{ 1 if ?j ? TsimL(i, j) > 0.5
0 otherwise
? Binary Matching: this measure is the same
that the previous one but modifying the simi-
larity function:
similarity(i) =
{ 1 if ?j ? T i = j
0 otherwise
? Consecutive Subsequence Matching: this
technique relies on forming subsequences of
consecutive stems in the hypothesis and match-
ing them in the text. The minimal size of the
consecutive subsequences is two and the max-
imum is the maximum size of the hypothesis.
Every correct matching increases in one the fi-
nal weight. The sum of the obtained weights of
the matching between subsequences of a cer-
tain size or length is normalized by the number
of sets of consecutive subsequences of the hy-
pothesis created for this length. These weights
are accumulated and normalized by the size of
the hypothesis less one. The Consecutive Sub-
sequence Matching technique is defined in the
following equations:
CSSmatching =
?|H|
i=2 f(SHi)
|H| ? 1
where SHi is the set that contains the subse-
quences of the hypothesis with i size or length
and f(SHi) is defined like:
f(SHi) =
?
j?SHi matching(j)
|H| ? i+ 1
where
matching(i) =
{ 1 if ?k ? STi k = j
0 otherwise
where STi represents the set that contains the
subsequences with i size from text.
? Trigrams: this technique relies on forming tri-
grams of words in the hypothesis and match-
ing them in the text. A trigram is a group of
79
three words. If a hypothesis trigram matches in
text, then the similarity weight value increases
in one. The accumulated weight is normalized
dividing it by the number of trigrams of the hy-
pothesis.
1.2 Syntactic tree comparison
Some features have been extracted from pairs
hypothesis-text related to the syntactic information
that some parser can produce. The rationale be-
hind it consists in measuring the similarity between
the syntactic trees of both hypothesis and associated
text. To do that, terms appearing in both trees are
identified (we call this alignment) and then, graph
distances (number of nodes) between those terms in
both trees are compared, producing certain values as
result.
In our experiments, we have applied the
COLLINS (Collins, 1999) parser to generate the
syntactic tree of both pieces of text. In Figure 3 the
output of the syntactic parsing for a sample pair is
shown. This data is the result of the syntactical anal-
ysis performed by the mentioned parser. A graph
based view of the tree corresponding to the hypoth-
esis is drawn in Figure 4. This graph will help us to
understand how certain similarity measures are ob-
tained.
Figure 3: Syntactic trees of sample hypothesis and
its associated text
<t>
(TOP (S (LST (LS 0302) (. .)) (NP (JJ Next) (NN year))
(VP (VBZ is) (NP (NP (DT the) (JJ 50th) (NN anniversary))
(PP (IN of) (NP (NP (DT the) (NNP Normandy) (NN invasion)
(, ,)) (NP (NP (DT an)(NN event)) (SBAR (IN that) (S (VP
(MD would) (RB n?t) (VP (VB have) (VP (VBN been) (ADJP
(JJ possible)) (PP (IN without) (NP (NP (DT the) (NNP
Liberty) (NN ships.)) (SBAR (S (NP (DT The) (NNS
volunteers)) (VP (VBP hope) (S (VP (TO to) (VP (VB raise)
(NP (JJ enough) (NN money)) (S (VP (TO to) (VP (VB sail)
(NP (DT the) (NNP O?Brien)) (PP (TO to) (NP (NNP France)))
(PP (IN for)(NP (DT the) (JJ big) (NNP D-Day) (NN celebration)
(. .))))))))))))))))))))))))))
</t>
<h>
(TOP (S (NP (NP (CD 50th) (NNP Anniversary)) (PP (IN of)
(NP (NNP Normandy) (NNP Landings)))) (VP (VBZ lasts) (NP
(DT a) (NN year) (. .)))))
</h>
From the sample above, the terms normandy, year
and anniversary appear in both pieces of text. We
say that these terms are ?aligned?. Therefore, for
the three possible pairs of aligned terms we can com-
pute the distance, in nodes, to go from one term to
the other at each tree. Then, the difference of these
Figure 4: Syntact tree of sample hypothesis
distances is computed and some statistics are gener-
ated. We can summarize the process of computing
this differences in the algorithm detailed in Figure 6.
Figure 5: Tree comparison process
For instance, in the tree represented in Figure 4
we can see that we have to perform 5 steps to go
from node Anniversary to node Normandy. Since
there are no more possible occurrences of these two
terms, then the minimal distance between them is
5. This value is also measured on the tree corre-
80
sponding to the text, and the absolute difference be-
tween these two minimal distances is stored in order
to compute final feature weights consisting in basic
statistical values. The algorithm to obtain the distri-
bution of distance differences is detailed in Figure 6.
Figure 6: Extraction of features based on syntactic
distance
Input:
a syntactic tree of the hypothesis Sh
a syntactic tree of the text St
Output :
the set of distance differences
Dd = {ddij : ti, tj ? T}
Pseudo code:
T ? aligned terms between Sh and St
Dd ? ?
for i = 1..n do
for j = i+ 1..n do
disth ? minimal distance between
nodes ti and tj in Sh
distt ? minimal distance between
nodes ti and tj in St
ddij ? |disth ? distt|
Dd ? {ddij} ?Dd
end-for
end-for
The statistics generated from the resulting list of
distances differences Dd are the following:
1. The number of aligned terms (3 in the given
example).
2. The number of matched POS values of aligned
terms, that is, if the term appears with the same
POS label in both texts (in the example An-
niversary differs in the POS label assigned).
3. The number of unmatched POS labels of
aligned terms.
4. The average distance in nodes through the syn-
tactic tree to go from one aligned term to an-
other.
5. The minimal distance difference found.
Table 1: Results with TiMBL and BBR classifiers
(Exp5 is the only official result reported in this pa-
per).
Experiment Classifier Accuracy
Exp1 BBR 0.6475
Exp2 BBR 0.64625
Exp3 BBR 0.63875
Exp4 TiMBL 0.6062
Exp5 TiMBL 0.6037
Exp6 TiMBL 0.57
6. The maximal distance difference found.
7. The standard deviation of distance differences.
In a similar way, differences in the depth level of
nodes for aligned terms are also calculated. From
the example exposed the following values were
computed:
* Aligned 3
* MatchedPOS 2
* UnmatchedPOS 1
* AvgDistDiff 0.0392156863
* MinDistDiff 0.0000000000
* MaxDistDiff 0.0588235294
* StdevDistDiff 0.0277296777
* AvgDepthDiff 2.0000000000
* MinDepthDiff 1.0000000000
* MaxDepthDiff 3.0000000000
* StdevDepthDiff 0.8164965809
2 Experiments and results
The algorithms used as binary classifiers are two:
Bayesian Logistic Regression (BBR)2 and TiMBL
(Daelemans et al, 1998). Both algorithms have been
trained with the devel data provided by the organiza-
tion of the Pascal challange. As has been explained
in previous sections, a model is generated via the
supervised learning process. This model m is then
feed into the classification variant of the algorithm,
which will decide whether a new hypothesis sample
is entailed by the given text or not.
The experiments and results are shown in Table 1:
where:
? Exp1 uses four features: three lexical similari-
ties (SIMmatching + CSSmatching + Trigrams)
and Syntactic tree comparison.
2http://www.stat.rutgers.edu/?madigan/BBR/ [available at
March 27, 2007]
81
? Exp2 uses five features: four lexical similari-
ties (SIMmatching + CSSmatching + Trigrams
+ BINmatching) and Syntactic tree compari-
son.
? Exp3 uses only three lexical similarities
(SIMmatching + CSSmatching + Trigrams).
? Exp4 uses the four lexical similarities
(SIMmatching + CSSmatching + Trigrams +
BINmatching)
? Exp5 uses only three lexical similarities
(SIMmatching + CSSmatching + Trigrams).
? Exp6 uses four features: three lexical similari-
ties (SIMmatching + CSSmatching + Trigrams)
and Syntactic tree comparison.
As we expected, the best result we have obtained
is by means of the integration of the whole of the
features available. More surprising is the good result
obtained by using lexical features only, even better
than experiments based on syntactical features only.
On the other hand, we expected that the integration
of both sort of features improve significatively the
performance of the system, but the improvement re-
spect of lexical features is poor (less than 2%). .
Similar topics share similar vocabulary, but not sim-
ilar syntax at all. Thus, we think we should to inves-
tigate semantic features better than the syntactical
ones.
3 Conclusions and future work
In spite of the simplicity of the approach, we have
obtained remarkable results: each set of features has
reported to provide relevant information concerning
to the entailment judgement determination. On the
other hand, these two approaches can be merged into
one single system by using different features all to-
gether and feeding with them several binary classi-
fiers that could compose a voting system. We will
do that combining TiMBL, SVM and BBR.We ex-
pect to improve the performance of the entailment
recognizer by this integration.
Finally, we want to implement a hierarchical ar-
chitecture based on constraint satisfaction networks.
The constraints will be given by the set of avail-
able features and the maintenance of the integration
across the semantic interpretation process.
4 Acknowledgements
This work has been partially financed by the
TIMOM project (TIN2006-15265-C06-03) granted
by the Spanish Government Ministry of Science and
Technology and the RFC/PP2006/Id 514 granted by
the University of Jae?n.
References
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: An experimental, application-
oriented evaluation of five measures.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 1998. Timbl: Tilburg memory
based learner, version 1.0, reference guide.
Oscar Ferrandez, Daniel Micolo, Rafael Mu noz, and
Manuel Palomar. 2007. Te?cnicas le?xico-sinta?cticas
para reconocimiento de inmplicacio?n textual. . Tec-
nolog??as de la Informaco?n Multilingu?e y Multimodal.
In press.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings of International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal.
82
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 402?407, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SINAI: Machine Learning and Emotion of the Crowd for Sentiment
Analysis in Microblogs
E. Mart??nez-Ca?mara
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
emcamara@ujaen.es
A. Montejo-Ra?ez
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
amontejo@ujaen.es
M. T. Mart??n-Valdivia
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
maite@ujaen.es
L. A. Uren?a-Lo?pez
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
laurena@ujaen.es
Abstract
This paper describes the participation of
the SINAI research group in the 2013 edi-
tion of the International Workshop Se-
mEval. The SINAI research group has
submitted two systems, which cover the
two main approaches in the field of sen-
timent analysis: supervised and unsuper-
vised.
1 Introduction
In the last years, the sentiment analysis (SA) re-
search community wants to go one step further,
which consists in studying different texts that
usually can be found in commerce websites or
opinions websites. Currently, the users publish
their opinions through other platforms, being one
of the most important the microblogging plat-
form Twitter1. Thus, the SA research commu-
nity is focused on the study of opinions that users
publish through Twitter. This interest is shown in
several workshops focused on the study of SA in
Twitter:
1. RepLab 2012 at CLEF2 (Amigo? et al,
2012): Competition carried out within the
CLEF conference, where the participants
had to develop a system for measuring the
reputation of commercial brands.
1http://twitter.com
2http://limosine-project.eu/events/
replab2012
2. TASS 2012 at SEPLN3(Villena-Roma?n et
al., 2013): Satellite event of the SEPLN
2012 Conference to foster the research in
the field of SA in social media, specifically
focused on the Spanish language.
In this paper is described the participation of
the SINAI4 research group in the second task of
the 2013 edition of the International Workshop
SemEval (Wilson et al, 2013). We have submit-
ted two systems (constrained and unconstrained).
The constrained system follows a supervised ap-
proach, while the unconstrained system is based
on an unsupervised approach which used two lin-
guistic resources: the Sentiment Analysis Lexi-
con5 (Hu and Liu, 2004) andWeFeelFine6 (Kam-
var and Harris, 2011).
The paper is organized as follows: first we
present a description of the preparing data pro-
cess. Then the constrained system is outlined.
The participation overview finishes with the de-
scription of the unconstrained system.
2 Preparing data
The organizers provided two sets of data, one for
training and another for the development. The
data was concerned by a set of identification
number of tweets with their corresponding po-
larity label. We used the script provided by the
organizers to download the two sets of tweets.
3http://www.daedalus.es/TASS/
4http://sinai.ujaen.es
5http://www.cs.uic.edu/?liub/FBS/
opinion-lexicon-English.rar
6http://wefeelfine.org
402
The python script was no able to download all the
tweets. The training set was composed by 8,633
tweets and the development set by 1,053 tweets.
The data preparation is a step in the workflow
of most data mining tasks. Also, in Natural Lan-
guage Processing is usual the preparation of the
documents or the texts for their further process-
ing. Internet is usually the source of texts for SA
tasks, so the application of a specific processing
to those texts with the aim of extracting their po-
larity is recommended. The texts published in
Twitter have several issues that must be resolved
before processing them:
1. The linguistic style of tweets is usually in-
formal, with a intensive usage of abbrevia-
tions, idioms, and jargon.
2. The users do not care about the correct use
of grammar, which increases the difficulty
of carrying out a linguistic analysis.
3. Because the maximum length of a tweet is
140 characters, the users normally refer to
the same concept with a large variety of
short and irregular forms. This problems is
known as data sparsity, and it is a challenge
for the sentiment-topic task.
4. The lack of context, which makes difficult
to extract the semantics of these sort pieces
of text.
Before applying a cleaning process to the cor-
pus with the aim of overcoming the issues de-
scribed above, we have studied the different
kinds of marks, like emoticons, question and ex-
clamation marks or hashtags in the tweets.
Regarding the issues listed above and the
marks in the tweets, we have carried out a clean-
ing and a normalization process which imply the
following operations:
1. The uppercase characters have been ex-
changed by lowercase characters.
2. Links have been replaced by the token
? ULR ?.
3. Question and exclamation marks have been
switched to the tokens ? QUESTION ? and
? EXCLAMATION ? respectively.
4. Mentions7 have been exchanged by the to-
ken ? MENTION ?.
5. All the HTML tags have been removed.
6. The hashtags8 have been normalized with
the token ? HASHTAG ?.
7. Tokens that express laughing (hahaha,
hehehe...) have been normalized with the
token ? LAUGH ?.
8. Users usually write expressions or abbrevi-
ations for surprise phrases like omg. All
these kind of expressions are replaced by the
token ? SURPRISE ?.
9. Positive emoticons like :), ;) or :, have been
normalized with the token ? HAPPY ?.
10. Negative emoticons like :(, :?( or :-( have
been normalized with the token ? SAD ?.
11. Twitter users usually repeat letters to em-
phasize the idea that they want to express.
Therefore, all the words with a letter re-
peated more than two times have been re-
duced to only two instances. For exam-
ple, the word ?aaaamaaaaaziiiing? in tweet
111733236627025920 is transformed into
?aamaaziing?.
After applying a normalization process to the
training and development sets, we have used for
the constrained system and the unsconstrained
system a dataset of 9,686 tweets.
3 Constrained System
The guidelines of the task define a constrained
system as a system that only can use the train
data provided by the organizers. Due to this re-
striction we decided to follow a supervised ap-
proach. It is required to define a set of parame-
ters when the supervised method is the elected.
The first step is to choose the minimum unit of
information, i.e. what segments of text are con-
sidered as features. Pang et al (2002) assert that
7A twitter mention is a reference to another user which
has the pattern ?@user name?
8A hashtag is the way to refer a topic in Twitter, which
has the pattern ?#topic name?
403
Class Precision Recall F1-score
Positive 0.6983 0.6295 0.6621
Neutral 0.6591 0.8155 0.7290
Negative 0.5592 0.2710 0.3651
Average 0.6652
Table 1: Assessment with TF-IDF weighting scheme
opinions or reviews should be represented with
unigrams, but other work shows bigrams and tri-
grams outperformed the unigrams features (Dave
et al, 2003). Therefore, there is not agreement
in the SA research community about what is the
best choice, unigrams or n-grams. Before several
validations on the training set of the task we de-
cided to use unigrams as feature for the polarity
classification process. Thus, for the supervised
algorithm, we have represented each tweet as a
vector of unigrams.
The next decision was about the application
of a stemmer process and getting rid off the En-
glish stop words. We only have applied stemmer
process to the data because in previous works
(Mart??nez-Ca?mara et al, 2013a) we did not reach
good results removing the stop words in texts
from Twitter. Another topic of discussion in the
SA research community is the weighting scheme.
Pang et al (2002) weighted each unigram fol-
lowing a binary scheme. Also, in the most cited
survey about SA (Pang and Lee, 2008) the au-
thors indicated that the overall sentiment may not
usually be highlighted through repeated use of
the same terms. On the other hand, Mart??nez-
Ca?mara et al (2011) achieved the best results
using TF-IDF as weighting scheme. Due to the
lack of agreement in the SA research community
about the use of a specific weight scheme, we
have carried out several assessments with aim of
deciding the most suitable one for the task. The
machine learning algorithm selected for the eval-
uation was SVM. The results are shown in Tables
1 and 2.
The results achieved with the two weighting
schemes are very similar. Regarding the posi-
tive class, the binary weighting scheme obtains
better results than the TF-IDF one, so the pres-
ence of positive keywords is more useful than
Class Precision Recall F1-score
positive 0.7037 0.6335 0.6668
neutral 0.6506 0.8313 0.7299
negative 0.5890 0.2105 0.3112
Average 0.6654
Table 2: Assessment with a binary weighting scheme
the frequent occurrence of those keywords. For
the neutral class, regarding precision and F1-
score, the TF-IDF scheme outperformed the bi-
nary scheme, but the recall had a higher value
when the terms are weighted binary. The pre-
cision of the classification for the neutral class
is only 1.2% better than the case where TF-IDF
is used, while recall and the F1-score is better
when the weighting of the features is binary. Al-
though the negative class has a similar perfor-
mance to that of the positive one with the two
weighting schemes, we highlighted the high dif-
ference between the other two classes and the
negative. The difference is more evident in the
recall value, while the neutral class has a value
of 0.8313 (binary), the negative one has a value
of 0.2105 (binary). Therefore, due to the fact that
the binary weighting scheme achieved better re-
sults in average, we decided to use it in the final
system.
The last step in the configuration of a su-
pervised approach based on machine learning is
the selection of the algorithm. The algorithm
selected was Support Vector Machine (SVM)
(Cortes and Vapnik, 1995). Our decision is based
on the widely used SVM by the research com-
munity of SA. The first application of SVM for
SA was in (Pang et al, 2002) with good re-
sults. Since the publication of the previous work,
other researchers have used SVM, and some of
them are: (Zhang et al, 2009), (Pang and Lee,
2004) and (Jindal and Liu, 2006). Also, the al-
gorithm SVM has been used to classify the po-
larity over tweets (Go et al, 2009) (Zhang et al,
2011) (Jiang et al, 2011). A broader review of
the research about SA in Twitter can be found in
(Mart??nez-Ca?mara et al, 2013b). Furthermore,
our decision is supported by previous in-house
experimentation.
404
For the experimentation we have used the
framework for data mining RapidMiner9. In
RapidMiner there are several implementations
of SVM, among which we have selected Lib-
SVM10(Chang and Lin, 2011) with built-in de-
fault parametrization.
To sum up, the configuration of the SINAI
constrained system is:
1. Machine learning approach: Supervised
2. Features: Unigrams.
3. Weighted scheme: Binary. If the term is
presence the value is 1, 0 in other case.
4. Stemmer: Yes
5. Stopper: No
6. Algorithm: SVM.
The results reached during the development
period are shown in Table 2
4 Unconstrained System
Our unconstrained system follows a two level
categorization approach, determining whether
the tweet is subjective or not at a first stage, and,
for the subjective classified ones, whether the
tweet is positive or negative. Both classification
phases are fully based on knowledge resources.
A predefined list of affective words is used for
subjectivity detection, and a search process over
the collection of emotions generated from a web
resource is applied for final polarity classifica-
tion. Figure 1 shows a general diagram of the
system.
4.1 Step 1: determining subjectivity
The system based in WeFeelFine only catego-
rizes between positive and negative texts, so a
preliminary classification into subjective and ob-
jective (i.e. neutral) must be performed. To this
end, a lexical approach is followed: those tweets
containing at least one affective term from a list
of predefined ones are considered subjective. If
9http://rapid-i.com/
10http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
Figure 1: Unconstrained system general diagram
affective terms are not found, then the tweet is
directly labeled as neutral. This list is called Sen-
timent Analysis Lexicon (SAL), which is defined
in the work of Bing Liu (Hu and Liu, 2004). The
list has two differentiated groups: a list of posi-
tive terms (agile, enjoy, improving) and another
with negative ones (anger, refusing, unable...).
At this phase, the polarity is not considered, so
both lists are merged into a list of around 6,800
subjectivity terms.
4.2 Step 2: determining polarity
The WeFeelFine project (Kamvar and Harris,
2011) has been used as knowledge base for po-
larity classification following the approach pro-
posed by (Montejo-Ra?ez, 2013). WeFeelFine11
gathers affective texts from several blogs, cre-
ating a huge database of mood-related expres-
sions. Almost two millions ?feelings? are col-
lected and indexed by the system. It is possible
to retrieve related sentences and expressions by
using its API. In this way, we have obtained the
11http://wefeelfine.org
405
top 200 most frequent feelings. For each feeling,
about 1,500 sentences are include in a document
that represents such a feeling. Then, using the
Lucene12 search engine, these documents have
been indexed. In this way, we can use an incom-
ing tweet as query and retrieve a ranked list of
feelings, as shown in Figure 2.
Figure 2: Polarity classification
The ranked list with the top 100 feelings (i.e.
those feelings more related to the tweet) is taken
for computing the final polarity by a summation
of the manually assigned polarity of the feeling
weighted with the score value returned by the en-
gine, as shown in Equation 1.
p(t) = 1
|R|
?
r?R
RSVr ? lr (1)
where
p(t) is the polarity of tweet t
R is the list of retrieved feelings
lr is the polarity label of feeling r
RSVr is the Ranking Status Value of the feel-
ing determined by Lucene.
As we did with the constrained system, we
also assess the unconstrained system before ap-
plying the test data. The results reached during
the evaluation phase are shown in Table 3. It is
remarkable the fact that the precision value of the
unconstrained system is a bit higher than the one
12http://lucene.apache.org/
Class Precision Recall F1-score
positive 0.5004 0.6341 0.5593
neutral 0.6772 0.5416 0.6018
negative 0.3580 0.3456 0.3516
Average 0.5094
Table 3: Assessment of the unconstrained system
reached by the constrained configuration. Thus,
SAL is a good resource for subjective classifi-
cation tasks. The unconstrained system reached
worse results with positive and negative classes,
but it is an expected result because supervised
approaches usually obtain better results than the
unsupervised and knowledge based approaches.
However, the polarity classification has reached
acceptable results, so it encourage us to follow
improving the method based of the use of We-
FeelFine.
Acknowledgments
This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
(FEDER), TEXT-COOL 2.0 project (TIN2009-
13391-C04-02) and ATTOS project (TIN2012-
38536-C03-0) from the Spanish Government.
Also, this paper is partially funded by the Eu-
ropean Commission under the Seventh (FP7
- 2007-2013) Framework Programme for Re-
search and Technological Development through
the FIRST project (FP7-287607). This publica-
tion reflects the views only of the authors, and
the Commission cannot be held responsible for
any use which may be made of the information
contained therein.
References
Enrique Amigo?, Adolfo Corujo, Julio Gonzalo, Edgar
Meij, and Md Rijke. 2012. Overview of replab
2012: Evaluating online reputation management
systems. In CLEF 2012 Labs and Workshop Note-
book Papers.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20:273?297.
406
Kushal Dave, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: opinion
extraction and semantic classification of product
reviews. In Proceedings of the 12th international
conference on World Wide Web, WWW ?03, pages
519?528, New York, NY, USA. ACM.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervi-
sion. CS224N Project Report, Stanford, pages 1?
12.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
tenth ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ?04,
pages 168?177, New York, NY, USA. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ?11, pages 151?160, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Nitin Jindal and Bing Liu. 2006. Identifying com-
parative sentences in text documents. In Proceed-
ings of the 29th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?06, pages 244?251, New
York, NY, USA. ACM.
Sepandar D. Kamvar and Jonathan Harris. 2011. We
feel fine and searching the emotional web. In Pro-
ceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM ?11,
pages 117?126, New York, NY, USA. ACM.
Eugenio Mart??nez-Ca?mara, M. Teresa Mart??n-
Valdivia, Jose? M. Perea-Ortega, and L. Al-
fonso Ure na Lo?pez. 2011. Opinion classification
techniques applied to a spanish corpus. Proce-
samiento de Lenguaje Natural, 47.
Eugenio Mart??nez-Ca?mara, M. Teresa Mart??n-
Valdivia, L. Alfonso Ure na Lo?pez, and Ruslan
Mitkov. 2013a. Detecting sentiment polarity in
spanish tweets. Information Systems Management,
In Press.
Eugenio Mart??nez-Ca?mara, M. Teresa Mart??n-
Valdivia, L. Alfonso Ure na Lo?pez, and Arturo
Montejo-Ra?ez. 2013b. Sentiment analysis
in twitter. Natural Language Engineering,
FirstView:1?28, 2.
Arturo Montejo-Ra?ez. 2013. Wefeelfine as resource
for unsupervised polarity classification. Proce-
samiento del Lenguaje Natural, 50:29?35.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification us-
ing machine learning techniques. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 79?86, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez-
Cristo?bal. 2013. Tass - workshop on sentiment
analysis at sepln. Procesamiento del Lenguaje
Natural, 50(0).
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. SemEval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ?13, June.
Changli Zhang, Daniel Zeng, Jiexun Li, Fei-Yue
Wang, and Wanli Zuo. 2009. Sentiment analy-
sis of chinese documents: From sentence to docu-
ment level. Journal of the American Society for In-
formation Science and Technology, 60(12):2474?
2487.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil,
Meichun Hsu, and Bing Liu. 2011. Combining
lexiconbased and learning-based methods for twit-
ter sentiment analysis. HP Laboratories, Technical
Report HPL-2011-89.
407
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 566?571,
Dublin, Ireland, August 23-24, 2014.
SINAI: Voting System for Aspect Based Sentiment Analysis
Salud Mar??a Jim
?
enez-Zafra, Eugenio Mart??nez-C
?
amara,
M. Teresa Mart??n-Valdivia, L. Alfonso Ure
?
na-L
?
opez
SINAI Research Group
University of Ja?en
E-23071, Ja?en (Spain)
{sjzafra, emcamara, maite, laurena}@ujaen.es
Abstract
This paper describes the participation of
the SINAI research group in Task 4 of the
2014 edition of the International Work-
shop SemEval. This task is concerned
with Aspect Based Sentiment Analysis
and its goal is to identify the aspects of
given target entities and the sentiment ex-
pressed towards each aspect.
1 Introduction
The web has evolved progressively since its be-
ginning in 1990. At first, the user was almost a
passive subject who received the information or
published it, without many possibilities to gener-
ate an interaction. The emergence of the Web 2.0
was a social revolution, because it offered users
the possibility of producing and sharing contents,
opinions, experiences, etc.
Some years ago it was common to ask family
and friends to know their opinion about a particu-
lar topic, but after the emergence of the Web 2.0,
the number of Internet users has been greatly in-
creased. The exponential growth of the subjective
information in the last years has created a great in-
terest in the treatment of this information.
Opinion Mining (OM), also known as Senti-
ment Analysis (SA) is the discipline that focuses
on the computational treatment of opinion, sen-
timent and subjectivity in texts (Pang and Lee,
2008). Currently, OM is a trendy task in the field
of Natural Language Processing due mainly to the
fact of the growing interest in the knowledge of
the opinion of people from different sectors of the
society. However, the study on Opinion Mining
goes back to 2002 when two of the most cited arti-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
cles in this task were published (Pang et al., 2002)
(Turney, 2002).
OM or SA can be divided into two subtasks
that are known as subjectivity classification and
polarity classification. Subjectivity classification
is the task concentrated on the identification of
subjectivity in texts, that is, these systems are bi-
nary classifiers that separate the documents in two
classes, objective and subjective ones. On the
other hand, polarity classification is the task of de-
termining the semantic orientation of a subjective
text. The ideal OM system has to be composed
by a subjectivity classifier and a polarity classifier.
However, most of the works in the field of OM are
carried out considering the documents as subjec-
tive, so polarity classification systems have been
more studied than subjectivity classification ones.
The reader can find a complete overview about the
research in OM in (Pang and Lee, 2008) and (Liu,
2012).
As Liu asserts in (Liu, 2012), the polarity clas-
sification systems can be divided into three levels:
? Document level polarity classification:
This kind of systems assumes that each doc-
ument expresses an opinion on a single entity
(Pang et al., 2002) (Turney, 2002).
? Sentence level polarity classification: In
this case the polarity classification systems
are focused on the identification of the level
of polarity of each sentence of the docu-
ment (Wilson et al., 2005) (Yu and Hatzivas-
siloglou, 2003).
? Entity and Aspect level polarity classifi-
cation: These systems accomplish a finer-
grained sentiment classification. Whereas the
document-level and sentiment-level only dis-
cover the overall sentiment expressed by the
author, the goal of the entity and aspect po-
larity classification is the identification of the
566
sentiment of the author towards each entity or
aspect.
An entity usually is composed by several as-
pects, for example a telephone is formed by a
headset, which also consists of a speaker and an
earphone. An entity can be regarded as a hierarchy
of all the aspects whose head is the entity, so the
entity can also be considered as an aspect or gen-
eral aspect. Therefore, the task ?entity and aspect
level polarity classification? can be called ?aspect
polarity classification?.
The main objective of OM at aspect level is to
discover every quintuple (e
i
, a
ij
, s
ijkl
, h
k
, t
l
) in
a given document, where e
i
is the entity, a
ij
is
one of the aspects of the entity or the entity and
s
ijkl
is the orientation of the opinion expressed
by the opinion holder h
k
in a certain moment t
l
.
To achieve the objective of populate the quintu-
ple is needed the splitting of the task into several
subtasks that correspond with the identification of
the aspect, the author or the holder of the opinion
and the moment when the opinion is expressed or
posted. But in a real scenario, OM at aspect level
is also limited like OM at sentence and document
level, and most of the research works are only fo-
cused on the identification of the aspect and in the
calculation of the level of intensity of the senti-
ment stated about the aspect. However, there are
some papers that are closely to the goal of finding
out each of the components of the quintuple (Kim
and Hovy, 2004) (Kim and Hovy, 2006).
The task four of the 2014 edition of SemEval
workshop aims to promote the research polarity
classification systems at aspect level. The task is
divided into four subtasks, two of them related to
the aspect identification and the other with the po-
larity classification. Due to the fact that OM is a
domain-dependent task, the organization proposes
the four subtasks in two different domains, Restau-
rants and Laptops. Task one and three are the
ones linked to the aspect identification. Subtask
one is focused on the identification of the aspects
in each review of the two given corpus. Subtask
three goes one step further, in which the main ob-
jective is for a given predefined set of aspect cate-
gories, identify the aspect categories discussed in
the given sentence. Subtask two proposes the clas-
sification of the sentiment expressed by the author
about each of the aspects extracted, and subtask
four has as challenge the classification of the po-
larity of each of the categories of the aspects. A
wider description of the task and the datasets used
can be found in the task description paper (Pontiki
et al., 2014).
The rest of the paper is organized as follows.
Section two outlines the two main parts of our pro-
posed system, firstly the strategy to solve the sub-
task 1 and 2 and then the method used to resolve
the subtask 3 and 4. To sum up the paper, an anal-
ysis of the results and the conclusion of this work
are shown in section three and four respectively.
2 System description
The guidelines of this task indicate that each team
may submit two runs: constrained (using only the
provided training data and other resources, such as
lexicons) and unconstrained (using additional data
for training). We decided to follow an unsuper-
vised approach that we present below.
Our system is divided into two subsystems (Fig-
ure 1). The aim of the first subsystem is to extract
the aspect terms related to a given target entity
(subtask 1) and calculate the sentiment expressed
towards each aspect in the opinion (subtask 2).
The goal of the second is, for a given set of cat-
egories, to identify the categories discussed in the
review (subtask 3) and determine its polarity (sub-
task 4).
2.1 Subsystem 1: Aspects Identification and
Polarity Classification
To identify the aspects related with the target en-
tity (laptops or restaurants) we decided to use
a bag of words built from all the aspect terms
present in the training data. But this method
only detects previously tagged aspect in the train-
ing data, so, we enriched the list of words with
data automatically extracted from the collabora-
tive knowledge base Freebase
1
, in order to im-
prove the identification. For this, we obtained
all categories in restaurants domain and in com-
puters domain
2
(types in a domain) using MQL
3
(Metaweb Query Language) (Figure 2).
Then, for each domain category we extracted all
terms (instances of a type) to enrich the bag. In
Figure 3 we can see an example to get all terms of
1
http://www.freebase.com/
2
Nowadays, Freebase has more than 70 different domains.
But, for this task, we are only interested in these two.
3
MQL is a language which is used to express Metaweb
queries. This allows you to incorporate knowledge from the
Freebase database into your own applications and websites.
567
Figure 1: Arquitecture of the system.
Figure 2: Query for list all categories in food do-
main.
a category, in particular cheese category of food
domain.
Figure 3: Query for list all term in cheese category.
In this way, given a review of the test data, the
first step is to tokenize it to get a vector of uni-
grams with all single words in the text (we do not
divide the reviews into sentences because there is
only one sentence per review). The second step
is to represent each review as a list of n lists of
unigrams, bigrams, . . . , n-grams where n is the
number of tokens in the sentence. This is because
an aspect term can be a nominal phrase, a word
formed from a verb but functioning as a different
part of speech (e.g. gerunds and participles) or a
simple term. For example, the review ?The salad
was excellent as was the lamb chettinad? is repre-
sented as shown in Figure 4.
After obtaining the possible terms of a review,
the next step is to go over the list of lists to ex-
tract the aspects. Each list is traversed backwards
matching each term with each aspect from the bag.
When an aspect is found or the top of the list is
reached the search begins in the next list. In the
review showed in Figure 4, the system will iden-
tify two aspects: salad and lamb chettinad. The
search in this example begins in the list 1 with
?The salad was excellent as was the lamb chetti-
nad?, ends with ?The? and continues with the next
list, because the top of the list is reached. The
search in the list 2 begins with ?salad was excel-
lent as was the lamb chettinad?, ends with ?salad?
because it is an aspect and continues with the list
3 and so on. At last, the search in the list 8 be-
gins with the term ?lamb chettinad?, ends with it
because it is an aspect presents in the bag of words
and continues with the list 9.
Once extracted the aspects related with the tar-
get entity, the next step is to determine the words
that modify each aspect. For this, we have used
the Stanford Dependencies Parser
4
. This parser
4
http://nlp.stanford.edu/software/lex-parser.shtml
568
Figure 4: Possible terms of the sentence ?The salad was excellent as was the lamb chettinad?.
was designed to provide a simple description of
the grammatical relationships in a sentence that
can easily be understood and effectively used by
people without linguistic expertise who want to
extract textual relations (De Marneffe and Man-
ning, 2008). It represents all sentence relation-
ships uniformly as typed dependency relations. In
this work, we have considered the main relation-
ships for expressing opinion about an aspect: us-
ing a verb (?nsubj? or ?nsubjpass?), an adjectival
modifier (?amod?) or a dependency relation with
another word (?dep?). In the review ?The salad
was excellent as was the lamb chettinad?, the sys-
tem will identify two modifiers words: the ad-
jective excellent that expresses how is the salad
through the relationship ?nsubj? and the adjective
excellent that also modified the aspect lamb chet-
tinad through the relationship ?dep? Figure 5.
To determine the sentiment expressed over an
aspect we have calculated the polarity of each
word that modifies it through a voting system
based on three classifiers: Bing Liu Lexicon (Hu
and Liu, 2004), SentiWordNet (Baccianella et al.,
2010) and MPQA (Wilson et al., 2005). The Bing
Liu Lexicon is a list of 2006 positive words and
another with 4783 negative ones. MPQA is also
a subjectivity lexicon with positive and negative
words and has extra information about each one:
the part-of-speech, the strength, etc. Finally, Sen-
tiWordNet is a lexical resource that assigns to each
synset of WordNet three sentiment scores: positiv-
ity, negativity and objectivity. Therefore, an aspect
is positive/negative if there are at least two clas-
sifiers that tag it as positive/negative and neutral
in another case. It may happen that a word is af-
fected by negation, to treat this problem we have
used a straightforward method, the fixed window
size method. We have considered the negative par-
ticles: ?not?, ?n?t?, ?no?, ?never?. So if any of the
preceding or following 3 words to one aspect is
one of these negative particles, the aspect polarity
is reversed (positive ?> negative, negative ?>
positive, neutral ?> neutral).
In the example showed in Figure 5, the aspect
salad is modified by the word excellent that also
modified the aspect lamb chettinad. This adjective
is part of the Bing Liu positive list, MPQA classi-
fies it as positive and SentiWordNet assigns it the
scores: 1 (positivity), 0(objectivity), 0 (objectiv-
ity). Then, the aspects salad and lamb chettinad
are classified as positive by the voting system.
Figure 5: Dependency analysis of the sentence:
?The salad was excellent as was the lamb chetti-
nad?.
569
2.2 Subsystem 2: Categories Identification
and Polarity Classification
As we have mentioned above, this subsystem fo-
cuses on the treatment of the categories and has
been used only with the dataset of restaurants.
On the one hand, we have built a bag of words
for each of the given categories related to the tar-
get entity (restaurants). We have tagged manu-
ally each aspect of the bag of words, built for the
first subsystem, in one of the categories of the
given set (food, service, price, ambience, anec-
dotes/miscellaneous). Thus, to determine the cat-
egories that are referenced in a review we have
searched each aspect identified with the first sub-
system in each bag, if the aspect belongs to any
category then this category is identified. If any as-
pect belongs to a category, then the category allo-
cated is ?anecdotes/miscellaneous?.
On the other hand, the sentiment expressed
about each category has been calculated as the
most frequent polarity of the aspects that belongs
to this category. In case of a tie between positive
and negative values, the polarity value conflict is
assigned to the category. If any aspect belongs to
the category, then the polarity value of the review
is assigned to the category.
In the above example, the aspects salad and
lamb chettinad belong to food?s bag of words, so
that the system will identify that the category food
is discussed in this review and will assign it the
polarity value positive, because the sentiment ex-
pressed about the two aspects that belongs to this
category is positive.
3 Analysis of the results
The aim of this section is to provide a meaningful
report of the results obtained after participation in
the task related to Aspect Based Sentiment Anal-
ysis (ABSA). Table 1 shows the evaluation results
for the aspect extraction subtask. As we can see,
the recall overcomes the mean value of results of
participants in both domains (laptops and restau-
rants), that is, the system identifies quite aspects
of the corpus. However, the precision is lower
because the system identifies aspects that are not
considered by the organization, due to the fact that
our bag of words contains more aspects than the
tagged by the organization.
The results reached in the aspect term extraction
subtask are similar (Table 2). It should be taken
into account that the system is a general-domain
Laptops Restaurants
SINAI Average SINAI Average
Precision 0.3729 0.6890 0.5961 0.7674
Recall 0.5765 0.5045 0.72487 0.6726
F-score 0.4529 0.5620 0.6542 0.7078
Table 1: Aspect Term Extraction results.
sentiment classifier, so it does not use specific
knowledge for each of the domains. This fact can
be shown in the results reached in the task of po-
larity classification for the two domains, which are
similar. Therefore, this subtask could be improved
by taking into account the domain and other rela-
tionships for expressing opinion about an aspect
apart from that we have treated (?nsubj?, ?nsubj-
pass?, ?amod?, ?dep?).
Laptops Restaurants
SINAI Average SINAI Average
Accuracy 0.5872 0.5925 0.5873 0.6910
Table 2: Aspect Term Polarity results.
On the other hand, the results in the identifica-
tion of the categories discussed in a review have
been high (Table 3) and even overcome the aver-
age recall of the participating systems. At last, Ta-
ble 4 shows the result evaluation of the aspect cat-
egory polarity subtask that are slightly lower than
the average. These tables show that is possible to
reach good results using a simple approach as de-
scribed in subsection 2.2.
Restaurants
SINAI Average
Precision 0.6659 0.76
Recall 0.8244 0.7226
F-score 0.7367 0.7379
Table 3: Aspect Category Detection results.
4 Conclusion and future works
In SA can be differentiated three levels of study of
a text: document level, sentence level and aspect
level. The document level analysis determines the
overall sentiment expressed in a review, while the
sentence level analysis specifies for each sentence
of a text, whether express a positive, negative or
neutral opinion. However, these two types of anal-
570
Restaurants
SINAI Average
Accuracy 0.6030 0.6951
Table 4: Aspect Category Polarity results.
ysis do not reach the level of detail that an user
wants when searches for information about a prod-
uct. The fact that the overall sentiment of a prod-
uct is positive does not mean that the author has a
positive opinion about all aspects of that product,
or the fact that is negative does not involve that
everything about the product is bad.
In addition, the large amount of sources and
the high volume of texts with reviews, make dif-
ficult for the user to select information of interest.
Therefore, it is necessary to develop classification
systems at aspect level that help users to make de-
cisions and, on the other hand, that show compa-
nies the opinion that consumers have about their
products, in order to help them to decide what to
keep, what to delete and what to improve.
In this paper we have presented our first ap-
proach for the Aspect Based Sentiment Analysis
that has been developed for the task four of the
2014 edition of SemEval workshop. After analyz-
ing the evaluation results we consider that is pos-
sible to introduce some improvements we are cur-
rently working: domain adaptation in the polarity
calculation, consideration of other relationships
to determine which words modify an aspect and
treatment of negation (in the system proposed we
have used the fixed window size method). Also, in
a near future we will try to extrapolate it to Span-
ish reviews.
Acknowledgments
This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
(FEDER), ATTOS project (TIN2012-38536-C03-
0) from the Spanish Government, AORESCU
project (P11-TIC-7684 MO) from the regional
government of Junta de Andaluc??a and CEATIC-
2013-01 project from the University of Ja?en.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200?2204.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. Stanford typed dependencies manual.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th International Conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, SST ?06,
pages 1?8, Stroudsburg, PA, USA.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natu-
ral Language Processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 417?424, Stroudsburg, PA,
USA.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?03, pages 129?136, Strouds-
burg, PA, USA.
571
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 572?577,
Dublin, Ireland, August 23-24, 2014.
SINAI: Voting System for Twitter Sentiment Analysis
Eugenio Mart
?
?nez-C
?
amara, Salud Mar
?
?a Jim
?
enez-Zafra,
M. Teresa Mart
?
?n-Valdivia, L. Alfonso Ure
?
na-L
?
opez
SINAI Research Group
University of Jae?n
E-23071, Jae?n (Spain)
{emcamara, sjzafra, maite, laurena}@ujaen.es
Abstract
This article presents the participation of
the SINAI research group in the task Sen-
timent Analysis in Twitter of the SemEval
Workshop. Our proposal consists of a
voting system of three polarity classifiers
which follow a lexicon-based approach.
1 Introduction
Opinion Mining (OM) or Sentiment Analysis (SA)
is the task focuses on the computational treatment
of opinion, sentiment and subjectivity in texts
(Pang and Lee, 2008). Currently, OM is a trendy
task in the field of Natural Language Processing
due mainly to the fact of the growing interest in
the knowledge of the opinion of people from dif-
ferent sectors of the society.
The interest in the research community for the
extraction of the sentiment in Twitter posts is re-
flected in the organization of several workshops
with the aim of promoting the research in this task.
Two are the most relevant, the first is the task
Sentiment Analysis in Twitter celebrated within
the SemEval workshop whose first edition was in
2013 (Nakov et al., 2013). The second is the work-
shop TASS
1
, which is a workshop for promot-
ing the research in sentiment analysis in Spanish
in Twitter. The first edition of the workshop took
place in 2012 (Villena-Roma?n et al., 2013).
The 2014 edition of the task Sentiment Analy-
sis in Twitter proposes a first subtask, which has
as challenge the sentiment classification at entity
level, and a second subtask that consists of the
polarity classification at document or tweet level.
The training corpus is the same than the former
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://www.daedalus.es/TASS
edition, but this year the test corpus is consider-
ably bigger than the prior one. A wider description
of the task and the corpus can be read in (Rosen-
thal et al., 2014).
We present an unsupervised polarity classifica-
tion system for the subtask B of the task Senti-
ment Analysis in Twitter. The system is based
on a voting strategy of three lexicon-based senti-
ment classifiers. The sentiment analysis research
community broadly knows the lexicons selected.
They are, SentiWordNet (Baccianella et al., 2010),
the lexicon developed by Hu and Liu (Hu and
Liu, 2004) and the MPQA lexicon (Wilson et al.,
2005).
The rest of the paper is organized as follows.
The following section focuses on the description
of the different sentiment resources used for de-
veloping the sentiment classifiers. The subsequent
section outlines the system proposed for the 2014
edition of the task. The last section exposes the
analysis of the results reached this year.
2 Sentiment lexical resources
Sentiment lexicons are lexical resources com-
posed of opinion-bearing words and some of them
also of sentiment phrases of idioms. Most of the
sentiment lexicons are formed by a list of words
without any additional information.
A sentiment classifier based on list of opinion-
bearing words usually consists of finding out the
words of the list in a given document. This method
can be considered very simple for the complexity
of OM, but it has reached acceptable results in dif-
ferent domains and also is applied in real systems
like Trayt.com
2
.
Our experience in the field of SA allows us to
assert that sentiment lexicons can be divided de-
pending on the information linked to each word,
2
Trayt.com is a search engine of reviews of restaurants.
The polarity classifier of Tryt.com is a lexicon-based system
which uses the opinion list compiled by Bing Liu.
572
so three groups can be found:
? List of opinion-bearing words: These lexi-
cons are usually two lists of polar words, one
of them of positive words and another one
of negative terms. Some examples of this
kind of sentiment lexicons are for English the
one compiled by (Hu and Liu, 2004), and for
Spanish, the iSOL lexicon (Molina-Gonza?lez
et al., 2013).
? List of opinion-bearing words with syntactic
information: As it is wider known, OM is a
domain-dependent task and can be also said
that a context-dependent task. Thus, some
lexicons add syntactic information with the
aim of offering some information for disam-
biguating the term, and also provide a differ-
ent orientation of the word depending on its
POS-tags. One example of this kind of lexi-
con is MPQA subjectivity lexicon (Wilson et
al., 2005).
? Knowledge base sentiment lexicons: These
lexicons usually indicate the semantic orien-
tation of the different senses of each word,
whereas the previous lexicons only indicate
the polarity of each word. Also, it is very
common that in the knowledge base senti-
ment lexicons each sense is linked to the like-
lihood of being positive, negative and neutral.
One example of this kind of polar lexicon is
SentiWordNet (Baccianella et al., 2010).
In the polarity classifier developed for the work-
shop a lexicon of each type has been utilised. The
sentiment linguistic resources used has been:
? Sentiment lexicon compiled by Bing Liu:
The lexicon was used the first time in (Hu
and Liu, 2004). Since then, the authors have
been updating the list, and currently the list is
formed by 2006 positive words and 4783 neg-
ative words. Also, the lexicon includes some
misspellings with the aim of better represent-
ing the language used in the Internet.
? MPQA Subjectivity lexicon (Wilson et al.,
2005): The lexicon is formed by over 8000
subjectivity clues. Subjectivity clues are
words and phrases that have subjective us-
ages. The lexicon was developed joining
words compiled by the authors and with
words taken from General Inquirer. Each
Figure 1: Architecture of the system.
word is linked with its grade of subjectivity,
with its part of speech tag and with its seman-
tic orientation. Due to the fact that each word
has its POS-tag there are some words that de-
pending on its POS have a different semantic
orientation.
? SentiWordNet 3.0 (Baccianella et al., 2010):
is a lexical resource which assigns three sen-
timent scores to each synset of WordNet:
positivity, negativity and objectivity.
3 Polarity classification
We wanted to take advantage from our experi-
ence in meta-classification in OM for the 2014
edition of the task, Sentiment Analysis in Twit-
ter. We have reached good results in OM us-
ing meta-classifiers in different domains (Perea-
Ortega et al., 2013) and (Mart??n-Valdivia et al.,
2013). Therefore, we propose a voting system that
combines three polarity classifiers. The general ar-
chitecture of the system is shown in Figure 1.
Tokenization is a common step of the three clas-
sifiers. Due to the specific characteristics of the
language used in Twitter, a specific tokenizer for
Twitter was preferred to use. The tokenizer pub-
lished by Christopher Potts
3
was selected and up-
dated, with the aim of recognizing a wider range
of tokens.
When the tweet is tokenized, the following step
is discover its polarity. Each of the three polarity
classifiers follows the same strategy for the clas-
sification, but they perform different operations
on each tweet. The classifier based on the lexi-
con compiled by Bing Liu (C BingL) consists of
seeking each token in the opinion-bearing words
3
http://sentiment.christopherpotts.net/tokenizing.html
573
list. Therefore, after the tokenization, any linguis-
tic operation has to be performed on the tweet.
This classifier classifies a tweet as positive if the
number of positive tokens is greater or equal than
the number of negative tokens. If there are not po-
lar tokens, the polarity of the tweet is neutral.
The second polarity classifier is the based on
MPQA lexicon (C MPQA). Some of the words
that are in the MPQA lexicon are lemmatized,
and also the sentiment depends on their POS-tag.
Thus, to take advantage of all the information of-
fered by MPQA is needed to perform a morpho-
logical analysis to each tweet. The morphological
analysis firstly identifies the POS-tag of each to-
ken of the tweet, and then the lemmatizer extracts
the lemma of the token.
Recently, some linguistic tools have been pub-
lished to carry out linguistic analysis in tweets.
Currently, two POS-taggers for Twitter are avail-
able. One of them, is the described in (Gimpel et
al., 2011) and the second one in (Derczynski et al.,
2013). Although the authors of the two systems
are competing for which of the two taggers are bet-
ter, our selection was based on the usability of the
two systems. To use the tagger developed by Gim-
pel et al. is needed to download their software,
meanwhile the one developed by Derczynski et al.
can be integrated in other taggers. On our point of
view, the tagger of Derczynski et al. has the ad-
vantage of offering the training model of the tag-
ger
4
, which allows us to integrate it in other POS-
tagging tools. The training model of the tagger
was integrated in the Stanford Part-of-Speech Tag-
ger
5
. When each token of the tweet is associated
with its corresponding POS-tag, the lemmatizer is
run over the tweet. The lemmatizer used is the of-
fered by the toolkit for Natural Language Process-
ing, NLTK (Bird et al., 2009). When each token
is accompanied by its corresponding POS-tag and
lemma, the polarity classifier can seek each token
in the MPQA subjective lexicon.
Besides the label of the polar class (positive or
negative), each entry in the MPQA corpus has a
field called type, which indicates whether the term
is considered strongly subjective or the term is
considered weakly subjective. Thus, in the calcu-
lation of the polarity score these two levels of sub-
jectivity are considered, so when the term is strong
subjective it is considered to have a score of 1, and
4
https://gate.ac.uk/wiki/twitter-postagger.html
5
http://nlp.stanford.edu/software/tagger.shtml
when the term is weak subjective the system con-
siders the term as less important and its score is
0.75.
The polarity classifier based on the use of Sen-
tiWordNet (C SWN) needs that each word of the
tweet is linked with its POS-tag and its lemma,
so the same pipeline that the classifier based on
MPQA follows is also followed by the classifier
based on SentiWordNet.
In the bibliography about OM can be found dif-
ferent ways to calculate the polarity class when
SentiWordNet is used as a sentiment knowledge
base. Some works perform a disambiguation
method with the aim of selecting only the synset
that corresponds with the sense of the word in
the context of the given document. But there are
other works that do not perform any disambigua-
tion method, and also reach good results. De-
necke in (Denecke, 2008) describes a very sim-
ple method to calculate the polarity of each of the
words of a document without the need of a dis-
ambiguation process. The method consists of cal-
culating per each word in the document, which is
in SentiWordNet, the arithmetic mean of the pos-
itive, negative and neutral score of each of the
synsets that the word has in SentiWordNet. When
the scores of each word are calculated, the score
of the document is determined as the arithmetic
mean of each score of the words. The class of the
document is corresponded with the greatest polar
score (positive, negative, neutral). Due to the ac-
ceptable results that the Denecke formula reaches,
we have introduced a soft disambiguation process
with the aim of improving the classification ac-
curacy. This soft disambiguation process consist
of only taking those synsets corresponding with
POS-tag of the word whose polarity are being cal-
culated. For example, the word ?good? can do the
function of an adverb, a noun or an adjective. In
SentiWordNet, there are two synsets of ?good? as
an adverb, four synset of ?good? as a noun, and
twenty-one synsets as an adjective. If the polar-
ity score is calculated with the Denecke formula,
the twenty-seven synsets are used. Meanwhile, if
it used our proposal, and the word ?good? in the
given sentence is acting as an adverb, then only
the two synsets of the word ?good? when it is ad-
verb are considered to calculate the polarity score.
During the development of the system, we no-
ticed that synsets have a lower probability to be
positive or negative, and most of them in Senti-
574
WordNet are neutral. With the aim of boosting the
likelihood to be positive or negative, the polarity
classifier does not consider the neutral score of the
synset. If the positive score is greater than the neg-
ative score and greater than 0.15 then the term is
positive. If the negative score is greater than the
positive score and greater than 0.15 then the word
is negative, in other case the word is neutral.
Each of the polarity classifiers take into consid-
eration the presence of emoticons, the expressions
of laughing and negation. The emoticons are pro-
cessed as words, so for determining their polarity
a sentiment lexicon of emoticons was built. The
polar lexicon of emoticons consists of fifty-eight
positive emoticons and forty-four negative ones.
Laughing expressions usually express a positive
sentiment, so when a laughing expression is de-
tected the counter of positive words is increased
by one. The strategy for negation identification
is a bit straightforward but effective. Due to the
specific linguistic characteristics of tweets, a strat-
egy based on windows of words has been imple-
mented. When a polar word is identified, it is
sought in the previous three words whether there
is a negative particle. In those cases that a nega-
tive particle is found, the polarity of the sentiment
word is reversed, that it to say if a positive (neg-
ative) word is negated the system considers it as
negative (positive).
The last step of the polarity classifier is the run-
ning of a voting system among the three polarity
classifiers. Three are the possible output values of
the three base classifiers {negative, neutral, pos-
itive}. When the majority class is positive, the
tweet is classified as positive, when the majority
class is negative then negative is the class assigned
to the tweet and when majority class is neutral or
there is not a majority class then the tweet is clas-
sified as neutral.
4 Analysis of the results
Before showing the results reached in the evalu-
ation of the task, the results accomplished in the
development phase of the system will be shown.
Three main systems were assessed during the de-
velopment phase:
? Baseline (BL): The three base classifiers
compose the baseline system, but the three
polarity scores of SentiWordNet are consid-
ered and negation is not taken into account.
? Neutral scores are not considered (NN): It is
the same than the Baseline system but the
neutral scores of SentiWordNet are not con-
sidered.
? Negation identification (NI): The neutral
scores of SentiWordNet are not taken into ac-
count and the negation is identified.
The results are show in Table 1.
Precision Recall F1 Accuracy Improve (Acc.)
BL 55.85% 52.02% 53.87% 60.32% -
NN 56.03% 52.27% 54.09% 60.46% 0.23%
NI 57.22% 53.41% 55.25% 61.12% 1.33%
Table 1: Results achieved during the developing
phase.
As can be seen in Table 1 the systems (NN) and
(NI) reach better results than the baseline, so all
the modifications to the baseline are good for the
polarity classification process. The results confirm
our hypothesis that the neutral score of the synsets
in SentiWordNet are not contributing positively
to the sentiment classification. Also, a straight-
forward strategy for identifying the scope of the
negation improves the accuracy of the classifica-
tion. The results help us to choose the final con-
figuration of the system. As is described in the
former section the final polarity classification sys-
tem follows a voting scheme of three base lexicon-
based polarity classifiers. The three base classi-
fiers take into consideration the presence of emoti-
cons, laughing expressions, identifies the scope of
negation, and the classifier based on SentiWord-
Net does not take into consideration the neutral
score of the synsets.
The edition 2014 of the task Sentiment Analysis
in Twitter has assessed the systems with five dif-
ferent corpus tests: LiveJournal2014, SMS2013,
Twitter2013, Twitter2014, Twitter2014Sarcasm.
The results reached with each of the test corpus
are shown in Table 2.
Some of the results shown in Table 2 are much
closed to the results reached during the develop-
ment phase, because all of the F1 scores are closed
to 55%. The lower results have been reached with
the corpus Twitter2014 and Twitter2014Sarcasm.
The poor results in Twitter2014Sarcasm are due to
the lack of a module in the system for the detection
of sarcasm. A sarcastic sentence is usually a sen-
tence with a sentiment that expresses the opposite
575
Precision Recall F1
LiveJournal2014
Positive 60.19% 76.95% 67.54%
Negative 36,51% 75,00% 49.12%
Neutral 82.48% 51.36% 63.31%
Overall ? ? 58.33%
SMS2013
Positive 63.01% 60.19% 61.57%
Negative 42.13% 71.86% 53.12%
Neutral 82.27% 73.72% 77,76%
Overall ? ? 57.34%
Twitter2013
Positive 60.56% 70.15% 65.01%
Negative 28.29% 50.15% 36.17%
Neutral 73.66% 57.06% 64.31%
Overall ? ? 50.59%
Twitter2014
Positive 57.13% 77.49% 65.77%
Negative 27.23% 42.64% 33.23%
Neutral 73.54% 49.20% 58.96%
Overall ? ? 49.50%
Twitter2014Sarcasm
Positive 57.58% 48.72% 52.78%
Negative 5.00% 100.00% 9.52%
Neutral 84.62% 24.44% 37.93%
Overall ? ? 31.15%
Table 2: Results reached with the test corpus.
sentiment, so a polarity classifier without a spe-
cific module to treat this linguistic phenomenon
will be probably misclassified the sarcastic sen-
tences. The results for Twitter2014Sarcasm for
the negative class indicate this problem. The low
value of the precision and the high value of the re-
call in the negative class mean that a high number
of negative sentences have been classified as posi-
tive.
The analysis of the results is completed with
the assessment of our method. We proceed from
the hypothesis that a combination of several clas-
sifiers will improve the final classification. Our
hypothesis is based on own previous publications,
(Perea-Ortega et al., 2013) and (Mart??n-Valdivia et
al., 2013). We have classified the test corpus with
each of the three base classifiers, with the aim of
knowing the performance of each one. The results
are shown in Table 3.
Table 3 shows that the classifier C BingL
reaches better results than the combination of
the three classifiers. The first conclusion we
draw from this fact is that the good perfor-
mance of meta-classifiers with large opinions is
not achieved with the short texts of Twitter. But,
this conclusion is preliminary, because the lower
results of the voting system may be due to a not
good combination of the three classifiers. So we
have to continue working in the analysis on how
to build a meta-classifier for OM in Twitter. The
rest of the classifiers reached lower results than the
voting system. Another reason that the voting sys-
tem achieved lower results than C BingL may be-
cause the three classifiers are not heterogeneous,
F1
C BingL C SWN C MPQA
LiveJournal2014
Positive 68.11% 42.62% 65.20%
Negative 55.43% 39.81% 49.60%
Neutral 64.03% 58.07% 58.43%
Overall 61.77% 41.21% 57.40%
SMS2013
Positive 61.67% 43.53% 53.56%
Negative 54.19% 28.79% 52.678%
Neutral 76.00% 75.85% 68.38%
Overall 57.93% 36.16% 53.12%
Twitter2013
Positive 68.30% 23.40% 62.37%
Negative 46.20% 11.60% 37.75%
Neutral 61.17% 62.11% 57.39%
Overall 57.25% 17.50% 50.06%
Twitter2014
Positive 69.33% 22.17% 66.74%
Negative 41.55% 9.79% 33.00%
Neutral 53.25% 55.63% 52.76%
Overall 55.44% 15.98% 49.87%
Twitter2014Sarcasm
Positive 56.10% 27.27% 52.06%
Negative 17.78% 9.52% 8.51%
Neutral 44.44% 30.24% 30.77%
Overall 36.94% 18.40% 30.28%
Table 3: Results reached by each base classifier
with the test corpus.
in other words, when one of the systems misclas-
sified a document the other ones classify it cor-
rectly, so the base classifiers help each other, and
the combination of systems reaches better results
than the individual systems. But, in our case may
be that the systems are not heterogeneous, so our
ongoing work is the study of the heterogeneity be-
tween the three classifiers.
If we focus only in the results achieved by
C BingL, it is remarkable that the higher differ-
ence is in the negative class. C BingL reaches
greater results than the voting system in negative
class, and it has the same negation treatment mod-
ule that the voting system. This fact allow us to say
that the low results in the negative class reached by
the voting system is not due to the negation treat-
ment module, and may because by the own com-
bination method.
To sum up, after analysing the results, we have
noticed that the same meta-classifier methodology
that we usually apply to large reviews cannot be
directly apply to tweets. Therefore, our ongoing
work is focused firstly on conducting a deep anal-
ysis of the results presented in this work, and sec-
ondly in the study on how to improve of polarity
classification in Twitter following a unsupervised
methodology, and thirdly on how to build a good
meta-classifier for OM in Twitter.
Acknowledgements
This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
576
(FEDER), ATTOS project (TIN2012-38536-C03-
0) from the Spanish Government, AORESCU
project (P11-TIC-7684 MO) from the regional
government of Junta de Andaluc??a and CEATIC-
2013-01 project from the University of Jae?n.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python. O?Reilly
Media, Inc.
Kerstin Denecke. 2008. Using SentiWordnet for mul-
tilingual sentiment analysis. In Data Engineering
Workshop, 2008. ICDEW 2008. IEEE 24th Interna-
tional Conference on, pages 507?512, April.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In
Proceedings of the International Conference Recent
Advances in Natural Language Processing RANLP
2013, pages 198?206, Hissar, Bulgaria, September.
INCOMA Ltd. Shoumen, BULGARIA.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
ACL: Human Language Technologies: Short Papers
- Volume 2, HLT ?11, pages 42?47, Stroudsburg, PA,
USA. ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Mar??a-Teresa Mart??n-Valdivia, Eugenio Mart??nez-
Ca?mara, Jose-M. Perea-Ortega, and L. Alfonso
Uren?a Lo?pez. 2013. Sentiment polarity detection in
Spanish reviews combining supervised and unsuper-
vised approaches. Expert Syst. Appl., 40(10):3934?
3942, August.
M. Dolores Molina-Gonza?lez, Eugenio Mart??nez-
Ca?mara, Maria Teresa Mart??n-Valdivia, and Jose? M.
Perea-Ortega. 2013. Semantic orientation for po-
larity classification in spanish reviews. Expert Syst.
Appl., 40(18):7250?7257.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. ACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135, January.
Jose? M. Perea-Ortega, M. Teresa Mart??n-Valdivia,
L. Alfonso Uren?a Lo?pez, and Eugenio Mart??nez-
Ca?mara. 2013. Improving polarity classification of
bilingual parallel corpora combining machine learn-
ing and semantic orientation approaches. Journal of
the American Society for Information Science and
Technology, 64(9):1864?1877.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez-
Cristo?bal. 2013. TASS - Workshop on sentiment
analysis at SEPLN. Procesamiento del Lenguaje
Natural, 50.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
ACL.
577
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 3?10,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Random Walk Weighting over SentiWordNet for
Sentiment Polarity Detection on Twitter
A. Montejo-Ra?ez, E. Mart??nez-Ca?mara, M. T. Mart??n-Valdivia, L. A. Uren?a-Lo?pez
University of Jae?n
E-23071, Jae?n (Spain)
{amontejo, emcamara, maite, laurena}@ujaen.es
Abstract
This paper presents a novel approach in Sen-
timent Polarity Detection on Twitter posts, by
extracting a vector of weighted nodes from the
graph of WordNet. These weights are used
on SentiWordNet to compute a final estima-
tion of the polarity. Therefore, the method
proposes a non-supervised solution that is
domain-independent. The evaluation over a
generated corpus of tweets shows that this
technique is promising.
1 Introduction
The birth of Web 2.0 supposed a breaking down of
the barrier between the consumers and producers of
information, i.e. the Web has changed from a static
container of information into a live environment in
which any user, in a very simple manner, can pub-
lish any type of information. This simplified means
of publication has led to the rise of several differ-
ent websites specialized in the publication of users
opinions. Some of the most well-known sites in-
clude Epinions1, RottenTomatoes2 and Muchocine3,
where users express their opinions or criticisms on a
wide range of topics. Opinions published on the In-
ternet are not limited to certain sites, but rather can
be found in a blog, forum, commercial website or
any other site allowing posts from visitors.
On of the most representative tools of the Web 2.0
are social networks, which allow millions of users
1http://epinions.com
2http://rottentomatoes.com
3http://muchocine.net
to publish any information in a simple way and to
share it with their network of contacts or ?friends?.
These social networks have also evolved and be-
come a continuous flow of information. A clear ex-
ample is the microblogging platform Twitter4. Twit-
ter publishes all kinds of information, disseminating
views on many different topics: politics, business,
economics and so on. Twitter users regularly pub-
lish their comments on a particular news item, a re-
cently purchased product or service, and ultimately
on everything that happens around them. This has
aroused the interest of the Natural Language Pro-
cessing (NLP) community, which has begun to study
the texts posted on Twitter, and more specifically re-
lated to Sentiment Analysis (SA) challenges.
In this manuscript we present a new approach to
resolve the scoring of posts according to the ex-
pressed positive or negative degree in the text. This
polarity detection problem is resolved by combin-
ing SentiWordNet scores with a random walk analy-
sis of the concepts found in the text over the Word-
Net graph. In order to validate our non-supervised
approach, several experiments have been performed
to analyze major issues in our method and to com-
pare it with other approaches like plain SentiWord-
Net scoring or machine learning solutions such as
Support Vector Machines in a supervised approach.
The paper is structured as follows: first, an introduc-
tion to the polarity detection problem is provided,
followed by the description of our approach. Then,
the experimental setup is given with a description of
the generated corpus and the results obtained. Fi-
nally, conclusions and further work are discussed.
4http://twitter.com
3
2 The polarity detection problem
In the literature related to the SA in long text a dis-
tinction is made between studies of texts where we
assume that the text is a opinion and therefore solely
need to calculate its polarity, and those in which be-
fore measuring polarity it is necessary to determine
whether the text is subjective or objective. A wide
study on SA can be found in (Pang and Lee, 2008),
(Liu, 2010) and (Tsytsarau and Palpanas, 2011).
Concerning the study of the polarity in Twitter, most
experiments assume that tweets5 are subjective. One
of the first studies on the classification of the polar-
ity in tweets was published in 2009 by (Go et al,
2009), in which the authors conducted a supervised
classification study of tweets in English.
Zhang et al (Zhang et al, 2011) proposed a hy-
brid method for the classification of the polarity in
Twitter, and they demonstrated the validity of their
method over an English corpus on Twitter. The clas-
sification is divided into two phases. The first one
consists on applying a lexicon-based method. In
the second one the authors used the SVM algorithm
to determine the polarity. For the machine learning
phase, it is needed a labelled corpus, so the purpose
of the lexicon-method is to tag the corpus. Thus, the
authors selected a set of subjective words from all
those available in English and added hash-tags with
a subjective meaning. After labelling the corpus, it
is used SVM for classifying new tweets.
In (Agarwal et al, 2011) a study was conducted
on a reduced corpus of tweets labelled manually.
The experiment tests different methods of polarity
classification and starts with a base case consisting
on the simple use of unigrams. Then a tree-based
model is generated. In a third step, several linguis-
tic features are extracted and finally a final model
learned as combination of the different models pro-
posed is computed. A common feature used both in
the tree-based model and in the feature-based one is
the polarity of the words appearing in each tweet. In
order to calculate this polarity the authors used DAL
dictionary (Whissell, 1989).
Most of the proposed systems for polarity detec-
tion compute a value of negativeness or positiveness.
Some of them even produce a neutrality value. We
will consider the following measurement of polar-
5The name of posts in Twitter.
ity (which is very common, indeed): a real value
in the interval [-1, 1] would be sufficient. Values
over zero would reflect a positive emotion expressed
in the tweet, while values below zero would rather
correspond to negative opinions. The closer to the
zero value a post is, the more its neutrality would
be. Therefore, a polarity detection system could be
represented as a function p on a text t such as:
p : RN ? R
so that p(t) ? [?1, 1]. We will define how to
compute this function, but before an explanation of
the techniques implied in such a computation is pro-
vided.
3 The approach: Random Walk and
SentiWordNet
3.1 The Random Walk algorithm
Personalized Page Rank vectors (PPVs) consists on
a ranked sequence of WordNet (Fellbaum, 1998)
synsets weighted according to a random walk algo-
rithm. Taking the graph of WordNet, where nodes
are synsets and axes are the different semantic re-
lations among them, and the terms contained in a
tweet, we can select those synsets that correspond to
the closest sense for each term and. Then, it starts
an iterative process so more nodes are selected if
they are not far from these ?seeds?. After a num-
ber of iterations or a convergence of the weights, a
final list of valued nodes can be retrieved. A simi-
lar approach has been used recently by (Ramage et
al., 2009) to compute text semantic similarity in rec-
ognizing textual entailment, and also as a solution
for word sense disambiguation (Agirre and Soroa,
2009). We have used the UKB software from this
last citation to generate the PPVs used in our system.
Random walk algorithms are inspired originally by
the Google PageRank algorithm (Page et al, 1999).
The idea behind it is to represent each tweet as a vec-
tor weighted synsets that are semantically close to
the terms included in the post. In some way, we are
expanding these sort texts by a set of disambiguated
concepts related to the terms included in the text.
As an example of a PPV,the text ?Overall, we?re
still having a hard time with it, mainly because we?re
not finding it in an early phase.? becomes the vector
of weighted synsets:
4
[02190088-a:0.0016, 12613907-n:0.0004,
01680996-a:0.0002, 00745831-a:0.0002, ...]
Here, the synset 02190088-a has a weight of
0.0016, for example.
3.2 SentiWordNet
SentiWordNet (Baccianella et al, 2008) is a lexi-
cal resource based on the well know WordNet (Fell-
baum, 1998). It provides additional information on
synsets related to sentiment orientation. A synset
is the basic item of information in WordNet and it
represents a ?concept? that is unambiguous. Most
of the relations over the lexical graph use synsets
as nodes (hyperonymy, synonymy, homonymy and
more). SentiWordNet returns from every synset a
set of three scores representing the notions of ?pos-
itivity?, ?negativity? and ?neutrality?. Therefore,
every concept in the graph is weighting accord-
ing to its subjectivity and polarity. The last ver-
sion of SentiWordNet (3.0) has been constructed
starting from manual annotations of previous ver-
sions, populating the whole graph by applying a ran-
dom walk algorithm. This resource has been used
by the opinion mining community, as it provides a
domain-independent resource to get certain informa-
tion about the degree of emotional charge of its con-
cepts (Denecke, 2008; Ogawa et al, 2011).
3.3 Computing the final estimation
As a combination of SentiWordNet scores with ran-
dom walk weights is wanted, it is important that
the final equation leads to comparable values. To
this end, the weights associated to synsets after the
random walk process are L1 normalized so vectors
of ?concepts? sum up the unit as maximum value.
The final polarity score is obtained by the product of
this vector with associated SentiWordNet vector of
scores, as expressed in equation 1.
p = r ? s|t| (1)
where p is the final score, r is the vector of
weighted synsets computed by the random walk al-
gorithm of the tweet text over WordNet, s is the vec-
tor of polarity scores from SentiWordNet, t is the
set of concepts derived from the tweet. The idea be-
hind it is to ?expand? the set of concepts with addi-
tional ones that are close in the WordNet graph, cor-
responding to those synset nodes which have been
activated during the random walk process. There-
fore, terms like dog and bite (both mainly neutral
in SentiWordNet) appearing in the same tweet could
eventually be expanded with a more emotional term
like hurt, which holds, in SentiWordNet, a negative
score of 0.75.
4 Experiments and results
Our experiments are focused in testing the validity
of applying this unsupervised approach compared to
a classical supervised one based on Support Vector
Machines (Joachims, 1998). To this end, the corpus
has been processed obtaining lemmas, as this is the
preferred input for the UKB software. The algorithm
takes the whole WordNet graph and performs a dis-
ambiguation process of the terms as a natural con-
sequence of applying random walk over the graph.
In this way, the synsets that are associated to these
terms are all of them initialized. Then, the iterative
process of the algorithm (similar to Page Rank but
optimized according to an stochastic solution) will
change these initial values and propagate weights to
closer synsets. An interesting effect of this process is
that we can actually obtain more concepts that those
contained in the tweet, as all the related ones will
also finalize with a certain value due to the propaga-
tion of weights across the graph. We believe that our
approach benefits from this effect, as texts in tweets
use to suffer from a very sort length, allowing us to
expand short posts.
Another concern is, therefore, the final size of the
PPV vector. If too many concepts are taken into ac-
count we may introduce noise in the understanding
of the latent semantic of the text. In order to study
this fact, different sizes of the vector have been ex-
plored and evaluated.
4.1 Our Twitter corpus
The analysis of the polarity on microblogging is a
very recent task, so there are few free resources
(Sas?a et al, 2010). Thus, we have collected our
own English corpus in order to accomplish the ex-
periments. The work of downloading tweets is not
nearly difficult due to the fact that Twitter offers two
kinds of API to those purposes. We have used the
5
Search API of Twitter6 for automatically accessing
tweets through a query. For a supervised polarity
study and to evaluate our approach, we need to gen-
erate a labelled corpus. We have built a corpus of
tweets written in English following the procedure
described in (Read, 2005) and (Go et al, 2009).
According to (Read, 2005), when authors of an
electronic communication use an emotion, they are
effectively marking up their own text with an emo-
tional state. The main feature of Twitter is that the
length of the messages must be 140 characters, so
the users have to express their opinions, thoughts,
and emotional states with few words. Therefore,
frequently users write ?smileys? in their tweets.
Thus, we have used positive emoticons to label pos-
itive tweets and negative emoticons to tag negative
tweets. The full list of emoticons that we have con-
sidered to label the retrieved tweets can be found in
Table 1. So, following (Go et al, 2009), the pre-
sumption in the construction of the corpus is that the
query ?:)? returns tweets with positive smileys, and
the query ?:(? retrieves negative emotions. We have
collected a set of 376,296 tweets (181,492 labelled
as positive tweets and 194,804 labelled as negative
tweets), which were published on Twitter?s public
message board from September 14th 2010 to March
19th 2011. Table 2 lists other characteristics of the
corpus.
On the other hand, the language used in Twit-
ter has some unique attributes, which have been re-
moved because they do not provide relevant infor-
mation for the polarity detection process. These spe-
cific features are:
1. Retweets: A retweet is the way to repeat a mes-
sage that users consider interesting. Retweets
can be done through the web interface using
the Retweet option, or as the old way writing
RT, the user name and the post to retweet. The
first way is not a problem because is the same
tweet, so the API only return it once, but old
way retweets are different tweets but with the
same content, so we removed them to avoid pit-
ting extra weight on any particular tweet.
2. Mentions: Other feature of Twitter is the so
called Mentions. When a user wants to refer
6https://dev.twitter.com/docs/api/1/get/search
Emoticons mapped to :)
(positive tweets)
:) : ) :-)
;) ;-) =)
? ? :-D :D
:d =D C:
Xd XD xD
Xd (x (=
?? ?o? ?u?
n n *-* *O*
*o* * *
Emoticons mapped to :(
(negative tweets)
:-( :( :((
: ( D: Dx
?n? :\ /:
):-/ :? =?[
: ( /T T TOT
; ;
Table 1: Emoticons considered as positives and negatives
to another one, he or she introduces a Mention.
A Mention is easily recognizable because all of
them start with the symbol ?@? followed by the
user name. We consider that this feature does
not provide any relevance information, so we
have removed the mentions in all the tweets.
3. Links: It is very common that tweets include
web directions. In our approach we do not ana-
lyze the documents that links those urls, so we
have eliminated them from all tweets.
4. Hash-tags: A hash-tag is the name of a topic
in Twitter. Anybody can begin a new topic by
typing the name of the topic preceded by the
symbol ?#?. For this work we do not classify
topics so we have neglected all the hash-tags.
Due to the fact that users usually write tweets
with a very casual language, it is necessary to pre-
process the raw tweets before feeding the sentiment
analyzer. For that purpose we have applied the fol-
lowing filters:
1. Remove new lines: Some users write tweets
in two or three different lines, so all newlines
symbols were removed.
2. Opposite emoticons: Twitter sometimes con-
siders positive or negative a tweet with smileys
6
Total
Positive tweets 181,492
Negative tweets 194,804 376,296
Unique users in positive
tweets
157,579
Unique users in negative
tweets
167,479 325,058
Words in positive tweets 418,234
Words in negative tweets 334,687 752,921
Average number of
words per positive tweet
9
Average number of
words per negative tweet
10
Table 2: Statistical description of the corpus.
that have opposite senses. For example:
@Harry Styles I have all day to try
get a tweet off you :) when are
you coming back to dublin i missed
you last time,I was in spain :(
The tweet has two parts one positive and the
other one negative, so the post cannot be con-
sidered as positive, but the search API returns
as a positive tweet because it has the positive
smiley ?:)?. We have removed this kind of
tweets in order to avoid ambiguity.
3. Emoticons with no clear sentiment: The
Twitter Search API considers some emoticons
like ?:P? or ?:PP? as negative. However, some
users do not type them to express a negative
sentiment. Thus, we have got rid of all tweets
with this kind of smileys (see Table 3).
Fuzzy emoticons :-P :P :PP \(
Table 3: Emoticons considered as fuzzy sentiments
4. Repeated letters: Users frequently repeat sev-
eral times letters of some words to emphasize
their messages. For example:
Blood drive todayyyy!!!!! :)
Everyone donateeeee!!
This can be a problem for the classification pro-
cess, because the same word with different rep-
etitions of the same letter would be considered
as a different word. Thus, we have normalized
all the repeated letters, and any letter occurring
more than two times in a word is replaced with
two occurrences. The example above would be
converted into:
blood drive todayy :) everyone
donatee!!
5. Laugh: There is not a unique manner to ex-
press laugh. Therefore, we have normalized
the way to write laugh. Table 4 lists the con-
versions.
Laugh Conversion
hahahaha... haha
hehehehe... hehe
hihihihi... hihi
hohohoho... hoho
huhuhuhu... huhu
Lol haha
Huashuashuas huas
muahahaha Buaha
buahahaha Buaha
Table 4: Normalization for expressions considered as
?Laugh?
Finally, although the emoticons have been used
to tag the positive and negative samples, the fi-
nal corpora does not include these emoticons.
In addition, all the punctuation characters have
been neglected in order to reduce the noise in
the data. Figure 1 shows the process to gener-
ate our Twitter corpus.
4.2 Results obtained
Our first experiment consisted on evaluating a super-
vised approach, like Support Vector Machines, us-
ing the well know vector space model to build the
vector of features. Each feature corresponds to the
TF.IDF weight of a lemma. Stop words have not
been removed and the minimal document frequency
required was two, that is, if the lemma is not present
in two o more tweets, then it is discarded as a di-
mension in the vectors. The SVM-Light7 software
was used to compute support vectors and to evaluate
them using a random leave-one-out strategy. From
7http://svmlight.joachims.org/
7
Figure 1: Corpus generation work-flow
a total of 376,284 valid samples 85,423 leave-one-
out evaluations were computed. This reported the
following measurements:
Precision Recall F1
0.6429 0.6147 0.6285
In our first implementation of our method, the fi-
nal polarity score is computed as described in equa-
tion 1. More precisely, it is the average of the prod-
uct between the difference of positive and negative
SentiWordNet scores, and the weight obtained with
the random walk algorithm, as unveiled in equa-
tion 2.
p =
?
?s?t rws ? (swn+s ? swn?s )
|t| (2)
Where s is a synset in the tweet t, rws is the
weight of the synset s after the random walk pro-
cess over WordNet, swn+s and swn?s ) are positive
and negative scores for the synset s retrieved from
SentiWordNet.
The results obtained are graphically shown in fig-
ures 2, 3 and 4 for precision, recall and F1 values
respectively. As can be noticed from the shapes
of the graphs, the size of the PPV vectors affects
the performance. Sizes above 10 presents an sta-
ble behavior, that is, considering a large number of
synsets does not improves the performance of the
system, but it gets worse neither. The WordNet
graph considered for the random walk algorithm in-
cludes antonyms relations, so we wanted to check
whether discarding these connections would affect
the system. From these graphs we can extract the
conclusion that antonyms relations are worth keep-
ing.
Figure 2: Precision values against PPV sizes
Figure 3: Recall values against PPV sizes
Comparing our best configuration to the SVM ap-
proach, the results are not better, but quite close (ta-
ble 5). Therefore, this unsupervised solution is an
interesting alternative to the supervised one.
8
Figure 4: F1 values against PPV sizes
Precision Recall F1
SVM 0.6429 0.6147 0.6285
RW?SWN 0.6259 0.6207 0.6233
Table 5: Approaches comparative table
5 Conclusions and further work
A new unsupervised approach to the polarity detec-
tion problem in Twitter posts has been proposed. By
combining a random walk algorithm that weights
synsets from the text with polarity scores provided
by SentiWordNet, it is possible to build a system
comparable to a SVM based supervised approach in
terms of performance. Our solution is a general ap-
proach that do not suffer from the disadvantages as-
sociated to supervised ones: need of a training cor-
pus and dependence on the domain where the model
was obtained.
Many issues remain open and they will drive our
future work. How to deal with negation is a ma-
jor concern, as the score from SentiWordNet should
be considered in a different way in the final com-
putation if the original term comes from a negated
phrase. Our ?golden rules? must be taken carefully,
because emoticons are a rough way to classify the
polarity of tweets. Actually, we are working in the
generation of a new corpus in the politics domain
that is now under a manual labeling process. An-
other step is to face certain flaws in the computation
of the final score. In this sense, we plan to study
the context of a tweet among the time line of tweets
from that user to identify publisher?s mood and ad-
just final scores. As an additional task, the process-
ing of original texts is important. The numerous
grammatical and spelling errors found in this fast
way of publication demand for a better sanitization
of the incoming data. An automatic spell checker is
under development.
As final conclusion, we believe that this first at-
tempt is very promising and that it has arose many
relevant questions on the subject of sentiment analy-
sis. More extensive research and experimentation is
being undertaken from the starting point introduced
in this paper.
Acknowledgments
This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
(FEDER), TEXT-COOL 2.0 project (TIN2009-
13391-C04-02) from the Spanish Government. This
paper is partially funded by the European Commis-
sion under the Seventh (FP7 - 2007-2013) Frame-
work Programme for Research and Technologi-
cal Development through the FIRST project (FP7-
287607). This publication reflects the views only
of the author, and the Commission cannot be held
responsible for any use which may be made of the
information contained therein.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Language in Social Media (LSM 2011), pages 30?38,
Portland, Oregon, jun. Association for Computational
Linguistics.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In EACL
?09: Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 33?41, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2008. Sentiwordnet 3.0 : An enhanced lexical
resource for sentiment analysis and opinion mining.
Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation LREC10,
0:2200?2204.
K. Denecke. 2008. Using sentiwordnet for multilingual
sentiment analysis. In Data Engineering Workshop,
9
2008. ICDEW 2008. IEEE 24th International Confer-
ence on, pages 507 ?512, april.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
T. Joachims. 1998. Text categorization with support vec-
tor machines: learning with many relevant features. In
European Conference on Machine Learning (ECML).
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, 2nd ed.
Tatsuya Ogawa, Qiang Ma, and Masatoshi Yoshikawa.
2011. News Bias Analysis Based on Stakeholder Min-
ing. IEICE TRANSACTIONS ON INFORMATION
AND SYSTEMS, E94D(3):578?586, MAR.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford
University.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135.
Daniel Ramage, Anna N. Rafferty, and Christopher D.
Manning. 2009. Random walks for text seman-
tic similarity. In TextGraphs-4: Proceedings of the
2009 Workshop on Graph-based Methods for Natural
Language Processing, pages 23?31, Morristown, NJ,
USA. Association for Computational Linguistics.
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL Stu-
dent Research Workshop, ACLstudent ?05, pages 43?
48, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Petrovic? Sas?a, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceed-
ings of the NAACL HLT 2010 Workshop on Compu-
tational Linguistics in a World of Social Media, WSA
?10, pages 25?26, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mikalai Tsytsarau and Themis Palpanas. 2011. Survey
on mining subjective data on the web. Data Mining
and Knowledge Discovery, pages 1?37, October.
C M Whissell, 1989. The dictionary of affect in lan-
guage, volume 4, pages 113?131. Academic Press.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-
ichun Hsu, and Bing Liu. 2011. Combining lexicon-
based and learning-based methods for twitter senti-
ment analysis. Technical Report HPL-2011-89, HP,
21/06/2011.
10
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 87?93,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Bilingual Experiments on an Opinion Comparable Corpus
E. Mart??nez-Ca?mara
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
emcamara@ujaen.es
M. T. Mart??n-Valdivia
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
maite@ujaen.es
M. D. Molina-Gonza?lez
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
mdmolina@ujaen.es
L. A. Uren?a-Lo?pez
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
laurena@ujaen.es
Abstract
Up until now most of the methods published
for polarity classification are applied to En-
glish texts. However, other languages on the
Internet are becoming increasingly important.
This paper presents a set of experiments on
English and Spanish product reviews. Us-
ing a comparable corpus, a supervised method
and two unsupervised methods have been as-
sessed. Furthermore, a list of Spanish opinion
words is presented as a valuable resource.
1 Introduction
Opinion Mining (OM) is defined as the computa-
tional treatment of opinion, sentiment, and subjec-
tivity in text. The OM discipline combines Natural
Language Processing (NLP) with data mining tech-
niques and includes a large number of tasks (Pang
and Lee, 2008). One of the most studied tasks
is polarity classification of reviews. This task fo-
cuses on determining which is the overall sentiment-
orientation (positive or negative) of the opinions
contained within a given document.
Two main appraoches are followed by researches
to tackle the OM task. On the one hand, the Ma-
chine Learning (ML) approach (also known as the
supervised approach) is based on using a collection
of data to train the classifiers (Pang et al, 2002). On
the other hand, (Turney, 2002) proposed an unsuper-
vised method based on the semantic orientation of
the words and phrases in the reviews. Both method-
ologies have their advantages and drawbacks. For
example, the ML approach depends on the avail-
ability of labelled data sets (training data), which
in many cases are impossible or difficult to achieve,
partially due to the novelty of the task. On the
contrary, the unsupervised method requires a large
amount of linguistic resources which generally de-
pend on the language, and often this approach ob-
tains lower recall because it depends on the presence
of the words comprising the lexicon in the document
in order to determine the polarity of opinion.
Although opinions and comments on the Inter-
net are expressed in any language, most of research
in OM is focused on English texts. However, lan-
guages such as Chinese, Spanish or Arabic, are ever
more present on the web. Thus, it is important to
develop resources for these languages. The work
presented herein is mainly motivated by the need
to develop polarity classification systems and re-
sources in languages other than English. We present
an experimental study over the SFU Review Corpus
(Taboada, 2008), a comparable corpus that includes
opinions of several topics in English and in Span-
ish. We have followed this line of work: Firstly,
we have taken as baseline a supervised experiment
using Support Vector Machine (SVM). Then we
have tried different unsupervised strategies. The first
one uses the method presented in (Montejo-Ra?ez et
al., 2012). This approach combines SentiWordNet
scores with a random walk analysis of the concepts
found in the text over the WordNet graph in order to
determine the polarity of a tweet. This method ob-
tained very good results in short texts (tweets) and
so, we want to try it using larger document. Al-
though we have carried out several experiments us-
ing different parameters and modifications, the re-
sults are not as good as we hoped. For this, we have
87
tried a very simple experiment using a list of opin-
ionated words in order to classify the polarity of the
reviews. For English we have used the Bin Liu En-
glish lexicon (BLEL) (Hu and Liu, 2004) and for
Spanish we have automatically translated the BLEL
lexicon into Spanish. In addition, we have also
checked manually and improved the Spanish list.
The paper is organized as follows: Section 2
briefly describes papers that study non-English sen-
timent polarity classification and, specifically work
related to Spanish OM. In Section 3 we explain
the resources used in the unsupervised methods as-
sessed. Section 4 presents the experiments carried
out and discusses the main results obtained. Finally,
we outline conclusions and further work.
2 Related Work
There are some interesting papers that have stud-
ied the problem using non-English collections. De-
necke (2008) worked on German comments col-
lected from Amazon. These reviews were translated
into English using standard machine translation soft-
ware. Then the translated reviews were classified as
positive or negative, using three different classifiers:
LingPipe7, SentiWordNet (Baccianella et al, 2010)
with classification rule, and SentiWordNet with ma-
chine learning. Ghorbel and Jacot (2011) used a cor-
pus with movie reviews in French. They applied a
supervised classification combined with SentiWord-
Net in order to determine the polarity of the reviews.
In (Rushdi-Saleh et al, 2011a) a corpus of movies
reviews in Arabic annotated with polarity was pre-
sented and several supervised experiments were per-
formed. Subsequently, they generated the parallel
EVOCA corpus (English version of OCA) by trans-
lating the OCA corpus automatically into English.
The results showed that they are comparable to other
English experiments, since the loss of precision due
to the translation process is very slight, as can be
seen in (Rushdi-Saleh et al, 2011b).
Regarding Spanish, there are also some interest-
ing studies. Banea et al (2008) showed that au-
tomatic translation is a viable alternative for the
construction of resources and tools for subjectivity
analysis in a new target language. In (Brooke et
al., 2009) several experiments are presented deal-
ing with Spanish and English resources. They con-
clude that although the ML techniques can provide
a good baseline performance, it is necessary to inte-
grate language-specific knowledge and resources in
order to achieve an improvement. Cruz et al (2008)
manually recollected the MuchoCine (MC) corpus
to develop a sentiment polarity classifier based on
the semantic orientation of the phrases and words.
The corpus contains annotated Spanish movie re-
views from the MuchoCine website. The MC cor-
pus was also used in (Mart??nez-Ca?mara et al, 2011)
to carry out several experiments with a supervised
approach applying different ML algorithms. Finally,
(Mart??n-Valdivia et al, 2012) also dealt with the MC
corpus to present an experimental study of super-
vised and unsupervised approaches over a Spanish-
English parallel corpus.
3 Resources for the unsupervised methods
In order to tackle the unsupervised experiments we
have chosen several well-known resources in the
OM research community. In addition, we have also
generated a new Spanish linguistic resource.
Comparable corpora are those consisted of texts
in two or more languages about the same topic, but
they are not the translated version of the texts in the
source language. For the experiments, we chose the
comparable corpus SFU Review Corpus. The SFU
Review Corpus is composed of reviews of prod-
ucts in English and Spanish. The English version
(Taboada and Grieve, 2004) has 400 reviews (200
positive and 200 negative) of commercial products
downloaded in 2004 from the Epinions web which
are divided into eight categories: books, cars, com-
puters, cookware, hotels, movies, music and phones.
Each category includes 25 positive reviews and 25
negative reviews. Recently, the authors of SFU Re-
view Corpus have made available the Spanish ver-
sion of the corpus1. The Spanish reviews are divided
into the same eight categories, and also each cate-
gory has 25 positive and 25 negative reviews.
In the unsupervised experiments we have anal-
ysed the performance of two approaches, the first
one is based on lexicon and the other one in a graph-
based method. We have selected the BLEL lexicon
(Hu and Liu, 2004) to carry out the experiment based
1http://www.sfu.ca/?mtaboada/download/
downloadCorpusSpa.html
88
on lexicon on the English version of the corpus. The
lexicon is composed by 6,787 opinion words that
indicate positive or negative opinions, which 2,005
are positive and 4,782 are negative. With the aim of
following the same approach over the Spanish ver-
sion, firstly we have translated the BLEL lexicon
with the Reverso machine translator, and them we
have checked manually the resultant list. The Span-
ish Opinion Lexicon2 (SOL) is composed by 2,509
positive and 5,627 negative words, thus in total SOL
has 8,136 opinion words. If a review has more or
the same positive words than negative the polarity is
positive, otherwise negative.
The graph-based method is a modular system
which is made up of different components and
technologies. The method was first presented in
(Montejo-Ra?ez et al, 2012) with a good perfor-
mance over a corpus of English tweets. The main
idea of the algorithm is to represent each review as a
vector of polarity scores of the senses in the text and
senses related to the context of the first ones. Be-
sides, the polarity score is weighted with a measure
of importance. Taking a review as input, the work-
flow of the algorithm is the following:
1. Disambiguation: To get the corresponding
sense of the words that are in the text is required
to disambiguate them. Thus, the output of this
first step is one unique synset from WordNet3
(Miller, 1995) for each term. The input of the
algorithm is the set of words with a POS-Tag
allowed in WordNet. The graph nature of the
WordNet structure is the basis for the UKB dis-
ambiguation method proposed by (Agirre and
Soroa, 2009). The UKB disambiguation algo-
rithm apply PageRank (Page et al, 1999) on
the WordNet graph starting from term nodes,
where each term node points to all its possible
senses or synsets. The output of the process is a
ranked list of synsets for each input word, and
the highest rank synset is chosen as candidate
sense.
For the Spanish disambiguation process we
have chosen the Spanish WordNet version
offered by the project Multilingual Central
2http://sinai.ujaen.es/wiki/index.php/
SOL
3We have used the 3.0 release of WordNet.
Repository (MCR) (Gonzalez-Agirre et al,
2012). The Spanish WordNet of MCR has
38,702 synsets while WordNet has 117,659, i.e.
the MCR covers the 32.89% of WordNet.
2. PPV: Once the synsets for the reviews are com-
puted, the following step performs a second run
of PageRank described in (Agirre and Soroa,
2009). Using the Personalized PageRank, a
set of Personalized PageRank Vectors (PPVs)
is obtained. This vector is a list of synsets with
their ranked values. The key of this approach
is to take from this vector additional synsets
not related directly to the set of synsets disam-
biguated in the first step. The result is a longer
list of pair <synset, weight> where the weight
is the rank value obtained by the propagation of
the weights of original synsets across theWord-
Net graph.
3. Polarity: The following step is to calculate the
polarity score. For this purpose it is necessary a
semantic resource to take the polarity score for
each retrieved synset in the two previous steps.
The semantic resource selected is SentiWord-
Net (Baccianella et al, 2010). According to
these values, the three following equations have
been applied to obtain the final polarity value:
p(r) = 1
|r|
?
s?r
1
|s|
?
i?s
(p+i ? p
?
i )wi (1)
p(r) = 1
|r|
?
s?r
1
|s|
?
i?s
f(pi)
f(pi) =
{
p+i if p
+
i > p
?
i
p?i if p
+
i <= p
?
i
(2)
p(r) = 1
|r|
?
s?r
1
|s|
?
i?s
f(pi)
f(pi) =
?
?
?
?
?
?
?
1 if i ? [positive words]
?1 if i ? [negative words]
p+i if p
+
i > p
?
i
p?i if p
+
i <= p
?
i
(3)
where p(r) is the polarity of the review; |r| is
the number of sentences in the review r; s is a
sentence in r, being itself a set of synsets; i is a
synset in s; p+i is the positive polarity of synset
i; p?i is the negative polarity of synset i and wi
is the weight of synset i.
89
4 Experiments and Results
Systems based on supervised approach are the most
successfully in the OM literature. Therefore, we be-
gan the set of experiments applying a machine learn-
ing algorithm to the SFU corpus. Also, we have car-
ried out a set of unsupervised experiments following
a lexicon-based approach and a graph-based algo-
rithm. For all the experiments the evaluation mea-
sures have been: precision, recall, F1 and Accuracy
(Acc.). The validation approach followed for the
supervised approach has been the well-known 10-
cross-validation.
The algorithm chose for the supervised experi-
ments is SVM (Cortes and Vapnik, 1995) because
is one of the most successfully used in OM. Lib-
SVM4 (Chang and Lin, 2011) was the implementa-
tion selected to carry out several experiments using
SVM. We have evaluated unigrams and bigrams as
minimum unit of information. Also, the influence of
stemmer have been assessed. The weight scheme for
representing each unit of information is TF-IDF. The
same configuration has been applied to English and
Spanish version of SFU corpus. Table 1 and Table
2 show the results for English version and Spanish
version respectively.
Precision Recall F1 Acc.
Unigrams 79.07% 78.50% 78.78% 78.50%
Unigrams
& stemmer 79.82% 79.50% 79.66% 79.50%
Bigrams 78.77% 78.25% 78.51% 78.25%
Bigrams
& stemmer 80.64% 80.25% 80.44% 80.25%
Table 1: SVM results for English SFU corpus
Precision Recall F1 Acc.
Unigrams 73.65% 73.25% 73.45% 73.25%
Unigrams
& stemmer 74.10% 73.75% 73.92% 73.75%
Bigrams 74.02% 73.50% 73.76% 73.50%
Bigrams
& stemmer 73.90% 73.50% 73.70% 73.50%
Table 2: SVM results for Spanish SFU corpus
The results show one of the differences between
the works published in SA, the use of unigrams or
4http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
bigrams. In (Pang et al, 2002) is asserted that the
reviews should be represented with unigrams, but
in (Dave et al, 2003) bigrams and trigrams outper-
formed the unigrams features. In our case, regarding
the results reached without using a stemmer, the use
of unigrams as minium unit of information achieves
better result than the use of bigrams when the lan-
guage is English, but bigrams outperform unigrams
when the texts are in Spanish. On the other hand, the
best result both in English and Spanish is reached
when a stemmer algorithm is applied. So, one con-
clusion of the supervised experiments is that the use
of stemmer enhances the polarity classification in re-
views. The following conclusion is that in English
the presence of pair of words separate better the pos-
itive and negative classes, while in Spanish the use
of unigrams is enough to classify the polarity when
a stemmer algorithm is used.
The set of unsupervised experiments begins with
a lexicon-based method. The method consists of find
the presence in the reviews of opinion words which
are included in a lexicon of opinion words. BLEL
has been used for the English reviews, and SOL for
the Spanish reviews. The results are presented in
Table 3.
Precision Recall F1 Acc.
BLEL lexicon 69.56% 64.42% 66.89% 64.75%
SOL 66.91% 61.94% 64.33% 62.25%
Table 3: Lexicon-based approch results
The differences in the results between the En-
glish and Spanish version of SFU Review Corpus
are lower when a lexicon is used instead of a ma-
chine learning algorithm is applied. In a lexicon-
based method is very important the recall value, be-
cause it indicates whether the set of words covers
the vocabulary of the corpus. The recall value is
upper 60% regarding English and Spanish, although
is not an excellent value, is good for the two small
and independent-domain lexicons. In the case of
Spanish the supervised method is only 15.59% bet-
ter regarding Accuracy. The results show that may
be considered the use of a lexicon-based method for
Spanish due to the few computer resources needed.
Moreover, it must be highlighted the performance of
SOL, so it is the first time that this resource is used
to resolve a polarity classification problem.
90
The graph-based method has been described as a
modular and flexible algorithm. Due to its modular
nature we have carried out several experiments:
1. wnet ant+ eq1 [en|es]: As baseline, we have
run the algorithm with the same configuration
as is described in (Montejo-Ra?ez et al, 2012),
i.e. using the equation 1.
2. wnet ant- eq1 [en|es]: We have assessed the
algorithm with a version of WordNet without
the antonym relation.
3. wnet ant+ eq2 [en|es]: The equation to calcu-
late the polarity is 2
4. wnet ant- eq2 [en|es]: The same as
wnet ant+ eq2 [en|es] but the antonym
relation is not considered.
5. wnet ant+ eq3 [en|es]: The same as
wnet ant+ eq2 [en|es] but the equation 3
is used to calculate the polarity.
6. wnet ant- eq3 [en|es]: The same as
wnet ant+ eq3 [en|es] but the antonym
relation is not considered.
Furthermore, one of the key elements of the al-
gorithm is the possibility of setting the number of
related synsets to get from WordNet. In all of the ex-
periments we have evaluated from an expansion of 0
sysnsets to 100 synsets. In Table 4 are the best re-
sults obtained with the English and the Spanish ver-
sion of SFU corpus.
Regarding the results, only for English is evident
that the selection of the right equation to calculate
the polarity score is important. On the other hand,
the initial assumption that the relation of antonym
could complicate the calculation of the final polarity,
and the use of a graph of WordNet without antonym
could enhance the results cannot be demonstrated
because these experiments have reached the same
results as the obtained ones using the graph with
the relation of antonym. The equation 3, which in-
cludes additional information (in this case the BLEL
lexicon) to calculate the final polarity score, out-
performs the original way to get the polarity score
(equation 1). The equation 3 for the English version
of the corpus reaches 5.84% and 8.4% better results
than equation 1 regarding F1 and Accuracy respec-
tively.
The results obtained with the Spanish reviews are
a bit different. In this case, the results are always
improved when the antonym relation is not taking
into account. So the first conclusion is the relation
of antonym is not convenient for the calculation of
the polarity value on Spanish texts. The process of
expansion with related senses has not been relevant
for the final results on the English reviews, but when
the language of the reviews is Spanish the expan-
sion is more decisive. For the wnet ant- eq3 es ex-
periment the best result has been reached consider-
ing 71 related senses, so we can conclude that for
Spanish the context should be considered. Although
the best results is obtained with the configuration
wnet ant+ eq3 es, it must be highlighted the pre-
cision value of 68.03% reached by the configura-
tion wnet ant+ eq2 es. In some OM experiments is
more important the precision of the system than the
recall or other evaluation measures, so for Spanish
reviews should be taken account this configuration
too.
Regarding English and Spanish results, Table 4
shows similar performance, i.e. the graph-based al-
gorithm obtained better results when the antonym is
not considered and the use of a lexicon of opinion
words enhances considerably the results.
The supervised approach clearly outperforms the
two unsupervised approaches. The results obtained
by the two unsupervised approaches are closer. The
lexicon based method has a better performance on
English reviews regarding the four different eval-
uation measures considered. Thus, the lexicon
method not only has better results but also it is sim-
pler, faster and more efficient than the graph-based
method. Nevertheless, the graph-based method on
Spanish reviews outperforms in precision regard-
ing the configuration wnet ant+ eq2 es and in the
other three measures take into account the configu-
ration wnet ant+ eq3 es. However, the graph-based
method is only 1.64% better regarding the precision
value, and 0.54% better regarding F1. Therefore, we
also considered the lexicon-based approach as the
more suitable approach than the graph-based one.
91
Expansion Precision Recall F1 Accuracy
wnet ant+ eq1 en 2 66.86% 57.25% 61.68% 57.25%
wnet ant- eq1 en 2 66.86% 57.25% 61.68% 57.25%
wnet ant+ eq2 en 0 65.27% 55.5% 59.99% 55.50%
wnet ant- eq2 en 0 65.27% 55.5% 59.99% 55.50%
wnet ant+ eq3 en 3 68.83% 62.50% 65.51% 62.50%
wnet ant- eq3 en 3 68.83% 62.50% 65.51% 62.50%
wnet ant+ eq1 es 0 65.42% 54.5% 59.46% 54.5%
wnet ant- eq1 es 19 64.39% 57.75% 60.89% 57.75%
wnet ant+ eq2 es 0 68.03% 52.75% 59.42% 52.75%
wnet ant- eq2 es 70 64.62% 58.00% 61.13% 58.00%
wnet ant+ eq3 es 71 65.91% 63.50% 64.68% 63.05%
wnet ant- eq3 es 71 65.91% 63.50% 64.68% 63.05%
Table 4: Results of the graph-based algorithm
5 Conclusion and future work
In this work, we have presented a set of experiments
with a comparable corpora in English and Spanish.
As it is usual, the supervised experiment has outper-
forms the unsupervised ones. The unsupervised ex-
periments have included the evaluation of two differ-
ent approaches: lexicon-based and graph-based. In
the lexicon-based approach we have presented a new
resource for the Spanish OM research community,
being an important contribution of this paper. The
results reached with SOL are very closed to the ones
obtained with graph-based methods. Although, for
short texts the graph-based method performed well,
for the kind of reviews used in these experiments is
not as good. Due to the fact that for English the
BLEL lexicon has reached better results, for Span-
ish the results of SOL are nearly the same ones ob-
tained by the graph method, and the use of a lexicon
is more efficient, we conclude that the lexicon-based
method is most suitable.
Currently we are improving the SOL lexicon, and
also we are adding domain information to the words
in SOL. Furthermore, one of our main objectives is
the treatment of the negation because we considered
that is essential for OM.
Acknowledgments
This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
(FEDER), TEXT-COOL 2.0 project (TIN2009-
13391-C04-02) and ATTOS project (TIN2012-
38536-C03-0) from the Spanish Government. Also,
this paper is partially funded by the European
Commission under the Seventh (FP7 - 2007-2013)
Framework Programme for Research and Techno-
logical Development through the FIRST project
(FP7-287607). This publication reflects the views
only of the authors, and the Commission cannot be
held responsible for any use which may be made of
the information contained therein.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ?09, pages 33?41, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?08, pages 127?135,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From en-
glish to spanish. In Proceedings of the International
Conference RANLP-2009, pages 50?54, Borovets,
92
Bulgaria, September. Association for Computational
Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20:273?297.
Ferm??n L. Cruz, Jose A. Troyano, Fernando Enriquez,
and Javier Ortega. 2008. Clasificacio?n de documen-
tos basada en la opinio?n: experimentos con un cor-
pus de cr??ticas de cine en espan?ol. Procesamiento del
Lenguaje Natural, 41:73?80.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of the 12th international conference on World
Wide Web, WWW ?03, pages 519?528, New York, NY,
USA. ACM.
Kerstin Denecke. 2008. Using sentiwordnet for multilin-
gual sentiment analysis. In Data Engineering Work-
shop, 2008. ICDEW 2008. IEEE 24th International
Conference on, pages 507?512. IEEE.
Hatem Ghorbel and David Jacot. 2011. Sentiment anal-
ysis of french movie reviews. Advances in Distributed
Agent-Based Retrieval Tools, pages 97?108.
Aitor Gonzalez-Agirre, Egoitz Laparra, and German
Rigau. 2012. Multilingual central repository version
3.0. In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet Ug?ur Dog?an,
Bente Maegaard, Joseph Mariani, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC?12), Istanbul, Turkey, may. Euro-
pean Language Resources Association (ELRA).
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Eugenio Mart??nez-Ca?mara, M. Teresa Mart??n-Valdivia,
and L. Alfonso Uren?a Lo?pez. 2011. Opinion clas-
sification techniques applied to a spanish corpus. In
Proceedings of the 16th international conference on
Natural language processing and information sys-
tems, NLDB?11, pages 169?176, Berlin, Heidelberg.
Springer-Verlag.
M. Teresa Mart??n-Valdivia, Eugenio Mart??nez-Ca?mara,
Jose M. Perea-Ortega, and L. Alfonso Uren?a Lo?pez.
2012. Sentiment polarity detection in spanish reviews
combining supervised and unsupervised approaches.
Expert Systems with Applications. In press.
George A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Arturo Montejo-Ra?ez, Eugenio Mart??nez-Ca?mara,
M. Teresa Mart??n-Valdivia, and L. Alfonso Uren?a
Lo?pez. 2012. Random walk weighting over senti-
wordnet for sentiment polarity detection on twitter. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analy-
sis, pages 3?10, Jeju, Korea, July. Association for
Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number =
SIDL-WP-1999-0120.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing - Volume 10, EMNLP ?02, pages
79?86, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Mohammed Rushdi-Saleh, M. Teresa Mart??n-Valdivia,
L. Alfonso Uren?a Lo?pez, and Jose? M. Perea-Ortega.
2011a. OCA: Opinion corpus for Arabic. Journal
of the American Society for Information Science and
Technology, 62(10):2045?2054, October.
Mohammed Rushdi-Saleh, Maria Teresa Martn-Valdivia,
Luis Alfonso Urea-Lpez, and Jos M. Perea-Ortega.
2011b. Bilingual Experiments with an Arabic-English
Corpus for Opinion Mining. In Galia Angelova,
Kalina Bontcheva, Ruslan Mitkov, and Nicolas Ni-
colov, editors, RANLP, pages 740?745. RANLP 2011
Organising Committee.
Maite Taboada and Jack Grieve. 2004. Analyzing ap-
praisal automatically. In Proceedings of AAAI Spring
Symposium on Exploring Attitude and Affect in Text
(AAAI Technical Re# port SS# 04# 07), Stanford Uni-
versity, CA, pp. 158q161. AAAI Press.
Maite Taboada. 2008. Sfu review corpus. http:
//www.sfu.ca/?mtaboada/research/SFU_
Review_Corpus.html.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, ACL ?02, pages 417?424, Stroudsburg, PA, USA.
Association for Computational Linguistics.
93
