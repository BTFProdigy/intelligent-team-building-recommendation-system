Dependency Annotation Scheme for Indian Languages 
 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra 
Sharma, Lakshmi Bai and Rajeev Sangal 
Language Technologies Research Center, 
IIIT, Hyderabad, India. 
{rafiya,samar}@research.iiit.ac.in, 
{dipti,lakshmi,sangal}@iiit.ac.in 
 
 
Abstract 
The paper introduces a dependency annota-
tion effort which aims to fully annotate a 
million word Hindi corpus. It is the first at-
tempt of its kind to develop a large scale 
tree-bank for an Indian language. In this 
paper we provide the motivation for fol-
lowing the Paninian framework as the an-
notation scheme and argue that the Pan-
inian framework is better suited to model 
the various linguistic phenomena manifest 
in Indian languages. We present the basic 
annotation scheme. We also show how the 
scheme handles some phenomenon such as 
complex verbs, ellipses, etc. Empirical re-
sults of some experiments done on the cur-
rently annotated sentences are also re-
ported. 
1 Introduction 
A major effort is currently underway to develop a 
large scale tree bank for Indian Languages (IL). 
The lack of such a resource has been a major limit-
ing factor in the development of good natural lan-
guage tools and applications for ILs. Apart from 
that, a rich and large-scale tree bank can be an in-
dispensable resource for linguistic investigations. 
Some notable efforts in this direction for other lan-
guages have been the Penn Tree Bank (Marcus et 
al., 1993) for English and the Prague Dependency 
Bank (Hajicova, 1998) for Czech.  
It is well known that context free grammar 
(CFG) is not well-suited for free-word order 
languages (Shieber, 1985); instead dependency 
framework appears to be better suited (Hudson, 
1984; Mel'Cuk, 1988, Bharati et al, 1995). Also, 
the dependency framework is arguably closer to 
semantics than the phrase structure grammar (PSG) 
if the dependency relations are judiciously chosen. 
In recent times many research groups have been 
shifting to the dependency paradigm due to this 
reason. Modern dependency grammar is attributed 
to Tesni?re (1959). In a dependency analysis, there 
is no hierarchical arrangement of phrases (or 
substrings) like in phrase structure grammar. 
Rather, we just have words connected via 
dependency relations between them.  
Prague Dependency Bank (PDT) for Czech 
(which has relatively free word order) is one such 
large-scale effort which implements a three-tier 
annotation scheme and annotates morphological 
information, analytical and tectogrammatical level 
annotations at these three levels. Out of the three 
levels, the analytical and tectogrammatical level 
are dependency based. The tectogrammatical level 
tries to capture the deep-semantics of the sentence; 
the annotation at this level is very rich and is 
linked to the other two lower levels. Other major 
efforts in the dependency framework are Alpino 
(van der Beek et. al, 2002) for Dutch, (Rambow et. 
al, 2002) for English, TUT (Bosco and Lombardo, 
2004) for Italian, TIGER (Brants et. al, 2002) 
(combines dependency with PSG) for German. In 
this paper we describe an approach to annotate ILs 
using the Paninian1 model. The paper is arranged 
as follows, Section 2 gives a brief overview of the  
           
          1Paninian theory was formulated by Panini about 
two thousand five hundred years ago for Sanskrit. It 
evolved with the contributions of grammarians that fol-
lowed. 
721
grammatical model and the motivation for 
following the framework. Section 3 talks about the 
chosen corpus and the annotation procedure. In 
Section 4 we discuss some dependency relations. 
Section 5 describes the evaluation procedure. We 
report the empirical results of experiments done on 
the annotated data in Section 6. Section 7, 
concludes the paper. 
2 Grammatical Model 
ILs are morphologically rich and have a relatively 
flexible word order. For such languages syntactic 
subject-object positions are not always able to ele-
gantly explain the varied linguistic phenomena. In 
fact, there is a debate in the literature whether the 
notions ?subject? and ?object? can at all be defined 
for ILs (Mohanan, 1982). Behavioral properties are 
the only criteria based on which one can confi-
dently identify grammatical functions in Hindi 
(Mohanan, 1994); it can be difficult to exploit such 
properties computationally. Marking semantic 
properties such as thematic role as dependency 
relation is also problematic. Thematic roles are 
abstract notions and will require higher semantic 
features which are difficult to formulate and to ex-
tract as well. So, thematic roles are not marked at 
this juncture. On the other hand, the notion of ka-
raka relations (explained shortly) provides us a 
level which while being syntactically grounded 
also helps in capturing some semantics. What is 
important to note here is that such a level can be 
exploited computationally with ease. This provides 
us with just the right level of syntactico-semantic 
interface. The experiments conducted on the pre-
sent annotated text provide empirical evidence for 
this claim (section 6). Paninian grammar is basi-
cally a dependency grammar (Kiparsky and Staal, 
1969; Shastri, 1973). In this section we briefly dis-
cuss the Paninian model for ILs and lay down 
some basic concepts inherent to this framework. 
The main problem that the Paninian approach 
addresses is to identify syntactico-semantic rela-
tions in a sentence. The Paninian approach treats a 
sentence as a series of modifier-modified relations. 
A sentence is supposed to have a primary modified 
(the root of the dependency tree) which is gener-
ally the main verb of the sentence. The elements 
modifying the verb participate in the action speci-
fied by the verb. The participant relations with the 
verb are called karaka. The appropriate mapping of 
the syntactic cues helps in identifying the appro-
priate karakas (?participants in an action?). The 
framework is inspired by an inflectionally rich lan-
guage like Sanskrit; it emphasizes the role of case 
endings or markers such as post-positions and ver-
bal inflections. 
There are six basic karakas, namely; adhikarana 
?location?,  apaadaan ?source?, sampradaan 
?recipient?,  karana ?instrument?, karma ?theme?, 
karta ?agent?. We must note here that although one 
can roughly map the first four karaka to their the-
matic role counterpart, karma and karta are very 
different from ?theme? and ?agent? respectively 
(see section 4.1.1).  
In our annotation scheme, we use chunks as a 
device for modularity. A chunk represents a set of 
adjacent words which are in dependency relations 
with each other, and are connected to the rest of 
the words by a single incoming dependency arc. 
The relations among the words in a chunk are not 
marked for now and hence allow us to ignore local 
details while building the sentence level depend-
ency tree. Thus, in our dependency tree each node 
is a chunk and the edge represents the relations 
between the connected nodes labeled with the ka-
raka or other relations. All the modifier-modified 
relations between the heads of the chunks (inter-
chunk relations) are marked in this manner. Intra-
chunk relations can be marked by a set of rules at a 
later point. Experiments have been conducted with 
high performance in automatically marking intra-
chunk dependencies.  
Using information such as karakas based on 
some vibhaktis (post-positions) and other informa-
tion like TAM (tense, aspect and modality) of the 
main verb seems very well suited for handling free 
word order languages. Other works based on this 
scheme like (Bharati et al, 1993; Bharati et al, 
2002; Pedersen et al, 2004) have shown promising 
results. We, therefore, propose the use of depend-
ency annotation based on the Paninian model in the 
Indian context. 
3 Annotation Procedure and Corpus De-
scription 
The annotation task is planned on a million word 
Hindi corpora obtained from CIIL (Central Insti-
tute for Indian Languages), Mysore, India. It is a 
representative corpus which contains texts from 
various domains like newspaper, literature, gov-
722
ernment reports, etc. The present subset on which 
the dependency annotation is being performed has 
already been manually tagged and chunked. Cur-
rently the annotation is being carried out by 2 an-
notators, who are graduate students with linguistic 
knowledge. The tool being used for the annotation 
is part of Sanchay (Singh, 2006) which is a collec-
tion of tools and APIs for South Asian languages. 
4 Scheme 
There are a total of 28 relations (see, 
http://ltrc.deptagset.googlepages.com/home) which 
we encode during the annotation. The total number 
of relations in the framework is few which has a 
direct bearing on the parser based on this frame-
work (both, rule based and statistical). We briefly 
discuss some of these relations in this section. 
4.1 Dependency relations 
As mentioned earlier, the proposed scheme uses 
the dependency relations from Paninian grammar. 
Section 4.1.1 below shows some karaka relations, 
section 4.1.2 shows some other relations; 
4.1.1 karaka relations 
(1) raama phala   khaataa hai 
?Ram?  ?fruit?     ?eat?    ?is? 
      ?Ram eats fruit? 
 
 
Figure1.  
 
(2) raama   chaaku   se     saiv     kaattaa hai 
?Ram?   ?knife?  -inst  ?apple? ?cut?     ?is? 
      ?Ram cuts the apple with a knife? 
 
 
Figure2.  
 
Examples (1), and (2) above show some simple 
cases which have karaka relations such as k3 
(karana; ?instrument?), k1 (karta), k2 (karma) (the 
term karta and karma can be roughly translated as 
?agent? and ?theme?). One must note here that the 
notion of karta, karma, etc, is not equivalent to that 
of the ?agent?, ?theme? thematic roles (although 
they might map to them sometimes). The reason 
for this divergence in the two notions (karaka and 
thematic role) is due to the difference in what they 
convey. Thematic role is purely semantic in nature 
whereas the karaka is syntactico-semantic. Exam-
ples (3), illustrates this point, 
 
(3) chaabhi  ne      darvaazaa   kholaa 
      ?key?     ?-erg?  ?door?      ?opened? 
      ?The key opened the door? 
 
In the above examples chaabhi is k1 (karta), 
whereas it takes instrument thematic role. While 
the karaka relations are primarily motivated via 
verbal semantics, syntactic cues like postpositions 
and verbal morphology play an important role too. 
For example in (3) above, the ergative case ?ne? 
provides a strong cue to identify karta. Panini de-
fines ?karta? as ?svatantra karta? which can be 
translated as ?the participant which is the most in-
dependent in a given action?. In (3) ?key? has such 
a property. When the speaker uses ?key? in (3), 
he/she intends to elevate the role of ?key? in the 
action of opening and does not communicate the 
actual agent of the action. The speaker uses ?key? 
as the independent participant in the act of open-
ing. Hence, ?key? is the karta (see Bharati et al, 
1995, pp. 65-73, for a more detailed discussion). 
4.2 Special Cases 
(a) POF (Part of relation) 
 
Conjunct verbs form a very challenging case for 
analysis in Indian languages. They have been ex-
tensively analyzed in the past. Some notable at-
tempts have been (Greaves, 1983; Kellogg, 1972; 
Mohanan, 1994; Butt, 2004). The example below 
shows a N+V conjunct verb usage; 
 
(4)  raama   ne    mujhase     prashna    kiyaa  
?Ram?  -erg  ?me-inst?  ?question?  ?do? 
           ?Ram asked me a question.? 
 
723
In example (4), prashna kiyaa is a conjunct verb 
and behaves as a single semantic unit. These verbs 
can also be discontiguous as in (5), 
 
(5) raama  ne    mujhase   prashna   pichle saal kiyaa   
     ?Ram? -erg  ?me-inst? ?question? ?last? ?year? ?did? 
     ?Ram asked me a question last year.? 
 
In the above example above a normal conjunct 
verb sequence prashna kiyaa is disjoint, making it 
rather difficult to annotate. In fact, practically 
anything can come between the disjointed 
elements. Ideally, the noun/adjective + verb 
sequence of the conjunct verb is placed in one 
chunk. Keeping this in mind, example (6) below is 
even more problematic, 
 
   (6)  maine     usase       ek       prashna  kiyaa 
   ?I-erg?  ?him-inst? ?one? ?question? ?did? 
         ?I asked him a question? 
 
The noun prashna ?question? within the con-
junct verb sequence prashna kiyaa is being modi-
fied by the adjective ek ?one? and not the entire 
noun-verb sequence; the annotation scheme should 
be able to account for this relation in the depend-
ency tree. If prashna kiyaa is grouped as a single 
verb chunk, it will not be possible to mark the ap-
propriate relation between ek and prashna. To 
overcome this problem it is proposed to break ek 
prashna kiyaa into two separate chunks, [ek 
prashna]/NP2 [kiyaa]/VG3. The dependency rela-
tion of prashna with kiyaa will be POF (?Part OF? 
relation), i.e. the noun or an adjective in the con-
junct verb sequence will have a POF relation with 
the verb. This way, the relation between ek and 
prashna becomes an intra-chunk relation as they 
will now become part of a single NP chunk. What 
makes such a sequence unique is the fact that the 
components which make up a conjunct verb are 
chunked separately, but semantically they consti-
tute a single unit. 
  The proposed scheme has the following advan-
tages: 
 
 (i) It captures the fact that the noun-verb seque 
ence is a conjunct verb by linking them with an 
appropriate tag, this information is extremely cruc- 
 
          2 Noun Phrase 
    3 Verb Group 
ial syntactically. 
 
(ii) It allows us to deal with the modifier-
modified relation between an adjective and its 
modified noun, as in example (6), which is a fre-
quent phenomenon.  
 
The tree below shows the proposed solution, 
where the adjective ek modifies the noun prashna 
instead of the entire prashna kiyaa, which would 
have been the case had we not separated prashna 
kiyaa into two separate chunks. 
 
 
Figure3.  
 
(b) ccof (?conjunct of? relation) and ellipses 
 
In the case of coordinating conjunction like aur 
?and? , the conjunct becomes the root and takes the 
two conjoined elements as children, the relation 
marked on the edges is ccof (conjunct of). This 
analysis captures the fact that neither of the con-
joined elements is the head. (The head of the two 
(or more) conjoined elements lies in the conjunct, 
and may be so computed when needed.) The ele-
ments participating in the coordination can belong 
to various categories, such as, noun, adjective, ad-
verbs etc; they can also be entire clauses, partici-
ples, etc. Other conjunct and punctuations which 
act like conjuncts are annotated similarly. 
When one or more element from a sentence is 
dropped, it is called ellipses. A null element 
marked with a special tag ?NULL? is introduced in 
cases of ellipses, where without inserting it the tree 
cannot be drawn. Null_NP, Null_VG, Null_CCP 
etc mark different kinds of ellipses.  
In this section, we have briefly discussed some 
of the relations and showed their actual usage us-
ing some sentences. The number of tags in the pro-
posed scheme is not very large. A limited set of 
tags helps immensely in developing high-
performance parsers (both rule based and statisti-
cal) and other related applications. We should note 
here that our tag-set, although small, is not a de-
724
limiting factor and is not a compromise on the se-
mantics, as these 28 relations are enough to fully 
parse the sentences in the language.  
5 Evaluation 
To make sure that the quality of the annotated cor-
pus is good, the annotators cross-validate each 
other?s work. A senior team member finally checks 
the annotated corpus (of both the annotators) to 
ensure that the errors are minimized. Note that 
such a setup is only temporary, we need such a 
thorough validation because we are still in the 
process of revising the guidelines. Once the guide-
lines become stable, the annotators won?t need to 
cross-validate. Of course, the task of final valida-
tion will still continue. 
6 Experiments 
Some preliminary experiments were conducted on 
a corpus of 1403 Hindi sentences that have been 
fully annotated. The aim was to access; 
 
1. Whether the syntactic cues can be ex-
ploited for better machine learnability. 
2. Whether certain generalization can be 
made for a constraint parser. 
3. How far would the automatic annotation 
help the annotators? 
 
We found a strong co-relation between most 
vibhakti-karaka occurrences (shaded cells in Table 
1). k7 (?place?) for example, overwhelmingly takes 
mem post-position, k3 (karana) takes se in all the 
cases. Of course, there are some competing rela-
tions which show preference for the same post-
position. In such cases only the post-position in-
formation will not be sufficient and we need to 
take into account other syntactic cues as well. 
These syntactic cues can be TAM (tense, aspect 
and modality) of the verb, verb class information, 
etc. For example, in case of karata karaka (k1), the 
following heuristics help resolve the ambiguities 
seen in Table 1. These heuristics are applied se-
quentially, i.e. if the first fails then the next follows. 
Note that the heuristics mentioned below are meant 
only for illustrative purpose. The cues mentioned 
in the heuristics will finally be used as features by 
an efficient ML technique to automate the task of 
annotation. 
H1: k1 agrees in gender, number and person 
with the verb if it takes a nominative case, 
H2: k1 takes a ko case-marker if the TAM of the 
verb has nA, 
H3: It takes a kaa/ke/ki if the verb is infinitive, 
H4: It takes a se or dvaara if the TAM of the 
verb is passive 
H5: It takes a ne case-marker if the verb is tran-
sitive and the TAM is perfective 
 
Table-2 shows the results when the heuristics 
were tested on the annotated corpus to test their 
effectiveness. 
 
 
Table 1. karaka-vibhakti correlation 
 
 
Table 2. Heuristics for k1 disambiguation 
 
The field ?Total? in Table-2 gives us the number 
of instances where a particular heuristic was ap-
plied. For example, there were 1801 instances 
where k1 appeared in a nominative case and H1 
correctly identified 1461 instances. H1 failed due 
to the errors caused by the morphological analyzer, 
presence of conjuncts, etc. Of particular interest 
are H2 and H3 which didn?t work out for large 
number of cases. It turns out that H2 failed for 
what is understood in the literature as dative sub-
jects. Dative subjects occur with some specific 
verbs, one possible solution could be to use such 
verbs for disambiguation. Automatic identification 
of conjunct verbs is a difficult problem; in fact, 
there isn?t any robust linguistic test which can be  
 
4 karaka relations (see, 
http://ltrc.deptagset.googlepages.com/home) 
    5 vibhakti (post-position) 
725
used to identify such verbs. Similar heuristics can 
be proposed for disambiguating other karaka based 
on some syntactic cues. Based on the above results 
one can safely conclude that arriving at some ro-
bust generalization (like, karaka-vibhakti correla-
tion) based on the syntactic cues is in fact possible. 
This can help us immensely in building an efficient 
parser for Hindi (and other ILs). It goes without 
saying that there exists a lot of scope for automat-
ing the annotation task. 
7 Conclusion 
In this paper we have introduced an ongoing effort 
to annotate Indian languages with dependency rela-
tion. We stated the motivation behind following 
the Paninian framework in the Indian Language 
scenario. We discussed the basic scheme along 
with some new relations such as ccof, POF, etc. 
We also showed the results of some experiments 
conducted on the annotated data which showed 
that there is a strong co-relation between vibhakti-
karaka relations. 
Acknowledgement 
Thanks are due to Prof. Ramakrishnamacharyulu 
who has been guiding us throughout the develop-
ment of the proposed scheme. 
References 
Akshar Bharati and Rajeev Sangal. 1993. Parsing Free 
Word Order Languages in the Paninian Framework, 
ACL93: Proc. of Annual Meeting of Association for 
Computational Linguistics. 
Akshar Bharati, Vineet Chaitanya and Rajeev Sangal. 
1995. Natural Language Processing: A Paninian 
Perspective, Prentice-Hall of India, New Delhi, pp. 
65-106. 
Akshar Bharati, Rajeev Sangal, T Papi Reddy. 2002. A 
Constraint Based Parser Using Integer Programming, 
In Proc. of ICON-2002: International Conference on 
Natural Language Processing, 2002. 
Cristina Bosco and V. Lombardo. 2004. Dependency 
and relational structure in treebank annotation. In 
Proceedings of Workshop on Recent Advances in 
Dependency Grammar at COLING'04. 
S. Brants, S. Dipper, S. Hansen, W. Lezius and G. 
Smith. 2002. The TIGER Treebank. In Proceedings 
of the Workshop on Treebanks and Linguistic Theo-
ries. 
M. Butt. 2004. The Light Verb Jungle. In G. Aygen, C. 
Bowern & C. Quinn eds. Papers from the 
GSAS/Dudley House Workshop on Light Verbs. 
Cambridge, Harvard Working Papers in Linguistics, 
p. 1-50. 
Edwin Greaves. 1983. Hindi Grammar. Asian Educa-
tional Services, New Delhi, pp. 335-340 
E. Hajicova. 1998. Prague Dependency Treebank: From 
Analytic to Tectogrammatical Annotation. In Proc. 
TSD?98. 
R. Hudson. 1984. Word Grammar, Basil Blackwell, 108 
Cowley Rd, Oxford, OX4 1JF, England. 
S. H. Kellogg. 1972. A Grammar of the Hindi Language. 
Munshiram Manoharlal, New Delhi, pp. 271-279. 
P. Kiparsky and J. F. Staal. 1969. ?Syntactic and Rela-
tions in Panini?, Foundations of Language 5, 84-117. 
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 
1993. Building a large annotated corpus of English: 
The Penn Treebank, Computational Linguistics 1993. 
I. A. Mel'cuk. 1988. Dependency Syntax: Theory and 
Practice, State University, Press of New York. 
K. P. Mohanan. 1982. Grammatical relations in Malaya-
lam, In Joan Bresnan (ed.), The Mental Representa-
tion of Grammatical Relations, MIT Press, Cam-
bridge. 
Tara Mohanan, 1994. Arguments in Hindi. CSLI Publi-
cations. 
M. Pedersen, D. Eades, S. K. Amin, and L. Prakash. 
2004. Relative Clauses in Hindi and Arabic: A Pan-
inian Dependency Grammar Analysis. In COLING 
2004 Recent Advances in Dependency Grammar, 
pages 9?16. Geneva, Switzerland. 
O. Rambow, C. Creswell, R. Szekely, H. Taber, and M. 
Walker. 2002. A dependency treebank for English. In 
Proceedings of the 3rd International Conference on 
Language Resources and Evaluation. 
Anil Kumar Singh. 2006. 
http://sourceforge.net/projects/nlp-sanchay 
Charudev Shastri. 1973. Vyakarana Chandrodya (Vol. 1 
to 5). Delhi: Motilal Banarsidass. (In Hindi) 
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. In Linguistics and Phi-
losophy, p. 8, 334?343. 
L. Tesni?re. 1959. El?ments de Syntaxe Structurale. 
Klincksiek, Paris.  
L. van der Beek, G. Bouma, R. Malouf, and G. van 
Noord. 2002. The Alpino dependency treebank. 
Computational Linguistics in the Netherlands. 
726
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 25?32,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Aggregating Machine Learning and Rule Based Heuristics for Named 
Entity Recognition 
 Karthik Gali, Harshit Surana, Ashwini Vaidya, Praneeth Shishtla and  
Dipti Misra Sharma 
Language Technologies Research Centre, 
International Institute of Information Technology, 
Hyderabad, India. 
karthikg@students.iiit.ac.in, surana.h@gmail.com,  
ashwini_vaidya@research.iiit.ac.in, praneethms@students.iiit.ac.in, 
dipti@iiit.ac.in 
 
 
 
Abstract 
This paper, submitted as an entry for the 
NERSSEAL-2008 shared task, describes a 
system build for Named Entity Recognition 
for South and South East Asian Languages.  
Our paper combines machine learning 
techniques with language specific heuris-
tics to model the problem of NER for In-
dian languages. The system has been tested 
on five languages: Telugu, Hindi, Bengali, 
Urdu and Oriya. It uses CRF (Conditional 
Random Fields) based machine learning, 
followed by post processing which in-
volves using some heuristics or rules. The 
system is specifically tuned for Hindi and 
Telugu, we also report the results for the 
other four languages. 
1 Introduction 
Named Entity Recognition (NER) is a task that 
seeks to locate and classify entities (?atomic ele-
ments?) in a text into predefined categories such as 
the names of persons, organizations, locations, ex-
pressions of times, quantities, etc. It can be viewed 
as a two stage process: 
  
1. Identification of entity boundaries 
2. Classification into the correct category 
 
For example, if ?Mahatma Gandhi? is a named 
entity in the corpus, it is necessary to identify the 
beginning and the end of this entity in the sentence. 
Following this step, the entity must be classified 
into the predefined category, which is NEP 
(Named Entity Person) in this case. 
This task is the precursor for many natural lan-
guage processing applications. It has been used in 
Question Answering (Toral et al 2005) as well as 
Machine Translation (Babych et al 2004). 
The NERSSEAL contest has used 12 categories 
of named entities to define a tagset. The data has 
been manually tagged for training and testing pur-
poses for the contestants. 
The task of building a named entity recognizer 
for South and South East Asian languages presents 
several problems related to their linguistic charac-
teristics. We will first discuss some of these lin-
guistic issues, followed by a description of the 
method used. Further, we show some of the heuris-
tics used for post-processing and finally an analy-
sis of the results obtained.  
2 Previous Work  
The linguistic methods generally use rules 
manually written by linguists. There are several 
rule based NER systems, containing mainly lexi-
calized grammar, gazetteer lists, and list of trigger 
words, which are capable of providing upto 92% f-
measure accuracy for English (McDonald, 1996; 
Wakao et al, 1996).  
Linguistic approach uses hand-crafted rules 
which need skilled linguistics. The chief disadvan-
tage of these rule-based techniques is that they re-
quire huge experience and grammatical knowledge 
of the particular language or domain and these sys-
tems are not transferable to other languages or do-
mains. However, given the closer nature of many 
Indian languages, the cost of adaptation of a re-
25
source from one language to another could be quite 
less (Singh and Surana, 2007). 
Various machine learning techniques have also 
been successfully used for the NER task. Generally 
hidden markov model (Bikel et al,1997), maxi-
mum entropy (Borthwick, 1999), conditional ran-
dom field (Li and Mccallum, 2004) are more popu-
lar machine learning techniques used for the pur-
pose of NER. 
Hybrid systems have been generally more effec-
tive at the task of NER. Given lesser data and more 
complex NE classes which were present in 
NERSSEAL shared task, hybrid systems make 
more sense. Srihari et al (2000) combines MaxEnt, 
hidden markov model (HMM) and handcrafted 
rules to build an NER system. 
Though not much work has been done for other 
South Asian languages, some previous work fo-
cuses on NER for Hindi. It has been previously 
attempted by Cucerzan and Yarowsky in their lan-
guage independent NER work which used morpho-
logical and contextual evidences (Cucerzan and 
Yarowsky, 1999). They ran their experiment with 
5 different languages. Among these the accuracy 
for Hindi was the worst. For Hindi the system 
achieved 42% f-value with a recall of 28% and 
about 85% precision. A result which highlights 
lack of good training data, and other various issues 
involved with linguistic handling of Indian lan-
guages. 
Later approaches have resulted in better results 
for Hindi. Hindi NER system developed by Wei Li 
and Andrew Mccallum (2004) using conditional 
random fields (CRFs) with feature induction have 
achieved f-value of 71%. (Kumar and Bhat-
tacharyya, 2006) used maximum entropy markov 
model to achieve f-value of upto 80%. 
3 Some Linguistic Issues 
3.1 Agglutinative Nature 
Some of the SSEA languages have agglutinative 
properties.  For example, a Dravidian language like 
Telugu has a number of postpositions attached to a 
stem to form a single word. An example is: 
 
guruvAraMwo = guruvAraM + wo  
up to Wednesday = Wednesday + up to 
 
Most of the NERs are suffixed with several dif-
ferent postpositions, which increase the number of 
distinct words in the corpus.  This in turn affects 
the machine learning process. 
3.2 No Capitalization 
All the five languages have scripts without graphi-
cal cues like capitalization, which could act as an 
important indicator for NER.  For a language like 
English, the NER system can exploit this feature to 
its advantage. 
3.3 Ambiguity 
One of the properties of the named entities in these 
languages is the high overlap between common 
names and proper names. For instance Kamal (in 
Hindi) can mean ?lotus?, which is not a named en-
tity, but it can also be a person?s name, in which 
case, it is a named entity. 
Among the named entities themselves, there is 
ambiguity between a location name Bangalore ek 
badzA shaher heI (Bangalore is a big city) or a per-
son?s surname ?M. Bangalore shikshak heI? (M. 
Bangalore is a teacher). 
3.4 Low POS Tagging Accuracy for Nouns 
For English, the available tools like POS (Part of 
Speech) tagger can be used to provide features for 
machine learning. This is not very helpful for 
SSEA languages because the accuracy for noun 
and proper noun tags is quite low (PVS and G., 
2006) Hence, features based on POS tags cannot 
be used for NER for these languages. 
To illustrate this difficulty, we conducted the 
following experiment. A POS tagger (described in 
PVS & G.,2006) was run on the Hindi test data.  
The data had 544 tokens with NEL, NEP, NEO 
tags.  The POS tagger should have given the NNP 
(proper noun) tag for all those named entities. 
However the tagger was able to tag only 80 tokens 
accurately. This meant that only 14.7% of the 
named entities were correctly recognized. 
3.5 Spelling Variation 
One other important language related issue is the 
variation in the spellings of proper names. For in-
stance the same name Shri Ram Dixit can be writ-
ten as Sri. Ram Dixit, Shree Ram Dixit, Sh. R. Dixit 
and so on. This increases the number of tokens to 
be learnt by the machine and would perhaps also 
require a higher level task like co-reference resolu-
tion. 
 
26
2.6 Pattern of suffixes We have converted this format into the BIO 
format as described in Ramshaw et. al. For exam-
ple, the above format will now be shown as: 
 
Named entities of Location (NEL) or Person 
(NEP) will share certain common suffixes, which 
can be exploited by the learning algorthm. For in-
stance, in Hindi, -pur (Rampur, Manipur) or -giri 
(Devgiri) are suffixes that will appear in the named 
entities for Location. Similarly, there are suffixes 
like -swamy (Ramaswamy, Krishnaswamy) or -
deva (Vasudeva, Mahadeva) which can be com-
monly found in named entities for person. These 
suffixes are cues for some of the named entities in 
the SSEA languages. 
 
Rabindranath  B-NEP 
Tagore   I-NEP 
ne   O 
kahaa   O 
 
The training data set contains (approximately) 
400,000 Hindi, 50,000 Telugu, 35,000 Urdu, 
93,000 Oriya and 120,000 Bengali words respec-
tively.  
A NER system can be rule-based, statistical or 
hybrid. A rule-based NER system uses hand-
written rules to tag a corpus with named entities. A 
statistical NER system learns the probabilities of 
named entities using training data, whereas hybrid 
systems use both. 
5 Conditional Random Fields 
Conditional Random Fields (CRFs) are undirected 
graphical models used to calculate the conditional 
probability of values on designated output nodes 
given values assigned to other designated input 
nodes. Developing rule-based taggers for NER can be cumbersome as it is a language specific process. 
Statistical taggers require large amount of anno-
tated data (the more the merrier) to train.  Our sys-
tem is a hybrid NER tagger which first uses Condi-
tional Random Fields (CRF) as a machine learning 
technique followed by some rule based post-
processing. 
In the special case in which the output nodes of 
the graphical model are linked by edges in a linear 
chain, CRFs make a first-order Markov independ-
ence assumption, and thus can be understood as 
conditionally-trained finite state machines (FSMs). 
Let o = (o,,o
We treat the named entity recognition problem 
as a sequential token-based tagging problem. 
According to Lafferty et. al. CRF outperforms 
other Machine Learning algorithms viz., Hidden 
Markov Models (HMM), Maximum Entropy 
Markov Model (MEMM) for  sequence labeling 
tasks.  
4 Training data 
The training data given by the organizers was in 
SSF format1. For example in SSF format, the 
named entity ?Rabindranath Tagore? will be shown 
in the following way: 
0 (( SSF 
1  ((  NP  <ne=NEP> 
1.1  Rabindranath 
1.2 Tagore 
)) 
2 ne 
3 kahaa 
 )) 
 
                                                          
1 http://shiva.iiit.ac.in/SPSAL2007/ssf-analysis-representation.pdf
2,o3 ,o4 ,... oT  ) be some observed in-
put data sequence, such as a sequence of words in 
text in a document,(the values on n input nodes of 
the graphical model). Let S be a set of FSM states, 
each of which is associated with a label, l ? ?. 
Let s = (s ,s ,s  ,s  ,... s1 2 3 4 T ) be some sequence of 
states, (the values on T output nodes). By the 
Hammersley-Clifford theorem, CRFs define the 
conditional probability of a state sequence given an 
input sequence to be: 
 
where Zo is a normalization factor over all state 
sequences is an arbitrary feature function over its 
arguments, and ?k is a learned weight for each fea-
ture function. A feature function may, for example, 
be defined to have value 0 or 1. Higher ? weights 
make their corresponding FSM transitions more 
likely. CRFs define the conditional probability of a 
label sequence based on the total probability over 
the state sequences, 
 
 
27
 
where l(s) is the sequence of labels correspond-
ing to the labels of the states in sequence s. 
Note that the normalization factor, Zo, (also 
known in statistical physics as the partition func-
tion) is the sum of the scores of all possible states. 
 
And that the number of state sequences is expo-
nential in the input sequence length, T. In arbitrar-
ily-structured CRFs, calculating the partition func-
tion in closed form is intractable, and approxima-
tion methods such as Gibbs sampling or loopy be-
lief propagation must be used. In linear-chain 
structured CRFs (in use here for sequence model-
ing), the partition function can be calculated effi-
ciently by dynamic programming. 
6 CRF Based Machine Learning 
We used the CRF model to perform the initial tag-
ging followed by post-processing. 
6.1 Statistical Tagging 
In the first phase, we have used language inde-
pendent features to build the model using CRF. 
Orthographic features (like capitalization, decimals), 
affixes (suffixes and prefixes), context (previous 
words and following words), gazetteer features, POS 
and morphological features etc. are generally used for 
NER. In English and some other languages, capitali-
zation features play an important role as NEs are 
 generally capitalized for these languages. Unfortu-
nately as explained above this feature is not applica-
ble for the Indian languages. 
Precision Recall F-Measure  
Pm Pn Pl Rm Rn Rl Fm Fn Fl  
Bengali 53.34 49.28 58.27 26.77 25.88 31.19 35.65 33.94 40.63 
Hindi 59.53 63.84 64.84 41.21 41.74 40.77 48.71 50.47 50.06 
Oriya 39.16 40.38 63.70 23.39 19.24 28.15 29.29 26.06 39.04 
Telugu 10.31 71.96 65.45 68.00 30.85 29.78 08.19 43.19 40.94 
Urdu 43.63 44.76 48.96 36.69 34.56 39.07 39.86 39.01 43.46 
Table 1: Evaluation of the NER System for Five Languages 
The exact set of features used are described be-
low. 
6.2 Window of the Words 
Words preceding or following the target word may 
be useful for determining its category. Following a 
few trials we found that a suitable window size is 
five. 
6.3 Suffixes 
Statistical suffixes of length 1 to 4 have been con-
sidered. These can capture information for named 
entities having the NEL tag like Hyderabad, 
Secunderabad, Ahmedabad etc., all of which end 
in -bad. We have collected lists of such suffixes for 
NEP (Named Entity Person) and NEL (Named En-
tity Location) for Hindi. In the machine learning 
model, this resource can be used as a binary fea-
ture. A sample of these lists is as follows: 
 
Type of NE Example suffixes 
(Hindi) 
NE- Location -desa, -vana, -nagara,  
-garh, -rashtra, -giri  
NE ? Person -raja, -natha, -lal, -bhai,-
pathi, -krishnan 
 Table 2: Suffixes for Hindi NER 
28
7 Heuristics Based Post Processing 6.4 Prefixes 
Statistical prefixes of length 1 to 4 have been con-
sidered. These can take care of the problems asso-
ciated with a large number of distinct tokens. As 
mentioned earlier, agglutinative languages can 
have a number of postpositions. The use of pre-
fixes will increase the probability of   Hyderabad 
and Hyderabadlo (Telugu for ?in Hyderabad?) be-
ing treated as the same token. 
Complex named entities like fifty five kilograms 
contain a Named Entity Number within a Named 
Entity Measure. We observed that these were not 
identified accurately enough in the machine learn-
ing based system. Hence, instead of applying ma-
chine learning to handle nested entities we make 
use of rule-based post processing.  
7.1 Second Best Tag 
Table 3: F-Measure (Lexical) for NE Tags 
 Bengali Hindi Oriya Telugu Urdu 
It was observed that the recall of the CRF model is 
low. In order to improve recall, we have used the 
following rule:  if the best tag given by the CRF 
model is O (not a named entity) and the confidence 
of the second best tag is greater than 0.15, then the 
second best tag is considered as the correct tag. 
NEP 35.22 54.05 52.22 01.93 31.22 
NED NA 42.47 01.97 NA 21.27 
NEO 11.59 45.63 14.50 NA 19.13 
NEA NA 61.53 NA NA NA 
We observed an increase of 7% in recall and 3% 
decrease in precision. This resulted in a 4% in-
crease in the F-measure, which is a significant in-
crease in performance. The decrease in precision is 
expected as we are taking the second tag. 
NEB NA NA NA NA NA 
NETP 42.30 NA NA NA NA 
NETO 33.33 13.77 NA 01.66 NA 
NEL 45.27 62.66 48.72 01.49 57.85 
7.2 Nested Entities NETI 55.85 79.09 40.91 71.35 63.47 
NEN 62.67 80.69 24.94 83.17 13.75 One of the important tasks in the contest was to 
identify nested named entities. For example if we 
consider eka kilo (Hindi: one kilo) as NEM 
(Named Entity Measure), it contains a NEN 
(Named Entity Number) within it. 
NEM 60.51 43.75 19.00 26.66 84.10 
NETE 19.17 31.52 NA 08.91 NA
The CRF model tags eka kilo as NEM and in or-
der to tag eka as NEN we have made use of other 
resources like a gazetteer for the list of numbers. 
We used such lists for four languages. 
6.5 Start of a sentence 
There is a possibility of confusing the NEN 
(Named Entity Number) in a sentence with the 
number that appears in a numbered list. The num-
bered list will always have numbers at the begin-
ning of a sentence and hence a feature that checks 
for this property will resolve the ambiguity with an 
actual NEN. 
7.3 Gazetteers 
For Hindi, we made use of three different kinds of 
gazetteers. These consisted of lists for measures 
(entities like kilogram, millimetre, lakh), numerals 
and quantifiers (one, first, second) and time ex-
pressions (January, minutes, hours) etc. Similar 
lists were used for all the other languages except 
Urdu. These gazetteers were effective in identify-
ing this relatively closed class of named entities 
and showed good results for these languages. 
6.6 Presence of digits 
Usually, the presence of digits indicates that the 
token is a named entity. For example, the tokens 
92, 10.1 will be identified as Named Entity Num-
ber based on the binary feature ?contains digits?. 
6.7 Presence of  four digits 8 Evaluation 
If the token is a four digit number, it is likelier to 
be a NETI (Named Entity Time). For example, 
1857, 2007 etc. are most probably years. 
The evaluation measures used for all the five lan-
guages are precision, recall and F-measure. These 
measures are calculated in three different ways: 
 
29
1. Maximal Matches: The largest possible 
named entities are matched with the refer-
ence data. 
The amount of annotated corpus available for 
Hindi was substantially more. This should have 
ideally resulted in better results for Hindi with the 
machine learning approach. But, the results were 
only marginally better than other languages. A ma-
jor reason for this was that a very high percentage 
(44%) of tags in Hindi were NETE. The tagset 
gives examples like ?Horticulture?, ?Conditional 
Random Fields? for the tag NETE. It has also been 
mentioned that even manual annotation is harder 
for NETE as it is domain specific. This affected the 
overall results for Hindi because the performance 
for NETE was low (Table 3). 
2. Nested Matches: The largest possible as 
well as nested named entities are matched. 
3. Lexical Item Matches: The lexical items 
inside largest possible named entities are 
matched. 
9 Results 
The results of evaluation as explained in the previ-
ous section are shown in the Table-1. The F-
measures for nested lexical match are also shown 
individually for each named entity tag separately in 
Table-3 
 Num of 
NE tokens
Num of 
known NE 
% of un-
known NE
Bengali 1185 277 23.37 
10 Unknown Words Hindi 1120 417 37.23 
Table 4 shows the number of unknown words pre-
sent in the test data when compared with the train-
ing data. 
Oriya 1310 563 42.97 
Telugu 1150 145 12.60 
First column shows the number of unique 
Named entity tags present in the test data for each 
language. Second column shows the number of 
unique known named entities present in the test 
data. Third column shows the percentage of unique 
unknown words present in the test data of different 
languages when compared to training data. 
Urdu 631 179 28.36 
Table 4: Unknown Word 
 
Also, the F-measures of NEN, NETI, and NEM 
could have been higher because they are relatively 
closed classes. However, certain NEN can be am-
biguous (Example: eka is a NEN for ?one? in 
Hindi, but in a different context it can be a non-
number. For instance eka-doosra is Hindi for ?each 
other?). 
11 Error Analysis 
We can observe from the results that the maximal 
F-measure for Telugu is very low when compared 
to lexical F-measure and nested F-measure. The 
reason is that the test data of Telugu contains a 
large number of long named entities (around 6 
words), which in turn contain around 4 - 5 nested 
named entities. Our system was able to tag nested 
named entities correctly unlike maximal named 
entity. 
In a language like Telugu, NENs will appear as 
inflected words. For example 2001lo, guru-
vaaramto. 
10     Conclusion and Further Work 
In this paper we have presented the results of using 
a two stage hybrid approach for the task of named 
entity recognition for South and South East Asian 
Languages. We have achieved decent Lexical F-
measures of 40.63, 50.06, 39.04, 40.94, and 43.46 
for Bengali, Hindi, Oriya, Telugu and Urdu respec-
tively without using many language specific re-
sources. 
We can also observe that the maximal F-
measure for Telugu is very low when compared to 
other languages. This is because Telugu test data 
has very few known words. 
Urdu results are comparatively low chiefly be-
cause gazetteers for numbers and measures were 
unavailable.  
We plan to extend our work by applying our 
method to other South Asian languages, and by 
using more language specific constraints and re-
sources. We also plan to incorporate semi-
supervised extraction of rules for NEs (Saha et. al, 
30
2008) and use transliteration techniques to produce 
Indian language gazetteers (Surana and Singh, 
2008). Use of character models for increasing the 
lower recalls (Shishtla et. al, 2008) is also under-
way. We also plan to enrich the Indian dependency 
tree bank (Begum et. al, 2008) by use of our NER 
system. 
 
11 Acknowledgments 
 
   We would like to thank the organizer Mr. Anil 
Kumar Singh deeply for his continuous support 
during the shared task.  
References 
B. Babych, and A. Hartley, Improving Machine transla-
tion Quality with Automatic Named Entity Recognition. 
www.mt-archive.info/EAMT-2003- Babych.pdf 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra 
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for Indian languages. In 
Proceedings of IJCNLP-2008, Hyderabad, India. 
M. Bikel Daniel, Miller Scott, Schwartz Richard and 
Weischedel Ralph. 1997. Nymble: A High Perfor 
mance Learning Name-finder. In Proceedings of the 
Fifth Conference on Applied Natural Language 
Processing. 
S. Cucerzan, and D. Yarowsky, 1999. Language inde-
pendent named entity recognition combining mor-
phological and contextual evidence. Proceedings of 
the Joint SIGDAT Conference on EMNLP and VLC. 
N. Kumar and Pushpak Bhattacharyya. 2006. Named 
Entity Recognition in Hindi using MEMM. In Tech-
nical Report, IIT Bombay, India. 
John Lafferty, Andrew McCallum and Fernando          
Pereira. 2001. Conditional Random Fields: Probabil-
istic Models for Segmenting and Labeling Sequence 
Data. Proc.   18th International Conf. on Machine 
Learning. 
D. McDonald 1996. Internal and external evidence in 
the identification and semantic categorization of 
proper names. In B. Boguraev and J. Pustejovsky, 
editors, Corpus Processing for Lexical Acquisition. 
Avinesh PVS and Karthik G. Part-Of-Speech Tagging 
and Chunking Using Conditional Random Fields and 
Transformation Based Learning. Proceedings of the 
SPSAL workshop during IJCAI?07. 
Lance Ramshaw and Mitch Marcus. Text Chunking 
Using Transformation-Based Learning. Proceedings 
of the Third Workshop on Very Large Corpora. 
S.K. Saha , S. Chatterji , S. Dandapat , S. Sarkar  and P. 
Mitra 2008. A Hybrid Approach for Named Entity 
Recognition in Indian Languages. In Proceedings of 
IJCNLP Workshop on NER for South and South East 
Asian Languages. 
Fei Sha and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. In the Proceedings 
of HLT-NAACL. 
P. Shishtla, P. Pingali , V. Varma  2008. A Character n-
gram Based Approach for Improved Recall in Indian 
Language NER. In Proceedings of IJCNLP Work-
shop on NER for South and South East Asian Lan-
guages. 
Cucerzan Silviu and Yarowsky David. 1999. Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and 
VLC. 
A. K. Singh and H. Surana  Can Corpus Based Meas-
ures be Used for Comparative Study of Languages? 
In Proceedings of Ninth Meeting of the ACL Special 
Interest Group in Computational Morphology and 
Phonology. ACL. 2007. 
R. Srihari, C. Niu and W. Li  2000. A Hybrid Approach 
for Named Entity and Sub-Type Tagging. In Pro-
ceedings of the sixth conference on Applied natural 
language processing. 
H. Surana and A. K. Singh 2008. A More Discerning 
and Adaptable Multilingual Transliteration Mecha-
nism for Indian Languages. In Proceedings of the 
Third International Joint Conference on Natural 
Language Processing. 
Charles Sutton, An Introduction to Conditional Random 
Fields for Relational Learning. 
T. Wakao , R. Gaizauskas  and Y. Wilks 1996. Evalua-
tion of an algorithm for the recognition and classifi-
cation of proper names. In Proceedings of COLING. 
 
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction. In 
ACM Transactions on Computational Logic. 
CRF++:.Yet another Toolkit. 
http://crfpp.sourceforge.net/ 
 
 
31
 32
Towards an Annotated Corpus of Discourse Relations in Hindi 
Rashmi Prasad*, Samar Husain?, Dipti Mishra Sharma? and Aravind Joshi* 
 
Abstract 
We describe our initial efforts towards 
developing a large-scale corpus of Hindi 
texts annotated with discourse relations. 
Adopting the lexically grounded approach 
of the Penn Discourse Treebank (PDTB), 
we present a preliminary analysis of 
discourse connectives in a small corpus. 
We describe how discourse connectives are 
represented in the sentence-level 
dependency annotation in Hindi, and 
discuss how the discourse annotation can 
enrich this level for research and 
applications. The ultimate goal of our work 
is to build a Hindi Discourse Relation Bank 
along the lines of the PDTB. Our work will 
also contribute to the cross-linguistic 
understanding of discourse connectives. 
1 Introduction 
An increasing interest in human language 
technologies such as textual summarization, 
question answering, natural language generation 
has recently led to the development of several 
discourse annotation projects aimed at creating 
large scale resources for natural language 
processing. One of these projects is the Penn 
Discourse Treebank (PDTB Group, 2006),1whose 
goal is to annotate the discourse relations holding 
between eventualities described in a text, for 
example causal and contrastive relations. The 
PDTB is unique in using a lexically grounded 
approach for annotation: discourse relations are 
anchored in lexical items (called ?explicit 
discourse connectives?) whenever they are 
                                                 
* University of Pennsylvania, Philadelphia, PA, USA, 
{rjprasad,joshi}@seas.upenn.edu 
? Language Technologies Research Centre, IIIT, Hyderabad, 
India, samar@research.iiit.ac.in, dipti@iiit.ac.in 
1 http://www.seas.upenn.edu/?pdtb 
 
explicitly realized in the text. For example, in (1), 
the causal relation between ?the federal 
government suspending US savings bonds sales? 
and ?Congress not lifting the ceiling on 
government debt? is expressed with the explicit 
connective ?because?.2 The two arguments of each 
connective are also annotated, and the annotations 
of both connectives and their arguments are 
recorded in terms of their text span offsets.3  
 
(1) The federal government suspended sales of U.S. 
savings bonds because Congress hasn?t lifted the 
ceiling on government debt. 
 
One of the questions that arises is how the 
PDTB style annotation can be carried over to 
languages other than English. It may prove to be a 
challenge cross-linguistically, as the guidelines and 
methodology appropriate for English may not 
apply as well or directly to other languages, 
especially when they differ greatly in syntax and 
morphology. To date, cross-linguistic 
investigations of connectives in this direction have 
been carried out for Chinese (Xue, 2005) and 
Turkish (Deniz and Webber, 2008). This paper 
explores discourse relation annotation in Hindi, a 
language with rich morphology and free word 
order. We describe our study of ?explicit 
connectives? in a small corpus of Hindi texts, 
discussing them from two perspectives. First, we 
consider the type and distribution of Hindi 
connectives, proposing to annotate a wider range 
                                                 
2 The PDTB also annotates implicit discourse relations, but 
only locally, between adjacent sentences. Annotation here 
consists of providing connectives (called ?implicit discourse 
connectives?) to express the inferred relation. Implicit 
connectives are beyond the scope of this paper, but will be 
taken up in future work. 
3 The PDTB also records the senses of the connectives, and 
each connective and its arguments are also marked for their 
attribution. Sense annotation and attribution annotation are not 
discussed in this paper. We will, of course, pursue these 
aspects in our future work concerning the building of a Hindi 
Discourse Relation Bank. 
 
The 6th Workshop on Asian Languae Resources, 2008
73
of connectives than the PDTB. Second, we 
consider how the connectives are represented in 
the Hindi sentence-level dependency annotation, in 
particular discussing how the discourse annotation 
can enrich the sentence-level structures. We also 
briefly discuss issues involved in aligning the 
discourse and sentence-level annotations.  
Section 2 provides a brief description of Hindi 
word order and morphology. In Section 3, we 
present our study of the explicit connectives 
identified in our texts, discussing them in light of 
the PDTB. Section 4 describes how connectives 
are represented in the sentence-level dependency 
annotation in Hindi. Finally, Section 5 concludes 
with a summary and future work. 
2 Brief Overview of Hindi Syntax and 
Morphology 
Hindi is a free word order language with SOV as 
the default order. This can be seen in (2), where 
(2a) shows the constituents in the default order, 
and the remaining examples show some of the 
word order variants of (2a). 
 
(2)  a. malaya       nao         samaIr         kao     iktaba    dI .  
           malay   ERG  sameer    DAT  book   gave 
           ?Malay gave the book to Sameer? (S-IO-DO-V)4 
       b. malaya nao iktaba samaIr kao dI. (S-DO-IO-V) 
       c. samaIr kao malaya nao iktaba dI. (IO-S-DO-V) 
       d. samaIr kao iktaba malaya nao dI. (IO-DO-S-V) 
       e. iktaba malaya nao samaIr kao dI. (DO-S-IO-V) 
        f. iktaba samaIr kao malaya nao dI.  (DO-IO-S-V) 
 
Hindi also has a rich case marking system, 
although case marking is not obligatory. For 
example, in (2), while the subject and indirect 
object are explicitly for the ergative (ERG) and 
dative (DAT) cases, the direct object is unmarked 
for the accusative. 
3 Discourse Connectives in Hindi 
Given the lexically grounded approach adopted for 
discourse annotation, the first question that arises 
is how to identify discourse connectives in Hindi. 
Unlike the case of the English connectives in the 
PDTB, there are no resources that alone or together 
provide an exhaustive list of connectives in the 
                                                 
4 S=Subject; IO=Indirect Object; DO=Direct Object; 
V=Verb; ERG=Ergative; DAT=Dative 
language. We did try to create a list from our own 
knowledge of the language and grammar, and also 
by translating the list of English connectives in the 
PDTB. However, when we started looking at real 
data, this list proved to be incomplete. For 
example, we discovered that the form of the 
complementizer ?ik? also functions as a temporal 
subordinator, as in (3). 
 
(3) [ vah  baalaTI      ko   gaMdo      panaI      sao      ApnaI    caaOklaoT  
      [he   bucket  of  dirty  water  from  his     chocolates 
      inakalanao          hI     vaalaa    qaa]    ik    {]sakI  mammaI       nao  
      taking-out  just doing was]  that  {his   mother ERG 
      ]sao    raok idyaa } 
      him stop did} 
?He was just going to take out the chocolates from 
the dirty water in the bucket when his mother stopped 
him.? 
 
The method of collecting connectives will 
therefore necessarily involve ?discovery during 
annotation?. However, we wanted to get some 
initial ideas about what kinds of connectives were 
likely to occur in real text, and to this end, we 
looked at 9 short stories with approximately 8000 
words. Our goal here is to develop an initial set of 
guidelines for annotation, which will be done on 
the same corpus on which the sentence-level 
dependency annotation is being carried out (see 
Section 4). Table 1 provides the full set of 
connectives we found in our texts, grouped by 
syntactic type. The first four columns give the 
syntactic grouping, the Hindi connective 
expressions, the English gloss, and the English 
equivalent expressions, respectively. The last 
column gives the number of occurrences we found 
of each expression. In the rest of this section, we 
describe the function and distribution of discourse 
connectives in Hindi based on our texts. In the 
discussion, we have noted our points of departure 
from the PDTB where applicable, both with 
respect to the types of relations being annotated as 
well as with respect to terminology. For argument 
naming, we use the PDTB convention: the clause 
with which the connective is syntactically 
associated is called Arg2 and the other clause is 
called Arg1. Two special conventions are followed 
for paired connectives, which we describe below. 
In all Hindi examples in this paper, Arg1 is 
enclosed in square brackets and Arg2 is in braces. 
The 6th Workshop on Asian Languae Resources, 2008
74
Connective Type Hindi Gloss English Num 
Sub. Conj. @yaaoMik 
(@yaaoM)ik..[salaIe 
(Agar|yadI)..tba|tao 
(jaba).. tba|tao 
jaba tk.. tba tk (ko ilae)  
jaOsao hI..(tao)  
[tnaa|eosaa..kI  
taik  
ik 
why-that 
(why)-that..this-for  
(if)..then 
(when)..then  
when till..then till (of for)  
as just..(then)  
so|such..that  
so-that  
that  
 
because  
because 
if..(then)  
when  
until  
as soon as  
so that  
so that  
when
  
2 
3 
15 
50 
2 
5 
12 
1 
5 
Sentential Relatives ijasasao 
jaao 
ijasako karNa 
which-with 
which 
which-of reason 
because of which 
because of which 
because of which 
5 
1 
1 
Subordinator pr 
(-kr|-ko|krko) 
samaya 
hue 
ko baad 
sao 
ko phlao 
ko ilae 
maoM 
ko karNa 
upon 
(do) 
time 
happening 
of later 
with 
of before 
of for 
in 
of reason
 
upon 
after|while 
while 
while 
after 
due to 
before 
in order to 
while 
because of
 
9 
111 
1 
28 
3 
1 
1 
4 
1 
3 
Coord. Conj. laoikna|pr|prntu 
AaOr|tqaa  
yaa  
yaaoM tao..pr  
naa kovala..balaik  
but  
and  
or  
such TOP..but  
not only..but
 
but  
and  
or  
but  
not only..but
 
51 
117 
2 
2 
1 
Adverbial tba  
baad maoM  
ifr  
[saIilae  
nahIM tao  
tBaI tao  
saao  
vahI|yahI nahIM  
then  
later in  
then  
this-for  
not then  
then-only TOP  
so  
that|this-only not  
then  
later  
then  
that is why  
otherwise  
that is why  
so  
not only that  
2 
5 
4 
7 
5 
1 
10 
1 
TOTAL    472 
     Table 1: A Partial List of Discourse Connectives in Hindi. Parentheses are used for optional  
     elements; ?|? is used for alternating elements; TOP = topic marker.
 
3.1 Types of Discourse Connectives 
3.1.1 Subordinating Conjunctions 
Finite adverbial subordinate clauses are 
introduced by independent lexical items called 
?subordinating conjunctions?, such as @yaaoMik 
(?because?), as in (4), and they typically occur as 
right or left attached to the main clause. 
 
(4) [maOM   [sa  saBaI    Qana        kao       rajya      ko   baadSaah  
      [I  this  all   wealth ACC kingdom of  king 
      kao      do      dota],    @yaaoMik       {vahI           samast 
     
 
 
  DAT give would], why-that {he-EMPH all 
      QartI     kI  sampda      ka  svaamaI   hO} 
      earth  of   wealth  of   lord  is} 
?I would give all this wealth to the king, because he 
alone is the lord of this whole world?s wealth.? 
 
As the first group in Table 1 shows, 
subordinating conjunctions in Hindi often come 
paired, with one element in the main clause and 
the other in the subordinate clause (Ex.5). One 
of these elements can also be implicit (Ex.6),  
The 6th Workshop on Asian Languae Resources, 2008
75
and in our texts, this was most often the 
subordinate clause element. 
 
(5)  @yaaoMik       {yah   tumharI  ja,maIna   pr   imalaa     hO},      [sailae 
       because {this  your  land  on  found  has}, this-for 
       [[sa       Qana      pr  tumhara  AiQakar  hO] 
       [this treasure on  your  right  is] 
?Because this was found on your land, you have the 
right to this treasure.? 
 
(6)  []saka  vaSa       calata]     tao    {vah   ]sao   Gar        sao 
       [her   power  walk] then  {she  it   home  from 
       baahr  inakala  dotI}  
       out  take  would} 
?Had it been in her power, she would have banished 
it from the house.? 
 
When both elements of the paired connective are 
explicit, their text spans must be selected 
discontinuously. The main clause argument is 
called Arg1 and the subordinate clause 
argument, Arg2. 
Subordinating conjunctions, whether single or 
paired, can occur in non-initial positions in their 
clause. However, this word order variability is 
not completely unconstrained. First, not all 
conjunctions display this freedom. For example, 
while ?jaba? (?when?) can be clause-medial (Ex. 
7), ?@yaaoMik? (?because?) cannot. Second, when the 
main clause precedes the subordinate clause, the 
main clause element, if explicit, cannot appear 
clause-initially at all. Consider the causal ?@yaaoMik.. 
[salaIe? (Ex.5), which represents the subordinate-
main clause order. In the reverse order, the 
explicit main clause ?[salaIe? (Ex.8) appears 
clause medially. Placing this element in clause-
initial position is not possible. 
 
(7) {lakD,haro         kI  p%naI      kao}    jaba    {yah 
      {woodcutter of  wife DAT} when {this 
       maalaUma            pD,a  ik     [sa  icaiDyaa  ko   karNa,  
       knowledge put  that this bird   of   reason 
       kama     CaoD,kr    Gar       Aa   gayaa     hO}   tao      [vah 
       work leaving home come went is} then  [she 
       ]sa    pr       barsa        pD,I]. 
       him on  anger-rain put] 
 ?When the woodcutter?s wife found out that he had 
left his work and come home to care for the bird, she 
raged at him.? 
 
(8)  [. . . pr  icaraga   kI  ba%tI    ]sakanaa  yaa   daohrI  
       [. . .but lamp of  light  light    or  another 
       ba%tI    lagaanaa]         Saayad [sailae         []icat nahIM  
       light  putting] perhaps this-for  [appropriate not 
       samaJato          qao]  ik     {tola  ka  Apvyaya   haogaa}. 
       Consider did]  that  {oil  of  waste   be-FUT}. 
?. . . but he did not consider it appropriate to light the 
lamp repeatedly or light another lamp, perhaps 
because it would be a waste of oil.? 
3.1.2 Sentential Relative Pronouns 
Since discourse relations are defined as holding 
between eventualities, we have also identified 
relations that are expressed syntactically as 
relative pronouns in sentential relative clauses, 
which modify the main clause verb denoting an 
eventuality, rather than some entity denoting 
noun phrase. For example, in (9), a 
result/purpose relation is conveyed between ?the 
man?s rushing home? and ?the bird being taken 
care of?, and we believe that this relation 
between the eventualities should be captured 
despite it?s syntactic realization as the relative 
pronoun ?ijasasao? (?because of which/so that?). (10) 
gives an example of a modified relative 
pronoun. 
 
(9) [saara  kama     caaoD,kr     vah  ]sa    baImaar   icaiD,yaa 
      [all  work  leaving  he  that  sick  bird 
      kao       ]zakr       dbaa     Gar     kI    Aaor       Baagaa], 
      ACC picking-up fast home of direction ran], 
      ijasasao             {]saka   sahI       [laaja   ikyaa  jaa    sako} 
      from-which {her    proper  care  do   go  able} 
?Leaving all his work, he picked up the bird and ran 
home very fast, so that the bird could be given proper 
care.? 
 
(10) [}M^TaoM       ko   hr     baar    kdma  rKnao       pr 
        [camels of  every time step keeping upon 
        icaiD,yaao M ko  isar     Aapsa          maoM   tqaa   }M^T      kI 
        birds of  head each-other in and camels of 
        gardna   sao    Tkra            rho     qao]   ijasako karNa 
        neck with hit-against be had] of-which reason 
       {]na     pixayaaoM   kI   drdBarI      caIKoM        inakla 
       {those birds  of   painful  screams come-out  
         rhI   qaIM}. 
         be had} 
?With each step of the camels, the birds heads were 
hitting against each other as well as with the camels? 
necks because of which the birds were screaming 
painfully.? 
3.1.3 Subordinators 
In contrast to the subordinating conjunctions, 
elements introducing non-finite subordinate 
clauses are called ?subordinators?. Unlike 
The 6th Workshop on Asian Languae Resources, 2008
76
English, where certain non-finite subordinate 
clauses, called ?free adjuncts?, appear without 
any overt marking so that their relationship with 
the main clause is unspecified, Hindi non-finite 
subordinate clauses almost always appear with 
overt marking. However, also unlike English, 
where the same elements may introduce both 
finite and non-finite clauses (cf. After leaving, 
she caught the bus vs. After she left, she caught 
the bus), different sets of elements are used in 
Hindi. In fact, as can be seen in the subordinator 
group in Table 1, the non-finite clause markers 
are either postpositions (Ex.11), particles 
following verbal participles (Ex.12), or suffixes 
marking serial verbs (Ex.13). 
 
(11) {mammaI          ko     manaa        krnao}      ko karNa     [ramaU 
        {mummy of  warning  doing} of reason [Ramu 
         qaaoD,I  qaaoD,I    caaOklaoT      baD,o     AnaMd        ko   saaqa 
         little little chocolate big  pleasure  of  with  
         Ka  rha      qaa]. 
         eat being be] 
?Because of his mother?s warning, Ramu was eating 
bits of chocolate with a lot of pleasure.? 
 
(12) . . . AaOr    {Kolato}      hue               [yah    BaUla     jaata hO 
        . . . and  {playing} happening [this forget go is 
         ik    yaid  ]saka  ima~        BaI    Apnao  iKlaaOnao    kao 
         that if    his   friends also their  toys     to 
         ]sao     haqa   nahIM   lagaanao         dota,  tao      ]sao 
         him hand not   touching did,  then  he  
           iktnaa        baura     lagata] 
         how-much bad   feel] 
?. . . and while playing, he forgets that if his friends 
too didn?t let him touch their toys, then how bad he 
would feel.? 
 
(13) {ApnaI  p%naI     sao     yah       sauna}kr      [lakD,hara  
        {self  wife from this   listen}-do  [woodcutter 
         bahut    duKI       huAa] 
         much sad  became] 
?Upon hearing this from his wife, the woodcutter 
became very sad.? 
 
While subordinators constitute a frequently-
used way to mark discourse relations, their 
annotation raises at least two difficult problems, 
both of which have implications for the 
reliability of annotation. The first is that these 
markers are used for marking both argument 
clauses and adjunct clauses, so that annotators 
would be required to make difficult decisions for 
distinguishing them: in the former case, the 
marker would not be regarded as a connective, 
while in the latter case, it would. Second, the 
clauses marked by these connectives often seem 
to be semantically weak. This is especially true 
of verbal participles, which are nonfinite verb 
appearing in a modifying relation with another 
finite verb. Whereas in some cases (Ex.12-13) 
the two verbs are perceived as each projecting 
?two distinct events? between which some 
discourse relation can be said to exist, in other 
cases (Ex.14), the two verbs seem to project two 
distinct actions but as part of a ?single complex 
event? (Verma, 1993). These judgments can be 
very subtle, however, and our final decision on 
whether to annotate such constructions will be 
made after some initial annotation and 
evaluation. 
 
(14) {doKto        hI         doKto      saba  baOla             Baagato } 
        {looking EMPH looking all buffalos running} 
         hue              [gaaoSaalaa   phu^Mca     gae] 
         happening [shed   reach  did] 
?Within seconds all the buffalos came running to the 
shed.? 
 
The naming convention for the arguments of 
subordinators is the same as for the 
subordinating conjunctions: the clause 
associated with the subordinator is called Arg2 
while its matrix clause is called Arg1. 
Unlike subordinating conjunctions, 
subordinators do not come paired and they can 
only appear clause-finally. Clause order, while 
not fixed, is restricted in that the nonfinite 
subordinate clause can appear either before the 
main clause or embedded in it, but never after 
the main clause.  
3.1.4 Coordinating Conjunctions 
Coordinating conjunctions in Hindi are found in 
both inter-sentential (Ex.15) and intra-sentential 
(Ex.16) contexts, they always appear as 
independent elements, and they almost always 
appear clause-initially. 5  For these connectives, 
                                                 
5 While the contrastive connectives  ?pr?, ?prntU? appear only 
clause-initially, it seems possible for the contrastive ?laoikna? 
to appear clause-medially, suggesting that these two types 
may correspond to the English ?but? and ?however?, 
respectively. However, we did not find any examples of 
clause-medial ?laoikna? in our texts, and this behavior will 
have to be verified with further annotation. 
The 6th Workshop on Asian Languae Resources, 2008
77
the first clause is called Arg1 and the second, 
Arg2. 
 
(15) [jaba      vah  laaOTta    tao       gaa-gaakr          ]saka  mana 
         [when he  return then sing-singing   his  mind 
         KuSa      kr    dotI].   laoikna  {]sakI p%naI   kao      vah 
         happy do gave].   But   {his wife DAT  the  
         icaiD,yaa   fUTI    AaM^K  nahIM    sauhatI   qaI}. 
         bird    torn  eye  not   bear  did} 
?Upon his return, she would make him happy by 
singing. But his wife could not tolerate the bird even 
a little bit.? 
 
(16) [ tBaI          drvaaja,a      Kulaa]  AaOr    {maalaikna  Aa 
        [then-only door opened]  and  {wife  come 
         ga[- }. 
        went} 
?Just then the door opened and the wife came in.? 
 
We also recognize paired coordinating 
conjunctions, such as ?naa kovala..balaik? (See Table 
1). The argument naming convention for these is 
the same as for the single conjunctions. 
3.1.5 Discourse Adverbials 
Discourse adverbials in Hindi modify their clau- 
ses as independent elements, and some of these 
are free to appear in non-initial positions in the 
clause. Example (17) gives an example of the 
consequence adverb, ?saao?. The Arg2 of discourse 
adverbials is the clause they modify, whereas 
Arg1 is the other argument. 
 
(17) [icaiD,yaa  jabaana      kT   jaanao     AaOr   maalaikna  ko  eosao 
        [bird    tongue  cut  going  and  wife  of  this  
        vyavahar       sao      Dr   ga[-     qaI]. saao    {vah     iksaI 
        behavior with fear go  had]. So  {she  some 
        trh        ]D,kr    calaI        ga[-}. 
        manner flying  walk    went}. 
?The bird was scared due to her tongue being cut and 
because of the wife?s behavior. So she somehow flew 
away.? 
 
As with the PDTB, one of our goals with the 
Hindi discourse annotation is to explore the 
structural distance of Arg1 from the discourse 
adverbial. If the Arg1 clause is found to be snon-
adjacent to the connective and the Arg2 clause, 
it may suggest that adverbials in Hindi behave 
anaphorically. In the texts we looked at, we did 
not find any instances of non-adjacent Arg1s. 
Addtional annotation will provide further 
evidence in this regard. 
4 Hindi Sentence-level Annotation 
andDiscourse Connectives 
The sentence-level annotation task in Hindi is 
an ongoing effort which aims to come up with a 
dependency annotated treebank for the NLP/CL 
community working on Indian languages. 
Presently a million word Hindi corpus is being 
manually annotated (Begum et al, 2008). The 
dependency annotation is being done on top of 
the corpus which has already been marked for 
POS tag and chunk information. The scheme has 
28 tags which capture various dependency 
relations. These relations are largely inspired by 
the Paninian grammatical framework. Given 
below are some relations, reflecting the 
argument structure of the verb. 
 
a) kta- (agent) (k1) 
b) kma- (theme) (k2) 
c) krNa (instrument) (k3) 
d) samp`dana sampradaan (recipient) (k4) 
e) Apadana (source) (k5) 
f) AiQakrNa (location) (k7) 
 
Figure 1 shows how Examples (2a-f) are 
represented in the framework. Note that agent 
and theme are rough translations for ?kta-? and 
?kma-? respectively. Unlike thematic roles, these 
relations are not purely semantic, and are 
motivated not only through verbal semantics but 
also through vibhaktis (postpositions) and TAM 
(Tense, aspect and modality) markers (Bharati et 
al., 1995). The relations are therefore syntactico-
semantic, and unlike thematic roles there is a 
greater binding between these relations and the 
syntactic cues. 
 
 
k1 k4 k2
 
 
 
Figure 1: Dependency Diagram for Example (2) 
Some discourse relations that we have identified 
are already clearly represented in the sentence-
level annotation. But for those that aren?t, the 
   dI 
   malaya    samaIr    iktaba 
The 6th Workshop on Asian Languae Resources, 2008
78
discourse level annotations will enrich the 
sentence-level. In the rest of this section, we 
discuss the representation of the different types 
of connectives at the sentence level, and discuss 
how the discourse annotation will add to the 
information present in the dependency 
structures. 
 
Subordinating Conjunctions Subordinating 
conjunctions are lexically represented in the 
dependency tree, taking the subordinating clause 
as their dependents while themselves attaching 
to the main verb (the root of the tree). Figure 2 
shows the dependency tree for Example (4) 
containing the subordinating conjunction ?@yaaoMik?. 
Note that the edge between the connective and 
the main verb gives us the causal relation 
between the two clauses, the relation label being 
?rh? (relation hetu ?cause?). Thus, the discourse 
level can be taken to be completely represented 
at the sentence-level. 
 
hE
k1 k2 k4 rh
ccofr6
k1sk1
r6
r6 
 
Figure 2: Dependency Tree for Subordinating 
Conjunction in Example (4) 
 
Paired Subordinating Conjunctions Unlike 
Example (4), however, the analysis for the 
paired connective in Example (5), given in 
Figure 3, is insufficient. Despite the lexical 
representation of the connective in the tree, the 
correct interpretation of the paired conjunction 
and the clauses which it relates is only possible 
at the discourse level. In particular, the 
dependencies don?t show that ?@yaaoMik? and ?[salaIe? 
are two parts of the same connective, expressing 
a single relation and taking the same two 
arguments. Thus, the discourse annotation will 
be able to provide the appropriate argument 
structure and semantics for these paired 
connectives.  
 
 
ccof
k2 k1 rh
ccof
k7p
r6
k1
 
 
Figure 3: Dependency Tree for Paired 
Subordinating Conjunction in Example (5) 
 
Subordinators As mentioned earlier, Hindi 
nonfinite subordinate clauses almost always 
appear with overt marking. But unlike the 
subordinating conjunctions, subordinators are 
not lexically represented in the dependency 
trees. Figure 4 gives the dependency 
representation for Example (11) containing a 
postposition subordinator ?ko karNa?, which relates 
the main and subordinate clauses causally. As 
the figure shows, while the causal relation label 
(?rh?) appears on the edge between the main and 
subordinate verbs, the subordinator itself is not 
lexically represented as the mediator of this 
relation. The lexically grounded annotation at 
the discourse level will thus provide the textual 
anchors of such relations, enriching the 
dependency representation. Furthermore, while 
many of the subordinators in Table 1 are fully 
specified in the dependency trees for the 
semantic relation they denote (e.g., ?pr? and ?maoM? 
marked as the ?k7t? (location in time) relation, 
and ?ko karNa? and ?sao? marked as the ?rh? 
(cause/reason) relation), others, like the particle 
?hue? are underspecified for their semantics, being 
marked only as ?vmod? (verbal modifier). The 
discourse-level annotation will thus be the 
source for the semantics of these subordinators. 
 
Coordinating Conjunctions Coordinating 
conjunctions at the sentence level anchor the 
root of the dependency tree. Figure 5 shows the 
 do dotao oo oo o   
   maOMOM O MO M   Qana   baadSaah    @yaaoMikoM oMoM   
rajya  
   vahI   svaamaI  
  sampda  
   QartI  
   [sailae 
    
AiQakar hOO OO 
    Qana  tuuuumhara  @yaaoMikoMoMo M   
  imalaa hOO OO 
    yah 
  
 ja,maIna, ,,  
 
  tumharIuuu  
The 6th Workshop on Asian Languae Resources, 2008
79
dependency representation of Example (16) 
containing a coordinating conjunction. 
 
 
rh k1 k2
vmod
k1
 
 
Figure 4: Dependency Tree for Subordinator in 
Example (11) 
 
 
ccof ccof
k7t k1 k1
 
 
Figure 5: Dependency Tree for Coordinating 
Conjunction in Example (16) 
 
While the sentence-level dependency analysis 
here is similar to the one we get at the discourse 
level, the semantics of these conjunctions are 
again underspecified, being all marked as ?ccof?, 
and can be obtained from the discourse level. 
 
Discourse Adverbials Like subordinating 
conjunctions, discourse adverbials are 
represented lexically in the dependency tree. 
They are attached to the verb of their clause as 
its child node and their denoted semantic 
relation is specified clearly. This can be seen 
with the temporal adverb ?tBaI? (?then-only?) and 
its semantic label ?k7t? in Figure 5. At the same 
time, since the Arg1 discourse argument of 
adverbials is most often in the prior context, the 
discourse annotation will enrich the semantics of 
these connectives by providing the Arg1 
argument. 
5 Summary and Future Work 
In this paper, we have described our study of 
discourse connectives in a small corpus of Hindi 
texts in an effort towards developing an 
annotated corpus of discourse relations in Hindi. 
Adopting the lexically grounded approach of the 
Penn Discourse Treebank, we have identified a 
wide range of connectives, analyzing their types 
and distributions, and discussing some of the 
issues involved in the annotation. We also 
described the representation of the connectives 
in the sentence-level dependency annotation 
being carried out independently for Hindi, and 
discussed how the discourse annotations can 
enrich the information provided at the sentence 
level. While we focused on explicit connectives 
in this paper, future work will investigate the 
annotation of implicit connectives, the semantic 
classification of connectives, and the attribution 
of connectives and their arguments. 
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. In Proceedings of IJCNLP-2008. 
Hyderabad, India. 
Akshar Bharati, Vineet Chaitanya, and Rajeev 
Sangal. 1995. Natural Language Processing: A 
Paninian Perspective. Prentice Hall of India. 
http://ltrc.iiit.ac.in/downloads/nlpbook/nlppanini.p
df. 
Manindra K. Verma (ed.). 1993. Complex Predicates 
in South Asian Languages. New Delhi: Manohar. 
The PDTB-Group. 2006. The Penn Discourse 
TreeBank 1.0 Annotation Manual. Technical 
Report IRCS-06-01, IRCS, University of 
Pennsylvania. 
Bonnie Webber, Aravind Joshi, Matthew Stone, and 
Alistair Knott. 2003. Anaphora and discourse 
structure. Computational Linguistics, 29(4):545?
587. 
Nianwen Xue. 2005. Annotating Discourse 
Connectives in the Chinese Treebank. In 
Proceedings of the ACL Workshop on Frontiers in 
Corpus Annotation II: Pie in the Sky. Ann Arbor, 
Michigan.  
Deniz Zeyrek and Bonnie Webber. 2008. A 
Discourse Resource for Turkish: Annotating 
Discourse Connectives in the METU Corpus. In 
Proceedings of IJCNLP-2008. Hyderabad, India. 
 
 
  Ka rha qaa 
manaa krnaoo oo rama   caaOO OOklaoTooo    AanaMdMMM  
 mammaI 
  AaOrOOO  
 Kulaauuu   Aa ga[-- -- 
maalaikna  drvaaja,a, ,,   tBaI  
The 6th Workshop on Asian Languae Resources, 2008
80
AnnCorra : Building Tree-banks in  Indian Languages
Akshar Bharati
Rajeev Sangal
Vineet Chaitanya
Amba Kulkarni
Dipti Misra Sharma
International Institute of Information Technology
Hyderabad, india
{sangal, vc, amba, dipti}@iiit.net
K.V. Ramakrishnamacharyulu
Rashtirya Sanskrit Vidyapeetha, Tirupati, India
kvrk@sansknet.org
ABSTRACT
This paper describes a dependency based
tagging scheme  for creating tree banks for
Indian languages.  The scheme has been so
designed that  it is comprehensive,  easy to use
with linear notation and economical in typing
effort.  It is based on Paninian grammatical
model.
1.BACKGROUND
The name AnnCorra, shortened for
"Annotated Corpora", is for an electronic
lexical resource of annotated corpora. The
purpose behind this effort is to fill the
lacuna in such resources for Indian
languages. It will be an important resource
for the development of Indian language
parsers, machine learning of grammars,
lakshancharts (discrimination nets for
sense disambiguation) and a host of other
such tools.
 2. AIMS AND OBJECTIVE
      The aim of the project is to :
- develop a generalised linear
            syntacto- semantic tag scheme  for
            all Indian  languages
- annotate training corpus for all
             Indian  languages
      -     develop parallel tree-banks for all
             Indian languages
To fulfill the above aim - a marathon task
-  a collaborative model has been
concieved. Any collaborative model
implies involvement of several people
with varying levels of expertise. This case,
becomes further complicated as the tag
scheme to be designed has to be equally
efficient for all the Indian languages.
These languages, though quite similar,
are not identical in their syntactic
structures.  Thus the tag scheme demands
the following properties  :-
-  comprehensive enough to capture
   various sysntactic relations across
    languages.
     -  simple enough for anyone, with some
        background in linguistics,  to use.
     -  economical  in typing effort (the
        corpus has to be manually
        annotated).
3. AN ILLUSTRATION
The task can be better understood with the
help of an  illustration. Look at the
following sentence  from Hindi
0:: rAma  ne            moHana   ko
     'Rama'  'ErgPostP'  'Mohan'   'PostP'
      nIlI kitAba dI
     'blue' 'book'  'gave'
   'Rama gave the blue book to Mohan.'
Tree-1 is a representation of the above
verb, argument relationship within the
various constituents of sentence  0 -
                             dI
                 -------------------------
                 |            |                   |
            k1 |        k4|               k2|
                 |            |                   |
      rAma_ne  moHana_ko   kitAba
                                                 |
                                                 |nmod
                                                 |
                                              nIlI
                     Tree-1
Since the input for tagging is a text corpus
and the marking has to be done manually,
the tagging scheme is linearly designed.
Therefore, Sentence 0 will be marked as
follows -
rAma_ne/k1 moHana_ko/k4  [nIlI
'Ram postp'    Mohan postp'    'blue '
kitAba]/k2 dI::v
?book?        'gave'
The markings here represent
-     'di' (?give?) is the verb node
- ?rAma_ne' is  the 'karta' or  'agent'
(k1)
 
    of the verb 'dI',
- 'moHana_ko' is 'sampraadana' or
 
     'beneficiary' (k4) of verb 'dI' (?give?)
- '[nIlI kitAba]' ? (blue book) a noun
phrase - is the  'karma' or 'object' (k2)
of  the verb.
The elements joined by an underscore
represent one unit. Postpositions which
are separated by white space in the written
texts are actually the inflections of the
preceding noun or verb units. Therefore,
they are conjoined.
The modifier-modified elements are
paranthesised within square brackets.
Tags showing the name of the ARC (or
branch) are marked by '/'   immediately
after the constituent they relate to. '/' is
followed by the appropriate tagname.
Thus '/' specifies a relationship of a word
or constituent with another word or
constituent. In this case it is the
relationship of verb 'dI' with the other
elements in the sentence.
Tags denoting a type of node are  marked
by '::'.   '::v' indicates that 'dI' is a verbal
node.
The idea here is to mark only the specific
grammatical information. Certain
DEFAULT CONVENTIONS are left
unmarked. For example, the adjective 'nIlI'
(?blue?) of  'kitAba' (?book) has been left
unmarked in the above example since
normally noun modifiers precede the noun
they modify (adjectives precede nouns).
Such DEFAULT CONVENTIONS save
unnecessary typing effort.
4. GRAMMATICAL MODEL
It was quite natural to use Paninian
grammatical model for sentence analysis
 ( hence the tagnames) because :-
1) Paninian grammatical model is based
 
     on the analysis of an Indian  language
 
     (Sanskrit) it can deal better with the
 
    type of  constructions Indian languages
 
     have.
 
2) The model not only offers a
      mechanism  for SYNTACTIC analysis
      but also incorporates the SEMANTIC
      information (nowadays called
      dependency analysis). Thus making
      the   relationships more transparent.
      (For  details refer Bharati (1995).)
Following tags (most of which are  based
on Paninian grammatical model) have
been used in the above example.
k1 : kartaa (subject or agent)
k2 : karma (object)
k4 : sampradaana (beneficiary)
v  : kriyaa (verb)
Obviously the task is not an easy one.
Standardization of these tags will take
some time. Issues, while deciding the tags,
are many. Some examples are illustrated
below to show the kind of structures
which the linear tagging scheme will have
to deal with.
4.1. Multiple Verb Sentences
To mark the nouns-verb relations with the
above tags in single verb sentences is a
simple task. However, consider the
following sentence with two verbs :-
    1:   rAma ne   khAnA  khAkara
          am' 'postp' 'food'    'having_eaten'
           pAnI    pIyA
           'water'  'drank'
          `Ram drank water after eating the
           food.`
Sentence 1 has more than two verbs - one
non-finite (khAkara) and one finite
(piyA). The finite verb is the main verb.
Noun 'khAnA' is the object of verb
'khAkara', whereas noun 'pAnI' is the
object of verb 'piyA'. 'k2' is the tag for
object relation in our tagging scheme. Co-
indexing becomes the obvious solution for
such multiple relations.   Since there are
two verbs the tagging scheme allows them
to be named as   'i' and 'j' (using notation 'i'
and 'j'). By default 'i' refers to the main
verb and any successive verb by other
characters ('j' in the present case):
 rAma_ne      khAnA   khAkara::vkr:j
  'Ram_postp'  'food'      'having_eaten:j'
   pAnI   piyA::v:i
   'water' 'drank:i'
    This provides the facility to mark every
noun verb relationship.
rAma_ne/k1>i   khAnA/k2>j
khAkara::vkr:j   pAnI/k2>i piyA::v:i
Fortunately, there is no need to mark it so
"heavily". A number of notations can be
left out, and the DEFAULT rules tell us
how to interpret such "abbreviated'
annotation. Thus, for the above sentence,
the following annotation is sufficient and
is completely equivalent to the above :
     rAma_ne/k1   khAnA/k2
khAkara::vkr:j   pAnI/k2   piyA::v
Even though there are two verbs, there is
no need to name the verbs and refer to
them. Two default rules help us achieve
such brevity (without any ambiguity) :
(1) karta or k1 kaaraka always attaches to
 
     the last verb in a  sentence (Thus
 
    'rAma_ne/k1' attaches to the verb at
 
     the   end).
(2) all other kaarakas except k1, attach to
      the nearest verb on the right. Thus
      'khAnA/k2' attaches to 'khAkara' and
      'pAnI/k2' attaches to 'piyA', their
      respective nearest verbs on the right.
4.2. Compound Units
Sometimes two words combine together to
form a unit which has its own demands
and modifiers, not derivable from its parts.
For example, a noun and verb join
together to operate as a single unit,
namely as a verb. In the sentence 'rAma
(Rama) ne (postp) snAna(bath) kiyA
(did)',  'snAna'   and 'kiyA' together stand
for a verb 'snAna+kiyA' (bathed). Such
verbal compounds are like any other verb
having their own kaarakas.This sentence
would be marked as follows :
    rAma_ne/k1   snAna::v+ kiyA::v-
    'Ram_postp'   'bath+'       'did-'
     `Ram took a bath`
A 'v+' or a 'v-' indicates that the word
'snAna' or 'kiyA' are parts of a whole  (a
verb in this case). Taken together they
function as a single verb unit.  Such a
device which may appear to be more
powerful was needed to mark the 'single
unitness' of parts which may appear
separately in a sentence. Thus, the above
notation  allows even distant words to be
treated as a  single compound. Such
occurrences are fairly common in all
Indian languages as illustrated in the
following example from Hindi :
snAna::v+ to        mEMne/k1
 'bath'      'emph'    'I_erg'
subaHa_HI           kara_liyA_thA::v-
'morning_emph'     'had_done'
I had bathed (taken a bath) in the morning
itself.
'+'and ' - ' help in marking this relation
explicitly. (a more detail description of the
notation in 5.1)
4.3. Embedded Sentence
Tags are also designed to mark the
relations within  a complex sentence.
Consider the example below where a
complete sentence (having verb  'piyA'
(drank)) is a kaaraka of the main verb
'kaHA' (said).
moHana    ne        kaHA  ki      {rAma
'Mohan'  'postp' 'said'  'that ' {'Rama'
ne         pAnI     khAnA    khAkara
'postp' 'water' 'food'    'having eaten'
 piyA}.
'drank}
     (Mohan said that Ram drank water
after having eaten the food)
The embedded sentence can be first
marked as follows -
     --------- {rAma_ne/k1 pAnI/k2>j
khAnA/k2 khAkara::vkr piyA::v:j}::s.
The whole embedded sentence is the
'karma'  (object) or k2 of 'piyA' (drank):
The  relation  of the embedded sentence
relation as the object of the main verb is
co-indexed in the following way :-
 moHana_ne        kaHA::v:i      ki
  'Mohan_postp'   'said'             'that'
 rAma_ne/k1     pAnI/k2>j    khAnA/k2
'Rama_postp'         'water'      'food'
khAkara::vkr     piyA::v:j::s/k2>i
'having_eaten'    'drank'
Thus the device of naming the elements
and co-indexing them with their respective
arguments can be used most effectively.
5. TAGGING SCHEME
The tagging scheme contains : notations,
defaults, and  tagsets.
5.1. NOTATION
Certain special symbols such as double
colon,underscore, paranthesis etc. are
introduced first. Two sets of tags have
been provided (to mark the crucial ARC
and node information). However,  apart
from these symbols and tags, some special
notation is required to explicitly mark
certain disjointed, scattered and missing
elements in a sentence. Following
notation is adopted for marking these
elements :-
5.1. 1.   X+ ... X- : disjointed elements
As shown above (4.2),  when a single
lexical unit composed of more  than one
elements is  separated by other intervening
lexical units, its 'oneness' is expressed by
using '+' on the first element in the linear
order and '-' on the second element. '+'
indicates to look ahead for the other part
till you find an element with '-'. '-'
suggests, 'an element marked '+' is left
behind, to which it should get itself
attached'.
    Example - Verb 'snAna_karanA' (to
bathe) in Hindi can occur  disjointedly
 snAna   to         mEMne kiyA_thA
 'bath'     'emph'   'I'          'did'
para    phira     gaMdA   Ho_gayA
'but'    'again'    'dirty'       'became'
 `Bathe I did , but got dirty again.'
'snAna_karanA' is one verb unit in Hindi.
But its two components 'snAna' and
'karanA' can occur separately. Notation
'X+....X-' can capture the 'oneness' of these
two elements. So 'snAna.karanA'
(?bathe?) in the above sentence would be
marked as follows :
 snAna::v+   to         mEMne
    'bath'        'emph'   'I'
 kiyA_thA::v-    para   phira    gaMdA
  'did'                  but'    'again'    'dirty'
 Ho_gayA
 'became'
Another  example of  'scattered elements'
is  'agara .... to' construction of  Hindi.
 agara   tuma    kaHate   to         mEM
   'if'       'you'     'said'      'then'     'I'
A_ jAtA
'would_have_come'
   `Had you asked I would have come'
?agara' and 'to' together give the
'conditionality' sense. Though they never
occur linearly together they have a
'oneness' of meaning. Their dependency
on each other can also be expressed
through 'X+....X-' notation.
 agara::yo+  tuma  kaHto::yo-  mEM A_
jAtA    (tag 'yo' is for conjuncts)
5.1.2.   >i ....:i   : explicitly marked
dependency (:i is the head)
(a)  Example -- The sentence 1a below has
the dependency  structure given in T-2
 1a. phala    rAma     ne
      'fruit'     'Rama' 'Ergpostp'
      naHA_ kara        khAyA
      'having_bathed'    'ate'
    ' Rama ate the fruit after taking a bath'
                                  khAyA
                                     |
                       |----------|-----------------|
                  k1 |       naHA_kara::vkr    |k2
                       |                                     |
                      rAma_ne                     phala
                                   T.2
Default (5.2.5) states that all kaarakas
attach themselves to the nearest available
verb on the right. In (1a) above, the
nearest verb available to 'phala' (fruit) is
'naHA_kara'. However, 'phala' (fruit) is
not the 'k2' of 'naHA_kara'. It is the 'k2' of
the main verb 'khA'. Therefore, an explicit
marking is required to show this
relationship. The notation '>i...:i' makes
this explicit.  Therefore,
           phala/k2>i   rAma_ne   naHA_kara
khAyA::v:i
Where 'khAyA' is the 'head', thus marked
':i' and 'phala' is the dependent element,
thus marked '>i'. An element marked '>i'
always looks for another element marked
':i'.
(b)  Another example of such attachments
which need to be marked explicitly is
given below -
  2a. rAma,     moHana  Ora    shyAma
       'Rama',  'Mohan'    'and'    'Shyama'
Ae
'came'
                                      Ora
                                        |
                          |----------|---------------|
                          |             |                   |
                  rAma      moHana    shyAma
                                T-3
To show their attachment to 'Ora' (and) the
three elements 'rAma','moHana', 'shyAma'
have to be marked (as in 2b.) the
following way in our linear tagging
scheme.
   rAma>i,   moHana>i   Ora::yo:i
shyAma>i
The justification to treat 'Ora' as the head
and show the 'wholeness' of   all the
elements joined by '>i' to ':i' is made
explicit by the following examples-
    rAma,   Ora Haz,       moHana   Ora
     'Rama'  'and''yeah',     'Mohana'  'and'
    shyAma   Ae_ the
    'Shyama'  'had_come'
In this case there is an intervening element
'Ora HAz' (?and_yeah) between 'rAma' and
'moHana' etc. So paranthesis alone will
not resolve the issue of grouping the
constituents of a whole. (By
paranthesising, elements which are not
part of the whole will also be included.)
To avoid this the 'Ora' (and) has to be
treated as a head.
5.1.3.   0    : explicit marking of an ellipted
element (missing  elements).  Example -
     rAma      bAjZAra   gayA,    moHana
     'Rama'     'market'      'went'     'Mohana'
       ghara     Ora   Hari    skUla
       'home'     'and'  'Hari'    'school'
 ?Rama went to the market, Mohana home
and Hari to the school.?
The sentence above has two ellipted
elements. The second and third occurrence
of the verb 'gayA'(?went?). To draw a
complete tree the information of the
missing elements is crucial here.
Arguments 'moHana', 'ghara', 'Hari', and
'skUla' are left without a head, and their
dependency cannot be shown unless we
mark the 'ellipted' element.
 rAma     bAjZAra   gayA,   moHana
 'Rama' 'market'    'went', 'Mohana'
ghara   0  Ora    Hari     skUla 0
'home'      'and'     'Hari'    'school'
In cases where this information can be
retrieved from some other source
(DEFAULT ) it need not be marked. In the
above case it need not be marked.
However, there may be cases where
marking of the missing element is crucial
to show various relationships. In such
cases it has to be marked. Look at the
following example -
eka       Ora       sajjana
'one'      'more'   'gentleman'
kaHate_HEM   bacce        baDZe
 'says'                'children'    'big'
Ho_gaye_ HEM      kisI
'become'                   'nobody'
kI       bAta     naHIM  mAnate
'gen'  'saying'   'not'        'agree'
' One more gentleman says that the kids
have grown older and do not listen to
anybody.'
The above sentence does not have any
explicit 'yojaka(conjunct)', between two
sentences,
        a) bacce baDZe Ho gaye HEM and
            `kids have grown older'
        b) kisI kI bAta naHIM mAnate
             `do not listen to anybody'
Both these sentences together form the
'vAkyakarma(sentential object)' of the
verb 'kaHate HEM' (?say?).
So the analysis would be -
 [eka Ora sajjana]/k1 kaHate_HEM::v:i
  ?one? ?more? ?gentleman? ?says?
{{bacce/k1ud    baDZe/k1vid
    ?children?      ?big?
Ho_gaye_HEM::v}::s {kisI_kI/6
?become?                        ?nobody?_s?
bAta]/k2 naHIM::neg
?words?   ?not?
 mAnate::v}::s}/k2>I
?listen?
It appears to be a neatly tagged sentence.
However, some crucial information is
missing from this analysis. In the sentence
the relationship between the two sentences
within the larger sentential object is not
expressed. The problem now is how to do
it.  Use of '>i...:i' notation can help express
this. However, it needs the ':i' information
and since there is no explicit 'yojaka'
(conjunct) element between the two
sentences it will not be possible to mark it.
The information of the presence of a
'yojaka' (conjunct) which is the head of a
co-ordinate structure is CRUCIAL here.
Without its presence its dependency tree
cannot be drawn. The notation '0' can be
of help in such situations. '0' can be
marked in the appropriate place. This will
allow the tagging of the dependent
elements. Therefore, the revised tagging
would be -
 [eka Ora sajjana]/k1 kaHate_HEM::v:i
{{bacce/k1ud baDZe/k1vid
ho_gaye_HEM::v}::s>j 0::yo:j
{kisI_kI/6  bAta]/k2 naHIM::neg
mAnate::v}::s>j}/k2>i
Here the information of missing conjunct
has been marked by a '0'.
5.2. DEFAULTS
Apart from tagsets and special notations
the scheme also relies on certain defaults.
Defaults have been specified to save
typing by the human annotator. For
example, no sentence has to be marked ba
a sentence tag till it is crucial for the
dependency analysis. For example :
    rAma      ne        yaHa    socA         ki
    'Rama' 'postp' 'this'     'thought'   'that'
     moHana      AegA
    'Mohana' 'would_come'
  `Rama thought that Mohana would come'
This is a complex sentence where the
subordinate sentence is the object
complement of the verb 'socA'(?thought?) .
To indicate the relation of the subordinate
clause with the main verb, it has to
marked.
Similarly,  within the square paranthesis,
right most element is the Head. So there is
no need to mark it. Postpositions's
attachment to the previous noun is also
covered by the default rule. There are
other defaults which take care of modifier-
modified relationships. In short, the
general rules have been accounted for by
defaults and only the specific relations
have to be marked. Elements preceding
the head within paranthesis are to be
accepted as modifiers of the head.
However, In case the number of elements
within paranthesis is more than two (Head
plus two) and one or more of them do not
modify the head then it should be marked.
 Example -   [HalkI    nIlI   kitAba],
                      'light'  'blue'  'book'
Here, 'halkI'(?light?) can qualify both
'nIlI'(?blue?) and 'kitAba'(?book?).  In case
it is modifying 'kitAba'(?book?), say, in
terms of light weight, then it should be left
unmarked. But if it modifies 'nIlI'(?blue?),
in terms of light shade, then it SHOULD
be marked by adding '>' on the right of the
modifying element.
       'halkI'  [HalkI> nIlI  kitAba].
        ?light? [?light?> ?blue? ?book?]
Let us look at another  case where the
dependency has to be explicitly marked.
Participle form 'tA_HuA', in Hindi, can
modify either a noun or a verb. For
example take the Hindi sentence -
   mEMne/k1  dODZate_Hue::vkr
    'I_erg'          'running'
    ghoDZe_ko/k2  dekhA::v
     'horse'                'saw'
This ambiguous sentence may mean either
the following  :-
a) mEMne dODZate_Hue::vkr>i
ghoDZe_ko:i/k2 dekhA ;
     'I saw the horse while the horse was
running'
      Or
b) mEMne dODZate_Hue::vkr>i
ghoDZe_ko/k2 dekhA::v:I
  'While I was running I saw the horse'
There is no need to mark ':i' in sentence
(a). However (b)  will need  explicit
marking.
5.3.TAGSETS
The tagsets used here have been divided
into two categories -
        1) TAGSET-1 - Tags which express
relationships are marked by  a preceding '/'
. For example kaarakas are grammatical
relationships, thus they are marked '/k1',
'/k2', '/k3' etc.
        2) TAGSET-2 - Tags expressing
nodes are marked by  a preceding '::' verbs
etc. are nodes, so they will be marked '::v',
Certain conventions regarding the naming
of the tags are ;
         k = kaaraka, --  all the kaaraka tags
will begin with k-,
                Therefore, k1, k2, k3 etc.
          n = noun
          v = verb  -- eg. v, vkr etc.
6.  CONCLUSIONS
A tagging scheme has been designed to
annotate corpora for various Indian
languages. The objective has been to use
uniform tags for all the Indian languages
thereby evolving a standard which can be
followed for various syntactic analysis for
machine processing. The scheme is yet to
be yet implemented on corpora from
various languages. Some trial workshops
have been conducted to see its
applicability in other Indian languages.
However, once the actual task of tagging
begins one may come across cases which
are not covered by the present scheme.
The idea is to provide a basic scheme
which can later be improved and revised.
7. REFERENCES
Bharati, Akshar, Vineet Chaitanya and
Rajeev Sangal, "Natural  Language
Processing: A Paninian Perspective",
Prentice-Hall of India,
New Delhi, 1995.
Bharati, Akshar, Dipti M Sharma, Vineet
Chaitanya, Amba P Kulkarni,
Rajeev Sangal, Durgesh D Rao, LERIL :
Collaborative Effort for Creating
Lexical Resources, In Proc. of Workshop
on Language Resources in
Asian Languages, together with 6th NLP
Pacific Rim Symposium, Tokyo.
Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 51?58,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Simple Preposition Correspondence: A problem  in English to Indian 
language Machine Translation 
Samar Husain, Dipti Misra Sharma, Manohar Reddy 
{samar@research.iiit.net,dipti@mail.iiit.net, 
manohar@students.iiit.net} 
Language Technologies Research Centre, 
IIIT, Hyderabad, India. 
 
 
Abstract 
The paper describes an approach to auto-
matically select from Indian Language the 
appropriate lexical correspondence of Eng-
lish simple preposition. The paper de-
scribes this task from a Machine Transla-
tion (MT) perspective. We use the proper-
ties of the head and complement of the 
preposition to select the appropriate sense 
in the target language. We later show that 
the results obtained from this approach are 
promising. 
1 Introduction 
The task of identifying the appropriate sense from 
some target language (here, Hindi and Telugu) for 
a given simple preposition in some source lan-
guage (here, English) is rather complex for an MT 
system, and noting that most foreign language 
learners are never able to get a firm hold on prepo-
sitions of a new language (Brala, 2000), this should 
not be surprising. A simple example illustrates the 
problem: 
 
(1a) He bought a shirt with tiny collars. 
       ?with? gets translated to vaalii in Hindi (hnd). 
        and as kaligi unna in Telugu (tlg). 
 (1b) He washed a shirt with soap. 
       ?with? gets translated to se in hnd. 
    and as to (suffixed to head noun) in tlg. 
 
   For the above English sentences, if we try to 
swap the senses of ?with? in their corresponding 
target translation, the resulting sentences either 
become ill-formed or unfaithful to their English 
source. The pervasive use of preposition (or its 
equivalent in a given language) in most of the lan-
guages makes it a crucial element during transla-
tion. Inappropriate sense selection of a preposition 
during machine translation can have a negative 
impact on the quality of the translation, sometimes 
changing the semantics of the sentence drastically,  
thereby making the preposition sense selection 
module a critical component of any reliable MT 
system. 
  Finding the proper attachment site for the prepo-
sition in English, i.e. getting the correct parse for 
the prepositional phrase (PP) is a classic problem 
in MT, and this information can be used to identify 
the sense of a preposition. Figure 1 and Figure 2 
below show the correct attachment site of PPs in 
example (1a) and (1b) respectively. 
 
 
 
51
   The correct parse of the PP helps us in selecting 
the appropriate sense. However, finding the appro-
priate attachment only reduces the problem. It does 
not lead to a ?complete solution?. The following 
examples (2a, 2b and 3a, 3b) have the same at-
tachment site but take different senses in the target 
language: 
 
(2a) He has had fever for two days now. 
       ?for? gets translated as se in hnd. 
    and as nundi in tlg. 
(2b) He had fever for two days. 
       ?for? gets translated as taka in hnd. 
    Not translated in tlg.  
 
(3a) He is going to Delhi. 
?to? gets translated as ko, or preferably left un-
translated in hnd. 
and in tlg as ki (suffixed to the head noun), or 
may be left un-translated. 
(3b) He is going to his mother. 
       ?to? gets translated as ke paasa in hnd. 
   and daggaraku in tlg 
 
After looking at cases such as (2a), (2b) and (3a), 
(3b) where the parse is same i.e., preposition ?for 
and ?to? get attached to the main verb ?have? and 
?go? respectively, it is clear that we need to come 
up with some criterion which can help us in 
achieving our task. 
There has been extensive work on understanding 
prepositions linguistically, often from various an-
gles. Syntactically (Jackendoff, 1977; Emonds, 
1985; Rauh, 1993; Pullum and Huddleton, 2002), 
from a Cognitive perspective (Lakoff and Johnson, 
1980; Langacker, 1987; Brala, 2000), Semantically 
by (Saint-Dizier and Vazquez, 2001; Saint-Dizier, 
2005), and the Pragmatic aspects by (Fauconnier, 
1994). 
   The work of automatically selecting the correct 
sense has also received good amount of attention 
and there have been many attempts to solve the 
problem. (Japkowicz et. al, 1991) attempts to trans-
late locative prepositions between English and 
French. The paper introduces the notion of ?repre-
sentation of conceptualization? based in turn on 
(Grimaud, 1988). The paper synthesizes this idea 
with the thesis of ideal meaning (Herskovits, 1986). 
(Tezuka et. al, 2001) have tried to resolve concep-
tual geographical prepositions using inference rule 
based on cognitive maps which people have of the 
external world. (Hartrumpf et al, 2005) use 
knowledge representation formalism for PP inter-
pretation. 
Some studies pertain to systems which have 
been implemented for MT; (Gustavii, 2005) uses 
aligned parallel corpora to induce automatic rules 
by applying transformation-based learning. (Alam, 
2004) make use of contextual information to de-
termine the meanings of over. (Trujillo, 1992) use 
a transfer rule based approach to translate locative 
PP-phrase, the approach uses the dependency rela-
tions marked as indices with individual word and a 
bilingual lexicon which has mapping between 
source and target lexical item (with indices). 
(Naskar and Bandyopadhyay, 2005) look at the 
semantics of the head noun of the reference object 
(this is their main criterion) to get the lexical 
meaning of prepositions in an English-Bengali MT 
system. 
The current paper presents a study of preposi-
tions at, for, in, on, to and with in context of Eng-
lish to Indian language MT system. The paper is 
arranged as follows; Section 2 describes our ap-
proach to solving the mentioned task, the 3rd sec-
tion shows the performance of our approach along 
with the error analysis during the testing phase, we 
conclude the paper along with some future direc-
tion in section 4. 
2 Our Approach 
All the previous attempts can be broadly classified 
into 3 main categories; one, where the preposition 
is the main focus, concentration is on the semantics 
(cognitive or lexical) of the preposition; second, 
focus on the verb and the PP which the verb takes 
as argument; and lastly, the head noun of the PP 
becomes the deciding factor to get the appropriate 
sense. 
Very few approaches, like (Alam, 2004; Saint-
Dizier and Vazquez, 2001),  consider both, the 
head (modified) and the complement (modifier) 
information, to decide the sense of the preposition. 
The modified (or head) is the head of the phrase to 
which the PP attaches. The modifier (or 
complement) is the head noun of the PP. The 
following examples show very clearly why given a 
preposition we cannot depend only on the modified 
or the modifier separately, and that we must 
consider them both to solve the problem. 
 
52
Considering only the modifier (the complement); 
 
 (4a) He apologized to his mother. 
        ?to? gets translated as se in hnd 
        & ki (suffixed to the head noun) in  tlg 
 (4b) He went to his mother. 
         ?to? gets translated as ke paasa in hnd 
         & as daggaraku in tlg 
 
Considering only the modified (the head); 
 
 (5a) He waits for her at night. 
        ?at? gets translated as meM in hnd 
        & not translated in tlg 
 (5b) He waits for her at the station. 
         ?at? gets translated as par 
         & as lo in tlg 
 
Only considering the modifer ?his mother? in 4a 
and 4b is not sufficient, likewise taking only the 
modified ?waits? in 5a and 5b will be insufficient, 
both the pairs take different senses and have the 
same partial contextual enviornment which is 
misleading. Hence, the combined context of 
complement-head forms a better candidate for 
solving the problem. We come across plenty of 
cases where isolated information of 
modifier/modified can be misleading. 
The task of preposition sense selection can be 
divided into; 
(a) Getting the correct parse (the task of PP at-
tachment, identification of phrasal verb, etc.), 
(b) Context and semantic extraction, 
(c) Sense selection. 
 
This paper describes the algorithm for achieving 
the above mentioned steps. We assume the input to 
our module has the correct parse, i.e. Step (a) 
above is assumed here. The proposed algorithm is 
a component in English to Indian language MT 
system1, therefore, the required input can be pre-
sumed to be available. Steps (b, c) above are rule 
based, which make use of the modifier-modified 
relation, these relations and the properties of modi-
fier/modified form the core of the context in step 
(b). We then apply a series of rules, which specify 
the context and semantics in which a sense  
 
         1 (http://shakti.iiit.ac.in). Note here that the proposed 
algorithm has been tested with Shakti version 0.83x which has 
still not been released. The released version is 0.73. 
is expected to occur. 
2.1 Context and semantic extraction 
Extraction of context and semantic information (of 
modifier/modified) is done automatically by vari-
ous sub-modules which are combined together to 
perform the overall task. We use the word ?con-
text? very loosely. A context for us is a combina-
tion of various properties which can be syntactic or 
lexical, or both; syntactic context can be modifier-
modified relation, lexical properties can be mor-
phological information such as TAM (tense, aspect 
and modality) of a verb, class of the verb (Levin, 
1993), category of the lexical item and in some 
cases the lexical item itself.  
The semantics of the modifier and the modified 
are captured using WordNet (Miller, 1990), and 
certain other resources such as person, place dic-
tionaries, place and time filters (these filters make 
use of syntactic cues to mark basic time and place), 
etc. We use WordNet to get the hypernyms of a 
word. By using this property we can easily get the 
broader, more general class/concept for a modi-
fier/modified. Although effective and very intui-
tive, this method has its own problems. We will 
elaborate these problems in section 3.2. WordNet 
is also used to identify person and place names by 
using the hyponym tree for person and place. 
Along with the WordNet, as mentioned above, 
we use certain other filters such as place and time. 
They are used prior to using WordNet. In case a 
rule requires the modifier to be a place (rules are 
explained in 2.2), this information is acquired from 
the place filter. If the filter?s result is negative we 
use WordNet. Dictionaries and POS tags are 
checked for identifying proper names, we use a 
proper name dictionary as POS taggers tend to 
have a fixed upper limit especially when it comes 
to the identification of named entities. In essence, 
the linguistic resources are used in the following 
order; 
(1) Dictionaries, 
(2) Time & Place filter, 
(3) WordNet. 
 
Preliminary results have shown that certain 
prepositions occurring in the PP complement of 
certain verb classes (Levin, 1993) translate to a 
specific sense in Hindi. For example, preposition 
?at? in the case of peer verbs always translates to 
kii tarapha or kii ora in Hindi. This knowledge can 
53
be very informational and we plan to pursue this 
aspect in the future. 
2.2 Sense Selection 
We have noticed in the previous examples that the 
prepositions from English either get translated as 
suffixes to the head noun of the PP (in Telugu) or 
as postpositions (in Hindi and Telugu). An 
example where a preposition in English gets 
translated as postposition in its Telugu translation 
is shown below; 
 
(6) The book is on the table.  
  ?buka     taibila     paiina   undi? 
   ?Book?  ?table?     ?on?     ?there? 
 
We select the correct sense of the preposition 
based on a series of rules which are applied 
linearly. These rules have been manually 
constructed. We have tried to make the rules 
mutually exclusive, so that there are no clashes. 
Also, by making sure that the rules are mutually 
exclusive we don?t need to worry about the order 
in which the rules are listed out in the rule file, thus 
making the rule file less fragile. These rules 
currently cover around 20 high frequency English 
prepositions, these prepositions vary in their 
degree of ambiguity; some are highly ambiguous 
(e.g. to, by, with, etc.), whereas some are less 
ambiguous (e.g. against, around, as, etc.), hence 
these are easier to handle. 
Various senses on the target side for a given 
English preposition are selected on the basis of 
rules listed out in a file. The rule file comprises of 
tuples, each having 6 attributes.  
 
The attributes are listed below; 
 
a)  Source Language preposition 
b)  Modified category 
c)  Constraints on the modified item 
d)  Modifier category 
e)  Constraints on the modifier item 
f) Dictionary sense id of the source language 
preposition 
 
 
An example of a tuple: 
# at, v, -, n, place_close, at%p%5 
 
 
(7) He has opened a school at his home. 
 ?usane   apne ghara   mem eka  skuula kholaa hei? 
?He erg? ?his? ?house? ?at? ?one? ?school??open? ?is? 
 
The rule above requires the modifier to be a 
noun and places a constraint ?place_close? on it. 
We map this constraint (place_close) with some set 
of lexical items found in a synset of a hypernym 
obtained from WordNet. For example, 
?place_close? might correspond to ?housing?, 
?lodging?, ?building?, etc in a synset. In essence 
?place_close? is place holder for different relations 
which might be present in a synset. The modified 
category and the modifier category can be ex-
tracted after the correct parse of the PP is known; 
the constraints applied on the modified and modi-
fier item (point c, e above) can be of various kinds, 
some of them are; 
 
? Semantic relations corresponding to 
WordNet hypernyms for a given word 
? Presence of the lexical item in some list 
(eg. verb class) 
? Semantic property such as ?time? or ?place? 
? Lexical property such as aspect, negativity 
etc. 
 
 
 
 
The constraints specified in a tuple can be com-
bined together using logical operators such as 
?and?, ?or?, ?negation?. So, for a single rule, multi-
54
ple constraints can be introduced. For a sense, if 
needed, complex constraints can be introduced 
which must be satisfied. 
 
#for, v, L2:for.dat && aspect:continuous, n, time, 
for%p%5 
 
 (8) He has been playing for years. 
     ?vaha   kaii      saalo   se    khela   rahaa   hai ? 
      ?He? ?many? ?years? ?for? ?play? ?cont.?  ?is? 
 
The above rule (for the Hindi translation) has 
two constraints for the modified (which is a verb in 
this case), the two constraints have been combined 
using an ?and? operator (represented using two 
ampersands, ?&&?). Only if the two constraints are 
satisfied, the constraint is considered as satisfied 
else it is considered as failed. The use of different 
logical operator gives a lot of expressive power to 
a single rule. Sometimes it might be desirable to 
place multiple constraints together, because for a 
given sense these constraints always occur together, 
and by listing them as separate rules we will miss 
out the fact that they co-occur.  
It is not always necessary (or possible) to fill the 
constraint fields. In fact, sometimes it is even de-
sirable to leave them unspecified. In such a case 
we place a hyphen in that field, such as the follow-
ing rule; 
 
# at, v, -, n, place_close, at%p%5 
 
In the above rule, the constraint for the modified 
field is unspecified. There are also cases when it is 
not desirable to have a translated preposition corre-
sponding to its source;  
 
# to, L: verbs.txt, -, n, place, ZZ 
 
(9) He went to Delhi. 
      ?vaha dilli     gayaa? (in hnd) 
       ?He? ?Delhi? ?went? 
 
The ?ZZ? in the above rule signifies that the 
translated sentence will have no preposition corre-
sponding to the preposition ?to? when it occurs 
with certain verbs which are specified by 
?L:verbs.txt? (?verbs.txt? is a list of verbs). For the 
above Hindi sentence post-position ?ko? can  
 
         2 List 
perhaps be introduced, i.e. ?vaha dilli ko gayaa?, 
but ?vaha dilli gayaa? is more natural, and the 
translated sentence is better off without a ?ko?.  
Finally, each preposition handled has a default 
rule, which is applied at the end when all the other 
rules for that preposition fail; the sense given by  
the default rule is based on the most frequent usage 
of the preposition at the target side. All the fields 
(except the first and last) in the default rule have 
hyphens. The default rule for ?to? is written below; 
 
to, -, -, -, -, to%p%1 
 
Some of the rules in the rule file are given below, 
for ease of comprehension, we mention the actual 
target sense instead of the dictionary id for the last 
field (the actual rule file has dictionary sense id) 
 
at, v, L:peer_verbs.txt, n, -, kii tarapha 
at, v, L:transaction_verbs.txt, n, price, meM 
for, v, -, n, distance, taka 
in, n, animate, n, place, kaa 
on, v, -, n, time, ko 
to, v, L:go_verbs.txt, n, animate|authority, ke 
paasa 
with, v, -, n, instrument, se 
2.3 Recap 
We briefly describe the various steps of the al-
gorithm again; 
 
(a) Given a raw sentence we feed it to the 
Shakti MT system which performs various 
source language analysis, for our algo-
rithm, information such as PP attachment 
and correct identification of the phrasal 
verb (if present) is crucial. 
(b) The output of step (a) is taken by our 
module which automatically constructs 
the six field tuple described above. At this 
point we can only fill some fields, which 
are field 1 (source language preposition), 
field 2 (modified category) and field 4 
(modifier category). 
(c) We then compare this constructed tuple 
with the appropriate tuples present in the 
rule file. For this constructed tuple to sat-
isfy the various constraints mentioned in 
the tuple with which it is compared re-
sources such as place filter, time filter, 
lists and WordNet are consulted automati-
55
cally. The order in which we use these re-
sources has been already been mentioned 
in section 2.1. The tuple for which all the 
constraints are satisfied is selected, the 
last field of this tuple contains the diction-
ary id of the sense. 
(d) Output the selected sense. 
3 Evaluation 
For the current study, experiments were conducted 
with 6 high frequency prepositions, they are; at, 
for, in, on, to, and with. The algorithm was tested 
on 100 sentences for each preposition in both the 
language pairs, i.e., 600 sentences for English-
Hindi and 600 sentences for English-Telugu. These 
sentences were randomly extracted from the 
ERDC3 corpus. The corpus contains text from dif-
ferent domains such as medicine, sports, history, 
etc. The input to the implemented system was 
manually checked and corrected to make sure that 
there were no errors in the information which is 
expected by the system. The bulk of these correc-
tions involved rectifying the wrong PP attachment 
given by the parser and the mistakes in phrasal 
verb identification.  
Prep4 Precision BL No. of Sense 
At 73.4 51.5 5 
For 84.05 69.5 6 
In 82 65.2 7 
On 85 70 3 
To 65.2 35.4 10 
With 66 50 6 
Table 1{English-Hindi}. 
 
Prep4 Precision BL No. of Sense 
At 68 48 5 
For 72 50 7 
In 82 82 3 
On 76 76 2 
To 80 80 2 
With 94 90 3 
Table 2{English-Telugu}. 
 
         3Electronic Research and Development Centre, NOIDA 
         4 Prepositions 
3.1 Performance 
The tables above show the performance of the sys-
tem and compares it with the baseline score (BL). 
BL is the precision of the system with only the de-
fault sense. The tables also show the number of 
sense which English prepositions can take on the 
target side. Table 1 and Table 2 show English-
Hindi and English-Telugu results respectively. 
The implemented system gives very promising 
results. Certain prepositions give comparably low 
precision. The reasons for the inappropriate sense 
selection are discussed in the next section. The 
English-Telugu results (Table 2)  show same 
system precision and BL for some preposition (?in? 
and ?to?). This is because these prepositions have 
less number of sense on the target side and all the 
instances found in the test data had the default 
sense.   
3.2 Error analysis 
The errors made by the system were analyzed and 
the major reasons for inappropriate sense selection 
were;  
  
(a) Noise generated by WordNet, 
(b) Special constructions, 
(c) Metonymy, 
(d) Ambiguous sentences, 
(e) Presence of very general constraints. 
 
The problem of noise generation by WordNet 
sometimes leads to surprising and unexpected 
sense selection; this is because in WordNet a noun 
or verb will have multiple sense, and each of these 
senses will have various levels of hypernym syn-
sets, so, while finding various concepts/features 
(specified by the rule for a preposition) we need to 
look at each one of these senses. We need to do 
this because we currently don?t have the sense in-
formation. So, an inappropriate sense might some-
times satisfy the constraint(s) and result in inap-
propriate selection. The solution for this will obvi-
ously be to identify the correct sense of modi-
fier/modified prior to getting its semantic property 
from the WordNet.  
There are certain constructions in which the 
head noun of the PP is a pronoun, which refers 
back to a noun. For us this will create a problem, in 
such cases we will first need to get the referent 
56
noun and then apply the constraints on it, take the 
following example; 
 
(10) The rate at which these reactions occur is 
known as rate of metabolism. 
 
In the above example, the head noun of the PP 
(at which) refers to the noun (rate) on which we 
need to apply the constraints. At present the 
coreference information is not available to us, 
therefore in such cases the algorithm fails to give 
the correct output. 
The other reason for failure was the ambiguity 
of the sentence itself which could be interpreted in 
various ways, like the example below; 
 
(11) Andamaan should go to India. 
 
The above sentence can be interpreted (and 
translated) in two ways, the hindi translations for 
the two interpretation are; 
 
(11a) ?andamaan    indiaa  ko   jaanaa  chahiye? 
          ?Andamaan? ?India?  ?to?  ?go?     ?should? 
   India should get Andaman. 
 
(11b) ?andamaan   ko   indiaa   jaanaa  chahiye? 
          ?Andamaan? ?to? ?India?  ?visit?    ?should? 
   Andaman should visit India. 
 
In (11a) we get the sense that the 
possesion/control of ?Andamaan? should go to 
?India?, and in (11b) it is ?Andamaan? (the 
government of ?Andamaan?) which is going to 
?India? (the government of India), as in, The United 
States should go to UK, also in (11b) we can have 
?Andamaan? as somebodys? name, as in, Ram 
should go to India. In such cases we failed to get 
the appropriate translation of the preposition as it 
in turn depends on the correct interpretation of the 
whole sentence. Ambiguity of numerals in a 
sentence is yet another case which lead to faliure, 
like the following example; 
 
(12) At 83, Vajpayee is overweight. 
 
In the above sentence, the number 83 can either 
mean this persons? (Vajpayee) age or his weight. 
The target side translation takes different 
preposition sense for these two interpretation. 
Hindi takes para and in Telugu ?at? is not-
translated when we treat 83 as weight, and when 
treated as age, we get mem and lo/ki in Hindi and 
Telugu respectively. 
We found that certain prepositions occur in large 
number of metonymical usage, like, ?with? and 
?at?. The constraints in a rule have been formulated 
for the general usage and not the extended usage of 
a given word. The example below shows one such 
instance; 
 
(13) Great bowlers spend hours after hours at 
the nets. 
 
While looking in WordNet for the various 
senses of ?net? not a single sense matches with the 
kind of usage in which ?net? is used in the above 
sentence. 
Certain rules for some of the preposition were 
found to be very general, the low performance of 
?for? and ?to? in telugu and hindi respectively are 
mainly due to this reason. In general, formulating 
rules (English-Hindi) for preposition ?to? was very 
difficult. This was because ?to? can have around 10 
senses in Hindi. The rules with very general 
constraints tend to satisfy cases where they should 
have failed. One has to revisit them and revise 
them. 
4 Conclusion and Future Work 
In this paper we described an approach to select 
the appropriate sense for a preposition from an 
English to Indian language MT perspective, we 
discussed the issues involved in the task, we ex-
plained the steps to achieve the required task; 
which are, semantic and context extraction, and 
sense selection. We reported the performance of 
the system, and showed that our approach gives 
promising results. We also discussed the identified 
problems during the error analysis; such as noise 
generation by WordNet. 
One of the pertinent tasks for the future would 
be to come up with a solution to reduce the noise 
generated by WordNet. The scope of rule file in 
terms of handling more prepositions needs to be 
broadened. We would like to extend this work to 
handle complex preposition. Finally, we would like 
to explore if ML techniques can be combined with 
the rule base to exploit the benefits of both the ap-
proaches. 
57
References 
Yukiko Sasaki Alam. 2004. Decision Trees for Sense 
Disambiguation of Prepositions: Case of Over. In 
HLT/NAACL-04. 
Marija M. Brala. 2000. Understanding and translating 
(spatial) prepositions: an exercise in cognitive se-
mantics for lexicographic purposes. 
Joseph Emonds. 1985. A unified theory of syntactic      
categories. Dordrecht: Foris. 
Gilles Fauconnier. 1994. Mental spaces. Cambridge: 
Cambridge University Press. 
M. Grimaud. 1988. Toponyms, Prepositions, and Cog-
native Maps in English and French, Journal of 
American Society of Geolinguistics. 
Ebba Gustavii. 2005. Target Language Preposition Se-
lection - an Experiment with Transformation-based 
Learning and Aligned Bilingual Data. In Proceedings 
of EAMT 2005. 
Sven Hartrumpf, Hermann Helbig and Rainer Osswald. 
2005. Semantic Interpretation of Prepositions for 
NLP Applications. In EACL 2006 Workshop: Third 
ACL-SIGSEM Workshop on Prepositions 
A. Herskovits. 1986. Language and Spatial Cognition, 
An Interdisciplinary Study of Prepositions in English. 
Cambridge University Press. 
Cliffort Hill. 1982. Up/down, front/back, left/right. A 
contrastive study of Hausa and English. In Weissen 
born and Klein, 13-42. 
Ray Jackendoff. 1977. The architecture of the language. 
Cambridge, MA: MIT Press. 
Nathalie Japkowicz and Janyce M. Wiebe. 1991. A Sys-
tem For Translating Locative Preposition From Eng-
lish Into French. In Proc. Association for Computa-
tional Linguistics. 
George Lakoff and Mark Johnson. 1980. Metaphors we 
live by. Chicago: University of Chicago Press. 
Beth Levin. 1993. English verb classes and alterna-
tions. Chicago/London: The University of Chicago-
Press.  
George A. Miller. 1990. WordNet: An online lexical 
database. International Journal of Lexicography. 
Sudip Kumar Naskar and Sivaji Bandyopadhyay. 2005. 
Handling of Prepositions in English to Bengali Ma-
chine Translation. In EACL 2006 Workshop: Third 
ACL-SIGSEM Workshop on Prepositions. 
Geoffrey Pullum and Rodney Huddleston. 2002. Prepo-
sitions and prepositional phrases. In Huddleston and 
Pullum (eds.), 597-661.  
Gisa Rauh. 1993. On the grammar of lexical and 
nonlexical prepositions in English. In Zelinskiy-
Wibbelt (eds.), 99-150. 
Patrick Saint-Dizier and Gloria Vazquez. 2001. A com-
positional framework for prepositions. In IWCS4, 
Tilburg, Springer, lecture notes, p. 165-179. 
Patrick Saint-Dizier. 2005. PrepNet: A framework for 
describing prepositions: Preliminary investigation re-
sults. In Proc. of IWCS 6, Tilburg. 
Joseph M. Sopena, Agusti LLoberas and Joan L. 
Moliner. 1998. A connectionist approach to preposi-
tional phrase attachment for real world texts. In COL-
ING-ACL ?98, 1233-1237. 
T. Tezuka, R. Lee, H. Takakura, and Y. Kambayashi. 
2001. Web-Based Inference Rules for Processing 
Conceptual Geo-graphical Relationships. In Proc. of 
the 2nd Int. Conf. on Web Information Systems Engi-
neering, The 1st Int. Workshop on Web Geographical 
Information Systems. 
Arturo Trujillo. 1992. Locations in the machine transla-
tion of prepositional phrases. In Proc. TMI-92. 
58
  
The Hindi Discourse Relation Bank 
Umangi Oza*, Rashmi Prasad?, Sudheer Kolachina*, Dipti Misra Sharma* and 
Aravind Joshi? 
*Language Technologies Research Centre 
IIIT Hyderabad, Gachibowli, Hyderabad, Andhra Pradesh, India 500032 
oza.umangi,sudheer.kpg08@gmail.com,dipti@iiit.ac.in 
 
?Institute for Research in Cognitive Science/Computer and Information Science 
3401 Walnut Street, Suite 400A 
Philadelphia, PA USA 19104 
rjprasad,joshi@seas.upenn.edu 
 
  
Abstract 
We describe the Hindi Discourse Relation 
Bank project, aimed at developing a large 
corpus annotated with discourse relations. 
We adopt the lexically grounded approach of 
the Penn Discourse Treebank, and describe 
our classification of Hindi discourse connec-
tives, our modifications to the sense classifi-
cation of discourse relations, and some cross-
linguistic comparisons based on some initial 
annotations carried out so far. 
1 Introduction 
To enable NLP research and applications beyond 
the sentence-level, corpora annotated with dis-
course level information have been developed. 
The recently developed Penn Discourse Tree-
bank (PDTB) (Prasad et al, 2008), for example, 
provides annotations of discourse relations (e.g., 
causal, contrastive, temporal, and elaboration 
relations) in the Penn Treebank Corpus. Recent 
interest in cross-linguistic studies of discourse 
relations has led to the initiation of similar dis-
course annotation projects in other languages as 
well, such as Chinese (Xue, 2005), Czech (Mla-
dov? et al, 2008), and Turkish (Deniz and Web-
ber, 2008). In this paper, we describe our ongo-
ing work on the creation of a Hindi Discourse 
Relation Bank (HDRB), broadly following the 
approach of the PDTB.1 The size of the HDRB 
corpus is 200K words and it is drawn from a 
400K word corpus on which Hindi syntactic de-
pendency annotation is being independently con-
ducted (Begum et al, 2008). Source corpus texts 
are taken from the Hindi newspaper Amar Ujala, 
and comprise news articles from several do-
mains, such as politics, sports, films, etc. We 
                                                 
1 An earlier study of Hindi discourse connectives towards 
the creation of HDRB is presented in Prasad et al (2008). 
present our characterization of discourse connec-
tives and their arguments in Hindi (Section 2), 
our proposals for modifying the sense classifica-
tion scheme (Section 3), and present some cross-
linguistics comparisons based on annotations 
done so far (Section 4). Section 5 concludes with 
a summary and future work.  
2 Discourse Relations and Arguments 
Following the PDTB approach, we take dis-
course relations to be realized in one of three 
ways: (a) as explicit connectives, which are 
?closed class? expressions drawn from well-
defined grammatical classes; (b) as alternative 
lexicalizations (AltLex), which are non-
connective expressions that cannot be defined as 
explicit connectives; and (c) as implicit connec-
tives, which are implicit discourse relations ?in-
ferred? between adjacent sentences not related by 
an explicit connective. When no discourse rela-
tion can be inferred between adjacent sentences, 
either an entity-based coherence relation (called 
EntRel) or the absence of a relation (called No-
Rel) is marked between the sentences. The two 
abstract object relata of a discourse relation are 
called the relation?s arguments (named Arg1 and 
Arg2), and argument annotation follows the ?mi-
nimality principle? in that only as much is se-
lected as the argument text span as is minimally 
necessary to interpret the relation. Finally, each 
discourse relation is assigned a sense label based 
on a hierarchical sense classification. 
2.1 Explicit Connectives 
In addition to the three major grammatical 
classes of Explicit connectives in the PDTB ? 
subordinating conjunctions, coordinating con-
junctions, and adverbials ? we recognize three 
other classes, described below. 
 
  
Sentential Relatives: These are relative pro-
nouns that conjoin a relative clause with its ma-
trix clause. As the name suggests, only relatives 
that modify verb phrases are treated as discourse 
connectives, and not those that modify noun 
phrases. Some examples are ????? (so that), 
????? ???? (because of which). 
 
1) [???? ??? ?????? ?? ?? ??????? ?? ????? ??? 
?? ?? ?? ????] ????? {???? ??? ???? ???? 
?? ???} 
?[Dropping all his work, he picked up the bird 
and ran towards the dispensary], so that {it 
could be given proper treatment}.? 
Subordinators: These include postpositions (Ex. 
2), verbal participles, and suffixes that introduce 
non-finite clauses with an abstract object inter-
pretation.2 
2) [?? ?? ????? ???]?? {??-??-?? ???????? ???? 
????? ???}? 
?Upon [hearing Baa?s words], {Gandhiji felt very 
ashamed}.? 
Particles: Particles such as ??, ?? act as dis-
course connectives. ?? is an emphatic inclusive 
particle used to suggest the inclusion of verbs, 
entities, adverbs, and adjectives. Instances of 
such particles which indicate the inclusion of 
verbs are taken as discourse connectives (Ex. 3) 
while others are not. 
3) ??? ?? ? ????? ????? ?? ??? ??? ? ???? ? ?? 
?????? ?? ??? ??? ??? ??? ???]?{?????? ??? 
??? ??? ???????? ???} ?? {?? ??? ???}? 
?[People see this as a consequence of the improv-
ing relation between the two countries]. {The 
Kashmiris are} also {learning an political lesson 
from this}.? 
2.2 Arguments of Discourse Relations 
In the PDTB, the assignment of the Arg1 and 
Arg2 labels to a discourse relation?s arguments is 
syntactically driven, in that the Arg2 label is as-
                                                 
2 Subordinators that denote the manner of an action are not 
discourse connectives, but since such disambiguation is a 
difficult task, we have decided to annotate subordinators in 
a later phase of the project.  
 
signed to the argument with which the connec-
tive was syntactically associated, while the Arg1 
label is assigned to the ?other? argument. In 
HDRB, however, the Arg1/Arg2 label assign-
ment is semantically driven, in that it is based on 
the ?sense? of the relation to which the argu-
ments belong.  Thus, each sense definition for a 
relation specifies the sense-specific semantic role 
of each of its arguments, and stipulates one of the 
two roles to be Arg1, and the other, Arg2.   For 
example, the ?cause? sense definition, which in-
volves a causal relation between two eventuali-
ties, specifies that one of its arguments is the 
cause, while the other is the effect, and further 
stipulates that the cause will be assigned the label 
Arg2, while the effect will be assigned the label 
Arg1.  Apart from giving meaning to the argu-
ment labels, our semantics-based convention has 
the added advantage simplifying the sense classi-
fication scheme. This is discussed further in Sec-
tion 3.  
2.3 Implicit Discourse Relations 
The HDRB annotation of implicit discourse rela-
tions largely follows the PDTB scheme. The only 
difference is that while implicit relations in 
PDTB are annotated only between paragraph-
internal adjacent sentences, we also annotate 
such relations across paragraph boundaries.  
3 Senses of Discourse Relations  
Broadly, we follow the PDTB sense classifica-
tion in that we take it to be a hierarchical classi-
fication, with the four top level sense classes of 
?Temporal?, ?Contingency?, ?Comparison?, and 
?Expansion?. Further refinements to the top class 
level are provided at the second type level and 
the third subtype level. Here, we describe our 
points of departure from the PDTB classification. 
The changes are partly motivated by general 
considerations for capturing additional senses, 
and partly by language-specific considerations. 
Figure 1 reflects the modifications we have made 
to the sense scheme. These are described below. 
 
Eliminating argument-specific labels: In the 
PDTB sense hierarchy, the tags at the type level 
are meant to express further refinements of the 
relations? semantics, while the tags at the subtype 
level are meant to reflect different orderings of 
the arguments (see Section 2.2). In HDRB, we 
eliminate these argument-ordering labels from 
the subtype level, since these labels don?t direct-
ly pertain to the meaning of discourse relations. 
  
All levels in the sense hierarchy thus have the 
purpose of specifying the semantics of the rela-
tion to different degrees of granularity. The rela-
tive ordering of the arguments is instead speci-
fied in the definition of the type-level senses, and 
is inherited by the more refined senses at the sub-
type level.   
 
 
 
     
 
Figure 1: HDRB (Modified) Sense Classification 
 
 
Uniform treatment of pragmatic relations: As 
in PDTB, discourse relations in HDRB are 
pragmatic when their relations have to be in-
ferred from the propositional content of the ar-
guments. However, we replace the PDTB prag-
matic senses with a uniform three-way classifica-
tion. Each pragmatic sense at the type level is 
further distinguished into three subtypes: ?epis-
temic? (Sweetser 1990), ?speech-act? (Sweetser 
1990), and ?propositional?. The propositional 
subtype involves the inference of a complete 
proposition. The relation is then taken to hold 
between this inferred proposition and the propo-
sitional content of one of the arguments. 
 
The ?Goal? sense: Under the ?Contingency? 
class, we have added a new type ?Goal?, which 
applies to relations where the situation described 
in one of the arguments is the goal of the situa- 
tion described in the other argument (which 
enables the achievement of the goal).   
4 Initial Annotation Experiments 
Based on the guidelines as described in this pa-
per, we annotated both explicit and implicit rela-
tions in 35 texts (averaging approx. 250 
words/text) from the HDRB corpus. A total of 
602 relation tokens were annotated. Here we 
present some useful distributions we were able to 
derive from our initial annotation, and discuss 
them in light of cross-linguistic comparisons of 
discourse relations.  
 
Types and Tokens of Discourse Relations: Ta-
ble 1 shows the overall distribution of the differ-
ent relation types, i.e., Explicit, AltLex, Implicit, 
EntRel, and NoRel. The second column reports 
the number of unique expressions used to realize 
the relation ? Explicit, Implicit and AltLex ? 
while the third column reports the total number 
of tokens and relative frequencies.  
 
Relations Types Tokens (%) 
Explicit 49 189 (31.4%) 
Implicit 35 185 (30.7%) 
AltLex 25 37 (6.14%) 
EntRel NA 140 (23.25%) 
NoRel NA 51 (8.5%) 
TOTAL 109 602 
Table 1: Distribution of Discourse Relations 
 
These distributions show some interesting simi-
larities and differences with the PDTB distribu-
tions (cf. Prasad et al, 2008). First, given that 
Hindi has a much richer morphological paradigm 
than English; one would have expected that it 
would have fewer explicit connectives. That is, 
one might expect Hindi to realize discourse rela-
tions morphologically more often than not, just 
as it realizes other syntactic relations.  However, 
even in the small data set of 602 tokens that we 
have annotated so far, we have found 49 unique 
explicit connectives, which is roughly half the 
number reported for the 1 million words anno-
tated in English texts in PDTB. It is expected that 
we will find more unique types as we annotate 
additional data. The relation type distribution 
  
thus seems to suggest that the availability of 
richer morphology in a language doesn?t affect 
connective usage. Second, the percentage of Alt-
Lex relations is higher in HDRB ? 6.14% com-
pared to 1.5% in PDTB, suggesting that Hindi 
makes greater usage of non-connective cohesive 
links with the prior discourse. Further studies are 
needed to characterize the forms and functions of 
AltLex expressions in both English and Hindi. 
 
Senses of Discourse Relations: We also ex-
amined the distributions for each sense class in 
HDRB and computed the relative frequency of 
the relations realized explicitly and implicitly. 
Cross-linguistically, one would expect languages 
to be similar in whether or not a relation with a 
particular sense is realized explicitly or implicit-
ly, since this choice lies in the domain of seman-
tics and inference, rather than syntax. Thus, we 
were interested in comparing the sense distribu-
tions in HDRB and PDTB. Table 2 shows these 
distributions for the top class level senses. (Here 
we counted the AltLex relations together with 
explicit connectives.) 
 
Sense Class Explicit (%) Implicit (%) 
Contingency 57 (58.2%) 41 (41.8%) 
Comparison 68 (76.5%) 21 (23.5%) 
Temporal 43 (65.2%) 23 (34.8%) 
Expansion 64(40%) 94(60%) 
Table 2: Distribution of Class Level Senses 
 
The table shows that sense distributions in 
HDRB are indeed similar to those reported in the 
PDTB (cf. Prasad et al, 2008). That is, the 
chances of ?Expansion? and ?Contingency? rela-
tions being explicit are lower compared to 
?Comparison? and ?Temporal? relations.    
5 Summary and Future Work 
This paper has reported on the Hindi Discourse 
Relation Bank (HDRB) project, in which dis-
course relations, their arguments, and their 
senses are being annotated. A major goal of our 
work was to investigate how well the Penn Dis-
course Treebank (PDTB) and its guidelines could 
be adapted for discourse annotation of Hindi 
texts. To a large extent, we have successfully 
adapted the PDTB scheme. Proposed changes 
have to do with identification of some new syn-
tactic categories for explicit connectives, and 
some general and language-driven modifications 
to the sense classification. From our initial anno-
tations, we found that (a) there doesn?t seem to 
be an inverse correlation between the usage fre-
quency of explicit connectives and the morpho-
logical richness of a language, although there 
does seem to be an increased use of cohesive 
devices in such a language; and (b) sense distri-
butions confirm the lack of expectation of cross-
linguistic ?semantic? differences. Our future goal 
is to complete the discourse annotation of a 200K 
word corpus, which will account for half of the 
400K word corpus being also annotated for syn-
tactic dependencies. We also plan to extend the 
annotation scheme to include attributions.   
 
Acknowledgements 
This work was partially supported by NSF grants 
EIA-02-24417, EIA-05-63063, and IIS-07-
05671.  
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. Proc. of IJCNLP-2008.  
Lucie Mladov?, ??rka Zik?nov? and Eva Haji?ov?. 
2008. From Sentence to Discourse: Building an 
Annotation Scheme for Discourse Based on Prague 
Dependency Treebank. Proc. of LREC-2008. 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie 
Webber. 2008. The Penn Discourse TreeBank 2.0. 
Proc. of LREC-2008.  
Rashmi Prasad, Samar Husain, Dipti Mishra Sharma, 
and Aravind Joshi. 2008. Towards an Annotated 
Corpus of Discourse Relations in Hindi. Proc. of 
IJCNLP-2008. 
 Eve Sweetser.1990. From etymology to pragmat-
ics: Metaphorical and cultural aspects of se-
mantic structure .   Cambridge University Press. 
Nianwen Xue. 2005. Annotating Discourse Connec-
tives in the Chinese Treebank. Proc. of the ACL 
Workshop on Frontiers in Corpus Annotation 
II: Pie in the Sky.  
Deniz Zeyrek and Bonnie Webber. 2008. A Discourse 
Resource for Turkish: Annotating Discourse Con-
nectives in the METU Corpus. Proc. of IJCNLP-
2008.  
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 162?165,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Simple Parser for Indian Languages in a Dependency Framework 
 
Akshar Bharati, Mridul Gupta, Vineet Yadav, Karthik Gali and Dipti Misra Sharma 
Language Technologies Research Center, 
IIIT-Hyderabad, India 
{mridulgupta,vineetyadav}@students.iiit.ac.in, 
karthikg@research.iiit.ac.in,dipti@iiit.ac.in 
 
 
Abstract 
This paper is an attempt to show that an inter-
mediary level of analysis is an effective way 
for carrying out various NLP tasks for linguis-
tically similar languages. We describe a 
process for developing a simple parser for 
doing such tasks. This parser uses a grammar 
driven approach to annotate dependency rela-
tions (both inter and intra chunk) at an inter-
mediary level. Ease in identifying a particular 
dependency relation dictates the degree of 
analysis reached by the parser. To establish ef-
ficiency of the simple parser we show the im-
provement in its results over previous gram-
mar driven dependency parsing approaches for 
Indian languages like Hindi. We also propose 
the possibility of usefulness of the simple 
parser for Indian languages that are similar in 
nature. 
1 Introduction and Related Work 
Broad coverage parsing is a challenging task. For 
languages such as the Indian languages, it be-
comes all the more difficult as these languages 
are morphologically richer and the word order 
for these languages is relatively variable and less 
bound. Although dependency grammar driven 
parsing is much better suited for such type of 
languages (Hudson, 1984; Mel?Cuk, 1988), ro-
bust broad coverage parsing (Bharati et. al, 2008) 
still involves extensive analysis. Achieving good 
results in parsing for these languages may re-
quire large amount of linguistic resources such as 
annotated corpora, verb frames, lexicon etc. On 
the other hand, pure shallow parsing techniques 
(PVS and Gali, 2007) are not enough for provid-
ing sufficient information for applications such 
as machine translation, query answering etc.  
It is here that the notion of a simple parser is 
born where the idea is to parse a sentence at a 
coarser level. One could go to a finer level of 
parse depending on the ease with which such a 
parse can be generated. The simple parser that 
we describe here is a grammar oriented model 
that makes use of linguistic features to identify 
relations. We have modeled the simple parser on 
the Paninian grammatical model (Begum et al, 
2008; Bharati et al, 1995) which provides a de-
pendency grammar framework. Paninian depen-
dency grammar works well for analyzing Indian 
languages (Bharati et al, 1993).  We have fol-
lowed karaka1 based approach for parsing. 
An effort has been previously made in gram-
mar driven parsing for Hindi by us (Gupta et al, 
2008) where the focus was not to mark relations 
in a broad coverage sense but to mark certain 
easily identifiable relations using a rule base. In 
this paper, we show improvements in results over 
our previous work by including some additional 
linguistic features which help in identifying rela-
tions better. Our previous work focused only on 
inter-chunk annotation. In this paper, however, 
we have worked on both inter as well as intra 
chunk annotation. We later show their effective-
ness and results at different levels of dependency 
annotation. We also propose how useful the sim-
ple parser is for Indian languages which are simi-
lar in nature. 
2 Paninian Dependency Annotation 
Scheme at Various Levels 
Paninian dependency scheme is based on a mod-
ifier-modified relationship (Bharati et al, 1995). 
The modified chunk (or group) is classified on 
the basis of its part of speech category. A hie-
rarchy of dependency relations is thus estab-
lished on the basis of this category. For example, 
all those relations whose parent (modified group) 
is a verb are classified under the verb modifier 
(vmod) category. Subsequent levels further clas-
sify these relations (or labels). Depth of a level in 
the hierarchy reflects the fineness of the depen-
dency relations/labels. There are five labels at the 
                                               
1 The elements modifying the verb participate in the action 
specified by the verb. These participant relations with the 
verb are called karakas. 
162
coarsest level namely, vmod, nmod (noun mod-
ifier), jjmod (adjective modifier), advmod (ad-
verbial modifier) and ccof (conjunct of). 
Athough, ccof is not strictly a dependency rela-
tion (Begum et al, 2008). Figure 1 shows the 
hierarchy of relations used in the scheme. 
 
 
Figure 1: Hierarchy of Dependency Labels. 
 
The next level comprises of varg (verb argu-
ment), vad (verb adjunct) and vmod_1 2  labels 
under vmod. Under the nmod label, nmod_adj 
(adjective), r6 (genitive) are classified. At the 
most fine grained level, varg and vad further 
branch out into labels like k1, k2, k3, k5, k7 and 
rh, rt, rd, k1s etc. The relations under varg are the 
six karakas that are the most essential partici-
pants in an action. All the other dependency la-
bels3 are non-karakas (for a more detailed expla-
nation see Begum et al (2008) and Bharati et al 
(1995)). 
Languages often have constructions that are 
ambiguous, owing to similar feature and context 
distribution. Thus, in such cases, it is appropriate 
to under-specify the relations (labels) or group 
some of them together. Also, some labels have 
very less frequency of occurrence in the corpus 
and it is thus appropriate to leave them out for 
marking by the simple parser. One can later, on 
the availability of more information, try to identi-
fy and mark such instances with appropriate la-
bels. 
The dependency tagset described in this sec-
tion is used to mark inter-chunk relations. For 
marking relations between words within a chunk 
(intra-chunk), a similar tagset has been devel-
oped. 
                                               
2 vmod_1: A dependency relation in the vmod category, that 
exists between a non-finite verb and its parent verb. It has 
been under-specified for simplicity. 
3A comprehensive list of the dependency tagset can be 
found at http://ltrc.iiit.ac.in/MachineTrans/research/tb/dep-
tagset.pdf 
3 Procedure 
Our approach is corpus based where rules have 
been crafted after studying the corpus. We used 
the Hyderabad Dependency Treebank (HyDT) 
for development and testing our rules. The tree-
bank consists of about 2100 sentences in Hindi, 
of which 1800 were part of the development set 
and 300 were used as test data. Each sentence is 
POS tagged and chunked (Bharati et al, 2006) in 
SSF format (Bharati et al, 2005). 
3.1 Approach 
The simple parser we propose here is a language 
independent engine that takes a rule file specific 
for a particular language (Gupta et. al, 2008). 
Indian languages are similar in various respects 
(Emeneau 1956; 1980). Hence, rules made for 
one language can be efficiently transferred for 
other similar languages. However, there can be 
cases where rules for one language may not work 
for another. These cases can be handled by add-
ing some new rules for that particular language. 
The relative closeness among such languages, 
determines the efficiency of transference of rules 
from one language to another. We have taken 
Hindi and Punjabi, as example languages to sup-
port our proposal. 1(a) below is in Hindi, 
 
1(a). raama  ko      mithaaii acchii    nahii 
     ?Ram - dat?      ?sweets?          ?good?     ?not? 
       lagatii. 
      ?appear? 
 
?Ram does not like sweets.? 
 
Its corresponding Punjabi sentence, 
1(b).  raama   nuu  mitthaai   changii        nii                   
        ?Ram - dat?   ?sweets?  ?good?       ?not? 
     lagadii. 
     ?appear? 
 
?Ram does not like sweets.? 
 
Now, the rules for identifying k14 and k2 in 
Hindi are similar to that of Punjabi. For instance, 
in both the cases, the noun chunk possessing a 
nominative case marker (chunks take the proper-
ties of their heads) and the TAM (tense, aspect 
and modality of the main verb) should agree in 
                                               
4
 k1 (karta) and k2 (karma) are syntactico-semantic labels 
which have some properties of both grammatical roles and 
thematic roles. k1 for example, behaves similar to subject 
and agent. Likewise, k2 behaves like object/theme (Begum 
et al, 2008) 
163
GNP for the noun to be a k2. It is easy to see 
how rules made for identifying certain relations 
in Hindi can be transferred to identify the same 
relations in Punjabi and similarly for other lan-
guages. However, not all rules can be transferred 
from one language to another. 
3.2 Intra-chunk Relations 
We also mark intra-chunk dependency relations. 
The procedure of marking intra-chunk labels is 
also rule based. Rules have been crafted using a 
common POS5 tagset for Indian languages (Bha-
rati et al, 2006). Rules can be applied to other 
languages. However, some rules may not work. 
In those cases we need to add some rules specific 
to the language. The rule format is a five-tuple 
containing the following fields, 
1. Modified word 
2. Modified constraints 
3. Modifier word 
4. Modifier constraints 
5. Dependency relation 
Rules for marking intra-chunk relations have 
been marked studying the POS tagged and 
chunked corpus. Commonly occurring linguistic 
patterns between two or more nodes are drawn 
out in the form of statistics and their figures are 
collected. Such patterns are then converted into 
robust rules. 
4 Experiments and Results 
We conducted experiments using the simple 
parser to establish its efficacy in identifying a 
particular set of relations explained in section 2. 
Experiments were conducted on gold standard 
test data derived from HyDT. The experiments 
were carried out on Hindi. 
4.1 Marking Relations at Various Levels 
We marked dependency labels at various levels 
described above using the proposed simple pars-
er. The results are shown below We report two 
measures for evaluation, labeled (L) and labeled 
attachment (LA). Table 1 shows results for mark-
ing relations at the top most level (cf. Figure 1).  
It should be noted that we have not marked re-
lations like jjmod and advmod because the fre-
quency of their occurrence in the treebank is 
quite low. The focus is only on those relations 
whose frequency of occurrence is above a bare 
minimum (>15). The frequency of labels like 
jjmod and advmod is not above that threshold 
                                               
5 POS: Part of Speech 
value (Relations like k1 and k2 occur more than 
1500 times in the treebank). 
 
Relation 
Precision 
L LA 
 
Recall 
L     LA 
 
vmod 93.7% 83.0% 76.1% 67.4% 
nmod 83.6% 79.1% 77.5% 73.3% 
ccof 92.9% 82.9% 53.5% 50.4% 
Total 91.8% 82.3% 72.9% 65.4% 
Table 1. Figures for relations at the highest level. 
 
Table 2 below depicts the figures obtained for 
the next level. 
Relation 
Precision 
L LA 
 
Recall 
L     LA 
 
varg 77.7% 69.3% 77.9% 69.4% 
vad 75.2% 66.6% 30.3% 26.9% 
vmod_1 89.6% 75.8% 46.0% 38.9% 
r6 83.2% 78.5% 90.2% 85.2% 
nmod__adj 77.8% 77.8% 10.9% 10.9% 
Total 79.1% 71.2% 64.6% 58.2% 
Table 2. Figures for level 2. 
 
In section 1, improvement in marking certain 
relations over our previous attempt (Gupta et. al, 
2008) was mentioned. We provide a comparison 
of the results for the simple parser as opposed to 
the previous results. Figures shown in table 3 
have been reproduced for comparing them 
against the results of the simple parser shown in 
this paper. 
 
Relation 
Precision 
L LA 
 
Recall 
L LA 
 
k1 66.0% 57.7% 65.1% 57.6% 
k2 31.3% 28.3% 27.8% 25.1% 
k7(p/t) 80.8% 77.2% 61.0% 58.4% 
r6 82.1% 78.7% 89.6% 85.8% 
nmod__adj 23.2% 21.9% 27.4% 25.8% 
Table 3. Figures reproduced from our previous 
work. 
 
Table 4 shows results of the simple parser. 
Note the improvement in precision values for all 
the relations.  
 
 
Relation 
Precision 
L LA 
 
Recall 
L LA 
 
k1 72.6% 68.0% 67.9% 63.5% 
k2 61.6% 54.1% 29.9% 26.2% 
k7(p/t) 84.6% 77.9% 73.5% 68.7% 
r6 83.2% 78.6% 90.2% 85.5% 
nmod__adj 77.8% 77.8% 10.9% 10.9% 
pof 89.4% 87.7% 25.7% 25.2% 
Table 4. Figures for simple parser. 
164
4.2 Intra-chunk Experiments 
We also carried out some experiments to deter-
mine the efficiency of the simple parser with re-
spect to annotating intra-chunk relations for Hin-
di. Results shown below were obtained after test-
ing the simple parser using gold standard test 
data of about 200 sentences. Table 5 shows fig-
ures for labeled accuracy as well as labeled at-
tachment accuracy. 
 
Relation 
Precision 
L LA 
 
Recall 
L LA 
 
nmod 100% 89.3% 70.0% 62.5% 
nmod__adj 100% 92.7% 85.2% 79.0% 
nmod__dem 100% 100% 100% 100% 
nmod__qf 97.0% 92.4% 80.0% 76.2% 
pof 84.5% 82.1% 94.5% 92.0% 
ccof 91.8% 80.0% 70.9% 62.0% 
jjmod__intf 100% 100% 100% 100% 
Total 96.2% 90.4% 82.6% 77.7% 
Table 5. Figures for intra-chunk annotation. 
5 Conclusion 
We introduced the notion of a simple parser for 
Indian languages which follows a grammar dri-
ven methodology. We compared its performance 
against previous similar attempts and reported its 
efficiency. We showed how by using simple yet 
robust rules one can achieve high performance in 
the identification of various levels of dependency 
relations. 
The immediate tasks for the near future would 
be to identify relative clauses in order to reduce 
labeled attachment errors and hence to come up 
with rules for better identification of clauses. We 
also intend to thoroughly test our rules for Indian 
languages that are similar in nature and hence 
evaluate the efficiency of the simple parser. 
Acknowledgements  
We sincerely thank Samar Husain, for his impor-
tant role in providing us with valuable linguistic 
inputs and ideas. The treebank (Hyderabad de-
pendency treebank, version 0.05) used, was pre-
pared at LTRC, IIIT-Hyderabad. 
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. In Proceedings of IJCNLP-2008. 
Akshar Bharati, Vineet Chaitanya and Rajeev Sangal. 
1995. Natural Language Processing: A Pani-
nian Perspective, Prentice-Hall of India, New 
Delhi, pp. 65-106. 
Akshar Bharati, Samar Husain, Dipti Misra Sharma, 
and Rajeev Sangal. 2008. A Two-Stage Constraint 
Based Dependency Parser for Free Word Order 
Languages. In Proc. of the COLIPS Interna-
tional Conference on Asian Language 
Processing 2008 (IALP). Chiang Mai, Thailand. 
2008. 
Akshar Bharati and Rajeev Sangal. 1993. Parsing Free 
Word Order Languages in the Paninian Frame-
work, ACL93: Proc. of Annual Meeting of As-
sociation for Computational Linguistics. 
Akshar Bharati, Rajeev Sangal and Dipti M. Sharma. 
2005. ShaktiAnalyser: SSF Representation.  
Akshar Bharati, Rajeev Sangal, Dipti Misra Sharma 
and Lakshmi Bai. 2006. AnnCorra: Annotating 
Corpora Guidelines for POS and Chunk Annota-
tion for Indian Languages. Technical Report 
(TR-LTRC-31), Language Technologies Re-
search Centre IIIT, Hyderabad 
http://ltrc.iiit.ac.in/MachineTrans/publications
/technicalReports/tr031/posguidelines.pdf 
Murray B. Emeneau. 1956. India as a linguistic area. 
Linguistics, 32:3-16. 
Murray B. Emeneau. 1980. Language and linguis-
tic area. Essays by Murray B. Emeneau. Se-
lected and introduced by Anwar S. Dil. Stan-
ford University Press. 
Mridul Gupta, Vineet Yadav, Samar Husain and Dipti 
M. Sharma. 2008. A Rule Based Approach for Au-
tomatic Annotation of a Hindi Treebank. In Proc. 
Of the 6th International Conference on Natu-
ral Language Processing (ICON-08), CDAC 
Pune, India. 
R. Hudson. 1984. Word Grammar, Basil Blackwell, 
Oxford, OX4 1JF, England. 
I. Mel'cuk . 1988. Dependency Syntax: Theory and 
Practice, State University, Press of New York. 
Avinesh PVS and Karthik Gali. 2007. Part-of-speech 
tagging and chunking using conditional random 
fields and transformation based learning. In Proc. 
Of IJCAI-07 Workshop on ?Shallow Parsing 
in South Asian Languages?, 2007. 
165
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 186?189,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Multi-Representational and Multi-Layered  Treebank for Hindi/Urdu     Rajesh Bhatt  U. of Massachusetts                      Amherst, MA, USA                                    bhatt@linguist.umass.edu  Owen Rambow      Columbia University     New York, NY, USA   rambow@ccls.columbia.edu           
Bhuvana Narasimhan  U. of Colorado                      Boulder, CO, USA                                    narasimb@colorado.edu  Dipti Misra Sharma    Int?l Institute of Info. Technology  Hyderabad, India   dipti@iiit.ac.in                                           
Martha Palmer U. of Colorado Boulder, CO, USA mpalmer@colorado.edu  Fei Xia     University of Washington Seattle, WA, USA fxia@u.washington.edu 
Abstract 
This paper describes the simultaneous develop-ment of dependency structure and phrase structure treebanks for Hindi and Urdu, as well as a Prop-Bank.  The dependency structure and the Prop-Bank are manually annotated, and then the phrase structure treebank is produced automatically.  To ensure successful conversion the development of the guidelines for all three representations are care-fully coordinated.  1 Introduction Annotated corpora have played an increasingly important role in the training of supervised natu-ral language processing components. Today, treebanks have been constructed for many lan-guages, including Arabic, Chinese, Czech, Eng-lish, French, German, Korean, Spanish, and Turkish.  This paper describes the creation of a Hindi/Urdu multi-representational and multi-layered treebank.  Multi-layered means that we design the annotation process from the outset to include both a syntactic annotation and a lexical semantic annotation such as the English Prop-Bank (Palmer et al 2005). Multi-representational means that we distinguish con-ceptually what is being represented from how it is represented; for example, in a case of long-distance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent).  Having made this choice, we can determine how to represent it: For exam-ple, we can use a discontinuous constituent 
(crossing arcs), or we can use a trace and co-indexation.      Flexibility of representation is important be-cause the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al 1999) the automatic mapping from depend-ency to phrase-structure was a major area of re-search. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic represen-tations that applications can make immediate use of (McDonald et al 2005, CoNLL 2006 Shared Task).  We first provide a comparison of de-pendency structure and phrase structure in Sec-tion 2.  Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our con-version approach. 2 Two Kinds of Syntactic Structure  Two different approaches to describing syntactic structure, dependency structure (DS) (Mel??uk 1979) and phrase structure (PS) (Chomsky, 1981), have in a sense divided the field in two, with parallel efforts on both sides.  Formally, in a PS tree, all and only the leaf nodes are labeled 
186
with words from the sentence (or empty catego-ries), while the interior nodes are labeled with nonterminal labels. In a dependency tree, all nodes are labeled with words from the sentence (or empty categories). Linguistically, a PS groups consecutive words hierarchically into phrases (or constituents), and each phrase is as-signed a syntactic label. In a DS, syntactic de-pendency (i.e., the relation between a syntactic head and its arguments and adjuncts) is the pri-mary syntactic relation represented. The notion of constituent is only derived.   In a dependency representation, a node stands for itself, for the lexical category (or ?preterminal?) spanning only the word itself (e.g., N), and for its maximal projection spanning the node and all words in the subtree it anchors (e.g., NP). Thus, intermediate projections which cover only some of the dependents of a word (such as N? or VP) do not directly correspond to anything in a de-pendency representation. Attachments at the dif-ferent levels of projection are therefore not dis-tinguished in a dependency tree. This has certain ramifications for annotation.  Conisder for ex-ample scope in conjunctions.  The two readings of young men and women can be distinguished (are the women young as well or not?). If a de-pendency representation represents conjunction by treating the conjunction as a dependent to the first conjunct, then the two readings do not re-ceive different syntactic representations, unless a scope feature is introduced for the adjective.  Suppose y depends on x in a DS, we need to ad-dress the following questions in order to devise a DS-to-PS conversion algorithm that builds the corresponding phrase structure: 1) What kinds of projections do x and y have? 2) How far should y project before it attaches to x's projection? 3) What position on x's projection chain should y's projec-tion attach to?  These questions are answered by the annotation manual of the target PS represen-tation ? there are many possible answers. If the source dependency representation contains the right kind of information (for example, the scope of adjectives in conjunctions), and if the target phrase structure representation is well docu-mented, then we can devise a conversion algo-rithm.  Another important issue is that of ?non-projectivity? which is used to represent discon-tinuous constituents. Non-projectivity is common in dependency-based syntactic theories, but rare in phrase structure-based theories.  The next sec-
tion highlights our most salient representation choices in Treebank design. 3 Treebank Design Our goal is the delivery of a treebank that is multi-representational: it will have a syntactic dependency version and a phrase structure ver-sion. Another recent trend in treebanking is the addition of deeper, semantic levels of annotation on top of the syntactic annotations of the PTB, for example PropBank (Palmer et al 2005).  A multi-layered approach is also found in the Pra-gue Dependency Treebank (Haji? et al 2001), or in treebanks based on LFG (King et al 2003) or HPSG (Oepen et al 2002). A lesson learned here is that the addition of deeper, more semantic lev-els may be complicated if the syntactic annota-tion was not designed with the possibility of mul-tiple layers of annotation in mind. We therefore also propose a treebank that is from the start multi-layered: we will include a PropBank-style predicate-argument annotation in the release. Crucially, the lexical subcategorization frames that are made explicit during the process of prop-banking should always inform the syntactic structure of the treebanking effort. In addition, some of the distinctions made by PS that are not naturally present in DS, such as unaccusativity and null arguments, are more naturally made dur-ing PropBank annotation. Our current approach anticipates that the addition of the PropBank an-notation to the DS will provide a rich enough structure for accurate PS conversion.   In order to ensure successful conversion from DS to PS, we are simultaneously developing three sets of guidelines for Hindi: dependency struc-ture, phrase structure, and PropBank. While al-lowing DS and PS guidelines to be based on dif-ferent, independently motivated principles (see Section 4), we have been going through a com-prehensive list of constructions in Hindi, care-fully exploring any potentially problematic is-sues.  Specifically, we make sure that both DS and PS represent the same syntactic facts (what is represented): we know that if PS makes a dis-tinction that neither DS nor PropBank make, then we cannot possibly convert automatically. Fur-thermore, we coordinate the guidelines for DS and PS with respect to the examples chosen to support the conversion process.  These examples form a conversion test suite.  
187
4 Syntactic Annotation Choices  4.1 Dependency Structure Guidelines  Our dependency analysis is based on the Pan-inian grammatical model (Bharati et al1999, Sharma et al 2007). The model offers a syntac-tico-semantic level of linguistic knowledge with an especially transparent relationship between the syntax and the semantics.  The sentence is treated as a series of modifier-modified relations which has a primary modified (generally the main verb). The appropriate syntactic cues (rela-tion markers) help in identifying various rela-tions.  The relations are of two types ? karaka and others. 'Karakas' are the roles of various par-ticipants in an action (arguments). For a noun to hold a karaka relation with a verb, it is important that they (noun and verb) have a direct syntactic relation. Relations other than 'karaka' such as purpose, reason, and possession are also captured using the relational concepts of the model (ad-juncts). These argument labels are very similar in spirit to the verb specific semantic role labels used by PropBank, which have already been suc-cessfully mapped to richer semantic role labels from VerbNet and FrameNet. This suggests that much of the task of PropBanking can be done as part of the dependency annotation. 4.2 Phrase Structure Guidelines Our PS guidelines are inspired by the Principles-and-Parameters methodology, as instantiated by the theoretical developments starting with Gov-ernment and Binding Theory (Chomsky 1981). We assume binary branching. There are three theoretical commitments/design considerations that underlie the guidelines. First, any minimal clause distinguishes at most two positions struc-turally (the core arguments). These positions can be identified as the specifier of VP and the com-plement of V. With a transitive predicate, these positions are occupied by distinct NPs while with an unaccusative or passive, the same NP occu-pies both positions. All other NPs are represented as adjuncts. Second, we represent any displace-ment of core arguments from their canonical po-sitions, irrespective of whether a clause boundary is crossed, via traces. The displacement of other arguments is only represented if a clause bound-ary is crossed. Third, syntactic relationships such as agreement and case always require c-command but do not necessarily require a [speci-fier, head] configuration. Within these con-straints, we always choose the simplest structure 
compatible with the word order. We work with a very limited set of category labels (NP, AP, AdvP, VP, CP) assuming that finer distinctions between different kinds of verbal functional heads can be made via features.  4.3 Two Constructions in Hindi We give examples for two constructions in Hindi and show the DS and PS for each. Simple Transitive Clauses:  (1) raam-ne   khiir              khaayii    ram-erg   rice-pudding     ate    ?Ram ate rice-pudding.? The two main arguments of the Hindi verb in Figure 1(b) have dependency types k1 and k2.  They correspond roughly to subject and object, and they are the only arguments that can agree with the verb.  In the PS, Figure 1(a), the two arguments that correspond to k1 and k2 have fixed positions in the phrase structure as ex-plained in Section 4.2. 
 Figure 1: PS and DS for transitive clause in (1).  Unaccusative verbs: (2) darwaazaa  khul   rahaa           hai       door.M        open  Prog.MSg   be.Prs.Sg      ?The door is opening.?  Here, the issue is that the DS guidelines treats unaccusatives like other intransitives, with the surface argument simply annotated as k1.  In contrast, PS shows a derivation in which the sub-ject originates in object position.   
 Figure 2: PS and DS for the unaccusative  in  (2). 5 Conversion Process  The DS-to-PS conversion process has three steps. First, for each (DS, PS) pair appearing in the conversion test suite, we run a consistency 
188
checking algorithm to determine whether the DS and the PS are consistent. The inconsistent cases are studied manually and if the inconsistency cannot be resolved by changing the analyses used in the guidelines, a new DS that is consis-tent with the PS is proposed. We call this new dependency structure ?DScons? (?cons? for ?con-sistency?; DScons is the same as DS for the con-sistent cases). Because the DS and PS guidelines are carefully coordinated, we expect the incon-sistent cases to be rare and well-motivated. Sec-ond, conversion rules are extracted automatically from these (DScons, PS) pairs. Last, given a new DS, a PS is created by applying conversion rules. Note that non-projective DSs will be converted to projective DScons.  (For an alternate account of handling non-projective DSs, see Kuhlman and M?hl (2007).)  A preliminary study on the Eng-lish Penn Treebank showed promising results and error analyses indicated that most conversion errors were caused by ambiguous DS patterns in the conversion rules. This implies that including sufficient information in the input DS could re-duce ambiguity, significantly improving the per-formance of the conversion algorithm. The de-tails of the conversion algorithm and the experi-mental results are described in (Xia et al, 2009). 6 Conclusion We presented our approach to the joint develop-ment of DS and PS treebanks and a PropBank for Hindi/Urdu.  Since from the inception of the pro-ject we have planned manual annotation of DS and automatic conversion to PS, we are develop-ing the annotation guidelines for all structures in parallel.  A series of linguistic constructions with specific examples are being carefully examined for any DS annotation decisions that might result in inconsistency between DS and PS and/or mul-tiple conversion rules with identical DS patterns. Our preliminary studies yield promising results, indicating that coordinating the design of DS/PS and PropBank guidelines and running the con-version algorithm in the early stages is essential to the success of building a multi-representational and multi-layered treebank.   Acknowledgments This work is supported by NSF grants CNS-0751089, CNS-0751171, CNS-0751202, and CNS-0751213.    
References  A. Bharati, V. Chaitanya and R. Sangal. 1999. Natu-ral Language Processesing: A Paninian Per-spective, Prentice Hall of India, New Delhi. N. Chomsky. 1981. Lectures on Government and Binding: The Pisa Lectures. Holland: Foris Pub-lications.  M. Collins, Jan Haji?, L. Ramshaw and C. Tillmann. 1999. A Statistical Parser for Czech. In the Proc of ACL-1999, pages 505-512. J. Haji?, E. Hajicova, M. Holub, P. Pajas, P. Sgall, B. Vidova-Hladka, and V. Reznickova. 2001. The Current Status of the Prague Dependency Tree-bank. Lecture Notes in Artificial Intelligence (LNAI) 2166, pp 11?20, NY. T. H. King, R. Crouch, S. Riezler, M. Dalrymple and R. Kaplan. 2003. The PARC700 Dependency Bank. In Proc. of the 4th Int? Workshop on Linguistically Interpreted Corpora (LINC-2003), Budapest, Hungary. D. Klein and C. D. Manning. 2003. Accurate Unlexi-calized Parsing. In the Proc of ACL-2003,.Japan M. Kuhlmann and M. M?hl. 2007. Mildly context-sensitive dependency language. In the Proc of ACL 2007. Prague, Czech Republic. R. McDonald, F. Pereira, K. Ribarov and J. Haji?. 2005.  Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proc. of HLT-EMNLP 2005.  I. Mel?uk. 1979. Studies in Dependency Syntax. Karoma Publishers, Inc. S. Oepen, K. Toutanova, S. M. Shieber, C. D. Man-ning, D. Flickinger, and T. Brants, 2002. The LinGO Redwoods Treebank: Motivation and Pre-liminary Applications. In Proc. of COLING, 2002. Taipei, Taiwan. M. Palmer, D. Gildea, P. Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Seman-tic Roles.Computational Linguistics, 31(1):71-106. D. M. Sharma, R. Sangal, L. Bai, R. Begam, and K.V. Ramakrishnamacharyulu. 2007. AnnCorra : TreeBanks for Indian Languages, Annotation Guidelines (manuscript), IIIT, Hyderabad, India.  F. Xia, O. Rambow, R. Bhatt, M. Palmer and D. Sharma, 2009. Towards a Multi-Representational Treebank.  In Proc. of the 7th Int?lWorkshop on Treebanks and Linguistic Theories (TLT-7).  
189
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 77?80,
Paris, October 2009. c?2009 Association for Computational Linguistics
Two stage constraint based hybrid approach to free word order lan-
guage dependency parsing
Akshar Bharati, Samar Husain, Dipti Misra and Rajeev Sangal
Language Technologies Research Centre, IIIT-Hyderabad, India
{samar, dipti, sangal}@mail.iiit.ac.in
Abstract
The paper describes the overall design of a 
new two stage constraint based hybrid ap-
proach to dependency parsing. We define 
the  two  stages  and  show  how  different 
grammatical construct are parsed at appro-
priate stages. This division leads to selec-
tive identification and resolution of specif-
ic dependency relations at the two stages. 
Furthermore,  we  show  how  the  use  of 
hard constraints and soft constraints helps 
us  build  an  efficient  and  robust  hybrid 
parser.  Finally,  we  evaluate  the  imple-
mented parser on Hindi and compare the 
results with that of two data driven depen-
dency parsers.
1 Introduction
Due to the availability of annotated corpora for 
various  languages  since  the  past  decade,  data 
driven parsing has proved to be immensely suc-
cessful.  Unlike  English,  however,  most  of  the 
parsers for morphologically rich free word order 
(MoR-FWO) languages (such as Czech, Turkish, 
Hindi, etc.) have adopted the dependency gram-
matical  framework.  It  is  well  known  that  for 
MoR-FWO  languages,  dependency  framework 
provides ease of linguistic analysis and is much 
better  suited to account  for  their  various struc-
tures (Shieber, 1975; Mel'Cuk, 1988; Bharati et 
al., 1995). The state of the art parsing accuracy 
for many MoR-FWO languages is still low com-
pared  to  that  of  English.  Parsing  experiments 
(Nivre et al,  2007; Hall  et  al.,  2007) for  these 
languages have pointed towards various reasons 
for this low performance. For Hindi1, (a) difficul-
ty in extracting relevant linguistic cues, (b) non-
projectivity,  (c)  lack  of  explicit  cues,  (d)  long 
distance  dependencies,  (e)  complex  linguistic  
phenomena,  and (f)  less corpus size, have been 
suggested (Bharati  et  al.,  2008) for  low perfor-
1  Hindi is a verb final language with free word order and 
a rich case marking system. It is one of the official lan-
guages of India, and is spoken by ~800 million people.
mance.  The  approach  proposed  in  this  paper 
shows how one can minimize these adverse ef-
fects and argues that a hybrid approach can prove 
to be a better option to parsing such languages. 
There have been, in the past, many attempts to 
parsing using constraint based approaches. Some 
recent  works include (Debusmann et  al.,  2004; 
Schr?der, 2002; Bharati et al, 1993).
The  paper  describes  the  overall  design  of  a 
new two stage constraint based hybrid approach 
to dependency parsing. We define the two stages 
and  show how different  grammatical  construct 
are  parsed  at  appropriate  stages.  This  division 
leads to selective identification and resolution of 
specific  dependency  relations  at  two  different 
stages.  Furthermore,  we  show  how  the  use  of 
hard  constraints  (H-constraints)  and  soft  con-
straints (S-constraints) helps us build an efficient 
and  robust  hybrid  parser.  Specifically,  H-con-
straints  incorporate  the  knowledge  base  of  the 
language  and  S-constraints  are  weights  corre-
sponding  to  various  constraints.  These  weights 
are automatically learnt from an annotated tree-
bank. Finally, we evaluate the implemented pars-
er on Hindi and compare the results with that of 
two data driven dependency parsers.
2 Two Stage Parsing
The parser tries to analyze the given input sen-
tence, which has already been POS tagged and 
chunked2, in 2 stages; it first tries to extract intra-
clausal3 dependency  relations.  These  relations 
generally correspond to the argument structure of 
the verb, noun-noun genitive relation, infinitive-
verb relation, infinitive-noun relation, adjective-
noun, adverb-verb relations, etc. In the 2nd stage 
it  then  tries  to  handle  more  complex  relations 
such as conjuncts, relative clause, etc. What this 
2  A chunk is a set of adjacent words which are in depen-
dency relation with each other, and are connected to the 
rest of the words by a single incoming arc. The parser 
marks relations between the head of the chunks (inter-
chunk relations); this is done to avoid local details and 
can be thought as a device for modularity.  
3  A clause is a group of word such that the group con-
tains a single finite verb chunk.
77
essentially means is  a 2-stage resolution of de-
pendencies, where the parser selectively resolves 
the dependencies of various lexical heads at their 
appropriate  stage,  for  example  verbs  in  the  1st 
stage  and  conjuncts  and  inter-verb  relations  in 
the 2nd  stage. The key ideas of the proposed lay-
ered  architecture  are:  (1)  There  are  two layers 
stages, (2) the 1st stage handles intra-clausal rela-
tions, and the 2nd stage handles inter-clausal rela-
tions, (3) the output of each layer is a linguisti-
cally valid partial parse that becomes, if neces-
sary, the input to the next layer, and (4) the out-
put of the final layer is the desired full parse.
By following the above approach we are able 
to get 4-fold advantage, (1) Each layer in effect 
does linguistically valid partial parsing, (2) by di-
viding  the  labels  into  different  functional  sets 
(intra-clausal  and  inter-clausal)  we localize  the 
dependencies  that  need  to  be  identified,  hence 
the  problem  of  long  distance  dependencies  is 
minimizes,  (3)  by  attacking  the  problem  in  a 
modular way, i.e. handling only individual claus-
es  at  1st stage,  we reduce non-projective  struc-
tures  significantly,  and  (4)  the  two stage  con-
straint  based approach can easily  capture  com-
plex linguistic cues that are difficult to learn via 
the data-driven parsers. We?ll revisit these points 
in Section 5. The 1st stage output for example 1 is 
shown in figure 1 (a).
Eg. 1: mai   ghar     gayaa   kyomki    mai 
          ?I?   ?home?  ?went?  ?because?  ?I?        
          bimaar   thaa
          ?sick?     ?was?
         ?I went home because I was sick?
Figure 1. Eg 1 (a): 1st stage output, (b): 2nd stage 
final parse
In figure 1a, the parsed matrix clause subtree 
?mai ghar gayaa? and the subordinate clause are 
attached to _ROOT_. The subordinating conjunct 
?kyomki? is  also seen attached to the _ROOT_. 
_ROOT_ ensures that the parse we get after each 
stage is connected and takes all the analyzed 1st 
stage sub-trees along with unprocessed nodes as 
its children. The dependency tree thus obtained 
in the 1st stage is partial, but linguistically sound. 
Later  in  the  2nd stage  the  relationship  between 
various clauses are identified. The 2nd stage parse 
for the above sentences is also shown in figure
1b.  Note  that  under  normal  conditions  the  2nd 
stage  does  not  modify  the  parse  sub-trees  ob-
tained from the 1st stage, it only establishes the 
relations between the clauses.
3 Hard and Soft Constraints
Both 1st and 2nd stage described in the previ-
ous  section  use  linguistically  motivated  con-
straints.  These  hard  constraints  (H-constraints) 
reflect that aspect of the grammar that in general 
cannot be broken. H-constraints comprise of lex-
ical  and  structural  knowledge  of  the  language. 
The H-constraints are converted into integer pro-
gramming  problem  and  solved  (Bharati  et  al., 
1995). The solution(s) is/are valid parse(s). The 
soft  constraints (S-constraints) on the other hand 
are learnt as weights from an annotated treebank. 
They reflect various preferences that a language 
has towards various linguistic phenomena. They 
are  used to  prioritize  the  parses  and select  the 
best parse. Both H & S constraints reflect the lin-
guistic realities of the language and together can 
be thought as the grammar of a language. Figure 
2 shows the overall design of the proposed parser 
schematically.
3.1 Hard Constraints 
The  core  language  knowledge  being  currently 
considered  that  cannot  be  broken  without  the 
sentence  being  called  ungrammatical  is  named 
H-constraints.  There  can  be  multiple  parses 
which can satisfy these H-constraints. This indi-
cates the  ambiguity in  the  sentence if  only the 
limited knowledge base is considered. Stated an-
other  way,  H-constraints  are  insufficient  to  re-
strict  multiple analysis of a given sentence and 
that  more  knowledge  (semantics,  other  prefer-
ences, etc.) is required to curtain the ambiguities. 
Moreover, we know that many sentences are syn-
tactically ambiguous unless one uses some prag-
matic knowledge, etc. For all such constructions 
there  are  multiple  parses.  As  described  earlier, 
H-constraints  are  used  during  intra-clausal  (1st 
stage)  and inter-clausal  (2nd stage)  analysis  (cf. 
Figure  2).  They  are  used  to  form  a  constraint 
graph which is converted into integer program-
ming equalities (or inequalities). These are then 
solved to get the final solution graph(s). Some of 
the H-constraints are: (1)  Structural constraints  
(ensuring the solution graph to be a tree,
78
Figure 2. Overall parser design
removing implausible language specific ungram-
matical  structures,  etc.),  (2)  Lexicon (linguistic 
demands  of various heads), and (3)  Other lexi-
cal constraints (some language specific  charac-
teristics), etc. 
3.2 Soft Constraints
The S-constraints on the other hand are the con-
straints which can be broken, and are used in the 
language as preferences. These are used during 
the prioritization stage. Unlike the H-constraints 
that are derived from a knowledge base and are 
used  to  form  a  constraint  graph,  S-constraints 
have  weights  assigned  to  them.  These  weights 
are automatically learnt using a manually anno-
tated  dependency  treebank.  The  tree  with  the 
maximum overall score is the best parse. Some 
such  S-constraints are,  (1)  Order of the argu-
ments, (2)  Relative position of arguments w.r.t.  
the verb, (3) Agreement principle, (4) Alignment  
of  prominence scale,  and (5)  Structural  prefer-
ences/General  graph properties  (mild  non-pro-
jectivity, valency, dominance, etc.), etc. 
4 Evaluation
Malt Parser (version 0.4) (Nivre et al, 2007), and 
MST  Parser  (version  0.4b)  (McDonald  et  al., 
2005) have been tuned for Hindi by Bharati et al 
(2008). Parsers were trained on a subset of a Hin-
di Treebank (Begum et al, 2008a). We use the 
same  experimental  setup  (parameters,  features, 
etc.) used by them and compare the results of the 
two data driven parsers with that of the proposed 
constraint  based  hybrid  parser  (CBP)  on  the 
same dataset4 in terms of
4 For details on the corpus type, annotation scheme, 
tagset, etc. see Begum et al (2008a).
unlabeled  attachments  (UA),  label  (L)  and  la-
beled  attachment  (LA)  accuracy.  In  Table  1, 
CBP? shows the performance of the system when 
a basic prioritizer is used, while CBP?? shows it 
for the best parse that is available in the first 25 
parses.  CBP  gives  the  accuracy  when  the  1st 
parse is selected. We show CBP?? to show that a 
good parse is available in as few as the first 25 
parses and that once the prioritizer is further im-
proved the overall performance will easily cross 
CBP??.
 UA LA L
CBP 86.1 63 65
CBP? 87.69 69.67 72.39
CBP? 90.1 75 76.9
MST 87.8 70.4 72.3
Malt 86.6 68.0 70.6
Table 1. Parser Evaluation
5 Observations
The initial results show that the proposed parser 
performs  better  than  the  state-of-the-art  data 
driven Hindi parsers. There are various reasons 
why we think that the proposed approach is bet-
ter  suited  to  parsing  MoR-FWO.  (1)  Complex 
linguistic cues can easily be encoded as part of 
various  constraints.  For  example,  it  has  been 
shown by Bharati  et  al.  (2008) that,  for  Hindi, 
complex  agreement  patterns,  though present  in 
the  data,  are  not  being  learnt  by  data  driven 
parsers. Such patterns along with other idiosyn-
cratic language properties can be easily incorpo-
rated as constraints, (2) Making clauses as basic 
parsing  unit  drastically  reduces  non-projective 
79
sentences.  Experiments  in  parsing  MoR-FOW 
have  shown that  such  non-projective  sentences 
impede parser performances (Bharati et al, 2008; 
Hall et al, 2007). Note that there will still remain 
some  intra-clausal  non-projective  structures  in 
the 1st stage, but they will be short distance de-
pendencies, (3) Use of H-constraints and S-con-
straints  together  reflect  the  grammar  of  a  lan-
guage. The rules in the form of H-constraints are 
complemented  by  the  weights  of  S-constraints 
learnt  from  the  annotated  corpus,  (4)  2  stage 
parsing lends  itself  seamlessly to  parsing com-
plex sentences by modularizing the task of over-
all parsing, (5) the problem of label bias (Bharati 
et  al.,  2008)  faced  by  the  data  driven  Hindi 
parsers for some cases does not arise here as con-
textually  similar  entities  are  disambiguated  by 
tapping  in  hard  to  learn  features,  (6)  Use  of 
clauses as basic parsing units reduces the search 
space at both the stages, (7) Parsing closely relat-
ed languages will become easy.
The performance of our parser is affected due 
to the following reasons,  (a)  Small lexicon (lin-
guistic  demands  of  various  heads):  The  total 
number of such demand frames which the parser 
currently uses is very low. There are a total of 
around 300 frames, which have been divided into 
20  verb  classes  (Begum et  al.,  2008b).  As  the 
coverage of this lexicon increases, the efficiency 
will automatically increase. (b)  Unhandled con-
structions: The parser still doesn?t handle some 
constructions, such as the case when a conjunct 
takes another conjunct as its dependent, and (c) 
Prioritization mistakes: As stated earlier the pri-
oritizer being used is basic and is still being im-
proved.  The  overall  performance  will  increase 
with the improvement of the prioritizer.
6 Conclusion
In this paper we proposed a new two stage con-
straint  based  hybrid  approach  to  dependency 
parsing.  We showed  how  by modularizing  the 
task of overall parsing into 2 stages we can over-
come many problems faced by data driven pars-
ing. We showed how in the 1st stage only intra-
clausal dependencies are handled and later in the 
2nd stage the inter-clausal dependencies are iden-
tified.  We also briefly  described the  use  of  H-
constraints  and  S-constraints.  We  argued  that 
such constraints complement each other in get-
ting the best parse and that together they repre-
sent the grammar of the language. We evaluated 
our  system  for  Hindi  with  two  data  driven 
parsers.  Initial  results  show  that  the  proposed 
parser performs better than those parsers. Finally, 
we argued why the proposed hybrid approach is 
better suited to handle the challenges posed by 
MoR-FWO and gave few pointers as how we can 
further improve our performance.
The proposed parser is still being improved at 
various  fronts.  To  begin  with  a  prioritization 
mechanism has to be improved. We need to en-
rich the verb frame lexicon along with handling 
some unhandled constructions. This will be taken 
up as immediate future work.
References 
R. Begum, S.  Husain, A. Dhwaj, D. Sharma, L. Bai, 
and  R.  Sangal.  2008a.  Dependency  annotation 
scheme for Indian languages. Proc. of IJCNLP08.
R. Begum, S. Husain, D. Sharma and L. Bai. 2008b. 
Developing  Verb  Frames  in  Hindi.  Proc.  of  
LREC08.
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma 
and R. Sangal. 2008. Two Semantic features make 
all  the  difference  in  Parsing  accuracy.  Proc.  of  
ICON-08.
A. Bharati and R. Sangal.  1993. Parsing Free Word 
Order  Languages  in  the  Paninian  Framework. 
Proc. of ACL: 93.
A. Bharati, V. Chaitanya and R. Sangal. 1995.  Natu-
ral Language Processing: A Paninian Perspective, 
Prentice-Hall of India, New Delhi. 
R. Debusmann, D. Duchier and G. Kruijff. 2004. Ex-
tensible  dependency grammar: A new methodolo-
gy.  Proceedings  of  the  Workshop on  Recent  Ad-
vances in Dependency Grammar, pp. 78?85.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, 
M.  Nilsson  and  M.  Saers.  2007.  Single  Malt  or 
Blended? A Study in Multilingual Parser Optimiza-
tion. Proc. of EMNLP-CoNLL shared task 2007.
R. McDonald,  F.  Pereira,  K. Ribarov,  and J.  Hajic. 
2005.  Non-projective  dependency  parsing  using 
spanning tree algorithms. Proc. of HLT/EMNLP.
I. A. Mel'Cuk. 1988. Dependency Syntax: Theory and 
Practice, State University Press of New York.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. NLE.
S.  M.  Shieber.  1985.  Evidence  against  the  context-
freeness  of  natural  language.  In  Linguistics  and 
Philosophy, p. 8, 334?343.
I.  Schr?der.  2002.  Natural  Language  Parsing  with 
Graded Constraints. PhD thesis, Hamburg Univ.
80
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 657?660,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Data Driven Dependency Parsing using Clausal Information 

Phani Gadde, Karan Jindal, Samar Husain, Dipti Misra Sharma, Rajeev Sangal 
Language Technologies Research Centre, IIIT-Hyderabad, India. 
phani.gadde@research.iiit.ac.in, karan_jindal@students.iiit.ac.in, 
{samar,dipti,sangal}@mail.iiit.ac.in 
 
 
 
 
 
Abstract 
The paper describes a data driven dependency 
parsing approach which uses clausal informa-
tion of a sentence to improve the parser per-
formance. The clausal information is added 
automatically during the parsing process. We 
demonstrate the experiments on Hindi, a lan-
guage with relatively rich case marking sys-
tem and free-word-order. All the experiments 
are done using a modified version of 
MSTParser. We did all the experiments on the 
ICON 2009 parsing contest data. We achieved 
an improvement of 0.87% and 0.77% in unla-
beled attachment and labeled attachment accu-
racies respectively over the baseline parsing 
accuracies. 
1 Introduction 
Linguistic analysis of morphologically rich free-
word-order languages (MoRFWO) using depen-
dency framework have been argued to be more 
effective (Shieber, 1985; Mel??uk, 1988, Bharati et 
al., 1993). Not surprisingly, most parsers for such 
languages are dependency based (Nivre et al, 
2007a; Bharati et al, 2008a; Hall et al, 2007). In 
spite of availability of annotated treebanks, state-
of-the-art parsers for MoRFWO have not reached 
the performance obtained for English. Some of the 
reasons stated for the low performance are small 
treebank size, complex linguistic phenomenon, 
long-distance dependencies, and non-projective 
structures (Nivre et al, 2007a, 2007b; Bharati et 
al., 2008a).   
Several approaches have been tried to handle these 
difficulties in such languages. For Hindi, Bharati et   
al. (2008a) and Ambati et al (2009) used semantic 
features in parsing to reduce the negative impact of 
unavailable syntactic features and showed that use 
of minimal semantics can help in identifying cer-
tain core dependency labels. Various attempts have 
proved to simplify the structure by dividing the 
sentence into suitable linguistic units (Attardi and 
Dell?Orletta 2008; Bharati et al, 1993, 2008b, 
2009; Husain et al, 2009). These approaches han-
dle complex structures by breaking the parsing 
process into several steps. Attardi and Dell'Orletta 
(2008) used chunk information as a feature to 
MaltParser (Nivre et al, 2007a) for parsing Eng-
lish. Bharati et al, 1993 used the notion of local 
word groups, while Bharati et al, 2009 and Husain 
et al, 2009 used clauses.   
In this paper, we describe a data driven depen-
dency parsing approach which uses clausal infor-
mation of a sentence to improve the parser 
performance. Previous attempts at data driven 
parsing for Hindi have failed to exploit this feature 
explicitly. The clausal information is added auto-
matically during the parsing process. We demon-
strate the experiments on Hindi1. All the 
experiments are done using a modified version of 
MSTParser (McDonald et al, 2005a and the refer-
ences therein) (henceforth MST) on the ICON 
2009 parsing contest2 (Husain, 2009) data. We 
achieved an improvement of 0.87% and 0.77% in 
unlabeled attachment and labeled attachment accu-
racies respectively over the baseline parsing accu-
racies. 
                                                        
1 Hindi is a verb final language with free word order and a rich 
case marking system. It is an official language of India and is 
spoken by ~800 million people. 
2 http://www.icon2009.in/contests.html 
657
2 Why Clausal Information?  
Traditionally, a clause is defined as a group of 
words having a subject and a predicate. Clause 
boundary identification is the process of dividing 
the given sentence into a set of clauses. It can be 
seen as a partial parsing step after chunking, in 
which one tries to divide the sentence into mea-
ningful units. It is evident that most of the depen-
dents of words in a clause appear inside the same 
clause; in other words the dependencies of the 
words in a clause are mostly localized within the 
clause boundary. 
In the dependency parsing task, a parser has to 
disambiguate between several words in the sen-
tence to find the parent/child of a particular word. 
This work is to see whether the clause boundary 
information can help the parser to reduce the 
search space when it is trying to find the correct 
parent/child for a word. The search space of the 
parser can be reduced by a large extent if we solve 
a relatively small problem of identifying the claus-
es. Interestingly, it has been shown recently that 
most of the non-projective cases in Hindi are inter-
clausal (Mannem et al, 2009). Identifying clausal 
boundaries, therefore, should prove to be helpful in 
parsing non-projective structures. The same holds 
true for many long-distance dependencies. 
3 Experimental Setup 
3.1 Dataset 
The experiments reported in this paper have been 
done on Hindi; the data was released as part of the 
ICON 2009 parsing contest (Husain, 2009). The 
sentences used for this contest are subset of the 
Hyderabad Dependency Treebank (HyDT) devel-
oped for Hindi (Begum et al, 2008). The depen-
dency relations in the treebank are syntactico-
semantic. The dependency tagset in the annotation 
scheme has around 28 relations. The dependency 
trees in the treebank show relations between chunk 
heads. Note, therefore, that the experiments and 
results described in this paper are based on parse 
trees that have chunk head as nodes. 
The data provided in the task contained morpho-
logical features along with the lemma, POS tag, 
and coarse POS tag, for each word. These are six 
morphological features namely category, gender, 
number, person, vibhakti3 or TAM4 markers of the 
node 
3.2 Clause Boundary Identifier 
We used the Stage15 parser of Husain et al (2009), 
to provide the clause boundary information that is 
then incorporated as features during the actual 
parsing process. The Stage1 parser uses MST to 
identify just the intra-clausal relations. To achieve 
this, Husain et al, introduce a special dummy node 
named _ROOT_ which becomes the head of the 
sentence. All the clauses are connected to this 
dummy node with a dummy relation. In effect the 
Stage1 parser gives only intra-clausal relations. In 
the current work, we used MaltParser6 (Nivre et al, 
2007b) (henceforth Malt) to do this task. This is 
because Malt performs better than MST in case of 
intra-clausal relations, which are mostly short dis-
tance dependencies. We use the same algorithm 
and feature setting of Bharati et al, (2008a) to train 
the Stage1 parser. 
Since the above tool parses clauses, therefore 
along with the clause boundary information we 
also know the root of the clausal sub-tree. Several 
experiments were done to identify the most optim-
al set of clausal features available from the partial 
parse. The best results are obtained when the 
clause boundary information, along with the head 
information i.e. head node of a clause, is given as a 
feature to each node. 
We trained the Stage1 parser by converting the 
treebank data into the stage1 format, following the 
steps that were given in Husain et al (2009). This 
conversion depends on the definition of the clause. 
We experimented with different definitions of 
clause in order to tune the tool to give the optimal 
clause boundary and head information required for 
parsing. For the results reported in this paper, a 
clause is a sequence of words, with a single verb, 
unless the verb is a child of another verb. 
 
 
                                                        
3 Vibhakti is a generic term for preposition, post-position and 
suffix. 
4TAM: Tense, Aspect and Modality. 
5Stage1 handles intra-clausal dependency relations. These 
relations generally correspond to the argument structure of the 
verb, noun-noun genitive relation, infinitive-noun relation, 
adjective-noun, adverb-verb relations, etc. 
6 Malt version 1.2 
658
 Precision Recall 
Clause Boundary 84.83% 91.23% 
Head Information 92.42% 99.40% 
Table 1. Accuracies of the features being used 
 
Table 1 gives the accuracy of the clausal informa-
tion being used as features in parsing. It is clear 
from Table1 that the tool being used doesn?t have 
very high clause boundary identification perfor-
mance; nevertheless, the performance is sufficient 
enough to make an improvement in parsing expe-
riments. On the other hand, the head of the clause 
(or, the root head in the clausal sub-tree) is identi-
fied efficiently. All the above experiments for pa-
rameter tuning were done on the development data 
of the ICON 2009 parsing contest. 
3.3 Parser  
We used MSTParser7 for the actual parsing step. 
MST uses Chu-Liu-Edmonds Maximum Spanning 
Tree Algorithm for non-projective parsing and 
Eisner's algorithm for projective parsing (Eisner, 
1996). It uses online large margin learning as the 
learning algorithm (McDonald et al, 2005b). 
We modified MST so that it uses the clause 
boundary. Unlike the normal features that MST 
uses, the clause boundary features span across 
many words. 
. 
4 Experiments and Results 
We experimented with different combinations of 
the information provided in the data (as mentioned 
in 3.1). Vibhakti and TAM fields gave better re-
sults than others. This is consistent with the best 
previous settings for Hindi parsing (Bharati et al, 
2008a, Ambati et al, 2009). We used the results 
obtained using this setting as our baseline (F1). 
We first experimented by giving only the clause 
inclusion (boundary) information to each node 
(F2). This feature should help the parser reduce its 
search space during parsing decisions. Then, we 
provided only the head and non-head information 
(whether that node is the head of the clause or not) 
(F3). The head or non-head information helps in 
handling complex sentences that have more than 
                                                        
7 MST version 0.4b 
one clause and each verb in the sentence has its 
own argument structure. We achieved the best per-
formance by using both as features (F4) during the 
parsing process. 
 
 LA (%) UA (%) L (%) 
F1 73.62 91.00 76.04 
F2 72.66 91.00 74.74 
F3 73.88 91.35 75.78 
F4 74.39 91.87 76.21 
Table 2. Parsing accuracies with different features 
 
Table 2 gives the results for all the settings. It is 
interesting to note that the boundary information 
(F1) alone does not cross the baseline; however 
this feature is reliable enough to give the best per-
formance when combined with F3. 
5 Observations  
We see from the above results (F4 in Table 2) that 
there is a rise of 0.87% in UA (unlabeled 
attachment) and 0.77% in LA (labeled attachment) 
over previous best (F1).  This shows the positive 
effect of using the clausal information during the 
parsing process. 
We analyzed the performance of both the pars-
ers in handling the long distance dependencies and 
non-projective dependencies. We found that the 
non-projective arcs handled by F4 have a precision 
and recall of 41.1% and 50% respectively for UA, 
compared to 30.5% and 39.2% for the same arcs 
during F1. 
 
 
Figure 1. Distance stats 
 
Figure 1 compares the accuracies of the depen-
dencies at various distances. It is clear that the ef-
fect of clausal information become more 
659
pronounced as the distance increases. This means 
F4 does help the parser in effectively handling long 
distance dependencies as well. 
6 Conclusion and Future Work  
The results show that there is a significant 
improvement in the parsing accuracy when the 
clausal information is being used.  
The clausal information is presently being used 
only as attachment features in MST. Experiments 
can be done in future, to find out if there is a label 
bias to the clause boundary, which also helps in 
reducing the search space for specific labels. Im-
proving the feature set for the labeled parse also 
improves the unlabeled attachment accuracy, as 
MST does attachments and labels in a single step, 
and the labels of processed nodes will also be tak-
en in features. 
We can see from Table1 that the precision of the 
clause boundary is 84.83%. Using a tool, targeted 
at getting just the clausal information, instead of 
using a parser can improve the accuracy of the 
clausal information, which helps improving pars-
ing. 
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009. Experi-
ments in Indian Language Dependency Parsing. In 
Proceedings of the ICON09 NLP Tools Contest: In-
dian Language Dependency Parsing, pp 32-37.  
B. R. Ambati, P. Gade and C. GSK. 2009. Effect ofMi-
nimal Semantics on Dependency Parsing. In the Pro-
ceedings of RANLP 2009 Student Research 
Workshop. 
G. Attardi and F. Dell?Orletta. Chunking and Depen-
dency Parsing. LREC Workshop on Partial Parsing: 
Between Chunking and Deep Parsing. Marrakech, 
Morocco. 2008. 
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. In Proceedings of IJCNLP-2008. 
A. Bharati and R. Sangal. 1993. Parsing Free Word Or-
der Languages in the Paninian Framework. Proceed-
ings of ACL:93.  
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma 
and R. Sangal. 2008a. Two Semantic features make 
all the difference in Parsing accuracy. In Proceed-
ings. of International Conference on Natural Lan-
guage Processing-2008. 
A. Bharati, S. Husain, D. Sharma, and R. Sangal. 2008b. 
A two stage constraint based dependency parser for 
free word order languages. In Proceedings. of 
COLIPS International Conference on Asian Lan-
guage Processing. Thailand. 2008. 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
Two stage constraint based hybrid approach to free 
word order language dependency parsing. In the Pro-
ceedings of the 11th International Conference on 
Parsing Technologies (IWPT09). Paris. 2009. 
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson,M. Saers.2007. Single Malt or Blended? A 
Study in Multilingual Parser Optimization. 
In Proceedings of the CoNLL Shared Task Session of 
EMNLP-CoNLL 2007. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. In Proceedings of ICON09 NLP Tools Con-
test:Indian Language Dependency Parsing. 
Hyderabad, India. 2009. 
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and Ra-
jeev Sangal. 2009. A modular cascaded approach to 
complete parsing. In the Proceedings of COLIPS In-
ternational Conference on Asian Language 
Processing. Singapore. 2009. 
P. Mannem and H. Chaudhry.2009. Insights into Non-
projectivity in Hindi. In ACL-IJCNLP Student paper 
workshop. 2009. 
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 
2005a. Non-projective dependency parsing using 
spanning tree algorithms. In the Proceedings of 
HLT/EMNLP, pp. 523?530. 
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In 
the Proceedings of ACL 2005. pp. 91?98. 
I. A. Mel'Cuk. 1988. Dependency Syntax: Theory and 
Practice, State University Press of New York. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proceedings 
of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering, 
13(2), 95-135. 
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. In Linguistics and Phi-
losophy, p. 8, 334?343. 
660
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 22?30,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Two methods to incorporate local morphosyntactic features in Hindi de-
pendency parsing 
 
Bharat Ram Ambati, Samar Husain, Sambhav Jain, Dipti Misra Sharma 
and Rajeev Sangal 
Language Technologies Research Centre, IIIT-Hyderabad, India - 500032. 
{ambati,samar}@research.iiit.ac.in, sambhav-
jain@students.iiit.ac.in,{dipti,sangal}@mail.iiit.ac.in 
 
 
Abstract 
In this paper we explore two strategies to in-
corporate local morphosyntactic features in 
Hindi dependency parsing. These features are 
obtained using a shallow parser. We first ex-
plore which information provided by the shal-
low parser is most beneficial and show that 
local morphosyntactic features in the form of 
chunk type, head/non-head information, 
chunk boundary information, distance to the 
end of the chunk and suffix concatenation are 
very crucial in Hindi dependency parsing. We 
then investigate the best way to incorporate 
this information during dependency parsing. 
Further, we compare the results of various ex-
periments based on various criterions and do 
some error analysis. All the experiments were 
done with two data-driven parsers, MaltParser 
and MSTParser, on a part of multi-layered and 
multi-representational Hindi Treebank which 
is under development. This paper is also the 
first attempt at complete sentence level pars-
ing for Hindi. 
1 Introduction 
The dependency parsing community has since a 
few years shown considerable interest in parsing 
morphologically rich languages with flexible word 
order. This is partly due to the increasing availabil-
ity of dependency treebanks for such languages, 
but it is also motivated by the observation that the 
performance obtained for these languages have not 
been very high (Nivre et al, 2007a). Attempts at 
handling various non-configurational aspects in 
these languages have pointed towards shortcom-
ings in traditional parsing methodologies (Tsarfaty 
and Sima'an, 2008; Eryigit et al, 2008; Seddah et 
al., 2009; Husain et al, 2009; Gadde et al, 2010). 
Among other things, it has been pointed out that 
the use of language specific features may play a 
crucial role in improving the overall parsing per-
formance. Different languages tend to encode syn-
tactically relevant information in different ways, 
and it has been hypothesized that the integration of 
morphological and syntactic information could be 
a key to better accuracy. However, it has also been 
noted that incorporating these language specific 
features in parsing is not always straightforward 
and many intuitive features do not always work in 
expected ways. 
In this paper we explore various strategies to in-
corporate local morphosyntactic features in Hindi 
dependency parsing. These features are obtained 
using a shallow parser. We conducted experiments 
with two data-driven parsers, MaltParser (Nivre et 
al., 2007b) and MSTParser (McDonald et al, 
2006). We first explore which information pro-
vided by the shallow parser is most beneficial and 
show that local morphosyntactic features in the 
form of chunk type, head/non-head information, 
chunk boundary information, distance to the end of 
the chunk and suffix concatenation are very crucial 
in Hindi dependency parsing. We then investigate 
the best way to incorporate this information during 
dependency parsing. All the experiments were 
done on a part of multi-layered and multi-
representational Hindi Treebank (Bhatt et al, 
2009)1.  
The shallow parser performs three tasks, (a) it 
gives the POS tags for each lexical item, (b) pro-
vides morphological features for each lexical item, 
and (c) performs chunking. A chunk is a minimal 
(non-recursive) phrase consisting of correlated, 
inseparable words/entities, such that the intra-
chunk dependencies are not distorted (Bharati et 
                                                          
1 This Treebank is still under development. There are currently 
27k tokens with complete sentence level annotation.  
22
al., 2006). Together, a group of lexical items with 
some POS tag and morphological features within a 
chunk can be utilized to automatically compute 
local morphosyntactic information. For example, 
such information can represent the postposi-
tion/case-marking in the case of noun chunks, or it 
may represent the tense, aspect and modality 
(TAM) information in the case of verb chunks. In 
the experiments conducted for this paper such local 
information is automatically computed and incor-
porated as a feature to the head of a chunk. In gen-
eral, local morphosyntactic features correspond to 
all the parsing relevant local linguistic features that 
can be utilized using the notion of chunk. Previous-
ly, there have been some attempts at using chunk 
information in dependency parsing. Attardi and 
Dell?Orletta (2008) used chunking information in 
parsing English. They got an increase of 0.35% in 
labeled attachment accuracy and 0.47% in unla-
beled attachment accuracy over the state-of-the-art 
dependency parser. 
Among the three components (a-c, above), the 
parsing accuracy obtained using the POS feature is 
taken as baseline. We follow this by experiments 
where we explore how each of morph and chunk 
features help in improving dependency parsing 
accuracy. In particular, we find that local morpho-
syntactic features are the most crucial. These expe-
riments are discussed in section 2. In section 3 we 
will then see an alternative way to incorporate the 
best features obtained in section 2. In all the pars-
ing experiments discussed in section 2 and 3, at 
each step we explore all possible features and ex-
tract the best set of features. Best features of one 
experiment are used when we go to the next set of 
experiments. For example, when we explore the 
effect of chunk information, all the relevant morph 
information from previous set of experiments is 
taken into account.  
This paper is also the first attempt at complete 
sentence level parsing for Hindi. Due to the availa-
bility of dependency treebank for Hindi (Begum et 
al., 2008), there have been some previous attempts 
at Hindi data-driven dependency parsing (Bharati 
et al, 2008; Mannem et al, 2009; Husain et al, 
2009). Recently in ICON-09 NLP Tools Contest 
(Husain, 2009; and the references therein), rule-
based, constraint based, statistical and hybrid ap-
proaches were explored for dependency parsing. 
Previously, constraint based approaches to Indian 
language (IL) dependency parsing have also been 
explored (Bharati et al, 1993, 1995, 2009b, 
2009c). All these attempts, however, were finding 
inter-chunk dependency relations, given gold-
standard POS and chunk tags. Unlike these pre-
vious parsers, the dependencies in this work are 
between lexical items, i.e. the dependency tree is 
complete.  
The paper is arranged as follows, in section 2 
and 3, we discuss the parsing experiments. In sec-
tion 4, we describe the data and parser settings. 
Section 5 gives the results and discusses some re-
lated issues. General discussion and possible future 
work is mentioned in section 6. We conclude the 
paper in section 7. 
2 Getting the best linguistic features  
As mentioned earlier, a shallow parser consists of 
three main components, (a) POS tagger, (b) mor-
phological analyzer and (c) chunker. In this section 
we systematically explore what is the effect of 
each of these components. We?ll see in section 2.3 
that the best features of a-c can be used to compute 
local morphosyntactic features that, as the results 
show, are extremely useful. 
2.1 Using POS as feature (PaF): 
In this experiment we only use the POS tag infor-
mation of individual words during dependency 
parsing. First a raw sentence is POS-tagged. This 
POS-tagged sentence is then given to a parser to 
predict the dependency relations. Figure 1, shows 
the steps involved in this approach for (1). 
 
(1)  raama   ne         eka     seba        khaayaa  
  ?Ram?   ERG    ?one?  ?apple?      ?ate? 
       ?Ram ate an apple? 
 
Figure 1: Dependency parsing using only POS informa-
tion from a shallow parser. 
23
 
In (1) above, ?NN?, ?PSP?, ?QC?, ?NN? and ?VM? 
are the POS tags2 for raama, ne, eka, seba and 
khaayaa respectively. This information is provided 
as a feature to the parser. The result of this experi-
ment forms our baseline accuracy. 
2.2 Using Morph as feature (MaF): 
In addition to POS information, in this experiment 
we also use the morph information for each token. 
This morphological information is provided as a 
feature to the parser. Morph has the following in-
formation 
 
? Root: Root form of the word 
? Category: Course grained POS 
? Gender: Masculine/Feminine/Neuter 
? Number: Singular/Plural 
? Person: First/Second/Third person 
? Case: Oblique/Direct case 
? Suffix: Suffix of the word 
 
Take raama in (1), its morph information com-
prises of root = ?raama?, category = ?noun? gender 
= ?masculine?, number = ?singular?, person = 
?third?, case = ?direct?, suffix = ?0?. Similarly, 
khaayaa (?ate?) has the following morph informa-
tion. root = ?khaa?, category = ?verb? gender = 
?masculine?, numer = ?singular?, person = ?third?, 
case = ?direct?, suffix = ?yaa?. 
Through a series of experiments, the most cru-
cial morph features were selected. Root, case and 
suffix turn out to be the most important features. 
Results are discussed in section 5. 
2.3 Using local morphosyntax as feature 
(LMSaF) 
Along with POS and the most useful morph fea-
tures (root, case and suffix), in this experiment we 
also use local morphosyntactic features that reflect 
various chunk level information. These features 
are: 
? Type of the chunk 
? Head/non-head of the chunk 
                                                          
2 NN: Common noun, PSP: Post position, QC: Cardinal, VM: 
Verb. A list of complete POS tags can be found here: 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/POS-Tag-
List.pdf. The POS/chunk tag scheme followed in the Treebank 
is described in Bharati et al (2006). 
? Chunk boundary information 
? Distance to the end of the chunk 
? Suffix concatenation 
 
In example 1 (see section 2.1), there are two 
noun chunks and one verb chunk. raama and seba 
are the heads of the noun chunks. khaayaa is the 
head of the verb chunk. We follow standard IOB3 
notation for chunk boundary. raama,  eka and 
khaayaa are at the beginning (B) of their respective 
chunks. ne and seba are inside (I) their respective 
chunks. raama is at distance 1 from the end of the 
chunk and ne is at a distance 0 from the end of the 
chunk. 
Once we have a chunk and morph feature like 
suffix, we can perform suffix concatenation auto-
matically. A group of lexical items with some POS 
tags and suffix information within a chunk can be 
utilized to automatically compute this feature. This 
feature can, for example, represent the postposi-
tion/case-marking in the case of noun chunk, or it 
may represent the tense, aspect and modality 
(TAM) information in the case of verb chunks. 
Note that, this feature becomes part of the lexical 
item that is the head of a chunk. Take (2) as a case 
in point: 
 
(2) [NP raama/NNP   ne/PSP]     [NP seba/NN]        
              ?Ram?           ERG                ?apple?   
      [VGF khaa/VM     liyaa/VAUX] 
                 ?eat?           ?PRFT? 
      ?Ram ate an apple? 
 
The suffix concatenation feature for khaa, which 
is the head of the VGF chunk, will be ?0+yaa? and 
is formed by concatenating the suffix of the main 
verb with that of its auxiliary. Similarly, the suffix 
concatenation feature for raama, which is head of 
the NP chunk, will be ?0+ne?. This feature turns 
out to be very important. This is because in Hindi 
(and many other Indian languages) there is a direct 
correlation between the TAM markers and the case 
that appears on some nominals (Bharati et al, 
1995). In (2), for example, khaa liyaa together 
gives the past perfective aspect for the verb khaa-
naa ?to eat?. Since, Hindi is split ergative, the sub-
ject of the transitive verb takes an ergative case 
marker when the verb is past perfective. Similar 
                                                          
3 Inside, Outside, Beginning of the chunk. 
24
correlation between the case markers and TAM 
exist in many other cases. 
3 An alternative approach to use best fea-
tures: A 2-stage setup (2stage) 
So far we have been using various information 
such as POS, chunk, etc. as features. Rather than 
using them as features and doing parsing at one go, 
we can alternatively follow a 2-stage setup. In par-
ticular, we divide the task of parsing into:  
 
? Intra-chunk dependency parsing 
? Inter-chunk dependency parsing 
 
We still use POS, best morphological features 
(case, suffix, root) information as regular features 
during parsing. But unlike LMSaF mentioned in 
section 2.3, where we gave local morphosyntactic 
information as a feature, we divided the task of 
parsing into sub-tasks. A similar approach was also 
proposed by Bharati et al (2009c). During intra-
chunk dependency parsing, we try to find the de-
pendency relations of the words within a chunk. 
Following which, chunk heads of each chunk with-
in a sentence are extracted. On these chunk heads 
we run an inter-chunk dependency parser. For each 
chunk head, in addition to POS tag, useful morpho-
logical features, any useful intra-chunk information 
in the form of lexical item, suffix concatenation, 
dependency relation are also given as a feature. 
 
Figure 2: Dependency parsing using chunk information: 
2-stage approach. 
Figure 2 shows the steps involved in this ap-
proach for (1). There are two noun chunks and one 
verb chunk in this sentence. raama and seba are 
the heads of the noun chunks. khaaya is the head 
of the verb chunk. The intra-chunk parser attaches 
ne to raama and eka to seba with dependency la-
bels ?lwg__psp? and ?nmod__adj?4 respectively. 
Heads of each chunk along with its POS, morpho-
logical features, local morphosyntactic features and 
intra-chunk features are extracted and given to in-
ter-chunk parser. Using this information the inter-
chunk dependency parser marks the dependency 
relations between chunk heads. khaaya becomes 
the root of the dependency tree. raama and seba 
are attached to khaaya with dependency labels ?k1? 
and ?k2?5 respectively. 
4 Experimental Setup 
In this section we describe the data and the parser 
settings used for our experiments.  
4.1 Data 
For our experiments we took 1228 dependency 
annotated sentences (27k tokens), which have 
complete sentence level annotation from the new 
multi-layered and multi-representational Hindi 
Treebank (Bhatt et al, 2009). This treebank is still 
under development. Average length of these sen-
tences is 22 tokens/sentence and 10 
chunks/sentence. We divided the data into two 
sets, 1000 sentences for training and 228 sentences 
for testing.  
4.2 Parsers and settings 
All experiments were performed using two data-
driven parsers, MaltParser6 (Nivre et al, 2007b), 
and MSTParser7 (McDonald et al, 2006).
                                                          
4 nmod__adj is an intra-chunk label for quantifier-noun mod-
ification. lwg__psp is the label for post-position marker. De-
tails of the labels can be seen in the intra-chunk guidelines 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/IntraChunk-
Dependency-Annotation-Guidelines.pdf 
5 k1 (karta) and k2 (karma) are syntactico-semantic labels 
which have some properties of both grammatical roles and 
thematic roles. k1 behaves similar to subject and agent. k2 
behaves similar to object and patient (Bharati et al, 1995; 
Vaidya et al, 2009). For complete tagset, see (Bharati et al, 
2009). 
6 Malt Version 1.3.1 
7 MST Version 0.4b 
25
 Malt MST+MaxEnt 
Cross-validation Test-set Cross-validation Test-set 
UAS LAS LS UAS LAS LS UAS LAS LS UAS LAS LS 
PaF 89.4 78.2 80.5 90.4 80.1 82.4 86.3 75.1 77.9 87.9 77.0 79.3 
MaF 89.6 80.5 83.1 90.4 81.7 84.1 89.1 79.2 82.5 90.0 80.9 83.9 
LMSaF 91.5 82.7 84.7 91.8 84.0 86.2 90.8 79.8 82.0 92.0 81.8 83.8 
2stage 91.8 83.3 85.3 92.4 84.4 86.3 92.1 82.2 84.3 92.7 84.0 86.2 
Table 1: Results of all the four approaches using gold-standard shallow parser information. 
 
Malt is a classifier based shift/reduce parser. It 
provides option for six parsing algorithms, namely, 
arc-eager, arc-standard, convington projective, co-
vington non-projective, stack projective, stack ea-
ger and stack lazy. The parser also provides option 
for libsvm and liblinear learning model. It uses 
graph transformation to handle non-projective trees 
(Nivre and Nilsson, 2005). MST uses Chu-Liu-
Edmonds (Chu and Liu, 1965; Edmonds, 1967) 
Maximum Spanning Tree algorithm for non-
projective parsing and Eisner's algorithm for pro-
jective parsing (Eisner, 1996). It uses online large 
margin learning as the learning algorithm (McDo-
nald et al, 2005). In this paper, we use MST only 
for unlabeled dependency tree and use a separate 
maximum entropy model8 (MaxEnt) for labeling. 
Various combination of features such as node, its 
parent, siblings and children were tried out before 
arriving at the best results. 
As the training data size is small we did 5-fold 
cross validation on the training data for tuning the 
parameters of the parsers and for feature selection. 
Best settings obtained using cross-validated data 
are applied on test set. We present the results both 
on cross validated data and on test data.  
For the Malt Parser, arc-eager algorithm gave 
better performance over others in all the approach-
es. Libsvm consistently gave better performance 
over liblinear in all the experiments. For SVM set-
tings, we tried out different combinations of best 
SVM settings of the same parser on different lan-
guages in CoNLL-2007 shared task (Hall et al, 
2007) and applied the best settings. For feature 
model, apart from trying best feature settings of the 
same parser on different languages in CoNLL-
2007 shared task (Hall et al, 2007), we also tried 
out different combinations of linguistically intui-
tive features and applied the best feature model. 
The best feature model is same as the feature mod-
el used in Ambati et al (2009a), which is the best 
                                                          
8 http://maxent.sourceforge.net/ 
performing system in the ICON-2009 NLP Tools 
Contest (Husain, 2009). 
For the MSTParser, non-projective algorithm, 
order=2 and training-k=5 gave best results in all 
the approaches. For the MaxEnt, apart from some 
general useful features, we experimented consider-
ing different combinations of features of node, par-
ent, siblings, and children of the node.  
5 Results and Analysis 
All the experiments discussed in section 2 and 3 
were performed considering both gold-standard 
shallow parser information and automatic shallow 
parser9 information. Automatic shallow parser uses 
a rule based system for morph analysis, a 
CRF+TBL based POS-tagger and chunker. The 
tagger and chunker are 93% and 87% accurate re-
spectively. These accuracies are obtained after us-
ing the approach of PVS and Gali, (2007) on larger 
training data. In addition, while using automatic 
shallow parser information to get the results, we 
also explored using both gold-standard and auto-
matic information during training. As expected, 
using automatic shallow parser information for 
training gave better performance than using gold 
while training.  
Table 1 and Table 2 shows the results of the four 
experiments using gold-standard and automatic 
shallow parser information respectively. We eva-
luated our experiments based on unlabeled attach-
ment score (UAS), labeled attachment score (LAS) 
and labeled score (LS) (Nivre et al, 2007a). Best 
LAS on test data is 84.4% (with 2stage) and 75.4% 
(with LMSaF) using gold and automatic shallow 
parser information respectively. These results are 
obtained using MaltParser. In the following sub-
section we discuss the results based on different 
criterion.
                                                          
9 http://ltrc.iiit.ac.in/analyzer/hindi/ 
26
 Malt MST+MaxEnt 
Cross-validation Test-set Cross-validation Test-set 
UAS LAS LS UAS LAS LS UAS LAS LS UAS LAS LS 
PaF 82.2 69.3  73.4  84.6  72.9  76.5  79.4  66.5  70.7  81.6  69.4  73.1  
MaF 82.5 71.6  76.1  84.0  73.6  77.6  82.3  70.4  75.4  83.4  72.7  77.3  
LMSaF 83.2 73.0  77.0  85.5  75.4  78.9  82.6  71.3  76.1  85.0  73.4  77.3  
2stage 79.0 69.5 75.6 79.6 71.1 76.8 78.8  66.6  72.6 80.1  69.7  75.4  
Table 2: Results of all the four experiments using automatic shallow parser information. 
 
POS tags provide very basic linguistic informa-
tion in the form of broad grained categories. The 
best LAS for PaF while using gold and automatic 
tagger were 80.1% and 72.9% respectively. The 
morph information in the form of case, suffix and 
root information proved to be the most important 
features. But surprisingly, gender, number and per-
son features didn?t help. Agreement patterns in 
Hindi are not straightforward. For example, the 
verb agrees with k2 if the k1 has a post-position; it 
may also sometimes take the default features. In a 
passive sentence, the verb agrees only with k2. The 
agreement problem worsens when there is coordi-
nation or when there is a complex verb. It is un-
derstandable then that the parser is unable to learn 
the selective agreement pattern which needs to be 
followed.  
LMSaF on the other hand encode richer infor-
mation and capture some local linguistic patterns. 
The first four features in LMSaF (chunk type, 
chunk boundary, head/non-head of chunk and dis-
tance to the end of chunk) were found to be useful 
consistently. The fifth feature, in the form of suffix 
concatenation, gave us the biggest jump, and cap-
tures the correlation between the TAM markers of 
the verbs and the case markers on the nominals. 
5.1 Feature comparison: PaF, MaF vs. 
LMSaF 
Dependency labels can be classified as two types 
based on their nature, namely, inter-chunk depen-
dency labels and intra-chunk labels. Inter-chunk 
dependency labels are syntacto-semantic in nature. 
Whereas intra-chunk dependency labels are purely 
syntactic in nature.  
Figure 3, shows the f-measure for top six inter-
chunk and intra-chunk dependency labels for PaF, 
MaF, and LMSaF using Maltparser on test data 
using automatic shallow parser information. The 
first six labels (k1, k2, pof, r6, ccof, and k7p) are 
the top six inter-chunk labels and the next six la-
bels (lwg__psp, lwg__aux, lwg__cont, rsym, 
nmod__adj, and pof__cn) are the top six intra-
chunk labels. First six labels (inter-chunk) corres-
pond to 28.41% and next six labels (intra-chunk) 
correspond to 48.81% of the total labels in the test 
data. The figure shows that with POS information 
alone, f-measure for top four intra-chunk labels 
reached more than 90% accuracy. The accuracy 
increases marginally with the addition of morph 
and local morphosytactic features. The results cor-
roborates with our intuition that intra-chunk de-
pendencies are mostly syntactic.  For example, 
consider an intra-chunk label ?lwg__psp?. This is 
the label for postposition marker. A post-position 
marker succeeding a noun is attached to that noun 
with the label ?lwg__psp?. POS tag for post-
position marker is PSP. So, if a NN (common 
noun) or a NNP (proper noun) is followed by a 
PSP (post-position marker), then the PSP will be 
attached to the preceding NN/NNP with the de-
pendency label ?lwg_psp?. As a result, providing 
POS information itself gave an f-measure of 98.3% 
for ?lwg_psp?.  With morph and local morphosy-
tactic features, this got increased to 98.4%. How-
ever, f-measure for some labels like ?nmod__adj? 
is around 80% only. ?nmod__adj? is the label for 
adjective-noun, quantifier-noun modifications. 
Low accuracy for these labels is mainly due to two 
reasons. One is POS tag errors. And the other is 
attachment errors due to genuine ambiguities such 
as compounding. 
For inter-chunk labels (first six columns in the 
figure 3), there is considerable improvement in the 
f-measure using morph and local morphosytactic 
features. As mentioned, local morphosyntactic fea-
tures provide local linguistic information. For ex-
ample, consider the case of verbs. At POS level, 
there are only two tags ?VM? and ?VAUX? for 
main verbs and auxiliary verbs respectively (Bha-
rati et al, 2006). Information about finite/non-
finiteness is not present in the POS tag. But, at 
chunk level there are four different chunk tags for
27
30
40
50
60
70
80
90
100
k1 k2 pof r6 ccof k7p lwg__psp lwg__vaux lwg__cont rsym nmod__adj pof__cn
PaF
MaF
LMaF
Figure 3: F-measure of top 6, inter-chunk and intra-chunk labels for PaF, MaF and LMSaF approaches using Malt-
parser on test data using automatic shallow parser information. 
 
verbs, namely VGF, VGNF, VGINF and VGNN. 
They are respectively, finite, non-finite, infinitival 
and gerundial chunk tags. The difference in the 
verbal chunk tag is a good cue for helping the 
parser in identifying different syntactic behavior of 
these verbs. Moreover, a finite verb can become 
the root of the sentence, whereas a non-finite or 
infinitival verb can?t. Thus, providing chunk in-
formation also helped in improving the correct 
identification of the root of the sentence. 
Similar to Prague Treebank (Hajicova, 1998), 
coordinating conjuncts are heads in the treebank 
that we use. The relation between a conjunct and 
its children is shown using ?ccof? label. A coordi-
nating conjuct takes children of similar type only. 
For example, a coordinating conjuct can have two 
finite verbs or two non-finite verbs as its children, 
but not a finite verb and a non-finite verb. Such 
instances are also handled more effectively if 
chunk information is incorporated. The largest in-
crease in performance, however, was due to the 
?suffix concatenation? feature. Significant im-
provement in the core inter-chunk dependency la-
bels (such as k1, k2, k4, etc.) due to this feature is 
the main reason for the overall improvement in the 
parsing accuracy. As mentioned earlier, this is be-
cause this feature captures the correlation between 
the TAM markers of the verbs and the case mark-
ers on the nominals. 
5.2 Approach comparison: LMSaF vs. 2stage 
Both LMSaF and 2stage use chunk information. In 
LMSaF, chunk information is given as a feature 
whereas in 2stage, sentence parsing is divided into 
intra-chunk and inter-chunk parsing. Both the ap-
proaches have their pros and cons. In LMSaF as 
everything is done in a single stage there is much 
richer context to learn from. In 2stage, we can pro-
vide features specific to each stage which can?t be 
done in a single stage approach (McDonald et al, 
2006). But in 2stage, as we are dividing the task, 
accuracy of the division and the error propagation 
might pose a problem. This is reflected in the re-
sults where the 2-stage performs better than the 
single stage while using gold standard information, 
but lags behind considerably when the features are 
automatically computed.  
During intra-chunk parsing in the 2stage setup, 
we tried out using both a rule-based approach and 
a statistical approach (using MaltParser). The rule 
based system performed slightly better (0.1% 
LAS) than statistical when gold chunks are consi-
dered. But, with automatic chunks, the statistical 
approach outperformed rule-based system with a 
difference of 7% in LAS. This is not surprising 
because, the rules used are very robust and mostly 
based on POS and chunk information. Due to er-
rors induced by the automatic POS tagger and 
chunker, the rule-based system couldn?t perform 
well. Consider a small example chunk given be-
low. 
 ((    NP 
 meraa ?my?   PRP  
 bhaaii ?brother? NN 
)) 
As per the Hindi chunking guidelines (Bharati et 
al., 2006), meraa and bhaaii should be in two sepa-
rate chunks. And as per Hindi dependency annota-
tion guidelines (Bharati et al, 2009), meraa is 
attached to bhaaii with a dependency label ?r6?10. 
When the chunker wrongly chunks them in a single 
                                                          
10?r6? is the dependency label for genitive relation. 
28
chunk, intra-chunk parser will assign the depen-
dency relation for meraa. Rule based system can 
never assign ?r6? relation to meraa as it is an inter-
chunk label and the rules used cannot handle such 
cases. But in a statistical system, if we train the 
parser using automatic chunks instead of gold 
chunks, the system can potentially assign ?r6? la-
bel.  
5.3 Parser comparison: MST vs. Malt 
In all the experiments, results of MaltParser are 
consistently better than MST+MaxEnt. We know 
that Maltparser is good at short distance labeling 
and MST is good at long distance labeling (McDo-
nald and Nivre, 2007). The root of the sentence is 
better identified by MSTParser than MaltParser. 
Our results also confirm this. MST+MaxEnt and 
Malt could identify the root of the sentence with an 
f-measure of 89.7% and 72.3% respectively. Pres-
ence of more short distance labels helped Malt to 
outperform MST. Figure 5, shows the f-measure 
relative to dependency length for both the parsers 
on test data using automatic shallow parser infor-
mation for LMSaF.  
30
40
50
60
70
80
90
100
0 5 10 15+
Dependency Length
f-
m
ea
su
re
Malt
MST+MaxEnt
 
Figure 5: Dependency arc f-measure relative to depen-
dency length. 
6 Discussion and Future Work 
We systematically explored the effect of various 
linguistic features in Hindi dependency parsing. 
Results show that POS, case, suffix, root, along 
with local morphosyntactic features help depen-
dency parsing. We then described 2 methods to 
incorporate such features during the parsing 
process. These methods can be thought as different 
paradigms of modularity. For practical reasons (i.e. 
given the POS tagger/chunker accuracies), it is 
wiser to use this information as features rather than 
dividing the task into two stages.  
As mentioned earlier, this is the first attempt at 
complete sentence level parsing for Hindi. So, we 
cannot compare our results with previous attempts 
at Hindi dependency parsing, due to, (a) The data 
used here is different and (b) we produce complete 
sentence parses rather than chunk level parses. 
As mentioned in section 5.1, accuracies of intra-
chunk dependencies are very high compared to 
inter-chunk dependencies. Inter-chunk dependen-
cies are syntacto-semantic in nature. The parser 
depends on surface syntactic cues to identify such 
relations. But syntactic information alone is always 
not sufficient, either due to unavailability or due to 
ambiguity. In such cases, providing some semantic 
information can help in improving the inter-chunk 
dependency accuracy. There have been attempts at 
using minimal semantic information in dependency 
parsing for Hindi (Bharati et al, 2008). Recently, 
Ambati et al (2009b) used six semantic features 
namely, human, non-human, in-animate, time, 
place, and abstract for Hindi dependency parsing. 
Using gold-standard semantic features, they 
showed considerable improvement in the core in-
ter-chunk dependency accuracy. Some attempts at 
using clause information in dependency parsing for 
Hindi (Gadde et al, 2010) have also been made. 
These attempts were at inter-chunk dependency 
parsing using gold-standard POS tags and chunks. 
We plan to see their effect in complete sentence 
parsing using automatic shallow parser information 
also.  
7 Conclusion 
In this paper we explored two strategies to incorpo-
rate local morphosyntactic features in Hindi de-
pendency parsing. These features were obtained 
using a shallow parser. We first explored which 
information provided by the shallow parser is use-
ful  and showed that local morphosyntactic fea-
tures in the form of chunk type, head/non-head 
info, chunk boundary info, distance to the end of 
the chunk and suffix concatenation are very crucial 
for Hindi dependency parsing. We then investi-
gated the best way to incorporate this information 
during dependency parsing. Further, we compared 
the results of various experiments based on various 
criterions and did some error analysis. This paper 
was also the first attempt at complete sentence lev-
el parsing for Hindi. 
29
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009a. Experi-
ments in Indian Language Dependency Parsing. In 
Proc of the ICON09 NLP Tools Contest: Indian Lan-
guage Dependency Parsing, pp 32-37.  
B. R. Ambati, P. Gade, C. GSK and S. Husain. 2009b. 
Effect of Minimal Semantics on Dependency Pars-
ing. In Proc of RANLP09 student paper workshop. 
G. Attardi and F. Dell?Orletta. 2008. Chunking and De-
pendency Parsing. In Proc of LREC Workshop on 
Partial Parsing: Between Chunking and Deep Pars-
ing. Marrakech, Morocco. 
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. In Proc of IJCNLP-2008. 
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural 
Language Processing: A Paninian Perspective, Pren-
tice-Hall of India, New Delhi. 
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma, 
and R. Sangal. 2008. Two semantic features make all 
the difference in parsing accuracy. In Proc of ICON. 
A. Bharati, R. Sangal, D. M. Sharma and L. Bai. 2006. 
AnnCorra: Annotating Corpora Guidelines for POS 
and Chunk Annotation for Indian Languages. Tech-
nical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad. 
A. Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begam 
and R. Sangal. 2009a. AnnCorra: TreeBanks for In-
dian Languages, Guidelines for Annotating Hindi 
TreeBank. 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
2009b. Two stage constraint based hybrid approach 
to free word order language dependency parsing. In 
Proc. of IWPT. 
A. Bharati, S. Husain, M. Vijay, K. Deepak, D. M. 
Sharma and R. Sangal. 2009c. Constraint Based Hy-
brid Approach to Parsing Indian Languages. In Proc 
of PACLIC 23. Hong Kong. 2009. 
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D. 
M. Sharma and F. Xia. 2009. Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu. In 
Proc. of the Third LAW at 47th ACL and 4th IJCNLP. 
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400. 
J. Edmonds. 1967. Optimum branchings. Journal of 
Research of the National Bureau of Standards, 
71B:233?240. 
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc of 
COLING-96, pp. 340?345. 
G. Eryigit, J. Nivre, and K. Oflazer. 2008. Dependency 
Parsing of Turkish. Computational Linguistics 34(3), 
357-389. 
P. Gadde, K. Jindal, S. Husain, D. M. Sharma, and R. 
Sangal. 2010. Improving Data Driven Dependency 
Parsing using Clausal Information. In Proc of 
NAACL-HLT 2010, Los Angeles, CA. 
E. Hajicova. 1998. Prague Dependency Treebank: From 
Analytic to Tectogrammatical Annotation. In Proc of 
TSD?98. 
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson and M. Saers. 2007. Single Malt or Blended? 
A Study in Multilingual Parser Optimization. In Proc 
of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, 933?939. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. In Proc of ICON09 NLP Tools Contest: In-
dian Language Dependency Parsing. Hyderabad, 
India. 
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and R. 
Sangal. 2009. A modular cascaded approach to com-
plete parsing. In Proc. of the COLIPS IALP. 
P. Mannem, A. Abhilash and A. Bharati. 2009. LTAG-
spinal Treebank and Parser for Hindi. In Proc of In-
ternational Conference on NLP, Hyderabad. 2009. 
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In 
Proc of ACL. pp. 91?98. 
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discri-
minative parser. In Proc of the Tenth (CoNLL-X), pp. 
216?220. 
R. McDonald and J. Nivre. 2007. Characterizing the 
errors of data-driven dependency parsing models. In 
Proc. of EMNLP-CoNLL. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,  S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proc of 
EMNLP/CoNLL-2007. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering, 
13(2), 95-135. 
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of ACL-2005, pp. 99?106. 
Avinesh PVS and K. Gali. 2007. Part-Of-Speech Tag-
ging and Chunking Using Conditional Random 
Fields and Transformation Based Learning. In Proc 
of the SPSAL workshop during IJCAI '07. 
D. Seddah, M. Candito and B. Crabb?. 2009. Cross 
parser evaluation: a French Treebanks study. In Proc. 
of IWPT, 150-161. 
R. Tsarfaty and K. Sima'an. 2008. Relational-
Realizational Parsing. In Proc. of CoLing, 889-896. 
A. Vaidya, S. Husain, P. Mannem, and D. M. Sharma. 
2009. A karaka-based dependency annotation scheme 
for English. In Proc. of CICLing, 41-52. 
30
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 18?21,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
On the Role of NLP in Linguistics
Dipti Misra Sharma
Language Technologies Research Centre
IIIT-H, Hyderabad, India
dipti@iiit.ac.in
Abstract
This paper summarizes some of the appli-
cations of NLP techniques in various lin-
guistic sub-fields, and presents a few ex-
amples that call for a deeper engagement
between the two fields.
1 Introduction
The recent success of data-driven approaches in
NLP has raised important questions as to what role
linguistics must now seek to play in further ad-
vancing the field. Perhaps, it is also time to pose
the same question from the other direction: As
to how NLP techniques can help linguists make
informed decisions? And how can the advances
made in one field be applied to the other?
Although, there has been some work on in-
corporating NLP techniques for linguistic field-
work and language documentation (Bird, 2009),
the wider use of NLP in linguistic studies is still
fairly limited. However, it is possible to deepen
the engagement between the two fields in a num-
ber of possible areas (as we shall see in the follow-
ing sections), and gain new insights even during
the formulation of linguistic theories and frame-
works.
2 Historical Linguistics and Linguistic
Typology
Computational techniques have been successfully
used to classify languages and to generate phylo-
genetic trees. This has been tried not just with
handcrafted word lists (Atkinson et al, 2005;
Atkinson and Gray, 2006; Huelsenbeck et al,
2001) or syntactic data (Barbac?on et al, 2007) but
with lists extracted from written corpus with com-
parable results (Rama and Singh, 2009; Singh and
Surana, 2007). These techniques are inspired from
the work in computational phylogenetics, which
was aimed at constructing evolutionary trees of
Figure 1: Phylogenetic tree using feature n-grams
biological species. Constructing a phylogenetic
tree for languages usually requires the calcula-
tion of distances between pairs of languages (usu-
ally based on word lists). These distances are
then given as input to a computational phyloge-
netic algorithm. Their successful use for lan-
guages has opened the possibility of using compu-
tational techniques for studying historical linguis-
tics. They have already been used for estimating
divergence times of language families (Atkinson
et al, 2005). Figure 1 shows a phylogenetic tree
created using feature n-grams (Rama and Singh,
2009).
Another area for the application of NLP tech-
niques is language typology. For example, lin-
guistic similarity and its estimation can be seen as
fundamental ideas in NLP. The systematic study
of different kinds of linguistic similarity offers
insights towards the theoretical studies of lan-
guages (Singh, 2010). In brief, the typology of
linguistic similarity for computational purposes
is related to linguistic levels (depth), differences
among languages (linguality) and linguistic units
(granularity). Thus, language can be seen as a
system of symbols whose meanings are defined
18
in terms of their estimated similarity and distance
with other symbols. Can this, together with what
Cognitive Linguists have been studying (Robin-
son and Ellis, 2008), which also involves linguistic
similarity, often directly, have some relevance for
linguists?
3 Lexical Correspondence and Linguistic
Units
A further case in point is lexical correspondence
across languages, which poses a problem for
cross-lingual and multilingual applications. To
address this and some other issues, a linguistic
unit that behaves similarly across languages can
be conceptualized. Such a unit, may include
morphological variation (inflectional and deriva-
tional), compounds, multi word expressions etc.
as in the Hindi and Telugu examples below:
? Single token content words: raama, raama
(Ram); vah, atanu (he); vyakti, manishii (per-
son) etc.
? Nouns with inflections: bacce, pillalu (chil-
dren); bacce ko, pillalaki (to the child);
raama se, raamudunundii (from Rama) etc.
? Verbs with inflections and tense, aspect and
modality (TAM) markers: karnaa-caahiye,
cayiyaalii (should do); ho sakataa thaa,
ayyiyedemo (could have happened) etc.
? Multi word expressions such as idioms,
phrasal verbs and ?frozen expressions?: pa-
haaD toDanaa (breaking mountains); muNha
ki khaana (getting defeated) etc.
? Compounds: jaati-prathaa (caste system);
vesh-bhuushaaoN (dresses); akkaDaa-
ikkaDaa (here and there) etc.
This unit might, among other things, form the
basis of the structure of lexical resources, such
that these resources have a direct correspondence
across languages. This can further facilitate com-
parative study of languages (Singh, 2010).
4 Applications
Computational techniques can also be used to de-
sign tools and material for language learning and
teaching. Here games can play a useful role. Al-
though, a large number of online games are avail-
able, most of them do not use the latest language
processing techniques. Games can also be used to
generate language resources.
The core idea in Human Computa-
tion (Von Ahn, 2005) is that computers should
do what they do best and that humans seamlessly
work with them to do what computers cannot.
One of the ways to merge the two is in the form of
carefully designed games.
Another insight comes from Machine Transla-
tion. More than any other sub-field in NLP, it is
the data-driven approaches to machine translation
that have proven to be particularly successful over
the past few years. We have been exploring vari-
ous approaches towards hybridization of our rule-
based MT system. Building the transfer-grammar
of such systems is perhaps one of the most time-
intensive tasks that involves careful analysis of test
data. However, data driven techniques can come
to the aid of linguists in this case. The recent
work on automatic acquisition of rules from par-
allel corpora (Lavie et al, 2004) can help iden-
tify a large number of common syntactic transfor-
mations across a pair of languages, and help un-
earth those transformations that might otherwise
be missed by a rule-based grammar. They can be
further used to prioritize the application of rules
based on the observed frequencies of certain syn-
tactic transformations.
5 NLP Tools and Linguistics
NLP techniques draw features from annotated cor-
pora which are a rich linguistic resource. How-
ever, these corpora can also be used to extract
grammars, which on one hand feed the parser
with features (Xia, 2001), and on the other, act
as a resource for linguistic studies. For exam-
ple, in Hindi dependency parsing the use of vib-
hakti (post-positions) and TAM labels has proven
to be particularly useful even in the absence of
large amounts of annotated corpora (Ambati et al,
2010). This also helped bring to light those fea-
tures of the grammar that govern certain struc-
ture choices and brought to notice some previously
overlooked linguistic constructions. Thus, the re-
sult is an iterative process, where both the gram-
mar and the features are refined.
Discourse Processing is another rapidly emerg-
ing research area with considerable potential for
interaction and collaboration between NLP and
Linguistics. In the absence of fully developed the-
ories/frameworks on both sides, focus on syner-
19
gizing research efforts in the two disciplines (such
as devising novel ways to empirically test linguis-
tic hypotheses) from the initial stage itself, can
yield a substantially richer account of Discourse.
Linguistic theories are formalized based on ob-
servations and abstractions of existing linguistic
facts. These theories are then applied to vari-
ous languages to test their validity. However, lan-
guages throw up new problems and issues before
theoreticians. Hence, there are always certain phe-
nomena in languages which remain a point of dis-
cussion since satisfactory solutions are not avail-
able. The facts of a language are accounted for
by applying various techniques and methods that
are offered by a linguistic framework. For exam-
ple, syntactic diagnostics have been a fairly re-
liable method of identifying/classifying construc-
tion types in languages. They work fairly well for
most cases. But in some cases even these tests fail
to classify certain elements. For example, Indian
languages show a highly productive use of com-
plex predicates (Butt, 1995; Butt, 2003). How-
ever, till date there are no satisfactory methods to
decide when a noun verb sequence is a ?complex
predicate? and when a ?verb argument? case. To
quote an example from our experience while de-
veloping a Hindi Tree Bank, annotators had to be
provided with guidelines to mark a N V sequence
as a complex predicate based on some linguistic
tests. However, there are instances when the na-
tive speaker/annotator is quite confident of a con-
struction being a complex predicate, even though
most syntactic tests might not apply to it.
Although, various theories provide frames to
classify linguistic patterns/items but none of them
enables us to (at least to my knowledge) handle
?transient/graded? or rather ?evolving? elements.
So, as of now it looks like quite an arbitrary/ad-
hoc approach whether to classify something as a
complex predicate or not. In the above cited ex-
ample, the decision is left to the annotator?s in-
tuition, since linguists don?t agree on the classfi-
cation of these elements or on a set of uniform
tests either. Can the insights gained from inter-
annotator agreement further help theory refine the
diagnostics used in these cases? And can NLP
techniques or advanced NLP tools come to the aid
of linguists here? Perhaps in the form of tools that
can (to an extent) help automate the application of
syntactic diagnostics over large corpora?
6 Collaborations
Interdisciplinary areas such as Computational
Linguistics/NLP need a much broader collabo-
ration between linguists and computer scientists.
Experts working within their respective fields
tend to be deeply grounded in their approaches
towards particular problems. Also, they tend
to speak different ?languages?. Therefore, it
becomes imperative that efforts be made to
bridge the gaps in communication between the
two disciplines. This problem is all the more
acute in India, since the separation of disciplines
happens at a very early stage. Objectives, goals,
methods and training are so different that starting
a communication line proves to be very difficult.
Thus, it is important for those people who have
synthesised the knowledge of the two disciplines
to a large degree, to take the lead and help
establish the initial communication channels. Our
own experiences while devising common tagsets
for Indian languages, made us realize the need
for both linguistic and computational perspectives
towards such problems. While a linguist?s instinct
is to look for exceptions in the grammar (or any
formalism), a computer scientist tends to look for
rules that can be abstracted away and modeled.
However, at the end, both ways of looking at data
help us make informed decisions.
Acknowledgements
Many thanks to Dr. Rajeev Sangal, Anil Kumar
Singh, Arafat Ahsan, Bharath Ambati, Rafiya Be-
gum, Samar Husain and Sudheer Kolachina for the
discussions and inputs.
References
B.R. Ambati, S. Husain, J. Nivre, and R. Sangal. 2010.
On the role of morphosyntactic features in Hindi de-
pendency parsing. In The First Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), page 94.
QD Atkinson and RD Gray. 2006. How old is the
Indo-European language family? Progress or more
moths to the flame. Phylogenetic Methods and the
Prehistory of Languages (Forster P, Renfrew C, eds),
pages 91?109.
Q. Atkinson, G. Nicholls, D. Welch, and R. Gray.
2005. From words to dates: water into wine, math-
emagic or phylogenetic inference? Transactions of
the Philological Society, 103(2):193?219.
20
S. Bird. 2009. Natural language processing and
linguistic fieldwork. Computational Linguistics,
35(3):469?474.
M. Butt. 1995. The structure of complex predicates in
Urdu. Center for the Study of Language and Infor-
mation.
M. Butt. 2003. The light verb jungle. In Workshop on
Multi-Verb Constructions. Citeseer.
J.P. Huelsenbeck, F. Ronquist, R. Nielsen, and J.P.
Bollback. 2001. Bayesian inference of phylogeny
and its impact on evolutionary biology. Science,
294(5550):2310?2314.
A. Lavie, K. Probst, E. Peterson, S. Vogel, L. Levin,
A. Font-Llitjos, and J. Carbonell. 2004. A trainable
transfer-based machine translation approach for lan-
guages with limited resources. In Proceedings of
Workshop of the European Association for Machine
Translation. Citeseer.
Taraka Rama and Anil Kumar Singh. 2009. From
bag of languages to family trees from noisy corpus.
In Proceedings of the Conference on Recent Ad-
vances in Natural Language Processing, Borovets,
Bulgaria.
Peter Robinson and Nick Ellis. 2008. Handbook of
Cognitive Linguistics and Second Language Acqui-
sition. Routledge, New York and London.
Anil Kumar Singh and Harshit Surana. 2007. Can cor-
pus based measures be used for comparative study
of languages? In Proceedings of the Ninth Meet-
ing of ACL Special Interest Group on Computational
Phonology and Morphology, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Anil Kumar Singh. 2010. Modeling and Application of
Linguistic Similarity. Ph.D. thesis, IIIT, Hyderabad,
India.
Luis Von Ahn. 2005. Human computation. Ph.D. the-
sis, Pittsburgh, PA, USA. Adviser-Blum, Manuel.
Fei Xia. 2001. Automatic Grammar Generation from
Two Different Perspectives. Ph.D. thesis, University
of Pennsylvania.
21
Proceedings of the 8th Workshop on Asian Language Resources, pages 120?128,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
A Preliminary Work on Causative Verbs in Hindi  
Rafiya Begum, Dipti Misra Sharma 
                Language Technologies Research Centre, IIIT. 
rafiya@research.iiit.ac.in, 
dipti@iiit.ac.in  
 
Abstract 
This paper introduces a preliminary 
work on Hindi causative verbs: their 
classification, a linguistic model for 
their classification and their verb 
frames. The main objective of this work 
is to come up with a classification of the 
Hindi causative verbs. In the classifica-
tion we show how different types of 
Hindi verbs have different types of 
causative forms. It will be a linguistic 
resource for Hindi causative verbs 
which can be used in various NLP ap-
plications. This resource enriches the al-
ready available linguistic resource on 
Hindi verb frames (Begum et al, 
2008b). This resource will be helpful in 
getting proper insight into Hindi verbs. 
In this paper, we present the morpholo-
gy, semantics and syntax of the causa-
tive verbs. The morphology is captured 
by the word generation process; seman-
tics is captured by the linguistic model 
followed for classifying the verbs and 
the syntax has been captured by the verb 
frames using relations given by Panini. 
1 Introduction  
Verbs play a major role in expressing the mean-
ing of a sentence and its syntactic behavior. 
They decide the number of participants that will 
participate in the action. Semantically verbs are 
classified into action verbs, state verbs and 
process verbs. Syntactically they are classified 
into intransitives, transitives and ditransitives. 
The morphological, semantic and syntactic 
properties of verbs play an important role in 
deeper level analysis such as parsing. 
Causative verbs are differently realized in dif-
ferent languages. These verbs have been an in-
teresting area of study. The study of causative 
constructions is important as it involves the in-
teraction of various components such as seman-
tics, syntax and morphology (Comrie, 1981). 
This paper presents the preliminary work on 
Hindi causative verbs. 
2 Causative Verbs 
Causative verbs mean that some actor makes 
somebody else do something or causes him to be 
in a certain state (Agnihotri, 2007). The causal 
verb indicates the causing of another to do 
something, instead of doing it oneself (Greaves, 
1983). Semantically causative verbs refer to a 
causative situation which has two components: 
(a) the causing situation or the antecedent; (b) 
the caused situation or the consequent. These 
two combine to make a causative situation (Ne-
dyalkov and Silnitsky, 1973). There are differ-
ent ways in which causation is indicated in dif-
ferent languages. There are three types of causa-
tives: Morphological causatives, Periphrastic 
causatives and Lexical causatives (Comrie, 
1981). 
Morphological Causatives indicate causa-
tion with the help of verbal affixes. Sanskrit, 
Hindi/Urdu, Persian, Arabic, Hebrew, Japanese, 
Khmer and Finnish languages have morpholog-
ical causatives. Periphrastic causatives indi-
cate causation with the help of a verb which 
occurs along with the main verb. For example, 
in English in a sentence such as:  
 
(1) John made the child drink milk. 
 
In the above example the verb make is ex-
pressing causation which is occurring along 
with the verb drink which in turn is expressing 
120
the main action. English, German and French 
are some of the languages which have periph-
rastic causatives. Lexical causatives are those 
in which there is no morphological similarity 
between the base verb root and the causative 
verb form. A different lexical item is used to 
indicate causation. For example, the causative 
of English eat is feed. English and Japanese 
have lexical causatives. English has both pe-
riphrastic and lexical causatives. 
3 Causative verbs in Hindi 
Causatives in Hindi are realized through a mor-
phological process. In Hindi, a base verb root  
changes to a causative verb when affixed by 
either an ?-aa? or a ?-vaa? suffix.  
 
Base verb First causal  Second causal 
so  sul-aa   sul-vaa  
?sleep?  ?put to sleep? ?cause to put to  
       sleep? 
 
In each step of causative derivation there is 
an increase in the valency of the verb (Kachru, 
2006; Comrie, 1981) 
 
(2) baccaa   soyaa                  
      child       sleep.Pst 
      ?The child slept.? 
 
(3) aayaa ne     bacce ko         sulaayaa   
      maid   Erg.  child Acc.      sleep.Caus.Pst 
      ?The maid put the child to sleep.? 
 
(4) maa.N  ne           aayaa se        bacce ko       
      mother Erg.        maid   by       child  Acc.      
      sulvaayaa   
      sleep.Caus.Pst       
      ?Mother caused the maid to put the child to    
       sleep.? 
 
Hindi verbs are divided into two groups 
based on their behaviour in causative sentences: 
affective verbs and effective verbs (Kachru, 
2006). The action of affective verbs benefits or 
affects the agent. Affective verbs will have both 
first and second causal forms. Verbs such as 
ronaa ?to cry? and dau.Dnaa ?to run? are affec-
tive intransitive verbs. Only verbs belonging to 
khaanaa ?to eat? class come under affective 
transitive verbs. The agent of the affective in-
transitive verb becomes the patient and the 
agent of the affective transitive verbs becomes 
the recipient in the first causal and they both 
will take a ko postpositon (Hindi case marker). 
Effective verbs and ditransitive verbs have only 
one causal form. The agent of the effective verb 
and ditransitive verb becomes the causative 
agent in the first causal. So this causative agent 
in the first causal takes a se ?with? postposition 
(Hindi case marker). Verbs belonging to karnaa 
?to do? class come under the effective verbs.  
The major studies in Hindi causatives: Kach-
ru (1966), Kachru (1980), Kachru (2006), 
McGregor (1995), Greaves (1983), Kellogg 
(1972), Agnihotri (2007), Sahaay (2004), Sa-
haay  (2007), Singh (1997) and Tripathi (1986). 
Kachru (1966) has given the classification of 
Hindi verbs based on their causativization beha-
vior. The others have mostly talked about the 
derivation process of the causative verbs. 
However the classification of causative verbs 
in Hindi remains an issue of discussion. Since 
they are morphologically related, the decision of 
what is the base verb form of these verbs re-
mains a point of discussion. There are two ap-
proaches which are followed in deciding the 
base verb: (1) causative formation based only 
on morphology, (2) causative formation based 
on morphology and semantics. 
 
I.Based on Morphology 
 
Base verb         First causal  Second causal 
(Intransitive)     (Transitive)        (Causative) 
khul         khol   khulvaa 
?open?    ?open?                ?cause to open? 
 
II. Based on Morphology and Semantics 
 
Intransitive         Base verb  Second causal 
(Intransitive)        (Transitive)     (Causative) 
khul            khol   khulvaa 
?open?                 ?open?             ?cause to open?          
 
In I, khul ?open? is taken as the base verb and 
khol ?open? and khulvaa ?cause to open? are de-
rived from it by adding suffix ?-aa? and ?-vaa? 
respectively to the base verb (Kachru, 1966; 
Kachru, 1980). The arrow denotes the direction 
of derivation from base verb. Here, the forward 
arrow denotes the increment of the argument 
from base to the causal forms. On the other 
121
hand, in II, khol ?open? is taken as the base verb. 
Here, other than morphology, the semantics of 
the verbs is also taken into consideration. Here 
khul ?open? and khulvaa ?cause to open? are de-
rived from the base verb khol ?open?. khulvaa 
?cause to open? is a causative verb which is de-
rived from the base verb by adding suffix ?-vaa? 
to it. khul ?open? is a derived intransitive form. 
The agent of the base verb khol ?open? is not 
realized on the surface level of the derived in-
transitive verb khul ?open? though it is implied 
semantically. Here there is both forward and 
backward derivation. From base verb to the de-
rived intransitive it is a backward derivation 
which means there is a reduction of one argu-
ment from base verb to the derived intransitive 
verb (Tripathi, 1986; Reinhart, 2005).  
In this paper, we motivate our work by pre-
senting our approach for classifying the causa-
tive verbs in Hindi. 
4 Our Approach 
4.1 Linguistic Model for Classifying Caus-
ative verbs 
We have followed Paninian Grammatical 
framework in this model as the theoretical basis 
for our approach. The meaning of every verbal 
root (dhaatu) consists of: (a) activity (vyaapaa-
ra) and; (b) result (phala). Activity denotes the 
actions carried out by the various participants 
(karakas) involved in the action. Result denotes 
the state that is reached, when the action is 
completed (Bharati et al, 1995). The partici-
pants of the action expressed by the verb root 
are called karakas. There are six basic karakas, 
namely; adhikarana ?location?, apaadaan 
?source?, sampradaan ?recipient?, karana ?in-
strument?, karma ?theme? and karta ?agent? 
(Begum et al, 2008a). Here the mapping be-
tween karakas and theta roles is a rough map-
ping. 
The karta karaka is the locus of the activity. 
Similarly karma karaka is the locus of the re-
sult. The locus of the activity implied by the 
verbal root can be animate or inanimate. Sen-
tence (2) given above, is the example where the 
locus of the activity is animate. Sentence (5) 
given below, is the example where the locus of 
the activity is inanimate.  
 
(5) darvaazaa   khulaa 
      door             open.Pst 
      ?Door opened.?   
   
(6) raam ne     darvaazaa    kholaa  
      ram   Erg.   door             open.Pst 
      ?Ram opened the door.? 
                       
(7) maiM ne      raam se      darvaazaa  
      I         Erg.   ram    by     door       
      khulvaayaa  
      open.Caus.Pst 
      ?I made Ram open the door.? 
 
 When we come to the causatives, the notion 
of prayojak karta ?causer?, prayojya karta 
?causee? and madhyastha karta ?mediator 
causer? are introduced. prayojak karta ?causer? 
is the initiator of the action. prayojya karta 
?causee? is the one who is made to do the action 
by the prayojak karta ?causer?. madhyasta karta 
?mediator causer? is the causative agent of the 
action. The karta of the base verb becomes the 
prayojya karta of the causative verb and the 
prayojak karta of the first causative becomes 
the madhyasta karta of the second causative.  
 This model takes both semantics and 
morphology into consideration. 
4.1.1 Semantics  
(8) caabii  ne    taalaa  kholaa   
      key      Erg   lock     open.Pst 
     ?The key opened the lock.? 
 
(9)* raam  ne   caabii  se   taalaa  khulvaayaa 
       ram   Erg.   key    by    lock    open.Caus.Pst    
       ?Ram caused the key to open the lock.?  
  
(10)raam ne   mohan  dvaaraa caabii  se  taalaa   
       ram  Erg. mohan  by           key     with lock   
      khulvaayaa   
      open.Caus.Pst. 
      ?Ram made Mohan open the lock with the   
       key.?  
 
In (8), caabii ?key? is the karta of the transi-
tive verb khol ?open?. caabii ?key? is an inani-
mate karta so this sentence can?t be causati-
vized. (8) has been tried to causativize in (9) 
which is unacceptable. (9) is actually interpreted 
as (10) where an inanimate  noun with a se 
?with? postposition acts as an instrument and not 
as a prayojya karta ?causee?. So in (10), caabii  
122
?key? is an inanimate noun and takes se ?with? 
postposition so caabii  se ?with the key? acts as 
instrument and mohan ?Mohan? acts as the 
prayojya karta ?causee? (Kachru, 1966). It 
seems that only those verbs can be causativized 
which take an animate karta.  
Out of the above two given approaches, we 
are following approach II where both morphol-
ogy and semantics are considered. In our ap-
proach we are saying that only those base verbs 
can be causativized which take an animate karta 
and it should also have volitionality (Tripathi, 
1986; Reinhart, 2005). Those base verbs which 
take an inanimate karta can?t be causativized. 
So in our approach the prayojya karta ?causee? 
in the causative sentence is always animate as 
the karta of the base verb becomes the prayojya 
karta ?causee? in the causative sentences. In our 
approach we have the notion of karmakartri 
which says an intransitive can be derived out of 
a basic transitive verb and the karma of the ba-
sic transitive verb becomes the karta of the de-
rived intransitive verb. So the karta of the de-
rived intransitive verb is called karmakartri. 
The derived intransitive verbs are like unaccusa-
tive verbs of English.  
Whereas in approach I, the intransitive base 
verbs that take both animate and inanimate kar-
ta can be causativized. But in case of transitives, 
base verbs which take only animate karta can be 
causativized. Ditransitives can also be causati-
vized. (Kachru 1966; Kachru 1980) 
We follow the dependency tagging scheme 
proposed by Begum et al (2008a) for the devel-
opment of a dependency annotation for Indian 
Languages. In this scheme prayojak karta 
?causer?, prayojya karta ?causee? and madhyas-
tha karta ?mediator causer? are represented as 
pk1, jk1 and mk1 respectively.  
Some of the base verb forms and their causa-
tive sentences are given below with dependency 
relations marked in the brackets for the appro-
priate arguments: 
 
(11) raam ne(k1)  seb(k2)       khaayaa  
      ram   Erg.      apple           eat.Pst 
     ?Ram ate an apple.?  
 
(12) siitaa ne(pk1) raam ko(jk1)  seb(k2)   
      sita    Erg.        ram   Acc.      apple   
      khilaayaa   
      eat.Caus.Pst 
     ?Sita fed Ram an apple.? 
 
(13)maa.N  ne(pk1) siitaa se(mk1) raam ko(jk1)                   
      mother  Erg.       sita     by          ram  Acc.        
      seb(k2)    khilvaayaa                             
      apple       eat.Caus.Pst. 
     ?Mother caused Sita to feed Ram an apple.? 
 
(14) naukar  ne(k1)  kaam(k2)  kiyaa   
        servant Erg.       work         do.Pst 
        ?The servant did the work.? 
 
(15) maalik ne(pk1) naukar  se(mk1)   kaam(k2) 
        master Erg.       servant by             work        
        karvaayaa 
        do.Caus.Pst 
       ?The master caused Ram to do the job.?  
 
(16) raam ne(k1)  siitaa ko(k4) kitaab (k2)  dii 
       ram    Erg.      sita    Dat.      book    give.Pst 
      ?Ram gave a book to Sita.? 
                             
(17) mohan ne(pk1) raam se(jk1) siitaa ko(k4)     
        mohan Erg.       ram   by        sita    Dat.   
        kitaab(k2)  dilaaii 
        book           give.Caus.Pst 
       ?Mohan made Ram give a book to Sita.? 
 
(18) mujhko(k4a)  chaa.Nd(k1)   dikhaa  
        I.Dat.              moon             appear.Pst 
       ?The moon became visible to me.? 
   
(19) maiM ne(k1)  chaa.Nd(k2)  dekhaa  
         I        Erg.       moon            see.Pst  
        ?I saw the moon.? 
 
(20) maiM    ne(pk1) raam ko(jk1)  chaa.Nd(k2) 
        mother Erg.         ram  Dat.       moon 
        dikhaayaa  
        see.Caus.Pst    
        ?Mother showed moon to Ram.? 
                  
(21) maiM ne(pk1)  mohan se(mk1)   
        I         Erg.        mohan by 
        raam ko(jk1)    chaa.Nd(k2)   dikhlaayaa  
        ram   Dat.          moon             see.Caus.Pst           
        ?Mother made Mohan show moon to Ram.? 
123
4.1.2 Morphology  
In this section we have given the derivation 
process of the Hindi causative verbs. We have 
studied 160 Hindi verbs and have come up with 
certain number of rules for the derivation 
process of causative verbs.  
When causative affixes are added to the base 
verb roots then some of the base verb roots 
change in form and some don?t. Various causal 
affixes are added to each verb type to form 
causatives. An example of affix addition for 
each verb type is discussed below. The affixes 
that are added are given in bold. The changes in 
the base verb root are underlined and made bold 
in both root form and the causal form. 
 
4.1.2.1 Type-1 and its causative forms: 
 
Suffix ?-aa? is added to the verb root to form the 
first causal and ?-vaa? to form the second 
causal.  
 
No Change in the Root: 
Chip  Chip-aa Chip-vaa 
?hide?  ?hide?  ?cause to hide? 
 
Change in the Root: 
? aa ??a 
 
naach        nach-aa         nach-vaa 
?dance?  ?make someone dance? ?cause to make  
                                                   someone dance? 
 
4.1.2.2 Type-2 and its causative forms: 
 
Suffix ?-aa? is added to the verb root to form the 
first causal and ?-vaa? to form the second 
causal.  
 
No Change in the Root: 
likh  likh-aa  likh-vaa 
?write?  ?dictate?          ?cause to dictate? 
 
Change in the root: 
? aa,  ii  ?  i;In addition, 'l' is    
      inserted here between the root and   
      the causative suffix.  
 
khaa          khi-l-aa khi-l-vaa 
?eat?         ?feed?   ?cause to feed? 
pii    pi-l-aa  pi-l-vaa 
?drink? ?make someone drink?  ?cause to make   
                                                    someone drink? 
 
4.1.2.3 Type-3 and its causative forms: 
 
Suffix ?-vaa? is added to the verb root to form 
the first causal. 
 
No Change in the Root: 
khariid   khariid-vaa 
?buy?   ?cause to buy? 
 
Change in the Root: 
? aa  ?  a 
 
gaa   ga-vaa 
?sing?   ?cause to sing? 
 
4.1.2.4 Type-4 and its causative forms: 
 
Suffix ?-aa/-vaa? is added to the verb root to 
form the first causal. 
 
No Change in the Root: 
paros   paros-vaa 
?serve?   ?cause to serve? 
 
Change in the root:  
? e  ?  i; In addition, 'l' is  
 inserted here between the root and  
 the causative suffix. 
 
de          di-l-aa /di-l-vaa 
?give?          ?cause to give? 
 
In case type-5 and type-6 verbs, we can de-
rive intransitive verbs out of transitive verbs. 
Here we have two types of word formations: 
 
? causative formation, 
?  Derived intransitive verb formation  
 
Causative derivation is the forward deriva-
tion and intransitive derivation is backward de-
rivation.  
 
4.1.2.5 Type-5 and its causative forms: 
 
Suffix ?-aa? is added to the verb root to form the 
first causal and ?vaa? to form  the second causal. 
In this verb type there is no example where the 
124
verb root form doesn?t change. 
 
Causative Formation: Change in the root  
? e  ?  i  
 
dekh     dikh-aa  dikh-vaa 
?see?              ?show?               ?cause to show? 
    
Derived intransitive formation: Change in 
the above root: 
? i          e 
     
dikh             dekh   
?appear?  ?see?      
 
4.1.2.6 Type-6 and its causative forms: 
 
Suffix ?-aa/-vaa? is added to the transitive verb 
root to form the first causal. 
 
Causative formation: No change in the root 
bhar   bhar-vaa /bhar-aa 
?fill?   ?cause someone to fill? 
  
Derived intransitive formation: No change in 
the root 
bhar                       bhar   
?to fill?                         ?to fill?  
   
Causative formation: Change in the root 
? o      ?    u; In addition, 'l' is inserted 
here between the root and the  
causative suffix  
 
dho   dhu-l-aa /dhu-l-vaa 
?wash?   ?cause to wash? 
 
Derived intransitive formation: Change in 
the above root: 
? u        o; In addition, 'l' is inserted at   
                    the end of the root 
   
dhu-l   dho   
?be washed?  ?cause to wash? 
 
In the implementation of the causative verbs, 
the causative feature of a verb is reflected in the 
morph analysis. There are two possible ways to 
implement causative information: (i) All the 
causative verb roots are included in the root dic-
tionary of the morph analyzer with an additional 
feature marking it a causative verb type. (ii) For 
all causative verbs the following information is 
marked; causative root, base root, verb type and 
causative suffix. In (i), the information of base 
verb root from which the causative root is de-
rived is missing which is captured in (ii). In the 
above mentioned two ways the latter gives more 
detailed information than the former. 
4.2 Methodology of the Work 
For this work, 160 base verbs were taken, their 
causative forms were given and were classified. 
Rules for deriving causative verb forms from 
their base forms were made. Verb frames for 
base verbs and their causative forms were de-
veloped. Based on the analysis of the base verbs 
certain problem cases were logged and generali-
zations regarding causativization were made. In 
this paper, we briefly discuss about all the 
points  mentioned above. 
4.3 Classification of Hindi Causative  
Verbs 
Here Hindi verbs have been classified into 6 
types based on their causativization behavior:  
 
? Type-1: Basic Intransitive verb  
 
Basic intransitive verb has two causal forms 
i.e., first causal and second causal form. First 
causal of the basic intransitive verb functions as 
a transitive verb. The subject of the basic intran-
sitive verb becomes the object of the transitive 
verb or the first causal form. The subject of the 
first causal form becomes the causative agent of 
the second causal form. Sentence (2) is the ex-
ample of basic intransitive and sentences (3) 
and (4) are its causative forms. 
 
? Type-2: Basic Transitive verb type-I (which 
is similar to khaanaa ?to eat? verb type giv-
en by Kachru (1966))  
 
? Type-3: Basic Transitive verb type-II 
(which is similar to karnaa ?to eat? verb 
type given by Kachru (1966)) 
 
Type-2 and type-3 are transitive verbs which 
are divided into two types based on their causa-
tivization behavior. Basic transitive verbs of 
type-I, like khaanaa ?to eat? have two causal 
forms. First causal of khaanaa ?to eat? type verb 
125
also functions as ditransitive. Whereas transitive 
verbs of type-II, like karnaa ?to do? have one 
causal form. First causal of karnaa ?to eat? type 
verb functions as causative. Sentences (11-13) 
are examples for type-2 verb. Sentences (14-15) 
are examples for type-3 verb.  
 
? Type-4: Basic Ditransitive verb  
 
Ditransitive verbs also have one causal form. 
Sentences (16-17) are examples for type-4 verb.  
 
? Type-5: Basic Transitive verb type-I, out of 
which intransitive verbs can be derived 
which takes a dative subject,  
 
? Type-6: Basic Transitive verb type-II, out 
of which intransitive verbs can be derived.  
 
Type-5 and type-6 are transitive verbs which 
have causal forms depending on whether it is 
type-I (khaanaa ?to eat?) transitive or type-II 
(karnaa ?to do?) transitive and in addition both 
have a derived intransitive form. Type-5 takes a 
dative subject in the base form. Sentences (18-
21) are examples for type-5 verb. Sentences (5-
7) are examples for type-6 verb. Other than the 
4 classes classified by Kachru (1966), we have 
two more extra classes, i.e., type-5 and type-6.  
An example for each verb type that goes into 
the classification is given below: 
 
? Type-1 
Base verb First causal Second Causal 
so  sulaa  sulvaa  
?sleep?       ?put to sleep? ?cause to put to sleep?          
                               
? Type-2 
Base verb First causal Second Causal 
khaa  khilaa  khilvaa 
?eat?  ?feed?  ?cause to feed? 
 
? Type-3 
Base verb First causal 
kar        karaa/karvaa 
?do?         ?cause to do? 
 
? Type-4 
Base verb First causal 
de         dilaa/dilvaa 
?give?         ?cause to give?  
 
? Type-5 
Intransitive  Base verb 
dikh             dekh 
?appear?            ?see? 
 
Base verb First causal Second Causal 
dekh  dikhaa  dikhvaa 
?see?  ?show?  ?cause to  
          show?  
? Type-6  
Intransitive  Base verb 
khul            khol  
?open?            ?open? 
 
Base verb   First causal 
khol      khulvaa 
?open?                 ?cause to open? 
5 Verb Frames 
In this section we list out the syntactic frames 
for all the causative types discussed in the pre-
vious sections. Verb frame (Begum et al, 
2008b) is given for the base form and for its 
first and second causal form. For ease of exposi-
tion, below we show only the relevant informa-
tion of a verb frame. Components not necessary 
for the present discussion have been left out. 
Here the structure of a verb frame is given in 
terms of dependency relation, postposition 
(Hindi case marker) and TAM. We have taken 
past tense (yaa is the past tense marker) in the 
TAM. Refer the examples given above for each 
type of causatives for a better understanding of 
the frames.   
 
I.Frame of Type-1 and its Causative Forms:  
 
        Relation-Postposition                    TAM 
 
(a)k1-0     yaa  
(b)pk1-ne      jk1-ko  yaa  
(c)pk1-ne      mk1-se jk1- ko  yaa  
 
II.Frame of Type-2 and its Causative Forms: 
 
        Relation-Postposition                    TAM 
 
(a)k1-ne   k2-0 yaa 
(b)pk1-ne          jk1-ko     k2-0 yaa  
126
(c)pk1-ne    mk1-se    jk1- ko     k2-0 yaa 
 
III. Frame of Type-3 and its Causative 
Forms: 
 
        Relation-Postposition                    TAM 
 
(a)k1-ne                           k2-0         yaa  
(b)pk1-ne       jk1-se        k2-0          yaa  
 
IV. Frame of Type-4 and its Causative 
Forms: 
 
        Relation-Postposition                    TAM 
 
 (a) k1-ne            k4-ko     k2-0        yaa  
 (b) pk1-ne     jk1-se    k4-ko      k2-0   yaa  
 
V. Frame of Type-5 and its Causative Forms: 
 
        Relation-Postposition                    TAM 
 
(a)k4a-ko  k1                      aa 
(b)k1-ne                   k2-0         aa 
(c)pk1-ne                        jk1-ko      k2-0       yaa  
(d)pk1-ne     mk1-se      jk1-ko       k2-0        yaa  
    
VI. Frame of Type-6 and its Causative 
Forms: 
          
        Relation-Postposition                    TAM 
 
(a)k1-0        aa 
(b)k1-ne       k2-0                   aa 
(c)pk1-ne       jk1-se           k2-0            yaa  
6 Issues and Observations 
There are some verbs which can?t be causati-
vized. Motion verbs like aa ?come? and jaa ?go? 
can? t be causativized. After analysing certain 
amount of corpus we have observed that not all 
motion verbs behave like the above verbs . aa-
naa ?to come? and jaanaa ?to go? verbs can't be 
causativized because these verbs always occur 
as main verbs and take the following verbs as 
manner adverbs: chalnaa, bhaagnaa, daudnaa. 
For instance, chalkar aayaa ?came running? and 
daudkar gayaa ?went running?. Those motion 
verbs which occur as manner adverbs and modi-
fy another motion verb can be causativized and  
those verbs which occur as main verbs and nev-
er occur as manner adverbs of another motion 
verb can't be causativized. Natural process verbs 
like khil ?blossom?, garajnaa ?thunder? and ug 
?rise? also can?t be causativized. 
There are three types of the verb nikal 
?leave?. All the three are used as intransitives. 
? derived intransitive: sense ? drain out  
(22) paanii  kamre se       nikal   gayaa 
        water   room  from   leave  go.Pst. 
        ?Water drained out of the room.? 
 
? Baisc Intransitive: sense ? walked out 
 
(23) raam  kamre  se     baahar  nikal    gayaa 
        ram    room  from  out         leave  go.Pst. 
        ?Ram walked out of the room.? 
 
? Baisc Intransitive which involves natural   
       process. 
 
(24) gangaa  gangotrii se       nikaltii            
        ganga     gangotri  from  emerge.Impf. 
        hai 
        be.Pres. 
        ?Ganga emerges from Gangotri.? 
 
The first type is a derived intransitive which 
is derived from the base transitive verb nikaal 
?remove?. This base transitive verb root can be 
causativized. The second type is basic intransi-
tive which can also be causativized. The third 
type which is natural process can?t be causati-
vized. This shows how important the property 
of animacy for making causatives is.  
7 Conclusion and Future Work 
In this work we flesh out the linguistic devices 
that work for causativization. In this paper we 
have introduced a preliminary work on Hindi 
causative verbs. We have given the classifica-
tion of causative verbs and the linguistic model 
followed for their classification. We have also 
given the verb frames of the causative verbs. 
These insights have been incorporated in the 
Hindi dependency treebank (Bhatt et al, 2009). 
We also plan to use the verb frames in a Hindi 
dependency parser (Bharati et al, 2009) to im-
prove its performance. 
 
127
References 
Agnihotri, Rama K. 2007. Hindi, An Essential 
Grammar. Routledge, London and New York, pp. 
121-126. 
Balachandran, Lakshmi B. 1973.  A Case Grammar 
of Hindi. Central Institute of India, Agra. 
Begum, Rafiya, Samar Husain, Arun Dhwaj, Dipti 
M. Sharma, Lakshmi Bai and Rajeev Sangal. 
2008. Dependency Annotation Scheme for Indian 
Languages. Proceedings of The Third Interna-
tional Joint Conference on Natural Language 
Processing (IJCNLP), Hyderabad, India.  
Begum, Rafiya, Samar Husain, Dipti M. Sharma and 
Lakshmi Bai. 2008. Developing Verb Frames in 
Hindi. In Proceedings of The Sixth International 
Conference on Language Resources and Evalua-
tion (LREC). Marrakech, Morocco.  
Bharati, Akshar, Vineet Chaitanya, and Rajeev San-
gal. 1995. Natural Language Processing: A Pani-
nian Perspective, Prentice-Hall of India, New 
Delhi, pp. 65-106.  
Bharati, Akshar, Samar Husain, Dipti Misra Sharma 
and Rajeev Sangal. 2009. Two stage constraint 
based hybrid approach to free word order lan-
guage dependency parsing. In Proceedings of the 
11th International Conference on Parsing Tech-
nologies (IWPT09). Paris.  
Bhatt, Rajesh, Bhuvana Narasimhan, Martha Palmer, 
Owen Rambow, Dipti Misra Sharma, Fei Xia. 
2009. A Multi-Representational and Multi-
Layered Treebank for Hindi/Urdu. In the Third 
Linguistic Annotation Workshop (The LAW III) in 
conjunction with ACL/IJCNLP 2009. Singapore.  
Comrie, Bernard. 1981. Language Universals and 
Linguistic typology: Syntax and Morphology. The 
University of Chicago Press. 
Greaves, Edwin. 1983. Hindi Grammar. Asian Edu-
cational Services, New Delhi, pp. 301-313  
Kachru, Yamuna. 1966. Introduction To Hindi Syn-
tax. University of Illinois, Illionois, pp. 62-81  
Kachru, Yamuna. 1980. Aspects of Hindi Grammar. 
Manohar Publications.  
Kachru, Yamuna. 2006. Hindi. London Oriental and 
African Language Libtary. 
Kale, M.R. 1960. A Higher Sanskrit Grammar. Mo-
tilal Banarasidass, Delhi. 
Kellogg, S. H. 1972. A Grammar of the Hindi Lan-
guage. Munshiram Manoharlal, New Delhi, pp. 
253-257.   
McGregor, R. S. 1995. Outline of Hindi Grammar. 
Oxford University Press, pp. 118-125.  
Nedyalkov, Vladmir P., Georgij G. Silnitsky. 1973. 
The typology of Morphological and Lexical 
Causatives, in: F. Kiefer, (ed.), Trends in Soviet 
Theoretical Linguistics, Foundations of Language, 
Supplementary Series 18, Dordrecht, 1-32. 
Reinhart, Tanya. 2005. Causativization and Decau-
sativization: Unpublished technical report. LSA 
2005.  
Sahaay, Chaturbhuj. 2004. Hindi ke Muul Vaakya 
Saanche, Kumar Prakaashan, Agra, pp. 91-100. 
Sahaay, Chaturbhuj. 2007. Hindi Padvigyaan, Ku-
mar Prakaashan, Agra. 
Shastri, Charu D. 1990. Panini: Re-Interpreted. Mo-
tilal Banarasidass, Delhi. 
Singh, Rajendra and Rama K. Agnihotri. 1997. Hindi 
Morphology: A Word-based Description. Motilal 
Banarasidass, Delhi. 
Tripathi, Acharya R. 1986. Hindi Bhashaanushasan. 
Bihar Granth Academy, Patna.  
 
128
Proceedings of the Fifth Law Workshop (LAW V), pages 119?123,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Creating an Annotated Tamil Corpus as a Discourse Resource 
 
 
Ravi Teja Rachakonda Dipti Misra Sharma   
International Institute of International Institute of   
Information Technology Information Technology   
Hyderabad, India Hyderabad, India   
raviteja.r@research.iiit.ac.in dipti@iiit.ac.in   
 
 
 
 
 
 
Abstract 
We describe our efforts to apply the Penn 
Discourse Treebank guidelines on a Tamil 
corpus to create an annotated corpus of dis-
course relations in Tamil. After conducting 
a preliminary exploratory study on Tamil 
discourse connectives, we show our obser-
vations and results of a pilot experiment 
that we conducted by annotating a small 
portion of our corpus. Our ultimate goal is 
to develop a Tamil Discourse Relation 
Bank that will be useful as a resource for 
further research in Tamil discourse. Fur-
thermore, a study of the behavior of dis-
course connectives in Tamil will also help 
in furthering the cross-linguistic under-
standing of discourse connectives. 
1 Introduction 
The study of discourse structure in natural lan-
guage processing has its applications in emerging 
fields such as coherence evaluation, question an-
swering, natural language generation and textual 
summarization. Such a study is possible in a given 
human language only if there are sufficient dis-
course annotated resources available for that lan-
guage. The Penn Discourse Treebank (PDTB) is a 
project whose goal is to annotate the discourse re-
lations holding between events described in a text. 
The PDTB is a lexically grounded approach where 
discourse relations are anchored in lexical items 
wherever they are explicitly realized in the text 
(Miltsakaki et al 2004, Prasad et al, 2008). To 
foster cross-linguistic studies in discourse rela-
tions, projects similar to the PDTB in discourse 
annotation were initiated in Czech (Mladov? et al, 
2008), Chinese (Xue, 2005), Turkish (Zeyrek and 
Webber, 2008) and Hindi (Prasad et al, 2008). We 
explore how the underlying framework and annota-
tion guidelines apply to Tamil, a morphologically 
rich, agglutinative, free word order language. 
In this paper, we present how a corpus of Tamil 
texts was created on which we performed our pilot 
experiment. Next, in Section 3 we cover the basics 
of the PDTB guidelines that we followed during 
our annotation process. In Section 4, we show var-
ious categories of Tamil discourse connectives that 
we identified after a preliminary study on dis-
course connectives in Tamil, illustrating each with 
examples. In Section 5, we discuss some interest-
ing issues specific to Tamil that we encountered 
during discourse annotation and present the results 
of the pilot experiment that we performed on our 
source corpus. We conclude this paper in Section 6 
by discussing about challenges that were unique to 
our work and our plans for the future. 
2 Source Corpus 
We collected Tamil encyclopedia articles from the 
June 2008 edition of the Wikipedia static HTML 
dumps1. Elements such as HTML metadata, navi-
gational links, etc. were then removed until only 
the text of the articles remained. A corpus was then 
built by collecting the texts from all the articles in 
the dump. The corpus thus created consists of 
                                                          
1 http://static.wikipedia.org/ 
119
about 2.2 million words from approximately 
200,000 sentences. 
Since the texts used in building the corpus were 
all encyclopedia articles featured in the Tamil lan-
guage version of Wikipedia, the corpus covers a 
wide variety of topics including arts, culture, biog-
raphies, geography, society, history, etc., written 
and edited by volunteers from around the world. 
3 Penn Discourse Treebank Guidelines  
The PDTB is a resource built on discourse struc-
ture in (Webber and Joshi, 1998) where discourse 
connectives are treated as discourse-level predi-
cates that always take exactly two abstract objects 
such as events, states and propositions as their ar-
guments. We now describe the types of connec-
tives and their senses from the PDTB framework 
and provide examples from Tamil sentences. 
3.1 Annotation Process 
The process of discourse annotation involves iden-
tifying discourse connectives in raw text and then 
annotating their arguments and semantics. Dis-
course connectives are identified as being explicit, 
implicit, AltLex, EntRel or NoRel (Prasad et al 
2008). These classes are described in detail in Sec-
tion 4. By convention, annotated explicit connec-
tives are underlined and implicit connectives are 
shown by the marker, ?(Implicit=)?. As can be 
seen in example (1), one of the arguments is shown 
enclosed between {} and the other argument is 
shown in []. The AltLex, EntRel or NoRel relations 
are shown by underlining, i.e., as ?(AltLex=)?, 
?(EntRel)? and ?(NoRel)?, respectively. 
 
(1) {eN kAl uDaindadaN}Al [eNNAl viLayADa 
muDiyAdu].  
?{My leg broke}, hence [I cannot play].? 
 
3.2 Sense Hierarchy 
The semantics of discourse relations are termed as 
senses and are then classified hierarchically using 
four top-level classes ?Comparison?, ?Contingen-
cy?, ?Expansion? and ?Temporal?. Each class is 
refined by its component types and these, in turn, 
are further refined by the subtype level. 
It is interesting to note that some connectives 
have multiple senses. In example (2) the affixed ?
um connective carries the sense of type Expan-
sion:Conjunction ?also? whereas in example (3) the 
same affix carries the sense of the subtype Contin-
gency:Concession ?however?. 
 
(2) {idaN mUlam avar oru nAL pOttiyil oNba-
dAyiram OttangaLai kaDanda pattAvadu vIrar 
eNra perumaiyai pettrAr}. [inda OttangaLai 
kaDanda mudal teNNAppirikka vIrar eNra 
sAdaNaiyaiy]um [nigaztiNAr]. 
?{By this, he became the tenth player to cross 
nine thousand runs in one-day internationals}. 
[He] also [attained the record of becoming the 
first South African player to cross these many 
runs].? 
(3)  {seNra murai kirikket ulagakkOppaiyiN pOthu 
pangu pattriyadai vida iraNDu aNigaL immurai 
kUDudalAga pangu pattriya pOd}um, 
[motthap pOttigaL inda muraiyil kuraivAN-
adAgum.]  
?Though {two more teams participated when 
compared to last Cricket World Cup}, [the total 
matches played during this time were fewer].? 
 
4 Discourse Connectives in Tamil 
Tamil is an agglutinative language where mor-
phemes are affixed to the roots of individual 
words, a trait that it shares with many other Dra-
vidian languages and languages like Turkish, Esto-
nian and Japanese. Here, each affix represents 
information such as discourse connective, gender, 
number, etc. We now describe how we try to cap-
ture various types of Tamil discourse connectives 
using a proposed scheme which is based on the 
existing PDTB guidelines proposed by (Prasad et 
al., 2007). 
 
4.1 Explicit Discourse Connectives 
 
Explicit discourse connectives are lexical items 
present in text that are used to anchor the discourse 
relations portrayed by them. In Tamil, they are 
found as affixes to the verb, as in example (4) 
where the affix -Al conveys the meaning ?so?. This 
is in a way similar to the simplex subordinators in 
Turkish, as described in (Zeyrek and Webber, 
2008). However, like in English, explicit discourse 
connectives are also realized as unbound lexical 
items, as can be seen in example (5) where the 
word eNavE means ?hence?. 
 
120
(4) {avaradu uDalnalam sariyillAmaiy}Al [nAngu 
mAdangaL avarAl viLayADa iyalavillai].  
?{He was suffering from ill health} so [he 
could not play for four months].? 
(5)  {tirukkuraL aNaittu madattiNarum paDittu 
payaNaDaiyum vagaiyil ezudappattuLLadu}. 
eNavE [innUl palarAl pArAttappaDuginradu]. 
?{Thirukkural has been written in such a way 
that people from all religions can benefit from 
it}. Hence, [this book is praised by many].? 
 
Syntactically, explicit connectives can be coordi-
nating conjunctions e.g., alladu (?or?), subordinat-
ing conjunctions e.g., -Al (?so?), sentential relatives 
e.g., -adaNAl (?because of which?), particles e.g., -
um (?also?) or adverbials e.g., -pOdu (?just then?). 
 
Explicit connectives also occur as conjoined 
connectives where two or more instances of con-
nectives share the same two arguments. Such con-
nectives are annotated as distinct types and are 
annotated discontinuously, as seen in example (6) 
where the connectives -um and -um are paired to-
gether to share the same arguments.  
 
(6)  {mANavargaLukku sattuNavu aLikkav}um 
[avargaL sariyAga uDarpayirchi seiyyav]um 
arasup paLLigaL udava vENDum. 
?Government schools should help in {providing 
nutritious food to the students} and [making 
sure they perform physical exercises]. 
 
4.2 Implicit Discourse Connectives 
 
Implicit discourse connectives are inserted be-
tween adjacent sentence pairs that are not related 
explicitly by any of the syntactically defined set of 
explicit connectives. In such a case, we attempted 
to infer a discourse relation between the sentences 
and a connective expression that best conveys the 
inferred relation is inserted. In example (7), the 
implicit expression uthAraNamAga (?for example?) 
has been inserted as an inferred discourse relation 
between the two sentences. 
 
 (7)  {IyOrA iNa makkaLiN moziyil irundu iNru 
Angilattil vazangum sorkaL uLLaNa}. (Implic-
it=uthAraNamAga) [dingO, vUmErA, vAlabi 
pONra sorkaL IyOravilirindu tONriya sorkaL 
dAN]. 
  ?{There are words that are present in English 
that originated from the language of the Eora 
people}. (Implicit= For example) [Dingo, 
Woomera and Wallaby are words with their or-
igins in Eora].? 
 
4.3 AltLex, EntRel and NoRel 
 
In cases where no implicit connective was appro-
priately found to be placed between adjacent sen-
tence-pairs, we now look at three distinct classes. 
AltLex relations, as seen in example (8) are dis-
course relations where the insertion of an implicit 
connective leads to a redundancy in its expression 
as the relation is already alternatively lexicalized 
by some other expression that cannot be labeled as 
an explicit connective. Example (9) shows an En-
tRel relation where no discourse relation can be 
inferred and the second sentence provides further 
description of an entity realized in the first sen-
tence. When neither a discourse relation nor entity-
based coherence can be inferred between the two 
adjacent sentences, it is described as a NoRel, 
shown in example (10). 
 
(8) {mudalAvadAga mAgim, jOgEshwari, 
pUrivilla rayil nilayangaLil guNDu vedittadu}. 
(AtlLex=idai toDarndu) [mErku rayilvEyiN 
aNaittu rayilgaLum niruttappaTTaNa]. 
 ?{Initially, bombs exploded in Mahim, Joge-
shwari and Poorivilla}. (AltLex=following 
this) [all the trains from the western railway 
were halted].? 
(9) {ivvANDu kirikket ulagakkOppai mErkindiyat 
tIvugaLil mArc padimUnril irundu Epral iru-
battu-ettu varai naDaipettradu}. (EntRel) [in-
dap pOttiyil pangupattriya padiNAru 
nADugaLaic cArnda aNigaLum ovvoru kuzu-
vilum nANgu aNigaL vIdamAga nANgu 
kuzukkaLAga pirikkapattu pOttigaL iDampet-
traNa]. 
  ?{This year?s Cricket World Cup was held in 
West Indies from the thirteenth of March to the 
twenty-eight of April}. (EntRel) [In this com-
petition, the teams representing the sixteen na-
tions were grouped into four groups with four 
teams in each group].? 
(10) {caccin TeNdUlkar ulagiNilEyE migac ciranda 
mattai vIccALarAga karudappadugirAr}. 
(NoRel) [indiya pandu vIccALargaL sariyANa 
muraiyil payirci peruvadillai]. 
?{Sachin Tendulkar is considered the best 
batsman in the world}. (NoRel) [Indian bow-
lers are not being given proper coaching].? 
 
121
5 Observations and Results 
5.1 Combined connectives 
There is a paired connective -um ? -um (?) that 
sometimes expresses an Expansion:Conjunction 
relation between the events where each -um is suf-
fixed to the verb that describes each event (see ex-
ample (6)). Also, there is a connective -Al which 
usually never occurs more than once and some-
times expresses a Contingency:Cause relation be-
tween two events. 
It is interesting to see that in sentences like 
(11), the -Al combines with the -um ? -um to ex-
press something like a new type of relation. In the 
process, the -um ? -um causes the -Al, which is 
usually not doubled, to become doubled, thereby 
forming an -Alum ? -Alum. We call this special 
type of connectives as combined connectives, as 
shown in example (11). 
 
(11)  {kirikket viLayADiyad}Alum {uDarpayirci 
seidad}Alum [sOrvaDaindEN]. 
?Because {I played cricket} and because {I did 
exercise} [I am tired].? 
 
5.2 Redundant connectives 
The connective -O ? -O (?) that conveys a dubi-
tative relation also combines with the -Al connec-
tive in a way similar to what was shown in Section 
5.1 to form the combined connective -AlO ? -AlO 
(?). 
However, in example (12), alladu, an equiva-
lent of the -O ? -O connective has also occurred in 
addition to the combined -AlO ? -AlO connective. 
This may be purely redundant, or could serve a 
purpose to emphasize the dubitative relation ex-
pressed by both alladu and -O ? -O. 
 
(12)  {pOtti samappatt}AlO alladu {muDivu pera-
paDAmal pON}AlO [piNvarum muraigaL mU-
lam aNigaL tarappaDuttapaDum]. 
?If {a game is tied} or if {there is no result}, 
[the qualified teams are chosen using the fol-
lowing rules].? 
5.3 Results of Pilot Study 
In this experiment, we looked at 511 sentences 
from the corpus mentioned in Section 2 and anno-
tated a total of 323 connectives. Table 1 shows the 
distribution of the annotated connectives across the 
different types such as Explicit, Implicit, EntRel, 
AltLex and NoRel. 
 
Connective 
Type 
Count Count 
(unique) 
Count 
(%) 
Senses 
Explicit 269 96 83.3 18 
Implicit 28 16 8.6 13 
EntRel 16 - 5.0 - 
AltLex 8 5 2.5 4 
NoRel 2 - 0.6 - 
 
Table 1: Results of Pilot Experiment 
 
While a higher percentage of the connectives 
annotated are those of the Explicit type, it can also 
be seen that there is a higher proportion of unique 
connectives in the Implicit and AltLex types. Note 
that since EntRel and NoRel connectives are not 
associated with a sense relation or a lexical item, 
their counts are left blank. 
6 Challenges and Future Work 
The agglutinative nature of the Tamil language 
required a deeper analysis to look into suffixes that 
act as discourse connectives in addition to those 
that occur as unbounded lexical items. We also 
found certain interesting examples that were dis-
tinct from those observed during similar approach-
es in relatively less morphologically rich languages 
like English. 
While this was a first attempt at creating a dis-
course annotated Tamil corpus, we are planning to 
conduct future work involving multiple annotators 
which would yield information on annotation met-
rics like inter-annotator agreement, for example. 
Our work and results would also be useful for 
similar approaches in other morphologically rich 
and related South Indian languages such as Mala-
yalam, Kannada, Telugu, etc. 
We will also work on a way in which the dis-
course annotations have been performed will help 
in augmenting the information provided during 
dependency annotations at the sentence-level. 
Acknowledgments 
We are grateful to Prof. Aravind Joshi and 
Prof. Rashmi Prasad of University of Pennsylvania 
and Prof. Bonnie Webber of University of Edin-
burgh for their valuable assistance and feedback. 
122
We would like to thank Prof. Rajeev Sangal of 
IIIT Hyderabad for his timely guidance and useful 
inputs. We also acknowledge the role of Sudheer 
Kolachina in the discussions we had in the writing 
of this paper. 
 
References  
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi and 
Bonnie Webber. 2004. The Penn Discourse Tree-
bank. Proceedings of LREC-2004. 
Rashmi Prasad, Samar Husain, Dipti Mishra Sharma 
and Aravind Joshi. 2008. Towards an Annotated 
Corpus of Discourse Relations in Hindi. Proceedings 
of IJCNLP-2008. 
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan 
Lee, Aravind Joshi, Livio Robaldo and Bonnie Web-
ber. 2007. The Penn Discourse Tree Bank 2.0 Anno-
tation Manual. December 17, 2007. 
Bonnie Webber and Aravind Joshi. 1998. Anchoring a 
lexicalized tree-adjoining grammar for discourse. In 
Manfred Stede, Leo Wanner, and Eduard Hovy, edi-
tors, Discourse Relations and Discourse Markers: 
Proceedings of the Conference, pages 86-92. Associ-
ation of Computational Linguistics. 
Nianwen Xue. 2005. Annotating Discourse Connectives 
in the Chinese Treebank. Proceedings of the ACL 
Workshop on Frontiers in Corpus Annotation II: Pie 
in the Sky. 
Deniz Zeyrek and Bonnie Webber. 2008. A Discourse 
Resource for Turkish: Annotating Discourse Connec-
tives in the METU Corpus. Proceedings of IJCNLP-
2008. 
 
123
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 159?167,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Animacy Annotation in the Hindi Treebank
Itisree Jena, Riyaz Ahmad Bhat, Sambhav Jain and Dipti Misra Sharma
Language Technologies Research Centre, IIIT-Hyderabad, India
{itisree|riyaz.bhat|sambhav.jain}@research.iiit.ac.in, dipti@iiit.ac.in
Abstract
In this paper, we discuss our efforts to anno-
tate nominals in the Hindi Treebank with the
semantic property of animacy. Although the
treebank already encodes lexical information
at a number of levels such as morph and part
of speech, the addition of animacy informa-
tion seems promising given its relevance to
varied linguistic phenomena. The suggestion
is based on the theoretical and computational
analysis of the property of animacy in the con-
text of anaphora resolution, syntactic parsing,
verb classification and argument differentia-
tion.
1 Introduction
Animacy can either be viewed as a biological prop-
erty or a grammatical category of nouns. In a
strictly biological sense, all living entities are ani-
mate, while all other entities are seen as inanimate.
However, in its linguistic sense, the term is syn-
onymous with a referent?s ability to act or instigate
events volitionally (Kittila? et al, 2011). Although
seemingly different, linguistic animacy can be im-
plied from biological animacy. In linguistics, the
manifestation of animacy and its relevance to lin-
guistic phenomena have been studied quite exten-
sively. Animacy has been shown, cross linguisti-
cally, to control a number of linguistic phenomena.
Case marking, argument realization, topicality or
discourse salience are some phenomena, highly cor-
related with the property of animacy (Aissen, 2003).
In linguistic theory, however, animacy is not seen
as a dichotomous variable, rather a range capturing
finer distinctions of linguistic relevance. Animacy
hierarchy proposed in Silverstein?s influential arti-
cle on ?animacy hierarchy? (Silverstein, 1986) ranks
nominals on a scale of the following gradience: 1st
pers> 2nd pers> 3rd anim> 3rd inanim. Several such
hierarchies of animacy have been proposed follow-
ing (Silverstein, 1986), one basic scale taken from
(Aissen, 2003) makes a three-way distinction as hu-
mans > animates > inanimates. These hierarchies can
be said to be based on the likelihood of a referent of
a nominal to act as an agent in an event (Kittila? et
al., 2011). Thus higher a nominal on these hierar-
chies higher the degree of agency/control it has over
an action. In morphologically rich languages, the
degree of control/agency is expressed by case mark-
ing. Case markers capture the degree of control a
nominal has in a given context (Hopper and Thomp-
son, 1980; Butt, 2006). They rank nominals on the
continuum of control as shown in (1)1. Nominals
marked with Ergative case have highest control and
the ones marked with Locative have lowest.
Erg > Gen > Inst > Dat > Acc > Loc (1)
Of late the systematic correspondences between
animacy and linguistic phenomena have been ex-
plored for various NLP applications. It has been
noted that animacy provides important informa-
tion, to mention a few, for anaphora resolution
(Evans and Orasan, 2000), argument disambiguation
(Dell?Orletta et al, 2005), syntactic parsing (?vre-
lid and Nivre, 2007; Bharati et al, 2008; Ambati et
al., 2009) and verb classification (Merlo and Steven-
1Ergative, Genitive, Instrumental, Dative, Accusative and
Locative in the given order.
159
son, 2001). Despite the fact that animacy could play
an important role in NLP applications, its annota-
tion, however, is not usually featured in a treebank
or any other annotated corpora used for developing
these applications. There are a very few annotation
projects that have included animacy in their anno-
tation manual, following its strong theoretical and
computational implications. One such work, mo-
tivated by the theoretical significance of the prop-
erty of animacy, is (Zaenen et al, 2004). They
make use of a coding scheme drafted for a para-
phrase project (Bresnan et al, 2002) and present
an explicit annotation scheme for animacy in En-
glish. The annotation scheme assumes a three-way
distinction, distinguishing Human, Other animates
and Inanimates. Among the latter two categories
?Other animates? is further sub-categorized into
Organizations and Animals, while the category of
?Inanimates? further distinguishes between con-
crete and non-concrete, and time and place nomi-
nals. As per the annotation scheme, nominals are
annotated according to the animacy of their referents
in a given context. Another annotation work that
includes animacy for nominals is (Teleman, 1974),
however, the distinction made is binary between hu-
man and non-human referents of a nominal in a
given context. In a recent work on animacy annota-
tion, Thuilier et al (2012) have annotated a multi-
source French corpora with animacy and verb se-
mantics, on the lines of (Zaenen et al, 2004). Apart
from the manual annotation for animacy, lexical re-
sources like wordnets are an important source of this
information, if available. These resources usually
cover animacy, though indirectly (Fellbaum, 2010;
Narayan et al, 2002). Although a wordnet is an
easily accessible resource for animacy information,
there are some limitations on its use, as discussed
below:
1. Coverage: Hindi wordnet only treats common
nouns while proper nouns are excluded (except
famous names) see Table 1. The problem is se-
vere where the domain of text includes more
proper than common nouns, which is the case
with the Hindi Treebank as it is annotated on
newspaper articles.
2. Ambiguity: Since words can be ambiguous, the
animacy listed in wordnet can only be used in
presence of a high performance word sense dis-
ambiguation system. As shown in Table 2, only
38.02% of nouns have a single sense as listed in
Hindi Wordnet.
3. Metonymy or Complex Types: Domains like
newspaper articles are filled with metonymic
expressions like courts, institute names, coun-
try names etc, that can refer to a building, a ge-
ographical place or a group of people depend-
ing on the context of use. These words are not
ambiguous per se but show different aspects of
their semantics in different contexts (logically
polysemous). Hindi wordnet treats these types
of nouns as inanimate.
Nominals in HTB Hindi WordNet Coverage
78,136 65,064 83.27%
Table 1: Coverage of Hindi WordNet on HTB Nominals.
HTB Nominals Single Unique Sense
with WN Semantics in Hindi WordNet
65,064 24,741 (38.02%)
Table 2: Nominals in HTB with multiple senses
Given these drawbacks, we have included ani-
macy information manually in the annotation of the
Hindi Treebank, as discussed in this work. In the
rest, we will discuss the annotation of nominal ex-
pressions with animacy and the motivation for the
same, the discussion will follow as: Section 2 gives
a brief overview of the Hindi Treebank with all its
layers. Section 3 motivates the annotation of nom-
inals with animacy, followed by the annotation ef-
forts and issues encountered in Section 4. Section
5 concludes the paper with a discussion on possible
future directions.
2 Description of the Hindi Treebank
In the following, we give an overview of the Hindi
Treebank (HTB), focusing mainly on its dependency
layer. The Hindi-Urdu Treebank (Palmer et al,
2009; Bhatt et al, 2009) is a multi-layered and
multi-representational treebank. It includes three
levels of annotation, namely two syntactic levels and
one lexical-semantic level. One syntactic level is a
dependency layer which follows the CPG (Begum
160
et al, 2008), inspired by the Pa?n. inian grammati-
cal theory of Sanskrit. The other level is annotated
with phrase structure inspired by the Chomskyan ap-
proach to syntax (Chomsky, 1981) and follows a bi-
nary branching representation. The third layer of an-
notation, a purely lexical semantic one, encodes the
semantic relations following the English PropBank
(Palmer et al, 2005).
In the dependency annotation, relations are
mainly verb-centric. The relation that holds between
a verb and its arguments is called a kar.aka relation.
Besides kar.aka relations, dependency relations also
exist between nouns (genitives), between nouns and
their modifiers (adjectival modification, relativiza-
tion), between verbs and their modifiers (adver-
bial modification including subordination). CPG
provides an essentially syntactico-semantic depen-
dency annotation, incorporating kar.aka (e.g., agent,
theme, etc.), non-kar.aka (e.g. possession, purpose)
and other (part of) relations. A complete tag set of
dependency relations based on CPG can be found in
(Bharati et al, 2009), the ones starting with ?k? are
largely Pa?n. inian kar.aka relations, and are assigned
to the arguments of a verb. Figure 1 encodes the de-
pendency structure of (5), the preterminal node is a
part of speech of a lexical item (e.g. NN,VM, PSP).
The lexical items with their part of speech tags are
further grouped into constituents called chunks (e.g.
NP, VGF) as part of the sentence analysis. The de-
pendencies are attached at the chunk level, marked
with ?drel? in the SSF format. k1 is the agent of
an action (KAyA ?eat?), whereas k2 is the object or
patient.
(5) s\@yA n
Sandhya-Erg
sb
apple-Nom
KAyA
eat-Perf
?
?Sandhya ate an apple.?
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k1:VGF?>
1.1 s\@yA NNP<fs af=?s\@yA,n,f,sg,3,o,0,0?>
1.2 n PSP <fs af=?n,psp,,,,,,?>
))
2 (( NP <fs name=?NP2? drel=?k2:VGF?>
2.1 sb NN <fs af=?sb,n,m,sg,3,d,0,0?>
))
3 (( VGF<fs name=?VGF?>
3.1 KAyA VM <fs af=?KA,v,m,sg,any,,yA,yA?>
))
</Sentence>
Figure 1: Annotation of an Example Sentence in SSF.
Despite the fact that the Hindi Treebank already
features a number of layers as discussed above, there
have been different proposals to enrich it further.
Hautli et al (2012) proposed an additional layer to
the treebank, for the deep analysis of the language,
by incorporating the functional structure (or
f-structure) of Lexical Functional Grammar which
encodes traditional syntactic notions such as sub-
ject, object, complement and adjunct. Dakwale et
al. (2012) have also extended the treebank with
anaphoric relations, with a motive to develop a data
driven anaphora resolution system for Hindi. Given
this scenario, our effort is to enrich the treebank
with the animacy annotation. In the following
sections, we will discuss in detail, the annotation of
the animacy property of nominals in the treebank
and the motive for the same.
3 Motivation: In the Context of
Dependency Parsing
Hindi is a morphologically rich language, gram-
matical relations are depicted by its morphology
via case clitics. Hindi has a morphologically
split-ergative case marking system (Mahajan, 1990;
Dixon, 1994). Case marking is dependent on the
aspect of a verb (progressive/perfective), transitivity
(transitive/intransitive) and the type of a nominal
(definite/indefinite, animate/inanimate). Given
this peculiar behavior of case marking in Hindi,
arguments of a verb (e.g. transitive) have a number
of possible configurations with respect to the case
marking as shown in the statistics drawn from
the Hindi Treebank released for MTPIL Hindi
Dependency parsing shared task (Sharma et al,
2012) in Table 3. Almost in 15% of the transitive
clauses, there is no morphological case marker on
any of the arguments of a verb which, in the context
of data driven parsing, means lack of an explicit
cue for machine learning. Although, in other cases
there is a case marker, at least on one argument of a
verb, the ambiguity in case markers (one-to-many
mapping between case markers and grammatical
functions as presented in Table 4) further worsens
the situation (however, see Ambati et al (2010) and
Bhat et al (2012) for the impact of case markers on
parsing Hindi/Urdu). Consider the examples from
161
(6a-e), the instrumental se is extremely ambiguous.
It can mark the instrumental adjuncts as in (6a),
source expressions as in (6b), material as in (6c),
comitatives as in (6d), and causes as in (6e).
K2-Unmarked K2-Marked
K1-Unmarked 1276 741
K1-Marked 5373 966
Table 3: Co-occurrence of Marked and Unmarked verb argu-
ments (core) in HTB.
n/ne ko/ko s/se m\/meN pr/par kA/kaa
(Ergative) (Dative) (Instrumental) (Locative) (Locative) (Genitive)
k1(agent) 7222 575 21 11 3 612
k2(patient) 0 3448 451 8 24 39
k3(instrument) 0 0 347 0 0 1
k4(recipient) 0 1851 351 0 1 4
k4a(experiencer) 0 420 8 0 0 2
k5(source) 0 2 1176 12 1 0
k7(location) 0 1140 308 8707 3116 19
r6(possession) 0 3 1 0 0 2251
Table 4 : Distribution of case markers across case function.
(6a) mohn n
Mohan-Erg
cAbF s
key-Inst
taAlA
lock-Nom
KolA
open
?
?Mohan opened the lock with a key.?
(6b) gFtaA n
Geeta-Erg
Ed?F s
Delhi-Inst
sAmAn
luggage-Nom
m\gvAyA
procure
?
?Geeta procured the luggage from Delhi.?
(6c) m EtakAr n
sculptor-Erg
p(Tr s
stone-Inst
m Eta
idol-Nom
bnAyF
make
?
?The sculptor made an idol out of stone.?
(6d) rAm kF
Ram-Gen
[yAm s
Shyaam-Inst
bAta
talk-Nom
h  I
happen
?
?Ram spoke to Shyaam.?
(6e) bAErf s
rain-Inst
kI Psl\
many crops-Nom
tabAh
destroy
ho gyF\
happen-Perf
?
?Many crops were destroyed due to the rain.?
(7) EcEwyA
bird-Nom
dAnA
grain-Nom
c  g rhF h{
devour-Prog
?
?A bird is devouring grain.?
A conventional parser has no cue for the disam-
biguation of instrumental case marker se in exam-
ples (6a-e) and similarly, in example (7), it?s hard
for the parser to know whether ?bird? or ?grain? is
the agent of the action ?devour?. Traditionally, syn-
tactic parsing has largely been limited to the use
of only a few lexical features. Features like POS-
tags are way too coarser to provide deep informa-
tion valuable for syntactic parsing while on the other
hand lexical items often suffer from lexical ambi-
guity or out of vocabulary problem. So in oder to
assist the parser for better judgments, we need to
complement the morphology somehow. A careful
observation easily states that a simple world knowl-
edge about the nature (e.g. living-nonliving, arti-
fact, place) of the participants is enough to disam-
biguate. For Swedish, ?vrelid and Nivre (2007) and
?vrelid (2009) have shown improvement, with an-
imacy information, in differentiation of core argu-
ments of a verb in dependency parsing. Similarly
for Hindi, Bharati et al (2008) and Ambati et al
(2009) have shown that even when the training data
is small simple animacy information can boost de-
pendency parsing accuracies, particularly handling
the differentiation of core arguments. In Table 5,
we show the distribution of animacy with respect to
case markers and dependency relations in the anno-
tated portion of the Hindi Treebank. The high rate
of co-occurrence between animacy and dependency
relations makes a clear statement about the role an-
imacy can play in parsing. Nominals marked with
dependency relations as k1 ?agent?, k4 ?recipient?,
k4a ?experiencer? are largely annotated as human
while k3 ?instrument? is marked as inanimate,
which confirms our conjecture that with animacy
information a parser can reliably predict linguistic
patterns. Apart from parsing, animacy has been re-
ported to be beneficial for a number of natural lan-
guage applications (Evans and Orasan, 2000; Merlo
and Stevenson, 2001). Following these computa-
tional implications of animacy, we started encoded
this property of nominals explicitly in our treebank.
In the next section, we will present these efforts fol-
162
lowed by the inter-annotator agreement studies.
Human Other-Animates Inanimate
k1
n/ne (Erg) 2321 630 108
ko/ko (Dat/Acc) 172 8 135
s/se (Inst) 6 0 14
m\/me (Loc) 0 0 7
pr/par (Loc) 0 0 1
kA/kaa (Gen) 135 2 99
? (Nom) 1052 5 3072
k2
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 625 200 226
s/se (Inst) 67 0 88
m\/me (Loc) 2 0 6
pr/par (Loc) 5 0 37
kA/kaa (Gen) 15 0 14
? (Nom) 107 61 2998
k3
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 2 0 199
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 0 0 20
k4
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 597 0 13
s/se (Inst) 53 0 56
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 7 0 8
k4a
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 132 0 8
s/se (Inst) 4 0 2
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 1 0 0
? (Nom) 56 0 1
k5
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 7 0 460
m\/me (Loc) 0 0 1
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 0 0 2
k7
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 4 0 0
s/se (Inst) 3 0 129
m\/me (Loc) 0 1977 1563
pr/par (Loc) 66 0 1083
kA/kaa (Gen) 0 0 8
? (Nom) 5 0 1775
r6
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 1 0 0
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 156 80 605
? (Nom) 13 3 25
Table 5: Distribution of semantic features with respect
to case markers and dependency relations a.
ak1 ?agent?, k2 ?patient?, k3 ?instrument?, k4 ?recipient?,
k4a ?experiencer?, k5 ?source?, k7 ?location?, r6 ?possession?
4 Animacy Annotation
Following Zaenen et al (2004), we make a three-
way distinction, distinguishing between Human,
Other Animate and In-animate referents of a
nominal in a given context. The animacy of a ref-
erent is decided based on its sentience and/or con-
trol/volitionality in a particular context. Since, pro-
totypically, agents tend to be animate and patients
tend to be inanimate (Comrie, 1989), higher ani-
mates such as humans, dogs etc. are annotated as
such in all contexts since they frequently tend to be
seen in contexts of high control. However, lower
animates such as insects, plants etc. are anno-
tated as ?In-animate? because they are ascribed
less or no control in human languages like inan-
imates (Kittila? et al, 2011). Non-sentient refer-
ents, except intelligent machines and vehicles, are
annotated as ?In-animate? in all contexts. Intel-
ligent machines like robots and vehicles, although,
lack any sentience, they possess an animal like be-
havior which separates them from inanimate nouns
with no animal resemblance, reflected in human lan-
guage as control/volitionality. These nouns unlike
humans and other higher animates are annotated as
per the context they are used in. They are anno-
tated as ?Other animate? only in their agentive
roles. Nominals that vary in sentience in varying
contexts are annotated based on their reference in a
given context as discussed in Subsection 4.2. These
nominals include country names referring to geo-
graphical places, teams playing for the country, gov-
ernments or their inhabitants; and organizations in-
cluding courts, colleges, schools, banks etc. Un-
like Zaenen et al (2004) we don?t further categorize
?Other Animate? and ?In-animate? classes. We
163
don?t distinguish between Organizations and Ani-
mals in ?Other Animate? and Time and Place in
?In-animates?.
The process of animacy annotation in the Hindi
Treebank is straight forward. For every chunk in a
sentence, the animacy of its head word is captured
in an ?attribute-value? pair in SSF format, as
shown in Figure 3. Hitherto, around 6485 sentence,
of the Hindi Treebank, have been annotated with
the animacy information.
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k1:VGF?
semprop=?human?>
1.1 mohn NNP <fs af=?mohn,n,m,sg,3,d,0,0?>
1.2 n PSP <fs af=?n,psp,,,,,,? name=?n?>
))
2 (( NP <fs name=?NP2? drel=?k4:VGF?
semprop=?other-animate?>
2.1 Eb?F NN <fs af=?Eb?F,n,f,sg,3,d,0,0?>
2.2 ko PSP <fs af=?ko,psp,,,,,,? name=?ko?>
))
3 (( NP <fs name=?NP3? drel=?k3:VGF?
semprop=?inanimate?>
3.1 botal NN <fs af=?botal ,n,f,sg,3,d,0,0?>
3.2 s PSP <fs af=?s,psp,,,,,,?>
))
4 (( NP <fs name=?NP4? drel=?k2:VGF?
semprop=?inanimate?>
4.1 d D NN <fs af=?d D,n,m,sg,3,d,0,0?>
))
5 (( VGF <fs name=?VGF?>
5.1 EplAyA VM <fs af=?EplA,v,m,sg,any,,yA,yA?>
))
</Sentence>
Figure 3: Semantic Annotation in SSF.
(8) mohn n
Mohan-Erg
Eb?F ko
cat-Dat
botal s
bottle-Inst
d D
milk-Nom
EplAyA
drink-Perf
?
?Mohan fed milk to the cat with a bottle.?
In the following, we discuss some of the interest-
ing cross linguistic phenomena which added some
challenge to the annotation.
4.1 Personification
Personification is a type of meaning extension
whereby an entity (usually non-human) is given
human qualities. Personified expressions are an-
notated, in our annotation procedure, as Human,
since it is the sense they carry in such contexts.
However, to retain their literal sense, two attributes
are added. One for their context bound sense
(metaphorical) and the other for context free sense
(literal). In example (9), waves is annotated with
literal animacy as In-animante and metaphoric
animacy as Human, as shown in Figure 4 (offset
2).
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k7p:VGF? >
1.1 sAgr NNC <fs af=?sAgr,n,m,sg,3,d,0,0?>
1.2 taV NN <fs af=?taV,n,m,sg,3,d,0,0?>
1.3 pr PSP <fs af=?pr,psp,,,,,,?>
))
2 (( NP <fs name=?NP2? drel=?k1:VGF?
semprop=?inanimate?
metaphoric=?human?>
2.1 lhr\ NN <fs af=?lhr\,n,f,pl,3,d,0,0?>
))
3 (( VGF <fs name=?VGF?>
3.1 nAc VM <fs af=?nAc,v,any,any,any,,0,0?>
3.2 rhF VAUX <fs af=?rhF,v,f,sg,any,,ya,ya?>
3.3 h{\ AUX <sf AF=?h{\,v,any,pl,1,,he,he?>
))
</Sentence>
Figure 4: Semantic Annotation in SSF.
(9) sAgr taV pr
sea coast-Loc
lhr\
waves-Nom
nAc rhF h{\
dance-Prog
?
?Waves are dancing on the sea shore.?
4.2 Complex Types
The Hindi Treebank in largely built on newspa-
per corpus. Logically polysemous expressions
(metonymies) such as government, court,
newspaper etc. are very frequent in news re-
porting. These polysemous nominals can exhibit
contradictory semantics in different contexts. In
example (10a), court refers to a person (judge) or
a group of persons (jury) while in (10b) it is a
building (see Pustejovsky (1996) for the semantics
of complex types). In our annotation procedure,
such expressions are annotated as per the sense or
reference they carry in a given context. So, in case
of (10a) court will be annotated as Human while
in (10b) it will be annotated as In-animante.
(10a) adAlta n
court-Erg
m  kdm kA
case-Gen
P{\slA
decision-Nom
s  nAyA
declare-Perf
?
?The court declared its decision on the case.?
164
(10b) m{\
I-Nom
adAlta m\
court-Loc
h ?
be-Prs
?I am in the court.?
4.3 Inter-Annotator Agreement
We measured the inter-annotator agreement on a
set of 358 nominals (?50 sentences) using Cohen?s
kappa. We had three annotators annotating the same
data set separately. The nominals were annotated
in context i.e., the annotation was carried consider-
ing the role and reference of a nominal in a partic-
ular sentence. The kappa statistics, as presented in
Table 6, show a significant understanding of anno-
tators of the property of animacy. In Table 7, we
report the confusion between the annotators on the
three animacy categories. The confusion is high for
?Inanimate? class. Annotators don?t agree on this
category because of its fuzziness. As discussed ear-
lier, although ?Inanimate? class enlists biologically
inanimate entities, some entities may behave like an-
imates in some contexts. They may be sentient and
have high linguistic control in some contexts. The
difficulty in deciphering the exact nature of the ref-
erence of these nominals, as observed, is the reason
behind the confusion. The confusion is observed for
nouns like organization names, lower animates and
vehicles. Apart from the linguistically and contextu-
ally defined animacy, there was no confusion, as ex-
pected, in the understanding of biological animacy.
Annotators ?
ann1-ann2 0.78
ann1-ann3 0.82
ann2-ann3 0.83
Average ? 0.811
Table 6: Kappa Statistics
Human Other-animate Inanimate
Human 71 0 14
Other-animate 0 9 5
Inanimate 8 10 241
Table 7: Confusion Matrix
5 Conclusion and Future Work
In this work, we have presented our efforts to enrich
the nominals in the Hindi Treebank with animacy
information. The annotation was followed by the
inter-annotator agreement study for evaluating the
confusion over the categories chosen for annotation.
The annotators have a significant understanding of
the property of animacy as shown by the higher val-
ues of Kappa (?). In future, we plan to continue the
animacy annotation for the whole Hindi Treebank.
We also plan to utilize the annotated data to build
a data driven automatic animacy classifier (?vrelid,
2006). From a linguistic perspective, an annotation
of the type, as discussed in this paper, will also be of
great interest for studying information dynamics and
see how semantics interacts with syntax in Hindi.
6 Acknowledgments
The work reported in this paper is supported by the
NSF grant (Award Number: CNS 0751202; CFDA
Number: 47.070). 2
References
Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language & Linguis-
tic Theory, 21(3):435?483.
B.R. Ambati, P. Gade, S. Husain, and GSK Chaitanya.
2009. Effect of minimal semantics on dependency
parsing. In Proceedings of the Student Research Work-
shop.
B.R. Ambati, S. Husain, J. Nivre, and R. Sangal. 2010.
On the role of morphosyntactic features in Hindi de-
pendency parsing. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 94?102. As-
sociation for Computational Linguistics.
R. Begum, S. Husain, A. Dhwaj, D.M. Sharma, L. Bai,
and R. Sangal. 2008. Dependency annotation scheme
for Indian languages. In Proceedings of IJCNLP. Cite-
seer.
Akshar Bharati, Samar Husain, Bharat Ambati, Sambhav
Jain, Dipti Sharma, and Rajeev Sangal. 2008. Two se-
mantic features make all the difference in parsing ac-
curacy. Proceedings of ICON, 8.
2Any opinions, findings, and conclusions or recommenda-
tions expressed in this material are those of the author(s) and do
not necessarily reflect the views of the National Science Foun-
dation.
165
A. Bharati, D.M. Sharma, S. Husain, L. Bai, R. Begum,
and R. Sangal. 2009. AnnCorra: TreeBanks for Indian
Languages Guidelines for Annotating Hindi TreeBank
(version?2.0).
R.A. Bhat, S. Jain, and D.M. Sharma. 2012. Experi-
ments on Dependency Parsing of Urdu. In Proceed-
ings of TLT11 2012 Lisbon Portugal, pages 31?36.
Edic?es Colibri.
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M.
Sharma, and F. Xia. 2009. A multi-representational
and multi-layered treebank for hindi/urdu. In Pro-
ceedings of the Third Linguistic Annotation Workshop,
pages 186?189. Association for Computational Lin-
guistics.
Joan Bresnan, Jean Carletta, Richard Crouch, Malvina
Nissim, Mark Steedman, Tom Wasow, and Annie Za-
enen. 2002. Paraphrase analysis for improved genera-
tion, link project.
Miriam Butt. 2006. The dative-ergative connection. Em-
pirical issues in syntax and semantics, 6:69?92.
N. Chomsky. 1981. Lectures on Government and Bind-
ing. Dordrecht: Foris.
Bernard Comrie. 1989. Language universals and lin-
guistic typology: Syntax and morphology. University
of Chicago press.
Praveen Dakwale, Himanshu Sharma, and Dipti M
Sharma. 2012. Anaphora Annotation in Hindi Depen-
dency TreeBank. In Proceedings of the 26th Pacific
Asia Conference on Language, Information, and Com-
putation, pages 391?400, Bali,Indonesia, November.
Faculty of Computer Science, Universitas Indonesia.
Felice Dell?Orletta, Alessandro Lenci, Simonetta Mon-
temagni, and Vito Pirrelli. 2005. Climbing the
path to grammar: A maximum entropy model of sub-
ject/object learning. In Proceedings of the Workshop
on Psychocomputational Models of Human Language
Acquisition, pages 72?81. Association for Computa-
tional Linguistics.
R.M.W. Dixon. 1994. Ergativity. Number 69. Cam-
bridge University Press.
Richard Evans and Constantin Orasan. 2000. Improv-
ing anaphora resolution by identifying animate entities
in texts. In Proceedings of the Discourse Anaphora
and Reference Resolution Conference (DAARC2000),
pages 154?162.
Christiane Fellbaum. 2010. WordNet. Springer.
A. Hautli, S. Sulger, and M. Butt. 2012. Adding an an-
notation layer to the Hindi/Urdu treebank. Linguistic
Issues in Language Technology, 7(1).
Paul J Hopper and Sandra A Thompson. 1980. Tran-
sitivity in grammar and discourse. Language, pages
251?299.
Seppo Kittila?, Katja Va?sti, and Jussi Ylikoski. 2011.
Case, Animacy and Semantic Roles, volume 99. John
Benjamins Publishing.
A.K. Mahajan. 1990. The A/A-bar distinction and move-
ment theory. Ph.D. thesis, Massachusetts Institute of
Technology.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb classification based on statistical distribu-
tions of argument structure. Computational Linguis-
tics, 27(3):373?408.
Dipak Narayan, Debasri Chakrabarty, Prabhakar Pande,
and Pushpak Bhattacharyya. 2002. An experience in
building the indo wordnet-a wordnet for hindi. In First
International Conference on Global WordNet, Mysore,
India.
Lilja ?vrelid and Joakim Nivre. 2007. When word or-
der and part-of-speech tags are not enough ? Swedish
dependency parsing with rich linguistic features. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP),
pages 447?451.
Lilja ?vrelid. 2006. Towards robust animacy classifica-
tion using morphosyntactic distributional features. In
Proceedings of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Student Research Workshop, pages 47?
54. Association for Computational Linguistics.
Lilja ?vrelid. 2009. Empirical evaluations of animacy
annotation. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. volume 31, pages 71?106. MIT Press.
M. Palmer, R. Bhatt, B. Narasimhan, O. Rambow, D.M.
Sharma, and F. Xia. 2009. Hindi Syntax: Annotat-
ing Dependency, Lexical Predicate-Argument Struc-
ture, and Phrase Structure. In The 7th International
Conference on Natural Language Processing, pages
14?17.
J. Pustejovsky. 1996. The Semantics of Complex Types.
Lingua.
Dipti Misra Sharma, Prashanth Mannem, Joseph van-
Genabith, Sobha Lalitha Devi, Radhika Mamidi, and
Ranjani Parthasarathi, editors. 2012. Proceedings of
the Workshop on Machine Translation and Parsing in
Indian Languages. The COLING 2012 Organizing
Committee, Mumbai, India, December.
Michael Silverstein. 1986. Hierarchy of features and
ergativity. Features and projections, pages 163?232.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
166
Juliette Thuilier, Laurence Danlos, et al 2012. Seman-
tic annotation of French corpora: animacy and verb
semantic classes. In LREC 2012-The eighth interna-
tional conference on Language Resources and Evalu-
ation.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M Catherine O?Connor, and Tom Wasow. 2004. Ani-
macy Encoding in English: why and how. In Proceed-
ings of the 2004 ACL Workshop on Discourse Anno-
tation, pages 118?125. Association for Computational
Linguistics.
167
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 51?56,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Reducing the Impact of Data Sparsity in Statistical Machine Translation
Karan Singla
1
, Kunal Sachdeva
1
, Diksha Yadav
1
, Srinivas Bangalore
2
, Dipti Misra Sharma
1
1
LTRC IIIT Hyderabad,
2
AT&T Labs-Research
Abstract
Morphologically rich languages generally
require large amounts of parallel data to
adequately estimate parameters in a statis-
tical Machine Translation(SMT) system.
However, it is time consuming and expen-
sive to create large collections of parallel
data. In this paper, we explore two strate-
gies for circumventing sparsity caused by
lack of large parallel corpora. First, we ex-
plore the use of distributed representations
in an Recurrent Neural Network based lan-
guage model with different morphological
features and second, we explore the use of
lexical resources such as WordNet to over-
come sparsity of content words.
1 Introduction
Statistical machine translation (SMT) models es-
timate parameters (lexical models, and distortion
model) from parallel corpora. The reliability of
these parameter estimates is dependent on the size
of the corpora. In morphologically rich languages,
this sparsity is compounded further due to lack of
large parallel corpora.
In this paper, we present two approaches that
address the issue of sparsity in SMT models for
morphologically rich languages. First, we use an
Recurrent Neural Network (RNN) based language
model (LM) to re-rank the output of a phrase-
based SMT (PB-SMT) system and second we use
lexical resources such as WordNet to minimize the
impact of Out-of-Vocabulary(OOV) words on MT
quality. We further improve the accuracy of MT
using a model combination approach.
The rest of the paper is organized as follows.
We first present our approach of training the base-
line model and source side reordering. In Section
4, we present our experiments and results on re-
ranking the MT output using RNNLM. In Section
5, we discuss our approach to increase the cover-
age of the model by using synset ID?s from the
English WordNet (EWN). Section 6 describes our
experiments on combining the model with synset
ID?s and baseline model to further improve the
translation accuracy followed by results and obser-
vations sections.We conclude the paper with future
work and conclusions.
2 Related Work
In this paper, we present our efforts of re-
ranking the n-best hypotheses produced by a PB-
MT (Phrase-Based MT) system using RNNLM
(Mikolov et al., 2010) in the context of an English-
Hindi SMT system. The re-ranking task in ma-
chine translation can be defined as re-scoring the
n-best list of translations, wherein a number of
language models are deployed along with fea-
tures of source or target language. (Dungarwal
et al., 2014) described the benefits of re-ranking
the translation hypothesis using simple n-gram
based language model. In recent years, the use
of RNNLM have shown significant improvements
over the traditional n-gram models (Sundermeyer
et al., 2013). (Mikolov et al., 2010) and (Liu et
al., 2014) have shown significant improvements in
speech recognition accuracy using RNNLM . Shi
(2012) also showed the benefits of using RNNLM
with contextual and linguistic features. We have
also explored the use of morphological features
(Hindi being a morphologically rich language) in
RNNLM and deduced that these features further
improve the baseline RNNLM in re-ranking the n-
best hypothesis.
Words in natural languages are richly diverse
so it is not possible to cover all source language
words when training an MT system. Untranslated
out-of-vocabulary (OOV) words tend to degrade
the accuracy of the output produced by an MT
model. Huang (2010) pointed to various types
of OOV words which occur in a data set ? seg-
51
mentation error in source language, named enti-
ties, combination forms (e.g. widebody) and ab-
breviations. Apart from these issues, Hindi being
a low-resourced language in terms of parallel cor-
pora suffers from data sparsity.
In the second part of the paper, we address the
problem of data sparsity with the help of English
WordNet (EWN) for English-Hindi PB-SMT. We
increase the coverage of content words (exclud-
ing Named-Entities) by incorporating sysnset in-
formation in the source sentences.
Combining Machine Translation (MT) systems
has become an important part of statistical MT in
past few years. Works by (Razmara and Sarkar,
2013; Cohn and Lapata, 2007) have shown that
there is an increase in phrase coverage when com-
bining different systems. To get more coverage of
unigrams in phrase-table, we have explored sys-
tem combination approaches to combine models
trained with synset information and without synset
information. We have explored two methodolo-
gies for system combination based on confusion
matrix(dynamic) (Ghannay et al., 2014) and mix-
ing models (Cohn and Lapata, 2007).
3 Baseline Components
3.1 Baseline Model and Corpus Statistics
We have used the ILCI corpora (Choudhary and
Jha, 2011) for our experiments, which contains
English-Hindi parallel sentences from tourism and
health domain. We randomly divided the data into
training (48970), development (500) and testing
(500) sentences and for language modelling we
used news corpus of English which is distributed
as a part of WMT?14 translation task. The data is
about 3 million sentences which also contains MT
training data.
We trained a phrase based (Koehn et al., 2003)
MT system using the Moses toolkit with word-
alignments extracted from GIZA++ (Och and Ney,
2000). We have used the SRILM (Stolcke and
others, 2002) with Kneser-Ney smoothing (Kneser
and Ney, 1995) for training a language model for
the first stage of decoding. The result of this base-
line system is shown in Table 1.
3.2 English Transformation Module
Hindi is a relatively free-word order language and
generally tends to follow SOV (Subject-Object-
Verb) order and English tends to follow SVO
(Subject-Verb-Object) word order. Research has
Number of Number of Number of
Training Development Evaluation BLEU
Sentences Sentences Sentences
48970 500 500 20.04
Table 1: Baseline Scores for Phrase-based Moses
Model
shown that pre-ordering source language to con-
form to target language word order significantly
improves translation quality (Collins et al., 2005).
We created a re-ordering module for transform-
ing an English sentence to be in the Hindi order
based on reordering rules provided by Anusaaraka
(Chaudhury et al., 2010). The reordering rules are
based on parse output produced by the Stanford
Parser (Klein and Manning, 2003).
The transformation module requires the text to
contain only surface form of words, however, we
extended it to support surface form along with its
factors such as lemma and Part of Speech (POS).
Input : the girl in blue shirt is my sister
Output : in blue shirt the girl is my sister.
Hindi : neele shirt waali ladki meri bahen hai (
blue) ( shirt) (Mod)(girl)(my)(sister)(Vaux)
With this transformation, the English sentence
is structurally closer to the Hindi sentence which
leads to better phrase alignments. The model
trained with the transformed corpus produces a
new baseline score of 21.84 BLEU score an
improvement over the earlier baseline of 20.04
BLEU points.
4 Re-Ranking Experiments
In this section, we describe the results of re-
ranking the output of the translation model us-
ing Recurrent Neural Networks (RNN) based lan-
guage models using the same data which is used
for language modelling in the baseline models.
Unlike traditional n-gram based discrete lan-
guage models, RNN do not make the Markov as-
sumption and potentially can take into account
long-term dependencies between words. Since the
words in RNNs are represented as continuous val-
ued vectors in low dimensions allowing for the
possibility of smoothing using syntactic and se-
mantic features. In practice, however, learning
long-term dependencies with gradient descent is
difficult as described by (Bengio et al., 1994) due
to diminishing gradients.
We have integrated the approach of re-scoring
52
100 200 300 400 500
22
24
26
28
30
Number of Hypotheses
B
L
E
U
s
c
o
r
e
s
Baseline
POS
NONE
Lemma
Oracle
All
Figure 1: BLEU Scores for Re-ranking experi-
ments with RNNLM using different feature com-
binations.
n-best output using RNNLM which has also been
shown to be helpful by (Liu et al., 2014). Shi
(2012) also showed the benefits of using RNNLM
with contextual and linguistic features. Follow-
ing their work, we used three type of features for
building an RNNLM for Hindi : lemma (root),
POS, NC (number-case). The data used was a
Wikipedia dump, MT training data, news arti-
cles which had approximately 500,000 Hindi sen-
tences. Features were extracted using paradigm-
based Hindi Morphological Analyzer
1
Figure 1 illustrates the results of re-ranking per-
formed using RNNLM trained with various fea-
tures. The Oracle score is the highest achievable
score in a re-ranking experiment. This score is
computed based on the best translation out of n-
best translations. The best translation is found us-
ing the cosine similarity between the hypothesis
and the reference translation. It can be seen from
Figure 1, that the LM with only word and POS in-
formation is inferior to all other models. However,
morphological features like lemma, number and
case information help in re-ranking the hypothesis
significantly. The RNNLM which uses all the fea-
tures performed the best for the re-ranking exper-
iments achieving a BLEU score of 26.91, after re-
scoring 500-best obtained from the pre-order SMT
model.
1
We have used the HCU morph-analyzer.
System BLEU
Baseline 21.84
Rescoring 500-best with RNNLM
Features
NONE 25.77
POS 24.36
Lemma(root) 26.32
ALL(POS+Lemma+NC) 26.91
Table 2: Rescoring results of 500-best hypotheses
using RNNLM with different features
5 Using WordNet to Reduce Data
Sparsity
We extend the coverage of our source data by us-
ing synonyms from the English WordNet (EWN).
Our main motivation is to reduce the impact of
OOV words on output quality by replacing words
in a source sentence with their corresponding
synset IDs. However, choosing the appropriate
synset ID based upon its context and morphologi-
cal information is important. For sense selection,
we followed the approach used by (Tammewar et
al., 2013), which is also described further in this
section in the context of our task. We ignored
words that are regarded as Named-Entities as in-
dicated by Stanford NER tagger, as they should
not have synonyms in any case.
5.1 Sense Selection
Words are ambiguous, independent of their sen-
tence context. To choose an appropriate sense ac-
cording to the context for a lexical item is a chal-
lenging task typically termed as word-sense dis-
ambiguation. However, the syntactic category of
a lexical item provides an initial cue for disam-
biguating a lexical item. Among the varied senses,
we filter out the senses that are not the same POS
tag as the lexical item. But words are not just am-
biguous across different syntactic categories but
are also ambiguous within a syntactic category. In
the following, we discuss our approaches to select
the sense of a lexical item best suited in a given
context within a given category. Also categories
were filtered so that only content words get re-
placed with synset IDs.
5.1.1 Intra-Category Sense Selection
First Sense: Among the different senses,we se-
lect the first sense listed in EWN corresponding to
the POS-tag of a given lexical item. The choice is
motivated by our observation that the senses of a
53
lexical item are ordered in the descending order of
their frequencies of usage in the lexical resource.
Merged Sense: In this approach, we merge all
the senses listed in EWN corresponding to the
POS-tag of the given lexical item. The motivation
behind this strategy is that the senses in the EWN
for a particular word-POS pair are too finely clas-
sified resulting in classification of words that may
represent the same concept, are classified into dif-
ferent synsets. For example : travel and go can
mean the same concept in a similar context but the
first sense given by EWN is different for these two
words. Therefore, we merge all the senses for a
word into a super sense ( synset ID of first word
occurred in data), which is given to all its syn-
onyms even if it occurs in different synset IDs.
5.2 Factored Model
Techniques such as factored modelling (Koehn
and Hoang, 2007) are quite beneficial for Trans-
lation from English to Hindi language as shown
by (Ramanathan et al., 2008). When we replace
words in a source sentence with the synset ID?as,
we tend to lose morphological information associ-
ated with that word. We add inflections as features
in a factored SMT model to minimize the impact
of this replacement.
We show the results of the processing steps on
an example sentence below.
Original Sentence : Ram is going to market to
buy apples
New Sentence : Ram is Synset(go.v.1)
to Synset(market.n.0) to Synset(buy.v.1)
Synset(apple.n.1)
Sentence with synset ID: Ram E is E
Synset(go.v.1) ing to E Synset(market.n.0) E
to E Synset(buy.v.1) E Synset(apple.n.1) s
Then English sentences were reordered to Hindi
word-order using the module discussed in Section
3.
Reordered Sentence: Ram E Synset(apple.n.1) s
Synset(buy.v.1) E to E Synset(market.n.0) E to E
Synset(go.v.1) ing is E
In Table 3, the second row shows the BLEU
scores for the models in which there are synset IDs
for the source side. It can be seen that the factored
model also shows significant improvement in the
results.
6 Combining MT Models
Combining Machine translation (MT) systems has
become an important part of Statistical MT in
the past few years. There are two dominant ap-
proaches. (1) a system combination approach
based on confusion networks (CN) (Rosti et al.,
2007), which can work dynamically in combin-
ing the systems. (2) Combine the models by lin-
early interpolating and then using MERT to tune
the combined system.
6.1 Combination based on confusion
networks
We used the tool MANY (Barrault, 2010) for sys-
tem combination. However, since the tool is con-
figured to work with TERp evaluation metric, we
modified it to use METEOR (Gupta et al., 2010)
metric since it has been shown by (Kalyani et al.,
2014), that METEOR evaluation metric is better
correlated to human evaluation for morphologi-
cally rich Indian Languages.
6.2 Linearly Interpolated Combination
In this approach, we combined phrase-tables of
the two models (Eng (sysnset) - Hindi and Base-
line) using linear interpolation. We combined the
two models with uniform weights ? 0.5 for each
model, in our case. We again tuned this model
with the new interpolated phrase-table using stan-
dard algorithm MERT.
7 Experiments and Results
As can be seen in Table 3, the model with synset
information led to reduction in OOV words. Even
though BLEU score decreased, but METEOR
score improved for all the experiments based on
using synset IDs in the source sentence, but it has
been shown by (Gupta et al., 2010) that METEOR
is a better evaluation metrics for morphologically
rich languages. Also, when synset ID?as are used
instead of words in the source language, the sys-
tem makes incorrect morphological choices. Ex-
ample : going and goes will be replaced by same
synset ID ?aSynset(go.v.1)?a, so this has lead to loss
of information in the phrase-table but METEOR
catches these complexities as it considers features
like stems, synonyms for its evaluation metrics
and hence showed better improvements compared
to BLEU metric. Last two rows of Table 3 show
results for combination experiments and Mixture
Model (linearly interpolated model) showed best
54
System #OOV words BLEU Meteor
Baseline 253 21.8 .492
Eng(Synset ID)-Hindi
Baseline 237 19.2 .494
*factor(inflections) 225 20.3 .506
Ensembled Decoding 213 21.0 .511
Mixture Model 210 21.2 .519
Table 3: Results for the model in which there were Synset ID?s instead of word in English data
results with significant reduction in OOV words
and also some gains in METEOR score.
8 Observations
In this section, we study the coverage of different
models by categorizing the OOV words into 5 cat-
egories.
? NE(Named Entities) : As the data was
from Health & Tourism domain, these words
were mainly the names of the places and
medicines.
? VB : types of verb forms
? NN : types of nouns and pronouns
? ADJ : all adjectives
? AD : adverbs
? OTH : there were some words which did not
mean anything in English
? SM : There were some occasional spelling
mistakes seen in the test data.
Note : There were no function words seen in the
OOV(un-translated) words
Cat. Baseline Eng(synset)-Hin MixtureModel
NE 120 121 115
VB 47 37 27
NN 76 60 47
ADJ 22 15 12
AD 5 5 4
OTH 2 2 2
SM 8 8 8
Table 4: OOV words in Different Models
As this analysis was done on a small dataset and
for a fixed domain, the OOV words were few in
number as it can be seen in Table 4. But the OOV
words across the different models reduced as ex-
pected. The NE words remained almost the same
for all the three models but OOV words from cate-
gory VB,NN,ADJ decreased for Eng(synset)-Hin
model and Mixture model significantly.
9 Future Work
In the future, we will work on using the two ap-
proaches discussed: Re-Ranking & using lexical
resources to reduce sparsity together in a system.
We will work on exploring syntax based features
for RNNLM and we are planning to use a better
method for sense selection and extending this con-
cept for more language pairs. Word-sense disam-
biguation can be used for choosing more appro-
priate sense when the translation model is trained
on a bigger data data set. Also we are looking for
unsupervised techniques to learn the replacements
for words to reduce sparsity and ways to adapt our
system to different domains.
10 Conclusions
In this paper, we have discussed two approaches
to address sparsity issues encountered in training
SMT models for morphologically rich languages
with limited amounts of parallel corpora. In the
first approach we used an RNNLM enriched with
morphological features of the target words and
show the BLEU score to improve by 5 points. In
the second approach we use lexical resource such
as WordNet to alleviate sparsity.
References
Lo??c Barrault. 2010. Many: Open source machine
translation system combination. The Prague Bul-
letin of Mathematical Linguistics, 93:147?155.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Sriram Chaudhury, Ankitha Rao, and Dipti M Sharma.
2010. Anusaaraka: An expert system based machine
translation system. In Natural Language Processing
55
and Knowledge Engineering (NLP-KE), 2010 Inter-
national Conference on, pages 1?6. IEEE.
Narayan Choudhary and Girish Nath Jha. 2011. Cre-
ating multilingual parallel corpora in indian lan-
guages. In Proceedings of Language and Technol-
ogy Conference.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Citeseer.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd annual
meeting on association for computational linguis-
tics, pages 531?540. Association for Computational
Linguistics.
Piyush Dungarwal, Rajen Chatterjee, Abhijit Mishra,
Anoop Kunchukuttan, Ritesh Shah, and Pushpak
Bhattacharyya. 2014. The iit bombay hindi-english
translation system at wmt 2014. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 90?96, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Sahar Ghannay, France Le Mans, and Lo?c Barrault.
2014. Using hypothesis selection based features for
confusion network mt system combination. In Pro-
ceedings of the 3rd Workshop on Hybrid Approaches
to Translation (HyTra)@ EACL, pages 1?5.
Ankush Gupta, Sriram Venkatapathy, and Rajeev San-
gal. 2010. Meteor-hindi: Automatic mt evaluation
metric for hindi as a target language. In Proceed-
ings of ICON-2010: 8th International Conference
on Natural Language Processing.
Chung-chi Huang, Ho-ching Yen, and Jason S Chang.
2010. Using sublexical translations to handle the
oov problem in mt. In Proceedings of The Ninth
Conference of the Association for Machine Transla-
tion in the Americas (AMTA).
Aditi Kalyani, Hemant Kamud, Sashi Pal Singh, and
Ajai Kumar. 2014. Assessing the quality of mt
systems for hindi to english translation. In In-
ternational Journal of Computer Applications, vol-
ume 89.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In EMNLP-CoNLL, pages 868?876.
Citeseer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
X Liu, Y Wang, X Chen, MJF Gales, and PC Wood-
land. 2014. Efficient lattice rescoring using recur-
rent neural network language models.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 440?447. Association for
Computational Linguistics.
Ananthakrishnan Ramanathan, Jayprasad Hegde,
Ritesh M Shah, Pushpak Bhattacharyya, and
M Sasikumar. 2008. Simple syntactic and morpho-
logical processing can help english-hindi statistical
machine translation. In IJCNLP, pages 513?520.
Majid Razmara and Anoop Sarkar. 2013. Ensemble
triangulation for statistical machine translation. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 252?
260.
Antti-Veikko I Rosti, Spyridon Matsoukas, and
Richard Schwartz. 2007. Improved word-level sys-
tem combination for machine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 312.
Citeseer.
Yangyang Shi, Pascal Wiggers, and Catholijn M
Jonker. 2012. Towards recurrent neural networks
language models with linguistic and contextual fea-
tures. In INTERSPEECH.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Martin Sundermeyer, Ilya Oparin, J-L Gauvain, Ben
Freiberg, R Schluter, and Hermann Ney. 2013.
Comparison of feedforward and recurrent neural
network language models. In Acoustics, Speech and
Signal Processing (ICASSP), 2013 IEEE Interna-
tional Conference on, pages 8430?8434. IEEE.
Aniruddha Tammewar, Karan Singla, Srinivas Banga-
lore, and Michael Carl. 2013. Enhancing asr by
mt using semantic information from hindiwordnet.
In Proceedings of ICON-2013: 10th International
Conference on Natural Language Processing.
56
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 47?55,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Adapting Predicate Frames for Urdu PropBanking
Riyaz Ahmad Bhat
?
, Naman Jain
?
, Dipti Misra Sharma
?
, Ashwini Vaidya
?
,
Martha Palmer
?
, James Babani
?
and Tafseer Ahmed
?
LTRC, IIIT-H, Hyderabad, India
?
University of Colorado, Boulder, CO 80309 USA
?
DHA Suffa University, Karachi, Pakistan
?
{riyaz.bhat, naman.jain}@research.iiit.ac.in, dipti@iiit.ac.in,
{vaidyaa,mpalmer, james.babani}@colorado.edu, tafseer@dsu.edu.pk
Abstract
Hindi and Urdu are two standardized reg-
isters of what has been called the Hindus-
tani language, which belongs to the Indo-
Aryan language family. Although, both
the varieties share a common grammar,
they differ significantly in their vocabulary
to an extent where both become mutually
incomprehensible (Masica, 1993). Hindi
draws its vocabulary from Sanskrit while
Urdu draws its vocabulary from Persian,
Arabic and even Turkish. In this paper,
we present our efforts to adopt frames of
nominal and verbal predicates that Urdu
shares with either Hindi or Arabic for
Urdu PropBanking. We discuss the fea-
sibility of porting such frames from either
of the sources (Arabic or Hindi) and also
present a simple and reasonably accurate
method to automatically identify the ori-
gin of Urdu words which is a necessary
step in the process of porting such frames.
1 Introduction
Hindi and Urdu, spoken primarily in northern In-
dia and Pakistan, are socially and even officially
considered two different language varieties. How-
ever, such a division between the two is not es-
tablished linguistically. They are two standard-
ized registers of what has been called the Hindus-
tani language, which belongs to the Indo-Aryan
language family. Masica (1993) explains that,
while they are different languages officially, they
are not even different dialects or sub-dialects in
a linguistic sense; rather, they are different liter-
ary styles based on the same linguistically defined
sub-dialect. He further explains that at the collo-
quial level, Hindi and Urdu are nearly identical,
both in terms of core vocabulary and grammar.
However, at formal and literary levels, vocabu-
lary differences begin to loom much larger (Hindi
drawing its higher lexicon from Sanskrit and Urdu
from Persian and Arabic) to the point where the
two styles/languages become mutually unintelligi-
ble. In written form, not only the vocabulary but
the way Urdu and Hindi are written makes one be-
lieve that they are two separate languages. They
are written in separate orthographies, Hindi be-
ing written in Devanagari, and Urdu in a modi-
fied Persio-Arabic script. Given such (apparent)
divergences between the two varieties, two paral-
lel treebanks are being built under The Hindi-Urdu
treebanking Project (Bhatt et al., 2009; Xia et al.,
2009). Both the treebanks follow a multi-layered
and multi-representational framework which fea-
tures Dependency, PropBank and Phrase Structure
annotations. Among the two treebanks the Hindi
treebank is ahead of the Urdu treebank across all
layers. In the case of PropBanking, the Hindi tree-
bank has made considerable progress while Urdu
PropBanking has just started.
The creation of predicate frames is the first step
in PropBanking, which is followed by the actual
annotation of verb instances in corpora. In this
paper, we look at the possibility of porting re-
lated frames from Arabic and Hindi PropBanks for
Urdu PropBanking. Given that Urdu shares its vo-
cabulary with Arabic, Hindi and Persian, we look
at verbal and nominal predicates that Urdu shares
with these languages and try to port and adapt their
frames from the respective PropBanks instead of
creating them afresh. This implies that identifi-
cation of the source of Urdu predicates becomes
a necessary step in this process. Thus, in order
to port the relevant frames, we need to first iden-
tify the source of Urdu predicates and then extract
their frames from the related PropBanks. To state
briefly, we present the following as contributions
of this paper:
? Automatic identification of origin or source
of Urdu vocabulary.
47
? Porting and adapting nominal and verbal
predicate frames from the PropBanks of re-
lated languages.
The rest of the paper is organised as follows: In
the next Section we discuss the Hindi-Urdu tree-
banking project with the focus on PropBanking.
In Section 3, we discuss our efforts to automati-
cally identify the source of Urdu vocabulary and
in Section 4, we discuss the process of adapting
and porting Arabic and Hindi frames for Urdu
PropBanking. Finally we conclude with some
future directions in Section 5.
2 A multi-layered,
multi-representational treebank
Compared to other existing treebanks, Hindi/Urdu
Treebanks (HTB/UTB) are unusual in that they are
multi-layered. They contain three layers of anno-
tation: dependency structure (DS) for annotation
of modified-modifier relations, PropBank-style
annotation (PropBank) for predicate-argument
structure, and an independently motivated phrase-
structure (PS). Each layer has its own framework,
annotation scheme, and detailed annotation guide-
lines. Due to lack of space and relevance to our
work, we only look at PropBanking with reference
to Hindi PropBank, here.
2.1 PropBank Annotation
The first PropBank, the English PropBank (Kings-
bury and Palmer, 2002), originated as a one-
million word subset of the Wall Street Journal
(WSJ) portion of Penn Treebank II (an English
phrase structure treebank). The verbs in the Prop-
Bank are annotated with predicate-argument struc-
tures and provide semantic role labels for each
syntactic argument of a verb. Although these
were deliberately chosen to be generic and theory-
neutral (e.g., ARG0, ARG1), they are intended
to consistently annotate the same semantic role
across syntactic variations. For example, in both
the sentences John broke the window and The win-
dow broke, ?the window? is annotated as ARG1
and as bearing the role of ?Patient?. This reflects
the fact that this argument bears the same seman-
tic role in both the cases, even though it is realized
as the structural subject in one sentence and as the
object in the other. This is the primary difference
between PropBank?s approach to semantic role la-
bels and the Paninian approach to karaka labels,
which it otherwise resembles closely. PropBank?s
ARG0 and ARG1 can be thought of as similar
to Dowty?s prototypical ?Agent? and ?Patient?
(Dowty, 1991). PropBank provides, for each sense
of each annotated verb, its ?roleset?, i.e., the possi-
ble arguments of the predicate, their labels and all
possible syntactic realizations. The primary goal
of PropBank is to supply consistent, simple, gen-
eral purpose labeling of semantic roles for a large
quantity of coherent text that can provide training
data for supervised machine learning algorithms,
in the same way that the Penn Treebank supported
the training of statistical syntactic parsers.
2.1.1 Hindi PropBank
The Hindi PropBank project has differed signif-
icantly from other PropBank projects in that the
semantic role labels are annotated on dependency
trees rather than on phrase structure trees. How-
ever, it is similar in that semantic roles are defined
on a verb-by-verb basis and the description at
the verb-specific level is fine-grained; e.g., a
verb like ?hit? will have ?hitter? and ?hittee?.
These verb-specific roles are then grouped into
broader categories using numbered arguments
(ARG). Each verb can also have a set of modifiers
not specific to the verb (ARGM). In Table 1,
PropBank-style semantic roles are listed for
the simple verb de ?to give?. In the table, the
numbered arguments correspond to the giver,
thing given and recipient. Frame file definitions
are created manually and include role information
as well as a unique roleset ID (e.g. de.01 in Table
1), which is assigned to every sense of a verb. In
addition, for Hindi the frame file also includes the
transitive and causative forms of the verb (if any).
Thus, the frame file for de ?give? will include
dilvaa ?cause to give?.
de.01 to give
Arg0 the giver
Arg1 thing given
Arg2 recipient
Table 1: A Frame File
The annotation process for the PropBank takes
place in two stages: the creation of frame files for
individual verb types, and the annotation of pred-
icate argument structures for each verb instance.
The annotation for each predicate in the corpus
is carried out based on its frame file definitions.
48
The PropBank makes use of two annotation tools
viz. Jubilee (Choi et al., 2010b) and Cornerstone
(Choi et al., 2010a) for PropBank instance annota-
tion and PropBank frame file creation respectively.
For annotation of the Hindi and Urdu PropBank,
the Jubilee annotation tool had to be modified to
display dependency trees and also to provide ad-
ditional labels for the annotation of empty argu-
ments.
3 Identifying the source of Urdu
Vocabulary
Predicting the source of a word is similar to lan-
guage identification where the task is to identify
the language a given document is written in. How-
ever, language identification at word level is more
challenging than a typical document level lan-
guage identification problem. The number of fea-
tures available at document level is much higher
than at word level. The available features for word
level identification are word morphology, syllable
structure and phonemic (letter) inventory of the
language(s).
In the case of Urdu, the problem is even more
complex as the borrowed words don?t necessarily
carry the inflections of their source language and
don?t retain their identity as such (they undergo
phonetic changes as well). For example, khabar
?news? which is an Arabic word declines as per
the morphological paradigm of feminine nom-
inals in Hindi and Urdu as shown in Table (2).
However, despite such challenges, if we look at
the character histogram in Figure (1), we can still
identify the source of a sufficiently large portion
of Urdu vocabulary just by using letter-based
heuristics. For example neither Arabic nor Persian
has aspirated consonants like bH, ph Aspirated
Bilabial Plosives; tSh, dZH Aspirated Alveolar
Fricatives; ?H Aspirated Retroflex Plosive; gH, kh
Aspirated Velar Plosives etc. while Hindi does.
Similarly, the following sounds occur only in
Arabic and Persian: Z Fricative Postalveolar; T,
D Fricative Dental; ? Fricative Pharyngeal; X
Fricative Uvular etc. Using these heuristics we
could identify 2,682 types as Indic, and 3,968
as either Persian or Arabic out of 12,223 unique
types in the Urdu treebank (Bhat and Sharma,
2012).
Singular Plural
Direct khabar khabarain
Oblique khabar khabaron
Table 2: Morphological Paradigm of khabar
This explains the efficiency of n-gram based ap-
proaches to either document level or word level
language identification tasks as reported in the re-
cent literature on the problem (Dunning, 1994;
Elfardy and Diab, 2012; King and Abney, 2013;
Nguyen and Dogruoz, 2014; Lui et al., 2014).
In order to predict the source of an Urdu word,
we frame two classification tasks: (1) binary clas-
sification into Indic and Persio-Arabic and, (2) tri-
class classification into Arabic, Indic and Persian.
Both the problems are modeled using smoothed n-
gram based language models.
3.1 N-gram Language Models
Given a word w to classify into one of k classes
c
1
, c
2
, ... , c
k
, we will choose the class with the
maximum conditional probability:
c
?
= argmax
c
i
p(c
i
|w)
= argmax
c
i
p(w|c
i
) ? p(c
i
)
(1)
The prior distribution p(c) of a class is esti-
mated from the respective training sets shown in
Table (3). Each training set is used to train a
separate letter-based language model to estimate
the probability of word w. The language model
p(w) is implemented as an n-gram model using
the IRSTLM-Toolkit (Federico et al., 2008) with
Kneser-Ney smoothing. The language model is
defined as:
p(w) =
n
?
i=1
p(l
i
|l
i?1
i?k
) (2)
where, l is a letter and k is a parameter indicat-
ing the amount of context used (e.g., k = 4 means
5-gram model).
3.2 Etymological Data
In order to prepare training and testing data
marked with etymological information for our
classification experiments, we used the Online
1
http://www.langsci.ucl.ac.uk/ipa/IPA chart %28C%
292005.pdf
49
bH Z T ? X D sQ tQ dQ Q DQ G f tSh q ?H gH khdZH N ? ph S ? th t?h d?H tS b d g H k dZ m l n p s r t t? V j d?
0
5 ? 10
?2
0.1
0.15
0.2
0.25
0.3
0.35
Alphabets in IPA
1
R
e
l
a
t
i
v
e
F
r
e
q
u
e
n
c
y
Arabic
Hindi
Persian
Urdu
Figure 1: Relative Distribution of Arabic, Hindi, Persian and Urdu Alphabets (Consonants only)
Urdu Dictionary
2
(henceforth OUD). OUD has
been prepared under the supervision of the e-
government Directorate of Pakistan
3
. Apart from
basic definition and meaning, it provides etymo-
logical information for more than 120K Urdu
words. Since the dictionary is freely
4
available
and requires no expertise for extraction of word
etymology which is usually the case with manual
annotation, we could mark the etymological infor-
mation on a reasonably sized word list in a limited
time frame. The statistics are provided in Table
(3). We use Indic as a cover term for all the words
that are either from Sanskrit, Prakrit, Hindi or lo-
cal languages.
Language Data Size Average Token Length
Arabic 6,524 6.8
Indic 3,002 5.5
Persian 4,613 6.5
Table 3: Statistics of Etymological Data
2
http://182.180.102.251:8081/oud/default.aspx
3
www.e-government.gov.pk
4
We are not aware of an offline version of OUD.
3.3 Experiments
We carried out a number of experiments in order
to explore the effect of data size and the order of
n-gram models on the classification performance.
By varying the size of training data, we wanted to
identify the lower bound on the training size with
respect to the classification performance. We var-
ied the training size per training iteration by 1%
for n-grams in the order 1-5 for both the classifi-
cation problems. For each n-gram order 100 ex-
periments were carried out, i.e overall 800 exper-
iments for binary and tri-class classification. The
impact of training size on the classification perfor-
mance is shown in Figures (2) and (3) for binary
and tri-class classification respectively. As ex-
pected, at every iteration the additional data points
introduced into the training data increased the per-
formance of the model. With a mere 3% of the
training data, we could reach a reasonable accu-
racy of 0.85 in terms of F-score for binary classi-
fication and for tri-class classification we reached
the same accuracy with 6% of the data.
Similarly, we tried different order n-gram mod-
els to quantify the effect of character context on
50
the classification performance. As with the in-
crease in data size, increasing the n-gram order
profoundly improved the results. In both the clas-
sification tasks, unigram based models converge
faster than the higher order n-gram based models.
The obvious reason for it is the small, finite set of
characters that a language operates with (? 37 in
Arabic, ? 39 in Persian and ? 48 in Hindi). A
small set of words (unique in our case) is probably
enough to capture at least a single instance of each
character. As no new n-gram is introduced with
subsequent additions of new tokens in the training
data, the accuracy stabilizes. However, the accu-
racy with higher order n-grams kept on increas-
ing with an increase in the data size, though it was
marginal after 5-grams. The abrupt increase after
8,000 training instances is probably due to the ad-
dition of an unknown bigram sequence(s) to the
training data. In particular, the Recall of Persio-
Arabic increased by 2.2%.
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 2: Learning Curves for Binary Classifica-
tion of Urdu Vocabulary
3.4 Results
We performed 10-fold cross validation over all the
instances of the etymological data for both the bi-
nary and tri-class classification tasks. We split the
data into training and testing sets with a ratio of
80:20 using the stratified sampling. Stratified sam-
pling distributes the samples of each class in train-
ing and testing sets with the same percentage as in
the complete set. For all the 10-folds, the order of
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 3: Learning Curves for Tri-class Classifi-
cation of Urdu Vocabulary
n-gram was varied again from 1-5. Tables (4) and
(5) show the consolidated results for these tasks
with a frequency based baseline to evaluate the
classification performance. In both the tasks, we
achieved highest accuracy with language models
trained with 5-gram letter sequence context. The
best results in terms of F-score are 0.96 and 0.93
for binary and tri-class classification respectively.
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.40 0.50 0.40
1-gram 0.89 0.89 0.89
2-gram 0.95 0.95 0.95
3-gram 0.96 0.96 0.96
4-gram 0.96 0.96 0.96
5-gram 0.96 0.96 0.96
Table 4: Results of 10-fold Cross Validation on
Binary Classification
Although, we have achieved quite reasonable
accuracies in both the tasks, a closer look at the
confusion matrices shown in Tables (6) and (7)
show that we can still improve the accuracies by
balancing the size of data across classes. In binary
classification our model is more biased towards
Persio-Arabic as the data is highly imbalanced.
Our binary classifier misclassifies 0.86% of Indic
tokens as Persio-Arabic since the prior probability
of the latter is much higher than that of the former.
While in the case of tri-class classification, using
51
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.15 0.33 0.21
1-gram 0.83 0.83 0.83
2-gram 0.89 0.89 0.89
3-gram 0.91 0.91 0.91
4-gram 0.93 0.93 0.93
5-gram 0.93 0.93 0.93
Table 5: Results of 10-fold Cross Validation on
Tri-Class Classification
higher order n-gram models can resolve the
prominent confusion between Arabic and Persian.
Since both Arabic and Persian share almost the
same phonetic inventory, working with lower
order n-gram models doesn?t seem ideal.
Class Indic Persio-Arabic
Indic 235 60
Persio-Arabic 15 1,057
Table 6: Confusion Matrix of Binary Classifica-
tion
Class Arabic Indic Persian
Arabic 605 5 26
Indic 11 268 18
Persian 22 9 415
Table 7: Confusion Matrix of Tri-class Classifica-
tion
4 Adapting Frames from Arabic and
Hindi PropBanks
As discussed in Section 2.1.1, the creation of pred-
icate frames precedes the actual annotation of verb
instances in a given corpus. In this section, we de-
scribe our approach towards the first stage of Urdu
PropBanking by adapting related predicate frames
from Arabic and Hindi PropBanks (Palmer et al.,
2008; Vaidya et al., 2011). Since a PropBank
is not available for Persian, we could only adapt
those predicate frames which are shared with Ara-
bic and Hindi.
Although, Urdu shares or borrows most of its
literary vocabulary from Arabic and Persian, it re-
tains its simple verb (as opposed to compound or
complex verbs) inventory from Indo-Aryan ances-
try. Verbs from Arabic and Persian are borrowed
less frequently, although there are examples such
as ?khariid? buy, ?farma? say etc.
5
This over-
lap in the verb inventory between Hindi and Urdu
might explain the fact that they share the same
grammar.
The fact that Urdu shares its lexicon with these
languages, prompted us towards exploring the
possibility of using their resources for Urdu Prop-
Banking. We are in the process of adapting frames
for those Urdu predicates that are shared with ei-
ther Arabic or Hindi.
Urdu frame file creation must be carried out for
both simple verbs and complex predicates. Since
Urdu differs very little in simple verb inventory
from Hindi, this simplifies the development pro-
cess as the frames could be ported easily. How-
ever, this is not the case with nominal predicates.
In Urdu, many nominal predicates are borrowed
from Arabic or Persian as shown in Table (8).
Given that a PropBank for Persian is not available,
the task of creating the frames for nominal predi-
cates in Urdu would have been fairly daunting in
the paucity of the Arabic PropBank, as well.
Simple Verbs Nominal Predicates
Language Total Unique Total Unique
Arabic 12 1 6,780 765
Hindi 7,332 441 1,203 258
Persian 69 3 2,276 352
Total 7,413 445 10,259 1,375
Table 8: Urdu Treebank Predicate Statistics
4.1 Simple Verbs
The simple verb inventory of Urdu and Hindi is
almost similar, so the main task was to locate and
extract the relevant frames from Hindi frame files.
Fortunately, with the exception of farmaa ?say?,
all the other simple verbs which Urdu borrows
from Persian or Arabic (cf. Table (8)) were also
borrowed by Hindi. Therefore, the Hindi sim-
ple verb frame files sufficed for porting frames for
Urdu simple verbs.
There were no significant differences found be-
tween the Urdu and Hindi rolesets, which describe
either semantic variants of the same verb or its
causative forms. Further, in order to name the
frame files with their corresponding Urdu lemmas,
we used Konstanz?s Urdu transliteration scheme
5
Borrowed verbs often do not function as simple verbs
rather they are used like nominals in complex predicate con-
structions such as mehsoos in ?mehsoos karnaa? to feel.
52
(Malik et al., 2010) to convert a given lemma into
its romanized form. Since the Hindi frame files
use the WX transliteration scheme
6
, which is not
appropriate for Urdu due to lack of coverage for
Persio-Arabic phonemes or sounds like dQ ?pha-
ryngealized voiced alveolar stop?. The frame files
also contain example sentences for each predicate,
in order to make the PropBank annotation task eas-
ier. While adapting the frame files from Hindi
to Urdu, simply transliterating such examples for
Urdu predicates was not always an option, because
sentences consisting of words with Sanskrit origin
may not be understood by Urdu speakers. Hence,
all the examples in the ported frames have been
replaced with Urdu sentences by an Urdu expert.
In general we find that the Urdu verbs are quite
similar to Hindi verbs, and this simplified our task
of adapting the frames for simple verbs. The
nouns, however, show more variation. Since a
large proportion (up to 50%) of Urdu predicates
are expressed using verb-noun complex predi-
cates, nominal predicates play a crucial role in our
annotation process and must be accounted for.
4.2 Complex Predicates
In the Urdu treebank, there are 17,672 predicates,
of which more than half have been identified as
noun-verb complex predicates (NVC) at the de-
pendency level. Typically, a noun-verb complex
predicate chorii ?theft? karnaa ?to do? has two
components: a noun chorii and a light verb karnaa
giving us the meaning ?steal?. The verbal compo-
nent in NVCs has reduced predicating power (al-
though it is inflected for person, number, and gen-
der agreement as well as tense, aspect and mood)
and its nominal complement is considered the true
predicate. In our annotation of NVCs, we fol-
low a procedure common to all PropBanks, where
we create frame files for the nominal or the ?true?
predicate (Hwang et al., 2010). An example of a
frame file for a noun such as chorii is described in
Table (9).
The creation of a frame file for the set of
true predicates that occur in an NVC is impor-
tant from the point of view of linguistic annota-
tion. Given the large number of NVCs, a semi-
automatic method has been proposed for creating
Hindi nominal frame files, which saves the man-
ual effort required for creating frames for nearly
6
http://en.wikipedia.org/wiki/WX notation
Frame file for chorii-n(oun)
chorii.01: theft-n light verb: kar?do; to steal?
Arg0 person who steals
Arg1 thing stolen
chorii.02 : theft-n light verb: ho ?be/become; to
get stolen?
Arg1 thing stolen
Table 9: Frame file for predicate noun chorii
?theft? with two frequently occurring light verbs
ho and kar. If other light verbs are found to occur,
they are added as additional rolesets as chorii.03,
chorii.04 and so on.
3,015 unique Hindi noun and light verb combina-
tions (Vaidya et al., 2013).
For Urdu, the process of nominal frame file cre-
ation is preceded by the identification of the ety-
mological origin for each nominal. If that nomi-
nal has an Indic or Arabic origin, relevant frames
from Arabic or Hindi PropBanks were adapted for
Urdu. On the other hand, if the Urdu nominal orig-
inates from Persian, then frame creation will be
done either manually or using other available Per-
sian language resources, in the future.
In Table (8), there are around 258 nominal pred-
icates that are common in Hindi and Urdu, so we
directly ported their frames from Hindi PropBank
with minor changes as was done for simple verb
frames. Out of 765 nominal predicates shared with
Arabic, 308 nominal predicate frames have been
ported to Urdu. 98 of these nominal predicate
frames were already present in the Arabic Prop-
Bank and were ported as such. However, for the
remaining 667 unique predicates, frames are be-
ing created manually by Arabic PropBanking ex-
perts and will be ported to Urdu once they become
available.
Porting of Arabic frames to Urdu is not that triv-
ial. We observed that while Urdu borrows vocabu-
lary from Arabic it does not borrow all the senses
for some words. In such cases, the rolesets that are
irrelevant to Urdu have to be discarded manually.
The example sentences for all the frames ported
from Arabic PropBank have to be sourced from
either the web or manually created by an Urdu ex-
pert, as was the case with Hindi simple verbs.
5 Conclusion
In this paper we have exploited the overlap be-
tween the lexicon of Urdu, Arabic and Hindi for
the creation of predicate frames for Urdu Prop-
53
Banking. We presented a simple and accurate clas-
sifier for the identification of source or origin of
Urdu vocabulary which is a necessary step in the
overall process of extraction of predicate frames
from the related PropBanks. In the case of sim-
ple verbs that occur in the Urdu treebank, we have
extracted all the frames from the Hindi PropBank
and adapted them for Urdu PropBanking. Simi-
larly for complex predicates, frames for Urdu tree-
bank nominal predicates are extracted from Hindi
as well as from Arabic PropBanks. Since a Prop-
Bank is not available for Persian, the creation
of frames for shared predicates with Persian is a
prospect for future work. We plan to create these
frames either manually or semi-automatically, us-
ing the available Persian Dependency treebanks
(Rasooli et al., 2011; Rasooli et al., 2013).
Acknowledgments
We would like to thank Himani Chaudhry for her
valuable comments that helped to improve the
quality of this paper.
The work reported in this paper is supported by
the NSF grant (Award Number: CNS 0751202;
CFDA Number: 47.070)
7
.
References
Riyaz Ahmad Bhat and Dipti Misra Sharma. 2012.
A dependency treebank of urdu and its evaluation.
In Proceedings of the Sixth Linguistic Annotation
Workshop, pages 157?165. Association for Compu-
tational Linguistics.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. A multi-representational and multi-layered
treebank for hindi/urdu. In Proceedings of the Third
Linguistic Annotation Workshop, pages 186?189.
Association for Computational Linguistics.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010a. Propbank frameset annotation guidelines us-
ing a dedicated editor, cornerstone. In LREC.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010b. Propbank instance annotation guidelines us-
ing a dedicated editor, jubilee. In LREC.
David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language, 67(3):547?619.
7
Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the author(s)
and do not necessarily reflect the views of the National Sci-
ence Foundation.
Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Heba Elfardy and Mona T Diab. 2012. Token level
identification of linguistic code switching. In COL-
ING (Posters), pages 287?296.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
dling large scale language models. In Interspeech,
pages 1618?1621.
Jena D Hwang, Archna Bhatia, Clare Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and
Martha Palmer. 2010. Propbank annotation of mul-
tilingual light verb constructions. In Proceedings of
the Fourth Linguistic Annotation Workshop, pages
82?90. Association for Computational Linguistics.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110?1119.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In LREC. Citeseer.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation for Computational Linguistics, 2:27?40.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina B?ogel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliterat-
ing urdu for a broad-coverage urdu/hindi lfg gram-
mar. In LREC.
Colin P Masica. 1993. The Indo-Aryan Languages.
Cambridge University Press.
Dong Nguyen and A Seza Dogruoz. 2014. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Martha Palmer, Olga Babko-Malaya, Ann Bies,
Mona T Diab, Mohamed Maamouri, Aous Man-
souri, and Wajdi Zaghouani. 2008. A pilot arabic
propbank. In LREC.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
persian verbs: The first steps towards persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of a
persian syntactic dependency treebank. In Proceed-
ings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
54
Linguistics: Human Language Technologies, pages
306?314.
Ashwini Vaidya, Jinho D Choi, Martha Palmer, and
Bhuvana Narasimhan. 2011. Analysis of the hindi
proposition bank using dependency structure. In
Proceedings of the 5th Linguistic Annotation Work-
shop, pages 21?29. Association for Computational
Linguistics.
Ashwini Vaidya, Martha Palmer, and Bhuvana
Narasimhan. 2013. Semantic roles for nominal
predicates: Building a lexical resource. NAACL
HLT 2013, 13:126.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In The 7th International
Workshop on Treebanks and Linguistic Theories.
Groningen, Netherlands.
55
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85?91,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploring System Combination approaches for Indo-Aryan MT Systems
Karan Singla
1
, Nishkarsh Shastri
2
, Megha Jhunjhunwala
2
, Anupam Singh
3
,
Srinivas Bangalore
4
, Dipti Misra Sharma
1
1
LTRC IIIT Hyderabad,
2
IIT-Kharagpur,
3
NIT-Durgapur,
4
AT&T Labs-Research
Abstract
Statistical Machine Translation (SMT)
systems are heavily dependent on the qual-
ity of parallel corpora used to train transla-
tion models. Translation quality between
certain Indian languages is often poor due
to the lack of training data of good qual-
ity. We used triangulation as a technique
to improve the quality of translations in
cases where the direct translation model
did not perform satisfactorily. Triangula-
tion uses a third language as a pivot be-
tween the source and target languages to
achieve an improved and more efficient
translation model in most cases. We also
combined multi-pivot models using linear
mixture and obtained significant improve-
ment in BLEU scores compared to the di-
rect source-target models.
1 Introduction
Current SMT systems rely heavily on large quan-
tities of training data in order to produce good
quality translations. In spite of several initiatives
taken by numerous organizations to generate par-
allel corpora for different language pairs, train-
ing data for many language pairs is either not
yet available or is insufficient for producing good
SMT systems. Indian Languages Corpora Initia-
tive (ILCI) (Choudhary and Jha, 2011) is currently
the only reliable source for multilingual parallel
corpora for Indian languages however the number
of parallel sentences is still not sufficient to create
high quality SMT systems.
This paper aims at improving SMT systems
trained on small parallel corpora using various re-
cently developed techniques in the field of SMTs.
Triangulation is a technique which has been found
to be very useful in improving the translations
when multilingual parallel corpora are present.
Triangulation is the process of using an interme-
diate language as a pivot to translate a source lan-
guage to a target language. We have used phrase
table triangulation instead of sentence based tri-
angulation as it gives better translations (Utiyama
and Isahara, 2007). As triangulation technique ex-
plores additional multi parallel data, it provides
us with separately estimated phrase-tables which
could be further smoothed using smoothing meth-
ods (Koehn et al. 2003). Our subsequent approach
will explore the various system combination tech-
niques through which these triangulated systems
can be utilized to improve the translations.
The rest of the paper is organized as follows.
We will first talk about the some of the related
works and then we will discuss the facts about the
data and also the scores obtained for the baseline
translation model. Section 3 covers the triangu-
lation approach and also discusses the possibility
of using combination approaches for combining
triangulated and direct models. Section 4 shows
results for the experiments described in previous
section and also describes some interesting obser-
vations from the results. Section 5 explains the
conclusions we reached based on our experiments.
We conclude the paper with a section about our fu-
ture work.
2 Related Works
There are various works on combining the tri-
angulated models obtained from different pivots
with the direct model resulting in increased con-
fidence score for translations and increased cov-
erage by (Razmara and Sarkar, 2013; Ghannay et
al., 2014; Cohn and Lapata, 2007). Among these
techniques we explored two of the them. The first
one is the technique based on the confusion ma-
trix (dynamic) (Ghannay et al., 2014) and the other
one is based on mixing the models as explored
by (Cohn and Lapata, 2007). The paper also dis-
cusses the better choice of combination technique
85
among these two when we have limitations on
training data which in our case was small and re-
stricted to a small domain (Health & Tourism).
As suggested in (Razmara and Sarkar, 2013),
we have shown that there is an increase in phrase
coverage when combining the different systems.
Conversely we can say that out of vocabulary
words (OOV) always decrease in the combined
systems.
3 Baseline Translation Model
In our experiment, the baseline translation model
used was the direct system between the source and
target languages which was trained on the same
amount of data as the triangulated models. The
parallel corpora for 4 Indian languages namely
Hindi (hn), Marathi (mt), Gujarati (gj) and Bangla
(bn) was taken from Indian Languages Corpora
Initiative (ILCI) (Choudhary and Jha, 2011) . The
parallel corpus used in our experiments belonged
to two domains - health and tourism and the train-
ing set consisted of 28000 sentences. The develop-
ment and evaluation set contained 500 sentences
each. We used MOSES (Koehn et al., 2007) to
train the baseline Phrase-based SMT system for all
the language pairs on the above mentioned paral-
lel corpus as training, development and evaluation
data. Trigram language models were trained using
SRILM (Stolcke and others, 2002). Table 1 below
shows the BLEU score for all the trained pairs.
Language Pair BLEU Score
bn-mt 18.13
mt-bn 21.83
bn-gj 22.45
gj-mt 23.02
gj-bn 24.26
mt-gj 25.5
hn-mt 30.01
hn-bn 32.92
bn-hn 34.99
mt-hn 36.82
hn-gj 40.06
gj-hn 43.48
Table 1: BLEU scores of baseline models
4 Triangulation: Methodology and
Experiment
We first define the term triangulation in our con-
text. Each source phrase s is first translated to an
intermediate (pivot) language i, and then to a tar-
get language t. This two stage translation process
is termed as triangulation.
Our basic approach involved making triangu-
lated models by triangulating through different
pivots and then interpolating triangulated models
with the direct source-target model to make our
combined model.
In line with various previous works, we will
be using multiple translation models to overcome
the problems faced due to data sparseness and in-
crease translational coverage. Rather than using
sentence translation (Utiyama and Isahara, 2007)
from source to pivot and then pivot to target, a
phrase based translation model is built.
Hence the main focus of our approach is on
phrases rather than on sentences. Instead of using
combination techniques on the output of several
translation systems, we constructed a combined
phrase table to be used by the decoder thus avoid-
ing the additional inefficiencies observed while
merging the output of various translation systems.
Our method focuses on exploiting the availability
of multi-parallel data, albeit small in size, to im-
prove the phrase coverage and quality of our SMT
system.
Our approach can be divided into different steps
which are presented in the following sections.
4.1 Phrase-table triangulation
Our emphasis is on building an enhanced phrase
table that incorporates the translation phrase tables
of different models. This combined phrase table
will be used by the decoder during translation.
Phrase table triangulation depends mainly on
phrase level combination of the two different
phrase based systems mainly source (src) - pivot
(pvt) and pivot (pvt) - target (tgt) using pivot lan-
guage as a basis for combination. Before stating
the mathematical approach for triangulation, we
present an example.
4.1.1 Basic methodology
Suppose we have a Bengali-Hindi phrase-table
(T
BH
) and a Hindi-Marathi phrase-table (T
HM
).
From these tables, we have to construct a Bengali-
Marathi phrase-table (T
BM
). For that we need
86
Triangulated
System
Full-Triangulation
(phrase-table length)
Triangulation with top 40
(Length of phrase table)
Full Triangulation
(BLEU Score)
Triangulation with top 40
(BLEU SCORE)
gj - hn - mt 3,585,450 1,086,528 24.70 24.66
gj - bn - mt 7,916,661 1,968,383 20.55 20.04
Table 2: Comparison between triangulated systems in systems with full phrase table and the other having
top 40 phrase-table entries
to estimate four feature functions: phrase trans-
lation probabilities for both directions ?(
?
b|m?)
and ?(m?|
?
b), and lexical translation probabilities
for both directions lex(
?
b|m?) and lex(m?|
?
b) where
?
b and m? are Bengali and Marathi phrases that
will appear in our triangulated Bengali-Marathi
phrase-table T
BM
.
?(
?
b|m?) =
?
?
h?T
BH
?T
HM
?(
?
b|
?
h)?(
?
h|m?) (1)
?(m?|
?
b) =
?
?
h?T
BH
?T
HM
?(m?|
?
h)?(
?
h|
?
b) (2)
lex(
?
b|m?) =
?
?
h?T
BH
?T
HM
lex(
?
b|
?
h)lex(
?
h|m?) (3)
lex(m?|
?
b) =
?
?
h?T
BH
?T
HM
lex(m?|
?
h)lex(
?
h|
?
b) (4)
In these equations a conditional independence
assumption has been made that source phrase
?
b
and target phrase m? are independent given their
corresponding pivot phrase(s)
?
h. Thus, we can
derive ?(
?
b|m?), ?(m?|
?
b), lex(
?
b|m?), lex(m?|
?
b) by as-
suming that these probabilities are mutually inde-
pendent given a Hindi phrase
?
h.
The equation given requires that all phrases in
the Hindi-Marathi bitext must also be present in
the Bengali-Hindi bitext. Clearly there would be
many phrases not following the above require-
ment. For this paper we completely discarded the
missing phrases. One important point to note is
that although the problem of missing contextual
phrases is uncommon in multi-parallel corpora, as
it is in our case, it becomes more evident when the
bitexts are taken out from different sources.
In general, wider range of possible translations
are found for any source phrase through triangula-
tion. We found that in the direct model, a source
phrase is aligned to three phrases then there is
high possibility of it being aligned to three phrases
in intermediate language. The intermediate lan-
guage phrases are further aligned to three or more
phrases in target language. This results in increase
in number of translations of each source phrase.
4.1.2 Reducing the size of phrase-table
While triangulation is intuitively appealing, it suf-
fers from a few problems. First, the phrasal trans-
lation estimates are based on noisy automatic word
alignments. This leads to many errors and omis-
sions in the phrase-table. With a standard source-
target phrase-table these errors are only encoun-
tered once, however with triangulation they are en-
countered twice, and therefore the errors are com-
pounded. This leads to much noisier estimates
than in the source-target phrase-table. Secondly,
the increased exposure to noise means that trian-
gulation will omit a greater proportion of large or
rare phrases than the standard method. An align-
ment error in either of the source-intermediate bi-
text or intermediate-target bitext can prevent the
extraction of a source-target phrase pair.
As will be explained in the next section, the sec-
ond kind of problem can be ameliorated by using
the triangulated phrase-based table in conjunction
with the standard phrase based table referred to as
direct src-to-pvt phrase table in our case.
For the first kind of problem, not only the com-
pounding of errors leads to increased complex-
ity but also results in an absurdly large triangu-
lated phrase based table. To tackle the problem of
unwanted phrase-translation, we followed a novel
approach.
A general observation is that while triangulat-
ing between src-pvt and pvt-tgt systems, the re-
sultant src-tgt phrase table formed will be very
large since for a translation s? to
?
i in the src-to-
pvt table there may be many translations from
?
i to
?
t1,
?
t2...
?
tn. For example, the Bengali-Hindi
phrase-table(T
BH
) consisted of 846,106 transla-
tions and Hindi-Marathi phrase-table(T
HM
) con-
sisted of 680,415 translations and after triangu-
lating these two tables our new Bengali-Marathi
triangulated table(T
BM
) consisted of 3,585,450
translations as shown in Table 2. Tuning with
such a large phrase-table is complex and time-
consuming. To reduce the complexity of the
phrase-table, we used only the top-40 transla-
87
tions (translation with 40 maximum values of
P (
?
f |e?) for every source phrase in our triangulated
phrase-table(T
BM
) which reduced the phrase table
to 1,086,528 translations.
We relied on P (
?
f |e?)(inverse phrase translation
probability) to choose 40 phrase translations for
each phrase, since in the direct model, MERT
training assigned the most weight to this param-
eter.
It is clearly evident from Table 2 that we have
got a massive reduction in the length of the phrase-
table after taking in our phrase table and still the
results have no significant difference in our output
models.
4.2 Combining different triangulated models
and the direct model
Combining Machine translation (MT) systems has
become an important part of Statistical MT in the
past few years. There have been several works by
(Rosti et al., 2007; Karakos et al., 2008; Leusch
and Ney, 2010);
We followed two approaches
1. A system combination based on confusion
network using open-source tool kit MANY
(Barrault, 2010), which can work dynami-
cally in combining the systems
2. Combine the models by linearly interpolating
them and then using MERT to tune the com-
bined system.
4.2.1 Combination based on confusion
matrix
MANY tool was used for this and initially it was
configured to work with TERp evaluation matrix,
but we modified it to work using METEOR-Hindi
(Gupta et al., 2010), as it has been shown by
(Kalyani et al., 2014), that METEOR evaluation
metric is closer to human evaluation for morpho-
logically rich Indian Languages.
4.2.2 Linearly Interpolated Models
We used two different approaches while merging
the different triangulated models and direct src-tgt
model and we observed that both produced com-
parable results in most cases. We implemented the
linear mixture approach, since linear mixtures of-
ten outperform log-linear ones (Cohn and Lapata,
2007). Note that in our combination approaches
the reordering tables were left intact.
1. Our first approach was to use linear interpola-
tion to combine all the three models (Bangla-
Hin-Marathi, Bangla-Guj-Marathi and di-
rect Bangla-Marathi models) with uniform
weights, i.e 0.3 each in our case.
2. In the next approach, the triangulated phrase
tables are combined first into a single trian-
gulated phrase-table using uniform weights.
The combined triangulated phrase-table and
direct src-tgt phrase table is then combined
using uniform weights. In other words, we
combined all the three systems, Ban-Mar,
Ban-Hin-Mar, and Ban-Guj-Mar with 0.5,
0.25 and 0.25 weights respectively. This
weight distribution reflects the intuition that
the direct model is less noisy than the trian-
gulated models.
In the experiments below, both weight settings
produced comparable results. Since we performed
triangulation only through two languages, we
could not determine which approach would per-
form better. An ideal approach will be to train the
weights for each system for each language pair
using standard tuning algorithms such as MERT
(Zaidan, 2009).
4.2.3 Choosing Combination Approach
In order to compare the approaches on our data,
we performed experiments on Hindi-Marathi pair
following both approaches discussed in Section
4.2.1 and 4.2.2. We also generated triangulated
models through Bengali and Gujarati as pivot lan-
guages.
Also, the approach presented in section 4.2.1
depends heavily on LM (Language Model).In or-
der to study the impact of size, we worked on
training Phrase-based SMT systems with subsets
of data in sets of 5000, 10000, 150000 sentences
and LM was trained for 28000 sentences for com-
paring these approaches. The combination results
were compared following the approach mentioned
in 4.2.1 and 4.2.2.
Table 3, shows that the approach discussed in
4.2.1 works better if there is more data for LM
but we suffer from the limitation that there is no
other in-domain data available for these languages.
From the Table, it can also be seen that combin-
ing systems with the approach explained in 4.2.2
can also give similar or better results if there is
scarcity of data for LM. Therefore we followed the
88
#Training #LM Data Comb-1 Comb-2
5000 28000 21.09 20.27
10000 28000 24.02 24.27
15000 28000 27.10 27.63
Table 3: BLEU scores for Hindi-Marathi Model
comparing approaches described in 3.2.1(Comb-
1) and 3.2.2(Comb-2)
approach from Section 4.2.2 for our experiments
on other language pairs.
5 Observation and Resuslts
Table 4, shows the BLEU scores of triangulated
models when using the two languages out of the
4 Indian languages Hin, Guj, Mar, Ban as source
and target and the remaining two as the pivot lan-
guage. The first row mentions the BLEU score
of the direct src-tgt model for all the language
pairs. The second and third rows provide the tri-
angulated model scores through pivots which have
been listed. The fourth and fifth rows show the
BLEU scores for the combined models (triangu-
lated+direct) with the combination done using the
first and second approach respectively that have
been elucidated in the Section 4.2.2
As expected, both the combined models have
performed better than the direct models in all
cases.
Figure 1: Phrase-table coverage of the evaluation
set for all the language pairs
Figure 1, shows the phrase-table coverage of the
evaluation set for all the language pairs. Phrase-
table coverage is defined as the percentage of un-
igrams in the evaluation set for which translations
are present in the phrase-table. The first bar cor-
responds to the direct model for each language
pair, the second and third bars show the cover-
age for triangulated models through the 2 piv-
ots, while the fourth bar is the coverage for the
combined model (direct+triangulated). The graph
clearly shows that even though the phrase table
coverage may increase or decrease by triangula-
tion through a single pivot the combined model
(direct+triangulated) always gives a higher cover-
age than the direct model.
Moreover, there exists some triangulation mod-
els whose coverage and subsequent BLEU scores
for translation is found to be better than that of the
direct model. This is a particularly interesting ob-
servation as it increases the probability of obtain-
ing better or at least comparable translation mod-
els even when direct source-target parallel corpus
is absent.
6 Discussion
Dravidian languages are different from Indo-aryan
languages but they are closely related amongst
themselves. So we explored similar experiments
with Malayalam-Telugu pair of languages with
similar parallel data and with Hindi as pivot.
The hypothesis was that the direct model for
Malayalam-Telegu would have performed better
due to relatedness of the two languages. However
the results via Hindi were better as can be seen in
Table 5.
As Malayalam-Telegu are comparatively closer
than compared to Hindi, so the results via Hindi
should have been worse but it seems more like a
biased property of training data which considers
that all languages are closer to Hindi, as the trans-
lation data was created from Hindi.
7 Future Work
It becomes increasingly important for us to im-
prove these techniques for such languages having
rare corpora. The technique discussed in the paper
is although efficient but still have scope for im-
provements.
As we have seen from our two approaches of
combining the phrase tables and subsequent in-
terpolation with direct one, the best combination
among the two is also not fixed. If we can find the
89
BLEU scores gj-mt mt-gj gj-hn hn-gj hn-mt mt-hn
Direct model 23.02 25.50 43.48 40.06 30.01 36.82
Triangulated
through pivots
hn 24.66 hn 27.09 mt 36.76 mt 33.69 gj 29.27 gj 33.86
bn 20.04 bn 22.02 bn 35.07 bn 32.66 bn 26.72 bn 31.34
Mixture-1 26.12 27.46 43.23 39.99 33.09 38.50
Mixture-2 26.25 27.32 44.04 41.45 33.36 38.44
(a)
BLEU scores bn-gj gj-bn bn-hn hn-bn mt-bn bn-mt
Direct model 22.45 24.26 34.99 32.92 21.83 18.13
Triangulated
through pivots
hn 23.97 hn 26.26 gj 31.69 gj 29.60 hn 23.80 hn 21.04
mt 20.70 mt 22.32 mt 28.96 mt 27.95 gj 22.41 gj 18.15
Mixture-1 25.80 27.45 35.14 34.77 24.99 22.16
Mixture-2 24.66 27.39 35.02 34.85 24.86 22.75
(b)
Table 4: Table (a) & (b) show results for all language pairs after making triangulated models and then
combining them with linear interpolation with the two approaches described in 3.2.2. In Mixture-1,
uniform weights were given to all three models but in Mixture-2, direct model is given 0.5 weight relative
to the other models (.25 weight to each)
System Blue Score
Direct Model 4.63
Triangulated via Hindi 14.32
Table 5: Results for Malayalam-Telegu Pair for
same data used for other languages
best possible weights to be assigned to each table,
then we can see improvement in translation. This
can be implemented by making the machine learn
from various iterations of combining and adjusting
the scores accordingly.(Nakov and Ng, 2012) have
indeed shown that results show significant devia-
tions associated with different weights assigned to
the tables.
References
Lo??c Barrault. 2010. Many: Open source machine
translation system combination. The Prague Bul-
letin of Mathematical Linguistics, 93:147?155.
Narayan Choudhary and Girish Nath Jha. 2011. Cre-
ating multilingual parallel corpora in indian lan-
guages. In Proceedings of Language and Technol-
ogy Conference.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Citeseer.
Sahar Ghannay, France Le Mans, and Lo?c Barrault.
2014. Using hypothesis selection based features for
confusion network mt system combination. In Pro-
ceedings of the 3rd Workshop on Hybrid Approaches
to Translation (HyTra)@ EACL, pages 1?5.
Ankush Gupta, Sriram Venkatapathy, and Rajeev San-
gal. 2010. Meteor-hindi: Automatic mt evaluation
metric for hindi as a target language. In Proceed-
ings of ICON-2010: 8th International Conference
on Natural Language Processing.
Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, and
Ajai Kumar. 2014. Assessing the quality of mt sys-
tems for hindi to english translation. arXiv preprint
arXiv:1404.3992.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies: Short Papers, pages 81?84.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
90
Machine Translation and MetricsMATR, pages 315?
320. Association for Computational Linguistics.
Preslav Nakov and Hwee Tou Ng. 2012. Improv-
ing statistical machine translation for a resource-
poor language using related resource-rich lan-
guages. Journal of Artificial Intelligence Research,
44(1):179?222.
Majid Razmara and Anoop Sarkar. 2013. Ensemble
triangulation for statistical machine translation. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 252?
260.
Antti-Veikko I Rosti, Spyridon Matsoukas, and
Richard Schwartz. 2007. Improved word-level sys-
tem combination for machine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 312.
Citeseer.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Masao Utiyama and Hitoshi Isahara. 2007. A compari-
son of pivot methods for phrase-based statistical ma-
chine translation. In HLT-NAACL, pages 484?491.
Omar Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
91
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 66?76,
Dublin, Ireland, August 23rd 2014.
SSF: A Common Representation Scheme for Language Analysis for
Language Technology Infrastructure Development
Akshar Bharati
Akshar Bharati Group
Hyderabad
sangal@iiit.ac.in
Rajeev Sangal
IIT (BHU), Varanasi
sangal@iiit.ac.in
Dipti Sharma
IIIT, Hyderabad
dipti@iiit.ac.in
Anil Kumar Singh
IIT (BHU), Varanasi
nlprnd@gmail.com
Abstract
We describe a representation scheme and an analysis engine using that scheme, both of which
have been used to develop infrastructure for HLT. The Shakti Standard Format is a readable and
robust representation scheme for analysis frameworks and other purposes. The representation
is highly extensible. This representation scheme, based on the blackboard architectural model,
allows a very wide variety of linguistic and non-linguistic information to be stored in one place
and operated upon by any number of processing modules. We show how it has been successfully
used for building machine translation systems for several language pairs using the same architec-
ture. It has also been used for creation of language resources such as treebanks and for different
kinds of annotation interfaces. There is even a query language designed for this representation.
Easily wrappable into XML, it can be used equally well for distributed computing.
1 Introduction
Building infrastructures for human language technology is a non-trivial task. There can be numerous
issues that have to be addressed, whether linguistic or non-linguistic. Unless carefully managed, the
overall complexity can easily get out of control and seriously threaten the sustainability of the system.
This may apply to all large software systems, but the complexities associated with humans languages
(both within and across languages) only add to the problem. To make it possible to build various compo-
nents of an infrastructure that scales within and across languages for a wide variety of purposes, and to
be able to do it by re-using the representation(s) and the code, deserves to be considered an achievement.
GATE
1
(Cunningham et al., 2011; Li et al., 2009), UIMA
2
(Ferrucci and Lally, 2004; Bari et al., 2013;
Noh and Pad?o, 2013) and NLTK
3
(Bird, 2002) are well known achievements of this kind. This paper is
about one other such effort that has proved to be successful over the last decade or more.
2 Related Work
GATE is designed to be an architecture, a framework and a development environment, quite like UIMA,
although the two differ in their realization of this goal. It enables users to develop and deploy robust
language engineering components and resources. It also comes bundled with several commonly used
baseline Natural Language Processing (NLP) applications. It makes strict distinction between data, al-
gorithms, and ways of visualising them, such that algorithms + data + GUI = applications. Consequently,
it has three types of components: language resources, processing resources and visual resources.GATE
uses an annotation format with stand-off markup.
UIMA is a middleware architecture for processing unstructured information (UIM) (Ferrucci and
Lally, 2004), with special focus on NLP. Its development originated in the realization that the ability
to quickly discover each other?s results and rapidly combine different technologies and approaches ac-
celerates scientific advance. It has powerful search capabilities and a data-driven framework for the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://gate.ac.uk/
2
https://uima.apache.org/
3
http://www.nltk.org/
66
development, composition and distributed deployment of analysis engines. More than the development
of independent UIM applications, UIMA aims to enable accelerated development of integrated and ro-
bust applications, combining independent applications in diverse sub-areas of NLP, so as to accelerate
the research cycle as well as the production time. In UIMA, The original document and its analysis are
represented in a structure called the Common Analysis Structure, or CAS. Annotations in the CAS are
maintained separately from the document itself to allow greater flexibility than inline markup. There
is an XML specification for the CAS and it is possible to develop analysis engines that operate on and
output data in this XML format, which also (like GATE and NLTK) uses stand-off markup.
The Natural Language Toolkit (NLTK) is a suite of modules, data sets and tutorials (Bird, 2002). It
supports many NLP data types and can process many NLP tasks. It has a rich collection of educational
material (such as animated algorithms) for those who are learning NLP. It can also be used as a platform
for prototyping of research systems.
SSF and the Shakti Analyzer are similar to the above three but have a major difference when com-
pared with them. SSF is a ?powerful? notation for representing the NLP analysis, at all stages, whether
morphological, part-of-speech level, chunk level, or sentence level parse. The notation is so designed
that it is flexible, as well as readable. The notation can be read by human beings and can also be loaded
in memory, so that it can be used efficiently. It also allows the architecture to consist of modules which
can be configured easily under different settings. The power of the notation and the flexibility of the
resulting architecture gives enormous power to the system framework.
The readability of the format allows it to be used directly with any plain text editors, without requiring
the use of any special tools or editors. Many users prefer the data in plain text format as it allows them to
use the editors they are familiar with. Such readability and simplicity has turned out, in our experience,
to be an advantage even for experts like software developers and (computer savvy) linguists.
It would be an interesting exercise to marry SSF notation and the Shakti way of doing things with the
GATE and UIMA architecture. Our own feeling is that the resulting system/framework with a powerful
notation like SSF and the comprehensive framework like UIMA/GATE would lead to a new even more
powerful framework with a principled notation.
3 Shakti Standard Format
Shakti Standard Format (SSF) is a representation scheme (along with a corresponding format) that can be
used for most kinds of linguistically analyzed data. It allows information in a sentence to be represented
in the form of one or more trees together with a set of attribute-value pairs with nodes of the trees. The
attribute-value pairs allow features or properties to be specified with every node. Relations of different
types across nodes can also be specified using an attribute-value like representation. The representation is
specially designed to allow different levels and kinds of linguistic analyses to be stored. The developers
use APIs to store or access information regarding structure of trees and attribute-value pairs.
If a module is successful in its task, it adds a new analysis using trees and attribute values to the
representation. Thus, even though the format is fixed, it is extensible in terms of attributes or analyses.
This approach allows ready-made packages (such as, POS tagger, chunker, and parser) to be incorporated
easily using a wrapper (or a pair of converters). In order to interface such pre-existing packages to the
system, all that is required is to convert from (input) SSF to the input format required by that package and,
the output of the package to SSF. The rest of the modules of the system continue to operate seamlessly.
The format allows both in-memory representation as well as stream (or text) representation. They are
inter-convertible using a reader (stream to memory) and printer (memory to stream). The in-memory
representation is good in speed of processing, while the stream is good for portability, heterogenous
machines, and flexibility, in general.
SSF promotes the dictum: ?Simplify globally, and if unavoidable, complicate only locally.? Even if
the number of modules is large and each module does a small job, the local complexity (of individual
modules) remains under tight control for most of the modules. At worst, complexity is introduced only
locally, without affecting the global simplicity.
67
3.1 Text Level SSF
In SSF, a text or a document has a sequence of sentences with some structure such as paragraphs and
headings. It also includes meta information related to title, author, publisher, year and other information
related to the origin of the text or the document. Usually, there is also the information related to encoding,
and version number of the tagging scheme, etc. The text level SSF has two parts, header and body:
Figure 1: Document Structure in SSF
<document docid="..." docnumber="...">
<header>
...
</header>
<body>
...
</body>
The header contains meta information about the title, author, publisher, etc. as contained in the CML
(Corpus Markup Language) input
4
. The body contains sentences, each in SSF. The body of a text in SSF
contains text blocks given by the tag tb.
<body encode= ... >
<tb>
...
</tb>
...
</body>
A text block (tb) contains a sequence of sentences. Each sentence can be marked as a segment (to
indicate a heading, a partial sentence, etc.) or not a segment (to indicate a normal sentence).
3.2 Sentence Level SSF
Several formalisms have been developed for such descriptions, but the two main ones in the field of
NLP are Phrase Structure Grammar (PSG) (Chomsky, 1957) and Dependency Grammar (DG) (Tesniere,
1959). In PSG, a set of phrase structure rules are given for the grammar of a language. It is constituency
based and order of elements are a part of the grammar, and the resulting tree. DG, on the other hand, is
relational and shows relations between words or elements of a sentence. It, usually, tries to capture the
syntactico-semantic relations of the elements in a sentence. The resulting dependency tree is a tree with
nodes and edges being labelled.
The difference in the two approaches are shown below with the help of the following English example:
Ram ate the banana.
The phrase structure tree is drawn in Fig. 2 using a set of phrase structure rules. Fig. 3 shows the
dependency tree representation for this sentence. SSF can represent both these formats.
4
Thus SSF becomes a part of CML.
68
Figure 2: Phrase structure tree Figure 3: Dependency tree
Sentence level SSF is used to store the analysis of a sentence. It occurs as part of text level SSF. The
analysis of a sentence may mark any or all of the following kinds of information as appropriate: part
of speech of the words in the sentence; morphological analysis of the words including properties such
as root, gender, number, person, tense, aspect, modality; phrase-structure or dependency structure of the
sentence; and properties of units such as chunks, phrases, local word groups, bags, etc. Note that SSF
is theory neutral and allows both phrase structure as well as dependency structure to be coded, and even
mixed in well defined ways.
Though the format in SSF is fixed, it is extensible to handle new features. It also has a text represen-
tation, which makes it easy to read the output. The following example illustrates the SSF. For example,
the following English sentence,
Children are watching some programmes on television in the house. -- (1)
The representation for the above sentence is shown in SSF in Fig. 4. As shown in this figure, each line
represents a word/token or a group (except for lines with ?))? which only indicate the end of a group).
For each group, the symbol used is ?((?. Each word or group has 3 parts. The first part stores the tree
address of each word or group, and is for human readability only. The word or group is in the second
part, with part of speech tag or group/phrase category in the third part.
Address Token Category Attribute-value pairs
-----------------------------------------------
1 (( NP
1.1 children NNS <fs af=child,n,m,p,3,0,,>
))
2 (( VG
2.1 are VBP <fs af=be,v,m,p,3,0,,>
2.2 watching VBG <fs af=?watch,v,m,s,3,0,,? aspect=PROG>
))
3 (( NP
3.1 some DT <fs af=some,det,m,s,3,0,,>
3.2 programmes NNS <fs af=programme,n,m,p,3,0,,>
))
4 (( PP
4.1 on IN <fs af=on,p,m,s,3,0,,>
4.1.1 (( NP
4.1.2 television NN <fs af=television,n,m,s,3,0,,>
))
))
5 (( PP
5.1 in IN <fs af=in,p,m,s,3,0,,>
5.2 (( NP
5.2.1 the DT <fs af=the,det,m,s,3,0,,>
5.2.2 house NN <fs af=house,n,m,s,3,0,,>
))
))
-----------------------------------------------
Figure 4: Shakti Standard Format
69
The example below shows the SSF for the first noun phrase where feature information is also shown, as
the fourth part on each line. Some frequently occurring attributes (such as root, cat, gend, etc.) may be
abbreviated using a special attribute called ?af? or abbreviated attributes, as follows:
1 (( NP
1.1 children NNS <fs af=?child,n,m,p,3,0,,? >
| | | | | |
| | | | | \
root | | |pers |
| | | case
category | number
|
gender
The field for each attribute is at a fixed position, and a comma is used as a separater. Thus, in case no
value is given for a particular attribute, the field is left blank, e.g. last two fields in the above example.
Corresponding to the above SSF text stream, an in-memory data structure may be created using the
APIs. (However, note that value of the property Address is not stored in the in-memory data structure
explicitly. It is for human reference and readability only, and is computed when needed. A unique name,
however can be assigned to a node and saved in the memory, as mentioned later.)
There are two types of attributes: user defined or system defined. The convention that is used is that
a user defined attribute should not have an underscore at the end. System attribute may have a single
underscore at its end.
Values are of two types: simple and structured. Simple values are represented by alphanumeric strings,
with a possible underscore. Structured values have progressively more refined values separated by double
underscores. For example, if a value is:
vmod__varg__k1
it shows the value as ?vmod? (modifier of a verb), which is further refined as ?varg? (argument of the
verb) of type ?k1? (karta karaka).
3.3 Interlinking of Nodes
Nodes might be interlinked with each other through directed edges. Usually, these edges have nothing
to do with phrase structure tree, and are concerned with dependency structure, thematic structure, etc.
These are specified using the attribute value syntax, however, they do not specify a property for a node,
rather a relation between two nodes.
For example, if a node is karta karaka of another node named ?play1? in the dependency structure (in
other words, if there is a directed edge from the latter to the former) it can be represented as follows:
1 children NN < fs drel =
?
k1 : play1
?
>
2 played VB < fs name = play1 >
The above says that there is an edge labelled with ?k1? from ?played? to ?children? in the ?drel? tree
(dependency relation tree). The node with token ?played? is named as ?play1? using a special attribute
called ?name?.
So the syntax is as follows: if you associate an arc with a node C as follows:
<treename>=<edgelabel>:<nodename>
it means that there is an edge from < nodename > to C, and the edge is labelled with < edgelabel >.
Name of a node may be declared with the attribute ?name?:
name=<nodename>
3.4 Cross Linking across Sentences
There is a need to relate elements across sentences. A common case is that of co-reference of pronouns.
For example, in the following sentences:
Sita saw Ram in the house. He had come all by himself. -- (2)
70
the pronoun ?he? in the second sentence refers to the same person as referred to by ?Ram?. Similarly
?himself? refers to same person as ?he? refers to. This is show by means of a co-reference link from ?he?
to ?Ram?, and from ?himself? to ?he?. SSF allows such cross-links to be marked.
The above text of two sentences is shown in SSF below.
<document docid="gandhi-324" docnumber="2">
<header> ... </header>
<body>
<tb>
<sentence num=1>
...
2 Ram <fs name=R>
...
</sentence>
<sentence num=2>
1 He <fs coref="..%R" name=he>
...
6 himself <fs coref=he>
7 .
</sentence>
</tb>
Note that ?himself? in sentence 2 co-refers to ?he? in the same sentence. This is shown using attribute
?coref? and value ?he?. To show co-reference across sentences, a notation is used with ?%?. It is explained
next.
Name labels are defined at the level of a sentence: Scope of any name label is a sentence. It should be
unique within a sentence, and can be referred to within the sentence by using it directly.
To refer to a name label in another sentence in the same text block (paragraph), path has to be specified:
..%R
To refer to a name label R in a sentence in another text block numbered 3, refer to it as:
..%..%3%1%R
4 Shakti Natural Language Analyzer
Shakti Analyzer has been designed for analyzing natural languages. Originally, it was available for
analyzing English as part of the Shakti
5
English-Hindi machine translation system. It has now been
extended for analyzing a number of Indian languages as mentioned later (Section-6.1).
The Shakti Analyzer can incorporate new modules as black boxes or as open-source software. The
simplicity of the overall architecture makes it easy to do so. Different available English parsers have
been extensively adapted, and the version used by Shakti system runs using Collins parser.
Shakti analyzer combines rule-based approach with statistical approach. The SSF representation is
designed to keep both kinds of information. The rules are mostly linguistic in nature, and the statistical
approach tries to infer or use linguistic information. For example, statistical POS tagger tries to infer
linguistic (part-of-speech) tags, whereas WSD module uses grammatical relations together with statistics
to disambiguate the word sense.
The system has a number of innovative design principles which are described below.
4.1 System Organization Principles
A number of system organization principles have been used which have led to the rapid development of
the system. While the principles by themselves might not appear to be new, their application is perhaps
new.
4.1.1 Modularity
The system consists of a large number of modules, each one of which typically performs a small logical
task. This allows the overall machine translation task to be broken up into a large number of small sub-
tasks, each of which can be accomplished separately. Currently the system (as used in the Shakti system)
5
http:/shakti.iiit.ac.in
71
has 69 different modules. About 9 modules are used for analyzing the source language (English), 24
modules are used for performing bilingual tasks such as substituting target language roots and reordering
etc., and the remaining modules are used for generating target language.
4.1.2 Simplicity of Organization
The overall system architecture is kept extremely simple. All modules operate on data in SSF . They
communicate with each other via SSF.
The attribute value pairs allow features or properties to be specified with every node. Relations of
different types across nodes can also be specified using an attribute-value like representation. The repre-
sentation is specially designed to allow different levels and kinds of linguistic analyses to be stored. The
developer uses APIs to store or access information regarding structure of trees and attribute value pairs.
4.1.3 Designed to Deal with Failure
NLP analysis modules are known to have limited coverage. They are not always able to produce an out-
put. They fail to produce output either because of limits of the best known algorithms or incompleteness
of data or rules. For example, a sentential parser might fail to parse either because it does not know how
to deal with a construction or because a dictionary entry is missing. Similarly, a chunker or part of speech
tagger might fail, at times, to produce an analysis. The system is designed to deal with failure at every
step in the pipeline. This is facilitated by a common representation for the outputs of the POS tagger,
chunker and parser (all in SSF). The downstream modules continue to operate on the data stream, albeit
less effectively, when a more detailed analysis is not available. (If all modules were to fail, a default rule
of no-reordering and dictionary lookup would still be applied.)
As another example, if the word sense disambiguation (WSD) module fails to identify the sense of a
word in the input sentence, it does not put in the sense feature for the word. This only means that the
module which substitutes the target language root from the available equivalents from dictionary, will
use a default rule for selecting the sense because the detailed WSD was not successful (say, due to lack
of training data).
The SSF is designed to represent partial information, routinely. Appropriate modules know what to
do when their desired information is available and use defaults when it is not available. In fact, for many
modules, there are not just two but several levels at which they operate, depending on availability of
information corresponding to that level. Each level represents a graceful degradation of output quality.
The above flexibility is achieved by using two kinds of representation: constituent level representation
and feature-structure level representation. The former is used to store phrase level analysis (and partial
parse etc.) and the latter for outputs of many kinds of other tasks such as WSD, TAM computation, case
computation, dependency relations, etc.
4.1.4 Transparency for Developers
An extremely important characteristic for the successful development of complex software such as a ma-
chine translation system is to expose the input and output produced by every module. This transparency
becomes even more important in a research environment where new ideas are constantly being tried with
a high turnover of student developers.
In the Shakti system, unprecedented transparency is achieved by using a highly readable textual nota-
tion for the SSF, and requiring every module to produce output in this format. In fact, the textual SSF
output of a module is not only for the human consumption, but is used by the subsequent module in
the data stream as its input. This ensures that no part of the resulting analysis is left hidden in some
global variables; all analysis is represented in readable SSF (otherwise it is not processed at all by the
subsequent modules).
Experience has shown that this methodology has made debugging as well as the development of the
system convenient for programmers and linguists alike. In case an output is not as expected, one can
quickly find out which module went wrong (that is, which module did not function as expected). In fact,
linguists are using the system quite effectively to debug their linguistic data with ease.
72
5 Implementations
A considerable repository of implementations (in code) has evolved around SSF and the analyzer. In this
section we consider two of the kinds of implementations that have accumulated so far.
5.1 SSF API
Application Programming Interfaces (APIs) have been implemented in multiple programming languages
to allow programmers to transparently operate on any data stored in SSF. Of these, the better designed
APIs, such as those in Perl and Java, allow all kinds of operations to be performed on the SSF data.
These operation include basic operations such as reading, writing and modifying the data, as well as
for advanced operations such as search and bulk transformation of the data. The Java API is a part of
Sanchay
6
, which is a collection of tools and APIs for language processing, specially tailored for the
needs of Indian languages which were not (till very recently) well supported on computers and operating
systems.
The availability of decently designed APIs for SSF allow programmers to use SSF for arbitrary pur-
poses. And they have used it successfully to build natural language systems and tools as described below.
5.2 Sanchay Corpus Query Language
Trees have a quite constrained structure, whereas graphs have somewhat anarchic structure. Threaded
trees (Ait-Mokhtar et al., 2002; Larchevelque, 2002) provided a middle ground between the two. They
start with trees as the core structure, but they allow constrained links between the nodes of a tree that a
pure tree would not allow. This overlaying of constrained links over the core trees allows multiple layers
and/or types of annotation to be stored in the same structure. With a little more improvisation, we can
even have links across sentences, i.e., at the discourse level (see section-3.3). It is possible, for example,
to have a phrase structure tree (the core tree) overlaid with a dependency tree (via constrained links or
?threads?), just as it is possible to have POS tagged and chunked data to be overlaid with named entities
and discourse relations.
The Sanchay Corpus Query Language (SCQL) (Singh, 2012) is a query language designed for
threaded trees. It so turns out that SSF is also a representation that can be viewed as threaded trees.
Thus, the SCQL can work over data in SSF. This language has a simple, intuitive and concise syntax and
high expressive power. It allows not only to search for complicated patterns with short queries but also
allows data manipulation and specification of arbitrary return values. Many of the commonly used tasks
that otherwise require writing programs, can be performed with one or more queries.
6 Applications
6.1 Sampark Machine Translation Architecture
Overcoming the language barrier in the Indian sub-continent is a very challenging task
7
. Sampark
8
is
an effort in this direction. Sampark has been developed as part of the consortium project called Indian
Language to India Language Machine translation (ILMT) funded by TDIL program of Department of In-
formation Technology, Government of India. Work on this project is contributed to by 11 major research
centres across India working on Natural Language Processing.
Sampark, or the ILMT project, has developed language technology for 9 Indian languages resulting
in MT for 18 language pairs. These are: 14 bi-directional systems between Hindi and Urdu / Punjabi /
Telugu / Bengali / Tamil / Marathi / Kannada and 4 bi-directional systems between Tamil and Malayalam
/ Telugu. Out of these, 8 pairs have been exposed via a web interface. A REST API is also available to
acess the machine translation system over the Internet.
6
http://sanchay.co.in
7
There are 22 constitutionally recognized languages in India, and many more which are not recognized. Hindi, Bengali,
Telugu, Marathi, Tamil and Urdu are among the major languages of the world in terms of number of speakers, summing up to
a total of 850 million.
8
http://sampark.org.in
73
The Sampark system uses Computational Paninian Grammar (CPG) (Bharati et al., 1995), in combina-
tion with machine learning. Thus, it is a hybrid system using both rule-based and statistical approaches.
There are 13 major modules that together form a hybrid system. The machine translation system is based
on the analyze-transfer-generate paradigm. It starts with an analysis of the source language sentence.
Then a transfer of structure and vocabulary to target language is carried out. Finally the target language
is generated. One of the benefits of this approach is that the language analyzer for a particular language
can be developed once and then be combined with generators for other languages, making it easier to
build a machine translation system for new pairs of languages.
Indian languages have a lot of similarities in grammatical structures, so only shallow parsing was found
to be adequate for the purposes of building a machine translation system. Transfer grammar component
has also been kept simple. Domain dictionaries are used to cover domain specific aspects.
At the core of the Sampark architecture is an enhanced version of the Shakti Natural Language Ana-
lyzer. The individual modules may, of course, be different for different language pairs, but the pipelined
architecture bears close resemblance to the Shakti machine translation system. And it uses the Shakti
Standard Format as the blackboard (Erman et al., 1980) on which the different modules (POS taggers,
chunkers, named entity recognzier, transfer grammar module etc.) operate, that is, read from and write
to. SSF thus becomes the glue that ties together all the modules in all the MT systems for the various
language pairs. The modules are not only written in different programming languages, some of them are
rule-based, whereas others are statistical.
The use of SSF as the underlying default representation helps to control the complexity of the overall
system. It also helps to achieve unprecedented transparency for input and output for every module.
Readability of SSF helps in development and debugging because the input and output of any module
can be easily seen and read by humans, whether linguists or programmers. Even if a module fails, SSF
helps to run the modules without any effect on normal operation of system. In such a case, the output
SSF would have unfilled value of an attribute and downstream modules continue to operate on the data
stream.
6.2 Annotation Interfaces and Other Tools
Sanchay, mentioned above, has a syntactic annotation interface that has been used for development of
treebanks for Indian languages (Begum et al., 2008). These treebanks have been one of the primary
sources of information for the development the Sampark machine translation systems, among other
things. This syntactic annotation interface provides facilities for everything that is required to be done
to transform the selected data in the raw text format to the final annotated treebank. The usual stages of
annotation include POS tagging, morphological annotation, chunking and dependency annotation. This
interface has evolved over a period of several years based on the feedback received from the annotators
and other users. There are plans to use the interface for similar annotation for even more languages.
The underlying default format used in the above interface is SSF. The advantages of using SSF for this
purpose are similar to those mentioned earlier for purposes such as building machine translation systems.
The complete process of annotation required to build a full-fledged treebank is complicated and there are
numerous issues that have to be taken care of. The blackboard-like nature of SSF allows for a smooth
shifts between different stages of annotation, even going back to an earlier stage, if necessary, to correct
mistakes. It allows all the annotation information to be situated in one contiguous place.
The interface uses the Java API for SSF, which is perhaps the most developed among the different
APIs for SSF. The API (a part of Sanchay) again allows transparency for the programmer as far as
manipulating the data is concerned. It also ensures that there are fewer bugs when new programmers
work on any part of the system where SSF data is being used. One recent addition to the interface was a
GUI to correct mistakes in treebanks (Agarwal et al., 2012).
The syntactic annotation interface is not the only interface in Sanchay that uses SSF. Some other
interfaces do that too. For example, there are sentence alignment and word alignment interfaces, which
also use the same format for similar reasons. Thus, it is even possible to build parallel treebanks in SSF
using the Sanchay interfaces.
74
Then there are other tools in Sanchay such as the integrated tool for accessing language re-
sources (Singh and Ambati, 2010). This tool allows various kinds of language resources, including those
in SSF, to be accessed, searched and manipulated through the inter-connected annotation interfaces and
the SSF API. There is also a text editor in Sanchay that is specially tailored for Indian languages and it
can validate SSF (Singh, 2008).
The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can
be used for data in SSF is another big facilitator for anyone who wants to build new tools for language
processing and wants to operate on linguistic data.
Apart from these, a number of research projects have used SSF (the representation or the analyzer)
directly or indirectly, that is, either for theoretical frameworks or as part of the implementation (Bharati
et al., 2009; Gadde et al., 2010; Husain et al., 2011).
7 Conclusion
We described a readable representation scheme called Shakti Standard Format (SSF). We showed how
this scheme (an instance of the blackboard architectural model), which is based on certain organizational
principles such as modularity, simplicity, robustness and transparency, can be used to create not only
a linguistic analysis engine (Shakti Natural Language Analyzer), but can be used for arbitrary other
purposes wherever linguistic analysis is one of the tasks. We briefly described the machine translation
systems (Shakti and Sampark) which use this scheme at their core level. Similarly, we described how
it can be used for creation of language resources (such as treebanks) and the annotation interfaces used
to create these resources. It has also figured in several research projects so far. We mentioned one
query language (Sanchay Corpus Query Language) that operates on this representation scheme and has
been integrated with the annotation interfaces. Overall, the representation scheme has been successful at
building infrastructure for language technology over the last more than a decade. The scheme is theory
neutral and can be used for both phrase structure grammar and for dependency grammar.
References
Rahul Agarwal, Bharat Ram Ambati, and Anil Kumar Singh. 2012. A GUI to Detect and Correct Errors in Hindi
Dependency Treebank. In Proceedings of the Eighth International Conference on Language Resources and
Evaluation (LREC), Instanbul, Turkey. ELRA.
S. Ait-Mokhtar, J.P. Chanod, and C. Roux. 2002. Robustness beyond shallowness: incremental deep parsing.
Natural Language Engineering, 8(2-3):121144, January.
Alessandro Di Bari, Alessandro Faraotti, Carmela Gambardella, and Guido Vetere. 2013. A Model-driven ap-
proach to NLP programming with UIMA. In UIMA@GSCL, pages 2?9.
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. Depen-
dency Annotation Scheme for Indian Languages. In Proceedings of The Third International Joint Conference
on Natural Language Processing (IJCNLP), Hyderabad, India.
Ashkar Bharati, Vineet Chaitanya, and Rajeev Sangal. 1995. Natural Language Processing: A Paninian Perspec-
tive. Prentice-Hall of India Pvt. Ltd.
Akshar Bharati, Samar Husain, Phani Gadde, Bharat Ambati, Dipti M Sharma, and Rajeev Sangal. 2009. A
Modular Cascaded Approach to Complete Parsing. In Proceedings of the COLIPS International Conference on
Asian Language Processing 2009 (IALP), Singapore.
Steven Bird. 2002. NLTK: The Natural Language Toolkit. In In Proceedings of the ACL Workshop on Effective
Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadel-
phia: Association for Computational Linguistics.
Noam Chomsky. 1957. Syntactic Structures. The Hague/Paris: Mouton.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan, Niraj Aswani, Ian Roberts, Genevieve
Gorrell, Adam Funk, Angus Roberts, Danica Damljanovic, Thomas Heitz, Mark A. Greenwood, Horacio Sag-
gion, Johann Petrak, Yaoyong Li, and Wim Peters. 2011. Text Processing with GATE (Version 6).
75
Lee D. Erman, Frederick Hayes-Roth, Victor R. Lesser, and D. Raj Reddy. 1980. The Hearsay-II Speech-
Understanding System: Integrating Knowledge to Resolve Uncertainty. ACM Comput. Surv., 12(2):213?253,
June.
D. Ferrucci and A. Lally. 2004. UIMA: an architectural approach to unstructured information processing in the
corporate research environment. Natural Language Engineering, 10(3-4):327?348.
Phani Gadde, Karan Jindal, Samar Husain, Dipti Misra Sharma, and Rajeev Sangal. 2010. Improving Data
Driven Dependency Parsing using Clausal Information. In Proceedings of 11th Annual Conference of the North
American Chapter of the Association for Computational Linguistics (NAACL-HLT), Los Angeles.
Samar Husain, Phani Gadde, Joakim Nivre, and Rajeev Sangal. 2011. Clausal Parsing Helps Data-driven De-
pendency Parsing: Experiments with Hindi. In Proceedings of Fifth International Joint Conference on Natural
Language Processing (IJCNLP), Thailand.
J.M. Larchevelque. 2002. Optimal Incremental Parsing. ACM Transactions on Programing Languages and
Systems, 17(1):115, January.
Yaoyong Li, Kalina Bontcheva, and Hamish Cunningham. 2009. Adapting SVM for Data Sparseness and Imbal-
ance: A Case Study on Information Extraction. Natural Language Engineering, 15(2):241?271.
Tae-Gil Noh and Sebastian Pad?o. 2013. Using UIMA to Structure An Open Platform for Textual Entailment. In
UIMA@GSCL, pages 26?33.
Anil Kumar Singh and Bharat Ambati. 2010. An Integrated Digital Tool for Accessing Language Resources. In
Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC), Malta.
ELRA.
Anil Kumar Singh. 2008. A Mechanism to Provide Language-Encoding Support and an NLP Friendly Editor. In
Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP), Hyder-
abad, India. AFNLP.
Anil Kumar Singh. 2012. A Concise Query Language with Search and Transform Operations for Corpora with
Multiple Levels of Annotation. In Proceedings of the Eighth International Conference on Language Resources
and Evaluation (LREC), Instanbul, Turkey. ELRA.
L. Tesniere. 1959. Elements de syntaxe structurale. Paris: Klincksieck.
76
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 21?29,
Dublin, Ireland, August 24th 2014.
Exploring the effects of Sentence Simplification on Hindi to English
Machine Translation System
Kshitij Mishra Ankush Soni Rahul Sharma Dipti Misra Sharma
Language Technologies Research Centre
IIIT Hyderabad
{kshitij.mishra,ankush.soni,rahul.sharma}@research.iiit.ac.in,
dipti@iiit.ac.in
Abstract
Even though, a lot of research has already been done on Machine Translation, translating com-
plex sentences has been a stumbling block in the process. To improve the performance of ma-
chine translation on complex sentences, simplifying the sentences becomes imperative. In this
paper, we present a rule based approach to address this problem by simplifying complex sen-
tences in Hindi into multiple simple sentences. The sentence is split using clause boundaries and
dependency parsing which identifies different arguments of verbs, thus changing the grammatical
structure in a way that the semantic information of the original sentence stay preserved.
1 Introduction
Cognitive and psychological studies on ?human reading? state that the effort in reading and understand-
ing a text increases with the sentence complexity. Sentence complexity can be primarily classified
into ?lexical complexity? and ?syntactic complexity?. Lexical complexity deals with the vocabulary
practiced in the sentence while syntactic complexity is governed by the linguistic competence of
native speakers of a particular language. In this respect, the modern machine translation systems are
similar to humans. Processing complex sentences with high accuracy has always been a challenge in
machine translation. This calls for automatic techniques aiming at simplification of complex sentences
both lexically and syntactically. In context of natural language applications, lexical complexity can
be handled significantly by utilizing various resources like lexicons, dictionary, thesaurus etc. and
substituting infrequent words with their frequent counterparts. However, syntactic complexity requires
mature endeavors and techniques.
Machine Translation systems when dealing with highly diverges language pairs face difficulty in trans-
lation. It seems intuitive to break down the sentence into simplified sentences and use them for the task.
Phrase based translation systems exercise a similar approach where system divides the sentences into
phrases and translates each phrase independently, later reordering and concatenating them into a single
sentence. However, the focus of translation is not on producing a single sentence but to preserve the
semantics of the source sentence, with a decent readability at the target side.
We present a rule based approach which is basically an improvement on the work done by (Soni et al.,
2013) for sentence simplification in Hindi. The approach adapted by them has some limitations since it
uses verb frames to extract the core arguments of verb; there is no way to identify information like time,
place, manner etc. of the event expressed by the verb which could be crucial for sentence simplification.
A parse tree of a sentence could potentially address this problem. We use a dependency parser of Hindi
for this purpose. (Soni et al., 2013) didn?t consider breaking the sentences at finite verbs while we split
the sentences on finite verbs also.
This paper is structured as follows: In Section 2, we discuss the related work that has been done earlier
on sentence simplification. Section 3 addresses criteria for classification of complex sentences. In section
4, we discuss the algorithm used for splitting the sentences. Section 5 outlines evaluation of the systems
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
21
using both BLEU scores and human readability . In Section 6, we conclude and talk about future work
in this area.
2 Related Work
Siddharthan (2002) presents a three stage pipelined approach for text simplification. He has also looked
into the discourse level problems arising from syntactic text simplification and proposed solutions to
overcome them. In his later works (Siddharthan, 2006), he discussed syntactic simplification of sen-
tences. He has formulated the interactions between discourse and syntax during the process of sentence
simplification. Chandrasekar et al. (1996) proposed Finite state grammar and Dependency based ap-
proach for sentence simplification. They first build a stuctural representation of the sentence and then
apply a sequence of rules for extracting the elements that could be simplified. Chandrasekar and Srinivas
(1997) have put forward an approach to automatically induce rules for sentence simplification. In their
approach all the dependency information of a words is localized to a single structure which provides a
local domain of influence to the induced rules.
Sudoh et al. (2010) proposed divide and translate technique to address the issue of long distance re-
ordering for machine translation. They have used clauses as segments for splitting. In their approach,
clauses are translated separately with non-terminals using SMT method and then sentences are recon-
structed based on the non-terminals. Doi and Sumita (2003) used splitting techniques for simplifying
sentences and then utilizing the output for machine translation. Leffa (1998) has shown that simplifying
a sentence into clauses can help machine translation. They have built a rule based clause identifier to
enhance the performance of MT system.
Though the field of sentence simplification has been explored for enhancing machine translation for
English as source language, we don?t find significant work for Hindi. Poornima et al. (2011) has reported
a rule based technique to simplify complex sentences based on connectives like subordinating conjunc-
tion, relative pronouns etc. The MT system used by them performs better for simplified sentences as
compared to original complex sentences.
3 Complex Sentence
In this section we try to identify the definition of sentence complexity in the context of machine trans-
lation. In general, complex sentences have more than one clause (Kachru, 2006) and these clauses are
combined using connectives. In the context of machine translation, the performance of system generally
decreases with increase in the length of the sentence (Chandrasekar et al., 1996). Soni et al. (2013) has
also mentioned that the number of verb chunks increases with the length of sentence. They have also
mentioned the criteria for defining complexity of a sentence and the same criteria is apt for our purpose
also. We consider a sentence to be complex based on the following criteria:
? Criterion1 : Length of the sentence is greater than 5.
? Criterion2 : Number of verb chunks in the sentence is more than 1.
? Criterion3 : Number of conjuncts in the sentence is greater than 0.
Table 1 shows classification of a sentence based on the possible combinations of 3 criteria mentioned
above.
4 Sentence Simplification Algorithm
We propose a rule based system for sentence simplification, which first identifies the clause boundaries
in the input sentence, and then splits the sentence using those clause boundaries. Once different clauses
are identified, they are further processed to find shared argument for non-finite verbs. Then, the Tense-
Aspect-Modality(TAM) information of the non-finite verbs is changed. Below example (12) illustrates
the same,
22
Table 1: Classification of a sentence as simple or complex
Criterion1 Criterion2 Criterion3 Category
No No No Simple
No No Yes Simple
No Yes No Simple
No Yes Yes Simple
Yes No No Simple
Yes No Yes Complex
Yes Yes No Complex
Yes Yes Yes Complex
(1) raam
Ram
ne khaanaa
food
khaakara
after+eating
pani
water
piya
drink+past
?Ram drank water after eating.?
We first mark the boundaries of clauses for example (12). ?raam? and ?khaanaa? are starts, and ?khaakara?
and ?piya? are ends of two different clauses respectively. Once the start and end of clauses are identified
we break the sentence into those clauses. So for above example, the two clauses are:
1. ?raam ne pani piya?
2. ?khaanaa khaakara?
Once we have the clauses, we post process those clauses which contain non-finite verbs, and add the
shared argument and TAM information for these non-finite clauses. After post-processing, the two
simplified clauses are:
1. ?raam ne pani piya.?
2. ?raam ne khaanaa khaayaa.?
4.1 Algorithm
Our system comprises of a pipeline incorporating various modules. The first module determines the
boundaries of clauses (clause identification) and splits the sentence on the basis of those boundaries.
Then, the clauses are processed by a gerund handler - which finds the arguments of gerunds, shared
argument adder which fetches the shared arguments between verbs, TAM(Tense Aspect Modality)
generator which changes the TAM of other verbs on the basis of main verb. The figure 4.1 shows the
data flow of our system, components of which have been discussed in further detail in this section.
23
Input
Sentence
Preprocessing
Clause boundary
identification
and splitting
of sentences
Gerunds Handler
Shared
Argument
Adder
TAM
generator
Output
Figure 1: Data Flow
4.1.1 Preprocessing
In this module, raw input sentences are processed and each lexical item is assigned a POS tag, chunk and
dependency relations information in SSF format(Bharati et al., 2007; Bharati et al., 2009). We have used
(Jain et al., 2012) dependency parser for preprocessing. Example (2) shows the output of this step.
Input sentence:
(2) raam ne
Ram+erg
khaanaa
food
khaayaa
eat+past
aur
and
paani
water
piyaa.
drink+past
?Raam ate food and drank water?
Output: Figure (1) shows the different linguistic information in SSF format. Tag contains the Chunk
and POS information of the sentence, and drel in feature structure stores different dependency relations
in a sentence.
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k1:VGF?>
1.1 raama NNP <fs af=?raama,n,m,sg,3,d,0,0?>
1.2 ne PSP <fs af=?ne,psp,,,,,,?>
))
1 2 (( NP <fs name=?NP2? drel=?k2:VGF?>
2.1 khaanaa NN <fs af=?khaanaa,n,m,sg,3,d,0,0? name=?khaanaa?>
))
3 (( VGF <fs name=?VGF? drel=?ccof:CCP?>
3.1 khaayaa VM <fs af=?KA,v,m,sg,any,,yA,yA? name=?khaayaa?>
))
4 (( CCP <fs name=?CCP?>
4.1 aur CC <fs af=?Ora,avy,,,,,,? name=?aur?>
))
5 (( NP <fs name=?NP3? drel=?k2:VGF2?>
5.1 paani NN <fs af=?pAnI,n,m,sg,3,d,0,0? name=?paani?>
))
6 (( VGF <fs name=?VGF2? drel=?ccof:CCP?>
6.1 piyaa VM <fs af=?pIyA,unk,,,,,,? name=?piyaa?>
))
Figure 1: SSF representation for example 2
24
4.1.2 Clause boundary Identification and splitting of sentences
This module takes the input from preprocessing module and identifies the clause boundaries in the sen-
tence. Once clause boundaries are identified, the sentence is divided into different clauses. We have
used the technique mentioned in Sharma et al. (2013) which has shown how implicit clause information
present in dependency trees/relations can be used to extract clauses from a sentence. Once we mark the
clause boundaries using this approach, we break the sentence into different simple clauses along those
clause boundaries. The example(3) given below illustrates the same.
(3) raam
Ram
jisne
who+rel.
khaanaa
food
khaayaa
eat+past
ghar
home
gayaa
go+past
?Ram who ate food, went home?
Example(3) with clause boundaries marked is, ( raam ( jisne khaanaa khaayaa ) ghar gayaa). Once
the clause boundaries are marked, we break the sentence using those boundaries. So for Example(3),
split clauses are,
1. raam ghar gayaa.
2. jisne khaanaa khaayaa.
4.1.3 Gerunds Handler
Since, Sharma et al. (2013) identifies clause boundary for non-finite and finite verb only, gerunds are not
handled in the previous module. This module is used to handle gerunds in the given sentence. In this
module, the gerund chunks are first indentified and then further processed after getting the arguments.
Consider an example:
(4) logon ko
people
sambodhit
address
karne ke baad
doing after
dono
both
netaon ne
leaders
pradhanmantri
Prime minister
ko
to
istifa
resignation
saunpa
gave
?After addressing people, both leaders gave resignation to the prime minister?
In the above example, the clause boundary identifier module marks the entire sentence as a clause but
karne ke baad is a gerund chunk (verb chunk) here, which is marked as VGNN according to the tagset
of the POS tagger used. According to definition of complex sentence given in section 3 gerunds also
introduce complexity in a sentence. Therefore, in order to simplify such sentences, we use dependency
parsing information for extracting the arguments of gerund and splitting the sentence.
Here logon ko and sambodhit are the arguments of verb chunk karne ke baad. Here ke baad is postpo-
sition of verb karne so, ke baad is splitted from karne and it has been used with the pronoun is to make
the sentence more readable.
1. logon ko sambodhit karne
2. iske baad dono netaon ne pradhanmantri ko istifa saunpa
4.1.4 Shared Argument Adder
After identifying clauses and handling gerunds, the shared arguments are identified between the verbs
and sentences are formed accordingly. For example:
(5) (ram
(ram
(chai
(tea
aur
and
paani
water
peekar)
after drinking)
soyaa)
slept)
?ram slept after drinking tea and water?
25
Here ram is the shared argument(k1-karta) of both the verbs peekar and soyaa . The dependency
parser used, marks the inverse dependencies for shared arguments which helps in . So the output of this
module is:
1. ram chai aur paani peekar.
2. ram soyaa.
4.1.5 TAM generator
The split sentences given by the above module are converted into more readable sentences using this
module. The form of other verbs is changed using TAM information of the main verb provided by the
morph, as shown in Figure 1. For example:
INPUT:
1. ram chai aur paani peekar.
2. ram soyaa.
OUTPUT:
1. ram chai aur paani peeyaa.
2. ram soyaa.
Here soyaa is the main verb having yaa as TAM. Word generator
1
has been used to generate the
final verb given root form of the verb and TAM of the verb. Here pee is the root form of peekar and
yaa is given as the TAM. Word generator generates peeyaa as the final word which is used in the sentence.
5 Evaluation
We have taken a corpus of 100 complex sentences for the evaluation of our tool. These sentences
were taken from the Hindi treebank (Bhatt et al., 2009; Palmer et al., 2009). Evaluation of both sen-
tence simplification and its effects on google MT system for Hindi to English(google translate) was
performed. The evaluation of sentence simplification is a subjective task which considers both readabil-
ity and preservation of semantic information. Hence both manual as well as automatic evaluations have
been performed.
5.1 Automatic Evaluation
We have used BLEU score (Papineni et al., 2002) for automatic evaluation of both tasks; sentence sim-
plification and enhancing MT system. Higher the BLEU score, closer the target set is to the reference set.
The maximum attainable value is 1 while minimum possible value is 0. For our Automatic evaluation
we adopted the same technique as Specia (2010) using BLEU metric. We have achieved 0.6949 BLEU
score for sentence simplification task. For MT system, we have evaluated the system with and without
sentence simplification tool. It was observed that the system with sentence simplification tool achieved
0.4986 BLEU score whereas the system without sentence simplification gave BLEU score of 0.4541.
5.2 Human Evaluation
To ensure the simplification quality, manual evaluation was also done. 20 sentences were randomly
selected from the testing data-set of 100 sentences. Output of these 20 sentences, from the target set were
manually evaluated by 2 subjects, who have done basic courses in linguistics, for judging ?Readability?
and ?Simplification? quality on the scale of 0? 3, 0 being the worst and 3 being the best.
For Simplification performance, scores were given according to following criteria :
1
Taken from the ILMT pipeline.
26
? 0 = None of the expected simplifications performed.
? 1 = Some of the expected simplifications performed.
? 2 = Most of the expected simplifications performed.
? 3 = Complete Simplification.
After taking input from all the participants the results averaged out to be 2.5.
For Human evaluation of MT system, the subjects had to select the better translation between system with
sentence simplification tool and system without it. The subjects reportedly observed a better translation
of the system with sentence simplification tool. It was reported that 12 out of 20 sentences were translated
better after being simplified, and quality of 3 remained unchanged.
Translation quality of 5 was reported to be better before simplification. This happened because the
system breaks the sentences at every verb chunk it encounters, which in some cases makes the sentence
lose its semantic information.
For example the sentence below contains five verb chunks. The system breaks the sentence into five
sentences.:
(6) yah
this
poochne
ask
par
on
ki
that
kya
what
we
he
dobaara
again
congress
Congress
mein
in
lautenge
return
sangama
Sangama
ne kaha
told
ki
that
na
neither
to
then
iski
its
zarurat
requirement
hai
is
aur
and
na
nor
hi peeche
back
lautane
return
ka sawal
question
hi uthta
raises
hai
is
?On asking whether he would return again in Congress, Sangma replied that neither there is need
of this nor there is the question of reverting back.?
System?s Output
1. (7) kya
what
we
he
dobaara
again
congress
Congress
mein
in
lautenge
return
?Would they return again in Congress ??
2. (8) yah
this
poochane
ask
par
on
sangama
Sangama
ne kaha
told
?On asking this, Sangama said.?
3. (9) na
neither
to iski
its
zarurat
requirement
hai
is told
?Neither it is needed.?
4. (10) na
neither
hee peeche
back
lautana
return
hai
is
?Neither he will return.?
5. (11) iska
its
sawal
question
uthta
raises
hai
is
?The question arises.?
It is clearly observable that the simplified sentences failed to preserve the meaning of the original sen-
tence. Further, the system does not change the vibhakti (Bharati et al., 1995) of the simplified sentences
which, in some cases makes the sentence lose its meaning. For example
(12) machharon
Mosquitoes
ke
of
katne
bite
ke
of
baad
after
wo
they
beemar
sick
hue
became
?They became sick after being bitten by the mosquitoes.?
System?s Output:
27
1. (13) machharon
Mosquitoes
ke
of
kata
bite
?Not a valid sentence?
2. (14) is
this
ke
of
baad
after
wo
they
beemar
sick
hue
became
?After this they became sick.?
In the first simplified sentence the vibhakti ?ke? should have been changed to ?ne? for the formation of
a valid sentence.
6 Conclusion and Future Work
As shown in the results, after simplifying the sentences, BLEU score of the translation increases by 4.45.
The manual evaluation also got encouraging results in simplification and readability with a score of 2.5 on
a scale of 0?3. There is a clear indication that our tool can enhance the performance of MT for complex
sentences by simplifying them. Future work will include minimizing the lose of semantic information
while splitting the sentences and making simplified sentences more readable and grammatically correct.
In addition to extending the system, evaluating the impact of our tool on other NLP tasks like parsing,
dialog systems, summarisation, question-answering systems etc. is also a future goal.
Acknowledgements
We would like to thank Riyaz Ahmad Bhat, Rishabh Shrivastava and Prateek Saxena for their useful
comments and feedback which helped us to improve this paper and Anshul Bhargava, Arpita Batra,
Abhijat Sharma, Gaurav Kakkar, Jyoti Jha and Urmi Ghosh for helping us in annotation.
References
Akshar Bharati, Vineet Chaitanya, Rajeev Sangal, and KV Ramakrishnamacharyulu. 1995. Natural language
processing: a Paninian perspective. Prentice-Hall of India New Delhi.
Akshar Bharati, Rajeev Sangal, and Dipti M Sharma. 2007. Ssf: Shakti standard format guide. pages 1?25.
Akshara Bharati, Dipti Misra Sharma, Samar Husain, Lakshmi Bai, Rafiya Begam, and Rajeev Sangal. 2009.
Anncorra: Treebanks for indian languages, guidelines for annotating hindi treebank.
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M. Sharma, and F. Xia. 2009. A multi-representational
and multi-layered treebank for hindi/urdu. In Proceedings of the Third Linguistic Annotation Workshop, pages
186?189. Association for Computational Linguistics.
Raman Chandrasekar and Bangalore Srinivas. 1997. Automatic induction of rules for text simplification.
Knowledge-Based Systems, 10(3):183?190.
Raman Chandrasekar, Christine Doran, and Bangalore Srinivas. 1996. Motivations and methods for text sim-
plification. In Proceedings of the 16th conference on Computational linguistics-Volume 2, pages 1041?1044.
Association for Computational Linguistics.
Takao Doi and Eiichiro Sumita. 2003. Input sentence splitting and translating. In Proc. of Workshop on Building
and Using Parallel Texts, HLT-NAACL 2003, pages 104?110.
Naman Jain, Karan Singla, Aniruddha Tammewar, and Sambhav Jain, 2012. Proceedings of the Workshop on
Machine Translation and Parsing in Indian Languages, chapter Two-stage Approach for Hindi Dependency
Parsing Using MaltParser, pages 163?170. The COLING 2012 Organizing Committee.
Yamuna Kachru. 2006. Hindi, volume 12. John Benjamins Publishing.
Vilson J Leffa. 1998. Clause processing in complex sentences. In Proceedings of the First International Confer-
ence on Language Resources and Evaluation, volume 1, pages 937?943.
28
M. Palmer, R. Bhatt, B. Narasimhan, O. Rambow, D.M. Sharma, and F. Xia. 2009. Hindi Syntax: Annotating
Dependency, Lexical Predicate-Argument Structure, and Phrase Structure. In The 7th International Conference
on Natural Language Processing, pages 14?17.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics,
pages 311?318. Association for Computational Linguistics.
C Poornima, V Dhanalakshmi, Anand M Kumar, and KP Soman. 2011. Rule based sentence simplification for
english to tamil machine translation system. International Journal of Computer Applications, 25(8):38?42.
Rahul Sharma, Soma Paul, Riyaz Ahmad Bhat, and Sambhav Jain. 2013. Automatic clause boundary annotation
in the hindi treebank.
Advaith Siddharthan. 2002. An architecture for a text simplification system. In Language Engineering Confer-
ence, 2002. Proceedings, pages 64?71. IEEE.
Advaith Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language and Computation,
4(1):77?109.
Ankush Soni, Sambhav Jain, and Dipti Misra Sharma. 2013. Exploring verb frames for sentence simplification in
hindi. Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1082?
1086. Asian Federation of Natural Language Processing.
Lucia Specia. 2010. Translating from complex to simplified sentences. In Computational Processing of the
Portuguese Language, pages 30?39. Springer.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu Hirao, and Masaaki Nagata. 2010. Divide and translate:
improving long distance reordering in statistical machine translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and MetricsMATR, pages 418?427. Association for Computational
Linguistics.
29
