Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1928?1939, Dublin, Ireland, August 23-29 2014.
Latent Domain Translation Models in Mix-of-Domains Haystack
Hoang Cuong and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 107, 1098 XG Amsterdam, The Netherlands
Abstract
This paper addresses the problem of selecting adequate training sentence pairs from a mix-of-
domains parallel corpus for a translation task represented by a small in-domain parallel corpus.
We propose a novel latent domain translation model which includes domain priors, domain-
dependent translation models and language models. The goal of learning is to estimate the
probability of a sentence pair in mix-domain corpus to be in- or out-domain using in-domain
corpus statistics as prior. We derive an EM training algorithm and provide solutions for esti-
mating out-domain models (given only in- and mix-domain data). We report on experiments in
data selection (intrinsic) and machine translation (extrinsic) on a large parallel corpus consisting
of a mix of a rather diverse set of domains. Our results show that our latent domain invitation
approach outperforms the existing baselines significantly. We also provide analysis of the merits
of our approach relative to existing approaches.
Large parallel corpora are important for training statistical MT systems. Besides size, the relevance
of a parallel training corpus to the translation task at hand can be decisive for system performance, cf.
(Axelrod et al., 2011; Koehn and Haddow, 2012). In this paper we look at data selection where we
have access to a large parallel data repository C
mix
, representing a rather varied mix of domains, and
we are given a sample of in-domain parallel data C
in
, exemplifying a target translation task. Simply
concatenating C
in
with C
mix
does not always deliver best performance, because including irrelevant
sentences might be more harmful than beneficial, cf. (Axelrod et al., 2011). To make the best of
available data, we must select sentences from C
mix
for their relevance to translating sentences from C
in
.
Axelrod et al. (2011) and follow-up work, e.g., (Haddow and Koehn, 2012; Koehn and Haddow,
2012), select sentence pairs in C
mix
using the cross-entropy difference between in- and mix-domain lan-
guage models, both source and target sides, a modification of the Moore and Lewis method (Moore and
Lewis, 2010). In the translation context, however, often a source phrase has different senses/translations
in different domains, which cannot be distinguished with monolingual language models. The depen-
dence of translation choice on domain suggests that the word alignments themselves can better be con-
ditioned on domain information. However, in the data selection setting, corpus C
mix
often does not
contain useful domain markers, and C
in
contains only a small sample of in-domain sentence pairs.
In this paper we present a latent domain translation model which weights every sentence pair ?f , e? ?
C
mix
with a probability P (D | f , e) for being in-domain (D
1
) or out-domain (D
0
). Our model defines
P (e, f) =
?
D?{D
1
,D
0
}
P (D)P (e, f | D), using a latent domain variable D ? {D
0
, D
1
}. Using bi-
directional translation models, this leads to a domain prior P (D), domain-dependent translation models
P
t
(? |?, D) and language models P
lm
(? | D) as in Equation 1:
P (e, f | D) =
1
2
? {P
lm
(e | D)P
t
(f | e, D) + P
lm
(f | D)P
t
(e | f , D)} (1)
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1928
For efficiency we assume IBM Model I alignments a and translation tables t(?), e.g., P
t
(e |f , D) ?
?
a
?
i
t(e
i
|f
a
i
, D). Language models (LMs) P
lm
are trained separately, albeit one problem not ad-
dressed by earlier work is how to train out-domain LMs given only in- and mix-domain data?
In our model, initially both the translation and LM probabilities estimated from C
in
serve as priors for
weighting sentence pairs in C
mix
as being more relevant for in-domain translation than not. This initial
weighting reveals pseudo out-domain data in C
mix
, which we use to train out-domain language models
as well as initialize out-domain word alignment tables.
1
With these sharpened translation and language
models, training commences using a version of EM (Dempster et al., 1977). Because the potentially
relevant data in C
mix
might be a superset of any in-domain data, the estimates from C
in
serve merely
as initial model estimates. Metaphorically, iterative EM training resembles party invitations
on social networks (hence, the Invitation model): if initially in/out-domain sentence pairs (the hosts)
invite some sentence pairs from C
mix
, in the next iteration the new pseudo in/out-domain sentences
help invite more sentence pairs. In EM, sentence pairs receive weighted, rather than absolute, invitations
from in- and out-domain models.
We present extensive experiments on a rather difficult selection task exploiting a large mix-domain
corpus of 4.61M sentence pairs. Initially we conduct intrinsic evaluation on the mix-domain corpus
where we also hide in-domain data and seek to retrieve it. Subsequently we conduct full MT experiments
over the task. The results show that our Invitation model gives far better selections as well as translation
performance than the baseline trained on the large data C
mix
.
1 Invitation models of weighting and selection
By now training data selection from large mix-domain data is an accepted necessity, e.g., (Axelrod et
al., 2011; Gasc?o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013).
Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting
an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007;
Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model
adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and
Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting
(Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special
case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains.
Our model assigns to every sentence pair ?f , e? ? C
mix
a probability as in Equation 2:
P (D | f , e) =
P (f , e, D)
?
D?{D
1
,D
0
}
P (f , e, D)
(2)
P (f , e, D) =
1
2
? P (D)? {P
lm
(e | D)P
t
(f | e, D) + P
lm
(f | D)P
t
(e | f , D)}
Viewed as learning two latent corpora C
1
and C
0
, the task is to assign every ?f , e? ? C
mix
an expected
count P (D
x
| f , e) that it is in C
x
? {C
0
, C
1
}. Next we discuss the model components each in turn.
The domain-dependent translation models P
t
(? |D) can be viewed as modeling the probability that e
translates as f in domain D ? {D
0
, D
1
}. Given f = f
1
, f
2
, . . . , f
m
and e = e
1
, e
2
, . . . , e
l
, we assume
(hidden) alignments a = a
1
, a
2
, . . . , a
m
akin to IBM Model I (Brown et al., 1993):
P
t
(f ,a | e, D) =

(l + 1)
m
?
m
j=1
t(f
j
|e
a
j
, D) (3)
P
t
(f | e, D) =
?
a
P
t
(f ,a|e, D) =

(l + 1)
m
?
m
j=1
?
l
i=0
t(f
j
|e
i
, D). (4)
1
Earlier work on data selection exploits the contrast between in-domain and mix-domain instead of (pseudo) out-domain
language models. However, the mix-domain language models trained on a mix of rather diverse set of domains could be
considered kind of wide-coverage, which makes for a rather weak contrast with the in-domain language models.
1929
where t(f
j
|e
a
j
, D) is the domain-dependent lexical probability of f
j
given e
a
j
with respect to D. One
crucial aspect about model inspired by IBM-Model-I is that P
t
(f | e, D) can be estimated efficiently, as
in Equation 4. This makes the training particularly efficient as detailed in Section 2
The in-/out-domain source and target language models are not the same as in previous work, e.g.,
(Axelrod et al., 2011), which employ in-/mix-domain language models. This makes explicit the difficulty
in finding data to train out-domain language models, and we present a solution in Section 2.
The domain priors P (D
1
) and P (D
0
) represent the percentage of the pairs that are in-/ and out
domain respectively in C
mix
learned by our model. Their estimate during training might be a reasonable
selection cut-off threshold. However, we found that it is not entirely clear whether these cut-off criteria
might exclude other relevant/irrelevant pairs that are not exactly in-domain. We leave this extension for
future work.
2
Finally, it should be noted that the domain-dependent word alignment model, t(f |e,D) is a gener-
alization of the standard (domain-independent) word alignment model, t(f |e), in which, t(f |e,D) =
t(f |e)t(D|f,e)?
f
t(f |e)t(D|f,e)
. Here, t(D|f, e) can be thought of as the latent word-relevance models, i.e., the proba-
bility that a word pair is relevant for in- (D
1
) or out-domain (D
0
). Empirical results (beyond the scope
of this work) show that training the latent in-domain alignment model, t(f |e,D
1
) often gives better
translation systems than training the standard (domain-independent) alignment model, t(f |e).
2 Training
With all language models trained separately, our selection model can be viewed to have two sets of
domain-dependent parameters ? = {?
D
0
,?
D
1
}. The parameters ?
D
consist of the domain-dependent
lexical parameters (e.g., t
?
D
(f |e,D), t
?
D
(e|f,D)) and the domain prior parameter (e.g., P
?
D
(D)).
Our training procedure seeks the parameters ? that maximize the log-likelihood of C
mix
:
L =
?
f ,e
logP
?
(f , e) =
?
f ,e
log
?
D
?
a
P
?
D
(a, D, f , e) (5)
Because of the latent variables a and D, there is no closed form solution and the model is fit using the
EM algorithm (Dempster et al., 1977). EM can be seen to maximize L via block-coordinate ascent on a
lower bound F(q,?) using an auxiliary distribution over the latent variables q(a, D|f , e)
L ? F(q,?) =
?
f ,e
?
D
?
a
q(a, D | f , e) log
P
?
D
(a, D, f , e)
q(a, D | f , e)
(6)
where the inequality results from log being concave and Jensen?s inequality. We rewrite the Free energy
F(q,?) (Neal and Hinton, 1999) as follows:
F(q,?) =
?
f ,e
?
D
?
a
q(a, D | f , e) log
P
?
D
(a, D, f , e)
q(a, D | f , e)
=
?
f ,e
?
D,a
q(a, D | f , e) log
P
?
D
(a, D | f , e)
q(a, D | f , e)
+
?
f ,e
?
D,a
q(a, D | f , e) logP
?
(f , e)
=
?
f ,e
logP
?
(f , e)?KL[q(a, D | f , e) || P
?
D
(a, D|f , e)] (7)
where KL[?||?] is the KL-divergence. To find q
?
(a, D|f , e) that maximizes F(q,?):
q
?
(a, D|f , e) = argmax
q(a,D|f ,e)
F(q,?) = argmin
q(a,D|f ,e)
KL[q(a, D|f , e)||P
?
D
(a, D|f , e)]
= P
?
D
(a, D|f , e) = P
?
D
(D|f , e)P
?
D
(a|f , e, D). (8)
2
We especially thank an anonymous reviewer who gave valuable comments related to this point.
1930
Here
P
?
D
(a|f , e, D) =
P
?
D
(f ,a|e,D)
P
?
D
(f |e,D)
=
?
m
j=1
t(f
j
|e
a
j
, D)
?
m
j=1
?
l
i=0
t(f
j
|e
i
, D)
(9)
The distribution q
?
(a, D|f , e) together with q
?
(D|f , e) =
?
a
q
?
(a, D|f , e) = P
?
D
(D|f , e) can be
used to softly fill in the values of a and D respectively to estimate model parameters.
We now state our derived EM update formulas. We use the notation P
(c)
and t
(c)
for current iteration
estimates, and P
(+)
and t
(+)
for the re-estimates. We denote the expected counts that e aligns to f in
the translation (f |e) with respect to a domain D with c(f |e; f , e, D). Similarly, we denote the expected
count of (f |e) with respect to a domain D by c(D; f , e).
E-step ?D ? {D
0
, D
1
} do
c(D; f , e) = P
(c)
(D | f , e)
c(f |e; f , e, D) = P
(c)
(D | f , e)
t
(c)
(f | e,D)
?
l
i=0
t
(c)
(f | e
i
, D)
?
m
j=1
?(f, f
j
)
?
l
i=0
?(e, e
i
)
M-step ?D ? {D
0
, D
1
} do
t
(+)
(f |e,D) =
?
f ,e
c(f |e; f , e, D)
?
f
?
f ,e
c(f |e; f , e, D)
P
(+)
(D) =
?
f ,e
c(D; f , e)
?
D
?
f ,e
c(D; f , e)
To re-estimate P (D | f , e) we substitute the M-step estimates into Equations 3, 4 and 2. We initial-
ize translation tables t(f |e,D
1
) and t(e|f,D
1
) with non-zero estimates obtained from applying IBM
model I to in-domain corpus C
in
.
3
Before EM training starts we must train the LMs. The in-domain
LMs P
lm
(e|D
1
) and P
lm
(f |D
1
) are trained on the source and target sides of C
in
respectively. For the
out-domain LMs P
lm
(e|D
0
) and P
lm
(f |D
0
) we need an out-domain data set to train them. It would also
be reasonable to use the set to train the out-domain tables, t(? | ?, D
0
). This raises an hitherto unattended
question regarding how to construct such an out-domain data set.
Inspired by burn-in in sampling, initially we isolate all LMs from our model to train the translation
models for a single EM iteration; we initialize the model with a translation table constructed on C
in
and uniform otherwise. Using the re-estimates, we score sentence pairs in C
mix
with P (D
1
|f , e) and
select a burn-in subset of smallest scoring pairs as pseudo out-domain data which can be used to train
P
lm
(e|D
0
) and P
lm
(f |D
0
). Choosing the optimal size of this subset is difficult, but in practice, we
usually choose a subset that has similar size (number of words) to the given in-domain corpus. The
rationale behind this choice is to avoid the risk that pseudo out-domain models would dominate the in-
domain models during further training. We observe that choosing the same size for a pseudo out-domain
corpus is not guaranteed to always give optimal performance, and this point deserves further study.
Finally, once the domain-dependent LMs have been trained, the domain-dependent LM probabilities
stay fixed during EM. Crucially, it is important to scale the probabilities of the four LMs to make them
comparable: we normalize the probability that a LM assigns to a sentence by the total probability this
LM assigns to all sentences in C
mix
.
3 Experimental setting
We carry out experiments in data selection (intrinsic) as well as in machine translation (extrinsic). We
build an English-Spanish mix-domain corpus consisting of a large and rather varied set of domains (a
3
Note that in practice, we usually use only one iteration to train IBM Model I. To simplify the implementation, we ignore
factor

(l+1)
m
in the model (Equation 3), which serves a minor role. It should be also noted that we set a (small) threshold,
e.g., t(?|?, ?) = 0.0001 for all word pairs that do not occur in the in-domain corpus to avoid over-fitting.
1931
haystack) in a way that allows us to directly measure selection quality. Starting out from a general-
domain corpus C
g
consisting of 4.51M sentence pairs, collected from multiple resources including
EuroParl (Koehn, 2005), Common Crawl Corpus, UN Corpus, News Commentary, TAUS Software,
TAUS Hardware, and TAUS Pharmacy, and a 177K in-domain (TAUS Legal) sentence pairs.
We create C
mix
by selecting an arbitrary 100K pairs of in-domain set and adding them to C
g
; the
remaining 77K in-domain pairs constitute C
in
. We think of this as hiding in-domain data in C
mix
so
we can evaluate our ability to retrieve it; in this setting we can evaluate selection directly using pseudo-
precision/recall defined as the percentage of selected in-domain pairs to the total selected or to the hidden
100K pairs respectively.
Table 1 summarizes the data and the translation task. It should be noted that a mix-domain corpus,
that contains a large and rather varied set of domains, frequently contains subsets with a vocabulary that
is close to the in-domain adaptation task; in this case, e.g., Europarl and TAUS Legal share big portions
of their source vocabulary, whereas their translations could differ. This makes the selection task far more
difficult than assumed by previous approaches as we will show next.
Task Corpora English Spanish
Mix-Domain Corpus (4.51M sents) 125, 339, 057 139, 655, 311
TAUS Legal
In-Domain Corpus (77K sents) 1, 555, 342 1, 733, 370
Dev (2K sents) 27, 983 30, 501
Test (2K sents) 45, 736 48, 999
Table 1: The data preparation - training, dev and testing corpora (size in words). Note that the dev set
contains sentences of 10-25 words, while the test set contains sentences that vary substantially in length,
from 5-10 words up to 45-50 words.
Our Invitation model takes 3 EM-iterations to train.
4
We then weigh sentence pairs under our model
with P (D
1
| e, f). We test various baseline models, including the bilingual cross-entropy difference
model, and the two cross-entropy difference models (on the source language and on the target lan-
guage).
5
We report pseudo-precision/recall at the sentence-level using a range of cut-off criteria for
selecting the top scoring instances in the mix-domain corpus.
We use Moses (Koehn et al., 2007) with GIZA++ (Och and Ney, 2003) and k-best batch MIRA
(Cherry and Foster, 2012). Final MT systems use the same non-adapted language models trained on
2.2M English Europarl sentences plus 248.8K sentences from News Commentary Corpus (WMT 2013).
We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER
(Snover et al., 2006). Statistical significance uses 95% confidence intervals using paired bootstrap
re-sampling (Press et al., 1992; Koehn, 2004). The k-best batch MIRA optimizer (Cherry and Foster,
2012) was run at least three times to optimize any SMT system to avoid instability (Clark et al., 2011).
6
4 Results
Table 2 presents the results showing substantial improvement in selection performance compared to all
the baselines. Subsequently we build SMT systems over the selected subsets. We report the transla-
tion yielded by these systems over the task in Table 2 as well. It can be easily seen that the baseline
approaches that simply train on in- and mix-domain data do not work that well for a difficult selection
task from a mix-domain corpus consisting of a large and rather diverse set of domains. The SMT sys-
4
To train the LM probs, we construct interpolated 4-gram Kneser-Ney language models using BerkeleyLM (Pauls and
Klein, 2011). This setting for training language models is used for all experiments in this work.
5
The script we use to train these models is developed by Luke Orland and available at: https://github.com/
lukeorland/moore\_and\_lewis\_data\_selection.
6
Note that metric scores for the systems are averages over multiple runs.
1932
Cut-off Model
In-domain
Pairs
pseudo-
Precision
pseudo-
Recall
BLEU METEOR TER
50K
CE Difference (source side) 370 0.74 0.37 20.5 28.0 62.3
CE Difference (target side) 375 0.75 0.38 19.3 26.8 63.3
Bilingual CE Difference 413 0.83 0.41 18.7 26.3 64.3
Invitation 19156 38.31 19.16 36.5 36.4 47.1
100K
CE Difference (source side) 592 0.59 0.59 24.8 30.8 57.8
CE Difference (target side) 572 0.57 0.57 22.1 29.7 60.1
Bilingual CE Difference 649 0.65 0.65 23.1 30.0 58.9
Invitation 30474 30.47 30.47 37.1 36.9 47.0
150K
CE Difference (source side) 753 0.50 0.75 26.4 32.0 56.2
CE Difference (target side) 742 0.49 0.74 23.9 31.2 58.8
Bilingual CE Difference 793 0.53 0.79 24.4 30.9 58.1
Invitation 38424 25.62 38.42 37.1 37.0 46.7
200K
CE Difference (source side) 874 0.44 0.87 26.6 32.4 56.0
CE Difference (target side) 888 0.44 0.88 25.8 32.1 57.2
Bilingual CE Difference 932 0.93 0.65 25.7 32.0 57.0
Invitation 44392 22.17 44.39 37.5 37.4 46.2
250K
CE Difference (source side) 994 0.40 0.99 27.3 32.8 55.4
CE Difference (target side) 997 0.40 0.10 26.3 32.4 56.3
Bilingual CE Difference 1062 0.42 1.06 26.6 32.7 55.6
Invitation 49419 19.77 49.42 37.3 37.3 46.1
300K
CE Difference (source side) 1122 0.37 1.12 28.2 33.4 54.5
CE Difference (target side) 1093 0.36 1.09 26.4 32.7 56.0
Bilingual CE Difference 1169 0.39 1.17 27.8 33.3 54.9
Invitation 53892 17.96 53.89 37.7 37.5 46.0
Table 2: Systematic comparison between selection models.
tems trained on the selection of our model perform significantly and consistently better (with p-value
= 0.0001 for all cases) than the others trained on the selection of the baselines.
Sentences
Bilingual CE Difference
1
by assisting in the placement and financing of used and end-of-lease aircraft , atr asset management has helped broaden
atr ?s customer base , notably in emerging markets , by providing quality reconditioned aircraft at attractive prices and
has helped maintain residual values of used aircraft .
al participar en la colocaci?on y en la financiaci?on de los aviones usados al final del per??odo de arrendamiento , atr
gesti?on de activos ha podido ampliar la base de su clientela , en particular en los pa??ses de econom??as emergentes , al
proporcionar aparatos entregados en buen estado a precios interesantes y ha contribuido a mantener el valor residual
de los aviones usados .
2
in contrast , recent improvements in western europe are not expected to be reversed significantly .
en cambio no se espera que las recientes mejoras en europa occidental se inviertan significativamente .
3
creating xml file ...
creando el archivo xml ...
Invitation Model
1
as she has said , the harmonisation of the requirements for information to appear on the invoice will mean that traders
operating within the single market will be subject to a single legislation , while until now they have had to know , comply
with and apply fifteen different legislations .
como ella ha dicho , la armonizaci?on de los requisitos de informaci?on que deben constar en la factura permitir?a a los
comerciantes que operen en el mercado interior sujetarse a una sola legislaci?on , mientras que hasta ahora ten??an que
conocer , sujetarse y aplicar quince legislaciones diferentes .
2
the solicitation documents shall specify the estimated period of time following dispatch of the notice of acceptance that
will be required to obtain the approval .
en el pliego de condiciones se indicar?a el plazo de tiempo previsto , a partir de la expedici?on del aviso de aceptaci?on ,
que ser?a requerido para obtener la aprobaci?on .
3
there is no doubt that disadvantages will result for the consumer and for the manufacturer of branded goods , for example
with regard to consumer health protection .
ello generar?a , sin duda alguna , desventajas para el consumidor y el productor de art??culos de marca , entre otros
aspectos tambi?en en lo que se refiere a la protecci?on de la salud del consumidor .
Table 3: Top pairs from mix-domain corpus with highest scores according to models.
Table 3 presents some random top ranked sentence pairs from the bilingual cross-entropy difference
1933
Cut-off: 50K Cut-off: 100K Cut-off: 200K
Model English Spanish English Spanish English Spanish
CE Difference (source side) 8.65 8.70 11.92 12.21 15.50 16.22
CE Difference (target side) 8.14 10.09 11.61 14.13 15.45 18.50
Bilingual CE Difference 7.03 8.16 10.38 11.96 14.34 16.43
Invitation 40.16 44.70 37.30 41.59 34.32 38.32
Table 4: Average words in selected sentences.
model against our Invitation model for the task. This shows clearly more relevant pairs for our selection
model than for the baselines. It should be noted that the baseline models tend to prefer shorter sentences,
while our model suffers less from this kind of bias. Table 4 presents the average length (in words) of
selected sentences selected by different models over various cut-offs.
Cut-off Model
In-domain
Pairs
pseudo-
Precision
pseudo-
Recall
BLEU METEOR TER
300K
Without Translation Model 34156 11.39 34.16 35.8 36.6 47.3
Without Language Model 51991 17.33 51.99 37.4 37.4 46.6
Full model 53892 17.96 53.89 37.7 37.5 46.0
Table 5: Experiments exploring the roles of individual components in our model.
Which component type (language or translation models) contributes more to performance? We neu-
tralize each component in turn and build a selection system with the remaining model parameters. Ta-
ble 5 shows translation models are crucial for performance, while domain-dependent LMs make a small,
yet noteworthy contribution. It should also be noted that using the LMs derived separately from in- and
out-domain data yields far better performance than the LMs derived from in- and mix-domain data for
this task.
System Phrases BLEU METEOR TER
Large data C
mix
236.74M 36.8 37.2 47.1
Subset of 300K 22.47M 37.7 37.5 46.0
Table 6: Translation accuracy comparison.
Finally, we compare a system trained on a selection of the top scored 300K sentences to a baseline
large-scale SMT system trained on C
mix
(4.61M sentences). The baseline trained on C
mix
works with
236.74M phrase pairs, whereas the Invitation trained system employs a small table of 22.47M phrases.
Tabel 6 shows the results. It is interesting that the small MT system trained by Invitation performs
significantly better (with p-value = 0.0001 for all metrics) than the large-scale system baseline trained
on all of C
mix
.
Input
cada estado miembro supervisar?a la categor??a cient??fica de la evaluaci?on y las actividades de los miembros
de los comit?es y de los expertos que haya designado, pero se abstendr?a de darles instrucciones incompatibles
con las funciones que les competen.
Reference
each member state shall monitor the scientific level of the evaluation carried out and supervise the activities
of members of the committees and the experts it nominates, but shall refrain from giving them any instruction
which is incompatible with the tasks incumbent upon them.
Large C
mix
each member state will oversee the category scientific assessment and the activities of members of the com-
mittees and experts which designated, but abstain of instruct incompatible with their regulatory functions.
Subset 300K
each member state will monitor the scientific category of the evaluation and the activities of the members
of the committees and of experts who has designated, but refrain from giving them instructions incompatible
with the required functions assumed.
Table 7: Translation example yielded by systems.
To give a sense of the improvement in translation, we present an example in Table 7. The example
is indeed illuminating because it shows the difference in choice between the mix-domain system and
1934
our selection-trained system. The example shows different translation pairs: ?supervisar?a-monitor? vs.
?supervisar?a-oversee?, ?evaluaci?on-evaluation? vs. ?evaluaci?on-assessment?, and ?abstendr?a de-refrain
from? vs. ?abstendr?a de-abstain?. Table 8 presents phrase table entries, i.e., p(e | f) and p(f | e), for the
pairs of words in each system.
supervisar
?
a evaluaci
?
on abstendr
?
a de
System Entry monitor oversee evaluation assessment refrain from abstain
Large data C
mix
?(e|f) 0.002 0.020 0.579 0.429 0.002 0.013
?(f |e) 0.119 0.081 0.391 0.403 0.014 0.060
Subset of 300K
?(e|f) 0.012 0.024 0.487 0.357 0.015 ?
?(f |e) 0.203 0.072 0.338 0.417 0.143 ?
Table 8: Phrase entry examples. Note that the system trained on the subset of top 300K pairs of sentences
does not contain the phrase pair ?refrain from-abstain?.
5 Final Machine Translation experiments: Putting all data together
For final adaptation evaluations we follow (Koehn and Schroeder, 2007; Nakov, 2008) and (Axelrod et
al., 2011; Sennrich, 2012), by passing multiple phrase tables directly to the Moses decoder and tuning
a system using these different tables together. Table 9 presents the result, showing the consistent im-
provement of adaptation with Invitation model compared to the baselines (with p-value = 0.0001 for all
cases) over the mixture data C
mix
.
Data System BLEU METEOR TER
In-domain 36.66 37.19 44.76
50K
+ CE Difference (source side) 37.1 36.7 48.1
+ CE Difference (target side) 37.1 36.6 48.2
+ Bilingual CE Difference 37.1 36.6 48.2
+ Invitation 38.0 37.2 47.3
100K
+ CE Difference (source side) 37.3 36.8 47.9
+ CE Difference (target side) 37.2 36.8 48.0
+ Bilingual CE Difference 37.2 36.8 48.0
+ Invitation 38.4 37.4 46.9
150K
+ CE Difference (source side) 37.1 36.9 48.2
+ CE Difference (target side) 37.3 36.9 47.9
+ Bilingual CE Difference 37.0 36.8 48.1
+ Invitation 38.6 37.5 46.6
200K
+ CE Difference (source side) 37.3 36.9 47.7
+ CE Difference (target side) 37.3 36.9 47.9
+ Bilingual CE Difference 37.3 36.9 47.8
+ Invitation 38.4 37.6 46.7
250K
+ CE Difference (source side) 37.4 36.9 47.7
+ CE Difference (target side) 37.3 37.0 47.7
+ Bilingual CE Difference 37.3 37.0 47.8
+ Invitation 38.6 37.7 46.5
300K
+ CE Difference (source side) 37.3 37.0 47.8
+ CE Difference (target side) 37.1 37.0 48.0
+ Bilingual CE Difference 37.3 36.9 47.8
+Invitation 38.9 37.9 46.3
Table 9: Translation results from our domain-adapted SMT systems.
Finally, we also test the adaptation evaluations between the system trained on the small selection of
top 300K sentences against the large-scale SMT system trained on C
mix
when combined with the in-
domain trained system. Table 10 presents the results, revealing comparable translation performance,
although they are trained on data sets that are significantly different in size.
1935
System BLEU METEOR TER
In-domain + Large data C
mix
39.0 38.0 46.3
In-domain + Subset of 300K 38.9 37.9 46.3
Table 10: Translation results from our domain-adapted SMT system and the large-scale SMT system.
Note that the baseline is slightly better than our domain-adapted SMT system under BLEU and ME-
TEOR, however, not statistically significant.
6 Final notes on mix-domain data selection
The specific data selection scenario studied in this paper brings up different aspects that did not receive
(sufficient) attention in earlier work on data selection and domain adaptation:
? The mix-domain parallel corpus C
mix
contains a large variety of domains that overlap and but also
differ in lexical choice and translation. This is radically different from the in-/out-domain setting
usually assumed in adaptation and constitutes a major challenge for existing selection approaches.
? The way the small in-domain corpus relates to the large mix-domain corpus is also challenging
because translation performance often depends on selecting relevant sentence pairs, aside from
those that are clearly in-domain.
? The lack of out-domain data in a realistic mix-domain scenario, suggests that efforts are needed
at finding data that contrasts enough with the in-domain data. In this work we propose an initial
training period (burn-in) for isolating pseudo out-domain data. But it might be that relevance-
related approaches could also turn out more effective for this.
In our current model we implement the P (e | D) and P (f | D) as language models, inspired by the
approaches based on the contrast between the cross-entropies of in- and mix-domain language models
(Moore and Lewis, 2010; Axelrod et al., 2011). However, P (e | D) and P (f | D) should work with
relevance models, i.e., assessing the relevance of sentences to domain D. Relevance is a different con-
cept than fluency as embodied by language models, and this aspects demands special attention in future
work.
7
In ongoing large-scale experiments, we now explore the behavior of our Invitation model on a variety
of different data settings and compare that to a range of alternative existing approaches. We are also
exploring new variations of our Invitation model to find out what the optimal settings might be for
different mixes of domains. So far we find that the burn-in and size of pseudo out-domain selection
after burn-in can be important in certain situations. We also observe that estimating the suitable size
of the selection set is also a topic that demands more attention because the estimate of P (D
1
) with
the interpretation percentage of relevant data in C
mix
like likely to demand suitable relevance models
instead of language models.
We observe that the present Invitation model could be approached from a discriminative perspective,
which could be effective for specific data settings. Finally, it is theoretically not clear whether a single
approach will be most effective for all practical data scenarios.
7 Conclusions
This work looks at modeling the relevance of sentence pairs from the mix-domain corpus to a task repre-
sented by an in-domain sample. In contrast with previous work we cast this as a translation problem with
a latent domain variable. Our Invitation model based on iterative weighted Invitations using EM, offers
a new view on data selection for MT. Our model also offers principled cut-off points for selecting in-
domain and other relevant subsets. Experiments on the in-domain task shows our approach outperforms
the existing data selection for such a very complex mixture training data.
7
We thank Amir Kamran for bringing this difference to our attention through ongoing joint experimental work.
1936
The high accuracy in our experiments in this kind of data compared to the baseline suggests that our
model might also offer good estimates that can be used for data weighting. In future work we aim to test
the Invitation model for instance weighting and explore avenues for using it for selecting and weighting
sub-sentential translation pairs (e.g., phrase pairs) that can be used directly for building SMT systems.
A further issue is to improve the quality of word alignments induced for mix-domain corpora. We also
aim at exploring a discriminative learning approach in conjunction with our model.
Acknowledgements
The first author is supported by the EXPERT (EXPloiting Empirical appRoaches to Translation) Initial
Training Network (ITN) of the European Union?s Seventh Framework Programme. We thank Transla-
tion Automation Society (TAUS.com) for providing us with suitable data for the mix-domain scenario.
We also thank Amir Kamran and Bart Mellebeek for help and collaboration on experiments related to
data selection and domain adaptation. We thank Milo?s Stanojevi?c and three anonymous reviewers for
their valuable comments on earlier versions.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
355?362, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pratyush Banerjee, Sudip Kumar Naskar, Johann Roturier, Andy Way, and Josef van Genabith. 2012. Translation
quality-based supplementary data selection by incremental update of translation models. In Martin Kay and
Christian Boitet, editors, COLING 2012, 24th International Conference on Computational Linguistics, Pro-
ceedings of the Conference: Technical Papers, 8-15 December 2012, Mumbai, India, pages 149?166. Indian
Institute of Technology Bombay.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation. Comput. Linguist., 19:263?311, June.
Boxing Chen, George Foster, and Roland Kuhn. 2013. Adaptation of reordering models for statistical machine
translation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, pages 938?946, Atlanta, Georgia, June. Association
for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings
of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ?12, pages 427?436, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT
?11, pages 176?181, Stroudsburg, PA, USA. Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em
algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1?38.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation,
WMT ?11, pages 85?91, Stroudsburg, PA, USA. Association for Computational Linguistics.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Hajime Tsukada. 2013. Adaptation data selection using
neural language models: Experiments in machine translation. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pages 678?683, Sofia, Bulgaria, August.
Association for Computational Linguistics.
1937
Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model
adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Short Papers - Volume 2, ACL ?12, pages 115?119, Stroudsburg, PA, USA. Association for Computational
Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA. Association for
Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation
in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 451?459, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Guillem Gasc?o, Martha-Alicia Rocha, Germ?an Sanchis-Trilles, Jes?us Andr?es-Ferrer, and Francisco Casacuberta.
2012. Does more data always yield better translations? In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguistics, EACL ?12, pages 152?161, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Barry Haddow and Philipp Koehn. 2012. Analysing the effect of out-of-domain data on smt systems. In Proceed-
ings of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 422?432, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ann Irvine, John Morgan, Marine Carpuat, Hal Daume III, and Dragos Munteanu. 2013. Measuring machine
translation errors in new domains. Transactions of the Association for Computational Linguistics (TACL).
Philipp Koehn and Barry Haddow. 2012. Towards effective use of training data in statistical machine transla-
tion. In Proceedings of the Seventh Workshop on Statistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra Constantin, and
Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07, pages 177?180,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 388?395, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for
machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Pro-
cessing, pages 708?717, Singapore, August. Association for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings
of the ACL 2010 Conference Short Papers, ACLShort ?10, pages 220?224, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Preslav Nakov. 2008. Improving english-spanish statistical machine translation: Experiments in domain adapta-
tion, sentence paraphrasing, tokenization, and recasing. In Proceedings of the Third Workshop on Statistical
Machine Translation, StatMT ?08, pages 147?150, Stroudsburg, PA, USA. Association for Computational Lin-
guistics.
Radford M. Neal and Geoffrey E. Hinton. 1999. A view of the em algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learning in Graphical Models, pages 355?368. MIT Press,
Cambridge, MA, USA.
1938
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics, ACL ?02, pages 311?318, Stroudsburg, PA, USA. Association for Computational Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1,
HLT ?11, pages 258?267, Stroudsburg, PA, USA. Association for Computational Linguistics.
William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C
(2Nd Ed.): The Art of Scientific Computing. Cambridge University Press, New York, NY, USA.
Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine
translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-
tional Linguistics, EACL ?12, pages 539?549, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas, pages
223?231.
1939
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 566?576,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Latent Domain Phrase-based Models for Adaptation
Hoang Cuong and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 107, 1098 XG Amsterdam, The Netherlands
{c.hoang,k.simaan}@uva.nl
Abstract
Phrase-based models directly trained
on mix-of-domain corpora can be
sub-optimal. In this paper we equip
phrase-based models with a latent domain
variable and present a novel method for
adapting them to an in-domain task rep-
resented by a seed corpus. We derive an
EM algorithm which alternates between
inducing domain-focused phrase pair
estimates, and weights for mix-domain
sentence pairs reflecting their relevance
for the in-domain task. By embedding
our latent domain phrase model in a
sentence-level model and training the
two in tandem, we are able to adapt all
core translation components together
? phrase, lexical and reordering. We
show experiments on weighing sentence
pairs for relevance as well as adapting
phrase-based models, showing significant
performance improvement in both tasks.
1 Mix vs. Latent Domain Models
Domain adaptation is usually perceived as utiliz-
ing a small seed in-domain corpus to adapt an ex-
isting system trained on an out-of-domain corpus.
Here we are interested in adapting an SMT sys-
tem trained on a large mix-domain corpus C
mix
to an in-domain task represented by a seed paral-
lel corpus C
in
. The mix-domain scenario is in-
teresting because often a large corpus consists of
sentence pairs representing diverse domains, e.g.,
news, politics, finance, sports, etc.
At the core of a standard state-of-the-art phrase-
based system (Och and Ney, 2004) is a phrase
table {?e?,
?
f?} extracted from the word-aligned
training data together with estimates for P
t
(e? |
?
f)
and P
t
(
?
f | e?). Because the translations of
words often vary across domains, it is likely
that in a mix-domain corpus C
mix
the translation
ambiguity will increase with the domain diver-
sity. Furthermore, the statistics in C
mix
will re-
flect translation preferences averaged over the di-
verse domains. In this sense, phrase-based mod-
els trained on C
mix
can be considered domain-
confused. This often leads to suboptimal perfor-
mance (Gasc?o et al., 2012; Irvine et al., 2013).
Recent adaptation techniques can be seen as
mixture models, where two or more phrase ta-
bles, estimated from in- and mix-domain corpora,
are combined together by interpolation, fill-up, or
multiple-decoding paths (Koehn and Schroeder,
2007; Bisazza et al., 2011; Sennrich, 2012; Raz-
mara et al., 2012; Sennrich et al., 2013). Here
we are interested in the specific question how to
induce a phrase-based model from C
mix
for in-
domain translation? We view this as in-domain
focused training on C
mix
, a complementary adap-
tation step which might precede any further com-
bination with other models, e.g., in-, mix- or
general-domain.
The main challenge is how to induce from C
mix
a phrase-based model for the in-domain task,
given only C
in
as evidence? We present an ap-
proach whereby the contrast between in-domain
prior distributions and ?out-domain? distributions
is exploited for softly inviting (or recruiting) C
mix
phrase pairs to either camp. To this end we in-
566
troduce a latent domain variable D to signify in-
(D
1
) and out-domain (D
0
) respectively.
1
With the introduction of the latent variables, we
extend the translation tables in phrase-based mod-
els from generic P
t
(e? |
?
f) to domain-focused by
conditioning them on D, i.e., P
t
(e? |
?
f,D) and de-
composing them as follows:
P
t
(e? |
?
f,D) =
P
t
(e? |
?
f)P(D | e?,
?
f)
?
e?
P
t
(e? |
?
f)P(D | e?,
?
f)
. (1)
Where P(D | e?,
?
f) is viewed as the latent phrase-
relevance models, i.e., the probability that a
phrase pair is in- (D
1
) or out-domain (D
0
). In the
end, our goal is to replace the domain-confused
tables, P
t
(e? |
?
f) and P
t
(
?
f | e?), with the in-domain
focused ones, P
t
(e? |
?
f,D
1
) and P
t
(
?
f | e?, D
1
).
2
Note how P
t
(e? |
?
f,D
1
) and P
t
(
?
f | e?, D
1
) contains
P
t
(e? |
?
f) and P
t
(
?
f | e?) as special case.
Eq. 1 shows that the key to training the latent
phrase-based translation models is to train the la-
tent phrase-relevance models, P (D | e?,
?
f). Our
approach is to embed P (D | e?,
?
f) in asymmetric
sentence-level models P (D | e, f) and train them
on C
mix
. We devise an EM algorithm where at
every iteration, in- or out-domain estimates pro-
vide full sentence pairs ?e, f? with expectations
{P (D | e, f) | D ? {0, 1}}. Once these ex-
pectation are in C
mix
, we induce re-estimates for
the latent phrase-relevance models, P (D | e?,
?
f).
Metaphorically, during each EM iteration the cur-
rent in- or out-domain phrase pairs compete on
inviting C
mix
sentence pairs to be in- or out-
domain, which bring in new (weights for) in- and
out-domain phrases. Using the same algorithm we
also show how to adapt all core translation com-
ponents in tandem, including also lexical weights
and lexicalized reordering models.
Next we detail our model, the EM-based invita-
tion training algorithm and provide technical so-
lutions to a range of difficulties. We report exper-
1
Crucially, the lack of explicit out-domain data in C
mix
is
a major technical difficulty. We follow (Cuong and Sima?an,
2014) and in the sequel present a relatively efficient solution
based on a kind of ?burn-in? procedure.
2
It is common to use these domain-focused models as
additional features besides the domain-confused features.
However, here we are more interested in replacing the
domain-confused features rather than complementing them.
This distinguishes this work from other domain adaptation
literature for MT.
iments showing good instance weighting perfor-
mance as well as significantly improved phrase-
based translation performance.
2 Model and training by invitation
Eq. 1 shows that the key to training the latent
phrase-based translation models is to train the la-
tent phrase-relevance models, P (D | e?,
?
f). As
mentioned, for training P (D | e?,
?
f) on parallel
sentences in C
mix
we embed them in two asym-
metric sentence-level models {P (D | e, f) | D ?
{0, 1}}.
2.1 Domain relevance sentence models
Intuitively, sentence models for domain relevance
P (D | e, f) are somewhat related to data selec-
tion approaches (Moore and Lewis, 2010; Axel-
rod et al., 2011). The dominant approach to data
selection uses the contrast between perplexities
of in- and mix-domain language models.
3
In the
translation context, however, often a source phrase
has different senses/translations in different do-
mains, which cannot be distinguished with mono-
lingual language models (Cuong and Sima?an,
2014). Therefore, our proposed latent sentence-
relevance model includes two major latent com-
ponents - monolingual domain-focused relevance
models and domain-focused translation models
derives as follows:
P (D | e, f) =
P (e, f, D)
?
D?{D
1
,D
0
}
P (e, f, D)
, (2)
where P (e, f, D) can be decomposed as:
P (f, e, D) =
1
2
(
P (D)P
lm
(e | D)P
t
(f | e, D)
+ P (D)P
lm
(f | D)P
t
(e | f, D)
)
.
(3)
Here
? P
t
(e|f, D) and similarly P
t
(f|e, D): the latent
domain-focused translation models aim at cap-
turing the faithfulness of translation with re-
spect to different domains. We simplify this as
3
Note that earlier work on data selection exploits the con-
trast between in- and mix-domain. In (Cuong and Sima?an,
2014), we present the idea of using the language and transla-
tion models derived separately from in- and out-domain data,
and show how it helps for data selection.
567
?bag-of-possible-phrases? translation models:
4
P
t
(e|f, D) :=
?
?e?,
?
f??A(e,f)
P
t
(e?|
?
f,D)
c(e?,
?
f)
,
(4)
where A(e, f) is the multiset of phrases in
?e, f? and c(?) denotes their count. Sub-model
P
t
(e?|
?
f,D) is given by Eq. 1.
? P
lm
(e|D), P
lm
(f|D): the latent monolingual
domain-focused relevance models aim at cap-
turing the relevance of e and f for identifying
domain D but here we consider them language
models (LMs).
5
As mentioned, the out-domain
LMs differ from previous works, e.g., (Axel-
rod et al., 2011), which employ mix-domain
LMs. Here, we stress the difficulty in finding
data to train out-domain LMs and present a so-
lution based on identifying pseudo out-domain
data.
? P (D): the domain priors aim at modeling
the percentage of relevant data that the learn-
ing framework induces. It can be estimated
via phrase-level parameters but here we prefer
sentence-level parameters:
6
P (D) :=
?
?e,f??C
mix
P (D | e, f)
?
D
?
?e,f??C
mix
P (D | e, f)
(5)
2.2 Training by invitation
Generally, our model can be viewed to have latent
parameters ? = {?
D
0
,?
D
1
}. The training pro-
cedure seeks ? that maximize the log-likelihood
of the observed sentence pairs ?e, f? ? C
mix
:
L =
?
?e,f??C
mix
log
?
D
P
?
D
(D, e, f). (6)
It is obvious that there does not exist a closed-form
solution for Equation 6 because of the existence of
4
We design our latent domain translation models with ef-
ficiency as our main concern. Future extensions could in-
clude the lexical and reordering sub-models (as suggested by
an anonymous reviewer.)
5
Relevance for identification or retrieval could be differ-
ent from frequency or fluency. We leave this extension for
future work.
6
It should be noted that in most phrase-based SMT sys-
tems bilingual phrase probabilities are estimated heuristically
from word alignmened data which often leads to overfitting.
Estimating P (D) from sentence-level parameters rather than
from phrase-level parameters helps us avoid the overfitting
which often accompanies phrase extraction.
the log-term log
?
. The EM algorithm (Dempster
et al., 1977) comes as an alternative solution to fit
the model. It can be seen to maximizeL via block-
coordinate ascent on a lower bound F(q,?) using
an auxiliary distribution q(D | e, f)
F(q,?) =
?
?e,f?
?
D
q(D | e, f) log
P
?
D
(D, e, f)
q(D | e, f)
(7)
where the inequality results, i.e., L ? F(q,?),
derived from log being concave and Jensen?s in-
equality. We rewrite the Free Energy F(q,?)
(Neal and Hinton, 1999) as follows:
F =
?
?e,f?
?
D
q(D | e, f) log
P
?
D
(D | e, f)
q(D | e, f)
+
?
?e,f?
?
D
q(D | e, f) logP
?
(e, f)
=
?
?e,f?
logP
?
(e, f) (8)
?KL[q(D | e, f) || P
?
D
(D | e, f)],
where KL[? || ?] is the KL-divergence.
With the introduction of the KL-divergence, the
alternating E and M steps for our EM algorithm
are easily derived as
E-step : q
t+1
(9)
argmax
q(D | e,f)
F(q,?
t
) =
argmin
q(D | e,f)
KL[q(D|e, f) || P
?
t
D
(D|e, f)]
= P
?
t
D
(D | e, f)
M-step : ?
t+1
(10)
argmax
?
F(q
t+1
,?) =
argmax
?
?
?e,f?
?
D
q(D | e, f) logP
?
D
(D, e, f)
The iterative procedure is illustrated in Fig-
ure 1.
7
At the E-step, a guess for P (D | e?,
?
f) can
be used to update P
t
(
?
f | e?, D) and P
t
(e? |
?
f,D)
(i.e., using Eq. 1) and consequently P
t
(f | e, D)
and P
t
(e | f, D) (i.e., using Eq. 4). These resulting
table estimates, together with the domain-focused
LMs and the domain priors are served as expected
counts to update P (D | e, f).
8
At the M-step,
7
For simplicity, we ignore the LMs and prior models in
the illustration in Fig. 1.
8
Since we only use the in-domain corpus as priors to ini-
tilize the EM parameters, in technical perspective we do not
want P (D | e, f) parameters to go too far off from the initial-
ization. We therefore prefer the averaged style in practice,
i.e., at the iteration n we update the P (D |e, f) parameters,
P
(n)
(D|e, f) as
1
n
(P
(n)
(D | e, f) +
?
n?1
i=1
P
(i)
(D | e, f)).
568
P (e?|
?
f,D)
P (
?
f |e?, D)
P (e|f, D)
P (f|e, D)
P (f, e, D)
P (D|e?,
?
f)
P (D|e, f)
Phrase-level Sentence-level
Re-update phrase-level parameters
Update sentence-level parameters
Figure 1: Our probabilistic invitation framework.
the new estimates for P (D | e, f) can be used to
(softly) fill in the values of hidden variable D and
estimate parameters P (D | e?,
?
f) and P (D). The
EM is guaranteed to converge to a local maximum
of the likelihood under mild conditions (Neal and
Hinton, 1999).
Before EM training starts we must provide a
?reasonable? initial guess for P (D | e?,
?
f). We
must also train the out-domain LMs, which needs
the construction of pseudo out-domain data.
9
One simple way to do that is inspired by burn-
in in sampling, under the guidance of an in-
domain data set, C
in
as prior. At the begin-
ning, we train P
t
(e? |
?
f,D
1
) and P
t
(
?
f | e?, D
1
)
for all phrases learned from C
in
. We also train
P
t
(e? |
?
f) and P
t
(
?
f | e?) for all phrases learned
from C
mix
. During burn-in we assume that the
out-domain phrase-based models are the domain-
confused phrase-based models, i.e., P
t
(e? |
?
f,D
0
)
? P
t
(e? |
?
f) and P
t
(
?
f | e?, D
0
) ? P
t
(
?
f | e?). We
isolate all the LMs and the prior models from our
model, and apply a single EM iteration to update
P (D | e, f) based on those domain-focused mod-
els P
t
(e? |
?
f,D) and P
t
(
?
f | e?, D).
In the end, we use P (D | e, f) to fill in the val-
ues of hidden variable D in C
mix
, so it provides
us with an initialization for P (D | e?,
?
f). Subse-
quently, we also rank sentence pairs in C
mix
with
P (D
1
| e, f) and select a subset of smallest scor-
ing pairs as a pseudo out-domain subset to train
P
lm
(e | D
0
) and P
lm
(f | D
0
). Once the latent
domain-focused LMs have been trained, the LM
probabilities stay fixed during EM. Crucially, it
9
The in-domain LMs P
lm
(e | D
1
) and P
lm
(f | D
1
) can
be simply trained on the source and target sides of C
in
re-
spectively.
is important to scale the probabilities of the four
LMs to make them comparable: we normalize the
probability that a LM assigns to a sentence by the
total probability this LM assigns to all sentences
in C
mix
.
3 Intrinsic evaluation
We evaluate the ability of our model to retrieve
?hidden? in-domain data in a large mix-domain
corpus, i.e., we hide some in-domain data in a
large mix-domain corpus. We weigh sentence
pairs under our model with P (D
1
| e?,
?
f) and
P (D
1
| e, f) respectively. We report pseudo-
precision/recall at the sentence-level using a
range of cut-off criteria for selecting the top
scoring instances in the mix-domain corpus. A
good relevance model expects to score higher for
the hidden in-domain data.
Baselines Two standard perplexity-based se-
lection models in the literature have been
implemented as the baselines: cross-entropy
difference (Moore and Lewis, 2010) and bilingual
cross-entropy difference (Axelrod et al., 2011),
investigating their ability to retrieve the hiding
data as well. Training them over the data to learn
the sentences with their relevance, we then rank
the sentences to select top of pairs to evaluate the
pseudo-precision/recall at the sentence-level.
Results We use a mix-domain corpus C
g
of 770K
sentence pairs of different genres.
10
There is also
a Legal corpus of 183K pairs that serves as the
in-domain data. We create C
mix
by selecting an
arbitrary 83K pairs of in-domain pairs and adding
them to C
g
(the hidden in-domain data); we use
the remaining 100k in-domain pairs as C
in
.
To train the baselines, we construct interpo-
lated 4-gram Kneser-Ney LMs using BerkeleyLM
(Pauls and Klein, 2011). Training our model on
the data takes six EM-iterations to converge.
11
10
Count of sentence pairs: European Parliament (Koehn,
2005): 183, 793; Pharmaceuticals: 190, 443, Software:
196, 168, Hardware: 196, 501.
11
After the fifth EM iteration we do not observe any sig-
nificant increase in the likelihood of the data. Note that we
use the same setting as for the baselines to train the latent
domain-focused LMs for use in our model ? interpolated 4-
gram Kneser-Ney LMs using BerkeleyLM. This training set-
ting is used for all experiments in this work.
569
05
1015
2025
3035
4045
5055
6065
7075
8085
9095
100
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Pseu
do-P
reci
sion
 (Se
nten
ce-L
evel
) 
Top Percentage 
(a): Pseudo-Precision (Sentence-Level) 
Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
0
5
10
15
20
25
30
35
40
45
50
55
60
65
70
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Pse
udo
-Rec
all 
(Se
nte
nce
-Le
vel
) 
Top Percentage 
(b): Pseudo -Recall (Sentence -Level) 
Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
Figure 2: Intrinsic evaluation.
Fig. 2 helps us examine how the pseudo sen-
tence invitation are done during each EM iter-
ation. For later iterations we observe a better
pseudo-precision and pseudo-recall at sentence-
level (Fig. 2(a), Fig. 2(b)). Fig. 2 also reveals
a good learning capacity of our learning frame-
work. Nevertheless, we observe that the baselines
do not work well for this task. This is not new,
as pointed out in our previous work (Cuong and
Sima?an, 2014).
Which component type contributes more to the
performance, the latent domain language models
or the latent domain translation models? Further
experiments have been carried on to neutralize
each component type in turn and build a selection
system with the rest of our model parameters. It
turns out that the latent domain translation mod-
els are crucial for performance for the learning
framework, while the latent domain LMs make a
far smaller yet substantial contribution. We refer
readers to our previous work (Cuong and Sima?an,
2014), which provides detail analysis of the data
selection problem.
4 Translation experiments: Setting
Data We use a mix-domain corpus consisting of
4M sentence pairs, collected from multiple re-
sources including EuroParl (Koehn, 2005), Com-
mon Crawl Corpus, UN Corpus, News Commen-
tary. As in-domain corpus we use ?Consumer
and Industrial Electronics? manually collected
by Translation Automation Society (TAUS.com).
The corpus statistics are summarized in Table 1.
System We train a standard state-of-the-art
English Spanish
C
mix
Sents 4M
Words 113.7M 127.1M
Domain:
Electronics
C
in
Sents 109K
Words 1, 485, 558 1, 685, 716
Dev
Sents 984
Words 13130 14, 955
Test
Sents 982
Words 13, 493 15, 392
Table 1: The data preparation.
phrase-based system, using it as the baseline.
12
There are three main kinds of features for the
translation model in the baseline - phrase-based
translation features, lexical weights (Koehn et al.,
2003) and lexicalized reordering features (Koehn
et al., 2005).
13
Other features include the penal-
ties for word, phrase and distance-based reorder-
ing.
The mix-domain corpus is word-aligned using
GIZA++ (Och and Ney, 2003) and symmetrized
with grow(-diag)-final-and (Koehn et al., 2003).
We limit phrase length to a maximum of seven
words. The LMs are interpolated 4-grams with
Kneser-Ney, trained on 2.2M English sentences
from Europarl augmented with 248.8K sentences
from News Commentary Corpus (WMT 2013).
We tune the system using k-best batch MIRA
(Cherry and Foster, 2012). Finally, we use Moses
12
We use Stanford Phrasal - a standard state-of-the-art
phrase-based translation system developed by Cer et al.
(2010).
13
The lexical weights and the lexical reordering features
will be described in more detail in Section 6.
570
19.91 
20.48 20.5 
20.64 
20.51 20.52 
19.8
20
20.2
20.4
20.6
20.8
21
21.2
Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
Electrics (Training Data: 1 Million)  
Figure 3: BLEU averaged over multiple runs.
(Koehn et al., 2007) as decoder.
14
We report BLEU (Papineni et al., 2002), ME-
TEOR 1.4 (Denkowski and Lavie, 2011) and TER
(Snover et al., 2006), with statistical significance
at 95% confidence interval under paired bootstrap
re-sampling (Press et al., 1992). For every system
reported, we run the optimizer at least three times,
before running MultEval (Clark et al., 2011) for
resampling and significance testing.
Outlook In Section 5 we examine the effect of
training only the latent domain-focused phrase ta-
ble using our model. In Section 6 we proceed fur-
ther to estimate also latent domain-focused lexical
weights and lexicalized reordering models, exam-
ining how they incrementally improve the transla-
tion as well.
5 Adapting phrase table only
Here we investigate the effect of adapting the
phrase table only; we will delay adapting the
lexical weights and lexicalized reordering fea-
tures to Section 6. We build a phrase-based sys-
tem with the usual features as the baseline, in-
cluding two bi-directional phrase-based models,
plus the penalties for word, phrase and distortion.
We also build a latent domain-focused phrase-
based system with the two bi-directional latent
phrase-based models, and the standard penalties
described above.
We explore training data sizes 1M , 2M
and 4M sentence pairs. Three baselines are
trained yielding 95.77M , 176.29M and 323.88M
phrases respectively. We run 5 EM iterations to
14
While we implement the latent domain phrase-based
models using Phrasal for some advantages, we prefer to use
Moses for decoding.
train our learning framework. We use the pa-
rameter estimates for P (D | e?,
?
f) derived at each
EM iteration to train our latent domain-focused
phrase-based systems. Fig. 3 presents the results
(in BLEU) at each iteration in detail for the case of
1M sentence pairs. Similar improvements are ob-
served for METEOR and TER. Here, we consis-
tently observe improvements at p-value = 0.0001
for all cases.
It should be noted that when doubling the train-
ing data to 2M and 4M , we observe the similar
results.
Finally, for all cases we report their best result
in Table 2. Here, note how the improvement could
be gained when doubling the training data.
Data
System Avg ? p-value
1M
Baseline 19.91 ? ?
Our System 20.64 +0.73 0.0001
2M
Baseline 20.54 ? ?
Our System 21.41 +0.87 0.0001
4M
Baseline 21.44 ? ?
Our System 22.62 +1.18 0.0001
Table 2: BLEU averaged over multiple runs.
It is also interesting to consider the average
entropy of phrase table entries in the domain-
confused systems, i.e.,
?
?
?e?,
?
f?
p
t
(e?|
?
f) log p
t
(e?|
?
f)
number of phrases?e?,
?
f?
against that in the domain-focused systems
?
?
?e?,
?
f?
p
t
(e?|
?
f,D
1
) log p
t
(e?|
?
f,D
1
)
number of phrases?e?,
?
f?
.
Following (Hasler et al., 2014) in Table 3 we also
show that the entropy decreases significantly in
571
the adapted tables in all cases, which indicates that
the distributions over translations of phrases have
become sharper.
Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
0.210 0.187 0.186 0.185 0.185 0.184
Table 3: Average entropy of distributions.
In practice, the third iteration systems usually
produce best translations. This is somewhat ex-
pected because as EM invites more pseudo in-
domain pairs in later iterations, it sharpens the
estimates of P (D
1
| e?,
?
f), making pseudo out-
domain pairs tend to 0.0. Table 4 shows the per-
centage of entries with P (D
1
| e?,
?
f) < 0.01 at
every iteration, e.g., 34.52% at the fifth iteration.
This induced schism in C
mix
diminishes the dif-
ference between the relevance scores for certain
sentence pairs, limiting the ability of the latent
phrase-based models to further discriminate in the
gray zone.
Entries P (D
1
|
?
f, e?) < 0.01
Iter. 1 22.82%
Iter. 2 27.06%
Iter. 3 30.07%
Iter. 4 32.47%
Iter. 5 34.52%
Table 4: Phrase analyses.
Finally, to give a sense of the improvement
in translation, we (randomly) select cases where
the systems produce different translations and
present some of them in Table 5. These ex-
amples are indeed illuminating, e.g., ?can repro-
duce signs of audio?/?can play signals audio?,
?password teacher?/?password master?, reveal-
ing thoroughly the benefit derived from adapting
the phrase models from being domain-confused to
being domain-focused. Table 6 presents phrase ta-
ble entries, i.e., p
t
(e | f) and p
t
(e | f,D
1
), for the
?can reproduce signs of audio?/?can play signals
audio? example.
6 Fully adapted translation model
The preceding experiments reveal that adapting
the phrase tables significantly improves transla-
tion performance. Now we also adapt the lexical
se
?
nales reproducir
Entries signals signs play reproduce
Baseline 0.29 0.36 0.15 0.20
Iter. 1 0.36 0.23 0.29 0.16
Iter. 2 0.37 0.19 0.32 0.17
Iter. 3 0.37 0.17 0.34 0.16
Iter. 4 0.37 0.16 0.36 0.16
Iter. 5 0.37 0.15 0.37 0.16
Table 6: Phrase entry examples.
and reordering components. The result is a fully
adapted, domain-focused, phrase-based system.
Briefly, the lexical weights provide smooth es-
timates for the phrase pair based on word trans-
lation scores P (e | f) between pairs of words
?e, f?, i.e., P (e | f) =
c(e,f)?
e
c(e,f)
(Koehn et
al., 2003). Our latent domain-focused lexical
weights, on the other hand, are estimated ac-
cording to P (e | f, D
1
), i.e., P (e | f, D
1
) =
P (e | f)P (D
1
| e, f)?
f
P (e | f)P (D
1
| e, f)
.
The lexicalized reordering models with orien-
tation variable O, P (O | e?,
?
f), model how likely
a phrase ?e?,
?
f? directly follows a previous phrase
(monotone), swaps positions with it (swap), or
is not adjacent to it (discontinous) (Koehn et al.,
2005). We make these domain-focused:
P (O | e?,
?
f,D
1
) =
P (O | e?,
?
f)P (D
1
| O, e?,
?
f)
?
O
P (O | e?,
?
f)P (D
1
| O, e?,
?
f)
(11)
Estimating P (D
1
| O, e?,
?
f) and P (D
1
| e, f) is
similar to estimating P (D
1
| e?,
?
f) and hinges on
the estimates of P (D
1
| e, f) during EM.
The baseline for the following experiments is a
standard state-of-the-art phrase-based system, in-
cluding two bi-directional phrase-based transla-
tion features, two bi-directional lexical weights,
six lexicalized reordering features, as well as the
penalties for word, phrase and distortion. We de-
velop three kinds of domain-adapted systems that
are different at their adaptation level to fit the task.
The first (Sys. 1) adapts only the phrase-based
models, using the same lexical weights, lexical-
ized reordering models and other penalties as the
baseline. The second (Sys. 2) adapts also the lex-
ical weights, fixing all other features as the base-
line. The third (Sys. 3) adapts both the phrase-
based models, lexical weights and lexicalized re-
572
Translation Examples
Input El reproductor puede reproducir se?nales de audio grabadas en mix-mode cd, cd-g, cd-extra y cd text.
Reference The player can play back audio signals recorded in mix-mode cd, cd-g, cd-extra and cd text.
Baseline The player can reproduce signs of audio recorded in mix-mode cd, cd-g, cd-extra and cd text.
Our System The player can play signals audio recorded in mix-mode cd, cd-g, cd-extra and cd text.
Input Se puede crear un archivo autodescodificable cuando el archivo codificado se abre con la contrase?na maestra.
Reference A self-decrypting file can be created when the encrypted file is opened with the master password.
Baseline To create an file autodescodificable when the file codified commenced with the password teacher.
Our System You can create an archive autodescodificable when the file codified opens with the password master.
Input Repite todas las pistas (?unicamente cds de v??deo sin pbc)
Reference Repeat all tracks (non-pbc video cds only)
Baseline Repeated all avenues (only cds video without pbc)
Our System Repeated all the tracks (only cds video without pbc)
Table 5: Translation examples yielded by a domain-confused phrase-based system (the baseline) and a
domain-focused phrase-based system (our system).
ordering models
15
, fixing other penalties as the
baseline.
Metric
System Avg ? p-value
Consumer and Industrial Electronics
(In-domain: 109K pairs; Dev: 982 pairs; Test: 984 pairs)
BLEU
Baseline 22.9 ? ?
Sys. 1 23.4 +0.5 0.008
Sys. 2 23.9 +1.0 0.0001
Sys. 3 24.0 +1.1 0.0001
METEOR
Baseline
30.0
? ?
Sys. 1 30.4 +0.4 0.0001
Sys. 2 30.8 +0.8 0.0001
Sys. 3 30.9 +0.9 0.0001
TER
Baseline 59.5 ? ?
Sys. 1 58.8 -0.7 0.0001
Sys. 2 58.0 -1.5 0.0001
Sys. 3 57.9 -1.6 0.0001
Table 7: Metric scores for the systems, which are
averages over multiple runs.
Table 7 presents results for training data size
of 4M parallel sentences. It shows that the fully
domain-focused system (Sys. 3) significantly im-
proves over the baseline. The table also shows
that the latent domain-focused phrase-based mod-
els and lexical weights are crucial for the im-
proved performance, whereas adapting the re-
ordering models makes a far smaller contribution.
Finally we also apply our approach to other
15
We run three EM iterations to train our invitation frame-
work, and then use the parameter estimates for P (D
1
| e?,
?
f),
P (D
1
| e, f) and P (D
1
| O, e?,
?
f) to train these domain-
focused features. We adopt this training setting for all other
different tasks in the sequel.
tasks where the relation between their in-domain
data and the mix-domain data varies substantially.
Table 8 presents their in-domain, tuning and test
data in detail, as well as the translation results
over them. It shows that the fully domain-focused
systems consistently and significantly improve the
translation accuracy for all the tasks.
7 Combining multiple models
Finally, we proceed further to test our latent
domain-focused phrase-based translation model
on standard domain adaptation. We conduct ex-
periments on the task ?Professional & Business
Services? as an example.
16
For standard adap-
tation we follow (Koehn and Schroeder, 2007)
where we pass multiple phrase tables directly to
the Moses decoder and tune them together. For
baseline we combine the standard phrase-based
system trained on C
mix
with the one trained on
the in-domain data C
in
. We also combine our la-
tent domain-focused phrase-based system with the
one trained on C
in
. Table 9 presents the results
showing that combining our domain-focused sys-
tem adapted from C
mix
with the in-domain model
outperforms the baseline.
16
We choose this task for additional experiments because
it has very small in-domain data (23K). This is supposed
to make adaptation difficult because of the robust large-scale
systems trained on C
mix
.
573
Metric
System Avg ? p-value
Professional & Business Services
(In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs)
BLEU
Baseline 22.0 ? ?
Our System 23.1 +1.1 0.0001
METEOR
Baseline 30.8 ? ?
Our System 31.4 +0.6 0.0001
TER
Baseline 58.0 ? ?
Our System 56.6 -1.4 0.0001
Financials
(In-domain: 31K pairs; Dev: 1, 000 pairs; Test: 1, 000 pairs)
BLEU
Baseline 31.1 ? ?
Our System 31.8 +0.7 0.0001
METEOR
Baseline 36.3 ? ?
Our System 36.6 +0.3 0.0001
TER
Baseline 48.8 ? ?
Our System 48.3 -0.5 0.0001
Computer Hardware
(In-domain: 52K pairs; Dev: 1, 021 pairs; Test: 1, 054 pairs)
BLEU
Baseline 24.6 ? ?
Our System 25.3 +0.7 0.0001
METEOR
Baseline 32.4 ? ?
Our System 33.1 +0.7 0.0001
TER
Baseline 56.4 ? ?
Our System 55.0 -1.4 0.0001
Computer Software
(In-domain: 65K pairs; Dev: 1, 100 pairs; Test: 1, 000 pairs)
BLEU
Baseline 27.4 ? ?
Our System 28.3 +0.9 0.0001
METEOR
Baseline 34.0 ? ?
Our System 34.7 +0.7 0.0001
TER
Baseline 51.7 ? ?
Our System 50.6 -1.1 0.0001
Pharmaceuticals & Biotechnology
(In-domain: 85K pairs; Dev: 920 pairs; Test: 1, 000 pairs)
BLEU
Baseline 31.6 ? ?
Our System 32.4 +0.8 0.0001
METEOR
Baseline 34.0 ? ?
Our System 34.4 +0.4 0.0001
TER
Baseline 51.4 ? ?
Our System 50.6 -0.8 0.0001
Table 8: Metric scores for the systems, which are
averages over multiple runs.
8 Related work
A distantly related, but clearly complementary,
line of research focuses on the role of docu-
ment topics (Eidelman et al., 2012; Zhang et al.,
2014; Hasler et al., 2014). An off-the-shelf Latent
Dirichlet Allocation tool is usually used to infer
document-topic distributions. On one hand, this
setting may not require in-domain data as prior.
On the other hand, it requires meta-information
(e.g., document information).
Part of this work (the latent sentence-relevance
models) relates to data selection (Moore and
Lewis, 2010; Axelrod et al., 2011), where
sentence-relevance weights are used for hard-
Metric
System Avg ? p-value
Professional & Business Services
(In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs)
BLEU
In-domain 46.5 ? ?
+ Mix-domain 46.6 ? ?
+ Our system 47.9 +1.3 0.0001
METEOR
In-domain 39.8 ? ?
+ Mix-domain 40.1 ? ?
+ Our System 41.1 +1.0 0.0001
TER
In-domain 38.2 ? ?
+ Mix-domain 38.0 ? ?
+ Our System 36.9 -1.1 0.0001
Table 9: Domain adaptation experiments. Metric
scores for the systems, which are averages over
multiple runs.
filtering rather than weighting. The idea of using
sentence-relevance estimates for phrase-relevance
estimates relates to Matsoukas et al. (2009) who
estimate the former using meta-information over
documents as main features. In contrast, our work
overcomes the mutual dependence of sentence and
phrase estimates on one another by training both
models in tandem.
Adaptation using small in-domain data has
a different but complementary goal to another
line of research aiming at combining a domain-
adapted system with the another trained on the in-
domain data (Koehn and Schroeder, 2007; Bisazza
et al., 2011; Sennrich, 2012; Razmara et al., 2012;
Sennrich et al., 2013). Our work is somewhat re-
lated to, but markedly different from, phrase pair
weighting (Foster et al., 2010). Finally, our latent
domain-focused phrase-based models and invita-
tion training paradigm can be seen to shift atten-
tion from adaptation to making explicit the role of
domain-focused models in SMT.
9 Conclusion
We present a novel approach for in-domain fo-
cused training of a phrase-based system on a
mix-of-domain corpus by using prior distributions
from a small in-domain corpus. We derive an EM
training algorithm for learning latent domain rel-
evance models for the phrase- and sentence-levels
in tandem. We also show how to overcome the
difficulty of lack of explicit out-domain data by
bootstrapping pseudo out-domain data.
In future work, we plan to explore generative
Bayesian models as well as discriminative learn-
ing approaches with different ways for estimat-
574
ing the latent domain relevance models. We hy-
pothesize that bilingual, but also monolingual, rel-
evance models can be key to improved perfor-
mance.
Acknowledgements
We thank Ivan Titov for stimulating discussions,
and three anonymous reviewers for their com-
ments on earlier versions. The first author is sup-
ported by the EXPERT (EXPloiting Empirical ap-
pRoaches to Translation) Initial Training Network
(ITN) of the European Union?s Seventh Frame-
work Programme. The second author is sup-
ported by VICI grant nr. 277-89-002 from the
Netherlands Organization for Scientific Research
(NWO). We thank TAUS for providing us with
suitable data.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based smt adaptation. In IWSLT, pages 136?
143.
Daniel Cer, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2010. Phrasal: A toolkit
for statistical machine translation with facilities for
extraction and incorporation of arbitrary model fea-
tures. In Proceedings of the NAACL HLT 2010
Demonstration Session, HLT-DEMO ?10, pages 9?
12, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 427?436, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, HLT ?11, pages 176?181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hoang Cuong and Khalil Sima?an. 2014. La-
tent domain translation models in mix-of-domains
haystack. In Proceedings of COLING 2014, the
25th International Conference on Computational
Linguistics: Technical Papers, pages 1928?1939,
Dublin, Ireland, August. Dublin City University and
Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. JOURNAL OF THE ROYAL STATIS-
TICAL SOCIETY, SERIES B, 39(1):1?38.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 85?91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Short Papers - Volume 2, ACL
?12, pages 115?119, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 451?459, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Guillem Gasc?o, Martha-Alicia Rocha, Germ?an
Sanchis-Trilles, Jes?us Andr?es-Ferrer, and Francisco
Casacuberta. 2012. Does more data always yield
better translations? In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?12, pages
152?161, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 328?337,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Ann Irvine, John Morgan, Marine Carpuat, Daume Hal
III, and Dragos Munteanu. 2013. Measuring ma-
chine translation errors in new domains. pages 429?
440.
575
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 224?227, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Trans-
lation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 708?717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Radford M. Neal and Geoffrey E. Hinton. 1999.
Learning in graphical models. chapter A View
of the EM Algorithm That Justifies Incremental,
Sparse, and Other Variants, pages 355?368. MIT
Press, Cambridge, MA, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449, Decem-
ber.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 258?267, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C (2Nd Ed.): The Art of Scientific Com-
puting. Cambridge University Press, New York,
NY, USA.
Majid Razmara, George Foster, Baskaran Sankaran,
and Anoop Sarkar. 2012. Mixing multiple trans-
lation models in statistical machine translation. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 940?949, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013. A multi-domain translation model frame-
work for statistical machine translation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 832?840, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ?12,
pages 539?549, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micci-
ulla, and J. Makhoul. 2006. A study of translation
edit rate with targeted human annotation. In Pro-
ceedings of Association for Machine Translation in
the Americas, pages 223?231.
Min Zhang, Xinyan Xiao, Deyi Xiong, and Qun Liu.
2014. Topic-based dissimilarity and sensitivity
models for translation rule selection. Journal of Ar-
tificial Intelligence Research, 50(1):1?30.
576
