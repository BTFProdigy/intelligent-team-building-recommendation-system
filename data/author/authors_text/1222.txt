Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 105?112
Manchester, August 2008
Regenerating Hypotheses for Statistical Machine Translation 
Boxing Chen, Min Zhang, Aiti Aw and Haizhou Li 
Department of Human Language Technology 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, 119613, Singapore 
{bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sg 
 Abstract 
This paper studies three techniques that 
improve the quality of N-best hypotheses 
through additional regeneration process. 
Unlike the multi-system consensus ap-
proach where multiple translation sys-
tems are used, our improvement is 
achieved through the expansion of the N-
best hypotheses from a single system. We 
explore three different methods to im-
plement the regeneration process: re-
decoding, n-gram expansion, and confu-
sion network-based regeneration. Ex-
periments on Chinese-to-English NIST 
and IWSLT tasks show that all three 
methods obtain consistent improvements. 
Moreover, the combination of the three 
strategies achieves further improvements 
and outperforms the baseline by 0.81 
BLEU-score on IWSLT?06, 0.57 on 
NIST?03, 0.61 on NIST?05 test set re-
spectively. 
1 Introduction 
State-of-the-art Statistical Machine Translation 
(SMT) systems usually adopt a two-pass search 
strategy (Och, 2003; Koehn, et al, 2003) as 
shown in Figure 1. In the first pass, a decoding 
algorithm is applied to generate an N-best list of 
translation hypotheses, while in the second pass, 
the final translation is selected by rescoring and 
re-ranking the N-best translations through addi-
tional feature functions. The fundamental as-
sumption behind using a second pass is that the 
generated N-best list may contain better transla-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
tions than the best choice found by the decoder. 
Therefore, the performance of a two-pass SMT 
system can be improved from two aspects, i.e. 
scoring models and the quality of the N-best hy-
potheses. 
Rescoring pass improves the performance of 
machine translation by enhancing the scoring 
models with more global sophisticated and dis-
criminative feature functions. The idea for apply-
ing two passes instead of one is that some global 
feature functions cannot be easily decomposed 
into local scores and computed during decoding. 
Furthermore, rescoring allows some feature func-
tions, such as word and n-gram posterior prob-
abilities, to be estimated on the N-best list (Uef-
fing, 2003; Chen et al, 2005; Zens and Ney, 
2006). 
In this two-pass method, translation perform-
ance hinges on the N-best hypotheses that are 
generated in the first pass (since rescoring occurs 
on these), so adding the translation candidates 
generated by other MT systems to these hypothe-
ses could potentially improve the performance. 
This technique is called system combination 
(Bangalore et al, 2001; Matusov et al, 2006; 
Sim et al, 2007; Rosti et al, 2007a; Rosti et al, 
2007b). 
We have instead chosen to regenerate new hy-
potheses from the original N-best list, a tech-
nique which we call regeneration. Regeneration 
is an intermediate pass between decoding and 
rescoring as depicted in Figure 2. Given the 
original N-best list (N-best1) generated by the 
decoder, this regeneration pass creates new trans-
lation hypotheses from this list to form another 
N-best list (N-best2). These two N-best lists are 
then combined and given to the rescoring pass to 
derive the best translation. 
We implement three methods to regenerate 
new hypotheses: re-decoding, n-gram expansion 
and confusion network. Re-decoding (Rosti et al, 
2007a) based regeneration re-decodes the source 
sentence using original LM as well as new trans-
105
lation and reordering models that are trained on 
the source-to-target N-best translations generated 
in the first pass. N-gram expansion (Chen et al, 
2007) regenerates more hypotheses by continu-
ously expanding the partial hypotheses through 
an n-gram language model trained on the original 
N-best translations. And confusion network gen-
erates new hypotheses based on confusion net-
work decoding (Matusov et al, 2006), where the 
confusion network is built on the original N-best 
translations. 
Confusion network and re-decoding have been 
well studied in the combination of different MT 
systems (Bangalore et al, 2001; Matusov et al, 
2006; Sim et al, 2007; Rosti et al, 2007a; Rosti 
et al, 2007b). Researchers have used confusion 
network to compute consensus translations from 
the outputs of different MT systems and improve 
the performance over each single systems. (Rosti 
et al, 2007a) also used re-decoding to do system 
combination by extracting sentence-specific 
phrase translation tables from the outputs of dif-
ferent MT systems and running a phrase-based 
decoding with this new translation table. Finally, 
N-gram expansion method (Chen et al, 2007) 
collects sub-strings occurring in the N-best list to 
produce alternative translations. 
This work demonstrates that a state-of-the-art 
MT system can be further improved by means of 
regeneration which expands its own N-best 
translations other than taking the translation can-
didates from the other MT systems. 
 
Figure 1: Structure of a typical two-pass ma-
chine translation system. N-best translations are 
generated by the decoder and the 1-best transla-
tion is returned after rescored with additional 
feature functions. 
 
Figure 2: Structure of a three-pass machine 
translation system with the new regeneration 
pass. The original N-best translations list (N-
best1) is expanded to generate a new N-best 
translations list (N-best2) before the rescoring 
pass. 
2 SMT Process 
Phrase-based statistical machine translation sys-
tems are usually modeled through a log-linear 
framework (Och and Ney, 2002). By introducing 
the hidden word alignment variable a  (Brown et 
al., 1993), the optimal translation can be 
searched for based on the following criterion: 
*
1,
arg max( ( , , ))
M
m mme a
e h?== e f a?             (1) 
where  is a string of phrases in the target lan-
guage, 
e
f
f a
 is the source language string of 
phrases,  h e  are feature functions, 
weights 
( , , )m
m? are typically optimized to maximize 
the scoring function (Och, 2003). 
Our MT baseline system is based on Moses 
decoder (Koehn et al, 2007) with word align-
ment obtained from GIZA++ (Och et al, 2003). 
The translation model (TM), lexicalized word 
reordering model (RM) are trained using the 
tools provided in the open source Moses package. 
Language model (LM) is trained with SRILM 
toolkit (Stolcke, 2002) with modified Kneser-
Ney smoothing method (Chen and Goodman, 
1998). 
3 Regeneration Methods 
Given the original N-best translations, regenera-
tion pass is to generate M new target translations 
which are not seen in the original N-best choices. 
3.1 Regeneration with Re-decoding 
One way of regeneration is by running the de-
coding again to obtain new hypotheses through a 
re-decoding process (Rosti et al, 2007a). In this 
work, the same decoder (Moses) is used to pro-
duce the new M-best translations using a new 
translation model and reordering model trained 
over the word-aligned source input and original 
N-best target hypotheses. Although the target-to-
source phrase alignments are available in the 
original N-best hypotheses, to enlarge the differ-
ence between the new M-best translations and 
the original N-best translations, we re-align the 
words using GIZA++. 
Weights of the decoder are re-optimized by 
the tool in the Moses package over the develop-
ment set. The process of such a re-decoding is 
summarized as follows: 
106
1. Run GIZA++ to align the words between the 
source input and target N-best translations; 
2. Train translation and reordering model; 
3. Optimize the weights of the decoder with 
the new models; 
4. Decode the source input by using new mod-
els and new weights to generate N+M dis-
tinct translations (?distinct? here refers to 
the target language string only, not consider-
ing the phrase segmentation, etc.); 
5. Output M-best translations which are not 
seen in the original N-best translations. 
Re-decoding on test set follows the same steps, 
but without the tuning step, step 3. 
3.2 Regeneration with N-gram Expansion 
N-gram expansion (Chen et al, 2007) combines 
the sub-strings occurred in the original N-best 
translations to generate new hypotheses. Firstly, 
all n-grams from the original N-best translations 
are collected. Then the partial hypotheses are 
continuously expanded by appending a word 
through the n-grams collected in the first step. 
We explain this method in more detail using the 
following example. 
Suppose we have four original hypotheses 
shown in Figure 3. Firstly, we collect all the 3-
grams from the original hypotheses. The first n-
grams of all original entries in the N-best list are 
set as the initial partial hypotheses. They are: it's 
5 minutes, it is 5, it?s about 5 and i walk 5. Then 
the expansion of a partial hypothesis starts by 
computing the set of n-grams matching its last n-
1 words. As shown in Figure 4, the n-gram 5 
minutes on matches the last two words of the 
partial hypothesis it?s about 5 minutes. So the 
hypothesis is expanded to it?s about 5 minutes on. 
The expansion continues until the partial hy-
pothesis ends with a special end-of-sentence 
symbol that occurs at the end of all N-best strings. 
Figure 5 shows some new hypotheses that are 
generated from the example in Figure 3. This is 
an example excerpted from our development data. 
One reference is also given in Figure 5; the first 
new generated hypothesis is equal to this refer-
ence.  But unfortunately, there is no such hy-
pothesis in the original N-best translations. 
During the new hypotheses generation, the 
translation outputs of a given source sentence are 
computed through a beam-search algorithm with 
a log-linear combination of the feature functions. 
In addition to n-gram frequency and n-gram pos-
terior probability which have been used in (Chen 
et al, 2007), we also used language model, di-
rect/inverse IBM model 1, and word penalty in 
this work. The size of the beam is set to N+M, to 
ensure more than M new hypotheses are gener-
ated. 
 
 
Original 
hypotheses 
1. it's 5 minutes on foot . 
2. it is 5 minutes on foot . 
3. it?s about 5 minutes? to walk . 
4. i walk 5 minutes . 
 
n-grams 
it's 5 minutes, 5 minutes on, ??
on foot ., about 5 minutes ?? 
5 minutes . 
 
Figure 3: Example of original hypotheses and 3-
grams collected from them. 
 
partial hyp. it?s about 5 minutes  
n-gram +                    5    minutes    on
new partial hyp. it?s about 5 minutes on
 
Figure 4: Expanding a partial hypothesis via a 
matching n-gram. 
 
 
New 
hypotheses
it?s about 5 minutes on foot . 
it's 5 minutes . 
i walk 5 minutes on foot . 
?? 
Reference it's about five minutes on foot . 
 
Figure 5: New generated hypotheses through n-
gram expansion and one reference. 
3.3 Regeneration with Confusion Network 
Confusion network based regeneration builds a 
confusion network over the original N-best hy-
potheses, and then extracts M-best hypotheses 
from it. The word order in the N-best translations 
could be very different, so we need to choose a 
hypothesis with the ?most correct? word order as 
the confusion network skeleton (alignment refer-
ence), then align and reorder other hypotheses in 
this word order. 
Some previous work compute the consensus 
translation under MT system combination, which 
differ from ours in the way of choosing the skele-
ton and aligning the words. Matusov et al (2006) 
let every hypothesis play the role of the skeleton 
once and used GIZA++ to get word alignment. 
Bangalore et al (2001), Sim et al (2007), Rosti 
et al (2007a), and Rosti et al (2007b) chose the 
hypothesis that best agrees with other hypotheses 
on average as the skeleton. Bangalore et al 
(2001) used a WER based alignment and Sim et 
al. (2007), Rosti et al (2007a), and Rosti et al 
(2007b) used minimum Translation Error Rate 
107
(TER) based alignment to build the confusion 
network. 
1. it?s 5 minutes on foot .  
Original 
hypotheses
2. it is 5 minutes on foot . 
Choosing alignment reference: Since the N-
best translations are ranked, choosing the first 
best hypothesis as the skeleton is straightforward 
in our work. 
3. it?s about 5 minutes? to walk . 
4. i walk 5 minutes . ?  it?s 5 minutes on foot . 
Alignments it 5 minutes on foot . is 
Aligning words: As a confusion network can be 
easily built from a one-to-one alignment, we de-
velop our algorithm based on the one-to-one as-
sumption and use competitive linking algorithm 
(Melamed, 2000) for our word alignment. Firstly, 
an association score is computed for every possi-
ble word pair from the skeleton and sentence to 
be aligned. Then a greedy algorithm is applied to 
select the best word-alignment. In this paper, we 
use a linear combination of multiple association 
scores, as suggested in (Kraif and Chen, 2004). 
As the two sentences to be aligned are in the 
same language, the association scores are com-
puted on the following four clues. They are cog-
nate (S
aboutit?s 5 minutes? to walk .
1), word class (S2), synonyms (S3), and 
position difference (S4). The four scores are line-
arly combined with empirically determined 
weights as shown is Equation 2. 
4
1
( , )j i k k
k
S f e S?
=
= ??                  (2) 
Reordering words: After word alignment, the 
words in all other hypotheses are reordered to 
match the word order of the skeleton. The 
aligned words are reordered according to their 
alignment indices. The unaligned words are reor-
dered in two strategies: moved with its previous 
word or next word. In this work, additional ex-
periments suggested that moving the unaligned 
word with its previous word achieve better per-
formance. In the case that the first word is un-
aligned, it will be moved with its next word. 
Each word is assigned a score based on a simple 
voting scheme. Figure 6 shows an example of 
creating a confusion network. 
Extracting M-best translations: New transla-
tions are extracted from the confusion network. 
We again use beam-search algorithm to derive 
new hypotheses.  The same feature functions 
proposed in Section 3.2 are used to score the par-
tial hypotheses. Moreover, we also use position 
based word probability (i.e. in Figure 6, the 
words in position 5, ?on? scored a probability of 
0.5, and ?? ? scored a probability of 0.25) as a 
feature function. Figure 6 shows some examples 
of new hypotheses generated through confusion 
network regeneration. 
 
 
i 5 minutes ?  walk .   ?  it?s 5 minutes on foot . 
Confusion 
network 
it is 5 minutes on foot .
it?s about 5 minutes? to walk .?i  5 minutes ?  walk . 
 1. it's about five minutes on foot . 
New 2. it about five minutes on foot . 
hypotheses 3. it's about five minutes on walk . 
4. i about 5 minutes to work . 
 
Figure 6: Example of creating a confusion net-
work from the word alignments, and new hy-
potheses generated through the confusion net-
work. The sentence in bold is the alignment ref-
erence. 
4 Rescoring model 
Since the final N+M-best hypotheses are pro-
duced either from different methods or same de-
coder with different models, local feature func-
tions of each hypothesis are not directly compa-
rable, and thus inadequate for rescoring. We 
hence exploit rich global feature functions in the 
rescoring models to compensate the loss of local 
feature functions. We apply the following 10 fea-
ture functions and optimize the weight of each 
feature function using the tool in Moses package. 
? direct and inverse IBM model 1 and 3 
? association score, i.e. hyper-geometric distri-
bution probabilities and mutual information 
? lexicalized word/block reordering rules 
(Chen et al, 2006) 
? 6-gram target LM 
? 8-gram target word-class based LM, word-
classes are clustered by GIZA++ 
? length ratio between source and target sen-
tence 
? question feature (Chen et al, 2005) 
? linear sum of n-grams relative frequencies 
within N-best translations (Chen et al, 2005) 
? n-gram posterior probabilities within the N-
best translations (Zens and Ney, 2006) 
? sentence length posterior probabilities (Zens 
and Ney, 2006) 
108
5 Experiments data Chinese English 
5.1 Tasks 
We carried out two sets of experiments on two 
different datasets. One is in spoken language 
domain while the other is on newswire corpus. 
Both experiments are on Chinese-to-English 
translation. 
Experiments on spoken language domain were 
carried out on the Basic Traveling Expression 
Corpus (BTEC) (Takezawa et al, 2002) Chi-
nese- to-English data augmented with HIT-
corpus1. BTEC is a multilingual speech corpus 
which contains sentences spoken by tourists. 40K 
sentence-pairs are used in our experiment. HIT-
corpus is a balanced corpus and has 500K sen-
tence-pairs in total. We selected 360K sentence-
pairs that are more similar to BTEC data accord-
ing to its sub-topic. Additionally, the English 
sentences of Tanaka corpus2 were also used to 
train our LM. We ran experiments on an 
IWSLT 3  challenge track which uses IWSLT-
20064 DEV clean text set as development set and 
IWSLT-2006 TEST clean text as test set. Table 1 
summarizes the statistics of the training, dev and 
test data for IWSLT task. 
Experiments on newswire domain were car-
ried out on the FBIS5 corpus. We used NIST6 
2002 MT evaluation test set as our development 
set, and the NIST 2003, 2005 test sets as our test 
sets. Table 2 summarizes the statistics of the 
training, dev and test data for NIST task. 
 
data Chinese English
Sentences 406,122 
Words 4,443K 4,591K
 
Train 
Vocabulary 69,989 61,087 
Sentences 489 489?7Dev.  
Words 5,896 45,449 
Sentences 500 500?7Test 
Words 6,296 51,227 
Sentences - 155K Additional 
target data Words - 1.7M 
 
Table 1: Statistics of training, development and 
test data for IWSLT task. 
                                                 
1 http://mitlab.hit.edu.cn/
2 http://www.csse.monash.edu.au/~jwb/tanakacorpus.html 
3 International Workshop for Spoken Language Trans-
lation 
4 http:// www.slc.atr.jp/IWSLT2006/ 
5 LDC2003E14 
6 http://www.nist.gov/speech/tests/mt/ 
Sentences 238,761  
Train Words 7.0M 8.9M 
Vocabulary 56,223 63,941 
Sentences 878 878?4 NIST 02 
(dev) Words 23,248 108,616
Sentences 919 919?4 NIST 03 
(test) Words 25,820 116,547
Sentences 1,082 1,082?4NIST 05 
(test) Words 30,544 141,915
Sentences - 2.2M Additional
target data Words - 61.5M 
 
Table 2: Statistics of training, development and 
test data for NIST task. 
 
Dev set Test set   
System #hypo BLEU NIST BLEU NIST
1-best - 29.98 7.468 29.10 7.103
RESC1 1,200 31.60 7.657 30.42 7.165
RD 1,200 32.46 7.664 30.95 7.175
NE 1,200 32.58 7.660 31.02 7.178
CN 1,200 32.33 7.671 30.82 7.200
RESC2 2,000 31.72 7.659 30.55 7.166
32.98 7.673 31.36 7.202COMB 2,000
 
Table 3: Translation performances (BLEU% and 
NIST scores) of IWSLT task: decoder (1-best), 
rescoring on original 1,200 N-best (RESC1) and 
2,000 N-best hypotheses (RESC2), re-decoding 
(RD), n-gram expansion (NE), confusion net-
work (CN) and combination of all hypotheses 
(COMB). 
5.2 Results 
We set N = 800 and M = 400 for IWSLT task, i.e. 
800 distinct translations for each source input are 
extracted from the decoder and used for regen-
eration; and 400 new hypotheses are generated 
for each regeneration system: re-decoding (RD), 
n-gram expansion (NE) and confusion network 
(CN). System COMB combines the original N-
best and the three regenerated M-best hypotheses 
lists (totally, 2,000 distinct hypotheses: 800 + 
3?400). Then each system computes the 1-best 
translation through rescoring and re-ranking its 
hypotheses list. For comparison purpose, the per-
formance of rescoring on two sets of original N-
best translations are also computed and they are 
applied based on 1,200 (RESC1) and 2,000 
(RESC2) distinct hypotheses extracted from the 
decoder.  For NIST task, we set N = 1,600, and 
M = 800, thus, RESC2 and COMB compute 1-
109
NIST?02 (dev) NIST?03 (test) NIST?05 (test)  
System 
 
#hypo BLEU NIST BLEU NIST BLEU NIST 
1-best 1 27.67 8.498 26.68 8.271 24.82 7.856 
RESC1 2,400 28.13 8.519 27.09 8.312 25.29 7.868 
RD 2,400 28.46 8.518 27.34 8.320 25.54 7.897 
NE 2,400 28.52 8.539 27.47 8.329 25.65 7.907 
CN 2,400 28.40 8.545 27.30 8.332 25.54 7.913 
RESC2 4,000 28.27 8.522 27.21 8.320 25.43 7.875 
COMB 4,000 28.92 8.602 27.78 8.401 26.04 7.994 
 
Table 4: Translation performances (BLEU% and NIST scores) of NIST task: decoder (1-best), rescoring 
on original 2,400 N-best (RESC1) and 4,000 N-best hypotheses (RESC2), re-decoding (RD), n-gram 
expansion (NE), confusion network (CN) and combination of all hypotheses (COMB). 
 
Reference No tax is needed for this item . Thank you . 
RESC2 you don't have to do not need to pay duty on this . thank you . 
1 
COMB (RD) not need to pay duty on this . thank you . 
Reference Certainly . The fitting room is over there . Please come with me . 
RESC2 the fitting room is over there . can you come with me . 
2 
COMB (NE) yes , you can . the fitting room is over there . please come with me . 
Reference OK . I will bring it to you in five minutes . 
RESC2 a good five minutes , we will give you . 
3 
COMB (CN) ok . after five minutes , i will give it to you . 
 
Table 5: Translations output by system RESC2 and COMB on IWSLT task (case-insensitive). 
best from 4,000 (1,600 + 3 800) distinct hy-
potheses. 
?
Our evaluation metrics are BLEU (Papineni et 
al., 2002) and NIST, which are to perform case-
insensitive matching of n-grams up to n = 4. The 
translation performance of IWSLT task and 
NIST task is reported in Tables 3 and 4 respec-
tively. The row ?1-best? reports the scores of the 
translations produced by the decoder. The col-
umn ?#hypo? means the size of the N-best hy-
potheses involved in rescoring. Note that on top 
of the same global feature functions as men-
tioned in Section 4, the local feature functions 
used during decoding were also involved in res-
coring RESC1 and RESC2. 
First of all, we note that both BLEU and NIST 
scores of the first decoding step were improved 
through rescoring. If rescoring was applied after 
regeneration on the N+M best lists, additional 
improvements were gained for all the develop-
ment and test sets on all three regeneration sys-
tems. Absolute improvement on BLEU score of 
0.4-0.6 on IWSLT?06 test set and 0.25-0.35 on 
NIST test sets were obtained when compared 
with system RESC1. Comparing the performance 
of three regeneration methods, we can see that 
re-decoding and confusion network based 
method achieved very similar improvement; 
while n-gram expansion based regeneration ob-
tained slightly better improvement than the other 
two methods. Combining all regenerated hy-
potheses with the original hypotheses further in-
creased the scores on both tasks. Compared with 
RESC2, system COMB obtained absolute im-
provement of 0.81 (31.36 ? 30.55) BLEU score 
on IWSLT?06 test set, 0.57 (27.28 ? 27.21) 
BLEU score on NIST?03 and 0.61 (26.04 ? 25.43) 
BLEU score on NIST?05 respectively. 
We further illustrate the effectiveness of the 
regeneration mechanism using some translation 
examples obtained from system RESC2 and 
COMB as shown in Table 5. 
6 Discussion 
To better interpret the performance improvement; 
first let us check if the regeneration pass has pro-
duced better hypotheses. We computed the oracle 
scores on all four 1,200-best lists in IWSLT task. 
The oracle chooses the translation with the low-
est word error rate (WER) with respect to the 
references in all cases. The results are reported in 
Table 6.  It is worth noticing that the first 800-
best (original N-best) hypotheses are the same in 
110
all four lists, with differences found only in the 
remaining 400 hypotheses (M-best). The consis-
tent improvement of oracle scores shows that the 
tra
theses contain better ones 
than the original ones. 
 
nslation candidates have been really improved. 
From another viewpoint, Table 7 shows the 
number of translations generated by each method 
in the final translation output (translations of 
COMB). After re-ranking N+3M entries, it is 
observed that more than 25% (e.g. for IWSLT?06 
test set, (50+74+39)/500=32.6%; NIST?03 test 
set, (77+85+68)/919=25.1%; NIST?05 test set, 
(95+110+82)/1082=26.5%) of best scored out-
puts were generated by the regeneration pass, 
showing that new generated translations are quite 
often the rescoring winner. This also proved that 
the new-generated hypo
List BLEU NIST WER PER
M  oses 46.10 8.765 36.29 30.94
RD 46.91 8.764 35.29 30.62
NE 46.95 8.811 36.05 30.72
Dev. 
CN 46.85 8.769 36.17 30.83
M  oses 45.09 8.403 37.07 32.04
RD 45.67 8.418 36.50 31.82
NE 45.82 8.481 36.44 31.70
Test 
CN 45.68 8.471 36.55 31.81
 
Table 6: Oracle scores (BLEU%, NIST, WER% 
and PER%) on IWSLT task 1,200-best lists of 
four systems: decoder (Moses), re-decoding 
(RD), n-gram expansion (NE) and confusion 
etwork (CN). 
 
n
# sentence   
Set Tot. Orig. RD NE CN
Dev 489 325 52 76 36IWSLT 
Test 500 337 50 74 39
NIST 02 878 613 92 100 73
NIST 03 919 689 77 85 68
NIST 
NIST 05 1082 795 95 110 82
 
Table 7: Number of translations generated by 
each method in the final translation output of 
system COMB: decoder (Orig.), re-decoding 
(RD), n-gram expansion (NE) and confusion 
network (CN). ?Tot.? is the size of the dev/test 
set. 
ities of words occur in the N-
best translations. 
n, and confusion 
ne
ree methods further im-
pr
the N-
best list through hypotheses regeneration. 
S. 
, pages 351?354. Madonna di 
P. 
ation. Com-
B.
Federico. 2005. The ITC-irst SMT System for 
 
Then, let us consider each single regeneration 
method to understand why regeneration can pro-
duce better hypotheses. Re-decoding may intro-
duce new and better phrase-pairs which are ex-
tracted from the N-best hypotheses to the transla-
tion model thus generate better hypotheses. N-
gram expansion can (almost) fully exploit the 
search space of target strings, which can be gen-
erated by an n-gram LM. As a result, it can pro-
duce alternative translations which contain word 
re-orderings and phrase structures not considered 
by the search algorithm of the decoder (Chen, et 
al., 2007). Confusion network based regeneration 
reinforces the word choice by considering the 
posterior probabil
7 Conclusions 
In this paper, we proposed a novel three-pass 
SMT framework against the typical two-pass 
system. This framework enhanced the quality of 
the translation candidates generated by our pro-
posed regeneration pass and improved the final 
translation performance. Three regeneration 
methods were introduced, namely, re-decoding, 
word-based n-gram expansio
twork based regeneration.  
Experiments were based on the state-of-the-art 
phrase-based decoder and carried out on the 
IWSLT and NIST Chinese-to-English task. We 
showed that all three methods improved the per-
formance with the n-gram expansion method 
achieving the greatest improvement. Moreover, 
the combination of the th
oves the performance. 
We conclude that translation performance can 
be improved by increasing the potential of trans-
lation candidates to contain better translations. 
We have presented an alternative solution to 
ameliorate the quality of translation candidates in 
a way that differs from system combination 
which takes translations from other MT systems. 
We demonstrated that the translation perform-
ance could be self-boosted by expanding 
References 
Bangalore, G. Bordel, and G. Riccardi. 2001. 
Computing consensus translation from multiple 
machine translation systems. In Proceeding of 
IEEE workshop on Automatic Speech Recognition 
and Understanding
Campiglio, Italy. 
F. Brown, V. J. Della Pietra, S. A. Della Pietra & R. 
L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation: Parameter Estim
putational Linguistics, 19(2) 263-312. 
 Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. 
111
IWSLT-2005. In Proceeding of IWSLT-2005, 
pp.98-104, Pittsburgh, USA, October. 
B. Chen, M. Cettolo and M. Federico. 2006. Reorder-
ing Rules for Phrase-based Statistical Machine 
Translation. In Proceeding of IWSLT-2006, Kyoto, 
Japan. 
B. Chen, M. Federico and M. Cettolo. 2007. Better N-
best Translations through Generative n-gram Lan-
guage Models. In Proceeding of MT Summit XI. 
Copenhagen, Denmark.  
S. F. Chen and J. T. Goodman. 1998. An Empirical 
Study of Smoothing Techniques for Language 
Modeling. Technical Report TR-10-98, Computer 
Science Group, Harvard University. 
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical 
Phrase-based Translation. In Proceedings of 
HLT/NAACL, pp 127-133, Edmonton, Canada. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. 
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin 
and E. Herbst. 2007. Moses: Open Source Toolkit 
for Statistical Machine Translation. In Proceeding 
of ACL-2007, pp. 177-180, Prague, Czech Republic. 
O. Kraif, B. Chen. 2004. Combining clues for lexical 
level aligning using the Null hypothesis approach. 
In Proceeding of COLING-2004, Geneva, pp. 
1261-1264.  
E. Matusov, N. Ueffing, and H. Ney. 2006. Comput-
ing consensus translation from multiple machine 
translation systems using enhanced hypotheses 
alignment. In Proceeding of EACL-2006, Trento, 
Italy.  
I. D. Melamed. 2000. Models of translational equiva-
lence among words. Computational Linguistics, 
26(2), pp. 221-249. 
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL-
2003. Sapporo, Japan. 
F. J. Och and H. Ney. 2003. A Systematic Compari-
son of Various Statistical Alignment Models. 
Computational Linguistics, 29(1), pp. 19-51. 
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceeding of ACL-2002. 
A. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. 
Schwartz and B. Dorr. 2007a. Combining Outputs 
from Multiple Machine Translation Systems.  In 
Proceeding of NAACL-HLT-2007, pp. 228-235. 
Rochester, NY. 
A. Rosti, S. Matsoukas and R. Schwartz. 2007b. Im-
proved Word-Level System Combination for Ma-
chine Translation. In Proceeding of ACL-2007, 
Prague. 
K. C. Sim, W. J. Byrne, M. J.F. Gales, H. Sahbi, and 
P. C. Woodland. 2007. Consensus network decod-
ing for statistical machine translation system com-
bination. In Proceeding of  ICASSP-2007. 
A. Stolcke. 2002. SRILM - an extensible language 
modelling toolkit. In Proceeding of ICSLP-2002. 
901-904. 
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, 
and S. Yamamoto. 2002. Toward a broad-coverage 
bilingual corpus for speech translation of travel 
conversations in the real world. In Proceeding of 
LREC-2002, Las Palmas de Gran Canaria, Spain. 
R. Zens and H. Ney. 2006. N-gram Posterior Prob-
abilities for Statistical Machine Translation. In 
Proceeding of HLT-NAACL Workshop on SMT, pp. 
72-77, NY. 
 
112
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157?160,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Exploiting N-best Hypotheses for SMT Self-Enhancement 
 
Boxing Chen, Min Zhang, Aiti Aw and Haizhou Li 
Department of Human Language Technology 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, 119613, Singapore 
{bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sg 
 
 
 
Abstract 
Word and n-gram posterior probabilities esti-
mated on N-best hypotheses have been used to 
improve the performance of statistical ma-
chine translation (SMT) in a rescoring frame-
work. In this paper, we extend the idea to 
estimate the posterior probabilities on N-best 
hypotheses for translation phrase-pairs, target 
language n-grams, and source word re-
orderings. The SMT system is self-enhanced 
with the posterior knowledge learned from N-
best hypotheses in a re-decoding framework. 
Experiments on NIST Chinese-to-English task 
show performance improvements for all the 
strategies. Moreover, the combination of the 
three strategies achieves further improvements 
and outperforms the baseline by 0.67 BLEU 
score on NIST-2003 set, and 0.64 on NIST-
2005 set, respectively. 
1 Introduction 
State-of-the-art Statistical Machine Translation 
(SMT) systems usually adopt a two-pass search 
strategy. In the first pass, a decoding algorithm is 
applied to generate an N-best list of translation 
hypotheses; while in the second pass, the final 
translation is selected by rescoring and re-ranking 
the N-best hypotheses through additional feature 
functions. In this framework, the N-best hypothe-
ses serve as the candidates for the final translation 
selection in the second pass. 
These N-best hypotheses can also provide useful 
feedback to the MT system as the first decoding 
has discarded many undesirable translation candi-
dates. Thus, the knowledge captured in the N-best 
hypotheses, such as posterior probabilities for 
words, n-grams, phrase-pairs, and source word re-
orderings, etc. is more compatible with the source 
sentences and thus could potentially be used to 
improve the translation performance. 
Word posterior probabilities estimated from the 
N-best hypotheses have been widely used for con-
fidence measure in automatic speech recognition 
(Wessel, 2002) and have also been adopted into 
machine translation. Blatz et al (2003) and Uef-
fing et al (2003) used word posterior probabilities 
to estimate the confidence of machine translation. 
Chen et al (2005), Zens and Ney (2006) reported 
performance improvements by computing target n-
grams posterior probabilities estimated on the N-
best hypotheses in a rescoring framework. Trans-
ductive learning method (Ueffing et al, 2007) 
which repeatedly re-trains the generated source-
target N-best hypotheses with the original training 
data again showed translation performance im-
provement and demonstrated that the translation 
model can be reinforced from N-best hypotheses.  
In this paper, we further exploit the potential of 
the N-best hypotheses and propose several 
schemes to derive the posterior knowledge from 
the N-best hypotheses, in an effort to enhance the 
language model, translation model, and source 
word reordering under a re-decoding framework of 
any phrase-based SMT system. 
2 Self-Enhancement with Posterior 
Knowledge 
The self-enhancement system structure is shown in 
Figure 1. Our baseline system is set up using 
Moses (Koehn et al, 2007), a state-of-the-art 
phrase-base SMT open source package. In the fol-
lowings, we detail the approaches to exploiting the 
three different kinds of posterior knowledge, 
namely, language model, translation model and 
word reordering. 
157
2.1 Language Model 
We consider self-enhancement of language model 
as a language model adaptation problem similar to 
(Nakajima et al, 2002). The original monolingual 
target training data is regarded as general-domain 
data while the test data as a domain-specific data. 
Obviously, the real domain-specific target data 
(test data) is unavailable for training. In this work, 
the N-best hypotheses of the test set are used as a 
quasi-corpus to train a language model. This new 
language model trained on the quasi-corpus is then 
used together with the language model trained on 
the general-domain data (original training data) to 
produce a new list of N-best hypotheses under our 
self-enhancement framework. The feature function 
of the language model 1 1( , )
J I
LMh f e  is a mixture 
model of the two language models as in Equation 1. 
1 1 1 1 2 1( , ) ( ) ( )
J I I I
LM TLM QLMh f e h e h e? ?= +      (1) 
where 1
Jf is the source language words string, 
1
Ie is  the target language words string, TLM is the 
language model trained on target training data, and 
QLM is on the quasi-corpus of N-best hypotheses. 
The mixture model exploits multiple language 
models with weights 1?  and 2?  being optimized 
together with other feature functions. The proce-
dure for self-enhancement of the language model is 
as follows. 
1. Run decoding and extract N-best hypotheses. 
2. Train a new language model (QLM) on the N-
best hypotheses. 
3. Optimize the weights of the decoder which uses 
both original LM (TLM) and the new LM 
(QLM). 
4. Repeat step 1-3 for a fixed number of iterations. 
2.2 Translation Model 
In general, we can safely assume that for a given 
source input, phrase-pairs that appeared in the N-
best hypotheses are better than those that did not. 
We call the former ?good phrase-pairs? and the 
later ?bad phrase-pairs? for the given source input. 
Hypothetically, we can reinforce the translation 
model by appending the ?good phrase-pairs? to the 
original phrase table and changing the probability 
space of the translation model, as phrase-based 
translation probabilities are estimated using rela-
tive frequencies. The new direct phrase-based 
translation probabilities are computed as follows:   
( , ) ( , )( | )
( ) ( )
train nbest
train nbest
N f e N f ep e f
N f N f
+= +
% %% %%% % %       (2) 
where f%  is the source language phrase, e%  is  the 
target language phrase, (.)trainN is the frequencies 
observed in the training data, and (.)nbestN  is the 
frequencies observed in the N-best hypotheses. For 
those phrase-pairs that did not appear in the N-best 
hypotheses list (?bad phrase-pairs?), ( , )nbestN f e% %  
equals 0, but the marginal count of f%  is increased 
by ( )nbestN f% , in this way the phrase-based transla-
tion probabilities of ?bad phrase-pairs? degraded 
when compared with the corresponding probabili-
ties in the original translation model, and that of 
?good phrase-pairs? increased, hence improve the 
translation model. 
The procedure for translation model self-
enhancement can be summarized as follows. 
1. Run decoding and extract N-best hypotheses. 
2. Extract ?good phrase-pairs? according to the 
hypotheses? phrase-alignment information and 
append them to the original phrase table to gen-
erate a new phrase table. 
3. Score the new phrase table to create a new 
translation model. 
4. Optimize the weights of the decoder with the 
above new translation model. 
5. Repeat step 1-4 for a fixed number of iterations. 
2.3 Word Reordering 
Some previous work (Costa-juss? and Fonollosa, 
2006; Li et al, 2007) have shown that reordering a 
source sentence to match the word order in its cor-
 
Figure 1: Self-enhancement system structure, where 
TM is translation model, LM is language model, and 
RM is reordering model. 
158
responding target sentence can produce better 
translations for a phrase-based SMT system. We 
bring this idea forward to our word reordering self-
enhancement framework, which similarly trans-
lates a source sentence (S) to target sentence (T) in 
two stages: S S T?? ? , where S ?  is the reor-
dered source sentence.  
The phrase-alignment information in each hy-
pothesis indicates the word reordering for source 
sentence. We select the word reordering with the 
highest posterior probability as the best word reor-
dering for a given source sentence. Word re-
orderings from different phrase segmentation but 
with same word surface order are merged. The 
posterior probabilities of the word re-orderings are 
computed as in Equation 3. 
1
1 1
( )( | )
J
J J
hyp
N rp r f
N
=                        (3) 
where 1( )
JN r  is the count of word reordering 1
Jr , 
and hypN  is the number of N-best hypotheses.  
The words of the source sentence are then reor-
dered according to their indices in the best selected 
word reordering 1
Jr . The procedure for self-
enhancement of word reordering is as follows. 
1. Run decoding and extract N-best hypotheses. 
2. Select the best word re-orderings according to 
the phrase-alignment information. 
3. Reorder the source sentences according to the 
selected word reordering. 
4. Optimize the weights of the decoder with the 
reordered source sentences. 
5. Repeat step 1-4 for a fixed number of iterations. 
3 Experiments and Results 
Experiments on Chinese-to-English NIST transla-
tion tasks were carried out on the FBIS1 corpus. 
We used NIST 2002 MT evaluation test set as our 
development set, and the NIST 2003, 2005 test sets 
as our test sets as shown in Table 1. 
We determine the number of iteration empiri-
cally by setting it to 10. We then observe the 
BLEU score on the development set for each itera-
tion. The iteration number which achieved the best 
BLEU score on development set is selected as the 
iteration number of iterations for the test set.  
 
                                                          
1 LDC2003E14 
#Running words Data set type 
Chinese English 
parallel 7.0M 8.9M train 
monolingual - 61.5M 
NIST 02 dev 23.2K 108.6K 
NIST 03 test 25.8K 116.5K 
NIST 05 test 30.5K 141.9K 
Table 1: Statistics of training, dev and test sets. Evalua-
tion sets of NIST campaigns include 4 references: total 
numbers of running words are provided in the table. 
 
System #iter. NIST 02 NIST 03 NIST 05
Base - 27.67 26.68 24.82 
TM 4 27.87 26.95 25.05 
LM 6 27.96 27.06 25.07 
WR 6 27.99 27.04 25.11 
Comb 7 28.45 27.35 25.46 
Table 2: BLEU% scores of five systems: decoder (Base), 
self-enhancement on translation model (TM), language 
model (LM), word reordering (WR) and the combina-
tion of TM, LM and WR (Comb). 
 
Further experiments also suggested that, in this 
experiment scenario, setting the size of N-best list 
to 3,000 arrives at the greatest performance im-
provements. Our evaluation metric is BLEU (Pap-
ineni et al, 2002). The translation performance is 
reported in Table 2, where the column ?#iter.? re-
fers to the iteration number where the system 
achieved the best BLEU score on development set. 
Compared with the baseline (?Base? in Table 2), 
all three self-enhancement methods (?TM?, ?LM?, 
and ?WR? in Table 2) consistently improved the 
performance. In general, absolute gains of 0.23- 
0.38 BLEU score were obtained for each method 
on two test sets. While comparing the performance 
among all three methods, we can see that they 
achieved very similar improvement. Combining 
the three methods showed further gains in BLEU 
score. Totally, the combined system outperformed 
the baseline by 0.67 BLEU score on NIST?03, and 
0.64 on NIST?05 test set, respectively. 
4 Discussion 
As posterior knowledge applied in our models are 
posterior probabilities, the main difference be-
tween our work and all previous work is the use of 
knowledge source, where we derive knowledge 
from the N-best hypotheses generated from previ-
ous iteration. 
159
Comparing the work of (Nakajima et al, 2002), 
there is a slight difference between the two models. 
Nakajima et al used only 1-best hypothesis, while 
we use N-best hypotheses of test set as the quasi-
corpus to train the language model. 
In the work of  (Costa-juss? and Fonollosa, 2006;  
Li et al, 2007) which similarly translates a source 
sentence (S) to target sentence (T) in two stages: 
S S T?? ? , they derive S ? from training data; 
while we obtain S ?  based on the occurrence fre-
quency, i.e. posterior probability of each source 
word reordering in the N-best hypotheses list. 
An alternative solution for enhancing the trans-
lation model is through self-training (Ueffing, 
2006; Ueffing et al, 2007) which re-trains the 
source-target N-best hypotheses together with the 
original training data, and thus differs from ours in 
the way of new phrase pairs extraction. We only 
supplement those phrase-pairs appeared in the N-
best hypotheses to the original phrase table. Fur-
ther experiment showed that improvement ob-
tained by self-training method is not as consistent 
on both development and test sets as that by our 
method. One possible reason is that in self-training, 
the entire translation model is adjusted with the 
addition of new phrase-pairs extracted from the 
source-target N-best hypotheses, and hence the 
effect is less predictable. 
5 Conclusions 
To take advantage of the N-best hypotheses, we 
proposed schemes in a re-decoding framework and 
made use of the posterior knowledge learned from 
the N-best hypotheses to improve a phrase-based 
SMT system. The posterior knowledge include 
posterior probabilities for target n-grams, transla-
tion phrase-pairs and source word re-orderings, 
which in turn improve the language model, transla-
tion model, and word reordering respectively. 
Experiments were based on the state-of-the-art 
phrase-based decoder and carried out on NIST 
Chinese-to-English task. It has been shown that all 
three methods improved the performance. More-
over, the combination of all three strategies outper-
forms each individual method and significantly 
outperforms the baseline. We demonstrated that 
the SMT system can be self-enhanced by exploit-
ing useful feedback from the N-best hypotheses 
which are generated by itself. 
References 
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. 
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2003. 
Confidence estimation for machine translation. Final 
report, JHU/CLSP Summer Workshop. 
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. 
Federico. 2005. The ITC-irst SMT System for 
IWSLT-2005. In Proceeding of IWSLT-2005, pp.98-
104, Pittsburgh, USA, October. 
M. R. Costa-juss?, J. A. R. Fonollosa. 2006. Statistical 
Machine Reordering. In Proceeding of EMNLP 2006. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, 
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. 
Herbst. 2007. Moses: Open Source Toolkit for Statis-
tical Machine Translation. In Proceedings of ACL-
2007, pp. 177-180, Prague, Czech Republic. 
C.-H. Li, M. Li, D. Zhang, M. Li, M. Zhou and Y. Guan. 
2007.  A Probabilistic Approach to Syntax-based Re-
ordering for Statistical Machine Translation. In Pro-
ceedings of ACL-2007. Prague, Czech Republic. 
H. Nakajima, H. Yamamoto, T. Watanabe. 2002.  Lan-
guage model adaptation with additional text gener-
ated by machine translation. In Proceedings of 
COLING-2002. Volume 1, Pages: 1-7. Taipei. 
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu, 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceeding of ACL-2002, pp. 
311-318. 
N. Ueffing. 2006. Using Monolingual Source-Language 
Data to Improve MT Performance. In Proceedings of 
IWSLT 2006. Kyoto, Japan. November 27-28. 
N. Ueffing, K. Macherey, and H. Ney. 2003. Confi-
dence Measures for Statistical Machine Translation. 
In Proceeding of MT Summit IX, pages 394?401, 
New Orleans, LA, September. 
N. Ueffing, G. Haffari, A. Sarkar. 2007. Transductive 
learning for statistical machine translation. In Pro-
ceedings of ACL-2007, Prague. 
F. Wessel. 2002. Word Posterior Probabilities for Large 
Vocabulary Continuous Speech Recognition. Ph.D. 
thesis, RWTH Aachen University. Aachen, Germany, 
January. 
R. Zens and H. Ney. 2006. N-gram Posterior Probabili-
ties for Statistical Machine Translation. In Proceed-
ings of the HLT-NAACL Workshop on SMT, pp. 72-
77, NY. 
160
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 941?948,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Comparative Study of Hypothesis Alignment and its Improvement 
for Machine Translation System Combination 
Boxing Chen*, Min Zhang, Haizhou Li and Aiti Aw 
 
Institute for Infocomm Research 
1 Fusionopolis Way, 138632 Singapore 
{bxchen, mzhang, hli, aaiti}@i2r.a-star.edu.sg 
 
 
Abstract 
Recently confusion network decoding shows 
the best performance in combining outputs 
from multiple machine translation (MT) sys-
tems. However, overcoming different word 
orders presented in multiple MT systems dur-
ing hypothesis alignment still remains the 
biggest challenge to confusion network-based 
MT system combination. In this paper, we 
compare four commonly used word align-
ment methods, namely GIZA++, TER, CLA 
and IHMM, for hypothesis alignment. Then 
we propose a method to build the confusion 
network from intersection word alignment, 
which utilizes both direct and inverse word 
alignment between the backbone and hypo-
thesis to improve the reliability of hypothesis 
alignment. Experimental results demonstrate 
that the intersection word alignment yields 
consistent performance improvement for all 
four word alignment methods on both Chi-
nese-to-English spoken and written language 
tasks. 
1 Introduction 
Machine translation (MT) system combination 
technique leverages on multiple MT systems to 
achieve better performance by combining their 
outputs. Confusion network based system com-
bination for machine translation has shown 
promising advantage compared with other tech-
niques based system combination, such as sen-
tence level hypothesis selection by voting and 
source sentence re-decoding using the phrases or 
translation models that are learned from the 
source sentences and target hypotheses pairs 
(Rosti et al, 2007a; Huang and Papineni, 2007). 
In general, the confusion network based sys-
tem combination method for MT consists of four 
steps: 1) Backbone selection: to select a back-
bone (also called ?skeleton?) from all hypotheses. 
The backbone defines the word orders of the fi-
nal translation. 2) Hypothesis alignment: to build 
word-alignment between backbone and each hy-
pothesis. 3) Confusion network construction: to 
build a confusion network based on hypothesis 
alignments. 4) Confusion network decoding: to 
decode the best translation from a confusion 
network. Among the four steps, the hypothesis 
alignment presents the biggest challenge to the 
method due to the varying word orders between 
outputs from different MT systems (Rosti et al 
2007). Many techniques have been studied to 
address this issue. Bangalore et al (2001) used 
the edit distance alignment algorithm which is 
extended to multiple strings to build confusion 
network, it only allows monotonic alignment. 
Jayaraman and Lavie (2005) proposed a heuris-
tic-based matching algorithm which allows non-
monotonic alignments to align the words be-
tween the hypotheses. More recently, Matusov et 
al. (2006, 2008) used GIZA++ to produce word 
alignment for hypotheses pairs. Sim et al (2007), 
Rosti et al (2007a), and Rosti et al (2007b) used 
minimum Translation Error Rate (TER) (Snover 
et al, 2006) alignment to build the confusion 
network. Rosti et al (2008) extended TER algo-
rithm which allows a confusion network as the 
reference to compute word alignment. Karakos et 
al. (2008) used ITG-based method for hypothesis 
alignment. Chen et al (2008) used Competitive 
Linking Algorithm (CLA) (Melamed, 2000) to 
align the words to construct confusion network. 
Ayan et al (2008) proposed to improve align-
ment of hypotheses using synonyms as found in 
WordNet (Fellbaum, 1998) and a two-pass 
alignment strategy based on TER word align-
ment approach. He et al (2008) proposed an 
IHMM-based word alignment method which the 
parameters are estimated indirectly from a varie-
ty of sources. 
Although many methods have been attempted, 
no systematic comparison among them has been 
reported. A through and fair comparison among 
them would be of great meaning to the MT sys-
941
tem combination research. In this paper, we im-
plement a confusion network-based decoder. 
Based on this decoder, we compare four com-
monly used word alignment methods (GIZA++, 
TER, CLA and IHMM) for hypothesis alignment 
using the same experimental data and the same 
multiple MT system outputs with similar features 
in terms of translation performance. We conduct 
the comparison study and other experiments in 
this paper on both spoken and newswire do-
mains: Chinese-to-English spoken and written 
language translation tasks. Our comparison 
shows that although the performance differences 
between the four methods are not significant, 
IHMM consistently show slightly better perfor-
mance than other methods. This is mainly due to 
the fact the IHMM is able to explore more know-
ledge sources and Viterbi decoding used in 
IHMM allows more thorough search for the best 
alignment while other methods has to use less 
optimal greedy search.  
In addition, for better performance, instead of 
only using one direction word alignment (n-to-1 
from hypothesis to backbone) as in previous 
work, we propose to use more reliable word 
alignments which are derived from the intersec-
tion of two-direction hypothesis alignment to 
construct confusion network. Experimental re-
sults show that the intersection word alignment-
based method consistently improves the perfor-
mance for all four methods on both spoken and 
written language tasks. 
This paper is organized as follows. Section 2 
presents a standard framework of confusion net-
work based machine translation system combina-
tion. Section 3 introduces four word alignment 
methods, and the algorithm of computing inter-
section word alignment for all four word align-
ment methods. Section 4 describes the experi-
ments setting and results on two translation tasks. 
Section 5 concludes the paper. 
2 Confusion network based system 
combination 
In order to compare different hypothesis align-
ment methods, we implement a confusion net-
work decoding system as follows: 
Backbone selection: in the previous work, 
Matusov et al (2006, 2008) let every hypothesis 
play the role of the backbone (also called ?skele-
ton? or ?alignment reference?) once. We follow 
the work of (Sim et al, 2007; Rosti et al, 2007a; 
Rosti et al, 2007b; He et al, 2008) and choose 
the hypothesis that best agrees with other hypo-
theses on average as the backbone by applying 
Minimum Bayes Risk (MBR) decoding (Kumar 
and Byrne, 2004).  TER score (Snover et al 
2006) is used as the loss function in MBR decod-
ing. Given a hypothesis set H, the backbone can 
be computed using the following equation, where  
( , )TER ? ?  returns the TER score of two hypothes-
es. 
 
?
?arg min ( , )b
E H E H
E TER E E
? ?
= ?           (1) 
Hypothesis alignment: all hypotheses are 
word-aligned to the corresponding backbone in a 
many-to-one manner. We apply four word 
alignment methods: GIZA++-based, TER-based, 
CLA-based, and IHMM-based word alignment 
algorithm. For each method, we will give details 
in the next section. 
Confusion network construction: confusion 
network is built from one-to-one word alignment; 
therefore, we need to normalize the word align-
ment before constructing the confusion network.  
The first normalization operation is removing 
duplicated links, since GIZA++ and IHMM-
based word alignments could be n-to-1 mappings 
between the hypothesis and backbone. Similar to 
the work of (He et al, 2008), we keep the link 
which has the highest similarity measure 
( , )j iS e e?  based on surface matching score, such 
as the length of maximum common subsequence 
(MCS) of the considered word pair. 
2 ( ( , ))
( , )
( ) ( )
j i
j i
j i
len MCS e e
S e e
len e len e
??
? =
? +
          (2) 
where ( , )j iMCS e e?  is the maximum common 
subsequence of word je?  and ie ; (.)len  is a 
function to compute the length of letter sequence. 
The other hypothesis words are set to align to the 
null word. For example, in Figure 1, 1e? and 3e?  
are aligned to the same backbone word 
2e , we 
remove the link between 
2e  and 3e?  if 
3 2 1 2( , ) ( , )S e e S e e? ?< , as shown in Figure 1 (b). 
The second normalization operation is reorder-
ing the hypothesis words to match the word order 
of the backbone. The aligned words are reor-
dered according to their alignment indices. To 
reorder the null-aligned words, we need to first 
insert the null words into the proper position in 
the backbone and then reorder the null-aligned 
hypothesis words to match the nulls on the back-
bone side. Reordering null-aligned words varies 
based to the word alignment method in the pre-
942
vious work. We reorder the null-aligned word 
following the approach of Chen et al (2008) 
with some extension. The null-aligned words are 
reordered with its adjacent word: moving with its 
left word (as Figure 1 (c)) or right word (as Fig-
ure 1 (d)). However, to reduce the possibility of 
breaking a syntactic phrase, we extend to choose 
one of the two above operations depending on 
which one has the higher likelihood with the cur-
rent null-aligned word. It is implemented by 
comparing two association scores based on co-
occurrence frequencies. They are association 
score of the null-aligned word and its left word, 
or the null-aligned word and its right word. We 
use point-wise mutual information (MI) as Equa-
tion 3 to estimate the likelihood. 
 11
1
( )
( , ) log
( ) ( )
i i
i i
i i
p e e
MI e e
p e p e
+
+
+
? ?
? ? =
? ?
              (3) 
where 1( )i ip e e +? ?  is the occurrence probability of 
bigram 1i ie e +? ?  observed in the hypothesis list; 
( )ip e?  and 1( )ip e +?  are probabilities of hypothe-
sis word ie?  and 1ie +?  respectively. 
In example of Figure 1, we choose (c) 
if 2 3 3 4( , ) ( , )MI e e MI e e? ? ? ?> , otherwise, word is 
reordered as (d). 
a 
1e  2e  3e  
 
    
 
1e?  2e?  3e?  4e?  
b 
1e  2e  3e  
 
    
 
1e?  2e?  3e?  4e?  
c 
1e  2e  3e  
 
4e?  1e?  2e?  3e?  
d 
 
1e  2e  3e  
3e?  4e?  1e?  2e?  
 
Figure 1: Example of alignment normalization. 
 
Confusion network decoding: the output 
translations for a given source sentence are ex-
tracted from the confusion network through a 
beam-search algorithm with a log-linear combi-
nation of a set of feature functions. The feature 
functions which are employed in the search 
process are:  
? Language model(s), 
? Direct and inverse IBM model-1, 
? Position-based word posterior probabili-
ties (arc scores of the confusion network), 
? Word penalty, 
? N-gram frequencies (Chen et al, 2005), 
? N-gram posterior probabilities (Zens and 
Ney, 2006). 
The n-grams used in the last two feature func-
tions are collected from the original hypotheses 
list from each single system. The weights of fea-
ture functions are optimized to maximize the 
scoring measure (Och, 2003). 
3 Word alignment algorithms 
We compare four word alignment methods 
which are widely used in confusion network 
based system combination or bilingual parallel 
corpora word alignment. 
3.1 Hypothesis-to-backbone word align-
ment 
GIZA++: Matusov et al (2006, 2008) proposed 
using GIZA++ (Och and Ney, 2003) to align 
words between the backbone and hypothesis. 
This method uses enhanced HMM model boot-
strapped from IBM Model-1 to estimate the 
alignment model. All hypotheses of the whole 
test set are collected to create sentence pairs for 
GIZA++ training. GIZA++ produces hypothesis-
backbone many-to-1 word alignments. 
TER-based: TER-based word alignment 
method (Sim et al, 2007; Rosti et al, 2007a; 
Rosti et al, 2007b) is an extension of multiple 
string matching algorithm based on Levenshtein 
edit distance (Bangalore et al, 2001). The TER 
(translation error rate) score (Snover et al, 2006) 
measures the ratio of minimum number of string 
edits between a hypothesis and reference where 
the edits include insertions, deletions, substitu-
tions and phrase shifts. The hypothesis is modi-
fied to match the reference, where a greedy 
search is used to select the set of shifts because 
an optimal sequence of edits (with shifts) is very 
expensive to find. The best alignment is the one 
that gives the minimum number of translation 
edits.  TER-based method produces 1-to-1 word 
alignments. 
CLA-based: Chen et al (2008) used competi-
tive linking algorithm (CLA) (Melamed, 2000) 
to build confusion network for hypothesis rege-
neration. Firstly, an association score is com-
puted for every possible word pair from the 
backbone and hypothesis to be aligned. Then a 
greedy algorithm is applied to select the best 
word alignment. We compute the association 
score from a linear combination of two clues: 
943
surface similarity computed as Equation (2) and 
position difference based distortion score by fol-
lowing (He et al, 2008). CLA works under a 1-
to-1 assumption, so it produces 1-to-1 word 
alignments. 
IHMM-based: He et al (2008) propose an 
indirect hidden Markov model (IHMM) for hy-
pothesis alignment. Different from traditional 
HMM, this model estimates the parameters indi-
rectly from various sources, such as word seman-
tic similarity, surface similarity and distortion 
penalty, etc. For fair comparison reason, we also 
use the surface similarity computed as Equation 
(2) and position difference based distortion score 
which are used for CLA-based word alignment. 
IHMM-based method produces many-to-1 word 
alignments. 
3.2 Intersection word alignment and its ex-
pansion 
In previous work, Matusov et al (2006, 2008) 
used both direction word alignments to compute 
so-called state occupation probabilities and then 
compute the final word alignment. The other 
work usually used only one direction word 
alignment (many/1-to-1 from hypothesis to 
backbone). In this paper, we use more reliable 
word alignments which are derived from the in-
tersection of both direct (hypothesis-to-backbone) 
and inverse (backbone-to-hypothesis) word 
alignments with heuristic-based expansion which 
is widely used in bilingual word alignment. The 
algorithm includes two steps: 
1) Generate bi-directional word alignments. It 
is straightforward for GIZA++ and IHMM to 
generate bi-directional word alignments. This is 
simply achieved by switching the parameters of 
source and target sentences. Due to the nature of 
greedy search in TER, the bi-directional TER-
based word alignments by switching the parame-
ters of source and target sentences are not neces-
sary exactly the same. For example, in Figure 2, 
the word ?shot? can be aligned to either ?shoot? 
or ?the? as the edit cost of word pair (shot, shoot) 
and (shot, the) are the same when compute the 
minimum-edit-distance for TER score. 
 
 
I shot  killer 
I shoot the killer 
a 
 
I shoot the killer 
I  shot killer 
b 
Figure 2: Example of two directions TER-based 
word alignments. 
 
For CLA word alignment, if we use the same 
association score, direct and inverse CLA word 
alignments should be exactly the same. There-
fore, we use different functions to compute the 
surface similarities, such as using maximum 
common subsequence (MCS) to compute inverse 
word alignment, and using longest matched pre-
fix (LMP) for computing direct word alignment, 
as in Equation (4). 
2 ( ( , ))
( , )
( ) ( )
j i
j i
j i
len LMP e e
S e e
len e len e
??
? =
? +
         (4) 
2) When two word alignments are ready, we 
start from the intersection of the two word 
alignments, and then continuously add new links 
between backbone and hypothesis if and only if 
both of the two words of the new link are un-
aligned and this link exists in the union of two 
word alignments. If there are more than two links 
share a same hypothesis or backbone word and 
also satisfy the constraints, we choose the link 
that with the highest similarity score. For exam-
ple, in Figure 2, since MCS-based similarity 
scores ( , ) ( , )S shot shoot S shot the> , we 
choose alignment (a). 
4  Experiments and results 
4.1 Tasks and single systems 
Experiments are carried out in two domains. One 
is in spoken language domain while the other is 
on newswire corpus. Both experiments are on 
Chinese-to-English translation. 
Experiments on spoken language domain were 
carried out on the Basic Traveling Expression 
Corpus (BTEC) (Takezawa et al, 2002) Chi-
nese- to-English data augmented with HIT-
corpus1. BTEC is a multilingual speech corpus 
which contains sentences spoken by tourists. 
40K sentence-pairs are used in our experiment. 
HIT-corpus is a balanced corpus and has 500K 
sentence-pairs in total. We selected 360K sen-
tence-pairs that are more similar to BTEC data 
according to its sub-topic. Additionally, the Eng-
lish sentences of Tanaka corpus2 were also used 
to train our language model. We ran experiments 
on an IWSLT challenge task which uses IWSLT-
20063 DEV clean text set as development set and 
IWSLT-2006 TEST clean text as test set. 
                                                 
1 http://mitlab.hit.edu.cn/ 
2 http://www.csse.monash.edu.au/~jwb/tanakacorpus.html 
3 http:// www.slc.atr.jp/IWSLT2006/ 
944
Experiments on newswire domain were car-
ried out on the FBIS4 corpus. We used NIST5 
2002 MT evaluation test set as our development 
set, and the NIST 2005 test set as our test set.  
Table 1 summarizes the statistics of the train-
ing, dev and test data for IWSLT and NIST tasks. 
 
task data Ch En 
 
 
 
IWSLT 
Train Sent. 406K 
Words 4.4M 4.6M 
Dev Sent. 489 489?7 
Words 5,896 45,449 
Test Sent. 500 500?7 
Words 6,296 51,227 
Add. Words - 1.7M 
 
 
 
NIST 
Train Sent. 238K 
Words 7.0M 8.9M 
Dev 
2002 
Sent. 878 878?4 
Words 23,248 108,616 
Test 
2005 
Sent. 1,082 1,082?4 
Words 30,544 141,915 
Add. Words - 61.5M 
 
Table 1: Statistics of training, dev and test data 
for IWSLT and NIST tasks. 
 
In both experiments, we used four systems, as 
listed in Table 2,  they are phrase-based system 
Moses (Koehn et al, 2007), hierarchical phrase-
based system (Chiang, 2007), BTG-based lexica-
lized reordering phrase-based system (Xiong et 
al., 2006) and a tree sequence alignment-based 
tree-to-tree translation system (Zhang et al, 
2008). Each system for the same task is trained 
on the same data set. 
4.2 Experiments setting 
For each system, we used the top 10 scored hy-
potheses to build the confusion network. Similar 
to (Rosti et al, 2007a), each word in the hypo-
thesis is assigned with a rank-based score of 
1/ (1 )r+ , where r is the rank of the hypothesis. 
And we assign the same weights to each system. 
For selecting the backbone, only the top hypo-
thesis from each system is considered as a candi-
date for the backbone. 
Concerning the four alignment methods, we 
use the default setting for GIZA++; and use tool-
kit TERCOM (Snover et al, 2006) to compute 
the TER-based word alignment, and also use the 
default setting. For fair comparison reason, we 
                                                 
4 LDC2003E14 
5 http://www.nist.gov/speech/tests/mt/ 
decide to do not use any additional resource, 
such as target language synonym list, IBM model 
lexicon; therefore, only surface similarity is ap-
plied in IHMM-based and CLA-based methods. 
We compute the distortion model by following 
(He et al, 2008) for IHMM and CLA-based me-
thods. The weights for each model are optimized 
on held-out data. 
 
 System Dev Test 
 
IWSLT
Sys1 30.75 27.58 
Sys2 30.74 28.54 
Sys3 29.99 26.91 
Sys4 31.32 27.48 
 
NIST 
Sys1 25.64 23.59 
Sys2 24.70 23.57 
Sys3 25.89 22.02 
Sys4 26.11 21.62 
 
Table 2: Results (BLEU% score) of single sys-
tems involved to system combination. 
4.3 Experiments results 
Our evaluation metric is BLEU (Papineni et al, 
2002), which are to perform case-insensitive 
matching of n-grams up to n = 4.  
Performance comparison of four methods: 
the results based on direct word alignments are 
reported in Table 3, row Best is the best single 
systems? scores; row MBR is the scores of back-
bone; GIZA++, TER, CLA, IHMM stand for 
scores of systems for four word alignment me-
thods. 
z MBR decoding slightly improves the per-
formance over the best single system for both 
tasks. This suggests that the simple voting strate-
gy to select backbone is workable. 
z For both tasks, all methods improve the per-
formance over the backbone. For IWSLT test set, 
the improvements are from 2.06 (CLA, 30.88-
28.82) to 2.52 BLEU-score (IHMM, 31.34-
28.82). For NIST test set, the improvements are 
from 0.63 (TER, 24.31-23.68) to 1.40 BLEU-
score (IHMM, 25.08-23.68). This verifies that 
the confusion network decoding is effective in 
combining outputs from multiple MT systems 
and the four word-alignment methods are also 
workable for hypothesis-to-backbone alignment. 
z For IWSLT task where source sentences are 
shorter (12-13 words per sentence in average), 
the four word alignment methods achieve similar 
performance on both dev and test set. The big-
gest difference is only 0.46 BLEU score (30.88 
for CLA, vs. 31.34 for IHMM). For NIST task 
945
where source sentences are longer (26-28 words 
per sentence in average), the difference is more 
significant. Here IHMM method achieves the 
best performance, followed by GIZA++, CLA 
and TER. IHMM is significantly better than TER 
by 0.77 BLEU-score (from 24.31 to 25.08, 
p<0.05). This is mainly because IHMM exploits 
more knowledge source and Viterbi decoding 
allows more thorough search for the best align-
ment while other methods use less optimal gree-
dy search. Another reason is that TER uses hard 
matching in computing edit distance. 
 
 method Dev Test 
 
 
IWSLT 
Best 31.32 28.54 
MBR 31.40 28.82 
GIZA++ 34.16 31.06 
TER 33.92 30.96 
CLA 33.85 30.88 
IHMM 34.35 31.34 
 
 
NIST 
Best 26.11 23.59 
MBR 26.36 23.68 
GIZA++ 27.58 24.88 
TER 27.15 24.31 
CLA 27.44 24.51
IHMM 27.76 25.08
 
Table 3: Results (BLEU% score) of combined 
systems based on direct word alignments. 
 
Performance improvement by intersection 
word alignment: Table 4 reports the perfor-
mance of the system combinations based on in-
tersection word alignments. It shows that: 
z Comparing Tables 3 and 4, we can see that 
the intersection word alignment-based expansion 
method improves the performance in all the dev 
and test sets for both tasks by 0.2-0.57 BLEU-
score and the improvements are consistent under 
all conditions. This suggests that the intersection 
word alignment-based expansion method is more 
effective than the commonly used direct word-
alignment-based hypothesis alignment method in 
confusion network-based MT system combina-
tion. This is because intersection word align-
ments are more reliable compared with direct 
word alignments, and so for heuristic-based ex-
pansion which is based on the aligned words 
with higher scores. 
z TER-based method achieves the biggest 
performance improvement by 0.4 BLEU-score in 
IWSLT and 0.57 in NIST. Our statistics shows 
that the TER-based word alignment generates 
more inconsistent links between the two-
directional word alignments than other methods. 
This may give the intersection with heuristic-
based expansion method more room to improve 
performance. 
z On the contrast, CLA-based method obtains 
relatively small improvement of 0.26 BLEU-
score in IWSLT and 0.21 in NIST. The reason 
could be that the similarity functions used in the 
two directions are more similar. Therefore, there 
are not so many inconsistent links between the 
two directions. 
z Table 5 shows the number of links modified 
by intersection operation and the BLEU-score 
improvement. We can see that the more the mod-
ified links, the bigger the improvement.  
 
 method Dev Test 
 
 
IWSLT
MBR 31.40 28.82
GIZA++ 34.38 31.40
TER 34.17 31.36
CLA 34.03 31.14
IHMM 34.59 31.74
 
 
NIST 
MBR 26.36 23.68
GIZA++ 27.80 25.11
TER 27.58 24.88
CLA 27.64 24.72
IHMM 27.96 25.37
 
Table 4: Results (BLEU% score) of combined 
systems based on intersection word alignments. 
 
 
 
system 
IWSLT NIST 
Inc. Imp. Inc. Imp.
CLA 1.2K 0.26 9.2K 0.21 
GIZA++ 3.2K 0.36 25.5K 0.23 
IHMM 3.7K 0.40 21.7K 0.29 
TER 4.3K 0.40 40.2K 0.57 
#total links 284K 1,390K 
 
Table 5: Number of modified links and absolute 
BLEU(%) score improvement on test sets. 
 
Effect of fuzzy matching in TER: the pre-
vious work on TER-based word alignment uses 
hard match in counting edits distance. Therefore, 
it is not able to handle cognate words match, 
such as in Figure 2, original TER script count the 
edit cost of (shoot, shot) equals to word pair 
(shot, the). Following (Leusch et al, 2006), we 
modified the TER script to allow fuzzy matching: 
change the substitution cost from 1 for any word 
pair to 
946
 ( , ) 1 ( , )sub j i j iCOST e e S e e? ?= ?              (5) 
which ( , )j iS e e?  is the similarity score based on 
the length of longest matched prefix (LMP) 
computed as in Equation (4).  As a result, the 
fuzzy matching reports 
( , ) 1 (2 3) /(5 4) 1/ 3SubCost shoot shot = ? ? + =  and 
( , ) 1 (2 0) /(5 3) 1SubCost shoot the = ? ? + =  while in 
original TER, both of the two scores are equal to 
1. Since cost of word pair (shoot, shot) is smaller 
than that of word pair (shot, the), word ?shot? 
has higher chance to be aligned to ?shoot? (Fig-
ure 2 (a)) instead of ?the? (Figure 2 (b)). This 
fuzzy matching mechanism is very useful to such 
kind of monolingual alignment task as in hypo-
thesis-to-backbone word alignment since it can 
well model word variances and morphological 
changes. 
Table 6 summaries the results of TER-based 
systems with or without fuzzy matching. We can 
see that the fuzzy matching improves the per-
formance for all cases. This verifies the effect of 
fuzzy matching for TER in monolingual word 
alignment. In addition, the improvement in NIST 
test set (0.36 BLEU-score for direct alignment 
and 0.21 BLEU-score for intersection one) are 
more than that in IWSLT test set (0.15 BLEU-
score for direct alignment and 0.11 BLEU-score 
for intersection one). This is because the sen-
tences of IWSLT test set are much shorter than 
that of NIST test set. 
 
TER-based 
systems 
IWSLT NIST 
Dev Test Dev Test 
Direct align 
+fuzzy match 
33.92 
34.14
30.96 
31.11 
27.15 
27.53
24.31 
24.67
Intersect align 
    +fuzzy match 
34.17 
34.40
31.36 
31.47 
27.58 
27.79
24.88 
25.09
 
Table 6: Results (BLEU% score) of TER-based 
combined systems with or without fuzzy match. 
5 Conclusion 
Confusion-network-based system combination 
shows better performance than other methods in 
combining multiple MT systems? outputs, and 
hypothesis alignment is a key step. In this paper, 
we first compare four word alignment methods 
for hypothesis alignment under the confusion 
network framework. We verify that the confu-
sion network framework is very effective in MT 
system combination and IHMM achieves the best 
performance. Moreover, we propose an intersec-
tion word alignment-based expansion method for 
hypothesis alignment, which is more reliable as it 
leverages on both direct and inverse word align-
ment. Experimental results on Chinese-to-
English spoken and newswire domains show that 
the intersection word alignment-based method 
yields consistent improvements across all four 
word alignment methods. Finally, we evaluate 
the effect of fuzzy matching for TER. 
Theoretically, confusion network decoding is 
still a word-level voting algorithm although it is 
more complicated than other sentence-level vot-
ing algorithms. It changes lexical selection by 
considering the posterior probabilities of words 
in hypothesis lists. Therefore, like other voting 
algorithms, its performance strongly depends on 
the quality of the n-best hypotheses of each sin-
gle system. In some extreme cases, it may not be 
able to improve BLEU-score (Mauser et al, 
2006; Sim et al, 2007). 
 
References  
N. F. Ayan. J. Zheng and W. Wang. 2008. Improving 
Alignments for Better Confusion Networks for 
Combining Machine Translation Systems. In Pro-
ceedings of COLING 2008, pp. 33?40. Manchester, 
Aug. 
S. Bangalore, G. Bordel, and G. Riccardi. 2001. 
Computing consensus translation from multiple 
machine translation systems. In Proceeding of 
IEEE workshop on Automatic Speech Recognition 
and Understanding, pp. 351?354. Madonna di 
Campiglio, Italy. 
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. 
Federico. 2005. The ITC-irst SMT System for 
IWSLT-2005. In Proceeding of IWSLT-2005, 
pp.98-104, Pittsburgh, USA, October. 
B. Chen, M. Zhang, A. Aw and H. Li. 2008. Regene-
rating Hypotheses for Statistical Machine Transla-
tion. In: Proceeding of COLING 2008. pp105-112. 
Manchester, UK. Aug. 
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228. 
C. Fellbaum. editor. 1998. WordNet: An Electronic 
Lexical Database. MIT Press. 
X. He, M. Yang, J. Gao, P. Nguyen, R. Moore, 2008. 
Indirect-HMM-based Hypothesis Alignment for 
Combining Outputs from Machine Translation 
Systems. In Proceeding of EMNLP. Hawaii, US, 
Oct. 
F. Huang and K. Papinent. 2007. Hierarchical System 
Combination for Machine Translation. In Proceed-
ings of the 2007 Joint Conference on Empirical 
Methods in Natural Language Processing and 
947
Computational Natural Language Learning 
(EMNLP-CoNLL?2007), pp. 277 ? 286, Prague, 
Czech Republic, June. 
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching. 
In Proceeding of EAMT. pp.143?152. 
D. Karakos, J. Eisner, S. Khudanpur, and M. Dreyer. 
2008. Machine Translation System Combination 
using ITG-based Alignments. In Proceeding of 
ACL-HLT 2008, pp. 81?84. 
O. Kraif, B. Chen. 2004. Combining clues for lexical 
level aligning using the Null hypothesis approach. 
In: Proceedings of COLING 2004, Geneva, Au-
gust, pp. 1261-1264.  
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and 
E. Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In Proceedings of 
ACL-2007. pp. 177-180, Prague, Czech Republic. 
S. Kumar and W. Byrne. 2004. Minimum Bayes Risk 
Decoding for Statistical Machine Translation. In    
Proceedings of HLT-NAACL 2004, May 2004, 
Boston, MA, USA. 
G. Leusch, N. Ueffing and H. Ney. 2006. CDER: Ef-
ficient MT Evaluation Using Block Movements. In 
Proceedings of EACL. pp. 241-248. Trento Italy. 
E. Matusov, N. Ueffing, and H. Ney. 2006. Compu-
ting consensus translation from multiple machine 
translation systems using enhanced hypotheses 
alignment. In Proceeding of EACL, pp. 33-40, 
Trento, Italy, April. 
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. 
Dechelotte, M. Federico, M. Kolss, Y. Lee, J. B. 
Marino, M. Paulik, S. Roukos, H. Schwenk, and H. 
Ney. System Combination for Machine Translation 
of Spoken and Written Language. IEEE Transac-
tions on Audio, Speech and Language Processing, 
volume 16, number 7, pp. 1222-1237, September. 
A. Mauser, R. Zens, E. Matusov, S. Hasan, and H. 
Ney. 2006. The RWTH Statistical Machine Trans-
lation System for the IWSLT 2006 Evaluation. In 
Proceeding of IWSLT 2006, pp. 103-110, Kyoto, 
Japan, November. 
I. D. Melamed. 2000. Models of translational equiva-
lence among words. Computational Linguistics, 
26(2), pp. 221-249. 
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL-
2003. Sapporo, Japan. 
F. J. Och and H. Ney. 2003. A systematic comparison 
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19-51. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 
2002. BLEU: a method for automatic evaluation of 
machine translation. In Proceeding of ACL-2002, 
pp. 311-318. 
A. I. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. 
Schwartz and B. Dorr. 2007a. Combining Outputs 
from Multiple Machine Translation Systems.  In 
Proceeding of NAACL-HLT-2007, pp. 228-235. 
Rochester, NY. 
A. I. Rosti, S. Matsoukas and R. Schwartz. 2007b. 
Improved Word-Level System Combination for 
Ma-chine Translation. In Proceeding of ACL-2007, 
Prague. 
A. I. Rosti, B. Zhang, S. Matsoukas, and R. Schwartz. 
2008. Incremental Hypothesis Alignment for 
Building Confusion Networks with Application to 
Machine Translation System Combination, In Pro-
ceeding of the Third ACL Workshop on Statistical 
Machine Translation, pp. 183-186. 
K. C. Sim, W. J. Byrne, M. J.F. Gales, H. Sahbi, and 
P. C. Woodland. 2007. Consensus network decod-
ing for statistical machine translation system com-
bination. In Proceeding of  ICASSP-2007. 
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. 
Makhoul. 2006. A study of translation edit rate 
with targeted human annotation. In Proceeding of 
AMTA. 
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, 
and S. Yamamoto. 2002. Toward a broad-coverage 
bilingual corpus for speech translation of travel 
conversations in the real world. In Proceeding of 
LREC-2002, Las Palmas de Gran Canaria, Spain. 
D. Xiong, Q. Liu and S. Lin. 2006. Maximum Entro-
py Based Phrase Reordering Model for Statistical 
Machine Translation. In Proceeding of ACL-2006. 
pp.521-528.  
R. Zens and H. Ney. 2006. N-gram Posterior Prob-
abilities for Statistical Machine Translation. In 
Proceeding of HLT-NAACL Workshop on SMT, pp. 
72-77, NY. 
M. Zhang, H. Jiang, A. Aw, H. Li, C. L. Tan, and S. 
Li. 2008. A Tree Sequence Alignment-based Tree-
to-Tree Translation Model. In Proceeding of ACL-
2008. Columbus, US. June. 
Y. Zhang, S. Vogel, and A. Waibel 2004. Interpreting 
BLEU/NIST scores: How much improvement do 
we need to have a better system? In Proceedings of 
LREC 2004, pp. 2051-2054. 
                                                 
*  The first author has moved to National Research 
Council, Canada. His current email address is: Box-
ing.Chen@nrc.ca. 
948
A Web-based Demonstrator of a Multi-lingual Phrase-based
Translation System
Roldano Cattoni, Nicola Bertoldi, Mauro Cettolo, Boxing Chen and Marcello Federico
ITC-irst - Centro per la Ricerca Scientifica e Tecnologica
38050 Povo - Trento, Italy
{surname}@itc.it
Abstract
This paper describes a multi-lingual
phrase-based Statistical Machine Transla-
tion system accessible by means of a Web
page. The user can issue translation re-
quests from Arabic, Chinese or Spanish
into English. The same phrase-based sta-
tistical technology is employed to realize
the three supported language-pairs. New
language-pairs can be easily added to the
demonstrator. The Web-based interface al-
lows the use of the translation system to
any computer connected to the Internet.
1 Introduction
At this time, Statistical Machine Translation
(SMT) has empirically proven to be the most
competitive approach in international competi-
tions like the NIST Evaluation Campaigns1 and
the International Workshops on Spoken Language
Translation (IWSLT-20042 and IWSLT-20053).
In this paper we describe our multi-lingual
phrase-based Statistical Machine Translation sys-
tem which can be accessed by means of a Web
page. Section 2 presents the general log-linear
framework to SMT and gives an overview of
our phrase-based SMT system. In section 3
the software architecture of the demo is out-
lined. Section 4 focuses on the currently supported
language-pairs: Arabic-to-English, Chinese-to-
English and Spanish-to-English. In section 5 the
Web-based interface of the demo is described.
1http://www.nist.gov/speech/tests/mt/
2http://www.slt.atr.jp/IWSLT2004/
3http://www.is.cs.cmu.edu/iwslt2005/
2 SMT System Description
2.1 Log-Linear Model
Given a string f in the source language, the goal of
the statistical machine translation is to select the
string e in the target language which maximizes
the posterior distribution Pr(e | f). By introduc-
ing the hidden word alignment variable a, the fol-
lowing approximate optimization criterion can be
applied for that purpose:
e? = arg max
e
Pr(e | f)
= arg max
e
?
a
Pr(e,a | f)
? arg max
e,a
Pr(e,a | f)
Exploiting the maximum entropy (Berger et
al., 1996) framework, the conditional distribu-
tion Pr(e,a | f) can be determined through
suitable real valued functions (called features)
hr(e, f ,a), r = 1 . . . R, and takes the parametric
form:
p?(e,a | f) ? exp{
R
?
r=1
?rhr(e, f ,a)}
The ITC-irst system (Chen et al, 2005) is
based on a log-linear model which extends the
original IBM Model 4 (Brown et al, 1993)
to phrases (Koehn et al, 2003; Federico and
Bertoldi, 2005). In particular, target strings e are
built from sequences of phrases e?1 . . . e?l. For each
target phrase e? the corresponding source phrase
within the source string is identified through three
random quantities: the fertility ?, which estab-
lishes its length; the permutation pii, which sets
its first position; the tablet f? , which tells its word
string. Notice that target phrases might have fer-
tility equal to zero, hence they do not translate any
91
source word. Moreover, uncovered source posi-
tions are associated to a special target word (null)
according to specific fertility and permutation ran-
dom variables.
The resulting log-linear model applies eight fea-
ture functions whose parameters are either esti-
mated from data (e.g. target language models,
phrase-based lexicon models) or empirically fixed
(e.g. permutation models). While feature func-
tions exploit statistics extracted from monolingual
or word-aligned texts from the training data, the
scaling factors ? of the log-linear model are esti-
mated on the development data by applying a min-
imum error training procedure (Och, 2004).
2.2 Decoding Strategy
The translation of an input string is performed by
the SMT system in two steps. In the first pass a
beam search algorithm (decoder) computes a word
graph of translation hypotheses. Hence, either
the best translation hypothesis is directly extracted
from the word graph and output, or an N-best list
of translations is computed (Tran et al, 1996). The
N-best translations are then re-ranked by applying
additional features and the top ranking translation
is finally output.
The decoder exploits dynamic programming,
that is the optimal solution is computed by expand-
ing and recombining previously computed partial
theories. A theory is described by its state which is
the only information needed for its expansion. Ex-
panded theories sharing the same state are recom-
bined, that is only the best scoring one is stored
for further expansions. In order to output a word
graph of translations, backpointers to all expanded
theories are mantained, too.
To cope with the large number of generated the-
ories some approximations are introduced during
the search: less promising theories are pruned off
(beam search) and a new source position is se-
lected by limiting the number of vacant positions
on the left-hand and the distance from the left most
vacant position (re-ordering constraints).
2.3 Phrase extraction and model training
Training of the phrase-based translation model
requires a parallel corpus provided with word-
alignments in both directions, i.e. from source
to target positions, and viceversa. This pre-
processing step can be accomplished by applying
the GIZA++ toolkit (Och and Ney, 2003) that pro-
vides Viterbi alignments based on IBM Model-4.
Starting from the parallel training corpus, pro-
vided with direct and inverted alignments, the so-
called union alignment (Och and Ney, 2003) is
computed.
Phrase-pairs are extracted from each sentence pair
which correspond to sub-intervals of the source
and target positions, J and I , such that the union
alignment links all positions of J into I and all
positions of I into J . In general, phrases are ex-
tracted with maximum length in the source and tar-
get defined by the parameters Jmax and Imax. All
such phrase-pairs are efficiently computed by an
algorithm with complexity O(lImaxJ2max) (Cet-
tolo et al, 2005).
Given all phrase-pairs extracted from the train-
ing corpus, lexicon probabilities and fertility prob-
abilities are estimated.
Target language models (LMs) used by the de-
coder and rescoring modules are, respectively,
estimated from 3-gram and 4-gram statistics
by applying the modified Kneser-Ney smoothing
method (Goodman and Chen, 1998). LMs are es-
timated with an in-house software toolkit which
also provides a compact binary representation of
the LM which is used by the decoder.
3 Demo Architecture
Figure 1 shows the two-layer architecture of the
demo. At the bottom lie the programs that provide
the actual translation services: for each language-
pair a wrapper coordinates the activity of a special-
ized pre-processing tool and a MT decoder. The
translation programs run on a grid-based cluster
of high-end PCs to optimize the processing speed.
All the wrappers communicate with the MT front-
end whose main task is to forward translation re-
quests to the appropriate language-pair wrapper
and to report an error in case of wrong requests
(e.g. unsupported language-pair). It is worth
noticing here that a new language-pair can be eas-
ily added to the system with a minimal interven-
tion on the code of the MT front-end.
At the top of the architecture are the programs
that provide the interface with the user. This layer
is separated from the translation layer (hosted by
internal machines only) by means of a firewall.
The user interface is implemented as a Web page
in which a translation request (a source sentence
and a language-pair) is input by means of an
HTML form. The cgi script invocated by the form
manages the interaction with the MT front-end.
92
Web Page
(form)
script
CGI
lang 1
wrapper
prepro?
cessing
MT
decoder
prepro?
cessing
MT
decoder
wrapper
lang 2
prepro?
cessing
MT
decoder
wrapper
lang N
...
MT
front?end
firewall
external host
internal hosts
fast machines
Figure 1: Architecture of the demo. For each
language-pair a set of programs (in particular the
MT decoder) provides the translation service. The
request issued by the user on the Web page is
sent by the cgi script to the MT front-end. The
translation is then performed on the appropriate
language-pair service and the output sent back to
the Web browser.
When a user issues a translation request after
filling the form fields, the cgi script sends the re-
quest to the MT front-end and waits for its reply.
The input sentence is then forwarded to the wrap-
per of the appropriate language-pair. After a pre-
processing step, the actual translation is performed
by the specific MT decoder. The output in the tar-
get language is then sent back to the user?s Web
browser through the chain in the reverse order.
From a technical point of view, the inter-process
communication is realized by means of standard
TCP-IP sockets. As far as the encoding of texts is
concerned, all the languages are encoded in UTF-
8: this allows to manage the processing phase in
an uniform way and to render graphically different
character sets.
4 The supported language-pairs
Although there is no theoretical limit to the num-
ber of supported language-pairs, the current ver-
sion of the demo provides translations to English
from three source languages: Arabic, Chinese and
Spanish. For demonstration purpose, three differ-
ent application domains are covered too.
Arabic-to-English (Tourism)
The Arabic-to-English system has been trained
with the data provided by the International Work-
shop on Spoken Language Translation 2005 The
context is that of the Basic Traveling Expres-
sion Corpus (BTEC) task (Takezawa et al, 2002).
BTEC is a multilingual speech corpus which con-
tains sentences coming from phrase books for
tourists. Training set includes 20k sentences con-
taining 159K Arabic and 182K English running
words; vocabulary size is 18K for Arabic, 7K for
English.
Chinese-to-English (Newswire)
The Chinese-to-English system has been trained
with the data provided by the NIST MT Evaluation
Campaign 2005 , large-data condition. In this case
parallel data are mainly news-wires provided by
news agencies. Training set includes 71M Chinese
and 77M English running words; vocabulary size
is 157K for Chinese, 214K for English.
Spanish-to-English (European Parliament)
The Spanish-to-English system has been trained
with the data provided by the Evaluation Cam-
paign 2005 of the European integrated project TC-
STAR4. The context is that of the speeches of
the European Parliament Plenary sessions (EPPS)
from April 1996 to October 2004. Training set for
the Final Text Edition transcriptions includes 31M
Spanish and 30M English running words; vocabu-
lary size is 140K for Spanish, 94K for English.
5 The Web-based Interface
Figure 2 shows a snapshot of the Web-based in-
terface of the demo ? the URL has been removed
to make this submission anonymous. In the upper
part of the page the user provides the two informa-
tion required for the translation: the source sen-
tence can be input in a 80x5 textarea html struc-
ture, while the language-pair can be selected by
means of a set a radio-buttons. The user can reset
the input area or send the translation request by
means of standard reset and submit buttons. Some
examples of bilingual sentences are provided in
the lower part of the page.
4http://www.tc-star.org
93
Figure 2: A snapshot of the Web-based interface.
The user provides the sentence to be translated
in the desired language-pair. Some examples of
bilingual sentences are also available to the user.
The output of a translation request is simple: the
requested source sentence, the translation in the
target language and the selected language-pair are
presented to the user. Figure 3 shows an example
of an Arabic sentence translated into English.
We plan to extend the interface with the pos-
sibility for the user to ask additional information
about the translation ? e.g. the number of explored
theories or the score of the first-best translation.
6 Acknowledgements
This work has been funded by the European Union
under the integrated project TC-STAR - Technol-
ogy and Corpora for Speech to Speech Translation
- (IST-2002-FP6-506738, http://www.tc-star.org).
References
A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22(1):39?71.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263?313.
Figure 3: Example of an Arabic sentence trans-
lated into English.
Mauro Cettolo, Marcello Federico, Nicola Bertoldi,
Roldano Cattoni, and Boxing Chen. 2005. A look
inside the itc-irst smt system. In Proceedings of the
10th Machine Translation Summit, pages 451?457,
Phuket, Thailand, September.
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo, and
M. Federico. 2005. The ITC-irst SMT System for
IWSLT-2005. In Proceedings of the IWSLT 2005,
Pittsburgh, USA.
M. Federico and N. Bertoldi. 2005. A Word-to-Phrase
Statistical Translation Model. ACM Transactions on
Speech and Language Processing. to appear.
J. Goodman and S. Chen. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University, August.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of HLT-
NAACl 2003, pages 127?133, Edmonton, Canada.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F.J. Och. 2004. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, Sapporo, Japan.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a Broad-Coverage
Bilingual Corpus for Speech Translation of Travel
Conversations in the Real World. In Proceedings of
3rd LREC, pages 147?152, Las Palmas, Spain.
B. H. Tran, F. Seide, and V. Steinbiss. 1996. A Word
Graph based N-Best Search in Continuous Speech
Recognition. In Proceedings of ICLSP, Philadel-
phia, PA, USA.
94
	

	





	

	
		

	
	Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608?616,
Beijing, August 2010
Phrase Clustering for Smoothing TM Probabilities ? or, How to 
Extract Paraphrases from Phrase Tables  
1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford 
1National Research Council of Canada 
2University of Waterloo 
1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com 
 
Abstract 
This paper describes how to cluster to-
gether the phrases of a phrase-based sta-
tistical machine translation (SMT) sys-
tem, using information in the phrase table 
itself. The clustering is symmetric and 
recursive: it is applied both to source-
language and target-language phrases, 
and the clustering in one language helps 
determine the clustering in the other. The 
phrase clusters have many possible uses. 
This paper looks at one of these uses: 
smoothing the conditional translation 
model (TM) probabilities employed by 
the SMT system. We incorporated 
phrase-cluster-derived probability esti-
mates into a baseline loglinear feature 
combination that included relative fre-
quency and lexically-weighted condition-
al probability estimates. In Chinese-
English (C-E) and French-English (F-E) 
learning curve experiments, we obtained 
a gain over the baseline in 29 of 30 tests, 
with a maximum gain of 0.55 BLEU 
points (though most gains were fairly 
small). The largest gains came with me-
dium (200-400K sentence pairs) rather 
than with small (less than 100K sentence 
pairs) amounts of training data, contrary 
to what one would expect from the pa-
raphrasing literature. We have only be-
gun to explore the original smoothing 
approach described here.  
1 Introduction and Related Work 
The source-language and target-language ?phras-
es? employed by many statistical machine trans-
lation (SMT) systems are anomalous: they are 
arbitrary sequences of contiguous words ex-
tracted by complex heuristics from a bilingual 
corpus, satisfying no formal linguistic criteria. 
Nevertheless, phrase-based systems perform bet-
ter than word-based systems (Koehn 2010, pp. 
127-129). In this paper, we look at what happens 
when we cluster together these anomalous but 
useful entities.  
Here, we apply phrase clustering to obtain bet-
ter estimates for ?backward? probability P(s|t) 
and ?forward? probability P(t|s), where s is a 
source-language phrase, t is a target-language 
phrase, and phrase pair (s,t) was seen at least 
once in training data. The current work is thus 
related to work on smoothing P(s|t) and P(t|s) ? 
see (Foster et al, 2006). The relative frequency 
estimates for P(s|t) and P(t|s) are  
ttstsPRF /#),(#)|( = and stsstPRF /#),(#)|( = , 
where #(s,t) denotes the number of times phrase 
pair (s,t) was observed, etc. These estimates are 
typically smoothed with ?lexical? estimates 
found by breaking phrases s and t into words. 
We adopt a different idea, that of smoothing 
PRF(s|t) and PRF(t|s) with estimates obtained from 
phrases that have similar meanings to s and t. In 
our experiments, the two methods were com-
bined, yielding an improvement over lexical 
smoothing alone ? this indicates they provide 
complementary information. E.g., lexical esti-
mates don?t work well for non-compositional 
phrases like ?kick the bucket? - our method 
might cluster this phrase with ?die? and ?expire? 
and thus provide better smoothing. The research 
that comes closest to ours is the work of 
Schwenk et al (2007) on continuous space N-
gram models, where a neural network is em-
ployed to smooth translation probabilities. How-
ever, both Schwenk et al?s smoothing technique 
608
and the system to which it is applied are quite 
different from ours. 
Phrase clustering is also somewhat related to 
work on paraphrases for SMT. Key papers in this 
area include (Bannard and Callison-Burch, 2005), 
which pioneered the extraction of paraphrases 
from bilingual parallel corpora, (Callison-Burch 
et al, 2006) which showed that paraphrase gen-
eration could improve SMT performance, (Calli-
son-Burch, 2008) and (Zhao et al, 2008) which 
showed how to improve the quality of paraphras-
es, and (Marton et al, 2009) which derived pa-
raphrases from monolingual data using distribu-
tional information. Paraphrases typically help 
SMT systems trained on under 100K sentence 
pairs the most.  
The phrase clustering algorithm in this paper 
outputs groups of source-language and target-
language phrases with similar meanings: paraph-
rases. However, previous work on paraphrases 
for SMT has aimed at finding translations for 
source-language phrases in the system?s input 
that weren?t seen during system training. Our 
approach is completely useless in this situation: 
it only generates new information for target or 
source phrases that are already in the system?s 
phrase table. Thus, we find paraphrases for many 
of the source and target phrases that are in the 
phrase table, while the work cited above looks 
for paraphrases of source phrases that are not in 
the phrase table.  
Our work also differs from most work on pa-
raphrases in that information is extracted not 
from sources outside the SMT system (e.g., pivot 
languages or thesauri) but from the system?s 
phrase table. In this respect if no other, it is simi-
lar to Chiang?s classic work on hierarchical 
phrase-based systems (Chiang, 2005), though 
Chiang was mining a very different type of in-
formation from phrase tables. 
Because of all these differences between work 
on paraphrasing and the phrase clustering ap-
proach, both in terms of the input information 
and where they are best applied, we did not expe-
rimentally compare the two approaches.     
2 Deriving Conditional Probabilities 
from Phrase Clusters 
Given phrase clusters in the source and target 
languages, how would one derive estimates for 
conditional probabilities P(s|t) and P(t|s)? We 
assume that the clustering is ?hard?: each source 
phrase s belongs to exactly one cluster C(s), and 
each target phrase t belongs to exactly one 
cluster C(t). Some of these clusters will contain 
singleton phrases, and others will contain more 
than one phrase. Let ?#? denote the total number 
of observations in the training data associated 
with a phrase or phrase cluster. E.g., suppose the 
English cluster CS contains the three phrases 
?red?, ?dark red?, and ?burgundy?, with 50, 25, 
and 10 observations in the training data 
respectively ? then #(CS) = 85. Also, let #(CS,CT) 
be the number of co-occurrences in the training 
data of source-language cluster CS and target-
language cluster CT.  
The phrase-cluster-based probabilities PPC are: 
)(#
))(),((#
)(#
)(#
))(|)(())(|()|(
tC
tCsC
sC
s
tCsCPsCsPtsPPC
?=
?=
  (1) 
and 
)(#
))(),((#
)(#
)(#
))(|)(())(|()|(
sC
tCsC
tC
t
sCtCPtCtPstPPC
?=
?=
   (2) 
Note that the PPC will often be non-zero where 
the corresponding relative frequency estimates 
PRF were zero (the opposite can?t happen). Also, 
the PPC will be most useful where the phrase be-
ing conditioned on was seldom seen in the train-
ing data. If t was seen 1,000 times during train-
ing, the PRF(s|t) are reliable and don?t need 
smoothing; but if t was seen 6 times,  PPC(s|t) 
may yield valuable extra information. The same 
kind of argument applies to estimation of P(t|s). 
3 Clustering Phrases 
We used only information ?native? to phrase 
tables to cluster phrases. Two types of similarity 
metric between phrases or phrase clusters were 
employed: count-based metrics and edit-based 
metrics. The former are based on phrase co-
occurrence counts; the latter are based on the 
word sequences that make up the phrases. Each 
has its advantages. Count-based metrics can de-
duce from the similar translations of two phrases 
that they have similar meanings, despite dissimi-
larity between the two word sequences ? e.g., 
they can deduce that ?red? and ?burgundy? be-
long in the same cluster. However, these metrics 
are unreliable when total counts are low, since 
phrase co-occurrences are determined by a noisy 
alignment process. Edit-based metrics are inde-
pendent of how often phrases were observed. 
However, sometimes they can be fooled by 
phrases that have similar word sequences but 
different meanings (e.g., ?the dog bit the man? 
609
and ?the man bit the dog?, or ?walk on the 
beach? and ?don?t walk on the beach?). In our 
experiments, we used a combination of count-
based and edit-based metrics to cluster phrases 
(by simply multiplying the metrics together). 
However, we invested most of our effort in per-
fecting the count-based component: our edit-
based metric was fairly na?ve.  
If we rely mainly on count-based similarity 
between phrases to cluster them, and this kind of 
similarity is most reliable when phrases have 
high counts, yet we need phrase-cluster-based 
estimates most for phrases with low counts, 
aren?t we carrying out clustering on the phrases 
that need it least? Our hope was that there is a 
class of phrases with intermediate counts (e.g., 
with 3-15 observations in the training data) that 
can be clustered reliably, but still benefit from 
phrase-cluster-based probability estimates.  
3.1 Count-based clustering: overview  
Figure 1 shows count-based phrase clustering. 
One first arbitrarily picks a language (either 
source or target) and then clusters together some 
of the phrases in that language. One then switch-
es to the other language and clusters phrases in 
that language, then switches back to the first one, 
and so on until enough clustering has taken place.  
Each phrase or phrase cluster is represented by 
the vector of its co-occurrence counts. To calcu-
late the similarity between two phrase clusters, 
one first normalizes their count vectors. At the 
top of Figure 1, source phrase s1 occurred 9 
times: 7 times aligned with target phrase t1, 2 
times aligned with t4. For source similarity com-
putation, the entry for (s1,t1) is normalized to 7/9 
= 0.78 and the entry for (s1,t4) is normalized to 
2/9 = 0.22 (these normalized values are shown in 
brackets and italics after the counts).  
The two most similar normalized vectors at 
the top of Figure 1 are those associated with 
phrases s1 and s2. These phrases are merged by 
adding corresponding counts, yielding a new 
vector associated with the new phrase cluster {s1, 
s2}. In real life, one would now do more source-
language clustering on the source language side; 
in this example, we immediately proceed to tar-
get-language clustering (carried out in target lan-
guage space). Note that the target similarity cal-
culations are affected by the previous source 
clustering (because s1 and s2 are now 
represented by the same coordinate, t3 and t4 are 
now closer than they were in the initial table). In 
this manner, we can iterate back and forth be-
tween the two languages. The final output is a 
table of joint phrase cluster counts, which is used 
to estimate the PPC (see previous section).   
3.2 Count-based clustering: details 
Count-based similarity is computed as follows:   
1. Phrase alignment is a noisy process, so 
we first apply a transformation analogous 
to tf-idf in information retrieval (Salton 
and McGill, 1986) to phrase cluster 
 
Figure 1: Example of phrase clustering 
 
610
counts. For source similarity computation, 
each co-occurrence count #(CS,CT) be-
tween source cluster CS and target cluster 
CT is multiplied by a factor that reflects 
the information content of CT. Let 
#diff(CS) be number of clusters on the 
source side, and let #[CT>0] for a par-
ticular target cluster CT be the number of 
source clusters CS that co-occur with CT. 
Then let 
])0[/#)(log(#),(#),('# >?= TSTSTS CCdiffCCCC .   
Similarly, for target similarity computa-
tion, let 
])0[/#)(log(#),(#),('# >?= STTSTS CCdiffCCCC .   
E.g., in source similarity computation, if 
CT co-occurs with all source clusters, its 
contribution will be set to zero (because 
it carries little information).  
2. We normalize by dividing each vector of 
tf-idf counts ),('# TS CC  by the total num-
ber of observations in the vector. 
3. We compute the similarity between each 
pair of tf-idf vectors using either the co-
sine measure (Salton and McGill, 1986) 
or one of a family of probabilistic metrics 
described below.  
4. We cluster together the most similar vec-
tors; this involves summing the unmodi-
fied counts #(CS,CT) of the vectors (i.e., 
the tf-idf transformation is only applied 
for the purposes of similarity calculation 
and is not retained).  
Now, we?ll describe the probabilistic metrics 
we considered. For a count vector of dimension 
D, u
 
= (u1, u2, ?, uD), define a function 
)/log(...)/log()( 11 ?? ?++?= i iDDi i uuuuuuI u . 
I(u) is a measure of how well the data in u are 
modeled by the normalized vector (u1/?iui,  ?, 
uD/?iui).  Thus, when two count vectors u and v 
are merged (by adding them) we have the follow-
ing measure of the loss in modeling accuracy:  
 
Probability Loss (PL): 
 )()()(),( vuvuvu +?+= IIIPL .   (3) 
 
However, if we choose merges with the lowest 
PL, we will usually merge only vectors with 
small counts. We are more interested in the aver-
age impact of a merge, so we define 
 
Average Probability Loss (APL):  
  )/())()()((),( ?? ++?+= i ii i vuIIIAPL vuvuvu . (4) 
In our initial experiments, APL worked better 
than PL. However, APL had a strange side-effect. 
Most of the phrase clusters it induced made intui-
tive sense, but there were typically three or four 
clusters with large numbers of observations on 
both language sides that grouped together phras-
es with wildly disparate meanings. Why does 
APL induce these ?monster clusters?? 
Consider two count vectors u and v. If ?iui is 
very big and ?ivi is small, then I(u) and I(u + v) 
will be very similar, and APL will be approx-
imately I(v) /[?iui + ?ivi ] which will be close to 
zero. Thus, the decision will probably be made to 
merge u and v, even if they have quite different 
semantics. The resulting cluster, whose counts 
are represented by u + v, is now even bigger and 
even more likely to swallow up other small count 
vectors in the next rounds of merging: it becomes 
a kind of black hole.  
To deal with this problem, we devised another 
metric. Let 
)/log(...)/log()|( 11 ?? ?++?= i iDDi i vvuvvuI vu . 
This is a measure of how well the counts in v 
predict the distribution of counts in u. Then let  
 
Maximum Average Probability Loss (MAPL):  
))|()(,)|()(max(),( ??
+?+?
=
i ii i
v
II
u
IIMAPL vuvvvuuuvu
 .(5) 
 
The first term inside the maximum indicates the 
average probability loss for an observation in u 
when it is modeled by u+v instead of u; similarly, 
the second term indicates the average probability 
loss for an observation in v. If we merge vector 
pairs with the lowest values of MAPL, we will 
never merge vectors in a way that will cause a 
large loss to either of the two parents.  
In practice, we found that all these metrics 
worked better when multiplied by the Dice coef-
ficient based distance. For u and v, this is 
||||
||21),(
vu
vu
vu
+
??
?=Dice , where ?|u|? means 
the number of non-zero count entries in u, and 
?| vu ? |? is the number of count entries that are 
non-zero in u and v. 
3.3 Edit-based similarity 
In most of our experiments, count-based metrics 
were combined with edit-based metrics; we put 
little effort into optimizing the edit metrics. Let 
MCWS stand for ?maximum common word se-
quence?. For phrases p1 and p2, we define  
611
)()(
)),((21),(
21
21
21 plenplen
ppMCWSlenppEdit
+
?
?= .        (6) 
where len() returns the number of  words. This 
metric doesn?t take word identities into account; 
in future work, we may weight differences in-
volving content words more heavily.  
We also defined an edit-based metric for dis-
tance between phrase clusters. Let cluster 1 have 
phrases ?red? (10); ?burgundy? (5); ?resembling 
scarlet? (2) and cluster 2 have ?dark burgundy? 
(7); ?scarlet? (3) (number of observations in 
brackets). What is the edit distance between clus-
ters 1 and 2? We defined the distance as that be-
tween the two phrases with the most observa-
tions in each cluster. Thus, distance between 
clusters 1 and 2 would be Edit(?red?, ?dark bur-
gundy?)=1.0. Other definitions are possible.  
3.4 Examples of phrase clusters 
Figure 2 shows an English phrase cluster learned 
during C-E experiments by a metric combining 
count-based and edit-based information. Each 
phrase is followed by its count in brackets; we 
don?t show phrases with low counts. Since our 
edit distance sees words as atoms (it doesn?t 
know about morphology), the phrases containing 
?emancipating? were clustered with phrases con-
taining ?emancipation? based on count informa-
tion, rather than because of the common stem.  
Figure 3 shows part of a French phrase cluster 
learned during F-E experiments by the same 
mixed metric. The surface forms are quite varied, 
but most of the phrases mean ?to assure or to 
guarantee that something will happen?. An inter-
esting exception is ?pas faire? ? it means not to 
do something (?pas? is negative). This illustrates 
why we need a better edit distance that heavily 
weights negative words.  
 
emancipating (247), emancipate 
(167), emancipate our (73), emanci-
pating thinking (67), emancipate 
our minds (46), further emancipate 
(45), emancipate the (38), emanci-
pate the mind (38), emancipating 
minds (33), emancipate their (32), 
emancipate their minds (27), eman-
cipating our minds (24), emancipat-
ing our (21), emancipate our mind 
(21), further emancipate our (19), 
emancipate our thinking (14), fur-
ther emancipate their (11), emanci-
pating the minds (9), emancipate 
thinking (8), unfettering (8) ...  
 
Figure 2: partial English phrase cluster 
 
garantir que (64), assurer que 
(46), veiller ? ce que (27), afin 
de garantir (24), faire en sorte 
(19), de garantir que (16), afin de 
garantir que (14), faire des (14), 
de veiller ? ce (14), s' assurer 
que (13), de veiller ? ce que (13), 
pour garantir que (13), de faire en 
sorte (8), de faire en sorte que 
(7), ? garantir que (6), pas faire 
(5), de veiller (5)? 
 
Figure 3:  partial French phrase cluster 
4 Experiments  
We carried out experiments on a standard one-
pass phrase-based SMT system with a phrase 
table derived from merged counts of symme-
trized IBM2 and HMM alignments; the system 
has both lexicalized and distance-based distor-
tion components (there is a 7-word distortion 
limit) and employs cube pruning (Huang and 
Chiang, 2007). The baseline is a loglinear feature 
combination that includes language models, the 
distortion components, relative frequency esti-
mators PRF(s|t) and PRF(t|s) and lexical weight 
estimators PLW(s|t) and PLW(t|s). The PLW() com-
ponents are based on (Zens and Ney, 2004); Fos-
ter et al (2006) found this to be the most effec-
tive lexical smoothing technique. The phrase-
cluster-based components PPC(s|t) and PPC(t|s) 
are incorporated as additional loglinear feature 
functions. Weights on feature functions are 
found by lattice MERT (Macherey et al, 2008).  
4.1 Data 
We evaluated our method on C-E and F-E tasks. 
For each pair, we carried out experiments on 
training corpora of different sizes. C-E data were 
from the NIST1 2009 evaluation; all the allowed 
bilingual corpora except the UN corpus, Hong 
Kong Hansard and Hong Kong Law corpus were 
used to estimate the translation model. For C-E, 
we trained two 5-gram language models: the first 
on the English side of the parallel data, and the 
second on the English Gigaword corpus. 
Our C-E development set is made up mainly 
of data from the NIST 2005 test set; it also in-
cludes some balanced-genre web-text from the 
NIST training material. Evaluation was per-
formed on the NIST 2006 and 2008 test sets. 
Table 1 gives figures for training, development 
and test corpora for C-E tasks; |S| is the number 
of sentences, and |W| is the number of words. 
There are four references for dev and test sets. 
                                               
1
 http://www.nist.gov/speech/tests/mt 
612
   Chi Eng 
All parallel 
Train 
|S| 3.3M 
|W| 68.2M 66.5M 
Dev |S| 1,506 1,506?4 
Test NIST06 |S| 1,664 1,664?4 
NIST08 |S| 1,357 1,357?4 
Gigaword |S| - 11.7M 
 
Table 1: Statistics for Chinese-to-English tasks. 
 
 
   Fre Eng 
Train Europarl |S| 1.6M 
|W| 51.3M 46.6M 
Dev 2008 |S| 2,051 
Test 2009 |S| 2,525 
2010 |S| 2,489 
GigaFrEn |S| - 22.5M 
 
Table 2: Statistics for French-to-English tasks. 
 
 
Lang (#sent) C-E (3.3M) F-E (1.6M) 
  #count-1  #other  #count-1  #other 
 
 
Src 
Before 
clustering 
11.3M 5.7M 28.1M 21.2M 
After  
clustering 
11.3M 5.3M 28.1M 19.3M 
#clustered 0 0.4M 0 1.9M 
 
 
Tgt 
Before 
clustering 
11.9M 6.0M 25.6M 20.4M 
After  
clustering 
11.9M 5.6M 25.6M 18.5M 
#clustered 0 0.4M 0 1.9M 
 
Table 3: # phrase classes before & after clustering. 
 
For F-E tasks, we used WMT 20102 F-E track 
data sets. Parallel Europarl data are used for 
training; WMT Newstest 2008 set is the dev set, 
and WMT Newstest 2009 and 2010 are the test 
sets. One reference is provided for each source 
input sentence. Two language models are used in 
this task: one is the English side of the parallel 
data, and the second is the English side of the 
GigaFrEn corpus. Table 2 summarizes the train-
ing, development and test corpora for F-E tasks. 
4.2 Amount of clustering and metric 
For both C-E and E-F, we assumed that phrases 
seen only once in training data couldn?t be clus-
tered reliably, so we prevented these ?count 1? 
phrases from participating in clustering. The key 
                                               
2
 http://www.statmt.org/wmt10/ 
clustering parameter is the number of merge op-
erations per iteration, given as a percentage of 
the number of potential same-language phrase 
pairs satisfying a simple criterion (some overlap 
in translations to the other language). Prelimi-
nary tests involving the FBIS corpus (about 8% 
of the C-E data) caused us to set this parameter at 
5%. For C-E, we first clustered Chinese with this 
5% value, then English with the same amount. 
For F-E, we first clustered French, then English, 
using 5% in both cases.  
Table 3 shows the results. Only 2-4% of the 
total phrases in each language end up in a cluster 
(that?s 6.5-9% of eligible phrases, i.e., of phrases 
that aren?t ?count 1?). However, about 20-25% 
of translation probabilities are smoothed for both 
language pairs. Based on these preliminary tests, 
we decided to use MAPLDiceEdit ??  
( DMAPLEdit ? ) as our metric (though 
CosineEdit ?  was a close runner-up).  
4.3 Results and discussion 
Our evaluation metric is IBM BLEU (Papineni et 
al., 2002), which performs case-insensitive 
matching of n-grams up to n = 4. Our first expe-
riment evaluated the effects of the phrase cluster-
ing features given various amounts of training 
data. Figure 4 gives the BLEU score improve-
ments for the two language pairs, with results for 
each pair averaged over two test sets (training 
data size shown as #sentences). The improve-
ment is largest for medium amounts of training 
data. Since the F-E training data has more words 
per sentence than C-E, the two peaks would have 
been closer together if we?d put #words on the x 
axis: improvements for both tasks peak around 6-
8 M English words. For more details, refer to 
Table 4 and Table 5. The biggest improvement 
is 0.55 BLEU for the NIST06 test. More impor-
tantly, cluster features yield gains in 29 of 30 
experiments. Surprisingly, a reviewer asked if 
we?d done significance tests on the individual 
results shown in Tables 4 and 5. Most likely, 
many of these individual results are insignificant, 
but so what? Based on the tables, the probability 
of the null hypothesis that our method has no 
effect is equivalent to that of tossing a fair coin 
30 times and getting 29 heads (if we adopt an 
independence approximation).  
In the research on paraphrases cited earlier, 
paraphrases tend to be most helpful for small 
amounts of training data. By contrast, our 
approach seems to be most helpful for medium 
amounts of training data (200-400K sentence 
613
pairs). We attribute this to the properties of 
count-based clustering. When there is little 
training data, clustering is unreliable; when there 
is much data, clustering is reliable but unneeded, 
because most relative frequencies are well-
estimated. In between, phrase cluster probability 
estimates are both reliable and useful. 
 
 
 
Figure 4: Average BLEU improvement for C-E and 
F-E tasks (each averaged over two tests) vs. #training 
sent. 
 
Finally, we carried out experiments to see if 
some of our earlier decisions were correct. Were 
we right to use DMAPL instead of cosine as the 
count-based component of our metric? Experi-
ments with DMAPLEdit ?  vs. 
CosineEdit ? on 400K C-E (tested on NIST06 
and NIST08) and on 200K F-E (tested on News-
test2009 and 2010) showed a tiny advantage for 
DMAPLEdit ? of about 0.06 BLEU. So we 
probably didn?t make the wrong decision here 
(though it doesn?t matter much). Were we right 
to include the Edit component? Carrying out ana-
logous experiments with DMAPLEdit ? vs. 
DMAPL, we found that dropping Edit caused a 
loss of 0.1-0.2 BLEU for all four test sets. Here 
again, we made the right decision.  
In a final experiment, we allowed ?count 1? 
phrases to participate in clustering (using 
DMAPLEdit ? ). The resulting C-E system had 
somewhat more clustered phrases than the pre-
vious one (for both Chinese and English, about 
3.5% of phrases were in clusters compared to 
2.5% in the previous system). To our surprise, 
this led to a slight improvement in BLEU: the 
400K C-E system now yielded 30.25 on NIST06 
(up 0.09) and 23.88 on NIST08 (up 0.13). The F-
E system where ?count 1? clustering is allowed 
also had more phrases in clusters than the system 
where it?s prohibited (the former has just under 
10% of French and English phrases in clusters vs. 
 
Data size 
Nist06 Nist08 
Baseline +phrase-clustering Improv. Baseline +phrase-clustering Improv. 
25K 21.66 21.88 0.22 15.80 15.99 0.19 
50K 23.23 23.43 0.20 17.69 17.84 0.15 
100K 25.83 26.24 0.41 20.08 20.27 0.19 
200K 27.80 28.26 0.46 21.28 21.58 0.30 
400K 29.61 30.16 0.55 23.37 23.75 0.38 
800K 30.87 31.17 0.30 24.41 24.65 0.24 
1.6M 32.94 33.10 0.16 25.61 25.72 0.11 
3.3M 33.59 33.64 0.05 26.84 26.85 0.01 
 
Table 4: BLEU(%) scores for C-E with the various training corpora, including baseline results, results for with 
phrase clustering, and the absolute improvements. Corpus size is measured in sentences. 
 
 
Data size 
Newstest2009 Newstest2010 
Baseline +phrase-clustering Improv. Baseline +phrase-clustering Improv. 
25K 20.21 20.37 0.16 20.54 20.73 0.19 
50K 21.25 21.44 0.19 21.95 22.11 0.16 
100K 22.56 22.86 0.30 23.44 23.69 0.25 
200K 23.67 24.02 0.35 24.31 24.71 0.40 
400K 24.36 24.50 0.14 25.28 25.46 0.18 
800K 24.92 24.97 0.05 25.80 25.90 0.10 
1.6M 25.47 25.47 0.00 26.35 26.37 0.02 
 
Table 5: BLEU(%) scores for F-E with the various training corpora, including baseline results without phrase 
clustering feature, results for phrase clustering, and the absolute improvements. 
 
614
4% for the latter). For F-E, the 200K system al-
lowing ?count 1? clustering again yielded a 
slightly higher BLEU: 24.07 on Newstest2009 
and 24.76 on Newstest2010 (up 0.05 in both cas-
es). Thus, our decision not to allow ?count 1? 
phrases to participate in clustering in the Table 4 
and 5 experiments appears to have been a mis-
take. We suspect we can greatly improve han-
dling of ?count 1? phrases ? e.g., by weighting 
the Edit component of the similarity metric more 
heavily when assigning these phrases to clusters.  
5 Conclusion and Future Work 
We have shown that source-language and target-
language phrases in the phrase table can be clus-
tered, and that these clusters can be used to 
smooth ?forward? and ?backward? estimates 
P(t|s) and P(s|t), yielding modest but consistent 
BLEU gains over a baseline that included lexical 
smoothing. Though our experiments were done 
on a phrase-based system, this method could also 
be applied to hierarchical phrase-based SMT and 
syntactic SMT systems. There are several possi-
bilities for future work based on new applica-
tions for phrase clusters: 
? In the experiments above, we used 
phrase clusters to smooth P(t|s) and P(s|t) 
when the pair (s,t) was observed in train-
ing data. However, the phrase clusters 
often give non-zero probabilities for P(t|s) 
and P(s|t) when s and t were both in the 
training data, but didn?t co-occur. We 
could allow the decoder to consider such 
?invented? phrase pairs (s,t).  
? Phrase clusters could be used to con-
struct target language models (LMs) in 
which the basic unit is a phrase cluster 
rather than a word. For instance, a tri-
cluster model would estimate the proba-
bility of phrase p at time i as a function 
of its phrase cluster, Ci(p), and the two 
preceding phrase clusters Ci-1 and Ci-2: 
)|())(|()( 21 ???= iiii CCCfCfP ppp
.  
? Lexicalized distortion models could be 
modified so as to condition distortion 
events on phrase clusters.  
? We could build SMT grammars in which 
the terminals are phrases and the parents 
of terminals are phrase clusters.  
The phrase clustering algorithm described 
above could be improved in several ways: 
? In the above, the edit distance between 
phrases and between phrase clusters was 
crudely defined. If we improve edit dis-
tance, it will have an especially large 
impact on ?count 1? phrases, for which 
count-based metrics are unreliable and 
which are a large proportion of all phras-
es. The edit distance between two phras-
es weighted all words equally: preferably, 
weights for word substitution, insertion, 
or deletion would be learned from purely 
count-derived phrase clusters (content 
words and negative words might have 
heavier weights than other words). The 
edit distance between two phrase clusters 
was defined as the edit distance between 
the phrases with the most observations in 
each cluster. E.g., distance to the phrase 
cluster in Figure 2 is defined as the 
phrase edit distance to ?emancipating?. 
Instead, one could allow a cluster to be 
characterized by (e.g.) up to three phras-
es, and let distance between two clusters 
be the minimum or average pairwise edit 
distance between these characteristic 
phrases.  
? To cluster phrases, we only used infor-
mation derived from phrase tables. In fu-
ture, we could also use the kind of in-
formation used in work on paraphrases, 
such as the context surrounding phrases 
in monolingual corpora, entries in the-
sauri, and information from pivot lan-
guages. 
? The phrase clustering above was ?hard?: 
each phrase in either language belongs to 
exactly one cluster. We could modify 
our algorithms to carry out ?soft? clus-
tering. For instance, we could interpolate 
the probabilities associated with a phrase 
with probabilities from its neighbours.  
? Clustering is a primitive way of finding 
latent structure in the table of joint 
phrase counts. One could apply principal 
component analysis or a related algo-
rithm to this table. 
References 
C. Bannard and C. Callison-Burch. ?Paraphrasing 
with Bilingual Parallel Corpora?. Proc. ACL, pp. 
597-604, Ann Arbor, USA, June 2005.  
C. Callison-Burch, P. Koehn, and M. Osborne. ?Im-
proved Statistical Machine Translation Using Pa-
raphrases?. Proc. HLT/NAACL, pp. 17-24, New 
York City, USA, June 2006.  
615
C. Callison-Burch. ?Syntactic Constraints on Paraph-
rases Extracted from Parallel Corpora?. Proc. 
EMNLP, pp. 196-205, Honolulu, USA, October 
2008.  
D. Chiang. ?A hierarchical phrase-based model for 
statistical machine translation?. Proc. ACL, pp. 
263-270, Ann Arbor, USA, June 2005.  
G. Foster, R. Kuhn, and H. Johnson. ?Phrasetable 
smoothing for statistical machine translation?. 
Proc. EMNLP, pp. 53-61, Sydney, Australia, July 
2006.  
L. Huang and D. Chiang. ?Forest Rescoring: Faster 
Decoding with Integrated Language Models?. 
Proc. ACL, pp.  144-151, Prague, Czech Republic, 
June 2007.  
P. Koehn. 2010. Statistical Machine Translation. 
Cambridge University Press, Cambridge, UK.  
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 
?Lattice-based Minimum Error Rate Training for 
Statistical Machine Translation?. Proc. EMNLP, 
pp. 725-734, Honolulu, USA, October 2008.  
Y. Marton, C. Callison-Burch, and Philip Resnik. 
?Improved Statistical Machine Translation Using 
Monolingually-Derived Paraphrases?. Proc. 
EMNLP, pp. 381-390, Singapore, August 2009.  
K. Papineni, S. Roukos, T. Ward, and W. Zhu. ?Bleu: 
a method for automatic evaluation of machine 
translation?. Proc. ACL, pp. 311?318, Philadel-
phia, July 2002.  
G. Salton and M. McGill. 1986. Introduction to Mod-
ern Information Retrieval.  McGraw-Hill Inc., New 
York, USA. 
H. Schwenk, M. Costa-juss?, and J. Fonollosa. 
?Smooth Bilingual N-gram Translation?. Proc. 
Joint EMNLP/CoNLL, pp. 430-438, Prague, Czech 
Republic, June 2007.  
R.  Zens and H. Ney. ?Improvements in phrase-based 
statistical machine translation?. Proc. ACL/HLT, 
pp. 257-264, Boston, USA, May 2004. 
S. Zhao, H. Wang, T. Liu, and S. Li. ?Pivot Approach 
for Extracting Paraphrase Patterns from Bilingual 
Corpora?. Proc. ACL/HLT, pp. 780-788, Colum-
bus, USA, June 2008.  
 
616
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 607?615,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Bilingual Sentiment Consistency for Statistical Machine Translation 
 
Boxing Chen and Xiaodan Zhu 
National Research Council Canada 
1200 Montreal Road, Ottawa, Canada, K1A 0R6 
{Boxing.Chen, Xiaodan.Zhu}@nrc-cnrc.gc.ca 
  
 
Abstract 
In this paper, we explore bilingual sentiment 
knowledge for statistical machine translation 
(SMT). We propose to explicitly model the 
consistency of sentiment between the source 
and target side with a lexicon-based approach. 
The experiments show that the proposed mod-
el significantly improves Chinese-to-English 
NIST translation over a competitive baseline. 
1 Introduction 
The expression of sentiment is an interesting and 
integral part of human languages. In written text 
sentiment is conveyed by senses and in speech also 
via prosody. Sentiment is associated with both 
evaluative (positive or negative) and potency (de-
gree of sentiment) ? involving two of the three 
major semantic differential categories identified by 
Osgood et al. (1957). 
Automatically analyzing the sentiment of mono-
lingual text has attracted a large bulk of research, 
which includes, but is not limited to, the early ex-
ploration of (Turney, 2002; Pang et al., 2002; Hat-
zivassiloglou & McKeown, 1997). Since then, 
research has involved a variety of approaches and 
been conducted on various type of data, e.g., prod-
uct reviews, news, blogs, and the more recent so-
cial media text.  
As sentiment has been an important concern in 
monolingual settings, better translation of such 
information between languages could be of interest 
to help better cross language barriers, particularly 
for sentiment-abundant data. Even when we ran-
domly sampled a subset of sentence pairs from the 
NIST Open MT1 training data, we found that about 
48.2% pairs contain at least one sentiment word on 
both sides, and 22.4% pairs contain at least one 
                                                          
1 http://www.nist.gov/speech/tests/mt 
intensifier word on both sides, which suggests a 
non-trivial percent of sentences may potentially 
involve sentiment in some degree2.  
 
# snt. 
pairs 
% snt. with 
sentiment words 
% snt. with 
intensifiers 
103,369 48.2% 22.4% 
 
 
Table 1.  Percentages of sentence pairs that contain sen-
timent words on both sides or intensifiers3 on both sides. 
 
One expects that sentiment has been implicitly 
captured in SMT through the statistics learned 
from parallel corpus, e.g., the phrase tables in a 
phrase-based system. In this paper, we are interest-
ed in explicitly modeling sentiment knowledge for 
translation. We propose a lexicon-based approach 
that examines the consistency of bilingual subjec-
tivity, sentiment polarity, intensity, and negation. 
The experiments show that the proposed approach 
improves the NIST Chinese-to-English translation 
over a strong baseline. 
In general, we hope this line of work will help 
achieve better MT quality, especially for data with 
more abundant sentiment, such as social media text. 
2 Related Work  
Sentiment analysis and lexicon-based approach-
es Research on monolingual sentiment analysis can 
be found under different names such as opinion, 
stance, appraisal, and semantic orientation, among 
others. The overall goal is to label a span of text 
either as positive, negative, or neutral ? some-
times the strength of sentiment is a concern too. 
                                                          
2 The numbers give a rough idea of sentiment coverage; it 
would be more ideal if the estimation could be conducted on 
senses instead of words, which, however, requires reliable 
sense labeling and is not available at this stage. Also, accord-
ing to our human evaluation on a smaller dataset, two thirds of 
such potentially sentimental sentences do convey sentiment.  
3 The sentiment and intensifier lexicons used to acquire these 
numbers are discussed in Section 3.2. 
  
607
The granularities of text have spanned from words 
and phrases to passages and documents.  
Sentiment analysis has been approached mainly 
as an unsupervised or supervised problem, alt-
hough the middle ground, semi-supervised ap-
proaches, exists. In this paper, we take a lexicon-
based, unsupervised approach to considering sen-
timent consistency for translation, although the 
translation system itself is supervised. The ad-
vantages of such an approach have been discussed 
in (Taboada et al., 2011). Briefly, it is good at cap-
turing the basic sentiment expressions common to 
different domains, and certainly it requires no bi-
lingual sentiment-annotated data for our study. It 
suits our purpose here of exploring the basic role 
of sentiment for translation. Also, such a method 
has been reported to achieve a good cross-domain 
performance (Taboada et al., 2011) comparable 
with that of other state-of-the-art models.  
Translation for sentiment analysis A very inter-
esting line of research has leveraged labeled data in 
a resource-rich language (e.g., English) to help 
sentiment analysis in a resource-poorer language. 
This includes the idea of constructing sentiment 
lexicons automatically by using a translation dic-
tionary (Mihalcea et al., 2007), as well as the idea 
of utilizing parallel corpora or automatically trans-
lated documents to incorporate sentiment-labeled 
data from different languages (Wan, 2009; Mihal-
cea et al., 2007).  
Our concern here is different ? instead of uti-
lizing translation for sentiment analysis; we are 
interested in the SMT quality itself, by modeling 
bilingual sentiment in translation. As mentioned 
above, while we expect that statistics learned from 
parallel corpora have implicitly captured sentiment 
in some degree, we are curious if better modeling 
is possible. 
Considering semantic similarity in translation 
The literature has included interesting ideas of in-
corporating different types of semantic knowledge 
for SMT. A main stream of recent efforts have 
been leveraging semantic roles (Wu and Fung, 
2009; Liu and Gildea, 2010; Li et al., 2013) to im-
prove translation, e.g., through improving reorder-
ing. Also, Chen et al. (2010) have leveraged sense 
similarity between source and target side as addi-
tional features.  In this work, we view a different 
dimension, i.e., semantic orientation, and show that 
incorporating such knowledge improves the trans-
lation performance. We hope this work would add 
more evidences to the existing literature of lever-
aging semantics for SMT, and shed some light on 
further exploration of semantic consistency, e.g., 
examining other semantic differential factors. 
3 Problem & Approach 
3.1 Consistency of sentiment 
Ideally, sentiment should be properly preserved in 
high-quality translation. An interesting study con-
ducted by Mihalcea et al. (2007) suggests that in 
most cases the sentence-level subjectivity is pre-
served by human translators. In their experiments, 
one English and two Romanian native speakers 
were asked to independently annotate the senti-
ment of English-Romanian sentence pairs from the 
SemCor corpus (Miller et al., 1993), a balanced 
corpus covering a number of topics in sports, poli-
tics, fashion, education, and others. These human 
subjects were restricted to only access and annotate 
the sentiment of their native-language side of sen-
tence pairs. The sentiment consistency was ob-
served by examining the annotation on both sides. 
Automatic translation should conform to such a 
consistency too, which could be of interest for 
many applications, particularly for sentiment-
abundant data. On the other hand, if consistency is 
not preserved for some reason, e.g., alignment 
noise, enforcing consistency may help improve the 
translation performance. In this paper, we explore 
bilingual sentiment consistency for translation. 
3.2 Lexicon-based bilingual sentiment analysis 
To capture bilingual sentiment consistency, we use 
a lexicon-based approach to sentiment analysis. 
Based on this, we design four groups of features to 
represent the consistency. 
The basic idea of the lexicon-based approach is 
first identifying the sentiment words, intensifiers, 
and negation words with lexicons, and then calcu-
lating the sentiment value using manually designed 
formulas. To this end, we adapted the approaches 
of (Taboada et al., 2011) and (Zhang et al., 2012) 
so as to use the same formulas to analyze the sen-
timent on both the source and the target side.  
The English and Chinese sentiment lexicons we 
used are from (Wilson et al. 2005) and (Xu and Lin, 
2007), respectively. We further use 75 English in-
608
tensifiers listed in (Benzinger, 1971; page 171) and 
81 Chinese intensifiers from (Zhang et al., 2012). 
We use 17 English and 13 Chinese negation words.  
Similar to (Taboada et al., 2011) and (Zhang et 
al., 2012), we assigned a numerical score to each 
sentiment word, intensifier, and negation word. 
More specifically, one of the five values: -0.8, -0.4, 
0, 0.4, and 0.8, was assigned to each sentiment 
word in both the source and target sentiment lexi-
cons, according to the strength information anno-
tated in these lexicons. The scores indicate the 
strength of sentiment. Table 2 lists some examples. 
Similarly, one of the 4 values, i.e., -0.5, 0.5, 0.7 
and 0.9, was manually assigned to each intensifier 
word, and a -0.8 or -0.6 to the negation words. All 
these scores will be used below to modify and shift 
the sentiment value of a sentiment unit.  
Sentiment words Intensifiers Negation words 
impressive (0.8) 
good (0.4) 
actually (0.0) 
worn (-0.4) 
depressing (-0.8) 
extremely (0.9) 
very (0.7) 
pretty (0.5) 
slightly (-0.5) 
not (-0.8) 
rarely (-0.6) 
Table 2: Examples of sentiment words and their senti-
ment strength; intensifiers and their modify rate; nega-
tion words and their negation degree. 
 
Each sentiment word and its modifiers (negation 
words and intensifiers) form a sentiment unit. We 
first found all sentiment units by identifying senti-
ment words with the sentiment lexicons and their 
modifiers with the corresponding lexicon in a 7-
word window. Then, for different patterns of sen-
timent unit, we calculated the sentiment values 
using the formulas listed in Table 3, where these 
formulas are adapted from (Taboada et al., 2011) 
and (Zhang et al., 2012) so as to be applied to both 
languages.  
 
Sen. 
unit 
Sen. value  
formula 
 
Example 
Sen. 
value 
ws S(ws) good 0.40 
wnws D(wn)S(ws) not good -0.32 
wiws (1+R(wi))S(ws) very good 0.68 
wnwiws (1+ D(wn)R(wi))S(ws) not very good 0.176 
wiwnws D(wn)(1+R(wi))S(ws) very not good
4 -0.544 
Table 3: Heuristics used to compute the lexicon-based 
sentiment values for different types of sentiment units. 
                                                          
4 The expression ?very not good? is ungrammatical in English. 
However, in Chinese, it is possible to have this kind of expres-
sion, such as ??????, whose transliteration is ?very not 
beautiful?, meaning ?very ugly?. 
For notation, S(ws) stands for the strength of 
sentiment word ws, R(wi) is degree of the intensifi-
er word wi, and D(wn) is the negation degree of the 
negation word wn. 
Above, we have calculated the lexicon based 
sentiment value (LSV) for any given unit ui, and 
we call it lsv(ui) below. If a sentence or phrase s 
contains multiple sentiment units, its lsv-score is a 
merge of the individual lsv-scores of all its senti-
ment units: 
 
)))((()( 1 iN ulsvbasismergslsv ?             (1) 
 
where the function basis(.) is a normalization func-
tion that performs on each lsv(ui). For example, the 
basis(.) function could be a standard sign function 
that just examines if a sentiment unit is positive or 
negative, or simply an identity function (using the 
lsv-scores directly). The merg(.) is a function that 
merge the lsv-scores of individual sentiment units, 
which may take several different forms below in 
our feature design. For example, it can be a mean 
function to take the average of the sentiment units? 
lsv-scores, or a logic OR function to examine if a 
sentence or phrase contains positive or negative 
units (depending on the basis function). It can also 
be a linear function that gives different weights to 
different units according to further knowledge, e.g., 
syntactic information. In this paper, we only lever-
age the basic, surface-level analysis5. 
In brief, our model here can be thought of as a 
unification and simplification of both (Taboada et 
al., 2011) and (Zhang et al., 2012), for our bilin-
gual task. We suspect that better sentiment model-
ing may further improve the general translation 
performance or the quality of sentiment in transla-
tion. We will discuss some directions we think in-
teresting in the future work section. 
3.3 Incorporating sentiment consistency into 
phrase-based SMT 
In this paper, we focus on exploring sentiment 
consistency for phrase-based SMT. However, the 
approach might be used in other translation 
framework. For example, consistency may be con-
sidered in the variables used in hierarchical transla-
tion rules (Chiang, 2005).   
                                                          
5 Note that when sentiment-annotated training data are availa-
ble, merg(.) can be trained, e.g., if assuming it to be the wide-
ly-used (log-) linear form. 
609
We will examine the role of sentiment con-
sistency in two ways: designing features for the 
translation model and using them for re-ranking. 
Before discussing the details of our features, we 
briefly recap phrase-based SMT for completeness. 
Given a source sentence f, the goal of statistical 
machine translation is to select a target language 
string e which maximizes the posterior probability 
P(e|f). In a phrase-based SMT system, the transla-
tion unit is the phrases, where a "phrase" is a se-
quence of words. Phrase-based statistical machine 
translation systems are usually modeled through a 
log-linear framework (Och and Ney, 2002) by in-
troducing the hidden word alignment variable a 
(Brown et al., 1993). 
)),~,~((maxarg~ 1,* ? ?? Mm mmae afeHe ?
       (2) 
where e~ is a string of phrases in the target lan-
guage, f~ is the source language string, 
),~,~( afeHm  are feature functions, and weights 
m? are typically optimized to maximize the scoring 
function (Och, 2003). 
3.4 Feature design  
In Section 3.2 above, we have discussed our lexi-
con-based approach, which leverages lexicon-
based sentiment consistency. Below, we describe 
the specific features we designed for our experi-
ments. For a phrase pair ( ef ~,~ ) or a sentence pair 
(f, e)6, we propose the following four groups of 
consistency features. 
Subjectivity The first group of features is designed 
to check the subjectivity of a phrase or a sentence 
pair (f, e). This set of features examines if the 
source or target side contains sentiment units. As 
the name suggests, these features only capture if 
subjectivity exists, but not if a sentiment is positive, 
negative, or neutral. We include four binary fea-
tures that are triggered in the following condi-
tions?satisfaction of each condition gives the 
corresponding feature a value of 1 and otherwise 0. 
? F1: if neither side of the pair (f, e) contains at 
least one sentiment unit; 
                                                          
6 For simplicity, we hereafter use the same notation (f, e) to 
represent both a phrase pair and a sentence pair, when no con-
fusion arises. 
? F2: if only one side contains sentiment units;  
? F3: if the source side contains sentiment 
units; 
? F4: if the target side contains sentiment units. 
Sentiment polarity The second group of features 
check the sentiment polarity. These features are 
still binary; they check if the polarities of the 
source and target side are the same.  
? F5: if the two sides of the pair (f, e) have the 
same polarity; 
? F6: if at least one side has a neutral senti-
ment; 
? F7: if the polarity is opposite on the two 
sides, i.e., one is positive and one is negative.  
Note that examining the polarity on each side 
can be regarded as a special case of applying Equa-
tion 1 above. For example, examining the positive 
sentiment corresponds to using an indicator func-
tion as the basis function: it takes a value of 1 if 
the lsv-score of a sentiment unit is positive or 0 
otherwise, while the merge function is the logic 
OR function. The subjectivity features above can 
also be thought of similarly. 
Sentiment intensity The third group of features is 
designed to capture the degree of sentiment and 
these features are numerical. We designed two 
types of features in this group.  
Feature F8 measures the difference of the LSV 
scores on the two sides. As shown in Equation (3), 
we use a mean function7  as our merge function 
when computing the lsv-scores with Equation (1), 
where the basis function is simply the identity 
function. 
? ?? ni iulsvnslsv 01 )(1)(
                    (3) 
Feature F9, F10, and F11 are the second type in 
this group of features, which compute the ratio of 
sentiment units on each side and examine their dif-
ference. 
? F8: |)()(|),( 118 elsvflsvefH ??                                 
? F9: |)()(|),(9 elsvflsvefH ?? ??                          
                                                          
7 We studied several different options but found the average 
function is better than others for our translation task here, e.g., 
better than giving more weight to the last unit. 
610
? F10: |)()(|),(10 elsvflsvefH ?? ??                         
? F11: |)()(|),(11 elsvflsvefH ???? ??                             
lsv+(.) calculates the ratio of a positive sentiment 
units in a phrase or a sentence, i.e., the number of 
positive sentiment units divided by the total num-
ber of words of the phrase or the sentence. It corre-
sponds to a special form of Equation 1, in which 
the basis function is an indicator function as dis-
cussed above, and the merge function adds up all 
the counts and normalizes the sum by the length of 
the phrase or the sentence concerned. Similarly, 
lsv-(.) calculates the ratio of negative units and  
lsv+-(.) calculates that for both types of units.  The 
length of sentence here means the number of word 
tokens. We experimented with and without remov-
ing stop words when counting them, and found that 
decision has little impact on the performance. We 
also used the part-of-speech (POS) information in 
the sentiment lexicons to help decide if a word is a 
sentiment word or not, when we extract features; 
i.e., a word is considered to have sentiment only if 
its POS tag also matches what is specified in the 
lexicons8. Using POS tags, however, did not im-
prove our translation performance.  
Negation The fourth group of features checks the 
consistency of negation words on the source and 
target side. Note that negation words have already 
been considered in computing the lsv-scores of 
sentiment units. One motivation is that a negation 
word may appear far from the sentiment word it 
modifies, as mentioned in (Taboada et al., 2011) 
and may be outside the window we used to calcu-
late the lsv-score above. The features here addi-
tionally check the counts of negation words. This 
group of features is binary and triggered by the 
following conditions. 
? F12: if neither side of the pair (f, e) contain 
negation words; 
? F13: if both sides have an odd number of 
negation words or both sides have an even 
number of them; 
? F14:  if both sides have an odd number of 
negation words not appearing outside any 
sentiment units, or if both sides have an even 
number of such negation words; 
                                                          
8 The Stanford POS tagger (Toutanova et al., 2003) was 
used to tag phrase and sentence pairs for this purpose. 
? F15: if both sides have an odd number of 
negation words appearing in all sentiment 
units, or if both sides have an even number 
of such negation words. 
4 Experiments 
4.1 Translation experimental settings 
Experiments were carried out with an in-house 
phrase-based system similar to Moses (Koehn et 
al., 2007).  Each corpus was word-aligned using 
IBM model 2, HMM, and IBM model 4, and the 
phrase table was the union of phrase pairs extract-
ed from these separate alignments, with a length 
limit of 7. The translation model was smoothed in 
both directions with Kneser-Ney smoothing (Chen 
et al., 2011).  We use the hierarchical lexicalized 
reordering model (Galley and Manning, 2008), 
with a distortion limit of 7. Other features include 
lexical weighting in both directions, word count, a 
distance-based RM, a 4-gram LM trained on the 
target side of the parallel data, and a 6-gram Eng-
lish Gigaword LM. The system was tuned with 
batch lattice MIRA (Cherry and Foster, 2012). 
We conducted experiments on NIST Chinese-to-
English translation task. The training data are from 
NIST Open MT 2012. All allowed bilingual corpo-
ra were used to train the translation model and re-
ordering models. There are about 283M target 
word tokens. The development (dev) set comprised 
mainly data from the NIST 2005 test set, and also 
some balanced-genre web-text from NIST training 
data. Evaluation was performed on NIST 2006 and 
2008, which have 1,664 and 1,357 sentences, 
39.7K and 33.7K source words respectively. Four 
references were provided for all dev and test sets. 
4.2 Results  
Our evaluation metric is case-insensitive IBM 
BLEU (Papineni et al., 2002), which performs 
matching of n-grams up to n = 4; we report BLEU 
scores on two test sets NIST06 and NIST08. Fol-
lowing (Koehn, 2004), we use the bootstrap 
resampling test to do significance testing. In Table 
4-6, the sign * and ** denote statistically signifi-
cant gains over the baseline at the p < 0.05 and p < 
0.01 level, respectively. 
 
 
611
 NIST06 NIST08 Avg. 
Baseline 35.1 28.4 31.7 
+feat. group1 35.6** 29.0** 32.3 
+feat. group2 35.3* 28.7* 32.0 
+feat. group3 35.3 28.7* 32.0 
+feat. group4 35.5* 28.8* 32.1 
+feat. group1+2 35.8** 29.1** 32.5 
+feat. group1+2+3 36.1** 29.3** 32.7 
+feat. group1+2+3+4 36.2** 29.4** 32.8 
 
Table 4: BLEU(%) scores on two original test sets for 
different feature combinations. The sign * and ** indi-
cate statistically significant gains over the baseline at 
the p < 0.05 and p < 0.01 level, respectively. 
 
Table 4 summarizes the results of the baseline 
and the results of adding each group of features 
and their combinations. We can see that each indi-
vidual feature group improves the BLEU scores of 
the baseline, and most of these gains are signifi-
cant. Among the feature groups, the largest im-
provement is associated with the first feature 
group, i.e., the subjectivity features, which sug-
gests the significant role of modeling the basic sub-
jectivity. Adding more features results in further 
improvement; the best performance was achieved 
when using all these sentiment consistency fea-
tures, where we observed a 1.1 point improvement 
on the NIST06 set and a 1.0 point improvement on 
the NIST08 set, which yields an overall improve-
ment of about 1.1 BLEU score. 
To further observe the results, we split each of 
the two (i.e., the NIST06 and NIST08) test sets 
into three subsets according to the ratio of senti-
ment words in the reference. We call them low-
sen, mid-sen and high-sen subsets, denoting lower, 
middle, and higher sentiment-word ratios, respec-
tively. The three subsets contain roughly equal 
number of sentences.  Then we merged the two 
low-sen subsets together, and similarly the two 
mid-sen and high-sen subsets together, respective-
ly. Each subset has roughly 1007 sentences. 
 
 low-sen mid-sen high-sen 
baseline 33.4 32.3 29.3 
+all feat. 34.4** 33.5** 30.4** 
improvement 1.0 1.2 1.1 
 
Table 5: BLEU(%) scores on three sub test sets with 
different sentiment ratios. 
 
Table 5 shows the performance of baseline and 
the system with sentiment features (the last system 
of Table 4) on these subsets. First, we can see that 
both systems perform worse as the ratio of senti-
ment words increases. This probably indicates that 
text with more sentiment is harder to translate than 
text with less sentiment. Second, it is interesting 
that the largest improvement is seen on the mid-sen 
sub-set. The larger improvement on the mid-
sen/high-sen subsets than on the low-sen may indi-
cate the usefulness of the proposed features in cap-
turing sentiment information. The lower 
improvement on high-sen than on mid-sen proba-
bly indicates that the high-sen subset is hard any-
way and using simple lexicon-level features is not 
sufficient. 
Sentence-level reranking Above, we have incor-
porated sentiment features into the phrase tables. 
To further confirm the usefulness of the sentiment 
consistency features, we explore their role for sen-
tence-level reranking. To this end, we re-rank 
1000-best hypotheses for each sentence that were 
generated with the baseline system. All the senti-
ment features were recalculated for each hypothe-
sis. We then re-learned the weights for the 
decoding and sentiment features to select the best 
hypothesis. The results are shown in Table 6. We 
can see that sentiment features improve the per-
formance via re-ranking. The improvement is sta-
tistically significant, although the absolute 
improvement is less than that obtained by incorpo-
rating the sentiment features in decoding. Not that 
as widely known, the limited variety of candidates 
in reranking may confine the improvement that 
could be achieved. Better models on the sentence 
level are possible. In addition, we feel that ensur-
ing sentiment and its target to be correctly paired is 
of interest. Note that we have also combined the 
last system in Table 4 with the reranking system 
here; i.e., sentiment consistency was incorporated 
in both ways, but we did not see further improve-
ment, which suggests that the benefit of the senti-
ment features has mainly been captured in the 
phrase tables already. 
 
feature NIST06 NIST08 Avg. 
baseline 35.1 28.4 31.7 
+ all feat.  35.4* 28.9** 32.1 
 
Table 6: BLEU(%) scores on two original test sets on 
sentence-level sentiment features. 
612
Human evaluation We conducted a human evalu-
ation on the output of the baseline and the system 
that incorporates all the proposed sentiment fea-
tures (the last system in Table 4). For this purpose, 
we randomly sampled 250 sentences from the two 
NIST test sets according to the following condi-
tions. First, the selected sentences should contain 
at least one sentiment word?in this evaluation, we 
target the sentences that may convey some senti-
ment. Second, we do not consider sentences short-
er than 5 words or longer than 50 words; or where 
outputs of the baseline system and the system with 
sentiment feature were identical. The 250 selected 
sentences were split into 9 subsets, as we have 9 
human evaluators (none of the authors of this paper 
took part in this experiment). Each subset contains 
26 randomly selected sentences, which are 234 
sentences in total. The other 16 sentences are ran-
domly selected to serve as a common data set: they 
are added to each of the 9 subsets in order to ob-
serve agreements between the 9 annotators. In 
short, each human evaluator was presented with 42 
evaluation samples. Each sample is a tuple contain-
ing the output of the baseline system, that of the 
system considering sentiment, and the reference 
translation. The two automatic translations were 
presented in a random order to the evaluators. 
As in (Callison-Burch et al., 2012), we per-
formed a pairwise comparison of the translations 
produced by the systems. We asked the annotators 
the following two questions Q1 and Q2: 
? Q1(general preference): For any reason, 
which of the two translations do you prefer 
according to the provided references, other-
wise mark ?no preference?? 
? Q2 (sentiment preference):  Does the refer-
ence contains sentiment? If so, in terms of 
the translations of the sentiment, which of 
the two translations do you prefer, otherwise 
mark ?no preference?? 
 
We computed Fleiss?s Kappa (Fleiss, 1971) on 
the common set to measure inter-annotator agree-
ment, 
all? . Then, we excluded one and only one 
annotator at a time to compute i? (Kappa score 
without i-th annotator, i.e., from the other eight). 
Finally, we removed the annotation of the two an-
notators whose answers were most different from 
the others?: i.e., annotators with the biggest 
iall ?? ?  values. As a result, we got a Kappa score 
0.432 on question Q1 and 0.415 on question Q2, 
which both mean moderate agreement. 
 
 base win bsc win equal total 
Translation 58 
(31.86%) 
82 
(45.05%) 
42 
(23.09%) 
182 
Sentiment 30 
(22.39%) 
49 
(36.57%) 
55 
(41.04%) 
134 
 
Table 7: Human evaluation preference for outputs from 
baseline vs. system with sentiment features. 
 
This left 7 files from 7 evaluators. We threw 
away the common set in each file, leaving 182 
pairwise comparisons. Table 6 shows that the eval-
uators preferred the output from the system with 
sentiment features 82 times, the output from the 
baseline system 58 times, and had no preference 
the other 42 times. This indicates that there is a 
human preference for the output from the system 
that incorporated the sentiment features over those 
from the baseline system at the p<0.05 significance 
level (in cases where people prefer one of them). 
For question Q2, the human annotators regarded 48 
sentences as conveying no sentiment according to 
the provided reference, although each of them con-
tains at least one sentiment word (a criterion we 
described above in constructing the evaluation set). 
Among the remaining 134 sentences, the human 
annotators preferred the proposed system 49 times 
and the baseline system 30 times, while they mark 
no-preference 55 times. The result shows a human 
preference for the proposed model that considers 
sentiment features at the p<0.05 significance level 
(in the cases where the evaluators did mark a pref-
erence). 
 
4.3 Examples 
We have also manually examined the translations 
generated by our best model (the last model of Ta-
ble 4, named BSC below) and the baseline model 
(BSL), and we attribute the improvement to two 
main reasons: (1) checking sentiment consistency 
on a phrase pair helps punish low-quality phrase 
pairs caused by word alignment error, (2) such 
consistency checking also improves the sentiment 
of the translation to better match the sentiment of 
the source. 
613
(1) 
 
 
 
Phr. pairs   
REF 
BSL 
BSC 
     ?? ||| talks   vs.    ?? ||| peace talks  
? help the palestinians and the israelis to resume peace talks ? 
? help the israelis and palestinians to resumption of the talks ? 
? help the israelis and palestinians to resume peace talks ? 
(2) 
 
 
 
Phr. pairs 
REF 
BSL 
BSC 
     ?? ||| war    vs.   ?? ||| preparing for  
? the national team is preparing for matches with palestine and Iraq ? 
? the national team 's match with the palestinians and the iraq war ? 
? the national team preparing for the match with the palestinian and iraq ? 
(3) 
 
 
REF 
BSL 
BSC 
? in china we have top-quality people , ever-improving facilities ? 
? we have talents in china , an increasing number of facilities ? 
? we have outstanding talent in china , more and better facilities ? 
(4) 
 
 
REF 
BSL 
BSC 
? continue to strive for that ? 
? continue to struggle ? 
? continue to work hard to achieve ? 
 
Table 8: Examples that show how sentiment helps improve our baseline model. REF is a reference translation, BSL 
stands for baseline model, and BSC (bilingual sentiment consistency) is the last model of Table 4. 
 
In the first two examples of Table 8, the first 
line shows two phrase pairs that are finally chosen 
by the baseline and BSC system, respectively. The 
next three lines correspond to a reference (REF), 
translation from BSL, and that from the BSC sys-
tem. The correct translations of ???? should be 
?peace negotiations? or ?peace talks?, which have 
a positive sentiment, while the word ?talks? 
doesn?t convey sentiment at all. By punishing the 
phrase pair ??? ||| talks?, the BSC model was 
able to generate a better translation. In the second 
example, the correct translation of ???? should 
be ?prepare for?, where neither side conveys sen-
timent. The incorrect phrase pair ??? ||| war? is 
generated from incorrect word alignment. Since 
?war? is a negative word in our sentiment lexicon, 
checking sentiment consistency helps down-weight 
such incorrect translations. Note also that the in-
correct phrase pair ??? ||| war? is not totally irra-
tional, as the literal translation of ??? ? is 
?prepare for war?. 
Similarly, in the third example, ?outstanding tal-
ent? is closer with respect to sentiment to the refer-
ence ?top-quality people? than ?talent? is; ?more 
and better? is closer with respect to sentiment to 
the reference ?ever-improving? than ?an increasing 
number? is. These three examples also help us un-
derstand the benefit of the subjectivity features 
discussed in Section 3.4. In the fourth example, 
?work hard to achieve? has a positive sentiment, 
same as ?strive?, while ?struggle? is negative. We 
can see that the BSC model is able to preserve the 
original sentiment better (the 9 human evaluators 
who were involved in our human evaluation (Sec-
tion 4.3) all agreed with this). 
5 Conclusions and future work 
We explore lexicon-based sentiment consistency 
for statistical machine translation. By incorporating 
lexicon-based subjectivity, polarity, intensity, and 
negation features into the phrase-pair translation 
model, we observed a 1.1-point improvement of 
BLEU score on NIST Chinese-to-English transla-
tion. Among the four individual groups of features, 
subjectivity consistency yields the largest im-
provement. The usefulness of the sentiment fea-
tures has also been confirmed when they are used 
for re-ranking, for which we observed a 0.4-point 
improvement on the BLEU score. In addition, hu-
man evaluation shows the preference of the human 
subjects towards the translations generated by the 
proposed model, in terms of both the general trans-
lation quality and the sentiment conveyed. 
In the paper, we propose a lexicon-based ap-
proach to the problem. It is possible to employ 
more complicated models. For example, with the 
involvement of proper sentiment-annotated data, if 
available, one may train a better sentiment-analysis 
model even for the often-ungrammatical phrase 
pairs or sentence candidates. Another direction we 
feel interesting is ensuring that sentiment and its 
target are not only better translated but also better 
paired, i.e., their semantic relation is preserved. 
This is likely to need further syntactic or semantic 
analysis at the sentence level, and the semantic role 
labeling work reviewed in Section 2 is relevant. 
614
References 
C. Banea, R. Mihalcea, J. Wiebe and S. Hassan. 2008.  
Multilingual subjectivity analysis using machine  
translation. In Proc. of EMNLP. 
E. M. Benzinger. 1971. Intensifiers in current English. 
PhD. Thesis. University of Florida. 
P. F. Brown, S. Della Pietra, V. Della J. Pietra, and R. 
Mercer. 1993. The mathematics of Machine Transla-
tion: Parameter estimation. Computational Linguis-
tics, 19(2): 263-312. 
C. Callison-Burch, P. Koehn, C. Monz, R. Soricut, and 
L. Specia. 2012. Findings of the 2012 Workshop on 
Statistical Machine Translation. In Proc. of WMT. 
D. Chiang. 2005. A hierarchical phrase-based model for 
statistical machine translation. In Proc. of ACL, 263?
270. 
B. Chen, G. Foster, and R. Kuhn. 2010. Bilingual Sense 
Similarity for Statistical Machine Translation. In 
Proc. of ACL, 834-843.  
B. Chen, R. Kuhn, G. Foster, and H. Johnson. 2011. 
Unpacking and transforming feature functions: New 
ways to smooth phrase tables. In Proc. of MT Sum-
mit. 
C. Cherry and G. Foster. 2012. Batch tuning strategies 
for statistical machine translation. In Proc. of 
NAACL. 
J. L. Fleiss. 1971. Measuring nominal scale agreement 
among many raters. Psychological Bulletin, 76(5): 
378?382. 
M. Galley and C. D. Manning. 2008. A simple and ef-
fective hierarchical phrase reordering model. In Proc. 
of EMNLP: 848?856. 
V. Hatzivassiloglou and K. McKeown. 1997. Predicting 
the semantic orientation of adjectives. In Proc. of 
EACL: 174-181.  
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of EMNLP: 
388?395. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, 
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. 
Herbst. 2007. Moses: Open Source Toolkit for Statis-
tical Machine Translation. In Proc. of ACL, 177-180. 
J. Li, P. Resnik and H. Daume III. 2013. Modeling Syn-
tactic and Semantic Structures in Hierarchical 
Phrase-based Translation. In Proc. of NAACL, 540-
549.  
D. Liu and D. Gildea. 2010. Semantic role features for 
machine translation. In Proc. of COLING,  716?724. 
R. Mihalcea, C. Banea and J. Wiebe. 2007. Learning  
multilingual subjective language via cross-lingual  
projections. In Proc. of ACL. 
F. J. Och and H. Ney. 2002. Discriminative Training 
and Maximum Entropy Models for Statistical Ma-
chine Translation. In Proc. of ACL. 
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of ACL. 
C. E. Osgood, G. J. Suci, and  P. H. Tannenbaum. 1957. 
The measurement of meaning. University of Illinois 
Press. 
B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up?: 
sentiment classification using machine learning tech-
niques. In Proc. of EMNLP, 79-86.  
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
Bleu: a method for automatic evaluation of ma-chine 
translation. In Proc. of ACL, 311?318. 
M. Taboada, M. Tofiloski, J. Brooke, K. Voll, and M. 
Stede. 2011. Lexicon-Based Methods for Sentiment 
Analysis. Computational Linguistics. 37(2): 267-307. 
K. Toutanova, D. Klein, C. Manning, and Y. Singer. 
2003. Feature-Rich Part-of-Speech Tagging with a 
Cyclic Dependency Network. In Proc. of HLT-
NAACL, 252-259. 
P. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. of ACL, 417-424. 
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based 
word alignment in statistical translation. In Proc. of 
COLING. 
X. Wan. 2009. Co-Training for Cross-Lingual Senti-
ment Classification. In proc. of ACL, 235-243.  
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment 
Analysis. In Proc. of EMNLP. 
D. Wu and P. Fung. 2009. Semantic Roles for SMT: A 
Hybrid Two-Pass Model. In Proc. of NAACL, 13-16.  
L. Xu and H. Lin. 2007. Ontology-Driven Affective 
Chinese Text Analysis and Evaluation Method. In 
Lecture Notes in Computer Science Vol. 4738, 723-
724, Springer. 
C. Zhang, P. Liu, Z. Zhu, and M. Fang. 2012. A Senti-
ment Analysis Method Based on a Polarity Lexicon. 
Journal of Shangdong University (Natural Science). 
47(3): 47-50. 
615
Proceedings of NAACL-HLT 2013, pages 938?946,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Adaptation of Reordering Models for Statistical Machine Translation
Boxing Chen, George Foster and Roland Kuhn
National Research Council Canada
first.last@nrc-cnrc.gc.ca
Abstract
Previous research on domain adaptation (DA)
for statistical machine translation (SMT) has
mainly focused on the translation model (TM)
and the language model (LM). To the best of
our knowledge, there is no previous work on
reordering model (RM) adaptation for phrase-
based SMT. In this paper, we demonstrate
that mixture model adaptation of a lexical-
ized RM can significantly improve SMT per-
formance, even when the system already con-
tains a domain-adapted TM and LM. We find
that, surprisingly, different training corpora
can vary widely in their reordering character-
istics for particular phrase pairs. Furthermore,
particular training corpora may be highly suit-
able for training the TM or the LM, but unsuit-
able for training the RM, or vice versa, so mix-
ture weights for these models should be esti-
mated separately. An additional contribution
of the paper is to propose two improvements
to mixture model adaptation: smoothing the
in-domain sample, and weighting instances
by document frequency. Applied to mixture
RMs in our experiments, these techniques (es-
pecially smoothing) yield significant perfor-
mance improvements.
1 Introduction
A phrase-based statistical machine translation
(SMT) system typically has three main components:
a translation model (TM) that contains information
about how to translate word sequences (phrases)
from the source language to the target language,
a language model (LM) that contains information
about probable word sequences in the target lan-
guage, and a reordering model (RM) that indicates
how the order of words in the source sentence is
likely to influence the order of words in the target
sentence. The TM and the RM are trained on parallel
data, and the LM is trained on target-language data.
Usage of language and therefore the best translation
practice differs widely across genres, topics, and di-
alects, and even depends on a particular author?s or
publication?s style; the word ?domain? is often used
to indicate a particular combination of all these fac-
tors. Unless there is a perfect match between the
training data domain and the (test) domain in which
the SMT system will be used, one can often get bet-
ter performance by adapting the system to the test
domain.
In offline domain adaptation, the system is pro-
vided with a sample of translated sentences from
the test domain prior to deployment. In a popular
variant of offline adaptation, linear mixture model
adaptation, each training corpus is used to gener-
ate a separate model component that forms part of
a linear combination, and the sample is used to as-
sign a weight to each component (Foster and Kuhn,
2007). If the sample resembles some of the corpora
more than others, those corpora will receive higher
weights in the combination.
Previous research on domain adaptation for SMT
has focused on the TM and the LM. Such research
is easily motivated: translations across domains are
unreliable. For example, the Chinese translation
of the English word ?mouse? would most likely be
?laoshu ??? if the topic is the animal; if the topic
is computer hardware, its translation would most
938
likely be ?shubiao???. However, when the trans-
lation is for people in Taiwan, even when the topic
is computer hardware, its translation would more
likely be ?huashu ???. It is intuitively obvious
why TM and LM adaptation would be helpful here.
By contrast, it is not at all obvious that RM model
adaptation will improve SMT performace. One
would expect reordering behaviour to be characteris-
tic of a particular language pair, but not of particular
domains. At most, one might think that reordering
is lexicalized?perhaps, (for instance) in translating
from Chinese to English, or from Arabic to English,
there are certain words whose English translations
tend to undergo long-distance movement from their
original positions, while others stay close to their
original positions. However, one would not expect
a particular Chinese adverb or a particular Arabic
noun to undergo long-distance movement when be-
ing translated into English in one domain, but not in
others. Nevertheless, that is what we observe: see
section 5 below.
This paper shows that RM adaptation improves
the performance of our phrase-based SMT system.
In our implementation, the RM is adapted by means
of a linear mixture model, but it is likely that other
forms of RM adaptation would also work. We ob-
tain even more effective RM adaptation by smooth-
ing the in-domain sample and by weighting orienta-
tion counts by the document frequency of the phrase
pair. Both improvements could be applied to the TM
or the LM as well, though we have not done so.
Finally, the paper analyzes reordering to see why
RM adaptation works. There seem to be two fac-
tors at work. First, the reordering behaviour of
words and phrases often differs dramatically from
one bilingual corpus to another. Second, there are
corpora (for instance, comparable corpora and bilin-
gual lexicons) which may contain very valuable in-
formation for the TM, but which are poor sources
of RM information; RM adaptation downweights in-
formation from these corpora significantly, and thus
improves the overall quality of the RM.
2 Reordering Model
In early SMT systems, such as (Koehn, 2004),
changes in word order when a sentence is trans-
lated were modeled by means of a penalty that is in-
curred when the decoder chooses, as the next source
phrase to be translated, a phrase that does not imme-
diately follow the previously translated source sen-
tence. Thus, the system penalizes deviations from
monotone order, with the magnitude of the penalty
being proportional to distance in the source sentence
between the end of the previously translated source
phrase and the start of the newly chosen source
phrase.
Many SMT systems, including our own, still use
this distance-based penalty as a feature. However,
starting with (Tillmann and Zhang, 2005; Koehn
et al, 2005), a more sophisticated type of reorder-
ing model has often been adopted as well, and has
yielded consistent performance gains. This type of
RM typically identifies three possible orientations
for a newly chosen source phrase: monotone (M),
swap (S), and discontinuous (D). The M orientation
occurs when the newly chosen phrase is immedi-
ately to the right of the previously translated phrase
in the source sentence, the S orientation occurs when
the new phrase is immediately to the left of the pre-
vious phrase, and the D orientation covers all other
cases.1 This type of RM is lexicalized: the estimated
probabilities of M, S and D depend on the source-
language and target-language words in both the pre-
vious phrase pair and the newly chosen one.
Galley and Manning (2008) proposed a ?hierar-
chical? lexicalized RM in which the orientation (M,
S, or D) is determined not by individual phrase pairs,
but by blocks. A block is the largest contiguous se-
quence of phrase pairs that satisfies the phrase pair
consistency requirement of having no external links.
Thus, classification of the orientation of a newly
chosen phrase as M, S, or D is carried out as if the
decoder always chose the longest possible source
phrase in the past, and will choose the longest pos-
sible source phrase in the future.
The RM used in this paper is hierarchical and lex-
icalized. For a given phrase pair (f , e), we estimate
the probabilities that it will be in an M, S, or D ori-
entation o with respect to the previous phrase pair
and the following phrase pair (two separate distri-
butions). Orientation counts c(o, f, e) are obtained
from a word-aligned corpus using the method de-
1Some researchers have distinguished between left and right
versions of the D orientation, but this 4-orientation scheme has
not yielded significant gains over the 3-orientation one.
939
scribed in (Cherry et al, 2012), and corresponding
probabilities p(o|f, e) are estimated using recursive
MAP smoothing:
p(o|f, e) =
c(o, f, e) + ?f p(o|f) + ?e p(o|e)
c(f, e) + ?f + ?e
p(o|f) =
c(o, f) + ?g p(o)
c(f) + ?g
p(o) =
c(o) + ?u/3
c(?) + ?u
, (1)
where p(o|e) is defined analogously to p(o|f), and
the four smoothing parameters ?e, ?f , ?g, and ?u
are set to values that minimize the perplexity of the
resulting model on held-out data.
During decoding, orientations with respect to the
previous context are obtained from a shift-reduce
parser, and orientations with respect to following
context are approximated using the coverage vector
(Cherry et al, 2012).
3 RM Adaptation
3.1 Linear mixture model
Following previous work (Foster and Kuhn, 2007;
Foster et al, 2010), we adopt the linear mixture
model technique for RM adaptation. This technique
trains separate models for each training corpus, then
learns weights for each of the models and combines
the weighted component models into a single model.
If we have N sub-corpora, the global reordering
model probabilities p(o|f, e) are computed as in (2):
p(o|f, e) =
N?
i=1
?i pi(o|f, e) (2)
where pi(o|f, e) is the reordering model trained on
sub-corpus i, and ?i is its weight.
Following (Foster et al, 2010), we use the EM
algorithm to learn the weights that maximize the
probability of phrase-pair orientations in the devel-
opment set (in-domain data):
?? = argmax
?
?
o,f,e
p?(o, f, e) log
N?
i=1
?i pi(o|f, e)
(3)
where p?(o, f, e) is the empirical distribution of
counts in the dev set (proportional to c(o, f, e)). Two
separate sets of mixing weights are learned: one for
the distribution with respect to the previous phrase
pair, and one for the next phrase pair.
3.2 Development set smoothing
In Equation 3, p?(o, f, e) is extracted from the in-
domain development set. Since dev sets for SMT
systems are typically small (1,000-3,000 sentences),
we apply smoothing to this RM. We first obtain
a smoothed conditional distribution p(o|f, e) using
the MAP technique described above, then multiply
by the empirical marginal p?(e, f) to obtain a final
smoothed joint distribution p(o, f, e).
There is nothing about this idea that limits it to
the RM: smoothing could be applied to the statistics
in the dev that are used to estimate a mixture TM
or LM, in order to mitigate over-fitting. However,
we note that, compared to the TM, the over-fitting
problem is likely to be more acute for the RM, since
it splits counts for each phrase pair into three cate-
gories.
3.3 Document-frequency weighting
Mixture models, like the RM in this paper, depend
on the existence of multiple training corpora, with
each sub-corpus nominally representing a domain.
A recent paper suggests that some phrase pairs be-
long to general language, while others are domain-
specific (Foster et al, 2010). If a phrase pair exists
in all training corpora, it probably belongs to general
language; on the other hand, if it appears in only
one or two training corpora, it is more likely to be
domain-specific.
We were interested in seeing whether information
about domain-specificity could improve the estima-
tion of mixture RM weights. The intuition is that
phrase pairs that belong to general language should
contribute more to determining sub-corpus weights,
since they are the ones whose reordering behaviour
is most likely to shift with domain. To capture this
intuition, we multiplied the empirical distribution in
(3) by the following factor, inspired by the standard
document-frequency formula:
D(f, e) = log(DF (f, e) +K), (4)
where DF (f, e) is the number of sub-corpora
that (f, e) appears in, and K is an empirically-
determined smoothing term.
940
corpus # segs # en tok % genres
fbis 250K 10.5M 3.7 nw
financial 90K 2.5M 0.9 financial
gale bc 79K 1.3M 0.5 bc
gale bn 75K 1.8M 0.6 bn ng
gale nw 25K 696K 0.2 nw
gale wl 24K 596K 0.2 wl
hkh 1.3M 39.5M 14.0 Hansard
hkl 400K 9.3M 3.3 legal
hkn 702K 16.6M 5.9 nw
isi 558K 18.0M 6.4 nw
lex&ne 1.3M 2.0M 0.7 lexicon
others nw 146K 5.2M 1.8 nw
sinorama 282K 10.0M 3.5 nw
un 5.0M 164M 58.2 un
TOTAL 10.1M 283M 100.0 (all)
devtest
tune 1,506 161K nw wl
NIST06 1,664 189K nw bn ng
NIST08 1,357 164K nw wl
Table 1: NIST Chinese-English data. In the gen-
res column: nw=newswire, bc=broadcast conversa-
tion, bn=broadcast news, wl=weblog, ng=newsgroup,
un=United Nations proceedings.
4 Experiments
4.1 Data setting
We carried out experiments in two different settings,
both involving data from NIST Open MT 2012.2
The first setting uses data from the Chinese to En-
glish constrained track, comprising 283M English
tokens. We manually identified 14 sub-corpora on
the basis of genres and origins. Table 1 summarizes
the statistics and genres of all the training corpora
and the development and test sets; for the training
corpora, we show their size in number of words as
a percentage of all training data. Most training cor-
pora consist of parallel sentence pairs. The isi and
lex&ne corpora are exceptions: the former is ex-
tracted from comparable data, while the latter is a
lexicon that includes many named entities. The de-
velopment set (tune) was taken from the NIST 2005
evaluation set, augmented with some web-genre ma-
terial reserved from other NIST corpora.
2http://www.nist.gov/itl/iad/mig/openmt12.cfm
corpus # segs # en toks % genres
gale bc 57K 1.6M 3.3 bc
gale bn 45K 1.2M 2.5 bn
gale ng 21K 491K 1.0 ng
gale nw 17K 659K 1.4 nw
gale wl 24K 590K 1.2 wl
isi 1,124K 34.7M 72.6 nw
other nw 224K 8.7M 18.2 nw
TOTAL 1,512K 47.8M 100.0 (all)
devtest
NIST06 1,664 202K nw wl
NIST08 1,360 205K nw wl
NIST09 1,313 187K nw wl
Table 2: NIST Arabic-English data. In the gen-
res column: nw=newswire, bc=broadcast conversation,
bn=broadcase news, ng=newsgroup, wl=weblog.
The second setting uses NIST 2012 Arabic to En-
glish data, but excluding the UN data. There are
about 47.8 million English running words in these
training data. We manually grouped the training data
into 7 groups according to genre and origin. Ta-
ble 2 summarizes the statistics and genres of all the
training corpora and the development and test sets.
Note that for this language pair, the comparable isi
data represent a large proportion of the training data:
72% of the English words. We use the evaluation
sets from NIST 2006, 2008, and 2009 as our devel-
opment set and two test sets, respectively.
4.2 System
Experiments were carried out with an in-house
phrase-based system similar to Moses (Koehn et al,
2007). The corpus was word-aligned using IBM2,
HMM, and IBM4 models, and the phrase table was
the union of phrase pairs extracted from these sepa-
rate alignments, with a length limit of 7. The trans-
lation model was smoothed in both directions with
KN smoothing (Chen et al, 2011). The DF smooth-
ing term K in equation 4 was set to 0.1 using held-
out optimization. We use the hierarchical lexical-
ized RM described above, with a distortion limit of
7. Other features include lexical weighting in both
directions, word count, a distance-based RM, a 4-
gram LM trained on the target side of the parallel
data, and a 6-gram English Gigaword LM. The sys-
941
system Chinese Arabic
baseline 31.7 46.8
baseline+loglin 29.6 45.9
RMA 31.8 47.7**
RMA+DF 32.2* 47.9**
RMA+dev smoothing 32.3* 48.3**
RMA+dev smoothing+DF 32.8** 48.2**
Table 3: Results for variants of RM adaptation.
system Chinese Arabic
LM+TM adaptation 33.2 47.7
+RMA+dev-smoothing+DF 33.5 48.4**
Table 4: RM adaptation improves over a baseline con-
taining adapted LMs and TMs.
tem was tuned with batch lattice MIRA (Cherry and
Foster, 2012).
4.3 Results
For our main baseline, we simply concatenate all
training data. We also tried augmenting this with
separate log-linear features corresponding to sub-
corpus-specific RMs. Our metric is case-insensitvie
IBM BLEU-4 (Papineni et al, 2002); we report
BLEU scores averaged across both test sets. Follow-
ing (Koehn, 2004), we use the bootstrap-resampling
test to do significance testing. In tables 3 to 5, *
and ** denote significant gains over the baseline at
p < 0.05 and p < 0.01 levels, respectively.
Table 3 shows that reordering model adaptation
helps in both data settings. Adding either document-
frequency weighting (equation 4) or dev-set smooth-
ing makes the improvement significant in both set-
tings. Using both techniques together yields highly
significant improvements.
Our second experiment measures the improve-
ment from RM adaptation over a baseline that
includes adapted LMs and TMs. We use the
same technique?linear mixtures with EM-tuned
weights?to adapt these models. Table 4 shows that
adapting the RM gives gains over this strong base-
line for both language pairs; improvements are sig-
nificant in the case of Arabic to English.
The third experiment breaks down the gains in the
last line of table 4 by individual adapted model. As
shown in table 5, RM adaptation yielded the largest
system Chinese Arabic
baseline 31.7 46.8
LM adaptation 32.1* 47.0
TM adaptation 33.0** 47.5**
RM adaptation 32.8** 48.2**
Table 5: Comparison of LM, TM, and RM adaptation.
improvement on Arabic, while TM adaptation did
best on Chinese. Surprisingly, both methods sig-
nificantly outperformed LM adaptation, which only
achieved significant gains over the baseline for Chi-
nese.
5 Analysis
Why does RM adaptation work? Intuitively, one
would think that reordering behaviour for a given
phrase pair should not be much affected by domain,
making RM adaptation pointless. That is probably
why (as far as we know) no-one has tried it before.
In this section, we describe three factors that account
for at least part of the observed gains.
5.1 Weighting by corpus quality
One answer to the above question is that some cor-
pora are better for training RMs than others. Fur-
thermore, corpora that are good for training the LM
or TM are not necessarily good for training the RM,
and vice versa. Tables 6 and 7 illustrate this. These
list the weights assigned to various sub-corpora for
LM, TM, and RM mixture models.
The weights assigned to the isi sub-corpus in par-
ticular exhibit a striking pattern. These are high in
the LM mixtures, moderate in the TM mixtures, and
very low in the RM mixtures. When one considers
that isi contains 72.6% of the English words in the
Arabic training data (see table 2), its weight of 0.01
in the RM mixture is remarkable.
On reflection, it makes sense that EM would as-
sign weights in the order it does. The isi corpus
consists of comparable data: sentence pairs whose
source- and target-language sides are similar, but of-
ten not mutual translations. These are a valuable
source of in-domain n-grams for the LM; a some-
what noisy source of in-domain phrase pairs for the
TM; and an unreliable source of re-ordering patterns
for the RM. Figure 1 shows this. Although the two
942
LM TM RM
isi (0.23) un (0.29) un (0.21)
gale nw (0.11) fbis (0.15) gale nw (0.13)
un (0.11) hkh (0.10) lex&ne (0.12)
sino. (0.09) gale nw (0.09) hkh (0.08)
fbis (0.08) gale bn (0.07) fbis (0.08)
fin. (0.07) oth nw (0.06) gale bn (0.08)
oth nw (0.07) sino. (0.06) gale wl (0.06)
gale bn (0.07) isi (0.05) gale bc (0.06)
gale wl (0.06) hkn (0.04) hkn (0.04)
hkh (0.06) fin. (0.04) fin. (0.04)
hkn (0.03) gale bc (0.03) oth nw (0.03)
gale bc (0.02) gale wl (0.02) hkl (0.03)
lex&ne (0.00) lex&ne (0.00) isi (0.01)
hkl (0.00) hkl (0.00) sino. (0.01)
Table 6: Chinese-English sub-corpora for LM, TM, and
RM mixture models, ordered by mixture weight.
LM TM RM
isi (0.41) isi (0.35) gale bc (0.21)
oth nw (0.19) oth nw (0.29) gale ng (0.20)
gale ng (0.15) gale bc (0.10) gale nw (0.20)
gale wl (0.09) gale ng (0.08) oth nw (0.13)
gale nw (0.07) gale bn (0.07) gale ng (0.12)
gale bc (0.05) gale nw (0.07) gale wb (0.11)
gale bn (0.02) gale wl (0.05) isi (0.01)
Table 7: Arabic-English sub-corpora for LM, TM, and
RM mixture models, ordered by mixture weight.
sides of the comparable data are similar, they give
the misleading impression that the phrases labeled
1, 2, 3 in the Chinese source should be reordered as
2, 3, 1 in English. We show a reference translation
of the Chinese source (not found in the comparable
data) that reorders the phrases as 1, 3, 2.
Thus, RM adaptation allows the RM to learn that
certain corpora whose reordering information is of
lower quality corpora should have lower weights.
The optimal weights for corpora inside an RM may
be different from the optimal weights inside a TM or
LM.
5.2 Weighting by domain match
So is this all that RM adaptation does: downweight
poor-quality data? We believe there is more to
RM adaptation than that. Specifically, even if one
 REF: The American list of goods that would incur tariffs in retaliation would certainly not be accepted by the Chinese government.  SRC: ??(1) ? ?? ??? ??(2) ?? ?? ? ??  ?(3)?  TGT: And the Chinese(2) side would certainly not accept(3)  the unreasonable demands put forward by the Americans(1) concerning the protection of intellectual property rights .  
Figure 1: Example of sentence pair from comparable
data; underlined words with the same number are trans-
lations of each other
Corpus M S D Count
fbis 0.50 0.07 0.43 685
financial 0.32 0.28 0.41 65
gale bc 0.60 0.10 0.31 50
gale bn 0.47 0.15 0.37 109
gale nw 0.51 0.05 0.44 326
gale wl 0.42 0.26 0.32 52
hkh 0.29 0.23 0.48 130
hkl 0.28 0.16 0.56 263
hkn 0.30 0.27 0.43 241
isi 0.24 0.16 0.60 240
lex&ne 0.94 0.03 0.02 1
others nw 0.29 0.16 0.55 23
sinorama 0.44 0.07 0.49 110
un 0.37 0.10 0.53 15
dev 0.46 0.24 0.31 11
Table 8: Orientation frequencies for the phrase pair ??
? immediately?, with respect to the previous phrase.
considers only high-quality data for training RMs
(ignoring comparable data, etc.) one sees differ-
ences in reordering behaviour between different do-
mains. This isn?t just because of differences in word
frequencies between domains, because we observe
domain-dependent differences in reordering for the
same phrase pair. Two examples are given below:
one Chinese-English, one Arabic-English.
Table 8 shows reordering data for the phrase
pair ??? immediately? in various corpora. No-
tice the strong difference in behaviour between the
three Hong Kong corpora?hkh, hkl and hkn?and
some of the other corpora, for instance fbis. In the
943
Corpus M S D Count
gale bc 0.50 0.27 0.22 233
gale bn 0.56 0.21 0.23 226
gale ng 0.51 0.13 0.37 295
gale nw 0.47 0.20 0.33 167
gale wl 0.56 0.18 0.26 127
isi 0.50 0.06 0.44 5502
other nw 0.50 0.16 0.34 1450
dev 0.75 0.12 0.13 52
Table 9: Orientation frequencies for the phrase pair
?work AlEml? with respect to the previous phrase.
Hong Kong corpora, immediately is much less likely
(probability of around 0.3) to be associated with a
monotone (M) orientation than it is in fbis (proba-
bility of 0.5). This phrase pair is relatively frequent
in both corpora, so this disparity seems too great to
be due to chance.
Table 9 shows reordering behaviour for the phrase
pair ?work AlEml?3 across different sub-corpora.
As in the Chinese example, there appear to be sig-
nificant differences in reordering patterns for cer-
tain corpora. For instance, gale bc swaps this well-
attested phrase pair twice as often (probability of
0.27) as gale ng (probability of 0.13).
For Chinese, it is possible that dialect plays a role
in reordering behaviour. In theory, Mandarin Chi-
nese is a single language which is quite different,
especially in spoken form, from other languages of
China such as Cantonese, Hokkien, Shanghainese,
and so on. In practice, many speakers of Mandarin
may be unconsciously influenced by other languages
that they speak, or by other languages that they don?t
speak but that have an influence over people they in-
teract with frequently. Word order can be affected
by this: the Mandarin of Mainland China, Hong
Kong and Taiwan sometimes has slightly different
word order. Hong Kong Mandarin can be somewhat
influenced by Cantonese, and Taiwan Mandarin by
Hokkien. For instance, if a verb is modified by an
adverb in Mandarin, the standard word order is ?ad-
verb verb?. However, since in Cantonese, ?verb ad-
verb? is a more common word order, speakers and
writers of Mandarin in Hong Kong may adopt the
3We represent the Arabic word AlEml in its Buckwalter
transliteration.
  	
 

 	 

Figure 2: An example of different word ordering in Man-
darin from different area.
?verb adverb? order in that language as well. Figure
2 shows how a different word order in the Mandarin
source affects reordering when translating into En-
glish. Perhaps in situations where different training
corpora represent different dialects, RM adaptation
involves an element of dialect adaptation. We are ea-
ger to test this hypothesis for Arabic?different di-
alects of Arabic are much more different from each
other than dialects of Mandarin, and reordering is
often one of the differences?but we do not have ac-
cess to Arabic training, dev, and test data in which
the dialects are clearly separated.
It is possible that RM adaptation also has an el-
ement of genre adaptation. We have not yet been
able to confirm or refute this hypothesis. However,
whatever is causing the corpus-dependent reorder-
ing patterns for particular phrase pairs shown in the
two tables above, it is clear that they may explain
the performance improvements we observe for RM
adaptation in our experiments.
5.3 Penalizing highly-specific phrase pairs
In section 3.3 we described our strategy for giving
general (high document-frequency) phrase pairs that
occur in the dev set more influence in determining
mixing weights. An artifact of our implementation
applies a similar strategy to the probability estimates
for all phrase pairs in the model. This is that 0 prob-
abilities are assigned to all orientations whenever a
phrase pair is absent from a particular sub-corpus.
Thus, for example, a pair (f, e) that occurs only
in sub-corpus iwill receive a probability p(o|f, e) =
?i pi(o|f, e) in the mixture model (equation 2).
Since ?i ? 1, this amounts to a penalty on pairs
that occur in few sub-corpora, especially ones with
low mixture weights.
The resulting mixture model is deficient (non-
944
normalized), but easy to fix by backing off to a
global distribution such as p(o) in equation 1. How-
ever, we found that this ?fix? caused large drops in
performance, for instance from the Arabic BLEU
score of 48.3 reported in table 3 to 46.0. We there-
fore retained the original strategy, which can be seen
as a form of instance weighting. Moreover, it is one
that is particularly effective in the RM, since, com-
pared to a similar strategy in the TM (which we also
employ), it applies to whole phrase pairs and results
in much larger penalties.
6 Related work
Domain adaptation is an active topic in the NLP re-
search community. Its application to SMT systems
has recently received considerable attention. Previ-
ous work on SMT adaptation has mainly focused
on translation model (TM) and language model
(LM) adaptation. Approaches that have been tried
for SMT model adaptation include mixture models,
transductive learning, data selection, data weighting,
and phrase sense disambiguation.
Research on mixture models has considered both
linear and log-linear mixtures. Both were studied
in (Foster and Kuhn, 2007), which concluded that
the best approach was to combine sub-models of
the same type (for instance, several different TMs
or several different LMs) linearly, while combining
models of different types (for instance, a mixture
TM with a mixture LM) log-linearly. (Koehn and
Schroeder, 2007), instead, opted for combining the
sub-models directly in the SMT log-linear frame-
work.
In transductive learning, an MT system trained on
general domain data is used to translate in-domain
monolingual data. The resulting bilingual sentence
pairs are then used as additional training data (Ueff-
ing et al, 2007; Chen et al, 2008; Schwenk, 2008;
Bertoldi and Federico, 2009).
Data selection approaches (Zhao et al, 2004; Lu?
et al, 2007; Moore and Lewis, 2010; Axelrod et
al., 2011) search for bilingual sentence pairs that are
similar to the in-domain ?dev? data, then add them
to the training data. The selection criteria are typi-
cally related to the TM, though the newly found data
will be used for training not only the TM but also the
LM and RM.
Data weighting approaches (Matsoukas et al,
2009; Foster et al, 2010; Huang and Xiang, 2010;
Phillips and Brown, 2011; Sennrich, 2012) use a
rich feature set to decide on weights for the train-
ing data, at the sentence or phrase pair level. For
instance, a sentence from a corpus whose domain is
far from that of the dev set would typically receive
a low weight, but sentences in this corpus that ap-
pear to be of a general nature might receive higher
weights.
The 2012 JHU workshop on Domain Adapta-
tion for MT 4 proposed phrase sense disambiguation
(PSD) for translation model adaptation. In this ap-
proach, the context of a phrase helps the system to
find the appropriate translation.
All of the above work focuses on either TM or
LM domain adaptation.
7 Conclusions
In this paper, we adapt the lexicalized reordering
model (RM) of an SMT system to the domain in
which the system will operate using a mixture model
approach. Domain adaptation of translation mod-
els (TMs) and language models (LMs) has become
common for SMT systems, but to our knowledge
this is the first attempt in the literature to adapt the
RM. Our experiments demonstrate that RM adap-
tation can significantly improve translation quality,
even when the system already has TM and LM adap-
tation. We also experimented with two modifica-
tions to linear mixture model adaptation: dev set
smoothing and weighting orientation counts with
document frequency of phrase pairs. Both ideas
are potentially applicable to TM and LM adaptation.
Dev set smoothing, in particular, seems to improve
the performance of RM adaptation significantly. Fi-
nally, we investigate why RM adaptation helps SMT
performance. Three factors seem to be important:
downweighting information from corpora that are
less suitable for modeling reordering (such as com-
parable corpora), dialect/genre effects, and implicit
instance weighting.
4http://www.clsp.jhu.edu/workshops/archive/ws-
12/groups/dasmt
945
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In EMNLP 2011.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the 4th
Workshop on Statistical Machine Translation, Athens,
March. WMT.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for smt self-
enhancement. In ACL 2008.
Boxing Chen, Roland Kuhn, George Foster, and Howard
Johnson. 2011. Unpacking and transforming feature
functions: New ways to smooth phrase tables. In MT
Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In WMT 2012.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the ACL Work-
shop on Statistical Machine Translation, Prague, June.
WMT.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Boston.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model. In
EMNLP 2008, pages 848?856, Hawaii, October.
Fei Huang and Bing Xiang. 2010. Feature-rich discrimi-
native phrase rescoring for SMT. In COLING 2010.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch,
M. Osborne, D. Talbot, and M. White. 2005. Edin-
burgh system description for the 2005 NIST MT eval-
uation. In Proceedings of Machine Translation Evalu-
ation Workshop.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL 2007, Demon-
stration Session.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the 6th Conference of the As-
sociation for Machine Translation in the Americas,
Georgetown University, Washington D.C., October.
Springer-Verlag.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. In Proceedings
of the 2007 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Prague, Czech
Republic.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL
2010.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311?318, Philadel-
phia, July. ACL.
Aaron B. Phillips and Ralf D. Brown. 2011. Training
machine translation with a second-order taylor approx-
imation of weighted translation instances. In MT Sum-
mit 2011.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT 2008.
Rico Sennrich. 2012. Mixture-modeling with unsuper-
vised clusters for domain adaptation in statistical ma-
chine translation. In EACL 2012.
Christoph Tillmann and Tong Zhang. 2005. A localized
prediction model for statistical machine translation. In
Proceedings of the 43th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Ann Ar-
bor, Michigan, July. ACL.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Prague, Czech Republic, June. ACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING) 2004, Geneva, August.
946
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 834?843,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bilingual Sense Similarity for Statistical Machine Translation 
 
 
Boxing Chen, George Foster and Roland Kuhn 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca 
 
  
 
Abstract 
 
This paper proposes new algorithms to com-
pute the sense similarity between two units 
(words, phrases, rules, etc.) from parallel cor-
pora. The sense similarity scores are computed 
by using the vector space model.  We then ap-
ply the algorithms to statistical machine trans-
lation by computing the sense similarity be-
tween the source and target side of translation 
rule pairs. Similarity scores are used as addi-
tional features of the translation model to im-
prove translation performance. Significant im-
provements are obtained over a state-of-the-art 
hierarchical phrase-based machine translation 
system. 
1 Introduction 
The sense of a term can generally be inferred 
from its context. The underlying idea is that a 
term is characterized by the contexts it co-occurs 
with. This is also well known as the Distribu-
tional Hypothesis (Harris, 1954): terms occurring 
in similar contexts tend to have similar mean-
ings. There has been a lot of work to compute the 
sense similarity between terms based on their 
distribution in a corpus, such as (Hindle, 1990; 
Lund and Burgess, 1996; Landauer and Dumais, 
1997; Lin, 1998; Turney, 2001; Pantel and Lin, 
2002; Pado and Lapata, 2007).  
In the work just cited, a common procedure is 
followed. Given two terms to be compared, one 
first extracts various features for each term from 
their contexts in a corpus and forms a vector 
space model (VSM); then, one computes their 
similarity by using similarity functions. The fea-
tures include words within a surface window of a 
fixed size (Lund and Burgess, 1996), grammati-
cal dependencies (Lin, 1998; Pantel and Lin 
2002; Pado and Lapata, 2007), etc.  The similari-
ty function which has been most widely used is 
cosine distance (Salton and McGill, 1983); other 
similarity functions include Euclidean distance, 
City Block distance (Bullinaria and Levy; 2007), 
and Dice and Jaccard coefficients (Frakes and 
Baeza-Yates, 1992), etc. Measures of monolin-
gual sense similarity have been widely used in 
many applications, such as synonym recognizing 
(Landauer and Dumais, 1997), word clustering 
(Pantel and Lin 2002), word sense disambigua-
tion (Yuret and Yatbaz 2009), etc. 
Use of the vector space model to compute  
sense similarity has also been adapted to the mul-
tilingual condition,  based on the assumption that 
two terms with similar meanings often occur in 
comparable contexts across languages. Fung 
(1998) and Rapp (1999) adopted VSM for the 
application of extracting translation pairs from 
comparable or even unrelated corpora. The vec-
tors in different languages are first mapped to a 
common space using an initial bilingual dictio-
nary, and then compared. 
However, there is no previous work that uses 
the VSM to compute sense similarity for terms 
from parallel corpora. The sense similarities, i.e. 
the translation probabilities in a translation mod-
el, for units from parallel corpora are mainly 
based on the co-occurrence counts of the two 
units. Therefore, questions emerge: how good is 
the sense similarity computed via VSM for two 
units from parallel corpora? Is it useful for multi-
lingual applications, such as statistical machine 
translation (SMT)? 
In this paper, we try to answer these questions, 
focusing on sense similarity applied to the SMT 
task. For this task, translation rules are heuristi-
cally extracted from automatically word-aligned 
sentence pairs. Due to noise in the training cor-
pus or wrong word alignment, the source and 
target sides of some rules are not semantically 
equivalent, as can be seen from the following 
834
real examples which are taken from the rule table 
built on our training data (Section 5.1): 
?? ? X ?? ||| one of X (*) 
?? ? X ?? ||| one of X in the world    
?? ?? ||| many citizens 
?? ?? ||| many hong kong residents (*) 
The source and target sides of the rules with (*) 
at the end are not semantically equivalent; it 
seems likely that measuring the semantic similar-
ity from their context between the source and 
target sides of rules might be helpful to machine 
translation. 
In this work, we first propose new algorithms 
to compute the sense similarity between two 
units (unit here includes word, phrase, rule, etc.) 
in different languages by using their contexts. 
Second, we use the sense similarities between the 
source and target sides of a translation rule to 
improve statistical machine translation perfor-
mance.  
This work attempts to measure directly the 
sense similarity for units from different languag-
es by comparing their contexts1. Our contribution 
includes proposing new bilingual sense similarity 
algorithms and applying them to machine trans-
lation. 
We chose a hierarchical phrase-based SMT 
system as our baseline; thus, the units involved 
in computation of sense similarities are hierar-
chical rules. 
2 Hierarchical phrase-based MT system 
The hierarchical phrase-based translation method 
(Chiang, 2005; Chiang, 2007) is a formal syntax-
based translation modeling method; its transla-
tion model is a weighted synchronous context 
free grammar (SCFG). No explicit linguistic syn-
tactic information appears in the model. An 
SCFG rule has the following form: 
~,,???X
 
where X is a non-terminal symbol shared by all 
the rules; each rule has at most two non-
terminals. ?  (? ) is a source (target) string con-
sisting of terminal and non-terminal symbols. ~  
defines a one-to-one correspondence between 
non-terminals in ?  and ? . 
                                               
1
 There has been a lot of work (more details in Section 7) on 
applying word sense disambiguation (WSD) techniques in 
SMT for translation selection. However, WSD techniques 
for SMT do so indirectly, using source-side context to help 
select a particular translation for a source rule. 
 source target 
Ini. phr. ? ?? ? ?? he attended the meeting 
Rule 1 
Context 1 
? ?? ? X1 
?? 
he attended X1 
the, meeting 
Rule 2 
Context 2 
?? 
?, ??, ? 
the meeting 
he, attended 
Rule 3 
Context 3 
? X1?? 
??, ? 
he X1 the meeting 
attended 
Rule 4 
Context 4 
?? ? 
?,?? 
attended 
he, the, meeting 
 
Figure 1: example of hierarchical rule pairs and their 
context features. 
 
Rule frequencies are counted during rule ex-
traction over word-aligned sentence pairs, and 
they are normalized to estimate features on rules. 
Following (Chiang, 2005; Chiang, 2007), 4 fea-
tures are computed for each rule: 
? )|( ??P  and )|( ??P  are direct and in-
verse rule-based conditional probabilities; 
? )|( ??wP  and )|( ??wP are direct and in-
verse lexical weights (Koehn et al, 2003). 
Empirically, this method has yielded better 
performance on language pairs such as Chinese-
English than the phrase-based method because it 
permits phrases with gaps; it generalizes the 
normal phrase-based models in a way that allows 
long-distance reordering (Chiang, 2005; Chiang, 
2007). We use the Joshua implementation of the 
method for decoding (Li et al, 2009). 
3 Bag-of-Words Vector Space Model 
To compute the sense similarity via VSM, we 
follow the previous work (Lin, 1998) and 
represent the source and target side of a rule by 
feature vectors. In our work, each feature corres-
ponds to a context word which co-occurs with 
the translation rule. 
3.1 Context Features 
In the hierarchical phrase-based translation me-
thod, the translation rules are extracted by ab-
stracting some words from an initial phrase pair 
(Chiang, 2005). Consider a rule with non-
terminals on the source and target side; for a giv-
en instance of the rule (a particular phrase pair in 
the training corpus), the context will be the 
words instantiating the non-terminals. In turn, the 
context for the sub-phrases that instantiate the 
non-terminals will be the words in the remainder 
of the phrase pair. For example in Figure 1, if we 
835
have an initial phrase pair ? ?? ? ?? ||| he 
attended the meeting, and we extract four rules 
from this initial phrase: ? ?? ? X1 ||| he at-
tended X1, ?? ||| the meeting, ? X1?? ||| he 
X1 the meeting, and?? ? ||| attended. There-
fore, the and meeting are context features of tar-
get pattern he attended X1; he and attended are 
the context features of the meeting; attended is 
the context feature of he X1 the meeting;  also he, 
the and meeting are the context feature of at-
tended (in each case, there are also source-side 
context features).  
3.2 Bag-of-Words Model 
For each side of a translation rule pair, its context 
words are all collected from the training data, 
and two ?bags-of-words? which consist of col-
lections of source and target context words co-
occurring with the rule?s source and target sides 
are created. 
},...,,{
},...,,{
21
21
Je
If
eeeB
fffB
=
=
                        (1) 
where )1( Iifi ??  are source context words 
which co-occur with the source side of rule ? , 
and )1( Jje j ??  are target context words 
which co-occur with the target side of rule ? . 
Therefore, we can represent source and target 
sides of the rule by vectors fv
v
  and ev
v
 as in Eq-
uation (2): 
},...,,{
},...,,{
21
21
J
I
eeee
ffff
wwwv
wwwv
=
=
v
v
                     (2) 
where 
ifw  and jew are values for each source 
and target context feature; normally, these values 
are based on the counts of the words in the cor-
responding bags.  
3.3 Feature Weighting Schemes 
We use pointwise mutual information (Church et 
al., 1990) to compute the feature values. Let c 
( fBc ? or eBc ?  ) be a context word and 
),( crF  be the frequency count of a rule r (?  or 
? ) co-occurring with the context word c. The 
pointwise mutual information ),( crMI  is de-
fined as: 
N
cF
N
rF
N
crF
crMIcrw )(log)(log
),(log
),(),(
?
==
          (3) 
where N is the total frequency counts of all rules 
and their context words. Since we are using this 
value as a weight, following (Turney, 2001), we 
drop log, N and )(rF . Thus (3) simplifies to:  
)(
),(),(
cF
crF
crw =
                     (4) 
It can be seen as an estimate of )|( crP , the em-
pirical probability of observing r given c. 
A problem with )|( crP  is that it is biased 
towards infrequent words/features. We therefore 
smooth ),( crw  with add-k smoothing: 
kRcF
kcrF
kcrF
kcrF
crw R
i
i
+
+
=
+
+
=
?
=
)(
),(
)),((
),(),(
1
  (5) 
where k is a tunable global smoothing constant, 
and R is the number of rules. 
4 Similarity Functions 
There are many possibilities for calculating simi-
larities between bags-of-words in different lan-
guages. We consider IBM model 1 probabilities 
and cosine distance similarity functions. 
4.1 IBM Model 1 Probabilities 
For the IBM model 1 similarity function, we take 
the geometric mean of symmetrized conditional 
IBM model 1 (Brown et al, 1993) bag probabili-
ties, as in Equation (6). 
))|()|((),( feef BBPBBPsqrtsim ?=??       (6) 
To compute )|( ef BBP , IBM model 1 as-
sumes that all source words are conditionally 
independent, so that: 
 ?
=
=
I
i
eief BfpBBP
1
)|()|(                (7) 
To compute, we use a ?Noisy-OR? combina-
tion which has shown better performance than 
standard IBM model 1 probability, as described 
in (Zens and Ney, 2004): 
)|(1)|( eiei BfpBfp ?=                       (8) 
?
=
???
J
j
jiei efpBfp
1
))|(1(1)|(          (9) 
where )|( ei Bfp  is the probability that if  is not 
in the translation of eB , and  is the IBM model 1 
probability. 
4.2 Vector Space Mapping 
A common way to calculate semantic similarity 
is by vector space cosine distance; we will also 
836
use this similarity function in our algorithm. 
However, the two vectors in Equation (2) cannot 
be directly compared because the axes of their 
spaces represent different words in different lan-
guages, and also their dimensions I and J are not 
assured to be the same. Therefore, we need to 
first map a vector into the space of the other vec-
tor, so that the similarity can be calculated. Fung 
(1998) and Rapp (1999) map the vector one-
dimension-to-one-dimension (a context word is a 
dimension in each vector space) from one lan-
guage to another language via an initial bilingual 
dictionary. We follow (Zhao et al, 2004) to do 
vector space mapping.  
Our goal is ? given a source pattern ? to dis-
tinguish between the senses of its associated tar-
get patterns. Therefore, we map all vectors in 
target language into the vector space in the 
source language. What we want is a representa-
tion 
av
v
 in the source language space of the target 
vector 
ev
v
. To get 
av
v
, we can let ifaw , the weight 
of the ith source feature, be a linear combination 
over target features. That is to say, given a 
source feature weight for fi, each target feature 
weight is linked to it with some probability. So 
that we can calculate a transformed vector from 
the target vectors by calculating weights if
aw  us-
ing a translation lexicon: 
?
=
=
J
j
eji
f
a j
i wefw
1
)|Pr(                    (10) 
where )|( ji efp  is a lexical probability (we use 
IBM model 1 probability). Now the source vec-
tor and the mapped vector av
v
 have the same di-
mensions as shown in (11): 
},...,,{
},...,,{
21
21
I
I
f
a
f
a
f
aa
ffff
wwwv
wwwv
=
=
v
v
                   (11) 
4.3 Na?ve Cosine Distance Similarity 
The standard cosine distance is defined as the 
inner product of the two vectors fv
v
 and av
v
 nor-
malized by their norms. Based on Equation (10) 
and (11), it is easy to derive the similarity as fol-
lows: 
)()(
)|Pr(
||||),cos(),(
1
2
1
2
1 1
??
??
==
= =
=
?
?
==
I
i
f
a
I
I
f
I
i
J
j
ejif
af
af
af
i
i
ji
wsqrtwsqrt
wefw
vv
vv
vvsim vv
vv
vv??
         (12) 
where I and J are the number of the words in 
source and target bag-of-words; 
ifw  and jew are 
values of source and target features; ifaw  is the 
transformed weight mapped from all target fea-
tures to the source dimension at word fi. 
4.4 Improved Similarity Function 
To incorporate more information than the origi-
nal similarity functions ? IBM model 1 proba-
bilities in Equation (6) and na?ve cosine distance 
similarity function in Equation (12) ? we refine 
the similarity function and propose a new algo-
rithm.  
As shown in Figure 2, suppose that we have a 
rule pair ),( ?? . fullfC  and fulleC  are the contexts 
extracted according to the definition in section 3 
from the full training data for ?
 
and for ? , re-
spectively. coocfC and cooceC  are the contexts for 
?
 
  and ?   when ?
 
and ? co-occur. Obviously, 
they satisfy the constraints: fullf
cooc
f CC ?  and  
full
e
cooc
e CC ? .  Therefore, the original similarity 
functions are to compare the two context vectors 
built on full training data directly, as shown in 
Equation (13). 
),(),( fullefullf CCsimsim =??             (13) 
Then, we propose a new similarity function as 
follows: 
321 ),(),(),(
),(
???
??
cooc
e
full
e
cooc
e
cooc
f
cooc
f
full
f CCsimCCsimCCsim
sim
??
=
(14) 
where the parameters i? (i=1,2,3) can be tuned 
via minimal error rate training (MERT) (Och, 
2003). 
 
 
 
 
 
 
 
 
 
 
Figure 2: contexts for rule ?
 
  and ? . 
 
A unit?s sense is defined by all its contexts in 
the whole training data; it may have a lot of dif-
ferent senses in the whole training data. Howev-
er, when it is linked with another unit in the other 
language, its sense pool is constrained and is just 
?  
?  
full
fC  coocfC  
   
full
eC  cooceC  
837
a subset of the whole sense set. ),( coocffullf CCsim  
is the metric which evaluates the similarity be-
tween the whole sense pool of ?  and the sense 
pool when ?  co-occurs with ? ; 
),( coocefulle CCsim  is the analogous similarity me-
tric for ? . They range from 0 to 1. These two 
metrics both evaluate the similarity for two vec-
tors in the same language, so using cosine dis-
tance to compute the similarity is straightfor-
ward. And we can set a relatively large size for 
the vector, since it is not necessary to do vector 
mapping as the vectors are in the same language. 
),( coocecoocf CCsim  computes the similarity between 
the context vectors when ?
 
and ? co-occur. We 
may compute ),( coocecoocf CCsim by using IBM 
model 1 probability and cosine distance similari-
ty functions as Equation (6) and (12). Therefore, 
on top of the degree of bilingual semantic simi-
larity between a source and a target translation 
unit, we have also incorporated the monolingual 
semantic similarity between all occurrences of a 
source or target unit, and that unit?s occurrence 
as part of the given rule, into the sense similarity 
measure. 
5 Experiments 
We evaluate the algorithm of bilingual sense si-
milarity via machine translation. The sense simi-
larity scores are used as feature functions in the 
translation model. 
5.1 Data 
We evaluated with different language pairs: Chi-
nese-to-English, and German-to-English. For 
Chinese-to-English tasks, we carried out the ex-
periments in two data conditions. The first one is 
the large data condition, based on training data 
for the NIST 2  2009 evaluation Chinese-to-
English track. In particular, all the allowed bilin-
gual corpora except the UN corpus and Hong 
Kong Hansard corpus have been used for esti-
mating the translation model. The second one is 
the small data condition where only the FBIS3 
corpus is used to train the translation model. We 
trained two language models: the first one is a 4-
gram LM which is estimated on the target side of 
the texts used in the large data condition. The 
second LM is a 5-gram LM trained on the so-
                                               
2
 http://www.nist.gov/speech/tests/mt 
3
 LDC2003E14 
called English Gigaword corpus. Both language 
models are used for both tasks. 
We carried out experiments for translating 
Chinese to English. We use the same develop-
ment and test sets for the two data conditions. 
We first created a development set which used 
mainly data from the NIST 2005 test set, and 
also some balanced-genre web-text from the 
NIST training material. Evaluation was per-
formed on the NIST 2006 and 2008 test sets. Ta-
ble 1 gives figures for training, development and 
test corpora; |S| is the number of the sentences, 
and |W| is the number of running words. Four 
references are provided for all dev and test sets. 
 
   Chi Eng 
 
Parallel 
Train 
Large 
Data 
|S| 3,322K 
|W| 64.2M 62.6M 
Small 
Data 
|S| 245K 
|W| 9.0M 10.5M 
   Dev |S| 1,506 1,506?4 
Test NIST06 |S| 1,664 1,664?4 
NIST08 |S| 1,357 1,357?4 
Gigaword |S| - 11.7M 
 
Table 1: Statistics of training, dev, and test sets for 
Chinese-to-English task. 
 
For German-to-English tasks, we used WMT 
20064 data sets. The parallel training data con-
tains 21 million target words; both the dev set 
and test set contain 2000 sentences; one refer-
ence is provided for each source input sentence. 
Only the target-language half of the parallel 
training data are used to train the language model 
in this task.  
5.2 Results 
For the baseline, we train the translation model 
by following (Chiang, 2005; Chiang, 2007) and 
our decoder is Joshua5, an open-source hierar-
chical phrase-based machine translation system 
written in Java. Our evaluation metric is IBM 
BLEU (Papineni et al, 2002), which performs 
case-insensitive matching of n-grams up to n = 4. 
Following (Koehn, 2004), we use the bootstrap-
resampling test to do significance testing. 
By observing the results on dev set in the addi-
tional experiments, we first set the smoothing 
constant k in Equation (5) to 0.5. 
Then, we need to set the sizes of the vectors to 
balance the computing time and translation accu-
                                               
4
 http://www.statmt.org/wmt06/ 
5
 http://www.cs.jhu.edu/~ccb/joshua/index.html 
838
racy, i.e., we keep only the top N context words 
with the highest feature value for each side of a 
rule 6 . In the following, we use ?Alg1? to 
represent the original similarity functions which 
compare the two context vectors built on full 
training data, as in Equation (13); while we use 
?Alg2? to represent the improved similarity as in 
Equation (14). ?IBM? represents IBM model 1 
probabilities, and ?COS? represents cosine dis-
tance similarity function. 
After carrying out a series of additional expe-
riments on the small data condition and observ-
ing the results on the dev set, we set the size of 
the vector to 500 for Alg1; while for Alg2, we 
set the sizes of fullfC  and fulleC N1 to 1000, and the 
sizes of coocfC  and cooceC N2 to 100.  
The sizes of the vectors in Alg2 are set in the 
following process: first, we set N2 to 500 and let 
N1  range from 500 to 3,000, we observed that the 
dev set got best performance when N1 was 1000; 
then we set N1 to 1000 and let N1 range from 50 
to 1000, we got best performance when N1 =100. 
We use this setting as the default setting in all 
remaining experiments. 
 
Algorithm NIST?06 NIST?08 
Baseline 27.4 21.2 
Alg1 IBM 27.8* 21.5 
Alg1 COS 27.8* 21.5 
Alg2 IBM 27.9* 21.6* 
Alg2 COS 28.1** 21.7* 
 
Table 2: Results (BLEU%) of small data Chinese-to-
English NIST task. Alg1 represents the original simi-
larity functions as in Equation (13); while Alg2 
represents the improved similarity as in Equation 
(14). IBM represents IBM model 1 probability, and 
COS represents cosine distance similarity function. * 
or ** means result is significantly better than the 
baseline (p < 0.05 or p < 0.01, respectively). 
 
 Ch-En De-En 
Algorithm NIST?06 NIST?08 Test?06 
Baseline 31.0 23.8 26.9 
Alg2 IBM 31.5* 24.5** 27.2* 
Alg2 COS 31.6** 24.5** 27.3* 
 
Table 3: Results (BLEU%) of large data Chinese-to-
English NIST task and German-to-English WMT 
task. 
                                               
6
 We have also conducted additional experiments by remov-
ing the stop words from the context vectors; however, we 
did not observe any consistent improvement. So we filter 
the context vectors by only considering the feature values. 
Table 2 compares the performance of Alg1 
and Alg2 on the Chinese-to-English small data 
condition. Both Alg1 and Alg2 improved the 
performance over the baseline, and Alg2 ob-
tained slight and consistent improvements over 
Alg1. The improved similarity function Alg2 
makes it possible to incorporate monolingual 
semantic similarity on top of the bilingual se-
mantic similarity, thus it may improve the accu-
racy of the similarity estimate. Alg2 significantly 
improved the performance over the baseline. The 
Alg2 cosine similarity function got 0.7 BLEU-
score (p<0.01) improvement over the baseline 
for NIST 2006 test set, and a 0.5 BLEU-score 
(p<0.05) for NIST 2008 test set. 
Table 3 reports the performance of Alg2 on 
Chinese-to-English NIST large data condition 
and German-to-English WMT task. We can see 
that IBM model 1 and cosine distance similarity 
function both obtained significant improvement 
on all test sets of the two tasks. The two similari-
ty functions obtained comparable results. 
6 Analysis and Discussion 
6.1 Effect of Single Features 
In Alg2, the similarity score consists of three 
parts as in Equation (14): ),( coocffullf CCsim , 
),( coocefulle CCsim , and ),( coocecoocf CCsim ; where  
),( coocecoocf CCsim  could be computed by IBM mod-
el 1 probabilities ),( coocecoocfIBM CCsim  or cosine dis-
tance similarity function ),( coocecoocfCOS CCsim . 
Therefore, our first study is to determine which 
one of the above four features has the most im-
pact on the result. Table 4 shows the results ob-
tained by using each of the 4 features. First, we 
can see that ),( coocecoocfIBM CCsim  always gives a 
better improvement than ),( coocecoocfCOS CCsim . This 
is because  ),( coocecoocfIBM CCsim  scores are more 
diverse than the latter when the number of con-
text features is small (there are many rules that 
have only a few contexts.) For an extreme exam-
ple, suppose that there is only one context word 
in each vector of source and target context fea-
tures, and the translation probability of the two 
context words is not 0. In this case, 
),( coocecoocfIBM CCsim   reflects the translation proba-
bility of the context word pair, while 
),( coocecoocfCOS CCsim  is always 1.  
   Second, ),( coocffullf CCsim  and ),( coocefulle CCsim   
also give some improvements even when used 
839
independently. For a possible explanation, con-
sider the following example. The Chinese word 
?? ? can translate to ?red?, ?communist?, or 
?hong? (the transliteration of ?, when it is used 
in a person?s name).  Since these translations are 
likely to be associated with very different source 
contexts, each will have a low ),( coocffullf CCsim  
score.  Another Chinese word ?? may translate 
into synonymous words, such as ?brook?, 
?stream?, and ?rivulet?, each of which will have 
a high  ),( coocffullf CCsim  score. Clearly, ? is a 
more ?dangerous? word than??, since choos-
ing the wrong translation for it would be a bad 
mistake. But if the two words have similar trans-
lation distributions, the system cannot distinguish 
between them. The monolingual similarity scores 
give it the ability to avoid ?dangerous? words, 
and choose alternatives (such as larger phrase 
translations) when available. 
Third, the similarity function of Alg2 consis-
tently achieved further improvement by incorpo-
rating the monolingual similarities computed for 
the source and target side. This confirms the ef-
fectiveness of our algorithm. 
 
 CE_LD CE_SD 
testset (NIST) ?06 ?08 ?06 ?08 
Baseline 31.0 23.8 27.4 21.2 
),( coocffullf CCsim  31.1 24.3 27.5 21.3 
),( coocefulle CCsim  31.1 23.9 27.9 21.5 
),( coocecoocfIBM CCsim  31.4 24.3 27.9 21.5 
),( coocecoocfCOS CCsim  31.2 23.9 27.7 21.4 
Alg2 IBM 31.5 24.5 27.9 21.6 
Alg2 COS 31.6 24.5 28.1 21.7 
 
Table 4: Results (BLEU%) of Chinese-to-English 
large data (CE_LD) and small data (CE_SD) NIST 
task by applying one feature. 
6.2 Effect of Combining the Two Similari-
ties 
We then combine the two similarity scores by 
using both of them as features to see if we could 
obtain further improvement. In practice, we use 
the four features in Table 4 together.  
Table 5 reports the results on the small data 
condition. We observed further improvement on 
dev set, but failed to get the same improvements 
on test sets or even lost performance. Since the 
IBM+COS configuration has one extra feature, it 
is possible that it overfits the dev set. 
 
Algorithm Dev NIST?06 NIST?08 
Baseline 20.2 27.4 21.2 
Alg2 IBM 20.5 27.9 21.6 
Alg2 COS 20.6 28.1 21.7 
Alg2 IBM+COS 20.8 27.9 21.5 
 
Table 5: Results (BLEU%) for combination of two 
similarity scores. Further improvement was only ob-
tained on dev set but not on test sets. 
6.3 Comparison with Simple Contextual 
Features 
Now, we try to answer the question: can the si-
milarity features computed by the function in 
Equation (14) be replaced with some other sim-
ple features? We did additional experiments on 
small data Chinese-to-English task to test the 
following features: (15) and (16) represent the 
sum of the counts of the context words in Cfull, 
while (17) represents the proportion of words in 
the context of ?  that appeared in the context of 
the rule ( ?? , ); similarly, (18) is related to the 
properties of the words in the context of ? . 
? ?= fullfi Cf if fFN ),()( ??              (15) 
? ?= fullej Ce je eFN ),()( ??                (16) 
)(
),(
),(
?
?
??
f
Cf i
f N
fF
E
cooc
fi? ?
=           (17) 
)(
),(
),( ?
?
??
e
Ce j
e N
eF
E
cooc
ej? ?
=           (18)   
where ),( ifF ?  and ),( jeF ?  are the frequency 
counts of rule ?  or ?   co-occurring with the 
context word if  or je   respectively. 
 
Feature Dev NIST?06 NIST?08 
Baseline 20.2 27.4 21.2 
+Nf 20.5 27.6 21.4 
+Ne 20.5 27.5 21.3 
+Ef 20.4 27.5 21.2 
+Ee 20.4 27.3 21.2 
+Nf+Ne 20.5 27.5 21.3 
 
Table 6: Results (BLEU%) of using simple features 
based on context on small data NIST task. Some im-
provements are obtained on dev set, but there was no 
significant effect on the test sets. 
 
Table 6 shows results obtained by adding the 
above features to the system for the small data 
840
condition. Although all these features have ob-
tained some improvements on dev set, there was 
no significant effect on the test sets. This means 
simple features based on context, such as the 
sum of the counts of the context features, are not 
as helpful as the sense similarity computed by 
Equation (14). 
6.4 Null Context Feature 
There are two cases where no context word can 
be extracted according to the definition of con-
text in Section 3.1. The first case is when a rule 
pair is always a full sentence-pair in the training 
data. The second case is when for some rule 
pairs, either their source or target contexts are 
out of the span limit of the initial phrase, so that 
we cannot extract contexts for those rule-pairs. 
For Chinese-to-English NIST task, there are 
about 1% of the rules that do not have contexts; 
for German-to-English task, this number is about 
0.4%. We assign a uniform number as their bi-
lingual sense similarity score, and this number is 
tuned through MERT. We call it the null context 
feature. It is included in all the results reported 
from Table 2 to Table 6. In Table 7, we show the 
weight of the null context feature tuned by run-
ning MERT in the experiments reported in Sec-
tion 5.2. We can learn that penalties always dis-
courage using those rules which have no context 
to be extracted.  
 
 
Alg. 
Task 
CE_SD CE_LD DE 
Alg2 IBM -0.09 -0.37 -0.15 
Alg2 COS -0.59 -0.42 -0.36 
 
Table 7: Weight learned for employing the null con-
text feature. CE_SD, CE_LD and DE are Chinese-to-
English small data task, large data task and German-
to-English task respectively. 
6.5 Discussion 
Our aim in this paper is to characterize the se-
mantic similarity of bilingual hierarchical rules. 
We can make several observations concerning 
our features: 
1) Rules that are largely syntactic in nature, 
such as ? X ||| the X of, will have very diffuse 
?meanings? and therefore lower similarity 
scores. It could be that the gains we obtained 
come simply from biasing the system against 
such rules. However, the results in table 6 show 
that this is unlikely to be the case: features that 
just count context words help very little. 
2) In addition to bilingual similarity, Alg2 re-
lies on the degree of monolingual similarity be-
tween the sense of a source or target unit within a 
rule, and the sense of the unit in general. This has 
a bias in favor of less ambiguous rules, i.e. rules 
involving only units with closely related mean-
ings. Although this bias is helpful on its own, 
possibly due to the mechanism we outline in sec-
tion 6.1, it appears to have a synergistic effect 
when used along with the bilingual similarity 
feature. 
3) Finally, we note that many of the features 
we use for capturing similarity, such as the con-
text ?the, of? for instantiations of X in the unit 
the X of, are arguably more syntactic than seman-
tic. Thus, like other ?semantic? approaches, ours 
can be seen as blending syntactic and semantic 
information. 
7 Related Work 
There has been extensive work on incorporating 
semantics into SMT. Key papers by Carpuat and 
Wu (2007) and Chan et al(2007) showed that 
word-sense disambiguation (WSD) techniques 
relying on source-language context can be effec-
tive in selecting translations in phrase-based and 
hierarchical SMT. More recent work has aimed 
at incorporating richer disambiguating features 
into the SMT log-linear model (Gimpel and 
Smith, 2008; Chiang et al 2009); predicting co-
herent sets of target words rather than individual 
phrase translations (Bangalore et al 2009; Maus-
er et al 2009); and selecting applicable rules in 
hierarchical (He et al 2008) and syntactic (Liu et 
al, 2008) translation, relying on source as well as 
target context. Work by Wu and Fung (2009) 
breaks new ground in attempting to match se-
mantic roles derived from a semantic parser 
across source and target languages. 
Our work is different from all the above ap-
proaches in that we attempt to discriminate 
among hierarchical rules based on: 1) the degree 
of bilingual semantic similarity between source 
and target translation units; and 2) the monolin-
gual semantic similarity between occurrences of 
source or target units as part of the given rule, 
and in general. In another words, WSD explicitly 
tries to choose a translation given the current 
source context, while our work rates rule pairs 
independent of the current context. 
8 Conclusions and Future Work 
In this paper, we have proposed an approach that 
uses the vector space model to compute the sense 
841
similarity for terms from parallel corpora and 
applied it to statistical machine translation. We 
saw that the bilingual sense similarity computed 
by our algorithm led to significant improve-
ments. Therefore, we can answer the questions 
proposed in Section 1. We have shown that the 
sense similarity computed between units from 
parallel corpora by means of our algorithm is 
helpful for at least one multilingual application: 
statistical machine translation. 
Finally, although we described and evaluated 
bilingual sense similarity algorithms applied to a 
hierarchical phrase-based system, this method is 
also suitable for syntax-based MT systems and 
phrase-based MT systems. The only difference is 
the definition of the context. For a syntax-based 
system, the context of a rule could be defined 
similarly to the way it was defined in the work 
described above. For a phrase-based system, the 
context of a phrase could be defined as its sur-
rounding words in a given size window. In our 
future work, we may try this algorithm on syn-
tax-based MT systems and phrase-based MT sys-
tems with different context features. It would 
also be possible to use this technique during 
training of an SMT system ? for instance, to im-
prove the bilingual word alignment or reduce the 
training data noise. 
References  
S. Bangalore, S. Kanthak, and P. Haffner. 2009. Sta-
tistical Machine Translation through Global Lexi-
cal Selection and Sentence Reconstruction. In: 
Goutte et al(ed.), Learning Machine Translation. 
MIT Press. 
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & 
R. L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2) 263-312. 
J. Bullinaria and J. Levy. 2007. Extracting semantic 
representations from word co-occurrence statistics: 
A computational study. Behavior Research Me-
thods, 39 (3), 510?526. 
M. Carpuat and D. Wu. 2007. Improving Statistical 
Machine Translation using Word Sense Disambig-
uation. In:  Proceedings of EMNLP, Prague. 
M. Carpuat. 2009. One Translation per Discourse. In:  
Proceedings of NAACL HLT Workshop on Se-
mantic Evaluations, Boulder, CO. 
Y. Chan, H. Ng and D. Chiang. 2007. Word Sense 
Disambiguation Improves Statistical Machine 
Translation. In:  Proceedings of ACL, Prague. 
D. Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In: Proceedings 
of ACL, pp. 263?270. 
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics. 33(2):201?228. 
D. Chiang, W. Wang and K. Knight. 2009. 11,001 
new features for statistical machine translation. In: 
Proc. NAACL HLT, pp. 218?226. 
K. W. Church and P. Hanks. 1990. Word association 
norms, mutual information, and lexicography. 
Computational Linguistics, 16(1):22?29. 
W. B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval, Data Structure and Algo-
rithms. Prentice Hall. 
P. Fung. 1998. A statistical view on bilingual lexicon 
extraction: From parallel corpora to non-parallel 
corpora. In: Proceedings of AMTA, pp. 1?17. Oct. 
Langhorne, PA, USA. 
J. Gimenez and L. Marquez. 2009. Discriminative 
Phrase Selection for SMT. In: Goutte et al(ed.), 
Learning Machine Translation. MIT Press. 
K. Gimpel and N. A. Smith. 2008. Rich Source-Side 
Context for Statistical Machine Translation. In: 
Proceedings of WMT, Columbus, OH. 
Z. Harris. 1954. Distributional structure. Word, 
10(23): 146-162. 
Z. He, Q. Liu, and S. Lin. 2008. Improving Statistical 
Machine Translation using Lexicalized Rule Selec-
tion. In: Proceedings of COLING, Manchester, 
UK. 
D. Hindle. 1990. Noun classification from predicate-
argument structures. In: Proceedings of ACL. pp. 
268-275. Pittsburgh, PA. 
P. Koehn, F. Och, D. Marcu. 2003. Statistical Phrase-
Based Translation. In: Proceedings of HLT-
NAACL. pp. 127-133, Edmonton, Canada 
P.  Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In:  Proceedings of 
EMNLP, pp. 388?395. July, Barcelona, Spain. 
T. Landauer and S. T. Dumais. 1997. A solution to 
Plato?s problem: The Latent Semantic Analysis 
theory of the acquisition, induction, and representa-
tion of knowledge. Psychological Review. 104:211-
240. 
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. 
Khudanpur, L. Schwartz, W. Thornton, J. Weese 
and O. Zaidan, 2009. Joshua: An Open Source 
Toolkit for Parsing-based Machine Translation. In: 
Proceedings of the WMT.  March. Athens, Greece. 
D. Lin. 1998. Automatic retrieval and clustering of 
similar words. In: Proceedings of COLING/ACL-
98. pp. 768-774. Montreal, Canada.  
842
Q. Liu, Z. He, Y. Liu and S. Lin. 2008. Maximum 
Entropy based Rule Selection Model for Syntax-
based Statistical Machine Translation. In: Proceed-
ings of EMNLP, Honolulu, Hawaii. 
K. Lund, and C. Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28 (2), 203?208. 
A. Mauser, S. Hasan and H. Ney. 2009. Extending 
Statistical Machine Translation with Discrimina-
tive and Trigger-Based Lexicon Models. In: Pro-
ceedings of EMNLP, Singapore. 
F. Och. 2003. Minimum error rate training in statistic-
al machine translation. In: Proceedings of ACL. 
Sapporo, Japan. 
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational 
Linguistics, 33 (2), 161?199. 
P. Pantel and D. Lin. 2002. Discovering word senses 
from text. In: Proceedings of ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining, 
pp. 613?619. Edmonton, Canada. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, pp. 311?
318. July. Philadelphia, PA, USA. 
R. Rapp. 1999. Automatic Identification of Word 
Translations from Unrelated English and German 
Corpora. In: Proceedings of ACL, pp. 519?526. 
June. Maryland. 
G. Salton and M. J. McGill. 1983. Introduction to 
Modern Information Retrieval. McGraw-Hill, New 
York. 
P. Turney. 2001. Mining the Web for synonyms: 
PMI-IR versus LSA on TOEFL. In: Proceedings of 
the Twelfth European Conference on Machine 
Learning, pp. 491?502, Berlin, Germany.  
D. Wu and P. Fung. 2009. Semantic Roles for SMT: 
A Hybrid Two-Pass Model. In: Proceedings of 
NAACL/HLT, Boulder, CO. 
D. Yuret and M. A. Yatbaz. 2009. The Noisy Channel 
Model for Unsupervised Word Sense Disambigua-
tion. In: Computational Linguistics. Vol. 1(1) 1-18. 
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In: Proceed-
ings of NAACL-HLT. Boston, MA. 
B. Zhao, S. Vogel, M. Eck, and A. Waibel. 2004. 
Phrase pair rescoring with term weighting for sta-
tistical machine translation. In Proceedings of 
EMNLP, pp. 206?213. July. Barcelona, Spain. 
 
843
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 930?939,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
PORT:  a Precision-Order-Recall MT Evaluation Metric for Tuning 
 
 
Boxing Chen, Roland Kuhn and Samuel Larkin 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
{Boxing.Chen, Roland.Kuhn, Samuel.Larkin}@nrc.ca 
 
  
Abstract 
Many machine translation (MT) evaluation 
metrics have been shown to correlate better 
with human judgment than BLEU. In 
principle, tuning on these metrics should 
yield better systems than tuning on BLEU. 
However, due to issues such as speed, 
requirements for linguistic resources, and 
optimization difficulty, they have not been 
widely adopted for tuning. This paper 
presents PORT 1 , a new MT  evaluation 
metric which combines precision, recall 
and an ordering metric and which is 
primarily designed for tuning MT systems. 
PORT does not require external resources 
and is quick to compute. It has a better 
correlation with human judgment than 
BLEU. We compare PORT-tuned MT 
systems to BLEU-tuned baselines in five 
experimental conditions involving four 
language pairs. PORT tuning achieves 
consistently better performance than BLEU 
tuning, according to four automated 
metrics (including BLEU) and to human 
evaluation: in comparisons of outputs from 
300 source sentences, human judges 
preferred the PORT-tuned output 45.3% of 
the time (vs. 32.7% BLEU tuning  
preferences and 22.0% ties).  
1 Introduction 
Automatic evaluation metrics for machine 
translation (MT) quality are a key part of building 
statistical MT (SMT) systems. They play two 
                                                           
1
 PORT: Precision-Order-Recall Tunable metric. 
roles: to allow rapid (though sometimes inaccurate) 
comparisons between different systems or between 
different versions of the same system, and to 
perform tuning of parameter values during system 
training. The latter has become important since the 
invention of minimum error rate training (MERT) 
(Och, 2003) and related tuning methods. These 
methods perform repeated decoding runs with 
different system parameter values, which are tuned 
to optimize the value of the evaluation metric over 
a development set with reference translations. 
MT evaluation metrics fall into three groups:  
? BLEU (Papineni et al, 2002), NIST 
(Doddington, 2002), WER, PER, TER 
(Snover et al, 2006), and LRscore (Birch and 
Osborne, 2011) do not use external linguistic 
information; they are fast to compute (except 
TER).  
? METEOR (Banerjee and Lavie, 2005), 
METEOR-NEXT (Denkowski and Lavie 
2010), TER-Plus (Snover et al, 2009), 
MaxSim (Chan and Ng, 2008), TESLA (Liu 
et al, 2010), AMBER (Chen and Kuhn, 2011) 
and MTeRater (Parton et al, 2011) exploit 
some limited linguistic resources, such as 
synonym dictionaries, part-of-speech tagging, 
paraphrasing tables or word root lists.  
? More sophisticated metrics such as RTE 
(Pado et al, 2009), DCU-LFG (He et al, 
2010) and MEANT (Lo and Wu, 2011) use 
higher level syntactic or semantic analysis to 
score translations. 
Among these metrics, BLEU is the most widely 
used for both evaluation and tuning. Many of the 
metrics correlate better with human judgments of 
translation quality than BLEU, as shown in recent 
WMT Evaluation Task reports (Callison-Burch et 
930
al., 2010; Callison-Burch et al, 2011). However, 
BLEU remains the de facto standard tuning metric, 
for two reasons. First, there is no evidence that any 
other tuning metric yields better MT systems. Cer 
et al (2010) showed that BLEU tuning is more 
robust than tuning with other metrics (METEOR, 
TER, etc.), as gauged by both automatic and 
human evaluation. Second, though a tuning metric 
should correlate strongly with human judgment, 
MERT (and similar algorithms) invoke the chosen 
metric so often that it must be computed quickly.  
Liu et al (2011) claimed that TESLA tuning 
performed better than BLEU tuning according to 
human judgment. However, in the WMT 2011 
?tunable metrics? shared pilot task, this did not 
hold (Callison-Burch et al, 2011). In (Birch and 
Osborne, 2011), humans preferred the output from 
LRscore-tuned systems 52.5% of the time, versus 
BLEU-tuned system outputs 43.9% of the time. 
In this work, our goal is to devise a metric that, 
like BLEU, is computationally cheap and 
language-independent, but that yields better MT 
systems than BLEU when used for tuning. We 
tried out different combinations of statistics before 
settling on the final definition of our metric.  The 
final version, PORT, combines precision, recall, 
strict brevity penalty (Chiang et al, 2008) and 
strict redundancy penalty (Chen and Kuhn, 2011) 
in a quadratic mean expression. This expression is 
then further combined with a new measure of word 
ordering, v, designed to reflect long-distance as 
well as short-distance word reordering (BLEU only 
reflects short-distance reordering). In a later 
section, 3.3, we describe experiments that vary 
parts of the definition of PORT.  
Results given below show that PORT correlates 
better with human judgments of translation quality 
than BLEU does, and sometimes outperforms 
METEOR in this respect, based on data from 
WMT (2008-2010). However, since PORT is 
designed for tuning, the most important results are 
those showing that PORT tuning yields systems 
with better translations than those produced by 
BLEU tuning ? both as determined by automatic 
metrics (including BLEU), and according to 
human judgment, as applied to five data conditions 
involving four language pairs. 
2 BLEU and PORT 
First, define n-gram precision p(n) and recall r(n): 
)(grams-n#
)(grams-n#)(
T
RT
np ?=                 (1) 
)(grams-n#
)(grams-n#)(
R
RT
nr
?
=              (2) 
where T = translation, R = reference. Both BLEU 
and PORT are defined on the document-level, i.e. 
T and R are whole texts. If there are multiple 
references, we use closest reference length for each 
translation hypothesis to compute the numbers of 
the reference n-grams. 
2.1 BLEU 
BLEU is composed of precision Pg(N) and brevity 
penalty BP: 
BPNPBLEU g ?= )(                 (3)  
where Pg(N) is the geometric average of n-gram 
precisions 
NN
n
g npNP
1
1
)()( ???
?
???
?
= ?
=
               (4) 
The BLEU brevity penalty punishes the score if 
the translation length len(T) is shorter than the 
reference length len(R); it is: ( ))(/)(1,0.1min TlenRleneBP ?=         (5) 
2.2 PORT 
PORT has five components: precision, recall, strict 
brevity penalty (Chiang et al, 2008), strict 
redundancy penalty (Chen and Kuhn, 2011) and an 
ordering measure v. The design of PORT is based 
on exhaustive experiments on a development data 
set. We do not have room here to give a rationale 
for all the choices we made when we designed 
PORT. However, a later section (3.3) reconsiders 
some of these design decisions.  
2.2.1 Precision and Recall 
The average precision and average recall used in 
PORT (unlike those used in BLEU) are the 
arithmetic average of n-gram precisions Pa(N) and 
recalls Ra(N): 
?
=
=
N
n
a npN
NP
1
)(1)(                 (6) 
?
=
=
N
n
a nrN
NR
1
)(1)(                   (7) 
931
We use two penalties to avoid too long or too 
short MT outputs. The first, the strict brevity 
penalty (SBP), is proposed in (Chiang et al, 2008). 
Let ti be the translation of input sentence i, and let 
ri be its reference. Set 
???
?
???
?
?= ?
?
i ii
i i
rt
r
SBP |}||,min{|
||
1exp         (8) 
The second is the strict redundancy penalty (SRP), 
proposed in (Chen and Kuhn, 2011): 
???
?
???
?
?= ?
?
i i
i ii
r
rt
SRP ||
|}||,max{|
1exp         (9) 
To combine precision and recall, we tried four 
averaging methods: arithmetic (A), geometric (G), 
harmonic (H), and quadratic (Q) mean. If all of the 
values to be averaged are positive, the order is 
maxQAGHmin ????? , with equality 
holding if and only if all the values being averaged 
are equal. We chose the quadratic mean to 
combine precision and recall, as follows: 
2
))(())(()(
22 SRPNRSBPNPNQmean aa ?+?=   (10)  
2.2.2 Ordering Measure 
Word ordering measures for MT compare two 
permutations of the original source-language word 
sequence: the permutation represented by the 
sequence of corresponding words in the MT 
output, and the permutation in the reference. 
Several ordering measures have been integrated 
into MT evaluation metrics recently. Birch and 
Osborne (2011) use either Hamming Distance or 
Kendall?s ? Distance (Kendall, 1938) in their 
metric LRscore, thus obtaining two versions of 
LRscore. Similarly, Isozaki et al (2011) adopt 
either Kendall?s ? Distance or Spearman?s ? 
(Spearman, 1904) distance in their metrics.  
Our measure, v, is different from all of these. 
We use word alignment to compute the two 
permutations (LRscore also uses word alignment). 
The word alignment between the source input and 
reference is computed using GIZA++ (Och and 
Ney, 2003) beforehand with the default settings, 
then is refined with the heuristic grow-diag-final-
and; the word alignment between the source input 
and the translation is generated by the decoder with 
the help of word alignment inside each phrase pair. 
PORT uses permutations. These encode one-to-
one relations but not one-to-many, many-to-one, 
many-to-many or null relations, all of which can 
occur in word alignments. We constrain the 
forbidden types of relation to become one-to-one, 
as in (Birch and Osborne, 2011). Thus, in a one-to-
many alignment, the single source word is forced 
to align with the first target word; in a many-to-one 
alignment, monotone order is assumed for the 
target words; and source words originally aligned 
to null are aligned to the target word position just 
after the previous source word?s target position.  
After the normalization above, suppose we have 
two permutations for the same source n-word 
input. E.g., let P1 = reference, P2 = hypothesis: 
P1: 11p  
2
1p  
3
1p  
4
1p  ? 
ip1  ? 
np1  
 P2: 12p  
2
2p  
3
2p  
4
2p  ? 
ip2  ? 
np2
 
Here, each jip is an integer denoting position in the 
original source (e.g., 11p = 7 means that the first 
word in P1 is the 7th source word). 
The ordering metric v is computed from two 
distance measures. The first is absolute 
permutation distance: 
?
=
?=
n
i
ii ppPPDIST
1
21211 ||),(               (11) 
Let       
2/)1(
),(1 2111 +?= nn
PPDIST
?                     (12)                  
v1 ranges from 0 to 1; a larger value means more 
similarity between the two permutations. This 
metric is similar to Spearman?s ? (Spearman, 
1904). However, we have found that ? punishes 
long-distance reorderings too heavily. For instance, 
1? is more tolerant than ? of the movement of 
?recently? in this example:  
Ref: Recently, I visited Paris 
Hyp: I visited Paris recently  
Inspired by HMM word alignment (Vogel et al, 
1996), our second distance measure is based on 
jump width. This punishes a sequence of words 
that moves a long distance with its internal order 
conserved, only once rather than on every word. In 
the following, only two groups of words have 
moved, so the jump width punishment is light: 
Ref: In the winter of 2010, I visited Paris 
Hyp: I visited Paris in the winter of 2010  
So the second distance measure is 
932
?
=
??
???=
n
i
iiii ppppPPDIST
1
1
22
1
11212 |)()(|),(   (13) 
where we set 001 =p  and 0
0
2 =p . Let 
1
),(1 2 2122
?
?=
n
PPDIST
v                     (14) 
As with v1, v2 is also from 0 to 1, and larger values 
indicate more similar permutations. The ordering 
measure vs is the harmonic mean of v1 and v2:  
)/1/1/(2 21 vvvs +=
 
.                     (15) 
 vs in (15) is computed at segment level. For 
multiple references, we compute vs for each, and 
then choose the biggest one as the segment level 
ordering similarity. We compute document level 
ordering with a weighted arithmetic mean:  
?
?
=
=
?
= l
s s
l
s ss
Rlen
Rlenv
v
1
1
)(
)(
                    (16) 
where l is the number of segments of the 
document, and len(R) is the length of the reference. 
2.2.3 Combined Metric 
Finally, Qmean(N) (Eq. (10) and the word ordering 
measure v are combined in a harmonic mean: 
?vNQmeanPORT /1)(/1
2
+
=           (17) 
Here ?  is a free parameter that is tuned on held-
out data. As it increases, the importance of the 
ordering measure v goes up. For our experiments, 
we tuned ?  on Chinese-English data, setting it to 
0.25 and keeping this value for the other language 
pairs. The use of v means that unlike BLEU, PORT 
requires word alignment information. 
 
3 Experiments 
3.1 PORT as an Evaluation Metric 
We studied PORT as an evaluation metric on 
WMT data; test sets include WMT 2008, WMT 
2009, and WMT 2010 all-to-English, plus 2009, 
2010 English-to-all submissions. The languages 
?all? (?xx? in Table 1) include French, Spanish, 
German and Czech. Table 1 summarizes the test 
set statistics. In order to compute the v part of 
PORT, we require source-target word alignments 
for the references and MT outputs. These aren?t 
included in WMT data, so we compute them with 
GIZA++. 
We used Spearman?s rank correlation coefficient 
? to measure correlation of the metric with system-
level human judgments of translation. The human 
judgment score is based on the ?Rank? only, i.e., 
how often the translations of the system were rated 
as better than those from other systems (Callison-
Burch et al, 2008). Thus, BLEU, METEOR, and 
PORT were evaluated on how well their rankings 
correlated with the human ones. For the segment 
level, we follow (Callison-Burch et al, 2010) in 
using Kendall?s rank correlation coefficient ?.  
As shown in Table 2, we compared PORT with 
smoothed BLEU (mteval-v13a), and METEOR 
v1.0. Both BLEU and PORT perform matching of 
n-grams up to n = 4. 
 
Set Year Lang. #system #sent-pair 
Test1 2008 xx-en 43 7,804 
Test2 2009 xx-en 45 15,087 
Test3 2009 en-xx 40 14,563 
Test4 2010 xx-en 53 15,964 
Test5 2010 en-xx 32 18,508 
Table 1: Statistics of the WMT dev and test sets. 
 
 
 
Metric 
Into-En Out-of-En 
sys.  seg. sys.  seg. 
BLEU 0.792 0.215 0.777 0.240 
METEOR 0.834 0.231 0.835 0.225 
PORT 0.801 0.236 0.804 0.242 
Table 2: Correlations with human judgment on WMT 
 
PORT achieved the best segment level 
correlation with human judgment on both the ?into 
English? and ?out of English? tasks. At the system 
level, PORT is better than BLEU, but not as good 
as METEOR.  This is because we designed PORT 
to carry out tuning; we did not optimize its 
performance as an evaluation metric, but rather, to 
optimize system tuning performance. There are 
some other possible reasons why PORT did not 
outperform METEOR v1.0 at system level. Most 
WMT submissions involve language pairs with 
similar word order, so the ordering factor v in 
PORT won?t play a big role. Also, v depends on 
source-target word alignments for reference and 
test sets. These alignments were performed by 
GIZA++ models trained on the test data only.  
933
3.2 PORT as a Metric for Tuning 
3.2.1 Experimental details 
The first set of experiments to study PORT as a 
tuning metric involved Chinese-to-English (zh-en); 
there were two data conditions. The first is the 
small data condition where FBIS2 is used to train 
the translation and reordering models. It contains 
10.5M target word tokens. We trained two 
language models (LMs), which were combined 
loglinearly. The first is a 4-gram LM which is 
estimated on the target side of the texts used in the 
large data condition (below). The second is a 5-
gram LM estimated on English Gigaword.  
The large data condition uses training data from 
NIST3 2009 (Chinese-English track). All allowed 
bilingual corpora except UN, Hong Kong Laws and 
Hong Kong Hansard were used to train the 
translation model and reordering models. There are 
about 62.6M target word tokens. The same two 
LMs are used for large data as for small data, and 
the same development (?dev?) and test sets are also 
used. The dev set comprised mainly data from the 
NIST 2005 test set, and also some balanced-genre 
web-text from NIST. Evaluation was performed on 
NIST 2006 and 2008. Four references were 
provided for all dev and test sets. 
The third data condition is a French-to-English 
(fr-en). The parallel training data is from Canadian 
Hansard data, containing 59.3M word tokens. We 
used two LMs in loglinear combination: a 4-gram 
LM trained on the target side of the parallel 
training data, and the English Gigaword 5-gram 
LM. The dev set has 1992 sentences; the two test 
sets have 2140 and 2164 sentences respectively. 
There is one reference for all dev and test sets.  
The fourth and fifth conditions involve German-
-English Europarl data. This parallel corpus 
contains 48.5M German tokens and 50.8M English 
tokens. We translate both German-to-English (de-
en) and English-to-German (en-de). The two 
conditions both use an LM trained on the target 
side of the parallel training data, and de-en also 
uses the English Gigaword 5-gram LM. News test 
2008 set is used as dev set; News test 2009, 2010, 
2011 are used as test sets. One reference is 
provided for all dev and test sets. 
                                                           
2
 LDC2003E14 
3
 http://www.nist.gov/speech/tests/mt 
All experiments were carried out with ?  in Eq. 
(17) set to 0.25, and involved only lowercase 
European-language text. They were performed 
with MOSES (Koehn et al, 2007), whose decoder 
includes lexicalized reordering, translation models, 
language models, and word and phrase penalties.  
Tuning was done with n-best MERT, which is 
available in MOSES. In all tuning experiments, 
both BLEU and PORT performed lower case 
matching of n-grams up to n = 4. We also 
conducted experiments with tuning on a version of 
BLEU that incorporates SBP (Chiang et al, 2008) 
as a baseline. The results of original IBM BLEU 
and BLEU with SBP were tied; to save space, we 
only report results for original IBM BLEU here. 
3.2.2 Comparisons with automatic metrics 
First, let us see if BLEU-tuning and PORT-tuning 
yield systems with different translations for the 
same input. The first row of Table 3 shows the 
percentage of identical sentence outputs for the 
two tuning types on test data. The second row 
shows the similarity of the two outputs at word-
level (as measured by 1-TER): e.g., for the two zh-
en tasks, the two tuning types give systems whose 
outputs are about 25-30% different at the word 
level. By contrast, only about 10% of output words 
for fr-en differ for BLEU vs. PORT tuning.  
 
 zh-en 
small 
zh-en 
large 
fr-en 
Hans 
de-en 
WMT 
en-de 
WMT 
Same sent.  17.7% 13.5% 56.6% 23.7% 26.1% 
1-TER 74.2 70.9 91.6 87.1 86.6 
Table 3: Similarity of BLEU-tuned and PORT-tuned 
system outputs on test data. 
 
 
Task 
 
Tune 
Evaluation metrics (%) 
BLEU MTR 1-TER PORT 
zh-en 
small 
BLEU 
PORT 
26.8  
27.2* 
55.2 
55.7 
38.0 
38.0 
49.7 
50.0 
zh-en 
large 
BLEU 
PORT 
29.9  
30.3*  
58.4 
59.0 
41.2 
42.0 
53.0 
53.2 
fr-en 
Hans 
BLEU 
PORT 
38.8  
38.8  
69.8 
69.6 
54.2 
54.6 
57.1 
57.1 
de-en 
WMT 
BLEU 
PORT 
20.1  
20.3 
55.6 
56.0 
38.4 
38.4 
39.6 
39.7 
en-de 
WMT 
BLEU 
PORT 
13.6 
13.6 
43.3 
43.3 
30.1 
30.7 
31.7 
31.7 
Table 4: Automatic evaluation scores on test data. 
 * indicates the results are significantly better than the 
baseline (p<0.05). 
 
934
Table 4 shows translation quality for BLEU- and 
PORT-tuned systems, as assessed by automatic 
metrics. We employed BLEU4, METEOR (v1.0), 
TER (v0.7.25), and the new metric PORT. In the 
table, TER scores are presented as 1-TER to ensure 
that for all metrics, higher scores mean higher 
quality. All scores are averages over the relevant 
test sets. There are twenty comparisons in the 
table. Among these, there is one case (French-
English assessed with METEOR) where BLEU 
outperforms PORT, there are seven ties, and there 
are twelve cases where PORT is better. Table 3 
shows that fr-en outputs are very similar for both 
tuning types, so the fr-en results are perhaps less 
informative than the others. Overall, PORT tuning 
has a striking advantage over BLEU tuning.  
Both (Liu et al, 2011) and (Cer et al, 2011) 
showed that with MERT, if you want the best 
possible score for a system?s translations according 
to metric M, then you should tune with M. This 
doesn?t appear to be true when PORT and BLEU 
tuning are compared in Table 4. For the two 
Chinese-to-English tasks in the table, PORT tuning 
yields a better BLEU score than BLEU tuning, 
with significance at p < 0.05. We are currently 
investigating why PORT tuning gives higher 
BLEU scores than BLEU tuning for Chinese-
English and German-English. In internal tests we 
have found no systematic difference in dev-set 
BLEUs, so we speculate that PORT?s emphasis on 
reordering yields models that generalize better for 
these two language pairs. 
3.2.3 Human Evaluation 
We conducted a human evaluation on outputs from 
BLEU- and PORT-tuned systems. The examples 
are randomly picked from all ?to-English? 
conditions shown in Tables 3 & 4 (i.e., all 
conditions except English-to-German).  
We performed pairwise comparison of the 
translations produced by the system types as in 
(Callison-Burch et al, 2010; Callison-Burch et al, 
2011). First, we eliminated examples where the 
reference had fewer than 10 words or more than 50 
words, or where outputs of the BLEU-tuned and 
PORT-tuned systems were identical. The 
evaluators (colleagues not involved with this 
paper) objected to comparing two bad translations, 
so we then selected for human evaluation only 
translations that had high sentence-level (1-TER) 
scores. To be fair to both metrics, for each 
condition, we took the union of examples whose 
BLEU-tuned output was in the top n% of BLEU 
outputs and those whose PORT-tuned output was 
in the top n% of PORT outputs (based on (1-
TER)). The value of n varied by condition: we 
chose the top 20% of zh-en small, top 20% of en-
de, top 50% of fr-en and top 40% of zh-en large. 
We then randomly picked 450 of these examples to 
form the manual evaluation set. This set was split 
into 15 subsets, each containing 30 sentences. The 
first subset was used as a common set; each of the 
other 14 subsets was put in a separate file, to which 
the common set is added.  Each of the 14 
evaluators received one of these files, containing 
60 examples (30 unique examples and 30 examples 
shared with the other evaluators). Within each 
example, BLEU-tuned and PORT-tuned outputs 
were presented in random order. 
After receiving the 14 annotated files, we 
computed Fleiss?s Kappa (Fleiss, 1971) on the 
common set to measure inter-annotator agreement, 
all? . Then, we excluded annotators one at a time 
to compute i? (Kappa score without i-th annotator, 
i.e., from the other 13). Finally, we filtered out the 
files from the 4 annotators whose answers were 
most different from everybody else?s: i.e., 
annotators with the biggest iall ?? ?  values. 
This left 10 files from 10 evaluators. We threw 
away the common set in each file, leaving 300 
pairwise comparisons. Table 5 shows that the 
evaluators preferred the output from the PORT-
tuned system 136 times, the output from the 
BLEU-tuned one 98 times, and had no preference 
the other 66 times. This indicates that there is a 
human preference for outputs from the PORT-
tuned system over those from the BLEU-tuned 
system at the p<0.01 significance level (in cases 
where people prefer one of them). 
PORT tuning seems to have a bigger advantage 
over BLEU tuning when the translation task is 
hard. Of the Table 5 language pairs, the one where 
PORT tuning helps most has the lowest BLEU in 
Table 4 (German-English); the one where it helps 
least in Table 5 has the highest BLEU in Table 4 
(French-English). (Table 5 does not prove BLEU is 
superior to PORT for French-English tuning: 
statistically, the difference between 14 and 17 here 
is a tie). Maybe by picking examples for each 
condition that were the easiest for the system to 
translate (to make human evaluation easier), we 
935
mildly biased the results in Table 5 against PORT 
tuning. Another possible factor is reordering. 
PORT differs from BLEU partly in modeling long-
distance reordering more accurately; English and 
French have similar word order, but the other two 
language pairs don?t. The results in section 3.3 
(below) for Qmean, a version of PORT without 
word ordering factor v, suggest v may be defined 
suboptimally for French-English.  
 
 PORT win BLEU win equal total 
zh-en 
small 
19 
38.8% 
18 
36.7% 
12 
24.5% 
49 
zh-en 
large 
69 
45.7% 
46 
30.5% 
36 
23.8% 
151 
fr-en 
Hans 
14 
32.6% 
17 
39.5% 
12 
27.9% 
43 
de-en 
WMT 
34 
59.7% 
17 
29.8% 
6 
10.5% 
57 
All 136 
45.3% 
98 
32.7% 
66 
22.0% 
300 
Table 5: Human preference for outputs from PORT-
tuned vs. BLEU-tuned system. 
3.2.4 Computation time  
A good tuning metric should run very fast; this is 
one of the advantages of BLEU. Table 6 shows the 
time required to score the 100-best hypotheses for 
the dev set for each data condition during MERT 
for BLEU and PORT in similar implementations. 
The average time of each iteration, including 
model loading, decoding, scoring and running 
MERT4, is in brackets. PORT takes roughly 1.5 ? 
2.5 as long to compute as BLEU, which is 
reasonable for a tuning metric.  
 
 zh-en 
small 
zh-en 
large 
fr-en 
Hans 
de-en 
WMT 
en-de 
WMT 
BLEU 3 (13)  3 (17) 2 (19) 2 (20) 2 (11) 
PORT 5 (21) 5 (24) 4 (28) 5 (28) 4 (15) 
Table 6: Time to score 100-best hypotheses (average 
time per iteration) in minutes.  
3.2.5 Robustness to word alignment errors 
PORT, unlike BLEU, depends on word 
alignments. How does quality of word alignment 
between source and reference affect PORT tuning? 
We created a dev set from Chinese Tree Bank 
                                                           
4
 Our experiments are run on a cluster. The average time for 
an iteration includes queuing, and the speed of each node is 
slightly different, so bracketed times are only for reference. 
(CTB) hand-aligned data. It contains 588 sentences 
(13K target words), with one reference. We also 
ran GIZA++ to obtain its automatic word 
alignment, computed on CTB and FBIS.  The AER 
of the GIZA++ word alignment on CTB is 0.32.  
In Table 7, CTB is the dev set. The table shows 
tuning with BLEU, PORT with human word 
alignment (PORT + HWA), and PORT with 
GIZA++ word alignment (PORT + GWA); the 
condition is zh-en small. Despite the AER of 0.32 
for automatic word alignment, PORT tuning works 
about as well with this alignment as for the gold 
standard CTB one. (The BLEU baseline in Table 7 
differs from the Table 4 BLEU baseline because 
the dev sets differ).  
 
Tune BLEU MTR 1-TER PORT 
BLEU 25.1 53.7 36.4 47.8 
PORT + HWA 25.3 54.4 37.0 48.2 
PORT + GWA 25.3 54.6 36.4 48.1 
Table 7: PORT tuning - human & GIZA++ alignment 
 
Task Tune BLEU MTR 1-TER PORT 
zh-en 
small 
BLEU 
PORT 
Qmean 
26.8 
27.2 
26.8 
55.2 
55.7 
55.3 
38.0 
38.0 
38.2 
49.7 
50.0 
49.8 
zh-en 
large 
BLEU 
PORT 
Qmean 
29.9 
30.3 
30.2 
58.4 
59.0 
58.5 
41.2 
42.0 
41.8 
53.0 
53.2 
53.1 
fr-en 
Hans 
BLEU 
PORT 
Qmean 
38.8 
38.8 
38.8 
69.8 
69.6 
69.8 
54.2 
54.6 
54.6 
57.1 
57.1 
57.1 
de-en 
WMT 
BLEU 
PORT 
Qmean 
20.1 
20.3 
20.3 
55.6 
56.0 
56.3 
38.4 
38.4 
38.1 
39.6 
39.7 
39.7 
en-de 
WMT 
BLEU 
PORT 
Qmean 
13.6 
13.6 
13.6 
43.3 
43.3 
43.4 
30.1 
30.7 
30.3 
31.7 
31.7 
31.7 
Table 8: Impact of ordering measure v on PORT 
3.3 Analysis 
Now, we look at the details of PORT to see which 
of them are the most important. We do not have 
space here to describe all the details we studied, 
but we can describe some of them. E.g., does the 
ordering measure v help tuning performance? To 
answer this, we introduce an intermediate metric. 
This is Qmean as in Eq. (10): PORT without the 
ordering measure. Table 8 compares tuning with 
BLEU, PORT, and Qmean.  PORT outperforms 
Qmean on seven of the eight automatic scores 
shown for small and large Chinese-English. 
936
However, for the European language pairs, PORT 
and Qmean seem to be tied. This may be because 
we optimized ?  in Eq. (18) for Chinese-English, 
making the influence of word ordering measure v 
in PORT too strong for the European pairs, which 
have similar word order.  
Measure v seems to help Chinese-English 
tuning. What would results be on that language 
pair if we were to replace v in PORT with another 
ordering measure? Table 9 gives a partial answer, 
with Spearman?s ? and Kendall?s ? replacing v 
with ? or ? in PORT for the zh-en small condition 
(CTB with human word alignment is the dev set). 
The original definition of PORT seems preferable. 
 
Tune BLEU METEOR 1-TER 
BLEU 25.1 53.7 36.4 
PORT(v) 25.3 54.4 37.0 
PORT(?) 25.1 54.2 36.3 
PORT(?) 25.1 54.0 36.0 
Table 9: Comparison of the ordering measure: replacing 
? with ? or ? in PORT. 
 
 
Task 
 
Tune 
ordering measures 
? ? v 
NIST06 BLEU 
PORT 
0.979 
0.979 
0.926 
0.928 
0.915 
0.917 
NIST08 BLEU 
PORT 
0.980 
0.981 
0.926 
0.929 
0.916 
0.918 
CTB BLEU 
PORT 
0.973 
0.975 
0.860 
0.866 
0.847 
0.853 
Table 10: Ordering scores (?, ? and v) for test sets NIST 
2006, 2008 and CTB. 
 
A related question is how much word ordering 
improvement we obtained from tuning with PORT. 
We evaluate Chinese-English word ordering with 
three measures: Spearman?s ?, Kendall?s ? distance  
as applied to two permutations (see section 2.2.2) 
and our own measure v. Table 10 shows the effects 
of BLEU and PORT tuning on these three 
measures, for three test sets in the zh-en large 
condition. Reference alignments for CTB were 
created by humans, while the NIST06 and NIST08 
reference alignments were produced with GIZA++. 
A large value of ?, ?, or v implies outputs have 
ordering similar to that in the reference. From the 
table, we see that the PORT-tuned system yielded 
better word order than the BLEU-tuned system in 
all nine combinations of test sets and ordering 
measures. The advantage of PORT tuning is 
particularly noticeable on the most reliable test set: 
the hand-aligned CTB data.  
What is the impact of the strict redundancy 
penalty on PORT? Note that in Table 8, even 
though Qmean has no ordering measure, it 
outperforms BLEU. Table 11 shows the BLEU 
brevity penalty (BP) and (number of matching 1- 
& 4- grams)/(number of total 1- & 4- grams) for 
the translations. The BLEU-tuned and Qmean-
tuned systems generate similar numbers of 
matching n-grams, but Qmean-tuned systems 
produce fewer n-grams (thus, shorter translations). 
E.g., for zh-en small, the BLEU-tuned system 
produced 44,677 1-grams (words), while the 
Qmean-trained system one produced 43,555 1-
grams; both have about 32,000 1-grams matching 
the references. Thus, the Qmean translations have 
higher precision. We believe this is because of the 
strict redundancy penalty in Qmean. As usual, 
French-English is the outlier: the two outputs here 
are typically so similar that BLEU and Qmean 
tuning yield very similar n-gram statistics. 
 
Task Tune 1-gram 4-gram BP 
zh-en 
small 
BLEU 
Qmean 
32055/44677 
31996/43555 
4603/39716 
4617/38595 
0.967 
0.962 
zh-en 
large 
BLEU 
Qmean 
34583/45370 
34369/44229 
5954/40410 
5987/39271 
0.972 
0.959 
fr-en 
Hans 
BLEU 
Qmean 
28141/40525 
28167/40798 
8654/34224 
8695/34495 
0.983 
0.990 
de-en 
WMT 
BLEU 
Qmean 
42380/75428 
42173/72403 
5151/66425 
5203/63401 
1.000 
0.968 
en-de 
WMT 
BLEU 
Qmean 
30326/62367 
30343/62092 
2261/54812 
2298/54537 
1.000 
0.997 
Table 11: #matching-ngram/#total-ngram and BP score  
4 Conclusions 
In this paper, we have proposed a new tuning 
metric for SMT systems.  PORT incorporates 
precision, recall, strict brevity penalty and strict 
redundancy penalty, plus a new word ordering 
measure v.  As an evaluation metric, PORT 
performed better than BLEU at the system level 
and the segment level, and it was competitive with 
or slightly superior to METEOR at the segment 
level. Most important, our results show that PORT-
tuned MT systems yield better translations  than  
BLEU-tuned systems on several language pairs, 
according both to automatic metrics and human 
evaluations. In future work, we plan to tune the 
free parameter ? for each language pair. 
937
References 
S. Banerjee and A. Lavie. 2005. METEOR: An 
automatic metric for MT evaluation with improved 
correlation with human judgments. In Proceedings of 
ACL Workshop on Intrinsic & Extrinsic Evaluation 
Measures for Machine Translation and/or 
Summarization. 
A. Birch and M. Osborne. 2011. Reordering Metrics for 
MT. In Proceedings of ACL.  
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz and 
J. Schroeder. 2008. Further Meta-Evaluation of 
Machine Translation. In Proceedings of WMT. 
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. 
Re-evaluating the role of BLEU in machine 
translation research. In Proceedings of EACL. 
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, M. 
Przybocki and O. Zaidan. 2010. Findings of the 2010 
Joint Workshop on Statistical Machine Translation 
and Metrics for Machine Translation. In Proceedings 
of WMT. 
C. Callison-Burch, P. Koehn, C. Monz and O. Zaidan. 
2011. Findings of the 2011 Workshop on Statistical 
Machine Translation. In Proceedings of WMT. 
D. Cer, D. Jurafsky and C. Manning. 2010. The Best 
Lexical Metric for Phrase-Based Statistical MT 
System Optimization. In Proceedings of NAACL. 
Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maximum 
similarity metric for machine translation evaluation. 
In Proceedings of ACL. 
B. Chen and R. Kuhn. 2011. AMBER: A Modified 
BLEU, Enhanced Ranking Metric. In: Proceedings of 
WMT. Edinburgh, UK. July. 
D. Chiang, S. DeNeefe, Y. S. Chan, and H. T. Ng. 2008. 
Decomposability of translation metrics for improved 
evaluation and efficient algorithms. In Proceedings of 
EMNLP, pages 610?619. 
M. Denkowski and A. Lavie. 2010. Meteor-next and the 
meteor paraphrase tables: Improved evaluation 
support for five target languages. In Proceedings of 
the Joint Fifth Workshop on SMT and 
MetricsMATR, pages 314?317. 
G. Doddington. 2002. Automatic evaluation of machine 
translation quality using n-gram co-occurrence 
statistics. In Proceedings of HLT. 
J. L. Fleiss. 1971. Measuring nominal scale agreement 
among many raters. In Psychological Bulletin, Vol. 
76, No. 5 pp. 378?382. 
Y. He, J. Du, A. Way and J. van Genabith. 2010. The 
DCU dependency-based metric in WMT-
MetricsMATR 2010. In Proceedings of the Joint 
Fifth Workshop on Statistical Machine Translation 
and MetricsMATR, pages 324?328.  
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada. 
2010. Automatic Evaluation of Translation Quality 
for Distant Language Pairs. In Proceedings of 
EMNLP.  
M. Kendall. 1938. A New Measure of Rank Correlation. 
In Biometrika, 30 (1?2), pp. 81?89. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, 
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. 
Herbst. 2007. Moses: Open Source Toolkit for Statis-
tical Machine Translation. In Proceedings of ACL, 
pp. 177-180, Prague, Czech Republic. 
A. Lavie and M. J. Denkowski. 2009. The METEOR 
metric for automatic evaluation of machine 
translation. Machine Translation, 23. 
C. Liu, D. Dahlmeier, and H. T. Ng. 2010. TESLA: 
Translation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the 
Joint Fifth Workshop on Statistical Machine 
Translation and MetricsMATR, pages 329?334. 
C. Liu, D. Dahlmeier, and H. T. Ng. 2011. Better 
evaluation metrics lead to better machine translation. 
In Proceedings of EMNLP. 
C. Lo and D. Wu. 2011. MEANT: An inexpensive, 
high-accuracy, semi-automatic metric for evaluating 
translation utility based on semantic roles. In 
Proceedings of ACL. 
F. J. Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
ACL-2003. Sapporo, Japan. 
F. J. Och and H. Ney. 2003. A Systematic Comparison 
of Various Statistical Alignment Models. In 
Computational Linguistics, 29, pp. 19?51. 
S. Pado, M. Galley, D. Jurafsky, and C.D. Manning. 
2009. Robust machine translation evaluation with 
entailment features. In Proceedings of ACL-IJCNLP. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
BLEU: a method for automatic evaluation of 
machine translation. In Proceedings of ACL. 
K. Parton, J. Tetreault, N. Madnani and M. Chodorow. 
2011.  E-rating Machine Translation. In Proceedings 
of WMT. 
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. 
Makhoul. 2006. A Study of Translation Edit Rate 
938
with Targeted Human Annotation. In Proceedings of 
Association for Machine Translation in the Americas. 
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 
2009. Fluency, Adequacy, or HTER? Exploring 
Different Human Judgments with a Tunable MT 
Metric. In Proceedings of the Fourth Workshop on 
Statistical Machine Translation, Athens, Greece. 
C. Spearman. 1904. The proof and measurement of 
association between two things. In American Journal 
of Psychology, 15, pp. 72?101. 
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based 
word alignment in statistical translation. In 
Proceedings of COLING. 
939
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285?1293,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Vector Space Model for Adaptation in Statistical Machine Translation
Boxing Chen, Roland Kuhn and George Foster
National Research Council Canada
first.last@nrc-cnrc.gc.ca
Abstract
This paper proposes a new approach to
domain adaptation in statistical machine
translation (SMT) based on a vector space
model (VSM). The general idea is first to
create a vector profile for the in-domain
development (?dev?) set. This profile
might, for instance, be a vector with a di-
mensionality equal to the number of train-
ing subcorpora; each entry in the vector re-
flects the contribution of a particular sub-
corpus to all the phrase pairs that can be
extracted from the dev set. Then, for
each phrase pair extracted from the train-
ing data, we create a vector with features
defined in the same way, and calculate its
similarity score with the vector represent-
ing the dev set. Thus, we obtain a de-
coding feature whose value represents the
phrase pair?s closeness to the dev. This is
a simple, computationally cheap form of
instance weighting for phrase pairs. Ex-
periments on large scale NIST evaluation
data show improvements over strong base-
lines: +1.8 BLEU on Arabic to English
and +1.4 BLEU on Chinese to English
over a non-adapted baseline, and signifi-
cant improvements in most circumstances
over baselines with linear mixture model
adaptation. An informal analysis suggests
that VSM adaptation may help in making
a good choice among words with the same
meaning, on the basis of style and genre.
1 Introduction
The translation models of a statistical machine
translation (SMT) system are trained on parallel
data. Usage of language and therefore the best
translation practice differs widely across genres,
topics, and dialects, and even depends on a partic-
ular author?s or publication?s style; the word ?do-
main? is often used to indicate a particular combi-
nation of all these factors. Unless there is a per-
fect match between the training data domain and
the (test) domain in which the SMT system will
be used, one can often get better performance by
adapting the system to the test domain.
Domain adaptation is an active topic in the nat-
ural language processing (NLP) research commu-
nity. Its application to SMT systems has recently
received considerable attention. Approaches that
have been tried for SMT model adaptation include
mixture models, transductive learning, data selec-
tion, instance weighting, and phrase sense disam-
biguation, etc.
Research on mixture models has considered
both linear and log-linear mixtures. Both were
studied in (Foster and Kuhn, 2007), which con-
cluded that the best approach was to combine sub-
models of the same type (for instance, several
different TMs or several different LMs) linearly,
while combining models of different types (for in-
stance, a mixture TM with a mixture LM) log-
linearly. (Koehn and Schroeder, 2007), instead,
opted for combining the sub-models directly in the
SMT log-linear framework.
In transductive learning, an MT system trained
on general domain data is used to translate in-
domain monolingual data. The resulting bilingual
sentence pairs are then used as additional train-
ing data (Ueffing et al, 2007; Chen et al, 2008;
Schwenk, 2008; Bertoldi and Federico, 2009).
Data selection approaches (Zhao et al, 2004;
Hildebrand et al, 2005; Lu? et al, 2007; Moore
and Lewis, 2010; Axelrod et al, 2011) search for
bilingual sentence pairs that are similar to the in-
domain ?dev? data, then add them to the training
data.
Instance weighting approaches (Matsoukas et
al., 2009; Foster et al, 2010; Huang and Xiang,
2010; Phillips and Brown, 2011; Sennrich, 2012)1285
typically use a rich feature set to decide on weights
for the training data, at the sentence or phrase pair
level. For example, a sentence from a subcorpus
whose domain is far from that of the dev set would
typically receive a low weight, but sentences in
this subcorpus that appear to be of a general na-
ture might receive higher weights.
The 2012 JHU workshop on Domain Adapta-
tion for MT 1 proposed phrase sense disambigua-
tion (PSD) for translation model adaptation. In
this approach, the context of a phrase helps the
system to find the appropriate translation.
In this paper, we propose a new instance weight-
ing approach to domain adaptation based on a vec-
tor space model (VSM). As in (Foster et al, 2010),
this approach works at the level of phrase pairs.
However, the VSM approach is simpler and more
straightforward. Instead of using word-based fea-
tures and a computationally expensive training
procedure, we capture the distributional properties
of each phrase pair directly, representing it as a
vector in a space which also contains a representa-
tion of the dev set. The similarity between a given
phrase pair?s vector and the dev set vector be-
comes a feature for the decoder. It rewards phrase
pairs that are in some sense closer to those found
in the dev set, and punishes the rest. In initial ex-
periments, we tried three different similarity func-
tions: Bhattacharyya coefficient, Jensen-Shannon
divergency, and cosine measure. They all enabled
VSM adaptation to beat the non-adaptive baseline,
but Bhattacharyya similarity worked best, so we
adopted it for the remaining experiments.
The vector space used by VSM adaptation can
be defined in various ways. In the experiments
described below, we chose a definition that mea-
sures the contribution (to counts of a given phrase
pair, or to counts of all phrase pairs in the dev
set) of each training subcorpus. Thus, the vari-
ant of VSM adaptation tested here bears a super-
ficial resemblance to domain adaptation based on
mixture models for TMs, as in (Foster and Kuhn,
2007), in that both approaches rely on information
about the subcorpora from which the data origi-
nate. However, a key difference is that in this pa-
per we explicitly capture each phrase pair?s dis-
tribution across subcorpora, and compare it to the
aggregated distribution of phrase pairs in the dev
set. In mixture models, a phrase pair?s distribu-
1http://www.clsp.jhu.edu/workshops/archive/ws-
12/groups/dasmt
tion across subcorpora is captured only implicitly,
by probabilities that reflect the prevalence of the
pair within each subcorpus. Thus, VSM adapta-
tion occurs at a much finer granularity than mix-
ture model adaptation. More fundamentally, there
is nothing about the VSM idea that obliges us to
define the vector space in terms of subcorpora.
For instance, we could cluster the words in the
source language into S clusters, and the words in
the target language into T clusters. Then, treat-
ing the dev set and each phrase pair as a pair of
bags of words (a source bag and a target bag) one
could represent each as a vector of dimension S +
T, with entries calculated from the counts associ-
ated with the S + T clusters (in a way similar to
that described for phrase pairs below). The (dev,
phrase pair) similarity would then be independent
of the subcorpora. One can think of several other
ways of defining the vector space that might yield
even better results than those reported here. Thus,
VSM adaptation is not limited to the variant of it
that we tested in our experiments.
2 Vector space model adaptation
Vector space models (VSMs) have been widely
applied in many information retrieval and natural
language processing applications. For instance, to
compute the sense similarity between terms, many
researchers extract features for each term from its
context in a corpus, define a VSM and then ap-
ply similarity functions (Hindle, 1990; Lund and
Burgess, 1996; Lin, 1998; Turney, 2001).
In our experiments, we exploited the fact that
the training data come from a set of subcorpora.
For instance, the Chinese-English training data are
made up of 14 subcorpora (see section 3 below).
Suppose we have C subcorpora. The domain vec-
tor for a phrase-pair (f, e) is defined as
V (f, e) =< w1(f, e), ...wi(f, e), ..., wC(f, e) >,
(1)
where wi(f, e) is a standard tf ? idf weight, i.e.
wi(f, e) = tfi (f, e) ? idf (f, e) . (2)
To avoid a bias towards longer corpora, we nor-
malize the raw joint count ci(f, e) in the corpus
si by dividing by the maximum raw count of any
phrase pair extracted in the corpus si. Let1286
tfi (f, e) =
ci (f, e)
max {ci (fj , ek) , (fj , ek) ? si}
.
(3)
The idf (f, e) is the inverse document fre-
quency: a measure of whether the phrase-pair
(f, e) is common or rare across all subcorpora. We
use the standard formula:
idf (f, e) = log
( C
df (f, e) + ?
)
, (4)
where df(f, e) is the number of subcorpora that
(f, e) appears in, and ? is an empirically deter-
mined smoothing term.
For the in-domain dev set, we first run word
alignment and phrases extracting in the usual way
for the dev set, then sum the distribution of each
phrase pair (fj , ek) extracted from the dev data
across subcorpora to represent its domain informa-
tion. The dev vector is thus
V (dev) =< w1(dev), . . . , wC(dev) >, (5)
where
wi(dev) =
j=J?
j=0
k=K?
k=0
cdev (fj , ek)wi(fj , ek) (6)
J,K are the total numbers of source/target
phrases extracted from the dev data respectively.
cdev (fj , ek) is the joint count of phrase pair fj , ek
found in the dev set.
The vector can also be built with other features
of the phrase pair. For instance, we could replace
the raw joint count ci(f, e) in Equation 3 with the
raw marginal count of phrase pairs (f, e). There-
fore, even within the variant of VSM adaptation
we focus on in this paper, where the definition of
the vector space is based on the existence of sub-
corpora, one could utilize other definitions of the
vectors of the similarity function than those we uti-
lized in our experiments.
2.1 Vector similarity functions
VSM uses the similarity score between the vec-
tor representing the in-domain dev set and the vec-
tor representing each phrase pair as a decoder fea-
ture. There are many similarity functions we could
have employed for this purpose (Cha, 2007). We
tested three commonly-used functions: the Bhat-
tacharyya coefficient (BC) (Bhattacharyya, 1943;
Kazama et al, 2010), the Jensen-Shannon diver-
gence (JSD), and the cosine measure. According
to (Cha, 2007), these belong to three different fam-
ilies of similarity functions: the Fidelity family,
the Shannon?s entropy family, and the inner Prod-
uct family respectively. It was BC similarity that
yielded the best performance, and that we ended
up using in subsequent experiments.
To map the BC score onto a range from 0 to
1, we first normalize each weight in the vector by
dividing it by the sum of the weights. Thus, we get
the probability distribution of a phrase pair or the
phrase pairs in the dev data across all subcorpora:
pi(f, e) =
wi(f, e)?j=C
j=1 wj(f, e)
(7)
pi(dev) =
wi(dev)?j=C
j=1 wj(dev)
(8)
To further improve the similarity score, we ap-
ply absolute discounting smoothing when calcu-
lating the probability distributions pi(f, e). We
subtract a discounting value ? from the non-zero
pi(f, e), and equally allocate the remaining proba-
bility mass to the zero probabilities. We carry out
the same smoothing for the probability distribu-
tions pi(dev). The smoothing constant ? is deter-
mined empirically on held-out data.
The Bhattacharyya coefficient (BC) is defined
as follows:
BC(dev; f, e) =
i=C?
i=0
?
pi(dev) ? pi(f, e) (9)
The other two similarity functions we also
tested are JSD and cosine (Cos). They are defined
as follows:
JSD(dev; f, e) = (10)
1
2[
i=C?
i=1
pi(dev) log
2pi(dev)
pi(dev) + pi(f, e)
+
i=C?
i=1
pi(f, e) log
2pi(f, e)
pi(dev) + pi(f, e)
]
Cos(dev; f, e) =
?
i pi(dev) ? pi (f, e)??
i p2i (dev)
??
i p2i (f, e)
(11)1287
corpus # segs # en tok % genres
fbis 250K 10.5M 3.7 nw
financial 90K 2.5M 0.9 fin
gale bc 79K 1.3M 0.5 bc
gale bn 75K 1.8M 0.6 bn ng
gale nw 25K 696K 0.2 nw
gale wl 24K 596K 0.2 wl
hkh 1.3M 39.5M 14.0 hans
hkl 400K 9.3M 3.3 legal
hkn 702K 16.6M 5.9 nw
isi 558K 18.0M 6.4 nw
lex&ne 1.3M 2.0M 0.7 lex
other nw 146K 5.2M 1.8 nw
sinorama 282K 10.0M 3.5 nw
un 5.0M 164M 58.2 un
TOTAL 10.1M 283M 100.0 (all)
devtest
tune 1,506 161K nw wl
NIST06 1,664 189K nw bng
NIST08 1,357 164K nw wl
Table 1: NIST Chinese-English data. In the
genres column: nw=newswire, bc=broadcast
conversation, bn=broadcast news, wl=weblog,
ng=newsgroup, un=UN proc., bng = bn & ng.
3 Experiments
3.1 Data setting
We carried out experiments in two different set-
tings, both involving data from NIST Open MT
2012.2 The first setting is based on data from
the Chinese to English constrained track, compris-
ing about 283 million English running words. We
manually grouped the training data into 14 corpora
according to genre and origin. Table 1 summa-
rizes information about the training, development
and test sets; we show the sizes of the training sub-
corpora in number of words as a percentage of all
training data. Most training subcorpora consist of
parallel sentence pairs. The isi and lex&ne cor-
pora are exceptions: the former is extracted from
comparable data, while the latter is a lexicon that
includes many named entities. The development
set (tune) was taken from the NIST 2005 evalua-
tion set, augmented with some web-genre material
reserved from other NIST corpora.
The second setting uses NIST 2012 Arabic to
English data, but excludes the UN data. There are
about 47.8 million English running words in these
2http://www.nist.gov/itl/iad/mig/openmt12.cfm
corpus # segs # en toks % gen
gale bc 57K 1.6M 3.3 bc
gale bn 45K 1.2M 2.5 bn
gale ng 21K 491K 1.0 ng
gale nw 17K 659K 1.4 nw
gale wl 24K 590K 1.2 wl
isi 1,124K 34.7M 72.6 nw
other nw 224K 8.7M 18.2 nw
TOTAL 1,512K 47.8M 100.0 (all)
devtest
NIST06 1,664 202K nwl
NIST08 1,360 205K nwl
NIST09 1,313 187K nwl
Table 2: NIST Arabic-English data. In the gen
(genres) column: nw=newswire, bc=broadcast
conversation, bn=broadcast news, ng=newsgroup,
wl=weblog, nwl = nw & wl.
training data. We manually grouped the training
data into 7 groups according to genre and origin.
Table 2 summarizes information about the train-
ing, development and test sets. Note that for this
language pair, the comparable isi data represent a
large proportion of the training data: 72% of the
English words. We use the evaluation sets from
NIST 2006, 2008, and 2009 as our development
set and two test sets, respectively.
3.2 System
Experiments were carried out with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007). Each corpus was word-aligned using
IBM2, HMM, and IBM4 models, and the phrase
table was the union of phrase pairs extracted from
these separate alignments, with a length limit of
7. The translation model (TM) was smoothed in
both directions with KN smoothing (Chen et al,
2011). We use the hierarchical lexicalized reorder-
ing model (RM) (Galley and Manning, 2008), with
a distortion limit of 7. Other features include lex-
ical weighting in both directions, word count, a
distance-based RM, a 4-gram LM trained on the
target side of the parallel data, and a 6-gram En-
glish Gigaword LM. The system was tuned with
batch lattice MIRA (Cherry and Foster, 2012).
3.3 Results
For the baseline, we simply concatenate all train-
ing data. We have also compared our approach
to two widely used TM domain adaptation ap-1288
proaches. One is the log-linear combination
of TMs trained on each subcorpus (Koehn and
Schroeder, 2007), with weights of each model
tuned under minimal error rate training using
MIRA. The other is a linear combination of TMs
trained on each subcorpus, with the weights of
each model learned with an EM algorithm to max-
imize the likelihood of joint empirical phrase pair
counts for in-domain dev data. For details, refer to
(Foster and Kuhn, 2007).
The value of ? and ? (see Eq 4 and Section 2.1)
are determined by the performance on the dev
set of the Arabic-to-English system. For both
Arabic-to-English and Chinese-to-English exper-
iment, these values obtained on Arabic dev were
used to obtain the results below: ? was set to 8,
and ? was set to 0.01. (Later, we ran an exper-
iment on Chinese-to-English with ? and ? tuned
specifically for that language pair, but the perfor-
mance for the Chinese-English system only im-
proved by a tiny, insignificant amount).
Our metric is case-insensitive IBM BLEU (Pa-
pineni et al, 2002), which performs matching of
n-grams up to n = 4; we report BLEU scores av-
eraged across both test sets NIST06 and NIST08
for Chinese; NIST08 and NIST09 for Arabic.
Following (Koehn, 2004), we use the bootstrap-
resampling test to do significance testing. In ta-
bles 3 to 5, * and ** denote significant gains over
the baseline at p < 0.05 and p < 0.01 levels, re-
spectively.
We first compare the performance of differ-
ent similarity functions: cosine (COS), Jensen-
Shannon divergence (JSD) and Bhattacharyya co-
efficient (BC). The results are shown in Table 3.
All three functions obtained improvements. Both
COS and BC yield statistically significant im-
provements over the baseline, with BC performing
better than COS by a further statistically signifi-
cant margin. The Bhattacharyya coefficient is ex-
plicitly designed to measure the overlap between
the probability distributions of two statistical sam-
ples or populations, which is precisely what we are
trying to do here: we are trying to reward phrase
pairs whose distribution is similar to that of the
dev set. Thus, its superior performance in these
experiments is not unexpected.
In the next set of experiments, we compared
VSM adaptation using the BC similarity function
with the baseline which concatenates all training
data and with log-linear and linear TM mixtures
system Chinese Arabic
baseline 31.7 46.8
COS 32.3* 47.8**
JSD 32.1 47.1
BC 33.0** 48.4**
Table 3: Comparison of different similarity func-
tions. * and ** denote significant gains over the
baseline at p < 0.05 and p < 0.01 levels, respec-
tively.
system Chinese Arabic
baseline 31.7 46.8
loglinear tm 28.4 44.5
linear tm 32.7** 47.5**
vsm, BC 33.0** 48.4**
Table 4: Results for variants of adaptation.
whose components are based on subcorpora. Ta-
ble 4 shows that log-linear combination performs
worse than the baseline: the tuning algorithm
failed to optimize the log-linear combination even
on dev set. For Chinese, the BLEU score of the
dev set on the baseline system is 27.3, while on
the log-linear combination system, it is 24.0; for
Arabic, the BLEU score of the dev set on the base-
line system is 46.8, while on the log-linear com-
bination system, it is 45.4. We also tried adding
the global model to the loglinear combination and
it didn?t improve over the baseline for either lan-
guage pair. Linear mixture was significantly better
than the baseline at the p < 0.01 level for both lan-
guage pairs. Since our approach, VSM, performed
better than the linear mixture for both pairs, it is of
course also significantly better than the baseline at
the p < 0.01 level.
This raises the question: is VSM performance
significantly better than that of a linear mixture of
TMs? The answer (not shown in the table) is that
for Arabic to English, VSM performance is bet-
ter than linear mixture at the p < 0.01 level. For
Chinese to English, the argument for the superi-
ority of VSM over linear mixture is less convinc-
ing: there is significance at the p < 0.05 for one
of the two test sets (NIST06) but not for the other
(NIST08). At any rate, these results establish that
VSM adaptation is clearly superior to linear mix-
ture TM adaptation, for one of the two language
pairs.
In Table 4, the VSM results are based on the1289
system Chinese Arabic
baseline 31.7 46.8
linear tm 32.7** 47.5**
vsm, joint 33.0** 48.4**
vsm, src-marginal 32.2* 47.3*
vsm, tgt-marginal 32.6** 47.6**
vsm, src+tgt (2 feat.) 32.7** 48.2**
vsm, joint+src (2 feat.) 32.9** 48.4**
vsm, joint+tgt (2 feat.) 32.9** 48.4**
vsm, joint+src+tgt (3 feat.) 33.1** 48.6**
Table 5: Results for adaptation based on joint or
maginal counts.
vector of the joint counts of the phrase pair. In
the next experiment, we replace the joint counts
with the source or target marginal counts. In Ta-
ble 5, we first show the results based on source
and target marginal counts, then the results of us-
ing feature sets drawn from three decoder VSM
features: a joint count feature, a source marginal
count feature, and a target marginal count fea-
ture. For instance, the last row shows the results
when all three features are used (with their weights
tuned by MIRA). It looks as though the source and
target marginal counts contain useful information.
The best performance is obtained by combining all
three sources of information. The 3-feature ver-
sion of VSM yields +1.8 BLEU over the baseline
for Arabic to English, and +1.4 BLEU for Chinese
to English.
When we compared two sets of results in Ta-
ble 4, the joint count version of VSM and lin-
ear mixture of TMs, we found that for Arabic to
English, VSM performance is better than linear
mixture at the p < 0.01 level; the Chinese to
English significance test was inconclusive (VSM
found to be superior to linear mixture at p < 0.05
for NIST06 but not for NIST08). We now have
somewhat better results for the 3-feature version
of VSM shown in Table 5. How do these new re-
sults affect the VSM vs. linear mixture compari-
son? Naturally, the conclusions for Arabic don?t
change. For Chinese, 3-feature VSM is now su-
perior to linear mixture at p < 0.01 on NIST06
test set, but 3-feature VSM still doesn?t have a sta-
tistically significant edge over linear mixture on
NIST08 test set. A fair summary would be that 3-
feature VSM adaptation is decisively superior to
linear mixture adaptation for Arabic to English,
and highly competitive with linear mixture adap-
tation for Chinese to English.
Our last set of experiments examined the ques-
tion: when added to a system that already has
some form of linear mixture model adaptation,
does VSM improve performance? In (Foster and
Kuhn, 2007), two kinds of linear mixture were de-
scribed: linear mixture of language models (LMs),
and linear mixture of translation models (TMs).
Some of the results reported above involved lin-
ear TM mixtures, but none of them involved lin-
ear LM mixtures. Table 6 shows the results of
different combinations of VSM and mixture mod-
els. * and ** denote significant gains over the row
no vsm at p < 0.05 and p < 0.01 levels, re-
spectively. This means that in the table, the base-
line within each box containing three results is the
topmost result in the box. For instance, with an
initial Chinese system that employs linear mixture
LM adaptation (lin-lm) and has a BLEU of 32.1,
adding 1-feature VSM adaptation (+vsm, joint)
improves performance to 33.1 (improvement sig-
nificant at p < 0.01), while adding 3-feature VSM
instead (+vsm, 3 feat.) improves performance to
33.2 (also significant at p < 0.01). For Arabic, in-
cluding either form of VSM adaptation always im-
proves performance with significance at p < 0.01,
even over a system including both linear TM and
linear LM adaptation. For Chinese, adding VSM
still always yields an improvement, but the im-
provement is not significant if linear TM adapta-
tion is already in the system. These results show
that combining VSM adaptation and either or both
kinds of linear mixture adaptation never hurts per-
formance, and often improves it by a significant
amount.
3.4 Informal Data Analysis
To get an intuition for how VSM adaptation im-
proves BLEU scores, we compared outputs from
the baseline and VSM-adapted system (?vsm,
joint? in Table 5) on the Chinese test data. We
focused on examples where the two systems had
translated the same source-language (Chinese)
phrase s differently, and where the target-language
(English) translation of s chosen by the VSM-
adapted system, tV , had a higher Bhattacharyya
score for similarity with the dev set than did the
phrase that was chosen by the baseline system, tB .
Thus, we ignored differences in the two transla-
tions that might have been due to the secondary
effects of VSM adaptation (such as a different tar-1290
no-lin-adap lin-lm lin-tm lin-lm+lin-tm
no vsm 31.7 32.1 32.7 33.1
Chinese +vsm, joint 33.0** 33.1** 33.0 33.3
+vsm, 3 feat. 33.1** 33.2** 33.1 33.4
no vsm 46.8 47.0 47.5 47.7
Arabic +vsm, joint 48.4** 48.7** 48.6** 48.8**
+vsm, 3 feat. 48.6** 48.8** 48.7** 48.9**
Table 6: Results of combining VSM and linear mixture adaptation. ?lin-lm? is linear language model
adaptation, ?lin-tm? is linear translation model adaptation. * and ** denote significant gains over the row
?no vsm? at p < 0.05 and p < 0.01 levels, respectively.
get phrase being preferred by the language model
in the VSM-adapted system from the one preferred
in the baseline system because of a Bhattacharyya-
mediated change in the phrase preceding it).
An interesting pattern soon emerged: the VSM-
adapted system seems to be better than the base-
line at choosing among synonyms in a way that is
appropriate to the genre or style of a text. For in-
stance, where the text to be translated is from an
informal genre such as weblog, the VSM-adapted
system will often pick an informal word where the
baseline picks a formal word with the same or sim-
ilar meaning, and vice versa where the text to be
translated is from a more formal genre. To our
surprise, we saw few examples where the VSM-
adapted system did a better job than the baseline of
choosing between two words with different mean-
ing, but we saw many examples where the VSM-
adapted system did a better job than the baseline
of choosing between two words that both have the
same meaning according to considerations of style
and genre.
Two examples are shown in Table 7. In the
first example, the first two lines show that VSM
finds that the Chinese-English phrase pair (??,
assaulted) has a Bhattacharyya (BC) similarity of
0.556163 to the dev set, while the phrase pair (?
?, beat) has a BC similarity of 0.780787 to the
dev. In this situation, the VSM-adapted system
thus prefers ?beat? to ?assaulted? as a translation
for ??. The next four lines show the source
sentence (SRC), the reference (REF), the baseline
output (BSL), and the output of the VSM-adapted
system. Note that the result of VSM adaptation is
that the rather formal word ?assaulted? is replaced
by its informal near-synonym ?beat? in the trans-
lation of an informal weblog text.
?apprehend? might be preferable to ?arrest? in
a legal text. However, it looks as though the
VSM-adapted system has learned from the dev
that among synonyms, those more characteristic
of news stories than of legal texts should be cho-
sen: it therefore picks ?arrest? over its synonym
?apprehend?.
What follows is a partial list of pairs of phrases
(all single words) from our system?s outputs,
where the baseline chose the first member of a pair
and the VSM-adapted system chose the second
member of the pair to translate the same Chinese
phrase into English (because the second word
yields a better BC score for the dev set we used).
It will be seen that nearly all of the pairs involve
synonyms or near-synonyms rather than words
with radically different senses (one exception
below is ?center? vs ?heart?). Instead, the differ-
ences between the two words tend to be related to
genre or style: gunmen-mobsters, champion-star,
updated-latest, caricatures-cartoons, spill-leakage,
hiv-aids, inkling-clues, behaviour-actions, deceit-
trick, brazen-shameless, aristocratic-noble,
circumvent-avoid, attack-criticized, descent-born,
hasten-quickly, precipice-cliff, center-heart,
blessing-approval, imminent-approaching,
stormed-rushed, etc.
4 Conclusions and future work
This paper proposed a new approach to domain
adaptation in statistical machine translation, based
on vector space models (VSMs). This approach
measures the similarity between a vector repre-
senting a particular phrase pair in the phrase ta-
ble and a vector representing the dev set, yield-
ing a feature associated with that phrase pair that
will be used by the decoder. The approach is
simple, easy to implement, and computationally
cheap. For the two language pairs we looked
at, it provided a large performance improvement
over a non-adaptive baseline, and also compared1291
1 phrase ??? assaulted (0.556163)
pairs ??? beat (0.780787)
SRC ...???????????...
REF ... those local ruffians and hooligans who beat up villagers ...
BSL ... those who assaulted the villagers land hooligans ...
VSM ... hooligans who beat the villagers ...
2 phrase ??? apprehend (0.286533)
pairs ??? arrest (0.603342)
SRC ... ?????????????
REF ... catch the killers and bring them to justice .
BSL ... apprehend the perpetrators and bring them to justice .
VSM ... arrest the perpetrators and bring them to justice .
Table 7: Examples show that VSM chooses translations according to considerations of style and genre.
favourably with linear mixture adaptation tech-
niques.
Furthermore, VSM adaptation can be exploited
in a number of different ways, which we have only
begun to explore. In our experiments, we based
the vector space on subcorpora defined by the na-
ture of the training data. This was done purely
out of convenience: there are many, many ways to
define a vector space in this situation. An obvi-
ous and appealing one, which we intend to try in
future, is a vector space based on a bag-of-words
topic model. A feature derived from this topic-
related vector space might complement some fea-
tures derived from the subcorpora which we ex-
plored in the experiments above, and which seem
to exploit information related to genre and style.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In EMNLP 2011.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th Workshop on Statistical Machine Translation,
Athens, March. WMT.
A. Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bulletin of the Calcutta
Mathematical Society, 35:99?109.
Sung-Hyuk Cha. 2007. Comprehensive survey on dis-
tance/similarity measures between probability den-
sity functions. International Journal of Mathe-
matical Models ind Methods in Applied Sciences,
1(4):300?307.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for smt self-
enhancement. In ACL 2008.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In MT Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
ACL Workshop on Statistical Machine Translation,
Prague, June. WMT.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Boston.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model.
In EMNLP 2008, pages 848?856, Hawaii, October.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
EAMT Conference, Budapest, May.
Donald Hindle. 1990. Noun classification from predi-
cate.argument structures. In Proceedings of the 28th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 268?275, Pittsburgh,
PA, June. ACL.
Fei Huang and Bing Xiang. 2010. Feature-rich dis-
criminative phrase rescoring for SMT. In COLING
2010.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A1292
bayesian method for robust estimation of distribu-
tional similarities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 247?256, Uppsala, Swe-
den, July. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL 2007,
Demonstration Session.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Barcelona, Spain.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL-
98, pages 768?774, Montreal, Quebec, Canada.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Pro-
ceedings of the 2007 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
Prague, Czech Republic.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods Instru-
ments and Computers, 28(2):203?208.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL
2010.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, July. ACL.
Aaron B. Phillips and Ralf D. Brown. 2011. Train-
ing machine translation with a second-order taylor
approximation of weighted translation instances. In
MT Summit 2011.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT 2008.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In EACL 2012.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Twelfth European
Conference on Machine Learning, page 491?502,
Berlin, Germany.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Prague, Czech Republic, June.
ACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING) 2004, Geneva, Au-
gust.
1293
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 11?16,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Fast Consensus Hypothesis Regeneration for Machine Translation 
 
Boxing Chen, George Foster and Roland Kuhn 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca 
 
  
 
Abstract 
This paper presents a fast consensus hy-
pothesis regeneration approach for ma-
chine translation. It combines the advan-
tages of feature-based fast consensus de-
coding and hypothesis regeneration.  Our 
approach is more efficient than previous 
work on hypothesis regeneration, and it 
explores a wider search space than con-
sensus decoding, resulting in improved 
performance.  Experimental results show 
consistent improvements across language 
pairs, and an improvement of up to 0.72 
BLEU is obtained over a competitive 
single-pass baseline on the Chinese-to-
English NIST task. 
1 Introduction 
State-of-the-art statistical machine translation 
(SMT) systems are often described as a two-pass 
process. In the first pass, decoding algorithms are 
applied to generate either a translation N-best list 
or a translation forest.  Then in the second pass, 
various re-ranking algorithms are adopted to 
compute the final translation. The re-ranking al-
gorithms include rescoring (Och et al, 2004) and 
Minimum Bayes-Risk (MBR) decoding (Kumar 
and Byrne, 2004; Zhang and Gildea, 2008; 
Tromble et al, 2008). Rescoring uses more so-
phisticated additional feature functions to score 
the hypotheses. MBR decoding directly incorpo-
rates the evaluation metrics (i.e., loss function), 
into the decision criterion, so it is effective in 
tuning the MT performance for a specific loss 
function. In particular, sentence-level BLEU loss 
function gives gains on BLEU (Kumar and 
Byrne, 2004).  
The na?ve MBR algorithm computes the loss 
function between every pair of k hypotheses, 
needing O(k2) comparisons. Therefore, only 
small number k is applicable. Very recently, De-
Nero et al (2009) proposed a fast consensus de-
coding (FCD) algorithm in which the similarity 
scores are computed based on the feature expec-
tations over the translation N-best list or transla-
tion forest. It is equivalent to MBR decoding 
when using a linear similarity function, such as 
unigram precision.  
Re-ranking approaches improve performance 
on an N-best list whose contents are fixed. A   
complementary strategy is to augment the con-
tents of an N-best list in order to broaden the 
search space. Chen et al(2008) have proposed a 
three-pass SMT process, in which a hypothesis 
regeneration pass is added between the decoding 
and rescoring passes. New hypotheses are gener-
ated based on the original N-best hypotheses 
through n-gram expansion, confusion-network 
decoding or re-decoding. All three hypothesis 
regeneration methods obtained decent and com-
parable improvements in conjunction with the 
same rescoring model. However, since the final 
translation candidates in this approach are pro-
duced from different methods, local feature func-
tions (such as translation models and reordering 
models) of each hypothesis are not directly com-
parable and rescoring must exploit rich global 
feature functions to compensate for the loss of 
local feature functions. Thus this approach is de-
pendent on the use of computationally expensive 
features for rescoring, which makes it inefficient.  
In this paper, we propose a fast consensus hy-
pothesis regeneration method that combines the 
advantages of feature-based fast consensus de-
coding and hypothesis regeneration. That is, we 
integrate the feature-based similarity/loss func-
tion based on evaluation metrics such as BLEU 
score into the hypothesis regeneration procedure 
to score the partial hypotheses in the beam search 
and compute the final translations. Thus, our ap-
proach is more efficient than the original three-
pass hypothesis regeneration. Moreover, our ap-
proach explores more search space than consen-
11
sus decoding, giving it an advantage over the 
latter. 
In particular, we extend linear corpus BLEU 
(Tromble et al, 2008) to n-gram expectation-
based linear BLEU, then further extend the n-
gram expectation computed on full-length hypo-
theses to n-gram expectation computed on fixed-
length partial hypotheses. Finally, we extend the 
hypothesis regeneration with forward n-gram 
expansion to bidirectional n-gram expansion in-
cluding both the forward and backward n-gram 
expansion. Experimental results show consistent 
improvements over the baseline across language 
pairs, and up to 0.72 BLEU points are obtained 
from a competitive baseline on the Chinese-to-
English NIST task. 
2 Fast Consensus Hypothesis Regenera-
tion 
Since the three hypothesis regeneration methods 
with n-gram expansion, confusion network de-
coding and re-decoding produce very similar per-
formance (Chen et al, 2008), we consider only 
n-gram expansion method in this paper. N-gram 
expansion can (almost) fully exploit the search 
space of target strings which can be generated by 
an n-gram language model trained on the N-best 
hypotheses (Chen et al, 2007). 
2.1 Hypothesis regeneration with bidirec-
tional n-gram expansion 
N-gram expansion (Chen et al, 2007) works as 
follows: firstly, train an n-gram language model 
based on the translation N-best list or translation 
forest; secondly, expand each partial hypothesis 
by appending a word via overlapped (n-1)-grams 
until the partial hypothesis reaches the sentence 
ending symbol. In each expanding step, the par-
tial hypotheses are pruned through a beam-search 
algorithm with scoring functions. 
Duchateau et al (2001) shows that the back-
ward language model contains information com-
plementary to the information in the forward 
language model. Hence, on top of the forward n-
gram expansion used in (Chen et al, 2008), we 
further introduce backward n-gram expansion to 
the hypothesis regeneration procedure. Backward 
n-gram expansion involves letting the partial hy-
potheses start from the last words that appeared 
in the translation N-best list and having the ex-
pansion go from right to left. 
Figure 1 gives an example of backward n-
gram expansion. The second row shows bi-grams 
which are extracted from the original hypotheses 
in the first row. The third row shows how a par-
tial hypothesis is expanded via backward n-gram 
expansion method. The fourth row lists some 
new hypotheses generated by backward n-gram 
expansion which do not exist in the original hy-
pothesis list. 
 
 
original 
 hypotheses 
about weeks' work . 
one week's work 
about one week's 
about a week work 
about one week work 
bi-grams about weeks', weeks' work, ?, 
about one, ?,  week work. 
backward 
n-gram 
 expansion 
partial hyp.    week's work 
n-gram one week's  
new partial hyp. one week's work 
 
 
new 
 hypotheses 
about one week's work 
about week's work 
one weeks' work . 
one week's work . 
one week's work . 
 
Figure 1: Example of original hypotheses; bi-grams 
collected from them; backward expanding a partial 
hypothesis via an overlapped n-1-gram; and new hy-
potheses generated through backward n-gram expan-
sion. 
2.2 Feature-based scoring functions 
To speed up the search, the partial hypotheses 
are pruned via beam-search in each expanding 
step. Therefore, the scoring functions applied 
with the beam-search algorithm are very impor-
tant. In (Chen et al, 2008), more than 10 addi-
tional global features are computed to rank the 
partial hypothesis list, and this is not an efficient 
way. In this paper, we propose to directly incor-
porate the evaluation metrics such as BLEU 
score to rank the candidates. The scoring func-
tions of this work are derived from the method of 
lattice Minimum Bayes-risk (MBR) decoding 
(Tromble et al, 2008) and fast consensus decod-
ing (DeNero et al, 2009), which were originally 
inspired from N-best MBR decoding (Kumar and 
Byrne, 2004). 
From a set of translation candidates E, MBR 
decoding chooses the translation that has the 
least expected loss with respect to other candi-
dates. Given a hypothesis set E, under the proba-
bility model )|( feP , MBR computes the trans-
lation e~  as follows: 
 
12
)|(),(minarg~ fePeeLe
EeEe
??= ?
???
        (1) 
 
where f is the source sentence, ),( eeL ?  is the loss 
function of two translations e and e? . 
Suppose that we are interested in maximizing 
the BLEU score (Papineni et al, 2002) to optim-
ize the translation performance. The loss func-
tion is defined as ),(1),( eeBLEUeeL ??=? ,  
then the MBR objective can be re-written as 
 
)|(),(maxarg~ fePeeBLEUe
EeEe
??= ?
???
         (2) 
 
E represents the space of the translations. For 
N-best MBR decoding, this space is the N-best 
list produced by a baseline decoder (Kumar and 
Byrne, 2004). For lattice MBR decoding, this 
space is the set of candidates encoded in the lat-
tice (Tromble et al, 2008). Here, with hypothesis 
regeneration, this space includes: 1) the transla-
tions produced by the baseline decoder either in 
an N-best list or encoded in a translation lattice, 
and 2) the translations created by hypothesis re-
generation. 
However, BLEU score is not linear with the 
length of the hypothesis, which makes the scor-
ing process for each expanding step of hypothe-
sis regeneration very slow. To further speed up 
the beam search procedure, we use an extension 
of a linear function of a Taylor approximation to 
the logarithm of corpus BLEU which was devel-
oped by (Tromble et al, 2008).  The original 
BLEU score of two hypotheses e and e? are 
computed as follows. 
 
)),(log(
4
1
exp(),(),(
4
1
?
=
???=?
n
n
eePeeeeBLEU ?    (3) 
 
where ),( eePn ?  is the precision of n-grams in the 
hypothesis e given e? and  ),( ee ??  is a brevity 
penalty. Let |e| denote the length of e. The corpus 
log-BLEU gain is defined as follows: 
 
)),(log(
4
1)||
||1,0min()),(log(
4
1
?
=
?+
?
?=?
n
n eeP
e
e
eeBLEU  (4) 
 
Therefore, the first-order Taylor approxima-
tion to the logarithm of corpus BLEU is shown 
in Equation (5). 
 
?
=
??+=?
4
1
0 ),(4
1||),(
n
nn
eeceeeG ??                    (5) 
where ),( eecn ? are the counts of the matched n-
grams and 
n?  ( 40 ?? n ) are constant weights 
estimated with held-out data.  
Suppose we have computed the expected n-
gram counts from the N-best list or translation 
forest. Then we may extend linear corpus BLEU 
in (5) to n-gram expectation-based linear corpus 
BLEU to score the partial hypotheses h. That is 
 
? ?
= ?
??+=
4
1
0 ),()],'([4
1||)',(
n Tt
nnn
n
thtecEhehG ???       (6) 
 
where ),( thn?  are n-gram indicator functions that 
equal 1 if n-gram t  appears in h  and 0 other-
wise; )],'([ tecE n  ( 41 ?? n ) are the real-valued 
n-gram expectations. Different from lattice MBR 
decoding, n-gram expectations in this work are 
computed over the original translation N-best list 
or translation forest; 
nT  ( 41 ?? n ) are the sets of 
n-grams collected from translation N-best list or 
translation forest. Then we make a further exten-
sion: the expectations of the n-gram counts for 
each expanding step are computed over the par-
tial translations. The lengths of all partial hypo-
theses are the same in each n-gram expanding 
step. For instance, in the 5th n-gram expanding 
step, the lengths of all the partial hypotheses are 
5 words. Therefore, we use n-gram count expec-
tations computed over partial original transla-
tions that only contain the first 5 words. The rea-
son is that this solution contains more informa-
tion about word orderings, since some n-grams 
appear more than others at the beginning of the 
translations while they may appear with the same 
or even lower frequencies than others in the full 
translations.  
Once the expanding process of hypothesis re-
generation is finished, we use a more precise 
BLEU metric to score all the translation candi-
dates. We extend BLEU score in (3) to n-gram 
expectation-based BLEU. That is: 
 
?
?
?
?
?
?
?
?
?
?
+???
?
???
?
?=
=
? ?
?
=
?
?
4
1 ),(
)]),'([),,(min(
log
4
1
||
|]'[|1,0minexp
)',()(
n
Tt
n
Tt
nn
n
n
thc
tecEthc
h
eE
ehBLEUhScore
                                                        (7) 
 
where ),( thcn  is the count of  n-gram t in the 
hypothesis h. The step of choosing the final 
translation is the same as fast consensus decod-
ing (DeNero et al, 2009): first we compute n-
13
gram feature expectations, and then we choose 
the translation that is most similar to the others 
via expected similarity according to feature-
based BLEU score as shown in (7). The differ-
ence is the space of translations: the space of fast 
consensus decoding is the same as MBR decod-
ing, while the space of hypothesis regeneration is 
enlarged by the new translations produced via n-
gram expansion. 
2.3 Fast consensus hypothesis regeneration 
We first generate two new hypothesis lists via 
forward and backward n-gram expansion using 
the scoring function in Equation (6). Then we 
choose a final translation using the scoring func-
tion in Equation (7) from the union of the origi-
nal hypotheses and newly generated hypotheses. 
The original hypotheses are from the N-best list 
or extracted from the translation forest. The new 
hypotheses are generated by forward or back-
ward n-gram expansion or are the union of both 
two new hypothesis lists (this is called ?bi-
directional n-gram expansion?). 
3 Experimental Results 
We carried out experiments based on translation 
N-best lists generated by a state-of-the-art 
phrase-based statistical machine translation sys-
tem, similar to (Koehn et al, 2007). In detail, the 
phrase table is derived from merged counts of 
symmetrized IBM2 and HMM alignments; the 
system has both lexicalized and distance-based 
distortion components (there is a 7-word distor-
tion limit) and employs cube pruning (Huang and 
Chiang, 2007). The baseline is a log-linear fea-
ture combination that includes language models, 
the distortion components, translation model, 
phrase and word penalties. Weights on feature 
functions are found by lattice MERT (Macherey 
et al, 2008). 
3.1 Data 
We evaluated with different language pairs: Chi-
nese-to-English, and German-to-English. Chi-
nese-to-English tasks are based on training data 
for the NIST 1  2009 evaluation Chinese-to-
English track. All the allowed bilingual corpora 
have been used for estimating the translation 
model. We trained two language models: the first 
one is a 5-gram LM which is estimated on the 
target side of the parallel data. The second is a 5-
                                               
1
 http://www.nist.gov/speech/tests/mt 
gram LM trained on the so-called English Giga-
word corpus. 
 
   Chi Eng 
Parallel 
Train 
Large 
Data 
|S| 10.1M 
|W| 270.0M 279.1M 
   Dev |S| 1,506 1,506?4 
Test NIST06 |S| 1,664 1,664?4 
NIST08 |S| 1,357 1,357?4 
Gigaword |S| - 11.7M 
 
Table 1: Statistics of training, dev, and test sets for 
Chinese-to-English task. 
 
We carried out experiments for translating 
Chinese to English. We first created a develop-
ment set which used mainly data from the NIST 
2005 test set, and also some balanced-genre web-
text from the NIST training material. Evaluation 
was performed on the NIST 2006 and 2008 test 
sets. Table 1 gives figures for training, develop-
ment and test corpora; |S| is the number of the 
sentences, and |W| is the size of running words. 
Four references are provided for all dev and test 
sets. 
For German-to-English tasks, we used WMT 
20062 data sets. The parallel training data con-
tains about 1 million sentence pairs and includes 
21 million target words; both the dev set and test 
set contain 2000 sentences; one reference is pro-
vided for each source input sentence. Only the 
target-language half of the parallel training data 
are used to train the language model in this task. 
3.2 Results 
Our evaluation metric is IBM BLEU (Papineni et 
al., 2002), which performs case-insensitive 
matching of n-grams up to n = 4.  
Our first experiment was carried out over 
1000-best lists on Chinese-to-English task. For 
comparison, we also conducted experiments with 
rescoring (two-pass) and three-pass hypothesis 
regeneration with only forward n-gram expan-
sion as proposed in (Chen et al, 2008). In the 
?rescoring? and ?three-pass? systems, we used 
the same rescoring model. There are 21 rescoring 
features in total, mainly translation lexicon 
scores from IBM and HMM models, posterior 
probabilities for words, n-grams, and sentence 
length, and language models, etc. For a complete 
description, please refer to (Ueffing et al, 2007). 
The results in BLEU-4 are reported in Table 2. 
 
                                               
2
 http://www.statmt.org/wmt06/ 
14
testset NIST?06 NIST?08 
baseline 35.70 28.60 
rescoring 36.01 28.97 
three-pass 35.98 28.99 
FCD 36.00 29.10 
Fwd. 36.13 29.19 
Bwd. 36.11 29.20 
Bid. 36.20 29.28 
 
Table 2: Translation performances in BLEU-4(%) 
over 1000-best lists for Chinese-to-English task: ?res-
coring? represents the results of rescoring; ?three-
pass?, three-pass hypothesis regeneration with for-
ward n-gram expansion; ?FCD?, fast consensus de-
coding; ?Fwd?, the results of hypothesis regeneration 
with forward n-gram expansion; ?Bwd?, backward n-
gram expansion; and ?Bid?, bi-directional n-gram 
expansion. 
 
Firstly, rescoring improved performance over 
the baseline by 0.3-0.4 BLEU point. Three-pass 
hypothesis regeneration with only forward n-
gram expansion (?three-pass? in Table 2) ob-
tained almost the same improvements as rescor-
ing. Three-pass hypothesis regeneration exploits 
more hypotheses than rescoring, while rescoring 
involves more scoring feature functions than the 
former. They reached a balance in this experi-
ment. Then, fast consensus decoding (?FCD? in 
Table 2) obtains 0.3-0.5 BLEU point improve-
ments over the baseline. Both forward and back-
ward n-gram expansion (?Fwd.? and ?Bwd.? in 
Table 2) improved about 0.1 BLEU point over 
the results of consensus decoding. Fast consen-
sus hypothesis regeneration (Fwd. and Bwd. in 
Table 2) got better improvements than three-pass 
hypothesis regeneration (?three-pass? in Table 2) 
by 0.1-0.2 BLEU point. Finally, combining hy-
pothesis lists from forward and backward n-gram 
expansion (?Bid.? in Table 2), further slight 
gains were obtained. 
 
testset Average time 
three-pass 3h 54m 
Fwd. 25m 
Bwd. 28m 
Bid. 40m 
 
Table 3: Average processing time of NIST?06 and 
NIST?08 test sets used in different systems. Times 
include n-best list regeneration and re-ranking. 
 
Moreover, fast consensus hypothesis regenera-
tion is much faster than the three-pass one, be-
cause the former only needs to compute one fea-
ture, while the latter needs to compute more than 
20 additional features. In this experiment, the 
former is about 10 times faster than the latter in 
terms of processing time, as shown in Table 3. 
 
In our second experiment, we set the size of 
N-best list N equal to 10,000 for both Chinese-to-
English and German-to-English tasks. The re-
sults are reported in Table 4. The same trend as 
in the first experiment can also be observed in 
this experiment. It is worth noticing that enlarg-
ing the size of the N-best list from 1000 to 
10,000 did not change the performance signifi-
cantly. Bi-directional n-gram expansion obtained 
improvements of 0.24 BLEU-score for WMT 
2006 de-en test set; 0.55 for NIST 2006 test set; 
and 0.72 for NIST 2008 test set over the base-
line. 
 
Lang. ch-en de-en 
testset NIST?06 NIST?08 Test2006 
baseline 35.70 28.60 26.92 
FCD 36.03 29.08 27.03 
Fwd. 36.16 29.25 27.11 
Bwd. 36.17 29.22 27.12 
Bid. 36.25 29.32 27.16 
 
Table 4: Translation performances in BLEU-4 (%) 
over 10K-best lists. 
 
We then tested the effect of the extension ac-
cording to which the expectations over n-gram 
counts are computed on partial hypotheses rather 
than whole candidate translations as described in 
Section 2.2. As shown in Table 5, we got tiny 
improvements on both test sets by computing the 
expectations over n-gram counts on partial hypo-
theses. 
 
testset NIST?06 NIST?08 
full 36.11 29.14 
partial 36.13 29.19 
 
Table 5: Translation performances in BLEU-4 (%) 
over 1000-best lists for Chinese-to-English task: 
?full? represents expectations over n-gram counts that 
are computed on whole hypotheses; ?partial? 
represents expectations over n-gram counts that are 
computed on partial hypotheses. 
3.3 Discussion  
To speed up the search, the partial hypotheses in 
each expanding step are pruned. When pruning is 
applied, forward and backward n-gram expan-
sion would generate different new hypothesis 
lists. Let us look back at the example in Figure 1.  
15
Given 5 original hypotheses in Figure 1, if we set 
the beam size equal to 5 (the size of the original 
hypotheses), the forward and backward n-gram 
expansion generated different new hypothesis 
lists, as shown in Figure 2. 
 
forward backward 
one week's work . 
about week's work 
one week's work . 
about one week's work 
 
Figure 2: Different new hypothesis lists generated by 
forward and backward n-gram expansion. 
 
For bi-directional n-gram expansion, the cho-
sen translation for a source sentence comes from 
the decoder 94% of the time for WMT 2006 test 
set, 90% for NIST test sets; it comes from for-
ward n-gram expansion 2% of the time for WMT 
2006 test set, 4% for NIST test sets; it comes 
from backward n-gram expansion 4% of the time 
for WMT 2006 test set, 6% for NIST test sets. 
This proves bidirectional n-gram expansion is a 
good way of enlarging the search space. 
4 Conclusions and Future Work 
We have proposed a fast consensus hypothesis 
regeneration approach for machine translation. It 
combines the advantages of feature-based con-
sensus decoding and hypothesis regeneration. 
This approach is more efficient than previous 
work on hypothesis regeneration, and it explores 
a wider search space than consensus decoding, 
resulting in improved performance.  Experiments 
showed consistent improvements across lan-
guage pairs. 
Instead of N-best lists, translation lattices or 
forests have been shown to be effective for MBR 
decoding (Zhang and Gildea, 2008; Tromble et 
al., 2008), and DeNero et al (2009) showed how 
to compute expectations of n-grams from a trans-
lation forest. Therefore, our future work may 
involve hypothesis regeneration using an n-gram 
language model trained on the translation forest. 
References  
B. Chen, M. Federico and M. Cettolo. 2007. Better N-
best Translations through Generative n-gram Lan-
guage Models. In: Proceedings of MT Summit XI. 
Copenhagen, Denmark. September. 
B. Chen, M. Zhang, A. Aw, and H. Li. 2008. Regene-
rating Hypotheses for Statistical Machine Transla-
tion. In: Proceedings of COLING. pp105-112. 
Manchester, UK, August. 
J. DeNero, D. Chiang and K. Knight. 2009. Fast Con-
sensus Decoding over Translation Forests. In: Pro-
ceedings of ACL. Singapore, August. 
J. Duchateau, K. Demuynck, and P. Wambacq. 2001. 
Confidence scoring based on backward language 
models. In: Proceedings of ICASSP 2001. Salt 
Lake City, Utah, USA, May. 
L. Huang and D. Chiang. 2007. Forest Rescoring: 
Faster Decoding with Integrated Language Models. 
In: Proceedings of ACL. pp. 144-151, Prague, 
Czech Republic, June.  
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and 
E. Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In: Proceedings of 
ACL. pp. 177-180, Prague, Czech Republic. 
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk 
decoding for statistical machine translation. In: 
Proceedings of NAACL. Boston, MA, May. 
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 
2008. Lattice-based Minimum Error Rate Training 
for Statistical Machine Translation. In: Proceed-
ings of EMNLP. pp. 725-734, Honolulu, USA, 
October. 
F. Och. 2003. Minimum error rate training in statistic-
al machine translation. In: Proceedings of ACL. 
Sapporo, Japan. July. 
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. 
Eng, V. Jain, Z. Jin, and D. Radev. 2004. A Smor-
gasbord of Features for Statistical Machine Trans-
lation. In: Proceedings of NAACL. Boston. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
BLEU: A method for automatic evaluation of ma-
chine translation. In: Proceedings of the ACL 2002. 
R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 
2008. Lattice minimum Bayes-risk decoding for 
statistical machine translation. In: Proceedings of 
EMNLP. Hawaii, US. October. 
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.  
2007. NRC?s Portage system for WMT 2007. In: 
Proceedings of ACL Workshop on SMT. Prague, 
Czech Republic, June. 
H. Zhang and D. Gildea. 2008. Efficient multipass 
decoding for synchronous context free grammars. 
In: Proceedings of ACL. Columbus, US. June. 
16
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127?132,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Lessons from NRC?s Portage System at WMT 2010 
 
 
Samuel Larkin, Boxing Chen, George Foster, Ulrich Germann, Eric Joanis, 
Howard Johnson, and Roland Kuhn  
National Research Council of Canada (NRC) 
Gatineau, Qu?bec, Canada. 
Firstname.Lastname@cnrc-nrc.gc.ca 
 
  
 
Abstract 
 
NRC?s Portage system participated in the Eng-
lish-French (E-F) and French-English (F-E) 
translation tasks of the ACL WMT 2010 eval-
uation. The most notable improvement over 
earlier versions of Portage is an efficient im-
plementation of lattice MERT. While Portage 
has typically performed well in Chinese to 
English MT evaluations, most recently in the 
NIST09 evaluation, our participation in WMT 
2010 revealed some interesting differences be-
tween Chinese-English and E-F/F-E transla-
tion, and alerted us to certain weak spots in 
our system. Most of this paper discusses the 
problems we found in our system and ways of 
fixing them. We learned several lessons that 
we think will be of general interest.  
1 Introduction 
Portage, the statistical machine translation sys-
tem of the National Research Council of Canada 
(NRC), is a two-pass phrase-based system. The 
translation tasks to which it is most often applied 
are Chinese to English, English to French (hen-
ceforth ?E-F?), and French to English (hence-
forth ?F-E?): in recent years we worked on Chi-
nese-English translation for the GALE project 
and for NIST evaluations, and English and 
French are Canada?s two official languages. In 
WMT 2010, Portage scored 28.5 BLEU (un-
cased) for F-E, but only 27.0 BLEU (uncased) 
for E-F. For both language pairs, Portage tru-
ecasing caused a loss of 1.4 BLEU; other WMT 
systems typically lost around 1.0 BLEU after 
truecasing. In Canada, about 80% of translations 
between English and French are from English to 
French, so we would have preferred better results 
for that direction. This paper first describes the 
version of Portage that participated in WMT 
2010. It then analyzes problems with the system 
and describes the solutions we found for some of 
them.  
2 Portage system description 
2.1 Core engine and training data 
The NRC system uses a standard two-pass 
phrase-based approach. Major features in the 
first-pass loglinear model include phrase tables 
derived from symmetrized IBM2 alignments and 
symmetrized HMM alignments, a distance-based 
distortion model, a lexicalized distortion model, 
and language models (LMs) that can be either 
static or else dynamic mixtures. Each phrase ta-
ble used was a merged one, created by separately 
training an IBM2-based and an HMM-based 
joint count table on the same data and then add-
ing the counts. Each includes relative frequency 
estimates and lexical estimates (based on Zens 
and Ney, 2004) of forward and backward condi-
tional probabilities. The lexicalized distortion 
probabilities are also obtained by adding IBM2 
and HMM counts. They involve 6 features (mo-
notone, swap and discontinuous features for fol-
lowing and preceding phrase) and are condi-
tioned on phrase pairs in a model similar to that 
of Moses (Koehn et al, 2005); a MAP-based 
backoff smoothing scheme is used to combat 
data sparseness when estimating these probabili-
ties. Dynamic mixture LMs are linear mixtures 
of ngram models trained on parallel sub-corpora 
with weights set to minimize perplexity of the 
current source text as described in (Foster and 
Kuhn, 2007); henceforth, we?ll call them ?dy-
namic LMs?.  
Decoding uses the cube-pruning algorithm of 
(Huang and Chiang, 2007) with a 7-word distor-
tion limit. Contrary to the usual implementation 
of distortion limits, we allow a new phrase to end 
127
more than 7 words past the first non-covered 
word, as long as the new phrase starts within 7 
words from the first non-covered word. Notwith-
standing the distortion limit, contiguous phrases 
can always be swapped. Out-of-vocabulary 
(OOV) source words are passed through un-
changed to the target. Loglinear weights are 
tuned with Och's max-BLEU algorithm over lat-
tices (Macherey et al, 2008); more details about 
lattice MERT are given in the next section. The 
second pass rescores 1000-best lists produced by 
the first pass, with additional features including 
various LM and IBM-model probabilities; ngram, 
length, and reordering posterior probabilities and 
frequencies; and quote and parenthesis mismatch 
indicators. To improve the quality of the maxima 
found by MERT when using large sets of partial-
ly-overlapping rescoring features, we use greedy 
feature selection, first expanding from a baseline 
set, then pruning. 
We restricted our training data to data that was 
directly available through the workshop's web-
site; we didn?t use the LDC resources mentioned 
on the website (e.g., French Gigaword, English 
Gigaword). Below, ?mono? refers to all mono-
lingual data (Europarl, news-commentary, and 
shuffle); ?mono? English is roughly three times 
bigger than ?mono? French (50.6 M lines in 
?mono? English, 17.7 M lines in ?mono? French). 
?Domain? refers to all WMT parallel training 
data except GigaFrEn (i.e., Europarl, news-
commentary, and UN).   
2.2 Preprocessing and postprocessing 
We used our own English and French pre- and 
post-processing tools, rather than those available 
from the WMT web site. For training, all English 
and French text is tokenized with a language-
specific tokenizer and then mapped to lowercase. 
Truecasing uses an HMM approach, with lexical 
probabilities derived from ?mono? and transition 
probabilities from a 3-gram LM trained on tru-
ecase ?mono?. A subsequent rule-based pass ca-
pitalizes sentence-initial words. A final detokeni-
zation step undoes the tokenization. 
2.3 System configurations for WMT 2010 
In the weeks preceding the evaluation, we tried 
several ways of arranging the resources available 
to us. We picked the configurations that gave the 
highest BLEU scores on WMT2009 Newstest. 
We found that tuning with lattice MERT rather 
than N-best MERT allowed us to employ more 
parameters and obtain better results.  
E-F system components: 
1. Phrase table trained on ?domain?;  
2. Phrase table trained on GigaFrEn;  
3. Lexicalized distortion model trained on 
?domain?;  
4. Distance-based distortion model; 
5. 5-gram French LM trained on ?mono?;  
6. 4-gram LM trained on French half of 
GigaFrEn;  
7. Dynamic LM composed of 4 LMs, each 
trained on the French half of a parallel 
corpus (5-gram LM trained on ?domain?, 
4-gram LM on GigaFrEn, 5-gram LM on 
news-commentary and 5-gram LM on 
UN). 
 
The F-E system is a mirror image of the E-F sys-
tem.  
3 Details of lattice MERT (LMERT) 
Our system?s implementation of LMERT (Ma-
cherey et al, 2008) is the most notable recent 
change in our system. As more and more features 
are included in the loglinear model, especially if 
they are correlated, N-best MERT (Och, 2003) 
shows more and more instability, because of 
convergence to local optima (Foster and Kuhn, 
2009). We had been looking for methods that 
promise more stability and better convergence. 
LMERT seemed to fit the bill. It optimizes over 
the complete lattice of candidate translations af-
ter a decoding run. This avoids some of the prob-
lems of N-best lists, which lack variety, leading 
to poor local optima and the need for many de-
coder runs. 
Though the algorithm is straightforward and is 
highly parallelizable, attention must be paid to 
space and time resource issues during implemen-
tation. Lattices output by our decoder were large 
and needed to be shrunk dramatically for the al-
gorithm to function well. Fortunately, this could 
be achieved via the finite state equivalence algo-
rithm for minimizing deterministic finite state 
machines. The second helpful idea was to sepa-
rate out the features that were a function of the 
phrase associated with an arc (e.g., translation 
length and translation model probability fea-
tures). These features could then be stored in a 
smaller phrase-feature table. Features associated 
with language or distortion models could be han-
dled in a larger transition-feature table. 
The above ideas, plus careful coding of data 
structures, brought the memory footprint down 
sufficiently to allow us to use complete lattices 
from the decoder and optimize over the complete 
128
development set for NIST09 Chinese-English. 
However, combining lattices between decoder 
runs again resulted in excessive memory re-
quirements. We achieved acceptable perfor-
mance by searching only the lattice from the lat-
est decoder run; perhaps information from earlier 
runs, though critical for convergence in N-best 
MERT, isn?t as important for LMERT.  
Until a reviewer suggested it, we had not 
thought of pruning lattices to a specified graph 
density as a solution for our memory problems. 
This is referred to in a single sentence in (Ma-
cherey et al, 2008), which does not specify its 
implementation or its impact on performance, 
and is an option of OpenFst (we didn?t use 
OpenFst). We will certainly experiment with lat-
tice pruning in future.  
Powell's algorithm (PA), which is at the core 
of MERT, has good convergence when features 
are mostly independent and do not depart much 
from a simple coordinate search; it can run into 
problems when there are many correlated fea-
tures (as with multiple translation and language 
models). Figure 1 shows the kind of case where 
PA works well. The contours of the function be-
ing optimized are relatively smooth, facilitating 
learning of new search directions from gradients. 
Figure 2 shows a more difficult case: there is 
a single optimum, but noise dominates and PA 
has difficulty finding new directions. Search of-
ten iterates over the original co-ordinates, miss-
ing optima that are nearby but in directions not 
discoverable from local gradients. Probes in ran-
dom directions can do better than iteration over 
the same directions (this is similar to the method 
proposed for N-best MERT by Cer et al, 2008). 
Each 1-dimensional MERT optimization is exact, 
so if our probe stabs a region with better scores, 
it will be discovered. Figures 1 and 2 only hint 
at the problem: in reality, 2-dimensional search 
isn?t a problem. The difficulties occur as the di-
mension grows: in high dimensions, it is more 
important to get good directions and they are 
harder to find. 
For WMT 2010, we crafted a compromise 
with the best properties of PA, yet alowing for a 
more aggressive search in more directions. We 
start with PA. As long as PA is adding new di-
rection vectors, it is continued. When PA stops 
adding new directions, random rotation (ortho-
gonal transformation) of the coordinates is per-
formed and PA is restarted in the new space. PA 
almost always fails to introduce new directions 
within the new coordinates, then fails again, so 
another set of random coordinates is chosen. This 
process repeats until convergence. In future 
work, we will look at incorporating random res-
tarts into the algorithm as additional insurance 
against premature convergence.  
Our LMERT implementation has room for 
improvement: it may still run into over-fitting 
problems with many correlated features. Howev-
er, during preparation for the evaluation, we no-
ticed that LMERT converged better than N-best 
MERT, allowing models with more features and 
higher BLEU to be chosen.  
After the WMT submission, we discovered 
that our LMERT implementation had a bug; our 
submission was tuned with this buggy LMERT. 
Comparison between our E-F submission tuned 
with N-best MERT and the same system tuned 
with bug-fixed LMERT shows BLEU gains of 
+1.5-3.5 for LMERT (on dev, WMT2009, and 
WMT2010, with no rescoring). However, N-best 
MERT performed very poorly in this particular 
case; we usually obtain a gain due to LMERT of 
+0.2-1.0 (e.g., for the submitted F-E system).  
 
 
Figure 1: Convergence for PA (Smooth Feature 
Space)  
 
 
Figure 2: Convergence for PA with Random Rotation 
(Rough Feature Space) 
129
4 Problems and Solutions 
4.1 Fixing LMERT  
Just after the evaluation, we noticed a discrepan-
cy for E-F between BLEU scores computed dur-
ing LMERT optimization and scores from the 1-
best list immediately after decoding. Our 
LMERT code had a bug that garbled any ac-
cented word in the version of the French refer-
ence in memory; previous LMERT experiments 
had English as target language, so the bug hadn?t 
showed up. The bug didn?t affect characters in 
the 7-bit ASCII set, such as English ones, only 
accented characters. Words in candidate transla-
tions were not garbled, so correct translations 
with accents received a lower BLEU score than 
they should have. As Table 1 shows, this bug 
cost us about 0.5 BLEU for WMT 2010 E-F after 
rescoring (according to NRC?s internal version 
of BLEU, which differs slightly from WMT?s 
BLEU). Despite this bug, the system tuned with 
buggy LMERT (and submitted) was still better 
than the best system we obtained with N-best 
MERT. The bug didn?t affect F-E scores.  
 
 Dev WMT2009 WMT2010 
LMERT (bug) 25.26 26.85 27.55 
LMERT 
 (no bug) 
25.43 26.89 28.07 
 
Table 1: LMERT bug fix (E-F BLEU after rescoring) 
4.2 Fixing odd translations 
After the evaluation, we carefully studied the 
system outputs on the WMT 2010 test data, par-
ticularly for E-F. Apart from truecasing errors, 
we noticed two kinds of bad behaviour: transla-
tions of proper names and apparent passthrough 
of English words to the French side.  
Examples of E-F translations of proper names 
from our WMT 2010 submission (each from a 
different sentence): 
 
Mr. Onderka ? M. Roman, Luk?? Marvan ? G. 
Luk??, Janey ? The, Janette Tozer ? Janette, 
Aysel Tugluk ? joints tugluk, Tawa Hallae ? 
Ottawa, Oleson ?  production,  Alcobendas ?  ; 
 
When the LMERT bug was fixed, some but 
not all of these bad translations were corrected 
(e.g., 3 of the 8 examples above were corrected). 
Our system passes OOV words through un-
changed. Thus, the names above aren?t OOVs, 
but words that occur rarely in the training data, 
and for which bad alignments have a dispropor-
tionate effect. We realized that when a source 
word begins with a capital, that may be a signal 
that it should be passed through. We thus de-
signed a passthrough feature function that applies 
to all capitalized forms not at the start of a sen-
tence (and also to forms at the sentence start if 
they?re capitalized elsewhere). Sequences of one 
or more capitalized forms are grouped into a 
phrase suggestion (e.g., Barack Obama ? bar-
rack obama) which competes with phrase table 
entries and is assigned a weight by MERT. 
The passthrough feature function yields a tiny 
improvement over the E-F system with the bug-
fixed LMERT on the dev corpus (WMT2008): 
+0.06 BLEU (without rescoring). It yields a larg-
er improvement on our test corpus: +0.27 BLEU 
(without rescoring). Furthermore, it corrects all 
the examples from the WMT 2010 test shown 
above (after the LMERT bug fix 5 of the 8 ex-
amples above still had problems, but when the 
passthrough function is incorporated all of them 
go away). Though the BLEU gain is small, we 
are happy to have almost eradicated this type of 
error, which human beings find very annoying.  
The opposite type of error is apparent pass-
through. For instance, ?we?re? appeared 12 times 
in the WMT 2010 test data, and was translated 6 
times into French as ?we?re? - even though better 
translations had higher forward probabilities. The 
source of the problem is the backward probabili-
ty P(E=?we?re?|F=?we?re?), which is 1.0; the 
backward probabilities for valid French transla-
tions of ?we?re? are lower. Because of the high 
probability P(E=?we?re?|F=?we?re?) within the 
loglinear combination, the decoder often chooses 
?we?re? as the French translation of ?we?re?. 
The (E=?we?re?, F=?we?re?) pair in WMT 
2010 phrase tables arose from two sentence pairs 
where the ?French? translation of an English sen-
tence is a copy of that English sentence. In both, 
the original English sentence contains ?we?re?. 
Naturally, the English words on the ?French? 
side are word-aligned with their identical twins 
on the English side. Generally, if the training 
data has sentence pairs where the ?French? sen-
tence contains words from the English sentence, 
those words will get high backward probabilities 
of being translated as themselves. This problem 
may not show up as an apparent passthrough; 
instead, it may cause MERT to lower the weight 
of the backward probability component, thus 
hurting performance.  
We estimated English contamination of the 
French side of the parallel training data by ma-
130
nually inspecting a random sample of ?French? 
sentences containing common English function 
words. Manual inspection is needed for accurate 
estimation: a legitimate French sentence might 
contain mostly English words if, e.g., it is short 
and cites the title of an English work (this 
wouldn?t count as contamination). The degree of 
contamination is roughly 0.05% for Europarl, 
0.5% for news-commentary, 0.5% for UN, and 
1% for GigaFrEn (in these corpora the French is 
also contaminated by other languages, particular-
ly German). Foreign contamination of English 
for these corpora appears to be much less fre-
quent.  
Contamination can take strange forms. We ex-
pected to see English sentences copied over in-
tact to the French side, and we did, but we did 
not expect to see so many ?French? sentences 
that interleaved short English word sequences 
with short French word sequences, apparently 
because text with an English and a French col-
umn had been copied by taking lines from alter-
nate columns. We found many of these inter-
leaved ?French? sentences, and found some of 
them in exactly this form on the Web (i.e., the 
corruption didn?t occur during WMT data collec-
tion). The details may not matter: whenever the 
?French? training sentence contains words from 
its English twin, there can be serious damage via 
backward probabilities. 
To test this hypothesis, we filtered all parallel 
and monolingual training data for the E-F system 
with a language guessing tool called text_cat 
(Cavnar and Trenkle, 1994). From parallel data, 
we filtered out sentence pairs whose French side 
had a high probability of not being French; from 
LM training data, sentences with a high non-
French probability. We set the filtering level by 
inspecting the guesser?s assessment of news-
commentary sentences, choosing a rather aggres-
sive level that eliminated 0.7% of news-
commentary sentence pairs. We used the same 
level to filter Europarl (0.8% of sentence pairs 
removed), UN (3.4%), GigaFrEn (4.7%), and 
?mono? (4.3% of sentences).  
 
 Dev WMT2009 WMT2010 
Baseline 25.23 26.47 27.72 
Filtered 25.45 26.66 27.98 
 
Table 2: Data filtering (E-F BLEU, no rescoring) 
 
Table 2 shows the results: a small but consis-
tent gain (about +0.2 BLEU without rescoring). 
We have not yet confirmed the hypothesis that 
copies of source-language words in the paired 
target sentence within training data can damage 
system performance via backward probabilities.  
4.3 Fixing problems with LM training   
Post-evaluation, we realized that our arrange-
ment of the training data for the LMs for both 
language directions was flawed. The grouping 
together of disparate corpora in ?mono? and 
?domain? didn?t allow higher-quality, truly in-
domain corpora to be weighted more heavily 
(e.g., the news corpora should have higher 
weights than Europarl, but they are lumped to-
gether in ?mono?). There are also potentially 
harmful overlaps between LMs (e.g., GigaFrEn 
is used both inside and outside the dynamic LM).  
We trained a new set of French LMs for the E-
F system, which replaced all the French LMs 
(#5-7) described in section 2.3 in the E-F system: 
1. 5-gram LM trained on news-commentary 
and shuffle;  
2. Dynamic LM based on 4 5-gram LMs 
trained on French side of parallel data 
(LM trained on GigaFrEn, LM on UN, 
LM on Europarl, and LM on news-
commentary). 
We did not apply the passthrough function or 
language filtering (section 4.2) to any of the 
training data for any component (LMs, TMs, dis-
tortion models) of this system; we did use the 
bug-fixed version of LMERT (section 4.1). 
The experiments with these new French LMs 
for the E-F system yielded a small decrease of 
NRC BLEU on dev (-0.15) and small increases 
on WMT Newstest 2009 and Newstest 2010 
(+0.2 and +0.4 respectively without rescoring). 
We didn?t do F-E experiments of this type.  
4.4 Pooling improvements   
The improvements above were (individual un-
cased E-F BLEU gains without rescoring in 
brackets): LMERT bug fix (about +0.5); pass-
through feature function (+0.1-0.3); language 
filtering for French (+0.2). There was also a 
small gain on test data by rearranging E-F LM 
training data, though the loss on ?dev? suggests 
this may be a statistical fluctuation. We built 
these four improvements into the evaluation E-F 
system, along with quote normalization: in all 
training and test data, diverse single quotes were 
mapped onto the ascii single quote, and diverse 
double quotes were mapped onto the ascii double 
quote. The average result on WMT2009 and 
WMT2010 was +1.7 BLEU points compared to 
the original system, so there may be synergy be-
131
tween the improvements. The original system 
had gained +0.3 from rescoring, while the final 
improved system only gained +0.1 from rescor-
ing: a post-evaluation rescored gain of +1.5.  
An experiment in which we dropped lexica-
lized distortion from the improved system 
showed that this component yields about +0.2 
BLEU. Much earlier, when we were still training 
systems with N-best MERT, incorporation of the 
6-feature lexicalized distortion often caused 
scores to go down (by as much as 2.8 BLEU). 
This illustrates how LMERT can make incorpo-
ration of many more features worthwhile.  
4.5 Fixing truecasing  
Our truecaser doesn?t work as well as truecasers 
of other WMT groups: we lost 1.4 BLEU by tru-
ecasing in both language directions, while others 
lost 1.0 or less. To improve our truecaser, we 
tried: 1. Training it on all relevant data and 2. 
Collecting 3-gram case-pattern statistics instead 
of unigrams. Neither of these helped significant-
ly. One way of improving the truecaser would be 
to let case information from source words influ-
ence the case of the corresponding target words. 
Alternatively, one of the reviewers stated that 
several labs involved in WMT have no separate 
truecaser and simply train on truecase text. We 
had previously tried this approach for NIST Chi-
nese-English and discarded it because of its poor 
performance. We are currently re-trying it on 
WMT data; if it works better than having a sepa-
rate truecaser, this was yet another area where 
lessons from Chinese-English were misleading. 
5 Lessons  
LMERT is an improvement over N-best MERT. 
The submitted system was one for which N-best 
MERT happened to work very badly, so we got 
ridiculously large gains of +1.5-3.5 BLEU for 
non-buggy LMERT over N-best MERT. These 
results are outliers: in experiments with similar 
configurations, we typically get +0.2-1.0 for 
LMERT over N-best MERT. Post-evaluation, 
four minor improvements ? a case-based pass-
through function, language filtering, LM rear-
rangement, and quote normalization ? collective-
ly gave a nice improvement. Nothing we tried 
helped truecaser performance significantly, 
though we have some ideas on how to proceed. 
We learned some lessons from WMT 2010. 
Always test your system on the relevant lan-
guage pair. Our original version of LMERT was 
developed on Chinese-English and worked well 
there, but had a bug that surfaced only when the 
target language had accents.  
European language pairs are more porous to 
information than Chinese-English. Our WMT 
system reflected design decisions for Chinese-
English, and thus didn?t exploit case information 
in the source: it passed through OOVs to the tar-
get, but didn?t pass through upper-case words 
that are likely to be proper nouns.  
It is beneficial to remove foreign-language 
contamination from the training data.  
When entering an evaluation one hasn?t parti-
cipated in for several years, always read system 
papers from the previous year. Some of the 
WMT 2008 system papers mention passthrough 
of some non-OOVs, filtering out of noisy train-
ing data, and using the case of a source word to 
predict the case of the corresponding target word. 
References  
William Cavnar and John Trenkle. 1994. N-Gram-
Based Text Categorization. Proc. Symposium on 
Document Analysis and Information Retrieval, 
UNLV Publications/Reprographics, pp. 161-175. 
Daniel Cer, Daniel Jurafsky, and Christopher D. 
Manning. 2008. Regularization and search for min-
imum error rate training. Proc. Workshop on 
SMT, pp. 26-34. 
George Foster and Roland Kuhn. 2009. Stabilizing 
Minimum Error Rate Training. Proc. Workshop 
on SMT, pp. 242-249. 
George Foster and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. Proc. Workshop on 
SMT, pp. 128-135. 
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language 
Models.  Proc. ACL, pp.  144-151. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Transcription Evalua-
tion. MT Eval. Workshop. 
Wolfgang Macherey, Franz Josef Och, Ignacio Thay-
er, and Jakob Uszkoreit. 2008. Lattice-based Min-
imum Error Rate Training for Statistical Machine-
Translation. Conf. EMNLP, pp. 725-734. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation.  Proc. ACL, 
pp. 160-167.  
Richard Zens and Hermann Ney. 2004. Improvements 
in Phrase-Based Statistical Machine Translation. 
Proc. HLT/NAACL, pp. 257-264. 
132
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 71?77,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
AMBER: A Modified BLEU, Enhanced Ranking Metric 
 
Boxing Chen and Roland Kuhn 
National Research Council of Canada, Gatineau, Qu?bec, Canada 
First.Last@nrc.gc.ca 
 
  
  
Abstract 
This paper proposes a new automatic ma-
chine translation evaluation metric: 
AMBER, which is based on the metric 
BLEU but incorporates recall, extra penal-
ties, and some text processing variants. 
There is very little linguistic information in 
AMBER. We evaluate its system-level cor-
relation and sentence-level consistency 
scores with human rankings from the 
WMT shared evaluation task; AMBER 
achieves state-of-the-art performance. 
1 Introduction 
Automatic evaluation metrics for machine transla-
tion (MT) quality play a critical role in the devel-
opment of statistical MT systems. Several metrics 
have been proposed in recent years.  Metrics such 
as BLEU (Papineni et al, 2002), NIST (Dodding-
ton, 2002), WER, PER, and TER (Snover et al, 
2006) do not use any linguistic information - they 
only apply surface matching. METEOR (Banerjee 
and Lavie, 2005), METEOR-NEXT (Denkowski 
and Lavie 2010), TER-Plus (Snover et al, 2009), 
MaxSim (Chan and Ng, 2008), and TESLA (Liu et 
al., 2010) exploit some limited linguistic resources, 
such as synonym dictionaries, part-of-speech tag-
ging or paraphrasing tables. More sophisticated 
metrics such as RTE (Pado et al, 2009) and DCU-
LFG (He et al, 2010) use higher level syntactic or 
semantic analysis to score translations. 
Though several of these metrics have shown bet-
ter correlation with human judgment than BLEU, 
BLEU is still the de facto standard evaluation me-
tric. This is probably due to the following facts: 
1. BLEU is language independent (except for 
word segmentation decisions).  
2. BLEU can be computed quickly. This is im-
portant when choosing a metric to tune an 
MT system. 
3. BLEU seems to be the best tuning metric 
from a quality point of view - i.e., models 
trained using BLEU obtain the highest 
scores from humans and even from other 
metrics (Cer et al, 2010). 
When we developed our own metric, we decided 
to make it a modified version of BLEU whose 
rankings of translations would (ideally) correlate 
even more highly with human rankings. Thus, our 
metric is called AMBER: ?A Modified Bleu, En-
hanced Ranking? metric. Some of the AMBER 
variants use an information source with a mild lin-
guistic flavour ? morphological knowledge about 
suffixes, roots and prefixes ? but otherwise, the 
metric is based entirely on surface comparisons.  
2 AMBER 
Like BLEU, AMBER is composed of two parts: a 
score and a penalty. 
 
penaltyscoreAMBER ?=                 (1)  
 
To address weaknesses of BLEU described in 
the literature (Callison-Burch et al, 2006; Lavie 
and Denkowski, 2009), we use more sophisticated 
formulae to compute the score and penalty. 
2.1 Enhancing the score 
First, we enrich the score part with geometric av-
erage of n-gram precisions (AvgP), F-measure de-
rived from the arithmetic averages of precision and 
recall (Fmean), and arithmetic average of F-
measure of precision and recall for each n-gram 
(AvgF). Let us define n-gram precision and recall 
as follows: 
71
)(#
)(#)(
Tngrams
RTngrams
np ?=               (2) 
  )(#
)(#)(
Rngrams
RTngrams
nr
?
=               (3) 
where T = translation, R = reference.  
Then the geometric average of n-gram preci-
sions AvgP, which is also the score part of the 
BLEU metric, is defined as: 
NN
n
npNAvgP
1
1
)()( ???
?
???
?
= ?
=
             (4) 
 
The arithmetic averages for n-gram precision 
and recall are: 
?
=
=
N
n
np
N
NP
1
)(1)(                       (5) 
?
=
=
M
n
nr
M
MR
1
)(1)(                      (6) 
 The F-measure that is derived from P(N) and 
R(M), (Fmean), is given by: 
)()1()(
)()(),,(
MRNP
MRNPMNFmean
??
?
?+
=     (7) 
 
The arithmetic average of F-measure of preci-
sion and recall for each n-gram (AvgF) is given by: 
?
=
?+
=
N
n nrnp
nrnp
N
NAvgF
1 )()1()(
)()(1),(
??
?    (8) 
The score is the weighted average of the three 
values: AvgP, Fmean, and AvgF. 
),()1(
),,(
)()(
21
2
1
???
??
?
NAvgF
MNFmean
NAvgPNscore
???+
?+
?=
  
 (9) 
The free parameters N, M,? , 1?  and 2?  were  
manually tuned on a dev set.  
2.2 Various penalties 
Instead of the original brevity penalty, we experi-
mented with a product of various penalties: 
?
=
=
P
i
w
i
ipenpenalty
1
                  (10) 
where wi is the weight of each penalty peni.  
Strict brevity penalty (SBP): (Chiang et al, 
2008) proposed this penalty. Let ti be the transla-
tion of input sentence i, and let ri be its reference 
(or if there is more than one, the reference whose 
length in words || ir  is closest to length || it ). Set 
???
?
???
?
?= ?
?
i ii
i i
rt
r
SBP |}||,min{|
||
1exp     (11) 
Strict redundancy penalty (SRP): long sen-
tences are preferred by recall. Since we rely on 
both recall and precision to compute the score, it is 
necessary to punish the sentences that are too long.  
???
?
???
?
?= ?
?
i i
i ii
r
rt
SRP ||
|}||,max{|
1exp      (12) 
Character-based strict brevity penalty 
(CSBP) and Character-based strict redundancy 
penalty (CSRP) are defined similarly. The only 
difference with the above two penalties is that 
here, length is measured in characters. 
Chunk penalty (CKP): the same penalty as in 
METEOR: 
?
? ???
?
???
?
??= )(#
#1
wordmatches
chunksCKP       (13) 
? and ?  are free parameters. We do not compute 
the word alignment between the translation and 
reference; therefore, the number of chunks is com-
puted as )(#)(## wordmatchesbigrammatcheschunks ?= . 
For example, in the following two-sentence trans-
lation (references not shown), let ?mi? stand for a 
matched word, ?x? stand for zero, one or more 
unmatched words:  
S1: m1 m2 x m3 m4 m5 x m6  
S2: m7 x m8 m9 x m10 m11 m12 x m13 
If we consider only unigrams and bigrams, there 
are 13 matched words and 6 matched bigrams (m1 
m2, m3 m4, m4 m5, m8 m9, m10 m11, m11 m12), so there 
are 13-6=7 chunks (m1 m2, m3 m4 m5, m6, m7, m8 m9, 
m10 m11 m12, m13).  
Continuity penalty (CTP): if all matched 
words are continuous, then 
segmentRTgramsn
RTngrams
#)()1(#
)(#
???
?
 equals 1.  
Example: 
S3: m1 m2 m3 m4 m5m6  
S4: m7 m8 m9m10 m11 m12 m13 
There are 13 matched unigrams, and 11 matched 
bi-grams; we get 11/(13-2)=1. Therefore, a conti-
nuity penalty is computed as: 
72
 ???
?
???
?
???
?
?
?= ?
=
N
n segmentRTgramsn
RTngrams
N
CTP
2 #)()1(#
)(#
1
1
exp (14) 
 
Short word difference penalty (SWDP): a 
good translation should have roughly the same 
number of stop words as the reference. To make 
AMBER more portable across all Indo-European 
languages, we use short words (those with fewer 
than 4 characters) to approximate the stop words.   
))(#
||
exp(
runigram
baSWDP ??=           (15) 
where a and b are the number of short words in the 
translation and reference respectively. 
Long word difference penalty (LWDP): is de-
fined similarly to SWDP.  
))(#
||
exp(
runigram
dcLWDP ??=           (15) 
where c and d  are the number of long words (those 
longer than 3 characters) in the translation and ref-
erence respectively. 
Normalized Spearman?s correlation penalty 
(NSCP): we adopt this from (Isozaki et al, 2010). 
This penalty evaluates similarity in word order be-
tween the translation and reference. We first de-
termine word correspondences between the 
translation and reference; then, we rank words by 
their position in the sentences. Finally, we compute 
Spearman?s correlation between the ranks of the n 
words common to the translation and reference. 
)1()1(1
2
?+
?=
?
nnn
d
i i?                (16) 
where di indicates the distance between the ranks 
of the i-th element. For example: 
T: Bob reading book likes
 
R: Bob likes reading book
 
 
The rank vector of the reference is [1, 2, 3, 4], 
while the translation rank vector is [1, 3, 4, 2]. The 
Spearman?s correlation score between these two 
vectors is 
)14(4)14(
)42()34()23(01
222
???+
?+?+?+
?
=0.90. 
In order to avoid negative values, we normalized 
the correlation score, obtaining the penalty NSCP: 
2)1( /?NSCP +=                     (17) 
Normalized Kendall?s correlation penalty 
(NKCP):  this is adopted from (Birch and Os-
borne, 2010) and (Isozaki et al, 2010). In the pre-
vious example, where the rank vector of the 
translation is [1, 3, 4, 2], there are 624 =C  pairs of 
integers. There are 4 increasing pairs: (1,3), (1,4), 
(1,2) and (3,4). Kendall?s correlation is defined by:  
1
#
#2 ??=
pairsall
pairsasingincre
?         (18) 
Therefore, Kendall?s correlation for the transla-
tion ?Bob reading book likes? is 16/42 ?? =0.33. 
Again, to avoid negative values, we normalized 
the coefficient score, obtaining the penalty NKCP: 
2)1( /NKCP ?+=                     (19) 
2.3 Term weighting 
The original BLEU metric weights all n-grams 
equally; however, different n-grams have different 
amounts of information. We experimented with 
applying tf-idf to weight each n-gram according to 
its information value. 
2.4 Four matching strategies 
In the original BLEU metric, there is only one 
matching strategy: n-gram matching. In AMBER, 
we provide four matching strategies (the best 
AMBER variant used three of these): 
1. N-gram matching: involved in computing 
precision and recall. 
2. Fixed-gap n-gram: the size of the gap be-
tween words ?word1 [] word2? is fixed; 
involved in computing precision only. 
3. Flexible-gap n-gram:  the size of the gap 
between words ?word1 * word2? is flexi-
ble; involved in computing precision only. 
4. Skip n-gram: as used ROUGE (Lin, 2004); 
involved in computing precision only. 
2.5 Input preprocessing 
The AMBER score can be computed with different 
types of preprocessing. When using more than one 
type, we computed the final score as an average 
over runs, one run per type (our default AMBER 
variant used three of the preprocessing types): 
?
=
=
T
t
tAMBER
T
AMBERFinal
1
)(1_
 
We provide 8 types of possible text input: 
0. Original - true-cased and untokenized. 
73
1. Normalized - tokenized and lower-cased.  
(All variants 2-7 below also tokenized and 
lower-cased.)  
2. ?Stemmed? - each word only keeps its first 
4 letters. 
3. ?Suffixed? - each word only keeps its last 
4 letters. 
4. Split type 1 - each longer-than-4-letter 
word is segmented into two sub-words, 
with one being the first 4 letters and the 
other the last 2 letters. If the word has 5 
letters, the 4th letter appears twice: e.g., 
?gangs? becomes ?gang? + ?gs?. If the 
word has more than 6 letters, the middle 
part is thrown away 
5. Split type 2 - each word is segmented into 
fixed-length (4-letter) sub-word sequences, 
starting from the left.  
6. Split type 3 - each word is segmented into 
prefix, root, and suffix. The list of English 
prefixes, roots, and suffixes used to split 
the word is from the Internet1; it is used to 
split words from all languages. Linguistic 
knowledge is applied here (but not in any 
other aspect of AMBER).  
7. Long words only - small words (those with 
fewer than 4 letters) are removed. 
3 Experiments 
3.1 Experimental data 
We evaluated AMBER on WMT data, using WMT 
2008 all-to-English submissions as the dev set. 
Test sets include WMT 2009 all-to-English, WMT 
2010 all-to-English and 2010 English-to-all sub-
missions. Table 1 summarizes the dev and test set 
statistics. 
Set Dev Test1 Test2 Test3 
Year 2008 2009 2010 2010 
Lang. xx-en xx-en xx-en en-xx 
#system 43 39 53 32 
#sent-pair 7,861 13,912 14,212 13,165 
Table 1: statistics of the dev and test sets. 
                                                           
1http://en.wikipedia.org/wiki/List_of_Greek_and_Latin_roots_
in_English 
3.2 Default settings 
Before evaluation, we manually tuned all free pa-
rameters on the dev set to maximize the system-
level correlation with human judgments and de-
cided on the following default settings for 
AMBER:   
1. The parameters in the formula  
),()1(
),,(
)()(
21
2
1
???
??
?
NAvgF
MNFmean
NAvgPNscore
???+
?+
?=
 
are set as  N=4, M=1, ? =0.9, 1? = 0.3 
and 2? = 0.5.  
2. All penalties are applied; the manually set 
penalty weights are shown in Table 2. 
3. We took the average of runs over input text 
types 1, 4, and 6 (i.e. normalized text, 
split type 1 and split type 3).  
4. In Chunk penalty (CKP), 3=? , and 
? =0.1. 
5. By default, tf-idf is not applied.  
6. We used three matching strategies: n-gram, 
fixed-gap n-gram, and flexible-gap n-
gram; they are equally weighted. 
 
Name of penalty Weight value 
SBP 0.30 
SRP 0.10 
CSBP 0.15 
CSRP 0.05 
SWDP 0.10 
LWDP 0.20 
CKP 1.00 
CTP 0.80 
NSCP 0.50 
NKCP 2.00 
Table 2: Weight of each penalty 
3.3 Evaluation metrics 
We used Spearman?s rank correlation coefficient to 
measure the correlation of AMBER with the hu-
man judgments of translation at the system level. 
The human judgment score we used is based on the 
?Rank? only, i.e., how often the translations of the 
system were rated as better than the translations 
from other systems (Callison-Burch et al, 2008). 
Thus, AMBER and the other metrics were eva-
luated on how well their rankings correlated with 
74
the human ones. For the sentence level, we use 
consistency rate, i.e., how consistent the ranking of 
sentence pairs is with the human judgments. 
3.4 Results 
All test results shown in this section are averaged 
over all three tests described in 3.1. First, we com-
pare AMBER with two of the most widely used 
metrics: original IBM BLEU and METEOR v1.0. 
Table 3 gives the results; it shows both the version 
of AMBER with basic preprocessing, AMBER(1) 
(with tokenization and lowercasing) and the default 
version used as baseline for most of our experi-
ments (AMBER(1,4,6)). Both versions of AMBER 
perform better than BLEU and METEOR on both 
system and sentence levels. 
 
Metric  
 Dev     3 tests average   ? tests 
BLEU_ibm 
(baseline) 
sys 
sent 
0.68            0.72               N/A 
0.37            0.40               N/A 
METEOR 
     v1.0 
sys 
sent 
0.80            0.80              +0.08 
0.58            0.56              +0.17 
AMBER(1) 
(basic preproc.) 
sys 
sent 
0.83            0.83              +0.11 
0.61            0.58              +0.19 
AMBER(1,4,6) 
(default)  
sys 
sent 
0.84            0.86              +0.14 
0.62            0.60              +0.20 
 
 Table 3: Results of AMBER vs BLEU and METEOR 
 
Second, as shown in Table 4, we evaluated the 
impact of different types of preprocessing, and 
some combinations of preprocessing (we do one 
run of evaluation for each type and average the 
results). From this table, we can see that splitting 
words into sub-words improves both system- and 
sentence-level correlation. Recall that input 6 pre-
processing splits words according to a list of Eng-
lish prefixes, roots, and suffixes: AMBER(4,6) is 
the best variant. Although test 3 results, for target 
languages other than English, are not broken out 
separately in this table,  they are as follows: input 1 
yielded 0.8345  system-level correlation and 
0.5848 sentence-level consistency, but input 6 
yielded 0.8766 (+0.04 gain) and 0.5990 (+0.01) 
respectively. Thus, surprisingly, splitting non-
English words up according to English morpholo-
gy helps performance, perhaps because French, 
Spanish, German, and even Czech share some 
word roots with English. However, as indicated by 
the underlined results, if one wishes to avoid the 
use of any linguistic information, AMBER(4) per-
forms almost as well as AMBER(4,6). The default 
setting, AMBER(1,4,6), doesn?t perform quite as 
well as AMBER(4,6) or AMBER(4), but is quite 
reasonable.  
Varying the preprocessing seems to have more 
impact than varying the other parameters we expe-
rimented with.  In Table 5, ?none+tf-idf? means 
we do one run without tf-idf and one run for ?tf-idf 
only?, and then average the scores. Here, applying 
tf-idf seems to benefit performance slightly. 
 
Input  
 Dev     3 tests average     ? tests 
0  
(baseline) 
sys 
sent 
0.84            0.79                 N/A 
0.59            0.58                 N/A 
1 sys 
sent 
0.83            0.83               +0.04 
0.61            0.58               +0.00 
2 sys 
sent 
0.83            0.84               +0.05 
0.61            0.59               +0.01 
3 sys 
sent 
0.83            0.84               +0.05 
0.61            0.58               +0.00 
4 sys 
sent 
0.84            0.87               +0.08 
0.62            0.60               +0.01 
5 sys 
sent 
0.82            0.86               +0.07 
0.61            0.56               +0.01 
6 sys 
sent 
0.83            0.88               +0.09 
0.62            0.60               +0.02 
7 sys 
sent 
0.34            0.56               -0.23 
0.58            0.53               -0.05 
1,4 sys 
sent 
0.84            0.85               +0.07 
0.62            0.60               +0.01 
4,6 sys 
sent 
0.83            0.88               +0.09 
0.62            0.60               +0.02 
1,4,6 sys 
sent 
0.84            0.86               +0.07 
0.62            0.60               +0.02 
 
Table 4: Varying AMBER preprocessing (best  
linguistic = bold, best non-ling. = underline) 
 
tf-idf  
  Dev     3 tests average    ? tests 
none 
(baseline) 
sys 
sent 
0.84             0.86                N/A 
0.62             0.60                N/A 
tf-idf 
only 
sys 
sent 
0.81             0.88              +0.02 
0.62             0.61              +0.01 
none+tf-
idf 
sys 
sent 
0.82             0.87              +0.01 
0.62             0.61              +0.01 
 
Table 5: Effect of tf-idf on AMBER(1,4,6) 
 
Table 6 shows what happens if you disable one 
penalty at a time (leaving the weights of the other 
penalties at their original values). The biggest sys-
tem-level performance degradation occurs when 
LWDP is dropped, so this seems to be the most 
75
useful penalty. On the other hand, dropping CKP, 
CSRP, and SRP may actually improve perfor-
mance. Firm conclusions would require retuning of 
weights each time a penalty is dropped; this is fu-
ture work.  
 
Penalties  
  Dev     3 tests average    ? tests 
All 
(baseline) 
sys 
sent 
0.84            0.86               N/A 
0.62            0.60               N/A 
-SBP sys 
sent 
0.82            0.84               -0.02 
0.62            0.60               -0.00 
-SRP sys 
sent 
0.83            0.88              +0.01 
0.62            0.60              +0.00 
-CSBP sys 
sent 
0.84            0.85               -0.01 
0.62            0.60              +0.00 
-CSRP sys 
sent 
0.83            0.87              +0.01 
0.62            0.60               -0.00 
-SWDP sys 
sent 
0.84            0.86               -0.00 
0.62            0.60              +0.00 
-LWDP sys 
sent 
0.83            0.83               -0.03 
0.62            0.60               -0.00 
-CTP sys 
sent 
0.82            0.84               -0.02 
0.62            0.60               -0.00 
-CKP sys 
sent 
0.83            0.87              +0.01 
0.62            0.60               -0.00 
-NSCP sys 
sent 
0.83            0.86               -0.00 
0.62            0.60              +0.00 
-NKCP sys 
sent 
0.82            0.85               -0.01 
0.62            0.60              +0.00 
 
Table 6: Dropping penalties from AMBER(1,4,6) ? 
biggest drops on test in bold 
 
Matching  
 Dev     3 tests avg     ? tests 
n-gram + fxd-
gap+ flx-gap 
(default) 
sys 
sent 
0.84             0.86         N/A 
0.62             0.60         N/A 
n-gram sys 
sent 
0.84             0.86         -0.00 
0.62             0.60         -0.00 
fxd-gap+ 
 n-gram 
sys 
sent 
0.84             0.86         -0.00 
0.62             0.60         -0.00 
flx-gap+ 
 n-gram 
sys 
sent 
0.83             0.86         -0.00 
0.62             0.60         -0.00 
skip+ 
 n-gram 
sys 
sent 
0.83             0.85         -0.01 
0.62             0.60         -0.00 
All four 
matchings 
sys 
sent 
0.83             0.86         -0.01 
0.62             0.60          0.00 
Table 7: Varying matching strategy for AMBER(1,4,6) 
 
Finally, we evaluated the effect of the matching 
strategy. According to the results shown in Table 
7, our default strategy, which uses three of the four 
types of matching (n-grams, fixed-gap n-grams, 
and flexible-gap n-grams) is close to optimal;  the 
use of skip n-grams (either by itself or in combina-
tion) may hurt performance at both system and 
sentence levels.  
4 Conclusion 
This paper describes AMBER, a new machine 
translation metric that is a modification of the 
widely used BLEU metric. We used more sophisti-
cated formulae to compute the score, we developed 
several new penalties to match the human judg-
ment, we tried different preprocessing types, we 
tried tf-idf, and we tried four n-gram matching 
strategies. The choice of preprocessing type 
seemed to have the biggest impact on performance. 
AMBER(4,6) had the best performance of any va-
riant we tried. However, it has the disadvantage of 
using some light linguistic knowledge about Eng-
lish morphology (which, oddly, seems to be help-
ful for other languages too). A purist may prefer 
AMBER(1,4) or AMBER(4), which use no linguis-
tic information and still match human judgment 
much more closely than either BLEU or 
METEOR. These variants of AMBER share 
BLEU?s virtues: they are language-independent 
and can be computed quickly. 
Of course, AMBER could incorporate more lin-
guistic information: e.g., we could use linguistical-
ly defined stop word lists in the SWDP and LWDP 
penalties, or use synonyms or paraphrasing in the 
n-gram matching.  
AMBER can be thought of as a weighted com-
bination of dozens of computationally cheap fea-
tures based on word surface forms for evaluating 
MT quality. This paper has shown that combining 
such features can be a very effective strategy for 
attaining better correlation with human judgment. 
Here, the weights on the features were manually 
tuned; in future work, we plan to learn weights on 
features automatically. We also plan to redesign 
AMBER so that it becomes a metric that is highly 
suitable for tuning SMT systems. 
References 
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of ACL 
Workshop on Intrinsic & Extrinsic Evaluation Meas-
ures for Machine Translation and/or Summarization. 
76
A. Birch and M. Osborne. 2010. LRscore for evaluating 
lexical and reordering quality in MT. In Proceedings 
of the Joint Fifth Workshop on Statistical Machine 
Translation and MetricsMATR, pages 302?307.  
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz and 
J. Schroeder. 2008. Further Meta-Evaluation of Ma-
chine Translation. In Proceedings of WMT. 
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. 
Re-evaluating the role of BLEU in machine transla-
tion research. In Proceedings of EACL. 
D. Cer, D. Jurafsky and C. Manning. 2010. The Best 
Lexical Metric for Phrase-Based Statistical MT Sys-
tem Optimization. In Proceedings of NAACL. 
Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maximum 
similarity metric for machine translation evaluation. 
In Proceedings of ACL. 
D. Chiang, S. DeNeefe, Y. S. Chan, and H. T. Ng. 2008. 
Decomposability of translation metrics for improved 
evaluation and efficient algorithms. In Proceedings 
of EMNLP, pages 610?619. 
M. Denkowski and A. Lavie. 2010. Meteor-next and the 
meteor paraphrase tables: Improved evaluation sup-
port for five target languages. In Proceedings of the 
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 314?317. 
George Doddington. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of HLT. 
Y. He, J. Du, A. Way, and J. van Genabith. 2010. The 
DCU dependency-based metric in WMT-
MetricsMATR 2010. In Proceedings of the Joint 
Fifth Workshop on Statistical Machine Translation 
and MetricsMATR, pages 324?328.  
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada. 
2010. Automatic Evaluation of Translation Quality 
for Distant Language Pairs. In Proceedings of 
EMNLP.  
A. Lavie and M. J. Denkowski. 2009. The METEOR 
metric for automatic evaluation of machine transla-
tion. Machine Translation, 23. 
C.-Y. Lin. 2004. ROUGE: a Package for Automatic 
Evaluation of Summaries. In Proceedings of the 
Workshop on Text Summarization Branches Out 
(WAS 2004), Barcelona, Spain.  
C. Liu, D. Dahlmeier, and H. T. Ng. 2010. Tesla: Trans-
lation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the 
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 329?334. 
S. Pado, M. Galley, D. Jurafsky, and C.D. Manning. 
2009. Robust machine translation evaluation with en-
tailment features. In Proceedings of ACL-IJCNLP. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL. 
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. 
Makhoul. 2006. A Study of Translation Edit Rate 
with Targeted Human Annotation. In Proceedings of 
Association for Machine Translation in the Americas. 
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 
2009. Fluency, Adequacy, or HTER? Exploring Dif-
ferent Human Judgments with a Tunable MT Metric. 
In Proceedings of the Fourth Workshop on Statistical 
Machine Translation, Athens, Greece. 
77
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 59?63,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Improving AMBER, an MT Evaluation Metric 
 
Boxing Chen, Roland Kuhn and George Foster 
 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
 
{Boxing.Chen, Roland.Kuhn, George.Foster}@nrc.ca 
 
  
Abstract 
A recent paper described a new machine 
translation evaluation metric, AMBER. This 
paper describes two changes to AMBER. The 
first one is incorporation of a new ordering 
penalty; the second one is the use of the 
downhill simplex algorithm to tune the 
weights for the components of AMBER. We 
tested the impact of the two changes, using 
data from the WMT metrics task. Each of the 
changes by itself improved the performance of 
AMBER, and the two together yielded even 
greater improvement, which in some cases 
was more than additive. The new version of 
AMBER clearly outperforms BLEU in terms 
of correlation with human judgment.  
1 Introduction 
AMBER is a machine translation evaluation metric 
first described in (Chen and Kuhn, 2011). It is de-
signed to have the advantages of BLEU (Papineni 
et al, 2002), such as nearly complete language 
independence and rapid computability, while at-
taining even higher correlation with human judg-
ment. According to the paper just cited: ?It can be 
thought of as a weighted combination of dozens of 
computationally cheap features based on word sur-
face forms for evaluating MT quality?. Many re-
cently defined machine translation metrics seek to 
exploit deeper sources of knowledge than are 
available to BLEU, such as external lexical and 
syntactic resources. Unlike these and like BLEU, 
AMBER relies entirely on matching surface forms 
in tokens in the hypothesis and reference, thus sac-
rificing depth of knowledge for simplicity and 
speed.  
In this paper, we describe two improvements to 
AMBER. The first is a new ordering penalty called 
?v? developed in (Chen et al, 2012). The second 
remedies a weakness in the 2011 version of 
AMBER  by carrying out automatic, rather than 
manual, tuning of this metric?s free parameters; we 
now use the simplex algorithm to do the tuning. 
2 AMBER 
AMBER is the product of a score and a penalty, as 
in Equation (1); in this, it resembles BLEU. How-
ever, both the score part and the penalty part are 
more sophisticated than in BLEU. The score part 
(Equation 2) is enriched by incorporating the 
weighted average of n-gram precisions (AvgP), the 
F-measure derived from the arithmetic averages of 
precision and recall (Fmean), and the arithmetic 
average of F-measure of precision and recall for 
each n-gram (AvgF). The penalty part is a 
weighted product of several different penalties 
(Equation 3). Our original AMBER paper (Chen 
and Kuhn, 2011) describes the ten penalties used at 
that time; two of these penalties, the normalized 
Spearman?s correlation penalty and the normalized 
Kendall?s correlation penalty, model word reorder-
ing.  
 
penaltyscoreAMBER ?=                 (1)  
AvgF
FmeanAvgPscore
???+
?+?=
)1( 21
21
??
??
  
      (2) 
?
=
=
P
i
w
i
ipenpenalty
1
                           (3) 
where 1?  and 2?  are weights of each score com-
ponent; wi is the weight of each penalty peni. 
59
In addition to the more complex score and pen-
alty factors, AMBER differs from BLEU in two 
other ways: 
? Not only fixed n-grams, but three different 
kinds of flexible n-grams, are used in com-
puting scores and penalties.  
? The AMBER score can be computed with 
different types of text preprocessing, i.e. 
different combinations of several text pre-
processing techniques: lowercasing, to-
kenization, stemming, word splitting, etc. 8 
types were tried in (Chen and Kuhn, 2011). 
When using more than one type, the final 
score is computed as an average over runs, 
one run per type. In the experiments re-
ported below, we averaged over two types 
of preprocessing. 
3 Improvements to AMBER 
3.1   Ordering penalty v 
We use a simple matching algorithm (Isozaki et 
al., 2010) to do 1-1 word alignment between the 
hypothesis and the reference.  
After word alignment, represent the reference by 
a list of normalized positions of those of its words 
that were aligned with words in the hypothesis, and 
represent the hypothesis by a list of positions for 
the corresponding words in the reference. For both 
lists, unaligned words are ignored. E.g., let P1 = 
reference, P2 = hypothesis: 
P1: 11p  
2
1p  
3
1p  
4
1p  ? 
ip1  ? 
np1  
 P2: 12p  
2
2p  
3
2p  
4
2p  ? 
ip2  ? 
np2
 
Suppose we have 
Ref: in the winter of 2010 , I visited Paris 
Hyp: I visited Paris in 2010 ?s winter 
Then we obtain 
P1: 1 2 3 4 5 6  (the 2nd word ?the?, 4th 
word ?of? and 6th word ?,? in the reference 
are not aligned to any word in the 
hypothesis. Thus, their positions are not in 
P1, so the positions of the matching words 
?in winter 2010 I visited Paris? are nor-
malized to 1 2 3 4 5 6) 
P2: 4 5 6 1 3 2 (the word ??s? was 
unaligned).  
The ordering metric v is computed from two 
distance measures. The first is absolute 
permutation distance: 
?
=
?=
n
i
ii ppPPDIST
1
21211 ||),(               (4) 
Let       
2/)1(
),(1 2111 +?= nn
PPDIST
?                     (5)                  
v1 ranges from 0 to 1; a larger value means more 
similarity between the two permutations. This 
metric is similar to Spearman?s ? (Spearman, 
1904). However, we have found that ? punishes 
long-distance reordering too heavily. For instance, 
1?
 
is more tolerant than ? of the movement of 
?recently? in this example:  
Ref: Recently , I visited Paris 
Hyp: I visited Paris recently  
P1: 1 2 3 4 
P2: 2 3 4 1 
Its 2.0-1 1)4(16
)9116(1
?==
?
+++? ; however, its  
4.0-1 1)/24(4 3111 == + +++1v . 
Inspired by HMM word alignment (Vogel et al, 
1996), our second distance measure is based on 
jump width. This punishes only once a sequence of 
words that moves a long distance with the internal 
word order conserved, rather than on every word. 
In the following, only two groups of words have 
moved, so the jump width punishment is light: 
Ref: In the winter of 2010, I visited Paris 
Hyp: I visited Paris in the winter of 2010  
The second distance measure is 
?
=
??
???=
n
i
iiii ppppPPDIST
1
1
22
1
11212 |)()(|),(   (6) 
where we set 001 =p  and 0
0
2 =p . Let 
1
),(1 2 2122
?
?=
n
PPDIST
v                     (7) 
As with v1, v2 is also from 0 to 1, and larger values 
indicate more similar permutations. The ordering 
measure vs is the harmonic mean of v1 and v2 (Chen 
et al, 2012):  
)11(2 21 /v/v/vs +=
 
.                     (8) 
 In (Chen et al, 2012) we found this to be slightly 
more effective than the geometric mean. vs in (8) is 
computed at segment level. We compute document 
level ordering vD with a weighted arithmetic mean:  
60
?
?
=
=
?
= l
s s
l
s ss
D
Rlen
Rlenv
v
1
1
)(
)(
                    (9) 
where l is the number of segments of the 
document, and len(R) is the length of the reference 
after text preprocessing. vs is the segment-level 
ordering penalty. 
Recall that the penalty part of AMBER is the 
weighted product of several component penalties. 
In the original version of AMBER, there were 10 
component penalties. In the new version, v is in-
corporated as an additional, 11th weighted penalty 
in (3). Thus, the new version of AMBER incorpo-
rates three reordering penalties: Spearman?s 
correlation, Kendall?s correlation, and v. Note that 
v is also incorporated in a tuning metric we recent-
ly devised (Chen et al, 2012).   
3.2   Automatic tuning 
In (Chen and Kuhn, 2011), we manually set the 17 
free parameters of AMBER (see section 3.2 of that 
paper). In the experiments reported below, we 
tuned the 18 free parameters ? the original 17 plus 
the ordering metric v described in the previous sec-
tion - automatically, using the downhill simplex 
method of (Nelder and Mead, 1965) as described 
in (Press et al, 2002). This is a multidimensional 
optimization technique inspired by geometrical 
considerations that has shown good performance in 
a variety of applications.  
4 Experiments 
The experiments are carried out on WMT metric 
task data: specifically, the WMT 2008, WMT 
2009, WMT 2010, WMT 2011 all-to-English, and 
English-to-all submissions. The languages ?all? 
(?xx? in Table 1) include French, Spanish, German 
and Czech. Table 1 summarizes the statistics for 
these data sets. 
 
Set Year Lang. #system #sent-pair 
Test1 2008 xx-En 43 7,804 
Test2 2009 xx-En 45 15,087 
Test3 2009 en-Ex 40 14,563 
Test4 2010 xx-En 53 15,964 
Test5 2010 en-xx 32 18,508 
Test6 2011 xx-En 78 16,120 
Test7 2011 en-xx 94 23,209 
 
Table 1: Statistics of the WMT dev and test sets. 
 
We used 2008 and 2011 data as dev sets, 2009 
and 2010 data as test sets. Spearman?s rank 
correlation coefficient ? was employed to measure 
correlation of the metric with system-level human 
judgments of translation. The human judgment 
score was based on the ?Rank? only, i.e., how 
often the translations of the system were rated as 
better than those from other systems (Callison-
Burch et al, 2008). Thus, BLEU and the new ver-
sion of AMBER were evaluated on how well their 
rankings correlated with the human ones. For the 
segment level, we followed (Callison-Burch et al, 
2010) in using Kendall?s rank correlation 
coefficient ?. 
In what follows, ?AMBER1? will denote a vari-
ant of AMBER as described in (Chen and Kuhn, 
2011). Specifically, it is the variant AMBER(1,4) ? 
that is, the variant in which results are averaged 
over two runs with the following preprocessing: 
1. A run with tokenization and lower-casing 
2. A run in which tokenization and lower-
casing are followed by the word splitting. 
Each word with more than 4 letters is seg-
mented into two sub-words, with one being 
the first 4 letters and the other the last 2 let-
ters. If the word has 5 letters, the 4th letter 
appears twice: e.g., ?gangs? becomes 
?gang? + ?gs?. If the word has more than 6 
letters, the middle part is thrown away.  
The second run above requires some explana-
tion. Recall that in AMBER, we wish to avoid use 
of external resources such as stemmers and mor-
phological analyzers, and we aim at maximal lan-
guage independence. Here, we are doing a kind of 
?poor man?s morphological analysis?. The first 
four letters of a word are an approximation of its 
stem, and the last two letters typically carry at least 
some information about number, gender, case, etc. 
Some information is lost, but on the other hand, 
when we use the metric for a new language (or at 
least, a new Indo-European language) we know 
that it will extract at least some of the information 
hidden inside morphologically complex words. 
The results shown in Tables 2-4 compare the 
correlation of variants of AMBER with human 
judgment; Table 5 compares the best version of 
AMBER (AMBER2) with BLEU. For instance, to 
calculate segment-level correlations using 
61
Kendall?s ?, we carried out 33,071 paired compari-
sons for out-of-English and 31,051 paired compar-
isons for into-English. The resulting ? was 
calculated per system, then averaged for each con-
dition (out-of-English and into-English) to obtain 
one out-of-English value and one into-English val-
ue. 
First, we compared the performance of 
AMBER1 with a version of AMBER1 that in-
cludes the new reordering penalty v, at the system 
and segment levels. The results are shown in Table 
2. The greatest impact of v is on ?out of English? at 
the segment level, but none of the results are par-
ticularly impressive.  
 
 AMBER1 +v Change 
Into-En 
System 
0.860 0.862 0.002 
(+0.2%) 
Into-En 
Segment 
0.178 0.180 0.002 
 (+1.1%) 
Out-of-En 
System 
0.637 0.637 0 
 (0%) 
Out-of-En 
Segment 
0.167 0.170 0.003 
(+1.8%) 
 
Table 2: Correlation with human judgment for 
AMBER1 vs. (AMBER1 including v). 
 
Second, we compared the performance of manu-
ally tuned AMBER1 with AMBER1 whose param-
eters were tuned by the simplex method. The 
tuning was run four times on the dev set, once for 
each possible combination of into/out-of English 
and system/segment level. Table 3 shows the re-
sults on the test set. This change had a greater im-
pact, especially on the segment level. 
 
 AMBER1 +Simplex Change 
Into-En 
 System 
0.860 0.862 0.002 
(+0.2%) 
Into-En 
Segment 
0.178 0.184 0.006  
(+3.4%) 
Out-of-En 
 System 
0.637 0.637 0 
 (0%) 
Out-of-En  
Segment 
0.167 0.182 0.015 
(+9.0%) 
 
Table 3: Correlation with human judgment for 
AMBER1 vs. simplex-tuned AMBER1. 
 
Then, we compared the performance of 
AMBER1 with AMBER1 that contains v and that 
has been tuned by the simplex method. We will 
denote the new version of AMBER containing 
both changes ?AMBER2?. It will be seen from 
Table 4 that AMBER2 is a major improvement 
over AMBER1 at the segment level. In the case of 
?into English? at the segment level, the impact of 
the two changes seems to have been synergistic: 
adding together the percentage improvements due 
to v and simplex from Tables 2 and 3, one would 
have expected an improvement of 4.5% for both 
changes together, but the actual improvement was 
6.2%. Furthermore, there was no improvement at 
the system level for ?out of English? when either 
change was tried separately, but there was a small 
improvement when the two changes were com-
bined.  
 
 AMBER1 AMBER2 Change 
Into-En 
System 
0.860 0.870 0.010 
(+1.2%) 
Into-En 
Segment 
0.178 0.189 0.011 
(+6.2%) 
Out-of-En 
System 
0.637 0.642 0.005 
(+0.8%) 
Out-of-En 
Segment 
0.167 0.184 0.017 
(+10.2%) 
 
Table 4: Correlation with human judgment for 
AMBER1 vs. AMBER2. 
 
Of course, the most important question is: does 
the new version of AMBER (AMBER2) perform 
better than BLEU? Table 5 answers this question 
(the version of BLEU used here was smoothed 
BLEU (mteval-v13a)). There is a clear advantage 
for AMBER2 over BLEU at both the system and 
segment levels, for both ?into English? and ?out of 
English?.  
 
 BLEU AMBER2 Change 
Into-En 
System 
0.773 0.870 0.097 
(+12.5%) 
Into-En 
Segment 
0.154 0.189 0.035 
(+22.7%) 
Out-of-En 
System 
0.574 0.642 0.068 
(+11.8%) 
Out-of-En 
Segment 
0.149 0.184 0.035 
(+23.5%) 
 
Table 5: Correlation with human judgment for 
 BLEU vs. AMBER2. 
 
62
5 Conclusion 
We have made two changes to AMBER, a metric 
described in (Chen and Kuhn, 2011). In our exper-
iments, the new version of AMBER was shown to 
be an improvement on the original version in terms 
of correlation with human judgment. Furthermore, 
it outperformed BLEU by about 12% at the system 
level and about 23% at the segment level.  
A good evaluation metric is not necessarily a 
good tuning metric, and vice versa. In parallel with 
our work on AMBER for evaluation, we have also 
been exploring a machine translation tuning metric 
called PORT (Chen et al, 2012). AMBER and 
PORT differ in many details, but they share the 
same underlying philosophy: to exploit surface 
similarities between hypothesis and references 
even more thoroughly than BLEU does, rather than 
to invoke external resources with richer linguistic 
knowledge. So far, the results for PORT have been 
just as encouraging as the ones for AMBER re-
ported here.  
Reference 
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, M. 
Przybocki and O. Zaidan. 2010. Findings of the 2010 
Joint Workshop on Statistical Machine Translation 
and Metrics for Machine Translation. In Proceedings 
of WMT. 
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz and 
J. Schroeder. 2008. Further Meta-Evaluation of Ma-
chine Translation. In Proceedings of WMT. 
B. Chen, R. Kuhn, and S. Larkin. 2012. PORT:  a Preci-
sion-Order-Recall MT Evaluation Metric for Tuning. 
Accepted for publication in Proceedings of ACL. 
B. Chen and R. Kuhn. 2011. AMBER: a Modified 
BLEU, Enhanced Ranking Metric. In Proceedings of 
the Sixth Workshop on Statistical Machine Transla-
tion, Edinburgh, Scotland.  
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada. 
2010. Automatic Evaluation of Translation Quality 
for Distant Language Pairs. In Proceedings of 
EMNLP.  
J. Nelder and R. Mead. 1965. A simplex method for 
function minimization. Computer Journal V. 7, pages 
308?313. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL. 
W. Press, S. Teukolsky, W. Vetterling and B. Flannery. 
2002. Numerical Recipes in C++. Cambridge Uni-
versity Press, Cambridge, UK.  
C. Spearman. 1904. The proof and measurement of as-
sociation between two things. In American Journal of 
Psychology, V. 15, pages 72?101. 
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based 
word alignment in statistical translation. In Proceed-
ings of COLING. 
63
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362?367,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
A Systematic Comparison of Smoothing Techniques for Sentence-Level
BLEU
Boxing Chen and Colin Cherry
National Research Council Canada
first.last@nrc-cnrc.gc.ca
Abstract
BLEU is the de facto standard machine
translation (MT) evaluation metric. How-
ever, because BLEU computes a geo-
metric mean of n-gram precisions, it of-
ten correlates poorly with human judg-
ment on the sentence-level. There-
fore, several smoothing techniques have
been proposed. This paper systemati-
cally compares 7 smoothing techniques
for sentence-level BLEU. Three of them
are first proposed in this paper, and they
correlate better with human judgments on
the sentence-level than other smoothing
techniques. Moreover, we also compare
the performance of using the 7 smoothing
techniques in statistical machine transla-
tion tuning.
1 Introduction
Since its invention, BLEU (Papineni et al., 2002)
has been the most widely used metric for both
machine translation (MT) evaluation and tuning.
Many other metrics correlate better with human
judgments of translation quality than BLEU, as
shown in recent WMT Evaluation Task reports
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012). However, BLEU remains the de facto stan-
dard evaluation and tuning metric. This is proba-
bly due to the following facts:
1. BLEU is language independent (except for
word segmentation decisions).
2. BLEU can be computed quickly. This is im-
portant when choosing a tuning metric.
3. BLEU seems to be the best tuning metric
from a quality point of view - i.e., models
trained using BLEU obtain the highest scores
from humans and even from other metrics
(Cer et al., 2010).
One of the main criticisms of BLEU is that it
has a poor correlation with human judgments on
the sentence-level. Because it computes a geomet-
ric mean of n-gram precisions, if a higher order
n-gram precision (eg. n = 4) of a sentence is
0, then the BLEU score of the entire sentence is
0, no matter how many 1-grams or 2-grams are
matched. Therefore, several smoothing techniques
for sentence-level BLEU have been proposed (Lin
and Och, 2004; Gao and He, 2013).
In this paper, we systematically compare 7
smoothing techniques for sentence-level BLEU.
Three of them are first proposed in this paper, and
they correlate better with human judgments on the
sentence-level than other smoothing techniques on
the WMT metrics task. Moreover, we compare
the performance of using the 7 smoothing tech-
niques in statistical machine translation tuning on
NIST Chinese-to-English and Arabic-to-English
tasks. We show that when tuning optimizes the
expected sum of these sentence-level metrics (as
advocated by Cherry and Foster (2012) and Gao
and He (2013) among others), all of these metrics
perform similarly in terms of their ability to pro-
duce strong BLEU scores on a held-out test set.
2 BLEU and smoothing
2.1 BLEU
Suppose we have a translation T and its reference
R, BLEU is computed with precision P (N,T,R)
and brevity penalty BP(T,R):
BLEU(N,T,R) = P (N,T,R)? BP(T,R) (1)
where P (N,T,R) is the geometric mean of n-
gram precisions:
P (N,T,R) =
(
N
?
n=1
p
n
)
1
N
(2)
362
and where:
p
n
=
m
n
l
n
(3)
m
n
is the number of matched n-grams between
translation T and its referenceR, and l
n
is the total
number of n-grams in the translation T . BLEU?s
brevity penalty punishes the score if the translation
length len(T ) is shorter than the reference length
len(R), using this equation:
BP(T,R) = min
(
1.0, exp
(
1?
len(R)
len(T )
))
(4)
2.2 Smoothing techniques
The original BLEU was designed for the
document-level; as such, it required no smooth-
ing, as some sentence would have at least one 4-
gram match. We now describe 7 smoothing tech-
niques that work better for sentence-level evalua-
tion. Suppose we consider matching n-grams for
n = 1 . . . N (typically, N = 4). Let m
n
be the
original match count, and m
?
n
be the modified n-
gram match count.
Smoothing 1: if the number of matched n-
grams is 0, we use a small positive value ? to re-
place the 0 for n ranging from 1 toN . The number
? is set empirically.
m
?
n
= ?, if m
n
= 0. (5)
Smoothing 2: this smoothing technique was
proposed in (Lin and Och, 2004). It adds 1 to the
matched n-gram count and the total n-gram count
for n ranging from 2 to N .
m
?
n
= m
n
+ 1, for n in 2 . . . N, (6)
l
?
n
= l
n
+ 1, for n in 2 . . . N. (7)
Smoothing 3: this smoothing technique is im-
plemented in the NIST official BLEU toolkit
mteval-v13a.pl.
1
The algorithm is given below. It
assigns a geometric sequence starting from 1/2 to
the n-grams with 0 matches.
1. invcnt = 1
2. for n in 1 to N
3. if m
n
= 0
4. invcnt = invcnt? 2
5. m
?
n
= 1/invcnt
6. endif
7. endfor
1
available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/
Smoothing 4: this smoothing technique is novel
to this paper. We modify Smoothing 3 to address
the concern that shorter translations may have in-
flated precision values due to having smaller de-
nominators; therefore, we give them proportion-
ally smaller smoothed counts. Instead of scaling
invcnt with a fixed value of 2, we replace line 4 in
Smoothing 3?s algorithm with Equation 8 below.
invcnt = invcnt?
K
ln(len(T ))
(8)
It assigns larger values to invcnt for shorter sen-
tences, resulting in a smaller smoothed count. K
is set empirically.
Smoothing 5: this smoothing technique is also
novel to this paper. It is inspired by the intuition
that matched counts for similar values of n should
be similar. To a calculate the n-gram matched
count, it averages the n ? 1, n and n + 1 ?gram
matched counts. We define m
?
0
= m
1
+ 1, and
calculate m
?
n
for n > 0 as follows:
m
?
n
=
m
?
n?1
+ m
n
+ m
n+1
3
(9)
Smoothing 6: this smoothing technique was
proposed in (Gao and He, 2013). It interpolates
the maximum likelihood estimate of the precision
p
n
with a prior estimate p
0
n
. The prior is estimated
by assuming that the ratio between p
n
and p
n?1
will be the same as that between p
n?1
and p
n?2
.
Formally, the precisions of lower order n-grams,
i.e., p
1
and p
2
, are not smoothed, while the pre-
cisions of higher order n-grams, i.e. n > 2, are
smoothed as follows:
p
n
=
m
n
+ ?p
0
n
l
n
+ ?
(10)
where ? is set empirically, and p
0
n
is computed as
p
0
n
= p
n?1
?
p
n?1
p
n?2
(11)
Smoothing 7: this novel smoothing technique
combines smoothing 4 and smoothing 5. That is,
we first compute a smoothed count for those 0
matched n-gram counts using Smoothing 4, and
then take the average of three counts to set the fi-
nal matched n-gram count as in Equation 9.
3 Experiments
We carried out two series of experiments. The
7 smoothing techniques were first compared in
363
set year lang. #system #seg. pair
dev 2008 xx-en 43 7,804
test1 2012 xx-en 49 34,909
test2 2013 xx-en 94 281,666
test3 2012 en-xx 54 47,875
test4 2013 en-xx 95 220,808
Table 1: Statistics of the WMT dev and test sets.
the metric task as evaluation metrics, then they
were compared as metrics for tuning SMT systems
to maximize the sum of expected sentence-level
BLEU scores.
3.1 Evaluation task
We first compare the correlations with human
judgment for the 7 smoothing techniques onWMT
data; the development set (dev) is the WMT 2008
all-to-English data; the test sets are theWMT 2012
and WMT 2013 all-to-English, and English-to-all
submissions. The languages ?all? (?xx? in Ta-
ble 1) include French, Spanish, German, Czech
and Russian. Table 1 summarizes the dev/test set
statistics.
Following WMT 2013?s metric task (Mach?a?cek
and Bojar, 2013), for the segment level, we use
Kendall?s rank correlation coefficient ? to measure
the correlation with human judgment:
? =
#concordant-pairs?#discordant-pairs
#concordant-pairs + #discordant-pairs
(12)
We extract all pairwise comparisons where one
system?s translation of a particular segment was
judged to be better than the other system?s trans-
lation, i.e., we removed all tied human judg-
ments for a particular segment. If two transla-
tions for a particular segment are assigned the
same BLEU score, then the #concordant-pairs
and #discordant-pairs both get a half count. In
this way, we can keep the number of total pairs
consistent for all different smoothing techniques.
For the system-level, we used Spearman?s rank
correlation coefficient ? and Pearson?s correla-
tion coefficient ? to measure the correlation of
the metric with human judgments of translation.
If we compute document-level BLEU as usual,
all 7 smoothing techniques actually get the same
result, as document-level BLEU does not need
smoothing. We therefore compute the document-
level BLEU as the weighted average of sentence-
level BLEU, with the weights being the reference
Into-English
smooth seg ? sys ? sys ?
crp ? 0.720 0.887
0 0.165 0.759 0.887
1 0.224 0.760 0.887
2 0.226 0.757 0.887
3 0.224 0.760 0.887
4 0.228 0.763 0.887
5 0.234 0.765 0.887
6 0.230 0.754 0.887
7 0.236 0.766 0.887
Table 2: Correlations with human judgment on
WMT data for Into-English task. Results are av-
eraged on 4 test sets. ?crp? is the origianl IBM
corpus-level BLEU.
lengths:
BLEU
d
=
?
D
i=1
len(R
i
)BLEU
i
?
D
i=1
len(R
i
)
(13)
where BLEU
i
is the BLEU score of sentence i,
and D is the size of the document in sentences.
We first set the free parameters of each smooth-
ing method by grid search to optimize the
sentence-level score on the dev set. We set ? to 0.1
for Smoothing 1; K = 5 for Smoothing 4; ? = 5
for Smoothing 6.
Tables 2 and 3 report our results on the met-
rics task. We compared the 7 smoothing tech-
niques described in Section 2.2 to a baseline with
no smoothing (Smoothing 0). All scores match n-
grams n = 1 to 4. Smoothing 3 is implemented
in the standard official NIST evaluation toolkit
(mteval-v13a.pl). Results are averaged across the
4 test sets.
All smoothing techniques improved sentence-
level correlations (? ) over no smoothing. Smooth-
ing method 7 got the best sentence-level results on
both the Into-English and Out-of-English tasks.
On the system-level, our weighted average of
sentence-level BLEU scores (see Equation 13)
achieved a better correlation with human judge-
ment than the original IBM corpus-level BLEU.
However, the choice of which smoothing tech-
nique is used in the average did not make a very
big difference; in particular, the system-level rank
correlation ? did not change for 13 out of 14 cases.
These methods help when comparing one hypoth-
esis to another, but taken as a part of a larger aver-
age, all seven methods assign relatively low scores
364
Out-of-English
smooth seg ? sys ? sys ?
crp ? 0.712 0.744
0 0.119 0.715 0.744
1 0.178 0.722 0.748
2 0.180 0.725 0.744
3 0.178 0.724 0.744
4 0.181 0.727 0.744
5 0.184 0.731 0.744
6 0.182 0.725 0.744
7 0.187 0.734 0.744
Table 3: Correlations with human judgment on
WMT data for Out-of-English task. Results are
averaged on 4 test sets. ?crp? is the origianl IBM
corpus-level BLEU.
to the cases that require smoothing, resulting in
similar system-level rankings.
3.2 Tuning task
In this section, we explore the various BLEU
smoothing methods in the context of SMT param-
eter tuning, which is used to set the decoder?s
linear model weights w. In particular, we use
a tuning method that maximizes the sum of ex-
pected sentence-level BLEU scores, which has
been shown to be a simple and effective method
for tuning with large feature sets by both Cherry
and Foster (2012) and Gao and He (2013), but
which requires a smoothed sentence-level BLEU
approximation. For a source sentence f
i
, the prob-
ability of the k
th
translation hypothesis e
k
i
is its ex-
ponentiated and normalized model score:
P
w
(e
k
i
|f
i
) =
exp(score
w
(e
k
i
, f
i
))
?
k
?
exp(score
w
(e
k
?
i
, f
i
))
where k
?
ranges over all hypotheses in a K-best
list.
2
We then use stochastic gradient descent
(SGD) to minimize:
?||w||
2
?
?
i
[
len(R
i
)? E
P
w
(
BLEU(e
k
i
, f
i
)
)]
Note that we scale the expectation by reference
length to place more emphasis on longer sen-
tences. We set the regularization parameter ?,
which determines the trade-off between a high ex-
pected BLEU and a small norm, to ? = 10.
Following Cherry and Foster (2012), we tune
with a MERT-like batch architecture: fixing a set
2
We useK = 100 in our experiments.
corpus # segs # en tok
Chinese-English
train 10.1M 283M
tune 1,506 161K
MT06 1,664 189K
MT08 1,357 164K
Arabic-English
train 1,512K 47.8M
tune 1,664 202K
MT08 1,360 205K
MT09 1,313 187K
Table 4: Statistics of the NIST Chinese-English
and Arabic-English data.
of K-best lists, optimizing, and then re-decoding
the entire dev set to K-best and aggregating with
previous lists to create a better K-best approxima-
tion. We repeat this outer loop 15 times.
We carried out experiments in two different set-
tings, both involving data from NIST Open MT
2012.
3
The first setting is based on data from the
Chinese-to-English constrained track, comprising
about 283 million English running words. The
second setting uses NIST 2012 Arabic-to-English
data, but excludes the UN data. There are about
47.8 million English running words in these train-
ing data. The dev set (tune) for the Chinese-to-
English task was taken from the NIST 2005 eval-
uation set, augmented with some web-genre mate-
rial reserved from other NIST corpora. We test on
the evaluation sets from NIST 2006 and 2008. For
the Arabic-to-English task, we use the evaluation
sets from NIST 2006, 2008, and 2009 as our dev
set and two test sets, respectively. Table 4 summa-
rizes the training, dev and test sets.
Experiments were carried out with an in-house,
state-of-the-art phrase-based system. Each corpus
was word-aligned using IBM2, HMM, and IBM4
models, and the phrase table was the union of
phrase pairs extracted from these separate align-
ments, with a length limit of 7. The translation
model (TM) was smoothed in both directions with
Kneser-Ney smoothing (Chen et al., 2011). We
use the hierarchical lexicalized reordering model
(RM) (Galley and Manning, 2008), with a dis-
tortion limit of 7. Other features include lexi-
cal weighting in both directions, word count, a
distance-based RM, a 4-gram LM trained on the
target side of the parallel data, and a 6-gram En-
3
http://www.nist.gov/itl/iad/mig/openmt12.cfm
365
Tune std MT06 std MT08 std
0 27.6 0.1 35.6 0.1 29.0 0.2
1 27.6 0.0 35.7 0.1 29.1 0.1
2 27.5 0.1 35.8 0.1 29.1 0.1
3 27.6 0.1 35.8 0.1 29.1 0.1
4 27.6 0.1 35.7 0.2 29.1 0.2
5 27.6 0.1 35.5 0.1 28.9 0.2
6 27.5 0.1 35.7 0.1 29.0 0.2
7 27.6 0.1 35.6 0.1 29.0 0.1
Table 5: Chinese-to-English Results for the small
feature set tuning task. Results are averaged across
5 replications; std is the standard deviation.
glish Gigaword LM.
We also conducted a set of experiments with a
much larger feature set. This system used only
GIZA++ for word alignment, increased the distor-
tion limit from 7 to 9, and is trained on a high-
quality subset of the parallel corpora used ear-
lier. Most importantly, it includes the full set of
sparse phrase-pair features used by both Hopkins
and May (2011) and Cherry and Foster (2012),
which results in nearly 7,000 features.
Our evaluation metric is the original IBM
BLEU, which performs case-insensitive matching
of n-grams up to n = 4. We perform random
replications of parameter tuning, as suggested by
Clark et al. (2011). Each replication uses a differ-
ent random seed to determine the order in which
SGD visits tuning sentences. We test for signifi-
cance using the MultEval tool,
4
which uses a strat-
ified approximate randomization test to account
for multiple replications. We report results aver-
aged across replications as well as standard devia-
tions, which indicate optimizer stability.
Results for the small feature set are shown in
Tables 5 and 6. All 7 smoothing techniques, as
well as the no smoothing baseline, all yield very
similar results on both Chinese and Arabic tasks.
We did not find any two results to be significantly
different. This is somewhat surprising, as other
groups have suggested that choosing an appropri-
ate BLEU approximation is very important. In-
stead, our experiments indicate that the selected
BLEU smoothing method is not very important.
The large-feature experiments were only con-
ducted with the most promising methods accord-
ing to correlation with human judgments:
4
available at https://github.com/jhclark/multeval
Tune std MT08 std MT09 std
0 46.9 0.1 46.5 0.1 49.1 0.1
1 46.9 0.0 46.4 0.1 49.1 0.1
2 46.9 0.0 46.4 0.1 49.0 0.1
3 47.0 0.0 46.5 0.1 49.2 0.1
4 47.0 0.0 46.5 0.1 49.2 0.1
5 46.9 0.0 46.4 0.1 49.1 0.1
6 47.0 0.0 46.4 0.1 49.1 0.1
7 47.0 0.0 46.4 0.1 49.0 0.1
Table 6: Arabic-to-English Results for the small
feature set tuning task. Results are averaged across
5 replications; std is the standard deviation.
Tune std MT06 std MT08 std
mira 29.9 0.1 38.0 0.1 31.0 0.1
0 29.5 0.1 37.9 0.1 31.4 0.3
2 29.6 0.3 38.0 0.2 31.1 0.2
4 29.9 0.2 38.1 0.1 31.2 0.2
6 29.7 0.1 37.9 0.2 31.0 0.2
7 29.7 0.2 38.0 0.2 31.2 0.1
Table 7: Chinese-to-English Results for the large
feature set tuning task. Results are averaged
across 5 replications; std is the standard deviation.
Significant improvements over the no-smoothing
baseline (p ? 0.05) are marked in bold.
0: No smoothing (baseline)
2: Add 1 smoothing (Lin and Och, 2004)
4: Length-scaled pseudo-counts (this paper)
6: Interpolation with a precision prior (Gao and
He, 2013)
7: Combining Smoothing 4 with the match in-
terpolation of Smoothing 5 (this paper)
The results of the large feature set experiments are
shown in Table 7 for Chinese-to-English and Ta-
ble 8 for Arabic-to-English. For a sanity check, we
compared these results to tuning with our very sta-
ble Batch k-best MIRA implementation (Cherry
and Foster, 2012), listed as mira, which shows that
all of our expected BLEU tuners are behaving rea-
sonably, if not better than expected.
Comparing the various smoothing methods in
the large feature scenario, we are able to see signif-
icant improvements over the no-smoothing base-
line. Notably, Method 7 achieves a significant
improvement over the no-smoothing baseline in 3
out of 4 scenarios, more than any other method.
Unfortunately, in the Chinese-English MT08 sce-
nario, the no-smoothing baseline significantly out-
366
Tune std MT08 std MT09 std
mira 47.9 0.1 47.3 0.0 49.3 0.1
0 48.1 0.1 47.2 0.1 49.5 0.1
2 48.0 0.1 47.4 0.1 49.7 0.1
4 48.1 0.2 47.4 0.1 49.6 0.1
6 48.2 0.0 47.3 0.1 49.7 0.1
7 48.1 0.1 47.3 0.1 49.7 0.1
Table 8: Arabic-to-English Results for the large
feature set tuning task. Results are averaged
across 5 replications; std is the standard deviation.
Significant improvements over the no-smoothing
baseline (p ? 0.05) are marked in bold.
performs all smoothed BLEU methods, making it
difficult to draw any conclusions at all from these
experiments. We had hoped to see at least a clear
improvement in the tuning set, and one does see
a nice progression as smoothing improves in the
Chinese-to-English scenario, but no correspond-
ing pattern emerges for Arabic-to-English.
4 Conclusions
In this paper, we compared seven smoothing
techniques for sentence-level BLEU. Three of
them are newly proposed in this paper. The
new smoothing techniques got better sentence-
level correlations with human judgment than other
smoothing techniques. On the other hand, when
we compare the techniques in the context of tun-
ing, using a method that requires sentence-level
BLEU approximations, they all have similar per-
formance.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 555?563, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In MT Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In ACL 2011.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model.
In EMNLP 2008, pages 848?856, Hawaii, October.
Jianfeng Gao and Xiaodong He. 2013. Training mrf-
based phrase translation models using gradient as-
cent. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 450?459, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP 2011.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 605?612, Barcelona,
Spain, July.
Matou?s Mach?a?cek and Ond?rej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45?51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, July. ACL.
367
