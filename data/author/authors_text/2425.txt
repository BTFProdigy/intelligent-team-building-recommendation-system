Proceedings of the ACL Student Research Workshop, pages 91?96,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Dependency-Based Statistical Machine Translation
Heidi J. Fox
Brown Laboratory for Linguistic Information Processing
Brown University, Box 1910, Providence, RI 02912
hjf@cs.brown.edu
Abstract
We present a Czech-English statistical
machine translation system which per-
forms tree-to-tree translation of depen-
dency structures. The only bilingual re-
source required is a sentence-aligned par-
allel corpus. All other resources are
monolingual. We also refer to an evalua-
tion method and plan to compare our sys-
tem?s output with a benchmark system.
1 Introduction
The goal of statistical machine translation (SMT) is
to develop mathematical models of the translation
process whose parameters can be automatically esti-
mated from a parallel corpus. Given a string of for-
eign words F, we seek to find the English string E
which is a ?correct? translation of the foreign string.
The first work on SMT done at IBM (Brown et al,
1990; Brown et al, 1992; Brown et al, 1993; Berger
et al, 1994), used a noisy-channel model, resulting
in what Brown et al (1993) call ?the Fundamental
Equation of Machine Translation?:
E? =
argmax
E P (E)P (F | E) (1)
In this equation we see that the translation prob-
lem is factored into two subproblems. P (E) is the
language model and P (F | E) is the translation
model. The work described here focuses on devel-
oping improvements to the translation model.
While the IBM work was groundbreaking, it was
also deficient in several ways. Their model trans-
lates words in isolation, and the component which
accounts for word order differences between lan-
guages is based on linear position in the sentence.
Conspicuously absent is all but the most elementary
use of syntactic information. Several researchers
have subsequently formulated models which incor-
porate the intuition that syntactically close con-
stituents tend to stay close across languages. Below
are descriptions of some of these different methods
of integrating syntax.
? Stochastic Inversion Transduction Grammars
(Wu and Wong, 1998): This formalism uses a
grammar for English and from it derives a pos-
sible grammar for the foreign language. This
derivation includes adding productions where
the order of the RHS is reversed from the or-
dering of the English.
? Syntax-based Statistical Translation (Yamada
and Knight, 2001): This model extends the
above by allowing all possible permutations of
the RHS of the English rules.
? Statistical Phrase-based Translation (Koehn
et al, 2003): Here ?phrase-based? means
?subsequence-based?, as there is no guarantee
that the phrases learned by the model will have
any relation to what we would think of as syn-
tactic phrases.
? Dependency-based Translation ( ?Cmejrek et al,
2003): This model assumes a dependency
parser for the foreign language. The syntactic
structure and labels are preserved during trans-
lation. Transfer is purely lexical. A generator
builds an English sentence out of the structure,
labels, and translated words.
91
2 System Overview
The basic framework of our system is quite similar
to that of ?Cmejrek et al (2003) (we reuse many of
their ancillary modules). The difference is in how
we use the dependency structures. ?Cmejrek et al
only translate the lexical items. The dependency
structure and any features on the nodes are preserved
and all other processing is left to the generator. In
addition to lexical translation, our system models
structural changes and changes to feature values, for
although dependency structures are fairly well pre-
served across languages (Fox, 2002), there are cer-
tainly many instances where the structure must be
modified.
While the entire translation system is too large to
discuss in detail here, I will provide brief descrip-
tions of ancillary components. References are pro-
vided, where available, for those who want more in-
formation.
2.1 Corpus Preparation
Our parallel Czech-English corpus is comprised of
Wall Street Journal articles from 1989. The English
data is from the University of Pennsylvania Tree-
bank (Marcus et al, 1993; Marcus et al, 1994).
The Czech translations of these articles are provided
as part of the Prague Dependency Treebank (PDT)
(Bo?hmova? et al, 2001). In order to learn the pa-
rameters for our model, we must first create aligned
dependency structures for the sentence pairs in our
corpus. This process begins with the building of de-
pendency structures.
Since Czech is a highly inflected language, mor-
phological tagging is extremely helpful for down-
stream processing. We generate the tags using
the system described in (Hajic? and Hladka?, 1998).
The tagged sentences are parsed by the Charniak
parser, this time trained on Czech data from the PDT.
The resulting phrase structures are converted to tec-
togrammatical dependency structures via the proce-
dure documented in (Bo?hmova?, 2001). Under this
formalism, function words are deleted and any in-
formation contained in them is preserved in features
attached to the remaining nodes. Finally, functors
(such as agent or patient) are automatically assigned
to nodes in the tree ( ?Zabokrtsky? et al, 2002).
On the English side, the process is simpler. We
japan automobile dealers association... ...
NNP NNP NNPS NN
japan automobile dealers association... ...NNP NNP NNPS NN
SPLIT
N N A N
CZ3
CZ2
CZ1
... obchodn??k japonsky? ...automobilasociace
EN2
EN1
EN2
EN1
EN3
Figure 1: Left SPLIT Example
parse with the Charniak parser (Charniak, 2000)
and convert the resulting phrase-structure trees to a
function-argument formalism, which, like the tec-
togrammatic formalism, removes function words.
This conversion is accomplished via deterministic
application of approximately 20 rules.
2.2 Aligning the Dependency Structures
We now generate the alignments between the pairs
of dependency structures we have created. We be-
gin by producing word alignments with a model very
similar to that of IBM Model 4 (Brown et al, 1993).
We keep fifty possible alignments and require that
each word has at least two possible alignments. We
then align phrases based on the alignments of the
words in each phrase span. If there is no satisfac-
tory alignment, then we allow for structural muta-
tions. The probabilities for these mutations are re-
fined via another round of alignment. The structural
mutations allowed are described below. Examples
are shown in phrase-structure format rather than de-
pendency format for ease of explanation.
92
BUD
CZ2
CZ1
bear stearns
N NN
spolec?nost
EN1
EN2
... stearns ...
NNP NNP
bear
... ...
Figure 2: BUD Example
? KEEP: No change. This is the default.
? SPLIT: One English phrase aligns with two
Czech phrases and splitting the English phrase
results in a better alignment. There are three
types of split (left, right, middle) whose proba-
bilities are also estimated. In the original struc-
ture of Figure 1, English node EN1 would align
with Czech nodes CZ1 and CZ2. Splitting the
English by adding child node EN3 results in a
better alignment.
? BUD: This adds a unary level in the English
tree in the case when one English node aligns
to two Czech nodes, but one of the Czech nodes
is the parent of the other. In Figure 2, the Czech
has one extra word ?spolec?nost? (?company?)
compared with the English. English node EN1
would normally align to both CZ1 and CZ2.
Adding a unary node EN2 to the English results
in a better alignment.
? ERASE: There is no corresponding Czech node
for the English one. In Figure 3, the English has
two nodes, EN1 and EN2, which have no corre-
sponding Czech nodes. Erasing them brings the
Czech and English structures into alignment.
? PHRASE-TO-WORD: An entire English
phrase aligns with one Czech word. This
operates similarly to ERASE.
NNJJ WDT VBD NNP
NNJJ WDT VBD NNP
ERASE ERASE
A N P V N
CZ2
CZ1
ktery?... rok srpen ...fiska?ln?? zar???
EN4
EN3
EN2EN1
year began august ...which... fiscal
EN4
EN3
year began august ...which... fiscal
Figure 3: ERASE Example
3 Translation Model
Given E , the parse of the English string, our trans-
lation model can be formalized as P (F | E). Let
E1 . . . En be the nodes in the English parse, F be
a parse of the Czech string, and F1 . . .Fm be the
nodes in the Czech parse. Then,
P (F | E) =
?
FforF
P (F1 . . .Fm | E1 . . . En) (2)
We initially make several strong independence as-
sumptions which we hope to eventually weaken.
The first is that each Czech parse node is generated
independently of every other one. Further, we spec-
ify that each English parse node generates exactly
one (possibly NULL) Czech parse node.
P (F | E) =
?
Fi?F
P (Fi | E1 . . . En) =
n?
i=1
P (Fi | Ei)
(3)
An English parse node Ei contains the following
information:
? An English word: ei
? A part of speech: tei
? A vector of n features (e.g. negation or tense):
< ?ei [1], . . . , ?
e
i [n] >
93
? A list of dependent nodes
In order to produce a Czech parse node Fi, we
must generate the following:
Lemma fi: We generate the Czech lemma fi de-
pendent only on the English word ei.
Part of Speech tfi : We generate Czech part of
speech tfi dependent on the part of speech of
the Czech parent tfpar(i) and the corresponding
English part of speech tei .
Features < ?fi [1], . . . , ?
f
i [n] >: There are several
features (see Table 1) associated with each
parse node. Of these, all except IND are typi-
cal morphological and analytical features. IND
(indicator) is a loosely-specified feature com-
prised of functors, where assigned, and other
words or small phrases (often prepositions)
which are attached to the node and indicate
something about the node?s function in the sen-
tence. (e.g. an IND of ?at? could indicate a
locative function). We generate each Czech
feature ?fi [j] dependent only on its correspond-
ing English feature ?ei [j].
Head Position hi: When an English word is
aligned to the head of a Czech phrase, the
English word is typically also the head of its
respective phrase. But, this is not always the
case, so we model the probability that the En-
glish head will be aligned to either the Czech
head or to one of its children. To simplify,
we set the probability for each particular child
being the head to be uniform in the number
of children. The head position is generated
independent of the rest of the sentence.
Structural Mutation mi: Dependency structures
are fairly well preserved across languages, but
there are cases when the structures need to be
modified. Section 2.2 contains descriptions
of the different structural changes which
we model. The mutation type is generated
independent of the rest of the sentence.
Feature Description
NEG Negation
STY Style (e.g. statement, question)
QUO Is node part of a quoted expression?
MD Modal verb associated with node
TEN Tense (past, present, future)
MOOD Mood (infinitive, perfect, progressive)
CONJ Is node part of a conjoined expression?
IND Indicator
Table 1: Features
3.1 Model with Independence Assumptions
With all of the independence assumptions described
above, the translation model becomes:
P (Fi | Ei) = P (fi | ei)P (t
f
i | t
e
i , t
f
par(i))
?P (hi)P (mi)
n?
j=1
P (?fi [j] | ?
e
i [j]) (4)
4 Training
The Czech and English data are preprocessed (see
Section 2.1) and the resulting dependency structures
are aligned (see Section 2.2). We estimate the model
parameters from this aligned data by maximum like-
lihood estimation. In addition, we gather the inverse
probabilities P (E | F ) for use in the figure of merit
which guides the decoder?s search.
5 Decoding
Given a Czech sentence to translate, we first pro-
cess it as described in Section 2.1. The resulting de-
pendency structure is the input to the decoder. The
decoder itself is a best-first decoder whose priority
queue holds partially-constructed English nodes.
For our figure of merit to guide the search, we use
the probability P (E | F ). We normalize this us-
ing the perplexity (2H ) to compensate for the differ-
ent number of possible values for the features ?[j].
Given two different features whose values have the
same probability, the figure of merit for the feature
with the greater uncertainty will be boosted. This
prevents features with few possible values from mo-
nopolizing the search at the expense of the other fea-
tures. Thus, for feature ?ei [j], the figure of merit is
FOM = P (?ei [j] | ?
f
i [j]) ? 2
H(?ei [j]|?
f
i [j]) (5)
94
Since our goal is to build a forest of partial trans-
lations, we translate each Czech dependency node
independently of the others. (As more conditioning
factors are added in the future, we will instead trans-
late small subtrees rather than single nodes.) Each
translated node Ei is constructed incrementally in the
following order:
1. Choose the head position hi
2. Generate the part of speech tei
3. For j = 1..n, generate ?ei [j]
4. Choose a structural mutation mi
English nodes continue to be generated until ei-
ther the queue or some other stopping condition
is reached (e.g. having a certain number of possi-
ble translations for each Czech node). After stop-
ping, we are left with a forest of English dependency
nodes or subtrees.
6 Language Model
We use a syntax-based language model which was
originally developed for use in speech recognition
(Charniak, 2001) and later adapted to work with a
syntax-based machine translation system (Charniak
et al, 2001). This language model requires a for-
est of partial phrase structures as input. Therefore,
the format of the output of the decoder must be
changed. This is the inverse transformation of the
one performed during corpus preparation. We ac-
complish this with a statistical tree transformation
model whose parameters are estimated during the
corpus preparation phase.
7 Evaluation
We propose to evaluate system performance with
version 0.9 of the NIST automated scorer (NIST,
2002), which is a modification of the BLEU sys-
tem (Papineni et al, 2001). BLEU calculates a score
based on a weighted sum of the counts of matching
n-grams, along with a penalty for a significant dif-
ference in length between the system output and the
reference translation closest in length. Experiments
have shown a high degree of correlation between
BLEU score and the translation quality judgments
of humans. The most interesting difference in the
NIST scorer is that it weights n-grams based on a
notion of informativeness. Details of the scorer can
be found in their paper.
For our experiments, we propose to use the data
from the PDT, which has already been segmented
into training, held out (or development test), and
evaluation sets. As a baseline, we will run the
GIZA++ implementation of IBM?s Model 4 trans-
lation algorithm under the same training conditions
as our own system (Al-Onaizan et al, 1999; Och and
Ney, 2000; Germann et al, 2001).
8 Future Work
Our first priority is to complete the final pieces so
that we have an end-to-end system to experiment
with. Once we are able to evaluate our system out-
put, our first priority will be to analyze the system
errors and adjust the model accordingly. We recog-
nize that our independence assumptions are gener-
ally too strong, and improving them is a hight pri-
ority. Adding more conditioning factors should im-
prove the quality of the decoder output as well as re-
ducing the amount of probability mass lost on struc-
tures which are not well formed. With this will come
sparse data issues, so it will also be important for us
to incorporate smoothing into the model.
There are many interesting subproblems which
deserve attention and we hope to examine at least a
couple of these in the near future. Among these are
discontinuous constituents, head switching, phrasal
translation, English word stemming, and improved
modeling of structural changes.
Acknowledgments
This work was supported in part by NSF grant
IGERT-9870676. We would like to thank Jan Hajic?,
Martin ?Cmejrek, Jan Cur???n for all of their assistance.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-
Josef Och, David Purdy, Noah A. Smith, and
David Yarowsky. 1999. Statistical machine
translation: Final report, JHU workshop 1999.
www.clsp.jhu.edu/ws99/projects/mt/final report/mt-
final-report.ps.
95
Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra,
Vincent J. Della Pietra, John R. Gillett, John D. Laf-
ferty, Robert L. Mercer, Harry Printz, and Lubos? Ures?.
1994. The Candide system for machine translation. In
Proceedings of the ARPA Human Language Technol-
ogy Workshop.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2001. The Prague Dependency Treebank:
Three-level annotation scenario. In Anne Abeille?, ed-
itor, Treebanks: Building and Using Syntactically An-
notated Corpora. Kluwer Academic Publishers.
Alena Bo?hmova?. 2001. Automatic procedures in tec-
togrammatical tagging. The Prague Bulletin of Math-
ematical Linguistics, 76.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Peter F. Brown, Stephen A. Della Petra, Vincent J.
Della Pietra, John D. Lafferty, and Robert L. Mer-
cer. 1992. Analysis, statistical transfer, and synthesis
in machine translation. In Proceedings of the Fourth
International Conference on Theoretical and Method-
ological Issues in Machine Translation, pages 83?100.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, June.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2001. Syntax-based language models for statistical
machine translation. In Proceedings of the 39th An-
nual Meeting of the Association for Computational
Linguistics, Toulouse, France, July.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics, pages 116?123, Toulouse, France, July.
Martin ?Cmejrek, Jan Cur???n, and Jir??? Havelka. 2003.
Czech-English Dependency-based Machine Transla-
tion. In EACL 2003 Proceedings of the Conference,
pages 83?90, April 12?17, 2003.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2002), July.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics.
Jan Hajic? and Barbora Hladka?. 1998. Tagging Inflec-
tive Languages: Prediction of Morphological Cate-
gories for a Rich, Structured Tagset. In Proceedings of
COLING-ACL Conference, pages 483?490, Montreal,
Canada.
Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, Edmonton, Canada, May.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 13(2):313?330, June.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of the ARPA Human Language Technol-
ogy Workshop, pages 114?119.
NIST. 2002. Automatic evaluation of machine trans-
lation quality using n-gram co-occurrence statistics.
www.nist.gov/speech/tests/mt/doc/ngram-study.pdf.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: A method for automatic evalu-
ation of machine translation. Technical report, IBM.
Dekai Wu and Hongsing Wong. 1998. Machine trans-
lation with a stochastic grammatical channel. In Pro-
ceedings of the 36th Annual Meeting of the Association
for Computational Linguistics, pages 1408?1414.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics.
Zdene?k ?Zabokrtsky?, Petr Sgall, and Sas?o Dz?eroski. 2002.
Machine learning approach to automatic functor as-
signment in the Prague Dependency Treebank. In Pro-
ceedings of LREC 2002 (Third International Confer-
ence on Language Resources and Evaluation), vol-
ume V, pages 1513?1520, Las Palmas de Gran Ca-
naria, Spain.
96
Phrasal Cohesion and Statistical Machine Translation
Heidi J. Fox
Brown Laboratory for Linguistic Information Processing
Department of Computer Science
Brown University, Box 1910, Providence, RI 02912
hjf@cs.brown.edu
Abstract
There has been much interest in us-
ing phrasal movement to improve statis-
tical machine translation. We explore
how well phrases cohere across two lan-
guages, specifically English and French,
and examine the particular conditions un-
der which they do not. We demonstrate
that while there are cases where coherence
is poor, there are many regularities which
can be exploited by a statistical machine
translation system. We also compare three
variant syntactic representations to deter-
mine which one has the best properties
with respect to cohesion.
1 Introduction
Statistical machine translation (SMT) seeks to de-
velop mathematical models of the translation pro-
cess whose parameters can be automatically esti-
mated from a parallel corpus. The first work in
SMT, done at IBM (Brown et al, 1993), developed
a noisy-channel model, factoring the translation pro-
cess into two portions: the translation model and the
language model. The translation model captures the
translation of source language words into the target
language and the reordering of those words. The
language model ranks the outputs of the translation
model by how well they adhere to the syntactic con-
straints of the target language.1
The prime deficiency of the IBM model is the re-
ordering component. Even in the most complex of
1Though usually a simple word n-gram model is used for the
language model.
the five IBM models, the reordering operation pays
little attention to context and none at all to higher-
level syntactic structures. Many attempts have been
made to remedy this by incorporating syntactic in-
formation into translation models. These have taken
several different forms, but all share the basic as-
sumption that phrases in one language tend to stay
together (i.e. cohere) during translation and thus the
word-reordering operation can move entire phrases,
rather than moving each word independently.
(Yarowsky et al, 2001) states that during their
work on noun phrase bracketing they found a strong
cohesion among noun phrases, even when compar-
ing English to Czech, a relatively free word or-
der language. Other than this, there is little in the
SMT literature to validate the coherence assump-
tion. Several studies have reported alignment or
translation performance for syntactically augmented
translation models (Wu, 1997; Wang, 1998; Alshawi
et al, 2000; Yamada and Knight, 2001; Jones and
Havrilla, 1998) and these results have been promis-
ing. However, without a focused study of the be-
havior of phrases across languages, we cannot know
how far these models can take us and what specific
pitfalls they face.
The particulars of cohesion will clearly depend
upon the pair of languages being compared. Intu-
itively, we expect that while French and Spanish will
have a high degree of cohesion, French and Japanese
may not. It is also clear that if the cohesion between
two closely related languages is not high enough
to be useful, then there is no hope for these meth-
ods when applied to distantly related languages. For
this reason, we have examined phrasal cohesion for
French and English, two languages which are fairly
close syntactically but have enough differences to be
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 304-311.
                         Proceedings of the Conference on Empirical Methods in Natural
interesting.
2 Alignments, Spans and Crossings
An alignment is a mapping between the words in a
string in one language and the translations of those
words in a string in another language. Given an En-
glish string,   	
  

, and a French
string,

 

  

, an alignment a can
be represented by   A Novel Use of Statistical Parsing to Extract Information from 
Text 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel 
BBN Technologies 
70 Fawcett Street, Cambridge, MA 02138 
szmiller@bbn.com 
Abstract 
Since 1995, a few statistical parsing 
algorithms have demonstrated a 
breakthrough in parsing accuracy, as 
measured against the UPenn TREEBANK 
as a gold standard. In this paper we report 
adapting a lexicalized, probabilistic 
context-free parser to information 
extraction and evaluate this new technique 
on MUC-7 template elements and template 
relations. 
1 Introduction 
Since 1995, a few statistical parsing 
algorithms (Magerman, 1995; Collins, 1996 
and 1997; Charniak, 1997; Rathnaparki, 1997) 
demonstrated a breakthrough in parsing 
accuracy, as measured against he University 
of Pennsylvania TREEBANK as a gold 
standard. Yet, relatively few have embedded 
one of these algorithms in a task. Chiba, 
(1999) was able to use such a parsing 
algorithm to reduce perplexity with the long 
term goal of improved speech recognition. 
In this paper, we report adapting a lexicalized, 
probabilistic context-free parser with head 
rules (LPCFG-HR) to information extraction. 
The technique was benchmarked in the 
Seventh Message Understanding Conference 
(MUC-7) in 1998. 
Several technical challenges confronted us and 
were solved: 
? How could the limited semantic 
interpretation required in information 
extraction be integrated into the statistical 
learning algorithm? We were able to integrate 
both syntactic and semantic information into 
the parsing process, thus avoiding potential 
errors of syntax first followed by semantics. 
? Would TREEBANKing of the variety of 
news sources in MUC-7 be required? Or 
could the University of Pennsylvania's 
TREEBANK on Wall Street Journal 
adequately train the algorithm for New York 
Times newswire, which includes dozens of 
newspapers? Manually creating source- 
specific training data for syntax was not 
required. Instead, our parsing algorithm, 
trained on the UPenn TREEBANK, was run 
on the New York Times source to create 
unsupervised syntactic training which was 
constrained to be consistent with semantic 
annotation. 
* Would semantic annotation require 
computational linguists? We were able to 
specify relatively simple guidelines that 
students with no training in computational 
linguistics could annotate. 
2 Information Extraction Tasks 
We evaluated the new approach to information 
extraction on two of the tasks of the Seventh 
Message Understanding Conference (MUC-7) 
and reported in (Marsh, 1998). The Template 
Element (TE) task identifies organizations, 
persons, locations, and some artifacts (rocket 
and airplane-related artifacts). For each 
organization i an article, one must identify all 
of its names as used in the article, its type 
(corporation, government, or other), and any 
significant description of it. For each person, 
one must find all of the person's names within 
the document, his/her type (civilian or 
military), and any significant descriptions 
(e.g., titles). For each location, one must also 
give its type (city, province, county, body of 
water, etc.). For the following example, the 
226 
template element in  Figure I was to be 
generated: "...according to the report by 
Edwin Dorn, under secretary of defense for 
personnel and readiness . . . .  Dorn's conclusion 
that Washington..." 
<ENTITY-9601020516-13> := 
ENT_NAME: "Edwin Dorn" 
"Dorn" 
ENT_TYPE: PERSON 
ENT_DESCRIPTOR: "under secretary of 
defense for personnel and readiness" 
ENT_CATEGORY: PER_CIV 
Figure 1: An example of the information to be 
extracted for TE. 
The Template Relations (TR) task involves 
identifying instances of three relations in the 
text: 
? the products made by each company 
? the employees ofeach organization, 
? the (headquarters) location of each 
organization. 
TR builds on TE in that TR reports binary 
relations between elements of TE. For the 
following example, the template relation in 
Figure 2 was to be generated: "Donald M. 
Goldstein, a historian at the University of 
Pittsburgh who helped write..." 
<EMPLOYEE_OF-9601020516-5> := 
PERSON: <ENTITY-9601020516-18> 
ORGANIZATION: <ENTITY- 
9601020516-9> 
<ENTITY-9601020516-9> := 
ENT_NAME: "University of Pittsburgh" 
ENT_TYPE: ORGANIZATION 
ENT_CATEGORY: ORG_CO 
<ENTITY-9601020516-18> := 
ENT_NAME: "Donald M. Goldstein" 
ENT_TYPE: PERSON 
ENT_DESCRIPTOR: "a historian at the 
University of Pittsburgh" 
Figure 2: An example of information to be 
extracted for TR 
3 Integrated Sentential Processing 
Almost all approaches to information 
extraction - even at the sentence level - are 
based on the divide-and-conquer st ategy of 
reducing acomplex problem to a set of simpler 
ones. Currently, the prevailing architecture for 
dividing sentential processing is a four-stage 
pipeline consisting of: 
1. part-of-speech tagging 
2. name finding 
3. syntactic analysis, often limited to noun 
and verb group chunking 
4. semantic interpretation, usually based on 
pattern matching 
Since we were interested in exploiting recent 
advances in parsing, replacing the syntactic 
analysis tage of the standard pipeline with a 
modem statistical parser was an obvious 
possibility. However, pipelined architectures 
suffer from a serious disadvantage: rrors 
accumulate as they propagate through the 
pipeline. For example, an error made during 
part-of-speech-tagging may cause a future 
error in syntactic analysis, which may in turn 
cause a semantic interpretation failure. There 
is no opportunity for a later stage, such as 
parsing, to influence or correct an earlier stage 
such as part-of-speech tagging. 
An integrated model can limit the propagation 
of errors by making all decisions jointly. For 
this reason, we focused on designing an 
integrated model in which tagging, name- 
finding, parsing, and semantic interpretation 
decisions all have the opportunity to mutually 
influence ach other. 
A second consideration influenced our 
decision toward an integrated model. We were 
already using a generative statistical model for 
part-of-speech tagging (Weischedel et al 
1993), and more recently, had begun using a 
generative statistical model for name finding 
(Bikel et al 1997). Finally, our newly 
constructed parser, like that of (Collins 1997), 
was based on a generative statistical model. 
Thus, each component of what would be the 
first three stages of our pipeline was based on 
227 
the same general class of statistical model. 
Although each model differed in its detailed 
probability structure, we believed that the 
essential elements of all three models could be 
generalized in a single probability model. 
If the single generalized model could then be 
extended to semantic anal);sis, all necessary 
sentence level processing would be contained 
in that model. Because generative statistical 
models had already proven successful for each 
of the first three stages, we were optimistic 
that some of their properties - especially their 
ability to learn from large amounts of data, and 
their robustness when presented with 
unexpected inputs - would also benefit 
semantic analysis. 
4 Representing Syntax and Semantics 
Jointly 
Our integrated model represents yntax and 
semantics jointly using augmented parse trees. 
In these trees, the standard TREEBANK 
structures are augmented to convey semantic 
information, that is, entities and relations. An 
example of an augmented parse tree is shown 
in Figure 3. The five key facts in this example 
are: 
? "Nance" is the name of a person. 
? "A paid consultant to ABC News" 
describes a person. 
t "ABC News" is the name of an 
organization. 
? The person described as "a paid consultant 
to ABC News" is employed by ABC News. 
? The person named "Nance" and the person 
described as "a paid consultant to ABC News" 
are the same person. 
Here, each "reportable" name or description is 
identified by a "-r" suffix attached to its 
semantic label. For example, "per-r" identifies 
"Nance" as a named person, and "per-desc-r" 
identifies "a paid consultant to ABC News" as 
a person description. Other labels indicate 
relations among entities. For example, the co- 
reference relation between "Nance" and "a 
paid consultant to ABC News" is indicated by 
"per-desc-of." In this case, because the 
argument does not connect directly to the 
relation, the intervening nodes are labeled with 
semantics "-ptr" to indicate the connection. 
Further details are discussed in the section 
Tree Augmentation. 
5 Creating the Training Data 
To train our integrated model, we required a 
large corpus of augmented parse trees. Since it 
was known that the MUC-7 evaluation data 
would be drawn from a variety of newswire 
sources, and that the articles would focus on 
rocket launches, it was important that our 
training corpus be drawn from similar sources 
and that it cover similar events. Thus, we did 
not consider simply adding semantic labels to 
the existing Penn TREEBANK, which is 
drawn from a single source - the Wall Street 
Journal - and is impoverished in articles about 
rocket launches. 
Instead, we applied an information retrieval 
system to select a large number of articles 
from the desired sources, yielding a corpus 
rich in the desired types of events. The 
retrieved articles would then be annotated with 
augmented tree structures to serve as a training 
corpus. 
Initially, we tried to annotate the training 
corpus by hand marking, for each sentence, the 
entire augmented tree. It soon became 
painfully obvious that this task could not be 
performed in the available time. Our 
annotation staff found syntactic analysis 
particularly complex and slow going. By 
necessity, we adopted the strategy of hand 
marking only the semantics. 
Figure 4 shows an example of the semantic 
annotation, which was the only type of manual 
annotation we performed. 
To produce a corpus of augmented parse trees, 
we used the following multi-step training 
procedure which exploited the Penn 
TREEBANK 
228 
S 
per/np vp 
per-r/np 
I 
per/nnp 
I 
Nance , who is also a paid consultant to 
/ ~~-~1~p \ /// \ 
/ / I / o rg~ \ 
, wp vbz rb det vbn per-desc/nn to org'/nnporg/nnp , vbd 
I I I I I I I I I I I I 
ABe News , said ... 
Figure 3: An example of an augmented parse tree. 
1. The model (see Section 7) was first trained 
on purely syntactic parse trees from the 
TREEBANK, producing a model capable 
of broad-coverage syntactic parsing. 
parses that were consistent with the 
semantic annotation. A parse was 
considered consistent if no syntactic 
constituents crossed an annotated entity or 
description boundary. 
2. Next, for each sentence in the semantically 
annotated corpus: 
a. The model was applied to parse the 
sentence, constrained to produce only 
b. The resulting parse tree was then 
augmented to reflect semantic structure in 
addition to syntactic structure. 
/ 
F.?rso?l 
Nance 
coreference ~ employee  .ation 
person-descriptor -. 
Iorganization 1
, who is also a paid consultant to ABC News said ... 
Figure 4: An example of semantic annotation. 
229 
Applying this procedure yielded a new version 
of the semantically annotated corpus, now 
annotated with complete augmented trees like 
that in Figure 3. 
6 Tree Augmentation 
In this section, we describe the algorithm that 
was used to automatically produce augmented 
trees, starting with a) human-generated 
semantic annotations and b) machine- 
generated syntactic parse trees. For each 
sentence, combining these two sources 
involved five steps. These steps are given 
below: 
Tree Augmentation Algorithm 
. Nodes are inserted into the parse tree to 
distinguish names and descriptors that are 
not bracketed in the parse. For example, 
the parser produces a single noun phrase 
with no internal structure for "Lt. Cmdr. 
David Edwin Lewis". Additional nodes 
must be inserted to distinguish the 
description, "Lt. Cmdr.," and the name, 
"David Edwin Lewis." 
. Semantic labels are attached to all nodes 
that correspond to names or descriptors. 
These labels reflect he entity type, such as 
person, organization, or location, as well 
as whether the node is a proper name or a 
descriptor. 
. For relations between entities, where one 
entity is not a syntactic modifier of the 
other, the lowermost parse node that spans 
both entities is identified. A semantic tag 
is then added to that node denoting the 
relationship. For example, in the sentence 
"Mary Fackler Schiavo is the inspector 
general of the U.S. Department of 
Transportation," a co-reference semantic 
label is added to the S node spanning the 
name, "Mary Fackler Schiavo," and the 
descriptor, "the inspector general of the 
U.S. Department of Transportation." 
. Nodes are inserted into the parse tree to 
distinguish the arguments to each relation. 
In cases where there is a relation between 
two entities, and one of the entities is a 
syntactic modifier of the other, the inserted 
node serves to indicate the relation as well 
as the argument. For example, in the 
phrase "Lt. Cmdr. David Edwin Lewis," a 
node is inserted to indicate that "Lt. 
Cmdr." is a descriptor for "David Edwin 
Lewis." 
. Whenever a relation involves an entity that 
is not a direct descendant of that relation 
in the parse tree, semantic pointer labels 
are attached to all of the intermediate 
nodes. These labels serve to form a 
continuous chain between the relation and 
its argument. 
7 Model Structure 
In our statistical model, trees are generated 
according to a process imilar to that described 
in (Collins 1996, 1997). The detailed 
probability structure differs, however, in that it 
was designed to jointly perform part-of-speech 
tagging, name finding, syntactic parsing, and 
relation finding in a single process. 
For each constituent, the head is generated 
first, followed by the modifiers, which are 
generated from the head outward. Head 
words, along with their part-of-speech tags and 
features, are generated for each modifier as 
soon as the modifier is created. Word features 
are introduced primarily to help with unknown 
words, as in (Weischedel et al 1993). 
We illustrate the generation process by 
walking through a few of the steps of the parse 
shown in Figure 3. At each step in the 
process, a choice is made from a statistical 
distribution, with the probability of each 
possible selection dependent on particular 
features of previously generated elements. We 
pick up the derivation just after the topmost S 
and its head word, said, have been produced. 
The next steps are to generate in order: 
1. A head constituent for the S, in this case a 
VP. 
2. Pre-modifier constituents for the S. In this 
case, there is only one: a PER/NP. 
3. A head part-of-speech tag for the PER/NP, 
in this case PER/NNP. 
230 
4. A head word for the PER/NP, in this case 
nance. 
5. Word features for the head word of the 
PER/NP, in this case capitalized. 
6. A head constituent for the PER/NP, in this 
case a PER-R/NP. 
7. Pre-modifier constituents for the PER/NP. 
In this case, there are none. 
. Post-modifier constituents for the 
PER/NP. First a comma, then an SBAR 
structure, and then a second comma are 
each generated in turn. 
This generation process is continued until the 
entire tree has been produced. 
We now briefly summarize the probability 
structure of the model. The categories for 
head constituents, ch, are predicted based 
solely on the category of the parent node, cp: 
e(c h Icp), e.g. P(vpls )
Modifier constituent categories, Cm, are 
predicted based on their parent node, cp, the 
head constituent of their parent node, Chp, the 
previously generated modifier, Cm-1, and the 
head word of their parent, wp. Separate 
probabilities are maintained for left (pre) and 
right (post) modifiers: 
PL (Cm I Cp,Chp,Cm_l,Wp), e.g. 
PL ( per I np I s, vp, null, said) 
PR(c~ I Ce,Ch~,Cm-l, Wp), e.g. 
PR(null \[ s, vp, null, said) 
Part-of-speech tags, tin, for modifiers are 
predicted based on the modifier, Cm, the part- 
of-speech tag of the head word, th, and the 
head word itself, wh: 
P(t m ICm,th,wh), e.g. 
P(per / nnp \[ per /np, vbd, said) 
Head words, win, for modifiers are predicted 
based on the modifier, cm, the part-of-speech 
tag of the modifier word , t,,, the part-of- 
speech tag of the head word, th, and the head 
word itself, Wh: 
P(W m ICm,tmth,Wh),  e.g. 
P(nance I per / np, per / nnp, vbd, said) 
Finally, word features, fro, for modifiers are 
predicted based on the modifier, cm, the part- 
of-speech tag of the modifier word , tin, the 
part-of-speech tag of the head word , th, the 
head word itself, Wh, and whether or not the 
modifier head word, w,,, is known or unknown. 
P(fm \[Cm,tm,th,Wh,known(Wm)), e.g. 
P( cap I per I np, per / nnp, vbd, said, true) 
The probability of a complete tree is the 
product of the probabilities of generating each 
element in the tree. If we generalize the tree 
components (constituent labels, words, tags, 
etc.) and treat them all as simply elements, e, 
and treat all the conditioning factors as the 
history, h, we can write: 
P(tree) = H e(e I h) 
e~tree 
8 Training the Model 
Maximum likelihood estimates for the model 
probabilities can be obtained by observing 
frequencies in the training corpus. However, 
because these estimates are too sparse to be 
relied upon, we use interpolated estimates 
consisting of mixtures of successively lower- 
order estimates (as in Placeway et al 1993). 
For modifier constituents, 
components are: 
P'(cm I cp, chp, Cm_ l , w p) = 
21 P(c,, ICp,Chp,C,,_I,W,) 
+22 P(cm I%,chp,Cm_,) 
the mixture 
For part-of-speech tags, 
components are: 
P'(t m ICm,th,Wh)=21 P(t m Icm,wh) 
+'~2 e(tm I cm, th) 
+~3 P(t,, I C~,) 
the mixture 
For head words, the mixture components are: 
P'(w m I Cm,tm,th, wh) = JL 1 P(w m I Cm,tm, Wh) 
+22 P(wm Icm,tm,th) 
+23 P(w m I Cm,t,,) 
+~4 P(w, It,,) 
Finally, for word features, the mixture 
components are: 
231 
P'(f,, \[c,,,t~,t h, w h, known(w,,)) = 
21 P(f,, )c,,,t,,,wh,known(w,,)) 
+)\[2 e(f., \[c~,t,,,th,kn?wn(w,,)) 
+A3 e(L, \[c,,,t ,,known(w,,)) 
+As P(fm \[t,,,known(w,,)) 
9 Searching the Model 
Given a sentence to be analyzed, the search 
program must find the most likely semantic 
and syntactic interpretation. More precisely, it
must find the most likely augmented parse 
tree. Although mathematically the model 
predicts tree elements in a top-down fashion, 
we search the space bottom-up using a chart- 
based search. The search is kept tractable 
through a combination of CKY-style dynamic 
programming and pruning of low probability 
elements. 
9.1 Dynamic Programming 
Whenever two or more constituents are 
equivalent relative to all possible later parsing 
decisions, we apply dynamic programming, 
keeping only the most likely constituent in the 
chart. Two constituents are considered 
equivalent if: 
1. They have identical category labels. 
2. Their head constituents have identical 
labels. 
3. They have the same head word. 
4. Their leftmost modifiers have identical 
labels. 
. Their rightmost modifiers have identical 
labels. 
9.2 Pruning 
Given multiple constituents that cover 
identical spans in the chart, only those 
constituents with probabilities within a 
threshold of the highest scoring constituent are 
maintained; all others are pruned. For 
purposes of pruning, and only for purposes of 
pruning, the prior probability of each 
constituent category is multiplied by the 
generative probability of that constituent 
(Goodman, 1997). We can think of this prior 
probability as an estimate of the probability of 
generating a subtree with the constituent 
category, starting at the topmost node. Thus, 
the scores used in pruning can be considered 
as the product of: 
. The probability of generating a constituent 
of the specified category, starting at the 
topmost node. 
. The probability of generating the structure 
beneath that constituent, having already 
generated a constituent ofthat category. 
Given a new sentence, the outcome of this 
search process is a tree structure that encodes 
both the syntactic and semantic structure of the 
sentence. The semantics - that is, the entities 
and relations - can then be directly extracted 
from these sentential trees. 
10 Experimental Results 
Our system for MUC-7 consisted of the 
sentential model described in this paper, 
coupled with a simple probability model for 
cross-sentence merging. The evaluation 
results are summarized in Table 1. 
In both Template Entity (TE) and Template 
Relation (TR), our system finished in second 
place among all entrants. Nearly all of the 
work was done by the sentential model; 
disabling the cross-sentence model entirely 
reduced our overall F-Score by only 2 points. 
Task Recall Precision 
Entities (TE) 83% 84% 
Relations (TR) 64% 81% 
Table 1:MUC-7 scores. 
F-Score 
83.49% 
71.23% 
232 
Task Score 
Part-of-Speech Tagging 95.99 (% correct) 
Parsing (sentences <40 words) 85.06 (F-Score) 
Name Finding 92.28 (F-Score) 
Table 2: Component task performance. 
While our focus throughout the project was on 
TE and TR, we became curious about how 
well the model did at part-of-speech tagging, 
syntactic parsing, and at name finding. We 
evaluated part-of-speech tagging and parsing 
accuracy on the Wall Street Journal using a 
now standard procedure (see Collins 97), and 
evaluated name finding accuracy on the MUC- 
7 named entity test. The results are 
summarized in Table 2. 
While performance did not quite match the 
best previously reported results for any of 
these three tasks, we were pleased to observe 
that the scores were at or near state-of-the-art 
levels for all cases. 
11 Conclusions 
We have demonstrated, at least for one 
problem, that a lexicalized, probabilistic 
context-free parser with head rules (LPCFG- 
HR) can be used effectively for information 
extraction. A single model proved capable of 
performing all necessary sentential processing, 
both syntactic and semantic. We were able to 
use the Penn TREEBANK to estimate the 
syntactic parameters; no additional syntactic 
training was required. The semantic training 
corpus was produced by students according to 
a simple set of guidelines. This simple 
semantic annotation was the only source of 
task knowledge used to configure the model. 
Acknowledgements 
The work reported here was supported in part 
by the Defense Advanced Research Projects 
Agency. Technical agents for part of this work 
were Fort Huachucha and AFRL under 
contract numbers DABT63-94-C-0062, 
F30602-97-C-0096, and 4132-BBN-001. The 
views and conclusions contained in this 
document are those of the authors and should 
not be interpreted as necessarily representing 
the official policies, either expressed or 
implied, of the Defense Advanced Research 
Projects Agency or the United States 
Government. 
We thank Michael Collins of the University of 
Pennsylvania for his valuable suggestions. 
References 
Bikel, Dan; S. Miller; R. Schwartz; and R. 
Weischedel. (1997) "NYMBLE: A High- 
Performance Learning Name-finder." In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing, Association for 
Computational Linguistics, pp. 194-201. 
Collins, Michael. (1996) "A New Statistical Parser 
Based on Bigram Lexical Dependencies." In
Proceedings of the 34th Annual Meeting of the 
Association for Computational Linguistics, pp. 
184-191. 
Collins, Michael. (1997) "Three Generative, 
Lexicalised Models for Statistical Parsing." In 
Proceedings of the 35th Annual Meeting of the 
Association for Computational Linguistics, pp. 
16-23. 
Marcus, M.; B. Santorini; and M. Marcinkiewicz. 
(1993) "Building a Large Annotated Corpus of 
English: the Penn Treebank." Computational 
Linguistics, 19(2):313-330. 
Goodman, Joshua. (1997) "Global Thresholding 
and Multiple-Pass Parsing." In Proceedings of 
the Second Conference on Empirical Methods in 
Natural Language Processing, Association for 
Computational Linguistics, pp. 11-25. 
Placeway, P., R. Schwartz, et al (1993). "The 
Estimation of Powerful Language Models from 
Small and Large Corpora." IEEE ICASSP 
Weischedel, Ralph; Marie Meteer; Richard 
Schwartz; Lance Ramshaw; and Jeff Palmucci. 
(1993) "Coping with Ambiguity and Unknown 
Words through Probabilistic Models." 
Computational Linguistics, 19(2):359-382. 
233 
