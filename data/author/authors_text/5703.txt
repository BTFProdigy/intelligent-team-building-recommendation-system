Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 393?400,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Event Durations from Event Descriptions 
 
 
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs 
Information Sciences Institute (ISI), University of Southern California 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 
{pan, rutu, hobbs}@isi.edu 
 
  
 
Abstract 
We have constructed a corpus of news ar-
ticles in which events are annotated for 
estimated bounds on their duration. Here 
we describe a method for measuring in-
ter-annotator agreement for these event 
duration distributions. We then show that 
machine learning techniques applied to 
this data yield coarse-grained event dura-
tion information, considerably outper-
forming a baseline and approaching hu-
man performance. 
1 Introduction 
Consider the sentence from a news article: 
George W. Bush met with Vladimir Putin in 
Moscow. 
How long was the meeting?  Our first reaction 
to this question might be that we have no idea.  
But in fact we do have an idea.  We know the 
meeting was longer than 10 seconds and less 
than a year.  How much tighter can we get the 
bounds to be?  Most people would say the meet-
ing lasted between an hour and three days. 
There is much temporal information in text 
that has hitherto been largely unexploited, en-
coded in the descriptions of events and relying 
on our knowledge of the range of usual durations 
of types of events.  This paper describes one part 
of an exploration into how this information can 
be captured automatically.  Specifically, we have 
developed annotation guidelines to minimize dis-
crepant judgments and annotated 58 articles, 
comprising 2288 events; we have developed a 
method for measuring inter-annotator agreement 
when the judgments are intervals on a scale; and 
we have shown that machine learning techniques 
applied to the annotated data considerably out-
perform a baseline and approach human per-
formance.   
This research is potentially very important in 
applications in which the time course of events is 
to be extracted from news. For example, whether 
two events overlap or are in sequence often de-
pends very much on their durations.  If a war 
started yesterday, we can be pretty sure it is still 
going on today.  If a hurricane started last year, 
we can be sure it is over by now. 
The corpus that we have annotated currently 
contains all the 48 non-Wall-Street-Journal (non-
WSJ) news articles (a total of 2132 event in-
stances), as well as 10 WSJ articles (156 event 
instances), from the TimeBank corpus annotated 
in TimeML (Pustejovky et al, 2003). The non-
WSJ articles (mainly political and disaster news) 
include both print and broadcast news that are 
from a variety of news sources, such as ABC, 
AP, and VOA. 
In the corpus, every event to be annotated was 
already identified in TimeBank.  Annotators 
were instructed to provide lower and upper 
bounds on the duration of the event, encompass-
ing 80% of the possibilities, excluding anoma-
lous cases, and taking the entire context of the 
article into account. For example, here is the 
graphical output of the annotations (3 annotators) 
for the ?finished? event (underlined) in the sen-
tence 
After the victim, Linda Sanders, 35, had fin-
ished her cleaning and was waiting for her 
clothes to dry,... 
 
393
This graph shows that the first annotator be-
lieves that the event lasts for minutes whereas the 
second annotator believes it could only last for 
several seconds. The third annotates the event to 
range from a few seconds to a few minutes. A 
logarithmic scale is used for the output because 
of the intuition that the difference between 1 sec-
ond and 20 seconds is significant, while the dif-
ference between 1 year 1 second and 1 year 20 
seconds is negligible.  
A preliminary exercise in annotation revealed 
about a dozen classes of systematic discrepancies 
among annotators? judgments.  We thus devel-
oped guidelines to make annotators aware of 
these cases and to guide them in making the 
judgments.  For example, many occurrences of 
verbs and other event descriptors refer to multi-
ple events, especially but not exclusively if the 
subject or object of the verb is plural.  In ?Iraq 
has destroyed its long-range missiles?, there is 
the time it takes to destroy one missile and the 
duration of the interval in which all the individ-
ual events are situated ? the time it takes to de-
stroy all its missiles.  Initially, there were wide 
discrepancies because some annotators would 
annotate one value, others the other.  Annotators 
are now instructed to make judgments on both 
values in this case.  The use of the annotation 
guidelines resulted in about 10% improvement in 
inter-annotator agreement (Pan et al, 2006), 
measured as described in Section 2. 
There is a residual of gross discrepancies in 
annotators? judgments that result from differ-
ences of opinion, for example, about how long a 
government policy is typically in effect.  But the 
number of these discrepancies was surprisingly 
small. 
The method and guidelines for annotation are 
described in much greater detail in (Pan et al, 
2006).  In the current paper, we focus on how 
inter-annotator agreement is measured, in Sec-
tion 2, and in Sections 3-5 on the machine learn-
ing experiments.  Because the annotated corpus 
is still fairly small, we cannot hope to learn to 
make fine-grained judgments of event durations 
that are currently annotated in the corpus, but as 
we demonstrate, it is possible to learn useful 
coarse-grained judgments.   
Although there has been much work on tem-
poral anchoring and event ordering in text 
(Hitzeman et al, 1995; Mani and Wilson, 2000; 
Filatova and Hovy, 2001; Boguraev and Ando, 
2005), to our knowledge, there has been no seri-
ous published empirical effort to model and learn 
vague and implicit duration information in natu-
ral language, such as the typical durations of 
events, and to perform reasoning over this infor-
mation. (Cyc apparently has some fuzzy duration 
information, although it is not generally avail-
able; Rieger (1974) discusses the issue for less 
than a page; there has been work in fuzzy logic 
on representing and reasoning with imprecise 
durations (Godo and Vila, 1995; Fortemps, 
1997), but these make no attempt to collect hu-
man judgments on such durations or learn to ex-
tract them automatically from texts.) 
2 Inter-Annotator Agreement 
Although the graphical output of the annotations 
enables us to visualize quickly the level of agree-
ment among different annotators for each event, 
a quantitative measurement of the agreement is 
needed. 
The kappa statistic (Krippendorff, 1980; Car-
letta, 1996) has become the de facto standard to 
assess inter-annotator agreement. It is computed 
as: 
)(1
)()(
EP
EPAP
?
?=?  
P(A) is the observed agreement among the an-
notators, and P(E) is the expected agreement, 
which is the probability that the annotators agree 
by chance.  
In order to compute the kappa statistic for our 
task, we have to compute P(A) and P(E), but 
those computations are not straightforward.  
P(A): What should count as agreement among 
annotators for our task?  
P(E): What is the probability that the annota-
tors agree by chance for our task? 
2.1 What Should Count as Agreement? 
Determining what should count as agreement is 
not only important for assessing inter-annotator 
agreement, but is also crucial for later evaluation 
of machine learning experiments. For example, 
for a given event with a known gold standard 
duration range from 1 hour to 4 hours, if a ma-
chine learning program outputs a duration of 3 
hours to 5 hours, how should we evaluate this 
result? 
In the literature on the kappa statistic, most au-
thors address only category data; some can han-
dle more general data, such as data in interval 
scales or ratio scales. However, none of the tech-
niques directly apply to our data, which are 
ranges of durations from a lower bound to an 
upper bound. 
394
 
Figure 1: Overlap of Judgments of [10 minutes, 
30 minutes] and [10 minutes, 2 hours]. 
 
In fact, what coders were instructed to anno-
tate for a given event is not just a range, but a 
duration distribution for the event, where the 
area between the lower bound and the upper 
bound covers about 80% of the entire distribution 
area. Since it?s natural to assume the most likely 
duration for such distribution is its mean (aver-
age) duration, and the distribution flattens out 
toward the upper and lower bounds, we use the 
normal or Gaussian distribution to model our 
duration distributions. If the area between lower 
and upper bounds covers 80% of the entire dis-
tribution area, the bounds are each 1.28 standard 
deviations from the mean.  
Figure 1 shows the overlap in distributions for 
judgments of [10 minutes, 30 minutes] and [10 
minutes, 2 hours], and the overlap or agreement 
is 0.508706. 
2.2 Expected Agreement 
What is the probability that the annotators agree 
by chance for our task? The first quick response 
to this question may be 0, if we consider all the 
possible durations from 1 second to 1000 years 
or even positive infinity. 
However, not all the durations are equally pos-
sible. As in (Krippendorff, 1980), we assume 
there exists one global distribution for our task 
(i.e., the duration ranges for all the events), and 
?chance? annotations would be consistent with 
this distribution. Thus, the baseline will be an 
annotator who knows the global distribution and 
annotates in accordance with it, but does not read 
the specific article being annotated. Therefore, 
we must compute the global distribution of the 
durations, in particular, of their means and their 
widths. This will be of interest not only in deter-
mining expected agreement, but also in terms of  
-5 0 5 10 15 20 25 30
0
20
40
60
80
100
120
140
160
180
Means of Annotated Durations
N
um
be
r o
f A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 2: Distribution of Means of Annotated 
Durations. 
 
what it says about the genre of news articles and 
about fuzzy judgments in general. 
We first compute the distribution of the means 
of all the annotated durations. Its histogram is 
shown in Figure 2, where the horizontal axis 
represents the mean values in the natural loga-
rithmic scale and the vertical axis represents the 
number of annotated durations with that mean. 
There are two peaks in this distribution. One is 
from 5 to 7 in the natural logarithmic scale, 
which corresponds to about 1.5 minutes to 30 
minutes. The other is from 14 to 17 in the natural 
logarithmic scale, which corresponds to about 8 
days to 6 months. One could speculate that this 
bimodal distribution is because daily newspapers 
report short events that happened the day before 
and place them in the context of larger trends.  
We also compute the distribution of the widths 
(i.e., Xupper ? Xlower) of all the annotated durations, 
and its histogram is shown in Figure 3, where the 
horizontal axis represents the width in the natural 
logarithmic scale and the vertical axis represents 
the number of annotated durations with that 
width. Note that it peaks at about a half order of 
magnitude (Hobbs and Kreinovich, 2001).  
Since the global distribution is determined by 
the above mean and width distributions, we can 
then compute the expected agreement, i.e., the 
probability that the annotators agree by chance, 
where the chance is actually based on this global 
distribution. 
Two different methods were used to compute 
the expected agreement (baseline), both yielding 
nearly equal results. These are described in detail 
in (Pan et al, 2006). For both, P(E) is about 0.15. 
 
395
-5 0 5 10 15 20 25
0
50
100
150
200
250
300
350
400
Widths of Annotated Durations
N
um
be
r 
of
 A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 3: Distribution of Widths of Annotated 
Durations. 
3 Features 
In this section, we describe the lexical, syntactic, 
and semantic features that we considered in 
learning event durations. 
3.1 Local Context 
For a given event, the local context features in-
clude a window of n tokens to its left and n to-
kens to its right, as well as the event itself, for n 
= {0, 1, 2, 3}. The best n determined via cross 
validation turned out to be 0, i.e., the event itself 
with no local context. But we also present results 
for n = 2 in Section 4.3 to evaluate the utility of 
local context. 
A token can be a word or a punctuation mark. 
Punctuation marks are not removed, because they 
can be indicative features for learning event du-
rations. For example, the quotation mark is a 
good indication of quoted reporting events, and 
the duration of such events most likely lasts for 
seconds or minutes, depending on the length of 
the quoted content. However, there are also cases 
where quotation marks are used for other pur-
poses, such as emphasis of quoted words and 
titles of artistic works. 
For each token in the local context, including 
the event itself, three features are included: the 
original form of the token, its lemma (or root 
form), and its part-of-speech (POS) tag. The 
lemma of the token is extracted from parse trees 
generated by the CONTEX parser (Hermjakob 
and Mooney, 1997) which includes rich context 
information in parse trees, and the Brill tagger 
(Brill, 1992) is used for POS tagging. 
The context window doesn?t cross the bounda-
ries of sentences. When there are not enough to-
kens on either side of the event within the win-
dow, ?NULL? is used for the feature values. 
Features Original Lemma POS 
Event signed sign VBD 
1token-after the the DT 
2token-after plan plan NN 
1token-before Friday Friday NNP 
2token-before on on IN 
Table 1: Local context features for the ?signed? 
event in sentence (1) with n = 2. 
 
The local context features extracted for the 
?signed? event in sentence (1) is shown in Table 
1 (with a window size n = 2). The feature vector 
is [signed, sign, VBD, the, the, DT, plan, plan, 
NN, Friday, Friday, NNP, on, on, IN]. 
 
(1) The two presidents on Friday signed the 
plan. 
3.2 Syntactic Relations 
The information in the event?s syntactic envi-
ronment is very important in deciding the dura-
tions of events. For example, there is a difference 
in the durations of the ?watch? events in the 
phrases ?watch a movie? and ?watch a bird fly?. 
For a given event, both the head of its subject 
and the head of its object are extracted from the 
parse trees generated by the CONTEX parser. 
Similarly to the local context features, for both 
the subject head and the object head, their origi-
nal form, lemma, and POS tags are extracted as 
features. When there is no subject or object for 
an event, ?NULL? is used for the feature values. 
For the ?signed? event in sentence (1), the 
head of its subject is ?presidents? and the head of 
its object is ?plan?. The extracted syntactic rela-
tion features are shown in Table 2, and the fea-
ture vector is [presidents, president, NNS, plan, 
plan, NN]. 
3.3 WordNet Hypernyms 
Events with the same hypernyms may have simi-
lar durations. For example, events ?ask? and 
?talk? both have a direct WordNet (Miller, 1990) 
hypernym of ?communicate?, and most of the 
time they do have very similar durations in the 
corpus. 
However, closely related events don?t always 
have the same direct hypernyms. For example, 
?see? has a direct hypernym of ?perceive?, 
whereas ?observe? needs two steps up through 
the hypernym hierarchy before reaching ?per-
ceive?. Such correlation between events may be 
lost if only the direct hypernyms of the words are 
extracted. 
396
Features Original Lemma POS 
Subject presidents president NNS 
Object plan plan NN 
Table 2: Syntactic relation features for the 
?signed? event in sentence (1). 
 
Feature 1-hyper 2-hyper 3-hyper 
Event write communicate interact 
Subject corporate executive executive 
adminis-
trator 
Object idea content cognition 
Table 3: WordNet hypernym features for the 
event (?signed?), its subject (?presidents?), and 
its object (?plan?) in sentence (1). 
 
It is useful to extract the hypernyms not only 
for the event itself, but also for the subject and 
object of the event. For example, events related 
to a group of people or an organization usually 
last longer than those involving individuals, and 
the hypernyms can help distinguish such con-
cepts. For example, ?society? has a ?group? hy-
pernym (2 steps up in the hierarchy), and 
?school? has an ?organization? hypernym (3 
steps up). The direct hypernyms of nouns are 
always not general enough for such purpose, but 
a hypernym at too high a level can be too general 
to be useful. For our learning experiments, we 
extract the first 3 levels of hypernyms from 
WordNet. 
Hypernyms are only extracted for the events 
and their subjects and objects, not for the local 
context words. For each level of hypernyms in 
the hierarchy, it?s possible to have more than one 
hypernym, for example, ?see? has two direct hy-
pernyms, ?perceive? and ?comprehend?. For a 
given word, it may also have more than one 
sense in WordNet. In such cases, as in (Gildea 
and Jurafsky, 2002), we only take the first sense 
of the word and the first hypernym listed for each 
level of the hierarchy. A word disambiguation 
module might improve the learning performance. 
But since the features we need are the hypernyms, 
not the word sense itself, even if the first word 
sense is not the correct one, its hypernyms can 
still be good enough in many cases. For example, 
in one news article, the word ?controller? refers 
to an air traffic controller, which corresponds to 
the second sense in WordNet, but its first sense 
(business controller) has the same hypernym of 
?person? (3 levels up) as the second sense (direct 
hypernym). Since we take the first 3 levels of 
hypernyms, the correct hypernym is still ex-
tracted. 
 
P(A) P(E) Kappa 
0.528 0.740 0.877 
0.500 0.755 
Table 4: Inter-Annotator Agreement for Binary 
Event Durations. 
 
When there are less than 3 levels of hy-
pernyms for a given word, its hypernym on the 
previous level is used. When there is no hy-
pernym for a given word (e.g., ?go?), the word 
itself will be used as its hypernyms. Since 
WordNet only provides hypernyms for nouns 
and verbs, ?NULL? is used for the feature values 
for a word that is not a noun or a verb.  
For the ?signed? event in sentence (1), the ex-
tracted WordNet hypernym features for the event 
(?signed?), its subject (?presidents?), and its ob-
ject (?plan?) are shown in Table 3, and the fea-
ture vector is [write, communicate, interact, cor-
porate_executive, executive, administrator, idea, 
content, cognition]. 
4 Experiments 
The distribution of the means of the annotated 
durations in Figure 2 is bimodal, dividing the 
events into those that take less than a day and 
those that take more than a day. Thus, in our first 
machine learning experiment, we have tried to 
learn this coarse-grained event duration informa-
tion as a binary classification task. 
4.1 Inter-Annotator Agreement, Baseline, 
and Upper Bound 
Before evaluating the performance of different 
learning algorithms, the inter-annotator agree-
ment, the baseline and the upper bound for the 
learning task are assessed first.  
Table 4 shows the inter-annotator agreement 
results among 3 annotators for binary event dura-
tions. The experiments were conducted on the 
same data sets as in (Pan et al, 2006). Two 
kappa values are reported with different ways of 
measuring expected agreement (P(E)), i.e., 
whether or not the annotators have prior knowl-
edge of the global distribution of the task. 
The human agreement before reading the 
guidelines (0.877) is a good estimate of the upper 
bound performance for this binary classification 
task. The baseline for the learning task is always 
taking the most probable class. Since 59.0% of 
the total data is ?long? events, the baseline per-
formance is 59.0%. 
 
 
397
Class Algor. Prec. Recall F-Score
SVM 0.707 0.606 0.653 
NB 0.567 0.768 0.652 Short 
C4.5 0.571 0.600 0.585 
SVM 0.793 0.857 0.823 
NB 0.834 0.665 0.740 
Long 
 
C4.5 0.765 0.743 0.754 
Table 5: Test Performance of Three Algorithms. 
4.2 Data 
The original annotated data can be straightfor-
wardly transformed for this binary classification 
task. For each event annotation, the most likely 
(mean) duration is calculated first by averaging 
(the logs of) its lower and upper bound durations. 
If its most likely (mean) duration is less than a 
day (about 11.4 in the natural logarithmic scale), 
it is assigned to the ?short? event class, otherwise 
it is assigned to the ?long? event class. (Note that 
these labels are strictly a convenience and not an 
analysis of the meanings of ?short? and ?long?.) 
We divide the total annotated non-WSJ data 
(2132 event instances) into two data sets: a train-
ing data set with 1705 event instances (about 
80% of the total non-WSJ data) and a held-out 
test data set with 427 event instances (about 20% 
of the total non-WSJ data). The WSJ data (156 
event instances) is kept for further test purposes 
(see Section 4.4). 
4.3 Experimental Results (non-WSJ) 
Learning Algorithms. Three supervised learn-
ing algorithms were evaluated for our binary 
classification task, namely, Support Vector Ma-
chines (SVM) (Vapnik, 1995), Na?ve Bayes 
(NB) (Duda and Hart, 1973), and Decision Trees 
C4.5 (Quinlan, 1993). The Weka (Witten and 
Frank, 2005) machine learning package was used 
for the implementation of these learning algo-
rithms. Linear kernel is used for SVM in our ex-
periments. 
Each event instance has a total of 18 feature 
values, as described in Section 3, for the event 
only condition, and 30 feature values for the lo-
cal context condition when n = 2. For SVM and 
C4.5, all features are converted into binary fea-
tures (6665 and 12502 features). 
Results. 10-fold cross validation was used to 
train the learning models, which were then tested 
on the unseen held-out test set, and the perform-
ance (including the precision, recall, and F-score1  
                                                 
1 F-score is computed as the harmonic mean of the preci-
sion and recall: F = (2*Prec*Rec)/(Prec+Rec). 
Algorithm Precision  
Baseline 59.0% 
C4.5 69.1% 
NB 70.3% 
SVM 76.6% 
Human Agreement 87.7% 
Table 6: Overall Test Precision on non-WSJ 
Data. 
 
for each class) of the three learning algorithms is 
shown in Table 5. The significant measure is 
overall precision, and this is shown for the three 
algorithms in Table 6, together with human a-
greement (the upper bound of the learning task) 
and the baseline. 
We can see that among all three learning algo-
rithms, SVM achieves the best F-score for each 
class and also the best overall precision (76.6%). 
Compared with the baseline (59.0%) and human 
agreement (87.7%), this level of performance is 
very encouraging, especially as the learning is 
from such limited training data. 
Feature Evaluation. The best performing 
learning algorithm, SVM, was then used to ex-
amine the utility of combinations of four differ-
ent feature sets (i.e., event, local context, syntac-
tic, and WordNet hypernym features). The de-
tailed comparison is shown in Table 7.  
We can see that most of the performance 
comes from event word or phrase itself. A sig-
nificant improvement above that is due to the 
addition of information about the subject and 
object. Local context does not help and in fact 
may hurt, and hypernym information also does 
not seem to help2. It is of interest that the most 
important information is that from the predicate 
and arguments describing the event, as our lin-
guistic intuitions would lead us to expect. 
4.4 Test on WSJ Data 
Section 4.3 shows the experimental results with 
the learned model trained and tested on the data 
with the same genre, i.e., non-WSJ articles. 
In order to evaluate whether the learned model 
can perform well on data from different news 
genres, we tested it on the unseen WSJ data (156 
event instances). The performance (including the 
precision, recall, and F-score for each class) is 
shown in Table 8. The precision (75.0%) is very 
close to the test performance on the non-WSJ  
                                                 
2 In the ?Syn+Hyper? cases, the learning algorithm with and 
without local context gives identical results, probably be-
cause the other features dominate. 
398
Event Only (n = 0) Event Only + Syntactic Event + Syn + Hyper Class 
Prec. Rec. F Prec. Rec. F Prec. Rec. F 
Short 0.742 0.465  0.571 0.758 0.587 0.662 0.707    0.606 0.653 
Long 0.748 0.908 0.821 0.792 0.893 0.839 0.793 0.857 0.823 
Overall Prec. 74.7% 78.2% 76.6% 
 Local Context (n = 2) Context + Syntactic Context + Syn + Hyper 
Short 0.672 0.568 0.615 0.710 0.600    0.650 0.707    0.606 0.653 
Long 0.774 0.842 0.806 0.791 0.860 0.824 0.793 0.857 0.823 
Overall Prec. 74.2% 76.6% 76.6% 
Table 7: Feature Evaluation with Different Feature Sets using SVM. 
 
Class Prec. Rec. F 
Short 0.692   0.610 0.649
Long 0.779   0.835 0.806
Overall Prec. 75.0% 
Table 8: Test Performance on WSJ data. 
 
P(A) P(E) Kappa 
0.151 0.762 0.798 
0.143 0.764 
Table 9: Inter-Annotator Agreement for Most 
Likely Temporal Unit. 
 
data, and indicates the significant generalization 
capacity of the learned model. 
5 Learning the Most Likely Temporal 
Unit 
These encouraging results have prompted us to 
try to learn more fine-grained event duration in-
formation, viz., the most likely temporal units of 
event durations (cf. (Rieger 1974)?s ORDER-
HOURS, ORDERDAYS). 
For each original event annotation, we can ob-
tain the most likely (mean) duration by averaging 
its lower and upper bound durations, and assign-
ing it to one of seven classes (i.e., second, min-
ute, hour, day, week, month, and year) based on 
the temporal unit of its most likely duration.  
However, human agreement on this more fine-
grained task is low (44.4%). Based on this obser-
vation, instead of evaluating the exact agreement 
between annotators, an ?approximate agreement? 
is computed for the most likely temporal unit of 
events. In ?approximate agreement?, temporal 
units are considered to match if they are the same 
temporal unit or an adjacent one. For example, 
?second? and ?minute? match, but ?minute? and 
?day? do not. 
Some preliminary experiments have been con-
ducted for learning this multi-classification task. 
The same data sets as in the binary classification 
task were used. The only difference is that the 
class for each instance is now labeled with one 
Algorithm Precision  
Baseline 51.5% 
C4.5 56.4% 
NB 65.8% 
SVM 67.9% 
Human Agreement 79.8% 
Table 10: Overall Test Precisions. 
 
of the seven temporal unit classes. 
The baseline for this multi-classification task 
is always taking the temporal unit which with its 
two neighbors spans the greatest amount of data. 
Since the ?week?, ?month?, and ?year? classes 
together take up largest portion (51.5%) of the 
data, the baseline is always taking the ?month? 
class, where both ?week? and ?year? are also 
considered a match. Table 9 shows the inter-
annotator agreement results for most likely tem-
poral unit when using ?approximate agreement?. 
Human agreement (the upper bound) for this 
learning task increases from 44.4% to 79.8%. 
10-fold cross validation was also used to train 
the learning models, which were then tested on 
the unseen held-out test set. The performance of 
the three algorithms is shown in Table 10. The 
best performing learning algorithm is again SVM 
with 67.9% test precision. Compared with the 
baseline (51.5%) and human agreement (79.8%), 
this again is a very promising result, especially 
for a multi-classification task with such limited 
training data. It is reasonable to expect that when 
more annotated data becomes available, the 
learning algorithm will achieve higher perform-
ance when learning this and more fine-grained 
event duration information. 
Although the coarse-grained duration informa-
tion may look too coarse to be useful, computers 
have no idea at all whether a meeting event takes 
seconds or centuries, so even coarse-grained es-
timates would give it a useful rough sense of how 
long each event may take. More fine-grained du-
ration information is definitely more desirable 
for temporal reasoning tasks. But coarse-grained 
399
durations to a level of temporal units can already 
be very useful. 
6 Conclusion 
In the research described in this paper, we have 
addressed a problem -- extracting information 
about event durations encoded in event descrip-
tions -- that has heretofore received very little 
attention in the field.  It is information that can 
have a substantial impact on applications where 
the temporal placement of events is important.  
Moreover, it is representative of a set of prob-
lems ? making use of the vague information in 
text ? that has largely eluded empirical ap-
proaches in the past.  In (Pan et al, 2006), we 
explicate the linguistic categories of the phenom-
ena that give rise to grossly discrepant judgments 
among annotators, and give guidelines on resolv-
ing these discrepancies.  In the present paper, we 
describe a method for measuring inter-annotator 
agreement when the judgments are intervals on a 
scale; this should extend from time to other sca-
lar judgments.  Inter-annotator agreement is too 
low on fine-grained judgments.  However, for the 
coarse-grained judgments of more than or less 
than a day, and of approximate agreement on 
temporal unit, human agreement is acceptably 
high.  For these cases, we have shown that ma-
chine-learning techniques achieve impressive 
results.   
Acknowledgments 
This work was supported by the Advanced Re-
search and Development Activity (ARDA), now 
the Disruptive Technology Office (DTO), under 
DOD/DOI/ARDA Contract No. NBCHC040027. 
The authors have profited from discussions with 
Hoa Trang Dang, Donghui Feng, Kevin Knight, 
Daniel Marcu, James Pustejovsky, Deepak Ravi-
chandran, and Nathan Sobo. 
References 
B. Boguraev and R. K. Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning. 
In Proceedings of International Joint Conference 
on Artificial Intelligence (IJCAI). 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceedings of the Third Conference on 
Applied Natural Language Processing. 
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
gustics, 22(2):249?254. 
R. O. Duda and P. E. Hart. 1973. Pattern Classifica-
tion and Scene Analysis. Wiley, New York. 
E. Filatova and E. Hovy. 2001. Assigning Time-
Stamps to Event-Clauses. Proceedings of ACL 
Workshop on Temporal and Spatial Reasoning. 
P. Fortemps. 1997. Jobshop Scheduling with Impre-
cise Durations: A Fuzzy Approach. IEEE Transac-
tions on Fuzzy Systems Vol. 5 No. 4. 
D. Gildea and D. Jurafsky. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics, 
28(3):245-288. 
L. Godo and L. Vila. 1995. Possibilistic Temporal 
Reasoning based on Fuzzy Temporal Constraints. 
In Proceedings of International Joint Conference 
on Artificial Intelligence (IJCAI). 
U. Hermjakob and R. J. Mooney. 1997. Learning 
Parse and Translation Decisions from Examples 
with Rich Context. In Proceedings of the 35th An-
nual Meeting of the Association for Computational 
Linguistics (ACL). 
J. Hitzeman, M. Moens, and C. Grover. 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of EACL. Dublin, Ire-
land. 
J. R. Hobbs and V. Kreinovich. 2001. Optimal Choice 
of Granularity in Commonsense Estimation: Why 
Half Orders of Magnitude, In Proceedings of Joint 
9th IFSA World Congress and 20th NAFIPS Inter-
national Conference, Vacouver, British Columbia. 
K. Krippendorf. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications. 
I. Mani and G. Wilson. 2000. Robust Temporal Proc-
essing of News. In Proceedings of the 38th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL). 
G. A. Miller. 1990. WordNet: an On-line Lexical Da-
tabase. International Journal of Lexicography 3(4). 
F. Pan, R. Mulkar, and J. R. Hobbs. 2006. An Anno-
tated Corpus of Typical Durations of Events. In 
Proceedings of the Fifth International Conference 
on Language Resources and Evaluation (LREC), 
Genoa, Italy. 
J. Pustejovsky, P. Hanks, R. Saur?, A. See, R. Gai-
zauskas, A. Setzer, D. Radev, B. Sundheim, D. 
Day, L. Ferro and M. Lazo. 2003. The timebank 
corpus. In Corpus Linguistics, Lancaster, U.K. 
J. R. Quinlan. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Francisco. 
C. J. Rieger. 1974. Conceptual memory: A theory and 
computer program for processing and meaning 
content of natural language utterances. Stanford 
AIM-233. 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer-Verlag, New York. 
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques, 2nd 
Edition, Morgan Kaufmann, San Francisco. 
400
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 38?45,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extending TimeML with Typical Durations of Events 
 
 
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs 
Information Sciences Institute (ISI), University of Southern California 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 
{pan, rutu, hobbs}@isi.edu 
 
  
 
Abstract 
In this paper, we demonstrate how to ex-
tend TimeML, a rich specification lan-
guage for event and temporal expressions 
in text, with the implicit typical durations 
of events, temporal information in text 
that has hitherto been largely unexploited. 
Event duration information can be very 
important in applications in which the 
time course of events is to be extracted 
from text. For example, whether two 
events overlap or are in sequence often 
depends very much on their durations.     
1 Introduction 
Temporal information processing has become 
more and more important in many natural lan-
guage processing (NLP) applications, such as 
question answering (Harabagiu and Bejan, 2005; 
Moldovan et. al., 2005; Saur? et. al., 2005), 
summarization (Mani and Schiffman, 2005), and 
information extraction (Surdeanu et. al., 2003). 
Temporal anchoring and event ordering are 
among the most important kinds of temporal in-
formation needed for NLP applications. Al-
though there has been much work on extracting 
and inferring such information from texts 
(Hitzeman et al, 1995; Mani and Wilson, 2000; 
Filatova and Hovy, 2001; Boguraev and Ando, 
2005), none of this work has exploited the im-
plicit event duration information from the text.  
Consider the sentence from a news article: 
George W. Bush met with Vladimir Putin in 
Moscow. 
How long was the meeting?  Our first reaction 
to this question might be that we have no idea.  
But in fact we do have an idea.  We know the 
meeting was longer than 10 seconds and less 
than a year.  How much tighter can we get the 
bounds to be?  Most people would say the meet-
ing lasted between an hour and three days. 
There is much temporal information in text 
that has hitherto been largely unexploited, en-
coded in the descriptions of events and relying 
on our knowledge of the range of usual durations 
of types of events, which can be very important 
in applications in which the time course of events 
is to be extracted from news.  For example, 
whether two events overlap or are in sequence 
often depends very much on their durations.  If a 
war started yesterday, we can be pretty sure it is 
still going on today.  If a hurricane started last 
year, we can be sure it is over by now. 
To extract such implicit event duration infor-
mation from texts automatically, we developed a 
corpus annotated with typical durations of events 
(Pan et al, 2006a) which currently contains all 
the 48 non-Wall-Street-Journal (non-WSJ) news 
articles (a total of 2132 event instances), as well 
as 10 WSJ articles (156 event instances), from 
the TimeBank corpus annotated in TimeML 
(Pustejovky et al, 2003).  
Because the annotated corpus is still fairly 
small, we cannot hope to learn to make fine-
grained judgments of event durations that are 
currently annotated in the corpus, but as we show 
in greater detail in (Pan et al, 2006b), it is possi-
ble to learn useful coarse-grained judgments that 
considerably outperform a baseline and approach 
human performance. 
This paper describes our work on extending 
TimeML with annotations of typical durations of 
events, which can enrich the expressiveness of 
TimeML, and provides NLP applications that 
exploit TimeML with this additional implicit 
event duration information for their temporal 
information processing tasks. 
In Section 2 we first describe the corpus of 
typical durations of events, including the annota-
tion guidelines, the representative event classes 
with examples, the inter-annotator agreement 
38
study, and the machine learning results. TimeML 
and its event classes will be described in Section 
3, and we will discuss how to integrate event du-
ration annotations into TimeML in Section 4.  
2 Annotating and Learning Typical Du-
ration of Events 
In the corpus of typical durations of events, every 
event to be annotated was already identified in 
the TimeBank corpus.  Annotators are asked to 
provide lower and upper bounds on the duration 
of the event, and a judgment of level of confi-
dence in those estimates on a scale from one to 
ten. An interface was built to facilitate the anno-
tation. Graphical output is displayed to enable us 
to visualize quickly the level of agreement 
among different annotators for each event. For 
example, here is the output of the annotations (3 
annotators) for the ?finished? event (in bold) in 
the sentence 
After the victim, Linda Sanders, 35, had fin-
ished her cleaning and was waiting for her 
clothes to dry,... 
 
 
This graph shows that the first annotator believes 
that the event lasts for minutes whereas the sec-
ond annotator believes it could only last for sev-
eral seconds. The third annotates the event to 
range from a few seconds to a few minutes. A 
logarithmic scale is used for the output. 
2.1 Annotation Instructions 
Annotators are asked to identify upper and lower 
bounds that would include 80% of the possible 
cases, excluding anomalous cases.   
The judgments are to be made in context.  
First of all, information in the syntactic environ-
ment needs to be considered before annotating, 
and the events need to be annotated in light of 
the information provided by the entire article. 
Annotation is made easier and more consistent if 
coreferential and near-coreferential descriptions 
of events are identified initially. 
When the articles were completely annotated 
by the three annotators, the results were analyzed 
and the differences were reconciled. Differences 
in annotation could be due to the differences in 
interpretations of the event; however, we found 
that the vast majority of radically different judg-
ments can be categorized into a relatively small 
number of classes. Some of these correspond to 
aspectual features of events, which have been 
intensively investigated (e.g., Vendler, 1967; 
Dowty, 1979; Moens and Steedman, 1988; Pas-
sonneau, 1988). We then developed guidelines to 
cover those cases (see the next section). 
2.2 Event Classes 
Action vs. State: Actions involve change, such 
as those described by words like "speaking", 
"gave", and "skyrocketed". States involve things 
staying the same, such as being dead, being dry, 
and being at peace. When we have an event in 
the passive tense, sometimes there is an ambigu-
ity about whether the event is a state or an action. 
For example, 
Three people were injured in the attack. 
Is the ?injured? event an action or a state? This 
matters because they will have different dura-
tions. The state begins with the action and lasts 
until the victim is healed. Besides the general 
diagnostic tests to distinguish them (Vendler, 
1967; Dowty, 1979), another test can be applied 
to this specific case: Imagine someone says the 
sentence after the action had ended but the state 
was still persisting. Would they use the past or 
present tense? In the ?injured? example, it is 
clear we would say ?Three people were injured 
in the attack?, whereas we would say ?Three 
people are injured from the attack.? Our annota-
tion interface handles events of this type by al-
lowing the annotator to specify which interpreta-
tion he is giving. If the annotator feels it?s too 
ambiguous to distinguish, annotations can be 
given for both interpretations. 
 
Aspectual Events:  Some events are aspects of 
larger events, such as their start or finish. Al-
though they may seem instantaneous, we believe 
they should be considered to happen across some 
interval, i.e., the first or last sub-event of the lar-
ger event. For example,   
 After the victim, Linda Sanders, 35, had fin-
ished her cleaning and was waiting for her 
clothes to dry,? 
The ?finished? event should be considered as the 
last sub-event of the larger event (the ?cleaning? 
event), since it actually involves opening the 
door of the washer, taking out the clothes, clos-
ing the door, and so on. All this takes time. This 
39
interpretation will also give us more information 
on typical durations than simply assuming such 
events are instantaneous. 
 
Reporting Events: These are everywhere in the 
news. They can be direct quotes, taking exactly 
as long as the sentence takes to read, or they can 
be summarizations of long press conferences. We 
need to distinguish different cases: 
Quoted Report: This is when the reported 
content is quoted. The duration of the event 
should be the actual duration of the utterance of 
the quoted content. The time duration can be eas-
ily verified by saying the sentence out loud and 
timing it. For example, 
"It looks as though they panicked," a detective 
said of the robbers. 
This probably took between 1 and 3 seconds; it?s 
very unlikely it took more than 10 seconds. 
Unquoted Report: This is when the reporting 
description occurs without quotes that could be 
as short as just the duration of the actual utter-
ance of the reported content (lower bound), and 
as long as the duration of a briefing or press con-
ference (upper bound). 
If the sentence is very short, then it's likely 
that it is one complete sentence from the 
speaker's remarks, and a short duration should be 
given; if it is a long, complex sentence, then it's 
more likely to be a summary of a long discussion 
or press conference, and a longer duration should 
be given. For example, 
The police said it did not appear that anyone 
else was injured. 
A Brooklyn woman who was watching her 
clothes dry in a laundromat was killed Thursday 
evening when two would-be robbers emptied 
their pistols into the store, the police said. 
If the first sentence were quoted text, it would be 
very much the same. Hence the duration of the 
?said? event should be short. In the second sen-
tence everything that the spokesperson (here the 
police) has said is compiled into a single sen-
tence by the reporter, and it is unlikely that the 
spokesperson said only a single sentence with all 
this information. Thus, it is reasonable to give 
longer duration to this ?said? event. 
 
Multiple Events: Many occurrences of verbs 
and other event descriptors refer to multiple 
events, especially, but not exclusively, if the sub-
ject or object of the verb is plural.  For example,   
Iraq has destroyed its long-range missiles.  
Both single (i.e., destroyed one missile) and ag-
gregate (i.e., destroyed all missiles) events hap-
pened. This was a significant source in dis-
agreements in our first round of annotation. 
Since both judgments provide useful informa-
tion, our current annotation interface allows the 
annotator to specify the event as multiple, and 
give durations for both the single and aggregate 
events.  
 
Events Involving Negation: Negated events 
didn't happen, so it may seem strange to specify 
their duration. But whenever negation is used, 
there is a certain class of events whose occur-
rence is being denied. Annotators should con-
sider this class, and make a judgment about the 
likely duration of the events in it. In addition, 
there is the interval during which the nonoccur-
rence of the events holds. For example,  
He was willing to withdraw troops in ex-
change for guarantees that Israel would not be 
attacked. 
There is the typical amount of time of ?being 
attacked?, i.e., the duration of a single attack, and 
a longer period of time of ?not being attacked?. 
Similarly to multiple events, annotators are asked 
to give durations for both the event negated and 
the negation of that event.   
 
Positive Infinite Durations: These are states 
which continue essentially forever once they be-
gin. For example, 
He is dead. 
Here the time continues for an infinite amount 
of time, and we allow this as an annotation. 
2.3 Inter-Annotator Agreement 
Although the graphical output of the annotations 
enables us to visualize quickly the level of agree-
ment among different annotators for each event, 
a quantitative measurement of the agreement is 
needed. The kappa statistic (Krippendorff, 1980; 
Carletta, 1996) has become the de facto standard 
to assess inter-annotator agreement. It is com-
puted as: 
)(1
)()(
EP
EPAP
?
?=?  
P(A) is the observed agreement among the an-
notators, and P(E) is the expected agreement,  
 
40
 
Figure 1: Overlap of Judgments of [10 minutes, 
30 minutes] and [10 minutes, 2 hours]. 
 
which is the probability that the annotators agree 
by chance.  
2.3.1  What Should Count as Agreement? 
Determining what should count as agreement is 
not only important for assessing inter-annotator 
agreement, but is also crucial for later evaluation 
of machine learning experiments.  
We first need to decide what scale is most ap-
propriate. One possibility is just to convert all the 
temporal units to seconds. However, this would 
not correctly capture our intuitions about the 
relative relations between duration ranges. For 
example, the difference between 1 second and 20 
seconds is significant; while the difference be-
tween 1 year 1 second and 1 year 20 seconds is 
negligible. In order to handle this problem, we 
use a logarithmic scale for our data. After first 
converting from temporal units to seconds, we 
then take the natural logarithms of these values. 
This logarithmic scale also conforms to the half 
orders of magnitude (HOM) (Hobbs and Kreino-
vich, 2001) which was shown to have utility in 
several very different linguistic contexts. 
In the literature on the kappa statistic, most au-
thors address only category data; some can han-
dle more general data, such as data in interval 
scales or ratio scales (Krippendorff, 1980; Car-
letta, 1996). However, none of the techniques 
directly apply to our data, which are ranges of 
durations from a lower bound to an upper bound. 
In fact, what coders were instructed to anno-
tate for a given event is not just a range, but a 
duration distribution for the event, where the 
area between the lower bound and the upper 
bound covers about 80% of the entire distribution 
area. Since it?s natural to assume the most likely 
duration for such distribution is its mean (aver-
age) duration, and the distribution flattens out 
toward the upper and lower bounds, we use the  
-5 0 5 10 15 20 25 30
0
20
40
60
80
100
120
140
160
180
Means of Annotated Durations
N
um
be
r o
f A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 2: Distribution of Means of Annotated 
Durations. 
 
normal or Gaussian distribution to model our 
duration distributions. 
In order to determine a normal distribution, we 
need to know two parameters: the mean and the 
standard deviation. For our duration distributions 
with given lower and upper bounds, the mean is 
the average of the bounds. Under the assumption 
that the area between lower and upper bounds 
covers 80% of the entire distribution area, the 
lower and upper bounds are each 1.28 standard 
deviations from the mean.  
With this data model, the agreement between 
two annotations can be defined as the overlap-
ping area between two normal distributions. The 
agreement among many annotations is the aver-
age overlap of all the pairwise overlapping areas. 
For example, the overlap of judgments of [10 
minutes, 30 minutes] and [10 minutes, 2 hours] 
are as in Figure 1. The overlap or agreement is 
0.508706. 
2.3.2  Expected Agreement 
As in (Krippendorff, 1980), we assume there ex-
ists one global distribution for our task (i.e., the 
duration ranges for all the events), and ?chance? 
annotations would be consistent with this distri-
bution. Thus, the baseline will be an annotator 
who knows the global distribution and annotates 
in accordance with it, but does not read the spe-
cific article being annotated. Therefore, we must 
compute the global distribution of the durations, 
in particular, of their means and their widths. 
This will be of interest not only in determining 
expected agreement, but also in terms of what it 
says about the genre of news articles and about 
fuzzy judgments in general. 
We first compute the distribution of the means 
of all the annotated durations. Its histogram is 
shown in Figure 2, where the horizontal axis 
41
-5 0 5 10 15 20 25
0
50
100
150
200
250
300
350
400
Widths of Annotated Durations
N
um
be
r o
f A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 3: Distribution of Widths of Annotated 
Durations. 
 
represents the mean values in the natural loga-
rithmic scale and the vertical axis represents the 
number of annotated durations with that mean. 
We also compute the distribution of the widths 
(i.e., upper bound ? lower bound) of all the anno-
tated durations, and its histogram is shown in 
Figure 3, where the horizontal axis represents the 
width in the natural logarithmic scale and the 
vertical axis represents the number of annotated 
durations with that width. 
Two different methods were used to compute 
the expected agreement (baseline), both yielding 
nearly equal results. These are described in detail 
in (Pan et al, 2006a). For both, P(E) is about 
0.15. 
Experimental results show that the use of the 
annotation guidelines resulted in about 10% im-
provement in inter-annotator agreement, meas-
ured as described in this section, see (Pan et al, 
2006a) for details. 
2.4 Machine Learning Experiments 
2.4.1  Features 
Local Context. For a given event, the local con-
text features include a window of n tokens to its 
left and n tokens to its right, as well as the event 
itself. The best n was determined via cross vali-
dation. A token can be a word or a punctuation 
mark. For each token in the local context, includ-
ing the event itself, three features are included: 
the original form of the token, its lemma (or root 
form), and its part-of-speech (POS) tag. 
Syntactic Relations. The information in the 
event?s syntactic environment is very important 
in deciding the durations of events. For a given 
event, both the head of its subject and the head of 
its object are extracted from the parse trees gen-
erated by the CONTEX parser (Hermjakob and 
Mooney, 1997). Similarly to the local context 
features, for both the subject head and the object 
head, their original form, lemma, and POS tags 
are extracted as features. 
 WordNet Hypernyms. Events with the same 
hypernyms may have similar durations. But 
closely related events don?t always have the 
same direct hypernyms. We extract the hy-
pernyms not only for the event itself, but also for 
the subject and object of the event, since events 
related to a group of people or an organization 
usually last longer than those involving individu-
als, and the hypernyms can help distinguish such 
concepts. For our learning experiments, we ex-
tract the first 3 levels of hypernyms from Word-
Net (Miller, 1990). 
2.4.2  Learning Coarse-grained Binary 
Event Durations 
The distribution of the means of the annotated 
durations in Figure 2 is bimodal, dividing the 
events into those that take less than a day and 
those that take more than a day. Thus, in our first 
machine learning experiment, we have tried to 
learn this coarse-grained event duration informa-
tion as a binary classification task. 
Data. The original annotated data can be 
straightforwardly transformed for this binary 
classification task. For each event annotation, the 
most likely (mean) duration is calculated first by 
averaging (the logs of) its lower and upper bound 
durations. If its most likely (mean) duration is 
less than a day (about 11.4 in the natural loga-
rithmic scale), it is assigned to the ?short? event 
class, otherwise it is assigned to the ?long? event 
class. (Note that these labels are strictly a con-
venience and not an analysis of the meanings of 
?short? and ?long?.) 
We divide the total annotated non-WSJ data 
(2132 event instances) into two data sets: a train-
ing data set with 1705 event instances (about 
80% of the total non-WSJ data) and a held-out 
test data set with 427 event instances (about 20% 
of the total non-WSJ data). The WSJ data (156 
event instances) is kept for further test purposes. 
Results. The learning results in Figure 4 show 
that among all three learning algorithms explored 
(Na?ve Bayes (NB), Decision Trees C4.5, and 
Support Vector Machines (SVM)), SVM with 
linear kernel achieves the best overall precision 
(76.6%). Compared with the baseline (59.0%) 
and human agreement (87.7%), this level of per-
formance is very encouraging, especially as the 
learning is from such limited training data. 
 
42
 
Figure 4: Overall Test Precision on non-WSJ 
Data. 
 
Feature evaluation in (Pan et al, 2006b) shows 
that most of the performance comes from event 
word or phrase itself. A significant improvement 
above that is due to the addition of information 
about the subject and object. Local context does 
not help and in fact may hurt, and hypernym in-
formation also does not seem to help. It is grati-
fying to see that the most important information 
is that from the predicate and arguments describ-
ing the event, as our linguistic intuitions would 
lead us to expect. 
In order to evaluate whether the learned model 
can perform well on data from different news 
genres, we tested it on the unseen WSJ data (156 
event instances). A precision of 75.0%, which is 
very close to the test performance on the non-
WSJ data, proves the great generalization capac-
ity of the learned model. 
Some preliminary experimental results of 
learning the more fine-grained event duration 
information, i.e., the most likely temporal unit 
(cf. (Rieger 1974)?s ORDERHOURS, ORDERDAYS), 
are shown in (Pan et al, 2006b). SVM again 
achieves the best performance with 67.9% test 
precision (baseline 51.5% and human agreement 
79.8%) in ?approximate agreement? where tem-
poral units are considered to match if they are the 
same temporal unit or an adjacent one. 
3 TimeML and Its Event Classes 
TimeML (Pustejovsky et al, 2003) is a rich 
specification language for event and temporal 
expressions in natural language text. Unlike most 
previous attempts at event and temporal specifi-
cation, TimeML separates the representation of 
event and temporal expressions from the anchor-
ing or ordering dependencies that may exist in a 
given text. 
TimeML includes four major data structures: 
EVENT, TIMEX3, SIGNAL, AND LINK. 
EVENT is a cover term for situations that happen 
or occur, and also those predicates describing 
states or circumstances in which something ob-
tains or holds true. TIMEX3, which extends 
TIMEX2 (Ferro, 2001), is used to mark up ex-
plicit temporal expressions, such as time, dates, 
and durations. SIGNAL is used to annotate sec-
tions of text, typically function words that indi-
cate how temporal objects are related to each 
other (e.g., ?when?, ?during?, ?before?). The set 
of LINK tags encode various relations that exist 
between the temporal elements of a document, 
including three subtypes: TLINK (temporal 
links), SLINK (subordination links), and ALINK 
(aspectual links). 
Our event duration annotations can be inte-
grated into the EVENT tag. In TimeML each 
event belongs to one of the seven event classes, 
i.e., reporting, perception, aspectual, I-action, I-
state, state, occurrence. TimeML annotation 
guidelines1 give detailed description for each of 
the classes: 
Reporting. This class describes the action of a 
person or an organization declaring something, 
narrating an event, informing about an event, etc 
(e.g., say, report, tell, explain, state). 
Perception. This class includes events involv-
ing the physical perception of another event (e.g., 
see, watch, view, hear). 
Aspectual. In languages such as English and 
French, there is a grammatical device of aspec-
tual predication, which focuses on different fac-
ets of event history, i.e., initiation, reinitiation, 
termination, culmination, continuation (e.g., be-
gin, stop, finish, continue). 
I-Action. An I-Action is an Intensional Action. 
It introduces an event argument (which must be 
in the text explicitly) describing an action or 
situation from which we can infer something 
given its relation with the I-Action (e.g., attempt, 
try, promise). 
I-State.  This class of events are similar to the 
previous class. This class includes states that re-
fer to alternative or possible worlds (e.g., believe, 
intend, want). 
State. This class describes circumstances in 
which something obtains or holds true (e.g., on 
board, kidnapped, peace). 
Occurrence. This class includes all the many 
other kinds of events describing something that 
happens or occurs in the world (e.g., die, crash, 
build, sell). 
                                                 
1http://www.cs.brandeis.edu/~jamesp/arda/time/time
MLdocs/annguide12wp.pdf 
43
4 Integrating Event Duration Annota-
tions into TimeML 
Our event duration annotations can be integrated 
into TimeML by adding two more attributes to 
the EVENT tag for the lower bound and upper 
bound duration annotations (e.g., ?lowerBound-
Duration? and ?upperBoundDuration? attributes). 
To minimize changes of the existing TimeML 
specifications caused by the integration, we can 
try to share as much as possible our event classes 
as described in Section 2.2 with the existing ones 
in TimeML as described in Section 3.  
We can see that four event classes are shared 
with very similar definitions, i.e., reporting, as-
pectual, state, and action/occurrence. For the 
other three event classes that only belong to Ti-
meML (i.e., perception, I-action, I-state), the I-
action and perception classes can be treated as 
special subclasses of the action/occurrence class, 
and the I-state class as a special subclass of the 
state class. 
However, there are still three classes that only 
belong to the event duration annotations (i.e., 
multiple, negation, and positive infinite). The 
positive infinite class can be treated as a special 
subclass of the state class with a special duration 
annotation for positive infinity.  
Each multiple event has two annotations. For 
example, for 
Iraq has destroyed its long-range missiles.  
there is the time it takes to destroy one missile 
and the duration of the interval in which all the 
individual events are situated ? the time it takes 
to destroy all its missiles.  
Since the single event is usually more likely to 
be encountered in multiple documents, and thus 
the duration of the single event is usually more 
likely to be shared and re-used, to simplify the 
specification, we can take only the duration an-
notation of the single events for the multiple 
event class, and the single event can be assigned 
with one of the seven TimeML event classes. For 
example, the ?destroyed? event in the above ex-
ample is assigned with the occurrence class in 
TimeBank. 
The events involving negation can be simpli-
fied similarly. Since the event negated is usually 
more likely to be encountered in multiple docu-
ments, we can take only the duration annotation 
of the negated event for this class. For example, 
in 
He was willing to withdraw troops in ex-
change for guarantees that Israel would not be 
attacked. 
the event negated is the ?being attacked? event 
and it is assigned with the occurrence class in 
TimeBank.  Alternatively, TimeML could be 
extended to treat negations of events as states. 
The format used for annotated durations is 
consistent with that for the value of the DURA-
TION type in TimeML. For example, the sen-
tence 
The official said these sites could only be vis-
ited by a special team of U.N. monitors and dip-
lomats. 
can be marked up in TimeML as: 
 
The official <EVENT eid="e63" 
class="REPORTING"> said </EVENT> 
these sites <SIGNAL sid="s65" 
>could</SIGNAL> only be <EVENT 
eid="e64" class="OCCURRENCE"> 
visited </EVENT> by a special team 
of <ENAMEX TYPE="ORGANIZATION"> U.N. 
</ENAMEX> monitors and diplomats. 
 
If we annotate the ?said? event with the dura-
tion annotation of [5 seconds, 5 minutes], and the 
?visited? event with [10 minutes, 1 day], the ex-
tended mark-up becomes: 
 
The official <EVENT eid="e63" 
class="REPORTING" lowerBoundDura-
tion="PT5S" upperBoundDura-
tion="PT5M"> said </EVENT> these 
sites <SIGNAL sid="s65" 
>could</SIGNAL> only be <EVENT 
eid="e64" class="OCCURRENCE" lower-
BoundDuration="PT10M" upperBoundDu-
ration="P1D"> visited </EVENT> by a 
special team of <ENAMEX 
TYPE="ORGANIZATION"> U.N. </ENAMEX> 
monitors and diplomats. 
5 Conclusion 
In this paper we have demonstrated how to ex-
tend TimeML with typical durations of events. 
We can see that the extension is very straight-
forward. Other interesting temporal information 
can be extracted or learned. For example, for 
each event class, we can generate its own mean 
and widths graphs, and learn their durations 
separately from other classes, which may capture 
different duration characteristics associated with 
each event class.   
44
Acknowledgments 
This work was supported by the Advanced Re-
search and Development Activity (ARDA), now 
the Disruptive Technology Office (DTO), under 
DOD/DOI/ARDA Contract No. NBCHC040027. 
The authors have profited from discussions with 
Hoa Trang Dang, Donghui Feng, Kevin Knight, 
Daniel Marcu, James Pustejovsky, Deepak Ravi-
chandran, and Nathan Sobo. 
References 
B. Boguraev and R. K. Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning. 
In Proceedings of the International Joint Confer-
enceon Artificial Intelligence (IJCAI). 
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
gustics, 22(2):249?254. 
D. R. Dowty. 1979. Word Meaning and Montague 
Grammar, Dordrecht, Reidel. 
L. Ferro. 2001. Instruction Manual for the Annotation 
of Temporal Expressions. Mitre Technical Report 
MTR 01W0000046, the MITRE Corporation, 
McLean, Virginia. 
E. Filatova and E. Hovy. 2001. Assigning Time-
Stamps to Event-Clauses. Proceedings of ACL 
Workshop on Temporal and Spatial Reasoning. 
S. Harabagiu and C. Bejan. 2005. Question Answer-
ing Based on Temporal Inference. In Proceedings 
of the AAAI-2005 Workshop on Inference for Tex-
tual Question Answering, Pittsburgh, PA. 
U. Hermjakob and R. J. Mooney. 1997. Learning 
Parse and Translation Decisions from Examples 
with Rich Context. In Proceedings of the 35th An-
nual Meeting of the Association for Computational 
Linguistics (ACL). 
J. Hitzeman, M. Moens, and C. Grover. 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of EACL. Dublin, Ire-
land. 
J. R. Hobbs and V. Kreinovich. 2001. Optimal Choice 
of Granularity in Commonsense Estimation: Why 
Half Orders of Magnitude, In Proceedings of Joint 
9th IFSA World Congress and 20th NAFIPS Inter-
national Conference, Vacouver, British Columbia. 
K. Krippendorf. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications. 
I. Mani and G. Wilson. 2000. Robust Temporal Proc-
essing of News. In Proceedings of Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL). 
I. Mani and B. Schiffman. 2005. Temporally Anchor-
ing and Ordering Events in News. In J. Pustejovsky 
and R. Gaizauskas ed. Time and Event Recognition 
in Natural Language. John Benjamins. 
G. A. Miller. 1990. WordNet: an On-line Lexical Da-
tabase. International Journal of Lexicography 3(4). 
M. Moens and M. Steedman. 1988. Temporal Ontol-
ogy and Temporal Reference. Computational Lin-
guistics 14(2): 15-28. 
D. Moldovan, C. Clark, and S. Harabagiu. 2005. 
Temporal Context Representation and Reasoning. 
In Proceedings of the International Joint Confer-
enceon Artificial Intelligence (IJCAI). 
F. Pan, R. Mulkar, and J. R. Hobbs. 2006a. An Anno-
tated Corpus of Typical Durations of Events. To 
appear in Proceedings of the Fifth International 
Conference on Language Resources and Evalua-
tion (LREC), Genoa, Italy. 
F. Pan, R. Mulkar, and J. R. Hobbs. 2006b. Learning 
Event Durations from Event Descriptions. To ap-
pear in Proceedings of the 44th Conference of the 
Association for Computational Linguistics (COL-
ING-ACL), Sydney, Australia. 
R. J. Passonneau. 1988. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics 14:2.44-60. 
J. Pustejovsky, J. Castano, R. Ingria, R. Saur?, R. Gai-
zauskas, A. Setzer, and G. Katz. 2003. TimeML: 
Robust specification of event and temporal expres-
sions in text. In Proceedings of the AAAI Spring 
Symposium on New Directions in Question-
Answering. 
C. J. Rieger. 1974. Conceptual memory: A theory and 
computer program for processing and meaning 
content of natural language utterances. Stanford 
AIM-233. 
R. Saur?, R. Knippen, M. Verhagen and J. Puste-
jovsky. 2005. Evita: A Robust Event Recognizer 
for QA Systems. In Proceedings of HLT/EMNLP. 
M. Surdeanu, S. Harabagiu, J. Williams, and P. 
Aarseth. 2003. Using predicate-argument structures 
for information extraction. In Proceedings of the 
41th Annual Conference of the Association for 
Computational Linguistics (ACL-03), pages 8?15. 
Z. Vendler. 1967. Linguistics in Philosophy, Ithaca, 
Cornell University Press. 
45
Abductive Reasoning with a Large Knowledge Base
for Discourse Processing
Ekaterina Ovchinnikova
University of Osnabru?ck
eovchinn@uos.de
Niloofar Montazeri
USC ISI
niloofar@isi.edu
Theodore Alexandrov
University of Bremen
theodore@uni-bremen.de
Jerry R. Hobbs
USC ISI
hobbs@isi.edu
Michael C. McCord
IBM Research
mcmccord@us.ibm.com
Rutu Mulkar-Mehta
USC ISI
me@rutumulkar.com
Abstract
This paper presents a discourse processing framework based on weighted abduction. We elabo-
rate on ideas described in Hobbs et al (1993) and implement the abductive inference procedure in a
system called Mini-TACITUS. Particular attention is paid to constructing a large and reliable knowl-
edge base for supporting inferences. For this purpose we exploit such lexical-semantic resources as
WordNet and FrameNet. We test the proposed procedure and the obtained knowledge base on the
Recognizing Textual Entailment task using the data sets from the RTE-2 challenge for evaluation. In
addition, we provide an evaluation of the semantic role labeling produced by the system taking the
Frame-Annotated Corpus for Textual Entailment as a gold standard.
1 Introduction
In this paper, we elaborate on a semantic processing framework based on a mode of inference called
abduction, or inference to the best explanation. In logics, abduction is a kind of inference which arrives
at an explanatory hypothesis given an observation. Hobbs et al (1993) describe how abductive reasoning
can be applied to the discourse processing problem viewing the process of interpreting sentences in
discourse as the process of providing the best explanation of why the sentence would be true. In this
framework, interpreting a sentence means 1) proving its logical form, 2) merging redundancies where
possible, and 3) making assumptions where necessary. As the reader will see later in this paper, abductive
reasoning as a discourse processing technique helps to solve many pragmatic problems such as reference
resolution, the interpretation of noun compounds, the resolution of some kinds of syntactic, and semantic
ambiguity as a by-product. We adopt this approach. Specifically, we use a system we have built called
Mini-TACITUS1 (Mulkar et al, 2007) that provides the expressivity of logical inference but also allows
probabilistic, fuzzy, or defeasible inference and includes measures of the ?goodness? of abductive proofs
and hence of interpretations of texts and other situations.
The success of a discourse processing system based on inferences heavily depends on a knowledge
base. The main contribution of this paper is in showing how a large and reliable knowledge base can be
obtained by exploiting existing lexical semantic resources and can be successfully applied to reasoning
tasks on a large scale. In particular, we experiment with axioms extracted from WordNet, see Fellbaum
(1998), and FrameNet, see Ruppenhofer et al (2006). In axiomatizing FrameNet we rely on the study
described in Ovchinnikova et al (2010).
We evaluate our inference system and the obtained knowledge base in recognizing textual entailment
(RTE). As the reader will see in the following sections, inferences carried out by Mini-TACITUS are
fairly general and not tuned for a particular application. We decided to test our approach on RTE because
this is a well-defined task that captures major semantic inference needs across many natural language
1http://www.rutumulkar.com/download/TACITUS/tacitus.php
225
processing applications, such as question answering, information retrieval, information extraction, and
document summarization. For evaluation, we have chosen the RTE-2 data set (Bar-Haim et al, 2006),
because besides providing text-hypothesis pairs and a gold standard this data set has been annotated with
FrameNet frame and role labels (Burchardt and Pennacchiotti, 2008) which gives us the possibility of
evaluating our frame and role labeling based on the axioms extracted from FrameNet.
2 NL Pipeline and Abductive Reasoning
Our natural language pipeline produces interpretations of texts given the appropriate knowledge base. A
text is first input to the English Slot Grammar (ESG) parser (McCord, 1990, 2010). For each segment,
the parse produced by ESG is a dependency tree that shows both surface and deep structure. The deep
structure is exhibited via a word sense predication for each node, with logical arguments. These logical
predications form a good start on a logical form (LF) for the whole segment. An add-on to ESG converts
the parse tree into a LF in the style of Hobbs (1985). The LF is a conjunction of predications, which have
generalized entity arguments that can be used for showing relationships among the predications. These
LFs are used by the downstream components.
The interpretation of the text is carried out by an inference system called Mini-TACITUS using
weighted abduction as described in detail in Hobbs et al (1993). Mini-TACITUS tries to prove the logical
form of the text, allowing assumptions where necessary. Where the system is able to prove parts of the
LF, it is anchoring it in what is already known from the overall discourse or from a knowledge base.
Where assumptions are necessary, it is gaining new information. Obviously, there are many possible
proofs in this procedure. A cost function on proofs enables the system to chose the ?best? (the cheapest)
interpretation. The key factors involved in assigning a cost are the following: 1) proofs with fewer
assumptions are favored, 2) short proofs are favored over long ones, 3) plausible axioms are favored over
less plausible axioms, and 4) proofs are favored that exploit the inherent implicit redundancy in text.
Let us illustrate the procedure with a simple example. Suppose that we want to construct the best
interpretation of the sentence John composed a sonata. As a by-product, the procedure will disambiguate
between two readings of compose, namely between the ?form? reading instantiated for example in the
sentence Three representatives composed a committee, and the ?create art? meaning instantiated in the
given sentence. After being processed by the parser, the sentence will be assigned the following logical
form where the numbers (20) after every proposition correspond to the default costs of these proposi-
tions.2 The total cost of this logical form is equal to 60.
John(x1):20 & compose(e1,x1,x2):20 & sonata(x2):20
Suppose our knowledge base contains the following axioms:
1) form(e0,x1,x2):90 ? compose(e0,x1,x2)
2) create art(e0,x1,x2):50 & art piece(x2):40 ? compose(e0,x1,x2)
3) art piece(x1):90 ? sonata(x1)
Unlike deductive axioms, abductive axioms should be read ?right to left?. Thus, the propositions on
the right hand side (compose, sonata) correspond to an input, whereas the left hand side propositions
will be assumed given the input. The number assigned to each proposition on the left hand side shows
what percentage of the total input cost the assumption of this proposition will cost.3 For example, if the
proposition compose costs 20 then the assumption of form will cost 18.
Two interpretations can be constructed for the given logical form. The first one is the result of the
application of axioms 1 and 3. Note that the costs of the backchained propositions (compose, sonata) are
2The actual value of the default costs of the input propositions does not matter, because, as the reader will see in this section,
the axiom weights which affect the costs of the resulting interpretations are given as percentages of the input proposition costs.
The only heuristic we use here concerns setting all costs of the input propositions to be equal (all propositions cost 20 in the
discussed example). This heuristic needs a further investigation to be approved or modified.
3The axiom weights in the given example are arbitrary.
226
set to 0, because their costs are now carried by the newly introduces assumptions (form, art piece). The
total cost of the first interpretation I1 is equal to 56.
I1: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & form(e1,x1,x2):18 & art piece(x2):18
The second interpretation is constructed in two steps. First, axioms 2 and 3 are applied as follows.
I21: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 &
create art(e1,x1,x2):10 & art piece(x2):8 & art piece(x2):18
The total cost of I21 is equal to 56. This interpretation is redundant, because it contains the propo-
sition art piece twice. The procedure will merge propositions with the same predicate, setting the cor-
responding arguments of these propositions to be equal and assigning the minimum of the costs to the
result of merging. The idea behind such mergings is that if an assumption has already been made then
there is no need to make it again. The final form of the second interpretation I22 with the cost of 38
is as follows. The ?create art? meaning of compose has been brought forward because of the implicit
redundancy in the sentence which facilitated the disambiguation.
I22: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & create art(e1,x1,x2):10 &
art piece(x2):8
Thus, on each reasoning step the procedure 1) applies axioms to propositions with non-zero costs
and 2) merges propositions with the same predicate, assigning the lowest cost to the result of merging.
Reasoning terminates when no more axioms can be applied.4 The procedure favors the cheapest inter-
pretations. Among them, the shortest proofs are favored, i.e. if two interpretations have the same cost
then the one which has been constructed with fewer axiom application steps is considered to be ?better?.
It is easy to see that changing weights of axioms can crucially influence the reasoning process. Axiom
weights can help to propagate more frequent and reliable inferences and to distinguish between ?real?
abduction and deduction. For example, an axiom backchaining from dog to animal should in the general
case have a weight below 100, because it is cheap to assume that there is an animal if there is a dog; it is
a reliable deduction. On the contrary, assuming dog given animal should have a weight above 100.
In order to avoid undesirable mergings, we introduce non-merge constraints. For example, in the
sentence John reads a book and Bill reads a book the two read propositions should not be merged
because they refer to different actions. This is ensured by the following non-merge constraint: if not all
arguments of two propositions (which are not nouns) with the same predicate can be merged, then these
propositions cannot be merged. The constraint implies that in the sentence above two read propositions
cannot be merged, because John being the first argument of the first read cannot be merged with Bill.5
This constraint is a heuristic; it corresponds to the intuition that it is unlikely that the same noun refers to
different objects in a short discourse, while for other parts of speech it is possible. An additional corpus
study is needed in order to prove or disprove it.
The described procedure provides solutions to a whole range of natural language pragmatics prob-
lems, such as resolving ambiguity, discovering implicit relations in nouns compounds, prepositional
phrases, or discourse structure. Moreover, this account of interpretation solves the problem of where to
stop drawing inferences, which could easily be unlimited in number; an inference is appropriate if it is
part of the lowest-cost proof of the logical form.
Adapting Mini-TACITUS to a Large-Scale Knowledge Base
Mini-TACITUS (Mulkar et al, 2007) began as a simple backchaining theorem-prover intended to be a
more transparent version of the original TACITUS system, which was based on Stickel?s PTTP system
(Stickel, 1988). Originally, Mini-TACITUS was not designed for treating large amounts of data. A clear
and clean reasoning procedure rather than efficiency was in the focus of its developers. In order to make
the system work with the large-scale knowledge base, we had to perform several optimization steps and
add a couple of new features.
4In practice, we use the depth parameter d and do not allow an inference chain with more that d steps.
5Recall that only propositions with the same predicate can be merged, therefore John and Bill cannot be merged.
227
For avoiding the reasoning complexity problem, we have introduced two parameters. The time pa-
rameter t is used to restrict the processing time. After the processing time exceeds t the reasoning
terminates and the best interpretation so far is output. The time parameter ensures that an interpretation
will be always returned by the procedure even if reasoning could not be completed in a reasonable time.
The depth parameter d restricts the depth of the inference chain. Suppose that a proposition p occurring
in the input has been backchained and a proposition p? has been introduced as a result. Then, p? will be
backchained and so on. The number of such iterations cannot exceed d. The depth parameter reduces
the number of reasoning steps.
Since Mini-TACITUS processing time increases exponentially with the input size (sentence length
and number of axioms), making such a large set of axioms work was an additional issue. For speeding
up reasoning it was necessary to reduce both the number of the input propositions and the number of
axioms. In order to reduce the number of axioms, a two-step reduction of the axiom set is performed.
First, only the axioms which could be evoked by the input propositions or as a result of backchaining
from the input are selected for each reasoning task. Second, the axioms which could never lead to any
merging are filtered out. Concerning the input propositions, those which could never be merged with the
others (even after backchaining) are excluded from the reasoning process.
3 Knowledge Base
As described in the previous section, the Mini-TACITUS inferences are based on a knowledge base (KB)
consisting of a set of axioms. In order to obtain a reliable KBwith a sufficient coverage we have exploited
existing lexical-semantic resources.
First, we have extracted axioms from WordNet (Fellbaum, 1998), version 3.0, which has already
proved itself to be useful in knowledge-intensive NLP applications. The central entity in WordNet is
called a synset. Synsets correspond to word senses, so that every lexeme can participate in several
synsets. For every word sense, WordNet indicates the frequency of this particular word sense in the
WordNet annotated corpora. We have used the lexeme-synset mapping for generating axioms, with the
corresponding frequencies of word senses converted into the axiom weights. For example, in the axioms
below, the verb compose is mapped to its sense 2 in WordNet which participates in synset-X.
compose-2(e1,x1,x2):80 ? compose(e1,x1,x2)
synset-X(e0,e1):100 ? compose-2(e1,x1,x2)
Moreover, we have converted the following WordNet relations defined on synsets into axioms: hy-
pernymy, instantiation, entailment, similarity, meronymy. Hypernymy and instantiation relations pre-
suppose that the related synsets refer to the same entity (the first axiom below), whereas other types of
relations relate synsets referring to different entities (the second axiom below). All axioms based on
WordNet relations have the weights equal to 100.
synset-1(e0,e1):100 ? synset-2(e0,e1)
synset-1(e0,e1):100 ? synset-2(e2,e3)
WordNet alo provides morphosemantic relations which relate verbs and nouns, e.g., buy-buyer.
WordNet distinguishes between 14 types of such relations.We use relation types in order to define the
direction of the entailment and map the arguments. For example, the ?agent? relation (buy-buyer) stands
for a bi-directional entailment such that the noun is the first (agentive) argument of the verb:
buy-1(e0,x1,x2):100 ? buyer-1(x1)
buyer-1(x1):100 ? buy-1(e0,x1,x2)
Additionally, we have exploited the WordNet synset definitions. In WordNet the definitions are given
in natural language form. We have used the extended WordNet resource6 which provides logical forms
for the definition in WordNet version 2.0. We have adapted logical forms from extended WordNet to our
6http://xwn.hlt.utdallas.edu/
228
representation format and converted them into axioms; for example the following axiom represents the
meaning of the synset containing such lexemes as horseback. These axioms have the total weight of 100.
on(e2,e1,x2):25 & back(e3,x2):25 & of (e4,x2,x1):25 & horse(e5,x1):25 ? synset-X(e0,x0)
The second resource which we have used as a source of axioms is FrameNet, release 1.5, see Rup-
penhofer et al (2006). FrameNet has a shorter history in NLP applications thanWordNet, but lately more
and more researchers have been demonstrating its potential to improve the quality of question answering
(Shen and Lapata, 2007) and recognizing textual entailment (Burchardt et al, 2009). The lexical mean-
ing of predicates in FrameNet is represented in terms of frames which describe prototypical situations
spoken about in natural language. Every frame contains a set of roles corresponding to the participants of
the described situation. Predicates with similar semantics are assigned to the same frame; e.g. both give
and hand over refer to the GIVING frame. For most of the lexical elements FrameNet provides syntactic
patterns showing the surface realization of these lexical elements and their arguments. Syntactic patterns
also contain information about their frequency in the FrameNet annotated corpora. We have used the
patterns and the frequencies for deriving axioms such as for example the following.
GIVING(e1,x1,x2,x3):70 & DONOR(e1,x1):0 & RECIPIENT(e1,x2):0 & THEME(e1,x3):0 ?
give(e1,x1,x3) & to(e2,e1,x2)
HIRING(e1,x1,x3):90 & EMPLOYER(e1,x1) & EMPLOYEE(e1,x3) ?
give(e1,x1,x2,x3):10 & job(x2)
The first pattern above corresponds to the phrases like John gave a book to Mary and the second ?
less frequent ? to phrases like John gave Mary a job. It is interesting to note that application of such
axioms provides a solution to the problem of semantic role labeling as a by-product. As in the statis-
tical approaches, more frequent patterns will be favored. Moreover, patterns helping to detect implicit
redundancy will be brought forward.
FrameNet alo introduces semantic relations defined on frames such as inheritance, causation or
precedence; for example the GIVING and GETTING frames are connected with the causation relation.
Roles of the connected frames are also linked, e.g. DONOR in GIVING is linked with SOURCE in GETTING.
Frame relations have no formal semantics in FrameNet. In order to generate corresponding axioms, we
have used the previous work on axiomatizing frame relations and extracting new relations from corpora
(Ovchinnikova et al, 2010). Weights of the axioms derived from frame relations depend on corpus-based
similarity of the lexical items assigned to the corresponding frames. An example of an axiomatized
relation is given below.7
GIVING(e0,x1,x2,x3):120 & DONOR(e0,x1):0 & RECIPIENT(e0,x2):0 & THEME(e0,x3):0 &
causes(e0,e1):0 ? GETTING(e1,x2,x3,x1) & SOURCE(e1,x1) & RECIPIENT(e1,x2) & THEME(e1,x3)
Both WordNet and FrameNet are manually created resources which ensures a relatively high quality
of the resulting axioms as well as the possibility of exploiting the linguistic information provided for
structuring the axioms. Although manual creation of resources is a very time-consuming task, WordNet
and FrameNet, being long-term projects, have an extensive coverage of English vocabulary. The cover-
age of WordNet is currently larger than that of FrameNet (155 000 vs. 12 000 lexemes). However, the
fact that FrameNet introduces complex argument structures (roles) for frames and provides mappings of
these structures makes FrameNet especially valuable for reasoning.
The complete list of axioms we have extracted from these resources is given in table 1.
4 Recognizing Textual Entailment
As the reader can see from the previous sections, the discourse processing procedure we have presented
is fairly general and not tuned for any particular type of inferences. We have evaluated the procedure and
7The ?causes? predicate is supposed to be linked to an underlying causation theory, see for example
http://www.isi.edu/?hobbs/bgt-cause.text. However, in the described experimental settings we have left the abstract theories
out and evaluated only the axioms extracted from the lexical-semantic resources.
229
Table 1: Statistics for extracted axioms
Axiom type Source Numb. of axioms
Lexeme-synset mappings WN 3.0 422,000
Lexeme-synset mappings WN 2.0 406,000
Synset relations WN 3.0 141,000
Derivational relations WN 3.0 (annotated) 35,000
Synset definitions WN 2.0 (parsed, annotated) 120,500
Lexeme-frame mappings FN 1.5 50,000
Frame relations FN 1.5 + corpora 6,000
the KB derived from WordNet and FrameNet on the Recognizing Textual Entailment (RTE) task, which
is a generic task that seems to capture major semantic inference needs across many natural language
processing applications. In this task, the system is given a text and a hypothesis and must decide whether
the hypothesis is entailed by the text plus commonsense knowledge.
Our approach is to interpret both the text and the hypothesis using Mini-TACITUS, and then see
whether adding information derived from the text to the knowledge base will reduce the cost of the best
abductive proof of the hypothesis as compared to using the original knowledge base only. If the cost
reduction exceeds a threshold determined from a training set, then we predict entailment.
A simple example would be the text John gave a book to Mary and the hypothesis Mary got a book.
Our pipeline constructs the following logical forms for these two sentences.
T: John(x1):20 & give(e1,x1,x2):20 & book(x3):20 & to(e2,e1,x3):20 & Mary(x3):20
H: Mary(x1):20 & get(e1,x1,x2):20 & book(x2):20
These logical forms constitute the Mini-TACITUS input. Mini-TACITUS applies the axioms from
the knowledge base to the input logical forms in order to reduce the overall cost of the interpretations.
Suppose that we have three FrameNet axioms in our knowledge base. The first one maps give to to the
GIVING frame, the second one maps get to GETTING and the third one relates GIVING and GETTING with
the causation relation. The first two axioms have the weights of 90 and the third 120. As a result of the
application of the axioms the following best interpretations will be constructed for T and H.
I(T): John(x1):20 & give(e1,x1,x2):0 & book(x3):20 & to(e2,e1,x3):0 & Mary(x3):20 &
GIVING(e0,x1,x2,x3):18
I(H): Mary(x1):20 & get(e1,x1,x2):0 & book(x2):20 & GETTING(e0,x1,x2):18
The total cost of the best interpretation for H is equal to 58. Now the best interpretation of T will
be added to H with the zero costs (as if T has been totally proven) and we will try to prove H once
again. First of all, merging of the propositions with the same names will result in reducing costs of the
propositions Mary and book to 0, because they occur in T:
I(T+H): John(x1):0 & give(e1,x1,x2):0 & book(x3):0 & to(e2,e1,x3):0 & Mary(x3):0 &
GIVING(e0,x1,x2,x3):0 & get(e1,x1,x2):0 & GETTING(e0,x1,x2):18
The only proposition left to be proved is GETTING. Using the GETTING-GIVING relation as described
in the previous section, this proposition can be backchained on to GIVING which will merge with GIVING
coming from the T sentence. H appears to be proven completely with respect to T; the total cost of its
best interpretation given T is equal to 0. Thus, using knowledge from T helped to reduce the cost of the
best interpretation of H from 58 to 0.
The approach presented does not have any special account for logical connectors such as if, not, or
etc. Given a text If A then B and a hypothesis A and B our procedure will most likely predict entailment.
At the moment our RTE procedure mainly accounts for the informational content of texts, being able to
detect the ?aboutness? overlap of T and H. In our framework, a fuller treatment of the logical structure
230
of the natural language would presuppose a more complicated strategy of merging redundancies.
5 Evaluation Results
We have evaluated our procedure on the RTE-2 dataset 8, see Bar-Haim et al (2006) . The RTE-2
dataset contains the development and the test set, both including 800 text-hypothesis pairs. Each dataset
consists of four subsets, which correspond to typical success and failure settings in different applications:
information extraction (IE), information retrieval (IR), question answering (QA), and summarization
(SUM). In total, 200 pairs were collected for each application in each dataset.
As a baseline we have processed the datasets with an empty knowledge base. Then we have done 2
runs, first, using axioms extracted fromWordNet 3.0 plus FrameNet, and, second, using axioms extracted
from the WordNet 2.0 definitions. In both runs the depth parameter was set to 3. The development
set was used to train the threshold as described in the previous section.9 Table 2 contains results of
our experiments.10 Accuracy was calculated as the percentage of pairs correctly judged. The results
suggest that the proposed method seems to be promising as compared to the other systems evaluated
on the same task. Our best run gives 63% accuracy. Two systems participating the RTE-2 Challenge
had 73% and 75% accuracy, two systems achieved 62% and 63%, while most of the systems achieved
55%-61%, cf. Bar-Haim et al (2006). For our best run (WN 3.0 + FN), we present the accuracy data
for each application separately (table 2). The distribution of the performance of Mini-TACITUS on the
four datasets corresponds to the average performance of systems participating in RTE-2 as reported by
Garoufi (2007). The most challenging task in RTE-2 appeared to be IE. QA and IR follow, and finally,
SUM was titled the ?easiest? task, with a performance significantly higher than that of any other task.11
It is worth noting that the performance of Mini-TACITUS increases with the increasing time of pro-
cessing. This is not surprising. We use the time parameter t for restricting the processing time. The
smaller t is, the fewer chances Mini-TACITUS has for applying all relevant axioms. The experiments
carried out suggest that optimizing the system computationally could lead to producing significantly bet-
ter results. Tracing the reasoning process, we found out that given a long sentence and a short processing
time Mini-TACITUS had time to construct only a few interpretations, and the real best interpretation was
not always among them.
The lower performance of the system using the KB based on axioms extracted from extended Word-
Net can be easily explained. At the moment we define non-merge constraints (see section 2) for the input
propositions only. The axioms extracted from the synset definitions introduce a lot of new lexemes into
the logical form, since these axioms define words with the help of other words rather than abstract con-
cepts. These new lexemes, especially those which are frequent in English, result in undesired mergings
(e.g., mergings of frequent prepositions), since no non-merge constraints are defined for them. In order
to fix this problem, we will need to implement dynamic non-merge constraints which will be added on
the fly if a new lexeme is introduced during reasoning. The WN 3.0 + FN axiom set does not fall into
this problem, because these axioms operate on frames and synsets rather than on lexemes.
In addition, for the run using axioms derived from FrameNet, we have evaluated how well we do
in assigning frames and frame roles. For Mini-TACITUS, semantic role labeling is a by-product of
constructing the best interpretation. But since this task is considered to be important as such in the NLP
community, we provide an additional evaluation for it. As a gold standard we have used the Frame-
Annotated Corpus for Textual Entailment, FATE, see Burchardt and Pennacchiotti (2008). This corpus
provides frame and semantic role label annotations for the RTE-2 challenge test set.12 It is important to
8http://pascallin.ecs.soton.ac.uk/Challenges/RTE2/
9Interpretation costs were normalized to the number of propositions in the input.
10?Time? stands for the value of the time parameter ? processing time per sentence, in minutes; ?Numb. of ax.? stands for
the average number of axioms per sentence.
11In order to get a better understanding of which parts of our KB are useful for computing entailment and for which types of
entailment, in future, we are planning to use the detailed annotation of the RTE-2 dataset describing the source of the entailment
which was produced by Garoufi (2007). We would like to thank one of our reviewers for giving us this idea.
12FATE was annotated with the FrameNet 1.3 labels, while we have been using 1.5 version for extracting axioms. However,
231
Table 2: Evaluation results for the RTE-2 test set
KB Accuracy Time
Numb. of ax.
T H
No KB 57% 1 0 0
WN 3.0 + FN 62% 20 533 237
WN 3.0 + FN 63% 30 533 237
Ext. WN 2.0 60% 20 3700 1720
Ext. WN 2.0 61% 30 3700 1720
Task Accuracy
SUM 75%
IR 64%
QA 62%
IE 50%
Table 3: Evaluation of frames/roles labeling towards FATE
System
Frame match
Recall
Role match
Precision Recall
Shalmaneser 0.55 0.54 0.37
Shalmaneser + Detour 0.85 0.52 0.36
Mini-TACITUS 0.65 0.55 0.30
note that FATE annotates only those frames which are relevant for computing entailment. Since Mini-
TACITUS makes all possible frame assignments for a sentence, we provide only the recall measure for
the frame match and leave the precision out.
The FATE corpus was also used as a gold standard for evaluating the Shalmaneser system (Erk and
Pado, 2006) which is a state-of-the-art system for assigning FrameNet frames and roles. In table 2 we
replicate results for Shalmaneser alone and Shalmaneser boosted with the WordNet Detour to FrameNet
(Burchardt et al, 2005). The WN-FN Detour extended the frame labels assigned by Shalmaneser with
the labels related via the FrameNet hierarchy or by the WordNet inheritance relation, cf. Burchardt et al
(2009). In frame matching, the number of frame labels in the gold standard annotation that can also be
found in the system annotation (recall) was counted. Role matching was evaluated only on the frames
that are correctly annotated by the system. The number of role labels in the gold standard annotation
that can also be found in the system annotation (recall) as well as the number of role labels found by
the system which also occur in the gold standard (precision) were counted.13 Table 3 shows that given
FrameNet axioms, the performance of Mini-TACITUS on semantic role labeling is compatible with those
of the system specially designed to solve this task.
6 Conclusion and Future Work
This paper presents a discourse processing framework underlying the abductive reasoner called Mini-
TACITUS. We have shown that interpreting texts using weighted abduction helps solve pragmatic prob-
lems in discourse processing as a by-product. In this paper, particular attention was paid to the construc-
tion of a large and reliable knowledge base populated with axioms extracted from such lexical-semantic
resources as WordNet and FrameNet. The reasoning procedure as well as the knowledge base were eval-
uated in the Recognizing Textual Entailment task. The data for evaluation were taken from the RTE-2
Challenge. First, we have evaluated the accuracy of the entailment prediction. Second, we have eval-
in the new FN version the number of frames and roles increases and there is no message about removed frames in the General
Release Notes R1.5, see http://framenet.icsi.berkeley.edu. Therefore we suppose that most of the frames and roles used for the
FATE annotation are still present in FN 1.5.
13We do not compare filler matching, because the FATE syntactic annotation follows different standards as the one produced
by the ESG parser, which makes aligning fillers non-trivial.
232
uated frame and role labeling using the Frame-Annotated Corpora for Textual Entailment as the gold
standard. In both tasks our system showed performance compatible with those of the state-of-the art
systems. Since the inference procedure and the axiom set are general and not tuned for a particular task,
we consider the results of our experiments to be promising concerning possible manifold applications of
Mini-TACITUS.
The experiments we have carried out have shown that there is still a lot of space for improving the
procedure. First, for successful application of Mini-TACITUS on a large scale the system needs to be
computationally optimized. In its current state, Mini-TACITUS requires too much time for producing
satisfactory results. As our experiments suggest (cf. table 2), speeding up reasoning may lead to signif-
icant improvements in the system performance. Since Mini-TACITUS was not originally designed for
large-scale processing, its implementation is in many aspects not effective enough. We hope to improve
it by changing the data structure and re-implementing some of the main algorithms.
Second, in the future we plan to elaborate our treatment of natural language expressions standing for
logical connectors such as implication if, negation not, disjunction or and others. Quantifiers such as
all, each, some also require a special treatment. This advance is needed in order to achieve more precise
entailment inferences, which are at the moment based in our approach on the core information content
(?aboutness?) of texts. Concerning the heuristic non-merge constraints preventing undesired mergings
as well as the heuristic for assigning default costs (see section 2), in the future we would like to perform
a corpus study for evaluating and possibly changing these heuristics.
Another future direction concerns the enlargement of the knowledge base. Hand-crafted lexical-
semantic resources such as WordNet and FrameNet provide both an extensive lexical coverage and a
high-value semantic labeling. However, such resources still lack certain features essential for captur-
ing some of the knowledge required for linguistic inferences. First of all, manually created resources
are static; updating them with new information is a slow and time-consuming process. By contrast,
commonsense knowledge and the lexicon undergo daily updates. In order to accommodate dynamic
knowledge, we plan to make use of the distributional similarities of words in a large Web-corpus such
as for example Wikipedia. Many researchers working on RTE have already been using word similarity
for computing similarity between texts and hypotheses, e.g., Mehdad et al (2010). In our approach, we
plan to incorporate word similarities into the reasoning procedure making them affect proposition costs
so that propositions implied by the context (similar to other words in the context) will become cheaper
to prove. This extension might give us a performance improvement in RTE, because it will help to relate
those propositions from H for which there are no appropriate axioms in the KB to propositions in T.
Lexical-semantic resources as knowledge sources for reasoning have another shortcoming: They
imply too little structure. WordNet and FrameNet enable some argument mappings of related synsets or
frames, but they cannot provide a more detailed concept axiomatization. We are engaged in two types of
efforts to obtain more structured knowledge. The first effort is the manual encoding of abstract theories
explicating concepts that pervade natural language discourse, such as causality, change of state, and
scales, and the manual encoding of axioms linking lexical items to these theories. A selection of the core
theories can be found at http://www.isi.edu/ hobbs/csk.html. The second effort concerns making use of
the existing ontologies. The recent progress of the Semantic Web technologies has stimulated extensive
development of the domain-specific ontologies as well as development of inference machines specially
designed to reason with these ontologies.14 In practice, domain-specific ontologies usually represent
detailed and structured knowledge about particular domains (e.g. geography, medicine etc.). We intend
to make Mini-TACITUS able to use this knowledge through querying an externally stored ontology with
the help of an existing reasoner. This extension will give us a possibility to access elaborated domain-
specific knowledge which might be crucial for interpretation of domain-specific texts.
We believe that implementation of the mentioned improvements and extensions will make Mini-
TACITUS a powerful reasoning system equipped with enough knowledge to solve manifold NLP tasks on
a large scale. In our view, the experiments with the axioms extracted from the lexical-semantic resources
presented in this paper show the potential of weighted abduction for natural language reasoning and open
14www.w3.org/2001/sw/,http://www.cs.man.ac.uk/ sattler/reasoners.html
233
new ways for its application.
References
Bar-Haim, R., I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor (2006). The
second PASCAL recognising textual entailment challenge. In Proc. of the Second PASCAL Challenges
Workshop on Recognising Textual Entailment.
Burchardt, A., K. Erk, and A. Frank (2005). A WordNet Detour to FrameNet. In Sprachtechnologie,
mobile Kommunikation und linguistische Resourcen, Volume 8.
Burchardt, A. and M. Pennacchiotti (2008). FATE: a FrameNet-Annotated Corpus for Textual Entail-
ment. In Proc. of LREC?08.
Burchardt, A., M. Pennacchiotti, S. Thater, and M. Pinkal (2009). Assessing the impact of frame seman-
tics on textual entailment. Natural Language Engineering 15(4), 527?550.
Erk, K. and S. Pado (2006). Shalmaneser - a flexible toolbox for semantic role assignment. In Proc. of
LREC?06, Genoa, Italy.
Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical Database (First ed.). MIT Press.
Garoufi, K. (2007). Towards a better understanding of applied textual entailment: Annotation and eval-
uation of the rte-2 dataset. Master?s thesis, Saarland University.
Hobbs, J. R. (1985). Ontological promiscuity. In Proceedings, 23rd Annual Meeting of the Association
for Computational Linguistics, Chicago, Illinois, pp. 61?69.
Hobbs, J. R., M. Stickel, and P. Martin (1993). Interpretation as abduction. Artificial Intelligence 63,
69?142.
McCord, M. C. (1990). Slot grammar: A system for simpler construction of practical natural language
grammars. In Natural Language and Logic: International Scientific Symposium, Lecture Notes in
Computer Science, pp. 118?145. Springer Verlag.
McCord, M. C. (2010). Using Slot Grammar. Technical report, IBM T. J. Watson Research Center. RC
23978Revised.
Mehdad, Y., A. Moschitti, and F. M. Zanzotto (2010). Syntactic/semantic structures for textual entailment
recognition. In Proc. of HLT ?10: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, pp. 1020?1028.
Mulkar, R., J. R. Hobbs, and E. Hovy (2007). Learning from Reading Syntactically Complex Biol-
ogy Texts. In Proc.of the 8th International Symposium on Logical Formalizations of Commonsense
Reasoning. Palo Alto.
Ovchinnikova, E., L. Vieu, A. Oltramari, S. Borgo, and T. Alexandrov (2010). Data-Driven and Onto-
logical Analysis of FrameNet for Natural Language Reasoning. In Proc. of LREC?10, Valletta, Malta.
Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk (2006). FrameNet II: Extended
Theory and Practice. International Computer Science Institute.
Shen, D. and M. Lapata (2007). Using Semantic Roles to Improve Question Answering. In Proc. of
EMNLP-CoNLL, pp. 12?21.
Stickel, M. E. (1988). A prolog technology theorem prover: Implementation by an extended prolog
compiler. Journal of Automated Reasoning 4(4), 353?380.
234
Granularity in Natural Language Discourse
Rutu Mulkar-Mehta, Jerry Hobbs and Eduard Hovy
University of Southern California, Information Sciences Institute
me@rutumulkar.com, hobbs@isi.edu, hovy@isi.edu
Abstract
This paper discusses the phenomenon of granularity in natural language1. By ?granularity? we
mean the level of detail of description of an event or object. Humans can seamlessly shift their gran-
ularity perspective while reading or understanding a text. To emulate this mechanism, we describe
a set of features that identify the levels of granularity in text, and empirically verify this feature set
using a human annotation study for granularity identification. This theory is the foundation for any
system that can learn the (global) behavior of event descriptions from (local) behavior descriptions.
This is the first research initiative, to our knowledge, for identifying granularity shifts in natural
language descriptions.
1 Introduction
Granularity is the concept of breaking down an event into smaller parts or granules such that each indi-
vidual granule plays a part in the higher level event. For example, the activity of driving to the grocery
store involves some fine-grained events like opening the car door, starting the engine, planning the route,
and driving to the destination. Each of these may in turn be decomposed further into finer levels of
granularity. For instance, planning the route might involve entering an address into GPS and following
directions. The phenomenon of granularity is observed in various domains, including scientific literature,
game reports, and political descriptions. In scientific literature, the process of photosynthesis on closer
examination is made up of smaller individual fine-grained processes such as the light dependent reaction
and the light independent reaction.
Granularity is not a new concept. It has been studied actively in various disciplines. In philosophy, Bit-
tner and Smith (2001) have worked on formalizing granularity and part-hood relations. In information
retrieval, Lau et al (2009) have used granularity concepts to extract relevant detail of information result-
ing from a given search query. In theoretical computer science and ontology development, Keet (2008)
has worked on formalizing the concept of entity granularity and hierarchy and applied it biological sci-
ences. In natural language processing, Mani (1998) has worked on applying concepts of granularity to
polysemy and Hobbs (1985) has worked on using granularity for decomposing complex theories into
simple theories.
Although all of the above work emphasizes the importance of granularity relations for language un-
derstanding and formalization, none of it has attempted to observe whether granularity structures exist in
natural language texts, explored whether granularity structures can be identified and extracted automati-
cally, or tried to analyze how harvesting granularity relations can possibly help with other NLP problems.
This paper focuses on two items: First, we present a model of granularity as it exists in natural language
(Section 2); and second, we present an annotation study which we conducted to verify the proposed
model of granularity in natural language (Section 3).
1This research was supported by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program
under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172. Any opinions, findings, and conclusion
or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA,
AFRL, ONR, or the US government.
360
(a) (b)
Figure 1: 1(a): Granularity in Natural Language Descriptions; 1(b): Instantiating Natural Language to
the Granularity model
2 Modeling Granularity in Natural Language Texts
Humans can easily shift through various levels of granularity in understanding text. However, for auto-
mated granularity identification and extraction, it is necessary to explicitly recognize the identifiers that
indicate a shift in granularity. Figure 1(a) illustrates our theory of granularity. A granularity structure
exists only if at least two levels of information are present in text, such that the events at the coarse gran-
ularity can be decomposed into the events at the fine granularity, and the events at the fine granularity
combine together to form at least one segment of the event at the coarse granularity. In Figure 1(a),
Gc represents the phrase or sentence with coarse granularity information and Gf represents a phrase
or sentence with fine granularity information. Three types of relations can exist between the objects at
coarse and fine granularity: part-whole relationships between entities, part-whole relationships between
events, and causal relationships between the fine and coarse granularities. These relations signal a shift
in granularity. Instantiating text phrases into this model will expose granularities of text. For example,
consider the following sentence:
The San Francisco 49ers moved ahead 7?3 11 minutes into the game when William Floyd scored a two-yard
touchdown run.
The event of the player scoring a touchdown (the second clause of the sentence) is a decomposition
of the event of the team moving forward in the game (the first clause), and thus a finer granularity rep-
resentation of the San Francisco 49ers moving ahead in the game. When instantiated in our model of
granularity (Figure 1(a)), the graphical representation is shown in Figure 1(b).
Having described the overall model of granularity, we now elaborate on the components of the gran-
ularity model, namely part-whole relations and causal relations.
2.1 Part-Whole Relations
Two types of part-whole relations are present: meronymic and mereologic. Mereology (for more details
read Keet (2008)) is a partial ordering relation that is reflexive, transitive, and antisymmetric. According
to the concept of mereology, if x, y and z are three entities, then: x is a part of x; if x is part of y and y is
part of z then x is part of z; and if x is part of y then y cannot be part of x. However, various types of part-
whole relations that occur in natural language, such as member of, do not satisfy the transitivity relation,
in which case they will be mereologic but not meronymic: they might be ontologically accurate but
not linguistically correct. For instance, if John?s arm is part of John, and John is a member of a football
team, the transitivity relation that John?s arm is part of a football team, is not a valid meronymic relation.
Another instance which is mereologic but not meronymic is the following: A cup is made of steel, and
steel is made of molecules. Therefore a cup is made of molecules. The concept of mereology does not
361
reflect the way part of is used in natural language, and so mereology cannot be used for linguistic based
research.
One of the early works on part-whole relations in natural language (meronymy) Winston et al (1987)
was later refined in their empirical experiments Chaffin et al (1988). Winston et al discuss meronymic
relations and a taxonomy for representing them. They introduce six types of part-whole relationships:
(i) Component-Integral (e.g., pedal is a component of the integral bike), (ii) Member-Collection (e.g.,
a ship is a member of the collection, a fleet), (ii) Portion-Mass (e.g., a slice is a portion of the mass, a
pie), (iv) Stuff-Object (e.g., steel is one of the ingredients/stuff of the object car), (v) Feature-Activity
(e.g., paying is one of the features of the whole activity of shopping), (vi) Place-Area (e.g., Everglades
is a place within the area of Florida). The definition and classification in Winston et al (1987) for
part-whole relations is very relevant for language based analysis of part-whole relations. For granularity
identification in our work, the Feature-Activity type relation is used as the part-whole relation for events,
and the rest are part-whole relations for entities.
2.2 Causal Relations
Girju and Moldovan (2002) provide a broad compilation of causality research ranging from philosophy,
planning in AI, commonsense reasoning, and computational linguistics. Causation in computational
linguistics is the only form of causality that is relevant for granularity identification and extraction. The
following are the categories of causal constructs relevant for granularity identification and extraction:
? Causal Connectives: These are usually prepositional (such as because of, thanks to, due to), adver-
bial (such as for this reason, the result that), or clause links (such as because, since, for).
? Causation Verbs: These usually have a causal relation integrated with the verb. For example, kill,
melt (represent a causal link with the resulting situation), poison, hang, clean (represent a causal
link with the a part of the causing event)
? Conditionals: Girju and Moldovan (2002) describe conditionals as complex linguistic structures
typically of the form If S1 then S2. These structures represent causation, temporal relations, among
other relations, and are very complex structures in language.
3 Evaluation of the Granularity Model in Natural Language
We conducted an evaluation study to judge the ?goodness? of the granularity model proposed. In
this study the annotators were asked to annotate granularity relations between two given paragraphs.
Paragraph-based analysis was preferred to event-word-based analysis because people reason much more
easily with paragraph descriptions than with individual event mentions 2. The annotation set consisted of
paragraph pairs from three domains: travel articles (confluence.org), Timebank annotated data Pan et al
(2006), and Wikipedia articles on games. We selected a total of 37 articles: 10 articles about travel, 10
about games, and 17 from Timebank. Both paragraphs of a given question were selected from the same
article and referred to the same overall concept.
3.1 Annotation Task
The articles were uploaded to Mechanical Turk and were annotated by non-expert annotators (regular
Turkers). The entire set of 37 articles was annotated by 5 people. The annotators were given a pair
of paragraphs and were asked four questions about the relations between them: (i) Is one paragraph a
subevent of the other paragraph?, (ii) Did one paragraph cause the other paragraph?, (iii) Is one paragraph
less detailed and the other paragraph more detailed?, (iv) Did one paragraph happen after the other para-
graph? They were then presented with the comments of other annotators, and asked whether they agreed
2This was deduced as a result of an earlier annotation study for granularity identification using individual words as events.
362
(a) (b)
Figure 2: 2(a) shows the Inter-Annotator agreement for 37 articles and 2(b) shows the Pairwise Kappa
Agreement for 37 articles and 5 annotators
with any of the other annotations or explanations. The annotators were asked to provide a justification of
their choices.
3.2 Results
The Kappa statistic (Cohen (1960)) is the standard for measuring inter-annotator agreement: k =
(p(a)?p(e))
(1?p(e)) , where p(a) is the observed agreement and p(e) is the chance agreement between annota-
tors. More refined than simple Percentage Agreement, Kappa corrects for chance agreements.
In our study, two annotators were considered to be in agreement if they agreed with questions (i)
Subevents, (iii) More or less detail and (iv) Sequence. Unfortunately question (ii) Causality, as pro-
vided to the annotators, could not be taken into account for agreement measurement as individuals had
different conceptualizations of causality, and a crisp definition of causality was not provided to them.
For instance, consider the following two paragraphs:
1: I wanted to visit the confluence point located in the extreme southwest of Hunan Province.
2: To get to the confluence, I caught the Hong Kong-to-Shanghai intercity train on Friday afternoon.
Analysis: Some annotators annotated para2 causes para1, providing the explanation that the goal para1 could
be achieved due to the events of para2. Others annotated para1 causes para2, providing the justification that the
events of para2 only exist to fulfill the original goal para1. We are interested in the first type of causality, i.e.,
causality which explains how a given event happens. All the annotators agreed that a sub-event explains how an
event happens, or a sub-event causes an event. We counted this in lieu of our causality question (ii).
Figure 2(a) shows the overall agreement of the five annotators on the 37 articles and Figure 2(b) shows
the pairwise Kappa agreement for the five annotators. All the annotators agreed in 33/37 cases (23 article
pairs were annotated as having a granularity shift, 10 articles were annotated as having no granularity
shift). The average pairwise Kappa was 0.85. If the newspaper articles were removed, the overall agree-
ment was 100% for all the annotators. High agreement implied good quality of the annotation guidelines,
and provided evidence that people shift through various levels of granularity while reading and under-
standing text.
3.3 Analysis of the Causes of Disagreement
Where disagreements occurred, different interpretations of the same text were observed to be a major
cause. All these disagreements were limited to the newspaper articles. For instance, consider the follow-
ing:
363
1: Some 1,500 ethnic Albanians marched Sunday in downtown Istanbul, burning Serbian flags.
2: The police barred the crowd from reaching the Yugoslavian consulate in downtown Istanbul, but allowed them
to demonstrate on nearby streets.
Positive Granularity Shift: Some annotators commented that ?demonstrations? happen as a part of a ?march?.
So, para2 is a sub-event of para1.
Negative Granularity Shift: Other annotators felt that para2 happened after para1, and so there was no granular-
ity shift.
Overall, we can observe that although disagreement arises due to individual and unique interpretations
of text, people agree based on the discriminating features provided to them (part-whole relations and
causality) when identifying granularity shifts. This shows that part-whole relations and causality provide
a good set of features for identifying granularity shifts.
4 Conclusion and Future Work
In this paper we present the phenomenon of granularity as it occurs in natural language texts. We validate
our model of granularity with the help of an annotation study. We are currently developing a system for
automatic granularity extraction. We will compare its performance with state of the art techniques for
answering causality-style questions to empirically evaluate the significance of granularity structures for
automated Question Answering.
References
Bittner, T. and B. Smith (2001). Granular partitions and vagueness. Proceedings of the international
conference on Formal Ontology in Information Systems - FOIS ?01, 309?320.
Chaffin, R., D. J. Herrmann, and M. E. Winston (1988). An empirical taxonomy of part-whole relations:
Effects of part-whole relation type on relation identification. Language and Cognitive Processes 3(1).
Cohen, J. (1960). A coefficientof agreement for nominal scales. Educational and Psychological Mea-
surement 20, 37?46.
Girju, R. and D. Moldovan (2002). Mining Answers for Causation. Proceedings of American Association
of Artificial Intelligence, 15?25.
Hobbs, J. R. (1985). Granularity. In Proceedings of the Ninth International Joint Conference on Artificial
Intelligence, 432?435.
Keet, C. M. (2008). A Formal Theory of Granularity. Ph. D. thesis, Faculty of Computer Science, Free
University of Bozen-Balzano, Italy.
Lau, R. Y. K., C. C. L. Lai, and Y. Li (2009). Mining Fuzzy Ontology for a Web-Based Granular
Information Retrieval System. Lecture Notes in Computer Science, 239?246.
Mani, I. (1998). A Theory of Granularity and its Application to Problems of Polysemy and Underspec-
ification of Meaning. In Principles of Knowledge Representation and Reasoning: Proceedings of the
Sixth International Conference (KR?98), 245?255.
Pan, F., R. Mulkar, and J. R. Hobbs (2006). An Annotated Corpus of Typical Durations of Events. In
Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC),
77?83.
Winston, M. E., R. Chaffin, and D. Herrmann (1987, October). A Taxonomy of Part-Whole Relations.
Cognitive Science 11(4), 417?444.
364
