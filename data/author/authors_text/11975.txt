Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 84?92,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Quantitative analysis of treebanks using frequent subtree mining methods
Scott Martens
Centrum voor Computerlingu??stiek, KU Leuven
Blijde-Inkomststraat 13, bus 3315
3000 Leuven Belgium
scott@ccl.kuleuven.be
Abstract
The first task of statistical computational
linguistics, or any other type of data-
driven processing of language, is the ex-
traction of counts and distributions of phe-
nomena. This is much more difficult for
the type of complex structured data found
in treebanks and in corpora with sophisti-
cated annotation than for tokenized texts.
Recent developments in data mining, par-
ticularly in the extraction of frequent sub-
trees from treebanks, offer some solutions.
We have applied a modified version of the
TreeMiner algorithm to a small treebank
and present some promising results.
1 Introduction
Statistical corpus linguistics and many natural lan-
guage processing applications rely on extracting
the frequencies and distributions of phenomena
from natural language data sources. This is rela-
tively simple when language data is treated as bags
of tokens or as n-grams, but much more compli-
cated for corpora annotated with complex feature
schemes and for treebanks where syntactic depen-
dencies are marked. A great deal of useful infor-
mation is encoded in these more complex struc-
tured corpora, but access to it is very limited using
the traditional algorithms and analytical tools of
computational linguistics. Many of the most pow-
erful techniques available to natural language pro-
cessing have been built on the basis of n-gram and
bag of words models, but we already know that
these methods are inadequate to fully model the
information in texts or we would have little use
for treebanks or annotation schemes.
Suffix trees provide some improvement over
n-grams and bag-of-words schemes by identify-
ing all frequently occurring sequences regard-
less of length (Weiner, 1973; McCreight, 1976;
Ravichandran and Hovy, 2002). While this has
value in identifying some multi-word phenomena,
any algorithm that models languages on the basis
of frequent contiguous string discovery will have
trouble modeling a number of pervasive phenom-
ena in natural language. In particular:
? Long distance dependencies ? i.e., dependen-
cies between words that are too far apart to be
accessible to n-gram models.
? Flexible word orders ? languages usually
have contexts where word order can vary.
? Languages with very rich morphologies that
must be taken into account or where too much
important information is lost through lemma-
tization.
? Correlations between different levels of ab-
straction in annotation, such as between the
lemma of a verb and the semantic or syntac-
tic class of its arguments.
? Extra-syntactic correlations that may involve
any nearby word, such as semantic priming
effects.
In treebanks and other annotated corpora that
can be converted into rooted, directed graphs,
many of these phenomena are accessible as fre-
quently recurring subtrees. For example, consider
the Dutch idiom ?naar huis gaan?, (to go home).
The components of this phrase can appear in a va-
riety of orders and with words inserted between
the constituents:
1. Ik zou naar huis kunnen gaan. (I could go
home.)
2. We gaan naar huis. (We?re going home.)
In a treebank, these two sentences would share
a common subtree that encompasses the phrase
?naar huis gaan?, as in Figure 1. Note that for this
purpose, two subtrees are treated as identical if the
84
Figure 1: The two Dutch sentences Ik zou naar
huis kunnen gaan and We gaan naar huis, parsed,
and with the frequent section highlighted. Note
that these two subtrees are identical except for the
order of the nodes. (N.B.: This tree does not take
the difference between the infinitive and conju-
gated forms into account.)
only difference between them is the order of the
children of some or all the nodes.
Most theories of syntax use trees to represent in-
terlexical dependencies, and generally theories of
morphology and phonology use either hierarchical
tree structures to represent their formalisms, or use
unstructured bags that can be trivially represented
as trees. Most types of linguistic feature systems
are at least in part hierarchical and representable in
tree form. Because so many linguistic phenomena
are manifest as frequent subtrees within hierarchi-
cal representations that are motivated by linguistic
theories, efficient methods for extracting frequent
subtrees from treebanks are therefore potentially
very valuable to corpus and computational linguis-
tics.
2 Previous and Related Work
Tree mining research is a subset of graph min-
ing focused specifically on rooted, directed acyclic
graphs. Although there is research into extracting
frequent subtrees from free (unrooted and undi-
rected) trees, free tree mining usually proceeds by
deciding which node in any particular tree will
be treated as a root, and then treating it as if it
was a rooted and directed tree (Chi et al, 2003).
(a)
(b) (c) (d)
Figure 2: Tree (a) and different types of subtree:
(b) a bottom-up subtree of (a), (c) an induced sub-
tree of (a), and (d) an embedded subtree of (a).
Research on frequent subtree discovery generally
draws heavily on early work by Zaki (2002) and
Asai et al (2002) who roughly simultaneously be-
gan applying the Apriori algorithm to frequent tree
discovery (Agrawal et al, 1993). For a summary
of Apriori, which is widely used in data mining,
and a short review of its extensive literature, see
Kotsiantis and Kanellopoulos (2006). A broad
summary of algorithms for frequent subtree min-
ing can be found in Chi et al (2004).
Research into frequent substructures in compu-
tational linguistics is quite limited. The Data Ori-
ented Processing model (Bod et al, 2003) along
with its extension into machine translation - the
Data Oriented Translation model (Poutsma, 2000;
Poutsma, 2003; Hearne and Way, 2003) - is the
most developed approach to using frequent sub-
tree statistics to do natural language processing.
There is also growing work, largely stemming out
of DOP research, into subtree alignment in bilin-
gual parsed treebanks as an aid in the development
of statistical and example-based machine transla-
tions systems (Hassan et al, 2006; Tinsley et al,
2007; Zhechev and Way, 2008).
3 Key concepts
Among the key concepts in tree mining is the dif-
ference between bottom-up subtrees, induced sub-
trees and embedded subtrees. A bottom-up subtree
T
? of a tree T is a subtree where, for every node
in T ?, if its corresponding node in T has children,
then all those children are also in T ?. An induced
85
Figure 3: ?...between the European Commission and the government of the [German] Federal Repub-
lic...? This structure is a subtree of one of the sentences in the Alpino corpus of Dutch where a node has
two children with the same labels - two NPs. This often occurs with conjunctions and can prevent the
algorithm from discovering some frequent subtrees.
subtree T ? of T is a subtree where every node in
T
? is either the root of T ? or its parent in T is also
its parent in T ?. An embedded subtree T ? of T is
a subtree where every node in T ? is either the root
of T ? or its parent in T ? is one of its ancestors in
T . See Figure 2 for an example of these different
types of subtrees.
Linear time solutions exist for finding all fre-
quent bottom-up subtrees in a treebank because
this problem can be transformed into finding all
frequent substrings in a string, a problem for
which fast solutions are well known (Luccio et al,
2001; Luccio et al, 2004).
Solutions for induced and embedded subtrees
draw heavily on Zaki (2002) (the TreeMiner al-
gorithm) and Asai et al (2002) (the FREQT al-
gorithm), both of whom propose Apriori-style ap-
proaches. This type of solution has the general
property that runtime is proportionate to the size
of the output: the sum of the number of times
each frequent subtree appears in the treebank. This
is not readily predictable, because the number
and frequencies of subtrees is not formally de-
terminable from the size of the treebank and can
grow very rapidly.
3.1 Ordered and unordered trees
TreeMiner/FREQT approaches require all trees to
be ordered so that the nodes of any frequent sub-
tree will always appear in the same order every
time it appears. The children of each non-leaf
node are sorted into a lexicographic order, but
this only guarantees that frequent subtrees will al-
ways appear with the same ordering if no node
has more than one non-leaf child node with the
same label. This is not uniformly true of natural
language parse trees, as shown in Figure 3. So-
lutions exist that remove this limitation - notably
Chi et al (2003) - but they come at a significantly
increased processing cost.
3.2 Closed trees
Given that this type of approach to subtree discov-
ery has runtime bounds proportionate to the unpre-
dictable size of the output, one way to keep subtree
discovery within manageable bounds is to restrict
the output. Many of the frequent trees present in
treebanks are redundant, since they are identically
distributed with other, larger trees, as in Figure 4.
If a corpus has a sequence of tokens ABCDE
that appears f times, then that corpus also con-
tains at least f instances of the sequences A, B, C,
D, E, AB, BC, CD, DE, ABC, BCD, CDE,
ABCD, and BCDE. If any of these sequences
appears only in the context of ABCDE, then they
are redundant, because they have the same count
and distribution as the longer sequence ABCDE.
If a set of sequences is identically distributed -
appearing in all the same places - then the longest
of those sequences is called a closed sequence. In
more formal terms, a sequence S that appears f
times in a corpus is called closed if and only if
there is no prefix or suffix a such that aS or Sa
also appears f times in the corpus. This definition
extends easily to trees: A subtree T in a treebank
is closed if and only if there is no node that can be
added to it to produce a new subtree T ? such that
the frequency of T ? is equal to the frequency of T .
All subtrees in a corpus are either closed subtrees
or are subtrees of closed subtrees that appear in
exactly the same places in the treebank. The set of
closed subtrees in a treebank is the smallest set of
subtrees that encompasses all the distributions of
subtrees in the treebank. Any subtree that is not in
the list of closed subtrees is either a subtree of one
of the closed subtrees that appears exactly as often
and in all the same places, or does not appear in
the treebank at all.
There are algorithms that extract only
closed subtrees from treebanks - notably
Chi et al (2005a) - and thereby increase
their speed dramatically without producing less
86
(a) The common subtree
of the two parse trees in
Figure 1: ?naar huis gaan?
(b) Redundant subtrees of tree (a). There
are many more such structures.
Figure 4: Closed and non-closed subtrees in just
the two sentences in Figure 1. In a larger treebank,
some of these might not be redundant.
information, since any non-closed subtree present
in the treebank is a subtree of a closed one and
shares its distribution.
4 Algorithm and data structures
The algorithm used in this research is an extension
of the TreeMiner algorithm (Zaki, 2002), modified
to extract only closed subtrees. It takes a minimum
frequency threshold as a parameter and extracts
only those subtrees which are closed and whose
frequency is at least equal to the threshold. This
algorithm suffers from the same shortcoming of
Zaki?s original algorithm in that it is only guar-
anteed to find all frequent subtrees among ordered
trees where no node has two non-leaf children with
the same label.
It has one novel property which it appears not
to share with any other subtree extraction scheme
to date: This algorithm outputs subtrees in order
from the most frequent to the least. Given the
difficulty of predicting in advance how large the
output will be, and the large size of many natural
language data sources, this can be a real boon. If
output size or memory usage grow too large, or too
much time has passed, the program can be stopped
while still guaranteeing that it has not missed any
more frequent subtree than the last one outputted.
This section can only very briefly describe the
algorithm.
4.1 Definitions
A treebank is any collection of trees where each
node bears a label and each node is uniquely ad-
dressable in such a way that the address a
n
of a
node n is always greater than the address a
p
of its
parent p. This is accomplished by representing all
trees as ordered depth-first canonical strings. (See
Chi et al (2005b).)
Each appearance of a subtree within a treebank
is characterized by the address of its root in the
treebank and the address of its rightmost node.
This data structure will be called a Hit. The list
of all Hits corresponding to all the appearances
of some subtree in the treebank will be called a
HitList. So, for each subtree there is a correspond-
ing HitList and vice-versa. HitLists are always
constructed in sequential order, from first instance
in the treebank to last, and can never contain du-
plicates.
We will define the function queueKey on
HitLists to output an array of four numbers in a
specific order, given a HitList as input:
1. The number of Hits in the HitList.
2. The distance from the address of the root of
the first Hit to the end of the treebank.
3. The distance from the address of the right-
most node of the first Hit to the end of the
treebank.
4. The number of nodes in the subtree associ-
ated with that HitList.
These keys are sortable and designed to ensure
that HitLists from a single treebank can always be
sorted into a fixed order such that, for two HitLists
A and B, if A > B then:
1. A has more Hits than B.
2. If A has the same number of Hits as B, then
the root of the first Hit in A precedes the root
of the first Hit in B.
3. If A?s first root is identical to B?s, then the ad-
dress of the rightmost node of the first Hit in
A precedes the address of the rightmost node
of the first Hit in B.
4. If the first Hit in A is exactly the same the first
Hit in B, then the subtree associated with A
has more nodes than the subtree associated
with B.
87
A self-sorting queue is any data structure that
stores key-data pairs and stores the keys in order
from greatest to least. The data structure used to
implement a self-sorting queue in this research is
an AVL tree (Adelson-Velskii and Landis, 1962),
however, other structures could equally well have
been used. The self-sorting queue will be used to
maintain a sorted list of HitLists, sorted in the or-
der of their queueKeys as described above.
4.2 Initialization
Fix a minimum frequency threshold t for the sub-
trees you wish to extract from the treebank. Start
processing by initializing one HitList for each
unique label in the treebank with the set of Hits
that corresponds to each occurrence of that label.
We will treat each as a HitList with an associ-
ated subtree containing only one node. This set
is constructed in linear time by iterating over all
the nodes in the treebank.
Of the initial HitLists, throw away all those with
fewer than threshold frequency t Hits in them.
The remaining HitLists are inserted into the self-
sorting queue.
4.3 Extracting induced subtrees without
checking for closure
Extracting all the subtrees above a fixed frequency
- not just the closed subtrees - in order from the
most frequent to the least, proceeds as follows:
1. Initialize as described in Section 4.2.
2. Pop the top HitList hl and its associated sub-
tree s from the queue.
3. Extend hl:
(a) Visit each Hit in hl and find all the
nodes that can be added to the right side
of s to produce new induced subtrees.
(b) Generate new HitLists for all subtrees
that extend s by one node to the right.
(c) Test each new HitList to make sure it
appears more than threshold frequency
t times, and if it does, insert it into the
queue.
(d) Output s and hl.
4. Repeat until the queue is empty.
This is essentially identical to the TreeMiner
and FREQT algorithms already published by
Zaki (2002) and by Asai et al (2002), except that
it outputs frequent subtrees in order from the most
frequent to the least.
4.4 Extracting only closed induced subtrees
By controlling the order in which HitLists reach
the top of the queue, it is possible to efficiently
prevent any subtree which is not a closed sub-
tree or a prefix of a closed subtree from being
extended, and to prevent any subtree that is not
closed from being outputted.
Every subtree with a frequency of f is either
a closed subtree, a prefix of a closed subtree that
also has a frequency of f and can be constructed
by adding more nodes to the right, or is a redun-
dant non-closed subtree that need not be extended
or stored. Consider a redundant, non-closed sub-
tree x and a closed subtree or prefix of a closed
subtree y which has the same frequency, and has
the same set of addresses for the rightmost node of
each of its appearances in the treebank. The sort
order of the self-sorting queue (see Section 4.1)
ensures that if a prefix of a closed subtree y is in
the queue and some subtree of it x is also in the
queue, then y is closer to the top of the queue than
x is. Furthermore, it can be proven that the pre-
fix of a closed subtree with the same distribution
as any non-closed, redundant subtree will be gen-
erated, inserted into the queue, and removed from
the top of the queue before x can reach the top.
So, to prevent x from being extended or stored,
all that is necessary is to check to see there is some
closed subtree or prefix of a closed subtree y such
that:
? y has already been at the top of the queue.
? y has the same frequency as x.
? The set of rightmost nodes of every Hit in
y?s HitList is identical to the set of rightmost
nodes of every Hit in x?s HitList.
? x is a subtree of y
This can be checked by constructing a hash
value for each HitList based on its frequency and
some subset of the set of rightmost nodes of ev-
ery Hit. In our experiments, we used only the first
node of each HitList. If x?s hash value matches
some previously processed y?s hash value, then
check if x is a subtree of y and reject it if it is.
The result is to only instantiate closed subtrees and
their prefixes, and subtrees which are one node ex-
tensions of closed subtrees and their prefixes.
Like TreeMiner, worst case space and time
bounds are proportionate to the number of sub-
trees instantiated and the number of times each
appears in the corpus. This is smaller than the
88
worst case bounds for TreeMiner because it does
not instantiate all frequent subtrees. There is addi-
tional approximately constant time processing for
each instantiated subtree to check for closure and
to store it in the self-sorting queue. At the lowest
frequency thresholds, this can take up the major-
ity of runtime, but is generally negligible at high
frequencies.
5 Results
We applied this algorithm to a parsed and hand-
corrected 7137 sentence subset of the Alpino Tree-
bank of Dutch.1 The average sentence length in
this small treebank is roughly 20 words, and the
corresponding trees have an average of approx-
imately 32 nodes for a total of 230,673 nodes.
With the minimum frequency set to 2, this algo-
rithm extracted 342,401 closed subtrees in about
2000 seconds on a conventional workstation run-
ning Linux2. The same implementation but with-
out testing for closure - which makes this algo-
rithm equivalent to TreeMiner - extracted some 4.2
million trees in roughly 11,000 seconds. Closed
tree extraction contrasts quite favorably to extrac-
tion without closure, even over a small dataset.
Min. Freq. Subtrees extracted Runtime
Threshold
2 342401 1952.33s
3 243484 1004.30s
4 176885 588.58s
5 134495 402.26s
8 72732 209.51s
10 53842 163.22s
15 30681 112.39s
20 20610 85.24s
30 11516 66.05s
40 7620 54.14s
50 5549 47.98s
60 4219 43.24s
70 3365 39.97s
Table 1: Runtime and closed trees extracted at dif-
ferent minimum frequency thresholds, using the
7137 sentence sample of the Alpino Treebank.
Runtime and the number of trees produced fall
very dramatically as thresholds rise - so much so
1http://www.let.rug.nl/vannoord/trees/
2A Dell Precision 490 workstation with an Intel Dual-
Core Xeon processor and 8GB of memory. The algorithm
was not implemented to use two processors.
Sentences Total Subtrees Runtime
nodes extracted
2500 94528 37607 61.08s
5000 189170 98538 260.91s
10000 379980 264616 1495.19s
15000 573629 477750 3829.29s
20000 763502 704018 7998.57s
Table 2: Runtime and closed trees extracted from
automatically parsed samples of the Europarl
Dutch corpus, keeping the minimum frequency
threshold constant at 5 for all sizes of treebank.
that setting the minimum frequency to 3 instead of
2 halved the runtime. This pattern is characteristic
of a power law distribution like Zipf?s law. (See
Table 1 and Figure 5.) Given the pervasiveness
of power law distributions in word frequencies, it
should perhaps not be surprising to discover that
frequent closed subtrees in treebanks are similarly
distributed. This research may be the first effort
to empirically support such a conclusion, although
admittedly only very tentatively.
To test the impact of varying the size of the tree-
bank, but keeping the minimum frequency thresh-
old constant, we used a section of the Dutch por-
tion of the Europarl corpus (Koehn, 2005) auto-
matically parsed using the Alpino Dutch parser
(van Noord, 2006) without any manual correction.
Random samples of 2500, 5000, 10000, 15000
and 20000 sentences were selected, and all sub-
trees of frequency 5 or higher were extracted from
each, as summarized in Table 2. As treebank size
grows, the number of subtrees extracted at the
same minimum frequency threshold, and the time
and memory used extracting them, grows expo-
nentially. This is in sharp contrast to algorithms
that extract frequently recurring strings, which in-
crease linearly in time and memory usage as the
data grows.
However, if the minimum frequency threshold
is kept constant as a proportion of the size of the
treebank, then the number of trees extracted re-
mains roughly constant and the time and memory
used to extract them grows roughly linearly with
the size of the treebank. Table 3 shows the result
for different sized random samples of the parsed
Europarl corpus.
Lastly, since this algorithm has known difficul-
ties when presented with trees where more than
one non-leaf child of a node can have the same
89
(a) Runtime by minimum fre-
quency threshold.
(b) Subtrees extracted by mini-
mum frequency threshold.
(c) Log-log plot of (b).
Figure 5: Runtime (a) and subtrees extracted (b) from the Alpino sample using different minimum fre-
quency thresholds. Figure (c) is a log-log plot of (b). Figure (c) looks close to a straight line, which is
characteristic of a power law distribution.
Sents Total Min. Subtrees Run
nodes Freq. extracted time
Thres.
2500 99323 5 42905 72.95s
5000 194736 10 42783 122.18s
10000 382022 20 41988 216.23s
15000 574632 30 43078 325.86s
20000 770240 40 44416 435.19s
Table 3: Runtime and closed trees extracted from
automatically parsed samples of the Europarl
Dutch corpus, with minimum frequency thresh-
olds kept roughly constant as a proportion of the
sample size.
label (see sections 3.1 and 4), we attempted to
determine if this problem is marginal or perva-
sive. The 7137 sentence Alpino Treebank sample
contains 3833 nodes with more than one non-leaf
child node with identical labels or roughly 1.7% of
all nodes. Furthermore, these nodes are present in
2666 sentences - some 37% of all sentences! This
is a very large minority.
In order to estimate the effect this phenomenon
has on the extraction of closed trees, we looked
for outputted trees that are not closed by compar-
ing the HitLists of all outputted trees to all other
outputted trees with the same frequency. Table 4
shows the number of trees with identical distribu-
tions to other outputted trees - i.e. trees that ap-
peared to be closed to this algorithm, but in fact
are not. The number was surprisingly large, but
distributed overwhelmingly at the very lowest fre-
quencies.
Min. Freq. Non-closed as a % of
Threshold trees all trees extracted
2 2874 0.84%
3 693 0.28%
4 225 0.13%
5 101 0.08%
8 18 0.02%
10 11 0.02%
15 6 0.02%
20 3 0.01%
30 0 0.00%
Table 4: Non-closed trees from the 7137 sentence
sample of the Alpino Treebank, produced erro-
neously as closed trees because of repeated labels.
There were no non-closed trees extracted at fre-
quencies over 30.
6 Conclusions
The algorithm presented above opens up tree-
banks and annotated corpora to much more de-
tailed quantitative analysis, and extends the tools
available for data-driven natural language process-
ing. This makes a number of new applications
possible. We are developing treebank indexing for
fast retrieval by tree similarity, in order to make
full treebanks available for example-based parsing
and machine translation in real time. This algo-
rithm also has applications in constructing concor-
dances of syntactic, morphological and semantic
structures - types of information that are not tradi-
tionally amenable to indexing. Furthermore, sta-
tistical models of natural language data can take
90
advantage of comprehensive subtree censuses to
become fully syntax-aware, instead of relying on
bag of words and n-gram models.
However, there are a number of drawbacks and
caveats that must be highlighted.
Runtime, memory usage and output size are dif-
ficult to estimate in advance. This is mitigated in
part by the order in which subtrees are outputted,
making it possible to extract only the most fre-
quent subset of subtrees given fixed time and space
bounds. Empirically, it appears that resource re-
quirements and output size can also be estimated
by sampling, if minimum frequency thresholds
can be kept constant as a proportion of total tree-
bank size.
The formalisms used in most theories of syn-
tax allow nodes to have multiple non-leaf chil-
dren with the same labels. Although errors caused
by non-unique labels are overwhelmingly present
only among the lowest frequency subtrees, er-
rors appear often enough to pose a non-negligible
problem for this algorithm.
We are investigating the degree to which this
can be mitigated by making different choices of
linguistic formalism. Syntax trees that contain
only binary trees - for example, those constructed
using Chomsky Normal Form rules (Jurafsky and
Martin, 2009) - cannot have identically labelled
non-leaf children, but must suffer some loss of
generality in their frequent subtrees because if it.
Other theories can reduce the size of this source of
error, notably dependency syntax which often uses
fewer abstract labels (Tesnie`re, 1959; Mel?c?uk,
1988; Sugayama and Hudson, 2006), but will most
likely be poor sources of highly general rules as a
consequence.
Furthermore, tree mining algorithms exist that
eliminate this problem, but at some cost. We are
investigating a hybrid solution to the non-unique
label problem that identifies only those subtrees
where more resource-intensive closure checking is
necessary. This will guarantee the correct extrac-
tion of closed subtrees in all cases while minimiz-
ing the additional processing burden.
Among the open problems suggested by this re-
search is the degree to which the empirical results
obtained above are dependent on the language of
the underlying data and the linguistic formalisms
used to produce treebanks. Different linguistic
theories use different abstractions and use their ab-
stract categories differently. This has an immedi-
ate effect on the number of nodes in a treebank
and on the topology of the trees. Some theories
produce more compact trees than others. Some
produce deep trees, others produce shallow trees.
It is likely that the formalisms used in treebanks
have a pervasive influence on the number and kind
of frequent subtrees extracted. By doing quantita-
tive research on the structures found in treebanks,
it may become possible to make reliable opera-
tional choices about the linguistic formalisms used
in treebanks on the basis of the kinds of structures
one hopes to get out of them.
Acknowledgments
This research is supported by the AMASS++
Project3 directly funded by the Institute for the
Promotion of Innovation by Science and Technol-
ogy in Flanders (IWT) (SBO IWT 060051).
References
Georgiy M. Adelson-Velskii and Yevgeniy M. Landis.
1962. An algorithm for the organization of informa-
tion. Proceedings of the USSR Academy of Sciences,
146(2):263?266.
Rakesh Agrawal, Tomasz Imielinski and Arun Swami.
1993. Mining association rules between sets of
items in large databases. Proceedings of the 1993
ACM SIGMOD International Conference on Man-
agement of Data, 207?216.
Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hi-
roki Arimura, Hiroshi Sakamoto and Set-
suo Arikawa. 2002. Efficient substructure
discovery from large semi-structured data. Proceed-
ings of the Second SIAM International Conference
on Data Mining, 158?174.
Rens Bod, Khalil Sima?an and Remko Scha, editors.
2003. Data-Oriented Parsing. CLSI Publicatons,
Stanford, CA.
Yun Chi, Yirong Yang and Richard R. Muntz. 2003.
Indexing and Mining Free Trees. UCLA Computer
Science Department Technical Report No. 030041.
Yun Chi, Richard R. Muntz, Siegfried Nijssen and
Joost N. Kok. 2004. Frequent Subtree Mining ? An
Overview. Fundamenta Informaticae, 66(1-2):161?
198.
Yun Chi, Yi Xia, Yirong Yang and Richard R. Muntz.
2005a. Mining Closed and Maximal Frequent
Subtrees from Databases of Labeled Rooted Trees.
IEEE Transactions on Knowledge and Data Engi-
neering, 17(2):190?202.
3http://www.cs.kuleuven.be/?liir/
projects/amass/
91
Yun Chi, Yi Xia, Yirong Yang and Richard R. Muntz.
2005b. Canonical forms for labelled trees and
their applications in frequent subtree mining. IEEE
Transactions on Knowledge and Data Engineering,
8(2):203?234.
Hany Hassan, Mary Hearne, Khalil Sima?an and
Andy Way. 2006. Syntactic Phrase-Based Statisti-
cal Machine Translation. Proceedings of the IEEE
2006 Workshop on Spoken Language Translation,
238?241.
Mary Hearne and Andy Way. 2003. Seeing the Wood
for the Trees: Data-Oriented Translation. Proceed-
ings of the 9th Machine Translation Summit, 165?
172.
Daniel Jurafsky and James H. Martin. 2009. Speech
and Language Processing. Pearson Prentice Hall,
Upper Saddle River, NJ.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of the
10th Machine Translation Summit, 79?86.
Sotiris Kotsiantis and Dimitris Kanellopoulos. 2006.
Association Rules Mining: A Recent Overview.
GESTS International Transactions on Computer
Science and Engineering, 32(1):71?82.
Fabrizio Luccio, Antonio Enriquez, Pablo Rieumont
and Linda Pagli. 2001. Exact Rooted Subtree
Matching in Sublinear Time. Universita` Di Pisa
Technical Report TR-01-14.
Fabrizio Luccio, Antonio Enriquez, Pablo Rieumont
and Linda Pagli. 2004. Bottom-up subtree isomor-
phism for unordered labeled trees. Universita` Di
Pisa Technical Report TR-04-13.
Edward M. McCreight. 1976. A Space-Economical
Suffix Tree Construction Algorithm. Journal of the
Association for Computing Machinery, 23(2):262?
272.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press,
Albany, NY.
Arjen Poutsma. 2000. Data-Oriented Translation.
Proc. of the 18th International Conference on Com-
putational Linguistics (COLING 2000), 635?641.
Arjen Poutsma. 2003. Machine Translation with Tree-
DOP. Data-Oriented Parsing, 63?81.
Deepak Ravichandran and Eduard Hovy 2002. Learn-
ing surface text patterns for a question answering
system. Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics
(ACL-02), 41?47.
Kensei Sugayama and Richard Hudson, editors. 2006.
Word Grammar: New Perspectives on a Theory of
Language Structure. Continuum International Pub-
lishing, London.
Lucien Tesnie`re. 1959. Ele?ments de la syntaxe struc-
turale. Editions Klincksieck, Paris.
John Tinsley, Mary Hearne and Andy Way. 2007.
Exploiting Parallel Treebanks for use in Statisti-
cal Machine Translation. Proceedings of Treebanks
and Linguistic Theories (TLT ?07), Bergen, Norway
175?187.
Gertjan van Noord. 2006. At last parsing is now
operational. Verbum Ex Machina. Actes de la
13e confe?rence sur le traitement automatique des
langues naturelles (TALN6), 20?42.
Peter Weiner. 1973 Linear pattern matching algorithm.
14th Annual IEEE Symposium on Switching and Au-
tomata Theory, 1?11.
Mohammed J. Zaki. 2002. Efficiently mining fre-
quent trees in a forest. Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, 1021?1035.
Ventsislav Zhechev and Andy Way. 2008. Auto-
matic Generation of Parallel Treebanks. Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (COLING 2008), 1105?1112.
92
Coling 2010: Poster Volume, pages 810?818,
Beijing, August 2010
Varro: An Algorithm and Toolkit for Regular Structure Discovery in
Treebanks
Scott Martens
Centrum voor Computerlingu??stiek, KU Leuven
scott@ccl.kuleuven.be
Abstract
The Varro toolkit is a system for identi-
fying and counting a major class of reg-
ularity in treebanks and annotated nat-
ural language data in the form of tree-
structures: frequently recurring unordered
subtrees. This software has been designed
for use in linguistics to be maximally
applicable to actually existing treebanks
and other stores of tree-structurable nat-
ural language data. It minimizes mem-
ory use so that moderately large treebanks
are tractable on commonly available com-
puter hardware. This article introduces
condensed canonically ordered trees as a
data structure for efficiently discovering
frequently recurring unordered subtrees.
1 Credits
This research is supported by the AMASS++
Project1 directly funded by the Institute for the
Promotion of Innovation by Science and Technol-
ogy in Flanders (IWT) (SBO IWT 060051).
2 Introduction
Treebanks and similarly enhanced corpora are in-
creasingly available for research, but these more
complex structures are resistant to the techniques
used in NLP for the statistical analysis of strings.
This paper introduces a new treebank analysis
suite Varro, named after Roman philologist Mar-
cus Terentius Varro (116 BC-27 BC), who made
linguistic regularity and irregularity central to his
1http://www.cs.kuleuven.be/?liir/projects/amass/
philosophy of language in De Lingua Latina.
(Harris and Taylor, 1989)
The Varro toolkit focuses on a general problem
in performing statistical analyses on treebanks:
identifying, counting and extracting the distribu-
tions of frequently recurring unordered subtrees
in treebanks. From this base, it is possible to con-
struct more linguistically motivated schemes for
performing treebank analysis. Complex statistical
analyses are constructed from knowledge about
frequency and distribution, so this constitutes a
low level task on top of which higher level analy-
ses can be performed.
An algorithm that can efficiently extract fre-
quently recurring subtrees from treebanks has a
number of immediate applications in computa-
tional linguistics:
? Speeding up treebank search algorithms like
Tgrep2. (Rohde, 2001)
? Rule discovery for tree transducers used in
parsing and machine translation. (Knight and
Graehl, 2005; Knight, 2007)
? Generalizing lexical statistics techniques in
NLP ? e.g., collocation ? to a broader array
of linguistic structures. (Sinclair, 1991)
? Efficiently identifying useful features for tree
kernel methods. (Moschitti, 2006)
3 Theory and Previous Work
For the purposes of this paper, a treebank is any
collection of disjoint labeled trees. While in prac-
tice this mostly means parsed natural language
sentences, the approach described here is equally
applicable to other kinds of data, including seman-
tic feature structures, morphological analyses, and
810
doubtless many other kind of linguistically moti-
vated structures. Figure 1 is an example of a parse
tree from a Dutch-language treebank.
Figure 1: A tree from the Europarl Dutch cor-
pus. (Koehn, 2005) It has been parsed and labeled
automatically by the Alpino parser. (van Noord,
2006) A word-for-word translation is ?It also has
a legal reason.? (? ?There is also a legal reason
(for that).?)
In this paper, we are concerned with identify-
ing and counting frequent induced unordered sub-
trees in treebanks. The term subtree has a number
of definitions, but this paper will follow the ter-
minology of Chi et al (2004). Figure 2 contains
three examples of induced unordered subtrees of
the tree in Figure 1. Note that the ordering of
the vertices in the subtrees is different from that
of Figure 1. This is what makes them unordered
subtrees. Induced subtrees are more formally de-
scribed in Section 4.
3.1 Apriori
The research builds on frequent subtree discov-
ery algorithms based on the well-known Apri-
ori algorithm, which is used to discover fre-
quent itemsets in databases. (Agrawal et al,
1993) As a brief summary of Apriori, con-
sider a collection of ordered itemsets C =
{{a, b, c}, {a, b, d}, {b, c, d, e}}. Apriori discov-
ers all the subsets of those elements that appear at
least some user-determined ? times. As an exam-
ple, let us set ? = 2, and then count the number
of times each unique item appears in C. Any sin-
gle element in C that appears less than two times
cannot be a member of a set of elements that ap-
(a)
(b) (c)
Figure 2: Three induced unordered subtrees of the
tree in Figure 1
pears at least ? times (since ? = 2), so those
are rejected. Each of the remaining set elements
{a, b, c, d} is extended by counting the number
of two-element sets that include it and some el-
ement to the right in the ordered itemsets in C.
For b, these are {{b, c}, {b, d}, {b, e}}. Of this
set, only those that appear at least ? times are re-
tained: {{b, c}, {b, d}}. This process is repeated
for size three sets, and iterated over and over for
increasingly large subsets, until there are no ex-
tensions that appear at least ? times. This whole
procedure is then repeated for each unique item.
Finally, Apriori will have extracted and counted
all itemsets that appear at least ? times in C.
Extending Apriori to frequent subtree dis-
covery dates to the work of Zaki (2002) and
Asai et al (2002). Chi et al (2004) summa-
rizes much of this line of research. In Apriori,
larger and less frequent itemsets are discovered
811
(a) (b) (c)
(d)
Figure 3: 3(b) and 3(c) are a subtrees of 3(a). The
subtrees in 3(d) are possible extensions to 3(b),
while 3(c) is not.
and counted by adding items to shorter and more
frequent ones. This extends naturally to trees by
initially locating and counting all the one-vertex
trees in a treebank, and then constructing larger
trees by adding vertices and edges to their right
sides.
In Figure 3, subtree 3(b) has as valid extensions
subtrees 3(d), all of which extend 3(b) to the right.
An extension like subtree 3(c), which adds a node
to the left of the rightmost node of 3(b), is not a
valid extension.
3.2 Treebank applications
Applying these algorithms to natural language
treebanks, however, presents a number of chal-
lenges.
The approach described above, because it con-
structs and tests subtrees by moving from left to
right, is well-suited to finding ordered subtrees.
However, this paper will consider unordered sub-
trees as better motivated linguistically. Word or-
der is not completely fixed in any language, and
can be very free in many important contexts.
But there are other problems as well. Apriori-
style algorithms have the general property that
their run-time is proportionate to the size of the
output. Given a data-set D and a user-determined
minimum frequency threshold ?, this class of so-
lution outputs all the patterns that appear at least
? times in D. If D contains n patterns that ap-
pear at least ? times, P = {p1, p2, ..., pn};?pi ?
P : freq(pi) ? ?, then the time necessary to
identify and count all the patterns in P is pro-
portionate to
?n
i=1 freq(pi). In weakly corre-
lated data, this is a very efficient method of find-
ing patterns. In highly correlated data, however,
the number of patterns present can become pro-
hibitively large and extend run-time to unaccept-
able lengths, especially for small ? or large data-
sets. Each frequent pattern may have any number
of sub-patterns, each of which is also frequent and
must be separately counted.
If we identify patterns with subtrees, a subtree
with n vertices will, depending on its structure,
have a minimum of n(n ? 1) and a maximum
of (n ? 1)! + 1 subtrees. If each of those sub-
trees is also a pattern that must be counted, then
runtime grows very rapidly even for very small
data-sets. Since natural language data is highly
correlated, simple subtree-discovery extensions of
Apriori, like those proposed in (Zaki, 2002) and
(Asai, 2002), are not feasible for linguistic use. As
reported in Martens (2009b), run-times become
intractably long very quickly as data size increases
for really existing treebanks.
However, there are compact representations of
frequent patterns that are better suited to highly-
correlated data and which can be efficiently dis-
covered by modified Apriori schemes. This pa-
per will only address one such representation: fre-
quent closures. (Boulicaut and Bykowski, 2000)
Frequent closures are widely used in subtree dis-
covery and have an intuitive meaning when dis-
cussing natural language.
Given a treebank D, and a tree T that has a sup-
port of freq(T ) = ?, then T is closed if there is
no supertree T ? ? T where freq(T ?) = ?. In Fig-
ure 3, if subtree 3(c) is as frequent in some tree-
bank as 3(b), then 3(b) is not a closed subtree, nor
can any further extension of it to the right be a
closed subtree.
As a natural language example, given a corpus
812
of English sentences, let us assume we have found
a pattern of the form ?NP make up NP to VP?,
such as in ?He has made up his mind to study lin-
guistics.? If every time this pattern appears in the
corpus, the second NP contains ?mind?, then the
pattern is not closed. A larger pattern appears just
as often and in exactly the same places.
This makes the notion of frequent closed sub-
tree discovery a generalization of collocation and
coligation - well known in corpus-based lexicog-
raphy - to arbitrary tree structures. (Sinclair,
1991) J.R. Firth famously said, ?You shall know
a word by the company it keeps.? (Firth, 1957)
Frequent subtree discovery tells us exactly what
company entire linguistic structures keep.
3.3 Efficient closed subtree discovery
Chi et al (2005a) outlines a general method for ef-
ficiently finding frequent closed subtrees without
finding all frequent subtrees first. Their approach
requires each subtree found to be aligned with its
supertree before checking for closure and exten-
sions. However, the alignment between a subtree
and its supertree - the map from subtree vertices
to supertree vertices - is not necessarily unique. A
subtree may have a number of possible alignments
with its supertree, even if one or more of the ver-
tex alignments is specificed, as shown in Figure 4,
which uses an example from the hand-corrected
Alpino Treebank of Dutch.2
This can only be avoided by adding a restriction
to trees: the combination of edge and vertex labels
for each child of a vertex must be unique. This
guarantees that specifying just one vertex in the
alignment of a subtree to its supertree is enough
to determine the entire unique mapping, but it is
incompatible with most linguistic theories. Pro-
cesses like tree binarization can meet this require-
ment, but only with some loss of generality: Some
frequent closed subtrees in a collection of trees
like Figure 4(a) will no longer be frequent, or will
be less frequent, in a collection of binary trees.
Martens (2009a) describes an alternative
method of checking for closure which does not
require alignment and can, consequently, be much
faster. It has, however, two drawbacks: First,
it does not find all frequent closed unordered
2http://www.let.rug.nl/vannoord/trees/
subtrees. Figure 5 shows the kind of tree where
that approach is unable to correctly identify and
count an unordered subtree. Second, it requires a
great deal more memory than solutions that align
each subtree discovered and check directly for
closure, and is therefore of limited use with very
large corpora.
4 Definitions
A fully-labeled rooted tree is a rooted tree in
which each vertex and each edge has a label: T :=
?V,E, LV , LE?, where V is the set of vertices, E
is the set of edges, LV is a map LV : V ? LV
from the vertices to a set of labels; and similarly
LE maps the edges to labels LE : E ? LE . We
will designate an edge e connecting vertex v1 to
its child v2 by the notation e = ?v1, v2?. LV and
LE constitute collectively the lexicon. Figure 1 is
an example of a fully-labelled, rooted tree from
a Dutch-language treebank. This formalization
is broadly applicable to all linguistic formalisms
whose structures are tree-based or can be con-
verted one-to-one into trees without loss of gener-
ality. This may require some degree of restructur-
ing of the tree formats used in particular linguistic
theories. For example, in many formal linguistic
theories, labels are not atomic symbols, but may
have many parts or even whole structured feature
sets. In general, these can be mapped to trees with
atomic labels by inserting additional vertices, or
by taking advantage of edge labelling.
The algorithm described here is insufficient
for formal structures that require more powerful
graph formalisms like directed acyclic graphs.
The relations parent, child and sibling are taken
here in their ordinary sense in discussing trees. In
Figure 1, the vertex labeled adv is a child of the
vertex labeled smain, the parent of the vertex la-
beled ook, and a sibling of the vertex labeled verb
and the two vertices labeled np. To simplify defi-
nitions, the operator label(x) will indicate the la-
bel of vertex or edge x.
An induced unordered subtree is a connected
subset of the vertices of some tree that preserves
the vertex and edge labels and the parent-child re-
lations of that tree but need not preserve the or-
dering of siblings. Given a fully-labeled tree T :=
?VT , ET , LVT , LET ?, an induced subtree S of T is
813
(a) (b)
Figure 4: In 4(a) is a Dutch phrase conjoining multiple nouns. It translates as ?police work, recreation,
planning and court activities?. 4(b) has six unique unordered alignments with 4(a).
(a) (b) (c)
Figure 5: Subtree 5(c) is an unordered subtree of both 5(a) and 5(b), but the algorithm described in
Martens (2009a) is unable to capture this in all cases.
a fully-labeled tree S := ?VS , ES , LVS , LES ? for
which there is an injection M : VS ? VT from
the vertices of S to some subset of the vertices of
T , and for which:
?v ? VS :
a. label(v) = label(M(v))
b. e = ?parent(v), v? ? ES ?
e? = ?M(parent(v)),M(v)? ? ET
c. label(e) = label(e?)
See Figures 1 and 2 for examples of subtrees of
a particular tree.
We will further define all subtrees that are iden-
tical except in the ordering of their vertices to be
unordered isomorphic. If a tree T is a subtree of
tree T ?, we will follow set notation by denoting
this relation as T ? T ?.
4.1 Canonical Ordering
Using canonical orderings to solve frequent un-
ordered subtree problems was first proposed in
Luccio et al (2001) and expanded by other
researchers in frequent subtree discovery tech-
niques, notably in Chi et al (2005b). Since the
Apriori-style approaches described in Section 3.1
are suited only to finding subtrees whose vertices
appear in a particular order, this paper will de-
scribe a mechanism for converting fully-labeled
trees into canonical forms that guarantee that all
instances of any unordered subtree will have an
identical order to their vertices.
We must first define a strict total ordering over
vertex and edge labels. Given lexica for the edge
and vertex labels, LE and LV respectively, we
define a strict total ordering on each such that
?li, lj ? L either li ? lj or li  lj or li = lj
and if li ? lj and lj ? lk, then li ? lk.
In a collection of fully-labeled trees, ev-
ery vertex v that is not the root of some
tree can be associated with a full la-
bel which is the pair fullLabel(v) =
?label(?parent(v), v?), label(v)?, containing
the label of the edge leading to its parent and the
label of the vertex itself. For any pair of vertices
where the edge to their parent is different, we
814
order the vertices by the order of those edges.
Where the edges are the same, we order them
by the ordering of their vertex labels. Where
we have two sibling vertices vi and vj such that
fullLabel(vi) = fullLabel(vj), we recursively
order the descendants of vi and vj , and then
compare them. In this way, two nodes can only
have an undefined order if they have both exactly
the same full labels and identical descendants.
A canonically ordered tree is a tree T :=
?VT , ET , LVT , LET ?, where for each v ? VT , the
children of v are ordered in just that fashion.
4.2 Condensed trees
A condensed tree is a fully-labeled tree T :=
?VT , ET , LVT , LET ? with two additional proper-
ties:
a. Each vertex v ? V is associated to a list of
indices parentIndex(v) = {i1, i2, ..., in},
which we will call its parent index. Each en-
try i1, i2, ..., in is a non-negative integer.
b. No vertex v ? V has two children with the
same full label.
Condensed trees are constructed from non-
condensed trees as follows:
Given a tree T := ?V,E, LV , LE?, we first
canonically order it, as described in the previous
section. Then, we attach a parent index to each
vertex v ? V which is not the root of T . The ini-
tial parent index of each node consists of a single
zero.
We then traverse the vertices of the now or-
dered tree T in breadth-first order from the the
root downwards and from left to right. Given
some vj ? V , if it has no sibling to its right,
or if the sibling to its immediate right has a dif-
ferent vertex label or a different edge label on
the edge to its parent, we do nothing. Other-
wise, if vj has a sibling to its immediate right
vi with the same full label, we set `i to the size
of parentIndex(vi), and then we append the
parentIndex(vj) to parentIndex(vi). Then, we
take the children of vj , and for each one, we incre-
ment each value in its parent index by `i, and then
insert it under vi as one of vi?s children. We delete
vj and then we reorder the children of vi into the
canonical order defined in Section 4.1.
This is performed in breadth-first order over T .
The result is guaranteed to be a tree where each
vertex never has two children with the same edge
and vertex labels. Figure 6 shows how the trees
in Figure 5 look after they are converted into con-
densed trees. We will denote condensed trees as
T = cond(T ), to indicate that T has been con-
structed from T .
If two non-condensed trees are unordered iso-
morphic, then their condensed forms will be iden-
tical, including in their vertex orderings and par-
ent indexes. If two condensed trees are identical,
then the non-condensed trees from which they are
constructed are always unordered isomorphic.
Each vertex v of a condensed tree T = cond(T )
has a parent index containing some number of en-
tries corresponding to a set of vertices in non-
condensed tree T . We will designate that set as
orig(v), a subset of the vertices in T . Given a
condensed tree vertex v and its parent p, if the size
of orig(p) is larger than one, then the vertices in v
may have different parents in T . We can interpret
the integers in the parent index of each condensed
tree vertex as indicating which parent each mem-
ber of orig(v) has.
In this way, given T = cond(T ), there is a
one-to-one mapping from the vertices of T to a
pair ?v, i? consisting of some vertex in T and an
index to an entry in its parent index. If some
vertex v in T maps to ?v, i?, then all the chil-
dren of v, c ? children(v) map to pairs ?c, j?
such that parent(c) = v and the jth entry in
parentIndex(c) is i. We can use this to define
parent-child operations over condensed trees that
perfectly match parent-child operations in non-
condensed ones.
We will define a skeleton tree as a con-
densed tree stripped of its parent indices, and
denote it as skel(T). Note that for any non-
condensed tree T and any non-condensed sub-
tree S ? T , skel(cond(T )) will always contain
skel(cond(S)) as an ordered subtree, including
in cases like Figure 5, as shown in Figure 6.
4.3 Alignment
An alignment of a condensed subtree S with a
condensed tree T has two parts:
815
(a) (b) (c)
Figure 6: The trees in Figure 5 transformed into their condensed equivalents, with their parent arrays.
Note that 6(c) is visibly an ordered subtree of both 6(a) and 6(b) if you ignore the parent arrays.
a. Skeleton Alignment:
An injection M : VS ? VT from the vertices
of S to the vertices of T.
b. Index Alignment:
For each vertex vS ? VS, a bipartite map-
ping from the vertices in orig(vS) to the ver-
tices in orig(M(vS)).
The first part is an alignment of skel(S) with
skel(T). Given an alignment from the root of S
to some vertex in T, this can be performed in time
proportionate, in the worst case, to the number of
vertices in skel(T). If all the parent indices of the
aligned vertices in the subtree and supertree have
only one index in them, then the index alignment
is trivial and the alignment of S to T is complete.
In other cases, index alignment is non-
trival. The method here draws on the proce-
dure for unordered subtree alignment proposed by
Kilpela?inen (1992). In the worst case, it resolves
to the same algorithm, but can perform better on
the average because of the structure of condensed
trees.
Alignment proceeds from the bottom-up, start-
ing with the leaves of S. If vertex s is a leaf of
S and is aligned to some vertex t in T, then we
initially assume any member of orig(s) can map
to any member of orig(t). We then proceed up-
wards in S, checking each vertex s in S to find a
mapping from orig(s) to orig(t) such that if some
s ? orig(s) can be mapped to some t ? orig(t),
then the children of s can be mapped to children
of t.
Once we reach the root of S, we proceed back
downwards, removing those mappings from each
orig(s) to its corresponding orig(t) that are im-
possible because their parents do not align.
The remaining index alignments must still be
checked to verify that each one can form a part
of a one-to-one mapping from orig(s) to orig(t).
This is equivalent to finding a maximal bipar-
tite matching from orig(s) to orig(t) for each
possible alignment from orig(s) to orig(t). Bi-
partite matching is a problem with a number
of well-documented solutions. (Dijkstra (1959),
Lova?sz (1986), among others)
5 Algorithm
Having outlined condensed trees and how to align
them, we can build an algorithm for extracting all
frequent closed unordered subtrees from a tree-
bank of condensed trees, given a minimum fre-
quency threshold ?. Space restrictions preclude
a full formal description of the algorithm, but it
closely follows the general outline for closed tree
discovery schemes advanced by Chi et al (2005a):
1. Pass through the treebank collecting all the
subtrees that consist of a single vertex label
and all their locations.
2. Remove those that appear less than ? times.
3. Loop over each remaining subtree, aligning
it to each place it appears in the treebank
4. Collect all the possible extensions, creating a
new list of two vertex subtrees and all their
locations.
5. Use the extensions to the left of the rightmost
vertex in each alignment to check if the sub-
tree is closed to the left, and reject it if it is
not.
6. Use the extensions to the right of the right-
most vertex to check if the subtree is closed
to the right, and output it if it is.
816
7. Retain the extensions to the right of the right-
most vertex and their locations if those exten-
sions appear at least ? times.
8. Repeat for those subtrees.
6 Implementation and Performance
The Varro toolkit implements condensed trees and
the algorithm described above in Python 3.1 and
has been applied to treebanks as large as several
hundred thousand sentences. The software and
source code is available from sourceforge.net3 and
includes a small treebank of parsed Latin texts
provided by the Perseus Digital Library. (Bam-
man and Crane, 2007)
The worst case memory performance of this al-
gorithm is O(nm) where n is the number of ver-
tices in the treebank and m is the largest frequent
subtree found in it. However, only the most patho-
logically structured treebank could come close to
this ceiling, and in practice, the current implemen-
tation has so far never used as much twice the
memory required to store the original treebank.
The runtime performance is, as described in
Section 3.2, proportionate to the size of the out-
put. However, aligning each occurrence of each
subtree adds an additional factor. Given a con-
densed subtree S and its condensed supertree
T containing size(T) vertices, and one already
aligned vertex, the worst case alignment time is
O(size(T)2.5), but only a highly pathological tree
structure would approach this. The best case
alignment time is O(size(S)). Therefore, it al-
ways takes more time to align larger subtrees, and
since larger subtrees are less frequent than smaller
ones, setting lower minimum frequency thresh-
olds increases the average time required to process
a subtree.
Processing even the small Alpino Treebank
produces very large numbers of frequent closed
subtrees. After removing punctuation and the to-
kens themselves, leaving just parts-of-speech and
consituency labels - the Alpino treebank?s 7137
sentences are reduced to 206,520 vertices. Within
this small set, Varro took 1252 seconds to find
7307 frequent closed subtrees that appear at least
100 times. This is both considerably more sub-
3http://varro.sourceforge.net/
trees than reported by Martens (2009b) on the
same data and considerably more time.
Speed and memory performance are the major
practical issues in this line of research. Choos-
ing to design Varro with memory footprint mini-
mization in mind is a source of some performance
bottlenecks. Using Python also takes a heavy toll
on speed and a C++ implementation is planned.
The fast alignment-free closure checking scheme
in Martens (2009b) can also be implemented us-
ing condensed trees. On small treebanks this will
improve speed without loss of precision, but has
limited applicability to large treebanks.
7 Conclusions
The trade-off between memory usage, run-time
and completeness for this kind of algorithm is
punitive. The user must balance very long run-
times against excessive memory usage if they
want to accurately count all frequent unordered
induced subtrees. The Varro toolkit is designed to
make it possible to choose what tradeoffs to make.
Since any subtree can be extended and checked for
closure independently of other subtrees, Varro can
easily implement heuristics designed to further re-
duce the number of subtrees extracted. We believe
the future of this line of research lies in large part
in that direction and hope that public release of
Varro will aid in its development.
We have also discovered that there is a very
strong relationship between the concision and
consistency of linguistic formalisms and Varro?s
performance. We restructured the Alpino data
by promoting the head of each constituent, cre-
ating dependency-style trees along the lines de-
scribed by Tesnie`re (1959) and Mel?c?uk (1988).
This reduced the number of subtrees found by
50%-60% and reduced run-times consistently by
60%-70% across a range of minimum frequency
thresholds and treebank sizes. As a general rule,
increasing the degree of linguistic abstraction in-
creases the number of frequent subtrees, and con-
sequently slows Varro down dramatically. Iden-
tifying linguistic formalisms that lend themselves
to efficient and productive subtree discovery is an-
other significant direction for this research, and
one with immediate impact on other areas in lin-
guistics.
817
References
Agrawal, Rakesh, Tomasz Imielinski and Arun Swami.
1993. Mining association rules between sets of
items in large databases. Proceedings of the 1993
ACM SIGMOD International Conference on Man-
agement of Data, pp. 207?216.
Asai, Tatsuya, Kenji Abe, Shinji Kawasoe, Hiroki
Arimura, Hiroshi Sakamoto and Setsuo Arikawa.
2002. Efficient substructure discovery from large
semi-structured data. Proceedings of the Second
SIAM International Conference on Data Mining,
158?174.
Bamman, David and Gregory Crane. 2007.
The Latin Dependency Treebank in a Cultural
Heritage Digital Library. Proceedings of the
Workshop on Language Technology for Cul-
tural Heritage Data, LaTeCH 2007: pp. 33?40.
http://nlp.perseus.tufts.edu/syntax/treebank/
Boulicaut, J.-F. and A. Bykowski. 2000. Frequent
closures as a concise representation for binary data
mining. Knowledge discovery and data mining:
current issues and new applications, PAKDD 2000:
pp. 62?73.
Chi, Yun, Richard R. Muntz, Siegfried Nijssen and
Joost N. Kok. 2004. Frequent Subtree Mining - An
Overview. Fundamenta Informaticae, 66(1-2):161?
198.
Chi, Yun, Yi Xia, Yirong Yang and Richard R. Muntz.
2005a. Mining Closed and Maximal Frequent
Subtrees from Databases of Labeled Rooted Trees.
IEEE Transactions on Knowledge and Data Engi-
neering, 17(2):190?202.
Chi, Yun, Yi Xia, Yirong Yang and Richard R. Muntz.
2005b . Canonical forms for labelled trees and their
applications in frequent subtree mining. Knowledge
and Information Systems, 8(2):203?234.
Dijkstra, E. W. 1959. A note on two problems in
connexion with graphs. Numerische Mathematik
1:269?271.
Firth, J.R. 1957. Papers in Linguistics. London: OUP.
Harris, Roy and Talbot J. Taylor. 1989/1997. Varro on
Linguistic Regularity. In Harris and Taylor, Land-
marks in Linguistic Thought I: The Western Tra-
dition from Socrates to Saussure. 2nd ed. London:
Routledge. pp. 47-59.
Kilpela?inen, Pekka. 1992. Tree Matching Problems
with Applications to Structured Text Databases.
PhD dissertation. Univ. Helsinki, Dept. of Computer
Science.
Knight, Kevin. 2007. Capturing practical natural
language transformations. Machine Translation,
21:121?133.
Knight, Kevin and Graehl, Jonathan. 2005. An
Overview of Probabilistic Tree Transducers for Nat-
ural Language Processing. Proceedings of the 6th
CICLing, 1?24.
Koehn, Philipp. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of the
10th Machine Translation Summit, 79?86.
Lova?sz, La?szlo? and M.D. Plummer. 1986. Matching
Theory. Amsterdam: Elsevier Science.
Luccio, Fabrizio, Antonio Enriquez, Pablo Rieumont
and Linda Pagli. 2001. Exact Rooted Subtree
Matching in Sublinear Time. Universita` Di Pisa
Technical Report TR-01-14.
Moschitti, Alessandro. Making tree kernels practical
for natural language learning. Proceedings of the
11th Conference of the European Association for
Computational Linguistics (EACL 2006), 113?120.
Mel?c?uk, Igor A. 1988. Dependency syntax: Theory
and practice. Albany, NY: SUNY Press.
Martens, Scott. 2009a. Frequent Structure Discovery
in Treebanks. Proceedings of the 19th Computa-
tional Linguistics in the Netherlands (CLIN 19).
Martens, Scott 2009b. Quantitative analysis of
treebanks using frequent subtree mining methods.
Proceedings of the 2009 Workshop on Graph-
based Methods for Natural Language Processing
(TextGraphs-4), 84?92.
Rohde, Douglas. 2001. Tgrep2 User Manual.
http://tedlab.mit.edu/?dr/Tgrep2
Sinclair, John. 1991. Corpus, Concordance, Colloca-
tion. Oxford: OUP.
Tesnie`re, Lucien. 1959. ?Ele?ments de syntaxe struc-
turale. Paris: ?Editions Klincksieck.
van Noord, Gertjan. 2006. At last parsing is now
operational. Verbum Ex Machina. Actes de la
13e confe?rence sur le traitement automatique des
langues naturelles (TALN6), 20?42.
Mohammed J. Zaki. 2002. Efficiently mining fre-
quent trees in a forest. Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, 1021?1035.
818
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 85?88,
Beijing, August 2010
An Efficient, Generic Approach to Extracting Multi-Word Expressions
from Dependency Trees
Scott Martens and Vincent Vandeghinste
Centrum voor Computerlingu??stiek
Katholieke Universiteit Leuven
scott@ccl.kuleuven.be & vincent@ccl.kuleuven.be
Abstract
The Varro toolkit offers an intuitive mech-
anism for extracting syntactically mo-
tivated multi-word expressions (MWEs)
from dependency treebanks by looking for
recurring connected subtrees instead of
subsequences in strings. This approach
can find MWEs that are in varying orders
and have words inserted into their compo-
nents. This paper also proposes descrip-
tion length gain as a statistical correlation
measure well-suited to tree structures.
1 Introduction
Automatic MWE extraction techniques operate
by using either statistical correlation tests on the
distributions of words in corpora, syntactic pat-
tern matching techniques, or by using hypothe-
ses about the semantic non-compositionality of
MWEs. This paper proposes a purely statistical
technique for MWE extraction that incorporates
syntactic considerations by operating entirely on
dependency treebanks. On the whole, dependency
trees have one node for each word in the sentence,
although most dependency schemes vary from this
to some extent in practice. See Figure 1 for an
example dependency tree produced automatically
by the Stanford parser from the English language
data in the Europarl corpus. (Marneffe, 2008;
Koehn, 2005)
Identifying MWEs with subtrees in dependency
trees is not a new idea. It is close to the formal def-
inition offered in Mel?c?uk (1998), and is applied
computationally in Debusmann (2004) However,
using dependency treebanks to automatically ex-
tract MWEs is fairly new and few MWE extrac-
Figure 1. A dependency tree of the sentence
?The Minutes of yesterday?s sitting have been dis-
tributed.?
tion projects to date take advantage of dependency
information directly. There are a number of rea-
sons why this is the case:
? String-based algorithms are not readily ap-
plicable to trees.
? Tree structures yield a potentially combina-
torial number of candidate MWEs, a prob-
lem shared with methods that look for strings
with gaps.
? Statistical techniques used in MWE extrac-
tion, like pointwise mutual information, are
two-variable tests that are not easy to apply
to larger sets of words.
The tool and statistical procedures used in this
research are not language dependent and can op-
erate on MWE of any size, producing depen-
85
(a) ?The Minutes (...)
have been distributed?
(b) ?(...) Minutes of
(...) distributed.?
Figure 2. Two induced subtrees of the dependency
tree in Figure 1. Note that both correspond to dis-
continuous phrases in the original sentence.
dency pairs, short phrases of any syntactic cate-
gory, lengthy formulas and idioms. There are no
underlying linguistic assumptions in this method-
ology except that a MWE must consist of words
that have a fixed set of dependency links in a
treebank. Even word order and distance between
words is not directly assumed to be significant.
The input, however, requires substantial linguis-
tic pre-processing ? particularly, the identification
of at least some of the dependency relations in
the corpora used. Retrieving MWEs that contain
abstract categories, like information about the ar-
guments of verbs or part-of-speech information
for unincluded elements, requires using treebanks
that contain that information, rather than purely
lexical dependency trees.
2 Varro Toolkit for Frequent Subtree
Discovery
The Varro toolkit is an open-source application for
efficiently extracting frequent closed unordered
induced subtrees from treebanks with labeled
nodes and edges. It is publicly available under an
open source license.1 For a fuller description of
Varro, including the algorithm and data structures
used and a formal definition of frequent closed un-
ordered induced subtrees, see Martens (2010).
Given some tree like the one in Figure 1, an in-
duced subtree is a connected subset of its nodes
and the edges that connect them, as shown in
Figure 2. Subtrees do not necessarily represent
1http://varro.sourceforge.net/
fixed sequences of words in the original text,
they include syntactically motivated discontinu-
ous phrases. This dramatically reduces the num-
ber of candidate discontinuous MWEs when com-
pared to string methods. An unordered induced
subtree is a subtree where the words may appear
with different word orders, but the subtree is still
identified as the same if the dependency structure
is the same. A frequent closed subtree is a sub-
tree of a treebank that appears more than some
fixed number of times and where there is no sub-
tree that contains it and appears the same number
of times. Finding only closed subtrees reduces the
combinatorial explosion of possible subtrees, and
ensures that each candidate MWE includes all the
words the that co-occur with it every time it ap-
pears.
3 Preprocessing and Extracting Subtrees
The English language portion of the Europarl
Corpus, version 3 was parsed using the Stanford
parser, which produces both a constituentcy parse
and a dependency tree as its output.2 The depen-
dency information for each sentence was trans-
formed into the XML input format used by Varro.
The result is a treebank of 1.4 million individual
parse trees, each representing a sentence, and a to-
tal of 36 million nodes.
In order to test the suitability of Varro for large
treebanks and intensive extractions, all recurring
closed subtrees that appear at least twice were ex-
tracted. This took a total of 129,312.27 seconds
(just over 34 hours), producing 9,976,355 frequent
subtrees, of which 9,909,269 contain more than
one word and are therefore candidate MWEs.
A fragment of the Varro output can be seen in
Figure 3. The nodes of the subtrees returned are
not in a grammatical surface order. However, the
original source order can be recovered by using
the locations where each subtree appears to find
the order in the treebank. Doing so for the tree
in Figure 3 shows what kinds of MWEs this ap-
proach can extract from treebanks. The under-
lined words in the following sentences are the
ones included in the subtree in Figure 3:
2This portion of the work was done by our colleagues
Jo?rg Tiedemann and Gideon Kotze? at RU Groningen.
86
Figure 3. An example of a found subtree and can-
didate MWE. This subtree appears in 2581 unique
locations in the treebank, and only the locations
of the first few places in the treebank where it ap-
pears are reproduced here, but all 2581 are in the
Varro output data.
The vote will take place tomorrow at 9 a.m.
The vote will take place today at noon.
The vote will take place tomorrow, Wednesday
at 11:30 a.m.
4 Statistical Methods for Evaluating
Subtrees as MWEs
To evaluate the quality of subtrees as MWEs,
we propose to use a simplified form of de-
scription length gain (DLG), a metric derived
from algorithmic information theory and Mini-
mum Description Length methods (MDL). (Ris-
sanen, 1978; Gru?nwald, 2005) Given a quantity of
data of any kind that can be stored as a digital in-
formation in a computer, and some process which
transforms the data in a way that can be reversed,
DLG is the measure of how the space required to
store that data changes when it is transformed.
To calculate DLG, one must first decide how to
encode the trees in the treebank. It is not neces-
sary to actually encode the treebank in any par-
ticular format. All that is necessary is to be able
to calculate how many bits the treebank would re-
quire to encode it.
Space prevents the full description of the en-
coding mechanism used or the way DLG is cal-
culated. The encoding mechanism is largely the
same as the one described in Luccio et al (2001)
Converting the trees to strings makes it possible to
calculate the encoding size by calculating the en-
tropy of the treebank in that encoding using clas-
sical information theoric methods.
In effect, the procedure for calculating DLG is
to calculate the entropy of the whole treebank,
given the encoding method chosen, and then to
recalculate its entropy given some subtree which
is removed from the treebank and replaced with a
symbol that acts as an abbreviation. That subtree
is then be added back to the treebank once as part
of a look-up table. These methods are largely the
same as those used by common data compression
software.
DLG is the difference between these two en-
tropy measures.3
Because of the sensitivity of DLG to low fre-
quencies, it can be viewed as a kind of non-
parametric significance test. Any frequent struc-
ture that cannot be used to compress the treebank
has a negative DLG and is not frequent enough or
large enough to be considered significant.
Varro reports several statistics related to DLG
for each extracted subtree, as shown in Figure 3:
? Unique appearances (reported by the root-
Count attribute) is the number of times the
extracted subtree appears with a different
root node.
? Entropy is the entropy of the extracted sub-
tree, given the encoding scheme that Varro
uses to calculate DLG.
? Algorithmic mutual information (AMI) (re-
ported with the mi attribute) is the DLG of
the extracted subtree divided by its number
of unique appearances in the treebank.
? Compression is the AMI divided by the en-
tropy.
AMI is comparable to pointwise mutual infor-
mation (PMI) in that both are measures of redun-
dant bits, while compression is comparable to nor-
malized mutual information metrics.
3This is a very simplified picture of MDL and DLG met-
rics.
87
5 Results and Conclusions
We used the metrics described above to sort the
nearly 10 million frequent subtrees of the parsed
English Europarl corpus. We found that:
? Compression and AMI metrics strongly fa-
vor very large subtrees that represent highly
formulaic language.
? DLG alone finds smaller, high frequency ex-
pressions more like MWEs favoured by ter-
minologists and collocation analysis.
For example, the highest DLG subtree matches
the phrase ?the European Union?. This is not
unexpected given the source of the data and con-
stitutes a very positive result. Among the nearly
10 million candidate MWEs extracted, it also
places near the top discontinuous phrases like
?... am speaking ... in my ... capacity as ...?.
Using both compression ratio and AMI, the
same subtree appears first. It is present 26 times
in the treebank, with a compression score of 0.894
and an AMI of 386.92 bits. It corresponds to the
underlined words in the sentence below:
The next item is the recommendation for
second reading (A4-0245/99), on behalf of
the Committee on Transport and Tourism, on
the common position adopted by the Council
(13651/3/98 - C4-0037/99-96/0182 (COD) with
a view to adopting a Council Directive on the
charging of heavy goods vehicles for the use of
certain infrastructures.
This is precisely the kind of formulaic speech,
with various gaps to fill in, which is of great inter-
est for sub-sentential translation memory systems.
(Gotti et al, 2005; Vandeghinste and Martens,
2010)
We believe this kind of strategy can substan-
tially enhance MWE extraction techniques. It in-
tegrates syntax into MWE extraction in an intu-
itive way. Furthermore, description length gain
offers a unified statistical account of an MWE as
a linguistically motivated structure that can com-
press relevant corpus data. It is similar to the types
of statistical tests already used, but is also non-
parametric and suitable for the study of arbitrary
MWEs, not just two-word MWEs or phrases that
occur without gaps.
6 Acknowledgements
This research is supported by the AMASS++
Project,4 directly funded by the Institute for the
Promotion of Innovation by Science and Technol-
ogy in Flanders (IWT) (SBO IWT 060051) and by
the PaCo-MT project (STE-07007).
References
Debusmann, Ralph. 2004. Multiword expressions as
dependency subgraphs. Proceedings of the 2004
ACL Workshop on Multiword Expressions, pp. 56-
63.
Gotti, Fabrizio, Philippe Langlais, Eliott Macklovitch,
Didier Bourigault, Benoit Robichaud and Claude
Coulombe. 2005. 3GTM: A third-generation trans-
lation memory. Proceedings of the 3rd Computa-
tional Linguistics in the North-East Workshop, pp.
8?15.
Gru?nwald, Peter. 2005. A tutorial introduction to
the minimum description length principle. In: Ad-
vances in Minimum Description Length: Theory
and Applications, (Peter Gru?nwald, In Jae Myung,
Mark Pitt, eds.), MIT Press, pp. 23?81.
Koehn, Philipp. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of the
10th MT Summit, pp. 79?86.
Luccio, Fabrizio, Antonio Enriquez, Pablo Rieumont
and Linda Pagli. 2001. Exact Rooted Subtree
Matching in Sublinear Time. Universita` di Pisa
Technical Report TR-01-14.
de Marneffe, Marie-Catherine and Christopher D.
Manning. 2008. The Stanford typed dependencies
representation. Proceedings of the 2008 CoLing
Workshop on Cross-framework and Cross-domain
Parser Evaluation, pp. 1?8.
Martens, Scott. 2010. Varro: An Algorithm and
Toolkit for Regular Structure Discovery in Tree-
banks. Proceedings of the 2010 Int?l Conf. on Com-
putational Linguistics (CoLing), in press.
Mel?c?uk, Igor. 1998. Collocations and Lexical Func-
tions. In: Phraseology. Theory, Analysis, and Ap-
plications, (Anthony Cowie ed.), pp. 23?53.
Rissanen, Jorma. 1978. Modeling by shortest data
description. Automatica, vol. 14, pp. 465?471.
Vandeghinste, Vincent and Scott Martens. 2010.
Bottom-up transfer in Example-based Machine
Translation. Proceedings of the 2010 Conf. of the
European Association for Machine Translation, in
press.
4http://www.cs.kuleuven.be/?liir/projects/amass/
88
