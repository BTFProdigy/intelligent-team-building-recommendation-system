Analysis of an Iterative Algorithm for
Term-Based Ontology Alignment
Shisanu Tongchim, Canasai Kruengkrai, Virach Sornlertlamvanich,
Prapass Srichaivattana, and Hitoshi Isahara
Thai Computational Linguistics Laboratory,
National Institute of Information and Communications Technology,
112,Paholyothin Road, Klong 1, Klong Luang, Pathumthani 12120, Thailand
{shisanu, canasai, virach, prapass}@tcllab.org, isahara@nict.go.jp
Abstract. This paper analyzes the results of automatic concept align-
ment between two ontologies. We use an iterative algorithm to perform
concept alignment. The algorithm uses the similarity of shared terms in
order to find the most appropriate target concept for a particular source
concept. The results show that the proposed algorithm not only finds
the relation between the target concepts and the source concepts, but
the algorithm also shows some flaws in the ontologies. These results can
be used to improve the correctness of the ontologies.
1 Introduction
To date, several linguistic ontologies in different languages have been developed
independently. The integration of these existing ontologies is useful for many
applications. Aligning concepts between ontologies is often done by humans,
which is an expensive and time-consuming process. This motivates us to find an
automatic method to perform such task. However, the hierarchical structures of
ontologies are quite different. The structural inconsistency is a common problem
[1]. Developing a practical algorithm that is able to deal with this problem is a
challenging issue.
The objective of this research is to investigate an automated technique for
ontology alignment. The proposed algorithm links concepts between two ontolo-
gies, namely the MMT semantic hierarchy and the EDR concept dictionary. The
algorithm finds the most appropriate target concept for a given source concept
in the top-down manner. The experimental results show that the algorithm can
find reasonable concept mapping between these ontologies. Moreover, the results
also suggest that this algorithm is able to detect flaws and inconsistency in the
ontologies. These results can be used for developing and improving the ontologies
by lexicographers.
The rest of this paper is organized as follows: Section 2 discusses related
work. Section 3 provides the description of the proposed algorithm. Section
4 presents experimental results and discussion. Finally, Section 5 concludes
our work.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 346?356, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 347
2 Related Work
Daude? et al [2] used a relaxation labeling algorithm ? a constraint satisfaction
algorithm ? to map the verbal, adjectival and adverbial parts between two dif-
ferent WordNet versions, namely WordNet 1.5 and WordNet 1.6. The structural
constraints are used by the algorithm to adjust the weights for the connections
between WN1.5 and WN1.6. Later, some non-structural constraints are included
in order to improve the performance [3].
Asanoma [4] presented an alignment technique between the noun part of
WordNet and Goi-Taikei ?s Ontology. The proposed technique utilizes sets of
Japanese and/or English words and semantic classes from dictionaries in an MT
system, namely ALT-J/E.
Chen and Fung [5] proposed an automatic technique to associate the English
FrameNet lexical entries to the appropriate Chinese word senses. Each FrameNet
lexical entry is linked to Chinese word senses of a Chinese ontology database
called HowNet. In the beginning, each FrameNet lexical entry is associated with
Chinese word senses whose part-of-speech is the same and Chinese word/phrase
is one of the translations. In the second stage of the algorithm, some links are
pruned out by analyzing contextual lexical entries from the same semantic frame.
In the last stage, some pruned links are recovered if their scores are greater than
the calculated threshold value.
Ngai et al [6] also conducted some experiments by using HowNet. They
presented a method for performing alignment between HowNet and WordNet.
They used a word-vector based method which was adopted from techniques
used in machine translation and information retrieval. Recently, Yeh et al [7]
constructed a bilingual ontology by aligning Chinese words in HowNet with
corresponding synsets defined in WordNet. Their alignment approach utilized
the co-occurrence of words in a parallel bilingual corpus.
Khan and Hovy [8] presented an algorithm to combine an Arabic-English
dictionary with WordNet. Their algorithm also tries to find links from Arabic
words to WordNet first. Then, the algorithm prunes out some links by trying to
find a generalization concept.
Doan et al [9] proposed a three steps approach for mapping between ontologies
on the semantic web. The first step used machine learning techniques to determine
the joint distribution of any concept pair. Then, a user-supplied similarity function
is used to compute similarity of concept pairs based on the joint distribution from
the first step. In the final step, a relaxation labeling algorithm is used to find the
mapping configuration based on the similarity from the previous step.
3 Proposed Algorithm
In this section, we describe an approach for ontology alignment based on term
distribution. To alleviate the structural computation problem, we assume that
the considered ontology structure has only the hierarchical (or taxonomic) rela-
tion. One may simply think of this ontology structure as a general tree, where
each node of the tree is equivalent to a concept.
348 S. Tongchim et al
Given two ontologies called the source ontology Ts and the target ontology
Tt, our objective is to align all concepts (or semantic classes) between these
two ontologies. Each ontology consists of the concepts, denoted by C1, . . . , Ck. In
general, the concepts and their corresponding relations of each ontology can be
significantly different due to the theoretical background used in the construction
process. However, for the lexical ontologies such as the MMT semantic hierarchy
and the EDR concept dictionary, it is possible that the concepts may contain
shared members in terms of English words. Thus, we can match the concepts
between two ontologies using the similarity of the shared words.
In order to compute the similarity between two concepts, we must also con-
sider their related child concepts. Given a root concept Ci, if we flatten the
hierarchy starting from Ci, we obtain a nested cluster, whose largest cluster
dominates all sub-clusters. As a result, we can represent the nested cluster with
a feature vector ci = (w1, . . . , w
|V|
)T , where features are the set of unique En-
glish words V extracted from both ontologies, and wj is the number of the word
j occurring the nested cluster i. We note that a word can occur more than once,
since it may be placed in several concepts on the lexical ontology according to
its sense.
After concepts are represented with the feature vectors, the similarity be-
tween any two concepts can be easily computed. A variety of standard similarity
measures exists, such as the Dice coefficient, the Jaccard coefficient, and the co-
sine similarity [10]. In our work, we require a similarity measure that can reflect
the degree of the overlap between two concepts. Thus, the Jaccard coefficient is
suitable for our task. Recently, Strehl and Ghosh [11] have proposed a version
of the Jaccard coefficient called the extended Jaccard similarity that can work
with continuous or discrete non-negative features. Let ?xi? be the L2 norm of a
given vector xi. The extended Jaccard similarity can be calculated as follows:
JaccardSim(xi,xj) =
xTi xj
?xi?2 + ?xj?2 ? xTi xj
. (1)
We now describe an iterative algorithm for term-based ontology alignment.
As mentioned earlier, we formulate that the ontology structure is in the form of
the general tree. Our algorithm aligns the concepts on the source ontology Ts to
the concepts on the target ontology Tt by performing search and comparison in
the top-down manner.
Given a concept Ci ? Ts, the algorithm attempts to find the most appro-
priate concept B? ? Tt, which is located on an arbitrary level of the hierar-
chy. The algorithm starts by constructing the feature vectors for the current
root concept on the level l and its child concepts on the level l + 1. It then
calculates the similarity scores between a given source concept and candidate
target concepts. If the similarity scores of the child concepts are not greater
than the root concept, then the algorithm terminates. Otherwise, it selects a
child concept having the maximum score to be the new root concept, and it-
erates the same searching procedure. Algorithms 1 and 2 outline our ontology
alignment process.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 349
Algorithm 1. OntologyAlignment
input : The source ontology Ts and the target ontology Tt.
output : The set of the aligned concepts A.
begin
Set the starting level, l ? 0;
while Ts?l? ? Ts?max? do
Find all child concepts on this level, {Ci}ki=1 ? Ts?l?;
Flatten {Ci}ki=1 and build their corresponding feature vectors, {ci}ki=1;
For each ci, find the best matched concepts on Tt,
B ? FindBestMatched(ci);
A ? A ? {B, Ci};
Set l ? l + 1;
end
end
Algorithm 2. FindBestMatched(ci)
begin
Set the starting level, l ? 0;
BestConcept ? Tt(root concept);
repeat
stmp ? JaccardSim(ci, BestConcept);
if Tt?l? > Tt?max? then
return BestConcept;
Find all child concepts on this level, {B}hj=1 ? Tt?l?;
Flatten {Bj}hj=1 and build corresponding feature vectors, {bj}hi=1;
sj? ? argmaxjJaccardSim(ci, {bj}hj=1);
if sj? > stmp then
BestConcept ? Bj? ;
Set l ? l + 1;
until BestConcept does not change;
return BestConcept;
end
Figure 1 shows a simple example that describes how the algorithm works.
It begins with finding the most appropriate concept on Tt for the root concept
1 ? Ts. By flattening the hierarchy starting from given concepts (?1? on Ts,
and ?a?, ?a-b?, ?a-c? for Tt), we can represent them with the feature vectors and
measure their similarities. On the first iteration, the child concept ?a-c? obtains
the maximum score, so it becomes the new root concept. Since the algorithm
cannot find improvement on any child concepts in the second iteration, it stops
the loop and the target concept ?a-c? is aligned with the source concept ?1?. The
algorithm proceeds with the same steps by finding the most appropriate concepts
on Tt for the concepts ?1-1? and ?1-2?. It finally obtains the resulting concepts
?a-c-f? and ?a-c-g?, respectively.
350 S. Tongchim et al
Fig. 1. An example of finding the most appropriate concept on Tt for the root concept
1 ? Ts
4 Experiments and Evaluation
4.1 Data Sets
Two dictionaries are used in our experiments. The first one is the EDR Elec-
tronic Dictionary [12]. The second one is the electronic dictionary of Multilingual
Machine Translation (MMT) project [13].
The EDR Electronic Dictionary consists of lexical knowledge of Japanese
and English divided into several sub-dictionaries (e.g., the word dictionary, the
bilingual dictionary, the concept dictionary, and the co-occurrence dictionary)
and the EDR corpus. In the revised version (version 1.5), the Japanese word
dictionary contains 250,000 words, while the English word dictionary contains
190,000 words. The concept dictionary holds information on the 400,000 concepts
that are listed in the word dictionary. Each concept is marked with a unique
hexadecimal number.
For the MMT dictionary, we use the Thai-English Bilingual Dictionary that
contains around 60,000 lexical entries. The Thai-English Bilingual Dictionary
also contains semantic information about the case relations and the word con-
cepts. The word concepts are organized in a manner of semantic hierarchy. Each
word concept is a group of lexical entries classified and ordered in a hierarchical
level of meanings. The MMT semantic hierarchy is composed of 160 concepts.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 351
In our experiments, we used a portion of the MMT semantic hierarchy and the
EDR concept dictionary as the source and the target ontologies, respectively. We
considered the ?animal? concept as the root concepts and extracted its related con-
cepts. In the EDR concept dictionary, however, the relations among concepts are
very complex and organized in the form of the semantic network. Thus, we pruned
some links to transform the network to a tree structure. Starting from the ?animal?
concept, there are more than 200 sub-concepts (containing about 7,600 words) in
the EDR concept dictionary, and 14 sub-concepts (containing about 400 words) in
the MMT semantic hierarchy. It is important to note that these two ontologies are
considerably different in terms of the number of concepts and words.
4.2 Experimental Results
The proposed algorithm is used to find appropriate EDR concepts for each one of
14 MMT concepts. The results are shown in Table 1. From the table, there are 6 re-
lations (marked with the symbol ?*?) that aremanually classified as exact mapping.
This classification is done by inspecting the structures of both ontologies by hand.
If the definition of a given MMT concept appears in the EDR concept and the algo-
rithm seems to correctly match the most suitable EDR concept, this mapping will
be classified as exact mapping. The remaining 8 MMT concepts, e.g. ?cold-blood?
and ?amphibian?, are mapped to closely related EDR concepts, although they are
not considered to be exact mapping. The EDR concepts found by our algorithm for
these 8 MMT concepts are considered to be only the subset of the source concepts.
For example, the ?amphibian? concept of the MMT is mapped to the ?toad? concept
of the EDR. The analysis in the later section will explain why some MMT concepts
are mapped to specific sub-concepts.
Our algorithm works by flattening the hierarchy starting from the consid-
ered concept in order to construct a word list represented that concept. The
word lists are then compared to match the concepts. In practice, only a por-
tion of word list is intersected. Figure 2 illustrates what happens in general.
Note that the EDR concept dictionary is much larger than the MMT semantic
MMT
321
EDR
Fig. 2. A schematic of aligned concepts
352 S. Tongchim et al
Table 1. Results of aligned concepts between the MMT and the EDR
MMT concept EDR concept
vertebrate vertebrate ?
| ? warm-blood mammal
| | ? mammal mammal ?
| | ? bird bird ?
|
| ? cold-blood reptile
| ? fish fish ?
| ? amphibian toad
| ? reptile reptile ?
| ? snake snake ?
invertebrate squid
| ? worm leech
| ? insect hornet
| ? shellfish crab
| ? other sea creature squid
? These concepts are manually classified as exact mapping.
hierarchy. Thus, it always has EDR words that are not matched with any MMT
words. These words are located in the section 3 of the figure 2. The words
in the section 1 are more important since they affects the performance of the
algorithm. We assume that the EDR is much larger than the MMT. There-
fore, most MMT words should be found in the EDR. The MMT words that
cannot found any related EDR words may be results of incorrect spellings, spe-
cific words (i.e. only found in Thai language). In case of incorrect spelling and
other similar problems, the results of the algorithm can be used to improve the
MMT ontology.
By analyzing the results, we can classify the MMT words that cannot find
any associated EDR words into 4 categories.
1. Incorrect spelling or wrong grammar : Some English words in the MMT
semantic hierarchy are simply incorrect spelling, or they are written with
wrong grammar. For example, one description of a tiger species is written as
?KIND A TIGER?. Actually, this instance should be ?KIND OF A TIGER?.
The algorithm can be used to find words that possible have such a problem.
Then, the words can be corrected by lexicographers.
2. Inconsistency : The English translation of Thai words in the MMT semantic
hierarchy was performed by several lexicographers. When dealing with Thai
words that do not have exact English words, lexicographers usually enter
phrases as descriptions of these words. Since there is no standard of writing
the descriptions, these is incompatibility between descriptions that explain
the same concept. For example, the following phrases are used to describe
fishes that their English names are not known.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 353
? Species of fish
? A kind of fish
? Species of fresh water fish
3. Thai specific words : The words that we used in our experiments are animals.
Several animals are region specific species. Therefore, they may not have any
associated English words. In this case, some words are translated by using
short phrases as English descriptions of these Thai words. Another way to
translate these words is to use scientific names of species.
The problems mentioned earlier make it more difficult to match concepts by
the algorithm. However, we can use the algorithm to identify where the problems
occur. Then, we can use these results to improve the MMT ontology.
The proposed algorithm works in the top-down manner. That is, the algo-
rithm attempts to find the most appropriate concept from the top level, and
it will move down if the lower concepts yield better scores. In order to analyze
the algorithm, we trace the algorithm during moving through the EDR concepts.
The first example of the bird concept alignment is shown in Table 2. The concept
alignment of this example is considered to be exact mapping. The first column
indicates the level of EDR concepts. The second and third columns indicate the
number of MMT words and the number of EDR words after flattening respec-
tively. The fourth column shows the number of intersected words between the
MMT and the EDR. From the table, the algorithm moves through the EDR con-
cepts in order to find the most specific concept that still maintains shared terms.
This example shows that the algorithm passes through 3 concepts until it stops
at the ?bird? concept of the EDR. At the final step, the algorithm decides to trade
few shared terms for a more specific EDR concept. Note that the MMT is not
completely cleaned. When moving down to the EDR bird concept, three shared
terms are lost. Our analysis shows that these terms are bat species. They are
all wrongly classified to the MMT bird concept by some lexicographers. Thus,
these shared terms will not intersect with any words in the EDR bird concept
when the algorithm proceeds to the lower step. This result suggests that our
algorithm is quite robust. The algorithm still finds an appropriate concept even
the MMT ontology has some flaws.
Another analysis of exact mapping is shown in Table 3. The algorithm moves
through 4 concepts until matching the EDR snake concept with the MMT snake
concept. In this example, the number of members in the MMT snake concept is
quite small. However, the number of shared terms is sufficient to correctly locate
the EDR snake concept.
Table 2. Concept alignment for the ?bird? concept
Level MMT words EDR words Intersected words
1 67 2112 26
2 67 1288 26
3 67 373 23
354 S. Tongchim et al
Table 3. Concept alignment for the ?snake? concept
Level MMT words EDR words Intersected words
1 17 2112 8
2 17 1288 8
3 17 71 8
4 17 26 8
The third example shown in Table 4 illustrates the case that is considered to
be subset mapping. That is, the EDR concept selected by the algorithm is sub-
concept of the MMT concept. This case happens several times since the EDR
is more fine-grained than the MMT. If the members of MMT concept do not
cover enough, the algorithm tends to return only sub-concepts. From the table,
the MMT amphibian concept covers only toad and frog species (3 members).
Thus, the algorithm moves down to a very specific concept, namely the EDR
toad concept. Another example of subset mapping is shown in Table 5. This
example also shows that the members of MMT concept do not cover enough.
These results can be used to improve the MMT ontology. If the MMT con-
cepts are extended enough, we expect that the correctness of alignment should
be improved.
Table 4. Concept alignment for the ?amphibian? concept
Level MMT words EDR words Intersected words
1 3 2112 2
2 3 1288 2
3 3 23 2
4 3 16 2
5 3 2 1
Table 5. Concept alignment for the ?other sea creature? concept
Level MMT words EDR words Intersected words
1 17 2112 5
2 17 746 5
3 17 78 3
4 17 3 2
5 Conclusion
We have proposed an iterative algorithm to deal with the problem of automated
ontology alignment. This algorithm works in the top-down manner by using the
similarity of the terms from each ontology. We use two dictionaries in our exper-
iment, namely the MMT semantic hierarchy and the EDR concept dictionary.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 355
The results show that the algorithm can find reasonable EDR concepts for given
MMT concepts. Moreover, the results also suggest that the algorithm can be
used as a tool to locate flaws in the MMT ontology. These results can be used
to improve the ontology.
There are several possible extensions to this study. The first one is to examine
this algorithm with larger data sets or other ontologies. The second one is to
improve and correct the ontologies by using the results from the algorithm.
Then, we plan to apply this algorithm to the corrected ontologies, and examine
the correctness of the results. The third one is to use structural information of
ontologies in order to improve the correctness.
References
1. Ide, N. and Ve?ronis, J.: Machine Readable Dictionaries: What have we learned,
where do we go?. Proceedings of the International Workshop on the Future of
Lexical Research, Beijing, China (1994) 137?146
2. Daude?, J., Padro?, L. and Rigau, G.: Mapping WordNets Using Structural Informa-
tion. Proceedings of the 38th Annual Meeting of the Association for Computational
Linguistics, Hong Kong, (2000)
3. Daude?, J., Padro?, L. and Rigau, G.: A Complete WN1.5 to WN1.6 Mapping.
Proceedings of NAACL Workshop ?WordNet and Other Lexical Resources: Appli-
cations, Extensions and Customizations?, Pittsburg, PA, United States, (2001)
4. Asanoma, N.: Alignment of Ontologies: WordNet and Goi-Taikei. Proceedings of
NAACL Workshop ?WordNet and Other Lexical Resources: Applications, Exten-
sions and Customizations?, Pittsburg, PA, United States, (2001) 89?94
5. Chen, B. and Fung, P.: Automatic Construction of an English-Chinese Bilingual
FrameNet. Proceedings of Human Language Technology conference, Boston, MA
(2004) 29?32
6. Ngai, G., Carpuat , M. and Fung, P.: Identifying Concepts Across Languages: A
First Step towards a Corpus-based Approach to Automatic Ontology Alignment.
Proceedings of the 19th International Conference on Computational Linguistics,
Taipei, Taiwan (2002)
7. Yeh, J.-F., Wu, C.-H., Chen, M.-J. and Yu, L.-C.: Automated Alignment and
Extraction of a Bilingual Ontology for Cross-Language Domain-Specific Applica-
tions. International Journal of Computational Linguistics and Chinese Language
Processing. 10 (2005) 35?52
8. Khan, L. and Hovy, E.: Improving the Precision of Lexicon-to-Ontology Alignment
Algorithms. Proceedings of AMTA/SIG-IL First Workshop on Interlinguas, San
Diego, CA (1997)
9. Doan, A., Madhavan, J., Domingos, P., and Halevy, A.: Learning to Map Between
Ontologies on the Semantic Web. Proceedings of the 11th international conference
on World Wide Web, ACM Press (2002) 662?673
10. Manning, C. D., and Schu?tze, H.: Foundations of Statistical Natural Language
Processing. MIT Press. Cambridge, MA (1999)
356 S. Tongchim et al
11. Strehl, A., Ghosh, J., and Mooney, R. J.: Impact of Similarity Measures on Web-
page Clustering. Proceedings of AAAI Workshop on AI for Web Search (2000)
58?64
12. Miyoshi, H., Sugiyama, K., Kobayashi, M. and Ogino, T.: An Overview of the EDR
Electronic Dictionary and the Current Status of Its Utilization. Proceedings of the
16th International Conference on Computational Linguistics (1996) 1090?1093
13. CICC: Thai Basic Dictionary. Center of the International Cooperation for Com-
puterization, Technical Report 6-CICC-MT55 (1995)
A Practical Text Summarizer by Paragraph Extraction for Thai
Chuleerat Jaruskulchai and Canasai Kruengkrai
Intelligent Information Retrieval and Database Laboratory
Department of Computer Science, Faculty of Science
Kasetsart University, Bangkok, Thailand
fscichj,g4364115@ku.ac.th
Abstract
In this paper, we propose a practical ap-
proach for extracting the most relevant
paragraphs from the original document
to form a summary for Thai text. The
idea of our approach is to exploit both
the local and global properties of para-
graphs. The local property can be consid-
ered as clusters of significant words within
each paragraph, while the global property
can be though of as relations of all para-
graphs in a document. These two proper-
ties are combined for ranking and extract-
ing summaries. Experimental results on
real-world data sets are encouraging.
1 Introduction
The growth of electronic texts is becoming increas-
ingly common. Newspapers or magazines tend to
be available on the World-Wide Web. Summarizing
these texts can help users access to the information
content more quickly. However, doing this task by
humans is costly and time-consuming. Automatic
text summarization is a solution for dealing with this
problem.
Automatic text summarization can be broadly
classified into two approaches: abstraction and ex-
traction. In contrast to abstraction that requires using
heavy machinery from natural language processing
(NLP), including grammars and lexicons for pars-
ing and generation (Hahn and Mani, 2000), extrac-
tion can be easily viewed as the process of selecting
relevant excerpts (sentences, paragraphs, etc.) from
the original document and concatenating them into a
shorter form. Thus, most of recent works in this re-
search area are based on extraction (Goldstein et al,
1999). Although one may argue that extraction ap-
proach makes the text hard to read due to the lack of
coherence, it also depends on the objective of sum-
marization. If we need to generate summaries that
can be used to indicative what topics are addressed
in the original document, and thus can be used to
alert the uses as the source content, i.e., the indica-
tive function (Mani et al, 1999), extraction approach
is capable of handling this kind of tasks.
There have been many researches on text sum-
marization problem. However, in Thai, we are in
the initial stage of developing mechanisms for au-
tomatically summarizing documents. It is a chal-
lenge to summarize these documents, since they are
extremely different from documents written in En-
glish. Similar to Chinese or Japanese, for the Thai
writing system, there are no boundaries between ad-
joining words, and also there are no explicit sen-
tences boundaries within the document. Fortunately,
there is the use of the paragraph structure in the
Thai writing system, which is indicated by inden-
tations and blank lines. Therefore, extracting text
spans from Thai documents at the paragraph level is
a more practical way.
In this paper, we propose a practical approach to
Thai text summarization by extracting the most rel-
evant paragraphs from the original document. Our
approach considers both the local and global prop-
erties of these paragraphs, which their meaning will
become clear later. We also present an efficient ap-
proach for solving Thai word segmentation problem,
which can enhance a basic word segmentation algo-
rithm yielding more useful output. We provide ex-
perimental evidence that our approach achieves ac-
ceptable performance. Furthermore, our approach
does not require the external knowledge other than
the document itself, and be able to summarize gen-
eral text documents.
The remainder of this paper is organized as fol-
lows. In Section 2, we review some related work
and contrast it with our work. Section 3 describes
the preprocessing for Thai text, particularly on word
segmentation. In Section 4, we present our approach
for extracting relevant paragraphs in detail, includ-
ing how to find clusters of significant words, how to
discover relations of paragraphs, and an algorithm
for combining these two approaches. Section 5 de-
scribes our experiments. Finally, we conclude in
Section 6 with some directions of future work.
2 Related Work
A comprehensive survey of text summarization ap-
proaches can be found in (Mani, 1999). We
briefly review here based on extraction approach.
Luhn (1959) proposed a simple but effective ap-
proach by using term frequencies and their related
positions to weight sentences that are extracted to
form a summary. Subsequent works have demon-
strated the success of Luhn?s approach (Buyukkok-
ten et al, 2001; Lam-Adesina and Jones, 2001;
Jaruskulchai et al, 2003). Edmunson (1969) pro-
posed the use of other features such as title words,
sentence locations, and bonus words to improve sen-
tence extraction. Goldstein et al (1999) presented
an extraction technique that assigns weighted scores
for both statistical and linguistic features in the sen-
tence. Recently, Salton et al (1999) have developed
a model for representing a document by using undi-
rected graphs. The basic idea is to consider vertices
as paragraphs and edges as the similarity between
two paragraphs. They suggested that the most im-
portant paragraphs should be linked to many other
paragraphs, which are likely to discuss topic covered
in those paragraphs.
Statistical learning approaches have also been
studied in text summarization problem. The first
known supervised learning algorithm was proposed
by Kupiec et al (1995). Their approach estimates
the probability that a sentence should be included
in a summary given its feature values based on the
independent assumption of Bayes? Rule. Other su-
pervised learning algorithms have already been in-
vestigated. Chuang and Yang (2000) studied several
algorithms for extracting sentence segments, such as
decision tree, naive Bayes classifier, and neural net-
work. They also used rhetorical relations for rep-
resenting features. One drawback of the supervised
learning algorithms is that they require an annotated
corpus to learn accurately. However, they may per-
form well for summarizing documents in a specific
domain.
This paper presents an approach for extracting the
most relevant paragraphs from the original docu-
ment to form a summary. The idea of our approach
is to exploit both the local and global properties of
paragraphs. The local property can be considered as
clusters of significant words within each paragraph,
while the global property can be though of as re-
lations of all paragraphs in the document. These
two properties can be combined and tuned to pro-
duce a single measure reflecting the informativeness
of each paragraph. Finally, we can apply this combi-
nation measure for ranking and extracting the most
relevant paragraphs.
3 Preprocessing for Thai Text
The first step for working with Thai text is to tok-
enize a given text into meaningful words, since the
Thai writing system has no delimiters to indicate
word boundaries. Thai words are not delimited by
spaces. The spaces are only used to break the idea
or draw readers? attention. In order to determine
word boundaries, we employed the longest matching
algorithm (Sornlertlamvanich, 1993). The longest
matching algorithm starts with a text span that could
be a phrase or a sentence. The algorithm tries to
align word boundaries according to the longest pos-
sible matching character compounds in a lexicon. If
no match is found in the lexicon, it drops the right-
most character in that text according to the morpho-
logical rules and begins the same search. If a word is
found, it marks a boundary at the end of the longest
word, and then begins the same search starting at the
remainder following the match.
In our work, the lexicon contained 32675 words.
However, the limitation of this algorithm is that if
the target words are compound words or unknown
words, it tends to produce incorrect results. For ex-
ample, a compound word is segmented as the fol-
lowing:
      ??????????????????                            
(Human Rights Organization) 
 
 
   ??????_?????_???_??_?? 
Since this compound word does not appear in the
lexicon, it becomes small useless words after the
word segmentation process. We further describe an
efficient approach to alleviate this problem by using
an idea of phrase construction (Ohsawa et al, 1998).
Let wi be a word that is firstly tokenized by us-
ing the longest matching algorithm. We refer to
w1w2 . . . wn as a phrase candidate, if n > 1, and
no punctuation and stopwords occur between w1
and wn. It is well accepted in information retrieval
community that words can be broadly classified into
content-bearing words and stopwords. In Thai, we
found that words that perform as function words can
be used in place of stopwords similar to English.
We collected 253 most frequently occurred words
for making a list of Thai stopwords.
Given a phrase candidate consisting of n words,
we can generate a set of phrases in the following
form:
W =
?
???
???
w1w2 w1w2w3 . . . w1w2w3 . . . wn?1wn
w2w3 . . . w2w3 . . . wn?1wn
.
.
.
wn?1wn
?
???
???
(1)
For example, if a phrase candidate consists
of four words, w1w2w3w4, we then obtain W =
{w1w2, w1w2w3, w1w2w3w4, w2w3, w2w3w4, w3w4}.
Let l be the number of set elements that can be
computed from l = (n ? (n? 1))/2 = (4 ? 3)/2 = 6.
Since we use both stopwords and punctuation
for bounding the phrase candidate, this approach
produces a moderate number of set elements.
Let V be a temporary lexicon. After building
all the phrase candidates in the document and gen-
erating their sets of phrases, we can construct V
by adding phrases that the number of occurrences
exceeds some threshold. This idea is to exploit
redundancy of phrases occurring in the document.
If a generated phrase frequently occurs, this indi-
cates that it may be a meaningful phrase, and should
be included in the temporary lexicon using for re-
segmenting words.
We denote U to be a main lexicon. After obtain-
ing the temporary lexicon V , we then re-segment
words in the document by using U ? V . With us-
ing the combination of these two lexicons, we can
recover some words from the first segmentation. Al-
though we have to do the word segmentation pro-
cess twice, the computation time is not prohibitive.
Furthermore, we obtain more meaningful words that
can be extracted to form keywords of the document.
4 Generating Summaries by Extraction
4.1 Finding Clusters of Significant Words
In this section, we first describe an approach for
finding clusters of significant words in each para-
graph to calculate the local clustering score. Our
approach is reminiscent of Luhn?s approach (1959)
but uses the other term weighting technique instead
of the term frequency. Luhn suggested that the fre-
quency of a word occurrence in a document, as well
as its relative position determines its significance in
that document. More recent works have also em-
ployed Luhn?s approach as a basis component for
extracting relevant sentences (Buyukkokten et al,
2001; Lam-Adesina and Jones, 2001). This ap-
proach performs well despite of its simplicity. In our
previous work (Jaruskulchai et al, 2003), we also
applied this approach for summarizing and brows-
ing Thai documents through PDAs.
Let ? be a subset of a continuous sequence of
words in a paragraph, {wu . . . wv}. The subset ?
is called a cluster of significant words if it has these
characteristics:
? The first word wu and the last word wv in the
sequence are significant words.
? Significant words are separated by not more
than a predefined number of insignificant
words.
For example, we can partition a continuous se-
quence of words in a paragraph into clusters as
shown in Figure 1. The paragraph consists of twelve
words. We use the boldface to indicate positions
of significant words. Each cluster is enclosed with
brackets. In this example, we define that a cluster
is created whereby significant words are separated
by not more than three insignificant words. Note
that many clusters of significant words can be found
in the paragraph. The highest score of the clusters
found in the paragraph is selected to be the para-
graph score. Therefore, the local clustering score
for paragraph si can be calculated as follows:
Lsi = argmax?
ns(?, si)2
n(?, si)
, (2)
where ns(?, si) is the number of bracketed signif-
icant words, and n(?, si) is the total number of
bracketed words.
We can see that the first important step in this pro-
cess is to mark positions of significant words for
identifying the clusters. Our goal is to find topical
words, which are indicative of the topics underly-
ing the document. According to Luhn?s approach,
the term frequencies is used to weight all the words.
The other term weighting scheme frequently used
is TFIDF (Term Frequency Inverse Document Fre-
quency) (Salton and Buckley, 1988). However, this
technique needs a corpus for computing IDF score,
causing the genre-dependent problem for generic
text summarization task.
In our work, we decide to use TLTF (Term Length
Term Frequency) term weighting technique (Banko
et al, 1999) for scoring words in the document in-
stead of TFIDF. TLTF multiplies a monotonic func-
tion of the term length by a monotonic function of
the term frequency. The basic idea of TLTF is based
on the assumption that words that are used more
frequently tend to be shorter. Such words are not
strongly indicative of the topics underlying in the
document, such as stopwords. In contrast, words
that are used less frequently tend to be longer. One
significant benefit of using TLTF term weighting
technique for our task is that it does not require
any external resources, only using the information
within the document.
w1[w2w3w4] w5w6w7w8[w9w10w11w12]
Figure 1: Clusters of significant words.
4.2 Discovering Relations of Paragraphs
We now move on to describe an approach for dis-
covering relations of paragraphs. Given a docu-
ment D, we can represent it by an undirected graph
G = (V,E), where V = {s1, . . . , sm} is the set of
paragraphs in that document. An edge (si, sj) is
in E, if the cosine similarity between paragraphs
si and sj is above a certain threshold, denoted ?.
A paragraph si is considered to be a set of words
{wsi,1 , wsi,2 , . . . , wsi,t}. The cosine similarity be-
tween two paragraphs can be calculated by the fol-
lowing formula:
sim(si, sj) =
?t
k=1 wsi,kwsj,k
??t
k=1 w2si,k
?t
k=1 w2sj,k
. (3)
The graph G is called the text relationship map of
D (Salton et al, 1999). Let dsi be the degree of node
si. We then refer to dsi as the global connectivity
score. Generating a summary for a given document
can be processed by sorting all the nodes with dsi in
decreasing order, and then extracting n top-ranked
nodes, where n is the target number of paragraphs
in the summary.
This idea is based on Salton et al?s approach that
also performs extraction at the paragraph level. They
suggested that since a highly bushy node is linked
to a number of other nodes, it has an overlapping
vocabulary with several paragraphs, and is likely to
discuss topics covered in many other paragraphs.
Consequently, such nodes are good candidates for
extraction. They then used a global bushy path that
is constructed out of n most bushy nodes to form the
summary. Their experimental results on encyclope-
dia articles demonstrates reasonable results.
However, when we directly applied this approach
for extracting paragraphs from moderately-sized
documents, we found that using only the global con-
nectivity score is inadequate to measure the infor-
mativeness of paragraphs in some case. In order
to describe this situation, we consider an example
of a text relationship map in Figure 2. The map is
P1
P2
P3P4
P5
P6
P7
P8 P9
P10
Figure 2: Text relationship map of an online news-
paper article using ? = 0.10.
P1
P2
P3P4
P5
P6
P7
P8 P9
P10
Figure 3: Text relationship map of the same article,
but using ? = 0.20.
constructed from an online newspaper article.1 The
similarity threshold ? is 0.1. As a result, edges with
similarities less than 0.1 do not appear on the map.
Node P4 obtains the maximum global connectivity
score at 9. However, the global connectivity score
of nodes P3, P5, and P6 is 7, and nodes P7 and P8 is
6, which are slightly different. When we increase the
threshold ? = 0.2, we obtain a text relationship map
as shown in Figure 3. Nodes P4 and P7 now achieve
the same maximum global connectivity score at 5.
Nodes P3, P5, and P6 get the same score at 4.
From above example, it is hard to determine that
1The article is available at: http://mickey.sci.ku.
ac.th/?TextSumm/sample/t1.html
node P4 is more relevant than nodes such as P3 or
P5, since their scores are only different at 1 point.
Our preliminary experiments with many other docu-
ments lead to the suggestion that the global connec-
tivity score of nodes in the text relation map tends
to be slightly different on some document lengths.
Given a compression rate (ratio of the summary
length to the source length), if we immediately ex-
tract these nodes of paragraphs, many paragraphs
with the same score are also included in the sum-
mary.
4.3 Combining Local and Global Properties
In this section, we present an algorithm that takes
advantage of both the local and global properties
of paragraphs for generating extractive summaries.
From previous sections, we describe two differ-
ent approaches that can be used to extract relevant
paragraphs. However, these extraction schemes are
based on different views and concepts. The local
clustering score only captures the content of infor-
mation within paragraphs, while the global connec-
tivity score mainly considers the structural aspect
of the document to evaluate the informativeness of
paragraphs. This leads to our motivation for uni-
fying good aspects of these two properties. We
can consider the local clustering score as the local
property of paragraphs, and the global connectivity
score as the global property. Here we propose an
algorithm that combines the local clustering score
with the global connectivity score to get a single
measure reflecting the informativeness of each para-
graph, which can be tuned according to the relative
importance of properties.
Our algorithm proceeds as follows. Given a doc-
ument, we start by eliminating stopwords and ex-
tracting all unique words in the document. These
unique words are used to be the document vocabu-
lary. Therefore, we can represent a paragraph si as a
vector. We then compute similarities between all the
paragraph vectors using equation (3), and eliminate
edges with similarities less than a threshold in order
to build the text relationship map. This process auto-
matically yields the global connectivity scores of the
paragraphs. Next, we weight each word in the doc-
ument vocabulary using TLTF term weighting tech-
nique. All the words are sorted by their TLTF scores,
and top r words are selected to be significant words.
We mark positions of significant words in each para-
graph to calculate the local clustering score. After
obtaining both scores, for each paragraph si, we can
compute the combination score by using the follow-
ing ranking function:
F (si) = ?G
? + (1 ? ?)L? , (4)
where G? is the normalized global connectivity
score, and L? is the normalized local clustering
score. The normalized global connectivity score G?
can be calculated as follows:
G? =
dsi
dmax
, (5)
where dmax is the degree of the node that has the
maximum edges using for normalization, resulting
the score in the range of [0, 1]. Using equation (2),
L? is given by:
L? =
Lsi
Lmax
, (6)
where Lmax is the maximum local clustering score
using for normalization. Similarly, it results this
score in the range of [0, 1]. The parameter ? is var-
ied depending on the relative importance of the com-
ponents G? and L?. Therefore, we can rank all the
paragraphs according to their combination scores in
decreasing order. We finally extract n top-ranked
paragraphs corresponding to the compression rate,
and rearrange them in chronological order to form
the output summary.
5 Experiments
5.1 Data Sets
The typical approach for testing a summarization
system is to create an ?ideal? summary, either
by professional abstractors or merging summaries
provided by multiple human subjects using meth-
ods such as majority opinion, union, or intersec-
tion (Jing et al, 1998). This approach is known
as intrinsic method. Unlike in English, standard
data sets in Thai are not yet available for evaluat-
ing text summarization system. However, in order
to observe characteristics of our algorithm, we col-
lected Thai documents, including agricultural news
(D1.AN), general news (D2.GN), and columnist?s
articles (D3.CA) to make data sets. Each data set
consists of 10 documents, and document sizes range
from 1 to 4 pages. We asked a student in the Depart-
ment of Thais, Faculty of Liberal Arts, for manual
summarization by selecting the most relevant para-
graphs that can indicate the main points of the docu-
ment. These paragraphs are called extracts, and then
are used for evaluating our algorithm.
5.2 Performance Evaluations
We evaluate results of summarization by using the
standard precision, recall, and F1. Let J be the num-
ber of extracts in the summary, K be the number of
selected paragraphs in the summary, and M be the
number of extracts in the test document. We then
refer to precision of the algorithm as the fraction be-
tween the number of extracts in the summary and the
number of selected paragraphs in the summary:
Precision =
J
K
, (7)
recall as the fraction between the number of extracts
in the summary and the number of extracts in the test
document:
Recall =
J
M
. (8)
Finally, F1, a combination of precision and recall,
can be calculated as follows:
F1 =
2 ? Precision ?Recall
Precision + Recall
. (9)
5.3 Experimental Results
In this section, we provide experimental evidence
that our algorithm gives acceptable performance.
The compression rate of paragraph extraction to
form a summary is 20% and 30%. These rates yield
the number of extracts in the summary comparable
to the number of actual extracts in a given test doc-
ument. The threshold ? of the cosine similarity is
0.2. The parameter ? for combining the local and
global properties is 0.5. For the distance between
significant words in a cluster, we set that significant
words are separated by not more than three insignif-
icant words.
Table 1 and 2 show a summary of precision, re-
call, and F1 for each compression rate, respectively.
We can see that average precision values of our al-
gorithm slightly decrease, but average recall val-
ues increase when we increase the compression rate.
 
????????? (Keywords): 
????????????, ??????????????????, ???????????, ??????, ???????????????, ?????????, ?????????,
???????????, ?????????????, ????????????, ???????????? 
?????????????????? 20% (Summarization result at 20%): 
????????????????????? ??????????? ??? ?????????????? ???? ?????????????????????????????
??????????????????? ??????????????? ??????????????????????????????????????????????????
??????????????????????????????????????????????? ???????????????????????????? 
????????????????? ??????????????????????????????????????????????????????????????????
??????????? ???????????????????????????????????????? ??????????? ???????????????????????????????
?????????????????????????????????????????????????????????? 1-4 ??????? 
??????????????? ?????????????????????????????????????? ?????????? ?????????? (Centrino)
???????????????????????????? ???????????????????????????? ??????????????????????????????????
??? (WiFi) ??????????? 
Figure 4: An example of keywords and extracted summaries in Thai.
Data set Precision Recall F1
D1.AN 0.600 0.448 0.509
D2.GN 0.518 0.385 0.431
D3.CA 0.530 0.330 0.404
Table 1: Evaluation results obtained by using com-
pression rate 20%.
Since using higher compression rate tends to select
more paragraphs from the document, it increases the
chance that the selected paragraphs will be matched
with the target extracts. On the other hand, it also
selects irrelevant paragraphs to be included in the
summary, so precision can decrease. Further experi-
ments on larger text corpora are needed to determine
the performance of our summarizer. However, these
preliminary results are very encouraging. Figure
4 illustrates an example of keywords and extracted
summaries for a Thai document using compression
rate 20% . The implementation of our algorithm is
now available for user testing at http://mickey.
sci.ku.ac.th/?TextSumm/index.html. The com-
putation time to summarize moderately-sized docu-
ments, such as newspaper articles, is less one sec-
ond.
6 Conclusions and Future Work
In this paper, we have presented a practical ap-
proach to Thai text summarization by extracting the
Data set Precision Recall F1
D1.AN 0.550 0.577 0.555
D2.GN 0.464 0.467 0.453
D3.CA 0.523 0.462 0.488
Table 2: Evaluation results obtained by using com-
pression rate 30%.
most relevant paragraphs from the original docu-
ment. Our approach takes advantage of both the
local and global properties of paragraphs. The algo-
rithm that combines these two properties for ranking
and extracting paragraphs is given. Furthermore, the
algorithm does not require the external knowledge
other than the document itself, and be able to sum-
marize general text documents.
In future work, we intend to conduct experiments
with different document genres. We continue to fur-
ther develop standard data sets for evaluating Thai
text summarization system. Many research ques-
tions remain. Since extraction performs at the para-
graph level, the paragraph lengths may affect the
summarization results. The recent approach for
editing extracted text spans (Jing and McKeown,
2000) may also produce improvement for our algo-
rithm. We believe that our algorithm is language-
independent, which can summarize documents writ-
ten in many other languages. We plan to experimen-
tally test our algorithm with available standard data
sets in English.
Acknowledgments
This research was supported by the grant of the Na-
tional Research Council of Thailand, 2002. Many
thanks to Tan Sinthurahat (Thammasat University)
for manual summarizing the data sets.
References
Banko, M., Mittal, V., Kantrowitz, M., and Goldstein, J.
1999. Generating extraction-based summaries from
hand-written summaries by aligning text spans. In
Proceedings of PACLING?99.
Buyukkokten, O., Garcia-Molina, H., and Paepcke, A.
2001. Seeing the whole in parts: Text summarization
for web browsing on handheld devices. WWW10.
Chuang, W. T., and Yang, J. 2000. Extracting sentence
segments for text summarization: A machine learning
approach. In Proceedings of the 23rd ACM SIGIR,
152?159.
Edmundson, H. P. 1969. New methods in automatic ex-
traction. Journal of the ACM, 16(2):264?285.
Goldstein, J., Kantrowitz, M., Mittal, V., and Carbonell,
J. 1999. Summarizing text documents: Sentence se-
lection and evaluation metrics. In Proceedings of the
22nd ACM SIGIR, 121?128.
Hahn, U., and Mani, I. 2000. The challenges of auto-
matic summarization. IEEE Computer, 33(11):29?35.
Jaruskulchai, C., Khanthong, A., and Tantiprasongchai,
W. 2003. A Framework for Delivery of Thai Content
through Mobile Devices. Closing Gaps in the Digital
Divide Regional Conference on Digital GMS. Asian
Institute of Technology, 190?194.
Jing, H., Barzilay, R., McKeown, K., and Elhadad, M.
1998. Summarization evaluation methods: Experi-
ments and analysis. AAAI Intelligent Text Summariza-
tion Workshop, 60?68.
Jing, H., and McKeown, K. 2000. Cut and paste based
text summarization. In Proceedings of the 1st Confer-
ence of the North American Chapter of the Association
for Computational Linguistics.
Kupiec, J., Pedersen, J., and Chen, F. 1995. A train-
able document summarizer. In Proceedings of the 18th
ACM SIGIR, 68?73.
Lam-Adesina, M., and Jones, G. J. F. 2001. Applying
summarization techniques for term selection in rele-
vance feedback. In Proceedings of the 24th ACM SI-
GIR, 1?9.
Luhn, H. P. 1959. The automatic creation of literature
abstracts. IBM Journal of Research and Development,
159?165.
Mani, I., Firmin, T., House, D., Klein, G., Sundheim,
B., Hirschman, L. 1999. The TIPSTER SUMMAC
Text Summarization Evaluation. In Proceedings of
EACL?99.
Mani, I., and Maybury, M. T. 1999. Advances in ac-
tomatic text summarization. MIT Press.
Ohsawa, Y., Benson, N. E., and Yachida, M. 1998. Key-
Graph: Automatic indexing by cooccurrence graph
based on building construction metaphor. In Proceed-
ings of EAdvanced Digital Library Conference.
Salton, G., and Buckley, C. 1988. Term weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management, 24(5):513?523.
Salton, G., Singhal, A., Mitra, M., and Buckley, C. 1999.
Automatic text structuring and summarization. In
Mani, I. and Maybury, M. (Eds.), Advances in auto-
matic text summarization. MIT Press.
Sornlertlamvanich, V. 1993. Word segmentation for Thai
in machine translation system. Machine Translation,
National Electronics and Computer Technology Cen-
ter, 50?56.
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513?521,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
An Error-Driven Word-Character Hybrid Model
for Joint Chinese Word Segmentation and POS Tagging
Canasai Kruengkrai?? and Kiyotaka Uchimoto? and Jun?ichi Kazama?
Yiou Wang? and Kentaro Torisawa? and Hitoshi Isahara??
?Graduate School of Engineering, Kobe University
1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp
Abstract
In this paper, we present a discriminative
word-character hybrid model for joint Chi-
nese word segmentation and POS tagging.
Our word-character hybrid model offers
high performance since it can handle both
known and unknown words. We describe
our strategies that yield good balance for
learning the characteristics of known and
unknown words and propose an error-
driven policy that delivers such balance
by acquiring examples of unknown words
from particular errors in a training cor-
pus. We describe an efficient framework
for training our model based on the Mar-
gin Infused Relaxed Algorithm (MIRA),
evaluate our approach on the Penn Chinese
Treebank, and show that it achieves supe-
rior performance compared to the state-of-
the-art approaches reported in the litera-
ture.
1 Introduction
In Chinese, word segmentation and part-of-speech
(POS) tagging are indispensable steps for higher-
level NLP tasks. Word segmentation and POS tag-
ging results are required as inputs to other NLP
tasks, such as phrase chunking, dependency pars-
ing, and machine translation. Word segmenta-
tion and POS tagging in a joint process have re-
ceived much attention in recent research and have
shown improvements over a pipelined fashion (Ng
and Low, 2004; Nakagawa and Uchimoto, 2007;
Zhang and Clark, 2008; Jiang et al, 2008a; Jiang
et al, 2008b).
In joint word segmentation and the POS tag-
ging process, one serious problem is caused by
unknown words, which are defined as words that
are not found in a training corpus or in a sys-
tem?s word dictionary1. The word boundaries and
the POS tags of unknown words, which are very
difficult to identify, cause numerous errors. The
word-character hybrid model proposed by Naka-
gawa and Uchimoto (Nakagawa, 2004; Nakagawa
and Uchimoto, 2007) shows promising properties
for solving this problem. However, it suffers from
structural complexity. Nakagawa (2004) described
a training method based on a word-based Markov
model and a character-based maximum entropy
model that can be completed in a reasonable time.
However, this training method is limited by the
generatively-trained Markov model in which in-
formative features are hard to exploit.
In this paper, we overcome such limitations
concerning both efficiency and effectiveness. We
propose a new framework for training the word-
character hybrid model based on the Margin
Infused Relaxed Algorithm (MIRA) (Crammer,
2004; Crammer et al, 2005; McDonald, 2006).
We describe k-best decoding for our hybrid model
and design its loss function and the features appro-
priate for our task.
In our word-character hybrid model, allowing
the model to learn the characteristics of both
known and unknown words is crucial to achieve
optimal performance. Here, we describe our
strategies that yield good balance for learning
these two characteristics. We propose an error-
driven policy that delivers this balance by acquir-
ing examples of unknown words from particular
errors in a training corpus. We conducted our ex-
periments on Penn Chinese Treebank (Xia et al,
2000) and compared our approach with the best
previous approaches reported in the literature. Ex-
perimental results indicate that our approach can
achieve state-of-the-art performance.
1A system?s word dictionary usually consists of a word
list, and each word in the list has its own POS category. In
this paper, we constructed the system?s word dictionary from
a training corpus.
513
Figure 1: Lattice used in word-character hybrid model.
Tag Description
B Beginning character in a multi-character word
I Intermediate character in a multi-character word
E End character in a multi-character word
S Single-character word
Table 1: Position-of-character (POC) tags.
The paper proceeds as follows: Section 2 gives
background on the word-character hybrid model,
Section 3 describes our policies for correct path
selection, Section 4 presents our training method
based on MIRA, Section 5 shows our experimen-
tal results, Section 6 discusses related work, and
Section 7 concludes the paper.
2 Background
2.1 Problem formation
In joint word segmentation and the POS tag-
ging process, the task is to predict a path
of word hypotheses y = (y1, . . . , y#y) =
(?w1, p1?, . . . , ?w#y, p#y?) for a given character
sequence x = (c1, . . . , c#x), where w is a word,
p is its POS tag, and a ?#? symbol denotes the
number of elements in each variable. The goal of
our learning algorithm is to learn a mapping from
inputs (unsegmented sentences) x ? X to outputs
(segmented paths) y ? Y based on training sam-
ples of input-output pairs S = {(xt, yt)}Tt=1.
2.2 Search space representation
We represent the search space with a lattice based
on the word-character hybrid model (Nakagawa
and Uchimoto, 2007). In the hybrid model,
given an input sentence, a lattice that consists
of word-level and character-level nodes is con-
structed. Word-level nodes, which correspond to
words found in the system?s word dictionary, have
regular POS tags. Character-level nodes have spe-
cial tags where position-of-character (POC) and
POS tags are combined (Asahara, 2003; Naka-
gawa, 2004). POC tags indicate the word-internal
positions of the characters, as described in Table 1.
Figure 1 shows an example of a lattice for a Chi-
nese sentence: ? ? (Chongming is
China?s third largest island). Note that some nodes
and state transitions are not allowed. For example,
I and E nodes cannot occur at the beginning of the
lattice (marked with dashed boxes), and the transi-
tions from I to B nodes are also forbidden. These
nodes and transitions are ignored during the lattice
construction processing.
In the training phase, since several paths
(marked in bold) can correspond to the correct
analysis in the annotated corpus, we need to se-
lect one correct path yt as a reference for training.2
The next section describes our strategies for deal-
ing with this issue.
With this search space representation, we
can consistently handle unknown words with
character-level nodes. In other words, we use
word-level nodes to identify known words and
character-level nodes to identify unknown words.
In the testing phase, we can use a dynamic pro-
gramming algorithm to search for the most likely
path out of all candidate paths.
2A machine learning problem exists called structured
multi-label classification that allows training from multiple
correct paths. However, in this paper we limit our considera-
tion to structured single-label classification, which is simple
yet provides great performance.
514
3 Policies for correct path selection
In this section, we describe our strategies for se-
lecting the correct path yt in the training phase.
As shown in Figure 1, the paths marked in bold
can represent the correct annotation of the seg-
mented sentence. Ideally, we need to build a word-
character hybrid model that effectively learns the
characteristics of unknown words (with character-
level nodes) as well as those of known words (with
word-level nodes).
We can directly estimate the statistics of known
words from an annotated corpus where a sentence
is already segmented into words and assigned POS
tags. If we select the correct path yt that corre-
sponds to the annotated sentence, it will only con-
sist of word-level nodes that do not allow learning
for unknown words. We therefore need to choose
character-level nodes as correct nodes instead of
word-level nodes for some words. We expect that
those words could reflect unknown words in the
future.
Baayen and Sproat (1996) proposed that the
characteristics of infrequent words in a training
corpus resemble those of unknown words. Their
idea has proven effective for estimating the statis-
tics of unknown words in previous studies (Ratna-
parkhi, 1996; Nagata, 1999; Nakagawa, 2004).
We adopt Baayen and Sproat?s approach as
the baseline policy in our word-character hybrid
model. In the baseline policy, we first count the
frequencies of words3 in the training corpus. We
then collect infrequent words that appear less than
or equal to r times.4 If these infrequent words are
in the correct path, we use character-level nodes
to represent them, and hence the characteristics of
unknown words can be learned. For example, in
Figure 1 we select the character-level nodes of the
word ? ? (Chongming) as the correct nodes. As
a result, the correct path yt can contain both word-
level and character-level nodes (marked with as-
terisks (*)).
To discover more statistics of unknown words,
one might consider just increasing the threshold
value r to obtain more artificial unknown words.
However, our experimental results indicate that
our word-character hybrid model requires an ap-
propriate balance between known and artificial un-
3We consider a word and its POS tag a single entry.
4In our experiments, the optimal threshold value r is se-
lected by evaluating the performance of joint word segmen-
tation and POS tagging on the development set.
known words to achieve optimal performance.
We now describe our new approach to lever-
age additional examples of unknown words. In-
tuition suggests that even though the system can
handle some unknown words, many unidentified
unknown words remain that cannot be recovered
by the system; we wish to learn the characteristics
of such unidentified unknown words. We propose
the following simple scheme:
? Divide the training corpus into ten equal sets
and perform 10-fold cross validation to find
the errors.
? For each trial, train the word-character hybrid
model with the baseline policy (r = 1) us-
ing nine sets and estimate errors using the re-
maining validation set.
? Collect unidentified unknown words from
each validation set.
Several types of errors are produced by the
baseline model, but we only focus on those caused
by unidentified unknown words, which can be eas-
ily collected in the evaluation process. As de-
scribed later in Section 5.2, we measure the recall
on out-of-vocabulary (OOV) words. Here, we de-
fine unidentified unknown words as OOV words
in each validation set that cannot be recovered by
the system. After ten cross validation runs, we
get a list of the unidentified unknown words de-
rived from the whole training corpus. Note that
the unidentified unknown words in the cross val-
idation are not necessary to be infrequent words,
but some overlap may exist. Finally, we obtain the
artificial unknown words that combine the uniden-
tified unknown words in cross validation and in-
frequent words for learning unknown words. We
refer to this approach as the error-driven policy.
4 Training method
4.1 Discriminative online learning
Let Yt = {y1t , . . . , yKt } be a lattice consisting of
candidate paths for a given sentence xt. In the
word-character hybrid model, the lattice Yt can
contain more than 1000 nodes, depending on the
length of the sentence xt and the number of POS
tags in the corpus. Therefore, we require a learn-
ing algorithm that can efficiently handle large and
complex lattice structures.
Online learning is an attractive method for
the hybrid model since it quickly converges
515
Algorithm 1 Generic Online Learning Algorithm
Input: Training set S = {(xt, yt)}Tt=1
Output: Model weight vector w
1: w(0) = 0;v = 0; i = 0
2: for iter = 1 to N do
3: for t = 1 to T do
4: w(i+1) = update w(i) according to (xt, yt)
5: v = v +w(i+1)
6: i = i+ 1
7: end for
8: end for
9: w = v/(N ? T )
within a few iterations (McDonald, 2006). Algo-
rithm 1 outlines the generic online learning algo-
rithm (McDonald, 2006) used in our framework.
4.2 k-best MIRA
We focus on an online learning algorithm called
MIRA (Crammer, 2004), which has the de-
sired accuracy and scalability properties. MIRA
combines the advantages of margin-based and
perceptron-style learning with an optimization
scheme. In particular, we use a generalized ver-
sion of MIRA (Crammer et al, 2005; McDonald,
2006) that can incorporate k-best decoding in the
update procedure. To understand the concept of k-
best MIRA, we begin with a linear score function:
s(x, y;w) = ?w, f(x, y)? , (1)
where w is a weight vector and f is a feature rep-
resentation of an input x and an output y.
Learning a mapping between an input-output
pair corresponds to finding a weight vector w such
that the best scoring path of a given sentence is
the same as (or close to) the correct path. Given
a training example (xt, yt), MIRA tries to estab-
lish a margin between the score of the correct path
s(xt, yt;w) and the score of the best candidate
path s(xt, y?;w) based on the current weight vector
w that is proportional to a loss function L(yt, y?).
In each iteration, MIRA updates the weight vec-
tor w by keeping the norm of the change in the
weight vector as small as possible. With this
framework, we can formulate the optimization
problem as follows (McDonald, 2006):
w(i+1) = argminw?w ?w(i)? (2)
s.t. ?y? ? bestk(xt;w(i)) :
s(xt, yt;w)? s(xt, y?;w) ? L(yt, y?) ,
where bestk(xt;w(i)) ? Yt represents a set of top
k-best paths given the weight vector w(i). The
above quadratic programming (QP) problem can
be solved using Hildreth?s algorithm (Yair Cen-
sor, 1997). Replacing Eq. (2) into line 4 of Al-
gorithm 1, we obtain k-best MIRA.
The next question is how to efficiently gener-
ate bestk(xt;w(i)). In this paper, we apply a dy-
namic programming search (Nagata, 1994) to k-
best MIRA. The algorithm has two main search
steps: forward and backward. For the forward
search, we use Viterbi-style decoding to find the
best partial path and its score up to each node in
the lattice. For the backward search, we use A?-
style decoding to generate the top k-best paths. A
complete path is found when the backward search
reaches the beginning node of the lattice, and the
algorithm terminates when the number of gener-
ated paths equals k.
In summary, we use k-best MIRA to iteratively
update w(i). The final weight vector w is the av-
erage of the weight vectors after each iteration.
As reported in (Collins, 2002; McDonald et al,
2005), parameter averaging can effectively avoid
overfitting. For inference, we can use Viterbi-style
decoding to search for the most likely path y? for
a given sentence x where:
y? = argmax
y?Y
s(x, y;w) . (3)
4.3 Loss function
In conventional sequence labeling where the ob-
servation sequence (word) boundaries are fixed,
one can use the 0/1 loss to measure the errors of
a predicted path with respect to the correct path.
However, in our model, word boundaries vary
based on the considered path, resulting in a dif-
ferent numbers of output tokens. As a result, we
cannot directly use the 0/1 loss.
We instead compute the loss function through
false positives (FP ) and false negatives (FN ).
Here, FP means the number of output nodes that
are not in the correct path, and FN means the
number of nodes in the correct path that cannot
be recognized by the system. We define the loss
function by:
L(yt, y?) = FP + FN . (4)
This loss function can reflect how bad the pre-
dicted path y? is compared to the correct path yt.
A weighted loss function based on FP and FN
can be found in (Ganchev et al, 2007).
516
ID Template Condition
W0 ?w0? for word-level
W1 ?p0? nodes
W2 ?w0, p0?
W3 ?Length(w0), p0?
A0 ?AS(w0)? if w0 is a single-
A1 ?AS(w0), p0? character word
A2 ?AB(w0)? for word-level
A3 ?AB(w0), p0? nodes
A4 ?AE(w0)?
A5 ?AE(w0), p0?
A6 ?AB(w0), AE(w0)?
A7 ?AB(w0), AE(w0), p0?
T0 ?TS(w0)? if w0 is a single-
T1 ?TS(w0), p0? character word
T2 ?TB(w0)? for word-level
T3 ?TB(w0), p0? nodes
T4 ?TE(w0)?
T5 ?TE(w0), p0?
T6 ?TB(w0), TE(w0)?
T7 ?TB(w0), TE(w0), p0?
C0 ?cj?, j ? [?2, 2] ? p0 for character-
C1 ?cj , cj+1?, j ? [?2, 1] ? p0 level nodes
C2 ?c?1, c1? ? p0
C3 ?T (cj)?, j ? [?2, 2] ? p0
C4 ?T (cj), T (cj+1)?, j ? [?2, 1] ? p0
C5 ?T (c?1), T (c1)? ? p0
C6 ?c0, T (c0)? ? p0
Table 2: Unigram features.
4.4 Features
This section discusses the structure of f(x, y). We
broadly classify features into two categories: uni-
gram and bigram features. We design our feature
templates to capture various levels of information
about words and POS tags. Let us introduce some
notation. We write w?1 and w0 for the surface
forms of words, where subscripts ?1 and 0 in-
dicate the previous and current positions, respec-
tively. POS tags p?1 and p0 can be interpreted in
the same way. We denote the characters by cj .
Unigram features: Table 2 shows our unigram
features. Templates W0?W3 are basic word-level
unigram features, where Length(w0) denotes the
length of the word w0. Using just the surface
forms can overfit the training data and lead to poor
predictions on the test data. To alleviate this prob-
lem, we use two generalized features of the sur-
face forms. The first is the beginning and end
characters of the surface (A0?A7). For example,
?AB(w0)? denotes the beginning character of the
current word w0, and ?AB(w0), AE(w0)? denotes
the beginning and end characters in the word. The
second is the types of beginning and end charac-
ters of the surface (T0?T7). We define a set of
general character types, as shown in Table 4.
Templates C0?C6 are basic character-level un-
ID Template Condition
B0 ?w?1, w0? if w?1 and w0
B1 ?p?1, p0? are word-level
B2 ?w?1, p0? nodes
B3 ?p?1, w0?
B4 ?w?1, w0, p0?
B5 ?p?1, w0, p0?
B6 ?w?1, p?1, w0?
B7 ?w?1, p?1, p0?
B8 ?w?1, p?1, w0, p0?
B9 ?Length(w?1), p0?
TB0 ?TE(w?1)?
TB1 ?TE(w?1), p0?
TB2 ?TE(w?1), p?1, p0?
TB3 ?TE(w?1), TB(w0)?
TB4 ?TE(w?1), TB(w0), p0?
TB5 ?TE(w?1), p?1, TB(w0)?
TB6 ?TE(w?1), p?1, TB(w0), p0?
CB0 ?p?1, p0? otherwise
Table 3: Bigram features.
Character type Description
Space Space
Numeral Arabic and Chinese numerals
Symbol Symbols
Alphabet Alphabets
Chinese Chinese characters
Other Others
Table 4: Character types.
igram features taken from (Nakagawa, 2004).
These templates operate over a window of ?2
characters. The features include characters (C0),
pairs of characters (C1?C2), character types (C3),
and pairs of character types (C4?C5). In addi-
tion, we add pairs of characters and character types
(C6).
Bigram features: Table 3 shows our bigram
features. Templates B0-B9 are basic word-
level bigram features. These features aim to
capture all the possible combinations of word
and POS bigrams. Templates TB0-TB6 are the
types of characters for bigrams. For example,
?TE(w?1), TB(w0)? captures the change of char-
acter types from the end character in the previ-
ous word to the beginning character in the current
word.
Note that if one of the adjacent nodes is a
character-level node, we use the template CB0 that
represents POS bigrams. In our preliminary ex-
periments, we found that if we add more features
to non-word-level bigrams, the number of features
grows rapidly due to the dense connections be-
tween non-word-level nodes. However, these fea-
tures only slightly improve performance over us-
ing simple POS bigrams.
517
(a) Experiments on small training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270 3,046 75,169
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 32
OOV (word) 0.0987 (790/8,008)
OOV (word & POS) 0.1140 (913/8,008)
(b) Experiments on large training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270, 18,089 493,939
400-931,
1001-1151
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 35
OOV (word) 0.0347 (278/8,008)
OOV (word & POS) 0.0420 (336/8,008)
Table 5: Training, development, and test data
statistics on CTB 5.0 used in our experiments.
5 Experiments
5.1 Data sets
Previous studies on joint Chinese word segmen-
tation and POS tagging have used Penn Chinese
Treebank (CTB) (Xia et al, 2000) in experiments.
However, versions of CTB and experimental set-
tings vary across different studies.
In this paper, we used CTB 5.0 (LDC2005T01)
as our main corpus, defined the training, develop-
ment and test sets according to (Jiang et al, 2008a;
Jiang et al, 2008b), and designed our experiments
to explore the impact of the training corpus size on
our approach. Table 5 provides the statistics of our
experimental settings on the small and large train-
ing data. The out-of-vocabulary (OOV) is defined
as tokens in the test set that are not in the train-
ing set (Sproat and Emerson, 2003). Note that the
development set was only used for evaluating the
trained model to obtain the optimal values of tun-
able parameters.
5.2 Evaluation
We evaluated both word segmentation (Seg) and
joint word segmentation and POS tagging (Seg
& Tag). We used recall (R), precision (P ), and
F1 as evaluation metrics. Following (Sproat and
Emerson, 2003), we also measured the recall on
OOV (ROOV) tokens and in-vocabulary (RIV) to-
kens. These performance measures can be calcu-
lated as follows:
Recall (R) = # of correct tokens# of tokens in test data
Precision (P ) = # of correct tokens# of tokens in system output
F1 = 2 ?R ? PR+ P
ROOV = # of correct OOV tokens# of OOV tokens in test data
RIV = # of correct IV tokens# of IV tokens in test data
For Seg, a token is considered to be a cor-
rect one if the word boundary is correctly iden-
tified. For Seg & Tag, both the word boundary and
its POS tag have to be correctly identified to be
counted as a correct token.
5.3 Parameter estimation
Our model has three tunable parameters: the num-
ber of training iterations N ; the number of top
k-best paths; and the threshold r for infrequent
words. Since we were interested in finding an
optimal combination of word-level and character-
level nodes for training, we focused on tuning r.
We fixed N = 10 and k = 5 for all experiments.
For the baseline policy, we varied r in the range
of [1, 5] and found that setting r = 3 yielded the
best performance on the development set for both
the small and large training corpus experiments.
For the error-driven policy, we collected unidenti-
fied unknown words using 10-fold cross validation
on the training set, as previously described in Sec-
tion 3.
5.4 Impact of policies for correct path
selection
Table 6 shows the results of our word-character
hybrid model using the error-driven and baseline
policies. The third and fourth columns indicate the
numbers of known and artificial unknown words
in the training phase. The total number of words
is the same, but the different policies yield differ-
ent balances between the known and artificial un-
known words for learning the hybrid model. Op-
timal balances were selected using the develop-
ment set. The error-driven policy provides addi-
tional artificial unknown words in the training set.
The error-driven policy can improve ROOV as well
as maintain good RIV, resulting in overall F1 im-
provements.
518
(a) Experiments on small training corpus
# of words in training (75,169)
Eval type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 63,997 11,172 0.9587 0.9509 0.9548 0.7557 0.9809baseline 64,999 10,170 0.9572 0.9489 0.9530 0.7304 0.9820
Seg & Tag error-driven 63,997 11,172 0.8929 0.8857 0.8892 0.5444 0.9377baseline 64,999 10,170 0.8897 0.8820 0.8859 0.5246 0.9367
(b) Experiments on large training corpus
# of words in training (493,939)
Eval Type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 442,423 51,516 0.9829 0.9746 0.9787 0.7698 0.9906baseline 449,679 44,260 0.9821 0.9736 0.9779 0.7590 0.9902
Seg & Tag error-driven 442,423 51,516 0.9407 0.9328 0.9367 0.5982 0.9557baseline 449,679 44,260 0.9401 0.9319 0.9360 0.5952 0.9552
Table 6: Results of our word-character hybrid model using error-driven and baseline policies.
Method Seg Seg & Tag
Ours (error-driven) 0.9787 0.9367
Ours (baseline) 0.9779 0.9360
Jiang08a 0.9785 0.9341
Jiang08b 0.9774 0.9337
N&U07 0.9783 0.9332
Table 7: Comparison of F1 results with previous
studies on CTB 5.0.
Seg Seg & Tag
N&U07 Z&C08 Ours N&U07 Z&C08 Ours
Trial (base.) (base.)
1 0.9701 0.9721 0.9732 0.9262 0.9346 0.9358
2 0.9738 0.9762 0.9752 0.9318 0.9385 0.9380
3 0.9571 0.9594 0.9578 0.9023 0.9086 0.9067
4 0.9629 0.9592 0.9655 0.9132 0.9160 0.9223
5 0.9597 0.9606 0.9617 0.9132 0.9172 0.9187
6 0.9473 0.9456 0.9460 0.8823 0.8883 0.8885
7 0.9528 0.9500 0.9562 0.9003 0.9051 0.9076
8 0.9519 0.9512 0.9528 0.9002 0.9030 0.9062
9 0.9566 0.9479 0.9575 0.8996 0.9033 0.9052
10 0.9631 0.9645 0.9659 0.9154 0.9196 0.9225
Avg. 0.9595 0.9590 0.9611 0.9085 0.9134 0.9152
Table 8: Comparison of F1 results of our baseline
model with Nakagawa and Uchimoto (2007) and
Zhang and Clark (2008) on CTB 3.0.
Method Seg Seg & Tag
Ours (baseline) 0.9611 0.9152
Z&C08 0.9590 0.9134
N&U07 0.9595 0.9085
N&L04 0.9520 -
Table 9: Comparison of averaged F1 results (by
10-fold cross validation) with previous studies on
CTB 3.0.
5.5 Comparison with best prior approaches
In this section, we attempt to make meaning-
ful comparison with the best prior approaches re-
ported in the literature. Although most previous
studies used CTB, their versions of CTB and ex-
perimental settings are different, which compli-
cates comparison.
Ng and Low (2004) (N&L04) used CTB 3.0.
However, they just showed POS tagging results
on a per character basis, not on a per word basis.
Zhang and Clark (2008) (Z&C08) generated CTB
3.0 from CTB 4.0. Jiang et al (2008a; 2008b)
(Jiang08a, Jiang08b) used CTB 5.0. Shi and
Wang (2007) used CTB that was distributed in the
SIGHAN Bakeoff. Besides CTB, they also used
HowNet (Dong and Dong, 2006) to obtain seman-
tic class features. Zhang and Clark (2008) indi-
cated that their results cannot directly compare to
the results of Shi and Wang (2007) due to different
experimental settings.
We decided to follow the experimental settings
of Jiang et al (2008a; 2008b) on CTB 5.0 and
Zhang and Clark (2008) on CTB 4.0 since they
reported the best performances on joint word seg-
mentation and POS tagging using the training ma-
terials only derived from the corpora. The perfor-
mance scores of previous studies are directly taken
from their papers. We also conducted experiments
using the system implemented by Nakagawa and
Uchimoto (2007) (N&U07) for comparison.
Our experiment on the large training corpus is
identical to that of Jiang et al (Jiang et al, 2008a;
Jiang et al, 2008b). Table 7 compares the F1 re-
sults with previous studies on CTB 5.0. The result
of our error-driven model is superior to previous
reported results for both Seg and Seg & Tag, and
the result of our baseline model compares favor-
ably to the others.
Following Zhang and Clark (2008), we first
generated CTB 3.0 from CTB 4.0 using sentence
IDs 1?10364. We then divided CTB 3.0 into
ten equal sets and conducted 10-fold cross vali-
519
dation. Unfortunately, Zhang and Clark?s exper-
imental setting did not allow us to use our error-
driven policy since performing 10-fold cross val-
idation again on each main cross validation trial
is computationally too expensive. Therefore, we
used our baseline policy in this setting and fixed
r = 3 for all cross validation runs. Table 8 com-
pares the F1 results of our baseline model with
Nakagawa and Uchimoto (2007) and Zhang and
Clark (2008) on CTB 3.0. Table 9 shows a sum-
mary of averaged F1 results on CTB 3.0. Our
baseline model outperforms all prior approaches
for both Seg and Seg & Tag, and we hope that
our error-driven model can further improve perfor-
mance.
6 Related work
In this section, we discuss related approaches
based on several aspects of learning algorithms
and search space representation methods. Max-
imum entropy models are widely used for word
segmentation and POS tagging tasks (Uchimoto
et al, 2001; Ng and Low, 2004; Nakagawa,
2004; Nakagawa and Uchimoto, 2007) since they
only need moderate training times while they pro-
vide reasonable performance. Conditional random
fields (CRFs) (Lafferty et al, 2001) further im-
prove the performance (Kudo et al, 2004; Shi
and Wang, 2007) by performing whole-sequence
normalization to avoid label-bias and length-bias
problems. However, CRF-based algorithms typ-
ically require longer training times, and we ob-
served an infeasible convergence time for our hy-
brid model.
Online learning has recently gained popularity
for many NLP tasks since it performs comparably
or better than batch learning using shorter train-
ing times (McDonald, 2006). For example, a per-
ceptron algorithm is used for joint Chinese word
segmentation and POS tagging (Zhang and Clark,
2008; Jiang et al, 2008a; Jiang et al, 2008b).
Another potential algorithm is MIRA, which in-
tegrates the notion of the large-margin classifier
(Crammer, 2004). In this paper, we first intro-
duce MIRA to joint word segmentation and POS
tagging and show very encouraging results. With
regard to error-driven learning, Brill (1995) pro-
posed a transformation-based approach that ac-
quires a set of error-correcting rules by comparing
the outputs of an initial tagger with the correct an-
notations on a training corpus. Our approach does
not learn the error-correcting rules. We only aim to
capture the characteristics of unknown words and
augment their representatives.
As for search space representation, Ng and
Low (2004) found that for Chinese, the character-
based model yields better results than the word-
based model. Nakagawa and Uchimoto (2007)
provided empirical evidence that the character-
based model is not always better than the word-
based model. They proposed a hybrid approach
that exploits both the word-based and character-
based models. Our approach overcomes the limi-
tation of the original hybrid model by a discrimi-
native online learning algorithm for training.
7 Conclusion
In this paper, we presented a discriminative word-
character hybrid model for joint Chinese word
segmentation and POS tagging. Our approach
has two important advantages. The first is ro-
bust search space representation based on a hy-
brid model in which word-level and character-
level nodes are used to identify known and un-
known words, respectively. We introduced a sim-
ple scheme based on the error-driven concept to
effectively learn the characteristics of known and
unknown words from the training corpus. The sec-
ond is a discriminative online learning algorithm
based on MIRA that enables us to incorporate ar-
bitrary features to our hybrid model. Based on ex-
tensive comparisons, we showed that our approach
is superior to the existing approaches reported in
the literature. In future work, we plan to apply
our framework to other Asian languages, includ-
ing Thai and Japanese.
Acknowledgments
We would like to thank Tetsuji Nakagawa for his
helpful suggestions about the word-character hy-
brid model, Chen Wenliang for his technical assis-
tance with the Chinese processing, and the anony-
mous reviewers for their insightful comments.
References
Masayuki Asahara. 2003. Corpus-based Japanese
morphological analysis. Nara Institute of Science
and Technology, Doctor?s Thesis.
Harald Baayen and Richard Sproat. 1996. Estimat-
ing lexical priors for low-frequency morphologi-
cally ambiguous forms. Computational Linguistics,
22(2):155?166.
520
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Koby Crammer, Ryan McDonald, and Fernando
Pereira. 2005. Scalable large-margin online learn-
ing for structured classification. In NIPS Workshop
on Learning With Structured Outputs.
Koby Crammer. 2004. Online Learning of Com-
plex Categorial Problems. Hebrew Univeristy of
Jerusalem, PhD Thesis.
Zhendong Dong and Qiang Dong. 2006. Hownet and
the Computation of Meaning. World Scientific.
Kuzman Ganchev, Koby Crammer, Fernando Pereira,
Gideon Mann, Kedar Bellare, Andrew McCallum,
Steven Carroll, Yang Jin, and Peter White. 2007.
Penn/umass/chop biocreative ii systems. In Pro-
ceedings of the Second BioCreative Challenge Eval-
uation Workshop.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for chinese word segmentation and
part-of-speech tagging. In Proceedings of COLING.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP, pages 230?237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Ryan McDonald, Femando Pereira, Kiril Ribarow, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP, pages 523?530.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
University of Pennsylvania, PhD Thesis.
Masaki Nagata. 1994. A stochastic japanese mor-
phological analyzer using a forward-DP backward-
A* n-best search algorithm. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 201?207.
Masaki Nagata. 1999. A part of speech estimation
method for japanese unknown words using a statis-
tical model of morphology and context. In Proceed-
ings of ACL, pages 277?284.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sions.
Tetsuji Nakagawa. 2004. Chinese and japanese word
segmentation using word-level and character-level
information. In Proceedings of COLING, pages
466?472.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, pages 277?284.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP, pages 133?142.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of IJ-
CAI.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff. In
Proceedings of the 2nd SIGHAN Workshop on Chi-
nese Language Processing, pages 133?143.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a morpho-
logical analysis of japanese using maximum entropy
aided by a dictionary. In Proceedings of EMNLP,
pages 91?99.
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu dong Chiou, and
Shizhe Huang. 2000. Developing guidelines and
ensuring consistency for chinese text annotation. In
Proceedings of LREC.
Stavros A. Zenios Yair Censor. 1997. Parallel Op-
timization: Theory, Algorithms, and Applications.
Oxford University Press.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging on a single perceptron. In
Proceedings of ACL.
521
