Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 1?4,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Grammatical Inference and Computational Linguistics
Menno van Zaanen
Tilburg Centre for Creative Computing
Tilburg University
Tilburg, The Netherlands
mvzaanen@uvt.nl
Colin de la Higuera
University of Saint- ?Etienne
France
cdlh@univ-st-etienne.fr
1 Grammatical inference and its links to
natural language processing
When dealing with language, (machine) learning
can take many different faces, of which the most
important are those concerned with learning lan-
guages and grammars from data. Questions in
this context have been at the intersection of the
fields of inductive inference and computational
linguistics for the past fifty years. To go back
to the pioneering work, Chomsky (1955; 1957)
and Solomonoff (1960; 1964) were interested, for
very different reasons, in systems or programs that
could deduce a language when presented informa-
tion about it.
Gold (1967; 1978) proposed a little later a uni-
fying paradigm called identification in the limit,
and the term of grammatical inference seems to
have appeared in Horning?s PhD thesis (1969).
Out of the scope of linguistics, researchers and
engineers dealing with pattern recognition, under
the impulsion of Fu (1974; 1975), invented algo-
rithms and studied subclasses of languages and
grammars from the point of view of what could
or could not be learned.
Researchers in machine learning tackled related
problems (the most famous being that of infer-
ring a deterministic finite automaton, given ex-
amples and counter-examples of strings). An-
gluin (1978; 1980; 1981; 1982; 1987) introduced
the important setting of active learning, or learn-
ing for queries, whereas Pitt and his colleagues
(1988; 1989; 1993) gave several complexity in-
spired results with which the hardness of the dif-
ferent learning problems was exposed.
Researchers working in more applied areas,
such as computational biology, also deal with
strings. A number of researchers from that
field worked on learning grammars or automata
from string data (Brazma and Cerans, 1994;
Brazma, 1997; Brazma et al, 1998). Simi-
larly, stemming from computational linguistics,
one can point out the work relating language learn-
ing with more complex grammatical formalisms
(Kanazawa, 1998), the more statistical approaches
based on building language models (Goodman,
2001), or the different systems introduced to au-
tomatically build grammars from sentences (van
Zaanen, 2000; Adriaans and Vervoort, 2002). Sur-
veys of related work in specific fields can also
be found (Natarajan, 1991; Kearns and Vazirani,
1994; Sakakibara, 1997; Adriaans and van Zaa-
nen, 2004; de la Higuera, 2005; Wolf, 2006).
2 Meeting points between grammatical
inference and natural language
processing
Grammatical inference scientists belong to a num-
ber of larger communities: machine learning (with
special emphasis on inductive inference), com-
putational linguistics, pattern recognition (within
the structural and syntactic sub-group). There is
a specific conference called ICGI (International
Colloquium on Grammatical Inference) devoted
to the subject. These conferences have been held
at Alicante (Carrasco and Oncina, 1994), Mont-
pellier (Miclet and de la Higuera, 1996), Ames
(Honavar and Slutski, 1998), Lisbon (de Oliveira,
2000), Amsterdam (Adriaans et al, 2002), Athens
(Paliouras and Sakakibara, 2004), Tokyo (Sakak-
ibara et al, 2006) and Saint-Malo (Clark et al,
2008). In the proceedings of this event it is pos-
sible to find a number of technical papers. Within
this context, there has been a growing trend to-
wards problems of language learning in the field
of computational linguistics.
The formal objects in common between the
two communities are the different types of au-
tomata and grammars. Therefore, another meet-
ing point between these communities has been the
different workshops, conferences and journals that
focus on grammars and automata, for instance,
1
FSMNLP,GRAMMARS, CIAA, . . .
3 Goal for the workshop
There has been growing interest over the last few
years in learning grammars from natural language
text (and structured or semi-structured text). The
family of techniques enabling such learning is usu-
ally called ?grammatical inference? or ?grammar
induction?.
The field of grammatical inference is often sub-
divided into formal grammatical inference, where
researchers aim to proof efficient learnability of
classes of grammars, and empirical grammatical
inference, where the aim is to learn structure from
data. In this case the existence of an underlying
grammar is just regarded as a hypothesis and what
is sought is to better describe the language through
some automatically learned rules.
Both formal and empirical grammatical infer-
ence have been linked with (computational) lin-
guistics. Formal learnability of grammars has
been used in discussions on how people learn lan-
guage. Some people mention proofs of (non-
)learnability of certain classes of grammars as ar-
guments in the empiricist/nativist discussion. On
the more practical side, empirical systems that
learn grammars have been applied to natural lan-
guage. Instead of proving whether classes of
grammars can be learnt, the aim here is to pro-
vide practical learning systems that automatically
introduce structure in language. Example fields
where initial research has been done are syntac-
tic parsing, morphological analysis of words, and
bilingual modelling (or machine translation).
This workshop organized at EACL 2009 aimed
to explore the state-of-the-art in these topics. In
particular, we aimed at bringing formal and empir-
ical grammatical inference researchers closer to-
gether with researchers in the field of computa-
tional linguistics.
The topics put forward were to cover research
on all aspects of grammatical inference in rela-
tion to natural language (such as, syntax, seman-
tics, morphology, phonology, phonetics), includ-
ing, but not limited to
? Automatic grammar engineering, including,
for example,
? parser construction,
? parameter estimation,
? smoothing, . . .
? Unsupervised parsing
? Language modelling
? Transducers, for instance, for
? morphology,
? text to speech,
? automatic translation,
? transliteration,
? spelling correction, . . .
? Learning syntax with semantics,
? Unsupervised or semi-supervised learning of
linguistic knowledge,
? Learning (classes of) grammars (e.g. sub-
classes of the Chomsky Hierarchy) from lin-
guistic inputs,
? Comparing learning results in different
frameworks (e.g. membership vs. correction
queries),
? Learning linguistic structures (e.g. phonolog-
ical features, lexicon) from the acoustic sig-
nal,
? Grammars and finite state machines in ma-
chine translation,
? Learning setting of Chomskyan parameters,
? Cognitive aspects of grammar acquisition,
covering, among others,
? developmental trajectories as studied by
psycholinguists working with children,
? characteristics of child-directed speech
as they are manifested in corpora such
as CHILDES, . . .
? (Unsupervised) Computational language ac-
quisition (experimental or observational),
4 The papers
The workshop was glad to have as invited speaker
Damir ?Cavar, who presented a talk titled: On boot-
strapping of linguistic features for bootstrapping
grammars.
The papers submitted to the workshop and re-
viewed by at least three reviewers each, covered a
very wide range of problems and techniques. Ar-
ranging them into patterns was not a simple task!
There were three papers focussing on transduc-
ers:
2
? Jeroen Geertzen shows in his paper Dialogue
Act Prediction Using Stochastic Context-Free
Grammar Induction, how grammar induction
can be used in dialogue act prediction.
? In their paper (Experiments Using OSTIA for
a Language Production Task), Dana Angluin
and Leonor Becerra-Bonache build on previ-
ous work to see the transducer learning algo-
rithm OSTIA as capable of translating syn-
tax to semantics.
? In their paper titled GREAT: a finite-state
machine translation toolkit implementing a
Grammatical Inference Approach for Trans-
ducer Inference (GIATI), Jorge Gonza?lez and
Francisco Casacuberta build on a long his-
tory of GOATI learning and try to eliminate
some of the limitations of previous work.
The learning concerns finite-state transducers
from parallel corpora.
Context-free grammars of different types were
used for very different tasks:
? Alexander Clark, Remi Eyraud and Amaury
Habrard (A note on contextual binary fea-
ture grammars) propose a formal study of
a new formalism called ?CBFG?, describe
the relationship of CBFG to other standard
formalisms and its appropriateness for mod-
elling natural language.
? In their work titled Language models for con-
textual error detection and correction, Her-
man Stehouwer and Menno van Zaanen look
at spelling problems as a word prediction
problem. The prediction needs a language
model which is learnt.
? A formal study of French treebanks is made
by Marie-He?le`ne Candito, Benoit Crabbe? and
Djame? Seddah in their work: On statistical
parsing of French with supervised and semi-
supervised strategies.
? Franco M. Luque and Gabriel Infante-Lopez
study the learnability of NTS grammars with
reference to the Penn treebank in their paper
titled Upper Bounds for Unsupervised Pars-
ing with Unambiguous Non-Terminally Sep-
arated Grammars.
One paper concentrated on morphology :
? In A comparison of several learners for
Boolean partitions: implications for morpho-
logical paradigm, Katya Pertsova compares a
rote learner to three morphological paradigm
learners.
References
P. Adriaans and M. van Zaanen. 2004. Computational
grammar induction for linguists. Grammars, 7:57?
68.
P. Adriaans and M. Vervoort. 2002. The EMILE
4.1 grammar induction toolbox. In Adriaans et al
(Adriaans et al, 2002), pages 293?295.
P. Adriaans, H. Fernau, and M. van Zaannen, editors.
2002. Grammatical Inference: Algorithms and Ap-
plications, Proceedings of ICGI ?02, volume 2484
of LNAI, Berlin, Heidelberg. Springer-Verlag.
D. Angluin. 1978. On the complexity of minimum
inference of regular sets. Information and Control,
39:337?350.
D. Angluin. 1980. Inductive inference of formal lan-
guages from positive data. Information and Control,
45:117?135.
D. Angluin. 1981. A note on the number of queries
needed to identify regular languages. Information
and Control, 51:76?87.
D. Angluin. 1982. Inference of reversible languages.
Journal of the Association for Computing Machin-
ery, 29(3):741?765.
D. Angluin. 1987. Queries and concept learning. Ma-
chine Learning Journal, 2:319?342.
A. Brazma and K. Cerans. 1994. Efficient learning
of regular expressions from good examples. In AII
?94: Proceedings of the 4th International Workshop
on Analogical and Inductive Inference, pages 76?90.
Springer-Verlag.
A. Brazma, I. Jonassen, J. Vilo, and E. Ukkonen. 1998.
Pattern discovery in biosequences. In Honavar and
Slutski (Honavar and Slutski, 1998), pages 257?270.
A. Brazma, 1997. Computational learning theory and
natural learning systems, volume 4, chapter Effi-
cient learning of regular expressions from approxi-
mate examples, pages 351?366. MIT Press.
R. C. Carrasco and J. Oncina, editors. 1994. Gram-
matical Inference and Applications, Proceedings of
ICGI ?94, number 862 in LNAI, Berlin, Heidelberg.
Springer-Verlag.
N. Chomsky. 1955. The logical structure of linguis-
tic theory. Ph.D. thesis, Massachusetts Institute of
Technology.
3
N. Chomsky. 1957. Syntactic structure. Mouton.
A. Clark, F. Coste, and L. Miclet, editors. 2008.
Grammatical Inference: Algorithms and Applica-
tions, Proceedings of ICGI ?08, volume 5278 of
LNCS. Springer-Verlag.
C. de la Higuera. 2005. A bibliographical study
of grammatical inference. Pattern Recognition,
38:1332?1348.
A. L. de Oliveira, editor. 2000. Grammatical Infer-
ence: Algorithms and Applications, Proceedings of
ICGI ?00, volume 1891 of LNAI, Berlin, Heidelberg.
Springer-Verlag.
K. S. Fu and T. L. Booth. 1975. Grammatical infer-
ence: Introduction and survey. Part I and II. IEEE
Transactions on Syst. Man. and Cybern., 5:59?72
and 409?423.
K. S. Fu. 1974. Syntactic Methods in Pattern Recogni-
tion. Academic Press, New-York.
E. M. Gold. 1967. Language identification in the limit.
Information and Control, 10(5):447?474.
E. M. Gold. 1978. Complexity of automaton identi-
fication from given data. Information and Control,
37:302?320.
J. Goodman. 2001. A bit of progress in language mod-
eling. Technical report, Microsoft Research.
V. Honavar and G. Slutski, editors. 1998. Gram-
matical Inference, Proceedings of ICGI ?98, number
1433 in LNAI, Berlin, Heidelberg. Springer-Verlag.
J. J. Horning. 1969. A study of Grammatical Inference.
Ph.D. thesis, Stanford University.
M. Kanazawa. 1998. Learnable Classes of Categorial
Grammars. CSLI Publications, Stanford, Ca.
M. J. Kearns and U. Vazirani. 1994. An Introduction
to Computational Learning Theory. MIT press.
L. Miclet and C. de la Higuera, editors. 1996. Pro-
ceedings of ICGI ?96, number 1147 in LNAI, Berlin,
Heidelberg. Springer-Verlag.
B. L. Natarajan. 1991. Machine Learning: a Theoret-
ical Approach. Morgan Kauffman Pub., San Mateo,
CA.
G. Paliouras and Y. Sakakibara, editors. 2004. Gram-
matical Inference: Algorithms and Applications,
Proceedings of ICGI ?04, volume 3264 of LNAI,
Berlin, Heidelberg. Springer-Verlag.
L. Pitt and M. Warmuth. 1988. Reductions among
prediction problems: on the difficulty of predicting
automata. In 3rd Conference on Structure in Com-
plexity Theory, pages 60?69.
L. Pitt and M. Warmuth. 1993. The minimum consis-
tent DFA problem cannot be approximated within
any polynomial. Journal of the Association for
Computing Machinery, 40(1):95?142.
L. Pitt. 1989. Inductive inference, DFA?s, and com-
putational complexity. In Analogical and Induc-
tive Inference, number 397 in LNAI, pages 18?44.
Springer-Verlag, Berlin, Heidelberg.
Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino, and
E. Tomita, editors. 2006. Grammatical Infer-
ence: Algorithms and Applications, Proceedings of
ICGI ?06, volume 4201 of LNAI, Berlin, Heidelberg.
Springer-Verlag.
Y. Sakakibara. 1997. Recent advances of grammatical
inference. Theoretical Computer Science, 185:15?
45.
R. Solomonoff. 1960. A preliminary report on a gen-
eral theory of inductive inference. Technical Report
ZTB-138, Zator Company, Cambridge, Mass.
R. Solomonoff. 1964. A formal theory of inductive
inference. Information and Control, 7(1):1?22 and
224?254.
M. van Zaanen. 2000. ABL: Alignment-based learn-
ing. In Proceedings of COLING 2000, pages 961?
967. Morgan Kaufmann.
G. Wolf. 2006. Unifying computing and cognition.
Cognition research.
4
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1353?1362,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
PCFG Induction for Unsupervised Parsing and Language Modelling
James Scicluna
Universit?e de Nantes,
CNRS, LINA, UMR6241,
F-44000, France
james.scicluna@univ-nantes.fr
Colin de la Higuera
Universit?e de Nantes,
CNRS, LINA, UMR6241,
F-44000, France
cdlh@univ-nantes.fr
Abstract
The task of unsupervised induction of
probabilistic context-free grammars
(PCFGs) has attracted a lot of attention
in the field of computational linguistics.
Although it is a difficult task, work in this
area is still very much in demand since
it can contribute to the advancement of
language parsing and modelling. In this
work, we describe a new algorithm for
PCFG induction based on a principled
approach and capable of inducing accurate
yet compact artificial natural language
grammars and typical context-free gram-
mars. Moreover, this algorithm can work
on large grammars and datasets and infers
correctly even from small samples. Our
analysis shows that the type of grammars
induced by our algorithm are, in theory,
capable of modelling natural language.
One of our experiments shows that our
algorithm can potentially outperform the
state-of-the-art in unsupervised parsing on
the WSJ10 corpus.
1 Introduction
The task of unsupervised induction of PCFGs has
attracted a lot of attention in the field of compu-
tational linguistics. This task can take the form
of either parameter search or structure learning.
In parameter search, a CFG is fixed and the fo-
cus is on assigning probabilities to this grammar
using Bayesian methods (Johnson et al., 2007) or
maximum likelihood estimation (Lari and Young,
1990). In structure learning, the focus is on build-
ing the right grammar rules from scratch. We take
the latter approach.
Unsupervised structure learning of (P)CFGs is
a notoriously difficult task (de la Higuera, 2010;
Clark and Lappin, 2010), with theoretical results
showing that in general it is either impossible
to achieve (Gold, 1967; de la Higuera, 1997)
or requires impractical resources (Horning, 1969;
Yang, 2012). At the same time, it is well known
that context-free structures are needed for better
language parsing and modelling, since less expres-
sive models (such as HMMs) are not good enough
(Manning and Sch?utze, 2001; Jurafsky and Mar-
tin, 2008). Moreover, the trend is towards unsu-
pervised (rather than supervised) learning meth-
ods due to the lack in most languages of annotated
data and the applicability in wider domains (Merlo
et al., 2010). Thus, despite its difficulty, unsuper-
vised PCFG grammar induction (or induction of
other similarly expressive models) is still an im-
portant task in computational linguistics.
In this paper, we describe a new algorithm for
PCFG induction based on a principled approach
and capable of inducing accurate yet compact
grammars. Moreover, this algorithm can work on
large grammars and datasets and infers correctly
even from small samples. We show that our algo-
rithm is capable of achieving competitive results
in both unsupervised parsing and language mod-
elling of typical context-free languages and arti-
ficial natural language grammars. We also show
that the type of grammars we propose to learn are,
in theory, capable of modelling natural language.
2 Preliminaries
2.1 Grammars and Languages
A context-free grammar (CFG) is a 4-tuple
?N,?, P, I?, where N is the set of non-terminals,
? the set of terminals, P the set of production rules
and I a set of starting non-terminals (i.e. multi-
ple starting non-terminals are possible). The lan-
guage generated from a particular non-terminal A
is L(A) = {w|A
?
? w} and the language gen-
erated by a grammar G is L(G) =
?
S?I
L(S).
A CFG is in Chomsky Normal Form (CNF) if ev-
1353
ery production rule is of the form N ? NN or
N ? ?.
A probabilistic context-free grammar (PCFG)
is a CFG with a probability value assigned to every
rule and every starting non-terminal. The prob-
ability of a leftmost derivation from a PCFG is
the product of the starting non-terminal probabil-
ity and the production probabilities used in the
derivation. The probability of a string generated
by a PCFG is the sum of all its leftmost deriva-
tions? probabilities. The stochastic language gen-
erated from a PCFG G is (L(G), ?
G
), where ?
G
is the distribution over ?
?
defined by the probabil-
ities assigned to the strings by G. For a PCFG to
be consistent, the probabilities of the strings in its
stochastic language must add up to 1 (Wetherell,
1980). Any PCFG mentioned from now onwards
is assumed to be consistent.
2.2 Congruence Relations
A congruence relation? on ?
?
is any equivalence
relation on ?
?
that respects the following condi-
tion: if u ? v and x ? y then ux ? vy. The con-
gruence classes of a congruence relation are sim-
ply its equivalence classes. The congruence class
of w ? ?
?
w.r.t. a congruence relation ? is de-
noted by [w]
?
. The set of contexts of a substringw
with respect to a language L, denoted Con(w,L),
is {(l, r) ? ?
?
? ?
?
| lwr ? L}. Two strings u
and v are syntactically congruent with respect to
L, written u ?
L
v, if Con(u, L) = Con(v, L).
This is a congruence relation on ?
?
. The con-
text distribution of a substring w w.r.t. a stochastic
language (L, ?), denoted C
(L,?)
w
, is a distribution
whose support is all the possible contexts over al-
phabet ? (i.e. ?
?
? ?
?
) and is defined as follows:
C
(L,?)
w
(l, r) =
?(lwr)
?
l
?
,r
?
??
?
?(l
?
wr
?
)
Two strings u and v are stochastically congru-
ent with respect to (L, ?), written u
?
=
(L,?)
v, if
C
(L,?)
u
is equal to C
(L,?)
v
. This is a congruence
relation on ?
?
.
2.3 Congruential Grammars
Clark (2010a) defines Congruential CFGs (C-
CFGs) as being all the CFGs G which, for any
non-terminal A, if u ? L(A) then L(A) ?
[u]
?
L(G)
(where [u]
?
L(G)
is the syntactic congru-
ence class of u w.r.t. the language of G). This
class of grammars was defined with learnability
in mind. Since these grammars have a direct
relationship between congruence classes and the
non-terminals, their learnability is reduced to that
of finding the correct congruence classes (Clark,
2010a).
This class of grammars is closely related
to the class of NTS-grammars (Boasson and
S?enizergues, 1985). Any C-CFG is an NTS-
grammar but not vice-versa. However, it is not
known whether languages generated by C-CFGs
are all NTS-languages (Clark, 2010a). Note that
NTS-languages are a subclass of deterministic
context-free languages and contain the regular
languages, the substitutable (Clark and Eyraud,
2007) and k-l-substitutable context-free languages
(Yoshinaka, 2008), the very simple languages and
other CFLs such as the Dyck language (Boasson
and S?enizergues, 1985).
We define a slightly more restrictive class of
grammars, which we shall call Strongly Congru-
ential CFGs (SC-CFGs). A CFG G is a SC-
CFG if, for any non-terminal A, if u ? L(A)
then L(A) = [u]
?
L(G)
. The probabilistic equiv-
alent of this is the class of Strongly Congruential
PCFGs (SC-PCFGs), defined as all the PCFGs G
which, for any non-terminal A, if u ? L(A) then
L(A) = [u]
?
=
(L(G),?)
. In other words, the non-
terminals (i.e. syntactic categories in natural lan-
guage) of these grammars directly correspond to
classes of substitutable strings (i.e. substitutable
words and phrases in NL). One may ask whether
this is too strict a restriction for natural language
grammars. We argue that it is not, for the follow-
ing reasons.
First of all, this restriction complies with the ap-
proach taken by American structural linguists for
the identification of syntactic categories, as shown
by Rauh (2010): ?[Zellig and Fries] identified
syntactic categories as distribution classes, em-
ploying substitution tests and excluding semantic
properties of the items analysed. Both describe
syntactic categories exclusively on the basis of
their syntactic environments and independently of
any inherent properties of the members of these
categories?.
Secondly, we know that such grammars are ca-
pable of describing languages generated by gram-
mars that contain typical natural language gram-
matical structures (see Section 4.1; artificial natu-
ral language grammars NL1-NL7, taken from var-
ious sources, generate languages which can be de-
scribed by SC-PCFGs).
1354
3 Algorithm
COMINO (our algorithm) induces SC-PCFGs
from a positive sample S. The steps involved are:
1. Inducing the stochastically congruent classes
of all the substrings of S
2. Selecting which of the induced classes are
non-terminals and subsequently building a
CFG.
3. Assigning probabilities to the induced CFG.
The approach we take is very different from tra-
ditional grammar induction approaches, in which
grouping of substitutable substrings is done incre-
mentally as the same groups are chosen to rep-
resent non-terminals. We separate these two task
so that learning takes place in the grouping phase
whilst selection of non-terminals is done indepen-
dently by solving a combinatorial problem.
For the last step, the standard EM-algorithm for
PCFGs (Lari and Young, 1990) is used. In Sec-
tions 3.1 and 3.2, the first and second steps of the
algorithm are described in detail. We analyse our
algorithm in Section 3.3.
3.1 Inducing the Congruence Classes
We describe in Algorithm 1 how the congruence
classes are induced.
Algorithm 1: Learn Congruence Classes
Input: A multiset S; parameters: n, d, i;
distance function dist on local
contexts of size k
Output: The congruence classes CC over the
substrings of S
1 Subs? Set of all substrings of S ;
2 CC ? {{w} | w ? Subs} ;
3 while True do
4 Pairs? {(x, y) | x, y ? CC, x 6= y,
|S|
x
? n , |S|
y
? n} ;
5 if |Pairs| = 0 then exitloop ;
6 Order Pairs based on dist
k
;
7 (x, y)? Pairs[0] ;
8 init = {[w]
CC
| w ? S} ;
9 if dist
k
(x, y) ? d and |init| ? i then
exitloop ;
10 CC ? Merge(x, y, CC) ;
11 end
12 return CC ;
At the beginning, each substring (or phrase for
natural language) in the sample is assigned its own
congruence class (line 2). Then, pairs of frequent
congruence classes are merged together depend-
ing on the distance between their empirical con-
text distribution, which is calculated on local con-
texts. The following points explain each keyword:
? The empirical context distribution of a sub-
string w is simply a probability distribution
over all the contexts of w, where the prob-
ability for a context (l, r) is the number of
occurrences of lwr in the sample divided by
the number of occurrences of w. This is ex-
tended to congruence classes by treating each
substring in the class as one substring (i.e. the
sum of occurrences of lw
i
r, for all w
i
in the
class, divided by the sum of occurrences of
all w
i
).
? Due to the problem of sparsity with contexts
(in any reasonably sized corpus of natural
language, very few phrases will have more
than one occurrence of the same context),
only local contexts are considered. The lo-
cal contexts of w are the pairs of first k sym-
bols (or words for natural language) preced-
ing and following w. The lower k is, the less
sparsity is a problem, but the empirical con-
text distribution is less accurate. For natural
language corpora, k is normally set to 1 or 2.
? A frequent congruence class is one whose
substring occurrences in the sample add up
to more than a pre-defined threshold n. In-
frequent congruence classes are ignored due
to their unreliable empirical context dis-
tribution. However, as more merges are
made, more substrings are added to infre-
quent classes, thus increasing their frequency
and eventually they might be considered as
frequent classes.
? A distance function dist between samples
of distributions over contexts is needed by
the algorithm to decide which is the closest
pair of congruence classes, so that they are
merged together. We used L1-Distance and
Pearson?s chi-squared test for experiments in
Sections 4.1 and 4.2 respectively.
? After each merge, other merges are logically
deduced so as to ensure that the relation re-
1355
mains a congruence
1
. In practice, the vast
majority of the merges undertaken are logi-
cally deduced ones. This clearly relieves the
algorithm from taking unnecessary decisions
(thus reducing the chance of erroneous de-
cisions). On the downside, one bad merge
can have a disastrous ripple effect. Thus, to
minimize as much as possible the chance of
this happening, every merge undertaken is the
best possible one at that point in time (w.r.t.
the distance function used). The same idea is
used in DFA learning (Lang et al., 1998).
This process is repeated until either 1) no pairs
of frequent congruence classes are left to merge
(line 5) or 2) the smallest distance between the
candidate pairs is bigger or equal to a pre-defined
threshold d and the number of congruence classes
containing strings from the sample is smaller or
equal to a pre-defined threshold i (line 9).
The first condition of point 2 ensures that con-
gruence classes which are sufficiently close to
each other are merged together. The second con-
dition of point 2 ensures that the hypothesized
congruence classes are generalized enough (i.e. to
avoid undergeneralization). For natural language
examples, one would expect that a considerable
number of sentences are grouped into the same
class because of their similar structure. Obviously,
one can make use of only one of these conditions
by assigning the other a parameter value which
makes it trivially true from the outset (0 for d and
|Subs| for i).
3.2 Building the Context-Free Grammar
Deciding which substrings are constituents (in our
case, this translates into choosing which congru-
ence classes correspond to non-terminals) is a
problematic issue and is considered a harder task
than the previous step (Klein, 2004). A path fol-
lowed by a number of authors consists in using an
Ockham?s razor or Minimal Description Length
principle approach (Stolcke, 1994; Clark, 2001;
Petasis et al., 2004). This generally leads to choos-
ing as best hypothesis the one which best com-
presses the data. Applying this principle in our
case would mean that the non-terminals should be
1
for example, if a congruence class contains the phrases
?the big? and ?that small?, and another class contains ?dog
barked? and ?cat meowed?, it can be logically deduced that
the phrases ?the big dog barked?,?the big cat meowed?, ?that
small dog barked? and ?that small cat meowed? should be in
the same class.
assigned in such a way that the grammar built is
the smallest possible one (in terms of the number
of non-terminals and/or production rules) consis-
tent with the congruence classes. To our knowl-
edge, only local greedy search is used by systems
in the literature which try to follow this approach.
We propose a new method for tackling this
problem. We show that all the possible SC-CFGs
in CNF consistent with the congruence classes di-
rectly correspond to all the solutions of a boolean
formula built upon the congruence classes, where
the variables of this formula correspond to non-
terminals (and, with some minor adjustments, pro-
duction rules as well). Thus, finding the smallest
possible grammar directly translates into finding a
solution which has the smallest possible amount
of true variables. Finding a minimal solution for
this type of formula is a known NP-Hard problem
(Khanna et al., 2000). However, sophisticated lin-
ear programming solvers (Berkelaar et al., 2008)
can take care of this problem. For small examples
(e.g. all the examples in Table 1), these solvers
are able to find an exact solution in a few sec-
onds. Moreover, these solvers are capable of find-
ing good approximate solutions to larger formulas
containing a few million variables.
The formula contains one variable per congru-
ence class. All variables corresponding to congru-
ence classes containing strings from the sample
are assigned the value True (since there must be a
starting non-terminal that generates these strings).
All variables corresponding to congruence classes
containing symbols from ? are assigned the value
True (since for every a ? ?, there must be a rule
A ? a). Finally, and most importantly, for every
congruence class [w] and for every string w in [w]
(|w| = n), the following conditional statement is
added to the formula:
v(w) ? (v(w
1,1
) ? v(w
2,n
)) ? (v(w
1,2
) ?
v(w
3,n
)) ? . . . ? (v(w
1,n?1
) ? v(w
n,n
))
where v(x) is the variable corresponding to the
congruence class [x] and w
i,j
is the substring of w
from the i
th
to the j
th
symbol ofw. This statement
is representing the fact that if a congruence class
[w] is chosen as a non-terminal then for each string
in w ? [w], there must be at least one CNF rule
A ? BC that generates w and thus there must
be at least one division of w into w
1,k
w
k+1,n
such
that B corresponds to [w
1,k
] and C corresponds to
[w
k+1,n
].
The grammar extracted from the solution of this
1356
formula is made up of all the possible CNF pro-
duction rules built from the chosen non-terminals.
The starting non-terminals are those which corre-
spond to congruence classes that contain at least
one string from the sample.
The following is a run of the whole process on
a simple example:
Sample {ab, aabb, aaabbb}
Congruence Classes
1 : [a], 2 : [b], 3 : [ab, aabb, aaabbb], 4 : [aa],
5 : [bb], 6 : [aab, aaabb], 7 : [abb, aabbb],
8 : [aaa], 9 : [bbb], 10 : [aaab], 11 : [abbb]
Boolean Formula
There is one conditional statement per sub-
string. For example,X
6
? (X
1
?X
3
)?(X
4
?
X
2
) represents the two possible ways aab in
congruence class 6 can be split (a|ab , aa|b).
Variables X
1
, X
2
and X
3
are true.
X
3
? (X
1
?X
2
)
X
3
? (X
1
?X
7
) ? (X
4
?X
5
) ? (X
6
?X
2
)
X
3
? (X
1
?X
7
)? (X
4
?X
11
)? (X
8
?X
9
)?
(X
10
?X
5
) ? (X
6
?X
2
)
X
4
? (X
1
?X
1
)
X
5
? (X
2
?X
2
)
X
6
? (X
1
?X
3
) ? (X
4
?X
2
)
X
6
? (X
1
?X
3
)? (X
4
?X
7
)? (X
8
?X
5
)?
(X
10
?X
2
)
X
7
? (X
1
?X
5
) ? (X
3
?X
2
)
X
7
? (X
1
?X
11
)? (X
4
?X
9
)? (X
6
?X
5
)?
(X
3
?X
2
)
X
8
? (X
1
?X
4
) ? (X
4
?X
1
)
X
9
? (X
2
?X
5
) ? (X
5
?X
2
)
X
10
? (X
1
?X
6
)? (X
4
?X
3
)? (X
8
?X
2
)
X
11
? (X
1
?X
9
)? (X
3
?X
5
)? (X
7
?X
2
)
Solution
Running the solver on this formula will re-
turn the following true variables that make up
a minimal solution: X
1
, X
2
, X
3
and X
7
.
Grammar
For every statement x? . . .? (y? z)? . . .
where x,y and z are true, a production rule
x ? yz is added. So, the following grammar
is built:
X
3
is the starting non-terminal
X
3
? X
1
X
7
|X
1
X
2
X
7
? X
3
X
2
X
1
? a X
2
? b
3.3 Analysis
In the first phase of the algorithm, we are group-
ing all the substrings of the sample S according to
the congruence relation
?
=
(L,?)
, where (L, ?) is the
target stochastic language (for natural language,
this is the language model). To do so, we are as-
suming that S was i.i.d. generated from (L, ?).
In the second phase, we are representing the space
of all CFGs consistent with the classes obtained
in phase one as different solutions to a boolean
formula. Here we introduce our bias in favour of
smaller grammars by finding a minimal solution to
the formula. In the last phase, probabilities are as-
signed to the grammar obtained in phase two using
the standard MLE algorithm for PCFGs.
Unlike many other systems, in our case the hy-
pothesis space of grammars is well-defined. This
allows us to analyse our algorithm in a theoreti-
cal framework and obtain theoretical learnability
results. Moreover, this gives us an idea on the
types of syntactical features our system is capable
of learning.
Assuming our algorithm always takes correct
merge decisions, the sample required for identifi-
cation needs only to be structurally complete w.r.t.
the target grammar (i.e. every production rules is
used at least once in the generation of the sample).
This means that, in theory, our algorithm can work
with very small samples (polynomial size w.r.t. the
number of rules in the target grammar).
Some approaches in the literature assume that
whenever a particular substring is a constituent
in some sentence, then it is automatically a con-
stituent in all other sentences (whenever it does not
overlap with previously chosen constituents) (van
Zaanen, 2001; Clark, 2001; Adriaans et al., 2000).
In reality, this is clearly not the case. A simple
experiment on the WSJ10 corpus reveals that only
16 of the most frequent 1009 POS sequences (oc-
curring 10 or more times in the sample) which are
at least once constituents, are in fact always con-
stituents. This assumption does not hold for am-
biguous grammars in our class.
The approach we take to solve the smallest
grammar problem can be extended to other classes
of grammars. A similar formula can be built for
grammars whose non-terminals have a one-to-one
correspondence with congruence classes contain-
ing features of their language (Clark, 2010b).
1357
4 Experiments and Discussion
4.1 Experiments on Artificial Data
We tested our system on 11 typical context-free
languages and 9 artificial natural language gram-
mars taken from 4 different sources (Stolcke,
1994; Langley and Stromsten, 2000; Adriaans
et al., 2000; Solan et al., 2005). The 11 CFLs in-
clude 7 described by unambiguous grammars:
UC1: a
n
b
n
UC2: a
n
b
n
c
m
d
m
UC3: a
n
b
m
n ? m
UC4: a
p
b
q
, p 6= q UC5: Palindromes over alpha-
bet {a, b}with a central marker UC6:Palindromes
over alphabet {a, b} without a central marker
UC7: Lukasiewicz language (S ? aSS|b)
and 4 described by ambiguous grammars:
AC1: |w|
a
= |w|
b
AC2: 2|w|
a
= |w|
b
AC3: Dyck
language AC4: Regular expressions.
The 9 artificial natural language grammars are:
NL1: Grammar ?a?, Table 2 in (Langley and
Stromsten, 2000) NL2: Grammar ?b?, Table 2
in (Langley and Stromsten, 2000) NL3: Lexical
categories and constituency, pg 96 in (Stolcke,
1994) NL4:Recursive embedding of constituents,
pg 97 in (Stolcke, 1994) NL5: Agreement, pg
98 in (Stolcke, 1994) NL6: Singular/plural NPs
and number agreement, pg 99 in (Stolcke, 1994)
NL7: Experiment 3.1 grammar in (Adriaans et al.,
2000) NL8:Grammar in Table 10 (Adriaans et al.,
2000) NL9: TA1 grammar in (Solan et al., 2005).
The quality of the learned model depends on
its capacity to predict the correct structure (parse
trees) on the one hand and to predict the correct
sentence probabilities on the other (i.e. assigns
a probability distribution close to the target one).
To evaluate parse trees, we follow suggestions
given by van Zaanen and Geertzen (2008) and use
micro-precision and micro-recall over all the non-
trivial brackets. We take the harmonic mean of
these two values to obtain the Unlabelled brack-
ets F
1
score (UF
1
). The learned distribution can
be evaluated using perplexity (when the target dis-
tribution is not known) or some similarity metric
between distributions (when the target distribution
is known). In our case, the target distribution is
Ex. |?| |N | |P |
UC1 2 3 4
UC2 4 7 9
UC3 2 3 5
UC4 2 5 9
UC5 2 3 5
UC6 2 3 8
UC7 2 2 3
AC1 2 4 9
AC2 2 5 11
AC3 2 3 5
AC4 7 8 13
NL1 9 8 15
NL2 8 8 13
NL3 12 10 18
NL4 13 11 22
NL5 16 12 23
NL6 19 17 32
NL7 12 3 9
NL8 30 10 35
NL9 50 45 81
Table 1: Size of the alphabet, number of non-
terminals and productions rules of the grammars.
Relative Entropy UF
1
Ex. |S| COMINO ADIOS COMINO ABL
UC1 10 0.029 1.876 100 100
UC2 50 0.0 1.799 100 100
UC5 10 0.111 7.706 100 100
UC7 10 0.014 1.257 100 27.86
AC1 50 0.014 4.526 52.36 35.51
AC2 50 0.098 6.139 46.95 14.25
AC3 50 0.057 1.934 99.74 47.48
AC4 100 0.124 1.727 83.63 14.58
NL7 100 0.0 0.124 100 100
NL1 100 0.202 1.646 24.08 24.38
NL2 200 0.333 0.963 45.90 45.80
NL3 100 0.227 1.491 36.34 75.95
NL5 100 0.111 1.692 88.15 79.16
NL6 400 0.227 0.138 36.28 100
UC3 100 0.411 0.864 61.13 100
UC4 100 0.872 2.480 42.84 100
UC6 100 1.449 1.0 20.14 8.36
NL4 500 1.886 2.918 65.88 52.87
NL8 1000 1.496 1.531 57.77 50.04
NL9 800 1.701 1.227 12.49 28.53
Table 2: Relative Entropy and UF
1
results of our
system COMINO vs ADIOS and ABL respec-
tively. Best results are highlighted, close results
(i.e. with a difference of at most 0.1 for relative
entropy and 1% for UF
1
) are both highlighted
1358
known. We chose relative entropy
2
as a good mea-
sure of distance between distributions.
Our UF
1
results over test sets of one thousand
strings were compared to results obtained by ABL
(van Zaanen, 2001), which is a system whose
primary aim is that of finding good parse trees
(rather than identifying the target language). Al-
though ABL does not obtain state-of-the-art re-
sults on natural language corpora, it proved to be
the best system (for which an implementation is
readily available) for unsupervised parsing of sen-
tences generated by artificial grammars. Results
are shown in Table 1.
We calculated the relative entropy on a test set
of one million strings generated from the target
grammar. We compared our results with ADIOS
(Solan et al., 2005), a system which obtains com-
petitive results on language modelling (Waterfall
et al., 2010) and whose primary aim is of correctly
identifying the target language (rather than finding
good parse trees). Results are shown in Table 1.
For the tests in the first section of Table 1 (i.e.
above the first dashed line), our algorithm was ca-
pable of exactly identifying the structure of the tar-
get grammar. Notwithstanding this, the bracketing
results for these tests did not always yield perfect
scores. This happened whenever the target gram-
mar was ambiguous, in which case the most prob-
able parse trees of the target and learned grammar
can be different, thus leading to incorrect bracket-
ing. For the tests in the second section of Table 1
(i.e. between the two dashed lines), our algorithm
was capable of exactly identifying the target lan-
guage (but not the grammar). In all of these cases,
the induced grammar was slightly smaller than the
target one. For the remaining tests, our algorithm
did not identify the target language. In fact, it al-
ways overgeneralised. The 3 typical CFLs UC3,
UC4 and UC6 are not identified because they are
not contained in our subclass of CFLs. Inspite of
this, the relative entropy results obtained are still
relatively good. Overall, it is fair to say that the
results obtained by our system, for both language
modelling and unsupervised parsing on artificial
data, are competitive with the results obtained by
other methods.
2
The relative entropy (or Kullback-Leibler divergence)
between a target distribution D and a hypothesized distri-
bution D
?
is defined as
?
w??
?
ln
(
D(w)
D
?
(w)
)
D(w). Add-one
smoothing is used to solve the problem of zero probabilities.
4.2 Natural Language Experiments
We also experimented on natural language cor-
pora. For unsupervised parsing, we tested our
system on the WSJ10 corpus, using POS tagged
sentences as input. Due to time efficiency, we
changed the algorithm for finding congruence
classes. Instead of always choosing the best pos-
sible merge w.r.t. the distance function, a distance
threshold is set and all congruence classes whose
distance is smaller than the threshold are merged.
Also, we changed the distance function from L1-
Distance to Pearson?s ?
2
test.
In a first experiment (vaguely similar to the one
done by Luque and L?opez (2010)), we constructed
the best possible SC-CFG consistent with the
merges done in the first phase and assigned prob-
abilities to this grammar using Inside-Outside.
In other words, we ran the second phase of
our system in a supervised fashion by using the
treebank to decide which are the best congru-
ence classes to choose as non-terminals. The
CNF grammar we obtained from this experiment
(COMINO-UBOUND) gives very good parsing
results which outperform results from state-of-the-
art systems DMV+CCM (Klein, 2004), U-DOP
(Bod, 2006a), UML-DOP (Bod, 2006b) and In-
cremental (Seginer, 2007) as shown in Table 2.
Moreover, the results obtained are very close to
the best results one can ever hope to obtain from
any CNF grammar on WSJ10 (CNF-UBOUND)
(Klein, 2004). However, the grammar we obtain
does not generalise enough and does not describe a
good language model. In a second experiment, we
ran the complete COMINO system. The grammar
obtained from this experiment did not give com-
petitive parsing results.
The first experiment shows that the merge deci-
sions taken in the first phase do not hinder the pos-
sibility of finding a very good grammar for pars-
ing. This means that the merge decisions taken
by our system are good in general. Manual anal-
ysis on some of the merges taken confirms this.
This experiment also shows that there exists a non-
trivial PCFG in our restrictive class of grammars
that is capable of achieving very good parsing re-
sults. This is a positive sign for the question of
how adequate SC-PCFGs are for modelling natu-
ral languages. However, the real test remains that
of finding SC-PCFGs that generate good bracket-
ings and good language models. The second ex-
periment shows that the second phase of our al-
1359
Model UP UR UF
1
State-of-the-art
DMV+CCM 69.3 88.0 77.6
U-DOP 70.8 88.2 78.5
UML-DOP - - 82.9
Incremental 75.6 76.2 75.9
Upper bounds
COMINO-UBOUND 75.8 96.9 85.1
CNF-UBOUND 78.8 100.0 88.1
Table 3: Parsing results on WSJ10. Note that In-
cremental is the only system listed as state-of-the-
art which parses from plain text and can generate
non-binary trees
gorithm is not giving good results. This means
that the smallest possible grammar might not be
the best grammar for parsing. Therefore, other cri-
teria alongside the grammar size are needed when
choosing a grammar consistent with the merges.
4.3 Discussion and Future Work
In order to improve our system, we think that our
algorithm has to take a less conservative merging
strategy in the first phase. Although the merges
being taken are mostly correct, our analysis shows
that not enough merging is being done. The prob-
lematic case is that of taking merge decisions on
(the many) infrequent long phrases. Although
many logically deduced merges involve infrequent
phrases and also help in increasing the frequency
of some long phrases, this proved to be not enough
to mitigate this problem. As for future work, we
think that clustering techniques can be used to help
solve this problem.
A problem faced by the system is that, in cer-
tain cases, the statistical evidence on which merge
decisions are taken does not point to the intuitively
expected merges. As an example, consider the two
POS sequences ?DT NN? and ?DT JJ NN? in the
WSJ corpus. Any linguist would agree that these
sequences are substitutable (in fact, they have lots
of local contexts in common). However, statisti-
cal evidence points otherwise, since their context
distributions are not close enough. This happens
because, in certain positions of a sentence, ?DT
NN? is far more likely to occur than ?DT JJ NN?
(w.r.t. the ratio of their total frequencies) and in
other positions, ?DT JJ NN? occurs more than ex-
pected. The following table shows the frequencies
of these two POS sequences over the whole WSJ
corpus and their frequencies in contexts (#,VBD)
and (IN,#) (the symbol # represents the end or
beginning of a sentence):
Totals (#,VBD) (IN,#)
?DT NN? 42,222 1,034 2,123
?DT JJ NN? 15,243 152 1,119
Ratios 3.16 6.80 1.90
It is clear that the ratios do not match, thus lead-
ing to context distributions which are not close
enough. Thus, this shows that basic sequences
such as ?DT NN? and ?DT JJ NN?, which lin-
guists would group into the same concept NP, are
statistically derived from different sub-concepts of
NP. Our algorithm is finding these sub-concepts,
but it is being evaluated on concepts (such as NP)
found in the treebank (created by linguists).
From the experiments we did on artificial nat-
ural language grammars, it resulted that the tar-
get grammar was always slightly bigger than the
learned grammar. Although in these cases we still
managed to identify the target language or have
a good relative entropy result, the bracketing re-
sults were in general not good. This and our sec-
ond experiment on the WSJ10 corpus show that
the smallest possible grammar might not be the
best grammar for bracketing. To not rely solely on
finding the smallest grammar, a bias can be added
in favour of congruence classes which, according
to constituency tests (like the Mutual Information
criterion in Clark (2001)), are more likely to con-
tain substrings that are constituents. This can be
done by giving different weights to the congruence
class variables in the formula and finding the so-
lution with the smallest sum of weights of its true
variables.
The use of POS tags as input can also have its
problems. Although we solve the lexical spar-
sity problem with POS tags, at the same time we
lose a lot of information. In certain cases, one
POS sequence can include raw phrases which ide-
ally are not grouped into the same congruence
class. To mitigate this problem, we can use POS
tags only for rare words and subdivide or ignore
POS tags for frequent words such as determinants
and prepositions. This will reduce the number of
raw phrases represented by POS sequences whilst
keeping lexical sparsity low.
1360
5 Conclusion
We defined a new class of PCFGs that adequately
models natural language syntax. We described a
learning algorithm for this class which scales well
to large examples and is even capable of learning
from small samples. The grammars induced by
this algorithm are compact and perform well on
unsupervised parsing and language modelling of
typical CFLs and artificial natural language gram-
mars.
Acknowledgements
The authors acknowledge partial support by the
R?egion des Pays de la Loire.
References
Pieter W. Adriaans, Marten Trautwein, and Marco
Vervoort. Towards High Speed Grammar Induc-
tion on Large Text Corpora. In V?aclav Hlav?ac,
Keith G. Jeffery, and Jir?? Wiedermann, edi-
tors, SOFSEM, volume 1963 of Lecture Notes
in Computer Science, pages 173?186. Springer,
2000.
Michel Berkelaar et al. lpSolve: Interface to Lp
solve v. 5.5 to solve linear/integer programs. R
package version, 5(4), 2008.
Luc Boasson and G?eraud S?enizergues. NTS Lan-
guages Are Deterministic and Congruential. J.
Comput. Syst. Sci., 31(3):332?342, 1985.
Rens Bod. Unsupervised Parsing with U-DOP.
In Proceedings of the Tenth Conference on
Computational Natural Language Learning,
CoNLL-X ?06, pages 85?92, Stroudsburg, PA,
USA, 2006a. Association for Computational
Linguistics.
Rens Bod. An All-Subtrees Approach to Unsu-
pervised Parsing. In Proceedings of the 21st In-
ternational Conference on Computational Lin-
guistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages
865?872. Association for Computational Lin-
guistics, 2006b.
Alexander Clark. Unsupervised Language Acqui-
sition: Theory and Practice. PhD thesis, Uni-
versity of Sussex, 2001.
Alexander Clark. Distributional Learning of Some
Context-Free Languages with a Minimally Ad-
equate Teacher. In Sempere and Garc??a (2010),
pages 24?37.
Alexander Clark. Towards General Algorithms
for Grammatical Inference. In Marcus Hut-
ter, Frank Stephan, Vladimir Vovk, and Thomas
Zeugmann, editors, ALT, volume 6331 of Lec-
ture Notes in Computer Science, pages 11?30.
Springer, 2010b.
Alexander Clark and R?emi Eyraud. Polyno-
mial Identification in the Limit of Substitutable
Context-free Languages. Journal of Machine
Learning Research, 8:1725?1745, 2007.
Alexander Clark and Shalom Lappin. Unsuper-
vised Learning and Grammar Induction. In
Alexander Clark, Chris Fox, and Shalom Lap-
pin, editors, The Handbook of Computational
Linguistics and Natural Language Processing,
pages 197?220. Wiley-Blackwell, 2010.
Alexander Clark, Franc?ois Coste, and Laurent
Miclet, editors. Grammatical Inference: Al-
gorithms and Applications, 9th International
Colloquium, ICGI 2008, Saint-Malo, France,
September 22-24, 2008, Proceedings, volume
5278 of Lecture Notes in Computer Science,
2008. Springer.
Colin de la Higuera. Characteristic Sets for
Polynomial Grammatical Inference. Machine
Learning, 27(2):125?138, 1997.
Colin de la Higuera. Grammatical Inference:
Learning Automata and Grammars. 2010.
E. Mark Gold. Language Identification in the
Limit. Information and Control, 10(5):447?
474, 1967.
James Jay Horning. A Study of Grammatical In-
ference. PhD thesis, 1969.
Mark Johnson, Thomas L. Griffiths, and Sharon
Goldwater. Bayesian Inference for PCFGs via
Markov Chain Monte Carlo. In Candace L.
Sidner, Tanja Schultz, Matthew Stone, and
ChengXiang Zhai, editors, HLT-NAACL, pages
139?146. The Association for Computational
Linguistics, 2007.
Daniel Jurafsky and James H. Martin. Speech and
Language Processing (2nd Edition) (Prentice
Hall Series in Artificial Intelligence). Prentice
Hall, 2 edition, 2008.
Sanjeev Khanna, Madhu Sudan, Luca Trevisan,
and David P. Williamson. The Approximabil-
ity of Constraint Satisfaction Problems. SIAM
J. Comput., 30(6):1863?1920, 2000.
1361
Dan Klein. The Unsupervised Learning of Natu-
ral Language Structure. PhD thesis, Stanford
University, 2004.
Kevin J. Lang, Barak A. Pearlmutter, and Rod-
ney A. Price. Results of the Abbadingo
One DFA Learning Competition and a New
Evidence-Driven State Merging Algorithm. In
Vasant Honavar and Giora Slutzki, editors,
ICGI, volume 1433 of Lecture Notes in Com-
puter Science, pages 1?12. Springer, 1998.
Pat Langley and Sean Stromsten. Learning
Context-Free Grammars with a Simplicity Bias.
In Ramon L?opez de M?antaras and Enric Plaza,
editors, ECML, volume 1810 of Lecture Notes
in Computer Science, pages 220?228. Springer,
2000.
Karim Lari and Steve J. Young. The Estimation
of Stochastic Context-Free Grammars using the
Inside-Outside Algorithm. Computer Speech &
Language, 4(1):35 ? 56, 1990.
Franco M. Luque and Gabriel G. Infante L?opez.
Bounding the Maximal Parsing Performance of
Non-Terminally Separated Grammars. In Sem-
pere and Garc??a (2010), pages 135?147.
Christopher D. Manning and Hinrich Sch?utze.
Foundations of Statistical Natural Language
Processing. MIT Press, 2001.
Paola Merlo, Harry Bunt, and Joakim Nivre. Cur-
rent Trends in Parsing Technology. In Trends
in Parsing Technology, pages 1?17. Springer,
2010.
Georgios Petasis, Georgios Paliouras, Constan-
tine D. Spyropoulos, and Constantine Halat-
sis. eg-GRIDS: Context-Free Grammatical In-
ference from Positive Examples Using Genetic
Search. In Georgios Paliouras and Yasubumi
Sakakibara, editors, ICGI, volume 3264 of Lec-
ture Notes in Computer Science, pages 223?
234. Springer, 2004.
Gisa Rauh. Syntactic Categories: Their Identifi-
cation and Description in Linguistic Theories.
Oxford Surveys in Syntax & Morphology No.7.
OUP Oxford, 2010.
Yoav Seginer. Fast Unsupervised Incremental
Parsing. In John A. Carroll, Antal van den
Bosch, and Annie Zaenen, editors, ACL.
The Association for Computational Linguistics,
2007.
Jos?e M. Sempere and Pedro Garc??a, editors.
Grammatical Inference: Theoretical Results
and Applications, 10th International Collo-
quium, ICGI 2010, Valencia, Spain, September
13-16, 2010. Proceedings, volume 6339 of Lec-
ture Notes in Computer Science, 2010. Springer.
Zach Solan, David Horn, Eytan Ruppin, and Shi-
mon Edelman. Unsupervised learning of nat-
ural languages. Proceedings of the National
Academy of Sciences of the United States of
America, 102(33):11629?11634, 2005.
Andreas Stolcke. Bayesian learning of probabilis-
tic language models. PhD thesis, University of
California, Berkeley, 1994.
Menno van Zaanen. Bootstrapping Structure into
Language: Alignment-Based Learning. PhD
thesis, University of Leeds, 2001.
Menno van Zaanen and Jeroen Geertzen. Prob-
lems with Evaluation of Unsupervised Empiri-
cal Grammatical Inference Systems. In Clark
et al. (2008), pages 301?303.
Heidi R. Waterfall, Ben Sandbank, Luca Onnis,
and Shimon Edelman. An Empirical Genera-
tive Framework for Computational Modeling of
Language Acquisition. Journal of Child Lan-
guage, 37:671?703, 6 2010.
Charles S. Wetherell. Probabilistic Languages: A
Review and Some Open Questions. ACM Com-
put. Surv., 12(4):361?379, 1980.
Charles Yang. Computational Models of Syntactic
Acquisition. Wiley Interdisciplinary Reviews:
Cognitive Science, 3(2):205?213, 2012.
Ryo Yoshinaka. Identification in the Limit of k, l-
Substitutable Context-Free Languages. In Clark
et al. (2008), pages 266?279.
1362
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal and Empirical Grammatical Inference
Jeffrey Heinz, Colin de la Higuera and Menno van Zaanen
heinz@udel.edu, cdlh@univ-nantes.fr, mvzaanen@uvt.nl
1
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Outline of the tutorial
I. Formal GI and learning theory (de la Higuera)
II. Empirical approaches to regular and subregular natural
language classes (Heinz)
III. Empirical approaches to nonregular natural language
classes (van Zaanen)
2
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
I Formal GI and learning theory
What is grammatical inference?
What does learning or having learnt imply?
Reasons for considering formal learning
Some criteria to study learning in a probabilistic and a non
probabilistic setting
3
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A simple definition
Grammatical inference is about learning a grammar given
information about a language
Vocabulary
Learning = building, inferring
Grammar= finite representation of a possibly infinite set of
strings, or trees, or graphs
Information=you can learn from text, from an informant, by
actively querying
Language= possibly infinite set of strings, or trees, or graphs
4
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A Dfa (Ack: Jeffrey Heinz)
The (CV)* language representing licit sequences of sounds in many
languages in the world. Consonants and vowels must alternate;
words must begin with C and must end with V. States show the
regular expression indicating its ?good tails?.
(CV )? V (CV )?
C
V
5
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A context free grammar and a parse tree
(de la Higuera 2010)
S
NP VP
John V NP
hit Det N
the ball
S ? NP VP
VP? V NP
NP? Det N
6
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A categorial dependency grammar (Be?chet et al 2011)
elle 7? [pred ],
la 7? [#(? clit ? a? obj)]?clit?a?obj ,
lui 7? [#(? clit ? 3d ? obj)]?clit?3d?obj ,
a 7? [#(? clit ? 3d ? obj)\#(?
clit ? a ? obj)\pred\S/aux ? a ? d ],
donne?e 7? [aux ? a ? d ]?clit?3d?obj?clit?a?obj
7
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A finite state transducer (Ack: Jeffrey Heinz)
A subsequential transducer illustrating a common phonological rule
of palatalization ( k ?? >tS / i). States are labelled with a
number and then the output string given by the ? function for that
state.
0,? 1,k
k:?
k:kk, C:kC, V:kV
i:>tSi
C,V,i k
? = {C ,V , k , i}
8
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
So for example:
w t(w)
kata kata
kita >tSita
tak tak
taki ta>tSi
. . .
9
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Our definition
Grammatical inference is about learning a grammar given
information about a language
Questions
Why grammar and not language?
Why a and not the?
10
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Why not write ?learn a language??
Because you always learn a representation of a language
Paradox
Take two learners learning a context-free language, one is learning
a quadratic normal form and the other a Greibach normal form,
they cannot agree that they have learnt the same thing
(undecidable question).
Worth thinking about. . . is it a paradox? Do two English speakers
agree they speak the same language?
11
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Our definition
Grammatical inference is about learning a grammar given
information about a language
How can a become the?
Ask for the grammar to be the smallest, best (re a score). ?
Combinatorial characterisation
The learning problem becomes an optimisation problem!
Then we often have theorems saying that
If our algorithm does solve the optimisation problem, what we
have learnt is correct
If we can prove that we can?t solve the optimisation problem,
then the class is not learnable
12
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Optimal with respect of some score
Score should take into account:
Simplicity
Coverage
Usefulness
What scores?
Occam argument
Compression argument
Kolmogorov complexity
MDL argument
13
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Moreover
GI is not only about building a grammar from some data. It is
concerned with saying something about:
the quality of the result,
the quality of the learning process,
the properties of the process.
14
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Naive example
Suppose you are building a random number generator.
How are you convinced that it works?
Because it follows sound principles as defined by number
theory specialists?
Because you have tested and the number 772356191 has been
produced?
Because you have proved that the series of numbers that will
be produced is incompressible?
Empirical approach
Experimental approach
Formal approach
15
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical approach: using good (safe?) ideas
For example, genetic algorithms or neural networks
Or some mathematical principle (Occam, Kolmogorov,
MDL,. . . )
Can become a principled approach
Alternative point of view
Empirical approach is about imitating what nature (or humans) do
16
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Experimental approach
Benchmarks
Competitions
Necessary but not sufficient
How do we know that all the cases are covered?
How do we know that we dont have a hidden bias?
17
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal approach: showing that the algorithm has converged
Is impossible:
Just one run
Can?t prove that 23 is random
But we can say something about the algorithm:
That in the near future, given some string, we can predict if
this string belongs to the language or not;
Choose between defining clearly ?near future? and accepting
probable truths (or error bounds) or leaving it undefined and
using identification.
18
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What else would we like to say?
That if the solution we have returned is not good, then that is
because the initial data was bad (insufficient, biased)
Idea:
Blame the data, not the algorithm
19
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Suppose we cannot say anything of the sort?
Then that means that we may be terribly wrong even in a
favourable setting
Thus there is a hidden bias
Hidden bias: the learning algorithm is supposed to be able to
learn anything inside class L1, but can really only learn things
inside class L2, with L2 ? L1
20
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Saying something about the process itself
Key idea: if there is something to learn and the data is not
corrupt, then, given enough time, we will learn it
Replace the notion of learning by that of identifying
21
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
In practise, does it make sense?
No, because we never know if we are in the ideal conditions
(something to learn + good data + enough of it)
Yes, because at least we get to blame the data, not the
algorithm
22
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Complexity issues
Complexity theory should be used: the total or update
runtime, the size of the data needed, the number of mind
changes, the number and weight of errors. . .
. . . should be measured and limited.
23
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A linguistic criterion
One argument appealing to linguists (we hope) is that if the
criteria are not met for some class of languages that a human
is supposed to know how to learn, something is wrong
somewhere
(preposterously, the maths can?t be wrong. . . )
24
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Non probabilistic settings
Identification in the limit
Resource bounded identification in the limit
Active learning (query learning)
25
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Identification in the limit
Information is presented to the learner who updates its
hypothesis after inspecting each piece of data
At some point, always, the learner will have found the correct
concept and not change from it
(Gold 1967 & 1978)
26
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example
Number Presentation Analysis of hy-
pothesis
New hypothesis
(regexp)
1 a + a
2 aaa + inconsistent a?
3 aaaa - inconsistent a(aa)?
4 aaaaaa - consistent a(aa)?
9234 aaaaaaaa - consistent a(aa)?
45623416 aaaaaaaaa + consistent a(aa)?
27
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A presentation is
a function ? : N ? X
where X is some set,
and such that ? is associated to a language L through a
function Yields : Yields(?) = L
If ?(N) = ?(N) then Yields(?) = Yields(?)
28
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
text presentation
A text presentation of a language L ? ?? is a function
? : N ? ?? such that ?(N) = L
? is an infinite succession of all the elements of L
(note : small technical difficulty with ?)
informed presentation
An informed presentation (or an informant) of L ? ?? is a
function ? : N ? ?? ?{?,+} such that
?(N) = (L,+) ? (L,?)
? is an infinite succession of all the elements of ?? labelled to
indicate if they belong or not to L
29
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Active presentation
The learner interacts with the environment (modelled as an
oracle) through queries
A membership query
Learner presents string x
Oracle answer yes or no
A correction query (Becerra-Bonache et al 2005 & 2008)
Learner presents string x
Oracle answer yes or returns a close correction
An equivalence query
Learner presents hypothesis H
Oracle answer yes or returns a counter-example
30
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example: presentations for {anbn : n ? N}
Legal presentation from text: ?, a2b2, a7b7,. . .
Illegal presentation from text: ab, ab, ab,. . .
Legal presentation from informant : (?,+), (abab,?),
(a2b2,+), (a7b7,+), (aab,?), (abab,?),. . .
31
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example: presentation for Spanish
Legal presentation from text: En un lugar de la Mancha. . .
Illegal presentation from text: Goooool
Legal presentation from informant : (en,+), (whatever,-),
(un,+), (lugar,+), (lugor,-), (xwszrrzt,-),
32
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What happens before convergence?
On two occasions I have been asked [by members of Parliament],
?Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?? I am not able rightly to apprehend
the kind of confusion of ideas that could provoke such a question.
Charles Babbage
33
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Further definitions
Given a presentation ?, ?n is the set of the first n elements in
?.
A learning algorithm (learner) A is a function that takes as
input a set ?n and returns a grammar of a language.
Given a grammar G , L(G ) is the language
generated/recognised/ represented by G .
34
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Convergence to a hypothesis
A converges to G with ? if
?n ? N : A(?n) halts and gives an answer
?n0 ? N : n ? n0 =? A(?n) = G
If furthermore L(G ) = Yields(?) then we have identified.
35
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Identification in the limit
L
G
Pres(L)
L
Yields
A
Figure: The learning setting.
from (de la Higuera 2010)
36
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Consistency and conservatism
We say that the learner A is consistent if ?n is consistent with
A(?n) ?n
A consistent learner is always consistent with the past
Consistency and conservatism
We say that the learner A is conservative if whenever ?(n + 1)
is consistent with A(?n), we have A(?n) = A(?n+1)
A conservative learner doesn?t change his mind needlessly
37
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning from data
A learner is order dependent if it learns something different
depending on the order in which it receives the data.
Usually an order independent learner is better.
38
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What about efficiency?
We can try to bound
global time
update time
errors before converging (IPE)
mind changes (MC)
queries
good examples needed (characteristic samples)
(Pitt 1989, de la Higuera et al 2008)
39
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Definition: polynomial number of implicit prediction errors
Denote by G 6|= x if G is incorrect with respect to an element
x of the presentation (i.e. the learner producing G has made
an implicit prediction error.
G is polynomially identifiable in the limit from Pres if there exists
an identification learner A and a polynomial p() such that given
any G in G, and given any presentation ? of L(G ),
]i : A(?i ) 6|= ?(i + 1) ? p(|G |).
40
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Definition: polynomial characteristic sample
G has polynomial characteristic samples for identification learner A
if there exists a polynomial p() such that: given any G in G, ?Y
correct sample for G , such that whenever Y ? ?n, A(?n) ? G and
?Y ? ? p(?G?)
As soon as the CS is in the data, the result is correct;
The CS is small.
41
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Polynomial queries
(Angluin 1987)
Algorithm A learns with a polynomial number of queries if the
number of queries made before halting with a correct
grammar is polynomial in
the size of the target,
the size of the information received.
42
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Main negative results
Cannot learn Nfa, Cfgs from an informant in most
polynomial settings (Pitt 1989, de la Higuera 1997)
Cannot learn Dfa from text (Gold 1967)
Cannot learn Dfa from membership nor equivalence queries
(Angluin 1981 & 1987).
Main positive results
Can learn Dfa from an informant with polynomial resources
(Oncina and Garc??a 1992);
Can learn Dfa from membership and equivalence queries
(Angluin 1987).
43
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Probabilistic settings
Pac learning (about learning yes-no machines with fixed but
unknown distributions)
Identification with probability 1 (about identifying
distributions)
Pac learning distributions (about approximately learning
distributions)
44
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning a language from sampling
We have a distribution over ??
We sample twice:
once to learn,
once to see how well we have learned
The Pac setting: Les Valiant, Turing award 2010
45
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Pac-learning
(Valiant 1984, Pitt 1989)
L a class of languages
G a class of grammars
 > 0 and ? > 0
m a maximal length over the strings
n a maximal size of machines
H is -AC (approximately correct)*
if
PrD [H(x) 6= G (x)] < 
46
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Polynomial Pac learning
There is a polynomial p(?, ?, ?, ?) such that
in order to learn -AC machines of size at most n with error at
most ? we require at most p(m, n, 1? , 1? ) data and time;
we want the errors to be less than  and bad luck to be less
than ?.
(French radio)
Unless there is a surprise there should be no surprise
French radio, (after the last primary elections, on 3rd of June
2008)
First surprise is ?, second surprise is 
47
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results (Kearns and Valiant 1989, Kearns and Vazirani 1994)
Using cryptographic assumptions, we cannot Pac-learn Dfa
Cannot Pac-learn Nfa, Cfgs with membership queries either
Learning can be seen as finding the encryption function from
examples (Kearns & Vazirani)
48
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alternatively
Instead of learning classifiers in a probabilistic world, learn
directly the distributions!
Learn probabilistic finite automata (deterministic or not)
49
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
No error (Angluin 1988)
This calls for identification in the limit with probability 1
Means that the probability of not converging is 0
Goal is to identify the structure and the probabilities
Mainly a (nice) theoretic setting
Results
If probabilities are computable, we can learn with probability 1
finite state automata (Carrasco and Oncina, 1994)
But not with bounded (polynomial) resources (de la Higuera
and Oncina, 2004)
50
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
With error
Pac definition applies
But error should be measured by a distance between the
target distribution and the hypothesis
How do we measure the distance: L1, L2, L?,
Kullback-Leibler?
51
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results
Too easy to learn with L?
Too hard to learn with L1
Both results hold for the same algorithm! (de la Higuera and
Oncina, 2004)
Nice algorithms for biased classes of distributions
52
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Open problems
We conclude this section on ?what is language learning about?
with some open questions:
What is a good definition of polynomial identification?
How do we deal with shifting targets? (robustness issues)
Alternative views on learnability?
Is being learnable a good indicator of being linguistically
reasonable?
Can we learn transducers? Probabilistic transducers?
53
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
II. GI of Regular Patterns
Why regular?
What are the general GI strategies?
What are the main results?
The main techniques?
The main lessons?
54
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Logically Possible Computable Patterns
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
55
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
GI Strategies
#1. Define ?learning? so that large regions can be learned
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
56
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
GI Strategies
#2. Target non-superfinite cross-cutting classes
(instructor?s bias)
Recursively Enumerable
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
57
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Common Theme
1 Different learning frameworks may better characterize the
data presentations learners actually get (strategy #1).
2 Classes of formal languages may exist which better
characterize the patterns we are interested in (strategy #2).
3 Hard problems are easier to solve with better characterizations
because the instance space of the problem is smaller.
58
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Why Begin with Regular?
Insights obtained here can be (and have been) applied fruitfully to
nonregular classes.
Angluin 1982 showed a subclass of regular languages (the
reversible languages) was identifiable in the limit from positive
data by an incremental learner.
Yokomori?s (2004) Very Simple Languages are a subclass of
the context-free languages, but draws on ideas from the
reversible languages.
Similarly, Clark and Eryaud?s (2007) substitutable languages
(also subclass of context-free) are also based on insights from
this paper.
59
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Section Outline
1 Targets of Learning
2 Learning Frameworks
3 State-merging
4 Results for learning regular languages, relations, and
distributions
60
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Targets of Learning: Regular Languages
Multiple grammars (i.e. representations) for regular languages:
1 Regular expressions
2 Generalized regular expressions
3 Finite state acceptors
4 Words which satisfy formulae in monadic second order logic
5 Right or left branching rewrite rules
6 . . .
61
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Targets of Learning: Regular Relations
Multiple grammars (i.e. representations) for regular relations:
Regular expressions (for relations)
Generalized regular expressions (for relations)
Finite state transducers
. . .
62
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Targets of Learning: Regular distributions
Multiple grammars (i.e. representations) for distributions over
regular sets and relations:
Weighted finite state automata
Hidden Markov Models
Weighted right or left branching rewrite rules
. . .
63
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
This tutorial: Finite State Automata
Acceptors and subsequential transducers admit canonical forms
1 The smallest deterministic acceptor, syntactic monoids, . . .
2 Canonical forms relate to algebraic properties (Nerode
equivalence relation, i.e. states represent sets of ?good tails?)
3 In contrast, canonical regular expressions have yet to be
determined. For example, there are no canonical (e.g.
shortest) regular expressions for regular languages.
64
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Choices
Success required on which input data streams?
All possible vs. some restricted set
i.e. ?distribution-free? vs. ?non distribution-free?
What kind of samples?
Positive data vs. postive and negative data
Other choices (e.g. query learning) are not discussed here.
65
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Distribution-free? w/ positive and negative data
1 The class of r.e. languages is identifiable in the limit (Gold
1967)
2 Non-enumerative algorithms for regular languages:
1 Gold (1978)
2 RPNI (Oncina and Garc??a 1992)
66
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Distribution-free? with positive data only
1 No superfinite class (including regular, cf, etc.) is identifiable
in the limit (Gold 1967)
2 Not even the finite class is PAC-learnable (Blumer et al 1989)
3 No superfinite class is identifiable in the limit with probability
p (p > 2/3) (Pitt 1985, Wiehagen et al 1986, Angluin 1988)
4 But many subregular classes are learnable in this difficult
setting.
67
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Distribution-free? with positive data only: learnable subregular
classes
1 reversible languages (Angluin 1982)
2 strictly local languages (Garcia et al 1990)
3 locally testable and piecewise testable (Garcia and Ruiz 2004)
4 left-to-right and right-to-left iterative languages (Heinz 2008)
5 strictly piecewise languages (Heinz 2010)
6 . . .
7 subsequential functions (Oncina et al 1993)
8 . . .
68
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Non distribution-free? w/ positive data only
1 The class of r.e. languages are identifiable in the limit from
computable classes of r.e. texts (Gold 1967)
2 The class of r.e. distributions are identifiable from
?approximately computable? sequences (Angluin 1988, Chater
and Vitany?? 2007)
3 The class of distributions describable with Probabilistic
Deterministic FSAs (PDFAs) is learnable with probability one
(de la Higuera and Thollard 2000)
4 The class of distributions describable with PDFAs is learnable
in a modified PAC setting (Clark and Thollard, 2004)
69
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning regular languages: Key technique
State-merging
Angluin 1982 (reversible languages)
Muggleton 1990 (contextual languages)
Garcia et al 1990 (strictly local languages)
Oncina et al 1993 (subsequential functions)
Clark and Thollard 2004 (PDFA distributions)
. . .
70
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Other techniques
Lattice-climbing
Heinz 2010 (strictly local languages, strictly piecewise
languages, many others)
Kasprizk and Ko?tzing 2010 (function-distinguishable
lanaguages, pattern languages, many others)
State-splitting
Tellier (2008)
71
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Only so much can be covered. . .
It?s impossible to be fair to all
those who have contributed
and to cover all the variants,
even all the algorithms in a
short tutorial. That?s why
there are books!
72
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Overview of State-merging
1 Builds a FSA representation of the input
2 Generalize by merging states
73
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Illustrative Example: Stress pattern of Pintupi
a. pa??a ?earth? ?? ?
b. tju??aya ?many? ?? ? ?
c. ma??awa`na ?through from behind? ?? ? ?` ?
d. pu??iNka`latju ?we (sat) on the hill? ?? ? ?` ? ?
e. tja?mul`?mpatju`Nku ?our relation? ?? ? ?` ? ?` ?
f. ????ir`iNula`mpatju ?the fire for our benefit
flared up?
?? ? ?` ? ?` ? ?
g. ku?ranju`lul`?mpatju`?a ?the first one who is our
relation?
?? ? ?` ? ?` ? ?` ?
h. yu?ma?`?Nkama`ratju`?aka ?because of mother-in-
law?
?? ? ?` ? ?` ? ?` ? ?
Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):
Primary stress falls on the initial syllable
Secondary stress falls on alternating nonfinal syllables
74
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Illustrative Example: Stress pattern of Pintupi
Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):
Primary stress falls on the initial syllable
Secondary stress falls on alternating nonfinal syllables
Minimal deterministic FSA for Pintupi Stress
0 1 2
3
4
?? ?
?
?`
?
75
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Structured representations of Input
1 Each word its own FSA (Nondeterministic)
2 Prefix Trees (deterministic)
3 Suffix Trees (reverse determinstic)
76
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Examples of Prefix and Suffix Trees
S =
?
?
?
?? ?? ?
?? ? ? ?? ? ?` ?
?? ? ?` ? ? ?? ? ?` ? ?` ?
?
?
?
PT(S)
0 1 2
3
4
5
6
7
8
?? ?
?`
?
?
?`
?
?
ST(S)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
16
??
?
??
?
?`
?`
??
?
??
?`
?
??
??
?
77
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging Informally
Eliminate redundant environments by state-merging.
States are identified as equivalent and then merged.
All transitions are preserved.
This is one way in which generalizations may occur?because
the post-merged machine accepts everything the pre-merged
machine accepts, possibly more.
Machine A Machine B
0 1 2 3a a a 0 1-2 3a a
a
The merged machine may not be deterministic.
78
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging Formally
Definition
Given an acceptor A = (Q, I ,F , ?) and a partition pi of its states
state-merging returns the acceptor A/pi = (Q ?, I ?,F ?, ??):
1 Q ? = pi (the states are the blocks of pi)
2 I ? = {B ? pi : I ? B 6= ?}
3 F ? = {B ? pi : F ? B 6= ?}
4 For all B ? pi and a ? ?,
??(B , a) = {B ? ? pi : ?q ? B , q? ? B ? such that q? ? ?(q, a)}
79
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Theorem
Theorem
Given any regular language L, let A(L) denote the minimal
deterministic acceptor recognizing L. There exists a finite sample
S ? L and a partition pi over PT (S) such that PT (S)/pi = A(L).
Notes
The finite sample need only exercise every transition in A(L).
What is pi?
80
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Illustrative Example
Let?s merge states with the same incoming paths of length 2!
PT(S)
0 1 2
3
4
5
6
7
8
?? ?
?`
?
?
?`
?
?
81
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Result of State Merging
0 1 2
3-6
4-7
5-8
??
?`
?
?`
?
?
?
This acceptor is not the canonical acceptor we saw earlier but it
recognizes the same language.
Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):
Primary stress falls on the initial syllable
Secondary stress falls on alternating nonfinal syllables
82
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summary of Algorithm
1 States in the prefix tree are merged if they have the same
k-length suffix.
u ? v def?? ?x , y ,w such that |w | = k , u = xw , v = yw
2 The algorithm then is simply:
G = PT (S)/pi?
3 This algorithm provably identifies in the limit from positive
data the Strictly (k + 1)-Local class of languages (Garcia et
al. 1990).
83
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Back to the Illustrative Example
Results for stress patterns more generally
Out of 109 distinct stress patterns in the world?s languages
(encoded as FSAs), this state-merging strategy works for only
44 of them
If we merge states with the same paths up to length 5(!), only
81 are learned.
This is the case even permitting very generous input samples.
In other words, 44 attested stress patterns are Strictly 3-Local and
81 are Strictly 6-Local. 28 are not Strictly 6-Local In fact those 28
are not Strictly k-Local for any k (Edlefsen et al 2008).
84
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Other ways to merge states
If the current structure is ?ill-formed? then merge states to
eliminate source of ill-formedness
State equivalence relations
1 merge state with same incoming paths of length k (Garcia et. al 1990)
2 recursively eliminate reverse non-determinism (Angluin 1982)
3 merge states with same ?contexts? (Muggleton 1990, Clark and Eryaud
2007)
4 merge final states (Heinz 2008)
5 merge states with same ?neighborhood? (Heinz 2009)
6 . . .
7 merge states to maximize posterior probability (for HMMs, Stolcke 1994)
8 . . .
85
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Other ways to merge states
Merge states indiscriminately unless ?ill-formedness? arises
Merge unless something tells us not to
1 unless ?onward subsequentiality? is lost (for transducers,
Oncina et al 1993)
2 unless they are ??-distinguishable? (Clark and Thollard 2004)
3 . . .
86
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging as inference rules
Strictly k-Local languages (Garcia et al 1990)
merge states with same incoming paths of length k
?u, v ,w ? ?? : uv ,wv ,? Prefix(L) and |v | = k
?
TailsL(uv) = TailsL(wv) ? L
87
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging as inference rules
0-Reversible languages (Angluin 1982)
recursively eliminate reverse non-determinism
?u, v ,w , y ? ?? : uv ,wv , uy ? L ? wy ? L
88
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging summary
1 Distinctions maintained in the prefix tree are lost by state
merging, which results in generalizations.
2 The choice of partition corresponds to the generalization
strategy (i.e. which distinctions will be maintained and which
will be lost)
Gleitman (1990:12):
The trouble is that an observer who notices everything
can learn nothing for there is no end of categories known
and constructible to describe a situation [emphasis in
original].
89
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results for regular languages
Distribution-free with positive data
Identification in the limit from positive data
1 strictly k-local languages (each state corresponds to suffixes of
up to length k) (Garcia et al 1990)
2 reversible languages (acceptors are both forward and reverse
k-deterministic for some k) (Angluin 1982)
3 k-contextual languages (Muggleton 1990)
4 . . .
90
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Regular relations
Regular relations in CL
1 transliteration
2 translation
3 . . .
4 anything with finite state transducers
91
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
OSTIA (Oncina et al 1993)
distribution-free with positive data
OSTIA
1 identifies subsequential functions in the limit from positive
data.
2 Merges states greedily unless subsequentiality is violated
3 If the function is partial, exactness is guaranteed only where
the function is defined.
92
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
OSTIA (Oncina et al 1993)
Subsequential relations
1 are a subclass of the regular relations, recognizing functions.
2 are those which are recognized by subsequential transducers,
which are determinstic on the input and which have an
?output? string associated with every state.
3 have a canonical form.
4 have been generalized to permit up to p outputs for each
input (Mohri 1997).
93
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
OSTIA for learning phonological rules
Gildea and Jurafsky 1996
1 Show that OSTIA doesn?t learn the English tapping rule or
German word-final devoicing rule from data present in
adapted dictionaries of English or German
2 Applied additional phonologically motivated heuristics to
improve state-merging choices.
What about well-defined subclasses of subsequential relations?
94
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Weighted finite-state automata
non-distribution-free with positive data
The problem
Given a finite multiset of words drawn independently from the
target distribution, what grammar accurately describes the
distribution?
Theorem
The class of distributions describable with Non-deterministic
Probabilistic Finite-State Automata (NPFA) exactly matches the
class of distributions describable with Hidden Markov Models
(Vidal et al 2005).
95
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Maximum Likelihood Estimation
A : 13
a : 0
b : 13
c : 13
M
A : 15
a : 15
b : 15
c : 15
M?
?
{bc}
M represents a family of
distributions with 4 parameters.
M? represents a particular
distribution in this family.
Theorem
For a sample S and deterministic finite-state acceptor M, counting the
parse of S through M and normalizing at each state optimizes the
maximum-likelihood estimate.
(Vidal et. al 2005, de la Higuera 2010) 96
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly 2-Local Distributions are bigram models
?
a?
b?
c ?
a
b
c
a
b
c
a
b
c
a
b
c
Figure: The structure of a bigram model. The 16 parameters of this
model are given by associating probabilities to each transition and to
?ending? at each state.
97
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Subregular distributions
RegularFinite
Some well-defined
subregular class
1 When the structure of a Deterministic FSA is known in
advance, MLE is easy to do.
2 The DFA represents a subregular class of distributions.
98
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly Piecewise Distributions
1 N-gram models can?t describe long-distance dependencies.
Long-distance dependencies in phonology
1 Consonantal harmony
(Jensen 1974, Odden 1994, Hansson 2001, Rose and Walker
2004, and many others)
2 Vowel harmony
(Ringen 1988, Bakovic? 2000, and many others)
99
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Sibilant Harmony example from Samala (Inesen?o
Chumash)
[StojonowonowaS] ?it stood upright? (Applegate 1972:72)
cf. *[stojonowonowaS] and
cf. *[Stojonowonowas]
Hypothesis: *[stojonowonowaS] and *[Stojonowonowas] are
ill-formed because the discontiguous subsequences sS and Ss are
ill-formed.
100
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly Piecewise languages
Rogers et al 2010
1 solely make distinctions on the basis of potentially
discontiguous subsequences up to some length k
2 are mathematically natural. They have several chacterizations
in terms of formal language theory, automata theory, logic,
model theory, and the
3 algebraic theory of automata (Fu et al 2011)
101
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly Piecewise Distributions
Heinz and Rogers 2010
1 are defined in terms of the factored automata-theoretic
representations (Rogers et al 2010)
2 along with the co-emission probability as the product (Vidal et
al. 2005)
3 Estimation over the factors permits learnability of the patterns
like the ones in Samala.
Example with ? = {a, b, c} and k = 2.
A0 A1 B0 B1 C0 C1? ?a b c
a
b
c
a
b
c
a
b
c
b
c
a
c
a
b
102
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
SP2 learning results for Chumash
Training corpus 4800 words from a dictionary of Samala
x
P(x | y <)
s >ts S >tS
y
s 0.0325 0.0051 0.0013 0.0002
ts 0.0212 0.0114 0.0008 0.
S 0.0011 0. 0.067 0.0359
>
tS 0.0006 0. 0.0458 0.0314
Table: SP2 probabilities of sibilant occuring sometime after another one
(collapsing laryngeal distinctions)
103
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning larger classes of regular distributions
More non-distribution-free with positive data
The class of distributions describable with PDFA
1 are identifiable in the limit with probability one (de la Higuera
and Thollard 2000).
2 are learnable in modified-PAC setting (Clark and Thollard
2004).
3 The algorithms presented employ state-merging methods.
1 This is a (much!) larger class than that which is describable
with n-gram distributions or with SP distributions.
2 To my knowledge these approaches have not been applied to
tasks in CL.
104
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summary
#1. Define ?learning? so that large regions can be learned
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
Oncina et al 1993, de la Higuera and Thollard 2000, Clark and
Thollard 2004, . . .
105
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summary
#2. Target non-superfinite cross-cutting classes
Recursively Enumerable
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
Angluin 1982, Muggleton 1990, Garcia et al 1990, Heinz 2010, . . .
106
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Have we put the cart before the horse?
1 So far we have discussed algorithms that learn various classes
of languages.
2 But shouldn?t we first know which classes are relevant for our
goals?
3 E.g. for phonology, while ?being regular? may be a necessary
property of phonological patterns, it certainly is not sufficient.
107
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Have we put the cart before the horse?
Research strategy
Patterns ? Characterizations ? Learning algorithms
1 Identify the range and kind of patterns (linguistics).
2 Characterize the range and kind of patterns (computational
linguistics).
3 Create learning algorithms for these classes, prove their
success in a variety of settings, and otherwise demonstrate
their success (grammatical inference, formal learning theory,
computational linguistics)
108
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Subregular classes of regular sets
Regular
Star-Free=NonCounting
TSL LTT
LT PT
SL SP
Proper inclusion
relationships among
subregular language
classes.
instructor?s hunch for
phonology
TSL Tier-based Strictly Local PT Piecewise Testable
LTT Locally Threshold Testable SL Strictly Local
LT Locally Testable SP Strictly Piecewise
(McNaughton and Papert 1971, Simon 1975, Rogers and Pullum 2007, in
press, Rogers et al 2010, Heinz et al 2011)
109
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Conclusion to section 2 part 1
1 State-merging is a well-studied strategy for inferring
automata, including acceptors, transducers, and weighted
acceptors and transducers.
2 It has yielded theoretical results in many learning frameworks
including both distribution-free and non-distribution-free
learning frameworks.
110
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Conclusion to section 2 part 2
1 Many subclasses of regular languages are learnable even in the
hardest learning settings.
2 Recent advances yield algorithms for large classes
(probabilistic DFAs)
3 Computational linguists can explore which are relevant to
natural language and consequently which are useful for NLP!
4 There is a rich literature in GI which speaks to these classes,
and how such patterns in these classes can be learned.
111
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Overview
Empirical grammatical inference
Family of languages
Information contained in input
Overview of systems
Evaluation issues
From empirical to formal GI
112
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Introduction
Language learning
Starting from family of languages
Given set of samples
Identify language that is used to generate samples
Formal grammatical inference
Identify family of languages that can be learned efficiently
Under certain restrictions
Empirical grammatical inference
Exact underlying family of languages is unknown
Target language is approximation
113
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical GI
Try to identify language given samples
E.g. sentences (syntax), words (morphology), . . .
Underlying language class is unknown
For algorithm we still need to make a choice
If identification is impossible, provide approximation
Evaluation of empirical GI is different from formal GI
114
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Family of languages
What is the underlying family of languages?
Choice has impact on learning algorithm
Many possibilities
Use simple, fixed structures (n-grams)
Find probabilities
Extract structure from treebanks
Slightly more flexible structure
Find probabilities
Learn structure
Flexible structure
Find probabilities
115
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
N-grams
1 Starting from a plain text or collection of texts (corpus)
2 Extract all subsequences of length n (n-grams)
3 Count occurrences of n-grams in texts
4 Assign probabilities to each n-gram based on counts
Issues
Unseen n-grams
Back-off: use n-grams with smaller n
Smoothing: adjust probabilities for unseen n-grams
116
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Using n-gram models
How likely is the sentence ?John likes Mary??
Unigram language model
P(John likes Mary) ? P(John)P(likes)P(Mary)
Bigram language model
P(John likes Mary) ? P(John|?s?)P(likes|John)P(Mary|likes)
Trigram language model
P(John likes Mary) ?
P(John|?s??s?)P(likes|?s?John)P(Mary|John likes)
N-gram language model
P(wn1 ) ?
?n
k=1 P(wk |wk?1k?N+1)
N-grams provide a probability for each sequence
Probability describes how well sequence fits language
117
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Extract structure from treebanks
1 Starting from a treebank (sentences with structure)
2 Extract grammar rules that are used to create tree structures
For instance, context-free grammars (Charniak 1993)
or sub-trees (Data-Oriented Parsing) (Bod 1998)
3 Count occurrences of grammar rules in treebank
4 Assign probabilities to grammar rules based on counts
Issues
Over-generalization, ?incorrect? probabilities
Add information on applicability of grammar rules
(Johnson 1998)
Reestimate probabilities (EM)
(Dempster et al1977, Lari and Young 1990)
118
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Extract structure from tree
VB
PRP
He
VB1
adores
VB2
VB
listening
TO
TO
to
NN
music
VB ?PRP VB1 VB2
PRP?He
VB1?adores
VB2?VB TO
VB ?listening
TO ?TO NN
TO ?to
NN ?music
Extract counts from treebank ? probabilities
Reestimate probabilities
Improve fit of grammar and sentences
119
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learn structure
1 Starting from a corpus
2 Identify regularities that may serve as grammar rules
3 Output:
Structure assigned to sentences ? extract grammar
Extracted grammar rules (and probabilities) ? parse
Issues
Learning system has to deal with both
flexibility in structure
probabilities of structure
120
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summarizing fixed versus flexible structure
Fixed versus flexible is really a sliding scale
Language modelling using n-grams
Structure is very simple and very rigid
Requires plain sequences as input
Corresponds to k-testable languages (Garc??a 1990)
Language modelling using extracted grammar rules
Structure is more flexible, but restricted by treebank
Requires structured sequences as input
Corresponds to e.g. (limited) context-free languages
?Learning structure?
Structure is flexible, restricted by learning algorithm
Requires plain sequences as input
Corresponds to e.g. context-free languages
121
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical grammatical inference
Choices:
What type of grammar are we learning?
Regular language
K -testable language (n-grams)
Context-free language
. . .
What kind of input do we require?
Sequence of words (sentence)
Sequence of part-of-speech tags
(Partial) tree structures
. . .
What kind of output do we want?
Structured version of input
Explicit grammar
Binary or n-ary (context-free rules)
. . .
122
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Overview of systems
EMILE
Alignment-Based Learning (ABL)
ADIOS
CCM+DMV
U-DOP
. . .
123
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Underlying approach
Given a collection of plain sentences
On what basis are we going to assign structure?
Should structure be linguistically motivated?
or similar to what linguists would assign?
Perhaps we can use tests for constituency to find structure
124
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Substitutability
Elements of the same type are substitutable
Test for constituency (Harris, 1951)
What is (a family fare)NP
Replace noun phrase with another noun phrase
What is (the payload of an African Swallow)NP
Learning by reversing test
What is (a family fare)X
What is (the payload of an African Swallow)X
125
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
EMILE
Learns context-free grammars
Using plain sentences
Originally used to show formal learnability
of (a form of) Categorial Grammars in a PAC learning setting
(Adriaans 1992, Adriaans and Vervoort 2002, Vervoort 2000)
Approach
1 Starting from simple sentences
identify recurring subsequences
2 Store recurring subsequences and contexts
3 Introduce grammar rules when there is enough evidence
Practical implementation allows for several constraints
Context length, subsequence length, . . .
126
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example matrix
John walks
Mary walks
John sees Mary
(.) walks John (.) (.) sees Mary . . . contexts
John x x . . .
walks x . . .
Mary x . . .
sees . . .
... ... ... ... . . .
terms
127
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learn grammar rules
Terms that share (approximately) same context are clustered
?John? and ?Mary? are grouped together
Occurrences of terms in cluster are replaced by new symbol
Modified sequences may again contain terms/contexts
Terms may consist of multiple words
Example
John walks ?X walks
Mary walks ?X walks
John sees Mary ?X sees X
Mary slaps John?X slaps X
?sees? and ?slaps? now also share the same context
128
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment-Based Learning (ABL)
Based on substitutability test
Using plain sentences
Similar to EMILE, but
Clustered terms are not explicitly replaced by symbol
Terms and contexts are always separated
All terms are considered (and only selected afterwards)
Output is structured version of input or grammar
(van Zaanen 2000a, b, 2002)
129
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment-Based Learning (ABL)
Corpus Alignment
Learning
Hypothesis
Space
Hypothesis
Space
Selection
Learning
Structured
Corpus
Structured
Corpus
Grammar
Extraction
Grammar
130
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment-Based Learning (ABL)
Alignment learning
Align pairs of sentences
Unequal parts of sentences are stored as hypotheses
(Clustering)
Group hypotheses in same context together
Selection learning
Remove overlapping hypotheses
131
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment learning
Align pairs of sentences
using edit distance (Wagner and Fischer 1974)
or suffixtrees (Geertzen and van Zaanen 2004, Ukkonen 1995)
Unequal parts of sentences are stored as hypotheses
Align all sentences in a corpus to all others
Example
(Y1 I need (X1a dinner during the flight)X1)Y1
(Z1 I need)Z1 (X1to return on (Z2tuesday)Z2)X1
(Y1(Z1he wants)Z1 to return on (Z2wednesday)Z2)Y1
132
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Selection Learning
Alignment learning can generate overlapping brackets
Underlying grammar is considered context-free
Structure describes parse according to underlying grammar
?Wrong? brackets have to be removed
Based on e.g. chronological order or statistics
Example
from (Y1Tilburg (X2to)Y1 Portland)X2
from (X1Portland (Y2to)X1 Tilburg)Y2
133
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
ADIOS
Automatic Distillation of Structure (ADIOS) (Solan 2005)
Idea
1 Represent language as a graph
2 Compress graph
3 As long as possible, find significant patterns in paths
Using substitutability and significance tests
4 (Recursion may be added as a post-processing step)
134
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Graph
sees Mary
S John walks E
Mary slaps John
135
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Phases
1 Initialization
Load all sentences (as paths) in the graph
2 Pattern distilation
Find sub-paths
shared by significant number of partially-aligned paths
using motif-extraction (MEX) algorithm
3 Generalization
Group all nodes that occur in same pattern together
Cluster words/subsequences similarly to EMILE
4 Repeat 2 and 3 until no new patterns are found
136
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Graph
S e1 e2 e3 e4 e5 E
If e2 e3 e4 is a significant pattern
S e1 e2 e3 e4 e5 E
137
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
MEX
Compute probabilities depending on in-/out-degree of nodes
PR(e1; e2) =
# paths from e1 to e2
# paths to e1
PR(e1; e3) =
# paths from e1 to e3
# paths to e1
DR(e1; e3) =
PR(e1; e4)
PR(e1; e3)
PR describes path to the right
similarly PL describes path to the left
Significance is computed based on DR and DL wrt parameter
Informally: find significant changes in number of paths
Pick most significant pattern
138
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Constituent-Context Model (CCM)
Consider all possible binary tree structures on POS sequences
Define a probability distribution over the possible bracketings
A bracketing is a particular structure on a sequence
P(s,B) = Pbin(B)P(s|B)
P(s|B) = ?i ,j :i?jPspan(sij |Bij)Pctx(si?1, sj |Bij)
Run (iterative) Expectation-Maximization (EM) algorithm
to maximize likelihood ?s?SP(s)
(Klein 2002)
139
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Dependency Model with Valence (DMV)
DMV aims to learn dependency relations
in contrast to CCM which learns context-free grammar rules
Dependency parse links words in a head-dependent relation
Model describes likelihood of
left dependencies
right dependencies
stop condition (no more dependencies)
Again, iterative EM is used to maximize likelihood of corpus
140
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
CCM+DMV
CCM and DMV can be combined
Both models have different view on structure
Results of combined system are better than either systems
Strengths of both systems are combined
(Klein 2004)
141
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
U-DOP
Similar to CCM in that it
finds probability distribution over ?all? structures
uses POS sequences
U-DOP uses Data-Oriented Parsing (DOP) as formalism
Extends probabilistic model of context-free grammars
Requires practical implementation choices
Random sampling due to huge size of search space
(Bod 2006a, b)
Procedure
1 Generate all possible binary trees on example sentences
2 Extract all subtrees
3 Estimate probabilities on subtrees using EM
142
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Subtrees
S
NP
PN
VP
V NP
S
NP VP
V NP
S
NP
PN
VP
S
NP VP
VP
V NP
NP
PN
Remove either all or no elements on a level
Leads to many subtrees
Each subtree receives a probability
Longer distance dependencies may be modeled
143
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Parsing
Subtrees can be recombined into a larger tree
Similar to context-free grammar rules
Same parse may be created using different derivations
Statistical model has to take this into account
Example
S
NP VP
V NP
? NP
PN
? NP
PN
= S
NP
PN
VP
V NP
PN
144
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Underlying idea
U-DOP works because span of subtrees reoccur in a corpus
Likelihood of ?useful? spans increase
Hence, likelihood of contexts (also subtrees) increase
Essentially, U-DOP uses implied substitutability
while system leans heavily on probabilities
145
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Evaluation
Base
treebank
Extract
sentences
Compare
treebanks
Results
Plain
corpus
Learning
system
Learned
treebank
Recall (completeness)
Precision (correctness)
F-Score (combination of Precision and Recall)
(van Zaanen and Adriaans 2001)
146
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Evaluation settings
Air Travel Information System (ATIS)
Taken from Penn Treebank II
568 English sentences
Example
list the flights from baltimore to seattle that stop in minneapolis
does this flight serve dinner
the flight should arrive at eleven a.m. tomorrow
what airline is this
147
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results on ATIS
Micro Macro Macro2
Precision 47.01 46.18 46.18
Recall 44.94 50.98 50.98
F-Score 44.60 47.10 48.46
Explanation
Micro Count constituents, weighted average per sentence
Macro Count constituents and average per sentence
Macro2 Compute Macro Precision/Recall, average at end
148
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results on ATIS
remove remove remove
sentence empty both
Micro Precision 47.01 47.67 77.10 79.07
Micro Recall 44.94 45.30 44.95 45.29
Micro F-Score 44.60 45.09 55.31 56.13
Macro Precision 46.18 47.66 77.08 81.18
Macro Recall 50.98 52.96 51.07 52.80
Macro F-Score 47.10 48.62 60.00 62.47
Macro2 F-Score 48.46 50.17 61.43 63.99
Example
(bla bla bla)?bla bla bla
bla () bla ?bla bla
(bla () bla) ?bla bla
149
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Evaluation insights
No standard evaluation exists
but de facto evaluation datasets arise
ATIS (van Zaanen and Adriaans 2001)
WSJ10, WSJ40 (WSJ with sentence length limitations)
NEGRA10 (German)
CTB10 (Chinese)
Systems have different input/output
Evaluation settings influence results
Different metrics (micro/macro/macro2)
Included constituents (sentence/empty)
Formal grammatical inference does not have this problem
Evaluation performed through formal proofs
150
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Context-sensitive grammars
Learning context-free grammars is hard
Is learning context-sensitive grammars impossible?
That depends
To what degree is the grammar context-sensitive?
We may not need ?full? context-sensitiveness
Grammar rules: ?A? ? ???
Mildly context-sensitive grammars may be enough for NL
(Huybrechts 1984, Shieber 1985)
Perhaps the full power of context-freeness is not needed
151
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Family of languages
RegCFCSUnres b
Family to learn
152
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning context-sensitive languages
Open research area
Some work has already been done
Augmented Regular Expressions (Alque?zar 1997)
Variants of substitutability (Yoshinaka 2009)
Distributional Lattice Grammars (Clark 2010)
153
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Relationship between empirical and formal GI
Is there a relationship between empirical GI and formal GI?
Example: consider the case of substitutability
There are situations in which substitutability breaks:
John eats meat
John eats much
This suggests that learning based on substitutability
learns a different family of languages (not CFG)
Non-terminally separated (NTS) languages
Subclass of deterministic context-free grammars
154
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning NTS grammars
Grammar G=??,V ,P ,S? is NTS
? is vocabulary
V is set of non-terminals
P is set of production rules
S ? V is the start symbol
Additional restriction:
If N ? V
N ?? ???
M ?? ?
then N ?? ?M?
In other words:
non-terminals correspond exactly with substitutability
(Clark and Eyraud 2005, Clark 2006, Clark and Eyraud 2007)
155
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning NTS grammars
It can be shown that NTS grammars are
identifiable in the limit
PAC learnable
Unfortunately, natural language is not an NTS language
Ultimate goal:
Find family of languages that fits natural language
and is learnable in the right learning setting
156
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal GI and empirical GI
Relation between formal GI and empirical GI
Formal GI can show learnability
Under certain conditions
Emprical GI tries to learn structure from real data
Practically shows possibilities and limitations
Ultimate aim: Find family of languages that is
learnable under different conditions
fits natural languages
157
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
CONCLUSIONS
1 There have been new strong positive results in a recent past
for all the cases mentioned (subclasses of regular, PFA,
transducers, CFGs, MCSGs)
2 Look for ICGI! It?s the conference where these exciting results
happen (as well as exciting challenges, competitions,
benchmarks etc.)
3 The use of GI techniques both in computational linguistics
and natural language processing is taking place.
4 The future is bright!
158
References
P. W. Adriaans and M. van Zaanen. 2004. Computational
grammar induction for linguists. Grammars, 7:57?68.
Special issue with the theme ?Grammar Induction?.
P. W. Adriaans and M. van Zaanen. 2006. Computa-
tional grammatical inference. In D. E. Holmes and
L. C. Jain, editors, Innovations in Machine Learning,
volume 194 of Studies in Fuzziness and Soft Com-
puting, chapter 7. Springer-Verlag, Berlin Heidelberg,
Germany. To be published. ISBN: 3-540-30609-9.
P. W. Adriaans and M. Vervoort. 2002. The EMILE 4.1
grammar induction toolbox. In P. W. Adriaans, H. Fer-
nau, and M. van Zaanen, editors, Grammatical Infer-
ence: Algorithms and Applications (ICGI); Amster-
dam, the Netherlands, volume 2482 of Lecture Notes
in AI, pages 293?295, Berlin Heidelberg, Germany,
September 23?25. Springer-Verlag.
P. W. Adriaans. 1992. Language Learning from a Cat-
egorial Perspective. Ph.D. thesis, University of Ams-
terdam, Amsterdam, the Netherlands, November.
R. Alque?zar and A. Sanfeliu. 1997. Recognition and
learning of a class of context-sensitive languages de-
scribed by augmented regular expressions. Pattern
Recognition, 30(1):163?182.
D. Angluin and M. Kharitonov. 1991. When won?t mem-
bership queries help? In Proceedings of 24th ACM
Symposium on Theory of Computing, pages 444?454,
New York. ACM Press.
D. Angluin. 1981. A note on the number of queries
needed to identify regular languages. Information and
Control, 51:76?87.
D. Angluin. 1982. Inference of reversible languages.
Journal for the Association of Computing Machinery,
29(3):741?765.
D. Angluin. 1987a. Learning regular sets from
queries and counterexamples. Information and Con-
trol, 39:337?350.
D. Angluin. 1987b. Queries and concept learning. Ma-
chine Learning Journal, 2:319?342.
D. Angluin. 1988. Identifying languages from stochas-
tic examples. Technical Report YALEU/DCS/RR-614,
Yale University, March.
R. B. Applegate. 1972. Inesen?o Chumash Grammar.
Ph.D. thesis, University of California, Berkeley.
L. Beccera-Bonache, C. Bibire, and A. Horia Dediu.
2005. Learning DFA from corrections. In Henning
Fernau, editor, Proceedings of the Workshop on The-
oretical Aspects of Grammar Induction (TAGI), WSI-
2005-14, pages 1?11. Technical Report, University of
Tu?bingen.
L. Becerra-Bonache, C. de la Higuera, J. C. Janodet, and
F. Tantini. 2008. Learning balls of strings from edit
corrections. Journal of Machine Learning Research,
9:1841?1870.
D. Be?chet, A. Dikovsky, and A. Fore?t. 2011. Sur
les ite?rations disperse?es et les choix itr?e?s pour
l?apprentissage incre?mental des types dans les gram-
maires de de?pendances. In Proceedings of Confe?rence
d?Apprentissage.
A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K.
Warmuth. 1989. Learnability and the Vapnik-
Chervonenkis dimension. J. ACM, 36(4):929?965.
R. Bod. 1998. Beyond Grammar?An Experience-
Based Theory of Language, volume 88 of CSLI Lec-
ture Notes. Center for Study of Language and Infor-
mation (CSLI) Publications, Stanford:CA, USA.
R. Bod. 2006a. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of the 21st International
Conference on Computational Linguistics (COLING)
and 44th Annual Meeting of the Association of Com-
putational Linguistics (ACL); Sydney, Australia, pages
865?872. Association for Computational Linguistics.
R. Bod. 2006b. Unsupervised parsing with u-dop. In
CoNLL-X ?06: Proceedings of the Tenth Conference
on Computational Natural Language Learning, pages
85?92, Morristown, NJ, USA. Association for Com-
putational Linguistics.
R. C. Carrasco and J. Oncina. 1994. Learning stochastic
regular grammars by means of a state merging method.
In R. C. Carrasco and J. Oncina, editors, Grammatical
Inference and Applications, Proceedings of ICGI ?94,
number 862 in LNAI, pages 139?150. Springer-Verlag.
E. Charniak. 1993. Statistical Language Learning.
Massachusetts Institute of Technology Press, Cam-
bridge:MA, USA and London, UK.
N. Chater and P. Vita?nyi. 2007. ?ideal learning? of natu-
ral language: Positive results about learning from pos-
itive evidence. Journal of Mathematical Psychology,
51(3):135?163.
N. Chomsky. 1957. Syntactic Structures. Mouton & Co.,
Printers, The Hague.
A. Clark and R. Eyraud. 2005. Identification in the limit
of substitutable context-free languages. In S. Jain,
H. U. Simon, and E. Tomita, editors, Algorithmic
Learning Theory: 16th International Conference, ALT
2005, volume 3734 of Lecture Notes in Computer Sci-
ence, pages 283?296, Berlin Heidelberg, Germany.
Springer-Verlag.
A. Clark and R. Eyraud. 2007. Polynomial identification
in the limit of substitutable context-free languages.
Journal of Machine Learning Research, 8:1725?1745.
A. Clark and F. Thollard. 2004. Pac-learnability of prob-
abilistic deterministic finite state automata. Journal of
Machine Learning Research, 5:473?497.
A. Clark. 2006. PAC-learning unambiguous NTS lan-
guages. In Y. Sakakibara, S. Kobayashi, K. Sato,
T. Nishino, and E. Tomita, editors, Eighth Interna-
tional Colloquium on Grammatical Inference, (ICGI);
Tokyo, Japan, number 4201 in Lecture Notes in AI,
pages 59?71, Berlin Heidelberg, Germany. Springer-
Verlag.
A. Clark. 2010. Efficient, correct, unsupervised learn-
ing of context-sensitive languages. In CoNLL ?10:
Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 28?37,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
C. de la Higuera and J. Oncina. 2004. Learning proba-
bilistic finite automata. In G. Paliouras and Y. Sakak-
ibara, editors, Grammatical Inference: Algorithms and
Applications, Proceedings of ICGI ?04, volume 3264
of LNAI, pages 175?186. Springer-Verlag.
C. de la Higuera and F. Thollard. 2000. Identification in
the limit with probability one of stochastic determinis-
tic finite automata. In A.L. de Oliveira, editor, Gram-
matical Inference: Algorithms and Applications, Pro-
ceedings of ICGI ?00, volume 1891 of Lecture Notes
in Computer Science, pages 15?24. Springer-Verlag.
C. de la Higuera, J.-C. Janodet, and F. Tantini. 2008.
Learning languages from bounded resources: the case
of the DFA and the balls of strings. In A. Clark,
F. Coste, and L. Miclet, editors, Grammatical In-
ference: Algorithms and Applications, Proceedings
of ICGI ?08, volume 5278 of LNCS, pages 43?56.
Springer-Verlag.
C. de la Higuera. 1997. Characteristic sets for polyno-
mial grammatical inference. Machine Learning Jour-
nal, 27:125?138.
C. de la Higuera. 2010. Grammatical inference: learn-
ing automata and grammars. Cambridge University
Press, Cambridge, UK.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
Matt Edlefsen, Dylan Leeman, Nathan Myers, Nathaniel
Smith, Molly Visscher, and David Wellcome. 2008.
Deciding strictly local (SL) languages. In Jon Breit-
enbucher, editor, Proceedings of the Midstates Con-
ference for Undergraduate Research in Computer Sci-
ence and Mathematics, pages 66?73.
Jie Fu, J. Heinz, and Herbert Tanner. 2011. An alge-
braic characterization of strictly piecewise languages.
In The 8th Annual Conference on Theory and Applica-
tions of Models of Computation, volume 6648 of Lec-
ture Notes in Computer Science. Springer-Verlag.
P. Garc??a and J. Ruiz. 2004. Learning k-testable
and k-piecewise testable languages from positive data.
Grammars, 7:125?140.
P. Garcia and E. Vidal. 1990. Inference of k-testable
languages in the strict sense and application to syntac-
tic pattern recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 12:920?925.
P. Garcia, E. Vidal, and J. Oncina. 1990. Learning lo-
cally testable languages in the strict sense. In Proceed-
ings of the Workshop on Algorithmic Learning Theory,
pages 325?338.
G.Clements and J. Keyser. 1983. CV phonology: a gen-
erative theory of the syllable. Cambridge, MA: MIT
Press.
J. Geertzen and M. van Zaanen. 2004. Grammati-
cal inference using suffix trees. In G. Paliouras and
Y. Sakakibara, editors, Grammatical Inference: Algo-
rithms and Applications: Seventh International Collo-
quium, (ICGI); Athens, Greece, volume 3264 of Lec-
ture Notes in AI, pages 163?174, Berlin Heidelberg,
Germany, October 11?13. Springer-Verlag.
D. Gildea and D. Jurafsky. 1996. Learning bias and
phonological-rule induction. Computational Linguis-
tics, 24(4).
L. Gleitman. 1990. The structural sources of verb mean-
ings. Language Acquisition, 1(1):3?55.
E. M. Gold. 1967. Language identification in the limit.
Information and Control, 10(5):447?474.
E. M. Gold. 1978. Complexity of automaton identi-
fication from given data. Information and Control,
37:302?320.
K. C. Hansen and L. E. Hansen. 1969. Pintupi phonol-
ogy. Oceanic Linguistics, 8:153?170.
Z. S. Harris. 1951. Structural Linguistics. University of
Chicago Press, Chicago:IL, USA and London, UK, 7th
(1966) edition. Formerly Entitled: Methods in Struc-
tural Linguistics.
B. Hayes. 1995. Metrical Stress Theory. Chicago Uni-
versity Press.
J. Heinz and J. Rogers. 2010. Estimating strictly piece-
wise distributions. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 886?896, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
J. Heinz. 2008. Left-to-right and right-to-left iterative
languages. In Alexander Clark, Franc?ois Coste, and
Lauren Miclet, editors, Grammatical Inference: Al-
gorithms and Applications, 9th International Collo-
quium, volume 5278 of Lecture Notes in Computer
Science, pages 84?97. Springer.
J. Heinz. 2009. On the role of locality in learning stress
patterns. Phonology, 26(2):303?351.
J. Heinz. 2010. String extension learning. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 897?906, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
R. M. A. C. Huybrechts. 1984. The weak adequacy
of context-free phrase structure grammar. In G. J.
de Haan, M. Trommelen, and W. Zonneveld, editors,
Van periferie naar kern, pages 81?99. Foris, Dor-
drecht, the Netherlands.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613?
632, December.
A. Kasprzik and T. Ko?tzing. 2010. String extension
learning using lattices. In Henning Fernau Adrian-
Horia Dediu and Carlos Mart??n-Vide, editors, Pro-
ceedings of the 4th International Conference on Lan-
guage and Automata Theory and Applications (LATA
2010), volume 6031 of Lecture Notes in Computer Sci-
ence, pages 380?391, Trier, Germany. Springer.
M. Kearns and L. Valiant. 1989. Cryptographic lim-
itations on learning boolean formulae and finite au-
tomata. In 21st ACM Symposium on Theory of Com-
puting, pages 433?444.
M. J. Kearns and U. Vazirani. 1994. An Introduction to
Computational Learning Theory. MIT press.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In 40th Annual Meeting of the Association
for Computational Linguistics; Philadelphia:PA, USA,
pages 128?135. Association for Computational Lin-
guistics, July. yes.
D. Klein. 2004. Corpus-based induction of syntactic
structure: Models of dependency and constituency. In
42th Annual Meeting of the Association for Computa-
tional Linguistics; Barcelona, Spain, pages 479?486.
G. Kobele. 2006. Generating Copies: An Investigation
into Structural Identity in Language and Grammar.
Ph.D. thesis, University of California, Los Angeles.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4(35?56).
R. McNaughton and S. Papert. 1971. Counter-Free Au-
tomata. MIT Press.
M. Mohri. 1997. Finite-state transducers in language
and speech processing. Computational Linguistics,
23(2):269?311.
S. Muggleton. 1990. Inductive Acquisition of Expert
Knowledge. Addison-Wesley.
J. Oncina and P. Garc??a. 1992. Identifying regular lan-
guages in polynomial time. In H. Bunke, editor, Ad-
vances in Structural and Syntactic Pattern Recogni-
tion, volume 5 of Series in Machine Perception and
Artificial Intelligence, pages 99?108. World Scientific.
J. Oncina, P. Garc??a, and E. Vidal. 1993. Learning sub-
sequential transducers for pattern recognition tasks.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 15:448?458, May.
L. Pitt. 1985. Probabilistic Inductive Inference. Ph.D.
thesis, Yale University. Computer Science Depart-
ment, TR-400.
L. Pitt. 1989. Inductive inference, DFA?s, and compu-
tational complexity. In Analogical and Inductive In-
ference, number 397 in LNAI, pages 18?44. Springer-
Verlag.
J. Rogers and G. Pullum. to appear. Aural pattern recog-
nition experiments and the subregular hierarchy. Jour-
nal of Logic, Language and Information.
J. Rogers, J. Heinz, Gil Bailey, Matt Edlefsen, Molly
Visscher, David Wellcome, and Sean Wibel. 2010.
On languages piecewise testable in the strict sense. In
Christian Ebert, Gerhard Ja?ger, and Jens Michaelis,
editors, The Mathematics of Language, volume 6149
of Lecture Notes in Artifical Intelligence, pages 255?
265. Springer.
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333?343.
I. Simon. 1975. Piecewise testable events. In Automata
Theory and Formal Languages, pages 214?222.
Z. Solan, D. Horn, E. Ruppin, and S. Edelman. 2005.
Unsupervised learning of natural languages. Proceed-
ings of the National Academy of Sciences of the United
States of America, 102(33):11629?11634, August.
A. Stolcke. 1994. Bayesian Learning of Probabilistic
Language Models. Ph.D. thesis, University of Califor-
nia, Berkeley.
I. Tellier. 2008. How to split recursive automata. In
ICGI, pages 200?212.
E. Ukkonen. 1995. On-line construction of suffix trees.
Algorithmica, 14:249?260.
L. G. Valiant. 1984. A theory of the learnable. Commu-
nications of the Association for Computing Machinery,
27(11):1134?1142.
M. van Zaanen and P. W. Adriaans. 2001. Alignment-
Based Learning versus EMILE: A comparison. In
Proceedings of the Belgian-Dutch Conference on Ar-
tificial Intelligence (BNAIC); Amsterdam, the Nether-
lands, pages 315?322, October.
M. van Zaanen. 2000a. ABL: Alignment-Based
Learning. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING);
Saarbru?cken, Germany, pages 961?967. Association
for Computational Linguistics, July 31?August 4.
M. van Zaanen. 2000b. Bootstrapping syntax and recur-
sion using Alignment-Based Learning. In P. Langley,
editor, Proceedings of the Seventeenth International
Conference on Machine Learning; Stanford:CA, USA,
pages 1063?1070, June 29?July 2.
M. van Zaanen. 2002. Bootstrapping Structure into Lan-
guage: Alignment-Based Learning. Ph.D. thesis, Uni-
versity of Leeds, Leeds, UK, January.
Marco R. Vervoort. 2000. Games, Walks and Grammars.
Ph.D. thesis, University of Amsterdam, Amsterdam,
the Netherlands, September.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005a. Probabilistic finite-state
machines-part I. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(7):1013?1025.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005b. Probabilistic finite-state
machines-part II. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(7):1026?1039.
R. A. Wagner and M. J. Fischer. 1974. The string-to-
string correction problem. Journal of the Association
for Computing Machinery, 21(1):168?173.
R. Wiehagen, R. Frievalds, and E. Kinber. 1984. On the
power of probabilistic strategies in inductive inference.
Theoretical Computer Science, 28:111?133.
T. Yokomori. 2003. Polynomial-time identification of
very simple grammars from positive data. Theoretical
Computer Science, 298(1):179?206.
R. Yoshinaka. 2009. Learning mildly context-sensitive
languages with multidimensional substitutability from
positive data. In R. Gavalda`, G. Lugosi, T. Zeugmann,
and S. Zilles, editors, Proceedings of the Workshop on
Algorithmic Learning Theory, volume 5809 of Lecture
Notes in Computer Science, pages 278?292. Springer
Berlin / Heidelberg.
