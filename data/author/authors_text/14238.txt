Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 222?225,
Paris, October 2009. c?2009 Association for Computational Linguistics
Interactive Predictive Parsing 1
Ricardo Sa?nchez-Sa?ez, Joan-Andreu Sa?nchez and Jose?-Miguel Bened??
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
Cam?? de Vera s/n, Valencia 46022 (Spain)
{rsanchez, jandreu, jbenedi}dsic.upv.es
Abstract
This paper introduces a formal framework
that presents a novel Interactive Predic-
tive Parsing schema which can be oper-
ated by a user, tightly integrated into the
system, to obtain error free trees. This
compares to the classical two-step schema
of manually post-editing the erroneus con-
stituents produced by the parsing system.
We have simulated interaction and cal-
culated evalaution metrics, which estab-
lished that an IPP system results in a high
amount of effort reduction for a manual
annotator compared to a two-step system.
1 Introduction
The aim of parsing is to obtain the linguistic in-
terpretation of sentences, that is, their underlying
syntactic structure. This task is one of the fun-
damental pieces needed by a computer to uns-
derstand language as used by humans, and has
many applications in Natural Language Process-
ing (Lease et al, 2006).
A wide array of parsing methods exist, in-
cluding those based on Probabilistic Context-Free
Grammars (PCFGs). (Charniak, 2000; Collins,
2003; Johnson, 1998; Klein and Manning, 2003;
Matsuzaki et al, 2005; Petrov and Klein, 2007).
The most impressive results are achieved by sub-
tree reranking systems, as shown in the semi-
supervised method of (McClosky et al, 2006),
or the forest reranking approximation of (Huang,
2008) in which packed parse forests (compact
structures that contain many possible tree deriva-
tions) are used.
These state-of-the-art parsers provide trees of
excelent quality. However, perfect results are vir-
1Work supported by the MIPRCV ?Consolider Inge-
nio 2010? (CSD2007-00018), iTransDoc (TIN2006-15694-
CO2-01) and Prometeo (PROMETEO/2009/014) reserach
projects, and the FPU fellowship AP2006-01363.
tually never achieved. If the need of one-hundred-
percent error free trees arises, the supervision of a
user that post-edits and corrects the errors is un-
avoidable.
Error free trees are needed in many tasks such as
handwritten mathematical expressions recognition
(Yamamoto et al, 2006), or creation of new gold
standard treebanks (Delaclergerie et al, 2008)).
For example, in the creation of the Penn Tree-
bank grammar, a basic two-stage setup was em-
ployed: a rudimentary parsing system providad a
skeletal syntactic representation, which then was
manually corrected by human annotators (Marcus
et al, 1993).
In this paper, we introduce a new formal frame-
work that tightly integrates the user within the
parsing system itself, rather than keeping him iso-
lated from the automatic tools used in a classi-
cal two-step approach. This approach introduces
the user into the parsing system, and we will call
it ?interactive predictive parsing?, or simply IPP.
An IPP system is interactive because the user is in
continuous contact with the parsing process, send-
ing and receiving feedback. An IPP system is also
predictive because it reacts to the user corrections:
it predicts and suggest new parse trees taking into
account the new gold knowledge received from
the user. Interactive predictive methods have been
studied and successfully used in fields like Auto-
matic Text Recognition (Toselli et al, 2008) and
Statistical Machine Translation (Barrachina et al,
2009; Vidal et al, 2006) to ease the work of tran-
scriptor and translators.
Assessment of the amount of effort saved by the
IPP system will be measured by automatically cal-
culated metrics.
2 Interactive Predictive Parsing
A tree t, associated to a string x1|x|, is composed
by substructures that are usually referred as con-
stituents or edges. A constituent cAij is a span de-
222
fined by a nonterminal symbol (or syntactic tag) A
that covers the substring xij .
Assume that using a given probabilistic context-
free grammar G as the model, the parser analyzes
the input sentence x = x1 . . . x|x| and produces
the parse tree t?
t? = argmax
t?T
pG(t|x), (1)
where pG(t|x) is the probability of parse tree t
given the input string x using model G, and T is
the set of all possible parse trees for x.
In an interactive predictive scenario, after ob-
taining the (probably incorrect) best tree t?, the user
is able to modify the edges cAij that are incorrect.
The system reacts to each of the corrections intro-
duced by the human by proposing a new t?? that
takes into account the corrected edge. The order
in which incorrect constituents are reviewed deter-
mines the amount of effort reduction given by the
degree of correctness of the subsequent proposed
trees.
There exist several ways in which a human ana-
lyzes a sentende. A top-to-bottom may be consid-
ered natural way of proceeding, and we follow this
approach in this work. This way, when a higher
level constituent is corrected, possible erroneous
constituents at lower levels are expectedly auto-
matically recalculated.
The introduced IPP interaction process is sim-
ilar to the ones already established in Computer-
Assisted Text Recognition and Computer-Assisted
Translation 1.
Within the IPP framework, the user reviews the
constituents contained in the tree to assess their
correctness. When the user find an incorrect edge
he modifies it, setting the correct label and span.
This action implicitly validates a subtree that is
composed by the corrected edge plus all its ances-
tor edges, which we will call the validated prefix
tree tp. When the user replaces the constituent cAij
with the correct one c?Aij , the validated prefix tree
is:
tp(c?Aij ) = {cBmn : m ? i, n ? j
d(cBmn) ? d(c?Aij )}
(2)
with d(cDpq) being the depth of constituent cDpq.
1In these fields, the user reads the sentence from left to
right. When the user finds and corrects an erroneus word, he
is implicitly validating the prefix sentence up to that word.
The remaining suffix sentence is recalculated by the system
taking into account the validated prefix sentece.
When a constituent correction is performed, the
prefix tree tp(c?Aij ) is fixed and a new tree t?? that
takes into account the prefix is proposed
t?? = argmax
t?T
pG(t|x, tp(c?Aij )). (3)
Given that we are working with context-free
grammars, the only subtree that effectively needs
to be recalcuted is the one starting from the par-
ent of the corrected edge. Let the corrected edge
be c?Aij and its parent cDst, then the following tree is
proposed
t?? = argmax
t?T
pG(t|x, tp) = (t? \ t?Dst) ? t??Dst , (4)
with
t??Dst = argmax
tDst?Tst
pG(tDst|xmn, c?Aij ) . (5)
Expression (4) represents the newly proposed
tree t??, which consists of original proposed tree
t? minus the subpart of the original proposed tree
t?Dst (whose root is the parent of the corrected edge
cDst) plus the newly calculated subtree t??
D
st (whose
root is also the parent of the corrected constituent
cDst, but also takes into account the corrected one
as shown in Expression (5)).
In Figure 1 we show an example that intends to
clarify the interactive predictive process. First, the
system provides a proposed parse tree (Fig. 1.a).
Then the user, which has in his mind the correct
reference tree, notices that it has two wrong con-
stituents (cX23 and cZ44) (Fig. 1.b), and choses to re-
place cX23 by cB22 (Fig. 1.c). Here, cB22 corresponds
to c?Aij from expressions (3) and (5).
As the user does this correction, the system au-
tomatically validates the correct prefix: all the an-
cestors of the modified constituent (dashed line in
the figure, tp(c?Aij ) from expression (2)). The sys-
tem also invalidates the subtrees related to the cor-
rected constituent (dotted line line in the figure, t?Dst
from expression (4)).
Finally, the system automatically predicts a new
subtree (t??Dst from expression (4)) (Fig. 1.d). No-
tice how cZ34 changes its span and cD44 is introduced
which provides the correct reference parse.
Within the example shown in Figure 1, the user
would obtain the gold tree with just one correction,
rather than the three operations needed on a two-
step system (one deletion, one substitution and one
insertion).
223
SB Z
Y
ba c d
A
DC
(a) Reference tree
S
ba c d
A
CB
X
Y
Z
(b) Iteration 0:
Proposed out-
put tree 1
S
ba c d
A
CB
X Z 423 4
Y
(c) Iteration 0: Er-
roneus constituents
S
ba c d
A
B 2
2 ?
? ?
Y
(d) Iteration 1:
User corrected
constituent
S
B Z
Y
ba c d
A
DC
3
4
(e) Iteration 1:
Proposed output
tree 2
Figure 1: Synthetic example of user interaction with the IPP system.
3 IPP Evaluation
The objective of the experimentation presented
here is to evaluate the amount of effort saved for
the user using the IPP system, compared to the ef-
fort required to manually correct the trees without
the use of an interactive system. In this section, we
define a standard automatic evaluation protocol,
akin to the ones used in Computer-Aided Trans-
lation and Computer Aided Text Recognition.
In the absence of testing of an interactive sys-
tem with real users, the gold reference trees were
used to simulate system interaction by a human
corrector. In order to do this, the constituents in
the proposed tree were automatically reviewed in a
preorder manner 2. In each step, the constituent in
the proposed tree was compared to the correspond-
ing one in the reference tree: if the constituent was
equivalent no action was taken. When one incor-
rect constituent was found in the proposed tree, it
was replaced by the correct one from the reference
tree. This precise step simulated what a human su-
pervisor would do, that is, to type the correct con-
stituent in place of the erroneus one.
The system then performed the predictive step
(i.e. recalculation of subtrees related to the cor-
rected constituent). We kept a correction count,
which was incremented by one after each predic-
tive step.
3.1 Evaluation metrics
For evaluation, first we report a metric represent-
ing the amount of human correcting work needed
to obtain the gold tree in a classical two-step pro-
cess (i.e. the number of operations needed to post-
edit the proposed tree in orther to obtain the gold
2Interaction in this ordered manner guaranteed that the
evaluation protocol only needed to modify the label A and
the end point j of a given edge cAij , while i remained valid
given the modifications of previous constituents.
one). We then compare this value to a metric that
measures the amount of effort needed to obtain
the gold tree with the human interacting within the
presented IPP system.
Parsing quality is generally assessed by the clas-
sical evaluation metrics, precission, recall and F-
measure. We defined the following metric that
measures the amount of effort needed in order to
post-edit a proposed tree and obtain the gold ref-
erence parse tree, akin to the Word Error Rate
used in Statistical Machine Translation and related
fields:
? Tree Constituent Error Rate (TCER): Min-
imum number of constituent substitution,
deletion and insertion operations needed to
convert the proposed parse tree into the corre-
sponding gold reference tree, divided by the
total number of constituents in the reference
tree 3.
The TCER is in fact strongly related to the F-
measure: the higher the F-measure is, the lower
TCER will be.
Finally, the relevant evaluation metric that as-
sessed the IPP system performance represents the
amount effort that the operator would have to
spend using the system in order to obtain the gold
tree, and is directly comparable to the TCER:
? Tree Constituent Action Rate (TCAC): Num-
ber of constituent corrections performed us-
ing the IPP system to obtain the reference
tree, divided by the total number of con-
stituents in the reference tree.
4 Experimental results
An IPP system was implemented over the classical
CYK-Viterbi algorithm. Experimentation was run
3Edit distance is calcualted over the ordered set of tree
constituents. This is an approximation of the edit distance
between trees.
224
over the Penn Tree bank: sections 2 to 21 were
used to obtain a vanilla Penn Treebank Grammar;
test set was the whole section 23.
We obtained several binarized versions of the
train grammar for use with the CYK. The Chom-
sky Normal Form (CNF) transformation method
from the NLTK4 was used to obtain several right-
factored binary grammars of different sizes 5.
A basic schema was introduced for parsing sen-
tences with out-of-vocabulary words: when an
input word could not be derived by any of the
preterminals in the vanilla treebank grammar, a
very small probability for that word was uniformly
added to all of the preterminals.
Results for the metrics discussed on section 3.1
for different markovizations of the train grammar
can be seen in Table 1. We observe that the perc-
etage of corrections needed using the IPP system
is much lower than the rate of needed corrections
just post-editing the proposed trees: from 42% to
46% in effort reduction by the human supervisor.
These results clearly show that an interactive
predictive system can relieve manual annotators of
a lot of burden in their task.
Note that the presented experiments were done
using parsing models that perform far from the lat-
est F1 results; their intention was to assess the util-
ity of the IPP schema. Expected relative reduc-
tions with IPP systems incorporating state-of-the-
art parsers would not be so large.
PCFG Baseline IPP RelRedF1 TCER TCAC
h=0, v=1 0.67 0.40 0.22 45%
h=0, v=2 0.68 0.39 0.21 46%
h=0, v=3 0.70 0.38 0.22 42%
Table 1: Results for the test set: F1 and TCER
for the baseline system; TCAC for the IPP system;
relative reduction beteween TCER and TCAC.
5 Conclusions
We have introduced a novel Interactive Predictive
Parsing framewrok which can be operated by a
user to obtain error free trees. We have simulated
interaction with this system and calculated evalau-
tion metrics, which established that an IPP system
results in a high amount of effort reduction for a
manual annotator compared to a two-step system.
4http://nltk.sourceforge.net/
5This method implements the vertical (v value) and hori-
zontal (h value) markovizations (Klein and Manning, 2003).
Near term future work includes applying the
IPP scenario to state-of-the-art reranking and pars-
ing systems, as well as in the development of adap-
tative parsing systems
References
Barrachina, Sergio, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jess Toms, En-
rique Vidal, Juan-Miguel Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. In Com-
putational Linguistics, 35(1) 3-28.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In NAACL ?00, 132-139.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. In Computational
Linguistics, 29(4):589-637.
De la Clergerie, ?Eric, Olivier Hamon, Djamel Mostefa,
Christelle Ayache, Patrick Paroubek and Anne Vil-
nat. 2008. PASSAGE: from French Parser Evalua-
tion to Large Sized Treebank. In LREC?08.
Huang, Liang. 2008. Forest reranking: discriminative
parsing with non-local features. In ACL ?08.
Johnson, Mark. 1998. PCFG models of linguistic
tree representation. In Computational Linguistics,
24:613-632.
Klein, Dan and Chistopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In ACL ?03, 423-430.
Lease, Matthew, Eugene Charniak, Mark Johnson and
David McClosky. 2006. A look at parsing and its
applications. In National Conference on Artificial
Intelligence, vol. 21-II, 1642-1645.
Marcus, Mitchell P., Mary Ann Marcinkiewicz and
Beatrice Santorini. 1995. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics 19(2), 313-330.
Matsuzaki, Takuya, Yasuke Miyao and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL ?05, 75-82.
McClosky, David, Eugene Charniak and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL ?06
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT ?07.
Toselli, Alejandro, Vero?nica Romero and Enrique Vi-
dal. 2008. Computer Assisted Transcription of Text
Images and Multimodal Interaction. In MLMI ?08.
Vidal, Enrique, Francisco Casacuberta, Luis Ro-
driguez, Jorge Civera and Carlos D. Martnez Hinare-
jos. 2006. Computer-assisted translation using
speech recognition. In IEEE Trans. on Audio,
Speech, and Language Processing, 14(3), 941-951.
Yamamoto, Ryo, Shinji Sako, Takuya Nishimoto and
Shigeki Sagayama. 2006. On-line recognition
of handwritten mathematical expressions based on
stroke-based stochastic context-free grammar. In
10th International Workshop on Frontiers in Hand-
writing Recognition.
225
Coling 2010: Poster Volume, pages 1220?1228,
Beijing, August 2010
Confidence Measures for Error Discrimination
in an Interactive Predictive Parsing Framework1
Ricardo Sa?nchez-Sa?ez, Joan Andreu Sa?nchez and Jose? Miguel Bened
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
{rsanchez,jandreu,jbenedi}@dsic.upv.es
Abstract
We study the use of Confidence Measures
(CM) for erroneous constituent discrimi-
nation in an Interactive Predictive Parsing
(IPP) framework. The IPP framework al-
lows to build interactive tree annotation
systems that can help human correctors
in constructing error-free parse trees with
little effort (compared to manually post-
editing the trees obtained from an auto-
matic parser). We show that CMs can
help in detecting erroneous constituents
more quickly through all the IPP process.
We present two methods for precalculat-
ing the confidence threshold (globally and
per-interaction), and observe that CMs re-
main highly discriminant as the IPP pro-
cess advances.
1 Introduction
Within the Natural Language Processing (NLP)
field, we can tell apart two different usage scenar-
ios for automatic systems that output or work with
natural language. On one hand, we have the cases
in which the output of such systems is expected to
be used in a vanilla fashion, that is, without val-
idating or correcting the results produced by the
system. Within this usage scheme, the most im-
portant factor of a given automatic system is the
quality of the results. Although memory and com-
putational requirements of such systems are usu-
ally taken into account, the ultimate aim of most
1Work partially supported by the Spanish MICINN under
the MIPRCV ?Consolider Ingenio 2010? (CSD2007-00018),
MITTRAL (TIN2009-14633-C03-01), Prometeo (PROME-
TEO/2009/014) research projects, and the FPU fellowship
AP2006-01363.
research that relates to this scenario is to minimize
the amount of error (measured with metrics like
Word Error Rate, BLEU, F-Measure, etc.) present
within the results that are being produced.
The second usage scenario arises when there
exists the need for perfect and completely error-
free results, for example, flawlessly translated
sentences or correctly annotated syntactic trees.
In such cases, the intervention of a human valida-
tor/corrector is unavoidable. The corrector will
review and validate the results, making the suit-
able modifications before the system output can
be employed. In these kind of tasks, the most im-
portant factor to be minimized is the human ef-
fort that has to be applied to transform the sys-
tem?s potentially incorrect output into validated
and error-free output. Measuring user effort has
an intrinsic subjectivity that makes it hard to be
quantitatized. Given that the user effort is usually
inversely proportional to the quality of the system
output, most research about problems associated
to this scenario t to minimize just the system?s er-
ror rate as well.
Interactive Predictive NLP Systems
Only recently, more comparable and repro-
ducible evaluation methods for Interactive Natural
Language Systems have started to be developed,
within the context of Interactive Predictive Sys-
tems (IPS). These systems formally integrate the
correcting user into the loop, making him part of
the system right at its theoretical framework. IPSs
allow for human correctors to spare effort because
the system updates its output after each individ-
ual user correction, potentially fixing several er-
rors at each step. Interactive Predictive methods
have been studied and successfully used in fields
1220
like Handwritten Text Recognition (HTR) (Toselli
et al, 2008) and Statistical Machine Translation
(SMT) (Vidal et al, 2006; Barrachina et al, 2009)
to ease the work of transcriptors and translators.
In IPS related research the importance of the
system base error rate per se is diminished. In-
stead, the intention is to measure how well the
user and the system work together. For this, for-
mal user simulation protocols together with new
objective effort evaluation metrics such as the
Word Stroke Ratio (WSR) (Toselli et al, 2008) or
the Key-Stroke and Mouse-Ratio (KSMR) (Bar-
rachina et al, 2009) started to be used as a
benchmark. These ratios reflect the amount of
user effort (whole-word corrections in the case of
WSR; keystrokes plus mouse actions in the case of
KSMR) given a certain output. To get the amount
of user effort into context they should be measured
against the corresponding error ratios of compara-
ble non-interactive systems: Word Error Rate for
WSR and Character Error Rate for KSMR.
This dichotomy in evaluating either system per-
formance or user effort applies to Syntactic Pars-
ing as well. The objective of parsing is to pre-
cisely determine the syntactic structure of sen-
tences written in one of the several languages that
humans use. Very bright research has been carried
out in this field, resulting in several top perform-
ing completely automatic parsers (Collins, 2003;
Klein and Manning, 2003; McClosky et al, 2006;
Huang, 2008; Petrov, 2010). However, these pro-
duce results that are erroneous to some extent, and
as such unsuitable for some applications without a
previous manual correction. There are many prob-
lems where error-free results consisting in per-
fectly annotated trees are needed, such as hand-
written mathematical expression recognition (Ya-
mamoto et al, 2006) or construction of large new
gold treebanks (de la Clergerie et al, 2008).
When using automatic parsers as a baseline for
building perfect syntactic trees, the role of the
human annotator is usually to post-edit the trees
and correct the errors. This manner of operat-
ing results in the typical two-step process for er-
ror correcting, in which the system first gener-
ates the whole output and then the user verifies
or amends it. This paradigm is rather inefficient
and uncomfortable for the human annotator. For
example, a basic two-stage setup was employed
in the creation of the Penn Treebank annotated
corpus: a rudimentary parsing system provided a
skeletal syntactic representation, which then was
manually corrected by human annotators (Marcus
et al, 1994). Additional works within this field
have presented systems that act as a computerized
aid to the user in obtaining the perfect annotation
(Carter, 1997; Oepen et al, 2004; Hiroshi et al,
2005). Subjective measuring of the effort needed
to obtain perfect annotations was reported in some
of these works, but we feel that a more compara-
ble metric is needed.
With the objective of reducing the user effort
and making the laborious task of tree annotation
easier, the authors of (Sa?nchez-Sa?ez et al, 2009a)
devised an Interactive Predictive Parsing (IPP)
framework. That work embeds the human cor-
rector into the automatic parser, and allows him
to interact in real time within the system. In this
manner, the system can use the readily available
user feedback to make predictions about the parts
of the trees that have not been validated by the
corrector. The authors simulated user interaction
and calculated effort evaluation metrics, establish-
ing that an IPP system results in amounts slightly
above 40% of effort reduction for a manual anno-
tator compared to a two-step system.
Confidence Measures in NLP
Annotating trees syntactically, even with the
aid of automatic systems, generally requires hu-
man intervention with a high degree of special-
ization. This fact partially justifies the shortage
in large manually annotated treebanks. Endeavors
directed at easing the burden for the experts per-
forming this task could be of great help.
One approach that can be followed in reducing
user effort within an IPS is adding information
that helps the user to locate the individual errors
in a sentence, so he can correct them in a hastier
fashion. The use of the Confidence Measure (CM)
formalism goes in this direction, allowing us to
assign a probability of correctness for individual
erroneous constituents of a more complex output
block of a NLP system.
In fields such as HTR, SMT or Automatic
Speech Recognition (ASR), the output sentences
1221
have a global probability (or score) that reflects
the likeness of the output sentence being correct.
CMs allow precision beyond the sentence level in
predicting errors: they can be used to label the in-
dividual words as either correct or incorrect. Au-
tomatic systems can use CMs to help the user in
identifying the erroneous parts of the output in a
faster way or to aid with the amendments by sug-
gesting replacement words that are likely to be
correct.
Previous research shows that CMs have been
successfully applied within the ASR (Wessel et
al., 2001), HTR (Tarazo?n et al, 2009; Serrano
et al, 2010) and SMT (Ueffing and Ney, 2007)
fields. In these works, the ability of CMs in de-
tecting erroneous constituents is assessed by the
classical confidence metrics: the Confidence Er-
ror Rate (CER) and the Receiver Operating Char-
acteristic (ROC) (Ueffing and Ney, 2007).
However, until recent advances, the use of CMs
remained largely unexplored in Parsing. Assess-
ing the correctness of the different parts of a pars-
ing tree can be useful in improving the efficiency
and usability of an IPP system, not only by tag-
ging parts with low confidence for the user to re-
view, but also by automating part of the correction
process itself by presenting constituents that yield
a higher confidence when an error is confirmed by
the user.
CMs for parsing in the form of combinations
of features calculated from n-best lists were pro-
posed in (Bened?? et al, 2007). Later on, the au-
thors of (Sa?nchez-Sa?ez et al, 2009b) introduced
a statistical method for calculating a CM for each
of the constituents in a parse tree. In that work,
CMs are calculated using the posterior probability
of each tree constituent, approach which is similar
to the word-graph based methods in the ASR and
SMT fields.
In this paper, we apply Confidence Measures
to the Interactive Predictive Parsing framework to
asses how CMs are increasingly more accurate as
the user validates subtrees within the interactive
process. We prove that after each correction per-
formed by the user, the CMs of the remaining un-
validated constituents are more helpful to detect
errors.
2 Interactive Predictive Parsing
In this section we review the IPP framework
(Sa?nchez-Sa?ez et al, 2009a) and its underlying
operation protocol. In parsing, a syntactic tree t,
attached to a string x = x1 . . . x|x| is composed
by substructures called constituents. A constituent
cAij is defined by the nonterminal symbol (either
a syntactic label or a POS tag) A and its span
ij (the starting and ending indexes which delimit
the part of the input sentence encompassed by the
constituent).
Here follows a general formulation for the non-
interactive syntactic parsing scenario, which will
allow us to better introduce the IPP formulation.
Assume that using a given parsing model G, the
parser analyzes the input sentence x and produces
the most probable parse tree
t? = argmax
t?T
pG(t|x), (1)
where pG(t|x) is the probability of the parse tree
t given the input string x using model G, and T is
the set of all possible parse trees for x.
In the IPP framework, the manual corrector
provides feedback to the system by correcting any
of the constituents cAij from t?. The system reacts
to each of the corrections performed by the human
annotator by proposing a new t?? that takes into ac-
count the correction.
Within the IPP framework, the user reviews the
constituents contained in the tree to assess their
correctness. When the user finds an incorrect con-
stituent he modifies it, setting the correct span and
label. This action implicitly validates what it is
called the validated prefix tree tp.
We define the validated prefix tree to be com-
posed by the partially corrected constituent, all
of its ancestor constituents, and all constituents
whose end span is lower than the start span of the
corrected constituent. When the user replaces the
constituent cAij with the correct one c?Aij , the vali-
dated prefix tree is
tp(c?Aij ) = {cBmn : m ? i, n ? j ,
d(cBmn) ? d(c?Aij )} ?
{cDpq : q < i }
(2)
1222
with d(cZab) being the depth (distance from root)
of constituent cZab.
The validated prefix tree is parallel to the vali-
dated sentence prefix commonly used in Interac-
tive Machine Translation or Interactive Handwrit-
ten Recognition, and is established after each user
action.
This particular definition of the prefix tree de-
termines the fact that the user is expected to re-
view the parse tree in a preorder fashion (left-to-
right depth-first). Note that this specific explo-
ration order allows us to simulate the user inter-
action for the experimentation, as we will explain
below. Also note that other types of prefixes could
be defined, allowing for different tree review or-
ders.
Within the IPP formulation, when a constituent
correction is performed, the prefix tree tp(c?Aij ) is
validated and a new tree t?? that takes into account
the prefix is proposed. Incorporating this new
evidence into expression (1) yields the following
equation
t?? = argmax
t?T
pG(t|x, tp(c?Aij )). (3)
Given the properties of Probabilistic Context-
Free Grammars (PCFG) the only subtree that ef-
fectively needs to be recalculated is the one start-
ing from the parent of the corrected constituent.
This way, just the descendants of the newly intro-
duced constituent, as well as its right hand siblings
(along with their descendants) are calculated.
2.1 User Interaction Operation
The IPP formulation allows for a very straightfor-
ward operation protocol that is performed by the
manual corrector, in which he validates or corrects
the successive output parse trees:
1. The IPP system proposes a full parse tree t
for the input sentence.
2. Then, the user finds the first incorrect con-
stituent exploring the tree in a certain ordered
manner (preorder in our case, given by the
tree prefix definition) and amends it, by mod-
ifying its span and/or label (implicitly vali-
dating the prefix tree tp).
3. The IPP system produces the most probable
tree that is compatible with the validated pre-
fix tree tp as shown in expression (3).
4. These steps are iterated until a final, perfect
parse tree is produced by the system and val-
idated by the user.
It is worth noting that within this protocol, con-
stituents can be automatically deleted or inserted
at the end of any subtree in the syntactic struc-
ture by adequately modifying the span of the left-
neighbouring constituent.
The IPP interaction process is similar to the
ones already established in HTR and SMT. In
these fields, the user reads the output sentence
from left to right. When the user finds and corrects
an erroneous word, he is implicitly validating the
prefix sentence up to that word. The remaining
suffix sentence is recalculated by the system tak-
ing into account the validated prefix sentence.
Fig. 1 shows an example that intends to clar-
ify the Interactive Predictive process. First, the
system provides a tentative parse tree (Fig. 1.b).
Then the user, which has the correct reference tree
(Fig. 1.a) in mind, notices that it has two wrong
constituents (cX23 and cZ44) (Fig. 1.c), and chooses
to replace cX23 by cB22 (Fig. 1.d). Here, cB22 cor-
responds to c?Aij of expression (3). As the user
does this correction, the system automatically val-
idates the prefix (dashed line in Fig. 1.d, tp(c?Aij )
of expression (2)). The system also invalidates
the subtrees outside the prefix (dotted line line in
Fig. 1.d). Finally, the system automatically pre-
dicts a new subtree (Fig. 1.e). Notice how cZ34
changes its span and cD44 is introduced which pro-
vides the correct reference parse.
For further exemplification, Sa?nchez-Sa?ez
et al (2010) demonstrate an IPP based
annotation tool that can be accessed at
http://cat.iti.upv.es/ipp/.
Within the IPP scenario, the user has to man-
ually review all the system output and correct or
validate it, which is still a considerable amount of
effort. CMs can ease this work by helping to spot
the erroneous constituents.
1223
SB Z
Y
ba c d
A
DC
(a) Reference tree
S
ba c d
A
CB
X
Y
Z
(b) Iteration 0: Pro-
posed output tree 1
S
ba c d
A
CB
X Z 423 4
Y
(c) Iteration 0: Erro-
neous constituents
Y
S
ba c d
A
B 2
2 ?
? ?
(d) Iteration 1:
User corrected
constituent
S
B Z
Y
ba c d
A
DC
3
4
(e) Iteration 1:
Proposed output
tree 2
Figure 1: Synthetic example of user interaction with the IPP system.
3 Confidence Measures
Probabilistic calculation of Confidence Measures
(Sa?nchez-Sa?ez et al, 2009b) for all tree con-
stituents can be introduced within the IPP process.
The CM of each constituent is its posterior
probability, which can be considered as a measure
of the degree to which the constituent is believed
to be correct for a given input sentence x. This is
formulated as follows
pG(cAij |x) =
pG(cAij ,x)
pG(x)
=
?
t??T ; c?Aij ?t? ?(c
A
ij , c?Aij ) pG(t?|x)
pG(x)
(4)
with ?() being the Kronecker delta function. Nu-
merator in expression (4) stands for the probabil-
ity of all parse trees for x that contain the con-
stituent cAij (see Fig. 2).
S
A
?A(i, j)
?A(i, j)
x1 xi?1 xi xj xj+1 x|x|
Figure 2: The product of the inside and outside
probabilities for each constituent comprises the
upper part of expression (5)
The posterior probability is computed with the
inside ? and outside ? probabilities (Baker, 1979)
C(tAij) = pG(cAij |x) =
pG(cAij ,x)
pG(x)
= ?A(i, j) ?A(i, j)?S(1, |x|)
.
(5)
It should be clear that the calculation of con-
fidence measures reviewed here is generalizable
for any problem that employs PCFGs, and not
just NLP tasks. In the experiments presented in
the following section we show that CMs are in-
creasingly discriminant when used within the IPP
framework to detect erroneous constituents.
4 Experiments
Evaluation of the quality of CMs within the IPP
framework is done in a completely automatic
fashion by simulating user interaction. Section 4.1
introduces the evaluation protocol and metrics
measuring CM quality (i.e., their ability to de-
tect incorrect constituents). The experimentation
framework and the results are discussed in sec-
tion 4.2.
4.1 Evaluation Methods
4.1.1 IPP Evaluation
A good measure of the performance of an In-
teractive Predictive System is the amount of ef-
fort saved by the users of such a system. It is
subjective and expensive to test an IPS with real
users, so these systems are usually evaluated us-
ing automatically calculated metrics that assess
the amount of effort saved by the user.
1224
As already mentioned, the objective of an IPP
based system is to be employed by annotators to
construct correct syntactic trees with less effort.
Evaluation of an IPP system was previously done
by comparing the IPP usage effort (the number of
corrections using the IPP system) against the es-
timated effort required to manually post-edit the
trees after obtaining them with a traditional au-
tomatic parsing system (the amount of incorrect
constituents) (Sa?nchez-Sa?ez et al, 2009a).
In the case of IPP, the gold reference trees are
used to simulate system interaction by a human
corrector and provide a comparable benchmark.
This automatic evaluation protocol is similar to
the one presented in section 2.1:
1. The IPP system proposes a full parse tree t
for the input sentence.
2. The user simulation subsystem finds the first
incorrect constituent by exploring the tree in
the order defined by the prefix tree definition
(preorder) and comparing it with the refer-
ence. When the first erroneous constituent
is found, it is amended by being replaced in
the output tree by the correct one, operation
which implicitly validates the prefix tree tp.
3. The IPP system produces the most probable
tree that is compatible with the validated pre-
fix tree tp.
4. These steps are iterated until a final, perfect
parse tree is produced by the IPP system and
validated against the reference by the user
simulation subsystem.
In this work, metrics assessing the quality of
CM are introduced within this automatic protocol.
We calculate and report them after each of the it-
erations in the IPP process.
4.1.2 Confidence Measure Evaluation
Metrics
The CM of each tree constituent, computed as
shown in expression (4) can be seen as its prob-
ability of being correct. Once all CM are calcu-
lated, a confidence threshold ? ? [0, 1] can be
chosen. Constituents are then marked using ? : the
ones with a confidence above this threshold are
marked as correct, and the rest as incorrect. Com-
paring the confidence marks in the output tree
with the reference, we obtain the false rejection
Nf (?) ? [0, Nc] (number of correct constituents
in the output tree wrongly marked as incorrect by
their CM) and the true rejection Nt(?) ? [0, Ni]
(number of incorrect constituents in the output
tree that are indeed detected as incorrect by their
confidence).
The amount of correct and incorrect con-
stituents in each tree is Nc and Ni respectively. In
the ideal case of perfectly error discriminant CM,
using the best threshold would yield Nf (?) = 0
and Nt(?) = Ni.
A evaluation metric that assess the ability of
CMs in telling apart correct constituents from in-
correct ones is the Confidence Error Rate (CER):
CER(?) = Nf (?) + (Ni ?Nt(?))Nc +Ni
. (6)
The CER is the number of errors incurred by the
CMs divided by the total number of constituents.
The CER can be compared with the Absolute
Constituent Error Rate (ACER), which is the CER
obtained assuming that all constituents are marked
as correct (the only possible assumption when CM
are not available):
ACER = CER(0) = NiNc +Ni
. (7)
4.2 Experimental Framework
Our experiments were carried out over the Wall
Street Journal Penn Treebank (PTB) manually an-
notated corpus. Three sets were defined over the
PTB: train (sections 2 to 21), test (section 23),
and development (the first 346 sentences of sec-
tion 24). Before carrying out experimentation, the
NoEmpties transformation was applied to all sets
(Klein and Manning, 2001).
We implemented the CYK-Viterbi parsing al-
gorithm as the parse engine within the IPP
framework. This algorithm uses grammars in
the Chomsky Normal Form (CNF) so we em-
ployed the open source Natural Language Toolkit2
(NLTK) to obtain several right-factored binary
2http://www.nltk.org/
1225
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(a) PCFG: h=0,v=1
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(b) PCFG: h=0,v=2
Figure 3: CER results over IPP system interaction. Threshold fixed at before the interactive process.
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(a) PCFG: h=0,v=1
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(b) PCFG: h=0,v=2
Figure 4: CER results over IPP system interaction. Threshold optimized for each step of the interactive
process.
grammars with different markovization parame-
ters from the training set (Klein and Manning,
2003).
The purpose of our experimentation is to de-
termine if CMs can successfully discriminate er-
roneous constituents from correct ones within an
IPP process, that is, if they help the user to find
errors in a hastier manner. For this we need to
assess if there exists discriminant information in
the CMs corresponding to the constituents of the
unvalidated part of the successive IPP-proposed
trees.
With this objective in mind, we introduced a
CM calculation step after each user interaction
within the IPP process. CMs for all constituents in
each tree were obtained as described in section 3.
After each simulated interaction, we also calcu-
lated the ACER and CER over all the syntactic
constituents of the whole test set.
Each IPP user interaction yields a parse tree
which can be seen as the concatenation of two
parts: the validated prefix tree (which is known
to be correct because the user, or the user simula-
tion subsystem in this case, has already reviewed
it) and a new suffix tree which is calculated by
the IPP system based on the validated prefix, as
shown in section 2.
The fact that the validated prefix is already
known to be correct is taken into account by the
CM calculation process, and the confidence of the
constituents in the prefix tree is automatically set
to their maximum score, equal to 1. This fact
causes that the CMs become more discriminant
after each interaction, because a larger part of the
1226
tree (the prefix) has a completely correct confi-
dence. The key point here is to measure if this
increasingly reduced CER (CM error rate) main-
tains its advantage over the also increasingly re-
duced ACER (absolute constituent error rate with-
out taking CMs into account) which would mean
that the CMs retain their discriminant power and
can be useful as an aid for a human annotator us-
ing an IPP system.
Two batches of experiments were performed
and, in each of them, two different markovizations
of the vanilla PCFG were tested as the parsing
model.
In the first battery of experiments, the confi-
dence threshold ? was optimized over the devel-
opment set before starting the IPP process, re-
maining the same during the user interaction. The
results can be seen in Fig. 3, which shows the
obtained baseline ACER and the CER (the con-
fidence assessing metric) for the test set after each
user interaction. We see how CMs retain all of
their error detection capabilities during the IPP
process: in the h0v1 PCFG they are able to dis-
cern about 25% of incorrect constituents at most
stages of the IPP process, with a slight bump up to
27% after about 7 user interactions; for the h0v2
PCFG they are able to detect about 18% of incor-
rect constituents at the first interactions, but go up
to detect 27% of errors after about 7 or more in-
teractions.
In the second experimental setup, a different
threshold for each interaction step was calcu-
lated by performing the IPP user simulation pro-
cess over the development set and optimizing
the threshold value. The results can be seen in
Fig. 4. We observe improvements in the discrim-
inant ability of confidence values after 8 user in-
teractions, with them being capable to detect more
errors towards the end of each IPP session: about
34% of errors for h0v1, and 49% of them for h0v2.
The calculated thresholds have also been plot-
ted in the aforementioned figures. For the per-
interaction threshold experimentation, we can see
how the threshold gets fine-tuned as the IPP pro-
cess advances. The lower threshold values for the
last interactions were expected due to the fact that
more constituents have been validated and have
the maximum confidence. This method for pre-
calculating one specific threshold for each of the
iterations could be useful when incorporating CM
to a real IPP based annotator.
5 Conclusions and Future Work
We have proved that using Confidence Measures
can be used to discriminate incorrect constituents
from correct ones over an Interactive Predictive
Parsing process. We have show two methods
for calculating the threshold used to mark con-
stituents as correct/incorrect, showing the advan-
tage of precalculating a specific threshold for each
of the interaction steps.
Immediate future work involves implementing
CMs as a visual aid in a real IPP system like
the one presented in (Sa?nchez-Sa?ez et al, 2010).
Through he use of CMs, all constituents in the
successive trees could be color-coded according
to their correctness confidence, so the user could
focus and make corrections faster.
Future research paths can deal with applying
CMs to improve the output of completely auto-
matic parsers, for example, using them as a com-
ponent of an n-best re-ranking system.
Additionally, the IPP framework is also suit-
able for studying and applying training algorithms
within the Active Learning and Adaptative/Online
Parsing paradigms. This kind of systems could
improve their models at operating time, by incor-
porating new ground truth data as it is provided by
the user.
References
Baker, JK. 1979. Trainable grammars for speech
recognition. Journal of the Acoustical Society of
America, 65:132.
Barrachina, S., O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Toma?s,
E. Vidal, and J.M. Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. Compu-
tational Linguistics, 35(1):3?28.
Bened??, J.M., J.A. Sa?nchez, and A. Sanch??s. 2007.
Confidence measures for stochastic parsing. In
Proc. of RANLP, pages 58?63, Borovets, Bulgaria,
27-29 September.
Carter, D. 1997. The TreeBanker. A tool for super-
vised training of parsed corpora. In Proc. of EN-
VGRAM Workshop, pages 9?15.
1227
Collins, M. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
de la Clergerie, E.V., O. Hamon, D. Mostefa, C. Ay-
ache, P. Paroubek, and A. Vilnat. 2008. Passage:
from French parser evaluation to large sized tree-
bank. Proc. of LREC, 100:2.
Hiroshi, I., N. Masaki, H. Taiichi, T. Takenobu, and
T. Hozumi. 2005. eBonsai: An integrated environ-
ment for annotating treebanks. In Proc. of IJCNLP,
pages 108?113.
Huang, L. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL.
Klein, D. and C.D. Manning. 2001. Parsing with
treebank grammars: Empirical bounds, theoretical
models, and the structure of the Penn treebank. In
Proc. of ACL, pages 338?345, Morristown, USA.
ACL.
Klein, D. and C.D. Manning. 2003. Accurate unlex-
icalized parsing. In Proc. of ACL, volume 1, pages
423?430, Morristown, USA. ACL.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
McClosky, D., E. Charniak, and M. Johnson. 2006.
Effective self-training for parsing. In Proc. of
NAACL-HLT, pages 152?159.
Oepen, S., D. Flickinger, K. Toutanova, and C.D. Man-
ning. 2004. LinGO Redwoods. Research on Lan-
guage & Computation, 2(4):575?596.
Petrov, S. 2010. Products of Random Latent Variable
Grammars. Proc. of NAACL-HLT.
Sa?nchez-Sa?ez, R., J.A. Sa?nchez, and J.M. Bened??.
2009a. Interactive predictive parsing. In Proc. of
IWPT?09, pages 222?225, Paris, France, October.
ACL.
Sa?nchez-Sa?ez, R., J.A. Sa?nchez, and J.M. Bened??.
2009b. Statistical confidence measures for proba-
bilistic parsing. In Proc. of RANLP, pages 388?392,
Borovets, Bulgaria, September.
Sa?nchez-Sa?ez, R., L.A. Leiva, J.A. Sa?nchez, and J.M.
Bened??. 2010. Interactive predictive parsing using
a web-based architecture. In Proc. of NAACL-HLT,
Los Angeles, United States of America, June.
Serrano, N., A. Sanchis, and A. Juan. 2010. Bal-
ancing error and supervision effort in interactive-
predictive handwriting recognition. In Proc. of IUI,
pages 373?376. ACM.
Tarazo?n, L., D. Pe?rez, N. Serrano, V. Alabau,
O. Ramos Terrades, A. Sanchis, and A. Juan. 2009.
Confidence Measures for Error Correction in Inter-
active Transcription of Handwritten Text. In Proc.
of ICIAP, pages 567?574, Vietri sul Mare, Italy,
September. LNCS.
Toselli, A.H., V. Romero, and E. Vidal. 2008. Com-
puter assisted transcription of text images and mul-
timodal interaction. In Proc. MLMI, volume 5237,
pages 296?308. Springer.
Ueffing, N. and H. Ney. 2007. Word-level confidence
estimation for machine translation. Computational
Linguistics, 33(1):9?40.
Vidal, E., F. Casacuberta, L. Rodr??guez, J. Civera, and
C. Mart??nez. 2006. Computer-assisted translation
using speech recognition. IEEE TASLP, 14(3):941?
951.
Wessel, F., R. Schluter, K. Macherey, and H. Ney.
2001. Confidence measures for large vocabu-
lary continuous speech recognition. IEEE TSAP,
9(3):288?298.
Yamamoto, R., S. Sako, T. Nishimoto, and
S. Sagayama. 2006. On-line recognition of
handwritten mathematical expressions based on
stroke-based stochastic context-free grammar. In
Proc of ICFHR, pages 249?254.
1228
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 653?656,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Enlarged Search Space for SITG Parsing
Guillem Gasc?, Joan-Andreu S?nchez, Jos?-Miguel Bened?
Institut Tecnol?gic d?Inform?tica, Universitat Polit?cnica de Val?ncia
Cam? de Vera s/n, Val?ncia, 46022, Spain
ggasco@iti.upv.es , {jandreu,jbenedi}@dsic.upv.es
Abstract
Stochastic Inversion Transduction Grammars
constitute a powerful formalism in Machine
Translation for which an efficient Dynamic
Programming parsing algorithm exists. In this
work, we review this parsing algorithm and
propose important modifications that enlarge
the search space. These modifications allow
the parsing algorithm to search for more and
better solutions.
1 Introduction
Syntax Machine Translation has received great at-
tention in the last few years, especially for pairs of
languages that are sufficiently non-monotonic. Sev-
eral works have explored the use of syntax for Ma-
chine Translation (Wu, 1997; Chiang, 2007). In
(Wu, 1997), Stochastic Inverse Transduction Gram-
mars (SITGs) were introduced for describing struc-
turally correlated pairs of languages. SITGs can be
used to simultaneously analyze two strings from dif-
ferent languages and to correlate them. An efficient
Dynamic Programming parsing algorithm for SITGs
was presented in (Wu, 1997). This algorithm is sim-
ilar to the CKY algorithm for Probabilistic Context
Free Grammars. The parsing algorithm does not al-
low the association of two items that have the empty
string in one of their sides. This limitation restricts
the search space and prevents the algorithm from ex-
ploring some valid parse trees.
In this paper, we review Wu?s parsing algorithm
for SITGs (referred to as the original algorithm) and
propose some modifications to increase the search
space in order to make it possible to find these valid
parse trees.
2 SITG Parsing
SITGs (Wu, 1997) can be viewed as a restricted
subset of Stochastic Syntax-Directed Transduction
Grammars (Maryanski and Thomason, 1979). For-
mally, a SITG in Chomsky Normal Form can be
defined as a set of lexical rules that are noted as
A ? x/?, A ? ?/y, A ? x/y; direct syntac-
tic rules that are noted as A ? [BC]; and inverse
syntactic rules that are noted as A ? ?BC?, where
A,B,C are non-terminal symbols, x, y are terminal
symbols, ? is the empty string, and each rule has a
probability value p attached. The sum of the proba-
bilities of the rules with the same non-terminal in the
left side must be equal to 1. When a direct syntactic
rule is used in parsing, both strings are parsed with
the syntactic rule A ? BC . When an inverse rule is
used in parsing, one string is parsed with the syntac-
tic rule A ? BC , and the other string is parsed with
the syntactic rule A ? CB.
An efficient Viterbi-like parsing algorithm that is
based on a Dynamic Programming Scheme was pro-
posed in (Wu, 1997). It allows us to obtain the most
probable parse tree that simultaneously analyzes two
strings, X = x1...x|X| and Y = y1...x|Y |, i.e. the
bilingual string X/Y . It has a time complexity of
O(|X|3|Y |3|R|), where |R| is the number of rules
of the grammar.
The parsing algorithm is based on the definition
of:
?ijkl(A) = P?r(A ?? xi+1 ? ? ? xj/yk+1 ? ? ? yl)
as the maximum probability of any parsing tree that
simultaneously generates the substrings xi+1 ? ? ? xj
and yk+1 ? ? ? yl from the non-terminal symbol A .
In (Wu, 1997), the parsing algorithm was defined
as follows:
653
1. Initialization
?i?1,i,k?1,k(A) = p(A ? xi/yk)
1 ? i ? |X|, 1 ? k ? |Y |,
?i?1,i,k,k(A) = p(A ? xi/?)
1 ? i ? |X|, 0 ? k ? |Y |,
?i,i,k?1,k(A) = p(A ? ?/yk)
0 ? i ? |X|, 1 ? k ? |Y |,
2. Recursion
For all A ? N and
i, j, k, l such that
?
?
?
0 ? i < j ? |X|,
0 ? k < l ? |Y |,
j ? i + l ? k > 2,
(1)
?ijkl(A) = max(?[]ijkl(A), ?
??
ijkl(A))
where
?[]ijkl(A)
= max
B,C?N
i?I?j,k?K?l
(I?i)(j?I)+(K?k)(l?K)>0
p(A ? [BC])?iIkK(B)?IjKl(C) (2)
???ijkl(A)
= max
B,C?N
i?I?j,k?K?l
(I?i)(j?I)+(K?k)(l?K)>0
p(A ? ?BC?)?iIKl(B)?IjkK(C) (3)
This algorithm cannot provide the correct parsing
tree in some situations. For example, consider the
SITG shown in Fig. 1. If the input pair is a/b,
p S ? [SS] p S ? ?SS?
q S ? ?/b q S ? a/?
1? 2p? 2q S ? a/b
Figure 1: Example SITG.
this SITG provides the parse tree (a) that is shown in
Fig. 2 with probability 1 ? 2p ? 2q. However, the
parse tree (b) is more likely if 1 ? 2p ? 2q < 2pq.
The above parsing algorithm is not able to obtain
this parse tree due to the restriction j? i+ l?k > 2
in (1). This restriction does not allow the algo-
rithm to consider two subproblems in which each
substring has length 1 which have not been previ-
ously considered in the initialization step. Chang-
ing this restriction to j ? i + l ? k ? 2 is not
enough to tackle this situation since the restriction
(b)(a)
SS
SS
a/b
a/? ?/b
Figure 2: Parse tree (a) can be obtained with Wu?s algo-
rithm for a/b, but parse tree (b) cannot be obtained.
(I?i)(j?I)+(K?k)(l?K) 6= 0 in expression (2)
is not accomplished given that I = i or I = j, and
K = k or l = K (similarly in expression (3)).
From now on, we will use the term non-explored
trees to denote the set of trees that are possible when
rules of the grammar are applied but cannot be ex-
plored with Wu?s algorithm. In fact, this situation
appears for other paired strings (see Fig. 3) in which
a string in one side is associated with the empty
string in the other side through rules that are not lexi-
cal rules. For example, in Fig. 3b, substring aa could
be associated with ?. However,this parse tree cannot
be obtained with the algorithm due to the search re-
strictions described above.
(b)(a)
SS
S
S
S
SS
S
a/ba/?
a/?a/?
?/b
Figure 3: Parse tree (a) can be obtained with Wu?s algo-
rithm for aa/b, but parse tree (b) would be more probable
if pq2 > 1? 2p? 2q.
The changes needed in the algorithm to be able to
find the sort of parsing trees described above are the
following:
? Changing restriction j ? i+ l? k > 2 in (1) to
j? i+ l?k ? 2. Note that this new restriction
is redundant and could be removed.
? Changing restriction (I? i)(j?I)+(K?k)(l?
K) 6= 0 to ((j?I)+(l?K))?((I?i)+(K?k)) 6=
0 in (2) and to ((j ? I) + (K ? k)) ? ((I ? i) +
(l ? K)) 6= 0 in (3) in order to guarantee the
algorithm?s termination.
3 Search under SITG Constraints
The modifications that have been introduced in Sec-
tion 2 enlarge the search space and allow the parsing
654
algorithm to explore a greater number of possible so-
lutions. We illustrate this situation with an example.
Consider the SITG introduced in Figure 1. Fig. 4
shows the possible complete matched trees for the
input pair a/b that are considered in the search pro-
cess with the modifications introduced.
(b)(a) (c) (d) (e)
S
S
SS
S
SS
S
S
S
S
S
Sa/b
aaaa
bbbb
?
?
?
??
??
?
Figure 4: Parse trees for input pair a/b that are taken into
account in the search process with the modifications.
Without these modifications, the parsing algo-
rithm only takes into account tree (a) of Fig. 4. For
this grammar, we have computed the growth in num-
ber of complete matched trees. Table 1 shows how
the search space grows notably with the modifica-
tions introduced.
n Wu?s alg. Modified alg. ratio
1 1 5 0.200
2 34 290 0.117
3 1,928 34,088 0.057
4 131,880 5,152,040 0.026
5 10,071,264 890,510,432 0.011
6 827,969,856 167,399,588,160 0.005
Table 1: Growth in number of explored trees for the orig-
inal and modified parsing algorithms (n is the length of
the input pair strings and the last column represents the
ratio between columns two and three).
As a preliminary experiment and in order to eval-
uate empirically the Wu?s parsing algorithm versus
the modified algorithm, we parsed first 100K sen-
tence of German-English Europarl corpus. The lex-
ical rules in the Bracketing SITG used for pars-
ing were obtained from a probabilistic dictionary
by aligning with IBM3 model (NULL aligments
were also included). In this experiment, the modi-
fied algorithm obtained a more probable parse tree
for 6% of the sentences. If we added brackets to
the sentences separately with monolingual parsers,
we could use a parsing algorithm similar to the al-
gorithm that is described in (S?nchez and Bened?,
2006). The monolingual brackets restricted the
parse tree to those that were compatible with the
brackets. In that case the modified algorithm ob-
tained a more probable parse tree for 14% of the
sentences.
4 Inside Probability
The parsing algorithm described above computes
the most likely parse tree for a given paired string
X/Y . However, in some cases (Wu, 1995; Huang
and Zhou, 2009), we need the inside probability
(?0,|X|,0,|Y |(S)), i.e., the probability that the gram-
mar assigns to the whole set of parse trees that yield
X/Y . If the maximizations are replaced by sums,
the algorithm can be used to compute the inside
probability. However, as stated above, the origi-
nal algorithm cannot find the whole set of trees for
a given paired string in some cases. These non-
explored trees have a probability greater than 0.
As an example, we computed the amount of prob-
ability lost in the inside computation using the origi-
nal algorithm with the grammar shown in Fig. 1. Let
? be the amount of probability of the non-explored
trees (the lost probability). It must be noted that
since height 1 trees are all reachable, we must accu-
mulate lost probability for trees of height 2 or more.
Hence, let ? be the amount of lost probability for
trees of height 2 or more. Note that all such trees
must have initially used the production S ? SS in-
versely or directly. Thus, ? = 2p ? ?. Fig. 5 shows
the kinds of non-explored trees. Then ? is:
? = 4?q2+2?2p?(1?2p)??+(2p)2 ?(2?(1??)+?2)
The first addend is the probability of the non-
explored trees of height 2 (Fig. 5a). The second ad-
dend is the probability that one of the subtrees uses
a syntactic production, this new subtree produces
a non-explored tree (2p ? ?) and the other subtree
(a) (b) (c)
S
S
SS
S
SS
S
S
Figure 5: Partial representation of non-explored parse
trees from the non-terminal string SS introduced after
the first derivation step: (a) both non-terminals yield a
terminal in one side and the empty string in the other;
(b) one of the non-terminals uses a lexical production
and the other non-terminal yields a non-explored tree; (c)
both non-terminals use a syntactic production and one (or
both) yields a non-explored tree.
655
Figure 6: Amount of lost probability for values of p and q.
rewrites itself using a lexical production (1 ? 2p).
Note that the non-explored tree can be yielded from
either the left or the right non-terminal, (Fig. 5b).
The third addend is the probability that both non-
terminals use a syntactic production (2p)2 and ei-
ther one (2(?)(1??)) or both (?2) subtrees are non-
explored trees (Fig. 5c). If we isolate ?, we get
? = 2p ? 1? 4p ?
?
16p2 ? 8p + 1 + 64p2q2
4p2
Since the solution with the positive square root
takes values greater than 1, we can discard it.
Fig. 6 shows the probability accumulated in the
non-explored trees for values of p and q between
0 and 0.25 (higher values of p produce inconsistent
SITGs). That is the amount of probability lost in the
inside parsing for the whole language generated by
the grammar shown in Fig. 1.
In order to prove the loss of probability produced
by the original algorithm, we use the grammar in
Fig. 1 with p = q = 0.2. We parse all the paired
strings X/Y such that |X| + |Y | ? l, where l is a
fixed maximum length. We repeat the same exper-
iment using the modified algorithm. Fig. 7 shows
the accumulated inside probabilities for both origi-
nal and modified algorithms and the theoretical max-
imums (1?? for the original algorithm and 1 for the
modified algorithm). Note that the computed results
approach the theoretical maximums and the modi-
fied algorithm covers the whole search space.
5 Conclusions
SITGs have proven to be a powerful tool in Syntax
Machine Translation. However, the algorithms have
been proposed do not explore all the possible parse
trees. This work proposes modifications of the algo-
rithms to be able to explore the whole search space.
Figure 7: Accumulated inside probability for the original
and modified algorithms.
Using an example, we have shown that the modifi-
cations allow a complete search. As future work, we
plan to proove the correctness of the modified algo-
rithm and to study the impact of these modifications
on the use of SITGs for Machine Translation, and
the estimation of SITGs.
Acknowledgments
Work supported by the EC (FSE), the Spanish Gov-
ernment (MICINN, "Plan E") under grants MIPRCV
"Consolider Ingenio 2010" CSD2007-00018, iTrans2
TIN2009-14511 and the Generalitat Valenciana grant
Prometeo/2009/014 and BFPI/2007/117.
References
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
S. Huang and B. Zhou. 2009. An em algorithm for scfg
in formal syntax-based translation. In ICASSP, pages
4813?4816, Taiwan, China, April.
F.J. Maryanski and M.T. Thomason. 1979. Properties of
stochastic syntax-directed tranlation schemata. Jour-
nal of Computer and Information Sciences, 8(2):89?
110.
J.A. S?nchez and J.M. Bened?. 2006. Stochastic in-
version transduction grammars for obtaining word
phrases for phrase-based statistical machine transla-
tion. In Proc. of Workshop on Statistical Machine
Translation. HLT-NAACL 06, pages 130?133.
D. Wu. 1995. Trainable coarse bilingual grammars for
parallel text bracketing. In Proceedings of the Third
Annual Workshop on Very Large Corpora, pages 69?
81.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
656
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 37?40,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Interactive Predictive Parsing using a Web-based Architecture?
Ricardo Sa?nchez-Sa?ez? Luis A. Leiva? Joan-Andreu Sa?nchez? Jose?-Miguel Bened???
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
{rsanchez,luileito,jandreu,jbenedi}@{?dsic,?iti}.upv.es
Abstract
This paper introduces a Web-based demon-
stration of an interactive-predictive framework
for syntactic tree annotation, where the user is
tightly integrated into the interactive parsing
system. In contrast with the traditional post-
editing approach, both the user and the sys-
tem cooperate to generate error-free annotated
trees. User feedback is provided by means of
natural mouse gestures and keyboard strokes.
1 Introduction
There is a whole family of problems within the pars-
ing world where error-free results, in the form of
perfectly annotated trees, are needed. Constructing
error-free trees is a necessity in many tasks, such
as handwritten mathematical expression recognition
(Yamamoto et al, 2006), or new gold standard tree-
bank creation (de la Clergerie et al, 2008). It is
a fact that current state-of-the-art syntactic parsers
provide trees that, although of excellent quality, still
contain errors. Because of this, the figure of a human
corrector who supervises the annotation process is
unavoidable in this kind of problems.
When using automatic parsers as a baseline for
building perfect syntactic trees, the role of the hu-
man annotator is to post-edit the trees and correct the
errors. This manner of operating results in the typ-
ical two-step process for error correcting, in which
the system first generates the whole output and then
?Work partially supported by the Spanish MICINN under
the MIPRCV ?Consolider Ingenio 2010? (CSD2007-00018),
MITTRAL (TIN2009-14633-C03-01), Prometeo (PROME-
TEO/2009/014) research projects, and the FPU fellowship
AP2006-01363. The authors wish to thank Vicent Alabau for
his invaluable help with the CAT-API library.
the user verifies or amends it. This paradigm is
rather inefficient and uncomfortable for the human
annotator. For example, in the creation of the Penn
Treebank annotated corpus, a basic two-stage setup
was employed: a rudimentary parsing system pro-
vided a skeletal syntactic representation, which then
was manually corrected by human annotators (Mar-
cus et al, 1994). Other tree annotating tools within
the two-step paradigm exist, such as the TreeBanker
(Carter, 1997) or the Tree Editor TrEd1.
With the objective of reducing the user effort and
making this laborious task easier, we devised an In-
teractive Predictive framework. Our aim is to put
the user into the loop, embedding him as a part of
the automatic parser, and allowing him to interact in
real time within the system. In this manner, the sys-
tem can use the readily available user feedback to
make predictions about the parts that have not been
validated by the corrector.
In this paper, we present a Web-based demo,
which implements the Interactive Predictive Parsing
(IPP) framework presented in (Sa?nchez-Sa?ez et al,
2009). User feedback (provided by means of key-
board and mouse operations) allows the system to
predict new subtrees for unvalidated parts of the an-
notated sentence, which in turn reduces the human
effort and improves annotation efficiency.
As a back-end for our demo, we use a more pol-
ished version of the CAT-API library, the Web-based
Computer Assisted Tool introduced in (Alabau et al,
2009). This library allows for a clean application de-
sign, in which both the server side (the parsing en-
gine) and the client side (which draws the trees, cap-
tures and interprets the user feedback, and requests
1http://ufal.mff.cuni.cz/?pajas/tred/
37
(a) System: output tree 1 (b) User: span modification (c) System: output tree 2
Figure 1: An interaction example on the IPP system.
parsed subtrees to the server) are independent. One
of the features that steam from the CAT-API library
is the ability for several annotators to work concur-
rently on the same problem-set, each in a different
client computer sharing the same parsing server.
Interactive predictive methods have been success-
fully demonstrated to ease the work of transcrip-
tors and translators in fields like Handwriting Text
Recognition (Romero et al, 2009; Toselli et al,
2008) and Statistical Machine Translation (Ortiz et
al., 2010; Vidal et al, 2006). This new paradigm
enables the collaboration between annotators across
the globe, granting them a physical and geographical
freedom that was inconceivable in the past.
2 Interactive Predictive Parsing
A tree t, associated to a string x1|x|, is composed
by substructures that are usually referred as con-
stituents. A constituent cAij is defined by the non-
terminal symbol A (either a syntactic label or a POS
tag) and its span ij (the starting and ending indexes
which delimit the part of the input sentence encom-
passed by the constituent).
Here follows a general formulation for the non-
interactive parsing scenario. Using a grammatical
model G, the parser analyzes the input sentence x =
{x1, . . . , x|x|} and produces the parse tree t?
t? = arg max
t?T
pG(t|x), (1)
where pG(t|x) is the probability of parse tree t given
the input string x using model G, and T is the set of
all possible parse trees for x.
In the interactive predictive scenario, after obtain-
ing the (probably incorrect) best tree t?, the user is
able to individually correct any of its constituents
cAij . The system reacts to each of the corrections in-
troduced by the human, proposing a new t?? that takes
into account the afore-mentioned corrections.
The action of modifying an incorrect constituent
(either setting the correct span or the correct label)
implicitly validates a subtree that is composed by
the partially corrected constituent, all of its ancestor
constituents, and all constituents whose end span is
lower than the start span of the corrected constituent.
We will name this subtree the validated prefix tree
tp. When the user replaces the constituent cAij with
the correct one c?Aij , the validated prefix tree is:
tp(c?Aij ) = {cBmn : m ? i, n ? j,
d(cBmn) ? d(c?Aij )} ?
{cDpq : p >= 1 , q < i}
(2)
with d(cBmn) being the depth of constituent cBmn.
When a constituent correction is performed, the
prefix tree tp(c?Aij ) is fixed and a new tree t?? that takes
into account the prefix is proposed
t?? = arg max
t?T
pG(t|x, tp(c?Aij )). (3)
Given that we are working with context-free
grammars, the only subtree that effectively needs to
be recalculated is the one starting from the parent of
the corrected constituent.
3 Demo outline
A preview version of the demonstration can be ac-
cessed at http://cat.iti.upv.es/ipp/.
The user is presented with the sentences in the se-
lected corpus, and starts parsing them one by one.
They make corrections in the trees both with the key-
board and the computer mouse. The user feedback
38
is decoded on the client side which in turn requests
subtrees to the parse engine.
Two kind of operations can be performed over
constituents: span modification (performed either by
dragging a line from the constituent to the word that
corresponds to the span?s upper index, or deleting
a tree branch by clicking on it), and label substitu-
tion (done by typing the correct one on its text field).
Modifying the span of a constituent invalidates its
label, so the server recalculates it as part of the suf-
fix. Modifying the label of a constituent validates its
span.
When the user is about to perform an opera-
tion, the affected constituent and the prefix that will
be validated are highlighted. The target span of
the modified constituent is visually shown as well.
When the user obtains the correctly annotated tree,
they can accept it by by clicking on a new sentence.
As already mentioned, the user is tightly inte-
grated into the interactive parsing process. They fol-
low a predetermined protocol in which they correct
and/or validate the annotated parse trees:
1. The parsing server proposes a full parse tree t
for the input sentence. The tree t is shown to
the user by the client (Fig. 1a).
2. The user finds the first2 incorrect constituent c
and starts amending it, either by changing its
label or changing its span (Fig. 1b, note how
the label is greyed out as it is discarded with
the span modification). This operation implic-
itly validates the prefix tree tp (highlighted in
green).
3. The system decodes the user feedback (i.e.,
mouse gestures or keyboard strokes) which can
either affect the label or the span of the incor-
rect constituent c:
(a) If the span of c is modified, the label is
not assumed to be correct. A partial con-
stituent c?, which includes span but no la-
bel, is decoded from the user feedback.
(b) If the label of c is modified, the span is
assumed to be correct. The corrected con-
stituent c? is decoded from the user feed-
back.
2The tree visiting order is left-to-right depth-first.
This step only deals with analysing the user
feedback, the parsing server will not be con-
tacted until the next step.
4. Either the partially corrected constituent c? or
the corrected constituent c? is then used by the
client to create a new extended consolidated
prefix that combines the validated prefix and the
user feedback: either tpc? or tpc?. The client
sends the extended prefix tree to the parsing
server and requests a suitable continuation for
the parse tree, or tree suffix ts:
(a) If the extended prefix is partial (tpc?), the
first element of ts is the label completing
c?, followed by the remaining calculated
whole constituents.
(b) If the extended prefix is complete (tpc?),
the parsing server produces a suitable tree
suffix ts which contains the remaining cal-
culated whole constituents.
5. The client concatenates the suffix returned by
the server to the validated extended prefix, and
shows the whole tree to the client (Fig. 1c).
6. These previous steps are iterated until a final,
perfect parse tree is produced by the server and
validated by the user.
Note that within this protocol, constituents can be
deleted or inserted by adequately modifying the span
of the left-neighbouring constituent.
4 Demo architecture
The proposed system coordinates client-side script-
ing with server-side technologies, by using the CAT-
API library (Alabau et al, 2009).
4.1 Server side
The server side of our system is a parsing en-
gine based on a customized CYK-Viterbi parser,
which uses a Probabilistic Context-Free Grammar in
Chomsky Normal Form obtained from sections 2 to
21 of the UPenn Treebank as a model (see (Sa?nchez-
Sa?ez et al, 2009) for details).
The client can request to the parsing server the
best subtree for any given span of the input string.
For each requested subtree, the client can either pro-
vide the starting label or not. If the starting subtree
39
label is not provided, the server calculates the most
probable label. The server also performs transparent
tree debinarization/binarization when communicat-
ing with the client.
4.2 Client side
The client side has been designed taking into ac-
count ergonomic issues in order to facilitate the in-
teraction.
The prototype is accessed through a Web browser,
and the only requirement is the Flash plugin (98% of
market penetration) installed in the client machine.
The hardware requirements in the client are very
low on the client side, as the parsing is process per-
formed remotely on the server side: any computer
(including netbooks) capable of running a modern
Web browser is enough.
Each validated user interaction is saved as a log
file on the server side, so a tree?s annotation session
can be later resumed.
4.2.1 Communication protocol
This demo exploits the WWW to enable the con-
nection of simultaneous accesses across the globe.
This architecture also provides cross-platform com-
patibility and requires neither computational power
nor disk space on the client?s machine.
Client and server communicate via asynchronous
HTTP connections, providing thus a richer interac-
tive experience ? no page refreshes is required when
parsing a new sentence. Moreover, the Web client
communicates with the IPP engine through binary
TCP sockets. Thus, response times are quite slow ? a
desired requirement for the user?s solace. Addition-
ally, cross-domain requests are possible, so the user
could switch between different IPP engines within
the same UI.
5 Evaluation results
We have carried out experiments that simulate user
interaction using section 23 of the Penn Treebank.
The results suggest figures ranging from 42% to
46% of effort saving compared to manually post-
editing the trees without an interactive system. In
other words, for every 100 erroneous constituents
produced by a parsing system, an IPP user would
correct only 58 (the other 42 constituents being au-
tomatically recalculated by the IPP system). Again,
see (Sa?nchez-Sa?ez et al, 2009) for the details on ex-
perimentation.
5.1 Conclusions and future work
We have introduced a Web-based interactive-
predictive system that, by using a parse engine in
an integrated manner, aids the user in creating cor-
rectly annotated syntactic trees. Our system greatly
reduces the human effort required for this task com-
pared to using a non-interactive automatic system.
Future work includes improvements to the client
side (e.g., confidence measures as a visual aid, mul-
timodality), as well as exploring other kinds of pars-
ing algorithms for the server side (e.g., adaptative
parsing).
References
V. Alabau, D. Ortiz, V. Romero, and J. Ocampo. 2009. A
multimodal predictive-interactive application for com-
puter assisted transcription and translation. In ICMI-
MLMI ?09, 227?228.
D. Carter. 1997. The TreeBanker. A tool for supervised
training of parsed corpora. In ENVGRAM?97, 9?15.
E.V. de la Clergerie, O. Hamon, D. Mostefa, C. Ayache,
P. Paroubek, and A. Vilnat. 2008. Passage: from
French parser evaluation to large sized treebank. In
LREC?08, 100:P2.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):313?330.
D. Ortiz, L. A. Leiva, V. Alabau, and F. Casacuberta.
2010. Interactive machine translation using a web-
based architecture. In IUI?10, 423?425.
V. Romero, L. A. Leiva, A. H. Toselli, and E. Vidal.
2009. Interactive multimodal transcription of text
imagse using a web-based demo system. In IUI?09,
477?478.
R. Sa?nchez-Sa?ez, J.A. Sa?nchez, and J.M. Bened??. 2009.
Interactive predictive parsing. In IWPT?09, 222?225.
A.H. Toselli, V. Romero, and E. Vidal. 2008. Computer
assisted transcription of text images and multimodal
interaction. In MLMI?08, 5237: 296?308.
E. Vidal, F. Casacuberta, L. Rodr??guez, J. Civera, and
C. Mart??nez. 2006. Computer-assisted translation us-
ing speech recognition. IEEE Trans. on Audio, Speech
and Language Processing, 14(3):941?951.
R. Yamamoto, S. Sako, T. Nishimoto, and S. Sagayama.
2006. On-line recognition of handwritten mathe-
matical expressions based on stroke-based stochastic
context-free grammar. In 10th Frontiers in Handwrit-
ing Recognition, 249?254.
40
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 172?176,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
UPV-PRHLT English?Spanish system for WMT10
Germa?n Sanchis-Trilles and Jesu?s Andre?s-Ferrer and Guillem Gasco?
Jesu?s Gonza?lez-Rubio and Pascual Mart??nez-Go?mez and Martha-Alicia Rocha
Joan-Andreu Sa?nchez and Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{gsanchis|jandres|fcn}@dsic.upv.es
{ggasco|jegonzalez|pmartinez}@dsic.upv.es
{mrocha|jandreu}@dsic.upv.es
Abstract
In this paper, the system submitted by
the PRHLT group for the Fifth Work-
shop on Statistical Machine Translation of
ACL2010 is presented. On this evalua-
tion campaign, we have worked on the
English?Spanish language pair, putting
special emphasis on two problems derived
from the large amount of data available.
The first one, how to optimize the use of
the monolingual data within the language
model, and the second one, how to make
good use of all the bilingual data provided
without making use of unnecessary com-
putational resources.
1 Introduction
For this year?s translation shared task, the Pat-
tern Recognition and Human Language Technolo-
gies (PRHLT) research group of the Universidad
Polite?cnica de Valencia submitted runs for the
English?Spanish translation task. In this paper, we
report the configuration of such a system, together
with preliminary experiments performed to estab-
lish the final setup.
As in 2009, the central focus of the Shared Task
is on Domain Adaptation, where a system typi-
cally trained using out-of-domain data is adjusted
to translate news commentaries.
For the preliminary experiments, we used only a
small amount of the largest available bilingual cor-
pus, i.e. the United Nations corpus, by including
into our system only those sentences which were
considered similar.
Language model interpolation using a develop-
ment set was explored in this work, together with
a technique to cope with the problem of ?out of
vocabulary words?.
Finally, a reordering constraint using walls and
zones was used in order to improve the perfor-
mance of the submitted system.
In the final evaluation, our system was ranked
fifth, considering only primary runs.
2 Language Model interpolation
Nowadays, it is quite common to have very large
amounts of monolingual data available from sev-
eral different domains. Despite of this fact, in
most of the cases we are only interested in trans-
lating from one specific domain, as is the case in
this year?s shared task, where the provided mono-
lingual training data belonged to European parlia-
mentary proceedings, news related domains, and
the United Nations corpus, which consists of data
crawled from the web.
Although the most obvious thing to do is to con-
catenate all the data available and train a single
language model on the whole data, we also inves-
tigated a ?smarter? use of such data, by training
one language model for each of the available cor-
pora.
3 Similar sentences selection
Currently, it is common to of huge bilingual cor-
pora for SMT. For some common language pairs,
corpora of millions of parallel sentences are avail-
able. In some of the cases big corpora are used
as out-of-domain corpora. For example, in the
case of the shared task, we try to translate a news
text using a small in-domain bilingual news corpus
(News Commentary) and two big out-of-domain
corpora: Europarl and United Nations.
Europarl is a medium size corpus and can be
completely incorporated to the training set. How-
ever, the use of the UN corpus requires a big com-
putational effort. In order to alleviate this prob-
lem, we have chosen only those bilingual sen-
tences from the United Nations that are similar to
the in-domain corpus sentences. As a similarity
measure, we have chosen the alignment score.
Alignment scores have already been used as a
172
filter for noisy corpora (Khadivi and Ney, 2005).
We trained an IBM model 4 using GIZA++ (Och
and Ney, 2003) with the in-domain corpus and
computed the alignment scores over the United
Nations sentences. We assume that the alignment
score is a good measure of similarity.
An important factor in the alignment score is
the length of the sentences, so we clustered the
bilingual sentences in groups with the same sum of
source and target language sentence sizes. In each
of the groups, the higher the alignment score is,
the more similar the sentence is to the in-domain
corpus sentences. Hence, we computed the aver-
age alignment score for each one of the clusters
obtained for the corpus considered in-domain (i.e.
the News-Commentary corpus). This being done,
we assessed the similarity of a given sentence by
computing the probability of such sentence with
respect to the alignment model of the in-domain
corpus, and established the following similarity
levels:
? Level 1: Sentences with an alignment score
equal or higher than the in-domain average.
? Level 2: Sentences with an alignment score
equal or higher than the in-domain average,
minus one standard deviation.
? Level 3: Sentences with an alignment score
equal or higher than the in-domain average,
minus two standard deviations.
Naturally, such similarity levels establish parti-
tions of the out-of-domain corpus. Then, such par-
titions were included into the training set used for
building the SMT system, and re-built the com-
plete system from scratch.
4 Out of Vocabulary Recovery
As stated in the previous section, in order to avoid
a big computational effort, we do not use the
whole United Nations corpus to train the trans-
lation system. Out of vocabulary words are a
common problem for machine translation systems.
When translating the test set, there are test words
that are not in the reduced training set (out of vo-
cabulary words). Some of those out of vocabulary
words are present in the sentences discarded from
the United Nations Corpus. Thus, recovering the
discarded sentences with out of vocabulary words
is needed.
The out of vocabulary words recovery method
is simple: the out of vocabulary words from the
test, when taking into account the reduced training
set, are obtained and then discarded sentences that
contain at least one of them are retrieved. Then,
those sentences are added to the reduced training
set.
Finally, alignments with the resulting training
set were computed and the usual training proce-
dure for phrase-based systems was performed.
5 Walls and zones
In translation, as in other linguistics areas, punc-
tuation marks are essential as they help to un-
derstand the intention of a message and organise
the ideas to avoid ambiguity. They also indicate
pauses, hierarchies and emphasis.
In our system, punctuation marks have been
taken into account during decoding. Traditionally,
in SMT punctuation marks are treated as words
and this has undesirable effects (Koehn and Had-
dow, 2009). For example, commas have a high
probability of occurrence and many possible trans-
lations are generated. Most of them are not consis-
tent across languages. This introduces too much
noise to the phrase tables.
(Koehn and Haddow, 2009) established a
framework to specify reordering constraints with
walls and zones, where commas and end
of sentence are not mixed with various clauses.
Gains between 0.1 and 0.2 of BLEU are reported.
Specifying zones and walls with XML tags
in input sentences allows us to identify structured
fragments that the Moses decoder uses with the
following restrictions:
1. If a <zone> tag is detected, then a block
is identified and must be translated until a
</zone> tag is found. The text between tags
<zone> and </zone> is identified and trans-
lated as a block.
2. If the decoder detects a <wall/> tag, the text
is divided into a prefix and suffix and Moses
must translate all the words of the prefix be-
fore the suffix.
3. If both zones and walls are specified,
then local walls are considered where
the constraint 2 applies only to the area es-
tablished by zones.
173
corpus Language |S| |W | |V |
Europarl v5
Spanish
1272K
28M 154K
English 27M 106K
NC
Spanish
81K
1.8M 54K
English 1.6M 39K
Table 1: Main figures of the Europarl v5 and
News-Commentary (NC) corpora. K/M stands
for thousands/millions. |S| is the number of sen-
tences, |W | the number of running words, and |V |
the vocabulary size. Statistics are reported on the
tokenised and lowercased corpora.
We used quotation marks, parentheses, brackets
and dashes as zone delimiters. Quotation marks
(when appearing once in the sentence), com-
mas, colons, semicolons, exclamation and ques-
tion marks and periods are used as wall delimiters.
The use of zone delimiters do not alter the per-
formance. When using walls, a gain of 0.1
BLEU is obtained in our best model.
6 Experiments
6.1 Experimental setup
For building our SMT systems, the open-source
SMT toolkit Moses (Koehn et al, 2007) was used
in its standard setup. The decoder includes a log-
linear model comprising a phrase-based transla-
tion model, a language model, a lexicalised dis-
tortion model and word and phrase penalties. The
weights of the log-linear interpolation were opti-
mised by means of MERT (Och, 2003). In addi-
tion, a 5-gram LM with Kneser-Ney (Kneser and
Ney, 1995) smoothing and interpolation was built
by means of the SRILM (Stolcke, 2002) toolkit.
For building our baseline system, the News-
Commentary and Europarl v5 (Koehn, 2005) data
were employed, with maximum sentence length
set to 40 in the case of the data used to build the
translation models, and without restriction in the
case of the LM. Statistics of the bilingual data can
be seen in Table 1.
In all the experiments reported, MERT was run
on the 2008 test set, whereas the test set 2009 was
considered as test set as such. In addition, all the
experiments described below were performed in
lowercase and tokenised conditions. For the fi-
nal run, the detokenisation and recasing was per-
formed according to the technique described in the
Workshop baseline description.
corpus |S| |W | |V |
Europarl 1822K 51M 172K
NC 108K 3M 68K
UN 6.2M 214M 411K
News 3.9M 107M 512K
Table 2: Main figures of the Spanish resources
provided: Europarl v5, News-Commentary (NC),
United Nations (UN) and News-shuffled (News).
6.2 Language Model interpolation
The final system submitted to the shared task
included a linear interpolation of four language
models, one for each of the monolingual resources
available for Spanish (see Table 2). The results
can be seen in Table 3. As a first experiment, only
the in-domain corpus, i.e. the News-Commentary
data (NC data) was used for building the LM.
Then, all the available monolingual Spanish data
was included into a single LM, by concatenat-
ing all the data together (pooled). Next, in
interpolated, one LM for each one of the
provided monolingual resources was trained, and
then they were linearly interpolated so as to min-
imise the perplexity of the 2008 test set, and fed
such interpolation to the SMT system. We found
out that weights were distributed quite unevenly,
since the News-shuffled LM received a weight of
0.67, whereas the other three corpora received a
weight of 0.11 each. It must be noted that even
the in-domain LM received a weight of 0.11 (less
than the News-shuffled LM). The reason for this
might be that, although the in-domain LM should
be more appropriate and should receive a higher
weight, the News-shuffled corpus is also news re-
lated (hence not really out-of-domain), but much
larger. For this reason, the result of using only
such LM (News) was also analysed. As expected,
the translation quality dropped slightly. Never-
theless, since the differences are not statistically
significant, we used the News-shuffled LM for in-
ternal development purposes, and the interpolated
LM only whenever an improvement prooved to be
useful.
6.3 Including UN data
We analysed the impact of the selection technique
detailed in Section 3. In this case, the LM used
was the interpolated LM described in the previous
section. The result can be seen in Table 4. As
it can be seen, translation quality as measured by
174
Table 3: Effect of considering different LMs
LM used BLEU
NC data 21.86
pooled 23.53
interpolated 24.97
news 24.79
BLEU improves constantly as the number of sen-
tences selected increases. However, further sen-
tences were not included for computational rea-
sons.
In the same table, we also report the effect of
adding the UN sentences selected by our out-of-
vocabulary technique described in Section 4. In
this context, it should be noted that MERT was
not rerun once such sentences had been selected,
since such sentences are related with the test set,
and not with the development set on which MERT
is run.
Table 4: Effect of including selected sentences
system BLEU
baseline 24.97
+ oovs 25.08
+ Level 1 24.98
+ Level 2 25.07
+ Level 3 25.13
6.4 Final system
Since the News-shuffled, UN and Europarl cor-
pora are large corpora, a new LM interpolation
was estimated by using a 6-gram LM on each one
of these corpora, obtaining a gain of 0.17 BLEU
points by doing so. Further increments in the n-
gram order did not show further improvements.
In addition, preliminary experimentation re-
vealed that the use of walls, as described in
Section 5, also provided slight improvements, al-
though using zones or combining both did not
prove to improve further. Hence, only walls
were included into the final system.
Lastly, the final system submitted to the Work-
shop was the result of combining all the techniques
described above. Such combination yielded a fi-
nal BLEU score of 25.31 on the 2009 test set, and
28.76 BLEU score on the 2010 test set, both in
tokenised and lowercased conditions.
7 Conclusions and future work
In this paper, the SMT system presented by the
UPV-PRHLT team for WMT 2010 has been de-
scribed. Specifically, preliminary results about
how to make use of larger data collections for
translating more focused test sets have been pre-
sented.
In this context, there are still some things which
need a deeper investigation, since the results pre-
sented here give only a small insight about the po-
tential of the similar sentence selection technique
described.
However, a deeper analysis is needed in order
to assess the potential of such technique and other
strategies should be implemented to explore new
kids of reordering constraints.
Acknowledgments
This paper is based upon work supported by
the EC (FEDER/FSE) and the Spanish MICINN
under the MIPRCV ?Consolider Ingenio 2010?
program (CSD2007-00018),iTrans2 (TIN2009-
14511) project, and the FPU scholarship AP2006-
00691. This work was also supported by the Span-
ish MITyC under the erudito.com (TSI-020110-
2009-439) project and by the Generalitat Valen-
ciana under grant Prometeo/2009/014 and schol-
arships BFPI/2007/117 and ACIF/2010/226 and
by the Mexican government under the PROMEP-
DGEST program.
References
Shahram Khadivi and Hermann Ney. 2005. Automatic
filtering of bilingual corpora for statistical machine
translation. In Natural Language Processing and In-
formation Systems, 10th Int. Conf. on Applications
of Natural Language to Information Systems, vol-
ume 3513 of Lecture Notes in Computer Science,
pages 263?274, Alicante, Spain, June. Springer.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, II:181?184, May.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In The 4th EACL Workshop on Statistical
Machine Translation, ACL, pages 160?164, Athens,
Greece, March. Springer.
P. Koehn et al 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of
175
the ACL Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. of ICSLP?02, pages 901?
904, September.
176
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 296?300,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The UPV-PRHLT Combination System for WMT 2010
Jesu?s Gonza?lez-Rubio and Jesu?s Andre?s-Ferrer and Germa?n Sanchis-Trilles
Guillem Gasco? and Pascual Mart??nez-Go?mez and Martha-Alicia Rocha
Joan-Andreu Sa?nchez and Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{jegonzalez|jandres|gsanchis}@dsic.upv.es
{ggasco|pmartinez|mrocha}@dsic.upv.es
{jandreu|fcn}@dsic.upv.es
Abstract
UPV-PRHLT participated in the System
Combination task of the Fifth Workshop
on Statistical Machine Translation (WMT
2010). On each translation direction, all
the submitted systems were combined into
a consensus translation. These consen-
sus translations always improve transla-
tion quality of the best individual system.
1 Introduction
The UPV-PRHLT approach to MT system combi-
nation is based on a refined version of the algo-
rithm described in (Gonza?lez-Rubio and Casacu-
berta, 2010), with additional information to cope
with hypotheses of different quality.
In contrast to most of the previous approaches
to combine the outputs of multiple MT sys-
tems (Bangalore et al, 2001; Jayaraman and
Lavie, 2005; Matusov et al, 2006; Schroeder et
al., 2009), which are variations over the ROVER
voting scheme (Fiscus, 1997), we consider the
problem of computing a consensus translation as
the problem of modelling a set of string patterns
with an adequate prototype. Under this frame-
work, the translation hypotheses of each of the
MT systems are considered as individual patterns
in a set of string patterns. The (generalised) me-
dian string, which is the optimal prototype of a set
of strings (Fu, 1982), is the chosen prototype to
model the set of strings.
2 System Combination Algorithm
The median string of a set is defined as the string
that minimises the sum of distances to the strings
in the set. Therefore, defining a distance between
strings is the primary problem to deal with.
The most common definition of distance be-
tween two strings is the Levenshtein distance,
also known as edit distance (ED). This metric
computes the optimal sequence of edit operations
(insertions, deletions and substitutions of words)
needed to transform one string into the other. The
main problem with the ED is its dependence on the
length of the compared strings. This fact led to the
definition of a new distance whose value is inde-
pendent from the length of the strings compared.
This normalised edit distance (NED) (Vidal et al,
1995) is computed by averaging the number of edit
operations by the length of the edit path. The ex-
perimentation in this work was carried out using
the NED.
2.1 Median String
Given a set E = e1, . . . , en, . . . , eN of translation
hypotheses from N MT systems, let ? be the vo-
cabulary in the target language and ?? be the free
monoid over that vocabulary (E ? ??). The me-
dian string of the set E (noted as M(E)) can be
formally defined as:
M(E) = argmin
e????
N
?
n=1
[
wn ? D(e?, en)
]
, (1)
where D is the distance used to compare two
strings and the value wn, 1 ? n ? N weights
the contribution of the hypothesis n to the sum of
distances, and therefore, it denotes the significance
of hypothesis n in the computation of the median
string. The value wn can be seen as a measure of
the ?quality? of hypothesis n.
Computing the median string is a NP-Hard
problem (de la Higuera and Casacuberta, 2000),
therefore we can only build approximations to the
median string by using several heuristics. In this
work, we follow two different approximations: the
set median string (Fu, 1982) and the approximate
median string (Mart??nez et al, 2000).
296
2.2 Set Median String
The most straightforward approximation to the
median string corresponds to the search of a set
median string. Under this approximation, the
search is constrained to the strings in the given in-
put set. The set median string can be informally
defined as the most ?centred? string in the set. The
set median string of the set E (noted as Ms(E))
is given by:
Ms(E) = argmin
e??E
N
?
n=1
[
wn ? D(e?, en)
]
. (2)
The set median string can be computed in poly-
nomial time (Fu, 1982; Juan and Vidal, 1998).
Unfortunately, in some cases, the set median may
not be a good approximation to the median string.
For example, in the extreme case of a set of two
strings, either achieves the minimum accumulated
distance to the set. However, the set median string
is a useful initialisation in the computation of the
approximate median string.
2.3 Approximate Median String
A good approximation to efficiently compute the
median string is proposed in (Mart??nez et al,
2000). To compute the approximate median string
of the set E, the algorithm starts with an initial
string e which is improved by successive refine-
ments in an iterative process. This iterative pro-
cess is based on the application of different edit
operations over each position of the string e look-
ing for a reduction of the accumulated distance to
the strings in the set. Algorithm 1 describes this
iterative process.
The initial string can be a random string or
a string computed from the set E. Martinez et
al. (2000) proposed two kinds of initial strings: the
set median string of E and a string computed by a
greedy algorithm, both of them obtained similar
results. In this work, we start with the set median
string in the initialisation of the computation of the
approximate median string of the set E. Over this
initial string we apply the iterative procedure de-
scribed in Algorithm 1 until there is no improve-
ment. The final median string may be different
from the original hypotheses.
The computational time cost of Algorithm 1 is
linear with the number of hypotheses in the com-
bination, and usually only a moderate number of
iterations is needed to converge.
For each position i in the string e:
1. Build alternatives:
Substitution: Make x = e. For each word a ? ?:
? Make x? the result string of substituting the ith
word of x by a.
? If the accumulated distance of x? to E is lower
than the accumulated distance from x to E, then
make x = x?.
Deletion: Make y the result string of deleting the ith
word of e.
Insertion: Make z = e. For each word a ? ?:
? Make z? the result of inserting a at position i of
e.
? If the accumulated distance from z? to E is lower
than the accumulated distance from z to E, then
make z = z?.
2. Choose an alternative:
? From the set {e,x,y, z} take the string e? with
less accumulated distance to E. Make e = e?.
Algorithm 1: Iterative process to refine a string
e in order to reduce its accumulated distance to a
given set E.
3 Experiments
Experiments were conducted on all the 8 transla-
tion directions cz?en, en?cz, de?en, en?de,
es?en, en?es, fr?en and en?fr. Some of the
entrants to the shared translation task submit lists
of n-best translations, but, in our experience, if a
large number of systems is available, using n-best
translations does not allow to obtain better consen-
sus translations than using single best translations,
but raises computation time significantly. Conse-
quently, we compute consensus translations only
using the single best translation of each individ-
ual MT system. Table 1 shows the number of sys-
tems submitted and gives an overview of the test
corpus on each translation direction. The number
of running words is the average number of run-
ning words in the test corpora, from where the
consensus translations were computed; the vocab-
ulary is the merged vocabulary of these test cor-
pora. All the experiments were carried out with
the true-cased, detokenised version of the tuning
and test corpora, following the WMT 2010 sub-
mission guidelines.
3.1 Evaluation Criteria
We will present translation quality results in terms
of translation edit rate (TER) (Snover et al, 2006)
and bilingual evaluation understudy (BLEU) (Pa-
297
cz?en en?cz de?en en?de es?en en?es fr?en en?fr
Submitted systems 6 11 16 12 8 10 14 13
Avg. Running words 45K 37K 47K 41K 47K 47K 47K 49K
Distinct words 24K 51K 38K 40K 23K 30K 27K 37K
Table 1: Number of systems submitted and main figures of test corpora on each translation direction. K
stands for thousands of elements.
pineni et al, 2002). TER is computed as the num-
ber of edit operations (insertions, deletions and
substitutions of single words and shifts of word se-
quences) to convert the system hypothesis into the
reference translation. BLEU computes a geomet-
ric mean of the precision of n-grams multiplied by
a factor to penalise short sentences.
3.2 Weighted Sum of Distances
In section 2, we define the median string of a set
as the string which minimises a weighted sum of
distances to the strings in the set (Eq. (1)). The
weights wn in the sum can be tuned. We compute
a weight value for each MT system as a whole, i.e.
all the hypotheses of a given MT system share the
same weight value. We study the performance of
different sets of weight looking for improvements
in the quality of the consensus translations. These
weight values are derived from different automatic
MT evaluation measures:
? BLEU score of each system.
? 1.0 minus TER score of each system.
? Number of times the hypothesis of each sys-
tem is the best TER-scoring translation.
We estimate these scores on the tuning corpora.
A normalisation is performed to transform these
scores into the range [0.0, 1.0]. After the normal-
isation, a weight value of 0.0 is assigned to the
lowest-scoring hypothesis, i.e. the lowest-scoring
hypothesis is not taking into account in the com-
putation of the median string.
3.3 System Combination Results
Our framework to compute consensus translations
allows multiple combinations varying the median
string algorithm or the set of weight values used
in the weighted sum of distances. To assure the
soundness of our submission to the WMT 2010
system combination task, the experiments on the
tuning corpora were carried out in a leaving-one-
out fashion dividing the tuning data into 5 parts
and averaging translation results over these 5 par-
titions. On each of the experiments, 4 of the par-
titions are devoted to obtain the weight values for
the weighted sum of distances while BLEU and
TER scores are calculated on the consensus trans-
lations of the remaining partition.
Table 2 shows, on each translation direction,
the performance of the consensus translations on
the tuning corpora. The consensus translations
were computed with the set median string and the
approximated median string using different sets
of weight values: Uniform, all weights are set
to 1.0, BLEU-based weights, TER-based weights
and oracle-based weights. In addition, we display
the performance of the best of the individual MT
systems for comparison purposes. The number of
MT systems combined for each translation direc-
tion is displayed between parentheses.
On all the translation directions under study, the
consensus translations improved the results of the
best individual systems. E.g. TER improved from
66.0 to 63.3 when translating from German into
English. On average, the set median strings per-
formed better than the best individual system, but
its results were always below the performance of
the approximate median string. The use of weight
values computed from MT quality measures al-
lows to improve the quality of the consensus trans-
lation computed. Specially, oracle-based weight
values that, except for the cz?en task, always per-
form equal or better than the other sets of weight
values. We have observed that no improvements
can be achieved with uniform weight values; it is
necessary to penalise low quality hypotheses.
To compute our primary submission to the
WMT 2010 system combination task we choose
the configurations that obtain consensus transla-
tions with highest BLEU score on the tuning cor-
pora. The approximate median string using oracle-
based scores is the chosen configuration for all
translation directions, except on the cz?en trans-
lation direction for which TER-based weights per-
formed better. As our secondary submission we
298
Single Set median Approximated median
best Uniform Bleu Ter Oracle Uniform Bleu Ter Oracle
cz?en (6) BLEU 17.6 16.5 17.8 18.2 17.6 17.1 18.5 18.5 18.0TER 64.5 68.7 67.6 65.2 64.5 67.0 65.9 65.4 64.4
en?cz (11) BLEU 11.4 10.1 10.9 10.7 11.0 10.1 10.7 10.7 11.0TER 75.3 75.1 74.3 74.2 74.2 73.9 73.4 73.3 73.0
de?en (16) BLEU 19.0 19.0 19.1 19.3 19.7 19.3 19.8 19.9 20.1TER 66.0 65.4 65.2 65.0 64.6 64.4 63.4 63.4 63.3
en?de (12) BLEU 11.9 11.6 11.7 11.7 12.0 11.6 11.8 11.8 12.0TER 74.3 74.1 74.1 74.0 73.7 72.7 72.9 72.7 72.6
es?en (8) BLEU 23.2 23.0 23.3 23.2 23.6 23.1 23.9 23.8 24.2TER 60.2 60.6 59.8 59.8 59.5 60.0 59.2 59.4 59.1
en?es (10) BLEU 23.3 23.0 23.3 23.4 24.0 23.6 23.8 23.8 24.2TER 60.1 60.1 59.9 59.7 59.5 59.0 59.1 58.9 58.6
fr?en (14) BLEU 23.3 22.9 23.2 23.2 23.4 23.4 23.8 23.8 23.9TER 61.1 61.2 60.9 60.9 60.7 60.6 60.0 60.1 59.9
en?fr (13) BLEU 22.7 23.4 23.5 23.6 23.8 23.3 23.6 23.7 23.8TER 62.3 61.0 61.0 60.9 60.6 60.2 60.1 60.0 60.0
Table 2: Consensus translation results (case-sensitive) on the tuning corpora with the set median string
and the approximate median string using different sets of weights: Uniform, BLEU-based, TER-based
and oracle-based. The number of systems being combined for each translation direction is in parentheses.
Best consensus translation scores are in bold.
Best Secondary Primary
BLEU TER BLEU TER BLEU TER
cz?en 18.2 63.9 18.3 66.7 19.0 65.1
en?cz 10.8 75.2 11.3 73.6 11.6 71.9
de?en 18.3 66.6 19.1 65.4 19.6 63.9
en?de 11.6 73.4 11.7 72.9 11.9 71.7
es?en 24.7 59.0 24.9 58.9 25.0 58.2
en?es 24.3 58.4 24.9 57.3 25.3 56.3
fr?en 23.7 59.7 23.6 59.8 23.9 59.4
en?fr 23.3 61.3 23.6 59.9 24.1 58.9
Table 3: Translation scores (case-sensitive) on the
test corpora of our primary and secondary submis-
sions to the WMT 2010 system combination task.
chose the set median string using the same set of
weight values chosen for the primary submission.
We compute MT quality scores on the WMT
2010 test corpora to verify the results on the tuning
data. Table 3 displays, on each translation direc-
tion, the results on the test corpora of our primary
and secondary submissions and of the best indi-
vidual system. These results confirm the results
on the tuning data. On all translation directions,
our submissions perform better than the best indi-
vidual systems as measured by BLEU and TER.
4 Summary
We have studied the performance of two consen-
sus translation algorithms that based in the compu-
tation of two different approximations to the me-
dian string. Our algorithms use a weighted sum of
distances whose weight values can be tuned. We
show that using weight values derived from auto-
matic MT quality measures computed on the tun-
ing corpora allow to improve the performance of
the best individual system on all the translation di-
rections under study.
Acknowledgements
This paper is based upon work supported
by the EC (FEDER/FSE) and the Spanish
MICINN under the MIPRCV ?Consolider In-
genio 2010? program (CSD2007-00018), the
iTransDoc (TIN2006-15694-CO2-01) and iTrans2
(TIN2009-14511) projects and the FPU scholar-
ship AP2006-00691. This work was also sup-
ported by the Spanish MITyC under the eru-
dito.com (TSI-020110-2009-439) project and by
the Generalitat Valenciana under grant Prom-
eteo/2009/014 and scholarships BFPI/2007/117
and ACIF/2010/226 and by the Mexican govern-
ment under the PROMEP-DGEST program.
299
References
S. Bangalore, G. Bodel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In IEEE Workshop on ASRU,
pages 351?354.
C. de la Higuera and F. Casacuberta. 2000. Topology
of strings: Median string is np-complete. Theoreti-
cal Computer Science, 230:39?48.
J. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recogniser output voting
error reduction (rover).
K.S. Fu. 1982. Syntactic Pattern Recognition and Ap-
plications. Prentice Hall.
J. Gonza?lez-Rubio and F. Casacuberta. 2010. On the
use of median string for multi-source translation.
In Proceedings of 20th International Conference on
Pattern Recognition, Istambul, Turkey, May 27-28.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of EAMT, pages 143?152.
A. Juan and E. Vidal. 1998. Fast Median Search in
Metric Spaces. In Proc. of SPR, volume 1451 of
Lecture Notes in Computer Science, pages 905?912.
C. D. Mart??nez, A. Juan, and F. Casacuberta. 2000.
Use of Median String for Classification. In Proc. of
ICPR, volume 2, pages 907?910.
E. Matusov, N. Ueffing, and H-Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Proc. of EACL, pages 33?40.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
J. Schroeder, T. Cohn, and P. Koehn. 2009. Word lat-
tices for multi-source translation. In Proc. of EACL,
pages 719?727.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of TER with targeted
human annotation. In Proc. of AMTA, pages 223?
231.
E. Vidal, A. Marzal, and P. Aibar. 1995. Fast compu-
tation of normalized edit distances. IEEE Transac-
tions on PAMI, 17(9):899?902.
300
