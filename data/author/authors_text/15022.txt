Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 311?316,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving Classification of Medical Assertions in Clinical Notes 
  Youngjun Kim Ellen Riloff St?phane M. Meystre School of Computing School of Computing Department of Biomedical Informatics University of Utah University of Utah University of Utah Salt Lake City, UT Salt Lake City, UT Salt Lake City, UT youngjun@cs.utah.edu riloff@cs.utah.edu stephane.meystre@hsc.utah.edu       Abstract 
We present an NLP system that classifies the assertion type of medical problems in clinical notes used for the Fourth i2b2/VA Challenge. Our classifier uses a variety of linguistic fea-tures, including lexical, syntactic, lexico-syntactic, and contextual features. To overcome an extremely unbalanced distribution of asser-tion types in the data set, we focused our efforts on adding features specifically to improve the performance of minority classes. As a result, our system reached 94.17% micro-averaged and 79.76% macro-averaged F1-measures, and showed substantial recall gains on the minority classes.  
1 Introduction Since the beginning of the new millennium, there has been a growing need in the medical community for Natural Language Processing (NLP) technolo-gy to provide computable information from narra-tive text and enable improved data quality and de-cision-making. Many NLP researchers working with clinical text (i.e. documents in the electronic health record) are also realizing that the transition to machine learning techniques from traditional rule-based methods can lead to more efficient ways to process increasingly large collections of clinical narratives. As evidence of this transition, nearly all of the best-performing systems in the Fourth i2b2/VA Challenge (Uzuner and DuVall, 2010) used machine learning methods.  
In this paper, we focus on the medical assertions classification task. Given a medical problem men-tioned in a clinical text, an assertion classifier must look at the context and choose the status of how the medical problem pertains to the patient by as-signing one of six labels: present, absent, hypothet-ical, possible, conditional, or not associated with the patient. The corpus for this task consists of dis-charge summaries from Partners HealthCare (Bos-ton, MA) and Beth Israel Deaconess Medical Cen-ter, as well as discharge summaries and progress notes from the University of Pittsburgh Medical Center (Pittsburgh, PA). Our system performed well in the i2b2/VA Challenge, achieving a micro-averaged F1-measure of 93.01%. However, two of the assertion catego-ries (present and absent) accounted for nearly 90% of the instances in the data set, while the other four classes were relatively infrequent. When we ana-lyzed our results, we saw that our performance on the four minority classes was weak (e.g., recall on the conditional class was 22.22%). Even though the minority classes are not common, they are ex-tremely important to identify accurately (e.g., a medical problem not associated with the patient should not be assigned to the patient).  In this paper, we present our efforts to reduce the performance gap between the dominant asser-tion classes and the minority classes. We made three types of changes to address this issue: we changed the multi-class learning strategy, filtered the training data to remove redundancy, and added new features specifically designed to increase re-call on the minority classes. We compare the per-formance of our new classifier with our original 
311
i2b2/VA Challenge classifier and show that it per-forms substantially better on the minority classes, while increasing overall performance as well. 2 Related Work During the Fourth i2b2/VA Challenge, the asser-tion classification task was tackled by participating researchers. The best performing system (Berry de Bruijn et al, 2011) reached a micro-averaged F1-measure of 93.62%. Their breakdown of F1 scores on the individual classes was: present 95.94%, ab-sent 94.23%, possible 64.33%, conditional 26.26%, hypothetical 88.40%, and not associated with the patient 82.35%. Our system had the 6th best score out of 21 teams, with a micro-averaged F1-measure of 93.01%.     Previously, some researchers had developed sys-tems to recognize specific assertion categories. Chapman et al (2001) created the NegEx algo-rithm, a simple rule-based system that uses regular expressions with trigger terms to determine wheth-er a medical term is absent in a patient. They re-ported 77.8% recall and 84.5% precision for 1,235 medical problems in discharge summaries. Chap-man et al (2007) also introduced the ConText al-gorithm, which extended the NegEx algorithm to detect four assertion categories: absent, hypothet-ical, historical, and not associated with the patient.   Uzuner et al (2009) developed the Statistical As-sertion Classifier (StAC) and showed that a ma-chine learning approach for assertion classification could achieve results competitive with their own implementation of Extended NegEx algorithm (ENegEx). They used four assertion classes: pre-sent, absent, uncertain in the patient, or not asso-ciated with the patient. 3 The Assertion Classifier We approach the assertion classification task as a supervised learning problem. The classifier is giv-en a medical term within a sentence as input and must assign one of the six assertion categories to the medical term based on its surrounding context.    3.1 Pipeline Architecture We built a UIMA (Ferrucci and Lally, 2004; Apache, 2008) based pipeline with multiple com-ponents, as depicted in Figure 1. The architecture includes a section detector (adapted from earlier 
work by Meystre and Haug (2005)), a tokenizer (based on regular expressions to split text on white space characters), a part-of-speech (POS) tagger (OpenNLP (Baldridge et al, 2005) module with trained model from cTAKES (Savova et al, 2010)), a context analyzer (local implementation of the ConText algorithm (Chapman et al, 2001)), and a normalizer based on the LVG (Lexical Vari-ants Generation) (LVG, 2010) annotator from cTAKES to retrieve normalized word forms. 
Figure 1: System Pipeline  The assertion classifier uses features extracted by the subcomponents to represent training and test instances. We used LIBSVM, a library for support vector machines (SVM), (Chang and Lin, 2001) for multi-class classification with the RBF (Radial Basis Function) kernel. 3.2 Original i2b2 Feature Set The assertion classifier that we created for the i2b2/VA Challenge used the features listed below, which we developed by manually examining the training data: Lexical Features: The medical term itself, the three words preceding it, and the three words fol-lowing it. We used the LVG annotator in Lexical Tools (McCray et al, 1994) to normalize each word (e.g., with respect to case and tense). Syntactic Features: Part-of-speech tags of the three words preceding the medical term and the three words following it.  
312
Lexico-Syntactic Features: We also defined features representing words corresponding to sev-eral parts-of-speech in the same sentence as the medical term. The value for each feature is the normalized word string. To mitigate the limited window size of lexical features, we defined one feature each for the nearest preceding and follow-ing adjective, adverb, preposition, and verb, and one additional preceding adjective and preposition and one additional following verb and preposition. Contextual Features: We incorporated the ConText algorithm (Chapman et al, 2001) to de-tect four contextual properties in the sentence: ab-sent (negation), hypothetical, historical, and not associated with the patient. The algorithm assigns one of three values to each feature: true, false, or possible. We also created one feature to represent the Section Header with a string value normalized using (Meystre and Haug, 2005). The system only using contextual features gave reasonable results: F1-measure overall 89.96%, present 91.39%, ab-sent 86.58%, and hypothetical 72.13%.  Feature Pruning: We created an UNKNOWN feature value to cover rarely seen feature values. Lexical feature values that had frequency < 4 and other feature values that had frequency < 2 were all encoded as UNKNOWNs.  3.3 New Features for Improvements After the i2b2/VA Challenge submission, we add-ed the following new features, specifically to try to improve performance on the minority classes: Lexical Features: We created a second set of lexical features that were case-insensitive. We also created three additional binary features for each lexical feature. We computed the average tf-idf score for the words comprising the medical term itself, the average tf-idf score for the three words to its left, and the average tf-idf score for the three words to its right. Each binary feature has a value of true if the average tf-idf score is smaller than a threshold (e.g. 0.5 for the medical term itself), or false otherwise. Finally, we created another binary feature that is true if the medical term contains a word with a negative prefix.1 Lexico-Syntactic Features:  We defined two binary features that check for the presence of a                                                             1 Negative prefixes: ab, de, di, il, im, in, ir, re, un, no, mel, mal, mis. In retrospect, some of these are too general and should be tightened up in the future. 
comma or question mark adjacent to the medical term. We also defined features for the nearest pre-ceding and following modal verb and wh-adverb (e.g., where and when). Finally, we reduced the scope of these features from the entire sentence to a context window of size eight around the medical term.  Sentence Features: We created two binary fea-tures to represent whether a sentence is long (> 50 words) or short (<= 50 words), and whether the sentence contains more than 5 punctuation marks, primarily to identify sentences containing lists. 2 Context Features: We created a second set of ConText algorithm properties for negation restrict-ed to the six word context window around the medical term. According to the assertion annota-tion guidelines, problems associated with allergies were defined as conditional. So we added one bi-nary feature that is true if the section headers con-tain terms related to allergies (e.g., ?Medication allergies?). Feature Pruning: We changed the pruning strategy to use document frequency values instead of corpus frequency for the lexical features, and used document frequency > 1 for normalized words and > 2 for case-insensitive words as thresholds. We also removed 57 redundant in-stances from the training set. Finally, when a med-ical term co-exists with other medical terms (prob-lem concepts) in the same sentence, the others are excluded from the lexical and lexico-syntactic fea-tures. 3.4 Multi-class Learning Strategies Our original i2b2 system used a 1-vs-1 classifica-tion strategy. This approach creates one classifier for each possible pair of labels (e.g., one classifier decides whether an instance is present vs. absent, another decides whether it is present vs. condition-al, etc.). All of the classifiers are applied to a new instance and the label for the instance is deter-mined by summing the votes of the classifiers. However, Huang et al (2001) reported that this approach did not work well for data sets that had highly unbalanced class probabilities.     Therefore we experimented with an alternative 1-vs-all classification strategy. In this approach, we                                                             2 We hoped to help the classifier recognize lists for nega-tion scoping, although no scoping features were added per se. 
313
create one classifier for each type of label using instances with that label as positive instances and instances with any other label as negative instanc-es. The final class label is assigned by choosing the class that was assigned with the highest confidence value (i.e., the classifier?s score). 4 Evaluation After changing to the 1-vs-all multi-class strategy and adding the new feature set, we evaluated our improved system on the test data and compared its performance with our original system. 4.1 Data  The training set includes 349 clinical notes, with 11,967 assertions of medical problems. The test set includes 477 texts with 18,550 assertions. These assertions were distributed as follows (Table 1):    Training (%) Testing (%) Present 67.28    70.22    Absent  21.18    19.46    Hypothetical            5.44    3.87    Possible  4.47    4.76    Conditional 0.86    0.92    Not Patient 0.77    0.78     Table 1: Assertions Distribution 4.2 Results For the i2b2/VA Challenge submission, our system showed good performance, with 93.01% micro-averaged F1-measure. However, the macro F1-measure was much lower because our recall on the minority classes was weak. For example, most of 
the conditional test cases were misclassified as present. Table 2 shows the comparative results of the two systems (named ?i2b2? for the i2b2/VA Challenge system, and ?new? for our improved sys-tem).   Recall Precision F1-measure  i2b2 New i2b2 New i2b2 New Present 97.89 98.07 93.11 94.46 95.44 96.23 Absent 92.99 94.71 94.30 96.31 93.64 95.50 Possible 45.30 54.36 80.00 78.30 57.85 64.17 Conditional 22.22 30.41 90.48 81.25 35.68 44.26 Hypothetical 82.98 87.45 92.82 92.07 87.63 89.70 Not patient  78.62 81.38 100.0 97.52 88.03 88.72 Micro Avg.    93.01 94.17 93.01 94.17 93.01 94.17 Macro Avg. 70.00 74.39 91.79 89.99 76.38 79.76  Table 2: Result Comparison of Test Data  The micro-averaged F1-measure of our new system is 94.17%, which now outperforms the best official score reported for the 2010 i2b2 challenge (which was 93.62%). The macro-averaged F1-measure increased from 76.38% to 79.76% because perfor-mance on the minority classes improved. The F1-measure improved in all classes, but we saw espe-cially large improvements with the possible class (+6.32%) and the conditional class (+8.58%). Alt-hough the improvement on the dominant classes was limited in absolute terms (+.79% F1-measure for present and +1.86% for absent), the relative reduction in error rate was greater than for the mi-nority classes: -29.25% reduction in error rate for absent assertions, -17.32% for present assertions, and -13.3% for conditional assertions.  
 Present Absent Possible Conditional Hypothetical Not patient  R P R P R P R P R P R P i2b2 98.36 93.18 94.52 95.31 48.22 84.59 9.71 100.0 86.18 95.57 55.43 98.08 + 1-vs-all 97.28 94.56 95.07 94.88 57.38 75.25 27.18 77.78 90.32 93.33 72.83 95.71 + Pruning 97.45 94.63 94.91 94.75 60.34 79.26 33.01 70.83 89.40 94.48 69.57 95.52 +Lex+LS+Sen 97.51 94.82 95.11 95.50 63.35 78.74 33.98 71.43 88.63 93.52 70.65 97.01 + Context 97.60 94.94 95.39 95.97 63.72 78.11 35.92 71.15 88.63 93.52 69.57 96.97  Table 3: Cross Validation on Training Data: Results from Applying New Features Cumulatively (Lex=Lexical features; LS=Lexico-Syntactic features; Sen=Sentence features)  
314
4.3 Analysis We performed five-fold cross validation on the training data to measure the impact of each of the four subsets of features explained in Section 3. Ta-ble 3 shows the cross validation results when cu-mulatively adding each set of features. Applying the 1-vs-all strategy showed interesting results: recall went up and precision went down for all classes except present. Although the overall F1-measure remained almost same, it helped to in-crease the recall on the minority classes, and we were able to gain most of the precision back (with-out sacrificing this recall) by adding the new fea-tures.  The new lexical features including negative pre-fixes and binary tf-idf features primarily increased performance on the absent class. Using document frequency to prune lexical features showed small gains in all classes except absent. Sentence fea-tures helped recognize hypothetical assertions, which often occur in relatively long sentences. The possible class benefitted the most from the new lexico-syntactic features, with a 3.38% recall gain. We observed that many possible concepts were preceded by a question mark ('?') in the train-ing corpus. The new contextual features helped detect more conditional cases. Five allergy-related section headers (i.e. ?Allergies?, ?Allergies and Medicine Reactions?, ?Allergies/Sensitivities?, ?Allergy?, and ?Medication Allergies?) were asso-ciated with conditional assertions. Together, all the new features increased recall by 26.21% on the conditional class, 15.5% on possible, and 14.14% on not associated with the patient. 5.   Conclusions  We created a more accurate assertion classifier that now achieves state-of-the-art performance on as-sertion labeling for clinical texts. We showed that it is possible to improve performance on recogniz-ing minority classes by 1-vs-all strategy and richer features designed with the minority classes in mind. However, performance on the minority clas-ses still lags behind the dominant classes, so more work is needed in this area. Acknowledgments We thank the i2b2/VA challenge organizers for their efforts, and gratefully acknowledge the sup-
port and resources of the VA Consortium for Healthcare Informatics Research (CHIR), VA HSR HIR 08-374 Translational Use Case Projects; Utah CDC Center of Excellence in Public Health Infor-matics (Grant 1 P01HK000069-01), the National Science Foundation under grant IIS-1018314, and the University of Utah Department of Biomedical Informatics. We also wish to thank our other i2b2 team members: Guy Divita, Qing Z. Treitler, Doug Redd, Adi Gundlapalli, and Sasikiran Kandula. Finally, we truly appreciate Berry de Bruijn and Colin Cherry for the prompt responses to our in-quiry.  References  Apache UIMA 2008. Available at http://uima.apache.org. Jason Baldridge, Tom Morton, and Gann Bierner. 2005. OpenNLP Maxent Package in Java, Available at: http://incubator.apache.org/opennlp/. Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko, Joel Martin, and Xiaodan Zhu. 2011. Machine-Learned Solutions for Three Stages of Clinical In-formation Extraction: the State of the Art at i2b2 2010. J Am Med Inform Assoc. Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a Li-brary for Support Vector Machines, 2001. Available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. 2001.   A Simple Algorithm for Identifying Negated Find-ings and Diseases in Discharge Summaries. Journal of Biomedical Informatics, 34:301-310. Wendy W. Chapman, David Chu, and John N. Dowling. 2007. ConText: An Algorithm for Identifying Con-textual Features from Clinical Text. BioNLP 2007: Biological, translational, and clinical language pro-cessing, Prague, CZ. David Ferrucci and Adam Lally. 2004. UIMA: An Ar-chitectural Approach to Unstructured Information Processing in the Corporate Research Environment. Journal of Natural Language Engineering, 10(3-4): 327-348. Tzu-Kuo Huang, Ruby C. Weng, and Chih-Jen Lin. 2006. Generalized Bradley-Terry Models and Mul-ticlass Probability Estimates. Journal of Machine Learning Research, 7:85-115. i2b2/VA 2010 Challenge Assertion Annotation Guidelines. https://www.i2b2.org/NLP/Relations/assets/Assertion%20Annotation%20Guideline.pdf. 
315
LVG (Lexical Variants Generation). 2010. Available at: http://lexsrv2.nlm.nih.gov/LexSysGroup/Projects/lvg. Alexa T. McCray, Suresh Srinivasan, and Allen C. Browne. 1994. Lexical Methods for Managing Varia-tion in Biomedical Terminologies. Proc Annu Symp Comput Appl Med Care.:235?239. St?phane M. Meystre and Peter J. Haug. 2005. Automa-tion of a Problem List Using Natural Language Pro-cessing. BMC Med Inform Decis Mak, 5:30. Guergana K. Savova, James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper- Schuler, and Christopher G. Chute. 2010. Mayo clin-ical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. J Am Med Inform Assoc., 17(5):507-513. ?zlem Uzuner and Scott DuVall. 2010. Fourth i2b2/VA Challenge. In http://www.i2b2.org/NLP/Relations/. ?zlem Uzuner, Xiaoran Zhang, and Sibanda Tawanda. 2009. Machine Learning and Rule-based Approaches to Assertion Classification. J Am Med Inform Assoc., 16:109-115.
316
Proceedings of BioNLP Shared Task 2011 Workshop, pages 89?93,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
The Taming of Reconcile as a Biomedical Coreference Resolver 
  Youngjun Kim,    Ellen Riloff,    Nathan Gilbert School of Computing University of Utah Salt Lake City, UT {youngjun, riloff, ngilbert} @cs.utah.edu       Abstract 
To participate in the Protein Coreference section of the BioNLP 2011 Shared Task, we use Reconcile, a coreference resolution engine, by replacing some pre-processing components and adding a new mention detector. We got some improvement from training two separate classifiers for detecting anaphora and antecedent mentions. Our system yielded the highest score in the task, F-score 34.05% in partial mention, protein links, and system recall mode. We witnessed that specialized mention detection is crucial for coreference resolution in the biomedical domain.  
1 Introduction Coreference resolution is a mechanism that groups entity mentions in a text into coreference chains based on whether they refer to the same real-world entity or concept. Like other NLP applications, which must meet the need for aggressive and sophisticated methods of detecting valuable information in emerging domains, numerous coreference resolvers have been developed, including JavaRap (Qiu et al, 2004), GuiTaR (Poesio and Kabadjov, 2004) and BART (Versley et al, 2008). Our research uses a recently released system, Reconcile (Stoyanov et al 2009; 2010a; 2010b), which was designed as a general architecture for coreference resolution that can be used to easily create learning-based coreference resolvers. Reconcile is based on supervised learning approaches to coreference resolution and 
has showed relatively good performance compared with similar types of systems.  As a first step to adapting Reconcile for the biomedical domain, specifically the BioNLP Shared Task 2011 (Kim et al, 2011), we modified several subcomponents in Reconcile and revised the feature set for this task. Most importantly, we created a specialized mention detector trained for biomedical text. We trained separate classifiers for detecting anaphor and antecedent mentions, and experimented with several clustering techniques to discover the most suitable algorithm for producing coreference chains in this domain. 2 BioNLP 2011 Shared Task  Our system was developed to participate in a Protein Coreference (COREF) task (Nguyen et al, 2011), one of the supporting tasks in the BioNLP Shared Task 2011. The COREF task is to find all mentions participating in the coreference relation and to connect the anaphora-antecedent pairs. The corpus is based on the Genia-Medco coreference corpus. The Genia-Medco corpus was produced for the biomedical domain, and some comparative analysis with this corpus and other newswire domain data have been performed (Yang et al, 2004a; 2004b; Nguyen and Kim, 2008; Nguyen et al, 2008).  The COREF corpus consists of 800 text files for training, 150 for development, and 260 for testing, which all have gene/protein coreference annotations. The training set has 2,313 pairs of coreference links with 4,367 mentions. 2,117 mentions are antecedents, with an average of 4.21 tokens each (delimited by white space), and 2,301 
89
mentions are anaphora, with an average of 1.28 tokens each. The anaphora are much shorter because many of them are pronouns. The five most frequent anaphora are that (686 times), which (526), its (270), their (130), and it (100).  3 Our Coreference Resolver Reconcile was designed to be a research testbed capable of implementing the most current approaches to coreference resolution. Reconcile is written in Java, to be portable across platforms, and was designed to be easily reconfigurable with respect to sub-components, feature sets, parameter settings, etc. A mention detector and an anaphora-antecedent pairs generator are added for the COREF task. 3.1 Preprocessing For pre-processing, we used the Genia Tagger (Tsuruoka and Tsujii. 2005) for sentence splitting, tokenizing, and part-of-speech (POS) tagging. For parsing, we used the Enju parser (Miyao and Tsujii, 2008). We replaced Reconcile?s mention detection module with new classifiers because of poor performance on the biomedical domain with the provided classifiers. We reformatted the training data with IOB tags and trained a sequential classifier using CRF++ (Kudoh, 2007). For this sequence tagging, we borrowed the features generally used for named entity recognition in the biomedical literature (Finkel et al, 2005; Zhou et al, 2005; McDonald and Pereira, 2005), including word, POS, affix, orthographic features and combinations of these features. We extracted features from the target word, as well as two words to its left and two words to its right. Two versions of mention detectors were developed. The first (MD-I) trained one model without differentiating between anaphora and antecedents. For this method, we chose the longest mentions when multiple mentions overlapped. The other detector (MD-II) used two different models for the antecedent and anaphor, classifying them separately. MD-II?s classification result was used when generating the anaphora-antecedent pairs. Table 1 shows the performance of exact matching by these detectors compared with the performance of the Genia Noun Phrase (NP) chunker. Our classifiers did much better, 81.31% precision and 
64.78% recall (MD-II), than the Genia chunker, 6.58% precision and 72.67% recall. Only an average of six mentions occurred in each text, while the Genia chunker detected 66.27 noun phrases on average. The Genia annotation scheme was not limited to specific types of concepts, so the Genia NP chunker identifies every possible concept. In contrast, the COREF shared task only involves a subset of the concepts. Mention boundaries were also frequently mismatched. For example, ?its? was annotated as a mention in the COREF task when it appears as a possessive inside a noun phrase (e.g., ?its activity?), but the Genia NP Chunker tags the entire noun phrase as a mention.   Prec Rec F Genia NP Chunker   6.58 72.67 12.07 Mention Detector-I 80.85 63.33 71.03 Mention Detector-II 81.31 64.78 72.11     Antecedent 65.48 41.35 50.69     Anaphor 91.72 85.07 88.27 Table 1:Mention Detection Results on Dev. Set 3.2 Feature Generation We used the following four types of features: Lexical: String-based comparisons of the two mentions, such as exact string matching and head noun matching.  Proximity: Sentence measures of the distance between two mentions.   Grammatical: A wide variety of syntactic properties of the mentions, either individually or in pairs. These features are based on part-of-speech tags, or parse trees.  Semantic: Semantic information about one or both mentions, such as tests for gender and animacy.  Due to the unavailability of paragraph information in our training data, we excluded Reconcile?s paragraph features. Also, named entity and dependency parsing features were not used for training. Table 2 shows the complete feature set used for this task. In total, we excluded nine existing Reconcile features, mostly semantic features: WordNetClass, WordNetDist, WordNetSense, Subclass, ParNum, SameParagraph, IAntes, Prednom, WordOverlap. Full descriptions of these features can be found in Stoyanov (2010a).  
90
Lexical HeadMatch, PNStr, PNSubstr, ProStr, SoonStr, WordsStr, WordsSubstr Proximity ConsecutiveSentences, SentNum, SameSentence  Syntactic Binding, BothEmbedded, BothInQuotes, BothPronouns, BothProperNouns, BothSubjects, ContainsPN, Contraindices, Definite1, Definite2, Demonstrative2, Embedded1, Embedded2, Indefinite, Indefinite1, InQuote1, InQuote2, MaximalNP, Modifier, PairType, Pronoun, Pronoun1, Pronoun2, ProperNoun, ProResolve, RuleResolve, Span, Subject1, Subject2, Syntax Semantic Agreement, Alias, AlwaysCompatible, Animacy, Appositive, ClosestComp, Constraints, Gender, instClass, Number, ProComp, ProperName, Quantity, WNSynonyms  Table 2: Feature Set for Coreference Resolution 3.3 Clustering After Reconcile makes pairwise decisions linking each anaphor and antecedent, it produces a clustering of the mentions in a document to create coreference chains. Because the format of the COREF task submission was not chains but anaphora-antecedent pairs, it would have been possible to submit the direct results of Reconcile?s pairwise decisions. However, it was easier to use Reconcile as a black-box and post-process the chains to reverse-engineer coreferent pairs from them. Reconcile supports three clustering algorithms: Single-link Clustering (SL) (Transitive Closure) groups together all mentions that are connected by a path of coreferent links.  Best-first (BF) clustering uses the classifier?s confidence value to cluster each noun phrase with its most confident antecedent. Most Recent First (MRF) pairs each noun phrase with the single most recent antecedent that is labeled as coreferent.  Table 3 shows the MUC scores of each clustering method with gold standard mentions and with the mentions automatically detected by each of our two mention detectors. Not surprisingly, using gold mentions produced the highest score of 87.32%. Automatically detected mentions yielded much lower performance. MD-I performed best, in this evaluation, achieving 49.65%. The most recent 
first clustering algorithm produced the best results for both gold mentions and MD-I. The single link clustering algorithm, which is the default method used by Reconcile, produced the lowest results for both gold mentions and MD-I.   SL BF MRF Gold Mention 85.34 86.87 87.32 Mention Detector-I 48.64 48.82 49.65 Mention Detector-II 48.31 48.62 48.07 Table 3: MUC Scores of Dev. Set by Three Different Clustering Methods (SL: Single-link, BF: Best-first, MRF: Most recent first) 3.4 Pair Generation from Chains Reconcile generates coreference chains, but the output for the shared task required anaphora-antecedent pairs. Therefore, we needed to extract individual pairs from the chains. We used the chains produced by the most recent first clustering algorithm for pair generation. When using MD-I output, we took the earliest mention (i.e., the one occurring first in the source document) in the chain and paired it with each of the subsequent mentions in the same chain. Thus, each chain of size N produced N-1 pairs. When using the MD-II predictions, the classifiers gave us two separate lists of antecedent and anaphora mentions. In this case, we paired each anaphor in the chain with every antecedent in the same chain that preceded it in the source document.  3.5 Evaluation and Analysis The mention linking can be evaluated using three different scores: atom coreference links, protein coreference links, and surface coreference links. In the atom link option, only links containing given gene/protein annotations are considered while in the surface link option, every link is a target for the evaluation. Protein links are similar to atom links but loosen the boundary of gene/protein annotations. There were 202 protein links out of 469 surface links in development set.  For mention detection, exact match and partial match are supported in the task evaluation. Recall is measured in two modes. In system mode, every link is calculated for the linking evaluation. In algorithm mode, only links with correctly detected mentions are considered for evaluation. For 
91
detailed information refer to Nguyen et al (2011) or the task web site.1 Table 4 shows the mention linking results (F-score) for the COREF task evaluation using partial match and system recall. The surface link score on gold mentions reached 90.06%. For automatic mention detection, MD-I achieved a score of 45.38% score, but MD-II produced a substantially better score of 50.41%. MD-II, which was trained separately for antecedent and anaphora detection, performed about 5% higher than MD-I in every link mode.    Atom Protein Surface Gold Mention 84.09 84.09 90.06 Mention Detector-I 28.67 34.41 45.38 Mention Detector-II 33.45 39.27 50.41 Table 4: Dev. Set Results by Three Different Evaluation Options Table 5 shows the recall and precision breakdown for the protein evaluation results. Looking behind the composite F-score reveals that our system produced higher precision than recall. Looking back at Table 1, we saw that our anaphor detector performed much better than our antecedent detector. Since every coreference link requires one of each, the relatively poor performance of antecedent detection (especially in terms of recall) is a substantial bottleneck.     Prec Rec F Gold Mention 98.67 73.27 84.09 Mention Detector-I 62.34 23.76 34.41 Mention Detector-II 73.97 26.73 39.27 Table 5: Precision and Recall Breakdown for Protein Evaluation Coreference Links 3.6 Results: Submission for COREF Task We merged the training and development sets to use as training data for Reconcile. We used MD-II for mention detection and the most recent first algorithm for clustering to submit the final output on the test data. Table 6 shows the results of our final submission along with the five other participating teams for the protein evaluation coreference links (Nguyen et al, 2011). Our                                                             1 http://sites.google.com/site/bionlpst/home/protein-gene-coreference-task 
system produced a 34.05% F-score (73.26% precision and 22.18% recall) in protein coreference links and 25.41% F-score in atom links.   Team  Prec  Rec F University of Utah 73.26 22.18 34.05 University of Zurich 55.45 21.48 30.96 Concordia University 63.22 19.37 29.65 University of Turku 67.21 14.44 23.77 University of Szeged   3.47   3.17   3.31 University College Dublin   0.25   0.70   0.37 Table 6: Evaluation Results of Final Submissions (Protein Coreference Links) 4 Conclusions The effort to tame Reconcile as a coreference engine for the biomedical domain was successful and our team?s submission obtained satisfactory results. However, there is ample room for improvement in coreference resolution. We observed that mention detection is crucial - the MUC score reached 87.32% with gold mentions on the development set but only 49.65% with automatically detected mentions (Table 3). One possible avenue for future work is to develop domain-specific features to better identify mentions in biomedical domains.  Acknowledgments  We thank the BioNLP Shared Task 2011 organizers for their efforts, and gratefully acknowledge the support of the National Science Foundation under grants IIS-1018314 and DBI-0849977 and the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily express the view of the DARPA, AFRL, NSF, or the U.S. government. References Jenny Finkel, Shipra Dingare, Christopher D Manning, Malvina Nissim, Beatrice Alex, and Claire Grover. 2005. Exploring the Boundaries: Gene and Protein Identifica-tion in Biomedical Text. BMC Bioinformatics. 6:S5. 
92
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, and Jun?ichi Tsujii. 2011. Overview of BioNLP Shared Task 2011. Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. ACL 2011. Taku Kudoh. 2007. CRF++. http://crfpp.sourceforge.net/. Ryan McDonald and Fernando Pereira. 2005. Identifying Gene and Protein Mentions in Text Using Condi-tional Random Fields. BMC Bioinformatics. 6:S6. Yusuke Miyao and Jun'ichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. Computational Linguistics, 34(1):35?80. Ngan L. T. Nguyen and Jin-Dong Kim. 2008. Exploring Domain Differences for the Design of Pronoun Resolution Systems for Biomedical Text. Proceedings of COLING 2008:625-632 Ngan L. T. Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2008. Challenges in Pronoun Resolution System for Biomedical Text. Proceedings of LREC 2008.  Ngan L. T. Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011. Overview of the Protein Coreference task in BioNLP Shared Task 2011. Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. ACL 2011. Massimo Poesio and Mijail A. Kabadjov. 2004. A general-purpose, off-the-shelf anaphora resolution module: implementation and preliminary evaluation. Proceedings of LREC 2004. Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A Public Reference Implementation of the Rap Anaphora Resolution Algorithm. Proceedings of LREC 2004. Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010a. Reconcile: A Coreference Resolution Platform. Tech Report. Cornell University. Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010b. Coreference Resolution with Reconcile. Proceedings of ACL 2010. Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art. Proceedings of ACL-IJCNLP 2009. Yoshimasa Tsuruoka and Jun'ichi Tsujii. 2005. Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data. Proceedings of HLT/EMNLP 2005:467-474. Yannick Versley,  Simone P. Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng 
Yang, and Alessandro Moschitti. 2008. BART: A modular toolkit for coreference resolution. Proceedings of LREC 2008. Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew Lim Tan. 2004a. A NP-Cluster Based Approach to Coreference Resolution. Proceedings of COLING 2004:226-232. XiaoFeng Yang, GuoDong Zhou, Jian Su, and Chew Lim Tan. 2004b. Improving Noun Phrase Coreference Resolution by Matching Strings. Proceedings of IJCNLP 2004:226-333. GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su, and SoonHeng Tan. 2005. Recognition of Protein/Gene Names from Text Using an Ensemble of Classifiers. BMC Bioinformatics. 6:S7. 
93
