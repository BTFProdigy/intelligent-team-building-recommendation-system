A Robust Cross-Style Bilingual Sentences Alignment Model
Tz-Liang Kueng Keh-Yih Su
Behavior Design Corporation
2F, No.5, Industry E. Rd. IV,
Science-Based Industrial Park,
Hsinchu, Taiwan 30077, R.O.C.
{cavs, kysu}@bdc.com.tw
Abstract
Most current sentence alignment approaches adopt
sentence length and cognate as the alignment features;
and they are mostly trained and tested in the docu-
ments with the same style. Since the length distribu-
tion, alignment-type distribution (used by length-based
approaches) and cognate frequency vary significantly
across texts with different styles, the length-based ap-
proaches fail to achieve similar performance when tested
in corpora of different styles. The experiments show that
the performance in F -measure could drop from 98.2%
to 85.6% when a length-based approach is trained by a
technical manual and then tested on a general magazine.
Since a large percentage of content words in the source
text would be translated into the corresponding trans-
lation duals to preserve the meaning in the target text,
transfer lexicons are usually regarded as more reliable
cues for aligning sentences when the alignment task is
performed by human. To enhance the robustness, a
robust statistical model based on both transfer lexicons
and sentence lengths are proposed in this paper. Af-
ter integrating the transfer lexicons into the model, a
60% F -measure error reduction (from 14.4% to 5.8%) is
observed.
1 Introduction
Since the bilingual corpus is a valuable resource for
training statistical language models [Dagon, 91; Su
et al, 95; Su and Chang, 99] and sentence align-
ment is the first step for most such tasks, many
alignment approaches have been proposed in the
literature [Brown, 91; Gale and Church, 93; Wu,
94; Vogel et al, 96; Och and Ney, 2000]. Most of
those reported approaches use the sentence length
as the main feature to perform the alignment task.
For example, Brown et al (91) used the feature
of number-of-words for alignment, and [Gale and
Church,93] claimed that better performance can be
achieved (5.8% error rate for English-French cor-
pus) if the number-of-characters is adopted instead.
As cognates are reliable cues for language pairs de-
rived from the same family, Church (93) also at-
tacked this problem by considering cognates addi-
tionally. Because most of those reported work are
performed on those Indo-European language-pairs,
for testing the performance on non-Indo-European
languages, Wu (94) had tried both length and cog-
nate features on the Hong Kong Hansard English-
Chinese corpus, and 7.9% error rate has been re-
ported. Besides, sentence alignment can also be
indirectly achieved via more complicated word cor-
responding models [Brown et al, 93; Vogel et al,
96; Och and Ney, 2000]. Since those word corre-
sponding models, which also achieve similar per-
formance, are more complicated and run relatively
slow, they seems to be over-killed for the task of
aligning sentences and will not be discussed in this
paper.
Although length-based approaches above men-
tioned are simple and can achieve good perfor-
mance, they are usually trained and tested in
the text with the same style. Therefore, they
are style-dependent approaches. Since performing
supervised-training for each style is not feasible in
many applications, it would be interesting to know
whether those length-based approaches can still
achieve the similar performance if they are tested in
the text with different styles other than the train-
ing corpora. An experiment was thus conducted
to train the parameters with a machinery technical
manual; the performance is then tested on a general
magazine (for introducing Taiwan to foreign visi-
tors). It shows that the testing set performance of
the length-based model (with cognates considered)
would drop from 98.2% (tested in the same tech-
nical domain) to 85.6% (tested in the new general
magazine) in F -measure. After investigating those
errors, it has been found that the length distribu-
tion and alignment-type distribution (used by those
length-based approaches) vary significantly across
the texts of different styles (as would be shown in
Tables 5.2 and 5.3), and the cognate-frequency1
drops greatly from the technical manual to a gen-
eral magazine in non-Indo-European languages (as
would be shown in Table 5.3).
On the other hand, sentence length is seldom
used by a human to align bilingual sentences. They
usually do not align bilingual sentences by counting
the number of characters (or words) in the sentence
pairs. Instead, since a large percentage of content
words in the source text would be translated into
their translation-duals to preserve the meaning in
the target text, transfer-lexicons are usually used
for aligning sentences when the alignment task is
performed by human. To enhance the robustness
across different styles, transfer-lexicons are thus in-
tegrated into the traditional sentence-length based
model in the proposed robust statistical model de-
scribed below. After integrating transfer-lexicons
into the model, a 60% F -measure error reduction
(from 14.4% to 5.8%) has been observed, which cor-
responds to improving the cross-style performance
from 85.6% to 94.2% in F -measure.
The details of the proposed robust model, the as-
sociated features extracted from the bilingual cor-
pora, and the probabilistic scoring function will be
given in Section 2. In Section 3, we briefly men-
tion some implementation issues. The associated
performance evaluation is given in Section 4, and
Section 5 would address error analysis and discusses
the limitation of the proposed statistical model. Fi-
nally, the concluding remarks are given in Section
6.
1Here ?Cognate? mainly refers to those English proper
nouns (such as those company names of IBM, HP; or the
technical terms such as IEEE-1394, etc.) that appear in the
Chinese text. As they are most likely to be directly copied
from the English sentence into the corresponding Chinese
one, they are reliable cues.
2 Statistical Sentence Alignment Model
Since an English-Chinese bilingual corpus will be
adopted in our experiments, we will denote the
source text with m sentences as ESm1 , and its
corresponding target text, with n sentences, as
CSn1 . Let Mi = {typei,1, ? ? ? , typei,Ni} denote
the i-th possible alignment-candidate, consisting of
Ni Alignment-Passages of typei,j , j = 1, ? ? ? , Ni;
where typei,j is the matching type (e.g., 1?1, 0?1,
1?0, etc.) of the j-th Alignment-Passage in the i-th
alignment-candidate, and Ni denotes the number of
the total Alignment-Passages in the i-th alignment-
candidate. Then the statistical alignment model is
to find the Bayesian estimate M? among all pos-
sible alignment candidates, shown in the following
equation
M? = argmax
Mi
P (Mi|ES
m
1 , CS
n
1 ). (2.1)
According to the Bayesian rule, the maximization
problem in (2.1) is equivalent to solving the follow-
ing maximization equation
M? = argmax
Mi
P (ESm1 , CS
n
1 |Mi)P (Mi)
= argmax
Mi
{P (Aligned-Pair
i,Ni
i,1
|type
i,Ni
i,1
)P (type
i,Ni
i,1
)}
= argmax
Mi
Ni?
j=1
{P (Aligned-Pairi,j |Aligned-Pair
i,j?1
i,1
, typei,j
i,1
) ?
P (typei,j |type
i,j?1
i,1
)}, (2.2)
where Aligned-Pairi,j , j = 1, ? ? ? , Ni, denotes
the j-th aligned English-Chinese bilingual sentence
groups pair in the i-th alignment candidate.
Assume that
P (Aligned-Pairi,j |Aligned-Pair
i,j?1
i,1 , type
i,j
i,1)
? P (Aligned-Pairi,j |typei,j), (2.3)
and different typei,j in the i-th alignment can-
didate are statistically independent2, then the
above maximization problem can be approached by
searching for
M? ? argmax
Mi
Ni?
j=1
{P (Aligned-Pairi,j |typei,j)P (typei,j)},
(2.4)
where M? denotes the desired candidate.
2A more reasonable one should be the first-order Markov
model (i.e., Type-Bigram model); however, it will signifi-
cantly increase the searching time and thus is not adopted
in this paper.
2.1 Baseline Model
To make the above model feasible, Aligned-Pairi,j
should be first transformed into an appropriate
feature space. The baseline model will use both
the length of sentence [Brown et al, 91; Gale and
Church, 93] and English cognates [Wu, 94], and is
shown as follows:
argmax
Mi
Ni?
j=1
f(?c, ?w|typei,j)P (?cognate)P (typei,j),
(2.5)
where ?c and ?w denote the normalized differences
of characters and words as explained in the follow-
ing; ?c is defined to be (ltc ? clsc)/
?
lscs2c , where
lsc and ltc are the character numbers of the aligned
bilingual portions of source text and target text,
respectively, under consideration; c denotes the
proportional constant for target-character-count
and s2c denotes the corresponding target-character-
count variance per source-character. Similarly, ?w
is defined to be (ltw ? wlsw)/
?
lsws2w, where lsw
and ltw are the word numbers of the aligned bilin-
gual portions of source text and target text, re-
spectively; w denotes the proportional constant
for target-word-count and s2w denotes the corre-
sponding target-word-count variance per source-
word. Also, the random variables ?c and ?w are
assumed to have bivariate normal distribution and
each possesses a standard normal distribution with
mean 0 and variance 1. Furthermore, ?cognate de-
notes (?Number of English cognates found in the
given Chinese sentences???Number of correspond-
ing English cognates found in the given English
sentences?), and is Poisson3 distributed indepen-
dent of its associated matching-type; also assume
that ?cognate is independent of other features (i.e.,
character-count and word-count).
2.2 Proposed Transfer Lexicon Model
Since transfer-lexicons are usually regarded as
more reliable cues for aligning sentences when the
alignment task is performed by human, the above
baseline model is further enhanced by adding
3Since almost all those English cognates found in the
given Chinese sentences can be found in the corresponding
English sentences, ?cognate had better to be modeled as a
Poisson distribution for a rare event (rather than Normal
distribution as some papers did).
those associated transfer lexicons to it. Those
translated Chinese words, which are derived from
each English word (contained in given English
sentences) by looking up some kinds of dictionar-
ies, can be viewed as transfer-lexicons because
they are very likely to appear in the translated
Chinese sentence. However, as the distribution
of various possible translations (for each English
lexicon) found in our bilingual corpus is far more
diversified4 compared with those transfer-lexicons
obtained from the dictionary, only a small num-
ber of transfer-lexicons can be matched if the
exact-match is specified. Therefore, each Chinese-
Lexicon obtained from the dictionary is first
augmented with its associated Chinese characters,
and then the augmented transfer-lexicons set are
matched with the target Chinese sentence(s). Once
an element of the augmented transfer-lexicons set
is matched in the target Chinese sentence, it is
counted as being matched. So we compute the
Normalized-Transfer-Lexicon-Matching-Measure,
?Transfer?Lexicons which denotes [(?Number of
augmented transfer-lexicons matched???Number
of augmented transfer-lexicons unmatched?)/
?Total Number of augmented transfer-lexicons
sets?], and add it to the original model as another
additional feature.
Assume follows normal distribution and the asso-
ciated parameters are estimated from the training
set, Equation (2.5) is then replaced by
argmaxMi
Ni?
j=1
{f1(?c, ?w|typei,j)P (?cognate)?
f2(?Transfer?Lexicons)?
P (typei,j)}. (2.6)
3 Implementation
The best bilingual sentence alignment in those
above models can be found by utilizing a dynamic
programming algorithm, which is similar to the dy-
namic time warping algorithm used in speech recog-
nition [Rabiner and Juang, 93]. Currently, the
4For example, the English word ?number? are found to be
translated into ??K?, ? K?, ??K?, ?? K?, ???K?, ??
}?, ? ? ? etc., for a specific sense in the given corpus; however,
the transfer entries listed in the dictionary are ??K? and ??
}? only.
Case I (Length-Type Error)
(E1) Compared to this, modern people have relatively better nutrition and mature faster, working women marry later, and there
has been a great decrease in frequency of births, so that the number of periods in a lifetime correspondingly increases, so
it is not strange that the number of people afflicted with endometriosis increases greatly.
(C1) ???, ?HA??? ?o, <?v??wu, ?>?byu??, ??2~%V??b?$??, ??q??P66??6.??J7
(E2) The problem is not confined to women.
(E3) ?Sperm activity also noticeably decreases in men over forty,? says Taipei Medical College urologist Chang Han-sheng.
(C2) .?u?4,?4??uJ(, ???6}p???'? C????+Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1173?1181,
Beijing, August 2010
 A Character-Based Joint Model for Chinese Word Segmentation
Kun Wang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Science
{kunwang,cqzong}@nlpr.ia.ac.cn 
Keh-Yih Su 
Behavior Design Corporation
 
Kysu@bdc.com.tw 
 
Abstract 
The character-based tagging approach 
is a dominant technique for Chinese 
word segmentation, and both discrimi-
native and generative models can be 
adopted in that framework. However, 
generative and discriminative charac-
ter-based approaches are significantly 
different and complement each other. 
A simple joint model combining the 
character-based generative model and 
the discriminative one is thus proposed 
in this paper to take advantage of both 
approaches. Experiments on the Sec-
ond SIGHAN Bakeoff show that this 
joint approach achieves 21% relative 
error reduction over the discriminative 
model and 14% over the generative one. 
In addition, closed tests also show that 
the proposed joint model outperforms 
all the existing approaches reported in 
the literature and achieves the best F-
score in four out of five corpora. 
1 Introduction 
Chinese word segmentation (CWS) plays an 
important role in most Chinese NLP applica-
tions such as machine translation, information 
retrieval and question answering. Many statis-
tical methods for CWS have been proposed in 
the last two decades, which can be classified as 
either word-based or character-based. The 
word-based approach regards the word as the 
basic unit, and the desired segmentation result 
is the best word sequence found by the search 
process. On the other hand, the character-based 
approach treats the word segmentation task as 
a character tagging problem. The final segmen-
tation result is thus indirectly generated ac-
cording to the tag assigned to each associated 
character. Since the vocabulary size of possible 
character-tag-pairs is limited, the character-
based models can tolerate out-of-vocabulary 
(OOV) words and have become the dominant 
technique for CWS in recent years. 
On the other hand, statistical approaches can 
also be classified as either adopting a genera-
tive model or adopting a discriminative model. 
The generative model learns the joint probabil-
ity of the given input and its associated label 
sequence, while the discriminative model 
learns the posterior probability directly. Gen-
erative models often do not perform well be-
cause they make strong independence assump-
tions between features and labels. However, 
(Toutanova, 2006) shows that generative mod-
els can also achieve very similar or better per-
formance than the corresponding discrimina-
tive models if they have a structure that avoids 
unrealistic independence assumptions.  
In terms of the above dimensions, methods 
for CWS can be classified as:  
1) The word-based generative model (Gao et 
al., 2003; Zhang et al, 2003), which is a well-
known approach and has been used in many 
successful applications;  
2) The word-based discriminative model 
(Zhang and Clark, 2007), which generates 
word candidates with both word and character 
features and is the only word-based model that 
adopts the discriminative approach? 
3) The character-based discriminative model 
(Xue, 2003; Peng et al, 2004; Tseng et al, 
2005; Jiang et al, 2008), which has become 
the dominant method as it is robust on OOV 
words and is capable of handling a range of 
different features, and it has been adopted in 
many previous works;  
1173
4) The character-based generative model 
(Wang et al, 2009), which adopts a character-
tag-pair-based n-gram model and achieves 
comparable results with the popular character-
based discriminative model. 
In general, character-based models are much 
more robust on OOV words than word-based 
approaches do, as the vocabulary size of char-
acters is a closed set (versus the open set of 
that of words). Furthermore, among those 
character-based approaches, the generative 
model and the discriminative one complement 
each other in handling in-vocabulary (IV) 
words and OOV words. Therefore, a character-
based joint model is proposed to combine them. 
This proposed joint approach has achieved 
good balance between IV word recognition 
and OOV word identification. The experiments 
of closed tests on the second SIGHAN Bakeoff 
(Emerson, 2005) show that the joint model 
significantly outperforms the baseline models 
of both generative and discriminative ap-
proaches. Moreover, statistical significance 
tests also show that the joint model is signifi-
cantly better than all those state-of-the-art sys-
tems reported in the literature and achieves the 
best F-score in four of the five corpora tested. 
2 Character-Based Models for CWS 
The goal of CWS is to find the corresponding 
word sequence for a given character sequence. 
Character-based model is to find out the corre-
sponding tags for given character sequence. 
2.1 Character-Based Discriminative Model 
The character-based discriminative model 
(Xue, 2003) treats segmentation as a tagging 
problem, which assigns a corresponding tag to 
each character. The model is formulated as: 
1
1 1 1 1 2
1 1
( ) ( , ) (
n n
n n k n k
k k
k k
P t c P t t c P t c? ?
= =
= ?? ? 2 )k +        (1) 
Where tk is a member of {Begin, Middle, End, 
Single} (abbreviated as B, M, E and S from 
now on) to indicate the corresponding position 
of character ck in its associated word. For ex-
ample, the word ???? (Beijing City)? will 
be assigned with the corresponding tags as: ??
/B (North) ?/M (Capital) ?/E (City)?.  
Since this tagging approach treats characters 
as basic units, the vocabulary size of those 
possible character-tag-pairs is limited. There-
fore, this method is robust to OOV words and 
could possess a high recall of OOV words 
(ROOV). Although the dependency between ad-
jacent tags/labels can be addressed, the de-
pendency between adjacent characters within a 
word cannot be directly modeled under this 
framework. Lower recall of IV words (RIV) is 
thus usually accompanied (Wang et al, 2009).  
In this work, the character-based discrimina-
tive model is implemented by adopting the fea-
ture templates given by (Ng and Low, 2004), 
but excluding those ones that are forbidden by 
the closed test regulation of SIGHAN (e.g., 
Pu(C0): whether C0 is a punctuation). Those 
feature templates adopted are listed below: 
1
1 1
( ) ( 2, 1,0,1, 2);
( ) ( 2, 1,0,1);
( )
n
n n
a C n
b C C n
c C C
+
?
= ? ?
= ? ?  
For example, when we consider the third 
character ??? in the sequence ???????, 
template (a) results in the features as following: 
C-2=?, C-1=?, C0=?, C1=?, C2=?, and tem-
plate (b) generates the features as: C-2C-1=??, 
C-1C0=??, C0C1=??, C1C2=??, and tem-
plate (c) gives the feature C-1C1=??. 
2.2 Character-Based Generative Model 
To incorporate the dependency between adja-
cent characters in the character-based approach, 
(Wang et al, 2009) proposes a character-based 
generative model. In this approach, word wi is 
first replaced with its corresponding sequence 
of [character, tag] (denoted as [c, t]), where tag 
is the same as that adopted in the above char-
acter-based discriminative model. With this 
representation, this model can be expressed as:  
 
1 1 1 1
1 1 1 1
( ) ([ , ] )
( [ , ] ) ([ , ] ) ( )
m n n n
n n n n
P w c P c t c
P c c t P c t P c
?
= ?                    (2) 
Since 1 1( [ , ] ) 1
n nP c c t ?  and  is the same for 
various candidates, only should be 
considered. It can be further simplified with 
Markov Chain assumption as: 
1( )
nP c
([ ,P c 1] )
nt
 11
1
([ , ] ) ([ , ] [ , ] ).
n
n
i i k
i
P c t P c t c t ??
=
?? i                     (3) 
Compared with the character-based dis-
criminative model, this generative model keeps 
the capability to handle OOV words because it 
also regards the character as basic unit. In ad-
dition, the dependency between adjacent 
1174
? Gold and Discriminative Tag: M Generative Trigram Tag: E 
Tag probability:  B/0.0333 E/0.2236 M/0.7401 S/0.0030 
Feature 
Tag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1
B -1.4375 0.1572 0.0800 0.2282 0.7709 0.2741 0.0000 0.0000 -0.6718 0.0000
E 1.3558 0.1910 0.7229 -1.2696 -0.5970 0.0049 0.0921 0.0000 0.8049 0.0000
M 1.1071 -0.5527 -0.3174 2.9422 0.4636 -0.1708 0.0000 0.0000 -0.9700 0.0000
S -1.0254 0.2046 -0.4856 -1.9008 -0.6375 0.0000 0.0000 0.0000 0.8368 0.0000
? Gold and Discriminative Tag: E Generative Trigram Tag: S 
Tag probability:  B/0.0009 E/0.8138 M/0.0012 S/0.1841 
Feature 
Tag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1
B 0.3586 0.4175 0.0000 -0.7207 0.4626 0.0085 0.0000 0.0000 0.0000 0.0000
E 0.3666 0.0687 4.5381 2.8300 -0.0846 0.0000 0.0000 -1.0279 0.6127 0.0000
M -0.5657 -0.4330 1.8847 0.0000 -0.0918 0.0000 0.0000 0.0000 0.0000 0.0000
S -0.1595 -0.0532 2.7360 1.8223 -0.2862 -0.0024 0.0000 1.0494 0.7113 0.0000
Table 1: The corresponding lambda weight of features for ????? in the sentence ?[?] [?] [?] [???] 
[?] [?] [?] [?]?. In the Feature column and Tag row, the value is the corresponding lambda weight for 
the feature and tag under ME framework. The meanings of those features are explained in Section 2.1. 
 
characters is now directly modeled. This will 
give sharper preference when the history of 
assignment is given. Therefore, this approach 
not only holds robust IV performance but also 
achieves comparable results with the discrimi-
native model. However, the OOV performance 
of this approach is still lower than that of the 
discriminative model (see in Table 5), which 
would be discussed in the next section. 
3 Problems with the Character-Based 
Generative Model 
The character-based generative model can 
handle the dependency between adjacent char-
acters and thus performs well on IV words. 
However, this generative trigram model is de-
rived under the second order Markov Chain 
assumption. Future character context (i.e., C1 
and C2) is thus not utilized in the model when 
the tag of the current character (i.e., t0) is de-
termined. Nevertheless, the future context 
would help to select the correct tag when the 
associated trigram has not been observed in the 
training-set, which is just the case for those 
OOV words. In contrast, the discriminative 
one could get help from the future context in 
this case. The example given in the next para-
graph clearly shows the above situation. 
At the sentence ??(that) ?(place) ?(of) ?
??(street sleeper) ?(only) ?(have) ?(some) 
?(person) (There are only some street sleepers 
in that place)? in the CITYU corpus, ??/B?
/M?/E(street sleeper)? is observed to be an 
OOV word, while ?? /B? /E(sleep on the 
street)? is an IV word, where the associated tag 
of each character is given after the slash sym-
bol. The character-based generative model 
wrongly splits ????? into two words ??/B
?/E? and ??/S (person)?, as the associated 
trigram for ????? is not seen in the training 
set. However, the discriminative model gives 
the correct result for ??/M? and the dominant 
features come from its future context ??? and 
???. Similarly, the future context ??? helps 
to give the correct tag to ??/E?. Table 1 gives 
the corresponding lambda feature weights (un-
der the Maximum Entropy (ME) (Ratnaparkhi, 
1998) framework) for ????? in the dis-
criminative model. It shows that in the column 
of ?C1? below ???, the lambda value associ-
ated with the correct tag ?M? is 2.9422, which 
is the highest value in that column and is far 
greater than that of the wrong tag ?E? (i.e., -
1.2696) assigned by the generative model. 
Which indicates that the future feature ?C1? is 
the most useful feature for tagging ???. 
The above example shows the character-
based generative model fails to handle some 
OOV words such as ????? because this ap-
proach cannot utilize future context when it is 
indeed required. However, the future context 
for the generative model scanning from left to 
right is just its past context when it scans from 
right to left. It is thus expected that this kind of 
1175
errors will be fixed if we let the model scans 
from both directions, and then combine their 
results. Unfortunately, it is observed that these 
two scanning modes share over 90% of their 
errors. For example, in CITYU corpus, the 
left-to-right scan generates 1,958 wrong words 
and the right-to-left scan results 1,947 ones, 
while 1,795 of them are the same. Similar be-
havior can also be observed on other corpora. 
To find out what are the problems, 10 errors 
that are similar to ????? are selected to ex-
amine. Among those errors, only one of them 
is fixed, and ????? still cannot be correctly 
segmented. Having analyzed the scores of the 
model scanning from both directions, we found 
that the original scores (from left-to-right scan) 
at the stages ??? and ??? indeed get better if 
the model scans from right-to-left. However, 
the score at the stage ??? deteriorates because 
the useful feature ??? (a past non-adjacent 
character for ??? when scans form right-to-
left) still cannot be utilized when the past con-
text ???? as a whole is unseen, when the re-
lated probabilities are estimated via modified 
Kneser-Ney smoothing (Chen and Goodman, 
1998) technique. 
Two scanning modes seem not complement-
ing each other, which is out of our original ex-
pectation. However, we found that the charac-
ter-based generative model and the discrimina-
tive one complement each other much more 
than the two scanning modes do. It is observed 
that these two approaches share less than 50% 
of their errors. For example, in CITYU corpus, 
the generative approach generates 1,958 wrong 
words and the discriminative one results 2,338 
ones, while only 835 of them are the same. 
The statistics of the remaining errors re-
sulted from the generative model and the dis-
criminative model is shown in Table 2. As 
shown in the table, it can be seen that the gen-
erative model and the discriminative model 
complement each other on handling IV words 
and OOV words (In the ?IV Errors? column, 
the number of ?G+D-? is much more than the 
?G-D+?, while the behavior is reversed in the 
?OOV Errors? column). 
4 Proposed Joint Model 
Since the performance of both IV words and 
OOV words are important for real applications, 
IV Errors OOV Errors 
G+D- G-D+ G-D- G+D- G-D+ G-D-
12,027 4,723 7,481 2,384 6,139 3,975
Table 2: Statistics for remaining errors of the char-
acter-based generative model and the discriminative 
one on the second SIGHAN Bakeoff (?G+D-? in 
the ?IV Errors? column means that the generative 
model segments the IV words correctly but the dis-
criminative one gives wrong results. The meanings 
of other abbreviations are similar with this one.). 
we need to combine the strength from both 
models. Among various combining methods, 
log-linear interpolation combination is a sim-
ple but effective one (Bishop, 2006). Therefore, 
the following character-based joint model is 
proposed, and a parameter ?  is used to weight 
the generative model in a cross-validation set. 
 
1
2
2
2
( ) log( ([ , ] [ , ] ))
(1 ) log( ( ))
k
k k
k
k k
Score t P c t c t
P t c
?
?
?
?
+
?
= ?
+ ? ?
k
           (4) 
Where tk indicates the corresponding position 
of character ck, and (0.0 1.0)? ?? ?  is the 
weight for the generative model. Score(tk) will 
be used during searching the best sequence. It 
can be seen that these two models are inte-
grated naturally as both are character-based. 
Generally speaking, if the ?G(or D)+? has a 
strong preference on the desired candidate, but 
the ?D(or G)-? has a weak preference on its 
top-1 incorrect candidate, then this combining 
method would correct most ?G+D- (also  G-
D+)? errors. On the other hand, the advantage 
of combining two models would vanish if the 
?G(or D)+? has a weak preference while the 
?D(or G)-? has a strong preference over their 
top-1 candidates. In our observation, these two 
models meet this requirement quite well. 
5 Weigh Various Features Differently 
For a given observation, intuitively each 
feature should be trained only once under the 
ME framework and its associated weight will 
be automatically learned from the training cor-
pus. However, when we repeat the work of 
(Jiang et al, 2008), which reports to achieve 
the state-of-art performance in the data-sets 
that we adopt, it has been found that some fea-
tures (e.g., C0) are unnoticeably trained several 
times in their model (which are implicitly gen-
erated from different feature templates used in 
the paper). For example, the feature C0 actually 
1176
Corpus Abbrev. Encoding Training Size(Words/Type)
Test Size 
(Words/Type) OOV Rate
Academia Sinica (Taipei) AS Unicode/Big5 5.45M/141K 122K/19K 0.046 
City University of Hong Kong CITYU Unicode/Big5 1.46M/69K 41K/9K 0.074 
Microsoft Research (Beijing) MSR Unicode/CP936 2.37M/88K 107K/13K 0.026 
PKU(ucvt.) Unicode/CP936 1.1M/55K 104K/13K 0.058 Peking University 
PKU(cvt.) Unicode/CP936 1.1M/55K 104K/13K 0.035 
Table 3: Corpus statistics for the second SIGHAN Bakeoff 
 
appears twice, which is generated from two 
different templates Cn (with n=0, generates C0) 
and [C0Cn] (used in (Jiang et al, 2008), with 
n=0, generates [C0C0]). The meanings of fea-
tures are illustrated in Section 2.1. Those re-
petitive features also include [C-1C0] and 
[C0C1], which implicitly appear thrice. And it 
is surprising to discover that its better perform-
ance is mainly due to this implicit feature repe-
tition but the authors do not point out this fact. 
As all the features adopted in (Jiang et al, 
2008) possess binary values, if a binary feature 
is repeated n times, then it should behave like a 
real-valued feature with its value to be ?n?, at 
least in principle. Inspired by the above dis-
covery, accordingly, we convert all the binary-
value features into their corresponding real-
valued features. After having transformed bi-
nary features into their corresponding real-
valued ones, the original discriminative model 
is re-trained under the ME framework. 
This new implementation, which would be 
named as the character-based discriminative-
plus model, just weights various features dif-
ferently before conducting ME training. Af-
terwards, it is further combined with the gen-
erative trigram model, and is called the charac-
ter-based joint-plus model. 
6 Experiments 
The corpora provided by the second SIGHAN 
Bakeoff (Emerson, 2005) were used in our ex-
periments. The statistics of those corpora are 
shown in Table 3. 
Note that the PKU corpus is a little different 
from others. In the training set, Arabic num-
bers and English characters are in full-width 
form occupying two bytes. However, in the 
testing set, these characters are in half-width 
form occupying only one byte. Most research-
ers in the SIGHAN Bakeoff competition per-
formed a conversion before segmentation 
(Xiong et al, 2009). In this work, we conduct 
the tests on both unconverted (ucvt.) case and 
converted (cvt.) case. After the conversion, the 
OOV rate of converted corpus is obviously 
lower than that of unconverted corpus. 
To fairly compare the proposed approach 
with previous works, we only conduct closed 
tests1. The metrics Precision (P), Recall (R), 
F-score (F) (F=2PR/(P+R)), Recall of OOV 
(ROOV) and Recall of IV (RIV) are used to 
evaluate the results. 
6.1 Character-Based Generative Model 
and Discriminative Model 
As shown in (Wang et al, 2009), the character-
based generative trigram model significantly 
exceeds its related bigram model and performs 
the same as its 4-gram model. Therefore,  SRI 
Language Modeling  Toolkit2 (Stolcke, 2002) 
is used to train the trigram model with modi-
fied Kneser-Ney smoothing (Chen and Good-
man, 1998). Afterwards, a beam search de-
coder is applied to find out the best sequence. 
For the character-based discriminative 
model, the ME Package3 given by Zhang Le is 
used to conduct the experiments. Training was 
done with Gaussian prior 1.0 and 300, 150 it-
erations for AS and other corpora respectively.  
Ta
                                                
ble 5 gives the segmentation results of both 
the character-based generative model and the 
discriminative model. From the results, it can 
be seen that the generative model achieves 
comparable results with the discriminative one 
and they outperform each other on different 
corpus. However, the generative model ex-
ceeds the discriminative one on RIV (0.973 vs. 
0.956) but loses on ROOV (0.511 vs. 0.680). It 
illustrates that they complement each other. 
 
1 According to the second Sighan Bakeoff regulation, the 
closed test could only use the training data directly pro-
vided. Any other data or information is forbidden, includ-
ing the knowledge of characters set, punctuation set, etc. 
2 http://www.speech.sri.com/projects/srilm/ 
3 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 
1177
Joint model performance on Development sets
0.9300
0.9400
0.9500
0.9600
0.9700
0.9800
0.9900
0.0
0
0.1
0
0.2
0
0.3
0
0.4
0
0.5
0
0.6
0
0.7
0
0.8
0
0.9
0
1.0
0
alpha
F-
sc
or
e
AS
CITYU
MSR
PKU
 
Figure 1: Development sets performance of Charac-
ter-based joint model. 
Corpus Set Words  OOV Num OOV Rate
Development 17,243 445 0.026  AS 
Testing 122,610 5,308/5,311 0.043/0.043
Development 17,324 355 0.020 MSR 
Testing 106,873 2,829/2,833 0.026/0.027
Development 12,075 537 0.044 CITYU 
Testing 40,936 3,028/3,034 0.074/0.074
Development 13,576 532 0.039 
Testing (ucvt.) 104,372 6,006/6,054 0.058/0.058PKU 
Testing (cvt.) 104,372 3,611/3,661 0.035/0.035
Table 4: Corpus statistics for Development sets and 
Testing sets. A ?/? separates the OOV number (or 
OOV rate) with respect to the original training sets 
and the new training sets. 
6.2 Character-Based Joint Model 
For the character-based joint model, a devel-
opment set is required to obtain the weight ?  
for its associated generative model. A small 
portion of each original training corpus is thus 
extracted as the development set and the re-
maining data is regarded as the new training-
set, which is used to train two new parameter-
sets for both generative and discriminative 
models associated.  
The last 2,000, 600, 400, and 300 sentences 
for AS, MSR, CITYU, and PKU are extracted 
from the original training corpora as their cor-
responding development sets. The statistics for 
new data sets are shown in Table 4. It can be 
seen that the variation of the OOV rate could 
be hardly noticed. The F-scores of the joint 
model, versus different ? , evaluated on four 
development sets are shown in Figure 1. It can 
be seen that the curves are not sharp but flat 
near the top, which indicates that the character-
based joint model is not sensitive to the ?  
value selected. From those curves, the best 
suitable ?  for AS, CITYU, MSR and PKU are 
found to be 0.30, 0.60, 0.60 and 0.60, respec-
Corpus Model R P F ROOV RIV
tively. Those alpha values will then be adopted 
to conduct the experiments on the testing sets. 
G 0.958 0.938 0.948 0.518 0.978
D 0 0.946 0  0.967.955 .951 0.707 
D-Plus 0.960 0.948 0.954 0.680 0.973
J 0.962 0.950 0.956 0.679 0.975
AS 
J-Plus 0.963 0.949 0.956 0.652 0.977
G 0.951 0.937 0.944 0.609 0.978
D 0.941 0.944 0.942 0.708 0.959
D-Plus 0.951 0.952 0.952 0.720 0.970
J 0.957 0.951 0.954 0.691 0.979
CITYU
J-Plus 0.959 0.952 0.956 0.700 0.980
G 0.974 0.967 0.970 0.561 0.985
D 0.957 0.962 0.960 0.719 0.964
D-Plus 0.965 0.967 0.966 0.675 0.973
J 0.974 0.971 0.972 0.659 0.983
MSR 
J-Plus 0.975 0.970 0.972 0.632 0.984
G 0.929 0.933 0.931 0.435 0.959
D 0.922 0.941 0.932 0.620 0.941
D-Plus 0.934 0.949 0.941 0.649 0.951
J 0.935 0.946 0.941 0.561 0.958
PKU 
(ucvt.) 
J-Plus 0.937 0.947 0.942 0.556 0.960
G 0.952 0.951 0.952 0.503 0.968
D 0.940 0.951 0.946 0.685 0.949
D-Plus 0.949 0.958 0.953 0.674 0.958
J 0.954 0.958 0.956 0.616 0.966
PKU 
(cvt.) 
J-Plus 0.955 0.958 0.957 0.610 0.967
G 0.953 0.946 0.950 0.511 0.973
D 0.944 0.950 0.947 0.680 0.956
D-Plus 0.952 0.955 0.953 0.676 0.965
J 0.957 0.955 0.956 0.633 0.971
Overall
J-Plus 0.958 0.955 0.957 0.621 0.973
Table 5: ent e
based m n t G  
ificantly outperforms both the character-
ba
 Segm
odels o
ation r sults of various character-
he second SI HAN Bakeoff, the
generative trigram model (G), the discriminative 
model (D), the discriminative-plus model (D-Plus), 
the joint model (J) and the joint-plus model (J-Plus). 
 
As shown in Table 5, the joint model sig-
n
sed generative model and the discriminative 
one in F-score on all the testing corpora. Com-
pared with the generative approach, the joint 
model increases the overall ROOV from 0.510 to 
0.633, with the cost of slightly degrading the 
overall RIV from 0.973 to 0.971. This shows 
that the joint model holds the advantage of the 
generative model on IV words. Compared with 
the discriminative model, the proposed joint 
model improves the overall RIV from 0.956 to 
0.971, with the cost of degrading the overall 
ROOV from 0.680 to 0.633. It clearly shows that 
the joint model achieves a good balance be-
tween IV words and OOV words and achieves 
the best F-scores obtained so far (21% relative 
error reduction over the discriminative model 
and 14% over the generative model). 
1178
6.3 Weigh Various Features Differently 
Inspired by (Jiang et al, 2008), we set the real-
d 
 
Although Table 5 has shown that the proposed 
all the 
value of C0 to be 2.0, the value of C-1C0 an
C0C1 to be 3.0, and the values of all other fea-
tures to be 1.0 for the character-based dis-
criminative-plus model. Although it seems rea-
sonable to weight those closely relevant fea-
tures more (C0 should be the most relevant fea-
ture for assigning tag t0), both implementations 
seem to be equal if their corresponding 
lambda-values are also updated accordingly. 
However, Table 5 shows that this new dis-
criminative-plus implementation (D-Plus) sig-
nificantly outperforms the original one (overall 
F-score is raised from 0.947 to 0.953) when 
both of them adopt real-valued features. It is 
not clear how this change makes the difference. 
Similar improvements can be observed with 
two other ME packages. One anonymous re-
viewer pointed out that the duplicated features 
should not make difference if there is no regu-
larization. However, we found that the dupli-
cated features would improve the performance 
whether we give Gaussian penalty or not. 
Afterwards, this new implementation and 
the generative trigram model are further com-
bined (named as the joint-plus model). Table 5 
shows that this joint-plus model also achieves 
better results compared with the discrimina-
tive-plus model, which illustrates that our joint 
approach is an effective and robust method for 
CWS. However, compared with the original 
joint model, the new joint-plus approach does 
not show much improvement, regardless of the 
significant improvement made by the discrimi-
native-plus model, as the additional benefit 
generated by the discriminative-plus model has 
already covered by the generative approach 
(Among the 6,965 error words corrected by the 
discriminative-plus model, 6,292 (90%) of 
them are covered by the generative model). 
7 Statistical Significance Tests 
joint (joint-plus) model outperforms 
baselines mentioned above, we want to know 
if the difference is statistically significant 
enough to make such a claim. Since there is 
only one testing set for each training corpus, 
the bootstrapping technique (Zhang et al, 2004) 
is adopted to conduct the tests: Giving an  
Models  
A B 
AS CITYU MSR PKU (ucvt.) 
PKU
(cvt.)
G D <  ~ >  ~ >  
D-Plus G >  >  <  >  >  
D-Plus D >  >  >  >  >  
J G >  >  >  >  >  
J D >  >  >  >  >  
J-Plus G >  >  >  >  >  
J-Plus D-Plus >  >  >  ~ >  
J-Plus J ~ >  ~ >  >  
Table 6 atistic sign anc est F- e 
 v er-b d m ls. 
f T0) will 
be generated by repeatedly re-sampling data 
eas-
 the dis-
he confi-
 
the pro-
po
e-
ng 
d. 
tegory 
includes (Asahara et al, 2005) (denoted as 
: St al ific e t of scor
among arious charact ase ode
testing-set T0, additional M-1 new testing-sets 
T0,?,TM-1 (each with the same size o
from T0. Then, we will have a total of M 
testing-sets (M=2000 in our experiments). 
7.1 Comparisons with Baselines 
We then follow (Zhang et al, 2004) to m
ure the 95% confidence interval for
crepancy between two models. If t
dence interval does not include the origin point,
we then claim that system A is significantly 
different from system B. Table 6 gives the re-
sults of significant tests among various models 
mentioned above. In this table, ?>? means that 
system A is significantly better than B, where 
as ?<? denotes that system A is significantly 
worse than B, and ?~? indicates that these two 
systems are not significantly different. 
As shown in Table 6, the proposed joint 
model is significantly better than the two base-
line models on all corpora. Similarly, 
sed joint-plus model also significantly out-
performs the generative model and the dis-
criminative-plus model on all corpora except 
on the PKU(ucvt.). The comparison shows that 
the proposed joint (also joint-plus) model in-
deed exceeds each of its component models. 
7.2 Comparisons with Previous Works 
The above comparison mainly shows the sup
riority of the proposed joint model amo
those approaches that have been implemente
However, it would be interesting to know if the 
joint (and joint-plus) model also outperforms 
those previous state-of-the-art systems.  
The systems that performed best for at least 
one corpus in the second SIGHAN Bakeoff are 
first selected for comparison. This ca
1179
A-sets. In-
st
th
                                                
sahara05) and (Tseng et al, 2005) 4  
(Tseng05). (Asahara et al, 2005) achieves the 
best result in the AS corpus, and (Tseng et al, 
2005) performs best in the remaining three 
corpora. Besides, those systems that are re-
ported to exceed the above two systems are 
also selected. This category includes (Zhang et 
al., 2006) (Zhang06), (Zhang and Clark, 2007) 
(Z&C07) and (Jiang et al, 2008) (Jiang08). 
They are briefly summarized as follows. 
(Zhang et al, 2006) is based on sub-word tag-
ging and uses a confidence measure method to 
combine the sub-word CRF (Lafferty et al, 
2001) and rule-based models. (Zhang and 
Clark, 2007) uses perceptron (Collins, 2002) to 
generate word candidates with both word and 
character features. Last, (Jiang et al, 2008)5  
adds repeated features implicitly based on (Ng 
and Low, 2004). All of the above models, ex-
cept (Zhang and Clark, 2007), adopt the char-
acter-based discriminative approach. 
All the results of the systems mentioned 
above are shown in Table 7. Since the systems 
are not re-implemented, we cannot generate 
paired samples from those M testing
ead, we calculate the 95% confidence inter-
val of the joint (also joint-plus) model. After-
wards, those systems can be compared with 
our proposed models. If the F-score of system 
B does not fall within the 95% confidence in-
terval of system A (joint or joint-plus), then 
they are statistically significantly different. 
Table 8 gives the results of significant tests 
for those systems mentioned in this section. It 
shows that both our joint-plus model and joint 
model exceed (or are comparable to) almost all 
e state-of-the-art systems across all corpora, 
except (Zhang and Clark, 2007) at PKU(ucvt.). 
In that special case, (Zhang and Clark, 2007) 
 
4 We are not sure whether (Asahara et al, 2005) and 
(Tseng et al, 2005) performed a conversion before seg-
mentation in PKU corpus. In this paper, we followed 
previous works, which cited and compared with them. 
5 The data for (Jiang et al, 2008) given at Table 7 are 
different from what were reported at their paper. In the 
communication with the authors, it is found that the script 
for evaluating performance, provided by the SIGHAN 
Bakeoff, does not work correctly in their platform. After 
the problem is fixed, the re-evaluated real performances 
reported here deteriorate from their original version. 
Please see the announcement in Jiang?s homepage 
(http://mtgroup.ict.ac.cn/~jiangwenbin/papers/error_corre
ction.pdf). 
Corpus
Participants AS CITYU MSR 
PKU 
(ucvt.) 
PKU
(cvt.)
Asahara05 0.952 0.941 0.958 N/A 0.941
Tseng05 0.947 0.943 0.964 N/A 0.950
Zhang06 0.951 0.951 0.971 N/A 0.951
Z&C07 0.946 0.951 0.972 0.945 N/A
Jiang08 0.953 0.948 0.966 0.937 N/A
Our Joint 0.956 0.954 0.972 0.941 0.956
Our Joint-Plus 0.956 0.956 0.972 0.942 0.957
Table 7: Compari r  p u
the-art sy
sons of F-sco e with revio s 
state-of- stems. 
Systems 
A B 
AS CITYU MSR (ucvt.)
PKU 
 (cvt.)
PKU
Asahara05 > > > N/A > 
Tseng05 > > > N/A > 
Zhang06 > ~ ~ N/A > 
Z&C07 > > ~ < N/A
J 
Jiang08 > > > > N/A
Asahara05 > > > N/A > 
Tseng05 > > > N/A > 
Zhang06 > > ~ N/A > 
Z&C07 > > ~ < N/A
J-Plus
Jiang08 ~ > > > N/A
Table al s ific e te of r 
f-the  syst s. 
outpe he jo -plu model by .3%  
 and 0.5%, re-
ne, 
e two models complement 
dling IV words and OOV 
e-
nomenon.  
8: Statistic ign anc st  F-score fo
previous state-o -art em
rforms t int s  0  on
F- score (0.4% for the joint model). However, 
our joint-plus model exceeds it more over AS 
and CITYU corpora by 1.0%
spectively (1.0% and 0.3% for the joint model). 
Thus, it is fair to say that both our joint model 
and joint-plus model are superior to the state-
of-the-art systems reported in the literature. 
8 Conclusion 
From the error analysis of the character-based 
generative model and the discriminative o
we found that thes
each other on han
words. To take advantage of these two ap-
proaches, a joint model is thus proposed to 
combine them. Experiments on the Second 
SIGHAN Bakeoff show that the joint model 
achieves 21% error reduction over the dis-
criminative model (14% over the generative 
model). Moreover, closed tests on the second 
SIGHAN Bakeoff corpora show that this joint 
model significantly outperforms all the state-
of-the-art systems reported in the literature. 
Last, it is found that weighting various fea-
tures differently would give better result. How-
ever, further study is required to find out the 
true reason for this strange but interesting ph
1180
A Generic-Beam-Search code and 
o Ms. Nanyan Kuo for 
eric-Beam-Search code.  
m
optimum 
entation. In Proceedings of 
GHAN Workshop on Chinese Lan-
St
Th
Jia
W
Jo
Hw
Fu
ction using conditional random fields. 
Ad
MNLP, pages 
Hu
ld Word Segmenter 
Ku d Keh-Yih Su, 2009. 
Yi
scriminative 
Ni
ssing, 8 (1). pages 
Hu
Second 
Ru
2006. Subword-based Tagging for Con-
Yi  
 scores: How much im-
Yu
f ACL, pages 840-847, 
cknowledgement 
The authors extend sincere thanks to Wenbing 
Jiang for his helps with our experiments. Also, 
we thank Behavior Design Corporation for 
using their
show special thanks t
her helps with the Gen
The research work has been partially funded 
by the Natural Science Foundation of China 
under Grant No. 60975053, 90820303 and 
60736014, the National Key Technology R&D 
Program under Grant No. 2006BAH03B02, 
and also the Hi-Tech Research and Develop-
ent Program (?863? Program) of China under 
Grant No. 2006AA010108-4 as well. 
References 
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, 
Chooi-Ling Goh, Yotaro Watanabe, Yuji Ma-
tsumoto and Takashi Tsuzuki, 2005. Combina-
tion of machine learning methods for 
Chinese word segm
the Fourth SI
guage Processing, pages 134?137, Jeju, Korea. 
Christopher M. Bishop, 2006. Pattern recognition 
and machine learning. New York: Springer  
anley F. Chen and Joshua Goodman, 1998. An 
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, 
Harvard University Center for Research in 
Computing Technology. 
Michael Collins, 2002. Discriminative training 
methods for hidden markov models: theory and 
experiments with perceptron algorithms. In Pro-
ceedings of EMNLP, pages 1-8, Philadelphia. 
omas Emerson, 2005. The second international 
Chinese word segmentation bakeoff. In Proceed-
ings of the Fourth SIGHAN Workshop on Chi-
nese Language Processing, pages 123-133. 
nfeng Gao, Mu Li and Chang-Ning Huang, 2003. 
Improved Source-Channel Models for Chinese 
Word Segmentation. In Proceedings of ACL, 
pages 272-279. 
enbin Jiang, Liang Huang, Qun Liu and Yajuan 
Lu, 2008. A Cascaded Linear Model for Joint 
Chinese Word Segmentation and Part-of-Speech 
Tagging. In Proceedings of ACL, pages 897-904. 
hn Lafferty, Andrew McCallum and Fernando 
Pereira, 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling 
Sequence Data. In Proceedings of ICML, pages 
282-289. 
ee Tou Ng and Jin Kiat Low, 2004. Chinese 
part-of-speech tagging: one-at-a-time or all-at-
once? word-based or character-based. In Pro-
ceedings of EMNLP, pages 277-284. 
chun Peng, Fangfang Feng and Andrew 
McCallum, 2004. Chinese segmentation and new 
word dete
In Proceedings of COLING, pages 562?568. 
wait Ratnaparkhi, 1998. Maximum entropy 
models for natural language ambiguity resolu-
tion. University of Pennsylvania. 
Andreas Stolcke, 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the 
International Conference on Spoken Language 
Processing, pages 311-318. 
Kristina Toutanova, 2006. Competitive generative 
models with structure learning for NLP classifi-
cation tasks. In Proceedings of E
576-584, Sydney, Australia. 
ihsin Tseng, Pichuan Chang, Galen Andrew, 
Daniel Jurafsky and Christopher Manning, 2005. 
A Conditional Random Fie
for Sighan Bakeoff 2005. In Proceedings of the 
Fourth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 168-171. 
n Wang, Chengqing Zong an
Which is more suitable for Chinese word seg-
mentation, the generative model or the discrimi-
native one? In Proceedings of PACLIC, pages 
827-834, Hong Kong, China. 
ng Xiong, Jie Zhu, Hao Huang and Haihua Xu, 
2009. Minimum tag error for di
training of conditional random fields. Informa-
tion Sciences, 179 (1-2). pages 169-179. 
anwen Xue, 2003. Chinese Word Segmentation 
as Character Tagging. Computational Linguistics 
and Chinese Language Proce
29-48. 
aping Zhang, Hongkui Yu, Deyi Xiong and Qun 
Liu, 2003. HHMM-based Chinese lexical ana-
lyzer ICTCLAS. In Proceedings of the 
SIGHAN Workshop on Chinese Language Proc-
essing, pages 184?187. 
iqiang Zhang, Genichiro Kikui and Eiichiro 
Sumita, 
fidence-dependent Chinese Word Segmentation. 
In Proceedings of the COLING/ACL, pages 961-
968, Sydney, Australia. 
ng Zhang, Stephan Vogel and Alex Waibel, 2004.
Interpreting BLEU/NIST
provement do we need to have a better system. 
In Proceedings of LREC, pages 2051?2054. 
e Zhang and Stephen Clark, 2007. Chinese Seg-
mentation with a Word-Based Perceptron Algo-
rithm. In Proceedings o
Prague, Czech Republic. 
1181
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 398?408, Dublin, Ireland, August 23-29 2014.
 Dynamically Integrating Cross-Domain Translation Memory into 
Phrase-Based Machine Translation during Decoding 
 
 
Kun Wang?        Chengqing Zong?        Keh-Yih Su? 
?National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
?Institute of Information Science, Academia Sinica, Taiwan 
?
{kunwang, cqzong}@nlpr.ia.ac.cn 
?
kysu@iis.sinica.edu.tw 
 
 
Abstract 
Our previous work focuses on combining translation memory (TM) and statistical machine translation 
(SMT) when the TM database and the SMT training set are the same. However, the TM database will 
deviate from the SMT training set in the real task when time goes by. In this work, we concentrate on 
the task when the TM database and the SMT training set are different and even from different domains. 
Firstly, we dynamically merge the matched TM phrase-pairs into the SMT phrase table to meet the real 
application. Secondly, we propose an improved integrated model to distinguish the original and the new-
ly-added phrase-pairs. Thirdly, a simple but effective TM adaptation method is adopted to favor the 
consistent translations in cross-domain test. Our experiments have shown that merging the TM phrase-
pairs achieves significant improvements. Furthermore, the proposed approaches are significantly better 
than the TM, the SMT and previous integration works for both in-domain and cross-domain tests. 
1 Introduction 
Since the translation memory (TM) system and the statistical machine translation (SMT) system com-
plement each other in those matched sub-segments and unmatched sub-segments (Wang et al., 2013), 
combining them can improve the output quality significantly, especially when high-similarity fuzzy 
matches are available. Therefore, combining TM and SMT is drawing more and more attention in re-
cent years (He et al., 2010a; 2010b; 2011; Koehn and Senellart, 2010; Zhechev and van Genabith, 
2010; Ma et al., 2011; Dara et al., 2013; Wang et al., 2013). 
Those previous works on combining TM and SMT can be classified into four categories: (1) select-
ing the better translation sentence from TM and SMT (He et al., 2010a; 2010b; Dara et al., 2013); (2) 
incorporating TM matched sub-segments into SMT in a pipelined manner (Koehn and Senellart, 2010; 
He et al., 2011; Ma et al., 2011); (3) only enhancing the SMT phrase table with new TM phrase-pairs 
(Bi?ici and Dymetman, 2008; Simard and Isabelle, 2009); and (4) incorporating the associated TM 
information with each source phrase to guide the SMT decoding (Wang et al., 2013). 
However, all previous works mentioned above only focus on the case in which the TM database and 
the SMT training set share the same data-set. Nonetheless, in real applications, the TM database will 
deviate from the SMT training set when time goes by, because the TM database will be dynamically 
enlarged when more translations are generated by the human translator. Therefore, this paper will con-
centrate on a more realistic case, in which the TM database and the SMT training set are different and 
even from different domains. 
When the TM database and the SMT training set share the same data-set, the integrated model 
(Wang et al., 2013) can avoid the drawbacks of the pipeline approaches and outperforms the other ap-
proaches significantly. However, this integrated model only refers to the TM information but not 
adopts the matched TM phrase-pairs as candidates during decoding. Therefore, many TM phrase-pairs 
cannot be covered by the SMT phrase table when the TM database and the SMT training set are dif-
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organizers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
398
ferent. It is thus impossible to generate those unseen TM target phrases. This problem would even get 
worse when the TM database and the SMT training set are from different domains. 
To make the integrated model meet the real application, we dynamically merge the matched TM 
phrase-pairs into the SMT phrase table. In addition, an improved integrated model is proposed to dis-
tinguish the original SMT phrase-pairs and the newly-added ones extracted from TM. Furthermore, a 
simple but effective TM adaptation method is adopted to favor the consistent translation in cross-
domain test. To our best knowledge, this is the first unified framework for integrating TM into SMT 
during decoding when the TM database and the SMT training set are different (even from different 
domains). 
On the TM database which consists of Chinese?English computer technical documents, our experi-
ments have shown that merging the matched TM phrase-pairs achieves significant improvement when 
the fuzzy match score is above 0.5. Besides, the proposed approaches are significantly better than ei-
ther the SMT or the TM systems for both the in-domain and the cross-domain tests when the fuzzy 
match score is above 0.4. Furthermore, the proposed approaches also outperform previous integration 
works significantly in all test conditions. 
2 Integrated Model 
Wang et al. (2013) incorporated the TM information into the phrase-based SMT, and re-defined the 
translation problem as: 
 ?          ( |                         )  
Where   denotes the given source sentence,   is a corresponding target translation, and  ? is the final 
result; [                       ]  is the associated information of the best TM sentence-pairs; 
     and      are the corresponding TM source and target sentences, respectively;      denotes its 
corresponding fuzzy match score (from 0 to 1);     is the monolingual alignment information between 
  and     ; and      denotes the bilingual word alignment information between      and     . 
With the TM information, this problem can be simplified to: 
  ?        { (  ?
 | ? ( )
 ( ))  ?        ? ( )  (  |    ) 
 
   }  (1) 
Where  ? ( ) and   ? denote the k-th associated source and target phrases, respectively;     ? ( ) and 
     ?( ) are the corresponding TM source and target phrases associated with the given source phrase 
 ? ( ) (total K phrases without insertion).   is the corresponding TM target phrase matching status for 
the current target candidate   ?, which reflects the quality of the given candidate;    is the linking sta-
tus vector of  ? ( ) (the aligned source phrase, within  ? ( )
 ( )  of   ?), which indicates the matching and 
linking status in the source side (and is closely related to the matching status of the target side).      
is uniformly divided into ten fuzzy match intervals and the index   specifies the corresponding interval. 
In Equation (1), the first factor is just the typical phrase-based SMT model, and the second factor 
 (  |    ) is the information derived from the TM sentence pair. Afterwards, the factor  (  |    ) 
was further derived with TM matching status as follows: 
  (  |    )  {
 (    |                          )
  (    |                     )
  (    |                )
} (2) 
Where the first factor reflects the TM content matching status, the second factor is the relationship 
between various TM target phrases, and the third factor is the reordering information implied by TM. 
Equation (2) is adopted to guide the SMT decoding, and is denoted as the integrated Model-III in 
(Wang et al., 2013) (also called Model-III in this paper thereafter). 
For space limitation, only those features which are also adopted in our additional introduced proba-
bility factor (to be specified later) will be briefly introduced here: 
Target Phrase Content Matching Status (TCM): It indicates the content matching status between   ? 
and      ?( ) , and reflects the quality of   ? . It is a member of {Same, High, Low, NA (Not-
Applicable)}. 
399
Source Phrase Content Matching Status (SCM): It indicates the content matching status between 
 ? ( )  and     ? ( ) , and affects the matching status of   ?  and      ?( )  greatly. It is a member of 
{Same, High, Low, NA}. 
Number of Linking Neighbors (NLN): Usually, the context of a source phrase would affect its target 
translation. The more similar the context is, the more likely that the translation is the same. NLN is 
adopted to measure the context similarity. 
3 Proposed Approaches 
3.1 Merging the TM Phrase-Pairs 
Since all TM phrase-pairs are only referred while re-scoring the SMT candidates in Model-III, they are 
not regarded as candidates during decoding. When the TM database and the SMT training set are the 
same, this restriction is reasonable because the SMT phrase table can cover all the continuous TM 
phrase pairs within the phrase length limit. However, this would not be true when the TM database 
and the SMT training set are different. Therefore, the SMT phrase table should be further enhanced 
with those matched new TM phrase pairs in this case.  
According to their relations with the SMT phrase table, TM phrase pairs can be classified into three 
different categories: (1) the whole TM phrase-pair can be found in the original SMT phrase table; (2) 
only TM source phrase exists in the original SMT phrase table, but its corresponding target phrase 
does not; (3) even TM source phrase cannot be found in the original SMT phrase table. Since the first 
category has been covered by the original SMT phrase table, only the phrase-pairs from the second 
and the third categories should be added into the SMT phrase table dynamically for each input sen-
tence. To distinguish those newly added phrase-pairs from the original SMT phrase-pairs, we use eight 
additional feature weights    for the translation probability (lexical and phrase transfer in both direc-
tions) and two more feature weights for the phrase penalty (details will be specified later in Section 4). 
The above approach is inspired by the work of (Bi?ici and Dymetman, 2008). However, there are 
three differences between our approach and theirs. Firstly, we add all those matched TM phrase-pairs 
(include all associated sub-phrase pairs), while Bi?ici and Dymetman (2008) only added the longest 
matched one; Secondly, we add all the possible TM target phrase-pairs for a given TM source phrase 
while they extracted only one TM target phrase regardless of the existence of multiple TM target can-
didates; Lastly, we use different feature weights to distinguish those newly added TM phrase-pairs 
from the original SMT phrase-pairs, while they treated them equally. 
3.2 Distinguishing the TM Phrase-Pairs 
As mentioned in Section 3.1, we need to merge those TM matched phrase pairs into the SMT phrase 
table when the TM database and the SMT training set are different. However, the original integrated 
Model-III does not distinguish the newly added TM phrase-pairs from those original SMT phrase-
pairs in  (  |    ). Therefore, we introduce two new features Source Phrase Origin (SPO) and 
Target Phrase Origin (TPO), which are a member of {Original, Newly-Added}, to the original Mod-
el-III in (Wang et al., 2013) to favor the newly added TM phrase-pairs, and re-derive  (  |    ) as 
follows (assume that TPO is only dependent on SPO, NLN and  ): 
 
 (  |    ) 
  ([               ] |[                       ]   ) 
 
{
 
 
 (    |                          )
  (    |                     )
  (    |                )
  (    |           ) }
 
 
 
(2) 
The additional factor  (    |           ) in the above equation is added to handle those newly 
added TM phrase-pairs. This would be the proposed Distinguishing Model. For the phrases from the 
original SMT phrase table, both the SPO and TPO features would be ?Original?; for the phrases from 
the second category mentioned in Section 3.1, the SPO would be ?Original? but the TPO would be 
?Newly-Added?; for the phrases from the third category, both the SPO and TPO features would be 
?Newly-Added?. 
400
3.3 TM Adaptation 
In real applications, the TM database is usually not big enough to train an SMT system when it is ap-
plied to a special technical domain other than the news domain. Besides, many professional translators 
do not want to expose the whole TM database to the SMT system providers (Cancedda, 2012). In this 
situation, we will be forced to first train an SMT model on an out domain (usually the news domain) 
which possesses a lot of training data, and then fix the obtained phrase-based SMT model. Afterwards, 
we incorporate it on line with an additional TM database which is from another in domain. 
To simulate the above scenario, we will thus train our integrated model on the out domain. However, 
we have a domain-mismatch problem for this cross-domain test. Generally, in the technical domain, 
which is suitable for TM application, the translations (especially for technical terms) are much more 
consistent than that in the news domain. That is, the same source phrase in various places tends to 
have exactly the same translation in technical domains. Therefore, when we use Distinguishing Model 
to perform forced decoding, the obtained results would possess different statistics among the in-
domain development set and the out-domain training set. For example, at interval [0.9, 1.0), when 
SCM is ?Same?, 94.6% of TCM are ?Same? in the development set (in), while this ratio is only 65.1%  
in the training set (out). Therefore, the factor  (    |                          ) from the 
test set will possess a different probability distribution in comparison with that from the training set. 
However, the development set is not big enough (only a few hundreds sentence-pairs at each interval) 
to re-train all TM factors of the proposed model. Therefore, we simply add the following h1 feature to 
reflect the tendency of having high translation consistency in the development set: 
  ( ?  ?  ) {
                              
                                                         
 
Where  ? and  ? denote the source phrase, the target candidate, respectively. 
Furthermore, various source synonyms might generate the same translation (Zhu et al., 2013). 
Therefore, even SCM?Same, we still favor the SMT phrase-pair candidate which exactly matches TM 
target phrase. For example, if source words are synonyms such as ???? (want) and ??? (want), ??
?? (if) and ??? (if), ???? (at once) and ???? (at once), the target translations would be the same. 
Therefore, the issue of having high translation consistency in the technical domain is also applied. We 
thus further add the following h2 feature to reflect the tendency of having high translation consistency 
in this case (?High? and ?Low? are grouped into ?Other? for the SCM): 
  ( ?  ?  ) {
                               
                                                          
 
Afterwards, the associated feature weights are tuned on the development set. 
4 Experiments 
4.1 Experimental Setup 
We use the same TM data-set adopted by Wang et al. (2013), which is a Chinese?English TM data-
base consisting of computer technical documents. It includes about 267k sentence pairs. All the exper-
iments are conducted around this TM data-set. To compare the performances under different condi-
tions, the same development set and the test set will be shared by both in-domain and cross-domain 
tests. Since the associated SMT training-set and TM database will vary under different experimental 
configurations, they will be specified later in each sub-section. 
In this work, the translation memory system (denoted as TM) and the phrase-based machine transla-
tion system (denoted as SMT) are adopted as our two baseline systems. Following (Wang et al., 2013), 
for TM, the word-based fuzzy match score is adopted as the similarity measure; also, for the phrase-
based SMT system, the same Moses toolkit (Koehn et al., 2007) and the same set of following features 
are adopted: the phrase translation model, the language model, the distance-based reordering model, 
the lexicalized reordering model and the word penalty. The system configurations are as follows: GI-
ZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, ?intersec-
tion? refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use SRI Language Model 
401
toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 
1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights 
and the weight for each probability factor are tuned on the development set with minimum-error-rate 
training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. 
To compare our proposed models with those state-of-the-art methods, we re-implement two XML-
Markup approaches (Koehn and Senellart, 2010; and the upper bound version of (Ma et al, 2011)) and 
the Model-III (Wang et al., 2013) as three baseline systems, and denote them as Koehn-10, Ma-11-U 
and Model-III, respectively. Similar to (Wang et al., 2013), we only re-implement the XML-Markup 
method used in (Ma et al, 2011), but not their discriminative learning method. 
Following (Wang et al., 2013), we also train the TCM, LTC and CPM factors in the SMT training 
set with cross-fold translation. Since the TPO factor (conditioning on NLN and Distinguishing Model) 
is based on Model-III, we first use Model-III to generate the desired results on the development set via 
forced decoding, and then generate the training samples of TPO factor for Distinguishing Model.  
In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni 
et al., 2002) and TER score (Snover et al., 2006). Statistical significance tests are conducted with re-
sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level. 
4.2 In-Domain Translation Results 
In the in-domain test, the original TM dataset is first randomly divided into two parts. The first part is 
then adopted as the new TM database, while the second part is adopted as the SMT training set. The 
detailed corpus statistics is shown in Table 1. Since the TM database is different from that adopted in 
(Wang et al., 2013), the statistics shown in Table 2 at each interval is also different from theirs.  
All matched TM phrase-pairs are extracted according to the word alignment generated from the 
phrase-based SMT system. Since there are not enough samples to estimate the translation probabilities 
for those newly added TM phrase-pairs, we use the following method to assign the translation proba-
bilities. For those TM phrase-pairs that only their source phrases exist in the original SMT phrase table 
(the second category mentioned in Section 3.1), as their source phrases have already existed in the 
SMT phrase table, there is at least one associated target phrase in the original SMT phrase table. For 
each new TM phrase-pair, we thus directly assign the maximum probability among its associated orig-
inal target phrases to it. For those TM phrase-pairs that even their source phrase cannot be found in the 
original SMT phrase table (the third category), as there is no corresponding phrase-pair in the original 
SMT phrase table, we will simply assign probability ?1.0? (this value is not important as its associated 
weight will be tuned later) as their four translation probabilities. To distinguish those newly added 
phrase-pairs from the original SMT phrase-pairs, we use eight additional feature weights for the trans-
lation probability and two more feature weights for the phrase penalty. 
To evaluate the effectiveness of adding TM phrase-pairs, we compare the cases of whether merging 
TM phrase-pairs or not for both SMT and Model-III. Table 3 and Table 4 give the translation results in 
BLEU and TER, respectively. ?SMT? and ?Model-III? denote that we do not merge the TM phrase-
pairs into the SMT phrase table during decoding. That is, they only use the original SMT phrase table. 
  #Sentences #Chn. Words #Chn. VOC. #Eng. Words #Eng. VOC. 
New TM Database 130,953 1,808,992 30,164 1,811,413 30,807 
SMT Training Set 130,953 1,814,524 29,792 1,815,615 30,516 
Table 1: Corpus Statistics for In-Domain Tests 
Intervals 
[0.9, 
1.0) 
[0.8, 
0.9) 
[0.7, 
0.8) 
[0.6, 
0.7) 
[0.5, 
0.6) 
[0.4, 
0.5) 
[0.3, 
0.4) 
(0.0, 
0.3) 
(0.0, 
1.0) 
#Sentences 147 255 244 355 488 514 419 154 2,576 
#Words 2,431 3,438 3,299 4,674 6,125 7,525 7,082 4,074 38,648 
W/S 16.5 13.5 13.5 13.2 12.6 14.6 16.9 26.5 15.0 
Table 2: Corpus Statistics for In-Domain Test-Set (W/S: the average #words per sentence) 
402
?SMT+? and ?Model-III+? mean that we merge the TM phrase-pairs into the SMT phrase table dynam-
ically. In these tables, ?+? indicates that those newly added TM phrase-pairs significantly improve the 
translation results (?SMT? vs. ?SMT+?, ?Model-III? vs. ?Model-III+?, and ?Model-III? vs. ?Distin-
guishing?). 
It can be seen that adding TM phrase-pairs significantly improve the translation results when the 
fuzzy match score is above 0.5 (comparing SMT with SMT+, and Model-III with Model-III+). For ex-
ample, at interval [0.9, 1.0), those added TM phrase-pairs significantly improve the SMT system from 
63.65 to 73.55, and Model-III from 80.69 to 86.40. However, if Model-III+ is compared with Model-III, 
the improvements from merging the TM phrase-pairs get less when the fuzzy match score decreases, 
because the matched TM parts are fewer at low fuzzy match intervals. 
Also, with the same original SMT phrase table, Model-III exceeds the SMT system at each interval.  
For example, at interval [0.9, 1.0), the TM information significantly improve the translation result 
from 63.65 to 80.69. It thus shows that the TM information is very useful. However, it is still worse 
than the TM in TER (13.32 vs. 10.42). On the other hand, although Model-III has greatly exceeded the 
SMT at each interval, Model-III+ still significantly outperforms Model-III at most intervals. Therefore, 
the benefit of utilizing TM information and the benefit of adding TM phrase-pairs are not covered by 
each other and can be jointly enjoyed. Take the interval [0.9, 1.0) as an example, the TM information 
first improve the translation results from 63.65 (SMT) to 80.69 (Model-III), and then the added TM 
phrase-pairs further boosts it to 86.40 (Model-III+). 
Besides, Table 3 and Table 4 also present the translation results of our other two baselines (Koehn-
10 and Ma-11-U), and the proposed Distinguishing Model. Scores marked with  ?*?  indicate  that  
they are significantly better (p < 0.05) than both the TM and the SMT+ baselines, and those marked 
with ?#? are significantly better (p < 0.05) than Koehn-10. Scores marked with ?$? are significantly 
better than Model-III+. The bold entries are the best result at each interval. 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Koehn-10 Ma-11-U 
[0.9, 1.0) 79.89 63.65  73.55 + 80.69  86.40 +*# 86.69 +*# 82.21 67.58 
[0.8, 0.9) 72.65 60.75  74.04 + 78.95 * 83.35 +*# 83.44 +*# 79.50 * 67.03 
[0.7, 0.8) 59.59 60.57  65.52 + 68.55 * 71.37 +*# 72.06 +*# 67.52 62.60 
[0.6, 0.7) 41.57 53.38  56.14 + 55.61 # 57.75 +*# 58.73 +*#$ 51.83 56.74 
[0.5, 0.6) 25.17 45.60  46.95 + 47.40 # 48.39 +*# 48.27 *# 39.08 47.94 
[0.4, 0.5) 14.62 41.81  42.03  42.60 # 42.30 # 43.04 *#$ 31.60 42.93 
[0.3, 0.4) 7.50 35.95  35.49  36.10 # 35.31 # 35.34 # 25.25 36.58 
(0.0, 0.3) 4.94 32.64  33.22  33.45 # 33.23 # 33.23 # 23.70 33.10 
(0.0, 1.0) 31.11 46.68  49.41 + 51.00 *# 52.26 +*# 52.56 +*#$ 44.28 48.91 
Table 3: In-Domain Translation Results (BLEU). Scores marked with ?+? indicates that those newly 
added TM phrase-pairs significantly (p < 0.05) improve the translation results (?SMT? vs. ?SMT+?, 
?Model-III? vs. ?Model-III+?, and ?Model-III? vs. ?Distinguishing?). Scores marked with ?*? are sig-
nificantly better (p < 0.05) than both TM and SMT+ systems, and those marked with ?#? are signifi-
cantly better (p < 0.05) than Koehn-10. Scores marked with ?$? are significantly better  (p < 0.05) than 
Model-III+ (?Model-III+? vs. ?Distinguishing?) 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Koehn-10 Ma-11-U 
[0.9, 1.0) 10.42  27.14  17.64 + 13.32  8.76 +*# 8.22 +*# 12.95 23.94 
[0.8, 0.9) 16.07  28.73  17.66 + 14.69 * 10.46 +*# 10.49 +*# 14.72 * 23.83 
[0.7, 0.8) 28.68  29.47  24.99 + 22.01 * 20.15 +*# 19.33 +*# 23.96 27.43 
[0.6, 0.7) 48.59  33.76  31.53 + 31.57 # 29.77 +*# 28.95 +*#$ 36.89 30.98 
[0.5, 0.6) 63.13  40.57  39.00 + 38.79 # 38.00 *# 38.51 # 47.08 38.44 
[0.4, 0.5) 74.02  44.09  43.66  42.84 *# 43.43 # 42.88 *#$ 55.35 42.31 
[0.3, 0.4) 81.09  50.00  50.63  50.04 # 50.70 # 50.90 # 63.28 48.83 
(0.0, 0.3) 84.34  55.58  56.66  54.68 # 55.96 *# 55.96 *# 68.00 54.51 
(0.0, 1.0) 58.58  40.88  38.55 + 37.26 *# 36.47 +*# 36.28 +*# 45.63 38.73 
Table 4: In-Domain Translation Results (TER). The marks are the same as that in Table 3. 
403
In comparison with the TM and the SMT+ systems, Model-III+ is significantly better than both of 
them in either BLEU or TER scores when the fuzzy match score is above 0.5; also, Distinguishing 
Model outperforms both the TM and the SMT+ systems in either BLEU or TER scores when the fuzzy 
match score is above 0.4. Furthermore, the improvements from both Model-III+ and Distinguishing 
Model get less when the fuzzy match score decreases, as the TM information is less reliable at low 
fuzzy match intervals. 
Across all intervals (the last row in the table), Distinguishing Model not only achieves the best 
BLEU score (52.56), but also gets the best TER score (36.28). At those intervals when the fuzzy 
match score is above 0.4, Model-III+ and Distinguishing Model are the best two in either BLEU or 
TER scores. Besides, Distinguishing Model slightly exceeds Model-III+ at most intervals. However, 
both Model-III+ and Distinguishing Model achieve significant improvements over the TM and the 
SMT+. 
Compared with previous works, it can be seen that both Model-III+ and Distinguishing Model sig-
nificantly outperform Koehn-10 in either BLEU or TER scores at all intervals, and are significantly 
better than Model-III when the fuzzy match score is above 0.6. Furthermore, the proposed approaches 
(both Model-III+ and Distinguishing Model) achieve a much better TER score than the TM system 
does at the interval [0.9, 1.0); while Model-III and Koehn-10 are worse than the TM system at this 
interval. Also, both Model-III+ and Distinguishing Model exceed Ma-11-U at most intervals. There-
fore, it can be concluded that the proposed models outperform previous approaches significantly in 
this scenario. 
To further verify the proposed approaches in this case, we swap the TM database and the SMT 
training set and re-run the experiments. Similar and significant improvements are still observed: both 
Model-III+ and the Distinguishing Model achieve significant improvements over the TM and the 
SMT+. All those results have shown that the proposed approaches are robust. 
In real environments, the SMT training set and the TM database could be the same before transla-
tion projects starts. However, the TM database will gradually deviate from the SMT training set while 
the translation task progresses.  Nonetheless, our experiments have shown that the proposed Distin-
guishing Model is effective even when the TM database and the SMT training set are totally different 
(which would be the extreme case for real applications). Therefore, it can be concluded that this pro-
posed approach is robust. 
4.3 Cross-Domain Translation Results 
To evaluate the cross domain performance, we adopt the news corpora about computer and science 
from CWMT09 (Liu and Zhao, 2009) as the SMT training set, and adopt the whole TM dataset as the 
TM database. The SMT training set includes about 404k bilingual sentence-pairs (which includes 
about 9M Chinese words and 8.7M English words). Corpus statistics is shown in Table 5. Since the 
TM database and the test set (also the development set) are the same as that in (Wang et al., 2013), the 
statistics at each interval is the same as theirs but different from Table 2. 
The training procedure is the same as that mentioned in the last sub-section. Table 6 and Table 7 
present the translation results of TM, SMT, SMT+, two baselines (Koehn-10 and Model-III), and three 
proposed approaches (Model-III+, Distinguishing and Adaptation). The Adaptation approach means 
that we add two consistent related features based on Distinguishing Model (Section 3.3). All the for-
mats are the same as that adopted in Table 3 and Table 4. Besides, scores marked by ?&? are signifi-
cantly better than Distinguishing Model. 
Comparing the TM with the SMT, the performance of in-domain TM significantly exceeds that of 
out-domain SMT. Since the fuzzy match intervals are divided according to the TM database, the trans-
lation result of the SMT system at interval [0.8, 0.9) even slightly outperforms that at interval [0.9, 
1.0). Besides, adding TM phrase-pairs significantly improves the translation results when the fuzzy 
match score is above 0.5 (SMT vs. SMT+, and Model-III vs. Model-III+). Furthermore, the benefit of 
utilizing TM information and the benefit of adding TM phrase-pairs are not covered by each other, and 
can be jointly enjoyed. Furthermore, compared with TM, SMT, SMT+ and Model-III, both Model-III+ 
and Distinguishing Model achieve better translation results when the fuzzy match score is above 0.4. 
All observed trends are similar to that in the last sub-section. 
 
404
   #Sentences #Chn. Words #Chn. VOC. #Eng. Words #Eng. VOC. 
TM Database 261,906 3,623,516 43,112 3,627,028 44,221 
SMT Training Set 404,172 9,007,614 102,073 8,737,801 107,883 
Table 5: Corpus Statistics for Cross-Domain Tests 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Adaptation Koehn-10 
[0.9, 1.0) 81.31 30.87 64.74 + 64.79 82.28 + 83.19 +*$ 84.89 *#$& 81.52 
[0.8, 0.9) 73.25 31.94 60.13 + 61.91 74.21 + 74.72 +* 79.78 *#$& 76.47 * 
[0.7, 0.8) 63.62 30.63 51.64 + 51.44 62.94 + 63.32 + 67.74 *$& 67.12 *$& 
[0.6, 0.7) 43.64 28.95 39.94 + 38.28 46.28 +* 46.46 +* 49.49 *$& 48.47 * 
[0.5, 0.6) 27.37 27.61 32.49 + 28.85 34.50 +* 34.87 +* 37.12 *#$& 35.25 * 
[0.4, 0.5) 15.43 27.16 27.35 27.30 # 27.47 # 27.82 # 28.80 *#$& 25.10 
[0.3, 0.4) 8.24 23.85 22.66 23.81 # 22.41 # 22.41 # 22.95 # 20.72 
(0.0, 0.3) 4.13 24.64 24.25 24.24 # 23.65 # 24.12 # 24.31 # 18.79 
(0.0, 1.0) 40.17 28.30 40.59 + 40.47 47.37 +* 47.70 +*#$ 49.79 *#$& 47.09 * 
Table 6: Cross-Domain Translation Results (BLEU). The marks are the same as that in Table 3. Be-
sides, scores marked by ?$? are significantly better  (p < 0.05) than Model-III+, and those marked by 
?&? are significantly better than ?Distinguishing? (?Adaptation? vs. ?Distinguishing?). 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Adaptation Koehn-10 
[0.9, 1.0) 9.79 54.54 27.07 + 27.09 11.81 + 11.01 + 9.58 #$& 13.51 
[0.8, 0.9) 16.21 52.86 29.33 + 28.04 17.13 + 17.47 + 13.80 *#$& 17.29 
[0.7, 0.8) 27.79 52.42 36.48 + 35.56 27.07 + 26.40 +$ 23.04 *$& 24.31 *$& 
[0.6, 0.7) 46.40 54.74 47.39 + 48.06 41.13 +* 40.36 +*$ 37.45 *#$& 40.16 * 
[0.5, 0.6) 62.59 57.18 53.08 + 56.78 51.77 +* 51.60 +* 48.08 *#$& 51.57 
[0.4, 0.5) 73.93 57.19 56.57 57.19 # 56.82 # 56.53 # 54.42 *#$& 61.32 
[0.3, 0.4) 79.86 60.62 61.16 61.35 # 61.31 # 61.31 # 60.33 #$& 68.82 
(0.0, 0.3) 85.31 63.62 62.81 62.22 # 63.04 # 62.07 # 61.87 # 74.85 
(0.0, 1.0) 50.51 56.42 46.89 + 47.38 # 41.63 +*# 41.27 +*#$ 38.87 *#$& 43.95 * 
Table 7: Cross-Domain Translation Results (TER). The marks are the same as that in Table 6. 
However, both Model-III+ and Distinguishing Model are worse than Koehn-10 at some high fuzzy 
match intervals. The reason is that the TM factors are trained on the news domain but the test set is 
from computer technical domain. Therefore, it is not strange that the Adaptation approach achieves the 
best translation results at all intervals in either BLEU or TER when the fuzzy match score is above 0.4. 
At most intervals, the Adaptation approach significantly outperforms Koehn-10 in either BLEU or 
TER, especially for the high fuzzy match intervals such as [0.9, 1.0) and [0.8, 0.9). Furthermore, the 
Adaptation approach achieves better TER than the TM system and Koehn-10 at intervals [0.9, 1.0) and 
[0.8, 0.9). All obtained results have shown that the Adaptation approach is effective and robust for 
cross-domain test. Moreover, it can be seen that the h1 feature (mentioned in Section 3.3) is more ef-
fective than the h2 feature. 
5 Related Work 
According to the way of combination, those previous works can be classified into four categories (as 
specified in Section 1). The first category uses a classifier (or a re-ranker) to judge whether TM or 
SMT gives a better translation sentence, and then delivers the better one to the post-editor (He et al., 
2010a; He et al., 2010b; Dara et al., 2013). Since the outputs of SMT and TM are not merged but only 
re-ranked, the possible improvement resulted from those approaches is quite limited. 
The second category incorporates TM matched parts into the SMT input sentence in a pipelined 
manner (Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; He et al., 2011; Ma et al., 
2011). These approaches usually translate the sentence in two stages: (1) first determine whether the 
405
extracted TM sentence pair should be adopted or not, and then merge the relevant translations of 
matched parts into the input sentence; (2) then force the SMT system to only translate those un-
matched parts at decoding. There are three drawbacks for this kind of pipeline approaches (Wang et al., 
2013). Firstly, whether those matched parts should be adopted or not is determined at the sentence lev-
el. Secondly, they select only one TM target phrase before decoding. Thirdly, they do not utilize the 
SMT probabilistic information for the matched parts. 
The third category mainly adds the longest matched TM phrase pairs into the SMT phrase table 
(Bi?ici and Dymetman, 2008; Simard and Isabelle, 2009), and associates them with a fixed large prob-
ability value to favor the TM target phrase. However, they only add one aligned target phrase for each 
matched source phrase and did not distinguish the original and the newly-added phrase-pairs. 
The last category incorporates the associated TM information of each source phrase into the SMT 
during decoding (Wang et al., 2013). This category can avoid the drawbacks of the pipeline approach-
es, and thus achieves superior results when the TM database and the SMT training set are the same. 
However, they only refer to the TM information and do not regard the TM phrase-pairs as candidates 
during decoding. Therefore, the superiority of this approach disappears when the TM database and the 
SMT training set are different, because many TM phrase-pairs cannot be found in the original SMT 
phrase table in this case. 
Our approach combines the strength of both the third and the last categories. During decoding, the 
associated TM information is referred to re-score the SMT candidates. At the same time, all matched 
TM phrase-pairs are dynamically merged into the phrase table. Moreover, this is the first unified 
framework for integrating TM into SMT at decoding when the TM database and the SMT training set 
are different. Although some previous works of the second and third categories can be also applied 
when the TM database and the SMT training set are different, they did not explicitly focus on and test 
this case.  
Last, since the example-based machine translation (EBMT, [Nagao, 1984]) is similar to that of us-
ing TM, some approaches (Watanabe and Sumita, 2003; Smith and Clark, 2009; Dandapat et al., 2011; 
2012; Phillips, 2011) also combined EBMT with SMT. It would be interesting to compare our ap-
proaches with theirs in the future. 
6 Conclusion 
Combining TM and SMT can greatly improve the translation performance and reduce human post-
editing effort. In comparison with those previous approaches, our work makes the following contribu-
tions: 
(1) Dynamically merge the matched TM phrase-pairs into the SMT phrase table to meet the real ap-
plication;  
(2) Propose an improved integrated model to distinguish the original SMT phrase-pairs from the 
newly-added ones extracted from TM;  
(3) Adopt a simple but effective TM adaptation method to favor the consistent translation in cross-
domain test. 
This is the first work adopting a unified framework to integrate the TM information into the SMT 
model during decoding when the TM database and the SMT training set are different. On the TM da-
tabase which consists of Chinese?English computer technical documents, our experiments have shown 
that merging the TM phrase-pairs achieves significant improvements when the fuzzy match score is 
above 0.5. Furthermore, the proposed approaches are significantly better than either the SMT or the 
TM systems for both the in-domain and the cross-domain tests. Last, the proposed approaches outper-
form previous works significantly in all test conditions. 
Acknowledgements 
This research work was partially funded by the Natural Science Foundation of China under Grant No. 
61333018, the Hi-Tech Research and Development Program (?863? Program) of China under Grant 
No. 2012AA011101, the Key Project of Knowledge Innovation Program of Chinese Academy of Sci-
ences under Grant No. KGZD-EW-501, and Toshiba (China) R&D Center. 
 
406
Reference 
Ergun Bi?ici and Marc Dymetman. 2008. Dynamic translation memory: using statistical machine translation to 
improve translation memory fuzzy matches. In Proceedings of the 9th International Conference on Intelligent 
Text Processing and Computational Linguistics (CICLing 2008), pages 454?465. 
Nicola Cancedda. 2012. Private Access to Phrase Tables for Statistical Machine Translation. In Proceedings of 
the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 23?27. 
Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. 
Technical Report TR-10-98, Harvard University Center for Research in Computing Technology. 
Chiang, David. 2005. A hierarchical phrase-based model for statistical machine translation, In Proceedings of 
the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 263?270. 
Sandipan Dandapat, Sara Morrissey, Andy Way, and Mikel L Forcada. 2011. Using example-based MT to sup-
port statistical MT when translating homogeneous data in resource-poor settings, In Proceedings of the 15th 
Annual Meeting of the European Association for Machine Translation (EAMT 2011), pages 201?208. 
Sandipan Dandapat, Sara Morrissey, Andy Way, and Joseph Van Genabith. 2012. Combining EBMT, SMT, TM 
and IR technologies for quality and scale, In Proceedings of the Joint Workshop on Exploiting Synergies be-
tween Information Retrieval and Machine Translation (ESIRMT) and Hybrid Approaches to Machine Trans-
lation (HyTra), pages 48?58. 
Aswarth Dara, Sandipan Dandapat, Declan Groves, and Josef van Genabith. TMTprime: a recommender system 
for MT and TM integration. In Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10?13. 
Yifan He, Yanjun Ma, Josef van Genabith and Andy Way, 2010a. Bridging SMT and TM with translation rec-
ommendation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics 
(ACL), pages 622?630. 
Yifan He, Yanjun Ma, Andy Way, and Josef Van Genabith. 2010b. Integrating N-best SMT outputs into a TM 
system, In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), 
pages 374?382. 
Yifan He, Yanjun Ma, Andy Way and Josef van Genabith. 2011. Rich linguistic features for translation memory-
inspired consistent translation. In Proceedings of the Thirteenth Machine Translation Summit, pages 456?463. 
Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceed-
ings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181?184. 
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 
Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388?395, Barcelona, 
Spain. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke 
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer and Ond?ej Bojar. 2007. Moses: Open source 
toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 
177?180. 
Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of 
the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on 
Human Language Technology, pages 48?54. 
Philipp Koehn and Jean Senellart. 2010. Convergence of translation memory and statistical machine translation. 
In AMTA Workshop on MT Research and the Translation Industry, pages 21?31. 
Liu, Qun and Hongmei Zhao. 2009. Report on CWMT2009 MT Translation Evaluation. In Proceedings of the 
5th China Workshop on Machine Translation (CWMT2009), pages 1?31, Nanjing, China. 
Yanjun Ma, Yifan He, Andy Way and Josef van Genabith. 2011. Consistent translation using dis-criminative 
learning: a translation memory-inspired approach. In Proceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 1239?1248, Portland, Oregon. 
Makoto Nagao, 1984. A framework of a mechanical translation between Japanese and English by anal-ogy prin-
ciple. In: Banerji, Alick Elithorn and  Ran-an (ed). Artifiical and Human Intelligence: Edited Review Papers 
Presented at the International NATO Symposium on Artificial and Human Intelligence. North-Holland, Am-
sterdam, 173?180. 
407
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st 
Annual Meeting of the Association for Computational Linguistics (ACL), pages 160?167. 
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. 
Computational Linguistics, 29 (1). pages 19?51. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 311?318. 
Aaron B. Phillips, 2011. Cunei: open-source machine translation with relevance-based models of each transla-
tion instance. Machine Translation, 25 (2). pages 166-177. 
Michel Simard and Pierre Isabelle. 2009. Phrase-based machine translation in a computer-assisted translation 
environment. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), pages 120?127. 
James Smith and Stephen Clark. 2009. EBMT for SMT: a new EBMT-SMT hybrid. In Proceedings of the 3rd 
International Workshop on Example-Based Machine Translation (EBMT'09), pages 3?10, Dublin, Ireland. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In Proceedings of Association for Ma-chine Translation in the 
Americas (AMTA-2006), pages 223?231. 
Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the International 
Conference on Spoken Language Processing, pages 311?318. 
Taro Watanabe, Eiichiro Sumita. 2003. Example-based decoding for statistical machine translation, In Proceed-
ing of Machine Translation Summit IX, pages 410?417. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2013. Integrating translation memory into phrase-based machine 
translation during decoding. In Proceedings of the 51st Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 11?21. 
Ventsislav Zhechev and Josef van Genabith. 2010. Seeding statistical machine translation with translation 
memory output through tree-based structural alignment. In Proceedings of the 4th Workshop on Syntax and 
Structure in Statistical Translation, pages 43?51. 
Xiaoning Zhu, Zhongjun He, Hua Wu, Haifeng Wang, Conghui Zhu, and Tiejun Zhao. 2013. Improving pivot-
based statistical machine translation using random walk. In Proceedings of the 2013 Conference on Empirical 
Methods in Natural Language Processing, pages 524?534. 
408
A Joint Model to Identify and Align Bilingual
Named Entities
Yufeng Chen?
National Laboratory of Pattern
Recognition, Institute of Automation,
Chinese Academy of Sciences
Chengqing Zong??
National Laboratory of Pattern
Recognition, Institute of Automation,
Chinese Academy of Sciences
Keh-Yih Su?
Behavior Design Corporation
In this article, an integrated model is derived that jointly identifies and aligns bilingual named
entities (NEs) between Chinese and English. The model is motivated by the following obser-
vations: (1) whether an NE is translated semantically or phonetically depends greatly on its
entity type, (2) entities within an aligned pair should share the same type, and (3) the initially
detected NEs can act as anchors and provide further information while selecting NE candidates.
Based on these observations, this article proposes a translation mode ratio feature (defined as
the proportion of NE internal tokens that are semantically translated), enforces an entity type
consistency constraint, and utilizes additional new NE likelihoods (based on the initially detected
NE anchors).
Experiments show that this novel method significantly outperforms the baseline. The type-
insensitive F-score of identified NE pairs increases from 78.4% to 88.0% (12.2% relative im-
provement) in our Chinese?English NE alignment task, and the type-sensitive F-score increases
from 68.4% to 83.0% (21.3% relative improvement). Furthermore, the proposed model demon-
strates its robustness when it is tested across different domains. Finally, when semi-supervised
learning is conducted to train the adopted English NE recognition model, the proposed model
also significantly boosts the English NE recognition type-sensitive F-score.
No rights reserved. This work was authored as part of the Contributor?s official duties as an Employee of
the United States Government and is therefore a work of the United States Government. In accordance with
17 U.S.C. 105, no copyright protection is available for such works under U.S. law.
? No. 95, Zhongguancun East Road, Haidian District, Beijing 100190, China.
E-mail: chenyf@nlpr.ia.ac.cn.
?? No. 95, Zhongguancun East Road, Haidian District, Beijing 100190, China.
E-mail: cqzong@nlpr.ia.ac.cn.
? Hsinchu, Taiwan. E-mail: bdc.kysu@gmail.com.
Submission received: 9 October 2010; revised submission received: 15 February 2012; accepted for
publication: 27 March 2012.
doi:10.1162/COLI a 00122
Computational Linguistics Volume 39, Number 2
1. Introduction
Named entities (NEs), especially person names (PER), location names (LOC), and
organization names (ORG), deliver essential context and meaning in human languages.
Therefore, NE translation plays a critical role in trans-lingual language processing tasks,
such as machine translation (MT) and cross-lingual information retrieval. To learn NE
translation knowledge, bilingual NE alignment (which links source NEs and target
NEs to generate desired NE pairs) is the first step in producing the NE translation
table (which can then be used to train the NE translation model). Furthermore, with
additional alignment constraints from the other language, the alignment module can
also refine those initially recognized NEs, and thus can be adopted to conduct semi-
supervised learning to learn monolingual NE recognition models from a large untagged
bilingual corpus.
Because NE alignment can only be conducted after its associated NEs have been
identified, the NE recognition errors propagate into the alignment stage. The type-
insensitive inclusion rate1 of the initial recognition stage thus significantly limits the
final alignment performance. One way to alleviate this error propagation problem is to
jointly perform NE recognition and alignment. Such a combined approach is usually
infeasible, however, due to the high computational cost of evaluating alignment scores
for a large number2 of NE pair candidates.
In order to make the problem computationally tractable, a sequential approach is
usually used to first identify NEs and then align them. Two such kinds of sequential
strategies that alleviate the error propagation problem have been proposed. The first
strategy, named asymmetry alignment (Al-Onaizan and Knight 2002; Moore 2003;
Feng, Lv, and Zhou 2004; Lee, Chang, and Jang 2006), identifies NEs only on the source
side and then finds their corresponding NEs on the target side. Although this approach
avoids the NE recognition errors resulting from the target side, which would otherwise
be brought into the alignment process, the NE recognition errors from the source side
continue to affect alignment.
To further reduce the errors from the source side, the second strategy, denoted
symmetry alignment (Huang, Vogel, and Waibel 2003), expands the NE candidate
sets in both languages before conducting the alignment. This is achieved by using the
original results as anchors, and enlarging or shrinking the boundaries of the anchors to
generate new candidates. This strategy fails to work if the NE anchor has already been
missed in the initial NE recognition stage, however. In our data set (1,000 Chinese?
English sentence pairs randomly selected from the Chinese News Translation Text
corpus [LDC2005T06]), this strategy significantly improves the type-insensitive NE pair
inclusion rate from 83.9% to 96.1%;3 in the meantime, the type-insensitive Chinese NE
(CNE) recognition inclusion rate rises from 88.7% to 95.9%, and that of English NE
(ENE) from 92.8% to 97.2%. This strategy is thus adopted in this article.
Although the symmetric expansion strategy has substantially alleviated the prob-
lem of error propagation, the final alignment accuracy, in terms of type-sensitive F-score
1 This is the percentage of desired NE pairs that are included within the given candidate set, and is the
upper bound for NE alignment performance (type-insensitive means disregarding NE types).
2 This number will dramatically increase if the combined approach is adopted, as every possible string
will become a NE candidate.
3 This figure is based on the expanded candidates that are constructed without any range limitation. In
this case, Inclusion rate = 1 ? [Missing Rate of Initial NEs]. In addition, because not every Chinese NE is
linked in the given sentence pair, the Inclusion rate of Chinese NEs is even lower than that of NE pairs
(95.9% vs. 96.1%).
230
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
(achieved by the approach proposed by Huang, Vogel, and Waibel [2003]) continues
to be as low as 68.4% (see in Table 3 in Section 4.3). After having examined the data,
we found the following: (1) How a given NE is translated, either semantically (called
translation) or phonetically (called transliteration), depends greatly on its associated
entity type.4 The translation mode ratio, which is the percentage of NE internal tokens
that are translated semantically, thus can help to identify the NE type. (2) Entities within
an aligned pair should share the same type, and this restriction should be integrated
into the NE alignment model as a constraint. (3) In prior work, the initially identified
monolingual NEs were used only to construct the candidate set without playing any
role in final NE identification. Indeed, these monolingual NEs do carry other useful in-
formation and can act as anchors to giveNE likelihoods, which can provide additional
scope preference information to those regenerated candidates.
Based on these observations, we propose a novel joint model that adopts the transla-
tion mode ratio, enforces the entity type consistency constraint, and also utilizes the NE
likelihoods. This proposed approach jointly identifies and aligns bilingual NEs under
an integrated framework, which consists of three stages: Initial NE Recognition, NE-
Candidate Set Expansion, and NE Re-identification & Alignment. The Initial NE Recog-
nition stage identifies the initial NEs and their associated NE types in both the source
and target. In the next stage, NE Candidate Set Expansion regenerates the candidate
sets in both languages in order to remedy the initial NE recognition errors. In the final
stage, NE Re-identification & Alignment jointly recognizes and aligns bilingual NEs via
the proposed joint model. The experimental results validate our proposed three-step
method.
The integrated model that jointly identifies and aligns bilingual named entities
between Chinese and English was originally introduced in Chen, Zong, and Su (2010).
In this article, the problem has been re-formulated and derived. The new derivation
starts from two given NE sequences, whereas the original derivation only begins with
one given NE pair. We also give more details of the problem study, model analysis, and
experiments. Moreover, we report additional experiments, which include those that
study the effect of adopting different initial NE recognizers and the effectiveness of the
proposed model across different domains. Finally, a complete error analysis is given in
the current version.
The remainder of this article is organized as follows: Section 2 motivates the pro-
posed method. Afterwards, the proposed model is formally introduced in Section 3.
Section 4 describes experiments conducted on various configurations of the method.
The associated error analysis and discussion of results are presented in Section 5.
Section 6 gives applications of the proposed model. We review related work in Section 7.
Finally, conclusions are drawn in Section 8.
2. Motivation
By examining the NEs initially recognized in aligned sentence pairs, we have the
following two observations: (1) Alignment can help fix those NEs that are initially in-
correctly recognized when they are not the correct counterparts of each other. Therefore,
4 The proportions of semantic translation (which denote the ratios of semantically translated words
among all the associated words within NEs) for PER, LOC, and ORG are approximately 0%, 28.6%, and
74.8%, respectively, in the Chinese?English name entity list (2005T34) released by the Linguistic Data
Consortium (LDC). Because titles, such as ?sir? and ?chairman,? are not considered part of person
names in this corpus, all PERs are transliterated.
231
Computational Linguistics Volume 39, Number 2
alignment and recognition should be jointly optimized. (2) Alignment cannot help in
determining the appropriate scope when each word within an NE (or within its larger
context window covering the NE) is correctly matched to its counterpart. Therefore, the
information of those initial NEs should be utilized to decide the appropriate NE scope.
The following two sections further elaborate on these two observations.
2.1 Alignment Helps NE Recognition
In NE recognition, both boundary identification and type classification are required.
The complexity of these tasks varies with different languages, however. For example,
Chinese NE boundaries are not obvious because adjacent words are not separated by
spaces. In contrast, English NE boundaries are easier to identify with explicit words and
capitalization clues. On the other hand, classification of English NE type is considered
more challenging (Ji and Grishman 2006).
Because alignment would force the NEs in the linked NE pair to share the same
semantic meaning, the NE that is more reliably identified in one language can be
used to identify its less reliable counterpart in the other language. This benefit, which
is observed in both NE boundary identification and type classification, indicates that
alignment can be used to locate those NEs that are initially incorrectly recognized. For
example, once the correct boundaries are drawn in one language, word equivalences
inside an aligned NE pair can help identify NE boundaries in the language that does
not have explicit clues (e.g., Chinese). As shown in Example (1), even though the desired
Chinese NE ????????? is only partially recognized as ?????? in the initial
recognition stage, it can be recovered if the English counterpart North Korean [no?s]
Central News Agency is given. The reason for this is that News Agency is better aligned
to ?????, rather than be deleted, which would occur if ?????? is chosen as the
corresponding Chinese NE.
On the other hand, type consistency constraints can help correct the NE type that
is less reliably identified. Moreover, in identifying the NE type, it helps if we know
whether a word is translated or transliterated. As illustrated in Example (2), the word
lake in the English NE is linked to the Chinese character ???, and this mapping is
found to be a translation, not a transliteration. Because translation rarely occurs for
personal names (Chen, Yang, and Lin 2003), the desired NE type ?LOC? should be
shared between the English NE Lake Constance and its corresponding Chinese NE ???
???.? As a result, the original incorrect type ?PER? of the given English NE is fixed; it
thus corroborates the need for using the translation mode ratio and NE type consistency
constraint.
232
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
2.2 Initial NEs Carry NE Scope Information
In Huang, Vogel, and Waibel (2003), initial NE information was discarded once the
new candidate set was generated. Alignment scores alone, however, are incapable of
selecting the appropriate NE scope in some cases. For instance, when each word in an
NE is correctly matched to its counterpart, the alignment score might still prefer an
incorrect NE pair with smaller scope, even after the normalization of alignment terms
has been considered (explained in Section 4.3). On the other hand, when words sur-
rounding the desired NE are also correctly matched to their counterpart, the incorrect
NE pair with larger scope might be chosen. As illustrated in Example (3), the desired NE
pair {????::[Germany]} is initially correctly recognized, although the wrong NE pair
{??????::[Germany economy]} is finally selected from the regenerated candidate set
if only alignment scores are used in the final selection process. This is because the extra
words ???? and economy are a perfect translation of each other, thus resulting in the
incorrect pair {??????::[Germany economy]}, which receives a higher alignment
score than does the correct NE pair {????::[Germany]}.
It must also be noted that an NE alignment model usually ignores a fair amount of
potentially useful information that is commonly used in monolingual NE recognition
models. For example, the bigrams of text surrounding NEs are frequently adopted in
a monolingual NE recognition model, but not in a bilingual alignment model. Other
233
Computational Linguistics Volume 39, Number 2
examples include case information, part-of-speech (POS) triggers, gazetteer features,
and external macro context features mentioned in Zhou and Su (2006). The alignment
model fails to consider the monolingual context surrounding NEs while determining
NE scope, resulting in the error mentioned in Example 3. By ignoring the initial NEs
after their corresponding candidate sets have been generated, we lose the information
provided by these initial NEs that is otherwise available to the alignment model.
Therefore, though the initially detected NEs might be unreliable by themselves, they
should act as anchors to provide scope preference information, even after the expanded
candidate set has been generated from these NEs.
3. The Proposed Joint Model
Given a Chinese?English sentence pair (Sc,Se), with its initial (denoted by lower
case letter b, for ?beginning?) Chinese NEs [Cbi,Tci]
Nc
i=1,Nc ? 1 and English NEs
[Ebj,Tej]
Ne
j=1,Ne ? 1 , where Nc and Ne are the numbers of initially recognized NEs of
Chinese and English, respectively; Tci and Tej are the original NE types assigned to Cbi
and Ebj , respectively. (For the reader?s convenience, all adopted notations are listed in
Table 1 for quick reference.) We first regenerate two NE candidate sets (by enlarging
and shrinking the boundaries of those initial NEs) to include, we hope, the correct
corresponding candidates that failed to be recognized in the first stage. Let CKc1 and E
Ke
1
denote the two sets that include those regenerated candidates for Chinese and English
NEs, respectively (Kc and Ke are their set-sizes), and K = min (Nc,Ne). Then, the total K
pairs of the final Chinese and English NEs will be extracted from the Cartesian product
of CKc1 and E
Ke
1 . Here, only NE pairs in one-to-one mappings will be extracted, as most
applications are only interested in this kind of correspondence. Therefore, we will let CK1
and EK1 denote the two extracted candidate sets to be linked, and these sets will consist
of non-overlapping NE candidates from CKc1 and E
Ke
1 , respectively (for conciseness,
we will not explicitly distinguish between the indices CK1 and C
Kc
1 , or between
EK1 and E
Ke
1 ).
Let
{
Ca(k),Ek
}
denote a specific NE-linking-pair (where a(k) and k are the associated
indices of those regenerated Chinese and English NEs within CK1 and E
K
1 , respectively);
the subscript a(k) denotes that Ca(k) is aligned to Ek. Let Tk be the NE type to be re-
assigned and shared by Ca(k) and Ek (as they should denote the same entity). Assuming
that only one-to-one mappings of NE pairs will be extracted, the problem of getting
Table 1
Adopted notations (keywords have been italicized).
Symbols Symbols
Chinese Sentence Sc English Sentence Se
Initial/Beginning Chinese NE Cb Initial/Beginning English NE Eb
Regenerated Chinese NE C Regenerated English NE E
Initial Chinese NE Type Tc Initial English NE Type Te
Re-assigned NE Type T NE Sequence Alignment As
Internal Component Alignment A Internal Translation Mode M
Chinese Component cp English Word ew
Translation Mode Ratio ? Chinese Character cc
Left Distance dL Right Distance dR
Initial/Beginning NE Length Lb Regenerated NE Length L
234
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
the final desired aligned NE pairs
{
C?a(k),E
?
k ,T
?
k
}K
k=1
is that of finding the most likely
allowable combination of NE pairs (and their re-assigned NE types), given all the initial
NEs (i.e., [Cbi,Tci]
Nc
i=1 and [Ebj,Tej]
Ne
j=1) and the sentences that include them (i.e., Sc and
Se). This can be formulated as follows:
{
C?a(k),E
?
k ,T
?
k
}K
k=1
= arg max
{Ca(k),Ek}
K
k=1
?
?
?
max
TK1
P
(
{
Ca(k),Ek,Tk
}K
k=1
|[Cbi,Tci]Nci=1,Sc, [Ebj,Tej]
Ne
j=1,Se
)
?
?
?
(1)
For any given NE?pair sequence
{
Ca(k),Ek
}K
k=1
, the internal max operator will first
operate over each re-assigned type-sequences TK1 (where T belongs to PER, LOC, ORG
for each given NE pair included in the NE?pair sequence). The outer argmax operator
will cross every admissible NE?pair sequence.
This formulation implies that recognition and alignment are executed jointly
with respect to CK1 and E
K
1 , without making any independence assumptions among
those NE pairs included in the associated NE?pair sequence. This equation is
thus computationally infeasible due to a large search space. Therefore, it is fur-
ther simplified and derived as follows by first explicitly denoting the link between
Ca(k) and Ek as AS,k =
{
Ca(k),Ek
}
. Let {AS,k}
K
k=1 (abbreviated as A
K
S,1) denote one possi-
ble alignment between CK1 and E
K
1 . We will then have K! different possible align-
ments between them (i.e., total factorial of K different AKS,1). Finally, let [Cba(k),Tca(k)]
K
k=1
and [Ebk,Tek]
K
k=1 denote those initially recognized corresponding NEs that gener-
ate CK1 and E
K
1 (i.e., their associated anchors), respectively. Then we can replace
{
Ca(k),Ek, Tk}
K
k=1 in Equation (1) by [AS,k,Ca(k),Ek,Tk]
K
k=1, and derive the original prob-
ability P
(
{
Ca(k),Ek,Tk
}K
k=1
|[Cbi,Tci]Nci=1,Sc, [Ebj,Tej]
Ne
j=1,Se
)
as follows.
P
(
{
Ca(k),Ek,Tk
}K
k=1
|[Cbi,Tci]Nci=1,Sc, [Ebj,Tej]
Ne
j=1,Se
)
? P
(
[AS,k,Ca(k),Ek,Tk]
K
k=1|[Cba(k),Tca(k)]
K
k=1,Sc, [Ebk,Tek]
K
k=1,Se
)
?
K
?
k=1
[P
(
[AS,k,Ca(k),Ek,Tk]|[Cba(k),Tca(k)],Sc, [Ebk,Tek],Se
)
]
(2)
where P
(
[AS,k,Ca(k),Ek,Tk]|[Cba(k),Tca(k)],Sc, [Ebk,Tek],Se
)
can be further decomposed as
follows.
P
(
[AS,k,Ca(k),Ek,Tk]|[Cba(k),Tca(k)],Sc, [Ebk,Tek],Se
)
? P(AS,k|Ca(k),Ek,Tk) ? P
(
Tk|Tca(k),Tek,Sc,Se
)
? P(Ca(k)|Cba(k),Tca(k),Tk,Sc) ? P(Ek|Ebk,Tek,Tk,Se)
(3)
In Equation (3), P(AS,k|Ca(k),Ek,Tk) and P
(
Tk|Tca(k),Tek,Sc,Se
)
are defined as
NE Alignment Probability and NE Type Re-assignment Probability, respectively, for
finding the final alignment AKS,1 among C
K
1 and E
K
1 that have been selected. Both
P(Ca(k)|Cba(k),Tca(k),Tk,Sc) and P(Ek|Ebk,Tek,Tk,Se) are called NE likelihoods, and are
used to assign preference to each selected CK1 and E
K
1 , based on the initial NEs (which
act as anchors). For brevity, we drop the associated subscripts hereafter, if there is
235
Computational Linguistics Volume 39, Number 2
no confusion. These probabilities will be further described in Sections 3.1 and 3.2.
Finally, the joint identification and alignment framework that incorporates the initial
NE recognition process, candidate set construction, and the associated search process
are given in Section 3.3.
3.1 Bilingual Related Probabilities
The NE alignment probability represents the likelihood of a specific alignment AS,k,
given C and E and their associated T. Because Chinese word segmentation in-
troduces errors, especially for transliterated words, the NE alignment probability
P(AS,k|Ca(k),Ek,Tk) in Equation (3) is derived from E (i.e., starting from the English part).
In addition, because internal component alignment (denoted as A, to be defined later)
within a given NE pair carries important information (as illustrated in Section 2), the
internal component alignment will be introduced as follows.
P(AS,k|Ca(k),Ek,Tk) =
?
A
P(A|C,E,T)
? max
A
P(A|C,E,T) (4)
= max
A
[ 1
R
? P(A|E,T)]
where R =
?
A P(A|E,T) is a normalization value,
5 which will be ignored for simplicity,
leaving only the probability P(A|E,T) to be derived.
Let A be configured as A ? ?[cpa(n), ewn,Mn]Nn=1, ??, where [cpa(n), ewn,Mn] denotes
a linked pair of a Chinese component cpa(n) (which might contain several Chinese char-
acters) and an English word ewn within C and E, respectively, with their translation
mode Mn to be either translation (abbreviated as TS) or transliteration (abbreviated as
TL). We assume that there are N component transformations in total, including NTS
translation transformations [cpa(n), ewn,TS]
NTS
n=1 and NTL transliteration transformations
[cpa(n), ewn,TL]
NTL
n=1, such that N = NTS +NTL. Moreover, because the statistical distribu-
tion of internal translation mode varies greatly across various NE types (as illustrated in
footnote [4] of this article), the associated translation mode ratio ? = (NTS/N) is an impor-
tant feature and is included in the internal component alignment specified previously.
For example, if the A between ??????? and Constance Lake is [????, Constance,
TL] and [?, Lake, TS] (NTS = NTL = 1), then its associated translation mode ratio will be
0.5 (i.e., ? = 1/2).
Therefore, the internal alignment probability P(A|E,T) will be further deduced by
introducing the translation mode Mn and the translation mode ratio ? as follows:
P(A|E,T) ? P([cpa(n), ewn,Mn]Nn=1, ?|E,T)
?
N
?
n=1
[P(cpa(n)|Mn, ewn,T) ? P(Mn|ewn,T)] ? P(?|T) (5)
5 The summation will be taken over various A that can generate C.
236
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Combining Equations (4) and (5), the NE alignment probability P(AS,k|Ca(k),Ek,Tk),
which integrates internal component alignment information such as translation mode
ratio and NE type constraint, is finally obtained as follows.
P(AS,k|Ca(k),Ek,Tk)
? 1R ? maxA
[
NA
?
n=1
[P(cpA,a(n)|MA,n, ewA,n,T) ? P(MA,n|ewA,n,T)] ? P(?A|T)
]
(6)
where R is the normalization factor defined by Equation (4), and will be ignored in
the final selection. In Equation (6), the mappings between internal elements is trained
from the syllable/word alignment of NE pairs of different NE types. For translitera-
tion, the model adopted in Huang, Vogel, and Waibel (2003), which first romanizes
Chinese characters and then transliterates them into English characters, is used in
estimating P(cpa(n)|TL, ewn,T). For translation, conditional probability is directly used
for P(cpa(n)|TS, ewn,T).
On the other hand, the NE type re-assignment probability P (T|Tc,Te,Sc,Se), proposed
in Equation (3), is derived as follows.
P (T|Tc,Te,Sc,Se) ? P (T|Tc,Te) (7)
As Equation (7) shows, both the initially assigned Chinese NE type Tc and the initially
assigned English NE type Te are adopted to jointly identify their shared NE type T.
3.2 Monolingual NE Likelihoods
The monolingual related probabilities in Equation (3) represent the likelihood that a
regenerated NE candidate is the true NE, given its originally detected NE. For Chinese,
we derive the likelihood as follows.
P(C|Cb,Tc,T,Sc)
? P
(
dL, dR,String[C]|Lb,Tc,T
)
? P(dL|Lb,Tc,T) ? P(dR|Lb,Tc,T) ?
?L
l=1 P(ccl|ccl?1,T)
(8)
Here Lb is the length (in characters) of the original recognized Chinese NE Cb. Let dL and
dR denote the left and right distance, respectively (which are based on the numbers of
Chinese characters), that C shrinks/enlarges from the left and the right boundaries of
its anchor Cb. In Example (1) in Section 2.1, in the case where the given Cb and C are
?????? and ????????, respectively, then dL and dR are ?1 and +3, respec-
tively. Let String[C] denote the associated Chinese string of C, ccl denote the l-th Chinese
character within that string, and L denote the total number of Chinese characters within
C. Then we will have a range of bigram probabilities for candidates with different
lengths. Therefore, it is systematically biased6 (in probability value) towards candidates
with shorter lengths. On the English side, following Equation (8), P(E|Eb,Te,T,Se) can
be derived similarly; the unit is a word, however, rather than a character.
6 This bias is introduced by the conditional independence assumption made while decomposing
P
(
String[C]|T
)
into
?L
l=1 P(ccl|ccl?1, T).
237
Computational Linguistics Volume 39, Number 2
In summary, with factors dL and dR, the proposed NE likelihood is able to assign
scope preference to each regenerated NE based on its associated initial NE. The initial
NE therefore still plays a role in the final selection process, even after its related candi-
date set has been generated, which is important when all words involved are correctly
matched to their counterparts, as explained in Section 2.2. In contrast, Huang, Vogel,
and Waibel (2003) adopt only type-dependent bigrams as the NE likelihood. The initial
NE thus will not play any role in the final selection process after its related candidate
set has been generated. The scope preference information carried by the initial NE is
therefore not utilized in their model.
Having integrated all related probabilities (Equations (6), (7), and (8)) together,
we now have the final desired model. For simplicity, all the probabilities involved are
estimated by the Good-Turing smoothing technique (Chen and Goodman 1998) unless
otherwise specified.
3.3 Framework for Jointly Identifying and Aligning Bilingual NEs
In jointly identifying and aligning bilingual NEs, a three-stage framework is adopted:
(A) Initial NE Recognition, generating the initial NE anchors with off-the-shelf pack-
ages, (B) NE Candidate Set Expansion, expanding the associated NE Candidate set to
remedy the errors made in the previous stage, and (C) NE Re-identification & Align-
ment, extracting the final NE pairs from the Cartesian product of source and target
candidate sets (created in the second stage) via a search process. Figure 1 presents the
detailed procedure of this framework.
1 For each given bilingual sentence pair:
2 (A) Initial NE Recognition: The initial Chinese NEs and English NEs are first identified by their
corresponding NE recognition toolkits, respectively.
3 (B) NE Candidate Set Expansion: To rescue those NEs whose boundaries are incorrectly identified in
the previous stage?for each initially detected NE, several NE candidates will be regenerated from the
original NE by allowing its boundaries to be shrunk or enlarged within a pre-specified range.
4 (B.1) Create both C and E candidate sets, which are expanded from those initial NEs recognized in
the previous stage.
5 (B.2) Construct a NE pair candidate set (namedNE-Pair-Candidate set) by generating a Cartesian
product of C and E candidate sets created in the above step.
6 (C)NE Re-identification & Alignment: Rank each candidate in the NE-Pair-Candidate set constructed
above with the score specified by the proposed model. Let Nc and Ne be the numbers of those initial
Chinese and English NEs in the first stage, respectively, and set K = min(Nc,Ne). Extract top K final
NE pairs (with their re-assigned NE types) with the highest scores from the NE-Pair-Candidate set.
7 (C.1) FOR each NE pair in the NE-Pair-Candidate set created above:
FOR each re-assigned NE type within {PER, LOC, ORG}
Evaluate the score for the given candidate pair and the given NE type according to the
proposed model.
END FOR
Find the re-assigned NE type with the highest score, then attach it and its corresponding
score to the given NE pair
END FOR (C.1)
8 (C.2) Conduct a beam search process to select the top K non-overlapping NE pairs from the NE-Pair-
Candidate set with the scores assigned above. The searching process will keep removing those over-
lapping NEs from the candidate list before each state is branched.
Figure 1
A framework for jointly identifying and aligning bilingual NEs.
238
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Example 4 illustrates how the framework works. For simplicity, we will allow the
Chinese NE to enlarge/shrink its boundaries to four characters on each side, and only
allow two words for English.
Example 4. An example of candidate set construction
Each NE and its type in this example is separated by ?/?. Only partial and relevant
information is shown here.
(A.1) A Chinese tagged sentence: ??????????/PER? ????????????
?. . .??;
(A.2) Initial Chinese NE (Cb): ??????/PER?, Nc = 1;
(A.3) An English tagged sentence: ?The report said the [Galapagos/PER] [National
Park/ORG] and local fishermen were working together . . .?;
(A.4) Initial English NEs (Eb): [Galapagos/PER], [National Park / ORG], Ne = 2;
(B.1.1) Regenerated Chinese candidate set (CK1 ): {???????, ???????????, ???,
??????????, ????, ???, ???, ???, ???, ??????????????, ????
?????, . . . .}. Total 62 C candidates will be generated.
(B.1.2) Regenerated English candidate set (EK1 ): {[Galapagos], [National Park], [Galapagos Na-
tional Park], [said the Galapagos National Park], [National Park and local], [National],
[Park], . . .}. Total 24 E candidates will be generated.
(B.2) NE-Pair-Candidate set: {???????::[Galapagos], ???????????::[Galapagos
National Park], ???????::[National Park], ???????????::[Park], . . . . . .}.
Total 1,488 (62 ? 24) NE pairs will be generated.
(C) K = min(Nc,Ne) = 1. Therefore, only {???????????::[Galapagos National
Park], ORG} will be extracted.
This example shows that the desired Chinese NE ??????????? is partially
recognized as ??????? initially with an incorrect NE type PER. In addition, the
desired English NE [Galapagos National Park] is split into [Galapagos] and [National
Park], initially two NEs. After each NE Candidate set has been expanded, the desired
NEs ??????????? and [Galapagos National Park] are included in C and E
candidate sets, respectively. The desired NE pair {???????????::[Galapagos
National Park], ORG} can thus be located.
If we allow the boundaries to be enlarged/shrunk without any limitation during
the expansion step, all NEs that are initially incorrectly recognized can then be in-
cluded. The generated search space, however, would be too large to be tractable. In
our observation, four Chinese characters for both shrinking and enlarging, and two
English words for shrinking and three for enlarging, are found to be adequate in
most cases. (Note that only the candidate that contains at least one original character/
word is allowed.) Under this condition, the inclusion rates for NEs with correct
boundaries can be increased to 94.6% (from 88.7%) for Chinese, and 96.3% (from
92.8%) for English, respectively; the NE pair inclusion rate can even be increased to
95.3% from 83.9%. Because the inclusion rate achieved by this strategy (with limited
range) is only 0.8% lower than that obtained without any range limitation (which
is 96.1%, as some NEs might have been completely missed in the first stage), this
setting is adopted in this article to reduce the search space. Even with this expansion
strategy, however, those missing and spurious (false positive) errors still cannot be
remedied, because we will neither create additional anchors nor delete any existing
anchor.
239
Computational Linguistics Volume 39, Number 2
4. Experiments on Various Configurations
To evaluate the proposed approach, prior work (Huang, Vogel, and Waibel 2003) is
re-implemented as our baseline (see Section 4.2). This is because the work not only
adopts the same candidate set expansion strategy mentioned previously, but also uti-
lizes monolingual information when selecting NE pairs (only a simple bigram model is
used, however). This is in contrast to other works (Feng, Lv, and Zhou 2004; Lee, Chang,
and Jang 2006), which only used alignment scores.
The same training and test sets are used for the various experiment configura-
tions. The adopted training set includes two parts. The first part consists of 110,874
aligned sentence pairs from newswire data in the Foreign Broadcast Information Service
(LDC2003E147) corpus, which is denoted as Training Set I. The average length of the
Chinese sentences in this data set is 74.6 characters, and the average length of the
English sentences is 30.2 words. Training Set I is initially tagged by Chinese/English
NE taggers, and then reference NE boundaries and types are manually labeled. The
second part of the training set is the LDC2005T348 bilingual NE pair list with a total
of 218,772 NE pairs, which is denoted as Training Set II. The required features (e.g.,
NE type and translation-mode) are then manually labeled throughout the two training
sets. Because Training Set II only contains isolated NE pairs that are not associated with
their surrounding context, Training Set I is thus required to train those context-related
parameters.
In the baseline system, translation cost and transliteration cost models are trained
on Training Set II, and tagging cost is trained on Training Set I. For the proposed
approach, the NE likelihoods are trained on Training Set I, and Training Set II is used to
train the parameters relating to the NE alignment probability.
For the test set, 300 sentence pairs are randomly selected from the Linguistic Data
Consortium (LDC) Chinese?English News Text (LDC2005T06) corpus, which contains
at least one NE pair in each sentence. The average length of Chinese sentences is 59.4
characters, and the average length of English sentences is 24.8 words. The answer
keys to NE recognition and alignment are annotated manually, and used as the gold
standard to calculate the metrics of precision (P), recall (R), and F-score (F) for both
NE recognition and alignment. A total of 765 Chinese NEs and 747 English NEs are
manually identified in the test set, in which there are 718 reference NE pairs (including
214 PER pairs, 371 LOC pairs, and 133 ORG pairs). NE alignment result is a subset of
NE recognition results, because not all those recognized NEs can be aligned.
The development set for feature selection and weight training is composed of 200
sentence pairs selected from the LDC2005T06 corpus, which includes 482 manually
tagged NE pairs. The average length of Chinese sentences is 56.4 characters, and the
average length of English sentences is 23.2 words. There is no overlap between the
training, development, and test sets.
These data sets will be adopted in a series of experiments that investigate the
proposed model. Among them, the results of initial NE recognition are given in Sec-
tion 4.1, and those related to the baseline system are given in Section 4.2. In Section 4.3,
a series of experiments are conducted to examine the effect of various features adopted
in the proposed model. The weighted version of the proposed model is also tested.
7 FBIS multilingual text (http://projects.ldc.upenn.edu/TIDES/mt2003.html).
8 The LDC2005T34 data set consists of proofread bilingual entries: 73,352 person names, 76,460 location
names, and 68,960 organization names.
(http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2005T34).
240
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 2
Initial type-sensitive Chinese/English NER performance.
NE type P (%) R (%) F (%)
PER 80.2/79.2 87.7/85.3 83.8/82.1
LOC 89.8/85.9 87.3/81.5 88.5/83.6
ORG 78.6/82.9 82.8/79.6 80.6/81.2
ALL 83.4/82.1 86.0/82.6 84.7/82.3
Furthermore, the effectiveness of adopting different initial NE recognizers is shown in
Section 4.4, and the effectiveness of the proposed model across different domains is
illustrated in Section 4.5. Finally, the result of directly using all available features under
a Maximum Entropy framework without developing a principled model is given in
Section 4.6.
4.1 Initial NE Recognizers
Both the baseline alignment system and the proposed model share the same Initial
NE Recognition subtask. The systems adopt the Chinese NE recognizer reported in
Wu, Zhao, and Xu (2005), which is a hybrid statistical model incorporating multi-
knowledge sources, and the English NE recognizer included in the publicly available
Mallet toolkit9 (McCallum 2002) to generate initial NEs. These two initial NE recog-
nizers are adopted because their performance is comparable to other state-of-the-art
systems (Gao, Li, Wu, and Huang 2005; Zhou and Su 2006). The NE recognition baseline
performances reported subsequently are provided by these two packages. A total of 789
Chinese NEs and 752 English NEs are recognized.
Table 2 shows the initial NE recognition (NER) performance for both Chinese and
English (the highest performance in each column is in bold). It is observed that the
F-score of ORG type is the lowest among all NE types for both English and Chinese.
This is because many organization names are only partially recognized or missed alto-
gether. In addition, the precision rate of PER type is lowest among all English NE types
because many location names or abbreviated organization names tend to be incorrectly
recognized as person names in English. In general, the initial Chinese NER outperforms
the initial English NER, as the NE type classification turns out to be a more difficult
problem for this English NER system.
4.2 The Baseline System
The model of Huang, Vogel, and Waibel (2003) is re-implemented in our environment as
the baseline system, and is briefly sketched here for the reader?s convenience. There are
three cost features in Huang?s alignment model: (1) transliteration cost, which measures
the phonetic similarity of the aligned NEs; (2) translation cost, which is similar to IBM
model-1 (Brown et al 1993); and (3) tagging cost, which evaluates bigram probabilities
of the aligned NEs based on the same NE type.
9 http://mallet.cs.umass.edu/index.php/Main Page.
241
Computational Linguistics Volume 39, Number 2
Table 3
NEA type-insensitive (type-sensitive) performance on the test set.
Model P (%) R (%) F (%)
ExpB (Baseline) 77.1 (67.1) 79.7 (69.8) 78.4 (68.4)
Exp1 (B-Probabilities) 76.2 (72.3) 78.5 (74.6) 77.3 (73.4)
Exp2 (B-Probabilities N-Alignment) 77.7 (73.5) 79.9 (75.7) 78.8 (74.6)
Exp3 (N-Full Model) 83.7 (78.1) 86.2 (80.7) 84.9 (79.4)
Exp4 (MERT-W) 85.9 (80.5) 88.4 (83.0) 87.1 (81.7)
In our experiments, the translation cost of the baseline system is trained on Training
Set I (with 110,874 aligned sentence pairs) by the GIZA++ toolkit (Och and Ney 2003),
the transliteration cost is trained on all person names (all are transliterated) and translit-
erated location and organization names included in Training Set II. The tagging cost is
trained on the tagging result of Training Set I by the initial NE detection system.
When those initially identified NEs are directly used for alignment, only a 64.1%
F-score (regarding their NE types) is obtained from this baseline system. This relatively
poor performance is mainly due to errors in the initial NE recognition stage that are
brought into the subsequent alignment stage. To diminish the accumulative effect of
errors, the same expansion strategy described in Section 3.3 is then adopted to enlarge
the possible NE candidate set. However, only a slight improvement is obtained (from
64.1% to 68.4% for type-sensitive F-score), as shown in Table 3 in Section 4.3. Therefore,
it is conjectured that the baseline alignment model is unable to perform well if the
features proposed in this article are not adopted.
4.3 The Re-identification and Alignment Joint Model
To examine the individual effect of features adopted in the model, a series of experi-
ments are first conducted on the development set. All features mentioned in Section 3
are verified by their contributions and are then adopted for further experiments on the
test set. Table 3 lists only the representative performance of NE alignment (NEA) on
the test set, and gives two performance measures for the experiments. The first one
(named type-insensitive) only checks the scope of each NE without taking its associated
NE type into account (which is the approach adopted in most of the literature on NE
recognition), and is reported as the main metric in Table 3. The second one (named
type-sensitive) also evaluates the associated NE type of each NE. To evaluate the type-
sensitive performance for NE pairs with correct boundaries, we give one point to any
NE pair that also possesses the correct type-tags on both sides, and give 0.5% if only
one side is correct. Of course, zero points are given if both types are incorrect or the
boundary of any NE is incorrect. With the rules specified herein, the type-sensitive
results are also given within the parentheses in Table 3, and a large degradation is
observed. The configurations of various experiments are listed as follows.
ExpB: This is the baseline system (Huang, Vogel, and Waibel 2003), which is re-
implemented in our environment for comparison.
Exp1: Exp1 (named B-Probabilities) adopts all bilingual related probabilities involved in
Equations (6) and (7), to show the full power of bilingual probabilities.
242
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Exp2: Furthermore, because the NE alignment probability would favor the candidates
with fewer components,10 it is further normalized by converting Equation (6) into
the following form:
max
A
?
?
?
?
?
?
?
NA
?
n=1
P(cpA,a(n)|MA,n, ewA,n,T) ? P(MA,n|ewA,n,T)
?
?
1
NA
? P(?A|T)
?
?
?
?
?
The experiment covering complete bilingual probabilities, with the normalized
versions of Equations (6) and (7), is denoted as Exp2 (named B-Probabilities
N-Alignment).
Table 3 indicates that Exp2 achieves the best performance (both type-insensitive
and type-sensitive) among different combinations of the bilingual-related prob-
abilities. Due to its effectiveness, the normalized bilingual probabilities are hence
adopted in all subsequent experiments (i.e., all are based on Exp2).
Exp3: Exp3 (named N-Full Model) manifests the full power of the proposed recognition
and alignment joint model, by integrating all monolingual options into Exp2.
Note that we use the SRI Language Modeling Toolkit11 (Stolcke 2002) to train
various character-/word-based bigram models on different NE types. They are
trained with modified Kneser-Ney smoothing (Kneser and Ney 1995, Chen and
Goodman 1998). Note that monolingual bigrams are also normalized with their
numbers.
As Exp3 shows, the best configuration is to take advantage of all features proposed
in this article and to normalize all feature probabilities. This configuration will thus be
taken for further improvement in the following sections.
So far the proposed model weighs all features equally. It is reasonable to expect that
features should be weighted differently according to their contribution, however. Those
weighting coefficients can be learned from the development set via the well-known
Minimum Error Rate Training approach (Schlu?ter and Ney 2001; Och 2003) (commonly
abbreviated as MERT). To save computational cost, we only re-evaluate the scores of
candidate pairs in a pre-generated pool, instead of regenerating new candidate pairs
each time when Wt (the vector of weighting coefficients at i-th iteration) is updated to
Wt+1 (of the next iteration). For each sentence pair, its corresponding pool is first created
by using W0 (i.e., Exp3) to generate the top 50 NE pairs
12 resulting from the beam search
process. Subsequently, when we switch Wt to Wt+1, we only re-score (and then re-rank)
those candidate pairs inside the pool according to Wt+1.
Exp 4 presents the weighted version of the proposed joint model obtained from
MERT training (MERT-W, N-Full Model, abbreviated as MERT-W). The result demon-
strates that MERT is effective and useful. Entries in bold indicate that the model signif-
icantly outperforms the baseline system. (All statistical significance tests in this article
are measured with 95% confidence level on 1,000 re-sampling batches [Zhang, Vogel,
and Waibel 2004]).
10 It is biased by the E word-count due to the sufficiency (Freedman 2005; Liese and Miescke 2008)
assumption made during decomposition.
11 http://www.speech.sri.com/projects/srilm/.
12 Because most sentence pairs possess less than four NE pairs under Exp3, 50 NE pairs should be sufficient.
243
Computational Linguistics Volume 39, Number 2
Table 4
NER type-insensitive (type-sensitive) performance of different English NE recognizers.
English NE recognizers P (%) R (%) F (%)
Mallet Toolkit 91.8 (82.1) 92.4 (82.6) 92.1 (82.3)
Stanford NE recognizer 93.7 (84.7) 91.4 (82.3) 92.5 (83.5)
Minor Third 90.8 (82.1) 89.5 (80.7) 90.1 (81.4)
Table 5
NEA type-insensitive (type-sensitive) performance with the same Chinese NE recognizer
(Wu?s system) and different English NE recognizers.
NE alignment on P (%) R (%) F (%) Upper bound (%)
different recognizers
Mallet Toolkit 85.9 (80.5) 88.4 (83.0) 87.1 (81.7) 95.3
Stanford NE recognizer 85.9 (80.2) 88.4 (82.7) 87.1 (81.4) 95.0
Minor Third 85.7 (80.2) 88.1 (82.7) 86.9 (81.4) 94.2
Compared to the baseline system, the MERT-W version has substantially raised
the test set type-insensitive F-score of identified NE pairs from 78.4% to 87.1% (11.1%
relative improvement), and the type-sensitive F-score from 68.4% to 81.7% (19.4%
relative improvement). Therefore, this MERT-W version is adopted in all further
experiments.
4.4 Effect of Adopting Different Initial NE Recognizers
To study whether the final performance of NE alignment is sensitive to the choice of
initial NE recognizers, we investigate the final alignment performance across different
Chinese and English NE recognizers.
First, we test the NE alignment performance with the same Chinese NE recognizer
(Wu?s system, adopted earlier) but with different English NE recognizers that include
the Mallet toolkit (used before), the Stanford NE recognizer (Finkel, Grenager, and
Manning 2005), and Minor Third (Cohen 2004). Table 4 shows the type-insensitive and
type-sensitive (within parentheses) results. Table 5 shows the effect on NE alignment
performance. From Tables 4 and 5, we find that NE alignment performance is actually
not sensitive to the NE recognition result. Although the performance of different NE
recognizers are various (type-insensitive13 F-scores are 90.1%, 92.1%, and 92.5%, re-
spectively), the gaps among their corresponding NE alignment results are negligible
(type-sensitive F-scores of weighted versions are 81.7%, 81.4%, and 81.4%, respectively),
as their candidate sets are enlarged based on initially recognized NEs. It is also note-
worthy that although the F-score of the Stanford NE recognizer is higher than that
of the Mallet toolkit, its corresponding NE alignment performance is lower than the
model based on the Mallet toolkit. We conjecture that the lower recall of Stanford NE
13 Because the initial NE recognizer mainly provides NE anchors, NE type is less relevant to the following
alignment.
244
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 6
NER type-insensitive (type-sensitive) performance of different Chinese NE recognizers.
Chinese NE recognizers P (%) R (%) F (%)
Wu?s System 86.2 (83.4) 88.7 (86.0) 87.4 (84.7)
BaseNER 88.3 (85.9) 84.9 (82.5) 86.6 (84.2)
S-MSRSeg 86.8 (84.7) 81.5 (79.5) 84.1 (82.1)
Table 7
NEA type-insensitive (type-sensitive) performance with the same English NE recognizer (Mallet
system) and different Chinese NE recognizers.
NE alignment on P (%) R (%) F (%) Upper bound (%)
different recognizers
Wu?s System 85.9 (80.5) 88.4 (83.0) 87.1 (81.7) 95.3
BaseNER 85.6 (79.9) 88.1 (82.4) 86.8 (81.1) 94.2
S-MSRSeg 84.5 (78.9) 87.1 (81.4) 85.8 (80.1) 93.3
recognizer leads to lower NE alignment performance because the recall of NER is closely
related to the NE pair inclusion rate (please refer to footnote 1), which is the upper
bound of its corresponding NE alignment performance.
Similarly, we test the NE alignment performance with the same English NE rec-
ognizer (Mallet) but with different Chinese NE recognizers, including Wu?s system (as
before), BaseNER (Zhao and Kit 2008), and S-MSRSeg (Gao, Li, Wu, and Huang 2005).
The comparisons are given in Tables 6 and 7. From these tables we also see that the
NE alignment result is not sensitive to the NE recognition result (84.1% to 87.4% type-
insensitive for NER vs. 80.1% to 81.7% type-sensitive in F-score for NEA), although the
performance of NE alignment is related to the recall of the Chinese NE recognizer (the
weaker side). We also note that the type-insensitive F-score performance gap among
various English NE recognizers in Table 5 is less than that of the Chinese NE recognizers
in Table 7 (0.2% vs. 1.3%), which is mainly due to the different gaps among their original
performances (2.4% vs. 3.3%, shown by Tables 4 and 6).
Furthermore, NE alignment based on the worst Chinese NE recognizer (S-MSRSeg)
and the worst English NE recognizer (Minor Third) is conducted in Table 8. From Table 8
we see that the final NE alignment performance is primarily determined by the weaker
side, which is the one that gives the lower recognition recall rate. In this particular
case, the performance of the combination of S-MSRSeg and Minor Third (79.9% type-
sensitive F-score) is mainly driven by the performance of the Chinese S-MSRSeg (80.1%
Table 8
NEA type-insensitive (type-sensitive) performance with a different English NE recognizer and
another Chinese NE recognizer.
NE alignment on P (%) R (%) F (%) Upper bound (%)
different recognizers
Wu & Mallet 85.9 (80.5) 88.4 (83.0) 87.1 (81.7) 95.3
S-MSRSeg & Minor 84.7 (78.6) 87.3 (81.3) 86.0 (79.9) 93.9
245
Computational Linguistics Volume 39, Number 2
Table 9
Initial NE recognition type-insensitive (type-sensitive) performance across various domains.
Different domains Language P (%) R (%) F (%)
News (Table 2) Chinese (Wu) 86.2 (83.4) 88.7 (86.0) 87.4 (84.7)
English (Mallet) 91.8 (82.1) 92.4 (82.6) 92.1 (82.3)
HK Hansards Chinese (Wu) 91.4 (88.5) 89.1 (87.3) 90.2 (87.9)
English (Mallet) 93.5 (90.4) 94.3 (91.2) 93.9 (90.8)
Computer Chinese (Wu) 82.7 (81.4) 87.9 (86.5) 85.2 (83.9)
English (Mallet) 76.6 (72.9) 88.9 (85.2) 82.3 (78.6)
Table 10
The superiority of our joint model on three different domains indicated by type-insensitive
(type-sensitive) performance (those significant entries are marked in comparison with baseline).
Different domains Model P (%) R (%) F (%)
News (Table 3) Baseline 77.1 (67.1) 79.7 (69.8) 78.4 (68.4)
Proposed Model 85.9 (80.5) 88.4 (83.0) 87.1 (81.7)
HK Hansards Baseline 86.3 (83.3) 87.1 (84.1) 86.7 (83.7)
Proposed Model 88.2 (86.5) 89.1 (87.3) 88.6 (86.9)
Computer Baseline 69.4 (66.1) 80.3 (77.1) 74.5 (70.3)
Proposed Model 75.5 (72.4) 86.2 (83.1) 79.6 (76.5)
in Table 7). This is because the NE pair inclusion rate is usually dominated by the
weaker side.
4.5 Effectiveness of the Proposed Model Across Different Domains
To test the effectiveness of the joint model across domains, we compare the baseline
and our joint model on three different domains (News, HK Hansards, and Com-
puter Technology). To do this, two other test sets are selected from HK Hansards
(LDC2004T08) and from the computer domain (training data in CWMT08),14 respec-
tively (the test set used in the previous sections is from the News domain). Each of
these new test sets also includes 300 randomly selected sentence pairs.
Table 9 shows the initial NE recognition performance across those three different
domains. Also, it is clear from Table 10 that our joint model outperforms the baseline in
all three domains, which indicates that the advantage of our joint model holds over
various domains. On the other hand, the smaller improvement observed in the HK
Hansards domain might be due to the possibly easier task of initial NE recognition and
NE alignment.15 (Note that the baseline performance in this domain is much higher
than others?with an NE alignment type-sensitive F-score of 83.7% compared with
14 http://nlpr-web.ia.ac.cn/cwmt-2008.
15 Note that 40.3% sentence pairs in the HK Hansards corpus contains only one NE pair (alignment would
be trivial in this case); this ratio is 15.7% and 27.0% for News and Computer domains, respectively.
246
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 11
Comparison between a ME framework and the derived model on the same test set.
Model Data set?Size 400 4,000 40,000 90,412
ME Framework (Maxent) 38.9 (0%) 51.6 (0%) 63.8 (0%) 69.5 (0%)
ME Framework (YASMET) ?2.4 (?6.2%) ?1.2 (?2.3%) ?1.2 (?1.9%) ?1.6 (?2.3%)
Weighted-Joint-Model +2.6 (+6.9%) +3.5 (+6.9%) +3.4 (+5.3%) +2.9 (+4.2%)
68.4% and 70.3% in News and Computer domains, respectively). Therefore, those novel
features of our joint model are not crucial in easy cases.
4.6 Maximum Entropy Framework with Primitive Features
We propose and derive the model described previously in a principled manner. One
might wonder, however, whether it is worthwhile to derive such a model after all related
features have been proposed, as all proposed features can also be directly integrated
into the well-known maximum entropy (ME) framework (Berger, Della Pietra, and
Della Pietra 1996) without making any assumptions. To show that not only features,
but also the adopted model contributes to performance improvement, we build an
ME model that directly adopts all primitive features mentioned previously as its input
(including the internal component alignment-pair, initial and final NE type, NE bigram-
based string, and left/right distance), without involving any related probabilities
derived in the proposed model.
Because an ME approach can be trained only on linked NE pairs, those sentence
pairs that include at least one NE pair are first extracted from Training Set I. A total of
90,412 sentence pairs are obtained, as some sentence pairs only have either Chinese or
English NEs, and 298,302 NE pairs are identified. This ME method is implemented with
the YASMET16 package, and is tested under various training-set sizes (400, 4,000, 40,000,
and 90,412 sentence pairs). Because the NEs of the bilingual NE pair list (Training Set II)
do not contain their corresponding sentences, the ME approach lacks the necessary
context to extract specific ME features and hence this list is left out of our training data
for both the baseline ME model and our joint model.
In order to compare different ME approaches, we also try Zhang?s Maxent pack-
age17 with five classes (i.e., PER, LOC, ORG, Incorrect-Boundaries, Correct-Boundaries-
Incorrect-Type). A five-class approach outperforms a three-class approach (YASMET)
in this case (it has many more features as well). Table 11 shows only the type-sensitive
F-scores evaluated on the same test set to save space. The data within the parentheses
are relative improvements, and entries in bold indicate that the performance of the
derived model is statistically better than that of the ME models.
The improvement indicated in Table 11 clearly illustrates the benefit of deriving the
model. Because a reasonably derived model not only shares the same training set with
the primitive ME approach, but also enjoys the additional knowledge introduced by
the human researcher (i.e., the assumptions/constraints implied by the model), it is not
surprising that a good model does perform well, and the relative improvement becomes
more noticeable when the training set becomes smaller.
16 http://www.fjoch.com/YASMET.html.
17 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html.
247
Computational Linguistics Volume 39, Number 2
When we take a closer look at the 18 instances where our model correctly identifies
the NE alignments and Maxent fails to do so on the test set, we find that the internal
component alignment-pair feature in the Maxent approach to be dominant in causing
56% of the errors (10 out of 18). In contrast, our corresponding internal mapping prob-
ability P(cpa(n)|Mn, ewn,T) in Equation (5) makes the correct decisions 90% of the time
(9 out of 10). In fact, even P(cpa(n)|ewn,T) (the simpler form) makes correct predictions
80% of the time (8 out of 10).
One example of the ten errors made by Maxent is ???????? (Bolivia holds),
which is incorrectly linked to Bolivia holds as a NE pair by the ME approach, whereas
our model correctly aligns ?????? with Bolivia. This is because ?????? is
transliterated into Bolivia, but ???? is translated into holds. Given T = LOC, the in-
ternal mapping probability P(cpa(n)|ewn,T) thus disfavors the translation mode between
???? and hold within an LOC NE pair, and prefers the correct result. This example
illustrates the utility of the explicit dependency constraint imposed by the model, which
is not possible in the ME approach.
5. Discussion and Error Analysis
Although the proposed model substantially improves alignment performance, errors do
remain. Therefore, we would like to know what the limitations of the proposed model
are, and what kinds of problems still remain?an essential component in finding future
directions for further improvements. In the test set, a total of 718 NE reference-pairs and
739 aligned NE pairs are generated from the proposed joint model (MERT-W version).
Among the generated NE pairs, there are 104 (out of 739) boundary errors (regardless of
their re-assigned types), or 14.1%. Also, among the remaining 635 NE pairs with correct
boundaries, 41 (6.5%) are re-assigned to the incorrect NE type. Boundary identification,
therefore, is still a crucial problem.
Before investigating the errors made by MERT-W, we would like to understand its
limits. As mentioned in Section 3.3, the inclusion rate of those desired NE pairs (within
the Cartesian product of expanded candidate sets CK1 and E
K
1 ) is the upper bound for
the system adopted in the final selection stage, which in the current setting is 95.3%.
In comparison, the type-insensitive F-score of MERT-W is 87.1%, indicating that there
is still a significant 8.2% scope for improvement, even though a great improvement has
already been made over the baseline system. We examine this gap and propose solutions
to address the errors in the following section.
5.1 Classification of Type-Insensitive NE Pair Errors
There are 111 type-insensitive NE pair errors in the test set (104 boundary errors plus 7
others not included in the output list due to missing anchors), and these can be classified
into the following six main categories.
(I) Reference Inconsistency (11%): The NE references from Chinese and English
are not correctly matched, which rules out the possibility of generating the
correct NE pair.
(II) Missing Anchor (14%): Although the NE reference is consistent, not all their
associated NE anchors are generated in the initial recognition stage, which
cannot be remedied by the expansion strategy adopted in this article.
248
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
(III) Over-generating Anchors (10%): Similarly, additional spurious NE anchors
are also generated in the initial recognition stage, and result in incorrect
NE pairs.
The remaining cases with correct corresponding anchors are further classified as
follows.
(IV) Inconsistent Components (12%): Although their corresponding anchors are
correctly generated, the internal components of those reference pairs are
originally unmatched due to deletions or insertions occurring in the NE
translation. For example, in the reference {???????::[Berlingen]}, the
Chinese component ??? (town) is originally unmatched because its
English correspondent town does not exist in the given English sentence.
Therefore, only one incorrect pair {??????::[Berlingen]} is generated.
Other cases with all matched components are further classified in the
following.
(V) Expansion Limitation (5%): Even though all the internal components are
matched, the desired candidate (i.e., the reference) is still not covered by the
candidate set after its expansion.
(VI) Others (48%): Even if all internal components are matched and their
references are also included in the candidate set, some errors still remain,
mainly due to the limitations of the current model. These cases account for
the majority of the errors that will be further analyzed subsequently in the
article.
Table 12 shows the distribution of the six defined categories. {?CNE?::[ENE]} is
a specified NE pair, and the unmatched components are underlined. The numbers in
Table 12
Distribution of various error categories (type-insensitive).
Consistency
Problem
Anchor
Problem
Error
Categories
Reference NE pair Initially
Recognized NEs
Final Output Percentage
Inconsistent
References
(11%)
NA (I) Reference
Inconsistency
?????::[ ];
??::[Northam]
CNE: ????
ENE: [Northam]
??
??::[Northam]
11% (12)
Consistent
References
(89%)
Incorrect
Anchors
(24%)
(II) Missing
Anchor
????::[ASEAN] CNE: ?? ENE:
[ASEAN]
No such
alignment
14% (16)
(III) Over-
generating
Anchors
?????::[Lee
Nam-shin]
CNE: ?????
ENE: [Lee
Nam-shin];
[South]
???::[South] 10% (11)
Correct
Anchors
(65%)
(IV) Inconsistent
Components
?????
??::[Berlingen]
CNE: ?????
ENE: [Berlingen]
???
??::[Berlingen]
12% (13)
(V) Expansion
Limitation
(Matched
Components,
Excluded
Reference)
????????
???
??::[British
Pharmaceutical
Firm
GlaxoSmithKline]
CNE: ????; ??
???; ?????
ENE: [British
Pharmaceutical];
[Firm
GlaxoSmithKline]
??
??::[British]
5% (6)
(VI) Others
(Matched
Components,
Included
Reference)
?????::[South
and North
Koreas]
CNE: ??? ENE:
[North Koreas]
??
??::[North
Koreas]
48% (53)
249
Computational Linguistics Volume 39, Number 2
parentheses in the last column denote the number of NE pairs of the corresponding
category. Among all categories, Category (I) errors (Reference Inconsistency, 11%) are
irrelevant to the alignment model, and are attributed to the asymmetrical distribution
of bilingual NEs (corresponding NEs might sometimes be missed or replaced by the
pronoun it). As illustrated in Table 12, CNE ????? (Fossett) is initially recognized as
???? and finally linked to an irrelevant ENE Northam (??). This occurs because their
corresponding counterparts Fossett and ???? do not appear in the original sentence
pair, and our alignment model assumes that the linking between NEs is a one-to-
one mapping. One possible simple solution would be to set a minimal threshold on
alignment scores that filters out such spurious linking. This may introduce the risk that
some correct NE pairs might be pruned away at the same time, however.
Category (II) errors (Missing Anchor, 14%) are due to the absence of the associated
NE anchors in the initial recognition stage. As an example in Table 12, the corresponding
anchor of the Chinese NE ???? (ASEAN) is not initially identified. Because each
candidate set is generated from the given anchor, a missing anchor implies that its
associated candidate set will not exist, thereby making it impossible to generate the
corresponding NE pair. Although increasing the number of output anchors generated
from the initial recognition stage can relieve this problem, doing so makes the
subsequent alignment task harder. Additionally, the spurious anchors generated might
introduce even more errors.
The errors in Category (III) (Over-generating Anchors, 10%) are due to spurious
anchors generated in the initial recognition stage. For instance, the CNE ????? is
originally aligned with the ENE Lee Nam-shin by transliteration. A spurious ENE South is
also identified in the initial stage, however. This spurious ENE South is then incorrectly
linked to a virtual CNE with the highest score, ??,? which is a sub-string of the desired
CNE ????,? and also a Chinese translation for south. This prevents the correct NE
pair from being generated. Both missing and over-generating anchor problems are
largely dependent on the NE recognition toolkits adopted in the initial stage. Using the
current expansion strategy, the initial NE recognizers with lower recall (or precision)
tend to result in worse NE-pair recall (or precision) in the final alignment stage.
Category (IV) errors (Inconsistent Components, 12%) are caused by internal com-
ponents within NEs that were not originally matched. Because words in NEs are not
always translated literally, there are insertions and deletions during NE translation.
As an example, shown in Table 12 and illustrated previously, the incorrect result
{??????::[Berlingen]} is generated for its reference {???????::[Berlingen]}, as
its Chinese component ??? (town) is originally unmatched. In the worse case, those
unmatched components could interleave with matched components within the NE pair,
and thus prevent some matched components from being included. For example, in
the reference {?????????::[European Commission]}, both the Chinese compo-
nents ??? (alliance) and ???? (execution) have no counterparts, and they would have
prevented the matched portion {?????::[Commission]} from being included in the
final output. As a result, only {???::[European]} is eventually extracted. To tackle this
problem of component insertions/deletions that sometimes occur in English?Chinese
translation, the alignment model should be further enhanced to allow the component to
be linked to an empty element, NULL. Introducing this freedom, however, might have
the side effect of including additional spurious Chinese characters (or English words).
Further study is required to justify this idea; given that this category accounts for only
12% of the errors, we propose to defer this for later studies.
Furthermore, Category (V) errors (Expansion Limitation, 5%) are caused by the
problem that the desired candidate (i.e., reference) is excluded during the candidate
250
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
set expansion stage. Table 12 shows that the final output of the reference {??????
???????::[British Pharmaceutical Firm GlaxoSmithKline]} is {????::[British]}.
This reference has three Chinese initial anchors: ???? (British), ????? (Glaxo) and
????? (SmithKline), and it also has two English initial anchors: British Pharmaceuti-
cal and Firm GlaxoSmithKline. Because only four characters are allowed for boundary
enlarging/shrinking for Chinese anchors (three words for English anchors), the refer-
ence CNE is beyond the scope of any Chinese initial anchor during the expansion stage.
Therefore, it could not be included in the candidate set for final selection. In addition,
the adjacent Chinese component ????? could not be recovered due to the translation
re-ordering of the Chinese components?its counterpart Glaxo is far apart from British
in the given sentence. Similarly, the adjacent English words Pharmaceutical Firm could
not be recovered, as its counterpart ???? is far apart from ???.? Although loosening
the constraint during the expansion stage can increase the reference coverage, it must
be weighed against the corresponding lower precision.
Finally, for those NE pairs with aligned components and included references, the
proposed model still makes a significant number of mistakes. These kinds of errors,
Category (VI) (Others, 48%), account for the largest portion among all errors. There-
fore, they are further hierarchically classified in Table 13 according to their associated
transformation types and origins.
The incorrect NE pairs with aligned components and included references (i.e., [VI]
Other Category in Table 12) are first classified by their corresponding transformation
types: (VI.A) Abnormal Transformation (27%), whose transformation types are not as-
sumed by the model (i.e., neither normally translated nor normally transliterated); and
(VI.B) Normal Transformation (21%), whose components are either normally translated
or normally transliterated. A detailed explanation is given as follows.
Table 13
Distribution of Category (VI) error classes (type-insensitive).
Transformation
Type
Classes Reference NE pair Initially Recognized NEs Final Output Percentage
(VI.A)
Abnormal
Transformation
(27%)
(VI.A.1)
English
Acronym
??????
?::[ICC]
CNE: ????????;
?????
ENE: [ICC]
?????::[ICC] 11% (12)
(VI.A.2)
Chinese
Abbreviation
????
??::[Peace-
keeping
Troop]
CNE: ????
ENE: [Peace-keeping
Troop]
????::[Troop] 8% (9)
(VI.A.3)
Irregular
Translation
????::[Akihito] CNE: ???? ENE:
[Akihito]
????::[Hirohito] 8% (9)
(VI.B)
Normal
Transformation
(21%)
(VI.B.1) Bias
from NE
likelihoods
Translation:
?????
::[South and
North Koreas]
Transliteration:
??????
::[Beatrix]
Translation:
CNE: ???
ENE: [North Koreas]
Transliteration:
CNE: ????; ????
ENE: [Beatrix]
Translation:
????
::[North Koreas]
Transliteration:
?????
::[Beatrix]
18% (20)
(VI.B.2) Bias
from
Bilingual
Probabilities
Translation:
?????
::[World Cup]
Transliteration:
??????
::[Islamabad]
Translation:
CNE: ?????
ENE: [World Cup]
Transliteration:
CNE: ??????
ENE: [Islamabad]
Translation:
???????
::[championship
at World Cup]
Transliteration:
???????
::[Islamabad]
3% (3)
251
Computational Linguistics Volume 39, Number 2
(VI.A) Abnormal Transformation (27%): This category includes transformation
types that are not assumed by our alignment model. It can be further divided into
three classes according to their origins: (VI.A.1) English Acronym (11%), whose ENE is an
acronym; (VI.A.2) Chinese Abbreviation (8%), whose CNE is an abbreviation; and (VI.A.3)
Irregular Translation (8%), whose components are transformed neither semantically nor
phonetically. These cases are interesting and are illustrated herein.
In the row (VI.A.1) (English Acronym, 11%) of Table 13, a Chinese NE ????
???? (International Criminal Court) is tagged as ???????/ORG,? whereas
its English counterpart is the acronym ICC. Linking ???????? to ICC is thus
beyond the ability of our model. On the other hand, Chinese NEs are also occasionally
abbreviated. For example, in Class (VI.A.2) (Chinese Abbreviation, 8%), ???? is the
Chinese abbreviation of ?????? (Peace-keeping), which is also difficult to align
to its English counterpart. Such acronym and abbreviation cases are not rare in NE
translation. We believe that an expansion table (or even anaphora analysis) for acronyms
and abbreviations can help handle such issues.
It is also known that some loanwords or out-of-vocabulary terms are translated
neither semantically nor phonetically. As an example for Class (VI.A.3) (Irregular Trans-
lation, 8%), CNE ???? (which is the name of a Japanese emperor, and consists of
Japanese kanji characters) is incorrectly linked to an English word Hirohito (whose
Chinese translation should be ????), although it should be linked to ENE Akihito.
In this example, the Japanese kanji ???? is directly adopted as the corresponding
Chinese characters (as those characters are originally borrowed from Chinese), which
would be pronounced as ming-ren in Chinese and thus deviates significantly from the
English pronunciation of Akihito. Therefore, it is translated neither semantically nor
phonetically. This phenomenon mainly occurs in loanwords or out-of-vocabulary terms
and the model would have to be extended to cover those new conversion types. Such an
extension is very likely to be language-pair dependent (e.g., with an additional Japanese
phonetic table for cases such as the given example), however.
(VI.B) Normal Transformation (21%): Components of this category are translated
normally. It can be further divided into two classes according to their sources: (VI.B.1)
Bias from NE likelihoods (18%), which prefers the incorrect NE pair scope due to its
associated monolingual likelihood scores, and (VI.B.2) Bias from Bilingual Probabilities
(3%), which introduces extra non-NE words in the output due to high alignment scores
of words that are adjacent to the NE. Further illustration is given as follows.
As Class (VI.B.1) (Bias from NE likelihoods, 18%) shows in Table 13, the Chinese NE
????? and the English NE South and North Koreas are initially recognized as ???
(Korea) and North Koreas, respectively; the model finally chooses a partial alignment
result {????::[North Koreas]}. In this case, every component in either the CNE or
the ENE is well matched to its counterpart. Therefore, there is no significant differ-
ence among the alignment scores of various NE pair candidates with different scopes
(such as {???::[Koreas]}, {????::[North Koreas]}, and {?????::[South and North
Koreas]}, etc.)
The same situation also appears in the transliteration case. For example, the final
output of the reference {??????::[Beatrix]} is {?????::[Beatrix]}, and it has two
Chinese initial anchors: ???? (pronounced as bi-cui in Chinese) and ???? (ke-si).
According to the training data, both ??? (ke) and ???? (ke-si) could be aligned to
the English letter x. There is no significant difference, therefore, in the alignment scores
between {?????::[Beatrix]} and {??????::[Beatrix]}.
Because the NE alignment feature has only negligible discrimination power in these
cases (as described in Section 2.2), monolingual likelihood scores dominate the scope
252
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
preference. Addressing this shortcoming is beyond the capability of the alignment
model, and the adjacent contextual (non-NE) bigrams, to be proposed later, can only
correct 5 of the 20 errors in this class. No easy and effective solution for this kind of
problem can currently be found.
Contrary to this case, Class (VI.B.2) (Bias from Bilingual Probabilities, 3%) accounts
for those cases where incorrect NE pairs are selected due to the bilingual align-
ment score. For example, the final output of the reference {?????::[World Cup]} is
{???????::[championship at World Cup]}. Although both desired CNE and ENE
have already been correctly recognized in the initial stage, the bilingual NE alignment
feature prefers to include the additional Chinese common noun ???? and an extra
English word championship, because they are a perfect mutual translation. At first glance,
it seems that we could use the lower casing of championship as a feature. Other refer-
ences with lower case words could also be found (e.g., {??????????::[WTO
ministerial meeting]}), however. Therefore, additional features such as their relative
positions are also required.
The same situation also occurs in transliteration. For example, the Chinese NE
?????? (yi-si-lan-bao, in Chinese pronunciation) and the English NE Islamabad
are both correctly recognized in the initial stage. The model chooses a longer align-
ment result {???????::[Islamabad]} in the final stage, however. In this case, the
Chinese character ??? (land, pronounced as di in Chinese) could also be phoneti-
cally aligned to syllable ?d? with high probability. Therefore, there is no significant
difference in the alignment scores between ??????::[Islamabad] and ?????
??::[Islamabad]. We may need to resort to using a richer bilingual context (i.e., ??
?????????; In the map of Islamabad . . .) as features to resolve this issue. If its
Chinese adjacent contextual word ???? (map, pronounced as di-tu) could be aligned
to the corresponding English word map, and given that ???? and map are common
nouns in their respective languages, it is possible to determine that this extra Chinese
character ??? should not be linked to the ENE Islamabad.
Addressing these problems requires that both translation and transliteration models
be more complex and must use additional features (possibly knowledge-rich features).
Because this class accounts for only 3% of the errors, we leave the problem for future
work.
5.1.1 Features Contributing to Boundary Errors. Among the 111 alignment errors analyzed,
76 of them18 have their references covered by the expanded candidate set. The scores
of their associated features are then further inspected to determine which features
contribute to the errors. This is assessed by counting the number of times (denoted
by #Worst) that a specific weighted feature-score gets the worst difference when those
incorrect NE pairs are compared to their corresponding references. A large #Worst
would imply that this feature should get more attention in pursuing further perfor-
mance improvement.
The top four related statistics are given in Table 14, which indicates that F1 (Nor-
malized TS/TL Transformation) is the most dominant feature in making those errors.
Following this, F2 (Normalized Translation Mode), F7 (Normalized Chinese Bigram),
and F10 (Normalized English Bigram) are on the second tier. Both F1 and F2 are related
to alignment, which coincides with our observation that alignment-related Categories
18 This is the number of those entries under Categories (III), (IV), and (VI) in Table 12 after subtracting one
(76 = 11 + 13 + 53 ? 1), as one reference in Category (III) cannot be found after expansion.
253
Computational Linguistics Volume 39, Number 2
Table 14
Top four worst-case statistics of features for NE boundary errors.
Features #Worst
F1: [
?N
n=1 P(cpa(n)|Mn, ewn,T)]
1/N (Normalized TS/TL Transformation) 17
F2: [
?N
n=1 P(Mn|ewn, T)]
1/N (Normalized Translation Mode) 10
F7: [
?L
l=1 P(ccl|ccl?1,T)]
1/L (Normalized Chinese Bigram) 10
F10: [
?N
n=1 P(ewn|ewn?1,T)]
1/N(Normalized English Bigram) 9
(i.e., IV, VI.A, and VI.B.2) occupy the largest portion of errors (62% of 76 inspected
errors). The errors dominated by F7 and F10 are further discussed as follows.
Among the ten errors dominated by F7, except for three (in which one is due to
a spurious anchor, and two are due to abnormal transformations), all others selected
the sub-strings of their corresponding CNE references. Furthermore, each selected sub-
string included the Chinese bigrams that appear more frequently than those within the
remaining sub-string (of its reference CNE). In other words, F7 tends to select only the
portion with high frequency bigrams when all related components are aligned. For
example, for the reference {??????????::[National Center for Tobacco-Free
Kids]}, only {????::[Center]} is extracted, as this Chinese bigram is frequently used
in various organization names. Further inspection of those seven cases (which prefer
the sub-string) reveals that three of them are with unmatched components (Category
IV); therefore only four of them are, in fact, due to the problem of F7.
On the other hand, among the nine errors caused mainly by F10, only three of them
chose the sub-strings of their corresponding ENE reference, and the remaining six errors
selected the strings unrelated to the reference due to spurious anchors and an acronym.
It therefore seems that different languages possess different error patterns.
The problem of preferring a more frequent sub-string cannot be solved by normaliz-
ing related bigrams. This is because the current bigram model does not consider the im-
plied restriction on the context surrounding the given CNE. In other words, a given CNE
also implies that its left and right adjacent characters should not be a part of CNE (or its
left and right adjacent characters must be in a non-NE region). In our data set, two adja-
cent non-NE Chinese characters (or words for English) are found to be sufficient for both
left and right contexts. Therefore, the following additional terms are further proposed to
take care of this issue: P(cc?1|cc0,T) ? P(cc0|cc1,T) ? P(ccL+2|ccL+1,T) ? P(ccL+1|ccL,T),
where ccL1 is the given CNE, cc0 and ccL+1 are its left and right adjacent non-NE char-
acters, respectively. This formula can be easily derived from P(C|Cb,Tc,T,Sc), similar
to Equation (8). The derivation also applies to English. To test this supposition, the
related experiment (Exp4 [MERT-W, N+Full Model] specified in Table 3) is updated as
Experiment 5 with the probability features shown here.
Exp5: This experiment (named MERT-W, N-Full Model+Contextual-Bigram, and
denoted by MERT-W-CB) replaces [
?L
l=1 P(ccl|ccl?1,T)]
1
L in the original Exp4 with
P(cc?1|cc0,T) ? P(cc0|cc1,T) ? P(ccL+2|ccL+1,T) ? P(ccL+1|ccL,T) ? [
?L
l=1
P(ccl|ccl?1,T)]
1
L
The same is done for English.
Table 15 shows the performance on the test set (data from Exp4 are also listed for
comparison). The entries in bold indicate statistically significant improvements over
254
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 15
Effect of adjacent contextual (non-NE) bigrams on the test set.
Model P(%) R(%) F(%)
Exp4 (MERT-W) 85.9 (80.5) 88.4 (83.0) 87.1 (81.7)
Exp5 (MERT-W-CB): Add Contextual- 86.7 (81.7) 89.3 (84.1) 88.0 (83.0)
Bigram to Original N-Bigram
their counterparts. Results show that the performance has indeed improved, and six of
the targeted seven cases (four CNE and three ENE errors, as mentioned previously)
have been corrected (the remaining error is due to data sparseness and cannot be
corrected). According to coefficient weighting by MERT process, P(cc0|cc1,T) is more
important than P(cc?1|cc0,T), indicating that the closer a non-NE character is to the
given NE, the more influential it is. This observation confirms our intuition about the
context effect. A similar trend is also observed for other contextual non-NE characters.
5.2 NE Type Errors
In addition to the 111 boundary errors just analyzed, there are also NE type errors.
Among those 635 NE pairs with correct boundaries in the test set, there are 41 (6.5%)
NE type errors in Exp4 (MERT-W). Among them, 175 PER, 248 LOC, and 212 ORG
NE types are assigned. The associated confusion matrix of various NE types is shown
in Table 16 (the numbers within the parentheses are the relative ratios of their output
types). All except 5 of the 41 NE type errors originated from transliterated NE pairs (not
shown in the table). This is consistent with our observation that even a human annotator
finds it challenging to identify correct types for transliterated NE pairs in the absence of
context.
Table 16 shows that PER has the highest error rate, LOC follows as the second,
and ORG is a distant third. In addition, PER and LOC are the types that are most often
confused with one another. These observations match the distribution of transliterations
in each type (the transliteration mode ratios for PER, LOC, and ORG are 100%, 71.4%,
and 25.2%, respectively), as it is very difficult to determine the type when a NE is
transliterated without context.
To solve NE type errors originating from transliteration, the adjacent contextual
non-NE characters are also helpful. For example, {?????::[Myers]} is incorrectly
identified as LOC, when in fact it should have been PER in the context ???????
(president Myers). The left adjacent contextual bigrams ???? (president) should indicate
that the following NE is likely to be PER. Table 15 shows that an additional four type
errors are also corrected apart from the six boundary errors. Therefore, in comparison
with the original MERT-W, this new version (MERT-W-CB) gains more in type-sensitive
Table 16
Distribution of the NE type errors (MERT-W).
NE type Reference PER Type Reference LOC Type Reference ORG Type
Output Type = PER 153 (87.4%) 16 (9.1%) 6 (3.4%)
Output Type = LOC 14 (5.6%) 232 (93.5%) 2 (0.8%)
Output Type = ORG 0 (0%) 3 (1.4%) 209 (98.6%)
255
Computational Linguistics Volume 39, Number 2
F-score (an increase of 1.5%, from 81.5% to 83.0%) than in type-insensitive F-score
(an increase of 0.9%, from 87.1% to 88.0%).
The adjacent contextual non-NE characters have only limited power in disam-
biguating NE types, however. For instance, the reference {?????::[Kabul]} is incor-
rectly identified as PER, which should be LOC in the context ?????? (at Kabul).
Because ????? follows a preposition ??? (at) in the associated context, it indicates
that ????? is a location name. We can also easily find counterexamples, however,
such as ??????? (around Bush), in which ???? (Bush) is PER, not LOC. In
fact, both PER and LOC can be freely exchanged in this situation in either a Chinese
or English context. Therefore, more complicated syntactic or semantic information is
required in some cases involving transliteration.
Another interesting statistic not shown in Table 16 is the distribution of errors versus
initial NE types (i.e., Tc and Te) assigned in the first stage. Among 41 errors, 22 (54%)
have both incorrect Tc and Te, 16 (39%) have correct Tc but wrong Te, and only 3 (7%)
have correct Te but incorrect Tc. This distribution shows that Tc is more reliably assigned
in the first stage than is Te, which confirms the observation in Ji and Grishman (2006)
that English NE type assignment is more challenging.
5.2.1 Features Contributing to Type Errors. Similarly, the study for #Worst on weighted
feature-scores is also performed under MERT-W-CB (i.e., Exp5). There are 37 type errors
with correct boundaries, and we have examined that F4 (NE Type Re-assignment) is
the most dominant feature in making these errors. Because this feature LogP(T|Tc,Te)
(Equation [7]) always assigns an incorrect type when both Tc and Te are incorrect (11 out
of 13 F4 errors belong to this class), it is not surprising that it is ranked at the top
(22 [59%] out of a total of 37 errors have both incorrect Tc and Te). In addition, the feature
LogP(?A|T) in Equation (6) always prefers PER (which are always completely translit-
erated in the corpus) when all components in an NE are transliterated (i.e., ?A = 0).
Therefore, the feature is ranked second (31 out of 37 errors are complete transliterations,
although only 10 of them should have been assigned PER). It is found that all nine cases
in this category are not PER (only two of them are not transliterated), which further
supports our analysis. Solving these type errors requires that these two features be
conditioned on more features, and requires further study. Finally, the top four features in
making type errors are related to alignment (Equations [6] and [7]), which indicates that
monolingual lexicon information (both English and Chinese bigrams) is more reliable
in deciding NE type.
6. Applications of the Proposed Model
It would be interesting to know how the proposed model performs in real applications.
Because MERT-W performs best in our tests, it is adopted in this study on real applica-
tions. Section 6.1 presents the effectiveness of improving NE recognition, and Section 6.2
shows how the improved NE recognition can be used in learning a monolingual NE
recognition model (also NE translation table/models) in a semi-supervised manner.
6.1 On Improving Monolingual NE Recognition
As explained in Section 2.1, the alignment result can also be used to refine the initially
recognized NEs. The improvements that MERT-W made in refining the boundaries and
NE types of those initially recognized Chinese/English NEs are shown in Tables 17 and
18, respectively. For comparison, the rows associated with the initial recognizers and the
256
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 17
Type-insensitive improvement for Chinese/English NER.
NE type Model P (%) R (%) F (%)
PER Initial 85.9/91.3 89.3/91.1 87.6/91.2
Baseline 88.1 (+2.2)/ 92.9 (+1.6) 89.8 (+0.5)/ 92.2 (+1.1) 89.0 (+1.4)/ 92.6 (+1.4)
MERT-W 90.3 (+4.4)/ 94.5 (+3.2) 90.5 (+1.2)/ 93.0 (+1.9) 90.4 (+2.8)/ 93.8 (+2.6)
LOC Initial 90.9/93.6 91.4/93.4 91.1/93.5
Baseline 91.8 (+0.9)/ 94.7 (+1.1) 91.8 (+0.4)/ 94.2 (+0.8) 91.8 (+0.7)/ 94.5 (+1.0)
MERT-W 94.2 (+3.3)/ 95.8 (+2.2) 93.2 (+1.8)/ 95.2 (+1.8) 93.6 (+2.5)/ 95.5 (+2.0)
ORG Initial 81.1/88.9 83.9/87.7 82.5/88.3
Baseline 84.4 (+3.3)/ 90.7 (+1.8) 86.6 (+2.7)/ 89.2 (+1.5) 85.5 (+3.0)/ 90.0 (+1.7)
MERT-W 86.8 (+5.7)/ 92.8 (+3.9) 88.7 (+4.8)/ 89.9 (+2.2) 87.8 (+5.3)/ 91.4 (+3.1)
ALL Initial 86.0/92.2 88.7/92.2 87.3/92.5
Baseline 88.8 (+2.8)/ 93.6 (+1.4) 89.6 (+0.9)/ 93.9 (+1.1) 89.2 (+1.9)/ 93.7 (+1.2)
MERT-W 91.4 (+5.4)/ 95.2 (+3.0) 91.1 (+2.4)/ 94.7 (+1.9) 91.2 (+3.9)/ 94.9 (+2.4)
alignment baseline systems are also given in the two tables (as before, the entries in bold
indicate that differences are statistically significant). In addition, figures in parentheses
indicate the corresponding differences in performance compared to the initial version
(shown in Table 2).
Table 17 shows that both the baseline and MERT-W systems have significantly im-
proved the initial NE recognition type-insensitive results for both Chinese and English.
It also shows that MERT-W significantly outperforms the baseline. In particular, Chinese
ORG is observed to yield the largest improvement among NE types in both Chinese and
English, which matches our previous observations that the boundary of a Chinese ORG
is difficult to identify using only the information from the Chinese sentence.
The type-sensitive results are given in Table 18, which shows that MERT-W also
significantly improves the initial NE recognition results for both Chinese and English.
Table 18
Type-sensitive improvement for Chinese/English NER.
NE type Model P (%) R (%) F (%)
PER Initial 80.2/79.2 87.7/85.3 83.8/82.1
Baseline 79.4 (?0.8)/ 78.6 (?0.6) 88.1 (+0.4)/ 86.6 (+1.3) 83.5 (?0.3)/ 82.4 (+0.3)
MERT-W 85.6 (+5.4)/ 85.6 (+6.4) 89.9 (+2.2)/ 87.9 (+2.6) 87.7 (+3.9)/ 86.7 (+4.6)
LOC Initial 89.8/85.9 87.3/81.5 88.5/83.6
Baseline 90.5 (+0.7)/ 86.5 (+0.6) 83.6 (?3.7)/ 80.4 (?1.1) 86.9 (?1.6)/ 83.3 (?0.3)
MERT-W 93.8 (+4.0)/ 89.3 (+3.4) 87.1 (?0.2)/ 84.1(+2.7) 90.3 (+1.8)/ 86.6 (+3.0)
ORG Initial 78.6/82.9 82.8/79.6 80.6/81.2
Baseline 79.5 (?0.9)/ 82.2 (?0.7) 80.9 (?1.9)/ 80.1 (+0.5) 80.2 (?0.4)/ 81.1 (?0.1)
MERT-W 85.6 (+7.0)/ 86.8 (+3.9) 88.4 (+5.6)/ 88.7 (+9.1) 87.0 (+6.4)/ 87.7 (+6.5)
ALL Initial 83.4/82.1 86.0/82.6 84.7/82.3
Baseline 83.2 (?0.2)/ 82.2 (+0.1) 83.9 (?2.1)/ 82.3 (?0.3) 83.5 (?1.2)/ 82.2 (?0.1)
MERT-W 88.7 (+5.3)/ 87.3 (+5.2) 88.4 (+2.4)/ 86.6 (+4.0) 88.6 (+3.9)/ 86.9 (+4.6)
257
Computational Linguistics Volume 39, Number 2
Note that English ORG yields the largest gain among NE types in both Chinese and
English, again supporting our earlier observation that an English ORG cannot be easily
identified when only the English sentence is available. It must be noted that the baseline
alignment model deteriorates the original NE recognition in overall performance (even
though it can correct some NE initial boundary errors as shown in Table 17), because it
does not use the features/constraints proposed in the joint model.
6.2 On Learning NE Recognition Models via Semi-Supervised Learning
In many NLP applications, the associated process will be considerably simplified if
the included NEs can be identified first. Therefore, it is important to have a good NE
recognizer that has been well trained. Various domains frequently have different sets
of NEs, however, and new NEs also emerge over time. We thus need to periodically
update the NE recognition model (also the NE translation table/model, if it is for MT),
which necessitates the need to ensure short training times (including set-up time and
human effort). This requirement can be addressed well in a semi-supervised learning
set-up where parameters/tables are learned from a large unlabeled corpus with a small
(albeit human) annotated seed set.
Under the semi-supervised learning framework, however, maximizing likelihood
does not imply a minimizing of error rate at the same time. Without additional con-
straints, a monolingual NE model is usually unable to converge to the desired point in
the parameter space. On the other hand, as shown in the previous section, the align-
ment module can further refine the initially recognized NEs with additional mapping
constraints from the other language. The proposed joint model thus can be used to train
the monolingual NE recognition model via semi-supervised learning on a large un-
labeled bilingual corpus. In other words, when semi-supervised learning is conducted
for learning the NE recognition model, MERT-W is expected to guide the search pro-
cess for convergence towards the human annotation. This advantage is important for
regularly updating the NE recognition and translation table/models.
We now outline our semi-supervised learning procedure as follows.
(1) A small pre-labeled corpus acts as seed data. Based on these seed data,
train the Chinese/English NE recognition toolkit (described in Section 4.1)
and the adopted NE alignment model.
(2) Perform the Chinese/English NE recognition toolkits and the NE
alignment model, trained in Step (1), on a large unlabeled
(sentence-aligned) bilingual corpus to report those NE pairs that they
identify.
(3) Denote the located NE pairs as correctly labeled and combine them with
the seed data as new labeled training data.
(4) Re-train the Chinese/English NE recognition toolkit and also re-train the
NE alignment model on the newly labeled training data.
(5) Repeat Steps (2) through (4) until convergence. The final Chinese/English
NE recognizer and the NE alignment model are compared to their initial
versions.
Because the adopted Chinese NE recognizer (Wu, Zhao, and Xu 2005) cannot be
re-trained (because it is not an open source toolkit), only the English NE recognizer and
258
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 19
English NE recognition on test data after semi-supervised learning.
Model Seed Size (sentence pairs) 100 400 4,000 40,000
Initial-NER 36.7 58.6 71.4 79.1
(0%) (0%) (0%) (0%)
NER-Only ?2.3 ?0.5 ?0.3 ?0.1
(?6.3%) (?0.8%) (?0.4%) (?0.1%)
NER+Alignment-Baseline +4.9 +3.4 +1.7 +0.7
(+13.4%) (+5.8%) (+2.4%) (+0.9%)
NER+Alignment-Weighted +10.7 +8.7 +4.8 +2.3
(+29.2%) (+14.8%) (+6.7%) (+2.9%)
the alignment model are updated during training iterations. In our experiments, 50,412
sentence pairs are first extracted from Training Set I as unlabeled data. Various labeled
data sets are then extracted from the remaining data as seed corpora with different sizes
(100, 400, 4,000, and 40,000 sentence pairs). The test set is still the same 300 sentence
pairs that were adopted previously.
Table 19 shows the type-sensitive F-score of English NER on the test set at conver-
gence.19 The Initial-NER in Table 19 indicates the initial performance of the NER model
re-trained from different seed corpora. Three different approaches are tested: (1) English
NER only (NER-Only), (2) the alignment baseline model (NER+Alignment-Baseline),
and (3) our weighted joint model MERT-W (NER+Alignment-Weighted). The first case
performs semi-supervised learning only on English data without involving alignment,
whereas the last two cases include alignment, and both the English NER model and
the alignment model are re-trained during iterations. The numbers in parentheses
show relative improvements over Initial-NER. The entries in bold indicate statistically
significant improvements over Initial-NER.
As Table 19 shows, using the NER model alone, the performance may drop after
convergence. This is because maximizing likelihood does not imply minimizing the
error rate. With additional mapping constraints from the other language, however, the
alignment module can guide the search process to converge to a more desirable point in
the parameter space. It must be noted here that the contribution of additional constraints
increases with smaller seed corpora, because constraints become more important when
the labeled data set is smaller.
Table 20 shows only the type-sensitive F-score of NE pair alignment on the test set
before and after convergence due to space constraints. Two different models are tested:
(1) the alignment baseline model (NER+Alignment-Baseline), and (2) our weighted joint
model (NER+Alignment-Joint). The data in parentheses indicate relative improvements
over the performance before training. The entries in bold indicate statistically significant
improvements over the model before training.
As Table 20 demonstrates, the alignment performance can also be improved with
semi-supervised learning. Note that the improvement is greater on smaller data sets,
which is common in most semi-supervised learning tasks.
19 The iteration process will stop when the last two consecutive iterations share more than 99.9% of their
output.
259
Computational Linguistics Volume 39, Number 2
Table 20
NE alignment on test data after semi-supervised learning.
Model Seed Size (sentence pairs) 100 400 4,000 40,000
NER+Alignment-Baseline 33.5 50.2 62.6 67.3
(0%) (0%) (0%) (0%)
NER+Alignment-Baseline (After) +4.2 +3.0 +1.8 +1.2
(+12.5%) (+6.0%) (+2.9%) (+1.8%)
NER+Alignment-Joint 38.1 56.3 67.7 72.5
(0%) (0%) (0%) (0%)
NER+Alignment-Joint (After) +10.1 +8.7 +6.0 +3.3
(+26.5%) (+15.5%) (+8.9%) (+4.6%)
As shown by Tables 19 and 20, the proposed joint model gains more from the
learning process in comparison with the alignment-baseline model, because informa-
tion from the aligned sentence is utilized more effectively. Results demonstrate that
the proposed joint model, combined with semi-supervised learning, offers significant
improvement for semi-automatically updating the NE recognition model and the NE
translation table. Additionally, the impact is greater when less time is available for
labeling seed data.
7. Related Work
There is significant work on identifying NEs within monolingual texts across languages,
such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999)
and Chinese (Chen et al 1998a; Sun, Zhou, and Gao 2003), to name a few. Various
approaches to identifying NEs have also been proposed, such as hidden Markov
models (Bikel et al 1997; Bikel, Schwartz, and Weischedel 1999), conditional random
fields (McCallum and Li 2003; Jiao et al 2006), modified transformation-based
learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al 2002),
AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised
learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features
including local information (e.g., token, part-of-speech) and global information (e.g.,
label consistency, context features) from monolingual resources have been adopted
(Krishman and Manning 2006; Zhou and Su 2006). In prior work on the use of bilingual
NE alignment for NE recognition, Huang and Vogel (2004) used an iterative process
to extract a smaller but cleaner NE translation dictionary and then used the dictionary
to improve the monolingual NE annotation quality. Ji and Grishman (2007) adopted
several heuristic rules for using bilingual-text information to correct NE recognition
errors.
In aligning bilingual NEs from two given NE lists, the NE translation model is
usually adopted. Typically, an NE is either transliterated or semantically translated. For
transliteration, Knight and Graehl (1998) were pioneers in adopting the probabilistic
model to align the components within an NE pair. Since then, similar approaches have
been applied to various language pairs such as English/Arabic (Stalls and Knight
1998), English/Chinese (Chen et al 1998b; Wan and Verspoor 1998; Lin and Chen
2002; Lee and Chang 2003; Lee, Chang, and Jang 2003; Gao, Wong, and Lam 2004;
260
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Pervouchine, Li, and Lin 2009), English/Japanese (Knight and Graehl 1998; Tsuji 2002),
and English/Korean (Lee and Choi 1997; Oh and Choi 2002, 2005). Moreover, Li,
Zhang, and Su (2004), and Li et al (2007) presented a joint source channel model for
transliteration, and automated the semantic transliteration process, which takes origin
and gender into account for personal names.
In contrast, research on automatic NE semantic translation is less common. Zhang
et al (2005) proposed a phrase-based context-dependent joint probability model for
semantic translation, which is similar to phrase-level translation models in statistical
MT (Zong and Seligman 2005; Hu, Zong, and Xu 2006). Chen, Yang, and Lin (2003) and
Chen et al (2006) studied formulation and transformation rules for English?Chinese
NEs. They adopted a frequency-based approach for extracting key words of NEs with or
without dictionary assistance and constructed transformation rules from the bilingual
NE corpus. Their studies focused on transformation rules with particular attention
to distinguishing translated parts from transliterated parts; the performance of rule-
application in NE translation was not described, however. Chen and Zong (2008) pro-
posed a chunk-based probabilistic translation model for organization names, although,
its application to person and location names has not been studied.
Because new NEs emerge from time to time and can be transformed in various ways
(translation, transliteration, or other abnormal types), the NE transliteration/translation
models mentioned usually lead to unsatisfactory results, especially for infrequently
occurring NEs. Recent studies have therefore focused on extracting new NE pairs from
either bilingual corpora or Web resources, so that the corresponding human translation
can be directly adopted (or used for training). To do this, however, NE alignment is an
essential tool. Due to the relatively poor quality of Web data, such alignment approaches
are usually limited unless significant effort is devoted to data cleaning; therefore, we do
not discuss these approaches.
To extract the NE pairs from bilingual corpora, all NE alignment approaches we
found in the literature are conducted after an initial NE identification stage so that
the complexity of the task can be reduced. The associated cost is that those initial NE
recognition errors propagate into the following alignment stage. Both symmetric (which
first identifies NEs in both languages) and asymmetric (which first identifies NEs in only
one language) strategies have been proposed to mitigate this problem (Moore 2003), and
are described here.
For the symmetric strategy, Huang and Vogel (2004) proposed to extract the NE
translation dictionary from the bilingual corpus, and then used it to improve the
NE annotation performance iteratively. Huang, Vogel, and Waibel (2003) described
a multi-feature NE alignment model to extract NE equivalences (with translation,
transliteration, and tagging features), from which a NE translation dictionary was then
constructed. Kumano et al (2004) proposed a method to extract English?Chinese NE
pairs from a content-aligned corpus. This approach tries to find the correspondences
between bilingual NE groups based on the similarity in their order of appearance in
each document. Additionally, an abridged version of our work has been presented in
our ACL-10 paper (Chen, Zong, and Su 2010). Among those symmetric approaches,
only Huang, Vogel, and Waibel and Chen, Zong, and Su adopt the expansion strategy,
described below.
For the asymmetric strategy, Al-Onaizan and Knight (2002) proposed an algorithm
to translate NEs from Arabic to English using monolingual and bilingual resources.
Given an Arabic NE, they used transliteration models (including a phonetic-based and
a spelling-based model), a bilingual dictionary, and an English news corpus to first
generate a list of English candidates, which were then re-scored by a Web resource.
261
Computational Linguistics Volume 39, Number 2
Moore (2003) developed an approach to learning phrase translations from a parallel
corpus based on a sequence of cost models. A maximum entropy model for NE align-
ment was presented in Feng, Lv, and Zhou (2004). Lee, Chang, and Jang (2006) proposed
to align bilingual NEs in a bilingual corpus by incorporating a statistical model with
multiple sources. Turning to comparable corpora, Shao and Ng (2004) presented a
hybrid method to mine new translations from Chinese?English comparable corpora,
combining both transliteration and context information. Sproat, Tao, and Zhai (2006)
investigated the Chinese?English NE transliteration equivalence within comparable
corpora.
Although these asymmetry strategies can prevent NE recognition errors on the
target side from affecting alignment, errors on the source side continue to propagate
to later stages. To reduce error propagation from both the source and the target, Huang,
Vogel, and Waibel (2003) proposed to first identify the NEs in both the source and target,
and then enlarge the obtained NE candidate sets for both languages before conducting
alignment. Based on the observation that NE boundaries are frequently identified in-
correctly, the enlarging procedure is done by treating the original recognition results as
anchors and then increasing the number of candidates by expanding or shrinking the
boundaries of those originally recognized NEs in both languages.
Our approach also adopts the expansion strategy. It differs from the works of Huang
et al (2003) and others in several ways, however. First, in all the alignment papers
mentioned here, the adopted probabilities are directly used as features for log-linear
combination or ME training without derivation. In contrast, our work fully derives a
probabilistic joint model, for both identification and alignment, in a principled way.
Second, unlike previous approaches that discard the information of initially identified
NE anchors after the anchors have been expanded, our approach uses this information
in the final selection process. Third, we propose new features, such as translation mode
and its ratio, boundary shifting distance, and contextual bigrams. Fourth, we introduce
a normalization step that removes the systematic bias preferring shorter NEs. Fifth, the
effect of each individual feature, the influence of adopting different NE recognizers, the
effectiveness across different domains, the effect of using a derived model (compared
to ME), and the effect of the alignment model in semi-supervised learning are studied.
Finally, the causes of alignment errors and type re-assignment errors are extensively
investigated and categorized.
8. Conclusion
This article develops a novel and principled model for jointly conducting NE recogni-
tion and alignment. To the best of our knowledge, this is the first work that formally
captures the interactions between NE recognition and NE alignment. The joint model
not only greatly improves NE alignment performance, but also significantly boosts NE
recognition performance.
Our experiments show that the new NE likelihoods are more effective than the
bigram model used in the baseline system. Moreover, both the translation mode ratio
and the entity type consistency constraint are critical in identifying the associated
NE boundaries and types, as evidenced by the 21.3% relative improvement on type-
sensitive F-score (from 68.4% to 83.0%) in our Chinese?English NE alignment task. The
superiority of the proposed model has been shown to hold over the various domains
tested.
Furthermore, the joint alignment model can also be used to refine the initially
recognized NEs. This is achieved by utilizing additional mapping information from the
262
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
other language. In our experiments, when semi-supervised learning is conducted to
train the adopted English NE model (with only 100 seed sentence pairs), the proposed
model greatly boosts the English NE recognition type-sensitive F-score from 36.7% to
47.4% (29.2% relative improvement) in the test set.
Finally, the proposed model does not utilize language-dependent features. For
example, Chinese characters and English words adopted in the model are visible units
in the given languages, and no language-dependent features, such as morpheme/part-
of-speech (or prefix/suffix), are used. In addition, the model does not use linguistic
rules or tree banks. Therefore, although our experiments are conducted on Chinese?
English language pairs, it is expected that the proposed approach can be applied to
other language pairs with little adaptation effort.
Acknowledgments
This research has been funded by the Natural
Science Foundation of China under grant
nos. 61003160 and 60975053 and supported
by the Hi-Tech Research and Development
Program (?863? Program) of China under
grant no. 2011AA01A207. Thanks are also
given to the authors? associate, Tao Zhuang,
for his great help on the publication version.
References
Al-Onaizan, Yaser and Kevin Knight.
2002. Translating named entities using
monolingual and bilingual resources. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
pages 400?408, Philadelphia, PA.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39?71.
Bikel, Daniel M., Scott Miller, Richard
Schwartz, and Ralph Weischedel.
1997. Nymble: A high-performance
learning name-finder. In Proceedings
of the Fifth Conference on Applied Natural
Language Processing, pages 194?201,
Washington, DC.
Bikel, Daniel M., Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm
that learns what?s in a name. Machine
Learning, 34(1?3):211?231.
Black, W. J. and Argyrios Vasilakopoulos.
2002. Language independent named
entity classification by modified
transformation-based learning and by
decision tree induction. In Proceedings
of the Sixth Conference on Natural Language
Learning, pages 159?162, Taipei.
Borthwick, A. 1999. A Maximum Entropy
Approach to Named Entity Recognition. Ph.D.
thesis, New York University.
Brown, Perer F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Carreras, X., L. Marquez, and L. Padro. 2002.
Named entity extraction using adaboost.
In Proceedings of the Sixth Conference on
Natural Language Learning (CoNLL-2002),
pages 167?170, Taipei.
Chen, Hsin-His, Yung-Wei Ding, Shih-Chung
Tsai, and Guo-Wei Bian. 1998a. Description
of the NTU system used for met2. In
Proceedings of the 7th Message Understanding
Conference (MUC-7), pages 121?129,
Fairfax, VA.
Chen, Hsin-His, S.-J. Huang, Y.-W. Ding,
and S.-C. Tsai. 1998b. Proper name
translation in cross-language information
retrieval. In Proceedings of the 17th COLING
and 36th ACL Conference, pages 232?236,
Montreal.
Chen, Hsin-His, W.-C. Lin, C. Yang,
and W.-H. Lin. 2006. Translating/
transliterating named entities for
multilingual information access. Journal
of the American Society for Information
Science and Technology (Special Issue on
Multilingual Information Systems),
57(5):645?659.
Chen, Hsin-His, Changhua Yang, and
Ying Lin. 2003. Learning formulation and
transformation rules for multilingual
named entities. In Proceedings of the
ACL 2003 Workshop on Multilingual and
Mixed-language Named Entity Recognition,
pages 1?8, Sapporo.
Chen, Stanley F. and Joshua Goodman.
1998. An empirical study of smoothing
techniques for language modeling.
Technical Report TR-10-98, Computer
Science Group, Harvard University,
Cambridge, MA.
263
Computational Linguistics Volume 39, Number 2
Chen, Yufeng and Chengqing Zong. 2008.
A structure-based model for Chinese
organization name translation. ACM
Transactions on Asian Language Information
Processing (TALIP), 7(1):1?30.
Chen, Yufeng, Chengqing Zong, and Keh-Yih
Su. 2010. On jointly recognizing and
aligning bilingual named entities. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics,
pages 631?639, Uppsala.
Chinchor, Nancy. 1998. Overview of
muc-7/met-2. In Proceedings of Message
Understanding Conference MUC-7,
pages 1?4, Fairfax, VA.
Cohen, William W. 2004. MinorThird: methods
for identifying names and ontological relations
in text using heuristics for inducing
regularities from data. Available at
http://minorthird.sourceforge.net.
Collins, Michael. 2002. Ranking algorithms
for named-entity extraction: Boosting and
the voted perceptron. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 489?496,
Philadelphia, PA.
Feng, Donghui, Yajuan Lv, and Ming Zhou.
2004. A new approach for English-Chinese
named entity alignment. In Proceedings of
the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2004),
pages 372?379, Barcelona.
Finkel, Jenny Rose, Trond Grenager, and
Christopher Manning. 2005. Incorporating
non-local information into information
extraction systems by Gibbs sampling.
In Proceedings of the 43nd Annual Meeting of
the Association for Computational Linguistics
(ACL 2005), pages 363?370, Ann Arbor, MI.
Freedman, D. A. 2005. Statistical Models:
Theory and Practice. Cambridge
University Press.
Gao, Jianfeng, Mu Li, Andi Wu, and
Chang-Ning Huang. 2005. Chinese
word segmentation and named entity
recognition: A pragmatic approach.
Computational Linguistics, 31(4):531?574.
Gao, Wei, Kam-Fam Wong, and Wai Lam.
2004. Transliteration of foreign names
for OOV problem. In Proceedings of the
1st International Joint Conference on
Natural Language Processing (IJCNLP),
pages 110?119, Sanya.
Hu, Rile, Chengqing Zong, and Bo Xu. 2006.
An approach to automatic acquisition of
translation templates based on phrase
structure extraction and alignment. IEEE
Transactions on Audio, Speech, and Language
Processing, 14(5):1656?1663.
Huang, Fei and Stephan Vogel. 2004.
Improved named entity translation and
bilingual named entity extraction. In
Proceedings of the 4th IEEE International
Conference on Multimodal Interface,
pages 253?258, Pittsburgh, PA.
Huang, Fei, Stephan Vogel, and Alex Waibel.
2003. Automatic extraction of named
entity translingual equivalence based
on multi-feature cost minimization.
In Proceedings of ACL?03, Workshop on
Multilingual and Mixed-language Named
Entity Recognition, pages 9?16, Sapporo.
Ji, Heng and Ralph Grishman. 2006.
Analysis and repair of name tagger
errors. In Proceedings of COLING/ACL
2006, pages 420?427, Sydney.
Ji, Heng and Ralph Grishman. 2007.
Collaborative entity extraction and
translation. In Proceedings of International
Conference on Recent Advances in Natural
Language Processing, pages 73?84, Borovets.
Jiao, Feng, Shaojun Wang, Chi H. Lee,
Russell Greiner, and Dale Schuurmans.
2006. Semi-supervised conditional
random fields for improved sequence
segmentation and labeling. In Proceedings
of the 21st International Conference on
Computational Linguistics, pages 209?216,
Sydney.
Kneser, Reinhard and Hermann Ney.
1995. Improved backing-off for m-gram
language modeling. In Proceedings
of the IEEE International Conference on
Acoustics, Speech, and Signal Processing,
pages 181?184, Detroit, MI.
Knight, Kevin and Jonathan Graehl. 1998.
Machine transliteration. Computational
Linguistics, 24(4):599?612.
Krishman, Vijay and Christopher D.
Manning. 2006. An effective two-stage
model for exploiting non-local
dependencies in named entity recognition.
In Proceedings of 44th Annual Meeting of the
Association for Computational Linguistics,
pages 1,121?1,128, Sydney.
Kumano, T., H. Kashioka, H. Tanaka,
and T. Fukusima. 2004. Acquiring
bilingual named entity translations from
content-aligned corpora. In Proceedings
of the First International Joint Conference
on Natural Language Processing,
pages 177?186, Hainan Island.
Lee, Chun-Jen and Jason S. Chang.
2003. Acquisition of English-Chinese
transliterated word pairs from
parallel aligned texts using a statistical
machine transliteration model.
In Proceedings of HLT-NAACL 2003
264
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Workshop on Building and Using Parallel
Texts: Data Driven Machine Translation
and Beyond, pages 96?103, Edmonton.
Lee, Chun-Jen, Jason S. Chang, and
Jyh-Shing R. Jang. 2003. A statistical
approach to Chinese-to-English
back transliteration. In Proceedings
of the 17th Pacific Asia Conference on
Language, Information, and Computation,
pages 310?318, Singapore.
Lee, Chun-Jen, Jason S. Chang, and
Jyh-Shing R. Jang. 2006. Alignment of
bilingual named entities in parallel
corpora using statistical models and
multiple knowledge sources. ACM
Transactions on Asian Language Information
Processing (TALIP), 5(2):121?145.
Lee, Jae Sung and Key-Sun Choi. 1997.
A statistical method to generate
various foreign word transliterations in
multilingual information retrieval system.
In Proceedings of the 2nd International
Workshop on Information Retrieval with
Asian Languages (IRAL), pages 123?128,
Tsukuba.
Li, Haizhou, Khe Chai Sim, Jin Shea Kuo,
and Minghui Dong. 2007. Semantic
transliteration of personal names. In
Proceedings of 45th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 120?127, Prague.
Li, Haizhou, Min Zhang, and Jian Su. 2004.
A joint source channel model for machine
transliteration. In Proceedings of 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 159?166, Barcelona.
Liao, Wenhui and Sriharsha
Veeramachaneni. 2009. A simple
semi-supervised algorithm for named
entity recognition. In Proceedings of the
NAACL HLT Workshop on Semi-Supervised
Learning for Natural Language Processing,
pages 58?65, Boulder, CO.
Liese, Friedrich and Klaus-J. Miescke. 2008.
Statistical Decision Theory: Estimation,
Testing, and Selection. Springer, Berlin.
Lin, Wei-Hao and Hsin-Hsi Chen. 2002.
Backward transliteration by learning
phonetic similarity. In Proceedings of the
Sixth Conference on Natural Language
Learning, pages 139?145, Taipei.
McCallum, Andrew and Wei Li. 2003.
Early results for named entity recognition
with conditional random fields, feature
induction and Web-enhanced lexicons.
In Proceedings of the Conference on
Computational Natural Language Learning
(CoNLL 2003), pages 188?191, Edmonton.
McCallum, Andrew Kachites. 2002. MALLET:
A Machine Learning for Language Toolkit.
Available at http://mallet.cs.
umass.edu.
Mikheev, A., C. Grover, and M. Moens. 1998.
Description of the LTG system used for
MUC-7. In Proceedings of the 7th Message
Understanding Conference (MUC-7),
pages 1?12, Fairfax, VA.
Moore, R. C. 2003. Learning translations of
named-entity phrases from parallel
corpora. In Proceedings of the 10th
Conference of the European Chapter of ACL,
pages 259?266, Budapest.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 41st Annual Conference
of the Association for Computational
Linguistics (ACL), pages 160?167, Sapporo.
Och, Franz Josef and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Oh, Jong-Hoon and Key-Sun Choi. 2002.
An English-Korean transliteration model
using pronunciation and contextual rules.
In Proceedings of the 19th International
Conference on Computational Linguistics,
pages 758?764, Taipei.
Oh, Jong-Hoon and Key-Sun Choi. 2005.
An ensemble of grapheme and phoneme
for machine transliteration. In Proceedings
of the Second International Joint Conference
on Natural Language Processing,
pages 450?461, Jeju Island.
Pervouchine, Vladimir, Haizhou Li, and
Bo Lin. 2009. Transliteration alignment.
In Proceedings of the 47th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 136?144, Singapore.
Schlu?ter, Ralf and Hermann Ney. 2001.
Model-based MCE bound to the true Bayes
error. IEEE Signal Processing Letters,
8(5):131?133.
Shao, Li and Hwee Tou Ng. 2004. Mining
new word translations from comparable
corpora. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 618?624,
Geneva.
Sproat, Richard, Tao Tao, and ChengXiang
Zhai. 2006. Named entity transliteration
with comparable corpora. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 73?80, Sydney.
Stalls, B. G. and Kevin Knight. 1998.
Translating names and technical
265
Computational Linguistics Volume 39, Number 2
terms in Arabic text. In Proceedings of the
COLING/ACL Workshop on Computational
Approaches to Semitic Languages,
pages 34?41, Montreal.
Stolcke, Andreas. 2002. SRILM?An
extensible language modeling toolkit. In
Proceedings of the International Conference on
Spoken Language Processing, pages 901?904,
Denver, CO.
Sun, Jian, Ming Zhou, and Jianfeng Gao.
2003. A class-based language model
approach to Chinese named entity
identification. Computational Linguistics
and Chinese Language Processing, 8(2):1?28.
Tsuji, Keita. 2002. Automatic extraction of
translational Japanese-Katakana and
English word pairs from bilingual corpora.
International Journal of Computer Processing
of Oriental Languages, 15(3):261?279.
Wan, S. and C. M. Verspoor. 1998. Automatic
English-Chinese name transliteration for
development of multilingual resources.
In Proceedings of the 17th COLING and
36th ACL, pages 1,352?1,356, Montreal.
Wong, Yingchuan and Hwee Tou Ng.
2007. One class per named entity:
Exploiting unlabeled text for named
entity recognition. In Proceedings of
the International Joint Conferences on
Artificial Intelligence, pages 1,763?1,768,
Hyderabad.
Wu, D., G. Ngai, M. Carpuat, J. Larsen, and
Y. Yang. 2002. Boosting for named
entity recognition. In Proceedings of the
Sixth Conference on Natural Language
Learning, pages 195?198, Taipei.
Wu, Youzheng, Jun Zhao, and Bo Xu. 2005.
Chinese named entity recognition model
based on multiple features. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2005),
pages 427?434, Vancouver.
Zhang, Min, Haizhou Li, Jian Su, and
Hendra Setiawan. 2005. A phrase-based
context-dependent joint probability model
for named entity translation. In Proceedings
of the Second International Joint Conference
on Natural Language Processing,
pages 600?611, Jeju Island.
Zhang, Ying, S. Vogel, and A. Waibel. 2004.
Interpreting BLEU/NIST scores: How
much improvement do we need to have
a better system? In Proceedings of the
4th International Conference on Language
Resources and Evaluation, pages 2,051?2,054,
Lisbon.
Zhao, Hai and Chunyu Kit. 2008.
Unsupervised segmentation helps
supervised learning of character tagging
for word segmentation and named entity
recognition. In the Sixth SIGHAN
Workshop on Chinese Language Processing
(SIGHAN-6), pages 106?111, Hyderabad.
Zhou, GuoDong and Jian Su. 2006. Machine
learning-based named entity recognition
via effective integration of various
evidences. Natural Language Engineering,
11(2):189?206.
Zong, Chengqing and Mark Seligman.
2005. Toward practical spoken language
translation. Machine Translation,
19(2):113?137.
266
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 631?639,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
On Jointly Recognizing and Aligning Bilingual Named Entities 
 
Yufeng Chen, Chengqing Zong 
Institute of Automation, Chinese Academy of Sciences 
Beijing, China 
{chenyf,cqzong}@nlpr.ia.ac.cn 
Keh-Yih Su 
Behavior Design Corporation 
Hsinchu, Taiwan, R.O.C.  
bdc.kysu@gmail.com 
 
  
Abstract 
We observe that (1) how a given named en-
tity (NE) is translated (i.e., either semanti-
cally or phonetically) depends greatly on its 
associated entity type, and (2) entities within 
an aligned pair should share the same type. 
Also, (3) those initially detected NEs are an-
chors, whose information should be used to 
give certainty scores when selecting candi-
dates. From this basis, an integrated model is 
thus proposed in this paper to jointly identify 
and align bilingual named entities between 
Chinese and English. It adopts a new map-
ping type ratio feature (which is the propor-
tion of NE internal tokens that are semanti-
cally translated), enforces an entity type con-
sistency constraint, and utilizes additional 
monolingual candidate certainty factors 
(based on those NE anchors). The experi-
ments show that this novel approach has sub-
stantially raised the type-sensitive F-score of 
identified NE-pairs from 68.4% to 81.7% 
(42.1% F-score imperfection reduction) in 
our Chinese-English NE alignment task.  
1 Introduction 
In trans-lingual language processing tasks, such 
as machine translation and cross-lingual informa-
tion retrieval, named entity (NE) translation is 
essential. Bilingual NE alignment, which links 
source NEs and target NEs, is the first step to 
train the NE translation model.  
Since NE alignment can only be conducted af-
ter its associated NEs have first been identified, 
the including-rate of the first recognition stage 
significantly limits the final alignment perform-
ance. To alleviate the above error accumulation 
problem, two strategies have been proposed in 
the literature. The first strategy (Al-Onaizan and 
Knight, 2002; Moore, 2003; Feng et al, 2004; 
Lee et al, 2006) identifies NEs only on the 
source side and then finds their corresponding 
NEs on the target side. In this way, it avoids the 
NE recognition errors which would otherwise be 
brought into the alignment stage from the target 
side; however, the NE errors from the source 
side still remain.  
To further reduce the errors from the source 
side, the second strategy (Huang et al, 2003) 
expands the NE candidate-sets in both languages 
before conducting the alignment, which is done 
by treating the original results as anchors, and 
then re-generating further candidates by enlarg-
ing or shrinking those anchors' boundaries. Of 
course, this strategy will be in vain if the NE an-
chor is missed in the initial detection stage. In 
our data-set, this strategy significantly raises the 
NE-pair type-insensitive including-rate 1  from 
83.9% to 96.1%, and is thus adopted in this paper. 
Although the above expansion strategy has 
substantially alleviated the error accumulation 
problem, the final alignment accuracy is still not 
good (type-sensitive F-score only 68.4%, as indi-
cated in Table 2 in Section 4.2). After having 
examined the data, we found that: (1) How a 
given NE is translated, either semantically 
(called translation) or phonetically (called trans-
literation), depends greatly on its associated en-
tity type2. The mapping type ratio, which is the 
percentage of NE internal tokens which are 
translated semantically, can help with the recog-
nition of the associated NE type; (2) Entities 
within an aligned pair should share the same type, 
and this restriction should be integrated into NE 
alignment as a constraint; (3) Those initially 
identified monolingual NEs can act as anchors to 
give monolingual candidate certainty scores 
                                                 
1 Which is the percentage of desired NE-pairs that are in-
cluded in the expanded set, and is the upper bound on NE 
alignment performance (regardless of NE types).  
2 The proportions of semantic translation, which denote the 
ratios of semantically translated words among all the asso-
ciated NE words, for person names (PER), location names 
(LOC), and organization names (ORG) approximates 0%, 
28.6%, and 74.8% respectively in Chinese-English name 
entity list (2005T34) released by the Linguistic Data Con-
sortium (LDC). Since the title, such as ?sir? and ?chairman?, 
is not considered as a part of person names in this corpus, 
PERs are all transliterated there. 
 631
(preference weightings) for the re-generated can-
didates. 
Based on the above observation, a new joint 
model which adopts the mapping type ratio, en-
forces the entity type consistency constraint, and 
also utilizes the monolingual candidate certainty 
factors is proposed in this paper to jointly iden-
tify and align bilingual NEs under an integrated 
framework. This framework is decomposed into 
three subtasks: Initial Detection, Expansion, and 
Alignment&Re-identification. The Initial Detec-
tion subtask first locates the initial NEs and their 
associated NE types inside both the Chinese and 
English sides. Afterwards, the Expansion subtask 
re-generates the candidate-sets in both languages 
to recover those initial NE recognition errors. 
Finally, the Alignment&Re-identification subtask 
jointly recognizes and aligns bilingual NEs via 
the proposed joint model presented in Section 3. 
With this new approach, 41.8% imperfection re-
duction in type-sensitive F-score, from 68.4% to 
81.6%, has been observed in our Chinese-
English NE alignment task. 
2 Motivation 
The problem of NE recognition requires both 
boundary identification and type classification. 
However, the complexity of these tasks varies 
with different languages. For example, Chinese 
NE boundaries are especially difficult to identify 
because Chinese is not a tokenized language. In 
contrast, English NE boundaries are easier to 
identify due to capitalization clues. On the other 
hand, classification of English NE types can be 
more challenging (Ji et al, 2006). Since align-
ment would force the linked NE pair to share the 
same semantic meaning, the NE that is more re-
liably identified in one language can be used to 
ensure its counterpart in another language. This 
benefits both the NE boundary identification and 
type classification processes, and it hints that 
alignment can help to re-identify those initially 
recognized NEs which had been less reliable. 
As shown in the following example, although 
the desired NE ????????? is recognized 
partially as ?????? in the initial recognition 
stage, it would be more preferred if its English 
counterpart ?North Korean's Central News 
Agency? is given. The reason for this is that 
?News Agency? would prefer to be linked to ??
???, rather than to be deleted (which would 
happen if ?????? is chosen as the corre-
sponding Chinese NE).  
 
(I) The initial NE detection in a Chinese sentence: 
???  <ORG>????</ORG> ???????...  
(II) The initial NE detection of its English counterpart: 
Official <ORG>North Korean's Central News Agency 
</ORG> quoted the navy's statement? 
(III) The word alignment between two NEs: 
  
(VI) The re-identified Chinese NE boundary after alignment:  
??? <ORG>???????</ORG> ??????...  
As another example, the word ?lake? in the 
English NE is linked to the Chinese character 
??? as illustrated below, and this mapping is 
found to be a translation and not a transliteration. 
Since translation rarely occurs for personal 
names (Chen et al, 2003), the desired NE type 
?LOC? would be preferred to be shared between 
the English NE ?Lake Constance? and its corre-
sponding Chinese NE ???????. As a result, 
the original incorrect type ?PER? of the given 
English NE is fixed, and the necessity of using 
mapping type ratio and NE type consistency con-
straint becomes evident. 
(I) The initial NE detection result in a Chinese sentence: 
?  <LOC>?????</LOC> ?????????? 
(II) The initial NE detection of its English counterpart: 
The captain of a ferry boat who works on <PER>Lake Con-
stance </PER>? 
(III) The word alignment between two NEs: 
  
(VI) The re-identified English NE type after alignment: 
The captain of a ferry boat who works on <LOC>Lake 
Constance</LOC>? 
3 The Proposed Model 
As mentioned in the introduction section, given a 
Chinese-English sentence-pair ( , , with its 
initially recognized Chinese NEs 
)CS ES
1, ,Si i iCNE CType S? 1? ? ?
1[ , ] ,Tj j jENE EType T? ?
 and English NEs 
 (  and 1 ieCTyp jEty
iCNE
pe
EN
 are 
original NE types assigned to  and , 
respectively), we will first re-generate two NE 
candidate-sets from them by enlarging and 
shrinking the boundaries of those initially recog-
nized NEs. Let 
jE
1 C
KR  and CNE 1 EKRENE
C
 denote 
these two re-generated candidate sets for Chi-
nese and English NEs respectively ( K  and EK  
are their set-sizes), and ? ?min ,K S T? , then a 
total K  pairs of final Chinese and English NEs 
will be picked up from the Cartesian product of 
 632
1 C
KRCNE  and 1 EKRENE
( ,RCNE R? ?
[ ]kRENE
RType
? RE
iCNE
, according to their associ-
ated linking score, which is defined as follows. 
Let  denote the asso-
ciated linking score for a given candidate-pair 
 and , where  and  are 
the associated indexes of the re-generated Chi-
nese and English NE candidates, respectively. 
Furthermore, let  be the NE type to be re-
assigned and shared by RCNE  and  
(as they possess the same meaning). Assume 
that  and  are derived from ini-
tially recognized  and , respectively, 
and 
[ ]kre ENE
k
k? [ ]kNE
EN
Sco
kRCNE? ?
RCNE
)k
k? ?
k? ?
jE
[ ]k
RENE[ ]k
ICM  denotes their internal component map-
ping, to be defined in Section 3.1, then  
 is  defined as follows: [ ]( ,k RENE? ?
[ ]
, ,
k k
IC k
i i
RENE
M RType
NE CType
)kNEScore RC
,
max
IC kM RType
Score RCN
P? [ ]
( , )
,     , , ,[ , ],
k k
j j
E
RCNE RENE
C CS ENE EType ES
? ?
? ????? ??
| ????
 (1)                  
Here, the ?max? operator varies over each 
possible internal component mapping ICM  and 
re-assigned type (PER, LOC, and ORG). For 
brevity, we will drop those associated subscripts 
from now on, if there is no confusion. 
The associated probability factors in the above 
linking score can be further derived as follows. 
? ?
?
?
?
, , ,, ,    [ , ],
  , ,
  , ,
  , ,
IC
IC
CNE CType CSP M RType ENE EType ES
P M RTyp ENE
P RCNE CS RType
P RENE E ES RType
P RType Type EType
? ? ? ???
?
?
?
?
?
?
?
,
, ,
| ,
| ,
| ,
RCNE RENE
e RCNE R
CNE CType
NE EType
CNE ENE C
??
   (2) 
In the above equation, 
? ?, ,e RCNE
? | , ,ENE C
| ,CType
| ,NE EType
ICP M RTyp RENE
?
 and 
 are the Bilin-
gual Alignment Factor and the Bilingual Type 
Re-assignment Factor respectively, to represent 
the bilingual related scores (Section 3.1). Also, 
and 
 are Monolin-
gual Candidate Certainty Factors (Section 3.2) 
used to assign preference to each selected  
and , based on the initially recognized 
NEs (which act as anchors).  
,P RType CNE Type EType
? ?, ,P RCNE CNE CS RType
? ?, ,P RENE E ES RType
RENE
RCNE
3.1 Bilingual Related Factors 
The bilingual alignment factor mainly represents 
the likelihood value of a specific internal com-
ponent mapping ICM , given a pair of possible 
NE configurations RCNE  and  and their 
associated . Since Chinese word segmen-
tation is problematic, especially for transliterated 
words, the bilingual alignment factor 
RENE
RType
? ?, ,CNE REICP M RType R NE  in Eq (2) is derived 
to be conditioned on RE  (i.e., starting from 
the English part). 
NE
We define the internal component mapping 
ICM  to be [ ] 1[ , , ] ,NIC n n n nM cpn ew Mtype ?? ? ??? ?
[ ][ , , ]n n new Mtype
ncpn
, 
where  denotes a linked pair 
consisting of a Chinese component 
cpn? ?
? ?
[ ]new RCNE
 
(which might contain several Chinese characters) 
and an English word  within  and 
 respectively, with their internal mapping 
type 
RENE
nMtype
TLN
2[ ,n ew
 to be either translation (abbreviated 
as TS) or transliteration (abbreviated as TL). In 
total, there are N  component mappings, with 
 translation mappings  
and  transliteration mappings 
TSN
cpn
1 1[ ][ , , TSNn ncpn ew TS? ?
2 2[ ] 1, ] TLNn nTL
1 1]n ?
? ? ? TS TLN N N? ?, so that .  
Moreover, since the mapping type distribu-
tions of various NE types deviate greatly from 
one another, as illustrated in the second footnote, 
the associated mapping type ratio ? ?/TSN N? ?  is 
thus an important feature, and is included in the 
internal component mapping configuration speci-
fied above. For example, the ICM  between ???
???? and ?Constance Lake? is [????, 
Constance, TL] and [?, Lake, TS], so its asso-
ciated mapping type ratio will be ?0.5? (i.e., 1/2). 
Therefore, the internal mapping 
 is further deduced by in-
troducing the internal mapping type 
( | ,ICP M RType RENE)
nMtype  and 
the mapping type ratio ?  as follows: 
[ ] 1
[ ]
1 [ ]
( | , )
([ , , ] , | , )
( | , , )
( | , )
( | )
IC
N
n n n n
N n n n
n n n
P M RType RENE
P cpn ew Mtype RType RENE
P cpn Mtype ew RType
P Mtype ew RType
P RType
?
?
? ? ?
? ?
?
?
? ?? ? ??? ?? ?
?
?    (3) 
In the above equation, the mappings between 
internal components are trained from the sylla-
ble/word alignment of NE pairs of different NE 
types. In more detail? for transliteration, the 
model adopted in (Huang et al, 2003), which 
first Romanizes Chinese characters and then 
transliterates them into English characters, is 
 633
used for . For transla-
tion, conditional probability is directly used for 
.  
[ ]( | , ,n n nP cpn TL ew RType? ?
[ ]( | , , )n n nTS ew RType
)
?
P cpn? ?
Lastly, the bilingual type re-assignment factor 
 proposed in 
Eq (2) is derived as follows: 
? | , , ,P RType CNE ENE CType EType
? ?
? ?
| , , ,
| ,
P RType RCNE RENE CType EType
P RType CType EType?        (4) 
As Eq (4) shows, both the Chinese initial NE 
type and English initial NE type are adopted to 
jointly identify their shared NE type RType .  
3.2 Monolingual Candidate Certainty Factors 
On the other hand, the monolingual candidate 
certainty factors in Eq (2) indicate the likelihood 
that a re-generated NE candidate is the true NE 
given its originally detected NE. For Chinese, it 
is derived as follows: 
?
11
( , , , )
, , [ ] , ,
( , , )
 ( , , )
( | , )
C
C
C
M
m mm
P RCNE CNE CType CS RType
P LeftD RightD Str RCNE Len CType RType
P LeftD Len CType RType
P RightD Len CType RType
P cc cc RType??
?
?
?
??
|
|
|
|
?
  (5) 
Where, the subscript C  denotes Chinese, and 
 is the length of the originally recognized 
Chinese NE CN .  and  denote the 
left and right distance (which are the numbers of 
Chinese characters) that R  shrinks/enlarges 
from the left and right boundary of its anchor 
, respectively. As in the above example, 
assume that CN  and  are ?????? 
and ???????? respectively, Le  and 
 will be ?-1? and ?+3?. Also,  
stands for the associated Chinese string of , 
 denotes the m-th Chinese character within 
that string, and 
CLen
CNE
RightD
mcc
E
E
LeftD
R
RightD
CNE
CNE
ftD
Str R
R
[ ]CNE
CNE
M denotes the total number of 
Chinese characters within .  RCNE
On the English side, following Eq (5), 
? ?| , , ,P RENE ENE EType ES RType
ftD
E RENE
LeftD RightD
mcc
 can be derived 
similarly, except that Le  and  will be 
measured in number of English words. For in-
stance, with   EN  and  as  ?Lake Con-
stance? and ?on Lake Constance? respectively, 
 and  will be ?+1? and ?0?. Also, 
the bigram unit  of the Chinese NE string is 
replaced by the English word unit .  
RightD
new
All the bilingual and monolingual factors 
mentioned above, which are derived from Eq (1), 
are weighted differently according to their con-
tributions. The corresponding weighting coeffi-
cients are obtained using the well-known Mini-
mum Error Rate Training (Och, 2003; com-
monly abbreviated as MERT) algorithm by 
minimizing the number of associated errors in 
the development set. 
3.3 Framework for the Proposed Model  
The above model is implemented with a three-
stage framework: (A) Initial NE Recognition; (B) 
NE-Candidate-Set Expansion; and (C) NE 
Alignment&Re-identification. The Following 
Diagram gives the details of this framework: 
 
For each given bilingual sentence-pair: 
(A) Initial NE Recognition: generates the ini-
tial NE anchors with off-the-self packages. 
(B) NE-Candidate-Set Expansion: For each 
initially detected NE, several NE candi-
dates will be re-generated from the origi-
nal NE by allowing its boundaries to be 
shrunk or enlarged within a pre-specified 
range.  
(B.1) Create both RCNE and RENE 
candidate-sets, which are ex-
panded from those initial NEs 
identified in the previous stage.  
(B.2) Construct an NE-pair candidate-
set (named NE-Pair-Candidate-
Set), which is the Cartesian 
product of the RCNE and RENE 
candidate-sets created above.  
(C) NE Alignment&Re-identification: Rank 
each candidate in the NE-Pair-Candidate-
Set constructed above with the linking 
score specified in Eq (1). Afterwards, con-
duct a beam search process to select the 
top K non-overlapping NE-pairs from this 
set. 
Diagram 1. Steps to Generate the Final NE-Pairs 
 
It is our observation that, four Chinese charac-
ters for both shrinking and enlarging, two Eng-
lish words for shrinking and three for enlarging 
are enough in most cases. Under these conditions, 
the including-rates for NEs with correct bounda-
ries are raised to 95.8% for Chinese and 97.4% 
for English; and even the NE-pair including rate 
is raised to 95.3%. Since the above range limita-
tion setting has an including-rate only 0.8% 
lower than that can be obtained without any 
range limitation (which is 96.1%), it is adopted 
in this paper to greatly reduce the number of NE-
pair-candidates. 
 634
4 Experiments 
To evaluate the proposed joint approach, a prior 
work (Huang et al, 2003) is re-implemented in 
our environment as the baseline, in which the 
translation cost, transliteration cost and tagging 
cost are used. This model is selected for com-
parison because it not only adopts the same can-
didate-set expansion strategy as mentioned above, 
but also utilizes the monolingual information 
when selecting NE-pairs (however, only a simple 
bi-gram model is used as the tagging cost in their 
paper). Note that it enforces the same NE type 
only when the tagging cost is evaluated: 
11
11
min [ log( ( | , ))
                     log( ( | , ))]
RType
M
tag m mm
N
n nn
C P cc cc RType
P ew ew RType
??
??
? ?
?
?
? . 
To give a fairer comparison, the same train-
ing-set and testing-set are adopted. The training-
set includes two parts. The first part consists of 
90,412 aligned sentence-pairs newswire data 
from the Foreign Broadcast Information Service 
(FBIS), which is denoted as Training-Set-I. The 
second Part of the training set is the 
LDC2005T34 bilingual NE dictionary3, which is 
denoted as Training-Set-II. The required feature 
information is then manually labeled throughout 
the two training sets.  
In our experiments, for the baseline system, 
the translation cost and the transliteration cost 
are trained on Training-Set-II, while the tagging 
cost is trained on Training-Set-I. For the pro-
posed approach, the monolingual candidate cer-
tainty factors are trained on Training-Set-I, and 
Training-Set-II is used to train the parameters 
relating to bilingual alignment factors.  
For the testing-set, 300 sentence pairs are ran-
domly selected from the LDC Chinese-English 
News Text (LDC2005T06). The average length 
of the Chinese sentences is 59.4 characters, while 
the average length of the English sentences is 
24.8 words. Afterwards, the answer keys for NE 
recognition and alignment were annotated manu-
ally, and used as the gold standard to calculate 
metrics of precision (P), recall (R), and F-score 
(F) for both NE recognition (NER) and NE 
alignment (NEA). In Total 765 Chinese NEs and 
747 English NEs were manually labeled in the 
testing-set, within which there are only 718 NE 
pairs, including 214 PER, 371 LOC and 133 
ORG NE-pairs. The number of NE pairs is less 
                                                 
3 The LDC2005T34 data-set consists of proofread bilingual 
entries: 73,352 person names, 76,460 location names and 
68,960 organization names. 
than that of NEs, because not all those recog-
nized NEs can be aligned. 
Besides, the development-set for MERT 
weight training is composed of 200 sentence 
pairs selected from the LDC2005T06 corpus, 
which includes 482 manually tagged NE pairs. 
There is no overlap between the training-sets, the 
development-set and the testing-set.  
4.1 Baseline System 
Both the baseline and the proposed models share 
the same initial detection subtask, which adopts 
the Chinese NE recognizer reported by Wu et al  
(2005), which is a hybrid statistical model incor-
porating multi-knowledge sources, and the Eng-
lish NE recognizer included in the publicly 
available Mallet toolkit4 to generate initial NEs. 
Initial Chinese NEs and English NEs are recog-
nized by these two available packages respec-
tively.  
 
NE-type P (%): C/E R (%): C/E F (%): C/E
PER 80.2 / 79.2 87.7 / 85.3 83.8 / 82.1
LOC 89.8 / 85.9 87.3 / 81.5 88.5/ 83.6
ORG 78.6 / 82.9 82.8 / 79.6 80.6 / 81.2
ALL 83.4 / 82.1 86.0 / 82.6 84.7 / 82.3
Table 1. Initial Chinese/English NER 
 
Table 1 shows the initial NE recognition per-
formances for both Chinese and English (the 
largest entry in each column is highlighted for 
visibility). From Table 1, it is observed that the 
F-score of ORG type is the lowest among all NE 
types for both English and Chinese. This is be-
cause many organization names are partially rec-
ognized or missed. Besides, not shown in the 
table, the location names or abbreviated organi-
zation names tend to be incorrectly recognized as 
person names. In general, the initial Chinese 
NER outperforms the initial English NER, as the 
NE type classification turns out to be a more dif-
ficult problem for this English NER system. 
When those initially identified NEs are di-
rectly used for baseline alignment, only 64.1% F 
score (regard of their name types) is obtained. 
Such a low performance is mainly due to those 
NE recognition errors which have been brought 
into the alignment stage.  
To diminish the effect of errors accumulating, 
which stems from the recognition stage, the base-
line system also adopts the same expansion strat-
egy described in Section 3.3 to enlarge the possi-
                                                 
4 http://mallet.cs.umass.edu/index.php/Main_Page 
 635
ble NE candidate set. However, only a slight im-
provement (68.4% type-sensitive F-score) is ob-
tained, as shown in Table 2. Therefore, it is con-
jectured that the baseline alignment model is un-
able to achieve good performance if those fea-
tures/factors proposed in this paper are not 
adopted. 
4.2 The Recognition and Alignment Joint 
Model 
To show the individual effect of each factor in 
the joint model, a series of experiments, from 
Exp0 to Exp11, are conducted. Exp0 is the basic 
system, which ignores monolingual candidate 
certainty scores, and also disregards mapping 
type and NE type consistency constraint by ig-
noring  and [ ]( | ,n nP Mtype ew RType) ( | )P RType? , 
and also replacing P  
with  in Eq (3).  
[ ], ,n n new RType( |cpn? ?
[ ]( | )n nP cpn ew? ?
)
)
)
)
)n
Mtype
To show the effect of enforcing NE type con-
sistency constraint on internal component map-
ping, Exp1 (named Exp0+RType) replaces 
 in Exp0 with 
; On the other hand, Exp2 
(named Exp0+MappingType) shows the effect of 
introducing the component mapping type to Eq 
(3) by replacing  in Exp0 by 
; Then 
Exp3 (named Exp2+MappingTypeRatio) further 
adds 
[ ]( |n nP cpn ew? ?
[ ]( |n nP cpn ew? ?
( |n nP cpn Mtype? ?
( |P RTy
, RType
P c
[ ],ew
)pe
[ ]( |n npn ew? ?
) (n P Mtype e? [ ]|n w
?  to Exp2, to manifest the con-
tribution from the mapping type ratio. In addition, 
Exp4 (named Exp0+RTypeReassignment) adds 
the NE type reassignment score, Eq (4), to Exp0 
to show the effect of enforcing NE-type consis-
tency. Furthermore, Exp5 (named All-BiFactors) 
shows the full power of the set of proposed bi-
lingual factors by turning on all the options men-
tioned above. As the bilingual alignment factors 
would favor the candidates with shorter lengths, 
[ ] 1([ , , ] , | , ),Nn n n nP cpn ew Mtype RType RENE?? ? ? Eq (3), 
is further normalized into the following form: 
1
[ ]
1
[ ]
( | , , ) ( | ),
( | , )
N N
n n n
n
n n
P cpn Mtype ew RType P RType
P Mtype ew RType
?? ??
? ?? ? ?? ?? ??? ?
?
and is shown by Exp6 (named All-N-BiFactors). 
To show the influence of additional informa-
tion carried by those initially recognized NEs, 
Exp7 (named Exp6+LeftD/RightD) adds left and 
right distance information into Exp6, as that 
specified in Eq (5). To study the monolingual bi-
gram capability, Exp8 (named Exp6+Bigram) 
adds the NEtype dependant bigram model of 
each language to Exp6. We use SRI Language 
Modeling Toolkit5 (SRILM) (Stolcke, 2002) to 
train various character/word based bi-gram mod-
els with different NE types. Similar to what we 
have done on the bilingual alignment factor 
above, Exp9 (named Exp6+N-Bigram) adds the 
normalized NEtype dependant bigram to Exp6 
for removing the bias induced by having differ-
ent NE lengths. The normalized Chinese NEtype 
dependant bigram score is defined as 
1
11[ ( | , )
M ]Mm mm P cc cc RType??? . A Similar trans-
formation is also applied to the English side. 
Lastly, Exp10 (named Fully-JointModel) 
shows the full power of the proposed Recogni-
tion and Alignment Joint Model by adopting all 
the normalized factors mentioned above. The 
result of a MERT weighted version is further 
shown by Exp11 (named Weighted-JointModel). 
 
Model P (%) R (%) F (%)
Baseline 77.1  (67.1) 
79.7 
(69.8) 
78.4 
(68.4) 
Exp0 
(Basic System) 
67.9 
 (62.4) 
70.3 
(64.8) 
69.1 
(63.6) 
Exp1 
(Exp0 + Rtype) 
69.6 
 (65.7) 
71.9 
(68.0) 
70.8 
(66.8) 
Exp2 
(Exp0 + MappingType) 
70.5 
 (65.3) 
73.0 
(67.5) 
71.7 
(66.4) 
Exp3 
(Exp2 + MappingTypeRatio)
72.0 
(68.3) 
74.5 
(70.8) 
73.2 
(69.5) 
Exp4 
(Exp0 + RTypeReassignment)
70.2 
(66.7) 
72.7 
(69.2) 
71.4 
(67.9) 
Exp5 
(All-BiFactors) 
76.2 
 (72.3) 
78.5 
(74.6) 
77.3 
(73.4) 
Exp6 
(All-N-BiFactors) 
77.7 
(73.5) 
79.9 
(75.7) 
78.8 
(74.6) 
Exp7 
(Exp6 + LeftD/RightD) 
83.5 
(77.7) 
85.8 
(80.1) 
84.6 
(78.9) 
Exp8  
(Exp6 + Bigram) 
80.4 
(75.5) 
82.7 
(77.9) 
81.5 
(76.7) 
Exp9 
(Exp6 + N-Bigram) 
82.7 
(77.1) 
85.1 
(79.6) 
83.9 
(78.3) 
Exp10 
(Fully-JointModel) 
83.7 
(78.1) 
86.2 
(80.7) 
84.9 
(79.4) 
Exp11 
(Weighted-Joint Model) 
85.9 
(80.5) 
88.4 
(83.0) 
87.1 
(81.7) 
Table 2. NEA Type-Insensitive (Type-Sensitive) 
Performance  
 
Since most papers in the literature are evalu-
ated only based on the boundaries of NEs, two 
kinds of performance are thus given here. The 
first one (named type-insensitive) only checks 
the scope of each NE without taking its associ-
ated NE type into consideration, and is reported 
                                                 
5   http://www.speech.sri.com/projects/srilm/ 
 636
as the main data at Table 2. The second one 
(named type-sensitive) would also evaluate the 
associated NE type of each NE, and is given 
within parentheses in Table 2. A large degrada-
tion is observed when NE type is also taken into 
account. The highlighted entries are those that 
are statistically better6 than that of the baseline 
system. 
4.3 ME Approach with Primitive Features 
Although the proposed model has been derived 
above in a principled way, since all these pro-
posed features can also be directly integrated 
with the well-known maximum entropy (ME) 
(Berger et al, 1996) framework without making 
any assumptions, one might wonder if it is still 
worth to deriving a model after all the related 
features have been proposed. To show that not 
only the features but also the adopted model con-
tribute to the performance improvement, an ME 
approach is tested as follows for comparison. It 
directly adopts all those primitive features men-
tioned above as its inputs (including internal 
component mapping, initial and final NE type, 
NE bigram-based string, and left/right distance), 
without involving any related probability factors 
derived within the proposed model.  
This ME method is implemented with a public 
package YASMET7, and is tested under various 
training-set sizes (400, 4,000, 40,000, and 90,412 
sentence-pairs). All those training-sets are ex-
tracted from the Training-Set-I mentioned above 
(a total of 298,302 NE pairs included are manu-
ally labeled). Since the ME approach is unable to 
utilize the bilingual NE dictionary (Training-Set-
II), for fair comparison, this dictionary was also 
not used to train our models here. Table 3 shows 
the performance (F-score) using the same test-
ing-set. The data within parentheses are relative 
improvements. 
 
Model 400 4,000 40,000 90,412
ME framework 36.5 (0%) 
50.4 
(0%) 
62.6 
(0%) 
67.9 
(0%) 
Un-weighted- 
JointModel 
+4.6 
(+12.6%) 
+4.5 
(+8.9%) 
+4.3 
(+6.9%) 
+4.1 
(+6.0%)
Weighted- 
JointModel 
+5.0 
(+13.7%) 
+4.7 
(+9.3%) 
+4.6 
(+7.3%) 
+4.5 
(+6.6%)
Table 3. Comparison between ME Framework 
and Derived Model on the Testing-Set 
 
                                                 
6 Statistical significance test is measured on 95% confidence 
level on 1,000 re-sampling batches (Zhang et al, 2004) 
7 http://www.fjoch.com/YASMET.html 
The improvement indicated in Table 3 clearly 
illustrates the benefit of deriving the model 
shown in Eq (2). Since a reasonably derived 
model not only shares the same training-set with 
the primitive ME version above, but also enjoys 
the additional knowledge introduced by the hu-
man (i.e., the assumptions/constraints implied by 
the model), it is not surprising to find out that a 
good model does help, and that it also becomes 
more noticeable as the training-set gets smaller.  
5 Error Analysis and Discussion 
Although the proposed model has substantially 
improved the performance of both NE alignment 
and recognition, some errors still remain. Having 
examined those type-insensitive errors, we found 
that they can be classified into four categories: 
(A) Original NEs or their components are al-
ready not one-to-one mapped (23%). (B) NE 
components are one-to-one linked, but the asso-
ciated NE anchors generated from the initial rec-
ognition stage are either missing or spurious 
(24%). Although increasing the number of output 
candidates generated from the initial recognition 
stage might cover the missing problem, possible 
side effects might also be expected (as the com-
plexity of the alignment task would also be in-
creased). (C) Mapping types are not assumed by 
the model (27%). For example, one NE is abbre-
viated while its counterpart is not; or some loan-
words or out-of-vocabulary terms are translated 
neither semantically nor phonetically. (D) Wrong 
NE scopes are selected (26%). Errors of this type 
are uneasy to resolve, and their possible solutions 
are beyond the scope of this paper. 
Examples of above category (C) are interest-
ing and are further illustrated as follows. As an 
instance of abbreviation errors, a Chinese NE 
??????? (GlaxoSmithKline Factory)? is 
tagged as ???? /PRR ??? /n?, while its 
counterpart in the English side is simply abbrevi-
ated as ?GSK? (or  replaced by a pronoun ?it? 
sometimes). Linking ????? to ?GSK? (or to 
the pronoun ?it?) is thus out of reach of our 
model. It seems an abbreviation table (or even 
anaphora analysis) is required to recover these 
kind of errors.  
As an example of errors resulting from loan-
words; Japanese kanji ???? (the name of a 
Japanese emperor) is linked to the English word 
?Akihito?. Here the Japanese kanji ???? is di-
rectly adopted as the corresponding Chinese 
characters (as those characters were originally 
borrowed from Chinese), which would be pro-
 637
nounced as ?Mingren? in Chinese and thus devi-
ates greatly from the English pronunciation of 
?Akihito?. Therefore, it is translated neither se-
mantically nor phonetically. Further extending 
the model to cover this new conversion type 
seems necessary; however, such a kind of exten-
sion is very likely to be language pair dependent. 
6 Capability of the Proposed Model 
In addition to improving NE alignment, the pro-
posed joint model can also boost the perform-
ance of NE recognition in both languages. The 
corresponding differences in performance (of the 
weighted version) when compared with the ini-
tial NER ( ,   and P? R? F? ) are shown in Table 4. 
Again, those marked entries indicate that they are 
statistically better than that of the original NER.  
 
NEtype P? (%): C/E R? (%): C/E F? (%): C/E
PER +5.4 / +6.4 +2.2 / +2.6 +3.9 / +4.6 
LOC +4.0 / +3.4 -0.2 / +2.7 +1.8 / +3.0 
ORG +7.0 / +3.9 +5.6 / +9.1 +6.2 / +6.4 
ALL +5.3 /+5.2 +2.4 / +4.0 +3.9 / +4.6 
Table 4. Improvement in Chinese/English NER 
 
The result shows that the proposed joint model 
has a clear win over the initial NER for either 
Chinese or English NER. In particular, ORG 
seems to have yielded the greatest gain amongst 
NE types, which matches our previous observa-
tions that the boundaries of Chinese ORG are 
difficult to identify with the information only 
coming from the Chinese sentence, while the 
type of English ORG is uneasy to classify with 
the information only coming from the English 
sentence.  
Though not shown in the tables, it is also ob-
served that the proposed approach achieves a 
28.9% reduction on the spurious (false positive) 
and partial tags over the initial Chinese NER, as 
well as 16.1% relative error reduction compared 
with the initial English NER. In addition, total 
27.2% wrong Chinese NEs and 40.7% wrong 
English NEs are corrected into right NE types. 
However, if the mapping type ratio is omitted, 
only 21.1% wrong Chinese NE types and 34.8% 
wrong English NE types can be corrected. This 
clearly indicates that the ratio is essential for 
identifying NE types. 
With the benefits shown above, the alignment 
model could thus be used to train the monolin-
gual NE recognition model via semi-supervised 
learning. This advantage is important for updat-
ing the NER model from time to time, as various 
domains frequently have different sets of NEs 
and new NEs also emerge with time. 
Since the Chinese NE recognizer we use is not 
an open source toolkit, it cannot be used to carry 
out semi-supervised learning. Therefore, only the 
English NE recognizer and the alignment model 
are updated during training iterations. In our ex-
periments, 50,412 sentence pairs are first ex-
tracted from Training-Set-I as unlabeled data. 
Various labeled data-sets are then extracted from 
the remaining data as different seed corpora (100, 
400, 4,000 and 40,000 sentence-pairs). Table 5 
shows the results of semi-supervised learning 
after convergence for adopting only the English 
NER model (NER-Only), the baseline alignment 
model (NER+Baseline), and our un-weighted 
joint model (NER+JointModel) respectively. The 
Initial-NER row indicates the initial performance 
of the NER model re-trained from different seed 
corpora. The data within parentheses are relative 
improvement over Initial-NER. Note that the 
testing set is still the same as before.  
As Table 5 shows, with the NER model alone, 
the performance may even deteriorate after con-
vergence. This is due to the fact that maximizing 
likelihood does not imply minimizing the error 
rate. However, with additional mapping con-
straints from the aligned sentence of another lan-
guage, the alignment module could guide the 
searching process to converge to a more desir-
able point in the parameter space; and these addi-
tional constraints become more effective as the 
seed-corpus gets smaller. 
 
Model 100 400 4,000 40,000
Initial-NER 36.7 (0%) 
58.6 
(0%) 
71.4 
(0%) 
79.1 
(0%) 
NER-Only -2.3 (-6.3%)
-0.5 
(-0.8%) 
-0.3 
(-0.4%) 
-0.1 
(-0.1%)
NER+Baseline +4.9 (+13.4%)
+3.4 
(5.8%) 
+1.7 
(2.4%) 
+0.7 
(0.9%)
NER+Joint 
 Model 
+10.7 
(+29.2%)
+8.7 
(+14.8%) 
+4.8 
(+6.7%) 
+2.3 
(+2.9%)
Table 5. Testing-Set Performance for Semi-
Supervised Learning of English NE Recognition  
7 Conclusion 
In summary, our experiments show that the new 
monolingual candidate certainty factors are more 
effective than the tagging cost (only bigram 
model) adopted in the baseline system. Moreover, 
both the mapping type ratio and the entity type 
consistency constraint are very helpful in identi-
fying the associated NE boundaries and types. 
After having adopted the features and enforced 
 638
 the constraint mentioned above, the proposed 
framework, which jointly recognizes and aligns 
bilingual named entities, achieves a remarkable 
42.1% imperfection reduction on type-sensitive 
F-score (from 68.4% to 81.7%) in our Chinese-
English NE alignment task. 
Although the experiments are conducted on 
the Chinese-English language pair, it is expected 
that the proposed approach can also be applied to 
other language pairs, as no language dependent 
linguistic feature (or knowledge) is adopted in 
the model/algorithm used. 
Acknowledgments 
The research work has been partially supported 
by the National Natural Science Foundation of 
China under Grants No. 60975053, 90820303, 
and 60736014, the National Key Technology 
R&D Program under Grant No. 2006BAH03B02, 
and also the Hi-Tech Research and Development 
Program (?863? Program) of China under Grant 
No. 2006AA010108-4. 
References 
Al-Onaizan, Yaser, and Kevin Knight. 2002. Translat-
ing Named Entities Using Monolingual and Bilin-
gual resources. In Proceedings of the 40th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL), pages 400-408. 
Berger, Adam L., Stephen A. Della Pietra and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy 
Approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39-72, March. 
Chen, Hsin-His, Changhua Yang and Ying Lin. 2003. 
Learning Formulation and Transformation Rules 
for Multilingual Named Entities. In Proceedings of 
the ACL 2003 Workshop on Multilingual and 
Mixed-language Named Entity Recognition, pages 
1-8. 
Feng, Donghui, Yajuan Lv and Ming Zhou. 2004. A 
New Approach for English-Chinese Named Entity 
Alignment. In Proceedings of the Conference on 
Empirical Methods in Natural Language Process-
ing (EMNLP 2004), pages 372-379. 
Huang, Fei, Stephan Vogel and Alex Waibel. 2003. 
Automatic Extraction of Named Entity Translin-
gual Equivalence Based on Multi-Feature Cost 
Minimization. In Proceedings of ACL?03, Work-
shop on Multilingual and Mixed-language Named 
Entity Recognition. Sappora, Japan. 
Ji, Heng and Ralph Grishman. 2006. Analysis and 
Repair of Name Tagger Errors. In Proceedings of 
COLING/ACL 06, Sydney, Australia. 
Lee, Chun-Jen, Jason S. Chang and Jyh-Shing R. Jang. 
2006. Alignment of Bilingual Named Entities in 
Parallel Corpora Using Statistical Models and Mul-
tiple Knowledge Sources. ACM Transactions on 
Asian Language Information Processing (TALIP), 
5(2): 121-145. 
Moore, R. C.. 2003. Learning Translations of Named-
Entity Phrases from Parallel Corpora. In Proceed-
ings of 10th Conference of the European Chapter 
of ACL, Budapest, Hungary. 
Och, Franz Josef. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Conference of the Associa-
tion for Computational Linguistics (ACL). July 8-
10, 2003. Sapporo, Japan. Pages: 160-167. 
Stolcke, A. 2002. SRILM -- An Extensible Language 
Modeling Toolkit. Proc. Intl. Conf. on Spoken 
Language Processing, vol. 2, pp. 901-904, Denver. 
Wu, Youzheng, Jun Zhao and Bo Xu. 2005. Chinese 
Named Entity Recognition Model Based on Multi-
ple Features. In Proceedings of HLT/EMNLP 2005, 
pages 427-434. 
Zhang, Ying, Stephan Vogel, and Alex Waibel, 2004. 
Interpreting BLEU/NIST Scores: How Much Im-
provement Do We Need to Have a Better System? 
In Proceedings of the 4th International Conference 
on Language Resources and Evaluation, pages 
2051--2054.  
639
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11?21,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Integrating Translation Memory into Phrase-Based 
Machine Translation during Decoding 
 
 
Kun Wang?        Chengqing Zong?        Keh-Yih Su? 
?National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
?Behavior Design Corporation, Taiwan 
?
{kunwang, cqzong}@nlpr.ia.ac.cn, 
?
kysu@bdc.com.tw 
 
  
 
Abstract 
Since statistical machine translation (SMT) 
and translation memory (TM) complement 
each other in matched and unmatched regions, 
integrated models are proposed in this paper to 
incorporate TM information into phrase-based 
SMT. Unlike previous multi-stage pipeline 
approaches, which directly merge TM result 
into the final output, the proposed models refer 
to the corresponding TM information associat-
ed with each phrase at SMT decoding. On a 
Chinese?English TM database, our experi-
ments show that the proposed integrated Mod-
el-III is significantly better than either the 
SMT or the TM systems when the fuzzy match 
score is above 0.4. Furthermore, integrated 
Model-III achieves overall 3.48 BLEU points 
improvement and 2.62 TER points reduction 
in comparison with the pure SMT system. Be-
sides, the proposed models also outperform 
previous approaches significantly.  
1 Introduction 
Statistical machine translation (SMT), especially 
the phrase-based model (Koehn et al, 2003), has 
developed very fast in the last decade. For cer-
tain language pairs and special applications, 
SMT output has reached an acceptable level, es-
pecially in the domains where abundant parallel 
corpora are available (He et al, 2010). However, 
SMT is rarely applied to professional translation 
because its output quality is still far from satis-
factory. Especially, there is no guarantee that a 
SMT system can produce translations in a con-
sistent manner (Ma et al, 2011). 
In contrast, translation memory (TM), which 
uses the most similar translation sentence (usual-
ly above a certain fuzzy match threshold) in the 
database as the reference for post-editing, has 
been widely adopted in professional translation 
field for many years (Lagoudaki, 2006). TM is 
very useful for repetitive material such as updat-
ed product manuals, and can give high quality 
and consistent translations when the similarity of 
fuzzy match is high. Therefore, professional 
translators trust TM much more than SMT. 
However, high-similarity fuzzy matches are 
available unless the material is very repetitive. 
In general, for those matched segments1, TM 
provides more reliable results than SMT does. 
One reason is that the results of TM have been 
revised by human according to the global context, 
but SMT only utilizes local context. However, 
for those unmatched segments, SMT is more re-
liable. Since TM and SMT complement each 
other in those matched and unmatched segments, 
the output quality is expected to be raised signif-
icantly if they can be combined to supplement 
each other. 
In recent years, some previous works have in-
corporated TM matched segments into SMT in a 
pipelined manner (Koehn and Senellart, 2010; 
Zhechev and van Genabith, 2010; He et al, 2011; 
Ma et al, 2011). All these pipeline approaches 
translate the sentence in two stages. They first 
determine whether the extracted TM sentence 
pair should be adopted or not. Most of them use 
fuzzy match score as the threshold, but He et al 
(2011) and Ma et al (2011) use a classifier to 
make the judgment. Afterwards, they merge the 
relevant translations of matched segments into 
the source sentence, and then force the SMT sys-
tem to only translate those unmatched segments 
at decoding. 
There are three obvious drawbacks for the 
above pipeline approaches. Firstly, all of them 
determine whether those matched segments 
                                                 
1 We mean ?sub-sentential segments? in this work. 
11
should be adopted or not at sentence level. That 
is, they are either all adopted or all abandoned 
regardless of their individual quality. Secondly, 
as several TM target phrases might be available 
for one given TM source phrase due to insertions, 
the incorrect selection made in the merging stage 
cannot be remedied in the following translation 
stage. For example, there are six possible corre-
sponding TM target phrases for the given TM 
source phrase ???4 ?5 ??6? (as shown in 
Figure 1) such as ?object2 that3 is4 associated5?, 
and ?an1 object2 that3 is4 associated5  with6?, etc. 
And it is hard to tell which one should be adopt-
ed in the merging stage. Thirdly, the pipeline 
approach does not utilize the SMT probabilistic 
information in deciding whether a matched TM 
phrase should be adopted or not, and which tar-
get phrase should be selected when we have mul-
tiple candidates. Therefore, the possible im-
provements resulted from those pipeline ap-
proaches are quite limited. 
On the other hand, instead of directly merging 
TM matched phrases into the source sentence, 
some approaches (Bi?ici and Dymetman, 2008; 
Simard and Isabelle, 2009) simply add the long-
est matched pairs into SMT phrase table, and 
then associate them with a fixed large probability 
value to favor the corresponding TM target 
phrase at SMT decoding. However, since only 
one aligned target phrase will be added for each 
matched source phrase, they share most draw-
backs with the pipeline approaches mentioned 
above and merely achieve similar performance. 
To avoid the drawbacks of the pipeline ap-
proach (mainly due to making a hard decision 
before decoding), we propose several integrated 
models to completely make use of TM infor-
mation during decoding. For each TM source 
phrase, we keep all its possible corresponding 
target phrases (instead of keeping only one of 
them). The integrated models then consider all 
corresponding TM target phrases and SMT pref-
erence during decoding. Therefore, the proposed 
integrated models combine SMT and TM at a 
deep level (versus the surface level at which TM 
result is directly plugged in under previous pipe-
line approaches). 
On a Chinese?English computer technical 
documents TM database, our experiments have 
shown that the proposed Model-III improves the 
translation quality significantly over either the 
pure phrase-based SMT or the TM systems when 
the fuzzy match score is above 0.4. Compared 
with the pure SMT system, the proposed inte-
grated Model-III achieves 3.48 BLEU points im-
provement and 2.62 TER points reduction over-
all. Furthermore, the proposed models signifi-
cantly outperform previous pipeline approaches. 
2 Problem Formulation 
Compared with the standard phrase-based ma-
chine translation model, the translation problem 
is reformulated as follows (only based on the 
best TM, however, it is similar for multiple TM 
sentences): 
  (1) 
Where  is the given source sentence to be trans-
lated,  is the corresponding target sentence and  
is the final translation;  
are the associated information of the best TM 
sentence-pair;  and  denote the corre-
sponding TM sentence pair;  denotes its 
associated fuzzy match score (from 0.0 to 1.0); 
 is the editing operations between  and ; 
and  denotes the word alignment between 
 and . 
Let  and  denote the k-th associated 
source phrase and target phrase, respectively. 
Also,  and  denote the associated source 
phrase sequence and the target phrase sequence, 
respectively (total  phrases without insertion). 
Then the above formula (1) can be decomposed 
as below: 
 
(2) 
Afterwards, for any given source phrase , 
we can find its corresponding TM source phrase 
 and all possible TM target phrases (each 
of them is denoted by ) with the help of 
corresponding editing operations  and word 
alignment . As mentioned above, we can 
have six different possible TM target phrases for 
the TM source phrase ??? 4 ? 5 ?? 6?. This 
??0                    ?1  ??2  ??3  ??4  ?5  ??6  ?7
??0  ?1  ??2  ?3  ??4             ??5  ?6  ??7  ?8
gets0  n1  obj ct2  that3  is4  associated5  with6  the7  annotation8  label9  .10
Source
TM Source
TM Target
 
Figure 1: Phrase Mapping Example 
12
is because there are insertions around the directly 
aligned TM target phrase. 
In the above Equation (2), we first segment the 
given source sentence into various phrases, and 
then translate the sentence based on those source 
phrases. Also,  is replaced by , as they 
are actually the same segmentation sequence. 
Assume that the segmentation probability 
 is a uniform distribution, with the corre-
sponding TM source and target phrases obtained 
above, this problem can be further simplified as 
follows: 
 
(3) 
Where  is the corresponding TM phrase 
matching status for , which is a vector consist-
ing of various indicators (e.g., Target Phrase 
Content Matching Status, etc., to be defined lat-
er), and reflects the quality of the given candi-
date;  is the linking status vector of  (the 
aligned source phrase of  within ), and indi-
cates the matching and linking status in the 
source side (which is closely related to the status 
in the target side); also,  indicates the corre-
sponding TM fuzzy match interval specified later.  
In the second line of Equation (3), we convert 
the fuzzy match score  into its correspond-
ing interval , and incorporate all possible com-
binations of TM target phrases. Afterwards, we 
select the best one in the third line. Last, in the 
fourth line, we introduce the source matching 
status and the target linking status (detailed fea-
tures would be defined later). Since we might 
have several possible TM target phrases , 
the one with the maximum score will be adopted 
during decoding. 
The first factor  in the above for-
mula (3) is just the typical phrase-based SMT 
model, and the second factor  (to be 
specified in the Section 3) is the information de-
rived from the TM sentence pair. Therefore, we 
can still keep the original phrase-based SMT 
model and only pay attention to how to extract 
useful information from the best TM sentence 
pair to guide SMT decoding. 
3 Proposed Models 
Three integrated models are proposed to incorpo-
rate different features as follows: 
3.1 Model-I 
In this simplest model, we only consider Target 
Phrase Content Matching Status (TCM) for . 
For , we consider four different features at the 
same time: Source Phrase Content Matching 
Status (SCM), Number of Linking Neighbors 
(NLN), Source Phrase Length (SPL), and Sen-
tence End Punctuation Indicator (SEP). Those 
features will be defined below.  is 
then specified as: 
 
All features incorporated in this model are speci-
fied as follows: 
TM Fuzzy Match Interval (z): The fuzzy match 
score (FMS) between source sentence  and TM 
source sentence  indicates the reliability of 
the given TM sentence, and is defined as (Sikes, 
2007): 
 
Where  is the word-based 
Levenshtein Distance (Levenshtein, 1966) be-
tween  and . We equally divide FMS into 
ten fuzzy match intervals such as: [0.9, 1.0), [0.8, 
0.9) etc., and the index  specifies the corre-
sponding interval. For example, since the fuzzy 
match score between  and  in Figure 1 is 
0.667, then . 
Target Phrase Content Matching Status 
(TCM): It indicates the content matching status 
between   and , and reflects the quality 
of . Because  is nearly perfect when FMS 
is high, if the similarity between    and  
is high, it implies that the given  is possibly a 
good candidate. It is a member of {Same, High, 
Low, NA (Not-Applicable)}, and is specified as: 
(1) If  is not null: 
(a) if , ; 
(b) else if , ; 
(c) else, ; 
(2) If  is null, ; 
Here  is null means that either there is no 
corresponding TM source phrase  or 
there is no corresponding TM target phrase 
13
 aligned with . In the example of 
Figure 1, assume that the given  is ??? 5  
? 6  ?? 7? and  is ?object that is associated?. 
If  is ?object2 that3 is4 associated5?, 
; if  is ?an1 object2 that3 
is4 associated5?, . 
Source Phrase Content Matching Status 
(SCM): Which indicates the content matching 
status between  and , and it affects 
the matching status of  and  greatly. 
The more similar  is to , the more 
similar   is to . It is a member of {Same, 
High, Low, NA} and is defined as: 
(1) If  is not null: 
(a) if , ; 
(b) else if , 
; 
(c) else, ; 
(2) If  is null, ; 
Here  is null means that there is no corre-
sponding TM source phrase  for the giv-
en source phrase . Take the source phrase  
 ??? 5 ? 6 ?? 7? in Figure 1 for an ex-
ample, since its corresponding  is ??? 4 
? 5 ?? 6?, then . 
Number of Linking Neighbors (NLN): Usually, 
the context of a source phrase would affect its 
target translation. The more similar the context 
are, the more likely that the translations are the 
same. Therefore, this NLN feature reflects the 
number of matched neighbors (words) and it is a 
vector of <x, y>. Where ?x? denotes the number 
of matched source neighbors; and ?y? denotes 
how many those neighbors are also linked to tar-
get words (not null), which also affects the TM 
target phrase selection. This feature is a member 
of {<x, y>: <2, 2>, <2, 1>, <2, 0>, <1, 1>, <1, 0>, 
<0, 0>}. For the source phrase ??? 5 ? 6 ??
7? in Figure 1, the corresponding TM source 
phrase is ??? 4 ? 5 ?? 6? . As only their 
right neighbors ??8? and ??7? are matched, and 
??7? is aligned with ?.10?, NLN will be <1, 1>. 
Source Phrase Length (SPL): Usually the long-
er the source phrase is, the more reliable the TM 
target phrase is. For example, the corresponding 
 for the source phrase with 5 words 
would be more reliable than that with only one 
word. This feature denotes the number of words 
included in , and is a member of {1, 2, 3, 4, 
?5}. For the case ??? 5 ? 6 ?? 7?, SPL will 
be 3.  
Sentence End Punctuation Indicator (SEP): 
Which indicates whether the current phrase is a 
punctuation at the end of the sentence, and is a 
member of {Yes, No}. For example, the SEP for 
??? 5 ? 6 ?? 7? will be ?No?. It is intro-
duced because the SCM and TCM for a sen-
tence-end-punctuation are always ?Same? re-
gardless of other features. Therefore, it is used to 
distinguish this special case from other cases. 
3.2 Model-II 
As Model-I ignores the relationship among vari-
ous possible TM target phrases, we add two fea-
tures TM Candidate Set Status (CSS) and Long-
est TM Candidate Indicator (LTC) to incorporate 
this relationship among them.  Since CSS is re-
dundant after LTC is known, we thus ignore it 
for evaluating TCM probability in the following 
derivation: 
 
The two new features CSS and LTC adopted in 
Model-II are defined as follows: 
TM Candidate Set Status (CSS): Which re-
stricts the possible status of , and is a 
member of {Single, Left-Ext, Right-Ext, Both-Ext, 
NA}. Where ?Single? means that there is only 
one  candidate for the given source 
phrase ; ?Left-Ext? means that there are 
multiple  candidates, and all the candi-
dates are generated by extending only the left 
boundary; ?Right-Ext? means that there are mul-
tiple  candidates, and all the candidates 
are generated by only extending to the right; 
?Both-Ext? means that there are multiple  
candidates, and the candidates are generated by 
extending to both sides; ?NA? means that 
 is null. 
For ??? 4 ? 5 ?? 6? in Figure 1, the 
linked TM target phrase is ?object2 that3 is4 asso-
ciated5?, and there are 5 other candidates by ex-
tending to both sides. Therefore, 
. 
Longest TM Candidate Indicator (LTC): 
Which indicates whether the given  is the 
longest candidate or not, and is a member of 
{Original, Left-Longest, Right-Longest, Both-
Longest, Medium, NA}. Where ?Original? means 
that the given  is the one without exten-
sion; ?Left-Longest? means that the given 
14
 is only extended to the left and is the 
longest one; ?Right-Longest? means that the giv-
en  is only extended to the right and is 
the longest one; ?Both-Longest? means that the 
given  is extended to both sides and is the 
longest one; ?Medium? means that the given 
 has been extended but not the longest 
one; ?NA? means that  is null. 
For  ?object2 that3 is4 associated5? in 
Figure 1, ; for  ?an1 ob-
ject2 that3 is4 associated5?, ; 
for the longest  ?an1 object2 that3 is4 as-
sociated5 with6 the7?, . 
3.3 Model-III 
The abovementioned integrated models ignore 
the reordering information implied by TM. 
Therefore, we add a new feature Target Phrase 
Adjacent Candidate    Relative   Position   
Matching    Status (CPM) into Model-II and 
Model-III is given as: 
 
We assume that CPM is independent with SPL 
and SEP, because the length of source phrase 
would not affect reordering too much and SEP is 
used to distinguish the sentence end punctuation 
with other phrases.  
The new feature CPM adopted in Model-III is 
defined as: 
Target Phrase Adjacent Candidate Relative 
Position Matching Status (CPM): Which indi-
cates the matching status between the relative 
position of 
 
and the relative position of  
. It checks if  are 
positioned in the same order with 
, and reflects the quality of 
ordering the given target candidate . It is a 
member of {Adjacent-Same, Adjacent-Substitute, 
Linked-Interleaved, Linked-Cross, Linked-
Reversed, Skip-Forward, Skip-Cross, Skip-
Reversed, NA}. Recall that 
 
is always right ad-
jacent to , then various cases are defined as 
follows: 
(1) If both  and  are not null: 
(a) If  is on the right of  
and they are also adjacent to each other: 
i. If the right boundary words of  and 
 are the same, and the left 
boundary words of  and  are 
the same, ; 
ii. Otherwise, ; 
(b) If  is on the right of  
but they are not adjacent to each other, 
; 
(c) If  is not on the right of 
: 
i. If there are cross parts between  
and , ; 
ii. Otherwise, ; 
(2) If   is null but  is not null, 
then find the first which is 
not null (  starts from 2)2: 
(a) If  is on the right of , 
; 
(b) If  is not on the right of 
: 
i. If there are cross parts between  
and , ; 
ii. Otherwise, . 
(3) If  is null, . 
In Figure 1, assume that ,  and 
 are ?gets an?, ?object that is associat-
ed with? and ?gets0 an1?, respectively. For 
 ?object2 that3 is4 associated5?, because 
 is on the right of  and they are 
adjacent pair, and both boundary words (?an? 
and ?an1?; ?object? and ?object2?) are matched, 
; for  ?an1 object2 
that3 is4 associated5?, because there are cross 
parts ?an1? between  and , 
. On the other hand, as-
sume that ,  and  are ?gets?, ?ob-
ject that is associated with? and ?gets0?, respec-
tively. For  ?an1 object2 that3 is4 associ-
ated5?, because  and  are adja-
cent pair, but the left boundary words of  and 
 (?object? and ?an1?) are not matched, 
; for  ?object2 
that3 is4 associated5?, because  is on the 
right of  but they are not adjacent pair, 
therefore, . One more 
example, assume that ,  and  are 
?the annotation label?, ?object that is associated 
with? and ?the7 annotation8 label9?, respectively. 
For  ?an1 object2 that3 is4 associated5?, 
because  is on the left of , and 
there are no cross parts, .  
                                                 
2 It can be identified by simply memorizing the index of 
nearest non-null  during search. 
15
4 Experiments 
4.1 Experimental Setup 
Our TM database consists of computer domain 
Chinese-English translation sentence-pairs, 
which contains about 267k sentence-pairs. The 
average length of Chinese sentences is 13.85 
words and that of English sentences is 13.86 
words. We randomly selected a development set 
and a test set, and then the remaining sentence 
pairs are for training set. The detailed corpus sta-
tistics are shown in Table 1. Furthermore, devel-
opment set and test set are divided into various 
intervals according to their best fuzzy match 
scores. Corpus statistics for each interval in the 
test set are shown in Table 2.  
For the phrase-based SMT system, we adopted 
the Moses toolkit (Koehn et al, 2007). The sys-
tem configurations are as follows: GIZA++ (Och 
and Ney, 2003) is used to obtain the bidirectional 
word alignments. Afterwards, ?intersection? 3 
refinement (Koehn et al, 2003) is adopted to ex-
tract phrase-pairs. We use the SRI Language 
Model toolkit (Stolcke, 2002) to train a 5-gram 
model with modified Kneser-Ney smoothing 
(Kneser and Ney, 1995; Chen and Goodman, 
1998) on the target-side (English) training corpus. 
All the feature weights and the weight for each 
probability factor (3 factors for Model-III) are 
tuned on the development set with minimum-
error-rate training (MERT) (Och, 2003). The 
maximum phrase length is set to 7 in our exper-
iments. 
In this work, the translation performance is 
measured with case-insensitive BLEU-4 score 
(Papineni et al, 2002) and TER score (Snover et 
al., 2006). Statistical significance test is conduct-
ed with re-sampling (1,000 times) approach 
(Koehn, 2004) in 95% confidence level. 
4.2 Cross-Fold Translation 
To estimate the probabilities of proposed models, 
the corresponding phrase segmentations for bi-
lingual sentences are required. As we want to 
check what actually happened during decoding in 
the real situation, cross-fold translation is used to 
obtain the corresponding phrase segmentations. 
We first extract 95% of the bilingual sentences as 
a new training corpus to train a SMT system. 
Afterwards, we generate the corresponding 
phrase segmentations for the remaining 5% bi-
                                                 
3 ?grow-diag-final? and ?grow-diag-final-and? are also test-
ed. However, ?intersection? is the best option in our exper-
iments, especially for those high fuzzy match intervals.  
lingual sentences with Forced Decoding (Li et 
al., 2000; Zollmann et al, 2008; Auli et al, 2009; 
Wisniewski et al, 2010), which searches the best 
phrase segmentation for the specified output. 
Having repeated the above steps 20 times4, we 
obtain the corresponding phrase segmentations 
for the SMT training data (which will then be 
used to train the integrated models). 
Due to OOV words and insertion words, not 
all given source sentences can generate the de-
sired results through forced decoding. Fortunate-
ly, in our work, 71.7% of the training bilingual 
sentences can generate the corresponding target 
results. The remaining 28.3% of the sentence 
pairs are thus not adopted for generating training 
samples. Furthermore, more than 90% obtained 
source phrases are observed to be less than 5 
words, which explains why five different quanti-
zation levels are adopted for Source Phrase 
Length (SPL) in section 3.1. 
4.3 Translation Results 
After obtaining all the training samples via cross-
fold translation, we use Factored Language 
Model toolkit (Kirchhoff et al, 2007) to estimate 
the probabilities of integrated models with Wit-
ten-Bell smoothing (Bell et al, 1990; Witten et 
al., 1991) and Back-off method. Afterwards, we 
incorporate the TM information  for 
each  phrase  at  decoding.   All  experiments  are 
                                                 
4  This training process only took about 10 hours on our 
Ubuntu server (Intel 4-core Xeon 3.47GHz, 132 GB of 
RAM).  
  Train Develop Test 
#Sentences 261,906 2,569 2,576 
#Chn. Words 3,623,516 38,585 38,648 
#Chn. VOC. 43,112 3,287 3,460 
#Eng. Words 3,627,028 38,329 38,510 
#Eng. VOC. 44,221 3,993 4,046 
Table 1: Corpus Statistics 
Intervals #Sentences #Words W/S 
[0.9, 1.0) 269 4,468 16.6 
[0.8, 0.9) 362 5,004 13.8 
[0.7, 0.8) 290 4,046 14.0 
[0.6, 0.7) 379 4,998 13.2 
[0.5, 0.6) 472 6,073 12.9 
[0.4, 0.5) 401 5,921 14.8 
[0.3, 0.4) 305 5,499 18.0 
(0.0, 0.3) 98 2,639 26.9 
(0.0, 1.0) 2,576 38,648 15.0 
Table 2: Corpus Statistics for Test-Set 
16
Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U 
[0.9, 1.0) 81.31 81.38 85.44  * 86.47  *# 89.41  *# 82.79 77.72 82.78 
[0.8, 0.9) 73.25 76.16 79.97  * 80.89  * 84.04  *# 79.74  * 73.00 77.66 
[0.7, 0.8) 63.62 67.71 71.65  * 72.39  * 74.73  *# 71.02  * 66.54 69.78 
[0.6, 0.7) 43.64 54.56 54.88    # 55.88  *# 57.53  *# 53.06 54.00 56.37 
[0.5, 0.6) 27.37 46.32 47.32  *# 47.45  *# 47.54  *# 39.31 46.06 47.73 
[0.4, 0.5) 15.43 37.18 37.25    # 37.60    # 38.18  *# 28.99 36.23 37.93 
[0.3, 0.4) 8.24 29.27 29.52    # 29.38    # 29.15    # 23.58 29.40 30.20 
(0.0, 0.3) 4.13 26.38 25.61    # 25.32    # 25.57    # 18.56 26.30 26.92 
(0.0, 1.0) 40.17 53.03 54.57  *# 55.10  *# 56.51  *# 50.31 51.98 54.32 
Table 3: Translation Results (BLEU%). Scores marked by ?*? are significantly better (p < 0.05) than both TM 
and SMT systems, and those marked by ?#? are significantly better (p < 0.05) than Koehn-10. 
Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U 
[0.9, 1.0) 9.79 13.01 9.22      # 8.52    *# 6.77    *# 13.01 18.80 11.90 
[0.8, 0.9) 16.21 16.07 13.12  *# 12.74  *# 10.75  *# 15.27 20.60 14.74 
[0.7, 0.8) 27.79 22.80 19.10  *# 18.58  *# 17.11  *# 21.85 25.33 21.11 
[0.6, 0.7) 46.40 33.38 32.63    # 32.27  *# 29.96  *# 35.93 35.24 31.76 
[0.5, 0.6) 62.59 39.56 38.24  *# 38.77  *# 38.74  *# 47.37 40.24 38.01 
[0.4, 0.5) 73.93 47.19 47.03    # 46.34  *# 46.00  *# 56.84 48.74 46.10 
[0.3, 0.4) 79.86 55.71 55.38    # 55.44    # 55.87    # 64.55 55.93 54.15 
(0.0, 0.3) 85.31 61.76 62.38    # 63.66    # 63.51    # 73.30 63.00 60.67 
(0.0, 1.0) 50.51 35.88 34.34  *# 34.18  *# 33.26  *# 40.75 38.10 34.49 
Table 4: Translation Results (TER%). Scores marked by ?*? are significantly better (p < 0.05) than both TM and 
SMT systems, and those marked by ?#? are significantly better (p < 0.05) than Koehn-10. 
conducted using the Moses phrase-based decoder 
(Koehn et al, 2007). 
Table 3 and 4 give the translation results of 
TM, SMT, and three integrated models in the test 
set. In the tables, the best translation results (ei-
ther in BLEU or TER) at each interval have been 
marked in bold. Scores marked by ?*? are signif-
icantly better (p < 0.05) than both the TM and 
the SMT systems. 
It can be seen that TM significantly exceeds 
SMT at the interval [0.9, 1.0) in TER score, 
which illustrates why professional translators 
prefer TM rather than SMT as their assistant tool. 
Compared with TM and SMT, Model-I is signif-
icantly better than the SMT system in either 
BLEU or TER when the fuzzy match score is 
above 0.7; Model-II significantly outperforms 
both the TM and the SMT systems in either 
BLEU or TER when the fuzzy match score is 
above 0.5; Model-III significantly exceeds both 
the TM and the SMT systems in either BLEU or 
TER when the fuzzy match score is above 0.4. 
All these improvements show that our integrated 
models have combined the strength of both TM 
and SMT.  
However, the improvements from integrated 
models get less when the fuzzy match score de-
creases. For example, Model-III outperforms 
SMT 8.03 BLEU points at interval [0.9, 1.0), 
while the advantage is only 2.97 BLEU points at 
interval [0.6, 0.7). This is because lower fuzzy 
match score means that there are more un-
matched parts between  and ; the output of 
TM is thus less reliable. 
Across all intervals (the last row in the table), 
Model-III not only achieves the best BLEU score 
(56.51), but also gets the best TER score (33.26). 
If intervals are evaluated separately, when the 
fuzzy match score is above 0.4, Model-III out-
performs both Model-II and Model-I in either 
BLEU or TER. Model-II also exceeds Model-I in 
either BLEU or TER. The only exception is at 
interval [0.5, 0.6), in which Model-I achieves the 
best TER score. This might be due to that the 
optimization criterion for MERT is BLEU rather 
than TER in our work. 
4.4 Comparison with Previous Work 
In order to compare our proposed models with 
previous work, we re-implement two XML-
Markup approaches: (Koehn and Senellart, 2010) 
and (Ma et al 2011), which are denoted as 
Koehn-10 and Ma-11, respectively. They are 
selected because they report superior perfor-
mances in the literature. A brief description of 
them is as follows: 
17
Source 
?? 0 ?? 1 ? 2 ?? 3 ?? 4 ?5 internet6 explorer7 ? 8 ?? 9 internet10 ?? 11 ??? 12 
? 13 ? 14 ?? 15 ?16 ?? 17 ? 18 ? 19 ?? 20 ?? 21 ?? 22 ?23 
Reference 
if0 you1 disable2 this3 policy4 setting5 ,6 internet7 explorer8 does9 not10 check11 the12 internet13 
for14 new15 versions16 of17 the18 browser19 ,20 so21 does22 not23 prompt24 users25 to26 install27 
them28 .29 
TM 
Source 
?? 0 ? 1 ?? 2 ? 3 ?? 4 ?? 5 ?6 internet7 explorer8 ? 9 ?? 10 internet11 ?? 12 ??
? 13 ? 14 ? 15 ?? 16 ?17 ?? 18 ? 19 ? 20 ?? 21 ?? 22 ?? 23 ?24 
TM 
Target 
if0 you1 do2 not3 configure4 this5 policy6 setting7 ,8 internet9 explorer10 does11 not12 check13 the14 
internet15 for16 new17 versions18 of19 the20 browser21 ,22 so23 does24 not25 prompt26 users27 to28 
install29 them30 .31 
TM 
Alignment 
0-0 1-3 2-4 3-5 4-6 5-7 6-8 7-9 8-10 9-11 11-15 13-21 14-19 15-17 16-18 17-22 18-23 19-24 
21-26 22-27 23-29 24-31 
SMT 
if you disable this policy setting , internet explorer does not prompt users to install internet for 
new versions of the browser .    [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-
tion] 
Koehn-10 
if you do you disable this policy setting , internet explorer does not check the internet for new 
versions of the browser , so does not prompt users to install them .    [Insert two spurious target 
words] 
Ma-11 
if you disable this policy setting , internet explorer does not prompt users to install internet for 
new versions of the browser .    [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-
tion] 
Model-I 
if you disable this policy setting , internet explorer does not prompt users to install new ver-
sions of the browser , so does not check the internet .    [Miss 2 target words: 14, 28; Has one 
wrong permutation] 
Model-II 
if you disable this policy setting , internet explorer does not prompt users to install new ver-
sions of the browser , so does not check the internet .    [Miss 2 target words: 14, 28; Has one 
wrong permutation] 
Model-III 
if you disable this policy setting , internet explorer does not check the internet for new versions 
of the browser , so does not prompt users to install them .    [Exactly the same as the reference] 
Figure 2: A Translation Example at Interval [0.9, 1.0] (with FMS=0.920) 
Koehn et al (2010) first find out the un-
matched parts between the given source sentence 
and TM source sentence. Afterwards, for each 
unmatched phrase in the TM source sentence, 
they replace its corresponding translation in the 
TM target sentence by the corresponding source 
phrase in the input sentence, and then mark the 
substitution part. After replacing the correspond-
ing translations of all unmatched source phrases 
in the TM target sentence, an XML input sen-
tence (with mixed TM target phrases and marked 
input source phrases) is thus obtained. The SMT 
decoder then only translates the un-
matched/marked source phrases and gets the de-
sired results. Therefore, the inserted parts in the 
TM target sentence are automatically included. 
They use fuzzy match score to determine wheth-
er the current sentence should be marked or not; 
and their experiments show that this method is 
only effective when the fuzzy match score is 
above 0.8. 
Ma et al (2011) think fuzzy match score is not 
reliable and use a discriminative learning method 
to decide whether the current sentence should be 
marked or not. Another difference between Ma-
11 and Koehn-10 is how the XML input is con-
structed. In constructing the XML input sentence, 
Ma-11 replaces each matched source phrase in 
the given source sentence with the corresponding 
TM target phrase. Therefore, the inserted parts in 
the TM target sentence are not included. In Ma?s 
another paper (He et al, 2011), more linguistic 
features for discriminative learning are also add-
ed. In our work, we only re-implement the XML-
Markup method used in (He et al, 2011; Ma et al 
2011), but do not implement the discriminative 
learning method. This is because the features 
adopted in their discriminative learning are com-
plicated and difficult to re-implement. However, 
the proposed Model-III even outperforms the 
upper bound of their methods, which will be dis-
cussed later.  
Table 3 and 4 give the translation results of 
Koehn-10 and Ma-11 (without the discriminator). 
Scores marked by ?#? are significantly better (p 
< 0.05) than Koehn-10. Besides, the upper bound 
of (Ma et al 2011) is also given in the tables, 
which is denoted as Ma-11-U. We calculate this 
18
upper bound according to the method described 
in (Ma et al, 2011). Since He et al, (2011) only 
add more linguistic features to the discriminative 
learning method, the upper bound of (He et al, 
2011) is still the same with (Ma et al, 2011); 
therefore, Ma-11-U applies for both cases. 
It is observed that Model-III significantly ex-
ceeds Koehn-10 at all intervals. More important-
ly, the proposed models achieve much better 
TER score than the TM system does at interval 
[0.9, 1.0), but Koehn-10 does not even exceed 
the TM system at this interval. Furthermore, 
Model-III is much better than Ma-11-U at most 
intervals. Therefore, it can be concluded that the 
proposed models outperform the pipeline ap-
proaches significantly.  
Figure 2 gives an example at interval [0.9, 1.0), 
which shows the difference among different sys-
tem outputs. It can be seen that ?you do? is re-
dundant for Koehn-10, because they are inser-
tions and thus are kept in the XML input. How-
ever, SMT system still inserts another ?you?, 
regardless of ?you do? has already existed. This 
problem does not occur at Ma-11, but it misses 
some words and adopts one wrong permutation. 
Besides, Model-I selects more right words than 
SMT does but still puts them in wrong positions 
due to ignoring TM reordering information. In 
this example, Model-II obtains the same results 
with Model-I because it also lacks reordering 
information. Last, since Model-III considers both 
TM content and TM position information, it 
gives a perfect translation. 
5 Conclusion and Future Work 
Unlike the previous pipeline approaches, which 
directly merge TM phrases into the final transla-
tion result, we integrate TM information of each 
source phrase into the phrase-based SMT at de-
coding. In addition, all possible TM target 
phrases are kept and the proposed models select 
the best one during decoding via referring SMT 
information. Besides, the integrated model con-
siders the probability information of both SMT 
and TM factors. 
The experiments show that the proposed 
Model-III outperforms both the TM and the SMT 
systems significantly (p < 0.05) in either BLEU 
or TER when fuzzy match score is above 0.4. 
Compared with the pure SMT system, Model-III 
achieves overall 3.48 BLEU points improvement 
and 2.62 TER points reduction on a Chinese?
English TM database. Furthermore, Model-III 
significantly exceeds all previous pipeline ap-
proaches. Similar improvements are also ob-
served on the Hansards parts of LDC2004T08 
(not shown in this paper due to space limitation). 
Since no language-dependent feature is adopted, 
the proposed approaches can be easily adapted 
for other language pairs. 
Moreover, following the approaches of 
Koehn-10 and Ma-11 (to give a fair comparison), 
training data for SMT and TM are the same in 
the current experiments. However, the TM is 
expected to play an even more important role 
when the SMT training-set differs from the TM 
database, as additional phrase-pairs that are un-
seen in the SMT phrase table can be extracted 
from TM (which can then be dynamically added 
into the SMT phrase table at decoding time). Our 
another study has shown that the integrated mod-
el would be even more effective when the TM 
database and the SMT training data-set are from 
different corpora in the same domain (not shown 
in this paper). In addition, more source phrases 
can be matched if a set of high-FMS sentences, 
instead of only the sentence with the highest 
FMS, can be extracted and referred at the same 
time. And it could further raise the performance. 
Last, some related approaches (Smith and 
Clark, 2009; Phillips, 2011) combine SMT and 
example-based machine translation (EBMT) 
(Nagao, 1984). It would be also interesting to 
compare our integrated approach with that of 
theirs. 
 
Acknowledgments 
 
The research work has been funded by the Hi-
Tech Research and Development Program 
(?863? Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501.  
The authors would like to thank the anony-
mous reviewers for their insightful comments 
and suggestions. Our sincere thanks are also ex-
tended to Dr. Yanjun Ma and Dr. Yifan He for 
their valuable discussions during this study.  
References  
Michael Auli, Adam Lopez, Hieu Hoang and Philipp 
Koehn, 2009. A systematic analysis of translation 
model search spaces. In Proceedings of the Fourth 
Workshop on Statistical Machine Translation, pag-
es 224?232. 
19
Timothy C. Bell, J.G. Cleary and Ian H. Witten, 1990. 
Text compression: Prentice Hall, Englewood Cliffs, 
NJ. 
Ergun Bi?ici and Marc Dymetman. 2008. Dynamic 
translation memory: using statistical machine trans-
lation to improve translation memory fuzzy match-
es. In Proceedings of the 9th International Confer-
ence on Intelligent Text Processing and Computa-
tional Linguistics (CICLing 2008), pages 454?465. 
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language 
modeling. Technical Report TR-10-98, Harvard 
University Center for Research in Computing 
Technology. 
Yifan He, Yanjun Ma, Josef van Genabith and Andy 
Way, 2010. Bridging SMT and TM with transla-
tion recommendation. In Proceedings of the 48th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 622?630. 
Yifan He, Yanjun Ma, Andy Way and Josef van 
Genabith. 2011. Rich linguistic features for transla-
tion memory-inspired consistent translation. In 
Proceedings of the Thirteenth Machine Translation 
Summit, pages 456?463. 
Reinhard Kneser and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. In 
Proceedings of the IEEE International Conference 
on Acoustics, Speech and Signal Processing, pages 
181?184. 
Katrin Kirchhoff, Jeff A. Bilmes and Kevin Duh. 
2007. Factored language models tutorial. Technical 
report, Department of Electrical Engineering, Uni-
versity of Washington, Seattle, Washington, USA.  
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in 
Natural Language Processing (EMNLP), pages 
388?395, Barcelona, Spain. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer and Ond?ej Bojar. 2007. 
Moses: Open source toolkit for statistical machine 
translation. In Proceedings of the ACL 2007 Demo 
and Poster Sessions, pages 177?180. 
Philipp Koehn, Franz Josef Och and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, 
pages 48?54. 
Philipp Koehn and Jean Senellart. 2010. Convergence 
of translation memory and statistical machine 
translation. In AMTA Workshop on MT Research 
and the Translation Industry, pages 21?31. 
Elina Lagoudaki. 2006. Translation memories survey 
2006: Users? perceptions around tm use. In Pro-
ceedings of the ASLIB International Conference 
Translating and the Computer 28, pages 1?29. 
Qi Li, Biing-Hwang Juang, Qiru Zhou, and Chin-Hui 
Lee. 2000. Automatic verbal information verifica-
tion for user authentication. IEEE transactions on 
speech and audio processing, Vol. 8, No. 5, pages 
1063?6676. 
Vladimir Iosifovich Levenshtein. 1966. Binary codes 
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10 (8). pages 707?
710. 
Yanjun Ma, Yifan He, Andy Way and Josef van 
Genabith. 2011. Consistent translation using dis-
criminative learning: a translation memory-inspired 
approach. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1239?1248, Portland, Oregon. 
Makoto Nagao, 1984. A framework of a mechanical 
translation between Japanese and English by anal-
ogy principle. In: Banerji, Alick Elithorn and  Ran-
an (ed). Artifiical and Human Intelligence: Edited 
Review Papers Presented at the International 
NATO Symposium on Artificial and Human Intelli-
gence. North-Holland, Amsterdam, 173?180. 
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 160?167. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29 (1). pages 
19?51. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL), pages 311?
318. 
Aaron B. Phillips, 2011. Cunei: open-source machine 
translation with relevance-based models of each 
translation instance. Machine Translation, 25 (2). 
pages 166-177. 
Richard Sikes. 2007, Fuzzy matching in theory and 
practice. Multilingual, 18(6):39?43. 
Michel Simard and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted 
translation environment. In Proceedings of the 
Twelfth Machine Translation Summit (MT Summit 
XII), pages 120?127. 
James Smith and Stephen Clark. 2009. EBMT for 
SMT: a new EBMT-SMT hybrid. In Proceedings 
of the 3rd International Workshop on Example-
20
Based Machine Translation (EBMT'09), pages 3?
10, Dublin, Ireland. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla and John Makhoul. 2006. A 
study of translation edit rate with targeted human 
annotation. In Proceedings of Association for Ma-
chine Translation in the Americas (AMTA-2006), 
pages 223?231. 
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, pages 311?318. 
Guillaume Wisniewski, Alexandre Allauzen and 
Fran?ois Yvon, 2010. Assessing phrase-based 
translation models with oracle decoding. In Pro-
ceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pages 
933?943. 
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: estimating the probabilities of 
novel events in adaptive test compression. IEEE 
Transactions on Information Theory, 37(4): 1085?
1094, July. 
Ventsislav Zhechev and Josef van Genabith. 2010. 
Seeding statistical machine translation with transla-
tion memory output through tree-based structural 
alignment. In Proceedings of the 4th Workshop on 
Syntax and Structure in Statistical Translation, 
pages 43?51. 
Andreas Zollmann, Ashish Venugopal, Franz Josef 
Och and Jay Ponte, 2008. A systematic comparison 
of phrase-based, hierarchical and syntax-
augmented statistical MT. In Proceedings of the 
22nd International Conference on Computational 
Linguistics (Coling 2008), pages 1145?1152. 
 
 
21
 A Character-Based Joint Model 
for CIPS-SIGHAN Word Segmentation Bakeoff 2010 
Kun Wang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Science
{kunwang,cqzong}@nlpr.ia.ac.cn 
Keh-Yih Su 
Behavior Design Corporation
 
kysu@bdc.com.tw 
 
Abstract 
This paper presents a Chinese Word 
Segmentation system for the closed track 
of CIPS-SIGHAN Word Segmentation 
Bakeoff 2010. This system adopts a 
character-based joint approach, which 
combines a character-based generative 
model and a character-based discrimina-
tive model. To further improve the cross-
domain performance, we use an addi-
tional semi-supervised learning proce-
dure to incorporate the unlabeled corpus. 
The final performance on the closed 
track for the simplified-character text 
shows that our system achieves compa-
rable results with other state-of-the-art 
systems. 
1 Introduction 
The character-based tagging approach (Xue, 
2003) has become the dominant technique for 
Chinese word segmentation (CWS) as it can tol-
erate out-of-vocabulary (OOV) words. In the last 
few years, this method has been widely adopted 
and further improved in many previous works 
(Tseng et al, 2005; Zhang et al, 2006; Jiang et 
al., 2008). Among various character-based tag-
ging approaches, the character-based joint model 
(Wang et al, 2010) achieves a good balance be-
tween in-vocabulary (IV) words recognition and 
OOV words identification. 
In this work, we adopt the character-based 
joint model as our basic system, which combines 
a character-based discriminative model and a 
character-based generative model. The genera-
tive module holds a robust performance on IV 
words, while the discriminative module can 
handle the extra features easily and enhance the 
OOV words segmentation. However, the per-
formance of out-of-domain text is still not satis-
factory as that of in-domain text, while few pre-
vious works have paid attention to this problem. 
To further improve the performance of the ba-
sic system in out-of-domain text, we use a semi-
supervised learning procedure to incorporate the 
unlabeled corpora of Literature (Unlabeled-A) 
and Computer (Unlabeled-B). The final results 
show that our system performs well on all four 
testing-sets and achieves comparable segmenta-
tion results with other participants. 
2 Our system 
2.1 Character-Based Joint Model 
The character-based joint model in our system 
contains two basic components:  
? The character-based discriminative model.  
? The character-based generative model. 
The character-based discriminative model 
(Xue, 2003) is based on a Maximum Entropy 
(ME) framework (Ratnaparkhi, 1998) and can be 
formulated as follows: 
2
1 1 1 2
1
( ) ( ,
n
n n k
k k k
k
P t c P t t c +? ?
=
?? )  (1) 
Where tk is a member of {Begin, Middle, End, 
Single} (abbreviated as B, M, E and S from now 
on) to indicate the corresponding position of 
character ck in its associated word. For example, 
the word ???? (Beijing City)? will be as-
signed with the corresponding tags as: ?? /B 
(North) ?/M (Capital) ?/E (City)?.  
This discriminative module can flexibly in-
corporate extra features and it is implemented 
with the ME package1 given by Zhang Le. All 
training experiments are done with Gaussian 
prior 1.0 and 200 iterations. 
The character-based generative module is a 
character-tag-pair-based trigram model (Wang et 
al., 2009) and can be expressed as below: 
1
1
1
([ , ] ) ([ , ] [ , ] ).
n
n
i i
i
P c t P c t c t ??
=
?? 2i  (2) 
In our experiments, SRI Language Modeling  
Toolkit2 (Stolcke, 2002) is used to train the gen-
erative trigram model with modified Kneser-Ney 
smoothing (Chen and Goodman, 1998). 
The character-based joint model combines the 
above discriminative module and the generative 
module with log-linear interpolation as follows: 
1
2
2
1 2
( ) log( ([ , ] [ , ] ))
(1 ) log( ( , ))
k
k k
k
k k k
Score t P c t c t
P t t c
?
?
?
?
+
? ?
= ?
+ ? ?
k
 (3) 
Where the parameter (0.0 1.0)? ?? ?  is the 
weight for the generative model. Score(tk) will 
be directly used during searching the best se-
quence. We set an empirical value ( 0.3? = ) to 
this model as there is no development-set for 
various domains. 
2.2 Features 
In this work, the feature templates adopted in the 
character-based discriminative model are very 
simple and are listed below: 
1
1 1
2 1 0 1
( ) ( 2, 1,0,12);
( ) ( 2, 1,0,1);
( ) ;
( ) ( ) ( ) ( ) ( ) ( )
n
n n
a C n
b C C n
c C C
d T C T C T C T C T C
+
?
? ?
= ? ?
= ? ?
2
 
In the above templates, Cn represents a char-
acter and the index n indicates the position. For 
example, when we consider the third character 
??? in the sequence ???????, template (a) 
results in the features as following: C-2=?, C-1=
?, C0=?, C1=?, C2=?, and template (b) gen-
erates the features as: C-2C-1=??, C-1C0=??, 
                                                 
1 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 
2 http://www.speech.sri.com/projects/srilm/ 
C0C1=??, C1C2=??, and template (c) gives 
the feature C-1C1=??.  
Template (d) is the feature of character type. 
Five types classes are defined: dates (???, ???, 
???, the Chinese character for ?year?, ?month? 
and ?day? respectively) represents class 0; for-
eign alphabets represent class 1; Arabic and 
Chinese numbers represent class 2; punctuation 
represents class 3 and other characters represent 
class 4. For example, when we consider the 
character ??? in the sequence ?????Q?, 
the feature T C  will 
be set to ?20341?. 
2 1 0 1 2( ) ( ) ( ) ( ) ( )T C T C T C T C? ?
When training the character-based discrimina-
tive module, we convert all the binary features 
into real-value features, and set the real-value of 
C0 to be 2.0, the value of C-1C0 and C0C1 to be 
3.0, and the values of all other features to be 1.0. 
This method sounds a little strange because it is 
equal to duplicate some features for the maxi-
mum entropy training. However, it effectively 
improves the performance in our previous works. 
2.3 Restrictions in constructing lattice 
As the closed track allows the participants to use 
the character type information, we add some re-
strictions to our system when constructing the 
character-tag lattice. When we consider a char-
acter in the sequence, the type information of 
both the previous and the next character would 
be taken into account. The restrictions are list as 
follows: 
z If the previous, the current and the next 
characters are all English or numbers, we 
would fix the current tag to be ?M?; 
z If the previous and the next characters are 
both English or numbers, while the current 
character is a connective symbol such as ?-?, 
?/?, ?_?, ?\? etc., we would also fix the cur-
rent tag to be ?M?; 
z Otherwise, all four tags {B, E, M, S} would 
be given to the current character. 
It is shown that in the Computer domain these 
simple restrictions not only greatly reduce the 
number of words segmented, but also speed up 
the system. 
Domain Mark OOV Rate R P F1 ROOV RIV 
Literature A 0.069 0.937 0.937 0.937 0.652 0.958 
Computer B 0.152 0.941 0.940 0.940 0.757 0.974 
Medicine C 0.110 0.930 0.917 0.923 0.674 0.961 
Finance D 0.087 0.957 0.956 0.957 0.813 0.971 
Table 1: Official segmentation results of our system. 
Algorithm 1: Semi-Supervised Learning 
Given: 
z Labeled training corpus: L 
z Unlabeled training corpus: U  
1: Use L to train a segmenter S ;  0
2: Use S  to segment the unlabeled corpus U  
and then get labeled corpus U ; 
0
0
3: for i  to K  do = 1
4: Add U  to L and get a new corpus Li;i?1
Use Li to train a new segmenter Si; 5: 
6: Use Si to segment the unlabeled corpus 
 and then get labeled corpus Ui; U
7:     if convergence criterion meets 
8:          break 
8: end for 
Output: the last segmenter S  K
 
2.4 Semi-Supervised Learning 
In the last decade, Chinese word segmentation 
has been improved significantly and gets a high 
precision rate in performance. However, the per-
formance for out-of-domain text is still unsatis-
factory at the present. Also, few works have paid 
attention to the cross-domain problem in Chi-
nese word segmentation task so far. 
Self-training and Co-training are two simple 
semi-supervised learning methods to incorporate 
unlabeled corpus (Zhu, 2006). In this work, we 
use an iterative self-training method to incorpo-
rate the unlabeled data. A segmenter is first 
trained with the labeled corpus. Then this seg-
menter is used to segment the unlabeled data. 
Then the predicted data is added to the original 
training corpus as a new training-set. The seg-
menter will be re-trained and the procedure re-
peated. To simplify the task, we fix the weight  
0.3? =  for the generative module of our joint 
model in the training iterations. The procedure is 
shown in Algorithm 1. The iterations will not be 
ended until the similarity of two segmentation 
results Ui?1 and Ui reach a certain level. Here we 
used F-score to measure the similarity between 
?1 and Ui: treat Ui?1 as the benchmark, Ui as a 
testing-set. From our observation, this method 
converges quickly in only 3 or 4 iterations for 
both Literature and Computer corpora. 
Ui
3 Experiments and Discussion 
3.1 Results 
In this CIPS-SIGHAN bakeoff, we only partici-
pate the closed track for simplified-character text. 
There are two kinds of training corpora:  
z Labeled corpus from News Domain 
z Unlabeled corpora from Literature Do-
main (Unlabeled-A) and Computer Do-
main (Unlabeled-B). 
Also, the testing corpus covers four domains: 
Literature (Testing-A), Computer (Testing-B), 
Medicine (Testing-C) and Finance (Testing-D). 
As there are only two unlabeled corpora for 
Domain A and B, we thus adopt different strate-
gies for each testing-set: 
z Testing-A: Character-Based Joint Model 
with semi-supervised learning, training 
on Labeled corpus and Unlabeled-A; 
z Testing-B: Character-Based Joint Model 
with semi-supervised learning, training 
on Labeled corpus and Unlabeled-B; 
z Testing-C and D: Character-Based Joint 
Model, training on Labeled corpus; 
Table 1 shows that our system achieves F-
scores for various testing-sets: 0.937 (A), 0.940 
(B), 0.923 (C) and 0.957 (D), which are compa-
rable with other systems. Among those four test-
ing domains, our system performs unsatisfactor-
ily on Testing-C (Medicine) even the OOV rate 
of this domain is not the highest. There are pos-
sible reasons for this result: (1) Semi-supervised 
learning is not conducted for this domain; (2) the 
statistical property between News and Medicine 
are significantly different. 
Domain Model F1 ROOV 
J + R + S 0.937 0.652 
J + S 0.937 0.646 
J + R 0.936 0.646 
A 
J 0.936 0.642 
J + R + S 0.940 0.757 
J + S 0.931 0.721 
J + R 0.938 0.744 
B 
J 0.927 0.699 
J + R 0.923 0.674 C 
J 0.923 0.674 
J + R 0.957 0.813 
D 
J 0.954 0.786 
Table 2: Performance of various approaches 
J: Baseline, the character-based joint model 
R: Adding restrictions in constructing lattice 
S: Conduct Semi-Supervised Learning 
 
3.2 Discussion 
The aim of restrictions in constructing lattice is 
to improve the performance of English and nu-
merical expressions, both of which appear fre-
quently in Computer and Finance domain. 
Therefore, the improvements gained from these 
restrictions are significantly in these two do-
mains (as shown in Table 2). 
Besides, the adopted semi-supervised learning 
procedure improves the performance in Domain 
A and B., but the improvement is not significant. 
Semi-supervised learning aims to incorporate 
large amounts of unlabeled data. However, the 
size of unlabeled corpora provided here is too 
small. The semi-supervised learning procedure is 
expected to be more effective if a large amount 
of unlabeled data is available. 
4 Conclusion 
Our system is based on a character-based joint 
model, which combines a generative module and 
a discriminative module. In addition, we applied 
a semi-supervised learning method to the base-
line approach to incorporate the unlabeled cor-
pus. Our system achieves comparable perform-
ance with other participants. However, cross-
domain performance is still not satisfactory and 
further study is needed. 
Acknowledgement 
The research work has been partially funded by  
the Natural Science Foundation of China under 
Grant No. 60975053, 90820303 and 60736014, 
the National Key Technology R&D Program 
under Grant No. 2006BAH03B02, and also the 
Hi-Tech Research and Development Program 
(?863? Program) of China under Grant No. 
2006AA010108-4 as well. 
References 
Stanley F. Chen and Joshua Goodman, 1998. An em-
pirical study of smoothing techniques for language 
modeling. Technical Report TR-10-98, Harvard 
University Center for Research in Computing 
Technology. 
Wenbin Jiang, Liang Huang, Qun Liu and Yajuan Lu, 
2008. A Cascaded Linear Model for Joint Chinese 
Word Segmentation and Part-of-Speech Tagging. 
In Proceedings of ACL, pages 897-904. 
Adwait Ratnaparkhi, 1998. Maximum entropy mod-
els for natural language ambiguity resolution. Uni-
versity of Pennsylvania. 
Andreas Stolcke, 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Proc-
essing, pages 311-318. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, 
Daniel Jurafsky and Christopher Manning, 2005. A 
Conditional Random Field Word Segmenter for 
Sighan Bakeoff 2005. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Process-
ing, pages 168-171. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2009. 
Which is more suitable for Chinese word segmen-
tation, the generative model or the discriminative 
one? In Proceedings of the 23rd Pacific Asia Con-
ference on Language, Information and Computa-
tion (PACLIC23), pages 827-834. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2010. 
A Character-Based Joint Model for Chinese Word 
Segmentation. To appear in COLING 2010. 
Nianwen Xue, 2003. Chinese Word Segmentation as 
Character Tagging. Computational Linguistics and 
Chinese Language Processing, 8 (1). pages 29-48. 
Ruiqiang Zhang, Genichiro Kikui and Eiichiro 
Sumita, 2006. Subword-based Tagging for Confi-
dence-dependent Chinese Word Segmentation. In 
Proceedings of the COLING/ACL, pages 961-968. 
Xiaojin Zhu, 2006. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison. 
