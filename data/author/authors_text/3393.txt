Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1212?1221,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Fully Lexicalising CCGbank with Hat Categories
Matthew Honnibal and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{mhonn,james}@it.usyd.edu.au
Abstract
We introduce an extension to CCG that al-
lows form and function to be represented
simultaneously, reducing the proliferation
of modifier categories seen in standard
CCG analyses.
We can then remove the non-combinatory
rules CCGbank uses to address this prob-
lem, producing a grammar that is fully lex-
icalised and far less ambiguous.
There are intrinsic benefits to full lexi-
calisation, such as semantic transparency
and simpler domain adaptation. The clear-
est advantage is a 52-88% improvement
in parse speeds, which comes with only a
small reduction in accuracy.
1 Introduction
Deep grammars return parses that map transpar-
ently to semantic analyses, allowing information
extraction systems to deal directly with content
representations. Usually, this mapping is lexically
specified, by linking lexical entries to semantic
analyses. This property, lexicalisation, is central to
some of the linguistic theories behind deep gram-
mars, particularly Combinatory Categorial Gram-
mar (Steedman, 2000) and Lexicalised Tree Ad-
joining Grammar (Joshi, 1999).
Lexicalisation can also help deep grammars
achieve satisfactory parse times. Lexicalised
grammars use few rules, which simply manipu-
late the lexical categories. The categories can be
quickly assigned in a supertagging pre-process,
dramatically reducing the search space the parser
must explore (Bangalore and Joshi, 1999).
Combinatory Categorial Grammar (CCG) is
well suited to this strategy, and Clark and Curran
(2007) have highlighted the division of labour be-
tween the parser and the supertagger as one of the
critical aspects of their approach to statistical CCG
parsing. In their system, the division is managed
with parameters that control how many categories
the parser?s chart is seeded with. But even if the
parser is only given one category per word, it still
has a lot of freedom ? because the grammar it
uses is not fully lexicalised.
In a fully lexicalised CCG grammar, modifier
categories refer to the category of their head. This
category does not necessarily represent the head?s
constituent type. For instance, the category of an
adverb like still depends on whether it is modify-
ing a predicate verb (1), or a clausal adjunct (2):
(1) The lion
NP
was
VP/VP
lying
VP
still
VP\VP
(2) The lion
NP
waited,
VP
lying
VP\VP
still
(VP\VP)\(VP\VP)
Analyses like these are problematic because the
training data is unlikely to include examples of
each word in every syntactic environment that re-
quires a new category. Hockenmaier and Steed-
man?s (2007) solution was to add category spe-
cific phrase-structure rules to the grammar, which
disrupts the linguistic principles of the formalism,
and introduces over-generation and ambiguity as
shown in Figure 1.
This paper proposes a new way to balance lex-
ical and grammatical ambiguity in CCG. We in-
troduce an extension to the formalism that allows
type-changing rules to be lexically specified. The
extension adds a new field to the category objects,
and one additional rule to utilise it. This allows the
formalism to express type-changing operations in
a theoretically desirable way.
Lexically specifying the type-changing rules re-
duces the ambiguity in the grammar substantially,
which leads to substantial improvements in pars-
ing efficiency. After modifying the C&C parser and
CCGbank, the parser runs almost twice as quickly,
with only a 0.5% reduction in accuracy.
1212
Jamie Pat Robin loves
NP NP NP (S[dcl]\NP)/NP
PSG >T
S/(S/NP) S/(S\NP)
>B
S[dcl]/NP
PSG
>
S[dcl]
PSG
NP\NP
<
NP
Figure 1: Over-generation by CCGbank rules.
2 Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a lexicalised grammar formalism
based on categorial grammar (Bar-Hillel, 1953).
CCG can be distinguished from other CG exten-
sions, such as categorial type-logic (Moortgat,
1997) by its attention to linguistic minimalism.
One aim of the theory is to explain universal con-
straints on natural language syntax, so the genera-
tive power of the formalism is intended to closely
match what natural language seems to require.
Steedman and Baldridge (2007) argue that the
requirements can be fulfilled almost entirely by
two basic rule types: application and composition.
Direction specific instances of these types yields a
grammar that consists of just six rules.
Initially, it seemed that some of the rules
had to be restricted to certain contexts, particu-
larly in languages that did not allow scrambling.
Baldridge and Kruijff (2003) have since shown
that rules could be restricted lexically, using a hier-
archy of slash subtypes. This relieved the need for
any language specific meta-rules, allowing CCG to
offer a completely universal grammar, and there-
fore a theory of the innate human language faculty.
With a universal grammar, language specific
variation is confined to the lexicon. A CCG lexi-
cal category is either an atomic type, like N, or a
function that specifies an argument in a particular
direction, and a result, like S\NP (where S is the
result, NP the argument, and \ indicates the argu-
ment must be found to the left).
Hockenmaier and Steedman (2007) showed that
a CCG corpus could be created by adapting the
Penn Treebank (Marcus et al, 1993). CCGbank
has since been used to train fast and accurate CCG
parsers (Clark and Curran, 2007).
3 The Need for Type-changing in CCG
We argue that there is a clear need for some sort of
type-changing mechanism in CCG. The practical
need for this has been known since at least Hock-
enmaier (2003), who introduced a type-changing
mechanism into CCGbank in order to control the
problem referred to as modifier category prolifer-
ation. We briefly describe the problem, and then
the prominent solutions that have been proposed.
Unlike formalisms like LTAG and HPSG, CCG
does not use different grammatical rules for argu-
ments and adjuncts. Instead, modifier categories
take the form X
1
|X
1
, where X is the category of
the constituent being modified, and the subscript
indicates that the result should inherit from the ar-
gument via unification. The modifier can then use
the application rule to attach to its head, and return
the head unchanged:
(3) unusually
(S[adj]\NP)/(S[adj]\NP)
resilient
S[adj]\NP
unusually here modifies the predicative adjec-
tive resilient, attaching it as an argument using
forward application. This prevents resilient from
having to subcategorise for adjuncts, since they are
optional. The problem is that unusually must sub-
categorise for the function of its head. If resilient
changes function and becomes a noun modifier, its
modifiers must change category too:
(4) an
NP/N
unusually
(N/N)/(N/N)
resilient
N/N
strain
N
There is often a way to analyse around the need
for type-changing operations in CCG. However,
these solutions tend to cause new difficulties, and
the resulting category ambiguity is quite problem-
atic (Hockenmaier and Steedman, 2002). The fact
is that form-to-function coercions are quite com-
mon in English, so the grammar needs a way to
have a constituent be modified according to its
form, before undergoing a type-change to its func-
tion category.
One way to describe the problem is to say that
CCG categories have an over-extended domain of
locality (Joshi, 1999), the part of the derivation
that it describes. A category should specify all and
only the dependencies it governs, but CCG mod-
ifier categories are often forced to specify their
heads? dependencies as well. These undesirable
notational dependencies can also prevent modi-
fier categories from factoring recursion away from
their domain of locality.
1213
Sh
h
h
h
h
h
h
h
h
h
h
h
h
(
(
(
(
(
(
(
(
(
(
(
(
(
S
`
`
`
`
`
`
 
 
 
 
 
 
NP
It
S[dcl]\NP
h
h
h
h
h
h
h
h
(
(
(
(
(
(
(
(
(S[dc]\NP)/NP
is
NP
X
X
X
X
X





NP
P
P
P
P




NP
the fourth time
NP\NP
b
b
"
"
(NP\NP)/N
this
N
week
NP\NP
S[dcl]
it has happened
S\S
a
a
a
!
!
!
,
,
NP
P
P
P
P




NP/NP
almost
NP
P
P
P
P




NP
@ 
NP/N
a
N
way
NP\NP
b
b
"
"
(NP\NP)/NP
of
NP
N
life
Figure 2: CCGbank derivation showing PSG rules.
4 Problems with Existing Proposals
This section completes the motivation of the pa-
per by arguing that the existing proposals for type-
changing are linguistically unsatisfactory, practi-
cally difficult, or a combination of the two.
4.1 Problems with PSG Rules
Hockenmaier and Steedman (2002) includes a
brief discussion of the modifier category prolif-
eration problem, and introduces unary phrase-
structure rules to address the situation. Figure 2
shows two such rules. The <S[dcl] ? NP\NP>
1
rule allows the reduced relative clause, it has hap-
pened, to be analysed as a modifier without affect-
ing the category any modifiers that might attach to
it. The other PSG type-changing rule in the deriva-
tion, <, NP? S\S> enables the extraposition, us-
ing the punctuation to make the rule more precise.
One alternative to type-changing rules here
would be to have time subcategorise for the clause,
with a category like NP/S[dcl]. This would cap-
ture the constraint that only a restricted subset of
nouns can be extracted as adjuncts in this way.
The problem is that the extra argument would in-
terfere with the attachment of adjuncts like this
week to the NP, because the NP\NP category can-
not be allowed to participate in backwards cross-
composition rules (Baldridge and Kruijff, 2003).
There are 204 type-changing PSG rules in the
training partition of CCGbank. 53 of the frequent
rules transform produce modifier categories, 48
of them transforming verbal categories. The PSG
1
Phrase-structure rules are presented in bottom-up notation.
rules also handle a variety of other constructions,
such as form/function discrepancies like gerund
nominals. By far the most frequent rule, with
115,333 occurrences, is <N ?NP>, which trans-
forms bare nominals into noun phrases.
Steedman and Baldridge (2007) describes the
CCG grammar as consisting of just 6 language uni-
versal combinatory rules, plus two lexical opera-
tions (type raising). Not only do the 204 category
specific type-changing rules in CCGbank make
the grammar ambiguous, they also run contrary to
the design principles of the formalism.
CCG is a linguistically motivated formalism,
which means it is not only interested in providing a
convenient, computable notation for grammar de-
velopment. In addition, it constitutes a hypothesis
about the nature of the human language faculty.
Like other lexicalised formalisms, part of the the-
ory is that it is the grammar that is innate, and the
lexicon is acquired.
If the grammar is innate, it must be language
universal, confining all language specific varia-
tion to the lexicon. Baldridge and Kruijff (2003)
described how the remaining language specific
grammatical constraints described by Steedman
(2000) could be controlled in the lexicon, using
multi-modal slashes that have since become inte-
grated into the main body of the theory (Steedman
and Baldridge, 2007).
In addition to being linguistically undesirable,
the PSG rules in CCGbank produce practical dif-
ficulties. Every additional rule increases ambigu-
ity, motivating the C&C system to choose to im-
plement only the most frequent. This decreases
1214
the parser?s coverage, and introduces another di-
mension of domain sensitivity. For instance, the
type-changing rule that allows gerund nominals,
<S[ng]\NP ? NP>, occurs roughly 300 times in
the training data. The parser does not implement
this rule, so if it is ported to a new domain, where
the construction is frequent, the rule will have to
be added. Presumably, the parser would also ben-
efit from the removal of rules which are infrequent
in some new, target domain.
The restricted set of PSG rules the parser does
implement results in considerable added ambigu-
ity to the grammar. Figure 1 shows how the rules
interact to produce over-generation.
The PSG rules are also a barrier to the semantic
transparency of the theory, one of its most attrac-
tive properties for natural language engineering.
CCG derivations are isomorphic to semantic analy-
ses, because the derivation instantiates dependen-
cies between CCG categories that can be paired
with semantic categories. This isomorphism is
disrupted by the addition of PSG rules, since the
grammar is no longer lexicalised. Often, the rules
can be semantically annotated, restoring the iso-
morphism; but sometimes, this cannot be done.
For instance, the extraposition rule in Figure 2
transforms the NP category into S\S. There is no
syntactic argument on the NP category to map the
dependency to, so the dependency cannot be cre-
ated (and is in fact missing from CCGbank).
4.2 Lexical Rules and Zero Morphemes
The CCGbank PSG extension is closely related to
the zero morpheme categories proposed by Aone
and Wittenburg (1990), which they suggest be
compiled into unary type-changing rules for pro-
cessing. At first glance, it seems that conceptual-
ising the rules as zero morphemes offers a way to
locate them in the lexicon, avoiding the linguistic
difficulties of having a language-specific grammar.
However, CCG aims to provide a transparent inter-
face between the surface form and the semantic
analysis, so epsilon categories, traces, movement
rules and other unrealised structures are explicitly
banned (Steedman, 2000).
From a processing standpoint, if zero mor-
pheme categories are not compiled into phrase-
structure rules, then they will complicate the cate-
gory assignment phase considerably, since we can
no longer assume that exactly one category will be
assigned per word. We are not aware of any pro-
posal for how this difficulty might be overcome.
Carpenter (1992) provides a different sugges-
tion for how sparse modifier categories can be ac-
commodated. His solution is to use meta-rules
that systematically expand the lexicon, much like
the lexical rules used in HPSG (Flickinger, 1987),
which exploit structural regularities to ensure that
the lexicon is more complete.
The problem with this is that it does not actu-
ally make the category set less sparse, so the su-
pertagger?s task is just as difficult. The only ad-
vantage is that its dictionary will be more com-
plete. This is important, but does not solve the
underlying inefficiency in the grammar: CCG cat-
egories have an over-extended domain of locality,
because they cannot represent form and function
simultaneously. This is why some type-changing
mechanism is required.
5 Lexically Specified Type-Changing
This section describes our mechanism for lexi-
cally specifying the PSG rules in CCGbank. Figure
3 shows an example of a reduced relative clause
analysed using our extension, hat categories.
CCGbank deploys a solution that achieves form
transparency at the expense of type transparency,
by allowing type-changing rules that are not lexi-
cally specified. One way to recover the lost type
transparency would be to demand that lexical cat-
egories specify what type changing rule (if any)
the category can eventually undergo. For instance,
imagine we have two type-changing rules we wish
to include in our grammar:
a) S[ng]\NP ? NP\NP
b) S[ng]\NP ? VP\VP
2
With these two rules, there will be three ways
the S[ng]\NP category might behave in a deriva-
tion. What we need are two extra categories to
control this:
1. S[ng]\NP only allows combinatory rules.
2. (S[ng]\NP)
a
allows rule a, but not rule b.
3. (S[ng]\NP)
b
allows rule b, but not rule a.
Instead of encoding a reference to a rule, we
encode the production rule itself in the category.
2
S\NP is occasionally abbreviated as VP.
1215
asbestos once used in cigarette filters
N
N
P VP/VP (S[pss]\NP)
(NP\NP)
(VP\VP)/NP N/N N
N
P
H
> >
NP (S[pss]\NP)
(NP\NP)
N
H
NP
>
VP\VP
<
S[pss]
(
NP\NP)
H
NP\NP
<
NP
Figure 3: Analysis of a reduced relative clause with lexicalised type-changing.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
CAT (S[ng]\NP)
NP\NP
/NP
RES
?
?
?
?
?
?
?
?
?
?
?
CAT (S[ng]\NP)
NP\NP
RES
[
CAT S
FEAT ng
]
ARG NP
1
DIR \
HAT NP\NP
1
?
?
?
?
?
?
?
?
?
?
?
ARG NP
DIR /
HAT [-]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 4: AVM of a hat category.
This allows us to remove the rule from the gram-
mar. Since the bottom of the production will al-
ways be the category itself, we can just specify
how the category can be rewritten:
1. S[ng]\NP can be combined, but not rewritten.
2. (S[ng]\NP)
NP\NP
can be rewritten as NP\NP.
3. (S[ng]\NP)
VP\VP
can be rewritten as VP\VP.
We refer to the superscript category as a hat cat-
egory, as a reference to the notation, but also to
denote the fact that it allows the category to per-
form a different function, or put a different ?hat?
on. Categories that have a hat specified are re-
ferred to as hatted categories.
5.1 Changes to Category Objects
Figure 4 shows an AVM representation of the
(S[ng]\NP)
NP\NP
/NP category. A field, labelled
hat, has been added to store the destination cate-
gory of the result, NP\NP. The NP argument in
the hat category is co-indexed with the NP argu-
ment in the hatted category. The NP argument is
also co indexed with the result of the destination
category, reflecting the fact that the NP\NP cate-
gory is a modifier, whose head will be the head of
its argument.
Hat categories are handled the same as any other
field during unification. If the two hat fields can-
not be unified, unification fails; and if one hat field
has an empty value, it inherits the value of the hat
field of the other category when unification suc-
ceeds. CCG already requires a unification process
for agreement features (Hockenmaier and Steed-
man, 2007); the hat categories we have introduced
behave identically.
As Figure 4 shows, hat categories can be added
to inner results, allowing arguments to be applied
before the type-changing rule. We add a restriction
that prevents categories with an outermost hat field
from applying arguments ? essentially equiva-
lent to stipulating that the slash in a category like
S[ng]\NP must have a null mode.
We also stipulate that only adjuncts may ap-
ply hatted arguments, which can also be lexically
represented by assuming that all non-adjunct cate-
gories have a null value in their hat field, causing
unification with a hatted category to fail.
Together, these restrictions ensure that the unary
rule is used. The hatted category cannot function
as a non-hatted category, because it cannot use its
own arguments, and cannot be used as an argu-
ment of another category. This prevents hat cate-
gories from forming categories that are function-
ally disjunctive: the notation cannot be used to
simulate something like an optional argument.
5.2 The Type-Change Rule
To replace the 204 PSG rules in CCGbank, we only
need to introduce one extra schematic rule into the
grammar:
X
Y
? Y (5)
This rule simply unpacks the category, performing
the lexically specified type-change.
1216
5.3 Generative Power
Because hat fields are only transmitted when cate-
gories are successfully unified, there is no way to
produce a novel X ? Y unary production during a
derivation. This means that any derivation that can
be produced using the schematic type-change rule
we have added to the grammar can be produced by
adding a set of unary phrase-structure rules instead
? ensuring that we do not require any extra gen-
erative power than is required to parse CCGbank.
The hat categories do increase the strong gen-
erative power of a CCG grammar that does
not include the CCGbank type-changing rules.
We suggest that this is desirable, in line with
Joshi?s (1999) argument that formalisms should be
designed to have the maximum expressivity while
maintaining the minimum weak generative power
necessary to produce the constructions that have
been observed in natural language.
6 Lexicalising Type-raising
So far, we have focused on replacing the phrase-
structure rules added to CCGbank, which are not
part of the CCG linguistic theory. However, the
theory does include some type-changing rules, re-
ferred to as type-raising. Forward and backward
type-raising are used to transform a category X
into the logically equivalent categories T/(T\X)
and T\(T/X) respectively.
Type-raising is generally described as a lexical
operation, rather than a grammatical rule, because
only certain language specific combinations of T
and X produce valid type-raise categories. How-
ever, no specific mechanism for controlling type-
raising has been proposed.
Hat categories are an obvious candidate for this,
so we perform an additional set of experiments
which lexicalise the type-raising rules in CCG-
bank, in addition to the PSG rules.
7 Adapting CCGbank
This section describes how we converted CCG-
bank?s PSG rules into analyses that used hat cat-
egories. Most of the PSG rules are unary, which
meant that our changes were limited to adding hat
categories to the child of the unary production and
its subtree. The binary PSG rules that we con-
verted effectively just used punctuation as a cue
for a unary type-change, as seen in the extrapo-
sition rule in Figure 2. These were handled by
adding an extra node for the punctuation applica-
tion, leaving a unary production:
S\S
e%
, NP
?? S\S
e
e
%
%
, S\S
NP
(6)
An alternative analysis would be to assign the
punctuation mark a category to perform the type-
change ? in this case, (S\S)/NP. However, this
analysis will be unreliable for many genres, where
punctuation is used inconsistently, so we preferred
that hat category analysis, which we found pro-
duced slightly better results.
We used the same method to convert cases
where CCGbank used conjunctions to cue a type-
change, where the Penn Treebank conversion pro-
cess produced a derivation where two sides of a
coordination had different categories. There were
90 such conjunction coercion rules, which we have
not counted amongst the 204 PSG rules, since they
are ultimately caused by conversion noise.
The main complication when adapting CCG-
bank was the fact that CCG node labels are inter-
dependent through a derivation. If one node label
is changed, its immediate children have to change
node label too, and the changes must be propa-
gated further from there.
Since the dependency between the parent and its
two children is different for each combinator, our
node change rules determine the rule used for the
original production, and then invoke the appropri-
ate replacement rule. In general, the rules find the
result (A
r
) and argument (A
a
) of the original parent
A and replace them with the appropriate part of the
new parent B. If one of the children is an adjunct
category, a different rule is used. The node change
rules for forward combinators are:
App A/Y Y ? B/Y Y
Comp A
r
/Y Y/A
a
? B
r
/Y Y/B
a
Adj. app A/A A ? B/B B
Adj. comp A
r
/A
r
A
r
/A
a
? B
r
/B
r
B
r
/B
a
The translation rules for backward and crossed
combinators are directly analogous, with the
slashes permuted appropriately.
8 Adapting the CCG Parser
We took the standard 1.02 release of the C&C
parser Clark and Curran (2007) and implemented
the changes required for lexically specified type-
changing.
1217
Section 00 Section 23
LP LR LF LF
auto
sent cat cov LP LR LF LF
auto
sent cat cov
CCGbank derivs 87.18 86.31 86.74 84.78 35.15 94.04 99.06 87.76 86.99 87.38 84.84 37.03 94.26 99.63
Hat derivs 86.64 86.91 86.77 84.44 35.03 93.27 99.53 86.94 87.26 87.10 84.76 36.62 93.35 99.71
Hat+TR derivs 86.58 86.87 86.73 84.16 34.47 93.10 99.63 86.83 87.16 87.00 84.67 36.73 93.17 99.75
CCGbank hybrid 88.07 86.49 87.27 85.30 35.94 94.16 99.06 88.36 87.02 87.68 85.27 36.74 94.33 99.63
Hat hybrid 87.30 86.94 87.12 84.85 35.40 93.31 99.53 87.26 87.03 87.15 84.79 36.25 93.24 99.71
Hat+TR hybrid 85.79 85.30 85.55 83.13 31.90 92.48 99.63 85.93 85.65 85.79 83.39 32.03 92.46 99.75
Table 1: Labelled Precision, Recall and F-measure, coverage results on Section 00 and Section 23.
The most significant change was building hat
passing and unification into the existing unifica-
tion engine. For many parsers, this would have
been straightforward since they already support
unification with complex feature structures. How-
ever, one of the advantages of CCGbank is that
the unification required is quite simple, which is
one of the reasons why the C&C parser is very fast.
We would estimate that adding hat passing dou-
bled the complexity of the unification engine.
The second step was to add support for hat pass-
ing to all of the existing combinators, because they
do not use the unification engine to construct the
result category. Since each of the combinators
is hard-coded for speed, this was time-consuming
and error prone. However, we created a detailed
set of regression tests for the new versions which
greatly reduced our development time.
Finally, we needed to turn off the existing unary
rules in the parser, and add the simple additional
type-change rule.
9 Setting Dictionary Thresholds
The main parameterisation we performed on the
development section was to tune the K parame-
ter of the parser, which controls the frequency at
which a word?s tag dictionary is used during su-
pertagging. For words more frequent than K, the
supertagger is restricted to choosing between cat-
egories that have been assigned to the word in the
training data. Otherwise, the POS dictionary is
used instead. The K parameter has multiple val-
ues, because the supertagger and parser are inte-
grated such that the supertagger initially supplies
only a narrow beam of categories to the parser,
which is widened if parsing fails.
Since we have made the category set larger, the
default values of K = 20,20,20,20,150 produces
poor performance, up to 1.5% lower than the fig-
ures we report in Table 1. We set the K parameter
Section 00 Section 23
Training Gold Auto Gold Auto
CCGbank derivs 399 413 639 544
Hat derivs 552 566 1070 827
Hat+TR derivs 718 677 1072 906
CCGbank hybrid 369 379 564 480
Hat hybrid 505 513 921 678
Hat+TR hybrid 645 601 913 785
Table 2: Parse speeds in words per second.
Original Hat
Types Frequency Types Frequency
Binary CCG 2,714 1,097,809 3,840 1,097,358
Type-raise 52 3,998 52 3,996
Unhat 0 0 241 161,069
Binary PSG 215 1,615 74 172
Unary PSG 157 159,663 0 0
Table 3: Production types and frequencies.
to 50,300,80,80,3000. We investigated the effect
of this setting on the original model, and found
that it had little effect, so we continued using the
default values for the original model.
We also experimented with altering the ? values
for the hat parser, which did not improve the per-
formance of the state-of-the-art parsing models.
10 Parsing Results
The left side of Table 1 shows our performance
on the development data, Section 00. All of the
dependency results we report refer to the original
dependencies distributed with CCGbank. To en-
sure our results were comparable, we produced a
mapping table of dependency labels from sections
02-21, used for parser training. The table maps
the dependency labels in our corpus to the most
frequent label assigned to matching dependencies
in CCGbank. The correct label is assigned 99.94%
of the time. The hat categories move to the the lex-
icon information that used to be represented in the
grammar, resulting in a larger, more informative
1218
category set, making the category accuracies (the
cat column in the table) not comparable.
We experimented with two of the parsing mod-
els described by Clark and Curran (2007). The
derivs model uses features calculated over the
derivations, while the hybrid model uses features
calculated on the dependency structures. How-
ever, unlike the deps model Clark and Curran
(2007) describe, the hybrid model uses two sets
of derivation-based constraints. One set are the
normal form constraints, as described by Eisner
(1996). It also uses constraints that prevent the
parser from using productions that were not seen
in the training data. The hybrid model is slightly
more accurate, but also slightly slower, because
the dependency-based decoding is less efficient.
All of the systems were within 0.5% in accuracy
on the development set, with one exception. The
HAT+TR version performed very poorly with the
hybrid model, while its performance with the de-
rivs model was comparable to the other systems.
The same drop in performance occurs on the eval-
uation set. We do not currently have a convincing
explanation for this, but we presume it is the re-
sult of some unforeseen interaction between the
removal of the type-raising rules from the gram-
mar and the dependency-based features.
The accuracy results on the test data, Section
23, saw similar trends, except that the gap be-
tween the hat systems and the original CCGbank
increased slightly. The CCGbank hybrid model
was only 0.1% more accurate than the HAT hybrid
model on Section 00, but is 0.5% more accurate
on Section 23.
Table 2 compares the parse speeds for the lexi-
calised hat corpora against a parser trained on the
original CCGbank, using the two models. Exactly
the same settings were used to obtain parse times
as were used in the accuracy experiments. The
experiments were all performed on a single core
2.6GHz Pentium 4 Xeon. Speeds are reported as
words parsed per second.
On both Section 00 and Section 23, with both
the derivs and hybrid models, the HAT system was
substantially faster than the original parser. The
HAT+TR system was faster than the HAT system
using automatic POS tags, and slightly faster on
Section 00.
The hat categories allow quite favourable trade-
offs between speed and accuracy to be made. The
original models allow us to parse with automatic
POS tags at 480 words per second with 85.27%
accuracy with the hybrid model, or at 544 words
per second with 84.86% accuracy using the derivs
model. Using the HAT derivs model, we could in-
stead parse at 827 words per second with 84.76%
accuracy, or at 906 words per second and 84.67%
accuracy using the HAT+TR system.
In summary, the HAT and CCGbank derivs mod-
els are equivalent in accuracy, but the HAT ver-
sion is 52% faster. The CCGbank hybrid model
remains the most accurate, but there will also be
many tasks where the 88% improvement in speed
will make it worth using the HAT+TR derivs parser
instead of the CCGbank hybrid model, at a cost of
0.6% accuracy.
11 Corpus Statistics
Table 3 shows the number of types and the number
of occurrences of CCG combinatory rules and PSG
rules occurred in CCGbank and the hat corpus.
The hat corpus removes almost all unlicensed
productions, leaving only a long tail of rare pro-
ductions that are the result of noisy derivations.
These productions are generally symptomatic of
problematic analyses, and are difficult to address
automatically because they do not conform to any
consistent pattern. We have omitted the hat+TR
corpus in these figures, because it only differs
from the the hat corpus with respect to type-raising
productions.
Lexicalising the corpus increases the number of
categories required substantially. There are 407
categories that occur 10 or more times in the train-
ing section of CCGbank. The equivalent figure for
the HAT corpus is 507, and for the HAT+TR corpus
it is 540.
12 Cleaner Analyses with Hat Categories
The lexicalised type-changing scheme we have
proposed offers many opportunities for favourable
analyses, because it allows form and function to
be represented simultaneously. However, we have
limited our changes to replacing the existing CCG-
bank non-combinatory rules. This allows us to
compare the two strategies for controlling modi-
fier category proliferation more closely, but still
offers some improved analyses.
The most frequent unary production in CCG-
bank, the N?NP rule, ensures that nominals can
always take the N category, so adjectives sel-
dom need to be assigned NP/NP. Because ad-
1219
jectives and nouns are open class, and bare noun
phrases are fairly common, this reduction in cate-
gory sparseness is quite important.
Lexicalising the type changing rule forces the
head noun to acquire a different category, but does
ensure that its modifiers can attach at the N level
? which is also more linguistically desirable:
service lift maintenance contracts
N/N N
N/N
N
N/N
N
>
N
N/N
H
N/N
>
N
N/N
H
N/N
>
N
This analysis also prevents the extreme category
proliferation problem caused by left-branching
noun phrases:
service lift maintenance contracts
((N/N)...(N/N)) (N/N)/(N/N) N/N N
>
(N/N)/(N/N)
>
N/N
>
N
Figure 3 shows a more typical example of an
improved analysis. The non-finite clause is func-
tioning as an adnominal, but its modifier is able to
select its canonical category.
One of the advantages of the CCGbank phrase-
structure rules is that they allow the corpus to in-
clude derivations for which no valid CCG parse can
be formed. The C&C parser has difficulty taking
advantage of these extra sentences, however, be-
cause only so many of the arbitrary binary PSG
rules can be added to the grammar without making
it too ambiguous. Once these rules are lexicalised,
the categories that produce them can be added to
the lexicon as unexceptional, albeit rare, cases.
13 Conclusion
Lexicalised grammars represent most of the infor-
mation in a derivation with a sequence of lexi-
cal categories. Traditional CCG analyses require
redundancy between categories whenever there
is nested modification, which suggests that such
analyses will encounter sparse data problems.
While the addition of phrase-structure rules pre-
vents this proliferation of modifier categories, it
does so at a high price. The bulk of the type-
changing rules in CCGbank are not implemented
in the C&C parser, because to do so would increase
the ambiguity in the grammar enormously.
CCG parsers must carefully manage ambiguity,
because there are many ways to bracket the same
CCG derivation. Even with a restricted set of PSG
rules, the C&C parser experiences very large chart
sizes. In addition to making the grammar more
ambiguous, the PSG rules make it less theoreti-
cally sound, and more difficult to produce seman-
tic analyses from the parser?s output.
We have show how CCG analyses can be fully
lexicalised in a way that closely mirrors the in-
troduction of phrase-structure rules. The result is
a corpus that produces faster, accurate parsers, is
well suited for domain adaptation, and allows for
more transparent semantic analysis. We can also
use the same mechanism to lexically specify type-
raising, the first concrete proposal to handle type-
raising as a lexical transformation we are aware of.
From an immediate, empirical perspective, we
have substantially improved the parsing speed of
what is already the fastest deep parser available.
Improvements in parsing efficiency are important
in making parsing a practical technology, since the
volume of text we have available for processing is
growing even faster than the processing resources
we have available.
Acknowledgements
We would like to thank Stephen Clark and the
anonymous reviewers for EMNLP and the Gram-
mar Engineering Across Frameworks workshop
for their valuable feedback. This work was sup-
ported by the Australian Research Council under
Discovery Project DP0665973.
References
Chinatsu Aone and Kent Wittenburg. 1990. Zero
morphemes in unification-based combinatory
categorial grammar. In ACL, pages 188?193.
Jason Baldridge and Geert-Jan Kruijff. 2003.
Multi-Modal Combinatory Categorial Gram-
mar. In Proceedings of the European Associ-
ation of Computational Linguistics (EACL).
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237?265.
Yehoshua Bar-Hillel. 1953. A quasi-arithmetical
notation for syntactic description. Language,
29:47?58.
1220
Bob Carpenter. 1992. Categorial grammars, lex-
ical rules, and the English predicative, chap-
ter 3. Oxford University Press.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguis-
tics, 33(4):493?552.
Jason Eisner. 1996. Efficient normal-form parsing
for Combinatory Categorial Grammar. In Pro-
ceedings of the 34th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-
96), pages 79?86. Santa Cruz, CA, USA.
Dan Flickinger. 1987. Lexical Rules in the Hierar-
chical Lexicon. Ph.D. thesis, Stanford Univer-
sity, Stanford, CA.
Julia Hockenmaier. 2003. Data and Models for
Statistical Parsing with Combinatory Catego-
rial Grammar. Ph.D. thesis, University of Ed-
inburgh.
Julia Hockenmaier andMark Steedman. 2002. Ac-
quiring compact lexicalized grammars from a
cleaner treebank. In Third LREC, pages 1974?
1981.
Julia Hockenmaier and Mark Steedman. 2007.
CCGbank: a corpus of CCG derivations
and dependency structures extracted from the
Penn Treebank. Computational Linguistics,
33(3):355?396.
Aravind K. Joshi. 1999. Explorations of a domain
of locality: Lexicalized tree-adjoining gram-
mar. In CLIN.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313?330.
Michael Moortgat. 1997. Categorial type logics.
In Johan van Benthem and Alice ter Meulen, ed-
itors, Handbook of Logic and Language, chap-
ter 2, pages 93?177. Elsevier, Amsterdam and
MIT Press, Cambridge MA.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
Mark Steedman and Jason Baldridge. 2007.
Combinatory categorial grammar. In Robert
Borsley and Kersti Borjars, editors, Non-
Transformational Syntax. Blackwells.
1221
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 89?96,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Creating a Systemic Functional Grammar Corpus from the Penn Treebank
Matthew Honnibal and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{mhonn, james}@it.usyd.edu.au
Abstract
The lack of a large annotated systemic func-
tional grammar (SFG) corpus has posed a
significant challenge for the development of
the theory. Automating SFG annotation is
challenging because the theory uses a mini-
mal constituency model, allocating as much
of the work as possible to a set of hierarchi-
cally organised features.
In this paper we show that despite the un-
orthodox organisation of SFG, adapting ex-
isting resources remains the most practical
way to create an annotated corpus. We
present and analyse SFGBank, an automated
conversion of the Penn Treebank into sys-
temic functional grammar. The corpus is
comparable to those available for other lin-
guistic theories, offering many opportunities
for new research.
1 Introduction
Systemic functional grammar (Halliday and
Matthiessen, 2004) aims to describe the set of
meaningful choices a speaker makes when putting a
thought into words. Each of these choices is seen as
a resource for shaping the meaning in a particular
way, and the selection will have a distinct grammat-
ical outcome as well as a semantic implication. The
choices are presented hierarchically, so that early
selections restrict other choices. For instance, if a
speaker chooses imperative mood for a clause, they
cannot choose a tense. Each selection is linked to a
syntactic expression rule. When imperative mood
is selected, the subject of the clause is suppressed;
when interrogative mood is selected, the order of
the subject and first auxiliary are reversed.
Systemic grammars are very different from gram-
mars influenced by the formalist tradition. Systemic
analysis locates a constituent within a typology, and
yields a set of features that describe its salient prop-
erties. These features have proven useful for re-
search in applied linguistics, on topics such as stylis-
tics, discourse analysis and translation. As a gener-
ative theory, systemic grammars are less effective.
There have been a few attempts, such as those dis-
cussed by O?Donnell and Bateman (2005), but as yet
a wide coverage systemic grammar that can be used
for tractable parsing has not been developed.
The lack of a corpus and parser has limited re-
search on systemic grammars, as corpus studies have
been restricted to small samples of manually coded
examples, or imprecise queries of unannotated data.
The corpus we present, obtained by converting the
Penn Treebank, addresses this issue. It also suggests
a way to automatically code novel text, by convert-
ing the output of a parser for a different formalism.
This would also allow the use of SFG features for
NLP applications to be explored, and support current
research using SFG for applied linguistics.
The conversion process relies on a set of manually
coded rules. The first step of the process is to col-
lect SFG clauses and their constituents from parses in
the Penn Treebank. Each clause constituent is then
assigned up to three function labels, for the three si-
multaneous semantic and pragmatic structures Hal-
liday (1970) describes. Finally, the system features
are calculated, using rules referring to the function
labels assigned in the previous step. This paper ex-
tends the work described in Honnibal (2004).
89
2 Related Work
Converting the Penn Treebank is the standard ap-
proach to creating a corpus annotated according to a
specific linguistic theory. This has been the method
used to create LTAG (Frank, 2001), LFG (Frank
et al, 2003) and CCG (Hockenmaier and Steedman,
2005) corpora, among others. We employ a similar
methodology, converting the corpus using manually
specified rules.
Since the SFG annotation is semantically oriented,
the work also bears some resemblance to Prop-
bank (Palmer et al, 2005). However, Propbank is
concerned with manually adding information to the
Penn Treebank, rather than automatically reinter-
preting the same information through the lens of a
different linguistic theory.
We chose not to base our conversion on the Prop-
bank annotation, as it does not currently cover the
Brown or Switchboard sections of the Treebank.
The wider variety of genres provided by these sec-
tions makes the corpus much more useful for SFG,
since the theory devotes significant attention to prag-
matic phenomena and stylistic variation.
3 Systemic Functional Grammar
Generating a constituent using a systemic func-
tional grammar involves traversing a decision-tree-
like structure referred to as a system network. The
nodes of this tree are referred to as systems, and the
options from the systems are referred to as features.
At each system, the feature selected may add con-
straints on the type, number or order of the internal
structure of the constituent. When the entire net-
work has been traversed, the constraints are unified,
and the required constituents generated.
In order to annotate a sentence according to a sys-
temic functional grammar, we must specify the set
of features encountered as the system network is tra-
versed, and apply function labels to each constituent.
The function labeling is required because the con-
straints are always specified according to the child
constituents? function, rather than their form.
Constituents may have more than one function
label, as SFG describes three metafunctions, fol-
lowing Halliday?s (1969) argument that a clause is
structured simultaneously as a communicative act, a
piece of information, and a representation of reality.
Interpersonal function labels are assigned to clause
constituents in determining the clause?s communica-
tive status. The most important interpersonal func-
tions are Subject and Finite, since the relative posi-
tion of the constituents bearing these labels largely
determines whether the clause will be a question,
statement or command.
The textual structure of the clause includes the
functions Theme and Rheme, following Halliday?s
(1970) theory of information structure.
Finally, the experiential function of a constituent
is its semantic role, described in terms of a small
set of labels that are only minimally sensitive to the
semantics of the predicate.
4 Annotation Implemented
We base our annotation on the clause network in
the Nigel grammar (Mann and Matthiessen, 1983),
as it is freely available and discussed at length in
Matthiessen (1995). It is difficult to include annota-
tion from the group and phrase networks, because of
the flat bracketing of constituents in the Penn Tree-
bank. The converted corpus has full coverage over
all sections of the Penn Treebank 3 corpus.
We implement features from 41 systems from the
clause network, out of a possible 62. The most
prominent missing features relate to process type.
The process type system classifies clauses as one of
four broad semantic types: material, mental, verbal
or relational, with subsequent systems making finer
grained distinctions. This is mostly determined by
the argument structure of the verb, but also depends
on its lexical semantics. Process type assignment
therefore suffers from word sense ambiguity, so we
are unable to select from this system or others which
depend on its result. Figure 1 gives an example of
a clause with interpersonal, textual and experiential
function labels applied to its constituents.
5 Creating the Corpus
SFG specifies the structure of a clause from ?above?,
by setting constraints that are imposed by the set of
features selected from the system network. These
constraints describe the structure in terms of inter-
personal, textual and experiential function labels.
These functions then determine the boundaries of
the clause, by specifying its constituents.
90
Constituent Interpersonal Textual Ideational
and ? Txt. Theme ?
last year Adjunct Top. Theme Circumstance
prices Subject Rheme Participant
were Finite Rheme ?
quickly Adjunct Rheme Circumstance
plummeting Predicator Rheme Process
Table 1: SFG function labels assigned to clause constituents.
preprocess(parse)
clauses = []
for word in parse.words():
if isPredicate(word):
constituents = getConstituents(word)
clauses.append(constituents)
Figure 2: Conversion algorithm.
The Penn Treebank provides rich syntactic trees,
specifying the structure of the sentence. We there-
fore proceed from ?below?, using the Penn Treebank
to find clauses and their constituents, then applying
function labels to them, and using the function labels
as the basis for rules to traverse the system network.
5.1 Finding Constituents
In this stage, we search the Treebank parse for
SFG clauses, and collect their constituents. Clauses
are identified by searching for predicates that head
them, and constituents are collecting by traversing
upwards from the predicate, collecting the nodes?
siblings until we hit an S node.
There are a few common constructions which
present problems for one or both of these pro-
cedures. These exceptions are handled by pre-
processing the Treebank tree, changing its structure
to be compatible with the predicate and constituent
extraction algorithms. Figure 2 describes the con-
version process more formally.
5.1.1 Finding predicates
A predicate is the main verb in the clause. In the
Treebank annotation, the predicate will be the word
attached to the lowest node in a VP chain, because
auxiliaries attach higher up. Figure 3 describes the
function to decide whether a word is a predicate. Es-
sentially, we want words that are the last word at-
tached to a VP, that do not have a VP sibling.
Figure 1 marks the predicates and constituents in
a Treebank parse. The predicates are underlined, and
the constituents numbered to match the predicate.
if verb.parent.label == ?VP?:
for sibling in verb.parent.children:
if sibling.isWord():
if sibling.offset > verb.offset:
return False
if sibling.label == ?VP?:
return False
return True
Figure 3: Determining whether a word is a predicate.
node = predicate
constituents = [predicate]
while node.label not in clauseLabels:
for sibling in node.parent.children:
if sibling != node:
constituents.append(sibling)
for sibling in node.parent.children:
if sibling != node
and sibling.label in conjOrWHLabels:
constituents.append(sibling)
Figure 4: Finding constituents.
5.1.2 Getting Constituents
Once we have a predicate, we can traverse the tree
around it to collect the constituents in the clause it
heads. We do this by collecting its siblings and mov-
ing up the tree, collecting the ?uncle? nodes, until we
hit the top of the clause. Figure 4 describes the pro-
cess more formally. The final loop collects conjunc-
tions and WH constituents that attach alongside the
clause node, such as the ?which? in Figure 1.
5.1.3 Pre-processing Ellipsis and Gapping
Ellipsis and gapping involve two or more pred-
icates sharing some constituents. When the shar-
ing can be denoted using the tree structure, by plac-
ing the shared items above the point where the VPs
fork, we refer to the construction as ellipsis. Figure
5 shows a sentence with a subject and an auxiliary
shared between two predicates. 3.4% of predicates
share at least one constituent with another clause via
ellipsis. We pre-process ellipsis constructions by in-
serting an S node above each VP after the first, and
adding traces for the shared constituents.
91
Shhhhhhhhhhhh
((((((((((((
NP 1
XXXXX

NP
b
b
"
"
The plant
SBAR
PPPP

WHNP 2
which
S
VP
PPP

is 2 VP
aaa
!!!
owned 2 PP 2
HHH

by Vose Co
VP
XXXX

was 1 VP
PPPP

employed 1 S-PRP 1
VP
aaa
!!!
to 3 VP
b
b
"
"
make 3 NP 3
them
Figure 1: A parse tree with predicates underlined and constituents numbered.
In gapping constructions, the shared constituent
is the predicate itself, and what differs between the
two clauses are the arguments. The Treebank uses
special trace rules to describe which arguments must
be copied across to the gapped clause. We create
traces to the shared constituents and add them to
each gapped clause, so that the trace of the verb will
be picked up as a predicate later on. Gapping is a
very rare phenomenon ? only 0.02% clauses have
gapped predicates.
5.1.4 Pre-processing Semi-auxiliaries
In Figure 6 the verb ?continue? will match our
rules for predicate extraction, described in Section
5.1. SFG analyses this and other ?semi-auxiliaries?
(Quirk et al, 1991) as a serial verb construction,
rather than a matrix clause and a complement clause.
Since we want to treat the finite verb as though it
were an auxiliary, we pre-process these cases by
simply deleting the S node, and attaching its chil-
dren directly to the semi-auxiliary?s VP.
Defining the semi-auxiliary constructions is not
so simple, however. Quirk et al note that some
of these verbs are more like auxiliaries than others,
and organise them into a rough gradient according
to their formal properties. The problem is that there
is not clear agreement in the SFG literature about
where the line should be drawn. Matthiessen (1995)
describes all non-finite sentential complements as
serial-verb constructions. Martin et al (1997) argue
that verbs such as ?want? impose selectional restric-
S
PPPP

NP
Prices
VP
HH
continue S
VP
ll,,
to rise
Figure 6: Treebank representation of a sentence with
a semi-auxiliary.
tions on the subject, and therefore should be treated
as full verbs with a clause complement. Other com-
promises are possible as well.
Using Matthiessen?s definition, we collect 5.3%
fewer predicates than if we treated all semi-
auxiliaries as main verbs. If the complement clause
has a different subject from the parent clause, when
the two are merged the new verb will seem to have
extra arguments. 58% of these mergings introduce
an extra argument in this way. For example,
Investors want the market to boom
will be analysed as though boom has two argu-
ments, investors and market. We prevent this from
occurring by adding an extra condition for merg-
ing clauses, stating that the subject of the embedded
clause should be a trace co-indexed with the subject
of the parent clause.
92
S
XXXXXX

NP
Asbestos
VPhhhhhhh
(((((((
was VPhhhhhhhh@@
((((((((
VP
aaa
!!!
used PP
PPPP

in the early 1950s
and VP
HHH

replaced PP
ZZProceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 38?41,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Evaluating a Statistical CCG Parser on Wikipedia
Matthew Honnibal Joel Nothman
School of Information Technologies
University of Sydney
NSW 2006, Australia
{mhonn,joel,james}@it.usyd.edu.au
James R. Curran
Abstract
The vast majority of parser evaluation is
conducted on the 1984 Wall Street Journal
(WSJ). In-domain evaluation of this kind
is important for system development, but
gives little indication about how the parser
will perform on many practical problems.
Wikipedia is an interesting domain for
parsing that has so far been under-
explored. We present statistical parsing re-
sults that for the first time provide infor-
mation about what sort of performance a
user parsing Wikipedia text can expect.
We find that the C&C parser?s standard
model is 4.3% less accurate on Wikipedia
text, but that a simple self-training ex-
ercise reduces the gap to 3.8%. The
self-training also speeds up the parser on
newswire text by 20%.
1 Introduction
Modern statistical parsers are able to retrieve accu-
rate syntactic analyses for sentences that closely
match the domain of the parser?s training data.
Breaking this domain dependence is now one
of the main challenges for increasing the indus-
trial viability of statistical parsers. Substantial
progress has been made in adapting parsers from
newswire domains to scientific domains, espe-
cially for biomedical literature (Nivre et al, 2007).
However, there is also substantial interest in pars-
ing encyclopedia text, particularly Wikipedia.
Wikipedia has become an influential resource
for NLP for many reasons. In addition to its va-
riety of interesting metadata, it is massive, con-
stantly updated, and multilingual. Wikipedia is
now given its own submission keyword in general
CL conferences, and there are workshops largely
centred around exploiting it and other collabora-
tive semantic resources.
Despite this interest, there have been few in-
vestigations into how accurately existing NLP pro-
cessing tools work on Wikipedia text. If it is found
that Wikipedia text poses new challenges for our
processing tools, then our results will constitute
a baseline for future development. On the other
hand, if we find that models trained on newswire
text perform well, we will have discovered another
interesting way Wikipedia text can be exploited.
This paper presents the first evaluation of a sta-
tistical parser on Wikipedia text. The only pre-
vious published results we are aware of were de-
scribed by Ytrest?l et al (2009), who ran the
LinGo HPSG parser over Wikipedia, and found
that the correct parse was in the top 500 returned
parses for 60% of sentences. This is an interesting
result, but one that gives little indication of how
well a user could expect a parser to actually anno-
tate Wikipedia text, or how to go about adjusting
one if its performance is inadequate.
To investigate this, we randomly selected 200
sentences from Wikipedia, and hand-labelled them
with CCG annotation in order to evaluate the C&C
parser (Clark and Curran, 2007). C&C is the fastest
deep-grammar parser, making it a likely choice for
parsing Wikipedia, given its size.
Even at the parser?s WSJ speeds, it would
take about 18 days to parse the current English
Wikipedia on a single CPU. We find that the parser
is 54% slower on Wikipedia text, so parsing a full
dump is inconvenient at best. The parser is only
4.3% less accurate, however.
We then examine how these figures might be
improved. We try a simple domain adaptation
experiment, using self-training. One of our ex-
periments, which involves self-training using the
Simple English Wikipedia, improves the accuracy
of the parser?s standard model on Wikipedia by
0.8%. The bootstrapping also makes the parser
faster. Parse speeds on newswire text improve
20%, and speeds on Wikipedia improve by 34%.
38
Corpus Sentences Mean length
WSJ 02-21 39,607 23.5
FEW 889,027 (586,724) 22.4 (16.6)
SEW 224,251 (187,321) 16.5 (14.1)
Table 1: Sentence lengths before (and after) length filter.
2 CCG Parsing
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a linguistically motivated grammar
formalism with several advantages for NLP. Like
HPSG, LFG and LTAG, a CCG parse recovers the
semantic structure of a sentence, including long-
range dependencies and complement/adjunct dis-
tinctions, providing substantially more informa-
tion than skeletal brackets.
Clark and Curran (2007) describe how a fast and
accurate CCG parser can be trained from CCGbank
(Hockenmaier and Steedman, 2007). One of the
keys to the system?s success is supertagging (Ban-
galore and Joshi, 1999). Supertagging is the as-
signment of lexical categories before parsing. The
parser is given only tags assigned a high proba-
bility, greatly restricting the search space it must
explore. We use this system, referred to as C&C,
for our parsing experiments.
3 Processing Wikipedia Data
We began by processing all articles from the
March 2009 dump of Simple English Wikipedia
(SEW) and the matching Full English Wikipedia
(FEW) articles. SEW is an online encyclopedia
written in basic English. It has stylistic guidelines
that instruct contributors to use basic vocabulary
and syntax, to improve the articles? readability.
This might make SEW text easier to parse, mak-
ing it useful for our self-training experiments.
mwlib (PediaPress, 2007) was used to parse
the MediaWiki markup. We did not expand tem-
plates, and retained only paragraph text tokenized
according to the WSJ, after it was split into sen-
tences using the NLTK (Loper and Bird, 2002) im-
plementation of Punkt (Kiss and Strunk, 2006) pa-
rameterised on Wikipedia text. Finally, we dis-
carded incorrectly parsed markup and other noise.
We also introduced a sentence length filter for
the domain adaptation data (but not the evaluation
data), discarding sentences longer than 25 words
or shorter than 3 words. The length filter was used
to gather sentences that would be easier to parse.
The effect of this filter is shown in Table 1.
4 Self-training Methodology
To investigate how the parser could be improved
on Wikipedia text, we experimented with semi-
supervised learning. We chose a simple method,
self-training. Unlabelled data is annotated by the
system, and the predictions are taken as truth and
integrated into the training system.
Steedman et al (2003) showed that the selec-
tion of sentences for semi-supervised parsing is
very important. There are two issues: the accu-
racy with which the data can be parsed, which de-
termines how noisy the new training data will be;
and the utility of the examples, which determines
how informative the examples will be.
We experimented with a novel source of data
to balance these two concerns. Simple English
Wikipedia imposes editorial guidelines on the
length and syntactic style authors can use. This
text should be easier to parse, lowering the noise,
but the syntactic restrictions might mean its exam-
ples have lower utility for adapting the parser to
the full English Wikipedia.
We train the C&C supertagger and parser (Clark
and Curran, 2007) on sections 02-21 of the Wall
Street Journal (WSJ) marked up with CCG annota-
tions (Hockenmaier and Steedman, 2007) in the
standard way. We then parse all of the Sim-
ple English Wikipedia remaining after our pre-
processing. We discard the 826 sentences the
parser could not find an analysis for, and set aside
1,486 randomly selected sentences as a future de-
velopment set, leaving a corpus of 185,000 auto-
matically parsed sentences (2.6 million words).
We retrain the supertagger on a simple concate-
nation of the 39,607 WSJ training sentences and
the Wikipedia sentences, and then use it with the
normal-form derivations and hybrid dependencies
model distributed with the parser
1
.
We repeated our experiments using text from
the full English Wikipedia (FEW) for articles
whose names match an article in SEW. We ran-
domly selected a sample of 185,000 sentences
from these, to match the size of the SEW corpus.
We also performed a set of experiments where
we re-parsed the corpus using the updated su-
pertagger and retrained on output, the logic being
that the updated model might make fewer errors,
producing higher quality training data. This itera-
tive retraining was found to have no effect.
1
http://svn.ask.it.usyd.edu.au/trac/candc
39
Model WSJ Section 23 Wiki 200 Wiki 90k
P R F speed cov P R F speed cov speed cov
WSJ derivs 85.51 84.62 85.06 545 99.58 81.20 80.51 80.86 394 99.00 239 98.81
SEW derivs 85.06 84.11 84.59 634 99.75 81.96 81.34 81.65 739 99.50 264 99.11
FEW derivs 85.24 84.32 84.78 653 99.79 81.94 81.36 81.65 776 99.50 296 99.15
WSJ hybrid 86.20 84.80 85.50 481 99.58 81.93 80.51 81.22 372 99.00 221 98.81
SEW hybrid 85.80 84.30 85.05 571 99.75 82.16 80.49 81.32 643 99.50 257 99.11
FEW hybrid 85.94 84.46 85.19 577 99.79 82.49 81.03 81.75 665 99.50 275 99.15
Table 2: Parsing results with automatic POS tags. SEW and FEW models incorporate self-training.
5 Annotating the Wikipedia Data
We manually annotated a Full English Wikipedia
evaluation set of 200 sentences. The sentences
were sampled at random from the 5000 articles
that were linked to most often by Wikipedia pages.
Articles used for self-training were excluded.
The annotation was conducted by one annota-
tor. First, we parsed the sentences using the C&C
parser. We then manually corrected the supertags,
supplied them back to the parser, and corrected
the parses using a GUI. The interface allowed the
annotator to specify bracket constraints until the
parser selected the correct analysis. The annota-
tion took about 20 hours in total.
We used the CCGbank manual (Hockenmaier
and Steedman, 2005) as the guidelines for our
annotation. There were, however, some system-
atic differences from CCGbank, due to the faulty
noun phrase bracketing and complement/adjunct
distinctions inherited from the Penn Treebank.
6 Results
The results in this section refer to precision, re-
call and F -Score over labelled CCG dependencies,
which are 5-tuples (head, child, category, slot,
range). Speed is reported as words per second, us-
ing a single core 2.6 GHz Pentium 4 Xeon.
6.1 Out-of-the-Box Performance
Our experiments were performed using two mod-
els provided with v1.02 of the C&C parser. The
derivs model is calculated using features from the
Eisner (1996) normal form derivation. This is the
model C&C recommend for general use, because
it is simpler and faster to train. The hybrid model
achieves the best published results for CCG pars-
ing (Clark and Curran, 2007), so we also experi-
mented with this model. The models? performance
is shown in the WSJ rows of Table 2. We report ac-
curacy using automatic POS tags, since we did not
correct the POS tags in the Wikipedia data.
The derivs and hybrid models show a simi-
lar drop in performance on Wikipedia, of about
4.3%. Since this is the first accuracy evalua-
tion conducted on Wikipedia, it is possible that
Wikipedia data is simply harder to parse, possi-
bly due to its wider vocabulary. It is also possible
that our manual annotation made the task slightly
harder, because we did not reproduce the CCGbank
noun phrase bracketing and complement/adjunct
distinction errors.
We also report the parser?s speed and coverage
on Wikipedia. Since these results do not require
labelled data, we used a sample of 90,000 sen-
tences to obtain more reliable figures. Speeds var-
ied enormously between this sample and the 200
annotated sentences. A length comparison reveals
that our manually annotated sentences are slightly
shorter, with a mean of 20 tokens per sentence.
Shorter sentences are often easier to parse, so this
issue may have affected our accuracy results, too.
The 54% drop in speed on Wikipedia text is ex-
plained by the way the supertagger and parser are
integrated. The supertagger supplies the parser
with a beam of categories. If parsing fails, the
chart is reinitialised with a wider beam and it tries
again. These failures occur more often when the
supertagger cannot produce a high quality tag se-
quence, particularly if the problem is in the tag
dictionary, which constrains the supertagger?s se-
lections for frequent words. This is why we fo-
cused on the supertagger in our domain adaptation
experiments.
6.2 Domain Adaptation Experiments
The inclusion of parsed data from Wikipedia ar-
ticles in the supertagger?s training data improves
its accuracy on Wikipedia data, with the FEW en-
hanced model achieving 89.86% accuracy, com-
pared with the original accuracy of 88.77%. The
SEW enhanced supertagger achieved 89.45% ac-
curacy. The derivs model parser improves in ac-
curacy by 0.8%, the hybrid model by 0.5%.
40
The out-of-domain training data had little im-
pact on the models? accuracy on the WSJ, but
did improve parse speed by 20%, as it did on
Wikipedia. The speed increases because the su-
pertagger?s beam width is decided by its confi-
dence scores, which are more narrowly distributed
after the model has been trained with more data.
After self-training, the derivs and hybrid mod-
els performed equally accurately. With no reason
to use the hybrid model, the total speed increase is
34%. With our pre-processing, the full Wikipedia
dump had close to 1 billion words, so speed is an
important factor.
Overall, our simple self-training experiment
was quite successful. This result may seem sur-
prising given that the CoNLL 2007 participants
generally failed to use similar resources to adapt
dependency parsers to biomedical text (Dredze
et al, 2007). However, our results confirm Rimell
and Clark?s (2009) finding that the C&C parser?s
division of labour between the supertagger and
parser make it easier to adapt to new domains.
7 Conclusion
We have presented the first investigation into sta-
tistical parsing on Wikipedia data. The parser?s
accuracy dropped 4.3%, suggesting that the sys-
tem is still useable out-of-the-box. The parser is
also 54% slower on Wikipedia text. Parsing a full
Wikipedia dump would therefore take about 52
days of CPU time using our 5-year-old architec-
ture, which is inconvenient, but manageable over
multiple processors.
Using simple domain adaptation techniques,
we are able to increase the parser?s accuracy on
Wikipedia, with the fastest model improving in ac-
curacy by 0.8%. This closed the gap in accuracy
between the two parser models, removing the need
to use the slower hybrid model. This allowed us to
achieve an overall speed improvement of 34%.
Our results reflect the general trend that
NLP systems perform worse on foreign domains
(Gildea, 2001). Our results also support Rimell
and Clark?s (2009) conclusion that because C&C
is highly lexicalised, domain adaptation is largely
a process of adapting the supertagger.
A particularly promising aspect of these results
is that the parse speeds on the Wall Street Journal
improved, by 15%. This improvement came with
no loss in accuracy, and suggests that further boot-
strapping experiments are likely to be successful.
8 Acknowledgements
We would like to thank Stephen Clark and the
anonymous reviewers for their helpful feedback.
Joel was supported by a Capital Markets CRC
PhD scholarship and a University of Sydney Vice-
Chancellor?s Research Scholarship.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguis-
tics, 25(2):237?265.
Stephen Clark and James R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear models.
Computational Linguistics, 33(4):493?552.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman
Ganchev, Jo?ao Graca, and Fernando Pereira. 2007. Frus-
tratingly hard domain adaptation for dependency pars-
ing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 1051?1055. ACL, Prague,
Czech Republic.
Jason Eisner. 1996. Efficient normal-form parsing for Com-
binatory Categorial Grammar. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 79?86. Santa
Cruz, CA, USA.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the EMNLP Conference, pages
167?202. Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank
manual. Technical Report MS-CIS-05-09, Department of
Computer Science, University of Pennsylvania.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a
corpus of CCG derivations and dependency structures ex-
tracted from the Penn Treebank. Computational Linguis-
tics, 33(3):355?396.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual
sentence boundary detection. Computational Linguistics,
32(4):485?525.
Edward Loper and Steven Bird. 2002. NLTK: The natural
language toolkit.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session, pages
915?932. Prague, Czech Republic.
PediaPress. 2007. mwlib MediaWiki parsing library.
http://code.pediapress.com.
Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-
grammar parser to the biomedical domain. Journal of
Biomedical Informatics. (in press).
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Os-
borne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Example selec-
tion for bootstrapping statistical parsers. In Proceedings
of HLT-NAACL 2003. Edmonton, Alberta.
Gisle Ytrest?l, Stephan Oepen, and Daniel Flickinger. 2009.
Extracting and annotating Wikipedia sub-domains. In
Proceedings of the 7th International Workshop on Tree-
banks and Linguistic Theories, pages 185?197. Gronin-
gen, Netherlands.
41
Coling 2010: Poster Volume, pages 445?453,
Beijing, August 2010
Morphological analysis can improve a CCG parser for English
Matthew Honnibal, Jonathan K. Kummerfeld and James R. Curran
School of Information Technologies
University of Sydney
{mhonn,jono,james}@it.usyd.edu.au
Abstract
Because English is a low morphology lan-
guage, current statistical parsers tend to
ignore morphology and accept some level
of redundancy. This paper investigates
how costly such redundancy is for a lex-
icalised grammar such as CCG.
We use morphological analysis to split
verb inflectional suffixes into separate to-
kens, so that they can receive their own
lexical categories. We find that this im-
proves accuracy when the splits are based
on correct POS tags, but that errors in gold
standard or automatically assigned POS
tags are costly for the system. This shows
that the parser can benefit from morpho-
logical analysis, so long as the analysis is
correct.
1 Introduction
English is a configurational language, so gram-
matical functions are mostly expressed through
word order and function words, rather than with
inflectional morphology. Most English verbs have
four forms, and none have more than five. Most of
the world?s languages have far richer inflectional
morphology, some with millions of possible in-
flection combinations.
There has been much work on addressing the
sparse data problems rich morphology creates, but
morphology has received little attention in the En-
glish statistical parsing literature. We suggest that
English morphology may prove to be an under-
utilised aspect of linguistic structure that can im-
prove the performance of an English parser. En-
glish also has a rich set of resources available, so
an experiment that is difficult to perform with an-
other language may be easier to conduct in En-
glish, and a technique that makes good use of En-
glish morphology may transfer well to a morpho-
logically rich language. under-exploited in En-
glish natural language
In this paper, we show how morphological
information can improve an English statistical
parser based on a lexicalised formalism, Com-
binatory Categorial Grammar (CCG, Steedman,
2000), using a technique suggested for Turkish
(Bozsahin, 2002) and Korean (Cha et al, 2002).
They describe how a morphologically rich lan-
guage can be analysed efficiently with CCG by
splitting off inflectional affixes as morphological
tokens. This allows the affix to receive a cate-
gory that performs the feature coercion. For in-
stance, sleeping would ordinarily be assigned the
category S [ng ]\NP : a sentence with the [ng ] fea-
ture requiring a leftward NP argument. We split
the word into two tokens:
sleep -ing
S [b]\NP (S [ng ]\NP)\(S [b]\NP)
The additional token creates a separate space
for inflectional information, factoring it away
from the argument structure information.
Even with only 5 verb forms in English, we
found that accurate morphological analysis im-
proved parser accuracy. However, the system had
trouble recovering from analysis errors caused by
incorrect POS tags.
We then tested how inflection categories in-
teracted with hat categories, a linguistically-
motivated extension to the formalism, proposed
by Honnibal and Curran (2009), that introduces
some sparse data problems but improves parser
effiency. The parser?s accuracy improved by 0.8%
when gold standard POS tags were used, but not
with automatic POS tags. Our method addresses
problems caused by even low morphology, and
future work will make the system more robust to
POS tagging errors.
445
2 Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG, Steed-
man, 2000) is a lexicalised grammar, which means
that each word in the sentence is associated with
a category that specifies its argument structure
and the type and features of the constituent that
it heads. For instance, in might head a PP -typed
constituent with one NP -typed argument, written
as PP/NP . The / operator denotes an argument
to the right; \ denotes an argument to the left.
For example, a transitive verb is a function from
a rightward NP to and a leftward NP to a sen-
tence, (S\NP)/NP . The grammar consists of a
few schematic rules to combine the categories:
X /Y Y ?> X
Y X \Y ?< X
X /Y Y /Z ?>B X /Z
Y \Z X \Y ?<B X \Z
Y /Z X \Y ?<B? X /Z
CCGbank (Hockenmaier and Steedman, 2007)
extends this grammar with a set of type-changing
rules, designed to strike a better balance between
sparsity in the category set and ambiguity in the
grammar. We mark such productions TC.
In wide-coverage descriptions, categories are
generally modelled as typed feature structures
(Shieber, 1986), rather than atomic symbols. This
allows the grammar to include head indices, and
to unify under-specified features. In our nota-
tion features are annotated in square-brackets, e.g.
S [dcl ]. Head-finding indices are annotated on
categories as subscripts, e.g. (NPy\NPy)/NPz .
We occasionally abbreviate S\NP as VP , and
S [adj ]\NP as ADJ .
2.1 Statistical CCG parsing and morphology
In CCGbank, there are five features that are
largely governed by the inflection of the verb:
writes/wrote (S [dcl ]\NP)/NP
(was) written (S [pss]\NP)/NP
(has) written (S [pt ]\NP)/NP
(is) writing (S [ng ]\NP)/NP
(to) write (S [b]\NP)/NP
The features are necessary for satisfactory anal-
yses. Without inflectional features, there is no
way to block over-generation like has running or
was ran. However, the inflectional features also
create a level of redundancy if the different in-
flected forms are treated as individual lexical en-
tries. The different inflected forms of a verb will
all share the same set of potential argument struc-
tures, so some way of grouping the entries to-
gether is desirable.
Systems like the PET HPSG parser (Oepen et al,
2004) and the XLE LFG parser (Butt et al, 2006)
use a set of lexical rules that match morphologi-
cal operations with transformations on the lexical
categories. For example, a lexical rule is used to
ensure that an intransitive verb like sleeping re-
ceives the same argument structure as the base
form sleep, but with the appropriate inflectional
feature. This scheme works well for rule-based
parsers, but it is less well suited for statistical
parsers, as the rules propose categories but do not
help the model estimate their likelihood or assign
them feature weights.
Statistical parsers for lexicalised formalisms
such as CCG are very sensitive to the number of
categories in the lexicon and the complexity of
the mapping between words and categories. The
sub-task of assigning lexical categories, supertag-
ging (Bangalore and Joshi, 1999), is most of the
parsing task. Supertaggers mitigate sparse data
problems by using a label frequency threshold to
prune rare categories from the search space. Clark
and Curran (2007) employ a tag dictionary that re-
stricts the model to assigning word/category pairs
seen in the training data for frequent words.
The tag dictionary causes some level of under-
generation, because not all valid word/category
pairs will occur in the limited training data avail-
able. The morphological tokens we introduce help
to mitigate this, by bringing together what were
distinct verbs and argument structures, using lem-
matisation and factoring inflection away from ar-
gument structures. The tag dictionaries for the in-
flectional morphemes will have very high cover-
age, because there are only a few inflectional cat-
egories and a few inflectional types.
3 Inflectional Categories
We implement the morphemic categories that
have been discussed in the CCG literature
446
be ?ing good and do ?ing good
(S [b]\NP)/ADJ (S [ng ]\NP)\(S [b]\NP) ADJ conj (S [b]\NP)/NP (S [ng ]\NP)\(S [b]\NP) NP
<B? <B?
(S [ng ]\NP)/ADJ (S [ng ]\NP)/NP
> >
S [ng ]\NP S [ng ]\NP
<?>
(S [ng ]\NP)\(S [ng ]\NP)
<
S [ng ]\NP
Figure 1: A single inflection category (in bold) can serve many different argument structures.
(Bozsahin, 2002; Cha et al, 2002). The inflected
form is broken into two morphemes, and each is
assigned a category. The category for the inflec-
tional suffix is a function from a category with the
bare-form feature [b] to a category that has an in-
flectional feature. This prevents verbal categories
from having to express their inflectional features
directly. Instead, their categories only have to ex-
press their argument structure.
The CCG combinators allow multiple argument
structures to share a single inflectional category.
For instance, the (S [ng ]\NP)\(S [b]\NP) cate-
gory can supply the [ng ] feature to all categories
that have one leftward NP argument and any
number of rightward arguments, via the gener-
alised backward composition combinator. Fig-
ure 1 shows this category transforming two dif-
ferent argument structures, using the backward
crossed composition rule (<B?).
Table 1 shows the most frequent inflection cat-
egories we introduce. The majority of inflected
verbs in the corpus have a subject and some num-
ber of rightward arguments, so we can almost
assign one category per feature. The most fre-
quent exceptions are participles that function as
pre-nominal modifiers and verbs of speech.
Table 2 shows the inflectional token types we
introduce and which features they correspond to.
Our scheme largely follows the Penn Treebank
tag set (Bies et al, 1995), except we avoided dis-
tinguishing past participles from past tense (-en
vs -ed), because this distinction was a significant
source of errors for our morphological analysis
process, which relies on the part-of-speech tag.
3.1 Creating Training Data
We prepared a version of CCGbank (Hocken-
maier and Steedman, 2007) with inflectional to-
kens. This involved the following steps:
Correcting POS tags: Our morphological anal-
Freq. Category Example
32.964 (S [dcl ]\NP)\(S [b]\NP) He ran
11,431 (S [pss]\NP)\(S [b]\NP) He was run down
11,324 (S [ng ]\NP)\(S [b]\NP) He was running
4,343 (S [pt ]\NP)\(S [b]\NP) He has run
3,457 (N /N )\(S [b]\NP) the running man
2,011 S [dcl ]\S ?..?, he says
1,604 (S [dcl ]\S)\(S [b]\S) ?..?, said the boy
169 (S [dcl ]\ADJ )\(S [b]\ADJ ) Here ?s the deal
55 (S [dcl ]\PP)\(S [b]\PP) On it was a bee
Table 1: The inflectional categories introduced.
Token POS Feat Example
-es VBZ dcl He write -es letters
-e VBP dcl They write -e letters
-ed VBD dcl They write -ed letters
-ed VBN pt They have write -ed letters
-ed VBN pss Letters were write -ed
-ing VBG ng They are write -ing letters
Table 2: The inflectional token types introduced.
ysis relies on the part-of-speech tags provided
with CCGbank. We identified and corrected
words whose POS tags were inconsistent with their
lexical category, as discussed in Section 3.2.
Lemmatising verbs and removing features:
We used the morphy WordNet lemmatiser imple-
mented in NLTK1 to recover the lemma of the in-
flected verbs, identified by their POS tag (VBP,
VBG, VBN or VBZ). The verb?s categories were
updated by switching their features to [b].
Deriving inflectional categories: The gener-
alised backward composition rules allow a func-
tor to generalise over some sequence of ar-
gument categories, so long as they all share
the same directionality. For instance, a func-
tor (S\NP)\(S\NP) could backward cross-
compose into a category ((S\NP)/NP)/PP to
its left, generalising over the two rightward ar-
guments that were not specified by the functor?s
argument. It could not, however, compose into
a category like ((S\NP)\NP)/PP , because the
two arguments (NP and PP ) have differing direc-
1http://www.nltk.org
447
Freq. From To Examples
1056 VBG IN including, according, following
379 VBN JJ involved, related, concerned
351 VBN IN compared, based, given
274 VBG NN trading, spending, restructuring
140 VBZ NN is, ?s, has
102 VB VBP sell, let, have
53 VBZ MD does, is, has
45 VBG JJ pending, missing, misleading
41 VBP MD do, are, have
40 VBD MD did, were, was
334 All others
2,815 Total
Table 3: The most frequent POS tag conversions.
tionalities (leftward and rightward).
Without this restriction, we would only require
one inflection category per feature, using inflec-
tional categories like S [ng ]\S [b]. Instead, our in-
flectional categories must subcategorise for every
argument except the outermost directionally con-
sistent sequence. We discard this outermost con-
sistent sequence, remove all features, and use the
resulting category as the argument and result. We
then restore the result?s feature, and set the argu-
ment?s feature to [b].
Inserting inflectional tokens: Finally, the in-
flectional token is inserted after the verb, with a
new node introduced to preserve binarisation.
3.2 POS tag corrections
Hockenmaier and Steedman (2007) corrected sev-
eral classes of POS tag errors in the Penn Treebank
when creating CCGbank. We follow Clark and
Curran (2007) in using their corrected POS labels,
but found that there were still some words with in-
consistent POS tags and lexical categories, such as
building|NN|(S[dcl]\NP)/NP.
In order to make our morphological anal-
ysis more consistent, we identify and correct
such POS tagging errors as follows. We
use two regular expressions to identify ver-
bal lexical categories and verbal POS tags:
?\(*S\[(dcl|pss|ng|pt|b)\] and
AUX|MD|V.. respectively. If a word has a
verbal lexical category and non-verbal POS, we
correct its POS tag with reference to its suffix and
its category?s inflectional feature. If a word has a
verbal POS tag and a non-verbal lexical category,
we select the POS tag that occurs most frequently
with its lexical category.
The only exception are verbs functioning as
nominal modifiers, such as running in the running
man, which are generally POS tagged VBG but re-
ceive a lexical category of N /N . We leave these
POS tagged as verbs, and instead analyse their
suffixes as performing a form-function transfor-
mation that turns them from S [b]\NP verbs into
N /N adjectives ? (N /N )\(S [b]\NP).
Table 3 lists the most common before-and-
after POS tag pairs from our corrections, and the
words that most frequently exemplified the pair.
When compiling the table some clear errors came
to light, such as the ?correction? of is|VBZ to
is|NN. These errors may explain why the POS
tagger?s accuracy drops by 0.1% on the corrected
set, and suggest that the problem of aligning POS
tags and supertags is non-trivial.
In light of these errors, we experimented
with an alternate strategy. Instead of cor-
recting the POS tags, we introduced null
inflectional categories that compensated
for bad morphological tokenisation such as
accord|VBG|(S/S)/PP -ing|VIG|-.
The null inflectional category does not interact
with the rest of the derivation, much like a punc-
tuation symbol. This performed little better than
the baseline, showing that the POS tag corrections
made an important contribution, despite the
problems with our technique.
3.3 Impact on CCGbank Lexicon
Verbal categories in CCGbank (Hockenmaier and
Steedman, 2007) record both the valency and the
inflectional morphology of the verb they are as-
signed to. This means v ? i categories are re-
quired, where v and i are the number of distinct ar-
gument structures and inflectional features in the
grammar respectively.
The inflectional tokens we propose allow in-
flectional morphology to be largely factored away
from the argument structure, so that roughly v+ i
verbal categories are required. A smaller category
set leads to lower category ambiguity, making the
assignment decision easier.
Table 4 summarises the effects of inflection cat-
egories on the lexicon extracted from CCGbank.
Clark and Curran (2007) extract a set of 425 cate-
gories from the training data (Sections 02-21) that
448
consists of all categories that occur at least 10
times. The frequency cut off is used because the
model will not have sufficient evidence to assign
the other 861 categories that occur at least once,
and their distribution is heavy tailed: together,
they only occur 1,426 times. We refer to the fre-
quency filtered set as the lexicon. The parser can-
not assign a category outside its lexicon, so gaps
in it cause under-generation.
The CCGbank lexicon includes 159 verbal cat-
egories. There are 74 distinct argument structures
and 5 distinct features among these verbal cate-
gories. The grammar Clark and Curran (2007)
learn therefore under-generates, because 211 of
the 370 (5 ? 74) argument structure and feature
combinations are rare or unattested in the training
data. For instance, there is a (S [dcl ]\NP)/PP
category, but no corresponding (S [b]\NP)/PP ,
making it impossible for the grammar to generate
a sentence like I want to talk to you, as the cor-
rect category for talk in this context is missing. It
would be trivial to add the missing categories to
the lexicon, but a statistical model would be un-
able to reproduce them. There are 8 occurrences
of such missing categories in Section 00, the de-
velopment data.
The reduction in data sparsity brought by the in-
flection categories causes 22 additional argument
structures to cross the frequency threshold into
the lexicon. A grammar induced from this cor-
pus is thus able to generate 480 (96?5) argument
structure and feature combinations, three times as
many as could be generated before.
We introduce 15 inflectional categories in the
corpus. The ten most frequent are shown in Table
1. The combinatory rules allow these 15 inflection
categories to serve 96 argument structures, reduc-
ing the number of verbal categories in the lexicon
from 159 to 89 (74 + 15).
The statistics at frequency 1 are less reliable,
because many of the categories may be linguisti-
cally spurious: they may be artefacts caused by
annotation noise in the Penn Treebank, or the
conversion heuristics used by Hockenmaier and
Steedman (2007).
? CCGbank +Inflect
Inflection categories 10 0 15
Argument structures 10 74 96
Verb categories generated 10 159 480
All categories 10 425 375
Inflection categories 1 0 31
Argument structures 1 283 283
Verbs categories generated 1 498 1415
All categories 1 1285 1120
Table 4: Effect of inflection tokens on the category set for
categories with frequency ? 10 and ? 1
3.4 Configuration of parsing experiments
We conducted two sets of parsing experiments,
comparing the impact of inflectional tokens on
CCGbank (Hockenmaier and Steedman, 2007)
and hat CCGbank (Honnibal and Curran, 2009).
The experiments allow us to gauge the impact of
inflectional tokens on versions of CCGbank with
differing numbers of verbal categories.
We used revision 1319 of the C&C parser2
(Clark and Curran, 2007), using the best-
performing configuration they describe, which
used the hybrid dependency model. The most
important hyper-parameters in their configuration
are the ? and K values, which control the work-
flow between the supertagger and parser. We use
the Honnibal and Curran (2009) values of these
parameters in our hat category experiments, de-
scribed in Section 5.
Accuracy was evaluated using labelled depen-
dency F -scores (LF ). CCG dependencies are la-
belled by the head?s lexical category and the ar-
gument slot that the dependency fills. We evalu-
ated the baseline and inflection parsers on the un-
modified dependencies, to allow direct compari-
son. For the inflection parsers, we pre-processed
the POS-tagged input to introduce inflection to-
kens, and post-processed it to remove them.
We follow Clark and Curran (2007) in not
evaluating accuracy over sentences for which the
parser returned no analysis. The percentage of
sentences analysed is described as the parser?s
coverage (C). Speed (S) figures refer to sentences
parsed per second (including failures) on a dual-
CPU Pentium 4 Xeon with 4GB of RAM.
2http://trac.ask.it.usyd.edu.au/candc
449
4 Parsing Results on CCGbank
Table 5 compares the performance of the parser
on Sections 00 and 23 with and without inflection
tokens. Section 00 was used for development ex-
periments to test different approaches, and Section
23 is the test data. Similar effects were observed
on both evaluation sections.
The inflection tokens had no significant impact
on speed or coverage, but did improve accuracy
by 0.49% F -measure when gold standard POS
tags were used, compared to the baseline. How-
ever, some of the accuracy improvement can be
attributed to the POS tag corrections described in
Section 3.2, so the improvement from the inflec-
tion tokens alone was 0.39%.
The POS tag corrections caused a large drop in
performance when automatic POS tags were used.
We attribute this to the imperfections in our cor-
rection strategy. The inflection tokens improved
the accuracy by 0.39%, but this was not large
enough to correct for the drop in accuracy caused
by the POS changes.
Another possibility is that our morphological
analysis makes POS tagger errors harder to re-
cover from. Instead of an incorrect feature value,
POS tag errors can now induce poor morphologi-
cal splits such as starl|VBG -ing|VIG. POS
tagging errors are already problematic for the C&C
parser, because only the highest ranked tag is
forwarded to the supertagger as a feature. Our
morphological analysis strategy seems to exacer-
bate this error propagation problem. Curran et al
(2006) showed that using a beam of POS tags as
features in the supertagger and parser mitigated
the loss of accuracy from POS tagging errors. Un-
fortunately, with our morphological analysis strat-
egy, POS tag variations change the tokenisation
of a sentence, making parsing more complicated.
Perhaps the best solution would be to address the
tagging errors in the treebank more thoroughly,
and reform the annotation scheme to deal with
particularly persistant error cases. This might im-
prove POS tag accuracy to a level where errors are
rare enough to be unproblematic.
Despite the limited morphology in English, the
inflectional tokens improved the parser?s accuracy
when gold standard POS tags were supplied. We
Gold POS Auto POS
LF S C LF S C
Baseline 00 87.19 22 99.22 85.28 24 99.11
+POS 00 87.46 24 99.16 85.04 23 99.05
+Inflect 00 87.81 24 99.11 85.33 23 98.95
Baseline 23 87.69 36 99.63 85.50 36 99.58
+POS 23 87.79 36 99.63 85.06 36 99.50
+Inflect 23 88.18 36 99.58 85.42 33 99.34
Table 5: Effect of POS changes and inflection tokens on
accuracy (LF ), speed (S) and coverage (C) on 00 and 23.
attribute the increase in accuracy to the more ef-
ficient word-to-category mapping caused by re-
placing inflected forms with lemmas, and feature-
bearing verb categories with ones that only refer to
the argument structure. We examined this hypoth-
esis by performing a further experiment, to inves-
tigate how inflection tokens interact with hat cat-
egories, which introduce additional verbal cate-
gories that represent form-function discrepancies.
5 Inflection Tokens and Hat Categories
Honnibal and Curran (2009) introduce an exten-
sion to the CCG formalism, hat categories, as an
alternative way to solve the modifier category pro-
liferation (MCP) problem. MCP is caused when
a modifier is itself modified by another modi-
fier. For instance, in the sentence he was in-
jured running with scissors, with modifies run-
ning, which modifies injured. This produces the
category ((VP\VP)\(VP\VP))/NP for with, a
rare category that is sensitive to too much of the
sentence?s structure.
Hockenmaier and Steedman (2007) address
MCP by adding type-changing rules to CCGbank.
These type-changing rules transform specific cat-
egories. They are specific to the analyses in the
corpus, unlike the standard combinators, which
are schematic and language universal. Honnibal
and Curran?s (2009) contribution is to extend the
formalism to allow these type-changing rules to
be lexically specified, restoring universality to the
grammar ? but at the cost of sparse data problems
in the lexicon. Figure 2 shows how a reduced rel-
ative clause is analysed using hat categories. The
hat category (S [pss]\NP)NP\NP is subject to the
unhat rule, which unarily replaces it with its hat,
NP\NP , allowing it to function as a modifier.
Hat categories have a practical advantage for a
parser that uses a supertagging phase (Bangalore
450
The company bought by Google last year is profitable
NP/N N (S [pss]\NP)NP\NP (VP\VP)/NP NP NPVP\VP/N N (S [dcl ]\NP)/ADJ ADJ
> > > >
NP VP\VP NPVP\VP S [dcl ]\NP
<
(S [pss]\NP)NP\NP
<
(S [pss]\NP)NP\NP
H
NP\NP
<NP
<
S [dcl ]
Figure 2: CCG derivation showing hat categories and the unhat rule.
The company buy ?ed by Google last year
NP/N N S [b]\NP (S [pss]\NP)NP\NP\(S [b]\NP) (VP\VP)/NP NP NPVP\VP/N N
> < > >
NP (S [pss]\NP)NP\NP VP\VP NPVP\VP
< H
(S [pss]\NP)NP\NP VP\VP
<
(S [pss]\NP)NP\NP
H
NP\NP
<NP
Figure 3: CCG derivation showing how inflectional tokens interact with hat categories.
and Joshi, 1999), such as the C&C system (Clark
and Curran, 2007). By replacing type-changing
rules with additional lexical categories, more of
the work is shifted to the supertagger. The su-
pertagging phase is much more efficient than the
chart parsing stage, so redistribution of labour
makes the parser considerably faster.
Honnibal and Curran (2009) found that the
parser was 37% faster on the test set, at a cost
of 0.5% accuracy. They attribute the drop in ac-
curacy to sparse data problems for the supertag-
ger, due to the increase in the number of lexical
categories. We hypothesised that inflectional cate-
gories could address this problem, as the two anal-
yses interact well.
5.1 Analyses with inflectional hat categories
Using hat categories to lexicalise type-changing
rules offers attractive formal properties, and some
practical advantages. However, it also misses
some generalisations. A type-changing operation
such as S [ng ]\NP ? NP\NP must be avail-
able to any VP. If we encounter a new word, The
company is blagging its employees, we can gen-
eralise to the reduced relative form, She works for
that company blagging its employees with no ad-
ditional information.
This property could be preserved with some
form of lexical rule, but a novel word-category
pair is difficult for a statistical model to assign.
Inflection tokens offer an attractive solution to this
problem, as shown in Figure 3. Assigning the hat
category to the suffix makes it available to any
verb the suffix follows ? it is just another func-
tion the inflectional suffix can perform. This gen-
erality also makes it much easier to learn, because
it does not matter whether the training data hap-
pens to contain examples of a given verb perfom-
ing that grammatical function.
We prepared a version of the Honnibal and
Curran (2009) hat CCGbank, moving hats on to
inflectional categories wherever possible. The
hat CCGbank?s lexicon contained 105 hat cate-
gories, of which 77 were assigned to inflected
verbs. We introduced 33 inflection hat cate-
gories in their place, reducing the number of
hat categories by 27.9%. Fewer hat categories
were required because different argument struc-
tures could be served by the same inflection cat-
egory. For instance, the (S [ng ]\NP)NP\NP and
(S [ng ]\NP)NP\NP/NP categories were both re-
placed by the (S [ng ]\NP)NP\NP\(S [b]\NP)
category. Table 6 lists the most frequent inflection
hat categories we introduce.
451
Freq. Category
3332 (S [pss]\NP)NP\NP\(S [b]\NP)
1518 (S [ng ]\NP)NP\NP\(S [b]\NP)
1231 (S [ng ]\NP)(S\NP)\(S\NP)\(S [b]\NP)
360 ((S [dcl ]\NP)/NP)NP\NP\((S [b]\NP)/NP)
316 (S [ng ]\NP)NP\(S [b]\NP)
234 ((S [dcl ]\NP)/S)S/S\((S [b]\NP)/S)
209 (S [ng ]\NP)S/S\(S [b]\NP)
162 (S [dcl ]NP\NP\NP)\(S [b]\NP)
157 ((S [dcl ]\NP)/S)VP/VP\((S [b]\NP)/S)
128 (S [pss]\NP)S/S\(S [b]\NP)
Table 6: The most frequent inflection hat categories.
5.2 Parsing results
Table 7 shows the hat parser?s performance with
and without inflectional categories. We used the
values for the ? and K hyper-parameters de-
scribed by Honnibal and Curran (2009). These
hyper-parameters were tuned on Section 00, and
some over-fitting seems apparent. We also fol-
lowed their dependency conversion procedure, to
allow evaluation over the original CCGbank de-
pendencies and thus direct comparison with Table
5. We also merged the parser changes they de-
scribed into the development version of the C&C
parser we are using, for parse speed comparison.
Interestingly, incorporating the hat changes into
the current version has increased the advantage
of the hat categories. Honnibal and Curran re-
port a 37% improvement in speed for the hybrid
model (which we are using) on Section 23, using
gold standard POS tags. With our version of the
parser, the improvement is 86% (36 vs. 67 sen-
tences parsed per second).
With gold standard POS tags, the inflection to-
kens improved the hat parser?s accuracy by 0.8%,
but decreased its speed by 24%. We attribute
the decrease in speed to the increase in sentence
length coupled with the new uncertainty on the
inflectional tokens. Coverage increased slightly
with gold standard POS tags, but decreased with
automatic POS tags. We attribute this to the fact
that POS tagging errors lead to morphological
analysis errors.
The accuracy improvement on the hat corpus
was more robust to POS tagging errors than the
CCGbank results, however. This may be be-
cause POS tagging errors are already quite prob-
lematic for the hat category parser. POS tag fea-
Gold POS Auto POS
LF S C LF S C
Hat baseline 00 87.08 32 99.53 84.67 34 99.32
Hat inflect 00 87.85 37 99.63 84.99 30 98.95
Hat baseline 23 87.26 67 99.50 84.93 53 99.58
Hat inflect 23 88.06 54 99.63 85.25 43 99.38
Table 7: Effect of inflection tokens on accuracy (LF ),
speed (S) and coverage (C) on Sections 00 and 23.
tures are more important for the supertagger than
the parser, and the supertagger performs more of
the work for the hat parser.
6 Conclusion
Lexicalised formalisms like CCG (Steedman,
2000) and HPSG (Pollard and Sag, 1994) have
led to high-performance statistical parsers of En-
glish, such as the C&C CCG parser (Clark and
Curran, 2007) and the ENJU HPSG (Miyao and
Tsuji, 2008) parser. The performance of these
parsers can be partially attributed to their theoret-
ical foundations. This is particularly true of the
C&C parser, which exploits CCG?s lexicalisation
to divide the parsing task between two integrated
models (Clark and Curran, 2004).
We have followed this formalism-driven ap-
proach by exploiting morphology for English syn-
tactic parsing, using a strategy designed for mor-
phologically rich languages. Combining our tech-
nique with hat categories leads to a 20% improve-
ment in efficiency, with a 0.25% loss of accuracy.
If the POS tag error problem were addressed, the
two strategies combined would improve efficiency
by 50%, and improve accuracy by 0.37%. These
results illustrate that linguistically motivated solu-
tions can produce substantial practical advantages
for language technologies.
Acknowledgments
We would like to thank the anonymous reviewers
for their feedback, and the members of the CCG-
technicians mailing list for discussion about some
of our analyses. Matthew Honnibal was supported
by Australian Research Council (ARC) Discovery
Grant DP0665973. James Curran was supported
by ARC Discovery grant DP1097291 and the Cap-
ital Markets Cooperative Research Centre.
452
References
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237?265.
Ann Bies, Mark Ferguson, Karen Katz, and
Robert MacIntyre. 1995. Bracketing guidelines
for Treebank II style Penn Treebank project.
Technical report, MS-CIS-95-06, University of
Pennsylvania, Philadelphia, PA, USA.
Cem Bozsahin. 2002. The combinatory mor-
phemic lexicon. Computational Linguistics,
28(2):145?186.
Miriam Butt, Mary Dalrymple, and Tracy H.
King, editors. 2006. CSLI Publications, Stan-
ford, CA.
Jeongwon Cha, Geunbae Lee, and Jonghyeok
Lee. 2002. Korean Combinatory Categorial
Grammar and statistical parsing. Computers
and the Humanities, 36(4):431?453.
Stephen Clark and James R. Curran. 2004. The
importance of supertagging for wide-coverage
CCG parsing. In Proceedings of 20th Interna-
tional Conference on Computational Linguis-
tics, pages 282?288. Geneva, Switzerland.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguis-
tics, 33(4):493?552.
James R. Curran, Stephen Clark, and David
Vadas. 2006. Multi-tagging for lexicalized-
grammar parsing. In Proceedings of the Joint
Conference of the International Committee on
Computational Linguistics and the Association
for Computational Linguistics, pages 697?704.
Sydney, Austrailia.
Julia Hockenmaier and Mark Steedman. 2007.
CCGbank: a corpus of CCG derivations
and dependency structures extracted from the
Penn Treebank. Computational Linguistics,
33(3):355?396.
Matthew Honnibal and James R. Curran. 2009.
Fully lexicalising CCGbank with hat cate-
gories. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language
Processing, pages 1212?1221. Singapore.
Yusuke Miyao and Jun?ichi Tsuji. 2008. Feature
forest models for probabilistic HPSG parsing.
Computational Linguistics, 34(1):35?80.
Stepan Oepen, Daniel Flickenger, Kristina
Toutanova, and Christopher D. Manning. 2004.
LinGO Redwoods. a rich and dynamic treebank
for HPSG. Research on Language and Compu-
tation, 2(4):575?596.
Carl Pollard and Ivan Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press, Chicago.
Stuart M. Shieber. 1986. An Introduction to
Unification-Based Approaches to Grammar,
volume 4 of CSLI Lecture Notes. CSLI Pub-
lications, Stanford, CA.
Mark Steedman. 2000. The Syntactic Process.
The MIT Press, Cambridge, MA, USA.
453
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 790?799, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Sequence Labelling Approach to Quote Attribution
Tim O?Keefe? Silvia Pareti James R. Curran? Irena Koprinska? Matthew Honnibal?
? e-lab, School of IT School of Informatics ?Centre for Language Technology
University of Sydney University of Edinburgh Macquarie University
NSW 2006, Australia United Kingdom NSW 2109, Australia
{tokeefe,james,irena}@it.usyd.edu.au S.Pareti@sms.ed.ac.uk matthew.honnibal@mq.edu.au
Abstract
Quote extraction and attribution is the task of
automatically extracting quotes from text and
attributing each quote to its correct speaker.
The present state-of-the-art system uses gold
standard information from previous decisions
in its features, which, when removed, results
in a large drop in performance. We treat the
problem as a sequence labelling task, which
allows us to incorporate sequence features
without using gold standard information. We
present results on two new corpora and an aug-
mented version of a third, achieving a new
state-of-the-art for systems using only realis-
tic features.
1 Introduction
News stories are often driven by the quotes made
by politicians, sports stars, musicians, and celebri-
ties. When these stories exit the news cycle, the
quotes they contain are often forgotten by both read-
ers and journalists. A system that automatically ex-
tracts quotes and attributes those quotes to the cor-
rect speaker would enable readers and journalists to
place news in the context of all comments made by
a person on a given topic.
Though quote attribution may appear to be a
straightforward task, the simple rule-based ap-
proaches proposed thus far have produced disap-
pointing results. Going beyond these to machine
learning approaches presents several problems that
make quote attribution surprisingly difficult. The
main challenge is that while a large portion of quotes
can be attributed to a speaker based on simple rules,
the remainder have few or no contextual clues as
to who the correct speaker is. Additionally, many
quote sequences, such as dialogues, rely on the
reader understanding that there is an alternating se-
quence of speakers, which creates dependencies be-
tween attribution decisions made by a classifier.
Elson and McKeown (2010) is the only study that
directly uses machine learning in quote attribution,
treating the task as a classification task, where each
quote is attributed independently of other quotes. To
handle conversations and similar constructs they use
gold standard information about speakers of previ-
ous quotes as features for their model. This is an
unrealistic assumption, since gold standard informa-
tion is not available in practice.
The primary contribution of this paper is that we
reformulate quote attribution as a sequence labelling
task. This allows us to use sequence features with-
out having to use the unrealistic gold standard fea-
tures that were used in Elson and McKeown (2010).
We experiment with three sequence decoding mod-
els including greedy, Viterbi and a linear chain Con-
ditional Random Field (CRF).
Furthermore we present results on two new cor-
pora and an augmented version of a third. The two
new corpora are from news articles from the Wall
Street Journal and the Sydney Morning Herald re-
spectively, while the third corpus is an extension to
the classic literature corpus from Elson and McK-
eown (2010). Our results show that a quote attri-
bution system using only realistic features is highly
feasible for the news domain, with accuracies of
92.4% on the SMH corpus and 84.1% on the WSJ
corpus.
790
2 Background
Early work into quote attribution by Zhang et al
(2003) focused on identifying when different char-
acters were talking in children?s stories, so that a
speech synthesis system could read the quoted parts
in different voices. While they were able to ex-
tract quotes with high precision and recall, their at-
tribution accuracy was highly dependent on the doc-
ument in question, ranging from 47.6% to 86.7%.
Mamede and Chaleira (2004) conducted similar re-
search on children?s stories written in Portuguese.
Their system proved to be very good at extracting
quotes through simple rules, but when using a hand-
crafted decision tree to attribute those quotes to a
speaker, they achieved an accuracy of only 65.7%.
In the news domain, both Pouliquen et al2007)
and Sarmento and Nunes (2009) proposed rule-
based systems that work over large volumes of text.
Both systems aimed for high precision at the ex-
pense of low recall, as their data contained many re-
dundant quotes. More recently, SAPIENS, a French-
language quote extraction and attribution system,
was developed by de La Clergerie et al2011). It
conducts a full parse of the text, which allows it to
use patterns to extract direct and indirect quotes, as
well as the speaker of each quote. Their evaluation
found that 19 out of 40 quotes (47.5%) had a correct
span and author, while a further 19 had an incorrect
author, and 4 had an incorrect span. In related work,
Sagot et al2010) built a lexicon of French reported
speech verbs, and conducted some analysis of dif-
ferent types of quotes.
Glass and Bangay (2007) approached the task
with a three stage method. For each quote they
first find the nearest speech verb, they then find the
grammatical actor of that speech verb, and finally
they select the appropriate speaker for that actor. To
achieve each of these subtasks they built a model
with several manually weighted features that good
candidates should possess. For each subtask they
then choose the candidate with the largest weighted
sum of features. Their full approach yields an ac-
curacy of 79.4% on a corpus of manually annotated
fiction books.
Schneider et al2010) describe PICTOR, which
is principally a quote visualisation tool. Their task
was to find direct and indirect quotes, which they
attribute to a text span representing the speaker.
To do this they constructed a specialised grammar,
which was built with reference to a small develop-
ment corpus. With a permissive evaluation metric
their grammar-based approach yielded 86% recall
and 75% precision, however this dropped to 52% re-
call and 56% precision when measured in terms of
completely correct quote-speaker pairs.
The work most similar to ours is the work by El-
son and McKeown (2010). Their aim was to au-
tomatically identify both quotes and speakers, and
then to attribute each quote to a speaker, in a corpus
of classic literature that they compiled themselves.
To identify potential speakers they used the Stanford
NER tagger (Finkel et al2005) and a method out-
lined in Davis et al2003) that allowed them to find
nominal character references. They then grouped
name variants and pronominal mentions into a coref-
erence chain.
To attribute a quote to a speaker they first classi-
fied the quotes into categories. Several of the cat-
egories have a speaker explicit in their structure,
so they attribute quotes to those speakers with no
further processing. For the remaining categories,
they cast the attribution problem as a binary clas-
sification task, where each quote-speaker pair has
a ?speaker? or ?not speaker? label predicted by the
classifier. They then reconciled these independent
decisions using various techniques to produce a sin-
gle speaker prediction for each quote. For the sim-
ple category predictions they achieved 93-99% ac-
curacy, while for the more complicated categories
they achieved 63-64%, with an overall result of 83%
accuracy. This compares favourably with their rule-
based baseline, which achieved an accuracy of 52%.
While the results of Elson and McKeown (2010)
appear encouraging, they are misleading for two rea-
sons. First their corpus does not include quotes
where all three annotators chose different speakers.
While these quotes include some cases where the
annotators chose coreferent spans, it also includes
cases of legitimate disagreement about the speaker.
An automated system would likely find these cases
challenging. Second both their category predictions
and machine learning predictions rely on gold stan-
dard information from previous quotes, which is not
available in practice. In our study we address both
these issues.
791
Proportion (%) Accuracy (%)
LIT WSJ SMH LIT WSJ SMH
Quote-Said-Person 17.9 20.2 3.1 98.9 99.8 99.1
Quote-Person-Said 2.8 6.1 16.6 97.7 97.0 98.5
Other Trigram 0.1 2.3 0.3 66.7 56.2 54.5
Quote-Said-Pronoun 1.9 0.1 0.0 38.6 100.0 0.0
Quote-Pronoun-Said 5.9 8.8 13.5 36.5 92.2 93.9
Other Anaphors 0.1 0.1 0.2 0.0 100.0 62.5
Added* 24.6 28.3 23.9 89.7 76.3 97.5
Backoff 11.0 33.9 32.3 - - -
Alone 18.0 0.2 9.7 - - -
Conversation* 17.7 0.2 0.3 85.2 0.0 8.3
Total 100.0 100.0 100.0 60.5 57.2 55.8
Table 1: The proportion of quotes in each category and the accuracy of the speaker prediction based on the category.
The two categories marked with an asterisk (*) depend on previous decisions.
3 Corpora
We evaluate our methods on two new corpora com-
ing from the news domain, and an augmented ver-
sion of an existing corpus, which covers classic lit-
erature. They are described below.
3.1 Columbia Quoted Speech Attribution
Corpus (LIT)
The first corpus we use was originally created by
Elson and McKeown (2010). It is a set of excerpts
from 11 fictional 19th century works by six well-
known authors, split into 18 documents. In total it
contains 3,126 quotes annotated with their speakers.
Elson and McKeown used an automated system
to find named entity spans and nominal mentions in
the text, with the named entities being linked to form
a coreference chain (they did not link nominal men-
tions). The corpus was built using Amazon?s Me-
chanical Turk, with three annotations per quote. To
ensure quality, all annotations from poorly perform-
ing annotators were removed, as were quotes where
each annotator chose a different speaker. Though
excluding some quotes ensures quality annotations,
it causes gaps in the quote chains, which is a prob-
lem for sequence labelling. Furthermore, the cases
where annotators disagreed are likely to be challeng-
ing, so removing them from the corpus could make
results appear better than they would be in practice.
To rectify this, we conducted additional annota-
tion of the quotes that were excluded by the origi-
nal authors. Two postgraduates annotated 654 addi-
tional quotes, with a raw agreement of 79% over 48
double-annotated quotes. Our annotators reported
seeing some errors in existing annotations, so we
had one annotator check 400 existing annotations for
correctness. This additional check found that 92.5%
of the quotes were correctly annotated.
3.2 PDTB Attribution Corpus Extension (WSJ)
Our next corpus is an extension to the attribution
annotations found in the Penn Discourse TreeBank
(PDTB). The original PDTB contains several forms
of discourse, including assertions, beliefs, facts, and
eventualities. These can be attributed to named enti-
ties or to unnamed, pronominal, or implicit sources.
Recent work by Pareti (2012) conducted further an-
notation of this corpus, including reconstructing at-
tributions that were only partially annotated, and in-
troducing additional information. From this corpus
we use only direct quotes and the directly quoted
portions of mixed quotes, giving us 4,923 quotes.
For the set of potential speakers we use the
BBN pronoun coreference and entity type cor-
pus (Weischedel and Brunstein, 2005), with auto-
matically coreferred pronouns. We automatically
matched BBN entities to PDTB extension speakers,
and included the PDTB speaker where no matching
BBN entity could be found. This means an automatic
system has an opportunity to find the correct speaker
for all quotes in the corpus.
792
3.3 Sydney Morning Herald Corpus (SMH)
We compiled the final corpus from a set of news
documents taken from the Sydney Morning Her-
ald website1. We randomly selected 965 documents
published in 2009 that were not obituaries, opin-
ion pages, advertisements or other non-news sto-
ries. To conduct the annotation we employed 11
non-expert annotators via the outsourcing site Free-
lancer2, as well as five expert annotators from our
research group. A total of 400 news stories were
double-annotated, with at least 33 double-annotated
stories per annotator. Raw agreement on the speaker
of each quote was high at 98.3%. These documents
had already been annotated with named entities as
part of a separate research project (Hachey et al
2012), which includes manually constructed coref-
erence chains. The resulting corpus contains 965
documents, with 3,535 quotes.
3.4 Corpus Comparisons
In order to compare the corpora we categorise the
quotes into the categories defined by Elson and
McKeown (2010), as shown in Table 1. We assigned
quotes to these categories by testing (after text pre-
processing) whether the quote belonged to each cat-
egory, in the order shown below:
1. Trigram ? the quote appears consecutively with
a mention of an entity, and a reported speech
verb, in any order;
2. Anaphors ? same as above, except that the men-
tion is a pronoun;
3. Added ? the quote is in the same paragraph as
another quote that precedes it;
4. Conversation ? the quote appears in a para-
graph on its own, and the two paragraphs pre-
ceding the current paragraph each contain a sin-
gle quote, with alternating speakers;
5. Alone ? the quote is in a paragraph on its own;
6. Miscellaneous ? the quote matches none of the
preceding categories. This category is called
?Backoff? in Elson and McKeown (2010).
1http://www.smh.com.au
2http://www.freelancer.com
Unsurprisingly, the two corpora from the news do-
main share similar proportions of quotes in each
category. The main differences are that the SMH
uses a larger number of pronouns compared to the
WSJ, which tends to use explicit attribution more fre-
quently. The SMH also has a significant proportion
of quotes that appear alone in a paragraph, while
the WSJ has almost none. Finally, when attribut-
ing a quote using a trigram pattern, the SMH mostly
uses the Quote-Person-Said pattern, while the WSJ
mostly uses the Quote-Said-Person pattern. These
differences probably reflect the editorial guidelines
of the two newspapers.
The differences between the news corpora and
the literature corpus are more substantial. Most no-
tably the LIT corpus has a much higher proportion
of quotes that fall into the Conversation and Alone
categories. This is unsurprising as both monologues
and dialogues are common in fiction, but are rare in
newswire. The two news corpora have more quotes
in the Trigram and Backoff categories.
4 Quote Extraction
Quote extraction is the task of finding the spans that
represent quotes within a document. There are three
types of quotes that can appear:
1. Direct quotes appear entirely between quota-
tion marks, and are used to indicate that the
speaker said precisely what is written;
2. Indirect quotes do not appear between or con-
tain quotation marks, and are used to get the
speaker?s point across without implying that
the speaker used the exact words of the quote;
3. Mixed quotes are indirect quotes that contain a
directly quoted portion.
In this work, we limit ourselves to detecting direct
quotes and the direct portions of mixed quotes.
To extract quotes we use a regular expression that
searches for text between quotation marks. We also
deal with the special case of multi-paragraph quotes
where one quotation mark opens the quote and every
new paragraph that forms part of the quote, with a fi-
nal quotation mark only at the very end of the quote.
This straightforward approach yields over 99% ac-
curacy on all three corpora.
793
5 Quote Attribution
Given a document with a set of quotes and a set
of entities, quote attribution is the task of finding
the entity that represents the speaker of each quote,
based on the context provided by the document.
Identifying the correct entity can involve choosing
either an entire coreference chain representing an
entity, or identifying a specific span of text that rep-
resents the entity.
In practice, most applications only need to know
which coreference chain represents the speaker, not
which particular span in the text. Despite this, the
best evidence about which chain is the speaker is
found in the context of the individual text spans, and
most existing systems aim to get the particular entity
span correct. This presents a problem for evaluation,
as an incorrect entity span may be identified, but it
might still be part of the correct coreference chain.
We chose to count attributions as correct if they at-
tributed the quote to the correct coreference chain
for both the LIT and SMH corpora, while for the WSJ
corpus, where the full coreference chains do not ex-
ist, we evaluated an attribution as correct if it was to
the correct entity span in the text.
5.1 Rule-based Baseline
To establish the effectiveness of our method we built
a rule-based baseline system. For each quote it pro-
ceeds with the following steps:
1. Search backwards in the text from the end of
the sentence the quote appears in for a reported
speech verb
2. If the verb is found return the entity mention
nearest the verb (ignoring mentions in quotes),
in the current sentence or any sentence preced-
ing it
3. If not, return the mention of an entity near-
est the end of the quote (ignoring mentions in
quotes), in the current sentence or any sentence
preceding it
This forms a reasonable baseline as it is able to pick
up the quotes that fall into the more simple cate-
gories, such as the Trigram category and the Added
category. It is also able to make a guess at the more
complicated categories, without using gold standard
information as the category predictions do.
6 Experimental Setup
We use two classifiers: a logistic regression imple-
mentation available in LIBLINEAR (Fan et al2008),
and a Conditional Random Field (CRF) from CRF-
Suite (Okazaki, 2007). Both packages use maxi-
mum likelihood estimation with L2 regularisation.
We experimented with several values for the coef-
ficient on a development set, but found that it had
little impact, so stuck with the default value. All of
our machine learning experiments use the same text
encoding, which is explained below, and all use the
category predictions when they are available.
6.1 Text Encoding
We encode our text similarly to Elson and McKeown
(2010). The major steps are:
1. Replace all quotes and speakers with special
symbols;
2. Replace all reported speech verbs with a sym-
bol. Elson and McKeown (2010) provided us
with their list of reported speech verbs;
3. Part-of-Speech (POS) tag the text and remove
adjectives, adverbs, and other parts of speech
that do not contribute useful information. We
used the POS tagger from Curran and Clark
(2003);
4. Remove any paragraphs or sentences where no
quotes, pronouns or names occur.
All features that will be discussed are calculated
with respect to this encoding (e.g. word distance
would be the number of words in the encoded text,
rather than the number of words in the original text).
6.2 Features
In our experiments we use the feature set from Elson
and McKeown (2010). The features for a particu-
lar pair of target quote (q) and target speaker (s) are
summarised below.
Distance features including number of words be-
tween q and s, number of paragraphs between
q and s, number of quotes between q and s, and
number of entity mentions between q and s
794
Corpus
Sequence Features
Gold Pred None
LIT 74.7 49.0 49.6
WSJ 87.3 74.1 82.9
SMH 95.0 85.6 92.4
Table 2: Accuracy results comparing the E&M approach
with gold standard, predicted or no sequence features.
Paragraph features derived from the 10 para-
graphs preceding the quote (including the para-
graph the quote is in), includes number of men-
tions of s, number of mentions of other speak-
ers, number of words in each paragraph, and
number of quotes in each paragraph
Nearby features relating to the two tokens either
side of q and s, includes binary features for
each position indicating whether the position is
punctuation, s, q, a different speaker, a differ-
ent quote, or a reported speech verb
Quote features about q itself, including whether s
is mentioned within it, whether other speakers
are mentioned within it, how far the quote is
from the start of its paragraph and the length in
words of q
Sequence features that depend on the speakers
chosen for the previous quotes, includes num-
ber of quotes in the 10 paragraphs preceding
and including the paragraph where q appears
that were attributed to s, and the number that
were attributed to other speakers
6.3 Elson and McKeown Reimplementation
As part of our study we reproduce the core results
of Elson and McKeown (2010) (E&M ), as we be-
lieve it is a state-of-the-art system. This allows us
to determine the effectiveness of our approach when
compared to a state-of-the-art approach, and it also
allows us to determine how well the E&M approach
performs on other corpora. In this section we will
briefly summarise the key elements needed to repro-
duce their work.
The E&M approach makes a binary classification
between ?speaker? and ?not speaker? for up to 15
candidate speakers for each quote. They then recon-
cile these 15 classifications into one speaker predic-
tion for the quote. While E&M experimented with
several different reconciliation methods, we simply
chose the speaker with the highest probability at-
tached to its ?speaker? label.
We conducted an experiment using our imple-
mentation of the E&M method on the original,
unaugmented E&M corpus, to see how our result
compared with E&M ?s 83%. On our test set we
achieved 78.2%, however this rose to 82.3% when
performing 10-fold cross validation across the whole
corpus. Though this is a large difference, it is not
necessarily that surprising, as our test set contains
documents by authors which are unseen, whereas
both the original E&M test set and all the cross val-
idation test sets contain documents by authors that
the learner has seen before.
In their work, E&M make a simplifying assump-
tion that all previous attribution decisions were cor-
rect. Due to this, their sequence features use gold
standard labels from previous quotes, which makes
their results unrealistic. In Table 2 we show the ef-
fect of replacing the gold standard sequence features
with features based on the predicted labels, or with
no sequence features at all. All three corpora show a
significant drop in accuracy, with the LIT corpus in
particular suffering a drop of more than 25%. This
motivates our study into including sequence infor-
mation without using gold standard labels.
7 Class Models
We consider two class models for our experiments,
which are described in detail below. The binary
model is able to take advantage of more data but has
less competition between decisions, while the n-way
model has more competition with less data. Both
models are used with all the decoding methods, with
the exception that the binary model is unsuitable for
the CRF experiments.
7.1 Binary
When working with n previous speakers, a binary
class model works by predicting n independent
?speaker? versus ?not speaker? labels, one for each
quote-speaker pair. As the classifications are inde-
pendent the n decisions need to be reconciled, as
more than one speaker might be predicted. We rec-
oncile the n decisions by attributing the quote to the
795
speaker with the highest ?speaker? probability. Us-
ing a binary class with reconciliation in a greedy
decoding model is equivalent to the method in El-
son and McKeown (2010), except that the gold stan-
dard sequence features are replaced with predicted
sequence features.
7.2 n-way
A key advantage of the binary class model is that
when predicting ?speaker? versus ?not speaker? the
classifier only needs to predict one probability, and
thus can take into account the evidence of all other
quote-speaker pairs. The drawback to the binary
model is that the probabilities assigned to the can-
didate speakers do not need to directly compete
against each other. In other words when assigning
a binary probability to a candidate speaker, the clas-
sifier does not take into account how good the other
candidate speakers are.
To rectify these issues we experiment with a sin-
gle classification for each quote, where the classifier
directly decides between up to n candidate speakers
per quote. As speaker-specific evidence is far too
sparse, we encode the speakers with their ordinal po-
sition backwards from the quote. In other words, the
candidate speaker immediately preceding the quote
would be labelled ?speaker1?, the speaker preced-
ing it would be ?speaker2? and so on. The classifier
then directly predicts these labels. This representa-
tion means that candidate speakers need to directly
compete for probability mass, although it has the
drawback that the evidence for the higher-numbered
speakers is quite sparse.
The features we use for this representation are
similar to the features used in the E&M binary
model. The key difference is that where there were
individual features that were calculated with respect
to the speaker, there are now n features, one for each
of the speaker candidates. This allows the model to
account for the strength of other candidates when as-
signing a speaker label.
8 Sequence Decoding
We noted in the previous section that the E&M re-
sults are based on the unrealistic assumption that
all previous quotes were attributed correctly. In
this section we outline three sequence decoding ap-
proaches that remove this unrealistic assumption,
without removing all of the transition information
that it provides. We believe the transition infor-
mation is important as many quotes have no ex-
plicit attribution in the text, and instead rely on the
reader understanding something about the sequence
of speakers.
For these experiments we regard the set of speaker
attributions in a document as the sequence that we
want to decode. Each individual state therefore rep-
resents a sequence of w previous attribution deci-
sions, and a decision for the current quote. Obtain-
ing a probability for this state can be done in one
of two ways. Either the transition probabilities from
state to state can be learned explicitly, or the w pre-
vious attribution decisions can be used to build the
sequence features for the current state, which im-
plicitly encodes the transition probabilities.
8.1 Greedy Decoding
In sequence decoding the greedy algorithm calcu-
lates the probability of each label at a decision point
based on the predictions it has already made for pre-
vious decisions. More concretely this means we ap-
ply a standard classifier at each step, with the se-
quence features being calculated from the predic-
tions made in previous steps. Greedy decoding is
efficient in that it only considers one possible history
at each decision point, but it is consequently unable
to make trade-offs between good previous choices
and good current choices, which means that in gen-
eral it will not return the optimum sequence of la-
bels. As greedy decoding is an efficient algorithm
we do not restrict w, the number of previous deci-
sions, beyond the 10 paragraph restriction that is al-
ready in place.
8.2 Viterbi Decoding
Viterbi decoding finds the most probable path
through a sequence of decisions. It does this by de-
termining the probabilities of each of the labels at
the current decision point, with each of the possi-
ble histories of decisions within a given window w.
These probabilities can be multiplied together with
the previous decisions to retrieve a joint probability
for the entire sequence. The final decision for each
quote is then just the speaker which is predicted by
the sequence with the largest joint probability.
796
Although they do not come with probabilities,
we chose to include the category predictions in our
Viterbi model. As we already know that they are
accurate indicators of the speaker we assign them
a probability of 100%, which effectively forces the
Viterbi decoder to choose the category predictions
when they are available. It is worth noting that
quotes are only assigned to the Conversation cate-
gory if the two prior quotes had alternating speakers.
As such, during the Viterbi decoding the categori-
sation of the quote actually needs to be recalculated
with regard to the two previous attribution decisions.
By forcing the Viterbi decoder to choose category
predictions when they are available, we get the ad-
vantage that quote sequences with no intervening
text may be forced into the Conversation category,
which is typically under-represented otherwise.
Both the sequences using the binary class and
the n-way class can be decoded using the Viterbi
algorithm, so we experiment with both class mod-
els. We also experiment with varying window sizes
(w), in order to gain insight into how many previous
decisions impact the current decision. Though the
Viterbi algorithm is able to find the best sequence
of probabilities without the need for an exhaustive
search, it can still take an impractical amount of time
to run. As such we ignore all but the 10 most promis-
ing sequences at each decision point.
8.3 Conditional Random Field (CRF) Decoding
The key drawback with the logistic regression ex-
periments described thus far is that the sequence
features are trained with gold standard information.
This means that during the training phase the se-
quence features have perfect information about pre-
vious speakers and are thus unrealistically good pre-
dictors of the final outcome. When the resulting
model is used with the less accurate predicted se-
quence features, it is overconfident about the infor-
mation those features provide.
We account for this by using a first-order linear
chain CRF model, which learns the probabilities of
progressing from speaker to speaker more directly.
During training the CRF is able to learn the asso-
ciation between features and labels, as well as the
chance of transitioning from one label to the next.
It also has the advantage of avoiding the label bias
problem that would be present in the equivalent Hid-
den Markov Model (Lafferty et al2001).
Though the n-way class model can be used di-
rectly in a CRF, the binary class model is more chal-
lenging. The main problem is that the ?speaker?
versus ?not speaker? output of the binary classifier
does not directly form a meaningful sequence that
the CRF can learn over. If the reconciliation step is
included it effectively adds an extra layer to the lin-
ear chain, making learning more difficult. Due to
these difficulties we only use the n-way class model
in our CRF experiments.
9 Results
The main result of our experiments with the E&M
method is the large drop in accuracy that occurs
when the gold standard sequence features are re-
moved, which can be seen in Table 3. When using
the binary class model this results in a drop of 25.1%
for the LIT corpus, while for the WSJ and SMH cor-
pora the drop is less substantial at 4.4% and 2.6%,
respectively. For the LIT corpus the drop is so severe
that it actually performs worse than the simple rule-
based system. Even more surprisingly, when the
predictions from previous decisions are used with a
simple greedy decoder, the accuracy drops even fur-
ther for all three corpora. This indicates that the clas-
sifier is putting too much weight on the gold stan-
dard sequence features during training, and is mis-
led into making poor decisions when the predicted
features are used during test time.
Table 4 shows the results for the n-way class
model. Compared to the binary model, the n-way
class model generally produced lower results, al-
though the results were more stable to changes in
parameters and decoders. The only corpus that pro-
duced better results with the n-way class model was
the WSJ corpus, which does not have full entity
coreference information. This indicates that the n-
way model may be helpful when there is more vari-
ety in the choice of entities.
The final results we would like to discuss here are
the CRF results. On all three corpora the CRF results
are underwhelming. The major issue that we can
see when applying a CRF model to this task is that
the sequences that it needs to learn over are entire
documents. This means that for the LIT corpus the
training set consisted of only 12 sequences, while
797
Corpus E&M Rule No seq. Greedy Viterbi
w = 1 w = 2 w = 5
LIT 74.7 53.3 49.6 49.0 46.0 49.8 45.9
WSJ 87.3 77.9 82.9 74.1 82.3 83.1 83.1
SMH 95.0 91.2 92.4 85.6 91.7 90.5 84.1
Table 3: Accuracy on test set with the binary class model. Italicised results indicate gold standard information is used.
Bold results show the best realistic result for each corpus.
Corpus Gold seq. Rule No seq. Greedy Viterbi CRF
w = 1 w = 2 w = 5
LIT 68.6 53.3 47.1 46.7 42.5 46.5 44.4 48.6
WSJ 88.9 77.9 83.6 77.0 84.1 83.7 83.3 79.6
SMH 94.4 91.2 90.0 89.6 89.5 90.1 90.4 91.0
Table 4: Accuracy on test set with the n-way class model. Italicised results indicate gold standard information is used.
Bold results show the best realistic result for each corpus.
the test set consisted of 6 sequences. With so few
sequences it is unsurprising that the CRF model did
not perform well. The limited range of the first order
linear chain model could also have played a part in
the poor performance of the CRF models. However,
moving to a higher-order model is problematic as
the number of transition probabilities that need to be
calculated increases exponentially with the order of
the model.
10 Conclusion
In this paper, we present the first large-scale evalua-
tion of a quote attribution system on newswire from
the 1989 Wall Street Journal (WSJ) and the 2009
Sydney Morning Herald (SMH), as well as compar-
ing against previous work (Elson and McKeown,
2010) on 19th-century literature.
We show that when Elson and McKeown?s unre-
alistic use of gold-standard history information is re-
moved, accuracy on all three corpora drops substan-
tially. We demonstrate that by treating quote attribu-
tion as a sequence labelling task, we can achieve re-
sults that are very close to their results on newswire,
though not for literature.
In future work, we intend to further explore the
sequence features that have a large impact on accu-
racy, and to find similar features or proxies for the
sequence features that would be beneficial. We will
also explore other approaches to representing quote
attribution with a CRF. For the task more broadly,
it would be beneficial to compare methods of find-
ing indirect and mixed quotes, and to evaluate how
well quote attribution performs on those quotes as
opposed to just direct quotes.
Our newswire results, 92.4% for the SMH and
84.1% for the WSJ corpus, demonstrate it is possible
to develop an accurate and practical quote extraction
system. On the LIT corpus our best result was from
the simple rule-based system, which yielded 53.3%.
It is clear that literature poses an ongoing research
challenge.
Acknowledgements
We would like to thank David Elson for helping
us to reimplement his method and Bonnie Webber
for her feedback and assistance. O?Keefe has been
supported by a University of Sydney Merit schol-
arship and a Capital Markets CRC top-up scholar-
ship; Pareti has been supported by a Scottish Infor-
matics and Computer Science Alliance (SICSA) stu-
dentship. This work has been supported by ARC
Discovery grant DP1097291 and the Capital Mar-
kets CRC Computable News project.
References
James R. Curran and Stephen Clark. 2003. Investi-
gating GIS and smoothing for maximum entropy
taggers. In Proceedings of the tenth conference on
798
European chapter of the Association for Compu-
tational Linguistics, pages 91?98.
Peter T. Davis, David K. Elson, and Judith L. Kla-
vans. 2003. Methods for precise named entity
matching in digital collections. In Proceedings of
the 3rd ACM/IEEE-CS Joint Conference on Digi-
tal libraries, pages 125?127.
Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pas-
cal Denis, Gaelle Recource, and Victor Mignot.
2011. Extracting and visualizing quotations from
news wires. Human Language Technology. Chal-
lenges for Computer Science and Linguistics,
pages 522?532.
David. K Elson and Kathleen. R McKeown. 2010.
Automatic attribution of quoted speech in literary
narrative. In Proceedings of AAAI, pages 1013?
1019.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher Manning. 2005. Incorporating non-local in-
formation into information extraction systems by
gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, pages 363?370.
Kevin Glass and Shaun Bangay. 2007. A naive
salience-based method for speaker identification
in fiction books. In Proceedings of the 18th An-
nual Symposium of the Pattern Recognition Asso-
ciation of South Africa (PRASA07), pages 1?6.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2012. Evaluating
entity linking with Wikipedia. Artificial Intelli-
gence. (in press).
John Lafferty, Andrew McCallum, and Fernando
C.N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. International Conference on Ma-
chine Learning, pages 282?289.
Nuno Mamede and Pedro Chaleira. 2004. Charac-
ter identification in children stories. Advances in
Natural Language Processing, pages 82?90.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of Conditional Random Fields
(CRFs). URL http://www.chokkan.org/
software/crfsuite/.
Silvia Pareti. 2012. A database of attribution rela-
tions. In Proceedings of the Eight International
Conference on Language Resources and Evalua-
tion (LREC?12), pages 3213?3217.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In Proceedings of Recent Advances
in Natural Language Processing, pages 487?492.
Beno??t Sagot, Laurence Danlos, and Rosa Stern.
2010. A lexicon of french quotation verbs for au-
tomatic quotation extraction. In 7th international
conference on Language Resources and Evalua-
tion - LREC 2010.
Luis Sarmento and Sergio Nunes. 2009. Automatic
extraction of quotes and topics from news feeds.
In 4th Doctoral Symposium on Informatics Engi-
neering.
Nathan Schneider, Rebecca Hwa, Philip Gianfor-
toni, Dipanjan Das, Michael Heilman, Alan W.
Black, Frederik L. Crabbe, and Noah A. Smith.
2010. Visualizing topical quotations over time
to understand news discourse. Technical Report
CMU-LTI-01-013, Carnegie Mellon University.
Ralph Weischedel and Ada Brunstein. 2005. BBN
pronoun coreference and entity type corpus. Lin-
guistic Data Consortium, Philadelphia.
Jason Zhang, Alan Black, and Richard Sproat.
2003. Identifying speakers in children?s stories
for speech synthesis. In Proceedings of EU-
ROSPEECH, pages 2041?2044.
799
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 207?215,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Rebanking CCGbank for improved NP interpretation
Matthew Honnibal and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{mhonn,james}@it.usyd.edu.au
Johan Bos
University of Groningen
The Netherlands
bos@meaningfactory.com
Abstract
Once released, treebanks tend to remain
unchanged despite any shortcomings in
their depth of linguistic analysis or cover-
age of specific phenomena. Instead, sepa-
rate resources are created to address such
problems. In this paper we show how to
improve the quality of a treebank, by in-
tegrating resources and implementing im-
proved analyses for specific constructions.
We demonstrate this rebanking process
by creating an updated version of CCG-
bank that includes the predicate-argument
structure of both verbs and nouns, base-
NP brackets, verb-particle constructions,
and restrictive and non-restrictive nominal
modifiers; and evaluate the impact of these
changes on a statistical parser.
1 Introduction
Progress in natural language processing relies on
direct comparison on shared data, discouraging
improvements to the evaluation data. This means
that we often spend years competing to reproduce
partially incorrect annotations. It also encourages
us to approach related problems as discrete tasks,
when a new data set that adds deeper information
establishes a new incompatible evaluation.
Direct comparison has been central to progress
in statistical parsing, but it has also caused prob-
lems. Treebanking is a difficult engineering task:
coverage, cost, consistency and granularity are all
competing concerns that must be balanced against
each other when the annotation scheme is devel-
oped. The difficulty of the task means that we
ought to view treebanking as an ongoing process
akin to grammar development, such as the many
years of work on the ERG (Flickinger, 2000).
This paper demonstrates how a treebank can be
rebanked to incorporate novel analyses and infor-
mation from existing resources. We chose to work
on CCGbank (Hockenmaier and Steedman, 2007),
a Combinatory Categorial Grammar (Steedman,
2000) treebank acquired from the Penn Treebank
(Marcus et al, 1993). This work is equally ap-
plicable to the corpora described by Miyao et al
(2004), Shen et al (2008) or Cahill et al (2008).
Our first changes integrate four previously sug-
gested improvements to CCGbank. We then de-
scribe a novel CCG analysis of NP predicate-
argument structure, which we implement using
NomBank (Meyers et al, 2004). Our analysis al-
lows the distinction between core and peripheral
arguments to be represented for predicate nouns.
With this distinction, an entailment recognition
system could recognise that Google?s acquisition
of YouTube entailed Google acquired YouTube, be-
cause equivalent predicate-argument structures are
built for both. Our analysis also recovers non-
local dependencies mediated by nominal predi-
cates; for instance, Google is the agent of acquire
in Google?s decision to acquire YouTube.
The rebanked corpus extends CCGbank with:
1. NP brackets from Vadas and Curran (2008);
2. Restored and normalised punctuation;
3. Propbank-derived verb subcategorisation;
4. Verb particle structure drawn from Propbank;
5. Restrictive and non-restrictive adnominals;
6. Reanalyses to promote better head-finding;
7. Nombank-derived noun subcategorisation.
Together, these changes modify 30% of the la-
belled dependencies in CCGbank, demonstrating
how multiple resources can be brought together in
a single, richly annotated corpus. We then train
and evaluate a parser for these changes, to investi-
gate their impact on the accuracy of a state-of-the-
art statistical CCG parser.
207
2 Background and motivation
Formalisms like HPSG (Pollard and Sag, 1994),
LFG (Kaplan and Bresnan, 1982), and CCG (Steed-
man, 2000) are linguistically motivated in the
sense that they attempt to explain and predict
the limited variation found in the grammars of
natural languages. They also attempt to spec-
ify how grammars construct semantic representa-
tions from surface strings, which is why they are
sometimes referred to as deep grammars. Anal-
yses produced by these formalisms can be more
detailed than those produced by skeletal phrase-
structure parsers, because they produce fully spec-
ified predicate-argument structures.
Unfortunately, statistical parsers do not take ad-
vantage of this potential detail. Statistical parsers
induce their grammars from corpora, and the
corpora for linguistically motivated formalisms
currently do not contain high quality predicate-
argument annotation, because they were derived
from the Penn Treebank (PTB Marcus et al, 1993).
Manually written grammars for these formalisms,
such as the ERG HPSG grammar (Flickinger, 2000)
and the XLE LFG grammar (Butt et al, 2006)
produce far more detailed and linguistically cor-
rect analyses than any English statistical parser,
due to the comparatively coarse-grained annota-
tion schemes of the corpora statistical parsers are
trained on. While rule-based parsers use gram-
mars that are carefully engineered (e.g. Oepen
et al, 2004), and can be updated to reflect the best
linguistic analyses, statistical parsers have so far
had to take what they are given.
What we suggest in this paper is that a tree-
bank?s grammar need not last its lifetime. For a
start, there have been many annotations of the PTB
that add much of the extra information needed to
produce very high quality analyses for a linguis-
tically motivated grammar. There are also other
transformations which can be made with no addi-
tional information. That is, sometimes the existing
trees allow transformation rules to be written that
improve the quality of the grammar.
Linguistic theories are constantly changing,
which means that there is a substantial lag between
what we (think we) understand of grammar and
the annotations in our corpora. The grammar en-
gineering process we describe, which we dub re-
banking, is intended to reduce this gap, tightening
the feedback loop between formal and computa-
tional linguistics.
2.1 Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG; Steed-
man, 2000) is a lexicalised grammar, which means
that all grammatical dependencies are specified
in the lexical entries and that the production of
derivations is governed by a small set of rules.
Lexical categories are either atomic (S , NP ,
PP , N ), or a functor consisting of a result, direc-
tional slash, and argument. For instance, in might
head a PP -typed constituent with one NP -typed
argument, written as PP/NP .
A category can have a functor as its result, so
that a word can have a complex valency structure.
For instance, a verb phrase is represented by the
category S\NP : it is a function from a leftward
NP (a subject) to a sentence. A transitive verb
requires an object to become a verb phrase, pro-
ducing the category (S\NP)/NP .
A CCG grammar consists of a small number of
schematic rules, called combinators. CCG extends
the basic application rules of pure categorial gram-
mar with (generalised) composition rules and type
raising. The most common rules are:
X /Y Y ? X (>)
Y X \Y ? X (<)
X /Y Y /Z ? X /Z (>B)
Y \Z X \Y ? X \Z (<B)
Y /Z X \Y ? X /Z (<B?)
CCGbank (Hockenmaier and Steedman, 2007)
extends this compact set of combinatory rules with
a set of type-changing rules, designed to strike a
better balance between sparsity in the category set
and ambiguity in the grammar. We mark type-
changing rules TC in our derivations.
In wide-coverage descriptions, categories are
generally modelled as typed-feature structures
(Shieber, 1986), rather than atomic symbols. This
allows the grammar to include a notion of headed-
ness, and to unify under-specified features.
We occasionally must refer to these additional
details, for which we employ the following no-
tation. Features are annotated in square-brackets,
e.g. S [dcl ]. Head-finding indices are annotated on
categories in subscripts, e.g. (NPy\NPy)/NPz .
The index of the word the category is assigned to
is left implicit. We will sometimes also annotate
derivations with the heads of categories as they are
being built, to help the reader keep track of what
lexemes have been bound to which categories.
208
3 Combining CCGbank corrections
There have been a few papers describing correc-
tions to CCGbank. We bring these corrections to-
gether for the first time, before building on them
with our further changes.
3.1 Compound noun brackets
Compound noun phrases can nest inside each
other, creating bracketing ambiguities:
(1) (crude oil) prices
(2) crude (oil prices)
The structure of such compound noun phrases
is left underspecified in the Penn Treebank (PTB),
because the annotation procedure involved stitch-
ing together partial parses produced by the Fid-
ditch parser (Hindle, 1983), which produced flat
brackets for these constructions. The bracketing
decision was also a source of annotator disagree-
ment (Bies et al, 1995).
When Hockenmaier and Steedman (2002) went
to acquire a CCG treebank from the PTB, this posed
a problem. There is no equivalent way to leave
these structures under-specified in CCG, because
derivations must be binary branching. They there-
fore employed a simple heuristic: assume all such
structures branch to the right. Under this analysis,
crude oil is not a constituent, producing an incor-
rect analysis as in (1).
Vadas and Curran (2007) addressed this by
manually annotating all of the ambiguous noun
phrases in the PTB, and went on to use this infor-
mation to correct 20,409 dependencies (1.95%) in
CCGbank (Vadas and Curran, 2008). Our changes
build on this corrected corpus.
3.2 Punctuation corrections
The syntactic analysis of punctuation is noto-
riously difficult, and punctuation is not always
treated consistently in the Penn Treebank (Bies
et al, 1995). Hockenmaier (2003) determined
that quotation marks were particularly problem-
atic, and therefore removed them from CCGbank
altogether. We use the process described by Tse
and Curran (2008) to restore the quotation marks
and shift commas so that they always attach to the
constituent to their left. This allows a grammar
rule to be removed, preventing a great deal of spu-
rious ambiguity and improving the speed of the
C&C parser (Clark and Curran, 2007) by 37%.
3.3 Verb predicate-argument corrections
Semantic role descriptions generally recognise a
distinction between core arguments, whose role
comes from a set specific to the predicate, and pe-
ripheral arguments, who have a role drawn from a
small, generic set. This distinction is represented
in the surface syntax in CCG, because the category
of a verb must specify its argument structure. In
(3) as a director is annotated as a complement; in
(4) it is an adjunct:
(3) He
NP
joined
(S\NP)/PP
as a director
PP
(4) He
NP
joined
S\NP
as a director
(S\NP)\(S\NP)
CCGbank contains noisy complement and ad-
junct distinctions, because they were drawn from
PTB function labels which imperfectly represent
the distinction. In our previous work we used
Propbank (Palmer et al, 2005) to convert 1,543
complements to adjuncts and 13,256 adjuncts to
complements (Honnibal and Curran, 2007). If a
constituent such as as a director received an ad-
junct category, but was labelled as a core argu-
ment in Propbank, we changed it to a comple-
ment, using its head?s part-of-speech tag to infer
its constituent type. We performed the equivalent
transformation to ensure all peripheral arguments
of verbs were analysed as adjuncts.
3.4 Verb-particle constructions
Propbank also offers reliable annotation of verb-
particle constructions. This was not available in
the PTB, so Hockenmaier and Steedman (2007)
annotated all intransitive prepositions as adjuncts:
(5) He
NP
woke
S\NP
up
(S\NP)\(S\NP)
We follow Constable and Curran (2009) in ex-
ploiting the Propbank annotations to add verb-
particle distinctions to CCGbank, by introducing a
new atomic category PT for particles, and chang-
ing their status from adjuncts to complements:
(6) He
NP
woke
(S\NP)/PT
up
PT
This analysis could be improved by adding extra
head-finding logic to the verbal category, to recog-
nise the multi-word expression as the head.
209
Rome ?s gift of peace to Europe
NP (NP/(N /PP))\NP (N /PP)/PP)/PP PP/NP NP PP/NP NP
< > >
N /(N /PP) PP PP
>
(N /PP)/PP
>
N /PP
>
NP
Figure 1: Deverbal noun predicate with agent, patient and beneficiary arguments.
4 Noun predicate-argument structure
Many common nouns in English can receive
optional complements and adjuncts, realised by
prepositional phrases, genitive determiners, com-
pound nouns, relative clauses, and for some nouns,
complementised clauses. For example, deverbal
nouns generally have argument structures similar
to the verbs they are derived from:
(7) Rome?s destruction of Carthage
(8) Rome destroyed Carthage
The semantic roles of Rome and Carthage are the
same in (7) and (8), but the noun cannot case-
mark them directly, so of and the genitive clitic
are pressed into service. The semantic role de-
pends on both the predicate and subcategorisation
frame:
(9) Carthage?sp destructionPred.
(10) Rome?sa destructionPred. of Carthagep
(11) Rome?sa giftPred.
(12) Rome?sa giftPred. of peacep to Europeb
In (9), the genitive introduces the patient, but
when the patient is supplied by the PP, it instead
introduces the agent. The mapping differs for gift,
where the genitive introduces the agent.
Peripheral arguments, which supply generically
available modifiers of time, place, cause, quality
etc, can be realised by pre- and post-modifiers:
(13) The portrait in the Louvre
(14) The fine portrait
(15) The Louvre?s portraits
These are distinct from core arguments because
their interpretation does not depend on the pred-
icate. The ambiguity can be seen in an NP such as
The nobleman?s portrait, where the genitive could
mark possession (peripheral), or it could introduce
the patient (core). The distinction between core
and peripheral arguments is particularly difficult
for compound nouns, as pre-modification is very
productive in English.
4.1 CCG analysis
We designed our analysis for transparency be-
tween the syntax and the predicate-argument
structure, by stipulating that all and only the core
arguments should be syntactic arguments of the
predicate?s category. This is fairly straightforward
for arguments introduced by prepositions:
destruction of Carthage
N /PPy PPy/NPy NP
>
PPCarthage
>
Ndestruction
In our analysis, the head of of Carthage is
Carthage, as of is assumed to be a semantically
transparent case-marker. We apply this analysis
to prepositional phrases that provide arguments to
verbs as well ? a departure from CCGbank.
Prepositional phrases that introduce peripheral
arguments are analysed as syntactic adjuncts:
The war in 149 B.C.
NPy/Ny N (Ny\Ny)/NPz NP
>
(Ny\Ny)in
<
Nwar
>
NPwar
Adjunct prepositional phrases remain headed by
the preposition, as it is the preposition?s semantics
that determines whether they function as temporal,
causal, spatial etc. arguments. We follow Hocken-
maier and Steedman (2007) in our analysis of gen-
itives which realise peripheral arguments, such as
the literal possessive:
Rome ?s aqueducts
NP (NPy/Ny)\NPz N
<
(NPy/Ny)?s
>
NPaqueducts
Arguments introduced by possessives are a lit-
tle trickier, because the genitive also functions as
a determiner. We achieve this by having the noun
subcategorise for the argument, which we type
PP , and having the possessive subcategorise for
the unsaturated noun to ultimately produce an NP :
210
Google ?s decision to buy YouTube
NP (NPy/(Ny/PPz )y)\NPz (N /PPy)/(S [to]z\NPy)z (S [to]y\NPz )y/(S [b]y\NPz )y (S [b]\NPy)/NPz NP
< >
NPy/(Ny/PPGoogle)y S [b]\NPy
>B >
NPdecision/(S [to]y\NPGoogle)y S [to]buy\NPy
>
NP
Figure 2: The coindexing on decision?s category allows the hard-to-reach agent of buy to be recovered. A non-normal form
derivation is shown so that instantiated variables can be seen.
Carthage ?s destruction
NP (NPy/(Ny/PPz )y)\NPz N /PPy
<
(NPy/(Ny/PPCarthage)y)?s
>
NPdestruction
In this analysis, we regard the genitive clitic as a
case-marker that performs a movement operation
roughly analogous to WH-extraction. Its category
is therefore similar to the one used in object ex-
traction, (N \N )/(S/NP). Figure 1 shows an ex-
ample with multiple core arguments.
This analysis allows recovery of verbal argu-
ments of nominalised raising and control verbs, a
construction which both Gildea and Hockenmaier
(2003) and Boxwell and White (2008) identify as a
problem case when aligning Propbank and CCG-
bank. Our analysis accommodates this construc-
tion effortlessly, as shown in Figure 2. The cate-
gory assigned to decision can coindex the missing
NP argument of buy with its own PP argument.
When that argument is supplied by the genitive,
it is also supplied to the verb, buy, filling its de-
pendency with its agent, Google. This argument
would be quite difficult to recover using a shallow
syntactic analysis, as the path would be quite long.
There are 494 such verb arguments mediated by
nominal predicates in Sections 02-21.
These analyses allow us to draw comple-
ment/adjunct distinctions for nominal predicates,
so that the surface syntax takes us very close to
a full predicate-argument analysis. The only in-
formation we are not specifying in the syntac-
tic analysis are the role labels assigned to each
of the syntactic arguments. We could go further
and express these labels in the syntax, produc-
ing categories like (N /PP{0}y)/PP{1}z and
(N /PP{1}y)/PP{0}z , but we expect that this
would cause sparse data problems given the lim-
ited size of the corpus. This experiment would be
an interesting subject of future work.
The only local core arguments that we do not
annotate as syntactic complements are compound
nouns, such as decision makers. We avoided these
arguments because of the productivity of noun-
noun compounding in English, which makes these
argument structures very difficult to recover.
We currently do not have an analysis that allows
support verbs to supply noun arguments, so we
do not recover any of the long-range dependency
structures described by Meyers et al (2004).
4.2 Implementation and statistics
Our analysis requires semantic role labels for each
argument of the nominal predicates in the Penn
Treebank ? precisely what NomBank (Meyers
et al, 2004) provides. We can therefore draw our
distinctions using the process described in our pre-
vious work, Honnibal and Curran (2007).
NomBank follows the same format as Prop-
bank, so the procedure is exactly the same. First,
we align CCGbank and the Penn Treebank, and
produce a version of NomBank that refers to CCG-
bank nodes. We then assume that any preposi-
tional phrase or genitive determiner annotated as
a core argument in NomBank should be analysed
as a complement, while peripheral arguments and
adnominals that receive no semantic role label at
all are analysed as adjuncts.
We converted 34,345 adnominal prepositional
phrases to complements, leaving 18,919 as ad-
juncts. The most common preposition converted
was of, which was labelled as a core argument
99.1% of the 19,283 times it occurred as an ad-
nominal. The most common adjunct preposition
was in, which realised a peripheral argument in
59.1% of its 7,725 occurrences.
The frequent prepositions were more skewed to-
wards core arguments. 73% of the occurrences of
the 5 most frequent prepositions (of, in, for, on and
to) realised peripheral arguments, compared with
53% for other prepositions.
Core arguments were also more common than
peripheral arguments for possessives. There are
20,250 possessives in the corpus, of which 75%
were converted to complements. The percentage
was similar for both personal pronouns (such as
his) and genitive phrases (such as the boy?s).
211
5 Adding restrictivity distinctions
Adnominals can have either a restrictive or a non-
restrictive (appositional) interpretation, determin-
ing the potential reference of the noun phrase
it modifies. This ambiguity manifests itself in
whether prepositional phrases, relative clauses and
other adnominals are analysed as modifiers of
either N or NP, yielding a restrictive or non-
restrictive interpretation respectively.
In CCGbank, all adnominals attach to NPs,
producing non-restrictive interpretations. We
therefore move restrictive adnominals to N nodes:
All staff on casual contracts
NP/N N (N \N )/NP N /N N
>
N
TC
NP
>
N \N
<
N
>
NP
This corrects the previous interpretation, which
stated that there were no permanent staff.
5.1 Implementation and statistics
The Wall Street Journal?s style guide mandates
that this attachment ambiguity be managed by
bracketing non-restrictive relatives with commas
(Martin, 2002, p. 82), as in casual staff, who have
no health insurance, support it. We thus use punc-
tuation to make the attachment decision.
All NP\NP modifiers that are not preceded by
punctuation were moved to the lowest N node
possible and relabelled N \N . We select the low-
est (i.e. closest to leaf) N node because some ad-
jectives, such as present or former, require scope
over the qualified noun, making it safer to attach
the adnominal first.
Some adnominals in CCGbank are created by
the S\NP ? NP\NP unary type-changing rule,
which transforms reduced relative clauses. We in-
troduce a S\NP ? N \N in its place, and add a
binary rule cued by punctuation to handle the rela-
tively rare non-restrictive reduced relative clauses.
The rebanked corpus contains 34,134 N \N re-
strictive modifiers, and 9,784 non-restrictive mod-
ifiers. Most (61%) of the non-restrictive modifiers
were relative clauses.
6 Reanalysing partitive constructions
True partitive constructions consist of a quantifier
(16), a cardinal (17) or demonstrative (18) applied
to an NP via of. There are similar constructions
headed by common nouns, as in (19):
(16) Some of us
(17) Four of our members
(18) Those of us who smoke
(19) A glass of wine
We regard the common noun partitives as headed
by the initial noun, such as glass, because this
noun usually controls the number agreement. We
therefore analyse these cases as nouns with prepo-
sitional arguments. In (19), glass would be as-
signed the category N /PP .
True partitive constructions are different, how-
ever: they are always headed by the head of the NP
supplied by of. The construction is quite common,
because it provides a way to quantify or apply two
different determiners.
Partitive constructions are not given special
treatment in the PTB, and were analysed as noun
phrases with a PP modifier in CCGbank:
Four of our members
NP (NPy\NPy)/NPz NPy/Ny N
>
NPmembers
>
(NPy\NPy)of
<
NPFour
This analysis does not yield the correct seman-
tics, and may even hurt parser performance, be-
cause the head of the phrase is incorrectly as-
signed. We correct this with the following anal-
ysis, which takes the head from the NP argument
of the PP:
Four of our members
NPy/PPy PPy/NPy NPy/Ny N
>
NPmembers
>
PPmembers
>
NPmembers
The cardinal is given the category NP/PP ,
in analogy with the standard determiner category
which is a function from a noun to a noun phrase
(NP/N ).
212
Corpus L. DEPS U. DEPS CATS
+NP brackets 97.2 97.7 98.5
+Quotes 97.2 97.7 98.5
+Propbank 93.0 94.9 96.7
+Particles 92.5 94.8 96.2
+Restrictivity 79.5 94.4 90.6
+Part. Gen. 76.1 90.1 90.4
+NP Pred-Arg 70.6 83.3 84.8
Table 1: Effect of the changes on CCGbank, by percentage
of dependencies and categories left unchanged in Section 00.
6.1 Implementation and Statistics
We detect this construction by identifying NPs
post-modified by an of PP. The NP?s head must
either have the POS tag CD, or be one of the follow-
ing words, determined through manual inspection
of Sections 02-21:
all, another, average, both, each, another, any,
anything, both, certain, each, either, enough, few,
little, most, much, neither, nothing, other, part,
plenty, several, some, something, that, those.
Having identified the construction, we simply rela-
bel the NP to NP/PP , and the NP\NP adnom-
inal to PP . We identified and reanalysed 3,010
partitive genitives in CCGbank.
7 Similarity to CCGbank
Table 1 shows the percentage of labelled depen-
dencies (L. Deps), unlabelled dependencies (U.
Deps) and lexical categories (Cats) that remained
the same after each set of changes.
A labelled dependency is a 4-tuple consisting of
the head, the argument, the lexical category of the
head, and the argument slot that the dependency
fills. For instance, the subject fills slot 1 and the
object fills slot 2 on the transitive verb category
(S\NP)/NP . There are more changes to labelled
dependencies than lexical categories because one
lexical category change alters all of the dependen-
cies headed by a predicate, as they all depend on
its lexical category. Unlabelled dependencies con-
sist of only the head and argument.
The biggest changes were those described in
Sections 4 and 5. After the addition of nominal
predicate-argument structure, over 50% of the la-
belled dependencies were changed. Many of these
changes involved changing an adjunct to a com-
plement, which affects the unlabelled dependen-
cies because the head and argument are inverted.
8 Lexicon statistics
Our changes make the grammar sensitive to new
distinctions, which increases the number of lexi-
cal categories required. Table 2 shows the number
Corpus CATS Cats ? 10 CATS/WORD
CCGbank 1286 425 8.6
+NP brackets 1298 429 8.9
+Quotes 1300 431 8.8
+Propbank 1342 433 8.9
+Particles 1405 458 9.1
+Restrictivity 1447 471 9.3
+Part. Gen. 1455 474 9.5
+NP Pred-Arg 1574 511 10.1
Table 2: Effect of the changes on the size of the lexicon.
of lexical categories (Cats), the number of lexical
categories that occur at least 10 times in Sections
02-21 (Cats? 10), and the average number of cat-
egories available for assignment to each token in
Section 00 (Cats/Word). We followed Clark and
Curran?s (2007) process to determine the set of
categories a word could receive, which includes
a part-of-speech back-off for infrequent words.
The lexicon steadily grew with each set of
changes, because each added information to the
corpus. The addition of quotes only added two cat-
egories (LQU and RQU ), and the addition of the
quote tokens slightly decreased the average cate-
gories per word. The Propbank and verb-particle
changes both introduced rare categories for com-
plicated, infrequent argument structures.
The NP predicate-argument structure modifica-
tions added the most information. Head nouns
were previously guaranteed the category N in
CCGbank; possessive clitics always received the
category (NP/N )\NP ; and possessive personal
pronouns were always NP/N . Our changes in-
troduce new categories for these frequent tokens,
which meant a substantial increase in the number
of possible categories per word.
9 Parsing Evaluation
Some of the changes we have made correct prob-
lems that have caused the performance of a sta-
tistical CCG parser to be over-estimated. Other
changes introduce new distinctions, which a parser
may or may not find difficult to reproduce. To in-
vestigate these issues, we trained and evaluated the
C&C CCG parser on our rebanked corpora.
The experiments were set up as follows. We
used the highest scoring configuration described
by Clark and Curran (2007), the hybrid depen-
dency model, using gold-standard POS tags. We
followed Clark and Curran in excluding sentences
that could not be parsed from the evaluation. All
models obtained similar coverage, between 99.0
and 99.3%. The parser was evaluated using depen-
213
WSJ 00 WSJ 23
Corpus LF UF CAT LF UF CAT
CCGbank 87.2 92.9 94.1 87.7 93.0 94.4
+NP brackets 86.9 92.8 93.8 87.3 92.8 93.9
+Quotes 86.8 92.7 93.9 87.1 92.6 94.0
+Propbank 86.7 92.6 94.0 87.0 92.6 94.0
+Particles 86.4 92.5 93.8 86.8 92.6 93.8
All Rebanking 84.2 91.2 91.9 84.7 91.3 92.2
Table 3: Parser evaluation on the rebanked corpora.
Corpus Rebanked CCGbank
LF UF LF UF
+NP brackets 86.45 92.36 86.52 92.35
+Quotes 86.57 92.40 86.52 92.35
+Propbank 87.76 92.96 87.74 92.99
+Particles 87.50 92.77 87.67 92.93
All Rebanking 87.23 92.71 88.02 93.51
Table 4: Comparison of parsers trained on CCGbank and
the rebanked corpora, using dependencies that occur in both.
dencies generated from the gold-standard deriva-
tions (Boxwell, p.c., 2010).
Table 3 shows the accuracy of the parser on Sec-
tions 00 and 23. The parser scored slightly lower
as the NP brackets, Quotes, Propbank and Parti-
cles corrections were added. This apparent decline
in performance is at least partially an artefact of
the evaluation. CCGbank contains some depen-
dencies that are trivial to recover, because Hock-
enmaier and Steedman (2007) was forced to adopt
a strictly right-branching analysis for NP brackets.
There was a larger drop in accuracy on the
fully rebanked corpus, which included our anal-
yses of restrictivity, partitive constructions and
noun predicate-argument structure. This might
also be explained by the evaluation, as the re-
banked corpus includes much more fine-grained
distinctions. The labelled dependencies evaluation
is particularly sensitive to this, as a single category
change affects multiple dependencies. This can be
seen in the smaller gap in category accuracy.
We investigated whether the differences in per-
formance were due to the different evaluation data
by comparing the parsers? performance against the
original parser on the dependencies they agreed
upon, to allow direct comparison. To do this, we
extracted the CCGbank intersection of each cor-
pus?s Section 00 dependencies.
Table 4 compares the labelled and unlabelled re-
call of the rebanked parsers we trained against the
CCGbank parser on these intersections. Note that
each row refers to a different intersection, so re-
sults are not comparable between rows. This com-
parison shows that the declines in accuracy seen in
Table 3 were largely confined to the corrected de-
pendencies. The parser?s performance remained
fairly stable on the dependencies left unchanged.
The rebanked parser performed 0.8% worse
than the CCGbank parser on the intersection de-
pendencies, suggesting that the fine-grained dis-
tinctions we introduced did cause some sparse data
problems. However, we did not change any of
the parser?s maximum entropy features or hyper-
parameters, which are tuned for CCGbank.
10 Conclusion
Research in natural language understanding is
driven by the datasets that we have available. The
most cited computational linguistics work to date
is the Penn Treebank (Marcus et al, 1993)1. Prop-
bank (Palmer et al, 2005) has also been very
influential since its release, and NomBank has
been used for semantic dependency parsing in the
CoNLL 2008 and 2009 shared tasks.
This paper has described how these resources
can be jointly exploited using a linguistically moti-
vated theory of syntax and semantics. The seman-
tic annotations provided by Propbank and Nom-
Bank allowed us to build a corpus that takes much
greater advantage of the semantic transparency
of a deep grammar, using careful analyses and
phenomenon-specific conversion rules.
The major areas of CCGbank?s grammar left to
be improved are the analysis of comparatives, and
the analysis of named entities. English compar-
atives are diverse and difficult to analyse. Even
the XTAG grammar (Doran et al, 1994), which
deals with the major constructions of English in
enviable detail, does not offer a full analysis of
these phenomena. Named entities are also difficult
to analyse, as many entity types obey their own
specific grammars. This is another example of a
phenomenon that could be analysed much better
in CCGbank using an existing resource, the BBN
named entity corpus.
Our rebanking has substantially improved
CCGbank, by increasing the granularity and lin-
guistic fidelity of its analyses. We achieved this
by exploiting existing resources and crafting novel
analyses. The process we have demonstrated can
be used to train a parser that returns dependencies
that abstract away as much surface syntactic vari-
ation as possible ? including, now, even whether
the predicate and arguments are expressed in a
noun phrase or a full clause.
1http://clair.si.umich.edu/clair/anthology/rankings.cgi
214
Acknowledgments
James Curran was supported by Australian Re-
search Council Discovery grant DP1097291 and
the Capital Markets Cooperative Research Centre.
The parsing evaluation for this paper would
have been much more difficult without the assis-
tance of Stephen Boxwell, who helped generate
the gold-standard dependencies with his software.
We are also grateful to the members of the CCG
technicians mailing list for their help crafting the
analyses, particularly Michael White, Mark Steed-
man and Dennis Mehay.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIn-
tyre. 1995. Bracketing guidelines for Treebank II style
Penn Treebank project. Technical report, MS-CIS-95-06,
University of Pennsylvania, Philadelphia, PA, USA.
Stephen Boxwell and Michael White. 2008. Projecting prop-
bank roles onto the CCGbank. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08), pages 3112?3117. European Language Re-
sources Association (ELRA), Marrakech, Morocco.
Miriam Butt, Mary Dalrymple, and Tracy H. King, editors.
2006. Lexical Semantics in LFG. CSLI Publications, Stan-
ford, CA.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Rie-
zler, Josef van Genabith, and Andy Way. 2008. Wide-
coverage deep statistical parsing using automatic depen-
dency structure annotation. Computational Linguistics,
34(1):81?124.
Stephen Clark and James R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear models.
Computational Linguistics, 33(4):493?552.
James Constable and James Curran. 2009. Integrating verb-
particle constructions into CCG parsing. In Proceedings of
the Australasian Language Technology Association Work-
shop 2009, pages 114?118. Sydney, Australia.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srinivas,
and Martin Zaidel. 1994. Xtag system: a wide coverage
grammar for english. In Proceedings of the 15th confer-
ence on Computational linguistics, pages 922?928. ACL,
Morristown, NJ, USA.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar. In
Proceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 57?64. ACL,
Morristown, NJ, USA.
Donald Hindle. 1983. User manual for fidditch, a determin-
istic parser. Technical Memorandum 7590-142, Naval Re-
search Laboratory.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh, Edinburgh, UK.
Julia Hockenmaier and Mark Steedman. 2002. Acquiring
compact lexicalized grammars from a cleaner treebank.
In Proceedings of the Third Conference on Language Re-
sources and Evaluation Conference, pages 1974?1981.
Las Palmas, Spain.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a
corpus of CCG derivations and dependency structures ex-
tracted from the Penn Treebank. Computational Linguis-
tics, 33(3):355?396.
Matthew Honnibal and James R. Curran. 2007. Improving the
complement/adjunct distinction in CCGBank. In Proceed-
ings of the Conference of the Pacific Association for Com-
putational Linguistics, pages 210?217. Melbourne, Aus-
tralia.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
Functional Grammar: A formal system for grammatical
representation. In Joan Bresnan, editor, The mental repre-
sentation of grammatical relations, pages 173?281. MIT
Press, Cambridge, MA, USA.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Paul Martin. 2002. The Wall Street Journal Guide to Business
Style and Usage. Free Press, New York.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph Gr-
ishman. 2004. The NomBank project: An interim report.
In Frontiers in Corpus Annotation: Proceedings of the
Workshop, pages 24?31. Boston, MA, USA.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a
head-driven phrase structure grammar from the Penn Tree-
bank. In Proceedings of the First International Joint Con-
ference on Natural Language Processing (IJCNLP-04),
pages 684?693. Hainan Island, China.
Stepan Oepen, Daniel Flickenger, Kristina Toutanova, and
Christopher D. Manning. 2004. LinGO Redwoods. a rich
and dynamic treebank for HPSG. Research on Language
and Computation, 2(4):575?596.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase Struc-
ture Grammar. The University of Chicago Press, Chicago.
Libin Shen, Lucas Champollion, and Aravind K. Joshi. 2008.
LTAG-spinal and the treebank: A new resource for incre-
mental, dependency and semantic parsing. Language Re-
sources and Evaluation, 42(1):1?19.
Stuart M. Shieber. 1986. An Introduction to Unification-
Based Approaches to Grammar, volume 4 of CSLI Lecture
Notes. CSLI Publications, Stanford, CA.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA, USA.
Daniel Tse and James R. Curran. 2008. Punctuation normali-
sation for cleaner treebanks and parsers. In Proceedings of
the Australian Language Technology Workshop, volume 6,
pages 151?159. ALTW, Hobart, Australia.
David Vadas and James Curran. 2007. Adding noun phrase
structure to the Penn Treebank. In Proceedings of the 45th
Annual Meeting of the Association of Computational Lin-
guistics, pages 240?247. ACL, Prague, Czech Republic.
David Vadas and James R. Curran. 2008. Parsing noun phrase
structure with CCG. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguistics,
pages 335?343. ACL, Columbus, Ohio, USA.
215
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 228?232,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Event Linking: Grounding Event Reference in a News Archive
Joel Nothman? and Matthew Honnibal+ and Ben Hachey# and James R. Curran?
? e-lab, School of IT
University of Sydney
NSW, Australia
?Capital Markets CRC
55 Harrington St
Sydney
NSW, Australia
{joel,james}@it.usyd.edu.au
+Department of
Computing
Macquarie University
NSW, Australia
#R&D, Thomson
Reuters Corporation
St. Paul
MN, USA
{honnibal,ben.hachey}@gmail.com
Abstract
Interpreting news requires identifying its con-
stituent events. Events are complex linguis-
tically and ontologically, so disambiguating
their reference is challenging. We introduce
event linking, which canonically labels an
event reference with the article where it was
first reported. This implicitly relaxes corefer-
ence to co-reporting, and will practically en-
able augmenting news archives with semantic
hyperlinks. We annotate and analyse a corpus
of 150 documents, extracting 501 links to a
news archive with reasonable inter-annotator
agreement.
1 Introduction
Interpreting news requires identifying its constituent
events. Information extraction (IE) makes this feasi-
ble by considering only events of a specified type,
such as personnel succession or arrest (Grishman
and Sundheim, 1996; LDC, 2005), an approach not
extensible to novel events, or the same event types
in sub-domains, e.g. sport. On the other hand, topic
detection and tracking (TDT; Allan, 2002) disregards
individual event mentions, clustering together arti-
cles that share a topic.
Between these fine and coarse-grained ap-
proaches, event identification requires grouping ref-
erences to the same event. However, strict corefer-
ence is hampered by the complexity of event seman-
tics: poison, murder and die may indicate the same
effective event. The solution is to tag mentions with
a canonical identifier for each news-triggering event.
This paper introduces event linking: given a past
event reference in context, find the article in a news
archive that first reports that the event happened.
The task has an immediate practical application:
some online newspapers link past event mentions to
relevant news stories, but currently do so with low
coverage and consistency; an event linker can add
referentially-precise hyperlinks to news.
The event linking task parallels entity link-
ing (NEL; Ji and Grishman, 2011), considering a
news archive as a knowledge base (KB) of events,
where each article exclusively represents the zero or
more events that it first reports. Coupled with an ap-
propriate event extractor, event linking may be per-
formed for all events mentioned in a document, like
the named entity disambiguation task (Bunescu and
Pas?ca, 2006; Cucerzan, 2007).
We have annotated and analysed 150 news and
opinion articles, marking references to past, news-
worthy events, and linking where possible to canon-
ical articles in a 13-year news archive.
2 The events in a news story
Approaches to news event processing are subsumed
within broader notions of topics, scenario templates,
or temporal entities, among others. We illustrate key
challenges in processing news events and motivate
event linking through the example story in Figure 1.
Salience Our story highlights carjackings and a
police warning as newsworthy, alongside events like
feeding, drove and told which carry less individual
weight. Orthogonally, parts of the story are new
events, while others are previously reported events
that the reader may be aware of (illustrated in Fig-
ure 1). Online, the two background carjackings and
the police warning are hyperlinked to other SMH arti-
cles where they were reported. Event schemas tend
not to directly address salience: MUC-style IE (Gr-
228
N Sydney man carjacked at knifepoint
There has been another carjacking in Sydney,
B two weeks after two people were stabbed in their cars in
separate incidents.
N A 32-year-old driver was walking to his station wagon on
Hickson Road, Millers Point, after feeding his parking me-
ter about 4.30pm yesterday when a man armed with a
knife grabbed him and told him to hand over his car keys
and mobile phone, police said. The carjacker then drove
the black 2008 Holden Commodore. . . He was described
as a 175-centimetre-tall Caucasian. . .
B Police warned Sydney drivers to keep their car doors
locked after two stabbings this month. On September 4,
a 40-year-old man was stabbed when three men tried to
steal his car on Rawson Street, Auburn, about 1.20am.
The next day, a 25-year-old woman was stabbed in her
lower back as she got into her car on Liverpool Road. . .
Figure 1: Possible event mentions marked in an ar-
ticle from SMH, segmented into news (N) and back-
ground (B) event portions.
ishman and Sundheim, 1996) selects an event type
of which all instances are salient; TDT (Allan, 2002)
operates at the document level, which avoids differ-
entiating event mentions; and TimeML (Pustejovsky
et al, 2003) marks the main event in each sentence.
Critiquing ACE05 event detection for not addressing
salience, Ji et al (2009) harness cross-document fre-
quencies for event ranking. Similarly, reference to a
previously-reported event implies it is newsworthy.
Diversity IE traditionally targets a selected event
type (Grishman and Sundheim, 1996). ACE05 con-
siders a broader event typology, dividing eight
thematic event types (business, justice, etc.) into
33 subtypes such as attack, die and declare
bankruptcy (LDC, 2005). Most subtypes suffer from
few annotated instances, while others are impracti-
cally broad: sexual abuse, gunfire and the Holocaust
each constitute attack instances (is told considered
an attack in Figure 1?). Inter-annotator agreement
is low for most types.1 While ACE05 would mark
the various attack events in our story, police warned
would be unrecognised. Despite template adapta-
tion (Yangarber et al, 2000; Filatova et al, 2006;
Li et al, 2010; Chambers and Jurafsky, 2011), event
types are brittle to particular tasks and domains, such
as bio-text mining (e.g. Kim et al, 2009); they can-
not reasonably handle novel events.
1For binary sentence classification, we calculate an inter-
quartile range of ? ? [0.46, 0.64] over the 33 sub-types. Coarse
event type classification ranges from ? = 0.47 for business to
? = 0.69 for conflict.
Identity Event coreference is complicated by par-
titive (sub-event) and logical (e.g. causation) re-
lationships between events, in addition to lexical-
semantic and syntactic issues. When consider-
ing the relationship between another carjacking and
grabbed, drove or stabbed, ACE05 would apply the
policy: ?When in doubt, do not mark any corefer-
ence? (LDC, 2005). Bejan and Harabagiu (2008)
consider event coreference across documents, mark-
ing the ?most important events? (Bejan, 2010), al-
beit within Google News clusters, where multiple
articles reporting the same event are likely to use
similar language. Similar challenges apply to iden-
tifying event causality and other relations: Bejan
and Harabagiu (2008) suggest arcs such as feeding
precedes
?????
walking enables???? grabbed ? akin to instantia-
tions of FrameNet?s frame relations (Fillmore et al,
2003). However, these too are semantically subtle.
Explicit reference By considering events through
topical document clusters, TDT avoids some chal-
lenges of precise identity. It prescribes rules of in-
terpretation for which stories pertain to a seminal
event. However, the carjackings in our story are
neither preconditions nor consequences of a semi-
nal event and so would not constitute a TDT clus-
ter. TDT fails to account for these explicit event ref-
erences. Though Feng and Allan (2009) and Yang
et al (2009) consider event dependency as directed
arcs between documents or paragraphs, they gener-
ally retain a broad sense of topic with little attention
to explicit reference.
3 The event linking task
Given an explicit reference to a past event, event
linking grounds it in a given news archive. This ap-
plies to all events worthy of having been reported,
and harnesses explicit reference rather than more
general notions of relevance. Though analogous to
NEL, our task differs in the types of expressions that
may be linked, and the manner of determining the
correct KB node to link to, if any.
3.1 Event-referring expressions
We consider a subset of newsworthy events ? things
that happen and directly trigger news ? as candidate
referents. In TimeML?s event classification (Puste-
jovsky et al, 2003), newsworthy events would gen-
229
erally be occurrence (e.g. die, build, sell) or aspec-
tual (e.g. begin, discontinue), as opposed to percep-
tion (e.g. hear), intentional state (e.g. believe), etc.
Still, we are not confined to these types when other
classes of event are newsworthy. All references must
be explicit, reporting the event as factual and com-
pleted or ongoing.
Not all event references meeting these criteria are
reasonably LINKABLE to a single article:
MULTIPLE many distinct events, or an event type,
e.g. world wars, demand;
AGGREGATE emerges from other events over time,
e.g. grew 15%, scored 100 goals;
COMPLEX an event reported over multiple articles
in terms of its sub-events, e.g. 2012 election,
World Cup, scandal.
3.2 A news archive as a KB
We define a canonical link target for each event: the
earliest article in the archive that reports the given
event happened or is happening. Each archival arti-
cle implicitly represents zero or more related events,
just as Wikipedia entries represent zero or one entity
in NEL. Links target the story as a whole: closely
related, co-reported events link to the same article,
avoiding a problematically strict approach to event
identity. An archive reports only selected events, so
a valid target may not exist (NEL?s NIL).
4 An annotated corpus
We link to a digital archive of the Sydney Morn-
ing Herald: Australian and international news from
1986 to 2009, published daily, Monday to Saturday.2
We annotate a randomly sampled corpus of 150 arti-
cles from its 2009 News and Features and Business
sections including news reports, op-eds and letters.
For this whole-document annotation, a single
word of each past/ongoing, newsworthy event men-
tion is marked.3 If LINKABLE, the annotator
searches the archive by keyword and date, selecting
a target, reported here (a self-referential link) or NIL.
An annotation of our example story (Figure 1) would
produce five groups of event references (Table 1).
2The archive may be searched at http://newsstore.
smh.com.au/apps/newsSearch.ac
3We couple marking and linking since annotators must learn
to judge newsworthiness relative to the target archive.
Mentions Annotation category / link
carjacking; LINKABLE, reported here
grabbed [him]
[were] stabbed; MULTIPLE
incidents; stabbings
[Police] warned LINKABLE, linked: Sydney drivers
told: lock your doors
[man] stabbed LINKABLE, linked: Driver stabbed
after Sydney carjacking
[woman] stabbed LINKABLE, linked: Car attack:
Driver stabbed in the back
Table 1: Event linking annotations for Figure 1
Agreement unit AB AC JA JB JC
Token has a link 27 21 61 42 34
Link target on agreed token 48 73 84 83 74
Set of link targets per document 31 40 69 51 45
Link date on agreed token 61 80 87 93 89
Set of link dates per document 36 44 71 54 56
Table 2: Inter-annotator and adjudicator F1 scores
All documents were annotated by external anno-
tator A; external annotators B and C annotated 72
and 24 respectively; and all were adjudicated by the
first author (J). Pairwise inter-annotator agreement
in Table 2 shows that annotators infrequently select
the same words to link, but that reasonable agree-
ment on the link target can be achieved for agreed
tokens.4 Adjudicator-annotator agreements are gen-
erally much higher than inter-annotator agreements:
in many cases, an annotator fails to find a target
or selects one that does not first report the event;
J accepts most annotations as valid. In other cases,
there may be multiple articles published on the same
day that describe the event in question from differ-
ent angles; agreement increases substantially when
relaxed to accept date agreement. Our adjudicated
corpus of 150 documents is summarised in Table 3.
Where a definitive link target is not available, an
annotator may erroneously select another candidate:
an opinion article describing the event, an article
where the event is mentioned as background, or an
article anticipating the event.
The task is complicated by changed perspective
between an event?s first report and its later reference.
4? ? F1 for the binary token task (F1 accounts for the ma-
jority class) and for the sparse link targets/date selection.
230
Category Mentions Types Docs
Any markable 2136 655 149
LINKABLE 1399 417 144
linked 501 229 99
reported here 667 111 111
nil 231 77 77
COMPLEX 220 79 79
MULTIPLE 328 102 102
AGGREGATE 189 57 57
Table 3: Annotation frequencies: no. of mentions,
distinct per document, and document frequency
Can overpayed link to what had been acquired? Can
10 died be linked to an article where only nine are
confirmed dead? For the application of adding hy-
perlinks to news, such a link might be beneficial, but
it may be better considered an AGGREGATE.
The schema underspecifies definitions of ?event?
and ?newsworthiness?, accounting for much of the
token-level disagreement, but not directly affecting
the task of linking a specified mention to the archive.
Adjectival mentions such as Apple?s new CEO are
easy to miss and questionably explicit. Events are
also confused with facts and abstract entities, such
as bans, plans, reports and laws. Unlike many other
facts, events can be grounded to a particular time of
occurrence, often stated in text.
5 Analysis and discussion
To assess task feasibility, we present bag-of-words
(BoW) and oracle results (Figure 2). Using the whole
document as a query5 retrieves 30% of gold targets
at rank 10, but only 60% by rank 150. Term win-
dows around each event mention perform close to
our oracle consisting of successful search keywords
collected during annotation, with over 80% recall at
150. No system recalls over 30% of targets at 1-best,
suggesting a reranking approach may be required.
Constraining search result dates is essential; an-
notators? constraints improve recall by 20% at rank
50. These constraints may draw on temporal expres-
sions in the source article or external knowledge.
Successful automated linking will therefore require
extensive use of semantic and temporal information.
Our corpus also highlights distinctions between
5Using Apache Solr defaults: TFIDF-weighted cosine simi-
larity over stemmed and stopped tokens.
0 25 50 75 100 125 150 175 200Rank (number of documents returned)0
1020
3040
5060
7080
90100
Link ta
rgets f
ound (
%)
Annotator terms + date constraintAnnotator termsMention 31-word windowWhole document
Figure 2: Recall for BoW and oracle systems
explicit event reference and broader relationships.
Yang et al (2009) makes the reasonable assumption
that news events generally build on others that re-
cently precede them. We find that the likelihood
a linked article occurred fewer than d days ago re-
duces exponentially with respect to d, yet the rate
of decay is surprisingly slow: half of all link targets
precede their source by over 3 months.
The effect of coreporting rather than coreference
is also clear: like {carjacking, grabbed} in our ex-
ample, mention chains include {return, decide, re-
contest}, {winner, Cup} as well as more familiar in-
stances like {acquired, acquisition}.
6 Conclusion
We have introduced event linking, which takes a
novel approach to news event reference, associating
each newsworthy past event with a canonical arti-
cle in a news archive. We demonstrate task?s fea-
sibility, with reasonable inter-annotator agreement
over a 150 document corpus. The corpus highlights
features of the retrieval task and its dependence on
temporal knowledge. As well as using event link-
ing to add referentially precise hyperlinks to a news
archive, further characteristics of news will emerge
by analysing the graph of event references.
7 Acknowledgements
We are grateful to the reviewers for their comments.
The work was supported by Capital Markets CRC
post-doctoral fellowships (BH; MH) and PhD Schol-
arship (JN); a University of Sydney VCRS (JN); and
ARC Discovery Grant DP1097291 (JRC).
231
References
James Allan, editor. 2002. Topic Detection and Track-
ing: Event-based Information Organization. Kluwer
Academic Publishers, Boston, MA.
Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A
linguistic resource for discovering event structures and
resolving event coreference. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation, Marrakech, Morocco.
Cosmin Adrian Bejan. 2010. Private correspondence,
November.
Razvan Bunescu and Marius Pas?ca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 9?16.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 976?986, Portland, Ore-
gon, USA, June.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 708?716.
Ao Feng and James Allan. 2009. Incident threading
for news passages. In CIKM ?09: Proceedings of
the 18th ACM international conference on Information
and knowledge management, pages 1307?1316, Hong
Kong, November.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of do-
main templates. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 207?
214, Sydney, Australia, July.
Charles J. Fillmore, Christopher R. Johnson, and Miriam
R. L. Petruck. 2003. Background to FrameNet. Inter-
national Journal of Lexicography, 16(3):235?250.
Ralph Grishman and Beth Sundheim. 1996. Message un-
derstanding conference ? 6: A brief history. In COL-
ING 1996 Volume 1: The 16th International Confer-
ence on Computational Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1148?1158, Portland, Ore-
gon, June.
Heng Ji, Ralph Grishman, Zheng Chen, and Prashant
Gupta. 2009. Cross-document event extraction and
tracking: Task, evaluation, techniques and challenges.
In Proceedings of Recent Advances in Natural Lan-
guage Processing, September.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado, June.
LDC. 2005. ACE (Automatic Content Extraction) En-
glish annotation guidelines for events. Linguistic Data
Consortium, July. Version 5.4.3.
Hao Li, Xiang Li, Heng Ji, and Yuval Marton. 2010.
Domain-independent novel event discovery and semi-
automatic event annotation. In Proceedings of the
24th Pacific Asia Conference on Language, Informa-
tion and Computation, Sendai, Japan, November.
James Pustejovsky, Jos? Casta no, Robert Ingria, Roser
Saur?, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. TimeML: Robust specification of
event and temporal expressions in text. In Proceedings
of the Fifth International Workshop on Computational
Semantics.
Christopher C. Yang, Xiaodong Shi, and Chih-Ping Wei.
2009. Discovering event evolution graphs from news
corpora. IEEE Transactions on Systems, Man and Cy-
bernetics, Part A: Systems and Humans, 34(4):850?
863, July.
Roman Yangarber, Ralph Grishman, and Pasi
Tapanainen. 2000. Automatic acquisition of do-
main knowledge for information extraction. In In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 940?946.
232
Joint Incremental Disfluency Detection and Dependency Parsing
Matthew Honnibal
Department of Computing
Macquarie University
Sydney, Australia
matthew.honnibal@mq.edu.edu.au
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mark.johnson@mq.edu.edu.au
Abstract
We present an incremental dependency
parsing model that jointly performs disflu-
ency detection. The model handles speech
repairs using a novel non-monotonic tran-
sition system, and includes several novel
classes of features. For comparison,
we evaluated two pipeline systems, us-
ing state-of-the-art disfluency detectors.
The joint model performed better on both
tasks, with a parse accuracy of 90.5% and
84.0% accuracy at disfluency detection.
The model runs in expected linear time,
and processes over 550 tokens a second.
1 Introduction
Most unscripted speech contains filled pauses
(ums and uhs), and errors that are usually edited
on-the-fly by the speaker. Disfluency detection is
the task of detecting these infelicities in spoken
language transcripts. The task has some imme-
diate value, as disfluencies have been shown to
make speech recognition output much more dif-
ficult to read (Jones et al., 2003), but has also
been motivated as a module in a natural language
understanding pipeline, because disfluencies have
proven problematic for PCFG parsing models.
Instead of a pipeline approach, we build on re-
cent work in transition-based dependency parsing,
to perform the two tasks jointly. There have been
two small studies of dependency parsing on un-
scripted speech, both using entirely greedy pars-
ing strategies, without a direct comparison against
a pipeline architecture (Jorgensen, 2007; Rasooli
and Tetreault, 2013). We go substantially beyond
these pilot studies, and present a system that com-
pares favourably to a pipeline consisting of state-
of-the-art components. Our parser largely follows
the design of Zhang and Clark (2011). We use a
structured averaged perceptron model with beam-
search decoding (Collins, 2002). Our feature set
is based on Zhang and Clark (2011), and our
transition-system is based on the arc-eager system
of Nivre (2003).
We extend the transition system with a novel
non-monotonic transition, Edit. It allows sen-
tences like ?Pass the pepper uh salt? to be parsed
incrementally, without the need to guess early
that pepper is disfluent. This is achieved by re-
processing the leftward children of the word Edit
marks as disfluent. For instance, if the parser at-
taches the to pepper, but subsequently marks pep-
per as disfluent, the will be returned to the stack.
We also exploit the ease with which the model can
incorporate arbitrary features, and design a set of
features that capture the ?rough copy? structure of
some speech repairs, which motivated the Johnson
and Charniak (2004) noisy channel model.
Our main comparison is against two pipeline
systems, which use the two current state-of-the-
art disfluency detection systems as pre-processors
to our parser, minus the custom disfluency fea-
tures and transition. The joint model compared
favourably to the pipeline parsers at both tasks,
with an unlabelled attachment score of 90.5%, and
84.0% accuracy at detecting speech repairs. An ef-
ficient implementation is available under an open-
source license.1 The future prospects of the sys-
tem are also quite promising. Because the parser
is incremental, it should be well suited to un-
segmented text such as the output of a speech-
recognition system. We consider our main con-
tributions to be:
? a novel non-monotonic transition system, for
speech repairs and restarts,
1http://github.com/syllog1sm/redshift
131
Transactions of the Association for Computational Linguistics, 2 (2014) 131?142. Action Editor: Joakim Nivre.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
A flight to um????
FP
Boston? ?? ?
RM
I mean? ?? ?
IM
Denver? ?? ?
RP
Tuesday
Figure 1: A sentence with disfluencies annotated in
the style of Shriberg (1994) and the Switchboard cor-
pus. FP=Filled Pause, RM=Reparandum, IM=Interregnum,
RP=Repair. We follow previous work in evaluating the sys-
tem on the accuracy with which it identifies speech-repairs,
marked reparandum above.
? several novel feature classes,
? direct comparison against the two best disflu-
ency pre-processors, and
? state-of-the-art accuracy for both speech
parsing and disfluency detection.
2 Switchboard Disfluency Annotations
The Switchboard portion of the Penn Treebank
(Marcus et al., 1993) consists of telephone conver-
sations between strangers about an assigned topic.
Two annotation layers are provided: one for syn-
tactic bracketing (MRG files), and one for disflu-
encies (DPS files). The disfluency layer marks el-
ements with little or no syntactic function, such as
filled pauses and discourse markers, and annotates
speech repairs using the Shriberg (1994) system
of reparandum/interregnum/repair. An example is
shown in Figure 1.
In the syntactic annotation, edited words are
covered by a special node labelled EDITED. The
idea is to mark text which, if excised, would re-
sult in a grammatical sentence. The MRG files do
not mark other types of disfluencies. We follow
the evaluation defined by Charniak and Johnson
(2001), which evaluates the accuracy of identify-
ing speech repairs and restarts. This definition of
the task is the standard in recent work. The reason
for this is that filled pauses can be detected using
a simple rule-based approach, and parentheticals
have less impact on readability and down-stream
processing accuracy.
The MRG and DPS layers have high but im-
perfect agreement over what tokens they mark as
speech repairs: of the text annotated with both lay-
ers, 33,720 tokens are marked as disfluent in at
least one layer, 32,310 are only marked as disflu-
ent by the DPS files, and 32,742 are only marked
as disfluent by the MRG layer.
The Switchboard annotation project was not
fully completed. Because disfluency annotation is
cheaper to produce, many of the DPS training files
do not have matching MRG files. Only 619,236
of the 1,482,845 tokens in the DPS disfluency-
detection training data have gold-standard syntac-
tic parses. Our system requires the more expen-
sive syntactic annotation, but we find that it out-
performs the previous state-of-the-art (Qian and
Liu, 2013), despite training on less than half the
data.
2.1 Dependency Conversion
As is standard in statistical dependency parsing
of English, we acquire our gold-standard depen-
dencies from phrase-structure trees. We used the
2013-04-05 version of the Stanford dependency
converter (de Marneffe et al., 2006). As is standard
for English dependency parsing, we use the Ba-
sic Dependencies scheme, which produces strictly
projective representations.
At first we feared that the filled pauses, disfluen-
cies and meta-data tokens in the Switchboard cor-
pus might disrupt the conversion process, by mak-
ing it more difficult for the converter to recognise
the underlying production rules.
To test this, we performed a small experiment.
We prepared two versions of the corpus: one
where EDITED nodes, filled pauses and meta-data
were removed before the trees were transformed
by the Stanford converter, and one where the dis-
fluency removal was performed after the depen-
dency conversion. The resulting corpora were
largely identical: 99.54% of unlabelled and 98.7%
of labelled dependencies were the same. The fact
that the Stanford converter is quite robust to dis-
fluencies was useful for our baseline joint model,
which is trained on dependency trees that also in-
cluded governors for disfluent words.
We follow previous work on disfluency detec-
tion by lower-casing the text and removing punc-
tuation and partial words (words tagged XX and
words ending in ?-?). We also remove one-token
sentences, as their syntactic analyses are trivial.
We found that two additional simple pre-processes
improved our results: discarding all ?um? and ?uh?
tokens; and merging ?you know? and ?i mean? into
single tokens.
These pre-processes can be completed on the in-
put string without losing information: none of the
?um? or ?uh? tokens are semantically significant,
and the bigrams you know and i mean have a de-
pendency between the two tokens over 99.9% of
the times they occur in the treebank, with you and
I never having any children. This makes it easy
to unmerge the tokens deterministically after pars-
132
ing: all incoming and outgoing arcs will point to
know or mean. The same pre-processing was per-
formed for all our parsing systems.
3 Transition-based Dependency Parsing
A transition-based parser predicts the syntactic
structure of a sentence incrementally, by making
a sequence of classification decisions. We follow
the architecture of Zhang and Clark (2011), who
use beam-search for decoding, and a structured av-
eraged perceptron for training. Despite its simplic-
ity, this type of parser has produced highly com-
petitive results on the Wall Street Journal: with the
extended feature set described by Zhang and Nivre
(2011), it achieves 93.5% unlabelled accuracy on
Stanford basic dependencies (de Marneffe et al.,
2006). Converting the constituency trees produced
by the Charniak and Johnson (2005) reranking
parser results in similar accuracy.
Briefly, the transition-based parser consists of a
configuration (or ?state?) which is sequentially ma-
nipulated by a set of possible transitions. For us, a
state is a 4-tuple c = (?, ?,A,D), where ? and ?
are disjoint sets of word indices termed the stack
and buffer respectively, A is the set of dependency
arcs, and D is the set of word indices marked dis-
fluent. There are no arcs to or from members ofD,
so the dependencies and disfluencies can be imple-
mented as a single vector (in our parser, a token is
marked as disfluent by setting it as its own head).
We use the arc-eager transition system (Nivre,
2003, 2008), which consists of four parsing ac-
tions: Shift, Left-Arc, Right-Arc and Reduce. We
denote the stack with its topmost element to the
right, and the buffer with its first element to the
left. A vertical bar is used to indicate concate-
nation to the stack or buffer, e.g. ?|i indicates a
stack with the topmost element i and remaining
elements ?. A dependency from a governor i to
a child j is denoted i ? j. The four arc-eager
transitions are shown in Figure 2.
The Shift action moves the first item of the
buffer onto the stack. The Right-Arc does the
same, but also adds an arc, so that the top two
items on the stack are connected. The Reduce
move and the Left-Arc both pop the stack, but the
Left-Arc first adds an arc from the first word of
the buffer to the word on top of the stack. Con-
straints on the Reduce and Left-Arc moves ensure
that every word is assigned exactly one head in
the final configuration. We follow the suggestion
(?, i|?,A,D) ` (?|i, ?, A,D) S
(?|i, j|?,A,D) ` (?, j|?,A ? {j ? i}, D) L
Only if i does not have an incoming arc.
(?|i, j|?,A,D) ` (?|i|j, ?,A ? {i? j}, D) R
(?|i, ?, A,D) ` (?, ?,A,D) D
Only if i has an incoming arc.
(?|i, j|?,A,D) ` (?|[x1, xn], j|?,A?, D?) E
Where
A? = A \ {x? y or y ? x : ?x ? [i, j), ?y ? N}
D? = D ? [i, j)
x1...xn are the former left children of i
Figure 2: Our parser?s transition system. The first four
transitions are the standard arc-eager system; the fifth is our
novel Edit transition.
of Ballesteros and Nivre (2013) and add a dummy
token that governs root dependencies to the end of
the sentence. Parsing terminates when this token
is at the start of the buffer, and the stack is empty.
Disfluencies are added toD via the Edit transition,
E, which we now define.
4 A Non-Monotonic Edit Transition
One of the reasons disfluent sentences are hard to
parse is that there often appear to be syntactic re-
lationships between words in the reparandum and
the fluent sentence. When these relations are con-
sidered in addition to the dependencies between
fluent words, the resulting structure is not neces-
sarily a projective tree.
Figure 3 shows a simple example, where the re-
pair square replaces the reparandum rectangle. An
incremental parser could easily become ?garden-
pathed? and attach the repair square to the preced-
ing words, constructing the dependencies shown
dotted in Figure 3. Rather than attempting to de-
vise an incremental model that avoids construct-
ing such dependencies, we allow the parser to con-
struct these dependencies and later delete them if
the governor or child are marked disfluent.
Psycholinguistic models of human sentence
processing have long posited repair mechanisms
(Frazier and Rayner, 1982). Recently, Honnibal
et al. (2013) showed that a limited amount of ?non-
monotonic? behaviour can improve an incremen-
tal parser?s accuracy. We here introduce a non-
monotonic transition, Edit, for speech repairs.
The Edit transition marks the word i on top
of the stack ?|i as disfluent, along with its right-
ward descendents ? i.e., all words in the sequence
i...j ? 1, where j is the word at the start of the
buffer. It then restores the words both preceding
and formerly governed by i to the stack.
In other words, the word on top of the stack and
133
Pass me the red rectangle uh I mean square
Figure 3: Example where apparent dependencies between
the reparandum and the fluent sentence complicate parsing.
The dotted edges are difficult for an incremental parser to
avoid, but cannot be part of the final parse if it is to be a
projective tree. Our solution is to make the transition system
non-monotonic: the parser is able to delete edges.
its rightward descendents are all marked as dis-
fluent, and the stack is popped. We then restore
its leftward children to the stack, and all depen-
dencies to and from words marked disfluent are
deleted. The transition is non-monotonic in the
sense that it can delete dependencies created by
a previous transition, and replace tokens onto the
stack that had been popped.
Why revisit the leftward children, but not the
right? We are concerned about dependencies
which might be mirrored between the reparandum
and the repair. The rightward subtree of the disflu-
ency might well be incorrect, but if it is, it would
still be incorrect if the word on top of the stack
were actually fluent. We therefore regard these
as parsing errors that we will train our model to
avoid. In contrast, avoiding the Left-Arc transi-
tions would require the parser to predict that the
head is disfluent when it has not necessarily seen
any evidence indicating that.
4.1 Worked Example
Figure 4 shows a gold-standard derivation for
a disfluent sentence from the development data.
Line 1 shows the state resulting from the initial
Shift action. In the next three states, His is Left-
Arced to company, which is then Shifted onto the
stack, and Left-Arced to went in Line 4.
The dependency between went and company is
not part of the gold-standard, because went is dis-
fluent. The correct governor of company is the sec-
ond went in the sentence. The Left-Arc move in
Line 4 can still be considered correct, however, be-
cause the gold-standard analysis is still derivable
from the resulting configuration, via the Edit tran-
sition. Another non-gold dependency is created in
Line 6, between broke and went, before broke is
Reduced from the stack in Line 7.
Lines 9 and 10 show the states before and after
the Edit transition. The word on top of the stack in
Line 9, went, has one leftward child, and one right-
1. S His company went broke i mean went bankrupt
2. L His company went broke i mean went bankrupt
3. S His company went broke i mean went bankrupt
4. L His company went broke i mean went bankrupt
5. S His company went broke i mean went bankrupt
6. R His company went broke i mean went bankrupt
7. D His company went broke i mean went bankrupt
8. S His company went broke i mean went bankrupt
9. L His company went broke i mean went bankrupt
10. E His company went broke i mean went bankrupt
11. L His company went broke i mean went bankrupt
12. S His company went broke i mean went bankrupt
12. R His company went broke i mean went bankrupt
13. D His company went broke i mean went bankrupt
Figure 4: A gold-standard transition sequence using our
EDIT transition. Each line specifies an action and shows the
state resulting from it. Words on the stack are circled, and
the arrow indicates the start of the buffer. Disfluent words are
struck-through.
ward child. After the Edit transition is applied,
went and its rightward child broke are both marked
disfluent, and company is returned to the stack. All
of the previous dependencies to and from went and
broke are deleted.
Parsing then proceeds as normal, with the cor-
rect governor of company being assigned by the
Left-Arc in Line 11, and bankrupt being Right-
Arced to went in Line 12. To conserve space, we
have omitted the dummy ROOT token, which is
placed at the end of the sentence, following the
suggestion of Ballesteros and Nivre (2013). The
final action will be a Left-Arc from the ROOT to-
ken to went.
4.2 Dynamic Oracle Training Algorithm
Our non-monotonic transition system introduces
substantial spurious ambiguity: the gold-standard
parse can be derived via many different transition
134
sequences. Recent work has shown that this can
be advantageous (Sartorio et al., 2013; Honnibal
et al., 2013; Goldberg and Nivre, 2012), because
difficult decisions can sometimes be delayed until
more information is available.
Line 5 of Figure 4 shows a state that introduces
spurious ambiguity. From this configuration, there
are multiple actions that could be considered ?cor-
rect?, in the sense that the gold-standard analysis
can be derived from them. The Edit transition is
correct because went is disfluent, but the Left-Arc
and even the Right-Arc are also correct, in that
there are continuations from them that lead to the
gold-standard analysis.
We regard all transition sequences that can re-
sult in the correct analysis as equally valid, and
want to avoid stipulating one of them during train-
ing. We achieve this by following Goldberg and
Nivre (2012) in using a dynamic oracle to create
partially labelled training data.2 A dynamic oracle
is a function that determines the cost of applying
an action to a state, in terms of gold-standard arcs
that are newly unreachable.
We follow Collins (2002) in training an aver-
aged perceptron model to predict transition se-
quences, rather than individual transitions. This
type of model is often referred to as a struc-
tured perceptron, or sometimes a global percep-
tron. During training, if the model does not pre-
dict the correct sequence, an update is performed,
based on the gold-standard sequence and part of
the sequence predicted by the current weights.
Only part of the sequence is used to calculate the
weight update, in order to account for search er-
rors. We use the maximum violation strategy de-
scribed by Huang et al. (2012) to select the subse-
quence to update from.
To train our model using the dynamic oracle,
we use the latent-variable structured perceptron al-
gorithm described by Sun et al. (2009). Beam-
search is performed to find the highest-scoring
gold-standard sequence, as well as the highest-
scoring prediction. We use the same beam-width
for both search procedures.
4.3 Path Length Normalisation
One problem introduced by the Edit transition is
that the number of actions applied to a sentence is
2 The training data is partially labelled in the sense that in-
stances can have multiple true labels. Equivalently, one might
say that the transitions are latent variables, which generate the
dependencies.
no longer constant ? it is no longer guaranteed to
be 2n ? 1, for a sentence of length n. When the
Edit transition is applied to a word with leftward
children, those children are returned to the stack,
and processed again. This has little to no impact
on the algorithm?s empirical efficiency, although
worst-case complexity is no longer linear, but it
does pose a problem for decoding.
The perceptron model tends to assign large pos-
itive scores to its top prediction. We thus ob-
served a problem when comparing paths of differ-
ent lengths, at the end of the sentence. Paths that
included Edit transitions were longer, so the sum
of their scores tended to be higher.
The same problem has been observed during
incremental PCFG parsing, by Zhu et al. (2013).
They introduce an additional transition, IDLE, to
ensure that paths are the same length. So long as
one candidate in the beam is still being processed,
all other candidates apply the IDLE transition.
We adopt a simpler solution. We normalise the
figure-of-merit for a candidate state, which is used
to rank it in the beam, by the length of its transition
history. The new figure-of-merit is the arithmetic
mean of the candidate?s transition scores, where
previously the figure-of-merit was the sum of the
candidate?s transition scores.
Interestingly, Zhu et al. (2013) report that they
tried exactly this, and that it was less effective than
their solution. We found that the features associ-
ated with the IDLE transition were uninformative
(the state is at termination, so the stack and buffer
are empty), and had nothing to do with how many
edit transitions were earlier applied.
5 Features for the Joint Parser
Our baseline parser uses the feature set described
by Zhang and Nivre (2011). The feature set con-
tains 73 templates that mostly refer to the prop-
erties of 12 context tokens: the top of the stack
(S0), its two leftmost and rightmost children (S0L,
S0L2, S0R, S0R2), its parent and grand-parent
(S0h, S0h2), the first word of the buffer and its two
leftmost children (N0, N0L, N0LL), and the next
two words of the buffer (N1, N2).
Atomic features consist of the word, part-of-
speech tag, or dependency label for these tokens;
and multiple feature atoms are often combined for
feature templates. There are also features for the
string-distance between S0 and N0, and the left
and right valencies (total number of children) of
135
S0 and N0, as well as the set of their children?s de-
pendency labels. We restrict these to the first and
last 2 children for implementation efficiency, as
we found this had no effect on accuracy. Numeric
features (for distance and valency) are binned with
the function ?x : min(x, 5). There is only one bi-
lexical feature template, which pairs the words of
S0 and N0. There are also ten tri-tag templates.
Our feature set includes additional dependency
label features not used by Zhang and Nivre (2011),
as we found that disfluency detection errors often
resulted in ungrammatical dependency label com-
binations. The additional templates combine the
POS tag of S0 with two or three dependency la-
bels from its left and right subtrees. Details can be
found in the supplementary material.
5.1 Brown Cluster Features
The Brown clustering algorithm (Brown et al.,
1992) is a well known source of semi-supervised
features. The clustering algorithm is run over
a large sample of unlabelled data, to generate a
type-to-cluster map. This mapping is then used to
generate features that sometimes generalise better
than lexical features, and are helpful for out-of-
vocabulary words (Turian et al., 2010).
Koo and Collins (2010) found that Brown clus-
ter features greatly improved the performance of a
graph-based dependency parser. On our transition-
based parser, Brown cluster features bring a small
but statistically significant improvement on the
WSJ task (0.1-0.3% UAS). Other developers of
transition-based parsers seem to have found sim-
ilar results (personal communication). Since a
Brown cluster mapping computed by Liang (2005)
is easily available,3 the features are simple to im-
plement and cheap to compute.
Our templates follow Koo and Collins (2010)
in including features that refer to cluster prefix
strings, as well as the full clusters. We adapt their
templates to transition-based parsing by replacing
?head? with ?item on top of the stack? and ?child?
with ?first word of the buffer?. The exact templates
can be found in the supplementary material.
The Brown cluster features are used in our
?baseline? parser, and in the parsers we use as part
of our pipeline systems. They improved develop-
ment set accuracy by 0.4%. We experimented with
the other feature sets in these parsers, but found
that they did not improve accuracy on fluent text.
3http://www.metaoptimize.com/projects/wordreps
5.2 Rough Copy Features
Johnson and Charniak (2004) point out that in
speech repairs, the repair is often a ?rough copy?
of the reparandum. The simplest case of this is
where the repair is a single word repetition. It is
common for the repair to differ from the reparan-
dum by insertion, deletion or substitution of one
or more words.
To capture this regularity, we first extend the
feature-set with three new context tokens:4
1. S0re: The rightmost edge of S0 descendants;
2. S0le: The leftmost edge of S0 descendants;
3. N0le: The leftmost edge of N0 descendants.
If a word has no leftward children, it will be
its own left-edge, and similarly it will be its own
rightward edge if it has no rightward children.
Note that the token S0re is necessarily immedi-
ately before N0le, unless some of the tokens be-
tween them are disfluent. We use the S0le and N0le
to compute the following rough-copy features:
1. How long is the prefix word match between
S0le...S0 and N0le...N0?
If the parser were analysing the red the blue
square, with red on the stack and square at
N0, its value would be 1.
2. How long is the prefix POS tag match be-
tween S0le...S0 and N0le...N0?
3. Do the words in S0le...S0 and N0le...N0
match exactly?
4. Do the POS tags in S0le...S0 and N0le...N0
match exactly?
If the parser were analysing the red square
the blue rectangle, with square on the stack
and rectangle at N0, its value would be true.
The prefix-length features are binned using the
function ?x : min(x, 5).
5.3 Match Features
This class of features ask which pairs of the con-
text tokens match, in word or POS tag. The con-
text tokens in the Zhang and Nivre (2011) fea-
ture set are the top of the stack (S0), its head and
4As is common in this type of parser, our implementation
has a number of vectors for properties that are defined before
parsing, such as word forms, POS tags, Brown clusters, etc. A
context token is an index into these vectors, allowing features
considering these properties to be computed.
136
grandparent (S0h, S0h2), its two left- and right-
most children (S0L, S0L2, S0R, S0R2), the first
three words of the buffer (N0, N1, N2), and the
two leftmost children of N0 (N0L, N0LL). We ex-
tend this set with the S0le, S0re and N0le tokens
described above, and also the first left and right
child of S0 and N0 (S0L0, S0R0, N0L0).
All up, there are 18 context tokens, so(18
2
)
= 153 token pairs. For each pair of these
tokens, we add two binary features, indicating
whether the two tokens match in word form or POS
tag. We also have two further classes of features:
if the words do match, a feature is added indicat-
ing the word form; if the tags match, a feature is
added indicating the tag. These finer grained ver-
sions help the model adjust for the fact that some
words can be duplicated in grammatical sentences
(e.g. ?that that?), while most rare words cannot.
5.4 Edited Neighbour Features
Disfluencies are usually string contiguous, even if
they do not form a single constituent. In these situ-
ations, our model has to make multiple transitions
to mark a single disfluency. For instance, if an ut-
terance begins and the and a, the stack will contain
two entries, for and and the, and two Edit transi-
tions will be required.
To mitigate this disadvantage of our model, we
add four binary features. Two fire when the word
or pair of words immediately preceding N0 have
been marked disfluent; the other two fire when the
word or pair of words immediately following S0
have been marked disfluent. These features pro-
vide an additional string-based view that the parser
would otherwise be missing. Speakers tend be
disfluent in bursts: if the previous word is dis-
fluent, the next word is more likely to be disflu-
ent. These four features are therefore all associ-
ated with positive weights for the Edit transition.
Without these features, we would miss an aspect of
disfluency processing that sequence models natu-
rally capture.
6 Part-of-Speech Tagging
We adopt the standard strategy of using a POS
tagger as a pre-process before parsing. Most
transition-based parsers use a structured averaged
perceptron model with beam-search for tagging,
as this model achieves competitive accuracy and
matches the standard dependency parsing archi-
tecture. Our tagger also uses this architecture.
We performed some additional feature engi-
neering for the tagger, in order to improve its accu-
racy given the lack of case distinctions and punc-
tuation in the data. Our additional features use two
sources of unsupervised information. First, we
follow the suggestion of Manning (2011) by us-
ing Brown cluster features to improve the tagger?s
accuracy on unknown words. Second, we com-
pensate for the lack of case distinctions by includ-
ing features that ask what percentage of the time
a word form was seen title-cased, upper-cased and
lower-cased in the Google Web1T corpus.
Where most previous work uses cross-fold
training for the tagger, to ensure that the parser
is trained on tags that reflect run-time accuracies,
we do online training of the tagger alongside the
parser, using the current tagger model to produce
tags during parser training. This had no impact on
parse accuracy, and made it slightly easier to de-
velop our tagger alongside the parser.
The tagger achieved 96.5% accuracy on the de-
velopment data, but when we ran our final test
experiments, we found its accuracy dropped to
96.0%, indicating some over-fitting during our
feature engineering. On the development data, our
parser accuracy improves by about 1% when gold-
standard tags are used.
7 Experiments
We use the Switchboard portion of the Penn Tree-
bank (Marcus et al., 1993), as described in Sec-
tion 2, to train our joint models and evaluate them
on dependency parsing and disfluency detection.
The pre-processing and dependency conversion
are described in Section 2.1. We use the stan-
dard train/dev/test split from Charniak and John-
son (2001): Sections 2 and 3 for training, and Sec-
tion 4 divided into three held-out sections, the first
of which is used for final evaluation.
Our parser evaluation uses the SPARSEVAL
(Roark et al., 2006) metric. However, we wanted
to use the Stanford dependency converter, for the
reasons described in Section 2.1, so we used our
own implementation. Because we do not need to
deal with recognition errors, we do not need to
report our parsing results using P /R/F -measures.
Instead, we report an unlabelled accuracy score,
which refers to the percentage of fluent words
whose governors were assigned correctly. Note
that words marked as disfluent cannot have any in-
coming or out-going dependencies, so if a word is
137
incorrectly marked as disfluent, all of its depen-
dencies will be incorrect.
We follow Johnson and Charniak (2004) and
others in restricting our disfluency evaluation to
speech repairs, which we identify as words that
have a node labelled EDITED as an ancestor. Un-
like most other disfluency detection research, we
train only on the MRG files, giving us 619,236
words of training data instead of the 1,482,845
used by the pipeline systems. It may be possible
to improve our system?s disfluency detection by
leveraging the additional data that does not have
syntactic annotation in some way.
All parsing models were trained for 15 itera-
tions. We found that optimising the number of
iterations on a development set led to small im-
provements that did not transfer to a second devel-
opment set (part of Section 4, which Charniak and
Johnson (2001) reserved for ?future use?).
We test for statistical significance in our results
by training 20 models for each experimental con-
figuration, using different random seeds. The ran-
dom seeds control how the sentences are shuf-
fled during training, which the perceptron model
is quite sensitive to. We use the Wilcoxon rank-
sums non-parametric test. The standard deviation
in UAS for a sample was typically around 0.05%,
and 0.5% for disfluency F -measure.
All of our models use beam-search decoding,
with a beam width of 32. We found that a beam
width of 64 brought a very small accuracy im-
provement (about 0.1%), at the cost of 50% slower
run-time. Wider beams than this brought no ac-
curacy improvement. Accuracy seems to plateau
with slightly narrower beams than on newswire
text. This is probably due to the shorter sentences
in Switchboard.
The baseline and pipeline systems are config-
ured in the same way, except that the baseline
parser is modified slightly to allow it to predict
disfluencies, using a special dependency label,
ERASED. All descendants of a word attached to its
head by this label are marked as disfluent. Both the
baseline and pipeline/oracle parsers use the same
feature set: the Zhang and Nivre (2011) features,
plus our Brown cluster features.
The baseline system is a standard arc-eager
transition-based parser with a structured averaged
perceptron model and beam-search decoding. The
model is trained in the standard way, with a ?static?
oracle and maximum-violation update, following
(Huang et al., 2012).
7.1 Comparison with Pipeline Approaches
The accuracy of incremental dependency parsers
is well established on the Wall Street Journal, but
there are no dependency parsing results in the lit-
erature that make it easy to put our joint model?s
parsing accuracy into context. We therefore com-
pare our joint model to two pipeline systems,
which consist of a disfluency detector, followed by
our dependency parser. We also evaluate parse ac-
curacies after oracle pre-processing, to gauge the
net effect of disfluencies on our parser?s accuracy.
The dependency parser for the pipeline systems
was trained on text with all disfluencies removed,
following Charniak and Johnson (2001). The two
disfluency detection systems we used were the
Qian and Liu (2013) sequence-tagging model, and
a version of the Johnson and Charniak (2004)
noisy channel model, using the Charniak (2001)
syntactic language model and the reranking fea-
tures of Zwarts and Johnson (2011). They are the
two best published disfluency detection systems.
8 Results
Table 1 shows the development set accuracies for
our joint parser. Both the disfluency features and
the Edit transition make statistically significant
improvements, in both disfluency F -measure, un-
labelled attachment score (UAS), and labelled at-
tachment score (LAS).
The Oracle pipeline system, which uses the
gold-standard to clean disfluencies prior to pars-
ing, shows the total impact of speech-errors on the
parser. The baseline parser, which uses the Zhang
and Nivre (2011) feature set plus the Brown clus-
ter features, scores 1.8% UAS lower than the ora-
cle.
When we add the features described in Sec-
tions 5.2, 5.3 and 5.4, the gap is reduced to 1.2%
(+Features). Finally, the improved transition sys-
tem reduces the gap further still, to 0.8% UAS
(+Edit transition). We also tested these features
in the Oracle parser, but found they were ineffec-
tive on fluent text.
The w/s column shows the tokens analysed per
second for each system, including disfluencies,
with a single thread on a 2.4GHz Intel Xeon. The
additional features reduce efficiency, but the non-
monotonic Edit transition does not. The system is
easily efficient enough for real-time use.
138
P R F UAS LAS w/s
Baseline joint 79.4 70.1 74.5 89.9 86.9 711
+Features 86.0 77.2 81.3 90.5 87.5 539
+Edit transition 92.2 80.2 85.8 90.9 87.9 555
Oracle pipeline 100 100 100 91.7 88.6 782
Table 1: Development results for the joint models. For the
baseline model, disfluencies reduce parse accuracy by 1.7%
Unlabelled Attachment Score (UAS). Our features and Edit
transition reduce the gap to 0.7%, and improve disfluency de-
tection by 11.3% F -measure.
Disfl. F UAS
Johnson et al pipeline 82.1 90.3
Qian and Liu pipeline 83.9 90.1
Baseline joint parser 73.9 89.4
Final joint parser 84.1 90.5
Table 2: Test-set parse and disfluency accuracies. The joint
parser is improved by the features and Edit transition, and is
better than pre-processing the text with state-of-the-art disflu-
ency detectors.
Table 2 shows the final evaluation. Our main
comparison is with the two pipeline systems, de-
scribed in Section 7.1. The Johnson and Char-
niak (2004) system was 1.8% less accurate at dis-
fluency detection than the other disfluency detec-
tor we evaluated, the state-of-the-art Qian and Liu
(2013) system. However, when we evaluated the
two systems as pre-processors before our parser,
we found that the Johnson et al pipeline achieved
0.2% better unlabelled attachment score than the
Qian and Liu pipeline. We attribute this to the
use of the Charniak and Johnson (2001) syntac-
tic language model in the Johnson et al pipeline,
which would help the system produce more syn-
tactically consistent output.
Our joint model achieved an unlabelled at-
tachment score of 90.5%, out-performing both
pipeline systems. The Baseline joint parser,
which did not include the Edit transition or disflu-
ency features, scores 1.1% below the Final joint
parser. All of the parse accuracy differences were
found to be statistically significant (p < 0.001).
The Edit transition and disfluency features to-
gether brought a 10.1% improvement in disfluency
F -measure, which was also found to be statisti-
cally significant. The final joint parser achieved
0.2% higher disfluency detection accuracy than
the previous state-of-the-art, the Qian and Liu
(2013) system,5 despite having approximately half
as much training data (we require syntactic anno-
5 Our scores refer to an updated version of the system that
corrects minor pre-processing problems. We thank Qian Xian
for making his code available.
tation, for which there is less data).
Our significance testing regime involved using
20 different random seeds when training each of
our models, which the perceptron algorithm is sen-
sitive to. This could not be applied to the other two
disfluency detectors, so we cannot test those dif-
ferences for significance. However, we note that
the 20 samples for our disfluency detector ranged
in accuracy from 83.3-84.6, so we doubt that 0.2%
mean improvement over the Qian and Liu (2013)
result is meaningful.
Although we did not systematically optimise
on the development set, our test scores are lower
than our development accuracies. Much of the
over-fitting seems to be in the POS tagger, which
dropped in accuracy by 0.5%.
9 Analysis of Edit Behaviour
In order to understand how the parser applies
the Edit transition, we collected some additional
statistics over the development data. The parser
predicted 2,558 Edit transitions, which together
marked 2,706 words disfluent (2,495 correctly).
The Edit transition can mark multiple words dis-
fluent when S0 has one or more rightward descen-
dants. It turns out this case is uncommon; the
parser largely assigns disfluency labels word-by-
word, only sometimes marking words with right-
ward descendents as disfluent.
Of the 2,558 Edit transitions, there were 682
cases were at least one leftward child was returned
to the stack, and the total number of leftward chil-
dren returned was 1,132. The most common type
of construction that caused the parser to return
words to the stack were disfluent predicates, which
often have subjects and discourse conjunctions as
leftward children. An example of a disfluent pred-
icate with a fluent subject is shown in Figure 4.
There were only 48 cases of the same word be-
ing returned to the stack twice. The possibility of
words being returned to the stack multiple times
is what gives our system worse than linear worst-
case complexity. In the worst case, the ith word
of a sentence of length n could be returned to the
stack n? (i+1) times. Empirically, the Edit tran-
sition made no difference to run-time.
Once a word has been returned to the stack by
the Edit transition, how does the parser end up
analysing it? If it turned out that almost all of
the former leftward children of disfluent words are
subsequently marked as disfluent, there would be
139
little point in returning them to the stack ? we
could just mark them as disfluent in the original
Edit transition. On the other hand, if they are al-
most all marked as fluent, perhaps they can just be
attached as children to the first word of the buffer.
In fact the two cases are almost equally com-
mon. Of the 1,132 words returned to the stack,
547 were subsequently marked disfluent, and 584
were not. The parser was also quite accurate in
its decisions over these tokens. Of the 547 tokens
marked disfluent, 500 were correct ? similar to
the overall development set precision, 92.2%.
Accuracy over the words returned to the stack
might be improved in future by features referring
to their former heads. For instance, in He went
broke uh became bankrupt, we do not currently
have features that record the deleted dependency
became he and went. We thank one of the anony-
mous reviewers for this suggestion.
10 Related Work
The most similar system to ours was published
very recently. Rasooli and Tetreault (2013) de-
scribe a joint model of dependency parsing and
disfluency detection. They introduce a second
classification step, where they first decide whether
to apply a disfluency transition, or a regular pars-
ing move. Disfluency transitions operate either
over a sequence of words before the start of the
buffer, or a sequence of words from the start of
the buffer forward. Instead of the dynamic oracle
training method that we employ, they use a two-
stage bootstrap-style process.
Direct comparison between our model and
theirs is difficult, as they use the Penn2MALT
scheme, and their parser uses greedy decoding,
where we use beam search. They also use gold-
standard part-of-speech tags, which would im-
prove our scores by around 1%. The use of
beam-search may explain much of our perfor-
mance advantage: they report an unlabelled at-
tachment score of 88.6, and a disfluency detec-
tion F -measure of 81.4%. Our training algorithm
would be applicable to a beam-search version of
their parser, as their transition-system also intro-
duces substantial spurious ambiguity, and some
non-monotonic behaviour.
A hybrid transition system would also be possi-
ble, as the two types of Edit transition seem to be
complementary. The Rasooli and Tetreault system
offers a token-based view of disfluencies, which
is useful for examples such as, and the and the,
which would require two applications of our tran-
sition. On the other hand, our Edit transition may
have the advantage for more syntactically compli-
cated examples, particularly for disfluent verbs.
The importance of syntactic features for disflu-
ency detection was demonstrated by Johnson and
Charniak (2004). Despite this, most subsequent
work has used sequence models, rather than syn-
tactic parsers. The other disfluency system that
we compare our model to, developed by Qian and
Liu (2013), uses a cascade of Maximum Margin
Markov Models to perform disfluency detection
with minimal syntactic information.
One motivation for sequential approaches is that
most applications of these models will be over un-
segmented text, as segmenting unpunctuated text
is a difficult task that benefits from syntactic fea-
tures (Zhang et al., 2013).
We consider the most promising aspect of our
system to be that it is naturally incremental, so it
should be straightforward to extend the system to
operate on unsegmented text in subsequent work.
Due to its use of syntactic features, from the joint
model, the system is substantially more accurate
than the previous state-of-the-art in incremental
disfluency detection, 77% (Zwarts et al., 2010).
11 Conclusion
We have presented an efficient and accurate joint
model of dependency parsing and disfluency de-
tection. The model out-performs pipeline ap-
proaches using state-of-the-art disfluency detec-
tors, and is highly efficient, processing over 550
tokens a second. Because the system is incremen-
tal, it should be straight-forward to apply it to un-
segmented text. The success of an incremental,
non-monotonic parser at disfluent speech parsing
may also be of some psycholinguistic interest.
Acknowledgments
The authors would like to thank the anony-
mous reviewers for their valuable comments.
This research was supported under the Aus-
tralian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593).
References
Miguel Ballesteros and Joakim Nivre. 2013. Go-
ing to the roots of dependency parsing. Compu-
tational Linguistics. 39:1.
140
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural
language. Computational Linguistics, 18:467?
479.
Eugene Charniak. 2001. Immediate-head parsing
for language models. In Proceedings of 39th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 124?131. Associa-
tion for Computational Linguistics, Toulouse,
France.
Eugene Charniak and Mark Johnson. 2001. Edit
detection and parsing for transcribed speech. In
Proceedings of the 2nd Meeting of the North
American Chapter of the Association for Com-
putational Linguistics, pages 118?126. The As-
sociation for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt dis-
criminative reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 173?180. As-
sociation for Computational Linguistics, Ann
Arbor, Michigan.
Michael Collins. 2002. Discriminative training
methods for hidden Markov models: Theory
and experiments with perceptron algorithms. In
Proceedings of the 2002 Conference on Empir-
ical Methods in Natural Language Processing,
pages 1?8. Association for Computational Lin-
guistics.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalu-
ation (LREC).
Lyn Frazier and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology, 14(2):178?210.
Yoav Goldberg and Joakim Nivre. 2012. A dy-
namic oracle for arc-eager dependency parsing.
In Proceedings of the 24th International Con-
ference on Computational Linguistics (Coling
2012). Association for Computational Linguis-
tics, Mumbai, India.
Matthew Honnibal, Yoav Goldberg, and Mark
Johnson. 2013. A non-monotonic arc-eager
transition system for dependency parsing. In
Proceedings of the Seventeenth Conference on
Computational Natural Language Learning,
pages 163?172. Association for Computational
Linguistics, Sofia, Bulgaria.
Liang Huang, Suphan Fayong, and Yang Guo.
2012. Structured perceptron with inexact
search. In Proceedings of the 2012 Con-
ference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, pages 142?
151. Association for Computational Linguis-
tics, Montre?al, Canada.
Mark Johnson and Eugene Charniak. 2004. A
TAG-based noisy channel model of speech re-
pairs. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Lin-
guistics, pages 33?39.
Douglas A. Jones, Florian Wolf, Edward Gib-
son, Elliott Williams, Evelina Fedorenko, Dou-
glas A. Reynolds, and Marc A. Zissman. 2003.
Measuring the readability of automatic speech-
to-text transcripts. In INTERSPEECH. ISCA.
Fredrik Jorgensen. 2007. The effects of disflu-
ency detection in parsing spoken language. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of
the 16th Nordic Conference of Computational
Linguistics NODALIDA-2007, pages 240?244.
Terry Koo and Michael Collins. 2010. Efficient
third-order dependency parsers. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1?
11.
Percy Liang. 2005. Semi-supervised learning for
natural language. Ph.D. thesis, MIT.
Christopher D. Manning. 2011. Part-of-speech
tagging from 97linguistics? In Proceedings of
the 12th international conference on Computa-
tional linguistics and intelligent text processing
- Volume Part I, CICLing?11, pages 171?189.
Springer-Verlag, Berlin, Heidelberg.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings
141
of the 8th International Workshop on Parsing
Technologies (IWPT), pages 149?160.
Joakim Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computa-
tional Linguistics, 34:513?553.
Xian Qian and Yang Liu. 2013. Disfluency detec-
tion using multi-step stacked learning. In Pro-
ceedings of the 2013 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 820?825. Association for Com-
putational Linguistics, Atlanta, Georgia.
Mohammad Sadegh Rasooli and Joel Tetreault.
2013. Joint parsing and disfluency detection in
linear time. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 124?129. Association
for Computational Linguistics, Seattle, Wash-
ington, USA.
Brian Roark, Mary Harper, Eugene Charniak,
Bonnie Dorr, Mark Johnson, Jeremy Kahn,
Yang Liu, Mary Ostendorf, John Hale, Anna
Krasnyanskaya, Matthew Lease, Izhak Shafran,
Matthew Snover, Robin Stewart, and LisaYung.
2006. Sparseval: Evaluation metrics for pars-
ing speech. In Proceedings of Language Re-
source and Evaluation Conference, pages 333?
338. European Language Resources Associa-
tion (ELRA), Genoa, Italy.
Francesco Sartorio, Giorgio Satta, and Joakim
Nivre. 2013. A transition-based dependency
parser using a dynamic parsing strategy. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages
135?144. Association for Computational Lin-
guistics, Sofia, Bulgaria.
Elizabeth Shriberg. 1994. Preliminaries to a The-
ory of Speech Disfluencies. Ph.D. thesis, Uni-
versity of California, Berkeley.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara,
and Jun?ichi Tsujii. 2009. Latent variable per-
ceptron algorithm for structured classification.
In IJCAI, pages 1236?1242.
Joseph Turian, Lev-Arie Ratinov, and Yoshua
Bengio. 2010. Word representations: A simple
and general method for semi-supervised learn-
ing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguis-
tics, pages 384?394. Association for Computa-
tional Linguistics, Uppsala, Sweden.
Dongdong Zhang, Shuangzhi Wu, Nan Yang, and
Mu Li. 2013. Punctuation prediction with
transition-based parsing. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics, pages 752?760. As-
sociation for Computational Linguistics, Sofia,
Bulgaria.
Yue Zhang and Stephen Clark. 2011. Syntac-
tic processing using the generalized perceptron
and beam search. Computational Linguistics,
37(1):105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local
features. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 188?193. Association for Computational
Linguistics, Portland, USA.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min
Zhang, and Jingbo Zhu. 2013. Fast and accu-
rate shift-reduce constituent parsing. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages
434?443. Association for Computational Lin-
guistics, Sofia, Bulgaria.
Simon Zwarts and Mark Johnson. 2011. The im-
pact of language models and loss functions on
repair disfluency detection. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 703?711. Association for
Computational Linguistics, Portland, USA.
Simon Zwarts, Mark Johnson, and Robert Dale.
2010. Detecting speech repairs incrementally
using a noisy channel approach. In Proceedings
of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages
1371?1378. Coling 2010 Organizing Commit-
tee, Beijing, China.
142
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 313?316,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SCHWA: PETE using CCG Dependencies with the C&C Parser
Dominick Ng, James W. D. Constable, Matthew Honnibal and James R. Curran
e
-lab, School of Information Technologies
University of Sydney
NSW 2006, Australia
{dong7223,jcon6353,mhonn,james}@it.usyd.edu.au
Abstract
This paper describes the SCHWA system
entered by the University of Sydney in Se-
mEval 2010 Task 12 ? Parser Evaluation
using Textual Entailments (Yuret et al,
2010). Our system achieved an overall ac-
curacy of 70% in the task evaluation.
We used the C&C parser to build CCG de-
pendency parses of the truth and hypothe-
sis sentences. We then used partial match
heuristics to determine whether the sys-
tem should predict entailment. Heuristics
were used because the dependencies gen-
erated by the parser are construction spe-
cific, making full compatibility unlikely.
We also manually annotated the develop-
ment set with CCG analyses, establishing
an upper bound for our entailment system
of 87%.
1 Introduction
The SemEval 2010 Parser Evaluation using Tex-
tual Entailments (PETE) task attempts to address
the long-standing problems in parser evaluation
caused by the diversity of syntactic formalisms
and analyses in use. The task investigates the
feasibility of a minimalist extrinsic evaluation ?
that of detecting textual entailment between a truth
sentence and a hypothesis sentence. It is extrin-
sic in the sense that it evaluates parsers on a task,
rather than a direct comparison of their output
against some gold standard. However, it requires
only minimal task-specific logic, and the proposed
entailments are designed to be inferrable based on
syntactic information alone.
Our system used the C&C parser (Clark and
Curran, 2007a), which uses the Combinatory Cat-
egorial Grammar formalism (CCG, Steedman,
2000). We used the CCGbank-style dependency
output of the parser (Hockenmaier and Steedman,
2007), which is a directed graph of head-child re-
lations labelled with the head?s lexical category
and the argument slot filled by the child.
We divided the dependency graphs of the truth
and hypothesis sentences into predicates that con-
sisted of a head word and its immediate children.
For instance, the parser?s analysis of the sentence
Totals include only vehicle sales reported in pe-
riod might produce predicates like include(Totals,
sales), only(include), and reported(sales). If at
least one such predicate matches in the two parses,
we predict entailment. We consider a single pred-
icate match sufficient for entailment because the
lexical categories and slots that constitute our de-
pendency labels are often different in the hypothe-
sis sentence due to the generation process used in
the task.
The single predicate heuristic gives us an over-
all accuracy of 70% on the test set. Our precision
and recall over the test set was 68% and 80% re-
spectively giving an F-score of 74%.
To investigate how many of the errors were due
to parse failures, and how many were failures of
our entailment recognition process, we manually
annotated the 66 development truth sentences with
gold standard CCG derivations. This established
an upper bound of 87% F-score for our approach.
This upper bound suggests that there is still
work to be done before the system allows trans-
parent evaluation of the parser. However, cross-
framework parser evaluation is a difficult problem:
previous attempts to evaluate the C&C parser on
grammatical relations (Clark and Curran, 2007b)
and Penn Treebank-trees (Clark and Curran, 2009)
have also produced upper bounds between 80 and
90% F-score. Our PETE system was much easier
to produce than either of these previous attempts
at cross-framework parser evaluation, suggesting
that this may be a promising approach to a diffi-
cult problem.
313
Totals include only vehicle sales reported in period.
NP (S\NP )/NP (S\NP )\(S\NP ) N/N N S\NP ((S\NP )\(S\NP ))/NP NP
<B
?
> >
(S\NP )/NP N ? NP (S\NP )\(S\NP )
<
S\NP ? NP\NP
<
NP
>
S\NP
<
S
Figure 1: An example CCG derivation, showing how the categories assigned to words are combined to
form a sentence. The arrows indicate the direction of application.
2 Background
Combinatory Categorial Grammar (CCG, Steed-
man, 2000) is a lexicalised grammar formalism
based on combinatory logic. The grammar is di-
rectly encoded in the lexicon in the form of combi-
natory categories that govern how each word com-
bines with its neighbours. The parsing process de-
termines the most likely assignment of categories
to words, and finds a sequence of combinators that
allows them to form a sentence.
A sample CCG derivation for a sentence from
the test set is shown in Figure 1. The category for
each word is indicated beneath it. It can be seen
that some categories take other categories as ar-
guments; each argument slot in a category is num-
bered based on the order of application, from latest
to earliest. For example:
((S/NP
1
)/(S/NP )
2
)\NP
3
Figure 2 shows how the argument slots are
mapped to dependencies. The first two columns
list the predicate words and their categories, while
the second two show how each argument slot is
filled. For example, in the first row, only has the
category (S\NP )\(S\NP ), with argument slot
1 filled by include). It is these dependencies that
form the basis for our predicates in this task.
only (S\NP )\(S\NP ) 1 include
vehicle N/N 1 sales
in ((S\NP )\(S\NP ))/NP 2 period
in ((S\NP )\(S\NP ))/NP 1 reported
reported S\NP 1 sales
include (S\NP )/NP 2 sales
include (S\NP )/NP 1 Totals
Figure 2: The dependencies represented by the
derivation in Figure 1.
Recent work has seen the development of high-
performance parsers built on the CCG formalism.
Clark and Curran (2007a) demonstrate the use of
techniques like adaptive supertagging, parallelisa-
tion and a dynamic-programming chart parsing al-
gorithm to implement the C&C parser, a highly
efficient CCG parser that performs well against
parsers built on different formalisms (Rimell et al,
2009). We use this parser for the PETE task.
The performance of statistical parsers is largely
a function of the quality of the corpora they are
trained on. For this task, we used models derived
from the CCGbank corpus ? a transformation of
the Penn Treebank (Marcus et al, 1993) including
CCG derivations and dependencies (Hockenmaier,
2003a). It was created to further CCG research
by providing a large corpus of appropriately anno-
tated data, and has been shown to be suitable for
the training of high-performance parsers (Hocken-
maier, 2003b; Clark and Curran, 2004).
3 Method
Our system used the C&C parser to parse the truth
and hypothesis sentences. We took the dependen-
cies generated by the parser and processed these to
generate predicates encoding the canonical form
of the head word, its required arguments, and their
order. We then attempted to unify the predicates
from the hypothesis sentence with the predicates
in the truth sentence. A successful unification of
predicates a and b occurs when the head words of
a and b are identical and their argument slots are
also identical. If any predicate from the hypothe-
sis sentence unified with a predicate from the truth
sentence, our system returned YES, otherwise the
system returned NO.
We used the 66 sentence development set to
tune our approach. While analysing the hypoth-
esis sentences, we noticed that many examples re-
314
YES entailment NO entailment Overall
System correct incorrect A (%) correct incorrect A (%) accuracy (%) F-score
SCHWA 125 31 80 87 58 60 70 74
median 71 85 46 88 57 61 53 50
baseline 156 0 100 0 145 0 52 68
low 68 88 44 76 69 52 48 46
Table 1: Final results over the test set
YES entailment NO entailment Overall
System correct incorrect A (%) correct incorrect A (%) accuracy (%) F-score
Gold deps 34 6 85 22 4 90 87 87
Parsed deps 32 8 80 20 6 77 79 82
Table 2: Results over the development set
placed nouns from the truth sentence with indefi-
nite pronouns such as someone or something (e.g.
Someone bought something). In most of these cases
the indefinite would not be present in the truth sen-
tence at all, so to deal with this we converted in-
definite pronouns into wildcard markers that could
be matched to any argument. We also incorporated
sensitivity to passive sentences by adjusting the ar-
gument numbers of dependents.
In its most naive form our system is heavily
biased towards excellent recall but poor preci-
sion. We evaluated a number of heuristics to prune
the predicate space and selected those which im-
proved the performance over the development set.
Our final system used the part-of-speech tags gen-
erated by the parser to remove predicates headed
by determiners, prepositions and adjectives. We
note that even after predicate pruning our system
is still likely to return better recall performance
than precision, but this discrepancy was masked in
part by the nature of the development set: most hy-
potheses are short and so the potential number of
predicates after pruning is likely to be small. The
final predicates generated by the system for the ex-
ample derivation given in Figure 1 after heuristic
pruning are:
only(include)
reported(sales)
include(totals, sales)
4 Results
We report results over the 301 sentence test set in
Table 1. Our overall accuracy was 70%, and per-
formance over YES entailments was roughly 20%
higher than accuracy over NO entailments. This
bias towards YES entailments is a reflection of our
single match heuristic that only required one pred-
icate match before answering YES. Our system
performed nearly 20% better than the baseline sys-
tem (all YES responses) and placed second overall
in the task evaluation.
Table 2 shows our results over the development
corpus. The 17% drop in accuracy and 8% drop in
F-score between the development data and the test
data suggests that our heuristics may have over-
fitted to the limited development data. More so-
phisticated heuristics over a larger corpus would
be useful for further fine-tuning our system.
4.1 Results with Gold Standard Parses
Our entailment system?s errors could be broadly
divided into two classes: those due to incorrect
parses, and those due to incorrect comparison of
the parses. To investigate the relative contribu-
tions of these two classes of errors, we manually
annotated the 66 development sentences with CCG
derivations. This allowed us to evaluate our sys-
tem using gold standard parses. Only one anno-
tator was available, so we were unable to calcu-
late inter-annotator agreement scores to examine
the quality of our annotations.
The annotation was prepared with the annota-
tion tool used by Honnibal et al (2009). The tool
presents the user with a CCG derivation produced
by the C&C parser. The user can then correct the
lexical categories, or add bracket constraints to the
parser using the algorithm described by Djordjevic
and Curran (2006), and reparse the sentence until
the derivation desired is produced.
Our results with gold standard dependencies are
315
shown in Table 2. The accuracy is 87%, establish-
ing a fairly low upper bound for our approach to
the task. Manual inspection of the remaining er-
rors showed that some were due to incorrect parses
for the hypothesis sentence, and some were due to
entailments which the parser?s dependency anal-
yses could not resolve, such as They ate whole
steamed grains ? The grains were steamed. The
largest source of errors was our matching heuris-
tics, suggesting that our approach to the task must
be improved before it can be considered a trans-
parent evaluation of the parser.
5 Conclusion
We constructed a system to evaluate the C&C
parser using textual entailments. We converted the
parser output into a set of predicate structures and
used these to establish the presence of entailment.
Our system achieved an overall accuracy of 79%
on the development set and 70% over the test set.
The gap between our development and test accu-
racies suggests our heuristics may have been over-
fitted to the development data.
Our investigation using gold-standard depen-
dencies established an upper bound of 87% on
the development set for our approach to the task.
While this is not ideal, we note that previous ef-
forts at cross-parser evaluation have shown that it
is a difficult problem (Clark and Curran (2007b)
and Clark and Curran (2009)). We conclue that
the concept of a minimal extrinsic evaluation put
forward in this task is a promising avenue for
formalism-independent parser comparison.
References
Stephen Clark and James R. Curran. Parsing the
WSJ using CCG and log-linear models. In Pro-
ceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages
104?111, 2004.
Stephen Clark and James R. Curran. Wide-
Coverage Efficient Statistical Parsing with CCG
and Log-Linear Models. Computational Lin-
guistics, 33(4):493?552, 2007a.
Stephen Clark and James R. Curran. Formalism-
independent parser evaluation with CCG and
DepBank. In Proceedings of the 45th Annual
Meeting of the Association for Computational
Linguistics, pages 248?255, Prague, Czech Re-
public, 25?27 June 2007b.
Stephen Clark and James R. Curran. Compar-
ing the accuracy of CCG and Penn Treebank
Parsers. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 53?56,
Suntec, Singapore, August 2009.
Bojan Djordjevic and James R. Curran. Faster
wide-coverage CCG parsing. In Proceedings of
the Australasian Language Technology Work-
shop 2006, pages 3?10, Sydney, Australia, De-
cember 2006.
Julia Hockenmaier. Data and models for sta-
tistical parsing with Combinatory Categorial
Grammar. PhD thesis, 2003a.
Julia Hockenmaier. Parsing with generative mod-
els of predicate-argument structure. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 359?
366. Association for Computational Linguistics
Morristown, NJ, USA, 2003b.
Julia Hockenmaier and Mark Steedman. CCG-
bank: a corpus of CCG derivations and depen-
dency structures extracted from the Penn Tree-
bank. Computational Linguistics, 33(3):355?
396, 2007.
Matthew Honnibal, Joel Nothman, and James R.
Curran. Evaluating a Statistical CCG Parser on
Wikipedia. In Proceedings of the 2009 Work-
shop on The People?s Web Meets NLP: Collabo-
ratively Constructed Semantic Resources, pages
38?41, Singapore, August 2009.
Mitchell P. Marcus, Mary Ann Marcinkiewicz,
and Beatrice Santorini. Building a large an-
notated corpus of English: The Penn Tree-
bank. Computational Linguistics, 19(2):313?
330, 1993.
Laura Rimell, Stephen Clark, and Mark Steedman.
Unbounded Dependency Recovery for Parser
Evaluation. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, volume 2, pages 813?821,
2009.
Mark Steedman. The Syntactic Process. MIT
Press, Massachusetts Institute of Technology,
USA, 2000.
Deniz Yuret, Ayd?n Han, and Zehra Turgut.
SemEval-2010 Task 12: Parser Evaluation us-
ing Textual Entailments. In Proceedings of the
SemEval-2010 Evaluation Exercises on Seman-
tic Evaluation, 2010.
316
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 163?172,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
A Non-Monotonic Arc-Eager Transition System for Dependency Parsing
Matthew Honnibal
Department of Computing
Macquarie University
Sydney, Australia
matthew.honnibal@mq.edu.edu.au
Yoav Goldberg
Department of Computer Science
Bar Ilan University
Ramat Gan, Israel
yoav.goldberg@gmail.com
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mark.johnson@mq.edu.edu.au
Abstract
Previous incremental parsers have used
monotonic state transitions. However,
transitions can be made to revise previous
decisions quite naturally, based on further
information.
We show that a simple adjustment to the
Arc-Eager transition system to relax its
monotonicity constraints can improve ac-
curacy, so long as the training data in-
cludes examples of mistakes for the non-
monotonic transitions to repair. We eval-
uate the change in the context of a state-
of-the-art system, and obtain a statistically
significant improvement (p < 0.001) on
the English evaluation and 5/10 of the
CoNLL languages.
1 Introduction
Historically, monotonicity has played an im-
portant role in transition-based parsing systems.
Non-monotonic systems, including the one pre-
sented here, typically redundantly generate multi-
ple derivations for each syntactic analysis, leading
to spurious ambiguity (Steedman, 2000). Early,
pre-statistical work on transition-based parsing
such as Abney and Johnson (1991) implicitly as-
sumed that the parser searches the entire space
of possible derivations. The presence of spuri-
ous ambiguity causes this search space to be a di-
rected graph rather than a tree, which considerably
complicates the search, so spurious ambiguity was
avoided whenever possible.
However, we claim that non-monotonicity and
spurious ambiguity are not disadvantages in a
modern statistical parsing system such as ours.
Modern statistical models have much larger search
spaces because almost all possible analyses are al-
lowed, and a numerical score (say, a probability
distribution) is used to distinguish better analy-
ses from worse ones. These search spaces are so
large that we cannot exhaustively search them, so
instead we use the scores associated with partial
analyses to guide a search that explores only a mi-
nuscule fraction of the space (In our case we use
greedy decoding, but even a beam search only ex-
plores a small fraction of the exponentially-many
possible analyses).
In fact, as we show here the additional redun-
dant pathways between search states that non-
monotonicity generates can be advantageous be-
cause they allow the parser to ?correct? an ear-
lier parsing move and provide an opportunity to
recover from formerly ?fatal? mistakes. Infor-
mally, non-monotonicity provides ?many paths up
the mountain? in the hope of making it easier to
find at least one.
We demonstrate this by modifying the Arc-
Eager transition system (Nivre, 2003; Nivre et al,
2004) to allow a limited capability for non-
monotonic transitions. The system normally em-
ploys two deterministic constraints that limit the
parser to actions consistent with the previous his-
tory. We remove these, and update the transitions
so that conflicts are resolved in favour of the latest
prediction.
The non-monotonic behaviour provides an im-
provement of up to 0.2% accuracy over the cur-
rent state-of-the-art in greedy parsing. It is pos-
sible to implement the greedy parser we de-
scribe very efficiently: our implementation, which
can be found at http://www.github.com/
syllog1sm/redshift, parses over 500 sen-
tences a second on commodity hardware.
163
2 The Arc-Eager Transition System
In transition-based parsing, a parser consists of a
state (or a configuration) which is manipulated by
a set of actions. An action is applied to a state
and results in a new state. The parsing process
concludes when the parser reaches a final state, at
which the parse tree is read from the state. A par-
ticular set of states and actions yield a transition-
system. Our starting point in this paper is the pop-
ular Arc-Eager transition system, described in de-
tail by Nivre (2008).
The state of the arc-eager system is composed
of a stack, a buffer and a set of arcs. The stack and
the buffer hold the words of a sentence, and the set
of arcs represent derived dependency relations.
We use a notation in which the stack items are
indicated by Si, with S0 being the top of the stack,
S1 the item previous to it and so on. Similarly,
buffer items are indicated as Bi, with B0 being
the first item on the buffer. The arcs are of the
form (h, l,m), indicating a dependency in which
the word m modifies the word h with label l.
In the initial configuration the stack is empty,
and the buffer contains the words of the sentence
followed by an artificial ROOT token, as sug-
gested by Ballesteros and Nivre (2013). In the fi-
nal configuration the buffer is empty and the stack
contains the ROOT token.
There are four parsing actions (Shift, Left-Arc,
Right-Arc and Reduce, abbreviated as S,L,R,D re-
spectively) that manipulate stack and buffer items.
The Shift action pops the first item from the buffer
and pushes it on the stack (the Shift action has a
natural precondition that the buffer is not empty,
as well as a precondition that ROOT can only be
pushed to an empty stack). The Right-Arc action
is similar to the Shift action, but it also adds a
dependency arc (S0, B0), with the current top of
the stack as the head of the newly pushed item
(the Right action has an additional precondition
that the stack is not empty).1 The Left-Arc action
adds a dependency arc (B0, S0) with the first item
in the buffer as the head of the top of the stack,
and pops the stack (with a precondition that the
stack and buffer are not empty, and that S0 is not
assigned a head yet). Finally, the Reduce action
pops the stack, with a precondition that the stack
is not empty and that S0 is already assigned a head.
1For labelled dependency parsing, the Right-Arc and
Left-Arc actions are parameterized by a label L such that the
action RightL adds an arc (S0, L,B0), similarly for LeftL.
2.1 Monotonicty
The preconditions of the Left-Arc and Reduce ac-
tions ensure that every word is assigned exactly
one head, resulting in a well-formed parse tree.
The single head constraint is enforced by ensur-
ing that once an action has been performed, sub-
sequent actions must be consistent with it. We re-
fer to this consistency as the monotonicity of the
system.
Due to monotonicity, there is a natural pair-
ing between the Right-Arc and Reduce actions
and the Shift and Left-Arc actions: a word which
is pushed into the stack by Right-Arc must be
popped using Reduce, and a word which is pushed
by Shift action must be popped using Left-Arc. As
a consequence of this pairing, a Right-Arc move
determines that the head of the pushed token must
be to its left, while a Shift moves determines a
head to its right. Crucially, the decision whether
to Right-Arc or Shift is often taken in a state of
missing information regarding the continuation of
the sentence, forcing an incorrect head assignment
on a subsequent move.
Consider a sentence pair such as (a)?I saw Jack
and Jill? / (b)?I saw Jack and Jill fall?. In (a), ?Jack
and Jill? is the NP object of ?saw?, while in (b) it is
a subject of the embedded verb ?fall?. The mono-
tonic arc-eager parser has to decide on an analysis
as soon as it sees ?saw? on the top of the stack and
?Jack? at the front of the buffer, without access to
the disambiguating verb ?fall?.
In what follows, we suggest a non-monotonic
variant of the Arc-Eager transition system, allow-
ing the parser to recover from the incorrect head
assignments which are forced by an incorrect res-
olution of a Shift/Right-Arc ambiguity.
3 The Non-Monotonic Arc-Eager System
The Arc-Eager transition system (Nivre et al,
2004) has four moves. Two of them create depen-
dencies, two push a word from the buffer to the
stack, and two remove an item from the stack:
Push Pop
Adds dependency Right-Arc Left-Arc
No new dependency Shift Reduce
Every word in the sentence is pushed once and
popped once; and every word must have exactly
one head. This creates two pairings, along the di-
agonals: (S, L) and (R, D). Either the push move
adds the head or the pop move does, but not both
and not neither.
164
I saw Jack and Jill fall R
S L S R R D R D L R D L
1 2 3 4 5 6 7 8 9 10 11 12
I saw Jack and Jill fall R
2 4 5
7
2 5
7
9
Figure 1: State before and after a non-monotonic Left-
Arc. At move 9, fall is the first word of the buffer (marked
with an arrow), and saw and Jack are on the stack (circled).
The arc created at move 4 was incorrect (in red). Arcs are
labelled with the move that created them. After move 9 (the
lower state), the non-monotonic Left-Arc move has replaced
the incorrect dependency with a correct Left-Arc (in green).
Thus in the Arc-Eager system the first move de-
termines the corresponding second move. In our
non-monotonic system the second move can over-
write an attachment made by the first. This change
makes the transition system non-monotonic, be-
cause if the model decides on an incongruent pair-
ing we will have to either undo or add a depen-
dency, depending on whether we correct a prior
Right-Arc, or a prior Shift.
3.1 Non-monotonic Left-Arc
Figure 1 shows a before-and-after view of a non-
monotonic transition. The sequence below the
words shows the transition history. The words that
are circled in the upper and lower line are on the
stack before and after the transition, respectively.
The arrow shows the start of the buffer, and arcs
are labelled with the move that added them.
The parser began correctly by Shifting I and
Left-Arcing it to saw, which was then also Shifted.
The mistake, made at Move 4, was to Right-Arc
Jack instead of Shifting it.
The difficulty of this kind of a decision for an
incremental parser is fundamental. The leftward
context does not constrain the decision, and an ar-
bitrary amount of text could separate Jack from
fall. Eye-tracking experiments show that humans
often perform a saccade while reading such exam-
ples (Frazier and Rayner, 1982).
In moves 5-8 the parser correctly builds the rest
of the NP, and arrives at fall. The monotonicity
constraints would force an incorrect analysis, hav-
ing fall modify Jack or saw, or having saw modify
fall as an embedded verb with no arguments.
I saw Jack and Jill R
S L S S R D R D R D D L
1 2 3 4 5 6 7 8 9 10 11 12
I saw Jack and Jill R
2 5
7
2 5
7
9
Figure 2: State before and after a non-monotonic Reduce.
After making the wrong push move at 4, at move 11 the parser
has Jack on the stack (circled), with only the dummy ROOT
token left in the buffer. A monotonic parser must determinis-
tically Left-Arc Jack here to preserve the previous decision,
despite the current state. We remove this constraint, and in-
stead assume that when the model selects Reduce for a head-
less item, it is reversing the previous Shift/Right decision. We
add the appropriate arc, assigning the label that scored high-
est when the Shift/Right decision was made.
We allow Left-Arcs to ?clobber? edges set by
Right-Arcs if the model recommends it. The pre-
vious edge is deleted, and the Left-Arc proceeds
as normal. The effect of this is exactly as if the
model had correctly chosen Shift at move 4. We
simply give the model a second chance to make
the correct choice.
3.2 Non-monotonic Reduce
The upper arcs in Figure 2 show a state resulting
from the opposite error. The parser has Shifted
Jack instead of Right-Arcing it. After building the
NP the buffer is exhausted, except for the ROOT
token, which is used to wrongly Left-Arc Jack as
the sentence?s head word.
Instead of letting the previous choice lock us in
to the pair (Shift, Left-Arc), we let the later deci-
sion reverse it to (Right-Arc, Reduce), if the parser
has predicted Reduce in spite of the signal from its
previous decision. In the context shown in Figure
2, the correctness of the Reduce move is quite pre-
dictable, once the choice is made available.
When the Shift/Right-Arc decision is reversed,
we add an arc between the top of the stack (S0)
and the word preceding it (S1). This is the arc that
would have been created had the parser chosen to
Right-Arc when it chose to Shift. Since our idea is
to reverse this mistake, we select the Right-Arc la-
bel that the model scored most highly at that time.2
2An alternative approach to label assignment is to parame-
terize the Reduce action with a label, similar to the Right-Arc
and Left-Arc actions, and let that label override the previ-
ously predicted label. This would allow the parser to con-
165
To summarize, our Non-Monotnonic Arc-
Eager system differs from the monotonic
Arc-Eager system by:
? Changing the Left-Arc action by removing
the precondition that S0 does not have a head,
and updating the dependency arcs such previ-
ously derived arcs having S0 as a dependent
are removed from the arcs set.
? Changing the Reduce action by removing the
precondition that S0 has a head, and updating
the dependency arcs such that if S0 does not
have a head, S1 is assigned as the head of S0.
4 Why have two push moves?
We have argued above that it is better to trust the
second decision that the model makes, rather than
using the first decision to determine the second.
If this is the case, is the first decision entirely re-
dundant? Instead of defining how pop moves can
correct Shift/Right-Arc mistakes, we could instead
eliminate the ambiguity. There are two possibili-
ties: Shift every token, and create all Right-Arcs
via Reduce; or Right-Arc every token, and replace
them with Left-Arcs where necessary.
Preliminary experiments on the development
data revealed a problem with these approaches. In
many cases the decision whether to Shift or Right-
Arc is quite clear, and its result provides useful
conditioning context to later decisions. The in-
formation that determined those decisions is never
lost, but saving all of the difficulty for later is not
a very good structured prediction strategy.
As an example of the problem, if the Shift move
is eliminated, about half of the Right-Arcs created
will be spurious. All of these arcs will be assigned
labels making important features uselessly noisy.
In the other approach, we avoid creating spurious
arcs, but the model does not predict whether S0 is
attached to S1, or what the label would be, and we
miss useful features.
The non-monotonic transition system we pro-
pose does not have these problems. The model
learns to make Shift vs. Right-Arc decisions as
normal, and conditions on them ? but without
committing to them.
dition its label decision on the new context, which was suf-
ficiently surprising to change its move prediction. For effi-
ciency and simplicity reasons, we chose instead to trust the
label the model proposed when the reduced token was ini-
tially pushed into the stack. This requires an extra vector of
labels to be stored during parsing.
5 Dynamic Oracles
An essential component when training a
transition-based parser is an oracle which,
given a gold-standard tree, dictates the sequence
of moves a parser should make in order to derive
it. Traditionally, these oracles are defined as func-
tions from trees to sequences, mapping a gold tree
to a single sequence of actions deriving it, even
if more than one sequence of actions derives the
gold tree. We call such oracles static. Recently,
Goldberg and Nivre (2012) introduced the concept
of a dynamic oracle, and presented a concrete ora-
cle for the arc-eager system. Instead of mapping
a gold tree to a sequence of actions, the dynamic
oracle maps a ?configuration, gold tree? pair to a
set of optimal transitions. More concretely, the
dynamic oracle presented in Goldberg and Nivre
(2012) maps ?action, configuration, tree? tuples
to an integer, indicating the number of gold arcs
in tree that can be derived from configuration
by some sequence of actions, but could not be
derived after applying action to the configuration.
There are two advantages to this. First, the
ability to label any configuration, rather than only
those along a single path to the gold-standard
derivation, allows much better training data to be
generated. States come with realistic histories, in-
cluding errors ? a critical point for the current
work. Second, the oracle accounts for spurious
ambiguity correctly, as it will label multiple ac-
tions as correct if the optimal parses resulting from
them are equally accurate.
In preliminary experiments in which we trained
the parser using the static oracle but allowed the
non-monotonic repair operations during parsing,
we found that the the repair moves yielded no im-
provement. This is because the static oracle does
not generate any examples of the repair moves dur-
ing training, causing the parser to rarely predict
them in test time. We will first describe the Arc-
Eager dynamic oracle, and then define dynamic
oracles for the non-monotonic transition systems
we present.
5.1 Monotonic Arc-Eager Dynamic Oracle
We now briefly describe the dynamic oracle for the
arc-eager system. For more details, see Goldberg
and Nivre (2012). The oracle is computed by rea-
soning about the arcs which are reachable from a
given state, and counting the number of gold arcs
which will no longer be reachable after applying a
166
given transition at a given state. 3
The reasoning is based on the observations that
in the arc-eager system, new arcs (h, l,m) can be
derived iff the following conditions hold:
(a) There is no existing arc (h?, l?,m) such that
h? 6= h, and (b) Either both h and m are on the
buffer, or one of them is on the buffer and the other
is on the stack. In other words:
(a) once a word acquires a head (in a Left-Arc or
Right-Arc transition) it loses the ability to acquire
any other head.
(b) once a word is moved from the buffer to the
stack (Shift or Right-Arc) it loses the ability to ac-
quire heads that are currently on the stack, as well
as dependents that are currently on the stack and
are not yet assigned a head.4
(c) once a word is removed from the stack (Left-
Arc or Reduce) it loses the ability to acquire any
dependents on the buffer.
Based on these observations, Goldberg and Nivre
(2012) present an oracle C(a, c, t) for the mono-
tonic arc-eager system, computing the number of
arcs in the gold tree t that are reachable from a
parser?s configuration c and are no longer reach-
able from the configuration a(c) resulting from the
application of action a to configuration c.
5.2 Non-monotonic Dynamic Oracles
Given the oracle C(a, c, t) for the monotonic sys-
tem, we adapt it to a non-monotonic variant by
considering the changes from the monotonic to the
non-monotonic system, and adding ? terms ac-
cordingly. We define three novel oracles: CNML,
CNMD and CNML+D for systems with a non-
monotonic Left-Arc, Reduce or both.
CNML(a, c, t) = C(a, c, t) +?NML(a, c, t)
CNMD(a, c, t) = C(a, c, t) +?NMD(a, c, t)
CNML+D(a, c, t) = C(a, c, t) +?NML(a, c, t)
+?NMD(a, c, t)
The terms ?NML and ?NMD reflect the score
adjustments that need to be done to the arc-eager
oracle due to the changes of the Left-Arc and Re-
duce actions, respectively, and are detailed below.
3The correctness of the oracle is based on a property of
the arc-eager system, stating that if a set of arcs which can be
extended to a projective tree can be individually derived from
a given configuration, then a projective tree containing all of
the arcs in the set is also derivable from the same configura-
tion. This same property holds also for the non-monotonic
variants we propose.
4The condition that the words on the stack are not yet as-
signed a head is missing from (Goldberg and Nivre, 2012)
Changes due to non-monotonic Left-Arc:
? ?NML(RIGHTARC, c, t): The cost of Right-
Arc is decreased by 1 if the gold head of B0 is
on the buffer (because B0 can still acquire its
correct head later with a Left-Arc action). It
is increased by 1 for any word w on the stack
such that B0 is the gold parent of w and w
is assigned a head already (in the monotonic
oracle, this cost was taken care of when the
word was assigned an incorrect head. In the
non-monotonic variant, this cost is delayed).
? ?NML(REDUCE, c, t): The cost of Reduce is
increased by 1 if the gold head of S0 is on the
buffer, because removing S0 from the stack
precludes it from acquiring its correct head
later on with a Left-Arc action. (This cost is
paid for in the monotonic version when S0
acquired its incorrect head).
? ?NML(LEFTARC, c, t): The cost of Left-
Arc is increased by 1 if S0 is already assigned
to its gold parent. (This situation is blocked
by a precondition in the monotonic case).
The cost is also increased if S0 is assigned
to a non-gold parent, and the gold parent is
in the buffer, but not B0. (As a future non-
monotonic Left-Arc is prevented from setting
the correct head.)
? ?NML(SHIFT, c, gold): The cost of Shift is
increased by 1 for any word w on the stack
such that B0 is the gold parent of w and w is
assigned a head already. (As in Right-Arc, in
the monotonic oracle, this cost was taken care
of when w was assigned an incorrect head.)
Changes due to non-monotonic Reduce:
? ?NMD(SHIFT, c, gold): The cost of Shift is
decreased by 1 if the gold head of B0 is S0
(Because this arc can be added later on with
a non-monotonic Reduce action).
? ?NMD(LEFTARC, c, gold): The cost of
Left-Arc is increased by 1 if S0 is not as-
signed a head, and the gold head of S0 is
S1 (Because this precludes adding the correct
arc with a Reduce of S0 later).
? ?NMD(REDUCE, c, gold) = 0. While it may
seem that a change to the cost of a Reduce ac-
tion is required, in fact the costs of the mono-
tonic system hold here, as the head of S0 is
167
predetermined to be S1. The needed adjust-
ments are taken care of in Left-Arc and Shift
actions.5
? ?NMD(RIGHTARC, c, gold) = 0
6 Applying the Oracles in Training
Once the dynamic-oracles for the non-monotonic
system are defined, we could in principle just plug
them in the perceptron-based training procedure
described in Goldberg and Nivre (2012). How-
ever, a tacit assumption of the dynamic-oracles is
that all paths to recovering a given arc are treated
equally. This assumption may be sub-optimal
for the purpose of training a parser for a non-
monotonic system.
In Section 4, we explained why removing the
ambiguity between Shift and Right-Arcs alto-
gether was an inferior strategy. Failing to discrim-
inate between arcs reachable by monotonic and
non-monotonic paths does just that, so this oracle
did not perform well in preliminary experiments
on the development data.
Instead, we want to learn a model that will offer
its best prediction of Shift vs. Right-Arc, which
we expect to usually be correct. However, in those
cases where the model does make the wrong de-
cision, it should have the ability to later over-turn
that decision, by having an unconstrained choice
of Reduce vs. Left-Arc.
In order to correct for that, we don?t use the
non-monotonic oracles directly when training the
parser, but instead train the parser using both the
monotonic and non-monotonic oracles simultane-
ously by combining their judgements: while we
always prefer zero-cost non-monotonic actions to
monotonic-actions with non-zero cost, if the non-
monotonic oracle assigns several actions a zero-
cost, we prefer to follow those actions that are also
assigned a zero-cost by the monotonic oracle, as
these actions lead to the best outcome without re-
lying on a non-monotonic (repair) operation down
the road.
7 Experiments
We base our experiments on the parser described
by Goldberg and Nivre (2012). We began by im-
plementing their baseline system, a standard Arc-
Eager parser using an averaged Perceptron learner
and the extended feature set described by Zhang
5If using a labeled reduce transition, the label assignment
costs should be handled here.
Stanford MALT
W S W S
Unlabelled Attachment
Baseline (G&N-12) 91.2 42.0 90.9 39.7
NM L 91.4 43.1 91.0 40.1
NM D 91.4 42.8 91.1 41.2
NM L+D 91.6 43.3 91.3 41.5
Labelled Attachment
Baseline (G&N-12) 88.7 31.8 89.7 36.6
NM L 89.0 32.5 89.8 36.9
NM D 88.9 32.3 89.9 37.7
NM L+D 89.1 32.7 90.0 37.9
Table 1: Development results on WSJ 22. Both non-
monotonic transitions bring small improvements in per-token
(W) and whole sentence (S) accuracy, and the improvements
are additive.
and Nivre (2011). We follow Goldberg and Nivre
(2012) in training all models for 15 iterations, and
shuffling the sentences before each iteration.
Because the sentence ordering affects the
model?s accuracy, all results are averaged from
scores produced using 20 different random seeds.
The seed determines how the sentences are shuf-
fled before each iteration, as well as when to fol-
low an optimal action and when to follow a non-
optimal action during training. The Wilcoxon
signed-rank test was used for significance testing.
A train/dev/test split of 02-21/22/23 of the Penn
Treebank WSJ (Marcus et al, 1993) was used for
all models. The data was converted into Stan-
ford dependencies (de Marneffe et al, 2006) with
copula-as-head and the original PTB noun-phrase
bracketing. We also evaluate our models on de-
pendencies created by the PENN2MALT tool, to
assist comparison with previous results. Automat-
ically assigned POS tags were used during training,
to match the test data more closely. 6 We also eval-
uate the non-monotonic transitions on the CoNLL
2007 multi-lingual data.
8 Results and analysis
Table 1 shows the effect of the non-monotonic
transitions on labelled and unlabelled attachment
score on the development data. All results are av-
erages from 20 models trained with different ran-
dom seeds, as the ordering of the sentences at each
iteration of the Perceptron algorithm has an effect
on the system?s accuracy. The two non-monotonic
transitions each bring small but statistically signif-
icant improvements that are additive when com-
bined in the NM L+D system. The result is stable
6We thank Yue Zhang for supplying the POS-tagged files
used in the Zhang and Nivre (2011) experiments.
168
across both dependency encoding schemes.
Frequency analysis. Recall that there are two pop
moves available: Left-Arc and Reduce. The Left-
Arc is considered non-monotonic if the top of the
stack has a head specified, and the Reduce move
is considered non-monotonic if it does not. How
often does the parser select monotonic and non-
monotonic pop moves, and how often is its deci-
sion correct?
In Table 2, the True Positive column shows how
often non-monotonic transitions were used to add
gold standard dependencies. The False Positive
column shows how often they were used incor-
rectly. The False Negative column shows how
often the parser missed a correct non-monotonic
transition, and the True Negative column shows
how often the monotonic alternative was correctly
preferred (e.g. the parser correctly chose mono-
tonic Reduce in place of non-monotonic Left-
Arc). Punctuation dependencies were excluded.
The current system has high precision but low
recall for repair operations, as they are relatively
rare in the gold-standard. While we already
see improvements in accuracy, the upper bound
achievable by the non-monotonic operations is
higher, and we hope to approach it in the future
using improved learning techniques.
Linguistic analysis. To examine what construc-
tions were being corrected, we looked at the fre-
quencies of the labels being introduced by the
non-monotonic moves. We found that there were
two constructions being commonly repaired, and
a long-tail of miscellaneous cases.
The most frequent repair involved the mark la-
bel. This is assigned to conjunctions introducing
subordinate clauses. For instance, in the sentence
Results were released after the market closed, the
Stanford scheme attaches after to closed. The
parser is misled into greedily attaching after to re-
leased here, as that would be correct if after were a
preposition, as in Results were released after mid-
night. This construction was repaired 33 times, 13
where the initial decision was mark, and 21 times
the other way around. The other commonly re-
paired construction involved greedily attaching an
object that was actually the subject of a comple-
ment clause, e.g. NCNB corp. reported net income
doubled. These were repaired 19 times.
WSJ evaluation. Table 3 shows the final test
results. While still lagging behind search based
parsers, we push the boundaries of what can be
TP FP TN FN
Left-Arc 60 14 18,466 285
Reduce 52 26 14,066 250
Total 112 40 32,532 535
Table 2: True/False positive/negative rates for the predic-
tion of the non-monotonic transitions. The non-monotonic
transitions add correct dependencies 112 times, and produce
worse parses 40 times. 535 opportunities for non-monotonic
transitions were missed.
System O Stanford Penn2Malt
LAS UAS LAS UAS
K&C 10 n3 ? ? ? 93.00
Z&N 11 nk 91.9 93.5 91.8 92.9
G&N 12 n 88.72 90.96 ? ?
Baseline(G&N-12) n 88.7 90.9 88.7 90.6
NM L+D n 88.9 91.1 88.9 91.0
Table 3: WSJ 23 test results, with comparison against the
state-of-the-art systems from the literature of different run-
times. K&C 10=Koo and Collins (2010); Z&N 11=Zhang
and Nivre (2011); G&N 12=Goldberg and Nivre (2012).
achieved with a purely greedy system, with a sta-
tistically significant improvement over G&N 12.
CoNLL 2007 evaluation. Table 4 shows the ef-
fect of the non-monotonic transitions across the
ten languages in the CoNLL 2007 data sets. Statis-
tically significant improvements in accuracy were
observed for five of the ten languages. The accu-
racy improvement on Hungarian and Arabic did
not meet our significance threshold. The non-
monotonic transitions did not decrease accuracy
significantly on any of the languages.
9 Related Work
One can view our non-monotonic parsing system
as adding ?repair? operations to a greedy, deter-
ministic parser, allowing it to undo previous de-
cisions and thus mitigating the effect of incorrect
parsing decisions due to uncertain future, which
is inherent in greedy left-to-right transition-based
parsers. Several approaches have been taken to ad-
dress this problem, including:
Post-processing Repairs (Attardi and Ciaramita,
2007; Hall and Nova?k, 2005; Inokuchi and Ya-
maoka, 2012) Closely related to stacking, this line
of work attempts to train classifiers to repair at-
tachment mistakes after a parse is proposed by
a parser by changing head attachment decisions.
The present work differs from these by incorporat-
ing the repair process into the transition system.
Stacking (Nivre and McDonald, 2008; Martins
et al, 2008), in which a second-stage parser runs
over the sentence using the predictions of the first
parser as features. In contrast our parser works in
169
System AR BASQ CAT CHI CZ ENG GR HUN ITA TUR
Baseline 83.4 76.2 91.5 82.3 78.8 87.9 81.2 77.6 83.8 78.0
NM L+D 83.6 76.1 91.5 82.7 80.1 88.4 81.8 77.9 84.1 78.0
Table 4: Multi-lingual evaluation. Accuracy improved on Chinese, Czech, English, Greek and Italian (p < 0.001), trended
upward on Arabic and Hungarian (p < 0.005), and was unchanged on Basque, Catalan and Turkish (p > 0.4).
a single, left-to-right pass over the sentence.
Non-directional Parsing The EasyFirst parser
of Goldberg and Elhadad (2010) tackles similar
forms of ambiguities by dropping the Shift action
altogether, and processing the sentence in an easy-
to-hard bottom-up order instead of left-to-right,
resulting in a greedy but non-directional parser.
The indeterminate processing order increases the
parser?s runtime from O(n) to O(n log n). In con-
trast, our parser processes the sentence incremen-
tally, and runs in a linear time.
Beam Search An obvious approach to tackling
ambiguities is to forgo the greedy nature of the
parser and instead to adopt a beam search (Zhang
and Clark, 2008; Zhang and Nivre, 2011) or a
dynamic programming (Huang and Sagae, 2010;
Kuhlmann et al, 2011) approach. While these ap-
proaches are very successful in producing high-
accuracy parsers, we here explore what can be
achieved in a strictly deterministic system, which
results in much faster and incremental parsing al-
gorithms. The use of non-monotonic transitions in
beam-search parser is an interesting topic for fu-
ture work.
10 Conclusion and future work
We began this paper with the observation that
because the Arc-Eager transition system (Nivre
et al, 2004) attaches a word to its governor ei-
ther when the word is pushed onto the stack or
when it is popped off the stack, monotonicity (plus
the ?tree constraint? that a word has exactly one
governor) implies that a word?s push-move de-
termines its associated pop-move. In this paper
we suggest relaxing the monotonicity constraint
to permit the pop-move to alter existing attach-
ments if appropriate, thus breaking the 1-to-1 cor-
respondence between push-moves and pop-moves.
This permits the parser to correct some early in-
correct attachment decisions later in the parsing
process. Adding additional transitions means that
in general there are multiple transition sequences
that generate any given syntactic analysis, i.e., our
non-monotonic transition system generates spuri-
ous ambiguities (note that the Arc-Eager transition
system on its own generates spurious ambiguities).
As we explained in the paper, with the greedy de-
coding used here additional spurious ambiguity is
not necessarily a draw-back.
The conventional training procedure for
transition-based parsers uses a ?static? oracle
based on ?gold? parses that never predicts a
non-monotonic transition, so it is clearly not
appropriate here. Instead, we use the incremental
error-based training procedure involving a ?dy-
namic? oracle proposed by Goldberg and Nivre
(2012), where the parser is trained to predict the
transition that will produce the best-possible anal-
ysis from its current configuration. We explained
how to modify the Goldberg and Nivre oracle so
it predicts the optimal moves, either monotonic or
non-monotonic, from any configuration, and use
this to train an averaged perceptron model.
When evaluated on the standard WSJ training
and test sets we obtained a UAS of 91.1%, which
is a 0.2% improvement over the already state-of-
the-art baseline of 90.9% that is obtained with the
error-based training procedure of Goldberg and
Nivre (2012). On the CoNLL 2007 datasets, ac-
curacy improved significantly on 5/10 languages,
and did not decline significantly on any of them.
Looking to the future, we believe that it would
be interesting to investigate whether adding non-
monotonic transitions is beneficial in other parsing
systems as well, including systems that target for-
malisms other than dependency grammars. As we
observed in the paper, the spurious ambiguity that
non-monotonic moves introduce may well be an
advantage in a statistical parser with an enormous
state-space because it provides multiple pathways
to the correct analysis (of which we hope at least
one is navigable).
We investigated a very simple kind of non-
monotonic transition here, but of course it?s pos-
sible to design transition systems with many more
transitions, including transitions that are explicitly
designed to ?repair? characteristic parser errors. It
might even be possible to automatically identify
the most useful repair transitions and incorporate
them into the parser.
170
Acknowledgments
The authors would like to thank the anony-
mous reviewers for their valuable comments.
This research was supported under the Aus-
tralian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593).
References
Stephen Abney and Mark Johnson. 1991. Mem-
ory requirements and local ambiguities of pars-
ing strategies. Journal of Psycholinguistic Re-
search, 20(3):233?250.
Giuseppe Attardi and Massimiliano Ciaramita.
2007. Tree revision learning for dependency
parsing. In Human Language Technologies
2007: The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main Confer-
ence, pages 388?395. Association for Compu-
tational Linguistics, Rochester, New York.
Miguel Ballesteros and Joakim Nivre. 2013. Go-
ing to the roots of dependency parsing. Compu-
tational Linguistics. 39:1.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalu-
ation (LREC).
Lyn Frazier and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology, 14(2):178?210.
Yoav Goldberg and Michael Elhadad. 2010. An
efficient algorithm for easy-first non-directional
dependency parsing. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association
for Computational Linguistics (NAACL HLT),
pages 742?750.
Yoav Goldberg and Joakim Nivre. 2012. A dy-
namic oracle for arc-eager dependency parsing.
In Proceedings of the 24th International Con-
ference on Computational Linguistics (Coling
2012). Association for Computational Linguis-
tics, Mumbai, India.
Keith Hall and Vaclav Nova?k. 2005. Corrective
modeling for non-projective dependency pars-
ing. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT),
pages 42?52.
Liang Huang and Kenji Sagae. 2010. Dynamic
programming for linear-time incremental pars-
ing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguis-
tics (ACL), pages 1077?1086.
Akihiro Inokuchi and Ayumu Yamaoka. 2012.
Mining rules for rewriting states in a transition-
based dependency parser for English. In Pro-
ceedings of COLING 2012, pages 1275?1290.
The COLING 2012 Organizing Committee,
Mumbai, India.
Terry Koo and Michael Collins. 2010. Efficient
third-order dependency parsers. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1?
11.
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and
Giorgio Satta. 2011. Dynamic program-
ming algorithms for transition-based depen-
dency parsers. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 673?682. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313?
330.
Andre? Filipe Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking de-
pendency parsers. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 157?166.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings
of the 8th International Workshop on Parsing
Technologies (IWPT), pages 149?160.
Joakim Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computa-
tional Linguistics, 34:513?553.
Joakim Nivre, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing. In
171
Hwee Tou Ng and Ellen Riloff, editors, HLT-
NAACL 2004 Workshop: Eighth Conference
on Computational Natural Language Learn-
ing (CoNLL-2004), pages 49?56. Association
for Computational Linguistics, Boston, Mas-
sachusetts, USA.
Joakim Nivre and Ryan McDonald. 2008. In-
tegrating graph-based and transition-based de-
pendency parsers. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 950?958.
Mark Steedman. 2000. The Syntactic Process.
MIT Press, Cambridge, MA.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-
based and transition-based dependency parsing.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 562?571. Association for Computa-
tional Linguistics, Honolulu, Hawaii.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local
features. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 188?193. Association for Computational
Linguistics, Portland, Oregon, USA.
172
