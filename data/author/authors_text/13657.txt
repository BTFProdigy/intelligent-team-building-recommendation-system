Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 153?160
Manchester, August 2008
Hybrid processing for grammar and style checking
Berthold Crysmann?, Nuria Bertomeu?, Peter Adolphs?, Dan Flickinger?, Tina Klu?wer??
? Universita?t Bonn, Poppeldorfer Allee 47, D-53115 Bonn, {bcr,tkl}@ifk.uni-bonn.de
? Zentrum fu?r Allgemeine Sprachwissenschaft, Berlin, nuria.bertomeu@dfki.de
? DFKI GmbH, Berlin, {peter.adolphs,kluewer}@dfki.de
? CSLI, Stanford University, danf@csli.stanford.edu
Abstract
This paper presents an implemented hy-
brid approach to grammar and style
checking, combining an industrial pattern-
based grammar and style checker with bi-
directional, large-scale HPSG grammars
for German and English. Under this ap-
proach, deep processing is applied selec-
tively based on the error hypotheses of a
shallow system. We have conducted a com-
parative evaluation of the two components,
supporting an integration scenario where
the shallow system is best used for error de-
tection, whereas the HPSG grammars add
error correction for both grammar and con-
trolled language style errors.
1 Introduction
With the enormous amount of multilingual techni-
cal documentation produced by companies nowa-
days grammar and controlled language checking
(henceforth: style checking) is becoming an appli-
cation highly in demand. It is not only a helpful
tool for authors, but also facilitates the translation
of documents into foreign languages. Through the
use of controlled language by the authors, docu-
ments can be automatically translated more suc-
cessfully than with the use of free language. Style
checking should make authors aware of the con-
structions which should not be used, as well as
aiding in reformulating them. This can save a lot
of translation costs for companies producing large
amounts of mulitilingual documentation. Another
application of grammar and style checking is the
development of tutorial systems for learning a for-
eign language, as well as any kind of authoring sys-
tem for non-native speakers.
Previous approaches to grammar and style
checking can be divided into those based on fi-
nite state methods and those based on linguisti-
cally motivated grammars. To the former group be-
long e.g. the systems FLAG (Bredenkamp et al,
2000a; Bredenkamp et al, 2000b) and MultiLint
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
(Haller, 1996; Schmidt-Wigger, 1998). The basic
approach taken by such systems is the description
of error patterns through finite state automata. The
automata access the textual input enriched with
annotations from shallow linguistic analysis com-
ponents, such as part-of-speech tagging, morphol-
ogy and chunking. In FLAG, for instance, the an-
notation delivered by the shallow components is
integrated into a complex feature structure. Rules
are defined as finite state automata over feature
structures. The great advantages of such systems
are their robustness and efficient processing, which
make them highly suitable for real-life grammar
and style checking applications. However, since
shallow modules usually cannot provide a full syn-
tactic analysis, the coverage of these systems is
limited to error types not requiring a broader (non-
local) syntactic context for their detection. There-
fore their precision in the recognition of non-local
errors is not satisfactory.
Another short-coming of most shallow ap-
proaches to grammar checking is that they typi-
cally do not provide error correction: owing to the
absence of an integrated target grammar, genera-
tion of repairs cannot take the syntactic context
into account: as a result, some of the repairs sug-
gested by shallow systems are not globally well-
formed.
Grammar-based error checking constitutes the
other main strand in language checking technol-
ogy. These systems are typically equipped with a
model of target well-formedness. The main prob-
lem, when applied to the task of error checking
is that the sentences that are the focus of a gram-
mar checker are ideally outside the scope of the
grammar. To address this problem, grammar-based
checkers typically employ robustness techniques
(Ravin, 1988; Jensen et al, 1993; Douglas, 1995;
Menzel, 1998; Heinecke et al, 1998). The addi-
tion of robustness features, while inevitable for a
grammar-based approach, has the disadvantage of
considerably slowing down runtime performance.
Another issue with purely grammar-based check-
ing is related to the scarce distribution of actual
errors: thus, most effort is spent on the processing
of perfectly impeccable utterances. Finally, since
coverage of real-world grammars is never perfect,
these system also have difficulty to distinguish be-
153
tween extragrammatical and truly ungrammatical
sentences. Conversely, since grammars often over-
generate, a successful parse does not guarantee
wellformedness either.
One of the two major robustness techniques
used in the context of grammar-based language
checking are constraint relaxation (see e.g. (Dou-
glas, 1995; Menzel, 1998; Heinecke et al, 1998)),
which is typically realised by means of modifica-
tions to the parser (e.g. relaxation levels, robust uni-
fication). An alternative approach is error anticipa-
tion where errors are explicitly modelled by means
of grammar rules, so-called MAL-rules (McCoy et
al., 1996). This approach has already been inves-
tigated with an HPSG grammar, the ERG (Copes-
take and Flickinger, 2000), in the scenario of a tu-
torial system for language learning by (Bender et
al., 2004). We will follow this approach in the part
of our hybrid system based on deep processing.
Finite state methods and linguistically motivated
grammars are not only compatible, but also com-
plementary. Shallow methods are robust and effi-
cient, while deep processing based on grammars
provides high precision and detail. With the fo-
cussed application of deep analysis in finite state
based grammar and style checking systems, both
coverage and precision can be improved, while
the performance remains acceptable for real-world
applications. The combination of shallow and
deep components, hybrid processing, has already
been investigated in several modular architectures,
such as GATE (Gaizauskas et al, 1996), White-
board (Crysmann et al, 2002) and Heart-of-Gold
(Callmeier et al, 2004). Moreover, the improve-
ment in efficiency and robustness in deep process-
ing together with methods for its efficient applica-
tion makes the employment of deep processing in
real-world applications quite feasible. Hybrid pro-
cessing has been used for applications such as in-
formation extraction and question answering. But
to the best of our knowledge, the application of hy-
brid processing to grammar and style checking has
not been previously investigated.
In this paper, we present an implemented proto-
type of a hybrid grammar and style checking sys-
tem for German and English, called Checkpoint.
As the baseline shallow system we have taken
an industrial strength grammar and controlled lan-
guage style checker, which is based on the FLAG
technology. The deep processing platform used in
the project is the PET parser (Callmeier, 2000)
operating on wide-coverage English and German
HPSG grammars, the English Resource Grammar
(ERG) (Copestake and Flickinger, 2000) and the
German Grammar (GG) (Mu?ller and Kasper, 2000;
Crysmann, 2005; Crysmann, 2007), respectively.
The ERG and the GG have been developed for over
15 years and have already been used as deep pro-
cessing engines in the Heart-of-Gold hybrid pro-
cessing platform. We have developed an approach
for the selective application of deep processing
based on the error hypotheses of the shallow sys-
tem. Error detection in the deep system follows a
MAL-rule approach. In order to compare the ben-
efits of the selective application of deep process-
ing with its nonselective application, we have de-
veloped two scenarios: one parallel and one inte-
grated. While the parallel (nonselective) scenario
enables improvement in both recall and precision,
the integrated (selective) scenario only enables im-
provement in precision. However, the performance
of the integrated approach is much better. We have
also investigated several possibilities of integrating
deep processing in the selective scenario. Since the
HPSG grammars are suitable both for parsing and
generation, the system can successfully provide
both error corrections and paraphrases of stylistic
errors. For the purpose of investigation, evaluation
and statistical parse ranking, we have collected and
annotated several corpora of texts from technical
manuals. Finally, the approach has been evaluated
regarding error detection and performance.
2 The approach
Checkpoint has two main goals: (a) improving the
precision and recall of existing pattern-based gram-
mar and style checking systems for error types
whose detection requires considering more than
the strictly local syntactic context; and (b) gener-
ating error corrections for both grammar and style
errors. Accordingly, we have chosen to focus on
certain error types based on the difficulties of the
pattern-based system.
2.1 Anticipation of grammar errors
Grammar errors are detected by means of error
anticipation rules, or MAL-rules. MAL-rules ex-
actly model errors, so that erroneous sentences can
be parsed by the grammar. For this purpose we
enlarged two HPSG grammars for German, the
GG, and English, the ERG, with MAL-rules for
error types that were problematic for the pattern-
based shallow system. For German the following
phenomena have been handled: subject verb agree-
ment (subject verb agreement), NP internal agree-
ment (NP internal agreement), confusion of the
complementiser ?dass? with the homophonous pro-
noun or determiner ?das? (dass das), as well as
editing errors, such as local and non local repeti-
tion of words (repetitions). Here follow some ex-
amples (taken from the FLAG error corpus (Becker
et al, 2000), and die tageszeitung ?taz?, a German
newspaper):
(1) Auch in AOL gibt es Newsgroups, die
dieses Thema diskutiert [=diskutieren]. (FLAG)
Also in AOL are there newgroups, which (Pl)
this topic discuss (Sg).
?There are also newsgroups in AOL which dis-
cuss this topic.?
154
(2) Ich habe dem ganze [=ganzen] Geschehen
von meinem Sofa aus zugesehen. (FLAG)
I have the whole (wrong adj. form) events
from my couch out watched.
?I have watched the whole events from my
couch.?
(3) Vor allem im Su?den . . . fu?hrten [=haben] die
Liberalen der MR einen heftigen Wahlkampf
gegen die PS gefu?hrt. (taz, June 2007)
Above all in the south . . . led (past tense) the
liberals of the MR a hard election campaign
against the PS led (past participle).
?Particularly in the south, the liberals of the
MR led a hard election campaign against the
PS.?
For English, MAL-rules for errors concerning
subject verb agreement and missing determiners
were implemented.
2.2 Detection of stylistic errors
Stylistic errors are grammatical constructions that
are dispreferred in a particular register or type
of document. Sometimes certain constructions are
not desirable because machine translation systems
have problems dealing with them or because they
prevent easy understanding. In such cases a con-
trolled language approach is taken, where the prob-
lematic constructions are paraphrased into equiv-
alent less problematic constructions. Since these
constructions are grammatical they can be parsed
and, thus, detected. A generation of a paraphrase
is possible based on the semantic representation
obtained through parsing. For German the follow-
ing phenomena were handled: passive, future and
implicit conditional sentences, as in the following
example:
(4) Wartet man zulange, kriegt man keine Karten.
Waits one too long, gets one no tickets.
?If one waits too long one gets no tickets.?
Correct: Wenn man zulange wartet, kriegt
man keine Karten.
For English we focussed on the following phenom-
ena: passive (avoid passive), future (avoid future),
modal verbs (avoid modal verbs), subjunctive
(avoid subjunctive), stand-alone deictic pro-
nouns (use this that these those with noun) and
clause order in conditional sentences (condi-
tion must precede action).
2.3 Integrated vs. parallel scenarios
We have developed two integration scenarios: an
integrated one and a parallel one. In the parallel
scenario the pattern-based shallow system and the
deep processing parser run independently of each
other, that is, all sentences are parsed independent
of whether the shallow system has found an error
in them. In the integrated scenario the deep parser
is only called for those sentences where the shal-
low system has detected some error of the type
of those which Checkpoint is able to process (enu-
merated in subsection 2.1). The parallel scenario
allows improvement in the recall of the shallow
system, since Checkpoint can find errors that the
shallow system has not found. In the integrated
scenario, on the contrary, only the precision of the
shallow system can be improved, since Checkpoint
departs from the hypotheses of the shallow system.
The integrated scenario, however, promises to per-
form better in time than the parallel scenario, since
only a fraction of the whole text has to be scanned
for errors. Moreover, the performance of the inte-
grated system can also be improved with the se-
lective activation of the MAL-rules that model the
specific errors found by the shallow system. This
greatly reduces the enormous search space of the
parsing algorithms and the processing time result-
ing from the simultaneous processing of several
MAL-rules.
The integration of the shallow system and the
deep parser has been achieved through an exten-
sion of the PET parser that allows it to receive any
kind of input information and integrate this into
the chart. This preprocessing information can be,
for example, part-of-speech tagging, morphology
and lemmatisation, and already guides the parsing
process. It allows, for instance, recognition of un-
known words or identification of the correct lexi-
cal entry in cases where there is ambiguity. An in-
put format in terms of feature structures, the ?Fea-
ture Structure Chart? (FSC) format, has been devel-
oped for this purpose (Adolphs et al, 2008). The
shallow system, thus, produces a feature structure
chart, based on the information delivered by the
various shallow modules, and this information is
given as input to the PET deep parser, which reads
it and integrates it into the chart.
Error hypotheses from the shallow system are
passed to the deep parser by means of specific fea-
tures in the input feature structure (MAL-features)
of every input token in the FSC, permitting selec-
tive activation of MAL-rules. To this end, the origi-
nal FSC generated by the shallow system, which
contains information on the part-of-speech, the
lemma and morphological features such as num-
ber, gender and case, will be extended with MAL-
features. These MAL-features correspond to the
class of some MAL-rule in the grammar and have
boolean values. Signs in the grammar are speci-
fied for these MAL-features. MAL-rules are de-
fined such that they can only take as their daughters
edges with a positive value for the corresponding
MAL-feature. All information in the FSC input to-
kens is passed to the tokens in the chart through
a feature called TOKEN in lexical items. Thus, er-
ror hypotheses are passed from the input tokens to
the lexical items in the chart by stating that the val-
ues of the MAL-features in the lexical items are
155
equal to the values of the MAL-features in the cor-
responding input tokens in the FSC.
The values of the MAL-features are obtained
by checking the error report delivered by the shal-
low system. For certain errors detected by the shal-
low system there is a mapping to MAL-features.
The value of a MAL-feature will be set to ?+? if
the shallow system has found the corresponding
error. The rest of the MAL-features can be set to
?bool? if we want to allow other MAL-rules to
fire (which can improve recall, but increases am-
biguity and, consequently, has a negative effect on
performance). The values of the rest of the MAL-
features can also be set to ?-?, if we want to prevent
other MAL-rules from firing (which allows im-
provement only in precision, but limits ambiguity
and, consequently, results in better performance).
There is also the possibility of activating the rel-
evant MAL-features only for those tokens which
are, according to the shallow system, within the er-
ror span, instead of activating the MAL-features
for all tokens in the erroneous sentence.
2.4 Generation of corrections and
paraphrases
One of the advantages of using deep processing
in grammar and style checking is the possibility
of generating corrections and paraphrases which
obey the constraints imposed by the syntactic con-
text. Since the HPSG grammars that we are using
are suitable both for parsing and generation, this
is straightforward. Robust parsing delivers as out-
put a semantic representation in the Minimal Re-
cursion Semantics formalism (MRS) (Copestake
et al, 2006) of the sentence which can be used for
generation with the LKB (Carroll et al, 1999).
The MAL-rules directly assign well-formed se-
mantic representations from which a correct sur-
face string can be generated. In the case of stylis-
tic errors, transfer rules are used to generate the
desired paraphrase, using MRS-to-MRS mapping
rules modelled on the semantic transfer-based ma-
chine translation approach of (L?nning et al,
2004).
We identified two areas where generation of re-
pairs will actually provide a considerable added
value to a grammar checking system: first, for non-
native speakers, simple highlighting of the error
location is often insufficient, since the user may
not be familiar with the rules of the language. Sec-
ond, some areas, in particular stylistic ones may
involve considerable rearrangement of the entire
sentence. In these cases, generation of repairs and
paraphrases can reduce editing cost and also min-
imise the issue of editing errors associated with
non-local phenomena.
The generator and HPSG grammars we use are
able to provide a range of realisations for a given
semantic input. As a result, realisation ranking is
of utmost importance. In order to select repairs
which are both smooth and maximally faithful to
the input, modulo the error site, of course, we com-
bined two methods: a discriminative PCFG-model
trained on a generation treebank, enhanced by an
n-gram language model, cf. (Velldal and Oepen,
2005), and an alignment approach that chooses the
most conservative edit from a set of input realisa-
tions. As our similarity measure, we employed a
variant of BLEU score (NEVA), suggested in (Fors-
bom, 2003). The probabilistic ranking models we
trained achieve an exact match accuracy of 73%
for both English (Velldal and Oepen, 2005) and
German (as evaluated on the subset of TiGer the
error corpus was based on).
3 Error corpora
In order to learn more about the frequencies of the
different error types, to induce statistical models
that allow us to obtain the best parse in the do-
main of technical manuals and to evaluate our im-
plemented approach to grammar and style check-
ing, we collected and manually annotated corpora
from the domain of technical documentation.
Since errors in pre-edited text tend to be very
scarcely distributed, manual annotation is quite
costly. As a result, instance of certain well-known
error types cannot be tested in a greater variety of
linguistic environments. To overcome this problem,
we semi-automatically derived an additional error
corpus from a treebank of German.
English For purposes of evaluation in a real
world scenario, we constructed a corpus for En-
glish, consisting of 12241 sentences (169459
words) from technical manuals. The corpus was
semi-automatically annotated with several types of
grammar and style errors. For this purpose annota-
tion guidelines were developed, which contained
the description of the errors together with exam-
ples of each and their possible corrections. The an-
notation took place in two phases. First, we wanted
to find out about the precision of the shallow sys-
tem, so we ran the shallow system over the data.
This resulted in an annotation for each error found
consisting of the erroneous sentence, the error span
and the type of error. The annotators, who were na-
tive speakers, then decided whether the errors had
been correctly detected. In the second phase, we
aimed to create a gold standard, so as to be able to
evaluate both the shallow system and Checkpoint
regarding recall and precision. For this purpose, we
extracted the errors that had been annotated as cor-
rectly detected in the previous phase and the an-
notators only had to find the non-detected errors
in the rest of the corpus. For the latter, they also
marked the span and identified the error type.
Subsets of these two datasets were treebanked
with the corresponding HPSG grammars. We em-
ployed the treebanking methodology developed for
Redwoods (Oepen et al, 2002), which involved
156
first parsing a corpus and recording for each item
the alternative analyses (the parse forest) assigned
by the grammar, then manually identifying the cor-
rect analysis (if available) within that parse forest.
This approach provides both a gold standard syn-
tactic/semantic analysis for each parsed item, and
positive and negative training data for building an
accurate statistical model for automatic parse selec-
tion.
German For German, we pursued a complemen-
tary approach towards corpus construction. Here
the focus lay on creating a test and evaluation cor-
pus that provided instances of common error types
in a variety of linguistic contexts. Since manual
error annotation is highly costly, owing to scarce
error distributions in pre-edited text, we chose to
automatically derive an error corpus from an ex-
isting treebank resource. As for the error types, we
focussed on those errors which are arguably perfor-
mance errors, as e.g. missing final consonants in in-
flectional endings, the confusion of homophonous
complementiser and relative pronoun, or else, edit-
ing errors, such as local and non-local duplicates.
We introduced instances of errors in a sub-
corpus of the German TiGer treebank (Brants
et al, 2002), nicknamed TiG-ERR, consisting of
77275 words (5652 sentences) from newspaper
texts. All the sentences in this subcorpus were
parsable, so that an evaluation of Checkpoint in
the ideal situation of 100% coverage could be car-
ried out. The artificially introduced errors were
of the following types: subject verb agreement,
NP internal agreement, dass/das, and repetitions,
all of them already illustrated with examples in sec-
tion 2.1.
Additionally, we annotated a corpus of technical
documents for these error types to estimate the dis-
tribution of these error types in pre-edited text.
4 Error models
In order to construct a statistical parse-ranking
model which could determine the intended use of
a MAL-rule in the analysis of a sentence where the
grammar produced analyses both with and without
MAL-rules, the English treebank was constructed
using the version of the ERG which included the
MAL-rules. 4000 sentences from the English cor-
pus were presented to the parser, of which 86.8%
could be parsed with the ERG, and of these, the an-
notators found an intended analysis for 2500 sen-
tences, including some which correctly used MAL-
rules. From these annotations, a customised parse
selection model was computed and then used in
parsing all of the corpus, this time recording only
the one analysis determined to be most likely ac-
cording to this model. We also compared accu-
racy of error detection based on this new model
with the accuracy of a pre-existing parse-selection
model trained on tourism data for LOGON, and
confirmed that the new model indeed improved
over the old one.
For German, we have not created a specific sta-
tistical model yet, but, instead, we have used an ex-
isting parse selection model (Crysmann, 2008) and
combined it with some heuristics which enable us
to select the best error hypothesis. The heuristics
check for each parsed sentence whether there is an
analysis containing no MAL-rule. If there is one
and this is not ranked as the best parse, it is moved
to the first position in the parse list. As a result, we
can eliminate a high percentage of false alarms.
5 Evaluation results
We have evaluated the English and the German ver-
sions of Checkpoint against the corpora described
in section 3.
German For German we have taken as a test
corpus standard the TiG-ERR subcorpus contain-
ing the automatically introduced errors, and have
parsed all its sentences. The following table shows
the frequencies of the different types of handled er-
rors in the corpus of technical manuals, the FLAG
error corpus (Becker et al, 2000), and in the TiG-
ERR corpus. The electronic version of the FLAG
corpus consists of 14,492 sentences, containing
1,547 grammar or style errors.
ERROR TYPE MANUALS FLAG TiG-ERR
NP internal agr 119 180 2258
subject verb agr 17 63 748
dass/das 1 152 75
repetitions 19 n/a 2571
Table 1: Frequencies of the error types for German
The following charts show the values for recall
and precision for the shallow system and Check-
point. As you can see, Checkpoint improves the
recall for the error types subject verb agreement
and NP internal agreement, whereas the precision
remains more or less the same. For the error type
dass/das Checkpoint improves both recall and pre-
cision. For the error type repetitions, which is only
partially handled by the spell checker in the shal-
low system, Checkpoint reaches considerable re-
call and precision values.
Deep processing on average improves the recall
of the shallow system by 21% and the precision
remains equal at 0.83. According to the error fre-
quencies in the corpus of technical manuals, deep
processing would improve the recall of the shallow
system by only 1.7%, since the error types sub-
ject verb agreement, NP internal agreement and
dass/das only make up 6.57% of the total amount
of annotated errors. However, as we found out later,
the corpora of technical manuals consist of texts
that have already undergone correction, so the er-
rors are very sparse.
157
Figure 1: Checkpoint values for recall and preci-
sion for German
Figure 2: Values for recall and precision for the
shallow system for German
Through the MAL-rules the coverage of the GG
on the TiG-ERR corpus increased to 85% - 95%,
whereas without the MAL-rules the coverage was
10%. This 10% coverage included overgeneration
by the grammar, as well as sentences that, after the
automatic insertion of errors, still remained gram-
matical, although they didn?t express the intended
meaning any more.
The performance of the parallel and integrated
scenarios was compared. The ambiguity of the
MAL-rules, that is, the possibility of applying sev-
eral MAL-rules to a unique error, considerably de-
teriorates the performance when processing sen-
tences containing several errors. In a subcorpus
containing NP internal agreement errors, the aver-
age processing time per sentence increases from
8.3 seconds with the selective activation of MAL-
rules to 31.4 seconds with the activation of all
MAL-rules. Particularly the MAL-rules modeling
the error subject verb agreement are a source of
ambiguity. If these MAL-rules are only selectively
activated the average processing time per sentence
decreases to 14.9 seconds.
Finally, we have evaluated the performance of
the German grammar in the task of error correction,
using non-local duplicates and adjectival agree-
ment errors as a test bed. For these error types,
the German HPSG grammar generated repairs for
85.4% of the detected non-local duplicates and
90% of the detected agreement errors.
English For English we have only implemented
and evaluated the parallel scenario. The focus for
English evaluation was the recognition of those
stylistic errors whose correction requires a re-
structuring of the sentence, and the generation of
the corresponding paraphrases. The recognition of
such error types is not based on MAL-rules, but
on certain already existing rules in the grammar.
The approach was evaluated taking the manually
annotated English corpus of technical manuals as
a gold standard. The following table shows the fre-
quencies of the error types handled by Checkpoint.
ERROR TYPE OCCURRENCES
avoid future 404
avoid modal verbs 657
avoid passive 213
Table 2: Frequencies of the error types for English
The PET parser with the ERG reached 86.1%
coverage on the full corpus. The following charts
show the values for recall and precision for Check-
point and the shallow system.
Figure 3: Checkpoint values for recall and preci-
sion for English
Figure 4: Values for recall and precision for the
shallow system for English
As one can see, for the stylistic errors
avoid future and avoid modal verbs, Checkpoint
reaches values which, although relatively high, are
lower than the shallow system. In most cases a
paraphrase for these errors can be constructed,
so the improvement Checkpoint provides here is
the generation of corrections. For the error type
avoid passive the precision is not so high, which
is due in part to mistakes in the manual annotation.
The passive sentences found by Checkpoint are
actually passive sentences. However, these were
158
not annotated as passives, because the annotators
were told to annotate only those stylistic errors
for which a paraphrase was possible. The same
happens for stylistic errors like avoid subjunctive,
use this that these those with noun and condi-
tion must precede action. In principle, Check-
point is very good at finding these types of errors,
but we cannot yet present a reliable evaluation here,
since only those errors were annotated for which
a paraphrase was possible. This approach is rea-
sonable, since no error alarm should be produced
when there is no other possibility of expressing the
same. However, since we have not yet developed
a method which allows us to automatically distin-
guish those cases for which a paraphrase is possi-
ble from those for which none is, we would need
to annotate all occurrences of a phenomenon in the
corpus, and introduce a further annotation tag for
the paraphrase potential of the sentence.
Nevertheless, even if the grammar-based re-
search prototype cannot beat the industrial pattern-
based system in terms of f-measures, we still be-
lieve that the results are highly valuable in the con-
text of our integrated hybrid scenario: Since the
full reversibility of the ERG has already been estab-
lished independently by (Velldal and Oepen, 2005),
the combined system is able to generate error cor-
rection for a great proportion of the errors detected
by the shallow component. This includes 80% and
above for avoid future and avoid modal verbs.
6 Summary and conclusions
In this paper we have presented an implemented
approach to grammar and style checking based on
hybrid processing. The hybrid system has two com-
ponents: a shallow grammar and style checking
system based on the FLAG technology, and the
PET deep parser operating on linguistically moti-
vated grammars for German and English. The Ger-
man version of the hybrid system improves the re-
call and in certain cases the precision of the shal-
low system and generates error corrections. For
English, the hybrid system in most cases success-
fully generates paraphrases of sentences contain-
ing stylistic errors. Although we only have ex-
plored some of the possibilities of integrating deep
and shallow processing for the grammar and style
checking application, these results speak for the
feasibility of using hybrid processing in this task.
We have developed an integrated strategy which
forwards the output of the shallow system, includ-
ing both the output from several pre-processing
linguistic modules and the error hypotheses, as in-
put to the deep parser. This procedure not only im-
proves the robustness of the deep parser with the
recognition of unknown words and reduces ambi-
guity by instantiating only those lexical items con-
sistent with the hypotheses of the POS tagger or
the morphology; but it also allows the selective
application of grammar rules, which considerably
reduces the search space for parsing and, conse-
quently, improves performance. Based on the error
hypotheses of the shallow system, the selective ap-
plication of grammar rules is achieved by positing
features in the Feature Structure Chart whose par-
ticular values are a pre-condition for MAL-rules
to apply. The improvement in performance sug-
gests that this strategy can be extensible to parsing
in general based on pre-processing components.
Given the output of a chunker, for example, certain
syntactic configurations can already be excluded.
Having features whose values allow one to switch
off certain rules not compatible with these con-
figurations would considerably reduce the search
space.
On the other hand, we have run the two mod-
ules independently from each other to find out
how the recall of the shallow system can be im-
proved by deep processing. The fact that for sev-
eral error types, such as subject verb agreement
and NP internal agreement, recall can be consider-
ably improved suggests that, in order not to parse
all sentences, the shallow system should send an
error hypothesis to the deep system when finding
particular syntactic configurations which may indi-
cate the occurrence of such errors. In this way, such
error hypotheses, although not reliably detectable
by the shallow system alone, could be confirmed
or discarded with a focussed application of deep
processing, which would not be as resource con-
suming as parsing every sentence.
One of the results of the experiment has been
an on-line demonstration system. The running sys-
tem shows that the different modules can be eas-
ily combined with each other. Our hybrid approach,
however, is generic and portable. Although imple-
mented for our specific baseline system, it can in
principle be used with other shallow systems.
Acknowledgements
The research reported in this paper has been car-
ried out as part of the DFKI project Checkpoint,
running from February until November 2007. The
project was funded by the ProFIT program of the
Federal State of Berlin and the EFRE program of
the European Union.
References
Adolphs P., S. Oepen, U. Callmeier, B. Crysmann, and
B. Kiefer. 2008. Some Fine Points of Hybrid Natural
Language Parsing. Proceedings LREC-2008, Mar-
rakech, Morocco.
Becker M., A. Bredenkamp, B. Crysmann, and J. Klein.
2003. Annotation of error types for a German news-
group corpus. In A. Abeille?, editor, Treebanks.
Building and Using Parsed Corpora, number 20 in
Text, Speech And Language Technology. Kluwer,
Dordrecht.
159
Bender, E. M., D. Flickinger, S. Oepen, A. Walsh,
and T. Baldwin. 2004. Arboretum: Using a preci-
sion grammar for grammar checking in call. In In-
STIL/ICALL symposium 2004. NLP and speech tech-
nologies in advanced language learning systems.
Venice, Italy.
Brants, T., S. Dipper, S. Hansen, W. Lezius, and G.
Smith. 2002. The TIGER Treebank. In Proceedings
of the Workshop on Treebanks and Linguistic Theo-
ries. Sozopol.
Bredenkamp, A., B. Crysmann and M. Petrea. 2000.
Looking for errors: A declarative formalism for
resource-adaptive language checking. In Proceed-
ings LREC-2000. Athens, Greece.
Bredenkamp, A., B. Crysmann and M. Petrea. 2000.
Building multilingual controlled language perfor-
mance checkers. In Proceedings of the CLAW 2000.
Seattle, WA.
Callmeier, U., A. Eisele, U. Scha?fer, and M. Siegel.
2004. The Deepthought core architecture frame-
work. In Proceedings of LREC-2004, 1205?1208,
Lisbon, Portugal.
Callmeier, Ulrich. 2000. PET ? a platform for ex-
perimentation with efficient HPSG processing tech-
niques. Natural Language Engineering 6(1):99?
108.
Carroll, John and Ann Copestake and Dan Flickinger
and Victor Poznanski. 1999. An efficient chart gen-
erator for semi-lexicalist grammars. Proceedings of
ENLG, pp. 86?95.
Copestake, A., and D. Flickinger. 2000. An open-
source grammar development environment and
broad-coverage english grammar using HPSG. In
Proceedings LREC-2000. Athens, Greece.
Copestake, A., D. Flickinger, C. Pollard, and I. Sag.
2006. Minimal recursion semantics: an introduction.
Research on Language and Computation 3(4):281?
332.
Crysmann, B., A. Frank, B. Kiefer, S. Mu?ller, G. Neu-
mann, J. Piskorski, U. Scha?fer, M. Siegel, H. Uszko-
reit, F. Xu, M. Becker, and H.-U. Krieger. An inte-
grated architecture for shallow and deep processing.
In Proceedings of ACL 2002, University of Pennsyl-
vania, Philadelphia, 2002.
Crysmann B. 2005. Relative clause extraposition in
German: An efficient and portable implementation.
Research on Language and Computation, 3(1):61?
82.
Crysmann B. 2007 Local ambiguity packing and dis-
continuity in German. In T. Baldwin, M. Dras,
J. Hockenmaier, T. H. King, and G. van Noord, ed-
itors, Proceedings of the ACL 2007 Workshop on
Deep Linguistic Processing, pages 144?151, Prague,
Czech Republic.
Crysmann B. 2008. Parse Selection with a German
HPSG Grammar. In S. Ku?bler and G. Penn, editors,
Proceedings of the ACL 2008 Workshop on Parsing
German (PaGe), pages 9?15, Columbus, Ohio, USA.
Douglas, S. 1995. Robust PATR for error detec-
tion and correction. In A. Schoeter and C. Vogel
(Eds.)Nonclassical feature systems, Vol. 10, pp. 139-
156. Centre for Cognitive Science, University of Ed-
inburgh.
Forsbom, E. 2003. Training a Super Model Look-
Alike. Proceedings of the MT Summit IX Workshop
?Towards Systemizing MT Evaluation?, pp. 29-36.
Gaizauskas, R., H. Cunningham, Y. Wilks, P. Rodgers
and K. Humphreys. 1996. GATE: An environment
to support research and development in natural lan-
guage engineering. In Proceedings of the 8th IEEE
international conference on tools with artificial in-
telligence. Toulouse, France.
Jensen, K., G. E., Heidorn and S. D. Richardson (Eds.).
1993. Natural language processing: The PLNLP ap-
proach. Boston - Dordrecht - London.
Haller, J. 1996. MULTILINT: A technical documenta-
tion system with multilingual intelligence. In Trans-
lating and the computer 18. London.
Heinecke, J., J. Kunze, W. Menzel, and I. Schroeder.
1998. Eliminative parsing with graded constraints.
In Proceedings ACL/Coling 1998, Vol. I, pp. 526-
530. Universite de Montreal, Montreal, Quebec,
Canada.
L?nning J. T. , S. Oepen, D. Beermann, L. Hellan, J.
Carroll, H. Dyvik, D. Flickinger, J. B. Johannessen,
P. Meurer, T. Nordga?rd, V. Rose?n and E. Velldal.
2004. LOGON. A Norwegian MT effort. In Pro-
ceedings of the Workshop in Recent Advances in
Scandinavian Machine Translation. Uppsala, Swe-
den.
McCoy, K. F., C. A. Pennington, and L. Z. Suri. 1996.
English error correction: A syntactic user model
based on principled ?mal-rule? scoring. In Proceed-
ings of UM-96, the Fifth International Conference on
User Modeling, pp. 59-66. Kona, Hawaii.
Menzel, W. 1998. Constraint satisfaction for robust
parsing of natural language. In Theoretical and Ex-
perimental Artificial Intelligence, 10 (1), 77-89.
Mu?ller, S., and W. Kasper. 2000. HPSG analysis of
German. In W. Walster (Ed.), Verbmobil: Foun-
dations of Speech-to-Speech Translation, 238?253
Springer, Berlin.
Oepen, S., K. Toutanova, S. Shieber, C. Manning, D.
Flickinger and T Brants. 2002. The LinGO Red-
woods Treebank. Motivation and Preliminary Appli-
cations. In Proceedings of COLING 2002. Taipei,
Taiwan.
Ravin, Y. 1998. Grammar errors and style weaknesses
in a text-critiquing system. In IEEE Transactions on
Communication, 31 (3)
Schmidt-Wigger, A. 1998. Grammar and style check-
ing for German. In Proceedings of CLAW 98. Pitts-
burgh, PA.
Velldal, E. and S. Oepen. 2005. Maximum entropy
models for realization ranking. In Proceedings of
the 10th MT-Summit (X), Phuket, Thailand.
160
Coling 2010: Poster Volume, pages 570?578,
Beijing, August 2010
Using Syntactic and Semantic based Relations for Dialogue Act
Recognition
Tina Klu?wer, Hans Uszkoreit, Feiyu Xu
Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (DFKI)
Projektbu?ro Berlin
{tina.kluewer,uszkoreit,feiyu}@dfki.de
Abstract
This paper presents a novel approach to
dialogue act recognition employing multi-
level information features. In addition to
features such as context information and
words in the utterances, the recognition
task utilizes syntactic and semantic rela-
tions acquired by information extraction
methods. These features are utilized by
a Bayesian network classifier for our dia-
logue act recognition. The evaluation re-
sults show a clear improvement from the
accuracy of the baseline (only with word
features) with 61.9% to an accuracy of
67.4% achieved by the extended feature
set.
1 Introduction
Dialogue act recognition is an essential task for
dialogue systems. Automatic dialogue act clas-
sification has received much attention in the past
years either as an independent task or as an em-
bedded component in dialogue systems. Various
methods have been tested on different corpora us-
ing several dialogue act classes and information
coming from the user input.
The work presented in this paper is part of a
dialogue system called KomParse (Klu?wer et al,
2010), which is an application of a NL dialogue
system combined with various question answering
technologies in a three-dimensional virtual world
named Twinity, a web-based online product of the
Berlin startup company Metaversum1. The Kom-
Parse NPCs provide various services through con-
1http://www.metaversum.com/
versation with game users such as selling pieces
of furniture to users via text based conversation.
The main task of the input interpretation com-
ponent of the agent is the detection of the dialogue
acts contained in the user utterances. This classi-
fication is done via a cue-based method with var-
ious features from multi-level knowledge sources
extracted from the incoming utterance considering
a small context of the previous dialogue.
In contrast to existing systems using mainly
lexical features, i.e. words, single markers such as
punctuation (Verbree et al, ) or combinations of
various features (Stolcke et al, 2000) for the dia-
logue act classification, the results of the interpre-
tation component presented in this paper are based
on syntactic and semantic relations. The system
first gathers linguistic information coming from
different levels of deep linguistic processing sim-
ilar to (Allen et al, 2007). The retrieved informa-
tion is used as input for an information extraction
component that delivers the relations embedded in
the actual utterance (Xu et al, 2007). These rela-
tions combined with additional features (a small
dialogue context and mood of the sentence) are
then utilized as features for the machine-learning
based recognition.
The classifier is trained on a corpus originating
from a Wizard-of-Oz experiment which was semi-
automatically annotated. It contains automatically
annotated syntactic relations namely, predicate ar-
gument structures, which were checked and cor-
rected manually afterwards. Furthermore these re-
lations are enriched by manual annotation with se-
mantic frame information from VerbNet to gain an
additional level of semantic richness. These two
representations of relations, the syntax-based re-
570
lations and the VerbNet semantic relations, were
used in separate training steps to detect how much
the classifier can benefit from either notations.
A systematic analysis of the data has been con-
ducted. It turns out that a comparatively small set
of syntactic relations cover most utterances, which
can moreover be expressed by an even smaller set
of semantic relations. Because of this observation
as well as the overall performance of the classifier
the interpretation is extended with an additional
rule based approach to ensure the robustness of
the system.
The paper is organized as follows: Section 2
provides an overview about existing dialogue act
recognition systems and the features they use for
classification.
Section 3 introduces the original data used as ba-
sis for the annotation and the classification task.
In Section 4 the annotation that provides the nec-
essary information for the dialogue act classifi-
cation and involves the relation extraction is de-
scribed in detail. The annotation is split into three
main steps: The annotation of dialogue informa-
tion (section 4.1), the integration of syntactic in-
formation (section 4.2) and finally the manual an-
notation of VerbNet predicate and role informa-
tion in section 4.3.
Section 5 presents the results of the actual classifi-
cation task using different feature sets and in Sec-
tion 6 the results and methods are summarized.
Finally, Section 7 provides a brief description of
the rule-based interpretation and presents an out-
look on future work.
2 Related Work
Dialogue Acts (DAs) represent the functional
level of a speaker?s utterance, such as a greeting,
a request or a statement. Dialogue acts are ver-
bal or nonverbal actions that incorporate partic-
ipant?s intentions originating from the theory of
Speech Acts by Searle and Austin (Searle, 1969).
They provide an abstraction from the original in-
put by detecting the intended action of an utter-
ance, which is not necessarily inferable from the
surface input (see the two requests in the follow-
ing example).
Can you show me a red car please?
Please show me a red car!
To detect the action included in an utterance,
different approaches have been suggested in re-
cent years which can be clustered into two main
classes: The first class uses AI planning methods
to detect the intention of the utterance based on
belief states of the communicating agents and the
world knowledge. These systems are often part of
an entire dialogue system e.g. in a conversational
agent which provides the necessary information
about current beliefs and goals of the conversa-
tion participants at runtime. One example is the
TRIPS system (Allen et al, 1996). Because of the
huge amount of reasoning, systems in this class
generally gather as much linguistic information as
possible.
The second class uses cues derived from the
actual utterance to detect the right dialogue act,
mostly using machine learning methods. This
class gained much attention due to less computa-
tional costs. The probabilistic classifications are
carried out via training on labeled examples of
dialogue acts described by different feature sets.
Frequently used cues for dialogue acts are lexi-
cal features such as the words of the utterance or
ngrams of words for example in (Verbree et al,
), (Zimmermann et al, 2005) or (Webb and Liu,
2008). Although the performance of the classi-
fication task is difficult to compare, because of
the variety of different corpora, dialogue act sets
and algorithms used, these approaches do pro-
vide considerably good results. For example (Ver-
bree et al, ) achieve accuracy values of 89% on
the ICSI Meeting Corpus containing 80.000 ut-
terances with a dialogue act set of 5 distinct di-
alogue act classes and amongst others the features
?ngrams of words? and ?ngrams of POS informa-
tion?.
Another group of systems utilizes acoustic fea-
tures derived from Automatic Speech Recognition
for automatic dialogue act tagging (Surendran and
Levow, 2006), context features like the preceding
dialogue act or ngrams of previous dialogue acts
(Keizer and Akker, 2006).
However grammatical and semantic informa-
tion is not that often incorporated into feature sets,
with the exception of single features such as the
571
Dialogue Act Meaning Frequency
REQUEST The utterance contains a wish or demand 449
REQUEST INFO The utterance contains a wish or demand regarding information 154
PROPOSE The utterance serves as suggestion or showing of an object 216
ACCEPT The utterance contains an affirmation 167
REJECT The utterance contains a rejection 88
PROVIDE INFO The utterance provides an information 156
ACKNOWLEDGE The utterance is a backchannelling 9
Table 1: The used Dialogue Act Set
type of verbs or arguments or the presence or ab-
sence of special operators e.g. wh-phrases (An-
dernach, 1996). (Keizer et al, 2002) use among
others linguistic features like sentence type for
classification with Bayesian networks. Although
(Jurafsky et al, 1998) already noticed a strong
correlation between selected dialogue acts and
special grammatical structures, approaches using
grammatical structure were not very succesful.
While grammatical and semantic features are
not often incorporated into dialogue act recogni-
tion, they are a commonly used in related fields
like automatic classification of rhetorical rela-
tions. For example (Sporleder and Lascarides,
2008) and (Lapata and Lascarides, 2004) extract
verbs as well as their temporal features derived
from parsing to infer sentence internal temporal
and rhetorical relations. Their best model for
analysing temporal relations between two clauses
achieves 70.7% accuracy. (Subba and Eugenio,
2009) also show a significant improvement of a
discourse relation classifier incorporating compo-
sitional semantics compared to a model without
semantic features. Their VerbNet based frame se-
mantics yield in a better result of 4.5%.
3 The Data
The data serving as the basis for the relation iden-
tification as well as the training corpus for the di-
alogue act classifier is taken from a Wizard-of-Oz
experiment (Bertomeu and Benz, 2009) in which
18 users furnish a virtual living room with the help
of a furniture sales agent. Users buy pieces of fur-
niture and room decoration from the agent by de-
scribing their demands and preferences in a text
chat. During the dialogue with the agent, the pre-
ferred objects are then selected and directly put to
the right location in the apartment. In the exper-
iments, users spent one hour each on furnishing
the living room by talking to a human wizard con-
trolling the virtual sales agent. The final corpus
consists of 18 dialogues containing 3,171 turns
with 4,313 utterances and 23,015 alpha-numerical
strings (words). The following example shows a
typical part of such a conversation:
USR.1: And do we have a little side table for the TV?
NPC.1: I could offer you another small table or a side-
board.
USR.2: Then I?ll take a sideboard that is similar to my
shelf.
NPC.2: Let me check if we have something like that.
Table 2: Example Conversation from the Wizard-
of-Oz Experiment
4 Annotation
The annotation of the corpus is carried out in sev-
eral steps.
4.1 Pragmatic Annotation
The first annotation step consists of annotating
discourse and pragmatic information including di-
alogue acts, projects according to (Clark, 1996),
sentence mood, the topic of the conversation and
an automatically retrieved information state for
every turn of the conversations. From the anno-
tated information the following elements were se-
lected as features in the final recognition system:
? The dialogue acts which carry the intentions
of the actual utterance as well as the last pre-
ceding dialogue act. The set used for anno-
tation is a domain specific set containing the
dialogue acts shown in table 1.
? The sentence mood. Sentence mood was
annotated with one of the following values:
declarative, imperative, interrogative.
572
? The topic of the utterance. The topic value
is coreferent with the currently discussed ob-
ject. Topic can consist of an object class
(e.g. sofa) or an special object instance
(sofa 1836). The topic of the directly pre-
ceding utterance was chosen as a feature too.
4.2 Annotation with Predicate Argument
Structure
The second annotation step, applied to the ut-
terance level of the input, automatically enriches
the annotation with predicate argument structures.
Each utterance is parsed with a predicate argu-
ment parser and annotated with syntactic relations
organized according to PropBank (Palmer et al,
2005) containing the following features: Predi-
cate, Subject, Objects, Negation, Modifiers, Cop-
ula Complements.
A single relation mainly consists of a predi-
cate and the belonging arguments. Verb modi-
fiers like attached PPs are classified as ?argM?
together with negation (?argM neg?) and modal
verbs (?argM modal?). Arguments are labeled
with numbers according to the found information
for the actual structure. PropBank is organized in
two layers, the first one being an underspecified
representation of a sentence with numbered argu-
ments, the second one containing fine-grained in-
formation about the semantic frames for the predi-
cate comparable to FrameNet (Baker et al, 1998).
While the information in the second layer is sta-
ble for each verb, the values of the numbered ar-
guments can change from verb to verb. While
for one verb the ?arg0? may refer to the subject
of the verb, another verb may encapsulate a di-
rect object behind the same notation ?arg0?. This
is very complicated to handle in a computational
setup, which needs continuous labeling for the
successive components. Therefore the arguments
were in general named as in PropBank but con-
sistently numbered by syntactic structure. This
means for example that the subject is always la-
beled as ?arg1?.
Consider the example ?Can you put posters or
pictures on the wall??. The syntactic relation will
yield in the following representation:
<predicate: put>
<ArgM_modal: can>
<Arg1: you>
<Arg2: posters or pictures>
<ArgM: on the wall>
Predicate Argument Structure Parser The
syntactic predicate argument structure that consti-
tutes the syntactic relations and serves as basis for
the VerbNet annotation, is automatically retrieved
by a rule-based predicate argument parser. The
rules utilized by the parser describe subtrees of de-
pendency structures in XML by means of relevant
grammatical functions. For detecting verbs with
two arguments in the input, for instance, a rule
can be written describing the dependency struc-
ture for a verb with a subject and an object. This
rule would then detect every occurrence of the
structure ?Verb-Subj-Obj? in a dependency tree.
This sample rule would express the following con-
straints: The matrix unit should be of the part of
speech ?Verb? , The structure belonging to this
verb must contain a ?nsubj? dependency and an
?obj? dependency.
The rules deliver raw predicate argument struc-
tures, in which the detected arguments and the
verb serve as hooks for further information lookup
in the input. If a verb fulfills all requirements
described by the rule, in a second step all modi-
ficational arguments existing in the structure are
recursively acquired. The same is done for
modal arguments as well as modifiers of the ar-
guments such as determiners, adjectives or em-
bedded prepositions. After the generation of the
main predicate argument structure from the gram-
matical functions, the last step inserts the content
values present in the actual input into the structure
to get the syntactic relations for the utterance.
Before the input can be parsed with the predi-
cate argument parser, some preprocessing steps of
the corpus are needed. These include:
Input Cleaning The input data coming from the
users contain many errors. Some string
substitutions as well as the external Google
spellchecker were applied to the input before
any further processing.
Segmentation For clausal separation we apply a
simple segmentation via heuristics based on
punctuation.
POS Tagging Then the input is processed by
573
the external part-of-speech tagger TreeTag-
ger (Schmid, 1994).
The embedded dependency parser is the Stan-
ford Dependency Parser (de Marneffe and Man-
ning, 2008), but other dependency parsers could
be employed instead. The predicate argument
parser is an standalone software and can be used
either as a system component or for batch process-
ing of a text corpus.
4.3 VerbNet Frame Annotation
The last step of annotation consists of the man-
ual annotation of semantic predicate classes and
semantic roles. Moreover, the automatically de-
termined syntactic relations are checked and cor-
rected if possible. VerbNet (Schuler, 2005) is uti-
lized as a source for semantic information. The
VerbNet role set consists of 21 general roles used
in all VerbNet classes. Examples of roles in
this general role set are ?agent?, ?patient? and
?theme?.
For the manual addition of the semantic frame
information a web-based annotation tool has been
developed. The annotation tool shows the utter-
ance which should be annotated in the context of
the dialogue including the information from the
preceding annotation steps. All VerbNet classes
containing the current predicate are listed as pos-
sibilities for the predicate classification together
with their syntactic frames. The annotators can se-
lect the appropriate predicate class and frame ac-
cording to the arguments found in the utterance.
If an argument is missing in the input that is re-
quired in the selected frame a null argument is
added to the structure. If the right predicate class
is existing, but the predicate is not yet a member
of the class, it is added to the VerbNet files. In
case the right predicate class is found but the fit-
ting frame is missing, the frame is added to the
VerbNet files. Thus during annotation 35 new
members have been added to the existing VerbNet
classes, 4 Frames and 4 new subclasses. Via these
modifications, a version of VerbNet has been de-
veloped that can be regarded as a domain-specific
VerbNet for the sales domain.
During the predicate classification, the annota-
tors also assign the appropriate semantic roles to
the arguments belonging to the selected predicate.
The semantic roles are taken from the selected
VerbNet frame.
From the annotated semantic structure, seman-
tic relations are inferred such as the one in the fol-
lowing example:
<predicate: put-3.1>
<agent: you>
<theme: posters or pictures>
<destination: on the wall>
5 Dialogue Act Recognition
Two datasets are derived from the corpus: The
dataset containing the utterances of the users
(CST) and one dataset containing the utterances
of the wizard (NPC), whereas the NPC corpus is
cleaned from the ?protocol sentences?. Protocol
sentences are canned sentences the wizard used
in every conversation, for example to initialize
the dialogue. For the experiments, the two sin-
gle datasets ?NPC? and ?CST? as well as a com-
bined dataset called ?ALL? are used. Unfortu-
nately from the original 4,313 utterances in total,
many utterances could not be used for the final ex-
periments. First, fragments are removed and only
the utterances found by the parser to contain a
valid predicate argument structure are used. After
protocol sentences are taken out too, a dataset of
1702 valid utterances remains. Moreover, 292 ut-
terances are annotated to contain no valid dialogue
act and are therefore not suitable for the recogni-
tion task. Of the remaining utterances, 171 predi-
cate argument structures were annotated as wrong
because of completely ungrammatical input. In
this way we arrive at a dataset of 804 instances for
the users and 435 for the wizard, summing up to
1239 instances in total.
The features used for dialogue act recognition
exploit the information extracted from the differ-
ent annotation steps:
? Context features: The last preceding dia-
logue act, equality between the last preced-
ing topic and the actual topic, sentence mood
? Syntactic relation features: Syntactic predi-
cate class, arguments, negation
? VerbNet semantic relation features: VerbNet
predicate class, VerbNet frame arguments,
negation
574
? Utterance features: The original utterances
without any modifications
Different sets of features for training and eval-
uation are generated from these:
DATASET Syn: All utterances of the specified
dataset described via syntactic relation and
context features.
DATASET VNSem: All utterances of the speci-
fied dataset described via VerbNet semantic
relations and context features.
DATASET Syn Only: All utterances of the
specified dataset only described via the
syntactic relations.
DATASET VNSem Only: All utterances of the
specified dataset only described via the Verb-
Net semantic relations.
DATASET Context Only: All utterances of the
specified dataset described via the context
features and negation without any informa-
tion regarding relations.
DATASET Utterances Context: The utterances
of the specified dataset as strings combined
with the whole set of context features without
further relation extraction results.
DATASET Utterances: Only the utterances of
the specified dataset as strings. This and the
last ?Utterances?-set serve as baselines.
Dialogue Act Recognition is carried out via
the Bayesian network classifier AOEDsr from the
WEKA toolkit. AODEsr augments AODE, an
algorithm averaging over all of a small space
of alternative naive-Bayes-like models that have
weaker independence assumptions than naive
Bayes, with Subsumption Resolution (Zheng and
Webb, 2006). Evaluation is performed using
crossfolded evaluation.
All results of the experiments are given in terms
of accuracy.
Results for the dataset ?All? comparing the syn-
tactic relations with VerbNet relations as well as
the pure utterances and context are shown in table
4.
Dataset Accuracy
All Syn 67.4%
All VNSem 66.8%
All Utterances Context 61.9%
All Utterances 48.1%
Table 4: Dialogue Act Classification Results for
the ?ALL? Datasets
The best result is achieved with the syntactic in-
formation, although the VerbNet information pro-
vides an abstraction over the predicate classifica-
tion. Both the set containing the VerbNet relations
as well as the syntactic relations are much better
than the set containing only the context and the
original utterances. The dataset containing only
the utterances could not reach 50%.
Although the experiments show much better re-
sults using the relations instead of the original ut-
terance, the overall accuracy is not very satisfying.
Several reasons for this phenomenon come into
consideration. While it can to a certain extend be
the fault of the classifying algorithm (see table 8
for some tests with a ROCCHIO based classifier),
the main reason might as well lie in the impre-
cise boundaries of the dialogue act classes: Sev-
eral categories are hard to distinguish even for a
human annotator as you can see from the wrongly
classified examples in table 3. Another possibil-
ity can be the comparatively small number of total
training instances.
For the NPC dataset the results are slightly bet-
ter and much better still for the set CST, which
is due to a smaller number (6) of dialogue acts:
The dialogue act ?PROPOSE?, which is the act
for showing an object or proposing a possibility,
was not used by any user, but only by the wizard.
Dataset Accuracy
CST Syn 73.1%
NPC Syn 68.5%
Table 5: Dialogue Act Classification Results for
Datasets ?CST? and ?NPC?
To find out if one sort of features is espe-
cially important for the classification we reorga-
575
Utterance Right Classification Classified As
What do you think about this one? request info propose
Let see what you have and where we can put it request info request
Table 3: Wrongly classified instances
nize the training sets to contain only the context
features without the relations (All Context Only)
on the one hand and only the relational informa-
tion without the context features on the other hand
(All Syn Only and All VNSem Only). Results
are shown in table 6.
Dataset Accuracy
All Context Only 56.6%
All VNSem Only 53.5%
All Syn Only 50.8%
Table 6: Dialogue Act Classification Results for
Context and Relation sets
Table 6 shows that the results are considerably
worse if only parts of the features are used. The
set with context feature performs 3,1% better than
the best set with the relations only. Furthermore
the VerbNet semantic relation set leads to nearly
3% better accuracy, which may mean that the ab-
straction of semantic predicates provides a better
mapping to dialogue acts after all if used without
further features which may be ranked more impor-
tant by the classifier.
Besides the experiments with the Bayesian net-
works, additional experiments are performed us-
ing a modified ROCCHIO algorithm similar to the
one in (Neumann and Schmeier, 2002). Three dif-
ferent datasets were tested (see table 7).
Dataset Accuracy
All Utterances 70.1%
All Utterances Context 73.2%
All Syn 74.4%
Table 8: Dialogue Act Classification Results using
the ROCCHIO Algorithm
Table 8 shows that the baseline dataset contain-
ing only the utterances already provides much bet-
ter results with the ROCCHIO algorithm, deliv-
ering 70.1% which is more than 10% more ac-
curacy compared to the 48.1% of the Bayesian
classifier. If tested together with the context fea-
tures the accuracy of the utterance dataset raises to
73.2% and, after including the relational informa-
tion, even to 74.4%. Thus, the results of this ROC-
CHIO experiment also prove that the employment
of the relation information leads to improved ac-
curacy of the classification.
6 Conclusion
This paper reports on a novel approach to auto-
matic dialogue act recognition using syntactic and
semantic relations as new features instead of the
traditional features such as ngrams of words.
Different feature sets are constructed via an
automatic annotation of syntactic predicate argu-
ment structures and a manual annotation of Verb-
Net frame information. On the basis of this infor-
mation, both the syntactic relations as well as the
semantic VerbNet-based relations included in the
utterances can be extracted and added to the fea-
ture sets for the recognition task. Besides the re-
lation information the employed features include
information from the dialogue context (e.g. the
last preceding dialogue act) and other features like
sentence mood.
The feature sets have been evaluated with a
Bayesian network classifier as well as a ROC-
CHIO algorithm. Both classifiers demonstrate the
benefits gained from the relations by exploiting
the additionally provided information. While the
difference between the best baseline feature set
and the best relation feature set in the Bayesian
network classifier yields a 5,5% boost in accuracy
(61.9% to 67.4%), the ROCCHIO setup exceeds
the boosted accuracy by another 1,5% , starting
from a higher baseline of 73.2%. Based on the
observed complexity of the classification task we
expect that the benefit of the relational informa-
576
Predicate Instances Example
see-30.1 59 I would like to see a table in front of the sofa
put-9.1 74 Can you put it in the corner?
reflexive appearance-48.1.2 80 Show me the red one
own-100 137 Do you have wooden chairs?
want-32.1 153 I would like some plants over here
Table 7: The Main Semantic Relations Found in the Data Sorted by Predicate
tion may turn out to be even more significant on
larger learning data.
7 Future Work
The results in section 5 show that the pure classifi-
cation cannot be used as interpretation component
in isolation, but additional methods have to be in-
corporated. In a preceding analysis of the data
it was found that certain predicates are very fre-
quently uttered by the users. In the syntactic pred-
icate scenario the total number of different predi-
cates is 80, whereas the semantic predicates build
up a total number of 66. The class containing the
predicates with one to ten occurrences constitutes
137 of 1239 instances. The remaining 1101 in-
stances are covered by only 21 different predicate
classes. These predicates together with their ar-
guments constitute a set of common domain re-
lations for the sales domain. The main domain
relations found are shown in table 7.
The figures suggest that the interpretation at
least for the domain relations can be established in
a robust manner, wherefore the agent?s interpreta-
tion component was extended to a hybrid module
including a robust rule based method. To derive
the necessary rules a rule generator was developed
and the rules covering the used feature set (includ-
ing the context features, sentence mood and the
syntactic relations) were automatically generated
from the given data.
Future work will focus on the evaluation of
these automatically derived rules on a recently
collected but not yet annotated dataset from a sec-
ond Wizard-of-Oz experiment, carried out in the
same furniture sales setting.
Additional experiments are planned for evalu-
ating the relation-based features in dialogue act
recognition on other corpora tagged with differ-
ent dialogue acts in order to test the overall per-
formance of our classification approach on more
transparent dialogue act sets.
Acknowledgements
The work described in this paper was partially
supported through the project ?KomParse? funded
by the ProFIT program of the Federal State of
Berlin, co-funded by the EFRE program of the
European Union. Additional support came from
the project TAKE, funded by the German Min-
istry for Education and Research (BMBF, FKZ:
01IW08003).
577
References
Allen, James F., Bradford W. Miller, Eric K. Ringger,
and Teresa Sikorski. 1996. A robust system for nat-
ural spoken dialogue. In Proceedings of ACL 1996.
Allen, James, Mehdi Manshadi, Myroslava Dzikovska,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In DeepLP ?07: Pro-
ceedings of the Workshop on Deep Linguistic Pro-
cessing, Morristown, NJ, USA.
Andernach, Toine. 1996. A machine learning ap-
proach to the classification of dialogue utterances.
CoRR, cmp-lg/9607022.
Baker, Collin F., Charles J. Fillmore, and John B.
Lowe. 1998. The berkeley framenet project. In
Proceedings of COLING 1998.
Bertomeu, Nuria and Anton Benz. 2009. Annotation
of joint projects and information states in human-
npc dialogues. In Proceedings of CILC-09, Murcia,
Spain.
Clark, H.H. 1996. Using Language. Cambridge Uni-
versity Press.
de Marneffe, Marie C. and Christopher D. Manning.
2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, Manchester, UK.
Jurafsky, Daniel, Elizabeth Shriberg, Barbara Fox, and
Traci Curl. 1998. Lexical, prosodic, and syntactic
cues for dialog acts.
Keizer, Simon and Rieks op den Akker. 2006.
Dialogue act recognition under uncertainty using
bayesian networks. Nat. Lang. Eng., 13(4).
Keizer, Simon, Rieks op den Akker, and Anton Nijholt.
2002. Dialogue act recognition with bayesian net-
works for dutch dialogues. In Proceedings of the
3rd SIGdial workshop on Discourse and dialogue,
Morristown, NJ, USA.
Klu?wer, Tina, Peter Adolphs, Feiyu Xu, Hans Uszko-
reit, and Xiwen Cheng. 2010. Talking npcs in a
virtual game world. In Proceedings of the System
Demonstrations Section at ACL 2010.
Lapata, Mirella and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In Proceed-
ings of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 153?160.
Neumann, Gu?nter and Sven Schmeier. 2002. Shal-
low natural language technology and text mining.
Ku?nstliche Intelligenz. The German Artificial Intel-
ligence Journal.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1).
Schmid, Helmut. 1994. In Proceedings of the Inter-
national Conference on New Methods in Language
Processing.
Schuler, Karin Kipper. 2005. Verbnet: a broad-
coverage, comprehensive verb lexicon. Ph.D. the-
sis, Philadelphia, PA, USA.
Searle, John R. 1969. Speech acts : an essay in
the philosophy of language / John R. Searle. Cam-
bridge University Press, London.
Sporleder, Caroline and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: A critical assessment. Natural Lan-
guage Engineering, 14(3).
Stolcke, Andreas, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van, and Ess dykema
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26.
Subba, Rajen and Barbara Di Eugenio. 2009. An ef-
fective discourse parser that uses rich linguistic in-
formation. In NAACL ?09, Morristown, NJ, USA.
Surendran, Dinoj and Gina-Anne Levow. 2006. Di-
alog act tagging with support vector machines and
hidden markov models. In Interspeech.
Verbree, A.T., R.J. Rienks, and D.K.J. Heylen.
Dialogue-act tagging using smart feature selection:
results on multiple corpora. In Raorke, B., editor,
First International IEEE Workshop on Spoken Lan-
guage Technology SLT 2006.
Webb, Nick and Ting Liu. 2008. Investigating the
portability of corpus-derived cue phrases for dia-
logue act classification. In Proceedings of COLING
2008, Manchester, UK.
Xu, Feiyu, Hans Uszkoreit, and Hong Li. 2007. A
seed-driven bottom-up machine learning framework
for extracting relations of various complexity. In
Proceedings of ACl (07), Prague, Czech Republic.
Zheng, Fei and Geoffrey I. Webb. 2006. Efficient
lazy elimination for averaged one-dependence esti-
mators. In ICML, pages 1113?1120.
Zimmermann, Matthias, Yang Liu, Elizabeth Shriberg,
and Andreas Stolcke. 2005. Toward joint seg-
mentation and classification of dialog acts in mul-
tiparty meetings. In Proc. Multimodal Interaction
and Related Machine Learning Algorithms Work-
shop (MLMI05, page 187.
578
Proceedings of the ACL 2010 System Demonstrations, pages 36?41,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Talking NPCs in a Virtual Game World
Tina Klu?wer, Peter Adolphs, Feiyu Xu, Hans Uszkoreit, Xiwen Cheng
Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (DFKI)
Projektbu?ro Berlin
Alt-Moabit 91c
10559 Berlin
Germany
{tina.kluewer,peter.adolphs,feiyu,uszkoreit,xiwen.cheng}@dfki.de
Abstract
This paper describes the KomParse sys-
tem, a natural-language dialog system
in the three-dimensional virtual world
Twinity. In order to fulfill the various
communication demands between non-
player characters (NPCs) and users in
such an online virtual world, the system
realizes a flexible and hybrid approach
combining knowledge-intensive domain-
specific question answering, task-specific
and domain-specific dialog with robust
chatbot-like chitchat.
1 Introduction
In recent years multi-user online games in virtual
worlds such as Second Life or World of Warcraft
have attracted large user communities. Such vir-
tual online game worlds provide new social and
economic platforms for people to work and inter-
act in. Furthermore, virtual worlds open new per-
spectives for research in the social, behavioral, and
economic sciences, as well as in human-centered
computer science (Bainbridge, 2007). Depending
on the game type, non-player characters (NPCs)
are often essential for supporting the game plot,
for making the artificial world more vivid and ulti-
mately for making it more immersive. In addition,
NPCs are useful to populate new worlds by carry-
ing out jobs the user-led characters come in touch
with. The range of functions to be filled by NPCs
is currently still strongly restricted by their limited
capabilities in autonomous acting and communi-
cation. This shortcoming creates a strong need for
progress in areas such as AI and NLP, especially
their planning and dialog systems.
The KomParse system, described in this paper,
provides NPCs for a virtual online world named
Twinity, a product of the Berlin startup company
Metaversum1. The KomParse NPCs offer vari-
ous services through conversation with game users
using question-answering and dialog functional-
ity. The utilization of Semantic Web technology
with RDF-encoded generic and domain-specific
ontologies furthermore enables semantic search
and inference.
This paper is organized as follows: Section 2
presents the NPC modelling and explains the ap-
plication scenarios. Section 3 details the knowl-
edge representation and semantic inference in our
system. Section 4 explains the system architecture
and its key components. Section 5 describes the
KomParse dialog system. Section 7 gives a con-
clusion and closes off with our future work.
2 Application Scenario and NPC
Modelling
The online game Twinity extends the Second Life
idea by mirroring an urban part of the real world.
At the time of this writing, the simulated section of
reality already contains 3D models of the cities of
Berlin, Singapore and London and it keeps grow-
ing. Users can log into the virtual world, where
they can meet other users and communicate with
them using the integrated chat function or talk
to each other via Voice-over-IP. They can style
their virtual appearance, can rent or buy their own
flats and decorate them as to their preferences and
tastes.
Out of many types of NPCs useful for this appli-
cation such as pedestrians, city guides and person-
nel in stores, restaurants and bars, we start with
two specific characters: a female furniture sales
agent and a male bartender. The furniture seller
is designed for helping users furnish their virtual
apartments. Users can buy pieces of furniture and
room decoration from the NPC by describing their
demands and wishes in a text chat. During the di-
1http://www.metaversum.com/
36
Figure 1: The furniture sales NPC selling a sofa
alog with the NPC, the preferred objects are then
selected and directly put into a location in the
apartment, which can be further refined with the
user interfaces that Twinity provides.
In the second scenario, the bartender sells vir-
tual drinks. He can talk about cocktails with users,
but moreover, he can also entertain his guests by
providing trivia-type information about popular
celebrities and various relations among them.
We chose these two characters not only because
of their value for the Twinity application but also
for our research goals. They differ in many in-
teresting aspects. First of all, the furniture sales
agent is controlled by a complex task model in-
cluding ontology-driven and data-driven compo-
nents to guide the conversation. This agent also
possesses a much more fine-grained action model,
which allows several different actions to cover
the potential conversation situations for the sell-
ing task. The bartender agent on the other hand is
designed not to fulfill one strict task because his
clients do not follow a specific goal except order-
ing drinks. Our bartender has the role of a conver-
sation companion and is able to entertain clients
with his broad knowledge. Thus, he is allowed to
access to several knowledge bases and is able to
handle questions (and later conversations) about
a much larger domain called the ?gossip domain?
which enables conversation about pop stars, movie
actors and other celebrities as well as the relations
between these people. In order to achieve a high
robustness, we integrate a chatbot into the bar-
tender agent to catch chitchat utterances we cannot
handle.
Figure 2: Our bartender NPC in his bar in Twinity
3 Knowledge Representation and
Semantic Inference
Semantic Web technology is employed for mod-
elling the knowledge of the NPCs. The Resource
Description Format (RDF) serves as the base for
the actual encoding. An RDF statement is a binary
relation instance between two individuals, that is a
triple of a predicate and two arguments, called the
subject and the object, and written as subj pred obj
(e.g. f:Sofa Alatea f:hasMainColour
f:Burgundy).
All objects and properties the NPC can talk
about are modelled in this way. Therefore the
knowledge base has to reflect the physical prop-
erties of the virtual objects in Twinity as faithfully
as possible. For instance, specific pieces of furni-
ture are described by their main color, material or
style, whereas cocktails are characterized by their
ingredients, color, consistence and taste. Further-
more, references to the 3D models of the objects
are stored in order to create, find and remove such
objects in the virtual world.
The concepts and individuals of the particular
domain are structured and organized in domain-
specific ontologies. These ontologies are mod-
elled in the Web Ontology Language (OWL).
OWL allows us to define concept hierarchies, re-
lations between concepts, domains and ranges of
these relations, as well as specific relation in-
stances between instances of a concept. Our on-
tologies are defined by the freely available ontol-
ogy editor Prote?ge? 4.02. The advantage of using an
ontology for structuring the domain knowledge is
2http://protege.stanford.edu/, as accessed
27 Oct 2009
37
Twinity
Server
KomParse
Server
Twinity
Client
Conversational
AgentConversational
AgentConversational
Agent
Twinity
ClientTwinity
Client
Figure 3: Overall System Architecture ? Server/Client Architecture for NPC Control
the modular non-redundant encoding. When com-
bined with a reasoner, only a few statements about
an individual have to be asserted explicitely, while
the rest can be inferred from the ontology. We em-
ploy several ontologies, among which the follow-
ing are relevant for modelling the specific domains
of our NPCs:
? An extensive furniture ontology, created by
our project partner ZAS Berlin, defining
kinds of furniture, room parts, colors and
styles as well as the specific instances of fur-
niture in Twinity. This knowledge base con-
tains 95,258 triples, 123 furniture classes, 20
color classes, 243 color instances and various
classes defining styles and similar concepts.
? A cocktail ontology, defining 13 cocktail
classes with ingredients and tastes in 21,880
triples.
? A biographical ontology, the ?gossip on-
tology?, defining biographical and career-
specific concepts for people. This ontology is
accompanied by a huge database of celebri-
ties, which has been automatically acquired
from the Web and covers nearly 600,000 per-
sons and relations between these people like
family relationships, marriages and profes-
sional relations. (Adolphs et al, 2010)
The furniture ontology is the only knowledge
base for the furniture sales agent, whereas the bar-
tender NPC has access to both the cocktail as well
as the gossip knowledge base.
We use SwiftOwlim3 for storing and querying
the data. SwiftOwlim is a ?triple store?, a kind
of database which is specifically built for storing
and querying RDF data. It provides a forward-
chaining inference engine which evaluates the
domain definitions when loading the knowledge
repository, and makes implicit knowledge explicit
by asserting triples that must also hold true accord-
ing to the ontology. Once the reasoner is finished,
the triple store can be queried directly using the
RDF query language SPARQL.
3http://www.ontotext.com/owlim/
4 Overall System Architecture
Figure 3 shows the overall system architecture.
Twinity is a server/client application, in which the
server hosts the virtual world and coordinates the
user interactions. In order to use Twinity, users
have to download the Twinity client. The client
allows the user to control the physical represen-
tation of the user?s character in the virtual world,
also called the ?avatar?. Thus the client is respon-
sible for displaying the graphics, calculating the
effects of physical interactions, handling the user?s
input and synchronizing the 3D data and user ac-
tions with the Twinity server.
Each NPC comprises two major parts: whereas
its avatar is the physical appearance of the NPC in
the virtual world, the ?conversational agent? pro-
vides the actual control logic which controls the
avatar autonomously. It is in particular able to hold
a conversation with Twinity users in that it reacts
to a user?s presence, interprets user?s utterances in
dialog context and generates adequate responses.
The KomParse server is a multi-client, multi-
threaded server written in Java that hosts the con-
versational agents for the NPCs (section 5). The
NPC?s avatar, on the other hand, is realized by a
modified Twinity client. We utilize the Python in-
terface provided by the Twinity client to call our
own plugin which opens a bidirectional socket
connection to the KomParse server. The plugin is
started together with the Twinity client and serves
as a mediator between the Twinity server and the
KomParse server from then on (fig. 3). It sends all
in-game events relevant to our system to the server
and translates the commands sent by the server
into Twinity-specific actions.
The integration architecture allows us to be
maximally independent of the specific game plat-
form. Rather than using the particular program-
ming language and development environment of
the platform for realizing the conversational agent
or reimplementing a whole client/server proto-
col for connecting the avatar to the corresponding
agent, we use an interface tailored to the specific
needs of our system. Thus the KomParse system
38
can be naturally extended to other platforms since
only the avatar interfaces have to be adapted.
The integration architecture also has the advan-
tage that the necessary services can be easily dis-
tributed in a networked multi-platform environ-
ment. The Twinity clients require aMicrosoftWin-
dows machine with a 3D graphics card supporting
DirectX 9.0c or higher, 1 GB RAM and a CPU
core per instance. The KomParse server requires
roughly 1 GB RAM. The triple store is run as
a separate server process and is accessed by an
XML-RPC interface. Roughly 1.2 GB RAM are
required for loading our current knowledge base.
5 Conversational Agent: KomParse
Dialog System
Figure 4: Dialog System: Conversational Agent
The KomParse dialog system, the main func-
tionality of the conversational agent, consists of
the following three major components: input ana-
lyzer, dialog manager and output generator (fig.4).
The input analyzer is responsible for the lin-
guistic analysis of the user?s textual input includ-
ing preprocessing such as string cleaning, part-of-
speech tagging, named entity recognition, parsing
and semantic interpretation. It yields a semantic
representation which is the input for the dialog
manager.
The dialog manager takes the result of the input
analyzer and delivers an interpretation based on
the dialog context and the available knowledge. It
also controls the task conversation chain and han-
dles user requests. The dialog manager determines
the next system action based on the interpreted pa-
rameters.
The output generator realizes the action defined
by the dialog manager with its multimodal gener-
ation competence. The generated results can be
verbal, gestural or a combination of both.
As mentioned above, our dialog system has to
deal with two different scenarios. While the fo-
cal point of the bartender agent lies in the question
answering functionality, the furniture sales agent
is driven by a complex dialog task model based on
a dialog graph. Thus, the bartender agent relies
mainly on question answering technology, in that
it needs to understand questions and extract the
right answer from our knowledge bases, whereas
the sales agent has to accommodate various dialog
situations with respect to a sales scenario. It there-
fore has to understand the dialog acts intended
by the user and trigger the corresponding reac-
tions, such as presenting an object, memorizing
user preferences, negotiating further sales goals,
etc.
The task model for sales conversations is in-
spired by a corpus resulting from the annotation of
a Wizard-of-Oz experiment in the furniture sales
agent scenario carried out by our project partner at
ZAS (Bertomeu and Benz, 2009). In these exper-
iments, 18 users spent one hour each on furnish-
ing a virtual living room in a Twinity apartment by
talking to a human wizard controlling the virtual
sales agent. The final corpus consists of 18 di-
alogs containing 3,171 turns with 4,313 utterances
and 23,015 alpha-numerical strings (words). The
following example shows a typical part of such a
conversation:
USR.1: And do we have a little side table for the TV?
NPC.1: I could offer you another small table or a sideboard.
USR.2: Then I?ll take a sideboard thats similar to my shelf.
NPC.2: Let me check if we have something like that.
Table 1: Example Conversation from the Wizard-
of-Oz Experiment
The flow of the task-based conversation is con-
trolled by a data-driven finite-state model, which
is the backbone of the dialog manager. During
a sales conversation, objects and features of ob-
jects mentioned by the NPC and the user are ex-
tracted from the knowledge bases and added into
the underspecified graph nodes and egdes at run-
time. This strategy keeps the finite-state graph as
small as possible. Discussed objects and their fea-
tures are stored in a frame-based sub-component
named ?form?. The form contains entries which
correspond to ontological concepts in the furni-
39
ture ontology. During conversation, these entries
will be specified with the values of the properties
of the discussed objects. This frame-based ap-
proach increases the flexibility of the dialog man-
ager (McTear, 2002) and is particularly useful for
a task-driven dialog system. As long as the negoti-
ated object is not yet fully specified, the form rep-
resents the underspecified object description ac-
cording to the ontology concept. Every time the
user states a new preference or request, the form
is enriched with additional features until the set of
objects is small enough to be presented to the user
for final selection. Thus the actual flow of dia-
log according to the task model does not have to
be expressed by the graph but can be derived on
demand from the knowledge and handled by the
form which in turn activates the appropriate dia-
log subgraphs. This combination of graph-based
dialog models and form-based task modelling ef-
fectively accounts for the interaction of sequential
dialog strategies and the non-sequential nature of
complex dialog goals.
Given a resolved semantic representation, the
dialog manager triggers either a semantic search
in the knowledge bases to deliver factual answers
as needed in a gossip conversation or a further di-
alog response for example providing choices for
the user in a sales domain. The semantic search is
needed in both domains. In case that the semantic
representation can neither be resolved in the task
domain nor in the gossip domain, it gets passed to
the embedded A.L.I.C.E. chatbot that uses its own
understanding and generation components (Wal-
lace and Bush, 2001).
5.1 Semantic Representation
The input understanding of the system is imple-
mented as one single understanding pipeline.The
understanding pipeline delivers a semantic repre-
sentation which is the basis for the decision of the
dialog manager which action to perform next.
This semantic representation can be extracted
from the user input by our understanding com-
ponent via a robust hybrid approach: either via a
number of surface patterns containing regular ex-
pressions or via patterns reflecting the syntactic
analysis of a dependency parser (de Marneffe and
Manning, 2008).
The representation?s structure is inspired by our
knowledge representation design described in sec-
tion 3 as well as by predicate logic. The core of the
representation is a predicate-argument structure
limited to two arguments including message type
and the whole syntactic information found by the
analysis pipeline. The field ?Message Type? can
have one of the following values: wh-question,
yes/no-question, declarative. Predicates can often
be instantiated with the lemmatized matrix verb of
the successfully analysed piece of the input. If the
input contains a wh-question, the questioned fact
is marked as an unfilled argument slot. The gen-
eral structure can be simplified described as:
<PREDICATE, ARG1, ARG2, [message-type]>
The following examples show the structure used
for different input:
? ?Who is the boyfriend of Madonna??
<hasBoyfriend, Madonna, ?, [wh]>
? ?I want to buy a sofa.?
<buy, I, "a sofa", [declarative]>
5.2 Information Extraction
Both scenarios make use of state-of-the-art infor-
mation extraction approaches to extract the impor-
tant pieces from the user input. While the bar-
tender depends on relation extraction to detect the
fact or relation questioned by the user (Xu et al,
2007), the sales agent uses information extraction
methods to recognize user wishes and demands.
As a result, the questioned fact or the demanded
object feature equals the ontology structure con-
taining the knowledge needed to handle the user
input. The input ?Do you have any red couches??
for example needs to get processed by the system
in such a way that the information regarding the
sofa with red color is extracted.
This is done by the system in a data-driven way.
The input analysis first tries to find a demanded
object in the input via asking the ontology: Every
object which can be discussed in the scenario is
encoded in the sales agents knowledge base. This
can be seen as a Named Entity Recognition step.
In case of success, the system tries to detect one
of the possible relations of the object found in the
input. This is achieved by querying the ontology
about what kind of relations the identified object
can satisfy. Possible relations are encoded in the
class description of the given object. As a result
the system can detect a relation ?hasColour? for
the found object ?sofa? and the color value ?red?.
The found information gets inserted into the form
which gets more and more similar or if possible
equal to a search query via RDF.
40
Figure 5: Comparison of Input, Extracted Information and Knowledge Base
6 Conclusion and Future Work
The KomParse system demonstrates an attractive
application area for dialog systems that bears great
future potential. Natural language dialog with
NPCs is an important factor in making virtual
worlds more interesting, interactive and immer-
sive. Virtual worlds with conversing characters
will also find many additional applications in edu-
cation, marketing, and entertainment.
KomParse is an ambitious and nevertheless
pragmatic attempt to bring NLP into the world of
virtual games. We develop a new strategy to inte-
grate task models and domain ontologies into dia-
log models. This strategy is useful for task-driven
NPCs such as furniture sellers. With the chatty
bartender, a combination of task-specific dialog
and domain-specific question answering enables a
smart wide-domain off-task conversation. Since
the online game employs bubble-chat as a mode
of communication in addition to Voice-over-IP, we
are able to test our dialog system in a real-time
application without being hindered by imperfect
speech recognition.
The system presented here is still work in
progress. The next goals will include various eval-
uation steps. On the one hand we will focus on
single components like hybrid parsing of input ut-
terances and dialog interpretation in terms of pre-
cision and recall. On the other hand an evaluation
of the two different scenarios regarding the us-
ability are planned in experiments with end users.
Moreover we will integrate some opinion mining
and sentiment analysis functionality which can be
helpful to better detect and understand the user?s
preferences in the furniture sales agents scenario.
Acknowledgements
The project KomParse is funded by the ProFIT
programme of the Federal State of Berlin, co-
funded by the EFRE programme of the Euro-
pean Union. The research presented here is ad-
ditionally supported through a grant to the project
TAKE, funded by the German Ministry for Edu-
cation and Research (BMBF, FKZ: 01IW08003).
Many thanks go to our project partners at the Cen-
tre for General Linguistics (ZAS) in Berlin as well
as to the supporting company Metaversum.
References
Peter Adolphs, Xiwen Cheng, Tina Klu?wer, Hans
Uszkoreit, and Feiyu Xu. 2010. Question answering
biographic information and social network powered
by the semantic web. In Proceedings of LREC 2010,
Valletta, Malta.
William Sims Bainbridge. 2007. The scientific re-
search potential of virtual worlds. Science, 317.
Nuria Bertomeu and Anton Benz. 2009. Annotation of
joint projects and information states in human-npc
dialogues. In Proceedings of the First International
Conference on Corpus Linguistics (CILC-09), Mur-
cia, Spain.
Marie C. de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, Manchester, UK.
Michael F. McTear. 2002. Spoken dialogue tech-
nology: enabling the conversational user interface.
ACM Comput. Surv., 34(1).
Richard Wallace and Noel Bush. 2001. Artificial
intelligence markup language (aiml) version 1.0.1
(2001). Unpublished A.L.I.C.E. AI Foundation
Working Draft (rev 006).
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2007. A
seed-driven bottom-up machine learning framework
for extracting relations of various complexity. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 584?
591, Prague, Czech Republic, June. Association for
Computational Linguistics.
41
