Abstract
We demonstrate a new research approach to the
problem of predicting the reading difficulty of a
text passage, by recasting readability in terms of
statistical language modeling.  We derive a measure
based on an extension of multinomial na?ve Bayes
classification that combines multiple language
models to estimate the most likely grade level for a
given passage.  The resulting classifier is not spe-
cific to any particular subject and can be trained
with relatively little labeled data.  We perform pre-
dictions for individual Web pages in English and
compare our performance to widely-used semantic
variables from traditional readability measures.  We
show that with minimal changes, the classifier may
be retrained for use with French Web documents.
For both English and French, the classifier main-
tains consistently good correlation with labeled
grade level (0.63 to 0.79) across all test sets.  Some
traditional semantic variables such as type-token
ratio gave the best performance on commercial cal-
ibrated test passages, while our language modeling
approach gave better accuracy for Web documents
and very short passages (less than 10 words).
1 Introduction
In the course of constructing a search engine for stu-
dents, we wanted a method for retrieving Web pages
that were not only relevant to a student's query, but also
well-matched to their reading ability.  Widely-used tra-
ditional readability formulas such as Flesch-Kincaid
usually perform poorly in this scenario.  Such formulas
make certain assumptions about the text: for example,
that the sample has at least 100 words and uses well-
defined sentences.  Neither of these assumptions need
be true for Web pages or other non-traditional docu-
ments.  We seek a more robust technique for predicting
reading difficulty that works well on a wide variety of
document types.
To do this, we turn to simple techniques from statis-
tical language modeling.  Advances in this field in the
past 20 years, along with greater access to training data,
make the application of such techniques to readability
quite timely.  While traditional formulas are based on
linear regression with two or three variables, statistical
language models can capture more detailed patterns of
individual word usage.  As we show in our evaluation,
this generally results in better accuracy for Web docu-
ments and very short passages (less than 10 words).
Another benefit of a language modeling approach is that
we obtain a probability distribution across all grade
models, not just a single grade prediction.  
Statistical models of text rely on training data, so in
Section 2 we describe our Web training corpus and note
some trends that are evident in word usage.  Section 3
summarizes related work on readability, focusing on
existing vocabulary-based measures that can be thought
of as simplified language model techniques.  Section 4
defines the modified multinomial na?ve Bayes model.
Section 5 describes our smoothing and feature selection
techniques.  Section 6 evaluates our model's generaliza-
tion performance, accuracy on short passages, and sen-
sitivity to the amount of training data.  Sections 7 and 8
discuss the evaluation results and give our observations
and conclusions.
2 Description of Web Corpus
First, we define the following standard terms when
referring to word frequencies in a corpus.  A token is de-
fined as any word occurrence in the collection.  A type
refers to a specific word-string, and is counted only once
no matter how many times the word token of that type
occurs in the collection.
For training our model, we were aware of no signifi-
cant collection of Web pages labeled by reading diffi-
culty level, so we assembled our own corpus.  There are
numerous commercial reading comprehension tests
available that have graded passages, but this would have
reduced the emphasis we wanted on Web documents.
Also, some commercial packages themselves use read-
A Language Modeling Approach to Predicting Reading Difficulty
Kevyn Collins-Thompson       Jamie Callan 
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
4502 Newell Simon Hall 
Pittsburgh, PA 15213-8213 
{kct, callan}@cs.cmu.edu 
 
ability measures when authoring the graded passages,
making the data somewhat artificial and biased toward
traditional semantic variables.
We gathered 550 English documents across 12
American grade levels, containing a total of 448,715
tokens and 17,928 types.  The pages were drawn from a
wide variety of subject areas: fiction, non-fiction, his-
tory, science, etc.  We were interested in the accuracy
available at individual grade levels, so we selected
pages which had been assigned a specific grade level by
the Web site author.  For example, in some cases the
assigned grade level was that of the classroom page
where the document was acquired.
Before defining a classification model, we examined
the corpus for trends in word frequency.  One obvious
pattern was that more difficult words were introduced at
later grade levels.  Earlier researchers (e.g. Chall, 1983,
p. 63) have also observed that concrete words like ?red?
become less likely in higher grades.  Similarly, higher
grade levels use more abstract words with increased fre-
quency.  We observed both types of behavior in our Web
corpus.  Figure 1 shows four words drawn from our cor-
pus.  Data from each of the 12 grades in the corpus are
shown, ordered by ascending grade level.  The solid line
is a smoothed version of the word frequency data.  The
word ?red? does indeed show a steady decline in usage
with grade level, while the probability of the word
?determine? increases.  Other words like ?perimeter?
attain maximum probability in a specific grade range,
perhaps corresponding to the period in which these con-
cepts are emphasized in the curriculum.  The word ?the?
is very common and varies less in frequency across
grade levels.
Our main hypothesis in this work is that there are
enough distinctive changes in word usage patterns be-
tween grade levels to give accurate predictions with
simple language models, even when the subject domain
of the documents is unrestricted.
3 Related Work
There is a significant body of work on readability that
spans the last 70 years.  A comprehensive summary of
early readability work may be found in Chall (1958) and
Probability of the word "perimeter"
0
0.00005
0.0001
0.00015
0.0002
0.00025
0.0003
0.00035
0.0004
0.00045
0.0005
0 1 2 3 4 5 6 7 8 9 10 11 12
Grade Class
P(
wo
rd
|gr
ad
e)
Probability of the word "red"
0
0.0002
0.0004
0.0006
0.0008
0.001
0.0012
0.0014
0.0016
0 1 2 3 4 5 6 7 8 9 10 11 12
Grade Class
P(
wo
rd
|gr
ad
e)
Probability of the word "determine"
0
0.0002
0.0004
0.0006
0.0008
0.001
0.0012
0.0014
0.0016
0 1 2 3 4 5 6 7 8 9 10 11 12
Grade Class
P(
wo
rd
|g
ra
de
)
Probability of the word "the"
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0 1 2 3 4 5 6 7 8 9 10 11 12
Grade Class
P(
wo
rd
|gr
ad
e)
Figure 1.  Examples of four different word usage trends across grades 1-12, as sampled from our 400K-token
corpus of Web documents.  Curves showing word frequency data smoothed across grades using kernel regression
for the words (clockwise from top left): ?red?, ?determine?, ?the?, and ?perimeter?.
Klare (1963).  In 1985 a study by Mitchell (1985)
reviewed 97 different reading comprehension tests,
although few of these have gained wide use.
?Traditional? readability measures are those that rely
on two main factors: the familiarity of semantic units
(words or phrases) and the complexity of syntax.  Mea-
sures that estimate semantic difficulty using a word list
(as opposed to, say, number of syllables in a word) are
termed ?vocabulary-based measures?. 
Most similar to our work are the vocabulary-based
measures, such as the Lexile measure (Stenner et al,
1988), the Revised Dale-Chall formula (Chall and Dale,
1995) and the Fry Short Passage measure (Fry, 1990).
All of these use some type of word list to estimate
semantic difficulty: Lexile (version 1.0) uses the Car-
roll-Davies-Richman corpus of 86,741 types (Carroll et
al., 1971); Dale-Chall uses the Dale 3000 word list; and
Fry's Short Passage Measure uses Dale & O'Rourke's
?The Living Word Vocabulary? of 43,000 types (Dale
and O'Rourke, 1981).  Each of these word lists may be
thought of as a simplified language model.  The model
we present below may be thought of as a generalization
of the vocabulary-based approach, in which we build
multiple language models - in this study, one for each
grade - that capture more fine-grained information about
vocabulary  usage.
To our knowledge, the only previous work which
has considered a language modeling approach to read-
ability is a preliminary study by Si and Callan (2001).
Their work was limited to a single subject domain - sci-
ence - and three broad ranges of difficulty.  In contrast,
our model is not specific to any subject and uses 12 indi-
vidual grade models trained on a greatly expanded train-
ing set.  While our model is also initially based on na?ve
Bayes, we do not treat each class as independent.
Instead, we use a mixture of grade models, which
greatly improves accuracy.  We also do not include sen-
tence length as a syntactic component.  Si and Callan
did not perform any analysis of feature selection meth-
ods so it is unclear whether their classifier was conflat-
ing topic prediction with difficulty prediction.  In this
paper we examine feature selection as well as our
model's ability to generalize.
4 The Smoothed Unigram Model
Our statistical model is based on a variation of the mult-
inomial na?ve Bayes classifier, which we call the
?Smoothed Unigram? model.  In text classification
terms, each class is described by a language model cor-
responding to a predefined level of difficulty.  For
English Web pages, we trained 12 language models cor-
responding to the 12 American grade levels.
The language models we use are simple: they are
based on unigrams and assume that the probability of a
token is independent of the surrounding tokens, given
the grade language model.  A unigram language model
is defined by a list of types (words) and their individual
probabilities.  Although this is a weak model, it can be
trained from less data than more complex models, and
turns out to give good accuracy for our problem.
4.1 Prediction with Multinomial Na?ve Bayes
We define a generative model for a text passage T in
which we assume T was created by a hypothetical
author using the following algorithm:
1. Choose a grade language model Gi  from some
complete set of unigram models G according to a prior
distribution P(Gi).  Each Gi has a multinomial distribu-
tion over a vocabulary V.
2. Choose a passage length L in tokens according to
the distribution  P(L | Gi).
3. Assuming a ?bag of words? model for the passage,
sample L tokens from Gi ?s multinomial distribution
based on the ?na?ve? assumption that each token is inde-
pendent of all other tokens in the passage, given the lan-
guage model Gi.  
The probability of T given model Gi is therefore: 
where C(w) is the count of the type w in T.
Our goal is to find the most likely grade language
model given the text T, or equivalently, the model Gi that
maximizes .  We derive L(Gi | T)
from (1) via Bayes? Rule, which is:
However, we first make two further assumptions:
1. All grades are equally likely a priori, and there-
fore  where NG  is the number of grades.
2. The passage length probability P(L|Gi) is indepen-
dent of grade level.
Substituting (1) into (2), simplifying, and taking log-
arithms, we obtain:
where log Z represents combined factors involving pas-
sage length and the uniform prior P(Gi) which, accord-
ing to our assumptions, do not influence the prediction
outcome and may be ignored.  The sum in (3) is easily
computed: for each token in T, we simply look up its log
probability in the language model of Gi and sum over all
P T Gi( ) P L Gi( ) L !
P w Gi( )
C w( )
C w( )!-------------------------------
w T?
??=
L Gi T( ) P Gi T( )log=
P Gi T( )
P Gi( )P T Gi( )
P T( )----------------------------------=
P Gi( ) 1 NG?=
L Gi T( ) C w( ) P w Gi( )log
w T?

Zlog+=
(1)
(2)
(3)
tokens to obtain the total likelihood of the passage given
the grade.  We do this for all language models, and
select the one with maximum likelihood.  An example
of the set of log-likelihoods calculated across all 12
grade models, with a maximum point clearly evident, is
shown in Figure 2.
5 Implementation
Given the above theoretical model, we describe two fur-
ther aspects of our classification method: smoothing and
feature selection.
5.1 Smoothing
We will likely see some types in test documents that are
missing or rarely seen in our training documents.  This
is a well-known issue in language model applications,
and it is standard to compensate for this sparseness by
smoothing the frequencies in the trained models.  To do
this, we adjust our type probability estimates by shifting
part of the model?s probability mass from observed
types to unseen and rare types.
We first apply smoothing to each grade?s language
model individually.  We use a technique called Simple
Good-Turing smoothing, which is a popular method for
natural language applications.  We omit the details here,
which are available in Gale and Sampson (1995).
Next, we apply smoothing across grade language
models.  This is a departure from standard text classifi-
cation methods, which treat the classes as independent.
For reading difficulty, however, we hypothesize that
nearby grade models are in fact highly related, so that
even if a type is unobserved in one grade?s training data,
we can estimate its probability in the model by interpo-
lating estimates from nearby grade models.
For example, suppose we wish to estimate P(w|G)
for a type w in a grade model G.  If the type w occurs in
at least one grade language model, we can perform
regression with a Gaussian kernel (Hastie et al, 2001, p.
165) across all grade models to obtain a smoothed value
for P(w|G).  With training, we found the optimal kernel
width to be 2.5 grade levels.  If w does not occur in any
grade model (an ?out-of-vocabulary? type) we can back
off to a traditional semantic variable.  In this study, we
used an estimate which is a function of type length:
                         
where w is a type, i is a grade index between 1 and 12,
|w| is w?s length in characters, and C = -13, D = 10 based
on statistics from the Web corpus. 
5.2 Feature Selection
Feature selection is an important step in text classifica-
tion: it can lessen the computational burden by reducing
the number of features and increase accuracy by remov-
ing ?noise? words having low predictive power.
The first feature selection step for many text classifi-
ers is to remove the most frequent types (?stopwords?).
This must be considered carefully for our problem: at
lower grade levels, stopwords make up the majority of
token occurrences and removing them may introduce
bias.  We therefore do not remove stopwords.
Another common step is to remove low-frequency
types ? typically those that occur less than 2 to 5 times
in a model?s training data.  Because we smooth across
grade models, we perform a modified version of this
step, removing from all models any types occurring less
than 3 times in the entire corpus.
Unlike the usual text classification scenario, we also
wish to avoid some types that are highly grade-specific.
For example, a type that is very frequent in the grade 3
model but that never occurs in any other model seems
more likely to be site-specific noise than a genuine
vocabulary item.  We therefore remove any types occur-
ring in less than 3 grade models, no matter how high
their frequency.  Further study is needed to explore ways
to avoid over-fitting the classifier while reducing the
expense of removing possibly useful features.
We investigated scoring each remaining type based
on its estimated ability to predict (positively or nega-
tively) a particular grade.  We used a form of Log-Odds
Ratio, which has been shown to give superior perfor-
mance for multinomial na?ve Bayes classifiers (Mlad-
enic and Grobelnik, 1998).  Our modified Log-Odds
measure computes the largest absolute change in log-
likelihood between a given grade and all other grades.
P w Gi( )log C
w
D
------+ i w?( )??
Figure 2.  The log-likelihood of a typical 100-word Grade
5 passage relative to the language models for grades 1 to
12.  The maximum log-likelihood in this example is
achieved for the Grade 6 language model.  Note the nega-
tive scale.
L o g-L ike lih o o d  o f S a m p le G rad e 5 P as sa ge  
R elative  to  L an gu a ge  M o d els  fo r Gra d es 1  - 1 2
-810
-800
-790
-780
-770
-760
-750
1 2 3 4 5 6 7 8 9 10 11 12
G ra d e  Le ve l
Lo
g-
lik
eli
ho
od
We tried various thresholds for our Log-Odds measure
and found that the highest accuracy was achieved by
using all remaining features. 
5.3 Implementation Specifics
We found that we could reduce prediction variance with
two changes to the model.  First, rather than choosing
the single most likely grade language model, we calcu-
late the average grade level of the top N results,
weighted by the relative differences in likelihood
(essentially the expected class).  The tradeoff is a small
bias toward the middle grades.  All results reported here
use this averaging method, with N=2.
Second, to account for vocabulary variation within
longer documents, we partition the document text into
passages of 100 tokens each.  We then obtain a grade
level prediction for each passage.  This creates a distri-
bution of grade levels across the document.  Previous
work (Stenner, 1996, also citing Squires et al, 1983 and
Crawford et al, 1975) suggests that a comprehension
rate of 75% for a text is a desirable target.  We therefore
choose the grade level that lies at the 75th-percentile of
the distribution, interpolating if necessary, to obtain our
final prediction.
6 Evaluation
State-of-the-art performance for this classification task
is hard to estimate.  The results from the most closely
related previous work (Si and Callan, 2001) are not
directly comparable to ours; among other factors, their
task used a dataset trained on science curriculum
descriptions, not text written at different levels of diffi-
culty.  There also appear to be few reliable studies of
human-human interlabeler agreement.  A very limited
study by Gartin et al (1994) gave a mean interlabeler
standard deviation of 1.67 grade levels, but this study
was limited to just 3 samples across 10 judges.  Never-
theless, we believe that an objective element to readabil-
ity assessment exists, and we state our main results in
terms of correlation with difficulty level, so that at least
a broad comparison with existing measures is possible.
Our evaluation looked at four aspects of the model.
First, we measured how well the model trained on our
Web corpus generalized to other, previously unseen, test
data.  Second, we looked at the effect of passage length
on accuracy.  Third, we estimated the effect of addi-
tional training data on the accuracy of the model.
Finally, we looked at how well the model could be
extended to a language other than English ? in this
study, we give results for French.
6.1 Overall Accuracy and Generalization Ability
We used two methods for assessing how well our classi-
fier generalizes beyond the Web training data.  First, we
applied 10-fold cross-validation on the Web corpus
(Kohavi 1995).  This chooses ten random partitions for
each grade?s training data such that 90% is used for
training and 10% held back as a test set.  Second, we
used two previously unseen test sets: a set of 228 lev-
eled documents from Reading A-Z.com, spanning grade
1 through grade 6; and 17 stories from Diagnostic Read-
ing Scales (DRS) spanning grades 1.4 through 5.5.  The
Reading A-Z files were converted from PDF files using
optical character recognition; spelling errors were cor-
rected but sentence boundary errors were left intact to
simulate the kinds of problems encountered with Web
documents.  The DRS files were noise-free.
Because the Smoothed Unigram classifier only mod-
els semantic and not syntactic difficulty, we compared
its accuracy to predictions based on three widely-used
semantic difficulty variables as shown below.  All pre-
diction methods used a 100-token window size.
1. UNK:  The fraction of ?unknown? tokens in the
text, relative to the Dale 3000 word list.  This is the
semantic variable of the Revised Dale-Chall measure.
2. TYPES:  The number of types (unique words) in
a 100-token passage.
3. MLF: The mean log frequency of the passage rel-
ative to a large English corpus.  This is approximately
the semantic variable of the unnormalized Lexile (ver-
sion 1.0) score.  Because the Carroll-Davies-Richman
corpus was not available to us, we used the written sub-
set of the British National Corpus (Burnard, 1995)
which has 921,074 types. (We converted these to the
American equivalents.)
We also included a fourth predictor: the Flesch-
Kincaid score (Kincaid et al 1975), which is a linear
combination of the text?s average sentence length (in
tokens), and the average number of syllables per token.
This was included for illustration purposes only, to ver-
ify the effect of syntactic noise. The results of the evalu-
ation are summarized in Table 1.
On the DRS test collection, the TYPES and Flesch-
Kincaid predictors had the best correlation with labeled
grade level (0.93). TYPES also obtained the best corre-
lation (0.86) for the Reading A-Z documents. However,
Reading A-Z documents were written to pre-established
criteria which includes objective factors such as type/
token ratio (Reading A-Z.com, 2003), so it is not sur-
prising that the correlation is high.  The Smoothed Uni-
gram measure achieved consistently good correlation
(0.63 ? 0.67) on both DRS and Reading A-Z test sets.
Flesch-Kincaid performs much more poorly for the
Reading A-Z data, probably because of the noisy sen-
tence structure. In general, mean log frequency (MLF)
performed worse than expected ? the reasons for this
require further study but may be due to the fact the BNC
corpus may not be representative enough of vocabulary
found at earlier grades.
For Web data, we examined two subsets of the cor-
pus: grades 1? 6 and grades 1? 12. The correlation of all
variables with difficulty dropped substantially for Web
grades 1?6, except for Smoothed Unigram, which
stayed at roughly the same level (0.64) and was the best
performer.  The next best variable was UNK (0.38). For
the entire Web grades 1? 12 data set, the Smoothed Uni-
gram measure again achieved the best correlation
(0.79).  The next best predictor was again UNK (0.63).
On the Web corpus, the largest portions of Smoothed
Unigram?s accuracy gains were achieved in grades 4? 8.
Without cross-grade smoothing, correlation for Web
document predictions fell significantly, to 0.46 and 0.68
for the grade 1-6 and 1-12 subsets respectively.
We measured the type coverage of the language
models created from our Web training corpus, using the
Web (via cross-validation) and Reading A-Z test sets.
Type coverage tells us how often on average a type from
a test passage is found in our statistical model. On the
Reading A-Z test set (Grades 1 ? 6), we observed a
mean type coverage of 89.1%, with a standard deviation
of 6.65%. The mean type coverage for the Web corpus
was 91.69%, with a standard deviation of 5.86%. These
figures suggest that the 17,928 types in the training set
are sufficient to give enough coverage of the test data
that we only need to back off outside the language
model-based estimates for an average of 8-10 tokens in
any 100-token passage.
6.2 Effect of Passage Length on Accuracy
Most readability formulas become unreliable for pas-
sages of less than 100 tokens (Fry 1990).  With Web
applications, it is not uncommon for samples to contain
as few as 10 tokens or less.  For example, educational
Web sites often segment a story or lesson into a series of
image pages, with the only relevant page content being a
caption.  Short passages also arise for tasks such as esti-
mating the reading difficulty of page titles, user queries,
or questionnaire items.  Our hypothesis was that the
Smoothed Unigram model, having more fine-grained
models of word usage, would be less sensitive to pas-
sage length and give superior accuracy for very short
passages, compared to traditional semantic statistics.
In the extreme case, consider two single-word ?pas-
sages?: ?bunny? and ?bulkheads?.  Both words have two
syllables and both occur 5 times in the Carroll-Davies-
Richman corpus.  A variable such as mean log fre-
quency would assign identical difficulty to both of these
passages, while our model would clearly distinguish
them according to each word?s grade usage.
To test this hypothesis, we formed passages of
length L by sampling L consecutive tokens from near
the center of each Reading A-Z test document.  We
compared the RMS error of the Smoothed Unigram pre-
diction on these passages to that obtained from the UNK
semantic variable.  We computed different predictions
for both methods by varying the passage length L from 3
tokens to 100 tokens.
The results are shown in Figure 3.  Accuracy for the
two methods was comparable for passages longer than
about 50 tokens, but Smoothed Unigram obtained statis-
tically significant improvements at the 0.05 level for 4,
5, 6, 7, and 8-word passages.  In those cases, the predic-
tion is accurate enough that very short passages may be
reliably classified into low, medium, and high levels of
difficulty.
6.3 Effect of Training Set Size on Accuracy
We derived the learning curve of our classifier as a func-
tion of the mean model training set size in tokens.  The
lowest mean RMS error of 1.92 was achieved at the
maximum training set size threshold of 32,000 tokens
per grade model.  We fit a monotonically decreasing
power-law function to the data points (Duda et al 2001,
p. 492).  This gave extrapolated estimates for mean
RMS error of about 1.79 at 64,000 tokens per model,
1.71 at 128,000 tokens per model, and 1.50 at 1,000,000
tokens per model.
While doubling the current mean training set size to
64,000 tokens per model would give a useful reduction
in RMS error (about 6.7%), each further reduction of
Files Grade Range
Smoothed
Unigram UNK TYPES MLF FK
DRS 17 1.4 - 5.5 0.67 0.72 0.93 0.50 0.93
Reading A-Z 228 1.0 - 6.0 0.63 0.78 0.86 0.49 0.30
Web (Gr. 1-6) 250 1.0 - 6.0 0.64 0.38 0.26 0.36 0.25
Web (Gr. 1-12) 550 1.0 - 12 0.79 0.63 0.38 0.47 0.47
Table 1. Correlations between predictors and grade level, for the English collections used in our study.
All predictors were trained on the Web corpus, with the Web tests using 10-fold cross-validation.
that magnitude would require a corresponding doubling
of the training set size.  This is the trade-off that must be
considered between overall RMS accuracy and the cost
of gathering labeled data.
6.4 Application to French Web Pages
To test the flexibility of our language model approach,
we did a preliminary study for French reading difficulty
prediction.  We created a corpus of 189 French Web
pages labeled at 5 levels of difficulty, containing a total
of  394,410 tokens and 27,632 types (unstemmed).
The classification algorithm was identical to that
used for English except for a minor change in the fea-
ture selection step.  We found that, because of the
inflected nature of French and the relatively small train-
ing set, we obtained better accuracy by normalizing
types into ?type families? by using a simplified stem-
ming algorithm that removed plurals, masculine/femi-
nine endings, and basic verb endings.
A chart of the actual versus predicted difficulty label
is shown in Figure 4.  The classifier consistently under-
predicts difficulty for the highest level, while somewhat
over-predicting for the lowest level.  This may be partly
due to the bias toward central grades caused by averag-
ing the top 2 predictions.  More work on language-spe-
cific smoothing may also be needed.  With 10-fold
cross-validation, the French model obtained a mean cor-
relation of 0.64 with labeled difficulty.  For comparison,
using the type/token ratio gave a mean correlation of
0.48.  While further work and better training data are
needed, the results seem promising given that only a few
hours of effort were required to gather the French data
and adjust the classifier?s feature selection.
7 Discussion
While word difficulty is well-known to be an excellent
predictor of reading difficulty (Chall & Edgar, 1995), it
was not at all clear how effective our language model
approach would be for predicting Web page reading dif-
ficulty.  It was also unknown how much training data
would be required to get good vocabulary coverage on
Web data.  Although retraining for other applications or
domains may be desirable, two factors appear responsi-
ble for the fact that our classifier, trained on Web data,
generalizes reasonably well to unseen test data from
other sources.
First, smoothing across classes greatly reduces the
training data required for individual grade models. By
?borrowing? word frequency data from nearby grades,
the effective number of types for each grade model is
multiplied by a factor of five or more.  This helps
explain the type coverage of about 90% on our test data.
Second, because we are interested in the relative
likelihoods of grade levels, accurate relative type proba-
bilities are more important than absolute probabilities.
Indeed, trying to learn absolute type probabilities would
be undesirable since it would fit the model too closely to
whatever specific topics were in the training set.  The
important functions of relative likelihood appear to be
general indicators such as the grade when a word is first
introduced into usage, whether it generally increases or
decreases with grade level, and whether it is most fre-
quent in a particular grade range.
Further study is required to explore just how much
this model of vocabulary usage can be generalized to
other languages.  Our results with French suggest that
once we have normalized incoming types to accommo-
Passage Length vs. Prediction Accuracy
(Grade 4: ReadingA-Z)
0
1
2
3
4
5
6
7
1 10 100
Passage Length (Words)
Me
an
 R
MS
 E
rro
r
%-UNK
Smoothed
Unigram
Figure 3.  The effect of passage size on RMS predic-
tion error for Grade 4 documents, comparing
Smoothed Unigram to the UNK semantic variable.
Error bars show 95% confidence interval.  The grey
vertical lines mark logarithmic length.
Actual v s. P re dicte d D ifficulty Le v e l
Fre nch Le v e ls 1 - 5
0
0 .5
1
1 .5
2
2 .5
3
3 .5
4
4 .5
5
0 1 2 3 4 5
L ab e le d  L e ve l
Pr
ed
ict
ed
 L
ev
el
Figure 4.  Actual vs. predicted difficulty label for docu-
ments from the French Web corpus.  The data have
been ?jittered? to show clusters more clearly.  The diag-
onal line represents perfect prediction.
date the morphology of a language, the same core classi-
fier approach may still be applicable, at least for some
family of languages.
8 Conclusions
We have shown that reading difficulty can be estimated
with a simple language modeling approach using a mod-
ified na?ve Bayes classifier.  The classifier's effective-
ness is improved by explicitly modeling class
relationships and smoothing frequency data across
classes as well as within each class.
Our evaluation suggests that reasonably effective
models can be trained with small amounts of easily-
acquired data.  While this data is less-rigorously graded,
such material also greatly reduces the cost of creating a
readability measure, making it easy to modify for spe-
cific tasks or populations. 
As an example of retraining, we showed that the
classifier obtained good correlation with difficulty for at
least two languages, English and French, with the only
algorithm difference being a change in the morphology
handling during feature processing. 
We also showed that the Smoothed Unigram method
is robust for short passages and Web documents.  Some
traditional variables like type/token ratio gave excellent
correlation with difficulty on commercial leveled pas-
sages, but the same statistics performed inconsistently
on Web-based test sets.  In contrast, the Smoothed Uni-
gram method had good accuracy across all test sets.
The problem of reading difficulty prediction lies in
an interesting region between classification and regres-
sion, with close connections to ordinal regression (Mac-
Cullagh, 1980) and discriminative ranking models
(Crammer and Singer, 2001).  While simple methods
like modified na?ve Bayes give reasonably good results,
more sophisticated techniques may give more accurate
predictions, especially at lower grades, where vocabu-
lary progress is measured in months, not years.
Acknowledgements
This work was supported by NSF grant IIS-0096139
and Dept. of Education grant R305G03123.  Any opin-
ions, findings, conclusions, or recommendations
expressed in this material are the authors?, and do not
necessarily reflect those of the sponsors.  We thank the
anonymous reviewers for their comments and Luo Si for
helpful discussions.
References
Burnard, L. (ed.) 1995.  The Users Reference Guide for the
British National Corpus. Oxford: Oxford University
Computing Services.
Carroll, J. B., Davies, P., Richman, B.  1971.  Word Frequency
Book.  Boston: Houghton Mifflin.
Chall, J.S.  1958.  Readability: An appraisal of research and
application.  Bureau of Educational Research Mono-
graphs, No. 34.  Columbus, OH: Ohio State Univ. Press. 
Chall, J.S.  1983.  Stages of Reading Development. McGraw-
Hill.  
Chall, J.S. and Dale, E.  1995.  Readability Revisited: The New
Dale-Chall Readability Formula.  Cambridge, MA:
Brookline Books.
Crammer, K. and Singer, Y.  2001.  Pranking with ranking.
Proceedings of NIPS 2001. 641-647.
Dale, E. and O'Rourke, J.  1981.  The Living Word Vocabulary.
Chicago, IL: World Book/Childcraft International.
Duda, R. O., Hart, P. E., and Stork, D. G.  2001.  Pattern Clas-
sification (Second Edition), Wiley, New York.
Fry, E.  1990.  A readability formula for short passages. J. of
Reading, May 1990, 594-597.
Gale, W., Sampson, G.  1995.  Good-Turing frequency estima-
tion without tears, J. of Quant. Linguistics, v. 2, 217-237.
Gartin, S., et al 1994. W. Virginia Agriculture Teachers? Esti-
mates of Magazine Article Readability. J. Agr. Ed. 35(1).
Hastie, T., Tibshirani, R., Friedman, J.  2001.  The Elements of
Statistical Learning.  Springer-Verlag, New York.
Kincaid, J., Fishburne, R., Rodgers, R. and Chissom, B.  1975.
Derivation of new readability formulas for navy enlisted
personnel.  Branch Report 8-75.  Millington, TN: Chief
of Naval Training.
Klare, G. R.  1963.  The Measurement of Readability.  Ames,
IA.  Iowa State University Press.
Kohavi, R.  1995.  A study of cross-validation and bootstrap
for accuracy estimation and model selection.  Proc. of the
14th Int. Joint Conf. on Artificial Intelligence (IJCAI
1995).  Montreal, Canada.  1137 - 1145.
MacCullagh, P.  1980.  Regression models for ordinal data. J.
of the Royal Statistical Society B, vol.42, 109-142.
Mitchell, J.V.  1985.  The Ninth Mental Measurements Year-
book. Lincoln, Nebraska: Univ. of Nebraska Press.
Mladenic D., and Grobelnik, M.  1998.  Feature selection for
classification based on text hierarchy. Working Notes of
Learning from Text and the Web, CONALD-98. Carnegie
Mellon Univ., Pittsburgh, PA.
Reading A-Z.com  2003.  Reading A-Z Leveling and Correla-
tion Chart (HTML page). http://www.readinga-z.com/
newfiles/correlate.html
Si, L. and Callan, J.  2001.  A statistical model for scientific
readability.   Proc. of CIKM 2001.  Atlanta, GA, 574-576.
Stenner, A. J., Horabin, I., Smith, D.R., and Smith, M. 1988.
The Lexile Framework.  Durham, NC: Metametrics.
Proceedings of NAACL HLT 2007, pages 460?467,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Lexical and Grammatical Features to Improve Readability 
Measures for First and Second Language Texts 
Michael J. Heilman 
 
Kevyn Collins-
Thompson 
Jamie Callan 
 
Maxine Eskenazi 
 
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
4502 Newell Simon Hall 
Pittsburgh, PA 15213-8213 
 {mheilman,kct,callan,max}@cs.cmu.edu 
 
 
Abstract 
This work evaluates a system that uses in-
terpolated predictions of reading difficulty 
that are based on both vocabulary and 
grammatical features.  The combined ap-
proach is compared to individual gram-
mar- and language modeling-based 
approaches.  While the vocabulary-based 
language modeling approach outper-
formed the grammar-based approach, 
grammar-based predictions can be com-
bined using confidence scores with the 
vocabulary-based predictions to produce 
more accurate predictions of reading dif-
ficulty for both first and second language 
texts.  The results also indicate that gram-
matical features may play a more impor-
tant role in second language readability 
than in first language readability. 
1 Introduction 
The REAP tutoring system (Heilman, et al 2006), 
aims to provide authentic reading materials of the 
appropriate difficulty level, in terms of both vo-
cabulary and grammar, for English as a Second 
Language students.  An automatic measure of read-
ability that incorporated both lexical and gram-
matical features was thus needed. 
For first language (L1) learners (i.e., children 
learning their native tongue), reading level has 
been predicted using a variety of techniques, based 
on models of a student?s lexicon, grammatical sur-
face features such as sentence length (Flesch, 
1948), or combinations of such features (Schwarm 
and Ostendorf, 2005).  It was shown by Collins-
Thompson and Callan (2004) that a vocabulary-
based language modeling approach was effective at 
predicting the readability of grades 1 to 12 of Web 
documents of varying length, even with high levels 
of noise.   
Prior work on first language readability by 
Schwarm and Ostendorf (2005) incorporated 
grammatical surface features such as parse tree 
depth and average number of verb phrases.  This 
work combining grammatical and lexical features 
was promising, but it was not clear to what extent 
the grammatical features improved predictions.   
Also, discussions with L2 instructors suggest 
that a more detailed grammatical analysis of texts 
that examines features such as passive voice and 
various verb tenses can provide better features with 
which to predict reading difficulty.  One goal of 
this work is to show that the use of pedagogically 
motivated grammatical features (e.g., passive 
voice, rather than the number of words per sen-
tence) can improve readability measures based on 
lexical features alone. 
One of the differences between L1 and L2 read-
ability is the timeline and processes by which first 
and second languages are acquired.  First language 
acquisition begins at infancy, and the primary 
grammatical structures of the target language are 
acquired by age four in typically developing chil-
460
dren (Bates, 2003).  That is, most grammar is ac-
quired prior to the beginning of a child?s formal 
education.  Therefore, most grammatical features 
seen at high reading levels such as high school are 
present with similar frequencies at low reading 
levels such as grades 1-3 that correspond to ele-
mentary school-age children.  It should be noted 
that sentence length is one grammar-related differ-
ence that can be observed as L1 reading level in-
creases.  Sentences are kept short in texts for low 
L1 reading levels in order to reduce the cognitive 
load on child readers.  The average sentence length 
of texts increases with the age and reading level of 
the intended audience.  This phenomenon has been 
utilized in early readability measures (Flesch, 
1948).  Vocabulary change, however, continues 
even into adulthood, and has been shown to be a 
more effective predictor of L1 readability than 
simpler measures such as sentence length (Collins-
Thompson and Callan, 2005). 
Second language learners, unlike their L1 coun-
terparts, are still very much in the process of ac-
quiring the grammar of their target language.  In 
fact, even intermediate and advanced students of 
second languages, who correspond to higher L2 
reading levels, often struggle with the grammatical 
structures of their target language.  This phenome-
non suggests that grammatical features may play a 
more important role in predicting and measuring 
L2 readability.  That is not to say, however, that 
vocabulary cannot be used to predict L2 reading 
levels.  Second language learners are learning both 
vocabulary and grammar concurrently, and reading 
materials for this population are chosen or au-
thored according to both lexical and grammatical 
complexity.  Therefore, the authors predict that a 
readability measure for texts intended for second 
language learners that incorporates both grammati-
cal and lexical features could clearly outperform a 
measure based on only one of these two types of 
features. 
This paper begins with descriptions of the lan-
guage modeling and grammar-based prediction 
systems.  A description of the experiments follows 
that covers both the evaluation metrics and corpora 
used.  Experimental results are presented, followed 
by a discussion of these results, and a summary of 
the conclusions of this work.  
2 Language Model Readability Prediction 
for First Language Texts 
Statistical language modeling exploits patterns of 
use in language.  To build a statistical model of 
text, training examples are used to collect statistics 
such as word frequency and order.  Each training 
example has a label that tells the model the ?true? 
category of the example.  In this approach, one 
statistical model is built for each grade level to be 
predicted. 
The statistical language modeling approach has 
several advantages over traditional readability 
formulas, which are usually based on linear regres-
sion with two or three variables.  First, a language 
modeling approach generally gives much better 
accuracy for Web documents and short passages 
(Collins-Thompson and Callan, 2004).  Second, 
language modeling provides a probability distribu-
tion across all grade models, not just a single pre-
diction.  Third, language modeling provides more 
data on the relative difficulty of each word in the 
document.  This might allow an application, for 
example, to provide more accurate vocabulary as-
sistance. 
The statistical model used for this study is 
based on a variation of the multinomial Na?ve 
Bayes classifier.  For a given text passage T, the 
semantic difficulty of T relative to a specific grade 
level Gi is predicted by calculating the likelihood 
that the words of T were generated from a repre-
sentative language model of Gi.  This likelihood is 
calculated for each of a number of language mod-
els, corresponding to reading difficulty levels.  The 
reading difficulty of the passage is then estimated 
as the grade level of the language model most 
likely to have generated the passage T. 
The language models employed in this work are 
simple: they are based on unigrams and assume 
that the probability of a token is independent of the 
surrounding tokens.  A unigram language model is 
simply defined by a list of types (words) and their 
individual probabilities.  Although this is a weak 
model, it can be effectively trained from less la-
beled data than more complex models, such as bi-
gram or trigram models.  Additionally, higher 
order n-gram models might capture grammatical as 
well as lexical differences.  The relative contribu-
tions of grammatical and lexical features were thus 
better distinguished by using unigram language 
461
models that more exclusively focus on lexical dif-
ferences. 
In this language modeling approach, a genera-
tive model is assumed for a passage T, in which a 
hypothetical author generates the tokens of T by: 
1. Choosing a grade language model, Gi, 
from the set G = {Gi} of 12 unigram language 
models, according to a prior probability distri-
bution P(Gi). 
2. Choosing a passage length |T| in tokens ac-
cording to a probability distribution P(|T|). 
3. Sampling |T| tokens from Gi?s multinomial 
word distribution according to the ?na?ve? as-
sumption that each token is independent of all 
other tokens in the passage, given the language 
model Gi. 
These assumptions lead to the following expres-
sion for the probability of T being generated by 
language model Gi according to a multinomial dis-
tribution: 
 
?
?
=
Vw
wC
i
i
wC
GwPTTPGTP )!(
)|(|!||)(|)|(
)(
 
 
Next, according to Bayes? Theorem: 
  
)(
)|()()|(
TP
GTPGPTGP iii = . 
 
Substituting (1) into (2), taking logarithms, and 
simplifying produces: 
 
SRwC
GwPwCTGP
Vw
i
Vw
i
loglog)!(log
)|(log)()|(log
++?
=


?
?
, 
 
where V is the list of all types in the passage T, w is 
a type in V, and C(w) is the number of tokens with 
type w in T.  For simplicity, the factor R represents 
the contribution of the prior P(Gi), and S represents 
the contribution of the passage length |T|, given the 
grade level.   
Two further assumptions are made to simplify 
the illustration: 
1. That all grades are equally likely a priori.   
That is, 
G
i N
GP 1)( =  where NG is the number 
of grade levels.  For example, if there are 12 
grade levels, then NG = 12.  This allows log R to 
be ignored. 
2. That all passage lengths (up to a maximum 
length M) are equally likely.  This allows log S 
to be ignored. 
These may be poor assumptions in a real appli-
cation, but they can be easily included or excluded 
in the model as desired.  The log C(w)! term can 
also be ignored because it is constant across levels.  
Under these conditions, an extremely simple form 
for the grade likelihood remains.  In order to find 
which model Gi maximizes Equation (3), the 
model which Gi that maximizes the following 
equation must be found: 
 
)|(log)()|( i
Vw
i GwPwCGTL 
?
=  
 
This is straightforward to compute: for each token 
in the passage T, the log probability of the token 
according to the language model of Gi is calcu-
lated.  Summing the log probabilities of all tokens 
produces the overall likelihood of the passage, 
given the grade.  The grade level with the maxi-
mum likelihood is then chosen as the final read-
ability level prediction. 
This study employs a slightly more sophisti-
cated extension of this model, in which a sliding 
window is moved across the text, with a grade pre-
diction being made for each window.  This results 
in a distribution of grade predictions.  The grade 
level corresponding to a given percentile of this 
distribution is chosen as the prediction for the en-
tire document.  The values used in these experi-
ments for the percentile thresholds for L1 and L2 
were chosen by accuracy on held-out data. 
3 Grammatical Construction Readability 
Prediction for Second Language Texts 
The following sections describe the approach to 
predicting readability based on grammatical fea-
tures.  As with any classifier, two components are 
required to classify texts by their reading level: 
first, a definition for and method of identifying 
features; second, an algorithm for using these fea-
tures to classify a given text.  A third component, 
training data, is also necessary in this classification 
462
task.  The corpus of materials used for training and 
testing is discussed in a subsequent section. 
3.1 Features for Grammar-based Prediction 
L2 learners usually learn grammatical patterns ex-
plicitly from grammar explanations in L2 text-
books, unlike their L1 counterparts who learn them 
implicitly through natural interactions.  Grammati-
cal features would therefore seem to be an essential 
component of an automatic readability measure for 
L2 learners, who must actively acquire both the 
lexicon and grammar of their target language. 
The grammar-based readability measure relies 
on being able to automatically identify grammati-
cal constructions in text.  Doing so is a multi-step 
process that begins by syntactically parsing the 
document.  The Stanford Parser (Klein and Man-
ning, 2002) was used to produce constituent struc-
ture trees.  The choice of parser is not essential to 
the approach, although the accuracy of parsing 
does play a role in successful identification of cer-
tain grammatical patterns. PCFG scores from the 
parser were also used to filter out some of the ill-
formed text present in the test corpora.  The default 
training set of Penn Treebank (Marcus et al 1993) 
was used for the parser because the domain and 
style of those texts actually matches fairly well 
with the domain and style of the texts on which a 
reading level predictor for second language learn-
ers might be used. 
Once a document is parsed, the predictor uses 
Tgrep2 (Rohde, 2005), a tree structure searching 
tool, to identify instances of the target patterns.  A 
Tgrep2 pattern defines dominance, sisterhood, 
precedence, and other relationships between nodes 
in the parse tree for a sentence.  A pattern can also 
place constraints on the terminal symbols (e.g., 
words and punctuation), such that a pattern might 
require a form of the copula ?be? to exist in a cer-
tain position in the construction.  An example of a 
TGrep2 search pattern for the progressive verb 
tense is the following: 
 
?VP < /^VB/ < (VP < VBG)? 
 
Searching for this pattern returns sentences in 
which a verb phrase (VP) dominates an auxiliary 
verb (whose symbol begins with VB) as well as 
another verb phrase, which in turn dominates a 
verb in gerund form (VBG).  An example of a 
matching sentence is, ?The student was reading a 
book,? shown in Figure 2. 
 
Figure 2: The parse tree for an example sentence 
that matches a pattern for progressive verb tense. 
 
A set of 22 relevant grammatical constructions 
were identified from grammar textbooks for three 
different ESL levels (Fuchs et al, 2005).  These 
grammar textbooks had different authors and pub-
lishers than the ones used in the evaluation corpora 
in order to minimize the chance of experimental 
results not generalizing beyond the specific materi-
als employed in this study.  The ESL levels corre-
spond to the low-intermediate (hereafter, level 3), 
high-intermediate (level 4), and advanced (level 5) 
courses at the University of Pittsburgh?s English 
Language Institute.  The constructions identified in 
these grammar textbooks were then implemented 
in the form of Tgrep2 patterns.   
 
Feature  Lowest Level Highest Level 
Passive Voice 0.11 0.71 
Past Participle 0.28 1.63 
Perfect Tense 0.01 0.33 
Relative Clause 0.54 0.60 
Continuous 
Tense 
0.19 0.27 
Modal 0.80 1.44 
Table 1: The rates of occurrence per 100 words of 
a few of the features used by the grammar-based 
predictor.  Rates are shown for the lowest (2) and 
highest (5) levels in the L2 corpus. 
 
The rate of occurrence of constructions was 
calculated on a per word basis.  A per-word rather 
a book 
The student 
S 
VP 
VBD VP 
VBG 
NP 
was 
reading 
NP 
463
than a per-sentence measure was chosen because a 
per-sentence measure would depend too greatly on 
sentence length, which also varies by level.  It was 
also desirable to avoid having sentence length con-
founded with other features.  Table 1 shows that 
the rates of occurrence of certain constructions be-
come more frequent as level increases.  This sys-
tematic variation across levels is the basis for the 
grammar-based readability predictions. 
A second feature set was defined that consisted 
of 12 grammatical features that could easily be 
identified without computationally intensive syn-
tactic parsing.  These features included sentence 
length, the various verb forms in English, includ-
ing the present, progressive, past, perfect, continu-
ous tenses, as well as part of speech labels for 
words.  The goal of using a second feature set was 
to examine how dependent prediction quality was 
on a specific set of features, as well as to test the 
extent to which the output of syntactic parsing 
might improve prediction accuracy. 
3.2 Algorithm for Grammatical Feature-
based Classification 
A k-Nearest Neighbor (kNN) algorithm is used for 
classification based on the grammatical features 
described above.  The kNN algorithm is an in-
stance-based learning technique originally devel-
oped by Cover and Hart (1967) by which a test 
instance is classified according to the classifica-
tions of a given number (k) of training instances 
closest to it.  Distance is defined in this work as the 
Euclidean distance of feature vectors.  Mitchell 
(1997) provides more details on the kNN algo-
rithm.  This algorithm was chosen because it has 
been shown to be effective in text classification 
tasks when compared to other popular methods 
(Yang 1999).  A k value of 12 was chosen because 
it provided the best performance on held-out data. 
Additionally, it is straightforward to calculate 
a confidence measure with which kNN predictions 
can be combined with predictions from other clas-
sifiers?in this case with predictions from the uni-
gram language modeling-based approach described 
above.  A confidence measure was important in 
this task because it provided a means with which to 
combine the grammar-based predictions with the 
predictions from the language modeling-based 
predictor while maintaining separate models for 
each type of feature.  These separate models were 
maintained to better determine the relative contri-
butions of grammatical and lexical features. 
A static linear interpolation of predictions us-
ing the two approaches led to only minimal reduc-
tions of prediction error, likely because predictions 
from the poorer performing grammar-based classi-
fier were always given the same weight.  However, 
with the confidence measures, predictions from the 
grammar-based classifier could be given more 
weight when the confidence measure was high, and 
less weight when the measure was low and the 
predictions were likely to be inaccurate.  The case-
dependent interpolation of prediction values al-
lowed for the effective combination of language 
modeling- and grammar-based predictions.  
The confidence measure employed is the pro-
portion of the k most similar training examples, or 
nearest neighbors, that agree with the final label 
chosen for a given test document.  For example, if 
seven of ten neighbors have the same label, then 
the confidence score will be 0.6.  The interpolated 
readability prediction value is calculated as fol-
lows: 
 
LI = LLM + CkNN * LGR, 
 
where LLM is the language model-based prediction, 
LGR is the grammar-based prediction from the kNN 
algorithm, and CkNN is the confidence value for the 
kNN prediction.  The language modeling approach 
is treated as a black box, but it would likely be 
beneficial to have confidence measures for it as 
well. 
4 Descriptions of Experiments 
This section describes the experiments used to test 
the hypothesis that grammar-based features can 
improve readability measures for English, espe-
cially for second language texts.  The measures 
and cross-validation setup are described.  A de-
scription of the evaluation corpora of labeled first 
and second language texts follows. 
4.1 Experimental Setup 
Two measurements were used in evaluating the 
effectiveness of the reading level predictions.  
First, the correlation coefficient evaluated whether 
the trends of prediction values matched the trends 
for human-labeled texts.  Second, the mean 
squared error of prediction values provided a 
464
measure of how correct each of the predictors was 
on average,  penalizing more severe errors more 
heavily.  Mean square error was used rather than 
simple accuracy (i.e., number correct divided by 
sample size) because the task of readability predic-
tion is more akin to regression than classification.  
Evaluation measures such as accuracy, precision, 
and recall are thus less meaningful for readability 
prediction tasks because they do not capture the 
fact that an error of 4 levels is more costly than an 
error of a single level. 
A nine-fold cross-validation was employed.  
The data was first split into ten sets.  One set was 
used as held-out data for selecting the parameter k 
for the kNN algorithm and the percentile value for 
the language modeling predictor, and then the re-
maining nine were used to evaluate the quality of 
predictions.  Each of these nine was in turn se-
lected as the test set, and the other eight were used 
as training data. 
4.2 Corpora of Labeled Texts 
Two corpora of labeled texts were used in the 
evaluation.  The first corpus was from a set of texts 
gathered from the Web for a prior evaluation of the 
language modeling approach.  The 362 texts had 
been assigned L1 levels (1-12) by grade school 
teachers, and consisted of approximately 250,000 
words.  For more details on the L1 corpus, see 
(Collins-Thompson and Callan, 2005). 
The second corpora consisted of textbook mate-
rials (Adelson-Goldstein and Howard, 2004, for 
level 2; Ediger and Pavlik, 2000, for levels 3 and 4; 
Silberstein, 2002, for level 5) from a series of Eng-
lish as a Second Language reading courses at the 
English Language Institute at the University of 
Pittsburgh.  The four reading practice textbooks 
that constitute this corpus were from separate au-
thors and publishers than the grammar textbooks 
used to select and define grammatical features.  
The reading textbooks in the corpus are used in 
courses intended for beginning (level 2) through 
advanced (level 5) students.  The textbooks were 
scanned into electronic format, and divided into 
fifty roughly equally sized files.  This second lan-
guage corpus consisted of approximately 200,000 
words. 
Although the sources and formats of the two 
corpora were different, they share a number of 
characteristics.  Their size was roughly equal. The 
documents in both were also fairly but not per-
fectly evenly distributed across the levels.  Both 
corpora also contained a significant amount of 
noise which made accurate prediction of reading 
level more challenging.  The L1 corpus was from 
the Web, and therefore contained navigation 
menus, links, and the like.  The texts in the L2 cor-
pus also contained significant levels of noise due to 
the inclusion of directions preceding readings, ex-
ercises and questions following readings, as well as 
labels on figures and charts.  The scanned files 
were not hand-corrected in this study, in part to test 
that the measures are robust to noise, which is pre-
sent in the Web documents for which the readabil-
ity measures are employed in the REAP tutoring 
system.  
The grammar-based prediction seems to be 
more significantly negatively affected by the noise 
in the two corpora because the features rely more 
on dependencies between different words in the 
text.  For example, if a word happened to be part of 
an image caption rather than a well-formed sen-
tence, the unigram language modeling approach 
would only be affected for that word, but the 
grammar-based approach might be affected for 
features spanning an entire clause or sentence. 
5 Results of Experiments 
The results show that for both the first and sec-
ond language corpora, the language modeling 
(LM) approach alone produced more accurate pre-
dictions than the grammar-based approach alone.  
The mean squared error values (Table 2) were 
lower, and the correlation coefficients (Table 3) 
were higher for the LM predictor than the gram-
mar-based predictor.   
The results also indicate that while grammar-
based predictions are not as accurate as the vo-
cabulary-based scores, they can be combined with 
vocabulary-based scores to produce more accurate 
interpolated scores.  The interpolated predictions 
combined by using the kNN confidence measure 
were slightly and in most tests significantly more 
accurate in terms of mean squared error than the 
predictions from either single measure.   Interpola-
tion using the first set of grammatical features led 
to 7% and 22% reductions in mean squared error 
on the L1 and L2 corpora, respectively.  These re-
sults were verified using a one-tailed paired t-test 
465
of the squared error values of the predictions, and 
significance levels are indicated in Table 2. 
 
Mean Squared Error Values 
Test Set (Num. Levels) L1(12) L2(4) 
Language Modeling 5.02 0.51 
Grammar 10.27 1.08 
Interpolation 4.65* 0.40** 
Grammar2 (feature set #2) 12.77 1.26 
Interp2. (feature set #2) 4.73 0.43* 
Table 2.  Comparison of Mean Squared Error of 
predictions compared to human labels for different 
methods.  Interpolated values are significantly bet-
ter compared to language modeling predictions 
where indicated (* = p<0.05, ** = p<0.01). 
 
Correlation Coefficients 
Test Set (Num. Levels) L1(12) L2(4) 
Language Modeling 0.71 0.80 
Grammar 0.46 0.55 
Interpolation 0.72 0.83 
Grammar2 (feature set #2) 0.34 0.48 
Interp2. (feature set #2) 0.72 0.81 
Table 3.  Comparison of Correlation Coefficients 
of prediction values to human labels for different 
prediction methods. 
 
The trends were similar for both sets of gram-
matical features.  However, the first set of features 
that included complex syntactic constructs led to 
better performance than the second set, which in-
cluded only verb tenses, part of speech labels, and 
sentence length.  Therefore, when syntactic parsing 
is not feasible because of corpora size, it seems 
that grammatical features requiring only part-of-
speech tagging and word counts may still improve 
readability predictions.  This is practically impor-
tant because parsing can be too computationally 
intensive for large corpora. 
All prediction methods performed better, in 
terms of correlations, on the L2 corpus than on the 
L1 corpus.  The L2 corpus is somewhat smaller in 
size and should, if only on the basis of training ma-
terial available to the prediction algorithms, actu-
ally be more difficult to predict than the L1 corpus.  
To ensure that the range of levels was not causing 
the four-level L2 corpus to have higher predictions 
than the twelve-level L1 corpus, the L1 corpus was 
also divided into four bins (grades 1-3, 4-6, 7-9, 
10-12).  The accuracy of predictions for the binned 
version of the L1 corpus was not substantially dif-
ferent than for the 12-level version. 
6 Discussion 
In the experimental tests, the LM approach was 
more effective for measuring both L1 and L2 read-
ability.  There are several potential causes of this 
effect.  First, the language modeling approach can 
utilize all the words as they appear in the text as 
features, while the grammatical features were cho-
sen and defined manually.  As a result, the LM 
approach can make measurements on a text for as 
many features as there are words in its lexicon.  
Additionally, the noise present in the corpora likely 
affected the grammar-based approach dispropor-
tionately more because that method relies on accu-
rate parsing of relationships between words. 
Additionally, English is a morphologically im-
poverished language compared to most languages.  
Text classification, information retrieval, and many 
other human language technology tasks can be ac-
complished for English without accounting for 
grammatical features such as morphological inflec-
tions.  For example, an information retrieval sys-
tem can perform reasonably well in English 
without performing stemming, which does not 
greatly increase performance except when queries 
and documents are short (Krovetz, 1993). 
However, most languages have a rich morphol-
ogy by which a single root form may have thou-
sands or perhaps millions of inflected or derived 
forms.  Language technologies must account for 
morphological features in such languages or the 
vocabulary grows so large that it becomes unman-
ageable.  Lee (2004), for example, showed that 
morphological analysis can improve the quality of 
statistical machine translation for Arabic.  Thus it 
seems that grammatical features could contribute 
even more to measures of readability for texts in 
other languages. 
That said, the use of grammatical features ap-
pears to play a more important role in readability 
measures for L2 than for L1.  When interpolated 
with grammar-based scores, the reduction of mean 
squared error over the language modeling approach 
for L1 was only 7%, while for L2 the reduction or 
squared error was 22%.  An evaluation on corpora 
with less noise would likely bring out these differ-
466
ences further and show grammar to be an even 
more important factor in second language readabil-
ity.  This result is consistent with the fact that sec-
ond language learners are still in the process of 
acquiring the basic grammatical constructs of their 
target language. 
7 Conclusion 
The results of this work suggest that grammatical 
features can play a role in predicting reading diffi-
culty levels for both first and second language texts 
in English.  Although a vocabulary-based language 
modeling approach outperformed the grammar-
based predictor, an interpolated measure using 
confidence scores for the grammar-based predic-
tions showed improvement over both individual 
measures.  Also, grammar appears to play a more 
important role in second language readability than 
in first language readability.  Ongoing work aims 
to improve grammar-based readability by reducing 
noise in training data, automatically creating larger 
grammar feature sets, and applying more sophisti-
cated modeling techniques. 
8 Acknowledgements 
We would like to acknowledge Lori Levin for use-
ful advice regarding grammatical constructions, as 
well as the anonymous reviewers for their sugges-
tions.  
This material is based on work supported by 
NSF grant IIS-0096139 and Dept. of Education 
grant R305G03123. Any opinions, findings, con-
clusions or recommendations expressed in this ma-
terial are the authors', and do not necessarily reflect 
those of the sponsors. 
References 
J. Adelson-Goldstein and L. Howard. 2004.  Read and 
Reflect 1.  Oxford University Press, USA. 
E. Bates. 2003. On the nature and nurture of language. 
In R. Levi-Montalcini, D. Baltimore, R. Dulbecco, F. 
Jacob, E. Bizzi, P. Calissano, & V. Volterra (Eds.), 
Frontiers of biology: The brain of Homo sapiens (pp. 
241?265). Rome: Istituto della Enciclopedia Italiana 
fondata da Giovanni Trecanni. 
M. Fuchs, M. Bonner, M. Westheimer. 2005.  Focus on 
Grammar, 3rd Edition.  Pearson ESL. 
K. Collins-Thompson and J. Callan. 2004.  A language 
modeling approach to predicting reading difficulty.  
Proceedings of the HLT/NAACL Annual Conference. 
T. Cover and P. Hart. 1967.  Nearest neighbor pattern 
classification.  IEEE Transactions on Information 
Theory, 13, 21-27. 
A. Ediger and C. Pavlik.  2000.  Reading Connections 
Intermediate.  Oxford University Press, USA. 
A. Ediger and C. Pavlik.  2000.  Reading Connections 
High Intermediate.  Oxford University Press, USA. 
M. Heilman, K. Collins-Thompson, J. Callan & M. Es-
kenazi. 2006. Classroom success of an Intelligent Tu-
toring System for lexical practice and reading 
comprehension. Proceedings of the Ninth Interna-
tional Conference on Spoken Language Processing. 
D. Klein and C. D. Manning. 2002. Fast Exact Inference 
with a Factored Model for Natural Language Parsing. 
Advances in Neural Information Processing Systems 
15 (NIPS 2002), December 2002. 
R. Krovetz. 1993. Viewing morphology as an inference 
process. SIGIR-93, 191?202. 
Y. Lee. 2004.  Morphological Analysis for Statistical 
Machine Translation.  Proceedings of the 
HLT/NAACL Annual Conference. 
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. 
"Building a large annotated corpus of English: the 
Penn Treebank." Computational Linguistics, 19(2). 
T. Mitchell. 1997. Machine Learning.  The McGraw-
Hill Companies, Inc.  pp. 231-236. 
D. Rohde. 2005. Tgrep2 User Manual. 
http://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf. 
S. Schwarm, and M. Ostendorf. 2005.  Reading Level 
Assessment Using Support Vector Machines and Sta-
tistical Language Models.  Proceedings of the Annual 
Meeting of the Association for Computational Lin-
guistics. 
S. Silberstein, B. K. Dobson, and M. A. Clarke. 2002. 
Reader's Choice, 4th edition.  University of Michigan 
Press/ESL. 
Y. Yang. 1999.  A re-examination of text categorization 
methods.  Proceedings of ACM SIGIR Conference on 
Research and Development in Information Retrieval 
(SIGIR'99, pp 42--49). 
467
Proceedings of NAACL HLT 2007, pages 476?483,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic and human scoring of word definition responses
Kevyn Collins-Thompson and Jamie Callan
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, U.S.A. 15213-8213
{kct|callan}@cs.cmu.edu
Abstract
Assessing learning progress is a critical
step in language learning applications and
experiments. In word learning, for exam-
ple, one important type of assessment is
a definition production test, in which sub-
jects are asked to produce a short defini-
tion of the word being learned. In current
practice, each free response is manually
scored according to how well its mean-
ing matches the target definition. Manual
scoring is not only time-consuming, but
also limited in its flexibility and ability to
detect partial learning effects.
This study describes an effective auto-
matic method for scoring free responses
to definition production tests. The algo-
rithm compares the text of the free re-
sponse to the text of a reference definition
using a statistical model of text semantic
similarity that uses Markov chains on a
graph of individual word relations. The
model can take advantage of both corpus-
and knowledge-based resources. Evalu-
ated on a new corpus of human-judged
free responses, our method achieved sig-
nificant improvements over random and
cosine baselines in both rank correlation
and label error.
1 Introduction
Human language technologies are playing an in-
creasingly important role in the science and prac-
tice of language learning. For example, intelligent
Computer Assisted Language Learning (CALL) sys-
tems are being developed that can automatically tai-
lor lessons and questions to the needs of individual
students (Heilman et al, 2006). One critical task
that language tutors, word learning experiments, and
related applications have in common is assessing the
learning progress of the student or experiment sub-
ject during the course of the session.
When the task is learning new vocabulary, a vari-
ety of tests have been developed to measure word
learning progress. Some tests, such as multiple-
choice selection of a correct synonym or cloze com-
pletion, are relatively passive. In production tests,
on the other hand, students are asked to write or say
a short phrase or sentence that uses the word being
learned, called the target word, in a specified way.
In one important type of production test, called a
definition production test, the subject is asked to de-
scribe the meaning of the target word, as they under-
stand it at that point in the session. The use of such
tests has typically required a teacher or researcher
to manually score each response by judging its sim-
ilarity in meaning to the reference definition of the
target word. The resulting scores can then be used
to analyze how a person?s learning of the word re-
sponded to different stimuli, such as seeing the word
used in context. A sample target word and its ref-
erence definition, along with examples of human-
judged responses, are given in Sections 3.3 and 4.1.
However, manual scoring of the definition re-
sponses has several drawbacks. First, it is time-
consuming and must be done by trained experts.
Moreover, if the researcher wanted to test a new hy-
476
pothesis by examining the responses with respect to
a different but related definition, the entire set of re-
sponses would have to be manually re-scored against
the new target. Second, manual scoring can often
be limited in its ability to detect when partial learn-
ing has taken place. This is due to the basic trade-
off between the sophistication of the graded scoring
scale, and the ease and consistency with which hu-
man judges can use the scale. For example, it may
be that the subject did not learn the complete mean-
ing of a particular target word, but did learn that this
target word had negative connotations. The usual
binary or ternary score would provide no or little
indication of such effects. Finally, because manual
scoring almost always must be done off-line after the
end of the session, it presents an obstacle to our goal
of creating learning systems that can adapt quickly,
within a single learning session.
This study describes an effective automated
method for assessing word learning by scoring free
responses to definition production tests. The method
is flexible: it can be used to analyze a response with
respect to whatever reference target(s) the teacher or
researcher chooses. Such a test represents a pow-
erful new tool for language learning research. It is
also a compelling application of human language
technologies research on semantic similarity, and
we review related work for that area in Section 2.
Our probabilistic model for computing text seman-
tic similarity, described in Section 3, can use both
corpus-based and knowledge-based resources. In
Section 4 we describe a new dataset of human def-
inition judgments and use it to measure the effec-
tiveness of the model against other measures of text
similarity. Finally, in Section 5 we discuss further
directions and applications of our work.
2 Related Work
The problem of judging a subject response against a
target definition is a type of text similarity problem.
Moreover, it is a text semantic similarity task, since
we require more than measuring direct word overlap
between the two text fragments. For example, if the
definition of the target word ameliorate is to improve
something and the subject response is make it better,
the response clearly indicates that the subject knows
the meaning of the word, and thus should receive a
high score, even though the response and the target
definition have no words in common.
Because most responses are short (1 ? 10 words)
our task falls somewhere between word-word simi-
larity and passage similarity. There is a broad field
of existing work in estimating the semantic similar-
ity of individual words. This field may be roughly
divided into two groups. First, there are corpus-
based measures, which use statistics or models de-
rived from a large training collection. These require
little or no human effort to construct, but are limited
in the richness of the features they can reliably repre-
sent. Second, there are knowledge-based measures,
which rely on specialized resources such as dictio-
naries, thesauri, experimental data, WordNet, and so
on. Knowledge-based measures tend to be comple-
mentary to a corpus-based approach and emphasize
precision in favor of recall. This is discussed further,
along with a good general summary of text semantic
similarity work, by (Mihalcea et al, 2006).
Because of the fundamental nature of the se-
mantic similarity problem, there are close connec-
tions with other areas of human language tech-
nologies such as information retrieval (Salton and
Lesk, 1971), text alignment in machine transla-
tion (Jayaraman and Lavie, 2005), text summariza-
tion (Mani and Maybury, 1999), and textual co-
herence (Foltz et al, 1998). Educational applica-
tions include automated scoring of essays, surveyed
in (Valenti et al, 2003), and assessment of short-
answer free-response items (Burstein et al, 1999).
As we describe in Section 3, we use a graph to
model relations between words to perform a kind
of semantic smoothing on the language models of
the subject response and target definition before
comparing them. Several types of relation, such
as synonymy and co-occurrence, may be combined
to model the interactions between terms. (Cao
et al, 2005) also formulated a term dependency
model combining multiple term relations in a lan-
guage modeling framework, applied to information
retrieval. Our graph-based approach may be viewed
as a probabilistic variation on the spreading activa-
tion concept, originally proposed for word-word se-
mantic similarity by (Quillian, 1967).
Finally, (Mihalcea et al, 2006) describe a text se-
mantic similarity measure that combines word-word
similarities between the passages being compared.
477
Due to limitations in the knowledge-based similarity
measures used, semantic similarity is only estimated
between words with the same part-of-speech. Our
graph-based approach can relate words of different
types and does not have this limitation. (Mihalcea
et al, 2006) also evaluate their method in terms of
paraphrase recognition using binary judgments. We
view our task as somewhat different than paraphrase
recognition. First, our task is not symmetric: we do
not expect the target definition to be a paraphrase
of the subject?s free response. Second, because we
seek sensitive measures of learning, we want to dis-
tinguish a range of semantic differences beyond a
binary yes/no decision.
3 Statistical Text Similarity Model
We start by describing relations between pairs of
terms using a general probability distribution. These
pairs can then combine into a graph, which we can
apply to define a semantic distance between terms.
3.1 Relations between individual words
One way to model word-to-word relationships is us-
ing a mixture of links, where each link defines a par-
ticular type of relationship. In a graph, this may be
represented by a pair of nodes being joined by mul-
tiple weighted edges, with each edge correspond-
ing to a different link type. Our link-based model
is partially based on one defined by (Toutanova et
al., 2004) for prepositional attachment. We allow
directed edges because some relationships such as
hypernyms may be asymmetric. The following are
examples of different types of links.
1. Stemming: Two words are based on common
morphology. Example: stem and stemming.
We used Porter stemming (Porter, 1980).
2. Synonyms and near-synonyms: Two words
share practically all aspects of meaning.
Example: quaff and drink. Our synonyms
came from WordNet (Miller, 1995).
3. Co-occurrence. Both words tend to appear to-
gether in the same contexts.
Example: politics and election.
4. Hyper- and hyponyms: Relations such as ?X
is a kind of Y ?, as obtained from Wordnet or
other thesaurus-like resources.
Example: airplane and transportation.
5. Free association: A relation defined by the fact
that a person is likely to give one word as a free-
association response to the other.
Example: disaster and fear. Our data was ob-
tained from the Univ. of South Florida associa-
tion database (Nelson et al, 1998).
We denote link functions using ?1, . . . , ?m to
summarize different types of interactions between
words. Each ?m(wi, wj) represents a specific type
of lexical or semantic relation or constraint between
wi and wj . For each link ?m, we also define a
weight ?m that gives the strength of the relationship
between wi and wj for that link.
Our goal is to predict the likelihood of a target
definition D given a test response R consisting of
terms {w0 . . . wk} drawn from a common vocabu-
lary V . We are thus interested in the conditional dis-
tribution p(D | R). We start by defining a simple
model that can combine the link functions in a gen-
eral purpose way to produce the conditional distribu-
tion p(wi|wj) given arbitrary terms wi and wj . We
use a log-linear model of the general form
p(wi|wj) =
1
Z
exp
L?
m=0
?m(i)?m(wi, wj) (1)
In the next sections we show how to combine the
estimate of individual pairs p(wi|wj) into a larger
graph of term relations, which will enable us to cal-
culate the desired p(D | R).
3.2 Combining term relations using graphs
Graphs provide one rich model for representing mul-
tiple word relationships. They can be directed or
undirected, and typically use nodes of words, with
word labels at the vertices, and edges denoting word
relationships. In this model, the dependency be-
tween two words represents a single inference step
in which the label of the destination word is inferred
from the source word. Multiple inference steps may
then be chained together to perform longer-range in-
ference about word relations. In this way, we can in-
fer the similarity of two terms without requiring di-
rect evidence for the relations between that specific
pair. Using the link functions defined in Section 3.1,
478
we imagine a generative process where an author A
creates a short text of N words as follows.
Step 0: Choose an initial word w0 with probabil-
ity P (w0|A). (If we have already generated N
words, stop.)
Step i: Given we have chosen wi?1, then with prob-
ability 1?? output the word wi?1 and reset the
process to step 0. Otherwise, with probability
?, sample a new word wi according to the dis-
tribution:
P (wi|wi?1) =
1
Z
exp
L?
m=0
?m(i)?m(wi, wi?1)
(2)
where Z is the normalization quantity.
This conditional probability may be interpreted
as a mixture model in which a particular link type
?m(.) is chosen with probability ?m(i) at timestep
i. Note that the mixture is allowed to change at each
timestep. For simplicity, we limit the number of
such changes by grouping the timesteps of the walk
into three stages: early, middle and final. The func-
tion ?(i) defines how timestep i maps to stage s,
where s ? {0, 1, 2}, and we now refer to ?m(s) in-
stead of ?m(i).
Suppose we have a definition D consisting of
terms {di}. For each link type ?m(.) we define a
transition matrix C(D,m) based on the definition
D. The reason D influences the transition matrix
is that some link types, such as proximity and co-
occurrence, are context-specific. Each stage s has
an overall transition matrix C(D, s) as the mixture
of the individual C(D,m), as follows.
C(D, s) =
M?
m=1
?m(s)C(D,m) (3)
Combining the stages over k steps into a single
transition matrix, which we denote Ck, we have
Ck =
k?
i=0
C(D,?(i)) (4)
We denote the (i, j) entry of a matrix Ak by Aki,j .
Then for a particular term di, the probability that a
chain reaches di after k steps, starting at word w is
Pk(di|w) = (1 ? ?)?
kCkw,di (5)
where we identify w and di with their corresponding
indices into the vocabulary V . The overall probabil-
ity p(di|w) of generating a definition term di given
a word w is therefore
P (di|w) =
??
k=0
Pk(di|w) = (1??)(
??
k=0
?kCk)w,di
(6)
The walk continuation probability ? can be
viewed as a penalty for long chains of inference. In
practice, to perform the random walk steps we re-
place the infinite sum of Eq. 6 with a small number
of steps (up to 5) on a sparse representation of the
adjacency graph. We obtained effective link weights
?m(i) empirically using held-out data. For simplic-
ity we assume that the same ? is used across all link
types, but further improvement may be possible by
extending the model to use link-specific decays ?m.
Fine-tuning these parameter estimation methods is a
subject of future work.
3.3 Using the model for definition scoring
In our study the reference definition for the target
word consisted of the target word, a rare synonym,
a more frequent synonym, and a short glossary-like
definition phrase. For example, the reference defini-
tion for abscond was
abscond; absquatulate; escape; to leave quickly
and secretly and hide oneself, often to avoid arrest
or prosecution.
In general, we define the score of a response R
with respect to a definition D as the probability
that the definition is generated by the response, or
p(D|R). Equivalently, we can score by log p(D|R)
since the log function is monotonic. So making the
simplifying assumption that the terms di ? D are
exchangable (the bag-of-words assumption), and
taking logarithms, we have:
log p(D|R) = log
?
di?D
p(di|R)
=
?
di?D
log[(1 ? ?)(
m?
k=0
?kCk)R,di ]
(7)
Suppose that the response to be scored is run from
the cops. In practical terms, Eq. 7 means that for our
479
example, we ?light up? the nodes in the graph cor-
responding to run, from, the and cops by assigning
some initial probability, and the graph is then ?run?
using the transition matrix C according to Eq. 7. In
this study, the initial node probabilities are set to val-
ues proportional to the idf values of the correspond-
ing term, so that P (di) = idf(di)P idf(di) . After m steps,
the probabilities at the nodes for each term in the
reference definition R are read off, and their log-
arithms summed. Similar to an AND calculation,
we calculate a product of sums over the graph, so
that responses reflecting multiple aspects of the tar-
get definition are rewarded more highly than a very
strong prediction for only a single definition term.
4 Evaluation
We first describe our corpus of gold standard human
judgments. We then explain the different text sim-
ilarity methods and baselines we computed on the
corpus responses. Finally, we give an analysis and
discussion of the results.
4.1 Corpus
We obtained a set of 734 responses to definition pro-
duction tests from a word learning experiment at the
University of Pittsburgh (Bolger et al, 2006). In
total, 72 target words, selected by the same group,
were used in the experiment. In this experiment,
subjects were asked to learn the meaning of target
words after seeing them used in a series of context
sentences. We set aside 70 responses for training,
leaving 664 responses in the final test dataset.
Each response instance was coded using the scale
shown in Table 1, and a sample set of subject re-
sponses and scores is shown in Table 2. The target
word was treated as having several key aspects of
meaning. The coders were instructed to judge a re-
sponse according to how well it covered the various
aspects of the target definition. If the response cov-
ered all aspects of the target definition, but also in-
cluded extra irrelevant information, this was treated
as a partial match at the discretion of the coders.
We obtained three codings of the final dataset.
The first two codings were obtained using an in-
dependent group, the QDAP Center at the Univer-
sity of Pittsburgh. Initially, five human coders, with
varying degrees of general coding experience, were
Score Meaning
0 Completely wrong
1 Some partial aspect is correct
2 One major aspect, or more than one
minor aspect, is correct
3 Covers all aspects correctly
Table 1: Scale for human definition judgements.
Response Human
Score
depart secretly 3
quietly make away, escape 3
to flee, run away 2
flee 2
to get away with 1
to steal or take 0
Table 2: Examples of human scores of responses for
the target word abscond.
trained by the authors using one set of 10 example
instances and two training sessions of 30 instances
each. Between the two training sessions, one of the
authors met with the coders to discuss the ratings
and refine the rating guidelines. After training, the
authors selected the two coders who had the best
inter-coder agreement on the 60 training instances.
These two coders then labeled the final test set of
664 instances. Our third coding was obtained from
an initial coding created by an expert in the Univer-
sity of Pittsburgh Psychology department and then
adjusted by one of the authors to resolve a small
number of internal inconsistencies, such as when the
same response to the same target had been given a
different score.
Inter-coder agreement was measured using lin-
ear weighted kappa, a standard technique for or-
dinal scales. Weighted kappa scores for all three
coder pairs are shown in Table 3. Overall, agree-
ment ranged from moderate (0.64) to good (0.72).
4.2 Baseline Methods
We computed three baselines as reference points for
lower and upper performance bounds.
Random. The response items were assigned la-
bels randomly.
480
Coder pair Weighted
Kappa
1, 2 0.68
2, 3 0.64
1, 3 0.72
Table 3: Weighted kappa inter-rater reliability for
three human coders on our definition response
dataset (664 items).
Method Spearman Rank
Correlation
Random 0.3661
Cosine 0.4731
LSA 0.4868
Markov 0.6111
LSA + Markov 0.6365
Human 0.8744
Table 4: Ability of methods to match human ranking
of responses, as measured by Spearman rank corre-
lation (corrected for ties).
Human choice of label. We include a method
that, given an item and a human label from one of the
coders, simply returns a label of the same item from
a different coder, with results repeated and averaged
over all coders. This gives an indication of an upper
bound based on human performance.
Cosine similarity using tf.idf weighting. Cosine
similarity is a widely-used text similarity method
for tasks where the passages being compared of-
ten have significant direct word overlap. We repre-
sent response items and reference definitions as vec-
tors of terms using tf.idf weighting, a standard tech-
nique from information retrieval (Salton and Buck-
ley, 1997) that combines term frequency (tf) with
term specificity (idf). A good summary of arguments
for using idf can be found in (Robertson, 2004). To
compute idf, we used frequencies from a standard
100-million-word corpus of written and spoken En-
glish 1. We included a minimal semantic similar-
ity component by applying Porter stemming (Porter,
1980) on terms.
1The British National Corpus (Burnage and Dunlop, 1992),
using American spelling conversion.
4.3 Methods
In addition to the baseline methods, we also ran the
following three algorithms over the responses.
Markov chains (?Markov?). This is the method
described in Section 3. A maximum of 5 random
walk steps were used, with a walk continuation
probability of 0.8. Each walk step used a mixture of
synonym, stem, co-occurrence, and free-association
links. The link weights were trained on a small set
of held-out data.
Latent Semantic Analysis (LSA). LSA (Lan-
dauer et al, 1998) is a corpus-based unsupervised
technique that uses dimensionality reduction to clus-
ter terms according to multi-order co-occurrence re-
lations. In these experiments, we obtained LSA-
based similarity scores between responses and target
definitions using the software running on the Univer-
sity of Colorado LSA Web site (LSA site, 2006). We
used the pairwise text passage comparison facility,
using the maximum 300 latent factors and a general
English corpus (Grade 1 ? first-year college).
Although LSA and the Markov chain approach
are based on different principles, we chose to ap-
ply LSA to this new response-scoring task and cor-
pus because LSA has been widely used as a text se-
mantic similarity measure for other tasks and shown
good performance (Foltz et al, 1998).
LSA+Markov. To test the effectiveness of com-
bining two different ? and possibly complemen-
tary ? approaches to response scoring, we created
a normalized, weighted linear combination of the
LSA and Markov scores, with the model combina-
tion weight being derived from cross-validation on a
held-out dataset.
4.4 Results
We measured the effectiveness of each scoring
method from two perspectives: ranking quality, and
label accuracy.
First, we measured how well each scoring method
was able to rank response items by similarity to the
target definition. To do this, we calculated the Spear-
man Rank Correlation (corrected for ties) between
the ranking based on the scoring method and the
ranking based on the human-assigned scores, aver-
aged over all sets of target word responses.
Table 4 summarizes the ranking results. For
481
Method Label error (RMS)
Top 1 Top 3
Random 1.4954 1.6643
Cosine 0.8194 1.0540
LSA 0.8009 0.9965
Markov 0.7222 0.7968
LSA + Markov 1.1111 1.0650
Human 0.1944 0.4167
Table 5: Root mean squared error (RMSE) of la-
bel(s) for top-ranked item, and top-three items for
all 77 words in the dataset.
overall quality of ranking, the Markov method had
significantly better performance than the other au-
tomated methods (p < 2.38e?5). LSA gave a
small, but not significant, improvement in overall
rank quality over the cosine baseline. 2 The sim-
ple combination of LSA and Markov resulted in a
slightly higher but statistically insignificant differ-
ence (p < 0.253).
Second, we examined the ability of each method
to find the most accurate responses ? that is, the re-
sponses with the highest human label on average ?
for a given target word. To do this, we calculated the
Root Mean Squared Error (RMSE) of the label as-
signed to the top item, and the top three items. The
results are shown in Table 5. For top-item detec-
tion, our Markov model had the lowest RMS error
(0.7222) of the automated methods, but the differ-
ences from Cosine and LSA were not statistically
significant, while differences for all three from Ran-
dom and Human baselines were significant. For
the top three items, the difference between Markov
(0.7968) and LSA (0.9965) was significant at the
p < 0.03 level.
Comparing the overall rank accuracy with top-
item accuracy, the combined LSA + Markov method
was significantly worse at finding the three best-
quality responses (RMSE of 1.0650) than Markov
(0.7968) or LSA (0.9965) alone. The reasons for
this require further study.
2All statistical significance results reported here used the
Wilcoxon Signed-Ranks test.
5 Discussion
Even though definition scoring may seem more
straightforward than other automated learning as-
sessment problems, human performance was still
significantly above the best automated methods in
our study, for both ranking and label accuracy. There
are certain additions to our model which seem likely
to result in further improvement.
One of the most important is the ability to identify
phrases or colloquial expressions. Given the short
length of a response, these seem critical to handle
properly. For example, to get away with something
is commonly understood to mean secretly guilty, not
a physical action. Yet the near-identical phrase to
get away from something means something very dif-
ferent when phrases and idioms are considered.
Despite the gap between human and automated
performance, the current level of accuracy of the
Markov chain approach has already led to some
promising early results in word learning research.
For example, in a separate study of incremental
word learning (Frishkoff et al, 2006), we used our
measure to track increments in word knowledge
across multiple trials. Each trial consisted of a sin-
gle passage that was either supportive ? containing
clues to the meaning of unfamiliar words ? or not
supportive. In this separate study, broad learning ef-
fects identified by our measure were consistent with
effects found using manually-scored pre- and post-
tests. Our automated method also revealed a pre-
viously unknown interaction between trial spacing,
the proportion of supportive contexts per word, and
reader skill.
In future applications, we envision using our auto-
mated measure to allow a form of feedback for intel-
ligent language tutors, so that the system can auto-
matically adapt its behavior based on the student?s
test responses. With some adjustments, the same
scoring model described in this study may also be
applied to the problem of finding supportive contexts
for students.
6 Conclusions
We presented results for both automated and hu-
man performance of an important task for language
learning applications: scoring definition responses.
We described a probabilistic model of text seman-
482
tic similarity that uses Markov chains on a graph of
term relations to perform a kind of semantic smooth-
ing. This model incorporated both corpus-based and
knowledge-based resources to compute text seman-
tic similarity. We measured the effectiveness of both
our method and LSA compared to cosine and ran-
dom baselines, using a new corpus of human judg-
ments on definition responses from a language learn-
ing experiment. Our method outperformed the tf.idf
cosine similarity baseline in ranking quality and in
ability to find high-scoring definitions. Because
LSA and our Markov chain method are based on
different approaches and resources, it is difficult to
draw definitive conclusions about performance dif-
ferences between the two methods.
Looking beyond definition scoring, we believe au-
tomated methods for assessing word learning have
great potential as a new scientific tool for language
learning researchers, and as a key component of in-
telligent tutoring systems that can adapt to students.
Acknowledgements
We thank D.J. Bolger and C. Perfetti for the use
of their definition response data, Stuart Shulman
for his guidance of the human coding effort, and
the anonymous reviewers for their comments. This
work supported by U.S. Dept. of Education grant
R305G03123. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are the authors? and do not necessarily reflect those
of the sponsors.
References
D.J. Bolger, M. Balass, E. Landen and C.A. Perfetti.
2006. Contextual Variation and Definitions in Learn-
ing the Meanings of Words. (In press.)
G. Burnage and D. Dunlop. 1992. Encoding the British
National Corpus. English Language Corpora: De-
sign, Analysis and Exploitation. The 13th Intl. Conf.
on Engl. Lang. Res. in Computerized Corpora. Ni-
jmegen. J. Aarts, P. de Haan, N. Oostdijk, Eds.
J. Burstein, S. Wolff, and L. Chi. 1999. Using Lexi-
cal Semantic Techniques to Classify Free-Responses.
Breadth and Depth of Semantic Lexicons. Kluwer
Acad. Press, p. 1?18.
G. Cao, J-Y. Nie, and J. Bai. Integrating Word Relation-
ships into Language Models. SIGIR 2005, 298?305.
P.W. Foltz, W. Kintsch, and T. Landauer. 1998. An Intro-
duction to Latent Semantic Analysis. Discourse Pro-
cesses, 25(2):285?307.
G. Frishkoff, K. Collins-Thompson, J. Callan, and C.
Perfetti. 2007. The Nature of Incremental Word
Learning: Context Quality, Spacing Effects, and Skill
Differences in Meaning Acquisition Across Multiple
Contexts. (In preparation.)
M. Heilman, K. Collins-Thompson, J. Callan and M. Es-
kanazi. 2006. Classroom Success of an Intelligent Tu-
toring System for Lexical Practice and Reading Com-
prehension. ICSLP 2006.
S. Jayaraman and A. Lavie. Multi-Engine Ma-
chine Translation Guided by Explicit Word Matching.
EAMT 2005.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An
Introduction to Latent Semantic Analysis. Discourse
Processes, 25:259?284.
LSA Web Site. http://lsa.colorado.edu
I. Mani and M.T. Maybury (Eds.) 1999. Advances in
Automatic Text Summarization. MIT Press.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and Knowledge-based Measures of Text
Semantic Similarity. AAAI 2006
G. Miller. 1998. WordNet: A Lexical Database for En-
glish. Communications of the ACM, 38(11) 39?41.
D.L. Nelson, C.L. McEvoy, and T.A. Schreiber.
1998. The University of South Florida word
association, rhyme, and word fragment norms.
http://www.usf.edu/FreeAssociation/
M. Porter. 1980. An Algorithm for Suffix-
stripping. Program, 14(3) 130?137.
http://www.tartarus.org/martin/PorterStemmer
M. Quillian. 1967. Word Concepts: A Theory and Sim-
ulation of some Basic Semantic Capabilities. Behav.
Sci., 12: 410?430.
S. Robertson. 2004. Understanding Inverse Document
Frequency: on Theoretical Arguments for IDF. J. of
Documentation, 60:503?520.
G. Salton and C. Buckley. 1997. Term Weighting Ap-
proaches in Automatic Text Retrieval. Reading in In-
formation Retrieval. Morgan Kaufmann.
G. Salton and M. Lesk. 1971. Computer Evaluation
of Indexing and Text Processing. Prentice-Hall. 143
? 180.
K. Toutanova, C.D. Manning, and A.Y. Ng. 2004. Learn-
ing Random Walk Models for Inducing Word Depen-
dency Distributions. ICML 2004.
S. Valenti, F. Neri, and A. Cucchiarelli. 2003. An
Overview of Current Research on Automated Essay
Grading. J. of Info. Tech. Ed., Vol. 2.
483
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 71?79,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Analysis of Statistical Models and Features for Reading Difficulty
Prediction
Michael Heilman, Kevyn Collins-Thompson and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,kct,max}@cs.cmu.edu
Abstract
A reading difficulty measure can be described
as a function or model that maps a text to a
numerical value corresponding to a difficulty
or grade level. We describe a measure of read-
ability that uses a combination of lexical fea-
tures and grammatical features that are derived
from subtrees of syntactic parses. We also
tested statistical models for nominal, ordinal,
and interval scales of measurement. The re-
sults indicate that a model for ordinal regres-
sion, such as the proportional odds model, us-
ing a combination of grammatical and lexical
features is most effective at predicting reading
difficulty.
1 Introduction
A reading difficulty, or readability, measure can be
described as a function or model that maps a text
to a numerical value corresponding to a difficulty or
grade level. Inputs to this function are usually statis-
tics for various lexical and grammatical features of
the text. The output is one of a set of ordered dif-
ficulty levels, usually corresponding to grade levels
for elementary school through high school. As such,
reading difficulty prediction can be viewed as a re-
gression of grade level on a set of textual features.
Early work on readability measures employed
simple proxies for grammatical and lexical complex-
ity, including sentence length and the number of syl-
lables in a word. Fairly simple features were often
employed because of a lack of computational power.
Such features exhibit high bias because they rely on
strong assumptions about what makes a text difficult
to read. For example, the use of sentence length as a
measure of grammatical complexity assumes that a
longer sentence is more grammatically complex than
a shorter one, which is often but not always the case.
In one early model, the Dale-Chall model (Dale and
Chall, 1948; Chall and Dale, 1995), reading diffi-
culty is a linear function of the mean sentence length
and the percentage of rare words, as defined by a list
of 3,000 words commonly known by 4th grade. In
this paper, sentence length is defined as the mean
number of words in the sentences of a text.
Many early measures did not employ direct esti-
mates of word frequency due to computational lim-
itations (e.g., (Gunning, 1952; McLaughlin, 1969;
Kincaid et al, 1975)). Instead, these measures relied
on the strong relationship between the frequency of
and the number of syllables in a word. More fre-
quent words are more likely to have fewer syllables
(e.g., ?the?) than less frequent words (e.g., ?vocab-
ulary?), an association that is related to Zipf?s Law
(Zipf, 1935). The Flesch-Kincaid measure (Kincaid
et al, 1975) is probably the most common reading
difficulty measure in use. It is implemented in com-
mon word processing programs. This measure is a
linear function of the mean number of syllables per
word and the mean number of words per sentence.
Klare (1974) provides a summary of other early
work on readability.
More recent approaches to reading difficulty em-
ploy more sophisticated models that make use of the
growth in computational power. The Lexile Frame-
work (e.g., (Stenner, 1996)) uses individual word
frequency estimates as a measure of lexical diffi-
culty. The word frequency estimates are derived
71
from a large, varied corpus of text. Lexile uses a
Rasch model (Rasch, 1980) with the mean log word
frequency as a lexical feature and the log of the mean
sentence length as a grammatical feature. The Rasch
model, related to logistic regression, is used to esti-
mate the level of a student that would comprehend
75% of a given text. The converted log odds ratio
called a ?Lexile? that is used as part of this measure
can be easily mapped to grade school levels.
A reading difficulty measure developed by
Collins-Thompson and Callan (2005) uses
smoothed unigram language modeling to capture
the predictive ability of individual words based
on their frequency at each reading difficulty level.
Collins-Thompson and Callan found that certain
words were very predictive of certain levels. For
example, ?grownup? was very predictive of grade
1, and ?essay? was very predictive of grade 12. For
a given text, this measure estimates the likelihood
that the text was generated by each level?s language
model. The prediction is the level of the model with
the highest likelihood of generating the text. There
are no grammatical features.
Natural language processing techniques enable
more sophisticated grammatical analysis for read-
ing difficulty measures. Rather than using sentence
length as a proxy, measures can employ tools for au-
tomatic analysis of the syntactic structure of texts
(e.g., (Charniak, 2000)). A measure by Schwarm
and Ostendorf (2005) incorporates syntactic analy-
ses, among a variety of other types of features. It in-
cludes four grammatical features derived from syn-
tactic parses of text: the mean parse tree height, the
mean number of noun phrases, mean number of verb
phrases, and mean number of ?SBARs.? ?SBARs?
are non-terminal nodes that are associated with sub-
ordinate clauses. Their system led to better pre-
dictions than the Flesch-Kincaid and Lexile mea-
sures, but the predictive value of the grammatical
features is not entirely clear. In initial experiments
using such course-grain grammatical features alone,
rather than in conjunction with language modeling
and other features as in Schwarm and Ostendorf?s
system, we found relatively poor prediction perfor-
mance. Our final approach using subtrees of syn-
tactic parses allows for a finer level of discrimina-
tion that may support the detection of differences in
grade levels between texts that exhibit the same high
level features.
A reading difficulty measure developed by Heil-
man, Collins-Thompson, Callan, and Eskenazi
(2007) uses the frequency of grammatical construc-
tions as a measure of grammatical difficulty. A set
of approximately twenty constructions were selected
from English as a Second Language grammar text-
books. This set includes grammatical constructions
such as the passive voice, relative clauses, and vari-
ous verb tenses. The frequencies are used as features
for a nearest neighbor classification algorithm. The
unigram language modeling approach of Collins-
Thompson and Callan (2005) is used to estimate
lexical difficulty in this measure. The final predic-
tion is a linear function of the lexical and grammat-
ical components. That model assumes that gram-
matical difficulty is adequately captured by a small
number of constructions chosen according to de-
tailed knowledge of English grammar. In that work,
the constructions were selected from an English as
a Second Language grammar textbook, a labor- and
knowledge-intensive task that may be less practical
for other languages.
We aim to identify the appropriate scale of mea-
surement for reading difficulty?nominal, ordinal, or
interval?by comparing the effectiveness of statistical
models for each type of data. We also extend pre-
vious work combining lexical and grammatical fea-
tures (Heilman et al, 2007) by making it possible
to include a large number of grammatical features
derived from syntactic structures without requiring
significant linguistic or pedagogical content knowl-
edge, such as a reference guide for the grammar of
the language of interest.
2 Types of Features
2.1 Lexical Features
This section and the following section describe the
lexical and grammatical features used in our read-
ing difficulty models. The lexical features are the
relative frequencies of word unigrams. The use of
word unigrams is a standard approach in text clas-
sification (Yang and Pedersen, 1997), and has also
been successfully used to predict reading difficulty
(Collins-Thompson and Callan, 2005). Higher order
n-grams such as bigrams and trigrams were not used
as features because they did not improve predictions
72
in preliminary tests. The specific set of lexical fea-
tures was chosen based on the frequencies of words
in the training corpus. The system performs mor-
phological stemming and stopword removal. The
remaining 5000 most common words comprised the
lexical feature set.
2.2 Grammatical Features
Grammatical features are extracted from automatic
context-free grammar parses of sentences. The sys-
tem computes relative frequencies of partial syn-
tactic derivations, which will be called ?subtrees?
hereafter. The approach extends (Heilman et al,
2007), where frequencies of manually defined syn-
tactic patterns were extracted from syntactic struc-
tures. In that approach, the features are defined man-
ually using linguistic knowledge of the target lan-
guage to implement tree search patterns, a labor- and
knowledge-intensive process. The approach advo-
cated in this paper, however, extracts frequencies for
an automatically defined set of subtree patterns. The
system considers all subtrees up to a given depth that
occur in the training corpus. Examples of grammati-
cal features at levels 0 through 2 are shown in Figure
1. The sentence for the parse tree shown was taken
from a third grade text.
For depth 0, the system includes all subtrees con-
sisting of just nonterminal nodes. This includes all
parts of speech, as well as non-terminal nodes for
noun phrases, adjective phrases, clauses, etc. For
depth 1, the system includes subtrees corresponding
to the application of a single context free grammar
rule in the derivation of the tree. An example of a
feature at this level would be a sentence node that
dominates nodes for noun phrases and verb phrases.
For deeper levels, the system includes subtrees cor-
responding to the successive application of rules on
non-terminals symbols until either a terminal sym-
bol is reached or the given depth is reached. An
example feature for level 2 is a subtree in which
a prepositional phrase node dominates a preposi-
tion node and noun phrase node, and the preposition
node in turn dominates a preposition, and the noun
phrase dominates determiner, adjective, and noun
nodes.
We used a maximum depth of 3 in our exper-
iments. Features of deeper levels occur less fre-
quently in general, and deeper levels were avoided
due to data sparseness. A depth first search algo-
rithm extracts candidate grammatical features from
the training corpus. First, a context-free grammar
parser (Klein and Manning, 2003) derives parse
trees for all texts in the training corpus. The algo-
rithm traverses these parses, at each node counting
all subtree features up to the given depth that are
rooted at that node. The subtree features are sorted
by their overall counts in the corpus. In our ex-
periments, frequencies of the most common 1000
subtrees were chosen as the final features. These
included 64 level 0 features corresponding to non-
terminal symbols, 334 level 1 features, 461 level 2
features, and 141 level 3 features. Deeper levels
have more possible features, but sparsity at level 3
resulted in fewer level 3 features being selected.
In our experiments, the subtrees included terminal
symbols for stopwords. However, the system effec-
tively removed content word terminals from parses
before extracting features. The system could be
modified to include terminal symbols for content
words, or even to ignore all nodes for terminal sym-
bols. Subtree features including terminal symbols
for content words would, of course, occur with low
frequency and not likely be included in the final fea-
ture set. Terminal symbols for content words were
omitted so that lexical information was not included
in the set of grammatical features. Similar to leaving
higher order n-grams out of the lexical feature set,
omitting terminal symbols for content words avoids
confounding grammatical and lexical information in
the grammatical feature set. Subtree counts are nor-
malized by the number of words in a text to compute
the relative frequencies. Normalization by the num-
ber of sentences in a text is also possible, but did not
perform as well in preliminary tests. The Stanford
Parser (Klein andManning, 2003) version 1.5.1 was
used to derive tree structures for sentences. We used
the unlexicalized model included in the distribution
which was trained on Wall Street Journal texts.
3 Statistical Models
3.1 Scales of Measurement for Reading
Difficulty
Several statistical models were tested for effective-
ness at predicting reading difficulty. The appropri-
ateness of these models depends on the nature of
73
Figure 1: Parse Tree for Sentence from Third Grade Text with Example Subtree Features.
reading difficulty data, particularly the scale of mea-
surement. The standard unit for reading difficulty is
the grade level. First through twelfth grade levels in
American schools have been used in previous work
(e.g., (Heilman et al, 2007; Collins-Thompson and
Callan, 2005)). English as a Second Language lev-
els have also been used (Heilman et al, 2007),
as well as grade levels for other languages such
as French (Collins-Thompson and Callan, 2005).
While these grades are assigned evenly spaced inte-
gers, the ranges of reading difficulty corresponding
to these grades are not necessarily evenly spaced. It
is possible, of course, that assuming even spacing
between levels might produce more parsimonious
and accurate statistical models. A more reasonable
assumption is that the grade numbers assigned to
each difficulty level denote an ordering: for exam-
ple, that grade 1 is in some sense less than grade 2,
which is less than grade 3, etc. Different statistical
models handle this assumption more or less well.
Statistics generally distinguish four scales of mea-
surement, which are, ordered by increasing assump-
tions about the relationships between values: nomi-
nal, ordinal, interval, and ratio (Stevens, 1946; Co-
hen et al, 2003). Nominal data involve no relation-
ships between the labels or classes of the data. An
example would be types of fruits, where a model
might be used to make decisions between apples and
oranges. This type of prediction is generally called
classification in machine learning and related fields.
Ordinal data have a natural ordering, but the val-
ues are not necessarily evenly spaced. For exam-
ple, data about the severity of illnesses might have
labels such as mild, moderate, severe, deceased, in
which the transitions between consecutive classes
all have the same direction but not the same mag-
nitude. Making predictions about such data is gen-
erally called ordinal regression (McCullagh, 1980).
Interval data, however, are both ordered and evenly
spaced. An example would be temperature as mea-
sured in Fahrenheit degrees. Such data have an ar-
bitrary zero point, and negative values may occur.
Ratio data, of which annual income is an example,
do have a meaningful zero point. We will not dis-
cuss ratio data further since its distinction from in-
terval data is not relevant to this paper. It is not clear
to which scale reading difficulty corresponds. The
assumption of an interval scale allows for simpler
models with fewer parameters. However, models for
ordinal or even nominal data might be more appro-
priate if the strong assumption of an interval scale
does not hold.
We experimented with three linear and log-linear
models corresponding to interval, ordinal, and nom-
inal data. Parameters were estimated using L2 reg-
ularization, which corresponds to a Gaussian prior
distribution with zero mean and a user-specified
variance over the parameters. We chose these mod-
els because they are commonly used in the statis-
tics, machine learning, and behavioral science com-
munities, and aimed to set up meaningful compar-
isons among the scales of measurement. Other ma-
chine learning algorithms might also be employed.
In fact, we briefly tested the maximummargin (Vap-
nik, 1995) approach, which led to comparable re-
sults and might be worth exploring in future work.
74
3.2 Linear Regression
Linear Regression (LIN) produces a linear model in
which the dependent, or outcome, variable is a lin-
ear function of the values for predictor variables,
or features. A prediction for a given text is the
inner product of a vector of feature values for the
text and a vector of regression coefficients estimated
from training data. For the case of reading difficulty,
the grade level is a linear combination of the lexi-
cal and/or grammatical feature values. LIN provides
continuous estimates of reading difficulty, such that
a prediction might fall between grade levels. The
estimates were not rounded to whole numbers in the
experiments. For rare cases of an LIN prediction
falling outside the appropriate range of grade levels,
the value was set to the maximum or minimum grade
level. LIN implicitly assumes that the data fall on
an interval scale, meaning that the levels are evenly
spaced. The LIN model has relatively few parame-
ters but makes strong assumptions about the scale of
measurement. For details, see (Hastie et al, 2001).
3.3 Proportional Odds Model
The Proportional Odds (PO) model, also called the
parallel regression model and the cumulative logit
model, is a form of log-linear, or exponential, model
for ordinal data (McCullagh, 1980). Given a new
unlabeled instance as input, the model provides es-
timates of the probability that the instance belongs
to a class at or above a particular level. In Equation
(1), P (y ? j) is this estimated probability, ?j is an
intercept parameter for the given level j, ? is vector
of regression coefficients, Xi is the vector of feature
values for instance i, and yi is the predicted reading
difficulty level.
P (yi ? j) =
exp(?j + ?TXi)
1 + exp(?j + ?TXi)
(1)
ln
P (yi ? j)
1? P (yi ? j)
= ?j + ?TXi (2)
The PO model has a parameter ?j for the thresh-
old, or intercept, at each level j, but only a single set
? of parameters for the features. These two types of
parameters correspond to an implicit assumption of
ordinality. Having a single set of parameters for fea-
tures across the levels means that changes in feature
values proportionally affect the odds of transitioning
from any one class to another.
The estimated probability of an instance belong-
ing to a particular class is the difference between es-
timates for that class and the next highest class. For
example, the estimated probability of a text being
at the eighth grade level would be the estimate for
being at or above eighth grade minus the estimate
for being at or above ninth grade. As in binary lo-
gistic regression, the PO model estimates log odds
ratios based on the values of features or predictor
variables. The numerator of the odds ratio is the
probability of being at or above a level, and the de-
nominator is the probability of being below a level.
Equation (2) shows the form of the model that is
linear in the parameters.
3.4 Multi-class Logistic Regression
Multi-class Logistic Regression (LOG), or multino-
mial logit regression, is a log-linear model for nom-
inal data. In contrast to the simpler PO model, the
model maintains parameters for all of the features
for every class except one category, which is used
for comparison. Thus, for reading difficulty, there
are about 11 times as many parameters to estimate
compared to LIN and PO. The increased difficulty
of parameter estimation for this model is offset for
domains in which assumptions of ordinality or lin-
earity do not hold. For more details, see (Hastie et
al., 2001).
4 Evaluation
4.1 Web Corpus
The corpus of materials used for training and test-
ing the models consists of the content text extracted
from Web pages with reading difficulty level labels.
Web pages were used because the system for pre-
dicting reading difficulty is being used as part of the
REAP tutoring system, which finds authentic and
appropriate Web pages for English vocabulary prac-
tice (Brown and Eskenazi, 2004; Heilman et al,
2006). Approximately half of these texts were au-
thored by students at the particular grade level, and
half were authored by teachers or writers and aimed
at readers at a particular grade level. Texts were
found for grade levels 1 through 12. The twelfth
grade level also included some post-secondary level
75
texts. Various genres and subjects were represented.
In all cases, either the text itself or a link to it iden-
tified it as having a certain level. The content text
was manually extracted from these Web pages so
that noisy information such as navigation menus and
advertisements were not included. Automatic con-
tent extraction may, however, be able to remove such
noisy information without human intervention (e.g.,
(Gupta et al, 2003)). This Web corpus is adapted
from the corpora used in prior work on reading dif-
ficulty predication (Collins-Thompson and Callan,
2005; Heilman et al, 2007). We modified that cor-
pus because it contained a number of documents
pertaining to mathematics and vocabulary practice.
The majority of tokens in these texts were not part
of well-formed, grammatical sentences suitable for
reading practice. Since our goal is to measure the
difficulty of reading passages, we removed these
documents and added additional texts consisting of
more suitable reading material. The corpus con-
sisted of approximately 150,000 words, distributed
among 289 texts. The number of texts for each grade
level was approximately the same, with at least 28
texts at each level. The mean length in words of
the texts was approximately 500 words, which corre-
sponds to about a page. Texts for lower grades were
necessarily shorter. We extracted excerpts for higher
level texts so that texts were otherwise roughly equal
in length across levels. For these excerpts, the first
500 or so words of text were extracted, while re-
specting sentence and paragraph boundaries.
4.2 Evaluation Metrics
Root mean square error (RMSE), Pearson?s correla-
tion coefficient, and accuracy within 1 grade level
served as metrics for evaluating the performance
of reading difficulty predictions. Multiple statistics
were used because it is not entirely clear what the
best measure of prediction quality is for reading dif-
ficulty. RMSE is the square root of the empirical
mean of the squared error of predictions. It more
strongly penalizes those errors that are further away
from the true value. It can be interpreted as the aver-
age number of grade levels that predictions measure
deviate from human-assigned labels.
Pearson?s correlation coefficient measures the
strength of the linear relationship, or similarity of
trends, between two random variables. A high corre-
lation would indicate that difficult texts would more
likely receive high predicted difficulty values, and
easier texts would be more likely to receive low pre-
dicted difficulty values. Correlations do not, how-
ever, measure the degree to which values match in
absolute terms.
Adjacent accuracy is the proportion of predic-
tions that were within one grade level of the human-
assigned label for the given text. Exact accuracy is
too stringent a measure because the human-assigned
reading levels are not always perfect and consis-
tent. For example, one school might read ?Romeo
and Juliet? in 9th grade while another school might
read it in 10th grade. The drawback of this accuracy
metric is that predictions that are two levels off are
treated the same as predictions that are ten levels off.
4.3 Baselines
The performance of other algorithms for estimat-
ing reading difficulty was estimated using the same
data. These comparison include Collins-Thompson
and Callan?s implementation of their language mod-
eling approach (2005), an implementation of the
Flesch-Kincaid reading level measure (Kincaid et
al., 1975), and a measure using word frequency and
sentence length similar to Lexile (Stenner et al,
1983). We did not directly test the approach de-
scribed by (Heilman et al, 2007). We observe
that its reported results for first language texts were
not significantly different in terms of correlation and
only slightly better in terms of mean squared er-
ror than the language modeling approach. Finally,
a simple uniform baseline, which always chose the
middle value of 6.5, was tested.
The Lexile-like measure (LX) used the same two
features as the Lexile measure: mean log frequency
or words and log mean sentence length. Instead of
using a Rasch model and converting scores to ?Lex-
iles,? however, the PO model was used to directly
predict grade levels. The log frequency values for
words were estimated from the second release of the
American National Corpus (Reppen et al, 2005),
a 20 million word corpus with texts in American
English from different genres on a variety of sub-
jects. Using the proportional odds models is effec-
tively equivalent to using Lexile?s Rasch model and
mapping its output to grade levels. The major differ-
ence between the Lexile measure and the implemen-
76
tation used in these experiments is the training data
sets used to estimated word frequencies and model
parameters.
4.4 Procedure
The Web Corpus was randomly split into training
and test sets. The test set consisted of 25% of the
individual texts at each level, a total of 84 texts.
Ten-fold stratified cross-validation on the training
set was employed to estimate the prediction per-
formance according to the evaluation metrics. In
cross-validation, data are partitioned randomly into
a given number of folds, and each fold is used for
testing while all others are used for training. For
more details and a discussion of validation meth-
ods, see (Hastie et al, 2001). The regularization
hyper-parameters were tuned on the training set dur-
ing cross-validation by a simple grid search. After
cross-validation, models were trained on the entire
training set, and then evaluated using the held-out
test data.
We tested whether each feature-set, algorithm pair
or baseline performed significantly differently than
our hypothesized best model, the PO model with
the combined feature set. We employed the bias-
corrected and accelerated (BCa) Bootstrap (Efron
and Tibshirani, 1993) with 50,000 replications of the
held-out test data to generate confidence intervals
for differences in evaluation results. If the (1??)%
confidence intervals for the difference do not con-
tain zero, which is the value corresponding to the
null hypothesis, then that difference is significant at
the ? level. For example, the 99% confidence inter-
val for the difference in adjacent accuracy between
the language modeling baseline and the PO model
with the combined feature set was (-1.86, -0.336),
indicating that this difference is significant at the .01
level since it does not contain zero.
5 Results
Table 1 presents correlation coefficients, RMSE val-
ues, and accuracy values for cross-validation and
held-out test data. Statistical significance was tested
only for the held-out test data since the hyper-
parameters were tuned during cross-validation. Our
discussion of the results pertains mostly to the eval-
uation on the test-set.
Of the various statistical models, the PO model
for ordinal data appears to provide superior perfor-
mance over the LIN and LOG models. Compared
to the LOG model, the PO model performs sig-
nificantly better in terms of correlation and RMSE
and comparably well in terms of adjacent accuracy.
Compared to the LINmodel, the POmodel performs
almost as well in terms of correlation, comparably
well in terms of RMSE, and far better in terms of
accuracy.
The performance of the methods when using dif-
ferent feature sets does not clearly indicate a best set
of features to use for predicting reading difficulty.
For the PO model, none of the feature sets lead to
significant gains over the others in terms of any of
the metrics. However, the combined feature set led
to the best performance in terms of correlation and
adjacent accuracy during cross-validation as well as
RMSE on the test set, suggesting at the very least
that including the extra features does not degrade
performance.
The PO model with the combined feature set out-
performed most of the baseline measures. LX had
the same accuracy value on the test set. The LX
method appears to perform the best in general of
the baselines models. Interestingly, LX uses pro-
portional odds logistic regression like PO, and thus
assumes an ordinal but not interval scale of measure-
ment. RMSE values were significantly lower for the
PO model than for LX and the language modeling
approach.
No statistically significant advantages are seen
for PO model when compared to Flesch-Kincaid.
We observe however, that for the sample of web
pages which constitutes the evaluation corpus the
PO model produced superior results across evalua-
tion metrics. That is, PO performed better in terms
of adjacent accuracy, RMSE, and correlation coeffi-
cients, both in cross-validation and testing with held-
out data.
6 Discussion
In our tests, the PO model, which assumes ordinal
data, lead to the most effective predictions of read-
ing difficulty in general. This result indicates that the
reading difficulty of texts, according to grade level,
lies on an ordinal scale of measurement. That is,
77
Method Features Cross-Validation Held-Out Test Set
Correl. RMSE Adj. Acc. Correl. RMSE Adj. Acc.
LIN Lexical .629 2.73 .242 .779 2.42 .167**
Grammatical .767 2.26 .294 .753 2.33 .274*
Combined .679 2.57 .284 .819** 2.21 .226**
PO Lexical .713 2.57 .498 .780 2.29 .464
Grammatical .762 2.22 .505 .734 2.42 .560
Combined .773 2.24 .519 .767 2.23 .440
LOG Lexical .517 3.24 .443 .619* 2.83* .548
Grammatical .632 2.87 .443 .506** 3.38** .464
Combined .582 2.94 .446 .652* 2.71* .556
LX - .659 2.77 .467 .731 2.67* .464
Lang. Modeling - .590 2.74 .370 .630 2.70** .381
Flesch-Kincaid - .697 2.66 .388 .718 2.54 .369
Uniform - .000 3.39 .170 .000** 3.45** .167**
Table 1: Results from Cross-Validation and Test Set Evaluations, as measured by Correlation Coefficients (Correl.),
Root Mean Square Error (RMSE), and Adjacent Accuracy. The best result for each metric for each evaluation is
given in bold. Asterisks indicate significant differences compared to the PO model with a Combined Feature Set. * =
p < .05, ** = p < .01.
reading difficulty appears to increase steadily but not
linearly with grade level. As such, the LIN approach
that produces linear models was less effective, par-
ticularly in terms of adjacent accuracy. The LOG
model, for nominal data, also led to inferior perfor-
mance compared to the PO model, which can be at-
tributed to the difficulty of accurately estimating a
more complex model with many parameters for each
level.
Our tests found that grammatical features alone
can be effective predictors of readability. This find-
ing disagrees with a previous result that found that a
model using a combination of lexical and manually
defined grammatical features (Heilman et al, 2007)
outperformed a model using grammatical features
alone. The superior predictive ability of the mod-
els we describe that use grammatical features can be
attributed to the automatic derivation of a grammat-
ical feature set that is more than an order of magni-
tude larger than in the previous approach. Our ap-
proach enables the use of much larger grammatical
feature sets because it does not require the extensive
linguistic knowledge and effort to manually define
the grammatical features. The automatic approach
also enables an easier transition to other languages,
assuming a parser is available. Using the combined
feature set did not hurt performance, however, and
since regularized statistical models can avoid over-
fitting large numbers of parameters, a combined fea-
ture set still seems appropriate.
Acknowledgments
We thank Jamie Callan for his comments and sug-
gestions. This research was supported in part by
the Institute of Education Sciences, U.S. Depart-
ment of Education, through Grant R305B040063 to
Carnegie Mellon University; Dept. of Education
grant R305G03123; the Pittsburgh Science of Learn-
ing Center which is funded by the National Sci-
ence Foundation, award number SBE-0354420; and
a National Science Foundation Graduate Research
Fellowship awarded to the first author. Any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this material are the authors, and do not
necessarily reflect those of the sponsors.
References
Jon Brown and Maxine Eskenazi. 2004. Retrieval of au-
thentic documents for reader-specific lexical practice.
Proceedings of InSTIL/ICALL Symposium 2004.
78
J. S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brookline
Books. Cambridge, MA.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. Proceedings of the NAACL.
J. Cohen, P. Cohen, S. G. West, and L. S. Aiken. 2003.
Applied Multiple Regression/Correlation Analysis for
the Behavioral Sciences, 3rd Edition. Lawrence Erl-
baum Associates, Inc.
Michael Collins and Nigel Duffy. 2002. Convolution
Kernels for Natural Language. Advances in Neural In-
formation Processing Systems..
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13). pp. 1448-1462..
E. Dale and J. S. Chall. 1948. A Formula for Predicting
Readability. Educational Research Bulletin Vol. 27,
No. 1.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman and Hall, New
York.
R. Gunning. 1952. The technique of clear writing..
McGraw-Hill, New York.
S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003.
DOM-based content extraction of HTML documents.
ACM Press, New York.
Trevor Hastie, Robert Tibshirani, Jerome Friedman.
2003. The Elements of Statistical Learning:Data Min-
ing, Inference, and Prediction. Springer.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining Lex-
ical and Grammatical Features to Improve Readability
Measures for First and Second Language Texts. Pro-
ceedings of the Human Language Technology Confer-
ence. Rochester, NY.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cess of an Intelligent Tutoring System for lexical prac-
tice and reading comprehension. Proceedings of the
Ninth International Conference on Spoken Language
Processing. Pittsburgh, PA.
J. Kincaid, R. Fishburne, R. Rodgers, and B. Chissom.
1975. Derivation of new readability formulas for navy
enlisted personnel. Branch Report 8-75. Chief of
Naval Training, Millington, TN.
G. R. Klare. 1974. Assessing Readability. Reading Re-
search Quarterly, Vol. 10, No. 1. pp. 62-102..
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics,
pp. 423-430.
G. H. McLaughlin. 1969. SMOG grading: A new read-
ability formula. Journal of Reading.
P. McCullagh. 1980. Regression Models for Ordinal
Data. Journal of the Royal Statistical Society. Series
B (Methodological), Vol. 42, No. 2. pp. 109-142.
G. Rasch. 1980. Probabilistic Models for Some Intelli-
gence and Attainment Tests. MESA Press, Chicago,
IL.
G. Rasch. 2005. American National Corpus (ANC) Sec-
ond Release.. Linguistic Data Consortium. Philadel-
phia, PA.
Sarah Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics.
A. J. Stenner, M. Smith, and D. S. Burdick. 1983. To-
ward a Theory of Construct Definition. Journal of Ed-
ucational Measurement, Vol. 20, No. 4. pp. 305-316.
A. J. Stenner. 1996. Measuring reading comprehension
with the Lexile framework. Fourth North American
Conference on Adolescent/Adult Literacy.
S. S. Stevens. 1946. On the theory of scales of measure-
ment. Science, 103, pp. 677-680.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
Y. Yang and J. P. Pedersen. 1997. A Comparative Study
on Feature Selection in Text Categorization. Proceed-
ings of the Fourteenth International Conference on
Machine Learning (ICML?97), pp. 412-420.
G. K. Zipf. 1935. The Psychobiology off Language.
Houghton Mifflin, Boston, MA.
79
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 900?909,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Statistical Estimation of Word Acquisition with
Application to Readability Prediction
Paul Kidwell
Department of Statistics
Purdue University
West Lafayette, IN
kidwellpaul@gmail.com
Guy Lebanon
College of Computing
Georgia Institute of Technology
Atlanta, GA
lebanon@cc.gatech.edu
Kevyn Collins-Thompson
Microsoft Research
Redmond, WA
kevynct@microsoft.com
Abstract
Models of language learning play a cen-
tral role in a wide range of applica-
tions: from psycholinguistic theories of
how people acquire new word knowledge,
to information systems that can automati-
cally match content to users? reading abil-
ity. We present a novel statistical ap-
proach that can infer the distribution of
a word?s likely acquisition age automati-
cally from authentic texts collected from
the Web. We then show that combining
these acquisition age distributions for all
words in a document provides an effective
semantic component for predicting read-
ing difficulty of new texts. We also com-
pare our automatically inferred acquisition
ages with norms from existing oral stud-
ies, revealing interesting historical trends
as well as differences between oral and
written word acquisition processes.
1 Introduction
Word acquisition refers to the temporal process by
which children learn the meaning and understand-
ing of new words. Some words are acquired at
a very early age, some are acquired at early pri-
mary school grades, and some are acquired at high
school or even later in life as the individual under-
goes experiences related to that word. A related
concept to acquisition age is document grade level
readability which refers to the school grade level
of the document?s intended audience. It applies
in situations where documents are written with the
expressed intent of being understood by children
in a certain school grade. For example, textbooks
authored specifically for fourth graders are said to
have readability grade level four.
We develop and evaluate a novel statistical
model that draws a connection between document
grade level readability and age acquisition distri-
butions. Based on previous work in the area, we
define a model for document readability using a
logistic Rasch model and the quantiles of the ac-
quisition age distributions. We then proceed to in-
fer the age acquisition distributions for different
words from document readability data collected
by crawling the web.
We examine the inferred acquisition distribu-
tions from two perspectives. First, we analyze and
contrast them with previous studies on oral word
acquisition, revealing interesting historical trends
as well as differences between oral and written
word acquisition processes. Second, the inferred
acquisition distributions serve as parameters for
the readability model, which enables us to predict
the readability level of novel documents.
To our knowledge, this is the first published
study of a method to ?reverse-engineer? individ-
ual word acquisition statistics from graded texts.
By obtaining such a fine-grained model of how
language evolves over time, we obtain a new,
rich source of semantic features for a document.
The increasing amounts of content available from
the Web and other sources also means that these
flexible models of authentic usage can be eas-
ily adapted for different tasks and populations.
Our work serves to complement the growing body
of research using statistics and machine learn-
ing for language learning tasks, and has appli-
cations including predicting reading difficulty for
Web pages and other non-traditional documents,
reader-specific example and question generation
for lexical practice in intelligent tutoring systems,
and analysis tools for language learning research.
2 A Model for Document Readability
and Word Acquisition
For a fixed word and a fixed population of indi-
viduals T the age of acquisition (AoA) distribu-
tion p
w
represents the age at which word w was
900
acquired by the population. Existing AoA norm
studies almost universally summarize AoA ratings
in terms of two parameters: mean and standard
deviation, ignoring higher-level moments such as
skew. For direct comparison with these studies we
follow this convention and thus our goal is to esti-
mate AoA for a word w in terms of mean ?
w
and
standard deviation ?
w
parameters using the (trun-
cated) normal distribution
p
w
(t) ? N(t ;?
w
, ?
w
) =
e
?(t??
w
)
2
/(2?
2
w
)
?
2??
2
w
(1)
where the proportionality constant ensures that the
distribution is normalized over the range of ages
under consideration e.g., t ? [6, 18] for school
grades. It is important to note that our model is
not restricted by the assumption of (1) and can be
readily extended to the Gamma family of distribu-
tions, if modeling asymmetric spread in the distri-
bution is appropriate.
For a fixed vocabulary V of distinct words the
age acquisition distributions for all words w ? V
are defined using 2|V | parameters
{(?
w
, ?
w
) : w ? V }. (2)
These parameters, which are the main objects of
interest, can in principle be estimated from data
using standard statistical techniques. Unfortu-
nately, data containing explicit acquisition ages is
very difficult to obtain reliably. Explicit word ac-
quisition data is based on interviewing adults re-
garding their age acquisition process during child-
hood and so may be unreliable and difficult to ob-
tain for a large representative group of people.
On the other hand, it is possible to reliably col-
lect large quantities of readability data defined as
pairs of documents and ages of intended audience.
As we demonstrate later in the paper, such data
may be automatically obtained by crawling spe-
cialized resources on the Web. We demonstrate
how to use such data to estimate the word acqui-
sition parameters (2) and to use the estimates to
predict future readability ages.
Traditionally, document readability has been
defined in terms of the school grade level at which
a large portion of the words have been acquired
by most children (Chall and Dale, 1995). We pro-
pose the following interpretation of that definition,
which is made appropriate for quantitative studies
by taking into account the inherent randomness in
the acquisition process.
Definition 1. A document d = (w
1
, . . . , w
m
) is
said to have (1 ? ?
1
, 1 ? ?
2
)-readability level t if
by age t no less than 1? ?
1
percent of the words in
d have been acquired each by no less than 1 ? ?
2
percent of the population.
We denote by q
w
the quantile function of the cdf
corresponding to the acquisition distribution p
w
.
In other words, q
w
(r) represents the age at which
r percent of the population T have acquired word
w. Despite the fact that it does not have a closed
form, it is a continuous and smooth function of the
parameters ?
w
, ?
w
in (1) (assuming T is infinite)
and can be tabulated before inference begins.
Following Definition 1 we define a logistic
Rasch readability model:
log
P (d is (s, r)-readable at age t)
1 ? P (d is (s, r)-readable at age t)
= ?(q
d
(s, r) ? t) (3)
where q
d
(s, r) is the s quantile of {q
w
i
(r) : i =
1, . . . ,m}. An equivalent formulation to (3) that
makes the probability model more explicit is
P (d is (s, r)-readable at age t)
=
exp(?(q
d
(s, r) ? t))
1 + exp(?(q
d
(s, r) ? t))
. (4)
In other words, the probability of a document d
being (s, r)-readable increases exponentially with
q
d
(s, r) which is the age at which s percent of the
words in d have been acquired each by r percent
of the population.
The parameter r = 1 ? ?
2
determines what it
means for a word to be acquired and is typically
considered to be a high value such as 0.8. The
parameter s = 1 ? ?
1
determines how many of
the document words need to be acquired for it to
be readable. It can be set to a high value such as
0.9 if a very precise understanding is required for
readability but can be reduced when a more mod-
est definition of readability applies.
We note that due to the discreteness of the set
{q
w
i
(r) : i = 1, . . . ,m}, neither q
d
(s, r) nor the
loglikelihood are differentiable in the parameters
(2). This raises some practical difficulties with
respect to the computational maximization of the
likelihood and subsequent estimation of (2). How-
ever, for long documents containing a large num-
ber of words, q
d
(s, r) is approximately smooth
which motivates a maximum likelihood procedure
using gradient descent on a smoothed version of
901
qd
(s). Alternative optimization techniques which
do not require smoothness may also be used.
In the case of a normal distribution (1) we have
that a word is acquired by r percent of the pop-
ulation at age w = ? + ??1(r)?, where ? is
the cumulative distribution function (cdf) of the
normal distribution. To investigate the distribu-
tion of acquisition ages we assume that the ?, ?
parameters corresponding to different words in a
document are drawn from Gamma distributions
? ? G(?
1
, ?
1
) and ? ? G(?
2
, ?
2
). The normal
and Gamma distributions are chosen in part be-
cause they are flexible enough to model many sit-
uations and also admit good statistical estimation
theory. Noting that ??1(r)? ? G(?
2
,?
?1
(r)?
2
),
we can write the distribution of the acquisition
ages as the following convolution
f
W
(w) =
w
?
1
+?
2
?1
e
?w/?
2
?(?
1
)?(?
2
)?
?
1
1
?
?
2
2
?
?
1
0
t
?
1
?1
e
(?
1
??
2
)tw
?
1
?
2
(1 ? t)
1??
2
dt
which reverts to a Gamma when ?
1
= ?
2
.
The distribution of the s-percentile of f
W
,
which amounts to (r, s)-readability of documents,
can be analyzed by combining f
W
above with a
standard normal approximation of order statistics
(e.g., (David and Nagaraja, 2003))
X
?mp?
? N
(
F
?1
W
(p),
p(1 ? p)
m[f
W
(F
?1
W
(p))]
2
)
where m is the document length and F
W
is the cdf
corresponding to f
W
.
Figure 1 shows the relationship between docu-
ment length and confidence interval (CI) width in
readability prediction. It contrasts the CI widths
for model based intervals and empirical intervals.
In both cases, documents of lengths larger than
100 words provide CI widths shorter than 1 year.
This finding is also noteworthy as it provides
empirical support for the long-standing ?rule-of-
thumb? that readability measures become unreli-
able for passages of less than 100 words (Fry,
1990).
3 Experimental Results
Our experimental study is divided into three parts.
The first part examines the word acquisition dis-
tributions that were estimated based on readabil-
ity data. The second part compares the estimated
Document Length
95
%
 C
I W
id
th
0.5
1.0
1.5
2.0
2.5
3.0
50 100 150 200
Figure 1: A comparison of model (dashed) vs. em-
pirical (solid) 95% confidence interval widths as a
function of document length (r = 0.9 and s =
0.7). CI widths were computed using 1000 Monte
Carlo samples generated from the f
W
model fit to
the data and from the empirical distribution. Word
distributions correspond to a 1577 word document
written for a 7th grade audience taken from the
Web 1-12 corpus.
(written) acquisition ages with oral acquisition
ages obtained from interview studies reported in
the literature. The third part focuses on using the
estimated word acquisition distributions to predict
document readability. These three experimental
studies are described in the three subsections be-
low.
In our experiments we used three readability
datasets. The corpora were compiled by crawl-
ing web pages containing documents authored for
audiences of specific grade levels. The Web 1-
12 data contains 373 documents, with each doc-
ument written for a particular school grade level
in the range 1-12. The Weekly Reader (WR)
dataset, was obtained by crawling the commercial
website www.wrtoolkit.com after receiving spe-
cial permission. That dataset contains a total of
1780 documents, with 4 readability levels rang-
ing from 2 to 5 indicating the school grade lev-
els of the intended audience. A total of 788 doc-
uments with readability between grades 2 and 5
and having length greater than 50 words were se-
lected from 1780 documents. The Reading A-Z
dataset, contains a set of 215 documents was ob-
tained from Reading A-Z.com, spanning grade 1
through grade 6.
The grade levels in these three corpora, which
correspond to US school grades, were either ex-
plicitly specified by the organization or authors
902
who created the text, or implicit in the class-
room curriculum page where the document was
acquired. The pages were drawn from a wide
range of subject areas, including history, science,
geography, and fiction.
To reduce the possibility of overfitting, we used
a common feature selection technique of eliminat-
ing words appearing in less than 4 documents. In
the experiments we used maximum likelihood to
estimate the model parameters {(?
w
, ?
2
w
) : w ?
V } for the Rasch model (3). The maximum likeli-
hood was obtained using a non-smooth coordinate
descent procedure.
3.1 Estimation of Word Acquisition
Distributions
Figure 2 displays the inferred age acquisition dis-
tributions and empirical word appearances of three
words: thought (left), multitude (middle),
and assimilate (right). In these plots, the em-
pirical cdf of word appearances is indicated by a
piecewise constant line while the probability den-
sity function of the estimated AoA distribution is
indicated by a dashed line. The vertical line in-
dicates the 0.8 quantile of the AoA distribution
which corresponds to the grade by which 80% of
the children have acquired the word.
The word assimilation appears in 2 doc-
uments having 12th grade readability. The high
grade level of these documents results in a high es-
timated acquisition age and the paucity of observa-
tions leads to a large uncertainty in this estimate as
seen by the variance of the acquisition age distri-
bution. The word thought appears several times
in multiple grades. It is first observed in the 1st
grade and not again until the 4th grade resulting in
an estimated acquisition age falling between the
two. The variance of this acquisition distribution
is relatively small due to the frequent use of this
word. The empirical cdf shows that multitude
is used in grades 6, 8, and 9. Relative to thought
and assimilation the word multitude was
used less and more frequently respectively, which
leads to an acquisition age distribution with a
larger variance than that of thought and smaller
than that of assimilation.
The relationship in Figure 2 between the em-
pirical word appearances and the age acquisition
distribution demonstrates the following behavior:
(a) The variance of the age acquisition distribu-
tion goes down as the word appears in more doc-
uments, and (b) the mean of the AoA distribution
tends to be lower than the mean of the empirical
word appearance distribution, and in many cases
even smaller than the first grade in which the word
appeared. This is to be expected as authors use
specific words only after they believe the words
were acquired by a large portion of the intended
audience.
3.2 Comparison with Oral Studies
Among the related work in the linguistic commu-
nity, are several studies concerning oral acquisi-
tions of words. These studies estimate the age
at which a word is acquired for oral use based
an interview processes with participating adults.
We focus specifically on the seminal study of ac-
quisition ages performed by Gilhooly and Logie
(GL) (1980) and made available through the MRC
database (Coltheart, 1981).
There are some substantial differences between
these previous studies and our approach. We an-
alyze the age acquisition process through docu-
ment readability which leads to a written, rather
than oral, notion of word acquisition. Further-
more, our estimates are based on documents writ-
ten with a specific audience in mind, while the pre-
vious studies are based on interviewing adults re-
garding their childhood word acquisition process
which is arguably less reliable due to the age dif-
ference between the acquisition and the interview.
Finally, the GL study was performed in the late
1970s while our study uses contemporary internet
data. Conceivably, the word acquisition process
changed over the past 3 decades.
Despite these differences, it is interesting to
contrast our inferred age acquisitions with the GL
study and consider the differences and similari-
ties. Figure 3 displays the relationship between
the GL age of acquisition (AoA) and the acquisi-
tion ages obtained from readability data based on
the s = 0.8 quantile. Some correlation is present
(r2 = 0.34) but the two measures differ consid-
erably. As expected, the acquisition ages obtained
from written readability data tend to be higher than
the oral studies. The distributions of differences
between the GL acquisition ages and the ones in-
ferred from the readability data appears in Fig-
ure 4.
Comparing the acquisition ages obtained from
readability data to the GL study results in a mean
absolute error of 0.9 to 1.5, depending on the spe-
903
5 10 15
mu: 1.9
sigma: 0.5
5 10 15
mu: 5
sigma: 1.2
5 10 15
mu: 9
sigma: 3.4
Figure 2: A comparison of empirical word appearances and AoA distributions for three words:
thought (left), multitude (middle), and assimilation (right). The empirical cdf of word ap-
pearances appears as a piecewise constant line and the estimated pdf is indicated by the dashed curve
with its 0.8 quantile indicated by a vertical line.
cific value of the Rasch parameter ?. Interestingly,
the tendency for the written acquisition age to ex-
ceed the oral one diminishes as the grade level in-
creases. This represents the notion that at higher
grades words are acquired in both oral and written
senses at the same age.
Predicted versus Oral Acquisition Age
GL AoA
Pr
ed
ict
ed
 A
oA
2
4
6
8
10
2 4 6 8 10
Figure 3: A scatter plot (s = 80, n = 50) of pre-
dicted age of acquisition versus Gilhooly and Lo-
gie?s values reveals the tendency for the written
estimate to exceed the oral estimate (r2 = 0.34).
A comparison to two more recent studies con-
firms relationships that are similar to those ob-
served with GL AoA. The Bristol Norm study
(Stadthagen-Gonzalez and Davis, 2006) was per-
formed in an identical way to the GL study and
comparing the lists of acquisition ages results
in a mean absolute error of approximately 0.5
which is much lower than the .9 to 1.5 relative to
GL. The recent AoA list of Cortese and Khanna
(2008) showed an increase in correlation relative
to the GL study (r2 = 0.43) potentially reflecting
change in the acquisition process due to temporal
effects.
Residual Distribution: Predicted AoA versus Oral AoA 
 S?percentile=80
Error (Predicted AoA ? Actual AoA)
Pe
rc
en
t
0
5
10
15
20
?4 ?2 0 2 4
Figure 4: The difference distribution between
the GL and the inferred AoA from Web 1-12 is
skewed to the right as would be expected since
written AoA is higher than oral AoA. Relaxing
the definition of readability by decreasing s re-
sults in higher inferred acquisition ages. Values
of s in [0.5, 0.9] produced reasonable results, with
s = 0.65 achieving smallest mean absolute error.
Those words that have the same written and
verbal acquisition age are partially attributable to
those words learned prior to first grade. Many
words are learned between the ages of 2 and 5,
while reading materials are typically not assigned
a grade level of less than 1 or age 6. Approxi-
mately 40% of the words assigned the same grade
level by both Gilhooly and our prediction had an
AoA of 1st grade.
In some cases, the ages of acquisition obtained
from readability data is actually lower than the
ages reported in the older oral studies. This phe-
nomenon is likely caused by a combination of
a shift in educational standards, a change in so-
cial standards, or estimation errors due to sample
size and modeling assumptions. Approximately
904
30 years have passed since Gilhooly and Logie?s
study was conducted. Specifically, society has
made efforts to enhance the safety and health of
children and to increase the attention to science
education in very early grades. For example, the
word drug appeared in writing 0.94 grades ear-
lier than the age in which it was acquired orally
according to the GL study. The newer Bristol
Norm study confirms this observation as it pre-
dicts a decrease in grade level for drug of 0.88
over GL as well. A similar decrease in acqui-
sition age relative to the GL norms was noted
for many other words such as hypothesis,
conclusion, engineer, diet, exercise,
and vitamin.
3.3 Global Readability Prediction
Once acquisition age distributions are available,
whether estimated statistically from data or ob-
tained from a survey, they may be used to predict
the grade level of novel documents. Specifically,
the model predicts readability level t? for a novel
document d if it is the minimal grade for which
readability is established:
t
?
= min{t : P (d is readable at age t) ? ?(t)}
(5)
where ?(t) is a parameter describing the strictness
of the readability requirement. Note that we allow
?(t) to vary as a function of time (grade level). We
discuss the justification for this below.
A critical issue for reading difficulty predic-
tion is how to handle words that appear in a new
document that have never been seen in the train-
ing/development texts. In a statistical approach,
the solution to this smoothing problem has two
steps. First, we must decide how much total proba-
bility mass to allocate to all unknown words. Sec-
ond, we must decide how to subdivide this total
mass for individual words or classes of words us-
ing word-specific priors.
Our experience suggests that the first step of
estimating total probability mass is particularly
important: the likelihood of seeing an unknown
word increases as a function of total vocabulary
size, which is continuously growing with time.
We model this by defining the following dynamic
threshold
?(t) =
exp(at? 0.5)
1 + exp(at? 0.5)
. (6)
We learn the growth rate parameter a in (6) from
the data at the same time as we learn the read-
ability model?s quantile parameters s = 1 ? ?
1
,
r = 1 ? ?
2
. The range of the resulting ?(t) is
typically 0.5 in lower grades, increasing to 0.9 in
higher grades. We discuss fitting these parameters
and their optimal values further in Sec. 3.3.1. We
found that using any fixed ? value for all grades
was generally much less effective than a dynamic
?(t) threshold, and so we focus on the latter in our
evaluation.
For the second (word-specific) smoothing step,
we simply assign uniform probability across
grades, once the total unseen mass is determined.
More sophisticated word-specific priors incorpo-
rating word length, morphological features, se-
mantic clusters and so on are certainly possible
and an interesting direction for future work.
In the following section we conduct three exper-
iments involving readability prediction. First, we
confirm the effectiveness of the AoA-based model
compared to other predictive models. Second, we
examine how prediction effectiveness is affected
when our learned (written) acquisition ages are re-
placed with existing oral AoA norms. Third, we
examine the ability of our model to generalize to
new content by training and testing on different
(non-overlapping) corpora.
3.3.1 Effectiveness of Readability Prediction
In order to assess the effectiveness of our model
in predicting the readability grade levels of novel
documents we apply the model to two corpora.
First, we use the Web 1-12 corpus to learn opti-
mal parameter values for a , r, and s and then as-
sess prediction error using a test-training paradigm
for the proposed model, Naive Bayes, and support
vector regression. Second, the trained model is ap-
plied with to the Reader A-Z corpus and the results
are compared with alternative semantic variables.
Because corpora can vary significantly in text ho-
mogeneity, amount of noise, document size, and
other factors, training and testing across different
corpora ? rather than relying on cross-validation
with a single pooled dataset ? gives valuable in-
formation about how a prediction method might
be expected to perform on data with widely differ-
ent characteristics. This particular choice of Web
1-12 for training and ReadingA-Z for testing was
arbitrary.
To evaluate the best values for the a parameter
in (6) and s, r parameters in Definition 1 we gen-
905
Readability Level Prediction: MAE and Correlation
S?th Percentile
M
ea
n 
Ab
so
lu
te
 E
rro
r
1
2
3
0.5 0.6 0.7 0.8 0.9
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Correlation
Figure 5: Mean absolute error (MAE) and correla-
tion coefficient as functions of the quantile param-
eter s at optimal levels of a and r, averaged over
100 training/test samples. The MAE is displayed
as the solid line and is aligned with the left axis
while the correlation is displayed as a dashed line
and is aligned with the right axis. 90% bootstrap
confidence intervals are displayed.
erated 100 independent test and training samples
and computed the mean absolute prediction error
(MAE) and the correlation coefficient between the
predicted and actual levels. Figure 5 (left) shows
these two quantities: in each group of three lines,
the top and bottom lines delineate the upper and
lower 90% confidence bounds for the middle line.
Each middle line gives mean error or correlation
as a function of the quantile parameter s at opti-
mal levels of r and a, averaged over the 100 train-
ing/test samples. The optimal value of s for both
quantities is around 0.6 (0.65 for the MAE). The
optimal value for parameter a was approximately
1.55. The best MAE is 1.4 which compares favor-
ably to the 2.92 MAE obtained by always predict-
ing Grade 6 which is the optimal ?dumb? classifier
in the sense that of all constant predictors it pro-
vides the smallest expected MSE over a uniform
grade distribution as is the case with the Web1-
12 corpus. Figure 6 is a scatter plot comparing
predicted grades vs. actual grades, with a strong
correlation of 0.89.
We compared the predictions of model (3) to
two standard classifiers: naive Bayes and support
vector regression (SVR). SVR was applied twice
using different sets of features - once with the doc-
ument word frequencies and once with the esti-
Predicted v Actual Grade Level
Actual Grade Level
Pr
ed
ict
ed
 G
ra
de
 L
ev
el
2
4
6
8
10
12
2 4 6 8 10 12
Figure 6: The scatter plot demonstrates the strong
relationship between predicted and actual global
readability levels.
Prediction Rule MAE LB UB
Age of Acquisition 1.40 1.19 1.67
Naive Bayes 1.98 1.71 2.26
SVR (word frequency) 1.86 1.69 2.06
SVR (AoA percentiles) 1.36 1.22 1.58
Grade 6 2.92 - -
Figure 7: A comparison of mean absolute error
(MAE) across prediction algorithms shows the age
of acquisition model compares favorably. The
confidence bounds (LB,UB) were computed by re-
peating each model building procedure 100 times.
mated AoA percentiles for the document words.
The document word frequency vector is compa-
rable to the semantic component of the machine
learning approach used by (Heilman et al, 2008).
The 75-25 training-test model building paradigm
was used over documents from grades 1 to 12
to obtain predicted values. The MAE for these
predictors and their 90% confidence intervals are
shown in Figure 7. Predicting readability using
word frequencies had inferior performance, with
the naive Bayes model performing poorly and the
SVR and Rasch model obtaining MAE around 1.4.
In the second experiment, we compared our
model to published correlation results (Collins-
Thompson and Callan, 2005) for multiple alter-
native semantic variables using the same Reading
A-Z corpus, with the results shown in Fig. 8. De-
tails on these semantic variables, which have been
used in previous statistical learning approaches,
are available in the same study. Interestingly, the
correlation of the model was comparable to ex-
906
Correlation Correlation
GL (Web) .65 UNK .78
GL (WR) .40 Type .86
Bristol (Web) .76 MLF .49
Bristol (WR) .57 FK .30
Inferred (Web) .59 Unigram .63
Figure 8: Comparison of the correlation of AoA
and other semantic variables with grade level for
the Reading A-Z corpus, showing the AoA model
with the dynamic threshold compares well to ex-
isting methods. The competitor methods used
are from (Collins-Thompson and Callan, 2005)
and comprise the Smoothed Unigram, UNK (rel-
ative to revised Dale-Chall), TYPE (number of
unique words), MLF (mean log frequency), and
FK (Flesch-Kincaid readability).
isting variables, but did vary depending upon the
source of AoA. Note that because the Reading A-Z
texts were assigned grades by their creators using
some of the same semantic variables (e.g. Type),
it is not surprising that those variables perform es-
pecially well on this dataset.
High quality readability prediction is a worth-
while result in itself; however, we can also use the
prediction mechanism to study the validity of Def-
inition 1 and the Rasch model. We do so by apply-
ing other predictive algorithms using the inferred
acquisition age distribution for each document as
the predictor variables and comparing the MAE
with the MAE obtained by the estimated Rasch
model. In particular, we examine the performance
of support vector regression (SVR) using the esti-
mated AoA percentiles for each document as pre-
dictor variables. The results displayed in Fig-
ure 7 show that SVR and the dynamic threshold
prediction rule perform similarly well, suggesting
that Definition 1 and the Rasch model are suitable
models for readability prediction.
3.3.2 Prediction with Existing Acquisition
Age Norms
We now examine how predicting readability of
novel documents using acquisition ages obtained
in surveys perform in comparison to the ages ob-
tained from the maximum likelihood estimation.
We use the GL and Bristol age of acquisition
norms. The intersection of AoA norm data and the
Web Corpus are 1217 and 1012 words respectively
for the GL and Bristol measure; additionally, the
highest grade level associated with these word sets
S-th Dynamic
Prediction Rule Percentile Threshold
Age of Acquisition 1.69 1.40
GL Norms 1.73 1.42
Bristol Norms 1.97 1.79
Figure 9: The Gilhooly and Logie AoA norms and
the Bristol norms are independent sources for ages
of acquisition. A comparison of the prediction
quality using these norms shows two things: 1) the
definition provides comparable prediction quality
using expert norms, and 2) the dynamic threshold
?(t) improves prediction over the static threshold
(optimal s-th percentile) for the norms.
AoA Weekly
Source Web 1-12 Reader
Inferred (Weekly Reader) - .91
Inferred (Web 1-12) 1.89 -
GL 2.05 1.14
Bristol 1.57 1.34
Figure 10: The readability of WR documents was
predicted using 4 sources of AoA data. The pa-
rameters of the prediction model were fit using
only the Web data, or the WR data, or both sources
in the case of the GL and Bristol norms AoA data.
are eight and seven respectively. When applying
the prediction rule using AoA norms r is implic-
itly selected in the norming process as the result
is a single value instead of a distribution. Interest-
ingly, the optimal ranges of s-percentile, from 92
to 100, were the same for both the GL and Bristol
norms. Table 9 shows that the prediction accuracy
obtained using the GL Norms was almost identical
to that obtained with the inferred AoA, while the
Bristol Norms performed as well as some of the
competitor procedures.
3.3.3 Prediction Effectiveness across
Different Corpora
To provide additional evidence for our model?s
ability to generalize to new corpora, we exam-
ine how the learned r and s values vary when the
model is learned on one corpus and evaluated on
another, and how this affects the accuracy of the
readability prediction.
Figure 10 demonstrates the corpus used for tun-
ing the readability prediction has a large impact
on the quality of the prediction. Comparing the
MAE of the readability predictions on WR data
907
when the age of acquisition is inferred from Web
data to the MAE when the AoA is inferred from
WR data shows the error rate more than doubles
from 0.90 to 1.89. The increase in error rate also
appears when the age of acquisition for WR data
is predicted using the AoA norm data. In this case
the prediction was performed using the parameters
identified when the model was trained on Web data
and when the model was trained on WR data. In
each case a tendency to overfit appears as the MAE
increases from 1.14 to 2.05 for the GL norms and
1.34 to 1.57 for the Bristol norms. Interestingly,
the Bristol norms perform better on WR data when
fit using the Web data, while the GL norms per-
form better when fit using the WR data.
4 Related Work
Age of acquisition for word reading and under-
standing has been extensively studied as a learn-
ing factor in the psycholinguistics literature, where
AoA norms have been obtained using surveys. Ex-
amples of relevant literature are (Gilhooly and Lo-
gie, 1980; Zevin and Seidenberg, 2002). Our ap-
proach differs by connecting AoA to readability
through Definition 1 and using readability data to
estimate AoA norms from large amounts of au-
thentic language data. A related study is that by
Crossley et al (2007) who used AoA to help dis-
criminate between authentic and simplified texts
for second-language readers.
In the past decade, there has been renewed in-
terest in corpus-based statistical models for read-
ability prediction. One example is the popular
Lexile measure (Stenner, 1996) which uses word
frequency statistics from a large English corpus.
Collins-Thompson and Callan (2005) introduced a
new approach based on statistical language mod-
eling, treating a document as a mixture of lan-
guage models for individual grades. Further re-
cent refinements in methods for readability predic-
tion include using machine learning methods such
as Support Vector Machines (Schwarm and Os-
tendorf, 2005), log-linear models (Heilman et al,
2008), k-NN classifiers and combining semantic
and grammatical features (Heilman et al, 2007).
The growing number of features investigated by
these machine learning approaches reflect the fact
that reading difficulty is a complex phenomenon
involving many factors, from semantic difficulty
(vocabulary) to syntax and discourse complex-
ity, reader background, and others. While a full-
featured comparison between previous approaches
that includes AoA features would be very inter-
esting, our goal in this study was to provide a
clear analysis of the most fundamental factor of
readability, semantic difficulty, which accounts for
80-90% of the variance in readability prediction
scores (Chall and Dale, 1995). Because AoA is
a semantic, vocabulary-based representation, we
compare its effectiveness with the correspond-
ing semantic components from previous machine-
learning approaches in Sec. 3.3.1.
5 Discussion
While there have been several recent studies re-
garding word acquisition and readability our work
is the first to provide a quantitative connection be-
tween these two concepts in a statistically mean-
ingful way. The core assumption that we make
is Definition 1 which is consistent with standard
readability definitions e.g., (Chall and Dale, 1995)
and states that document readability level is deter-
mined by most people understanding most words.
The connection between word acquisition and
readability is both intuitive and useful. It allows
two degrees of freedom s = 1? ?
1
and r = 1? ?
2
to handle situations where different readability no-
tions exist. Experiments validate the model and
demonstrate interesting trends in word acquisi-
tions as compared to older oral acquisition stud-
ies. Experimental results show that the proposed
model is also effective in terms of predicting read-
ability level of documents on multiple datasets.
It compares favorably to naive Bayes and sup-
port vector regression, the latter being one of the
strongest regression baselines.
Acknowledgments
The authors thank Joshua Dillon for downloading
the weekly reader data and pre-processing it. The
work described in this paper was funded in part by
NSF grant DMS-0604486.
References
J. S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brook-
line Books, Brookline, MA.
K. Collins-Thompson and J. Callan. 2005. Predicting
reading difficulty with statistical language models.
J. of the American Soc. for Info. Science and Tech.,
56(13):598?605.
908
M. Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psy-
chology, 33A:497?505.
M. Cortese and M. Khanna. 2008. Age acquisition
ratings for 3000 monosyllabic words. Behavior Re-
search Methods, 40:791?794.
S. A. Crossley, P. M. McCarthy, and D. S. McNa-
mara. 2007. Discriminating between second lan-
guage learning text-types. In Proc. of the Twenti-
eth International Florida Artificial Intelligence Re-
search Society Conference.
H. A. David and H. N. Nagaraja. 2003. Order Statis-
tics. Wiley, Marblehead, MA.
E. Fry. 1990. A readability formula for short passages.
Journal of Reading.
K. J. Gilhooly and R. H. Logie. 1980. Age of acquisi-
tion, imagery, concreteness, familiarity and ambigu-
ity measures for 1944 words. Behaviour Research
Methods and Instrumentation, 12:395?427.
M. Heilman, K. Collins-Thompson, J. Callan, and
M. Eskenazi. 2007. Combining lexical and gram-
matical features to improve readability measures for
first and second language texts. In Proc. of the Hu-
man Language Technology Conference.
M. Heilman, K. Collins-Thompson, and M. Eskenazi.
2008. An analysis of statistical models and features
for reading difficulty prediction. In The 3rd Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications.
S. E. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and sta-
tistical language models. In Proc. of the Association
of Computational Linguistics.
H. Stadthagen-Gonzalez and C. J. Davis. 2006. The
bristol norms for age of acquisition, imageabil-
ity, and familiarity. Behavior Research Methods,
38:598?605.
A. J. Stenner. 1996. Measuring reading comprehen-
sion with the Lexile Framework. Metametrics, Inc.,
Durham, NC.
J. D. Zevin and M. S. Seidenberg. 2002. Age of acqui-
sition effects in word reading and other tasks. Jour-
nal of Memory and Language, 47(1):1?29.
909
