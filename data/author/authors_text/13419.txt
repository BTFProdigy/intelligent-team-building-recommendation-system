2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 60?69,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Entity Clustering Across Languages
Spence Green*, Nicholas Andrews?, Matthew R. Gormley?,
Mark Dredze?, and Christopher D. Manning*
*Computer Science Department, Stanford University
{spenceg,manning}@stanford.edu
?Human Language Technology Center of Excellence, Johns Hopkins University
{noa,mrg,mdredze}@cs.jhu.edu
Abstract
Standard entity clustering systems commonly
rely on mention (string) matching, syntactic
features, and linguistic resources like English
WordNet. When co-referent text mentions ap-
pear in different languages, these techniques
cannot be easily applied. Consequently, we
develop new methods for clustering text men-
tions across documents and languages simulta-
neously, producing cross-lingual entity clusters.
Our approach extends standard clustering algo-
rithms with cross-lingual mention and context
similarity measures. Crucially, we do not as-
sume a pre-existing entity list (knowledge base),
so entity characteristics are unknown. On an
Arabic-English corpus that contains seven dif-
ferent text genres, our best model yields a 24.3%
F1 gain over the baseline.
1 Introduction
This paper introduces techniques for clustering co-
referent text mentions across documents and lan-
guages. On the web today, a breaking news item
may instantly result in mentions to a real-world entity
in multiple text formats: news articles, blog posts,
tweets, etc. Much NLP work has focused on model
adaptation to these diverse text genres. However, the
diversity of languages in which the mentions appear
is a more significant challenge. This was particularly
evident during the 2011 popular uprisings in the Arab
world, in which electronic media played a prominent
role. A key issue for the outside world was the aggre-
gation of information that appeared simultaneously
in English, French, and various Arabic dialects.
To our knowledge, we are the first to consider clus-
tering entity mentions across languages without a pri-
ori knowledge of the quantity or types of real-world
entities (a knowledge base). The cross-lingual set-
ting introduces several challenges. First, we cannot
assume a prototypical name format. For example,
the Anglo-centric first/middle/last prototype used in
previous name modeling work (cf. (Charniak, 2001))
does not apply to Arabic names like Abdullah ibn
Abd Al-Aziz Al-Saud or Chinese names like Hu Jin-
tao (referred to as Mr. Hu, not Mr. Jintao). Sec-
ond, organization names often require both translit-
eration and translation. For example, the Arabic
	PP?

K?? ?Q
	
g.

??Q?? ?General Motors Corp? contains
transliterations of 	PP?K?? ?Q
	
g. ?General Motors?,
but a translation of

??Q?? ?Corporation?.
Our models are organized as a pipeline. First, for
each document, we perform standard mention detec-
tion and coreference resolution. Then, we use pair-
wise cross-lingual similarity models to measure both
mention and context similarity. Finally, we cluster
the mentions based on similarity.
Our work makes the following contributions: (1)
introduction of the task, (2) novel models for cross-
lingual entity clustering of person and organization en-
tities, (3) cross-lingual annotation of the NIST Auto-
matic Content Extraction (ACE) 2008 Arabic-English
evaluation set, and (4) experimental results using both
gold and automatic within-document processing. We
will release our software and annotations to support
future research.
1.1 Task Description via a Simple Example
Consider the toy corpus in Fig. 1. The English docu-
ments contain mentions of two people: Steven Paul
Jobs and Mark Elliot Zuckerberg. Of course, the sur-
face realization of Mr. Jobs? last name in English is
also an ordinary nominal, hence the ambiguous men-
tion string (absent context) in the second document.
The Arabic document introduces an organization en-
tity (Apple Inc.) along with proper and pronominal
references to Mr. Jobs. Finally, the French document
refers to Mr. Jobs by the honorific ?Monsieur,? and to
60
Jobs program details delayed
Steve Jobs admired Mark Zuckerberg
M. Jobs, le fondateur d'Apple, est mort
	
		
?
=
? E1
E2
E3
=
=
doc1:
doc2:
doc3:
doc4:
Figure 1: Clustering entity mentions across languages and documents. The toy corpus contains English (doc1 and
doc2), Arabic (doc3), and French (doc4). Together, the documents make reference to three real-world entities, the
identification of which is the primary objective of this work. We use a separately-trained system for within-document
mention detection and coreference (indicated by the text boxes and intra-document links, respectively). Our experimental
results are for Arabic-English only.
Apple without its corporate designation.
Our goal is to automatically produce the cross-
lingual entity clusters E1 (Mark Elliot Zuckerberg),
E2 (Apple Inc.), and E3 (Steven Paul Jobs). Both the
true number and characteristics of these entities are
unobserved. Our models require two pre-processing
steps: mention detection and within-document coref-
erence/anaphora resolution, shown in Fig. 1 by the
text boxes and intra-document links, respectively. For
example, in doc3, a within-document coreference sys-
tem would pre-link 	QK. ?k. joobz ?Jobs? with the mascu-
line pronoun ? h ?his?. In addition, the mention detec-
tor determines that the surface form ?Jobs? in doc2
is not an entity reference. For this within-document
pre-processing we use Serif (Ramshaw et al, 2011).1
Our models measure cross-lingual similarity of the
coreference chains to make clustering decisions (?
in Fig. 1). The similarity models (indicated by the
= and 6= operators in Fig. 1) consider both mention
string and context similarity (?2). We use the men-
tion similarities as hard constraints, and the context
similarities as soft constraints. In this work, we inves-
tigate two standard constrained clustering algorithms
(?3). Our methods can be used to extend existing sys-
tems for mono-lingual entity clustering (also known
as ?cross-document coreference resolution?) to the
cross-lingual setting.
1Serif is a commercial system that assumes each document
contains only one language. Currently, there are no publicly avail-
able within-document coreference systems for Arabic and many
other languages. To remedy this problem, the CoNNL-2012
shared task aims to develop multilingual coreference systems.
2 Mention and Context Similarity
Our goal is to create cross-lingual sets of co-referent
mentions to real-world entities (people, places, orga-
nizations, etc.). In this paper, we adopt the following
notation. LetM be a set of distinct text mentions in a
collection of documents;C is a partitioning ofM into
document-level sets of co-referent mentions (called
coreference chains); E is a partitioning of C into sets
of co-referent chains (called entities). Let i, j be non-
negative integers less than or equal to |M | and a, b be
non-negative integers less than or equal to |C|. Our
experiments use a separate within-document corefer-
ence system to createC, which is fixed. We will learn
E, which has size no greater than |C| since the set of
mono-lingual chains is the largest valid partitioning.
We define accessor functions to access properties
of mentions and chains. For any mentionmi, define
the following functions: lang(mi) is the language;
doc(mi) is the document containingmi; type(mi) is
the semantic type, which is assigned by the within-
document coreference system. We also extract a set
of mention contexts S, which are the sentences con-
taining each mention (i.e., |S| = |M |).
We learn the partition E by considering mention
and context similarity, which are measured with sep-
arate component models.
2.1 Mention Similarity
We use separate methods for within- and cross-
language mention similarity. The pairwise similarity
61
Arabic Rules
H. ? b
H? t H? th h. ? j
h? h p? kh X? d
	
X? th
P? r 	P? z ?? s ?? sh
?? s 	?? d ?? t 	?? th
?? a
	
?? g
	
?? f

?? q
?? k ?? l ?? m 	?? n
?? h @? a ?? w ?? a

?? ah ?


? ? Z? ?
English Rules
k? c p? b x? ks e,i,o,u? ?
Table 1: English-Arabic mapping rules to a common or-
thographic representation. ??? indicates a null mapping.
For English, we also lowercase and remove determiners
and punctuation. For Arabic, we remove the determiner
?@ Al ?the? and the elongation character tatwil ??.
of any two mentionsmi andmj is:
sim(mi,mj) =
{
jaro-winkler(mi,mj) if lang(mi) = lang(mj)
maxent(mi,mj) otherwise
Jaro-Winkler Distance (within-language) If
lang(mi) = lang(mj), we use the Jaro-Winkler edit
distance (Porter and Winkler, 1997). Jaro-Winkler
rewards matching prefixes, the empirical justification
being that less variation typically occurs at the
beginning of names.2 The metric produces a score in
the range [0,1], where 0 indicates equality.
Maxent model (cross-language) When lang(mi)
6= lang(mj), then the two mentions might be in dif-
ferent writing systems. Edit distance calculations
no longer apply directly. One solution would be
full-blown transliteration (Knight and Graehl, 1998),
followed by application of Jaro-Winkler. However,
transliteration systems are complex and require sig-
nificant training resources. We find that a simpler,
low-resource approach works well in practice.
First, we deterministically map both languages to a
common phonetic representation (Tbl. 1).3 Next, we
align the mention pairs with the Hungarian algorithm,
2For multi-token names, we sort the tokens prior to computing
the score, as suggested by Christen (2006).
3This idea is reminiscent of Soundex, which Freeman et al
(2006) used for cross-lingual name matching.
Overlap Active for each bigram in
cbigrams(mi,u)
?
cbigrams(mj,v)
Bigram-Diff-mi Active for each bigram in
cbigrams(mi)? cbigrams(mj)
Bigram-Diff-mj Active for each bigram in
cbigrams(mj)? cbigrams(mi)
Bigram-Len-Diff Value of abs(size(cbigrams(mi)?
cbigrams(mj)))
Big-Edit-Dist Count of token pairs with
Lev(mi,u,mj,v) > 3.0
Total-Edit-Dist Sum of aligned token edit distances
Length Active for one of:
len(mi) > len(mj) or
len(mi) < len(mj) or
len(mi) = len(mj)
Length-Diff abs(len(mi)? len(mj))
Singleton Active if len(mi) = 1
Singleton-Pair Active if len(mi) = len(mj) = 1
Table 2: Cross-language Maxent feature templates for a
whitespace-tokenized mention pair ?mi,mj? with align-
ment Ami,mj . Let (u, v) ? Ami,mj indicate aligned to-
ken indices. Define the following functions for strings:
cbigrams(?) returns the set of character bigrams; len(?) is
the token length; Lev(?, ?) is the Levenshtein edit distance
between two strings. Prior to feature extraction, we add
unique start and end symbols to the mention strings.
which produces a word-to-word alignment Ami,mj .
4
Finally, we build a simple binary Maxent classifier
p(y|mi,mj ;?) that extracts features from the aligned
mentions (Tbl. 2). We learn the parameters ? using a
quasi-Newton procedure with L1 (lasso) regulariza-
tion (Andrew and Gao, 2007).
2.2 Context Mapping and Similarity
Mention strings alone are not always sufficient for
disambiguation. Consider again the simple exam-
ple in Fig. 1. Both doc3 and doc4 reference ?Steve
Jobs? and ?Apple? in the same contexts. Context co-
occurence and/or similarity can thus disambiguate
these two entities from other entities with similar ref-
erences (e.g., ?Steve Jones? or ?Apple Corps?). As
with the mention strings, the contexts may originate
in different writing systems. We consider both high-
and low-resource approaches for mapping contexts to
a common representation.
4The Hungarian algorithm finds an optimal minimum-cost
alignment. For pairwise costs between tokens, we used the Lev-
enshtein edit distance
62
Machine Translation (MT) For the high-resource
setting, if lang(mi) 6=English, then we translate both
mi and its context si to English with an MT system.
We use Phrasal (Cer et al, 2010), a phrase-based
system which, like most public MT systems, lacks a
transliteration module. We believe that this approach
yields the most accurate context mapping for high-
resource language pairs (like English-Arabic).
Polylingual Topic Model (PLTM) The polylin-
gual topic model (PLTM) (Mimno et al, 2009) is
a generative process in which document tuples?
groups of topically-similar documents?share a topic
distribution. The tuples need not be sentence-aligned,
so training data is easier to obtain. For example, one
document tuple might be the set of Wikipedia articles
(in all languages) for Steve Jobs.
Let D be a set of document tuples, where
there is one document in each tuple for each
of L languages. Each language has vocabu-
lary Vl and each document dlt has N
l
t tokens.
We specify a fixed-size set of topics K. The
PLTM generates the document tuples as follows:
Polylingual Topic Model
?t ? Dir(?K) [cross-lingual tuple-topic prior]
?lk ? Dir(?
Vl) [word-topic prior]
for each token wlt,n with n = {1, . . . , N
l
t}:
zt,n ? Mult(?t)
wlt,n ? Mult(?
l
zt,n)
For cross-lingual context mapping, we infer the 1-
best topic assignments for each token in all S mention
contexts. This technique reduces Vl = k for all l.
Moreover, all languages have a common vocabulary:
the set of K topic indices. Since the PLTM is not
a contribution of this paper, we refer the interested
reader to (Mimno et al, 2009) for more details.
After mapping each mention context to a common
representation, we measure context similarity based
on the choice of clustering algorithm.
3 Clustering Algorithms
We incorporate the mention and context similarity
measures into a clustering framework. We consider
two algorithms. The first is hierarchical agglomera-
tive clustering (HAC), with which we assume basic
familiarity (Manning et al, 2008). A shortcoming of
HAC is that a stop threshold must be tuned. To avoid
this requirement, we also consider non-parametric
probabilistic clustering in the form of a Dirichlet pro-
cess mixture model (DPMM) (Antoniak, 1974) .
Both clustering algorithms can be modified to ac-
commodate pairwise constraints. We have observed
better results by encoding mention similarity as a
hard constraint. Context similarity is thus the cluster
distance measure.5
To turn the Jaro-Winkler distance into a hard
boolean constraint, we tuned a threshold ? on held-out
data, i.e., jaro-winkler(mi,mj) ? ? ? mi = mj .
Likewise, the Maxent model is a binary classifier, so
p(y = 1|mi,mj ;?) > 0.5? mi = mj .
In both clustering algorithms, any two chains Ca
and Cb cannot share the same cluster assignment if:
1. Document origin: doc(Ca) = doc(Cb)
2. Semantic type: type(Ca) 6= type(Cb)
3. Mention Match: sim(mi,mj) = false,
wheremi = repr(Ca) andmj = repr(Cb).
The deterministic accessor function repr(Ca) returns
the representative mention of a chain. The heuristic
we used was ?first mention?: the function returns the
earliest mention that appears in the associated docu-
ment. In many languages, the first mention is typi-
cally more complete than later mentions. This heuris-
tic also makes our system less sensitive to within-
document coreference errors.6 The representative
mention only has special status for mention similar-
ity: context similarity considers all mention contexts.
3.1 Constrained Hierarchical Clustering
HAC iteratively merges the ?nearest? clusters accord-
ing to context similarity. In our system, each cluster
context is a bag of wordsW formed from the contexts
of all coreference chains in that cluster. For each word
inW we estimate a unigram Entity Language Model
(ELM) (Raghavan et al, 2004):
P (w) =
countW (w) + ?PV (w)
?
w? countW (w
?) + ?
PV (w) is the unigram probability in all contexts in
the corpus7 and ? is a smoothing parameter. For any
5Specification of a combined similarity measure is an inter-
esting direction for future work.
6These constraints are similar to the pair-filters of Mayfield
et al (2009).
7Recall that after context mapping, all languages have a com-
mon vocabulary V .
63
two entity clusters Ea and Eb, the distance between
PEa and PEb is given by a metric based on the Jensen-
Shannon Divergence (JSD) (Endres and Schindelin,
2003):
dist(PEa , PEb) =
?
2 ? JSD(PEa ||PEb)
=
?
KL(PEa ||M) +KL(M ||PEb)
where KL(PEa ||M) is the Kullback-Leibler diver-
gence andM = 12(PEa + PEb).
We initialize HAC to E = C, i.e., the initial clus-
tering solution is just the set of all coreference chains.
Thenwe remove all links in the HAC proximitymatrix
that violate pairwise cannot-link constraints. During
clustering, we do not merge Ea and Eb if any pair of
chains violates a cannot-link constraint. This proce-
dure propagates the cannot-link constraints (Klein et
al., 2002). To output E, we stop clustering when the
minimum JSD exceeds a stop threshold ?, which is
tuned on a development set.
3.2 Constrained Dirichlet Process Mixture
Model (DPMM)
Instead of tuning a parameter like ?, it would be prefer-
able to let the data dictate the number of entity clus-
ters. We thus consider a non-parametric Bayesian
mixture model where the mixtures are multinomial
distributions over the entity contexts S. Specifically,
we consider a DPMM, which automatically infers
the number of mixtures. Each Ca has an associated
mixture ?a:
Ca|?a ? Mult(?a)
?a|G ? G
G|?,G0 ? DP(?,G0)
? ? Gamma(1, 1)
where ? is the concentration parameter of the DP
prior and G0 is the base distribution with support V .
For our experiments, we set G0 = Dir(pi1, . . . , piV ),
where pii = PV (wi).
For inference, we use the Gibbs sampler of Vla-
chos et al (2009), which can incorporate pairwise
constraints. The sampler is identical to a standard col-
lapsed, token-based sampler, except the conditional
probability p(Ea = E|E?a, Ca) = 0 if Ca cannot
be merged with the chains in clusterE. This property
makes the model non-exchangeable, but in practice
non-exchangeable models are sometimes useful (Blei
and Frazier, 2010). During sampling, we also learn ?
using the auxiliary variable procedure of West (1995),
so the only fixed parameters are those of the vague
Gamma prior. However, we found that these hyper-
parameters were not sensitive.
4 Training Data and Procedures
We trained our system for Arabic-English cross-
lingual entity clustering.8
Maxent Mention Similarity The Maxent mention
similarity model requires a parallel name list for train-
ing. Name pair lists can be obtained from the LDC
(e.g., LDC2005T34 contains nearly 450,000 parallel
Chinese-English names) or Wikipedia (Irvine et al,
2010). We extracted 12,860 name pairs from the par-
allel Arabic-English translation treebanks,9 although
our experiments show that the model achieves high
accuracy with significantly fewer training examples.
We generated a uniform distribution of training ex-
amples by running a Bernoulli trial for each aligned
name pair in the corpus. If the coin was heads, we
replaced the English name with another English name
chosen randomly from the corpus.
MT Context Mapping For the MT context map-
ping method, we trained Phrasal with all data permit-
ted under the NIST OpenMT Ar-En 2009 constrained
track evaluation. We built a 5-gram language model
from the Xinhua and AFP sections of the Gigaword
corpus (LDC2007T07), in addition to all of the target
side training data. In addition to the baseline Phrasal
feature set, we used the lexicalized re-ordering model
of Galley and Manning (2008).
PLTM Context Mapping For PLTM training, we
formed a corpus of 19,139 English-Arabic topically-
aligned Wikipedia articles. Cross-lingual links in
Wikipedia are abundant: as of February 2010, there
were 77.07M cross-lingual links among Wikipedia?s
272 language editions (de Melo and Weikum, 2010).
To increase vocabulary coverage for our ACE2008
evaluation corpus, we added 20,000 document sin-
gletons from the ACE2008 training corpus. The
8We tokenized all English documents with packages from
the Stanford parser (Klein and Manning, 2003). For Arabic
documents, we used Mada (Habash and Rambow, 2005) for
orthographic normalization and clitic segmentation.
9LDC Catalog numbers LDC2009E82 and LDC2009E88.
64
topically-aligned tuples served as ?glue? to share top-
ics between languages, while the ACE documents
distribute those topics over in-domain vocabulary.10
We used the PLTM implementation in Mallet (Mc-
Callum, 2002). We ran the sampler for 10,000 itera-
tions and set the number of topicsK = 512.
5 Task Evaluation Framework
Our experimental design is a cross-lingual extension
of the standard cross-document coreference resolu-
tion task, which appeared in ACE2008 (Strassel et
al., 2008; NIST, 2008). We evaluate name (NAM)
mentions for cross-lingual person (PER) and organi-
zation (ORG) entities. Neither the number nor the
attributes of the entities are known (i.e., the task does
not include a knowledge base). We report results for
both gold and automatic within-document mention
detection and coreference resolution.
Evaluation Metrics We use entity-level evaluation
metrics, i.e., we evaluate the E entity clusters rather
than the mentions. For the gold setting, we report:
? B3 (Bagga and Baldwin, 1998a): Precision and
recall are computed from the intersection of the
hypothesis and reference clusters.
? CEAF (Luo, 2005): Precision and recall are
computed from a maximum bipartite matching
between hypothesis and reference clusters.
? NVI (Reichart and Rappoport, 2009):
Information-theoretic measure that uti-
lizes the entropy of the clusters and their mutual
information. Unlike the commonly-used Varia-
tion of Information (VI) metric, normalized VI
(NVI) is not sensitive to the size of the data set.
For the automatic setting, we must apply a different
metric since the number of system chains may differ
from the reference. We use B3sys (Cai and Strube,
2010), a variant of B3 that was shown to penalize
both twinless reference chains and spurious system
chains more fairly.
Evaluation Corpus The automatic evaluation of
cross-lingual coreference systems requires annotated
10Mimno et al (2009) showed that so long as the proportion
of topically-aligned to non-aligned documents exceeded 0.25,
the topic distributions (as measured by mean Jensen-Shannon
Divergence between distributions) did not degrade significantly.
Docs Tokens Entities Chains Mentions
Arabic 412 178,269 2,594 4,216 9,222
English 414 246,309 2,278 3,950 9,140
Table 3: ACE2008 evaluation corpus PER and ORG entity
statistics. Singleton chains account for 51.4% of the Arabic
data and 46.2% of the English data. Just 216 entities appear
in both languages.
multilingual corpora. Cross-document annotation
is expensive (Strassel et al, 2008), so we chose the
ACE2008 Arabic-English evaluation corpus as a start-
ing point for cross-lingual annotation. The corpus
consists of seven genres sampled from independent
sources over the course of a decade (Tbl. 3). The
corpus provides gold mono-lingual cross-document
coreference annotations for both PER and ORG enti-
ties. Using these annotations as a starting point, we
found and annotated 216 cross-lingual entities.11
Because a similar corpus did not exist for develop-
ment, we split the evaluation corpus into development
and test sections. However, the usual method of split-
ting by document would not confine all mentions of
each entity to one side of the split. We thus split the
corpus by global entity id. We assigned one-third of
the entities to development, and the remaining two-
thirds to test.
6 Comparison to Related Tasks and Work
Our modeling techniques and task formulation can be
viewed as cross-lingual extensions to cross-document
coreference resolution. The classic work on this task
was by Bagga and Baldwin (1998b), who adapted
the Vector Space Model (VSM) (Salton et al, 1975).
Gooi and Allan (2004) found effective algorithmic
extensions like agglomerative clustering. Successful
feature extensions to the VSM for cross-document
coreference have included biographical information
(Mann and Yarowsky, 2003) and syntactic context
(Chen and Martin, 2007). However, neither of these
feature sets generalize easily to the cross-lingual set-
ting with multiple entity types. Fleischman and Hovy
(2004) added a discriminative pairwise mention clas-
sifier to a VSM-like model, much as we do. More
11The annotators were the first author and another fluent
speaker of Arabic. The annotations, corrections, and corpus
split are available at http://www.spencegreen.com/research/.
65
recent work has considered new models for web-scale
corpora (Rao et al, 2010; Singh et al, 2011).
Cross-document work on languages other than En-
glish is scarce. Wang (2005) used a combination of
the VSM and heuristic feature selection strategies to
cluster transliterated Chinese personal names. For
Arabic, Magdy et al (2007) started with the output of
the mention detection and within-document corefer-
ence system of Florian et al (2004). They clustered
the entities incrementally using a binary classifier.
Baron and Freedman (2008) used complete-link ag-
glomerative clustering, wheremerging decisions were
based on a variety of features such as document topic
and name uniqueness. Finally, Sayeed et al (2009)
translated Arabic name mentions to English and then
formed clusters greedily using pairwise matching.
To our knowledge, the cross-lingual entity cluster-
ing task is novel. However, there is significant prior
work on similar tasks:
? Multilingual coreference resolution: Adapt
English within-document coreference models to
other languages (Harabagiu andMaiorano, 2000;
Florian et al, 2004; Luo and Zitouni, 2005).
? Named entity translation: For a non-English
document, produce an inventory of entities in
English. An ACE2007 pilot task (Song and
Strassel, 2008).
? Named entity clustering: Assign semantic
types to text mentions (Collins and Singer, 1999;
Elsner et al, 2009).
? Cross-language name search / entity linking:
Match a single query name against a list of
known multilingual names (knowledge base). A
track in the 2011NIST Text Analysis Conference
(TAC-KBP) evaluation (Aktolga et al, 2008;
McCarley, 2009; Udupa and Khapra, 2010; Mc-
Namee et al, 2011).
Our work incorporates elements of the first three tasks.
Most importantly, we avoid the key element of entity
linking: a knowledge base.
7 Experiments
We performed intrinsic evaluations for both mention
and context similarity. For context similarity, we
analyzed mono-lingual entity clustering, which also
facilitated comparison to prior work on the ACE2008
Genre #Train #Test Accuracy(%)
wb 125 16 87.5
bn 2,720 340 95.6
nw 7,443 930 96.6
all 10,288 1,286 97.1 (+7.55)
Table 4: Cross-lingual mention matching accuracy [%].
The training data contains names from three genres: broad-
cast news (bn), newswire (nw), and weblog (wb). We used
the full training corpus (all) for the cross-lingual clustering
experiments, but the model achieved high accuracy with
significantly fewer training examples (e.g., bn).
CEAF? NVI? B3 ?
#hyp P R F1
Mono-lingual Arabic (#gold=1,721)
HAC 87.2 0.052 1,669 89.8 89.8 89.8
Mono-lingual English (#gold=1,529)
HAC 88.5 0.042 1,536 93.7 89.0 91.4
Table 5: Mono-lingual entity clustering evaluation (test
set, gold within-document processing). Higher scores (?)
are better for CEAF and B3, whereas lower (?) is better
for NVI. #gold indicates the number of reference entities,
whereas #hyp is the size of E.
evaluation set. Our main results are for the new task:
cross-lingual entity clustering.
7.1 Intrinsic Evaluations
Cross-lingual Mention Matching We created a
random 80/10/10 (train, development, test) split of
the Maxent training corpus and evaluated binary clas-
sification accuracy (Tbl. 4). Of the mis-classified
examples, we observed three major error types. First,
the model learns that high edit distance is predictive
of a mismatch. However, singleton strings that do not
match often have a lower edit distance than longer
strings that do match. As a result, singletons often
cause false positives. Second, names that originate in
a third language tend to violate the phonemic corre-
spondences. For example, the model gives a false neg-
ative for a German football team: 	?QK???P 	Q
? ?

??
	
?@
(phonetic mapping: af s kazrslawtrn) versus ?FC
Kaiserslautern.? Finally, names that require trans-
lation are problematic. For example, the classifier
produces a false negative for ?God, gd?
?
= ? ?

<?

@, allh?.
66
#gold = 3,057 CEAF? NVI? B3 ? B3target ? (#gold = 146)
#hyp P R F1 #hyp P R F1
Singleton 64.9 0.165 5,453 100.0 56.1 71.8 1,587 100.0 9.20 16.9
No-context 57.4 0.136 2,216 65.6 75.2 70.1 517 78.3 41.8 54.5
HAC+MT 79.8 0.070 2,783 84.4 86.4 85.4 310 91.7 69.1 78.8
DPMM+MT 74.3 0.122 3,649 89.3 64.1 74.6 634 93.3 24.3 38.6
HAC+PLTM 72.1 0.110 2,746 76.9 77.6 77.3 506 84.4 44.6 58.4
DPMM+PLTM 57.2 0.180 2,609 64.0 62.8 63.4 715 73.9 22.2 34.1
Table 6: Cross-lingual entity clustering (test set, gold within-document processing). B3target is the standard B
3 metric
applied to the subset of target cross-lingual entities in the test set. For CEAF and B3, Singleton is the stronger baseline
due to the high proportion of singleton entities in the corpus. Of course, cross-lingual entities have at least two chains,
so No-context is a better baseline for cross-lingual clustering.
Mono-lingual Entity Clustering For comparison,
we also evaluated our system on a standard mono-
lingual cross-document coreference task (Arabic and
English) (Tbl. 5). We configured the system with
HAC clustering and Jaro-Winkler (within-language)
mention similarity. We built mono-lingual ELMs for
context similarity.
We used two baselines:
? Singleton: E = C, i.e., the cross-lingual clus-
tering solution is just the set of mono-lingual
coreference chains. This is a common baseline
for mono-lingual entity clustering (Baron and
Freedman, 2008).
? No-context: We run HAC with ? =?. There-
fore, E is the set of fully-connected components
in C subject to the pairwise constraints.
For HAC, we manually tuned the stop threshold ?,
the Jaro-Winkler threshold ?, and the ELM smoothing
parameter ? on the development set. For the DPMM,
no development tuning was necessary, and we evalu-
ated a single sample of E taken after 3,000 iterations.
To our knowledge, Baron and Freedman (2008)
reported the only previous results on the ACE2008
data set. However, they only gave gold results for
English, and clustered the entire evaluation corpus
(test+development). To control for the effect of
within-document errors, we considered their gold in-
put (mention detection and within-document coref-
erence resolution) results. They reported B3 for the
two entity types separately: ORG (91.5% F1) and
PER (94.3% F1). The different experimental designs
preclude a precise comparison, but the accuracy of
#gold = 3,057 B3sys ?
#hyp P R F1
Singleton 7,655 100.0 57.1 72.7
No-context 2,918 63.3 71.1 67.0
HAC+MT 3,804 75.6 77.8 76.7
DPMM+MT 4,491 77.1 62.5 69.0
HAC+PLTM 6,353 94.1 62.8 75.3
DPMM+PLTM 3,522 64.6 62.0 63.3
Table 7: Cross-lingual entity clustering (test set, automatic
(Serif) within-document processing). For HAC, we used
the same parameters as the gold setting.
the two systems are at least in the same range.
7.2 Cross-lingual Entity Clustering
We evaluated four system configurations on the new
task: HAC+MT, HAC+PLTM, DPMM+MT, and
DPMM+PLTM. First, we established an upper bound
by assuming gold within-document mention detection
and coreference resolution (Tbl. 6). This setting iso-
lated the new cross-lingual clustering methods from
within-document processing errors. Then we evalu-
ated with Serif (automatic) within-document process-
ing (Tbl. 7). This second experiment replicated an
application setting. We used the same baselines and
tuning procedures as in the mono-lingual clustering
experiment.
Results In the gold setting, HAC+MTproduces the
best results, as expected. The dimensionality reduc-
tion of the vocabulary imposed by PLTM significantly
reduces accuracy, but HAC+PLTM still exceeds the
67
baseline. We tried increasing the number of PLTM
topics k, but did not observe an improvement in task
accuracy. For both context-mapping methods, the
DPMM suffers from low-recall. Upon inspection, the
clustering solution of DPMM+MT contains a high
proportion of singleton hypotheses, suggesting that
the model finds lower similarity in the presence of a
larger vocabulary. When the context vocabulary con-
sists of PLTM topics, larger clusters are discovered
(DPMM+PLTM).
The effect of dimensionality reduction is also appar-
ent in the clustering solutions of the PLTM models.
For example, for the Serif output, DPMM+PLTM
produces a cluster consisting of ?White House?, ?Sen-
ate?, ?House of Representatives?, and ?Parliament?.
Arabic mentions of the latter three entities pass the
pairwise mention similarity constraints due to the
word ??m.? ?council?, which appears in text mentions
for all three legislative bodies. A cross-language
matching error resulted in the linking of ?White
House?, and the reduced granularity of the contexts
precluded further disambiguation. Of course, these
entities probably appear in similar contexts.
The caveat with the Serif results in Tbl. 7 is that
3,251 of the 7,655 automatic coreference chains are
not in the reference. Consequently, the evaluation is
dominated by the penalty for spurious system coref-
erence chains. Nonetheless, all models except for
DPMM+PLTM exceed the baselines, and the rela-
tionships between models depicted in the gold exper-
iments hold for the this setting.
8 Conclusion
Cross-lingual entity clustering is a natural step to-
ward more robust natural language understanding.
We proposed pipeline models that make clustering
decisions based on cross-lingual similarity. We inves-
tigated two methods for mapping documents in differ-
ent languages to a common representation: MT and
the PLTM. Although MT may achieve more accurate
results for some language pairs, the PLTM training
resources (e.g., Wikipedia) are readily available for
many languages. As for the clustering algorithms,
HAC appears to perform better than the DPMM on
our dataset, but this may be due to the small corpus
size. The instance-level constraints represent tenden-
cies that could be learned from larger amounts of data.
With more data, we might be able to relax the con-
straints and use an exchangeable DPMM,whichmight
be more effective. Finally, we have shown that sig-
nificant quantities of within-document errors cascade
into the cross-lingual clustering phase. As a result,
we plan a model that clusters the mentions directly,
thus removing the dependence on within-document
coreference resolution.
In this paper, we have set baselines and proposed
models that significantly exceeded those baselines.
The best model improved upon the cross-lingual en-
tity baseline by 24.3% F1. This result was achieved
without a knowledge base, which is required by previ-
ous approaches to cross-lingual entity linking. More
importantly, our techniques can be used to extend
existing cross-document entity clustering systems for
the increasingly multilingual web.
AcknowledgmentsWe thank Jason Eisner, David Mimno,
Scott Miller, Jim Mayfield, and Paul McNamee for helpful
discussions. This work was started during the SCALE
2010 summer workshop at Johns Hopkins. The first author
is supported by a National Science Foundation Graduate
Fellowship.
References
E. Aktolga, M. Cartright, and J. Allan. 2008. Cross-document
cross-lingual coreference retrieval. In CIKM.
G. Andrew and J. Gao. 2007. Scalable training of L1-regularized
log-linear models. In ICML.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The Annals
of Statistics, 2(6):1152?1174.
A. Bagga and B. Baldwin. 1998a. Algorithms for scoring coref-
erence chains. In LREC.
A. Bagga and B. Baldwin. 1998b. Entity-based cross-document
coreferencing using the vector space model. In COLING-ACL.
A. Baron and M. Freedman. 2008. Who is Who and What
is What: Experiments in cross-document co-reference. In
EMNLP.
D. Blei and P. Frazier. 2010. Distance dependent Chinese restau-
rant processes. In ICML.
J. Cai and M. Strube. 2010. Evaluation metrics for end-to-
end coreference resolution systems. In Proceedings of the
SIGDIAL 2010 Conference.
D. Cer, M. Galley, D. Jurafsky, and C. D.Manning. 2010. Phrasal:
A statistical machine translation toolkit for exploring new
model features. In HLT-NAACL, Demonstration Session.
E. Charniak. 2001. Unsupervised learning of name structure
from coreference data. In NAACL.
Y. Chen and J. Martin. 2007. Towards robust unsupervised
personal name disambiguation. In EMNLP-CoNLL.
68
P. Christen. 2006. A comparison of personal name matching:
Techniques and practical issues. Technical Report TR-CS-06-
02, Australian National University.
M. Collins and Y. Singer. 1999. Unsupervised models for named
entity classification. In EMNLP.
G. de Melo and G. Weikum. 2010. Untangling the cross-lingual
link structure of Wikipedia. In ACL.
M. Elsner, E. Charniak, and M. Johnson. 2009. Structured
generative models for unsupervised named-entity clustering.
In HLT-NAACL.
D. M. Endres and J. E. Schindelin. 2003. A new metric for
probability distributions. IEEE Transactions on Information
Theory, 49(7):1858 ? 1860.
M. Fleischman and E. Hovy. 2004. Multi-document person name
resolution. In ACL Workshop on Reference Resolution and its
Applications.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, et al
2004. A statistical model for multilingual entity detection and
tracking. In HLT-NAACL.
A. T. Freeman, S. L. Condon, and C. M. Ackerman. 2006. Cross
linguistic name matching in English and Arabic: a one to
many mapping extension of the Levenshtein edit distance
algorithm. In HLT-NAACL.
M. Galley and C. D. Manning. 2008. A simple and effective
hierarchical phrase reordering model. In EMNLP.
C. H. Gooi and J. Allan. 2004. Cross-document coreference on
a large scale corpus. In HLT-NAACL.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-of-
speech tagging and morphological disambiguation in one fell
swoop. In ACL.
S. M. Harabagiu and S. J. Maiorano. 2000. Multilingual corefer-
ence resolution. In ANLP.
A. Irvine, C. Callison-Burch, and A. Klementiev. 2010. Translit-
erating from all languages. In AMTA.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
D. Klein, S. D. Kamvar, and C. D.Manning. 2002. From instance-
level constraints to space-level constraints: Making the most
of prior knowledge in data clustering. In ICML.
K. Knight and J. Graehl. 1998. Machine transliteration. Compu-
tational Linguistics, 24:599?612.
X. Luo and I. Zitouni. 2005. Multi-lingual coreference resolution
with syntactic features. In HLT-EMNLP.
X. Luo. 2005. On coreference resolution performance metrics.
In HLT-EMNLP.
W. Magdy, K. Darwish, O. Emam, and H. Hassan. 2007. Arabic
cross-document person name normalization. In Workshop on
Computational Approaches to Semitic Languages.
G. S. Mann and D. Yarowsky. 2003. Unsupervised personal
name disambiguation. In NAACL.
C. D. Manning, P. Raghavan, and H. Sch?tze. 2008. Introduction
to Information Retrieval. Cambridge University Press.
J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed, et al
2009. Cross-document coreference resolution: A key technol-
ogy for learning by reading. In AAAI Spring Symposium on
Learning by Reading and Learning to Read.
A. K. McCallum. 2002. MALLET: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
J. S. McCarley. 2009. Cross language name matching. In SIGIR.
P. McNamee, J. Mayfield, D. Lawrie, D.W. Oard, and D. Doer-
mann. 2011. Cross-language entity linking. In IJCNLP.
D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and
A. McCallum. 2009. Polylingual topic models. In EMNLP.
NIST. 2008. Automatic Content Extraction 2008 evaluation
plan (ACE2008): Assessment of detection and recognition
of entities and relations within and across documents. Tech-
nical Report rev. 1.2d, National Institute of Standards and
Technology (NIST), 8 August.
E. H. Porter and W. E. Winkler, 1997. Approximate String Com-
parison and its Effect on an Advanced Record Linkage System,
chapter 6, pages 190?199. U.S. Bureau of the Census.
H. Raghavan, J. Allan, and A. McCallum. 2004. An explo-
ration of entity models, collective classification and relation
description. In KDD Workshop on Link Analysis and Group
Detection.
L. Ramshaw, E. Boschee, M. Freedman, J. MacBride,
R. Weischedel, and A. Zamanian. 2011. SERIF language
processing?effective trainable language understanding. In
J. Olive et al, editors,Handbook of Natural Language Process-
ing and Machine Translation: DARPA Global Autonomous
Language Exploitation, pages 636?644. Springer.
D. Rao, P. McNamee, and M. Dredze. 2010. Streaming cross
document entity coreference resolution. In COLING.
R. Reichart and A. Rappoport. 2009. The NVI clustering evalu-
ation measure. In CoNLL.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model
for automatic indexing. CACM, 18:613?620, November.
A. Sayeed, T. Elsayed, N. Garera, D. Alexander, T. Xu, et al
2009. Arabic cross-document coreference detection. In ACL-
IJCNLP, Short Papers.
S. Singh, A. Subramanya, F. Pereira, and A. McCallum. 2011.
Large-scale cross-document coreference using distributed in-
ference and hierarchical models. In ACL.
Z. Song and S. Strassel. 2008. Entity translation and alignment
in the ACE-07 ET task. In LREC.
S. Strassel, M. Przybocki, K. Peterson, Z. Song, and K. Maeda.
2008. Linguistic resources and evaluation techniques for
evaluation of cross-document automatic content extraction.
In LREC.
R. Udupa and M. M. Khapra. 2010. Improving the multilin-
gual user experience of Wikipedia using cross-language name
search. In HLT-NAACL.
A. Vlachos, A. Korhonen, and Z. Ghahramani. 2009. Unsuper-
vised and constrained Dirichlet process mixture models for
verb clustering. In Proc. of the Workshop on Geometrical
Models of Natural Language Semantics.
H. Wang. 2005. Cross-document transliterated personal name
coreference resolution. In L. Wang and Y. Jin, editors, Fuzzy
Systems and Knowledge Discovery, volume 3614 of Lecture
Notes in Computer Science, pages 11?20. Springer.
M. West. 1995. Hyperparameter estimation in Dirichlet process
mixture models. Technical report, Duke University.
69
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 783?792,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Shared Components Topic Models
Matthew R. Gormley Mark Dredze Benjamin Van Durme Jason Eisner
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Department of Computer Science
Johns Hopkins University, Baltimore, MD
{mrg,mdredze,vandurme,jason}@cs.jhu.edu
Abstract
With a few exceptions, extensions to latent
Dirichlet alocation (LDA) have focused on
the distribution over topics for each document.
Much less attention has been given to the un-
derlying structure of the topics themselves. As
a result, most topic models generate topics in-
dependently from a single underlying distri-
bution and require millions of parameters, in
the form of multinomial distributions over the
vocabulary. In this paper, we introduce the
Shared Components Topic Model (SCTM), in
which each topic is a normalized product of a
smaller number of underlying component dis-
tributions. Our model learns these component
distributions and the structure of how to com-
bine subsets of them into topics. The SCTM
can represent topics in a much more compact
representation than LDA and achieves better
perplexity with fewer parameters.
1 Introduction
Topic models are probabilistic graphical models
meant to capture the semantic associations underly-
ing corpora. Since the introduction of latent Dirich-
let alocation (LDA) (Blei et al, 2003), these mod-
els have been extended to account for more complex
distributions over topics, such as adding supervision
(Blei and McAuliffe, 2007), non-parametric priors
(Blei et al, 2004; Teh et al, 2006), topic correla-
tions (Li and McCallum, 2006; Mimno et al, 2007;
Blei and Lafferty, 2006) and sparsity (Williamson et
al., 2010; Eisenstein et al, 2011).
While much research has focused on modeling
distributions over topics, less focus has been given to
the makeup of the topics themselves. This emphasis
leads us to find two problems with LDA and its vari-
ants mentioned above: (1) independently generated
topics and (2) overparameterized models.
Independent Topics In the models above, the top-
ics are modeled as independent draws from a single
underlying distribution, typically a Dirichlet. This
violates the topic modeling community?s intuition
that these distributions over words are often related.
As an example, consider a corpus that supports two
related topics, baseball and hockey. These topics
likely overlap in their allocation of mass to high
probability words (e.g. team, season, game, play-
ers), even though the two topics are unlikely to ap-
pear in the same documents. When topics are gen-
erated independently, the model does not provide a
way to capture this sharing between related topics.
Many extensions to LDA have addressed a related
issue, LDA?s inability to model topic correlation,1
by changing the distributions over topics (Blei and
Lafferty, 2006; Li and McCallum, 2006; Mimno et
al., 2007; Paisley et al, 2011). Yet, none of these
change the underlying structure of the topic?s distri-
butions over words.
Overparameterization Topics are most often
parameterized as multinomial distributions over
words: increasing the topics means learning new
multinomials over large vocabularies, resulting in
models consisting of millions of parameters. This
issue was partially addressed in SAGE (Eisenstein
et al, 2011) by encouraging sparsity in the topics
which are parameterized by their difference in log-
frequencies from a fixed background distribution.
Yet the problem of overparameterization is also tied
1Two correlated topics, e.g. nutrition and exercise, are likely
to co-occur, but their word distributions might not overlap.
783
to the number of topics, and though SAGE reduces
the number of non-zero parameters, it still requires
a vocabulary-sized parameter vector for each topic.
We present the Shared Components Topic Model
(SCTM), which addresses both of these issues by
generating each topic as a normalized product of a
smaller number of underlying components. Rather
than learning each new topic from scratch, we model
a set of underlying component distributions that
constrain topic formation. Each topic can then be
viewed as a combination of these underlying com-
ponents, where in a model such as LDA, we would
say that components and topics stand in a one to one
relationship. The key advantages of the SCTM are
that it can learn and share structure between overlap-
ping topics (e.g. baseball and hockey) and that it can
represent the same number of topics in a much more
compact representation, with far fewer parameters.
Because the topics are products of components,
we present a new training algorithm for the sig-
nificantly more complex product case which re-
lies on a Contrastive Divergence (CD) objective.
Since SCTM topics, which are products of distri-
butions, could be represented directly by distribu-
tions as in LDA, our goal is not necessarily to learn
better topics, but to learn models that are substan-
tially smaller in size and generalize better to unseen
data. Experiments on two corpora show that our
model uses fewer underlying multinomials and still
achieves lower perplexity than LDA, which suggests
that these constraints could lead to better topics.
2 Shared Components Topic Models
The Shared Components Topic Model (SCTM) fol-
lows previous topic models in inducing admixture
distributions of topics that are used to generate each
document. However, here each topic multinomial
distribution over words itself results from a normal-
ized product of shared components, each a multino-
mial over words. Each topic selects a subset of com-
ponents. We begin with a review and then introduce
the SCTM.
Latent Dirichlet alocation (LDA) (Blei et al,
2003) is a probabilistic topic model which defines
a generative process whereby sets of observations
are generated from latent topic distributions. In the
SCTM, we use the same generative process of topic
assignments as LDA, but replace the K indepen-
dently generated topics (multinomials over words)
with products of C components.
Latent Dirichlet alocation generative process
For each topic k ? {1, . . . ,K}:
?k ? Dir(?) [draw distribution over words]
For each document m ? {1, . . . ,M}:
?m ? Dir(?) [draw distribution over topics]
For each word n ? {1, . . . , Nm}:
zmn ? Mult(1,?m) [draw topic]
xmn ? ?zmi [draw word]
LDA draws each topic ?k independently from a
Dirichlet. The model generates each document m
of length M , by first sampling a distribution over
topics ?m. Then, for each word n, a topic zmn is
chosen and a word type xmn is generated from that
topic?s distribution over words ?zmi .
A Product of Experts (PoE) model (Hinton,
1999) is the normalized product of the expert dis-
tributions. In the SCTM, each component (an ex-
pert) models an underlying multinomial word dis-
tribution. We let ?c be the parameters of the cth
component, where ?cv is the probability of the cth
component generating word v. If the structure of a
PoE included only components c ? C in the prod-
uct, it would have the form: p(x|?1, . . . ,?C) =Q
c?C ?cx
PV
v=1
Q
c?C ?cv
, where there are C components, and
the summation in the denominator is over the vocab-
ulary. In a PoE, each component can overrule the
others by giving low probability to some word. A
PoE can be viewed as a soft intersection of its com-
ponents, whereas a mixture is a soft union.
The Beta-Bernoulli model (Griffiths and
Ghahramani, 2006) is a distribution over binary
matrices with a fixed number of rows and columns.
It is the finite counterpart to the Indian Buffet
Process. In this work, we use the Beta-Bernoulli as
our prior for an unobserved binary matrix B with C
columns and K rows. In the SCTM, each row bk of
the matrix, a binary feature vector, defines a topic
distribution. The binary vector acts as a selector
for the structure of the PoE for that topic. The row
determines which components to include in the
product by which entries bkc are ?on? (equal to 1)
in that row. Under Beta-Bernoulli prior, for each
column, a coin with weight pic is chosen. For each
entry in the column, the coin is flipped to determine
if the entry is ?on? or ?off?. This corresponds to
784
the notion that some components are a priori more
likely to be included in topics.
The Beta-Bernoulli model generative process
For each component c ? {1, . . . , C}: [columns]
pic ? Beta(
?
C , 1) [draw probability of component c]
For each topic k ? {1, . . . ,K}: [rows]
bkc ? Bernoulli(pic) [draw whether topic includes cth
component in its PoE]
2.1 Shared Components Topic Models
The Shared Components Topic Model generates
each document just like LDA, the only difference
is the topics are not drawn independently from a
Dirichlet prior. Instead, topics are soft intersections
of underlying components, each of which is a multi-
nomial distribution over words. These components
are combined via a PoE model, and each topic is
constructed according to a length C binary vector
bk; where bkc = 1 includes and bkc = 0 excludes
component c. Stacking theK vectors forms aK?C
matrix; rows correspond to topics and columns to
components. Overlapping topics share components
in common.
Generative process SCTM?s generative process
generates topics and words, but must also generate
the binary matrix. For each of the C shared com-
ponents, we generate a distribution ?c over the V
words from a Dirichlet parametrized by ?. Next,
we generate a K ? C binary matrix using the Beta-
Bernoulli prior. These components and the binary
matrix implicitly define the complete set of K topic
distributions, each of which is a PoE.
p(x|bk,?) =
?C
c=1 ?
bkc
cx
?V
v=1
?C
c=1 ?
bkc
cv
(1)
The distribution p(?|bk,?) defines the kth topic.
Conditioned on these K topics, the remainder of the
generative process, which generates the documents,
is just like LDA.
The Shared Components Topic Model generative process
For each component c ? {1, . . . , C}:
?c ? Dir(?) [draw distribution over words]
pic ? Beta(
?
C , 1) [draw probability of component c]
For each topic k ? {1, . . . ,K}:
bkc ? Bernoulli(pic) [draw whether topic includes cth
component in its PoE]
For each document m ? {1, . . . ,M}
?m ? Dir(?) [draw distribution over topics]
For each word n ? {1, . . . , Nm}
zmn ? Mult(1,?m) [draw topic]
xmn ? p(? |bzmn ,?) given by Eq. (1) [draw word]
See Figure 1 for the graphical model.
Discussion An advantage of this formulation is the
ability to model many topics using few components.
While LDA must maintain V ?K parameters for the
topic distributions, the SCTM maintains just V ?C
parameters, plus an additional K?C binary matrix.
Since C < K  V this results in many fewer pa-
rameters for the SCTM.2 Extending the number of
topics (rows) requires storing additional binary vec-
tors, a lightweight requirement. In theory, we could
enable all 2C possible component combinations, al-
though we expect to use far less. On the other hand,
constraining the SCTM?s topics by the components
gives less flexible topics as compared to LDA. How-
ever, we find empirically that a large number of top-
ics can be effectively modeled with a smaller num-
ber of components.
Observe that we can reparameterize the SCTM as
LDA by assuming an identity square matrix; each
component corresponds to a topic in LDA, making
LDA a special case of the SCTM with an identity
matrix IC . Intuitively, SCTM learning could pro-
duce an LDA model where appropriate. Finally, we
can also think of the SCTM as learning the struc-
ture of many PoE models. In applications where ex-
perts abstain, the SCTM could learn in which setting
(row) each expert casts a vote.
3 Parameter Estimation
Parameter estimation infers values for model pa-
rameters ?, pi, and ? from data using an unsuper-
vised training procedure. Because exact inference
is intractable in the SCTM, we turn to approximate
methods. As is common in these models, we will
integrate out pi and ?, sample latent variables Z and
B, and optimize the components ?. Our algorithm
follows the outline of the Monte Carlo EM (MCEM)
algorithm (Wei and Tanner, 1990). In the Monte
Carlo E-step, we will re-sample the latent variables
Z and B based on current model parameters ? and
observed data X . In the M-step, we will find new
model parameters ?. Since these parameters corre-
spond to experts in the PoE, we rely on a contrastive
divergence (CD) objective (Hinton, 2002), popular
for PoE training, rather than maximizing the data
2The vocabulary size V could be much larger if n-grams or
relational triples are used, as opposed to unigrams.
785
log-likelihood. Normally, CD only estimates the pa-
rameters of the expert distributions. However, in our
model, the structure of the PoEs themselves change
based on the E-step. Since we generate multiple
samples in the E-step, we modify the CD objective
to compute the gradient for each E-step sample and
take the average to approximate the expectation un-
der B and Z.3
3.1 E-Step
The E-step approximates an expectation under
p(B,Z|X,?,?, ?) for latent topic assignments Z
and matrix B using Gibbs sampling. The Gibbs
sampler uses the full conditionals for both zi (7) and
bkc (12), which we derive in Appendix A. Using this
sampler, we obtain J samples of Z and B by iterat-
ing through each value of zi and bkc J times (in our
experiments, we use J=1, which appears to work as
well on this task as multiple samples). These J sam-
ples are then used in the M-step as an approximation
of the expectation of the latent variables.
3.2 M-Step
Given many samples of B and Z, the M-step opti-
mizes the component parameters ? which cannot be
collapsed out. We utilize the standard PoE training
procedure for experts: contrastive divergence (CD).
We approximate the CD gradient as the difference of
the data distribution and the one-step reconstruction
of the data according to the current parameters. As
in Generalized EM (Dempster et al, 1977), a single
gradient step in the direction of the contrastive di-
vergence objective is sufficient for each M-step. A
key difference in our model is that we must incor-
porate the expectation of the PoE model structure,
which in our case is a random variable instead of a
fixed observed structure. We achieve this by simply
3CD training within MCEM is not the only possible ap-
proach. One alternative would be to compute the CD gradient
summing over all values of B and Z, effectively training the
entire model using CD. This approach prevents the normal CD
objective derivation from being simplified into a more tractable
form. Another approach would be a pure MCMC algorithm,
which sampled ? directly. While using the natural parameters
allows the sampler to mix, it is too computationally intensive to
be practical. Finally, we could train with Generalized MCEM,
where the exact gradient of the log-likelihood (or log-posterior)
is used, but this easily gets stuck in local minima. After exper-
imenting with these and other options, we present our current
most effective estimation method.
computing the CD gradient for each PoE given each
of the J samples {Z,B}(j) from the E-Step, then
average the result.
Another difficulty arises from computing the gra-
dient directly for the multinomial?c due to the V ?1
degrees of freedom imposed by sum-to-one con-
straints. Therefore, we switch to the natural pa-
rameters, which obviates the need for considering
the sum-to-one constraint in the optimization, by
defining ?c in terms of V real valued parameters
{?c1, . . . , ?cV }:
?cv =
exp(?cv)
?V
t=1 exp(?cv)
(2)
The V parameters ?cv are then used to compute ?cv
for use in the E-step.
As explained above, the M-step does not maxi-
mize the data log-likelihood, but instead minimizes
contrastive divergence. Hinton (2002) explains that
maximizing data log-likelihood is equivalent to min-
imizing Q0||Q?? , the KL divergence between the
observed data distribution, Q0, and the model?s
equilibrium distribution,Q?? .
4 MinimizingQ0||Q??
would require the computation of an intractable ex-
pectation under the equilibrium distribution. We
avoid this by instead minimizing the contrastive di-
vergence objective,
CD(?|{Z,B}(j)) = Q0||Q?? ?Q
1
? ||Q
?
? , (3)
where Q1? is the distribution over one-step recon-
structions of the data, X given Z,B, ?, that are gen-
erated by a single step of Gibbs sampling.
Unlike standard applications of CD training, the
hidden variables (Z,B) are not contained within the
experts. Instead they define the structure of the PoE
model, where B indicates which experts to use in
each product (topic) andZ indicates which PoE gen-
erates each word. Unfortunately, CD training cannot
infer this structure since the CD derivation makes
use of a fixed structure in the one-step reconstruc-
tion. Therefore, we have taken a MCEM approach,
first sampling the PoE structure in the E-step, then
4Hinton (2002) used this notation because the data distribu-
tion,Q0, can be described as the state of a Markov chain at time
0 that was started at the data distribution. Similarly, the equilib-
rium distribution, Q?? could be obtained by running the same
Markov chain to time?.
786
M
Nm
C
K
xmn
zmn
?m
?
?c
bkc
pic
?
?
Figure 1: The graphical model for the SCTM.
fixing these samples for Z and B when computing
the one-step reconstruction of the data, X .
Contrastive Divergence Gradient We provide
the approximate derivative of the contrastive di-
vergence objective, where Z and B are treated as
fixed.5
dCD(?|{Z,B}(j))
d?
? ?
?
d log f(x|bz, ?)
d?
?
Q0
+
?
d log f(x|bz, ?)
d?
?
Q1?
where f(x|bz, ?) =
?C
c=1 ?
bzc
cx is the numerator of
p(x|bz, ?) and the derivative of its log is efficient to
compute:
d log f(x|bz, ?)
d?cv
=
{
bzc(1? ?cv) for x = v
?bzc?cv for x 6= v
To approximate the expectation under Q1? , we hold
Z,B, ? fixed and resample the data, X , using one
step of Gibbs sampling.
3.3 Summary
Our learning algorithm can be viewed
in terms of a Q function: Q(?|?(t)) ?
1
J
?J
j=1 CD(?|{Z,B}
(j))where we average over
J samples. The E-step computes Q(?|?(t)). The
M-step minimizes Q with respect to ? to obtain the
updated ?(t+1) by performing gradient descent on
the Q function as ?(t+1)cv = ?
(t)
cv ? ? ?
dQ(?|?(t))
d?cv
for
all values of c, v.
5The derivative is approximate because we drop the term:
?
dQ1?
d? ?
dQ1?||Q
?
?
dQ1?
, which is ?problematic to compute? (Hinton,
2002). This is the standard use of CD.
Algorithm 1 SCTM Training
Initialize parameters: ?c, bkc, zi.
while not converged do
{E-step:}
for j = 1 to J do
{Draw jth sample {Z,B}(j)}
for i = 1 to N do
Sample zi using Eq. (7)
for k = 1 to K do
for c = 1 to C do
Sample bkc using ratio in Eq. (12)
{M-step:}
for c = 1 to C do
for v = 1 to V do
Single gradient step over ?
?(t+1)cv = ?
(t)
cv ? ? ?
dQ(?|?(t))
d?cv
4 Related Models
The SCTM is closely related to the the Infinite
Overlapping Mixture Model (IOMM) (Heller and
Ghahramani, 2007), yet our model differs from and,
in some ways, extends theirs. The IOMM mod-
els the geometric overlap of Gaussian clusters us-
ing PoEs, and models the structure of the PoEs with
the rows of a binary matrix. The SCTM models a
finite number of columns, where the IOMM mod-
els an infinite number. The IOMM generates a row
for each data point, whereas the SCTM generates a
row for each topic. Thus, the SCTM goes beyond
the IOMM by allowing the rows to be shared among
documents and models document-specific mixtures
over the rows of the matrix.6
SAGE for topic modeling (Eisenstein et al, 2011)
can be viewed as a restricted form of the SCTM.
Consider an SCTM in which the binary matrix is re-
stricted such that the first column, b?,1, consists of
all ones and the remainder forms a diagonal matrix.
If we then set the first component, ?1, to the cor-
pus background distribution, and add a Laplace prior
on the natural parameters, ?cv, we have the SAGE
model. Note that by removing the restriction that
the matrix contain a diagonal, we could allow mul-
tiple components to combine in the SCTM fashion,
while incorporating SAGE?s sparsity benefits.
6The IOMM uses Metropolis-Hastings (MH) to sample the
parameters of the experts. This approach is computationally
feasible because their experts are Gaussian, unlike the SCTM
in which the experts are multinomials and the MH step too ex-
pensive.
787
The relation of TagLDA (Zhu et al, 2006) to
the SCTM is similar to that of SAGE and SCTM.
TagLDA has a PoE of exactly two experts: one ex-
pert for the topic, and one for the supervised word-
level tag. Examples of tags are abstract or body,
indicating which part of a research paper the word
appears in.
Unlike the SCTM and SAGE, most prior exten-
sions to LDA have enhanced the distribution over
topics for each document. One of the closest is hier-
archical LDA (hLDA) (Blei et al, 2004) and its ap-
plication to PAM (Mimno et al, 2007). Though top-
ics are still generated independently from a Dirich-
let prior, hLDA learns a tree structure underlying
the topics. Each document samples a single path
through the tree and samples words from topics
along that path. The SCTM models an orthogonal
issue to topic hierarchy: how the topics themselves
are represented as the intersection of components.
Finally, while prior work has primarily used mix-
tures for the sake of conjugacy, we take a fundamen-
tally different approach to modeling the structure by
using normalized product distributions.
5 Evaluation
We compare the SCTM with LDA in terms of over-
all model performance (held-out perplexity) as well
as parameter usage (varying numbers of components
and topics). We select LDA as our baseline since our
model differs only in how it forms topics, which fo-
cuses evaluation on the benefit of this model change.
We consider two popular data sets for compar-
ison: NIPS: A collection of 1,617 NIPS abstracts
from 1987 to 19997, with 77,952 tokens and 1,632
types. 20NEWS: 1,000 randomly selected articles
from the 20 Newsgroups dataset,8 with 70,011 to-
kens and 1,722 types. Both data sets excluded stop
words and words occurring in fewer than 10 docu-
ments. For 20NEWS, we used the standard by-date
train/test split. For NIPS, we randomly partitioned
the data by document into 75% train and 25% test.
We compare the SCTM to LDA by evaluating
the average perplexity-per-word of the held-out test
7We follow prior work (Blei et al, 2004; Li and Mc-
Callum, 2006; Li et al, 2007) in using only the abstracts:
http://www.cs.nyu.edu/?roweis/data.html
8Williamson et al (2010) created a similar subset:
http://people.csail.mit.edu/jrennie/20Newsgroups/
data, perplexity = 2? log2(data|model)/N . Exact com-
putation is intractable, so we use the left-to-right al-
gorithm (Wallach et al, 2009) as an accurate alter-
native. With the topics fixed, the SCTM is equiva-
lent to LDA and requires no adaptation of the left-
to-right algorithm.
We used a collapsed Gibbs sampler for training
LDA and the algorithm described above for training
the SCTM. Both were trained for 4000 iterations,
sampling topics every 10 iterations after a burn-in of
3000. The hyperparameter ? was optimized as an
asymmetric Dirichlet, ? as a symmetric Dirichlet,
and ? = 3.0 was fixed.9 Following the observation of
Hinton (2002) that CD training benefits from initial-
izing the experts to nearly uniform distributions, we
initialize the component distributions from a sym-
metric Dirichlet with parameter ?? = 1?106. We use
J = 1 samples per iteration and a decaying learning
rate centered at ? = 100.10 We ranged LDA from 10
to 200 topics, and the SCTM from 10 to 100 com-
ponents (C). We then selected the number of SCTM
topics (K) as K ? {C, 2C, 3C, 4C, 5C}. For each
model, we used five random restarts, selecting the
model with the highest training data likelihood.
5.1 Results
Our goal is to demonstrate that (1) modeling topics
as products of components is an expressive alterna-
tive to generating topics independently and (2) the
SCTM can both achieve lower perplexity than LDA
and use fewer model parameters in doing so.
Topics as Products of Components Figures 3b
and 3c show the perplexity for the held-out portions
of 20NEWS and NIPS for different numbers of com-
ponents C. The shaded region shows the full SCTM
perplexity range we observed for different K and
at each value of C, we label the number of topics
K (rows in the binary matrix). For each number of
components, LDA falls within the upper portion of
the shaded region. While for some (small) values of
K for the SCTM, LDA does better, the SCTM can
easily include more K (requiring few new param-
eters) to achieve better results. This supports our
hypothesis that topics can be comprised of the over-
lap between shared underlying components. More-
9On development data the model was rather insensitive to ?.
10We experimented with larger J but it had no effect.
788
Figure 2: SCTM binary matrix and topics from 3599 training documents of 20NEWS for C = 10, K = 20. Blue
squares are ?on? (equal to 1).
x
y
5
10
15
20
2 4 6 8 10
k ?k Top words for topic Top words for topic after ablating component c=1
? 1 0.306 subject organization israel return define law org organization subject israel law peace define israeli
? 2 0.031 encryption chip clipper keys des escrow security law administration president year market money senior
? 3 0.025 turkish armenian armenians war turkey turks armenia years food center year air russian war army
? 4 0.102 drive card disk scsi hard controller mac drives opinions drive hard power support cost research price
? 5 0.071 image jpeg window display code gif color mit pitt file program year center programs image division
? 6 0.018 jews israeli jewish arab peace land war arabs
? 7 0.074 org money back question years thing things point
? 8 0.106 christian bible church question christ christians life
? 9 0.011 administration president year market money senior
? 10 0.055 health medical center research information april
? 11 0.063 gun law state guns control bill rights states
? 12 0.160 world organization system israel state usa cwru reply
? 13 0.042 space nasa gov launch power wire ground air
? 14 0.038 space nasa gov launch power wire ground air
? 15 0.079 team game year play games season players hockey
? 16 0.158 car lines dod bike good uiuc sun cars
? 17 0.136 windows file government key jesus system program
? 18 0.122 article writes center page harvard virginia research
? 19 0.017 max output access digex int entry col line
? 20 0.380 lines people don university posting host nntp time # of Model Parameters (thousands)
Perp
lexi
ty
800
1000
1200
1400
l
l
l
l
l
l
l
l
l
10
100
11
120 140
201
40
60 80
10,2010,30
10,4010,50
100,200
100,300
100,400100,500
20,100
20,40
20,60
20,80
40,120
40,16040,200
40,80
60,120
60,180
60,24060,300
80,160
80,240
80,32080,400
0 100 200 300 400 500 600
l LDASCTM
(a)
# of Components
Perp
lexi
ty
800
1000
1200
1400
1600
1800
l
l
l
ll
l
l
l
10
2030
4050
100
200
300400500
100
20
40
6080
120
160200
40
80
120
180240300
60
160
240320400
80
0 20 40 60 80 100
l LDASCTM
(b)
# of Components
Perp
lexi
ty
300
400
500
600
700
l
l
l
ll
l
l
l
10
20
304050
100
2003400500
100
20
40
60
80
120160200
40
80
120
180240300
60
160240320400
80
0 20 40 60 80 100
l LDASCTM
(c)
# of Model Parameters (thousands)
Perp
lexit
y
300
350
400
450
500
550
600
l
l
l
l l l l
l
l
l
l
l
l
10
100
11
120 140 160 180
20
200
21
40
60
80
10,20
10,3010,4010,50
100,200100,300100,400100,500
20,100
20,40
20,60
20,80
40,120
40,16040,200
40,80
60,120
60,18060,24060,3 0
80,16080,240
80,32080,400
0 100 200 300 400
l LDASCTM
(d)
Figure 3: Perplexity results on held-out data for 20NEWS (b) and NIPS (c) showing the results of LDA and the SCTM
for the same number of components and varying K (SCTM). For the same number of components (multinomials), the
SCTM achieves lower perplexity by combining them into more topics. Results for 20NEWS (a) and NIPS (d) showing
non-square SCTM achieves lower perplexity than LDA with a more compact model.
over, this suggests that our products (PoEs) provide
additional and complementary expressivity over just
mixtures of topics.
Model Compactness Including an additional
topic in the SCTM only adds C binary parameters,
for an extra row in the matrix. Whereas in LDA, an
additional topic requires V (the size of the vocab-
ulary) additional parameters to represent the multi-
nomial. In both cases, the number of document-
specific parameters must increase as well. Figures
3a and 3d present held-out perplexity vs. number
of model parameters on 20NEWS and NIPS, exclud-
ing the case of square (C = K) binary matrices for
the SCTM. The regions show a confidence inter-
val (p = 0.05) around the smoothed fit to the data,
LDA labels show C, and SCTM labels show C,K.
The SCTM achieves lower perplexity with fewer
model parameters, even when the increase in non-
component parameters is taken into account. We ex-
pect that because of its smaller size the SCTM ex-
hibits lower sample complexity, allowing for better
generalization to unseen data.
5.2 Analysis
Figure 2 gives the binary matrix and topics learned
on a larger section of 20NEWS training documents.
These topics evidence that the SCTM is able to
achieve a diversity of topics by combining various
subsets of components, and we expect that the low
perplexity achieved by the SCTM can be attributed
789
k=12 ?k=0.13
problem statecontrolreinforcementproblems modelstime baseddecision markovsystems function
k=11 ?k=0.08
learningnetworks systemrecognition timenetworkdescribes handcontext viewsclassification
k=14 ?k=0.07
models imagesimage problemstructureanalysis mixtureclusteringapproach showcomputational
k=13 ?k=0.05
networksnetwork learningdistributedsystem weightvectors propertybinary pointoptimal real
k=16 ?k=0.11
training unitspaper hiddennumber outputproblem rule setorder unit showpresent methodweights task
k=15 ?k=0.12
cells neuronsvisual cortexmotion responseprocessingspatial cellpropertiespatterns spike
k=18 ?k=0.07
informationanalysiscomponent rulessignalindependentrepresentationsnoise basis
k=17 ?k=0.10
numberfunctionsweights functionlayergeneralizationerror resultsloss linear size
k=20 ?k=0.02
time networkweightsactivation delaycurrent chaoticconnecteddiscreteconnections
k=19 ?k=0.03
system networksset neuronsvisual phasefeatureprocessingfeatures outputassociative
c=1
modelinformationparameterskalman robustmatriceslikelihoodexperimentally
c=2
networknetworks datalearning optimallinear vectorindependentbinary naturalalgorithms pca
c=4
paper unitsoutput layernetworkspatterns unitpattern set rulenetwork rulesweights training
c=9
visual imageimages cellscortex scenesupport spatialfeature visioncues stimulusstatistics
k=10 ?k=0.09
neural neuronsanalog synapticneuron networksmemory timecapacity modelassociativenoise dynamics
k=9 ?k=0.02
vector featureclassificationsupport vectorskernelregressionweight inputsdimensionality
k=2 ?k=0.13
network inputinformation timerecurrent backpropagationunitsarchitectureforward layer
k=1 ?k=0.11
model learningsysteminformationparametersnetworks robustkalman rulesestimation
k=4 ?k=0.12
bayesianresults showestimationmethod basedparameterslikelihoodmethods models
k=3 ?k=0.06
objectrecognitionsystem objectsinformationvisual matchingproblem basedclassification
k=6 ?k=0.23
neural networkpaperrecognitionspeech systemsbased resultsperformanceartificial
k=5 ?k=0.04
objectrecognitionsystem objectsinformationvisual matchingproblem basedclassification
k=8 ?k=0.23
algorithmtraining errorfunction methodperformanceinputclassificationclassifier
k=7 ?k=0.08
data papernetworks networkoutput featurefeaturespatterns settrain introducedunit functions
Figure 4: Hasse diagram on NIPS for C = 10, K = 20 showing the top words for topics and unrepresented com-
ponents (in shaded box). Notice that some topics only consist of a single component. The shaded box contains the
components that didn?t appear as a topic. For the sake of clarity, we only show arrows for the subsumption rela-
tionships between the topics, and we omit the implicit arrows between the components in the shaded box and the
topics.
to the high-level of component re-use across topics.
Topics are typically interpreted by looking at the
top-N words, whereas the top-N words of a compo-
nent often do not even appear in the topics to which
it contributes. Instead, we find that the components
contribution to a topic is typically through vetoing
words. For example, the top words of component
c=1, corresponding to the first column of the binary
matrix in figure 2, are [subject organization posting apple mit
screen write window video port], yet only a few of these ap-
pear in topics k=1,2,3,4,5, which use it.
On the right of figure 2, we show what the top-
ics become when we ablate component c=1 from
the matrix by setting the column to all zeros. Topic
k=2 changes from being about information security
to general politics and is identical to k=9. Topic k=3
changes from the Turkish-Armenian War to a more
general war topic. Topic k=4 changes to a less fo-
cused version of itself. In this way, we can gain fur-
ther insight into the contribution of this component,
and the way in which components tend to increase
the specificity of a topic to which they are added.
The SCTM learns each topic as a soft intersec-
tion of its components, as represented by the binary
matrix. We can describe the overlap between topics
based on the components that they have in common.
One topic subsumes another topic when the parent
consists of a subset of the child?s components. In
this way, the binary matrix defines a Hasse diagram,
a directed acyclic graph describing all the subsump-
tion relationships between topics. Figure 4 shows
such a Hasse diagram on the NIPS data. Several top-
ics consist of only a single component, such as k=12
on reinforcement learning and k=8 on optimization.
These two topics combine with the component c=1
so that their overlap forms the topic k=4 on Bayesian
methods. These subsumption relationships are dif-
ferent from and complementary to hLDA (see ?4),
which models topic co-occurrence, not component
intersection. For example, topic k=10 on connec-
tionism and k=2 on neural networks intersect to
form k=20 which contains words that would only
appear in both of its subsuming topics, thereby ex-
plicitly modeling topic overlap.
790
The SCTM sometimes learns identical topics (two
rows with the same binary entries ?on?) such as
k=13 and k=14 in figure 2 and k=3 and k=5 in fig-
ure 4, which is likely due to the Gibbs sampler for
the binary matrix getting stuck in a local optimum.
6 Discussion
We have presented the Shared Components Topic
Model (SCTM), in which topics are products of
underlying component distributions. This model
change learns shared topic structures?as expressed
through components?as opposed to generating
each topic independently. Reducing the number of
components yields more compact models with lower
perplexity than LDA. The two main limitations of
the current SCTM are, when restricted to a square
binary matrix (C = K), the inference procedure is
unable to recover a model with perplexity as low as
a collapsed Gibbs sampler for LDA, and the compo-
nents are not consistently interpretable.
The use of components opens up interesting di-
rections of research. For example, task specific side
information can be expressed as priors or constraints
over the components, or by adding conditioning
variables tied to the components. Additionally, tasks
beyond document modeling may benefit from repre-
senting topics as products of distributions. For ex-
ample, in vision, where topics are classes of objects,
the components could be features of those objects.
For selectional preference, components could cor-
respond to semantic features that intersect to define
semantic classes (Gormley et al, 2011). We hope
new opportunities will arise as this work explores a
new research area for topic models.
Appendix A: Derivation of Full Conditionals
The model?s complete data likelihood over all
variables?observed words X , latent topic assign-
ments Z, matrix B, and component/expert distribu-
tions ?:
p(X,Z,B,?|?,?, ?) =
p(X|Z,B,?)p(Z|?)p(B|?)p(?|?) (4)
This follows from the conditional independence as-
sumptions. It is tractable to integrate out all parame-
ters except Z,B,? and hyperparameters ?,?, ?. 11
11For simplicity, we switch from indexing examples as xmn
to xi. In this presentation, xi is the ith example in the corpus,
Full conditional of zi Recall that p(Z|?) is
the Dirichlet-Multinomial distribution over topic
assignments, where ? has been integrated out.
The form of this distribution is identical to the
corresponding distribution over topics in LDA.
The derivation of the full conditional of zi ?
{1, . . . ,K}, follows from the factorization in Eq. 4:
p(zi|X,Z
?(i),B,?,?,?, ?) (5)
? p(X|Z,B,?)p(Z|?) (6)
? p(xi|bzi ,?)(n?
?(i)
mzi + ?zi) (7)
Z?(i) is the set of all topic assignments except zi.
We use the independence of each document, recall-
ing that example i belongs to document m. In prac-
tice, we cache p(x|bz,?) for all x, z (V ?K values)
and these are shared by all zi in a sampling iteration.
Above, just as in LDA, p(Z|?) is simplified by
proportionality to (n??(i)mzi + ?zi), where n?
?(i)
mk is the
count of examples for document m that are assigned
topic k excluding zi?s contribution (Heinrich, 2008).
Full conditional of bkc Recall that p(B|?) is the
prior for a Beta-Bernoulli matrix. The full condi-
tional distribution of a position in the binary vector
is (Griffiths and Ghahramani, 2006):
p(bkc = 1|B
?(kc), ?) =
n??(k)c +
?
C
K + ?C
(8)
where n??(k)c is the count of topics with component
c excluding topic k, and B?(kc) is the entire matrix
except for the entry bkc.
To find the full conditional for bkc ? {0, 1}, we
again start with the factorization from Eq. 4.
p(bkc|X,Z,B
?(kc),?,?,?, ?) (9)
? p(X|Z,B,?)p(B|?) (10)
?
[
?
i:zi=k
p(xi|bzi ,?)
]
p(bkc|B
?(kc), ?) (11)
where p(bkc|B?(kc), ?) is given by Eq. 8,
=
?
?
?
(?V
v=1 ?
n?kv
cv
)bkc
(?V
v=1
?C
j=1 ?
bkj
jv
)?||n?k||1
?
?
? p(bkc|B
?(kc), ?)
(12)
and where n?kv is the count of words assigned topic
k that are type v, and ||n?k||1 (the L1-norm of count
vector n?k) is the count of all words with topic k.
which corresponds to some m,n pair.
791
References
David Blei and John Lafferty. 2006. Correlated topic
models. In Advances in Neural Information Process-
ing Systems (NIPS), volume 18.
David Blei and Jon McAuliffe. 2007. Supervised topic
models. In Advances in Neural Information Process-
ing Systems (NIPS).
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet alocation. Journal of Machine Learning
Research, 3.
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2004. Hierarchical topic models
and the nested chinese restaurant process. In Advances
in Neural Information Processing Systems (NIPS), vol-
ume 16.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Interna-
tional Conference on Machine Learning (ICML).
Matthew R. Gormley, Mark Dredze, Benjamin Van
Durme, and Jason Eisner. 2011. Shared components
topic models with application to selectional prefer-
ence. In Learning Semantics Workshop at NIPS 2011,
December.
Thomas Griffiths and Zoubin Ghahramani. 2006. Infinite
latent feature models and the indian buffet process. In
Advances in Neural Information Processing Systems
(NIPS), volume 18.
Gregor Heinrich. 2008. Parameter estimation for text
analysis. Technical report, Fraunhofer IGD.
Katherine A. Heller and Zoubin Ghahramani. 2007. A
nonparametric bayesian approach to modeling over-
lapping clusters. In Artificial Intelligence and Statis-
tics (AISTATS), pages 187?194.
Geoffrey Hinton. 1999. Products of experts. In In-
ternational Conference on Artificial Neural Networks
(ICANN).
Geoffrey Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural Computa-
tion, 14(8):1771?1800.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In International Conference on Machine Learn-
ing (ICML), pages 577?584.
Wei Li, David Blei, and Andrew McCallum. 2007. Non-
parametric bayes pachinko allocation. In Uncertainty
in Artificial Intelligence (UAI).
David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with pachinko alloca-
tion. In International Conference on Machine Learn-
ing (ICML), pages 633?640.
John Paisley, Chong Wang, and David Blei. 2011. The
discrete infinite logistic normal distribution for Mixed-
Membership modeling. In International Conference
on Artificial Intelligence and Statistics (AISTATS).
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Hanna Wallach, Ian Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In International Conference on Machine
Learning (ICML), pages 1105?1112.
Greg Wei and Martin Tanner. 1990. A monte carlo im-
plementation of the EM algorithm and the poor man?s
data augmentation algorithms. Journal of the Ameri-
can Statistical Association, 85(411):699?704.
Sinead Williamson, Chong Wang, Katherine Heller, and
David Blei. 2010. The IBP compound dirichlet
process and its application to focused topic model-
ing. In International Conference on Machine Learn-
ing (ICML).
Xiaojin Zhu, David Blei, and John Lafferty. 2006.
TagLDA: bringing document structure knowledge into
topic models. Technical Report TR-1553, University
of Wisconsin.
792
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 5?9,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
Topic Models and Metadata for Visualizing Text Corpora
Justin Snyder, Rebecca Knowles, Mark Dredze, Matthew R. Gormley, Travis Wolfe
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
{jsnyde32,mdredze,mgormley,twolfe3}@jhu.edu, rknowles@haverford.edu
Abstract
Effectively exploring and analyzing large text
corpora requires visualizations that provide a
high level summary. Past work has relied on
faceted browsing of document metadata or on
natural language processing of document text.
In this paper, we present a new web-based tool
that integrates topics learned from an unsuper-
vised topic model in a faceted browsing expe-
rience. The user can manage topics, filter doc-
uments by topic and summarize views with
metadata and topic graphs. We report a user
study of the usefulness of topics in our tool.
1 Introduction
When analyzing text corpora, such as newspaper ar-
ticles, research papers, or historical archives, users
need an intuitive way to understand and summa-
rize numerous documents. Exploratory search (Mar-
chionini, 2006) is critical for large corpora that can
easily overwhelm users. Corpus visualization tools
can provide a high-level view of the data and help di-
rect subsequent exploration. Broadly speaking, such
systems can be divided into two groups: those that
rely on structured metadata, and those that use infor-
mation derived from document content.
Metadata Approaches based on metadata include
visualizing document metadata alongside a domain
ontology (Seeling and Becks, 2003), providing tools
to select passages based on annotated words (Cor-
rell et al, 2011), and using images and metadata for
visualizing related documents (Cataldi et al, 2011).
A natural solution for exploring via metadata is
faceted browsing (English et al, 2002; Hearst, 2006;
Smith et al, 2006; Yee et al, 2003), a paradigm
for filtering commonly used in e-commerce stores.
This consists of filtering based on metadata like
?brand? or ?size?, which helps summarize the con-
tent of the current document set (Ka?ki, 2005). Stud-
ies have shown improved user experiences by facil-
itating user interactions through facets (Oren et al,
2006) and faceted browsing has been used for aid-
ing search (Fujimura et al, 2006) and exploration
(Collins et al, 2009) of text corpora.
However, facets require existing structured meta-
data fields, which may be limited or unavailable. An
alternative is to use NLP to show document content.
Content Topic modeling (Blei et al, 2003), has
become very popular for corpus and document un-
derstanding. Recent research has focused on aspects
highlighted by the topic model, such as topic distri-
butions across the corpus, topic distributions across
documents, related topics and words that make up
each topic (Chaney and Blei, 2012; Eisenstein et al,
2012), or document relations through topic compo-
sitions (Chuang et al, 2012; Gardner et al, 2010).
Newer work has begun to visualize documents in
the context of their topics and their metadata, such as
topics incorporated with keywords and events (Cui
et al, 2011). Other examples include displaying
topic prevalence over time (Liu et al, 2009) or help-
ing users understand how real events shape textual
trends (Dou et al, 2011). While interfaces may be
customized for specific metadata types, e.g. the top-
ical map of National Institutes of Health funding
agencies (Talley et al, 2011), these interfaces do not
incorporate arbitrary metadata.
5
2 Combining Metadata and Topics
We present MetaToMATo (Metadata and Topic
Model Analysis Toolkit), a visualization tool that
combines both metadata and topic models in a single
faceted browsing paradigm for exploration and anal-
ysis of document collections. While previous work
has shown the value of metadata facets, we show that
topic model output complements metadata. Provid-
ing both in a single interface yields a flexible tool.
We illustrate MetaToMATo with an example
adapted from our user study. Consider Sarah, a
hypothetical intern in the New York Times archive
room who is presented with the following task.
Your boss explains that although the New
York Times metadata fields are fairly compre-
hensive, sometimes human error leads to over-
sights or missing entries. Today you?ve been
asked to keep an eye out for documents that
mention the New York Marathon but do not
include descriptors linking them to that event.
This is corpus exploration: a user is asked to dis-
cover relevant information by exploring the corpus.
We illustrate the tool with a walk-through.
Corpus Selection The corpus selection page (tool
home page) provides information about all available
corpora, and allows for corpora upload and deletion.
Sarah selects the New York Times corpus.
Corpus Overview After selecting a corpus, the
user sees the corpus overview and configuration
page. Across four tabs, the user is presented with
more detailed corpus statistics and can customize
her visualization experience. The first tab shows
general corpus information. The second allows for
editing the inferred type (date, quantity, or string)
for each metadata attribute to change filtering be-
havior, hide unhelpful attributes, and choose which
attributes to ?quick display? in the document col-
lapsed view. On the remaining two tabs, the user can
customize date display formats and manage tags.
She selects attributes ?Date? and ?Byline? for
quick display, hides ?Series Name?, and formats
?Date? to show only the date (no times).
Topics View Each topic is displayed in a box con-
taining its name (initially set to its top 3 words) and a
list of the top 10 words. Top words within a topic are
words with the highest probability of appearing in
the corpus. Each topic word is highlighted to show a
Figure 1: Topics Page A view of the first row of top-
ics, and the sorting selector at the top of the page. The
left topic is being renamed. The second topic has been
marked as junk.
normalized probability of that word within the topic.
(Figure 1) Clicking a topic box provides more infor-
mation. Users can rename topics, label unhelpful or
low-quality topics as JUNK, or sort them in terms of
frequency in the corpus,1 predicted quality,2 or junk.
Sarah renames several topics, including the topic
?{running, athletes, race}? as SPORTS and marks
the ?{share, listed, bath}? topic as JUNK.
Documents View The document view provides a
faceted browsing interface of the corpus. (Figure 2)
The pane on the right side displays the set of docu-
ments returned by the current filters (search). Each
document is summarized by the first 100 words and
any quick view metadata. Users can expand doc-
uments to see all document metadata, a graph of
the distribution of the topics in this document, and
a graph of topics distinctive to this document com-
pared to corpus-wide averages.3
Sarah begins by looking at the types of documents
in the corpus, opening and closing a few documents
as she scrolls down the page.
The facets pane on the left side of the page dis-
plays the available facets given the current filters.
Topics in a drop-down menu can be used to filter
given a threshold.
Sarah selects the value ?New York City? for the
Location attribute and a threshold of 5% for the
SPORTS topic, filtering on both facets.
Values next to each metadata facet show the num-
ber of documents in the current view with those at-
tribute values, which helps tell the user what to ex-
1Frequency is computed using topic assignments from a
Gibbs sampler (Griffiths and Steyvers, 2004).
2Topic quality is given by the entropy of its word distribu-
tion. Other options include Mimno and Blei (2011).
3The difference of the probability of a topic in the current
document and the topic overall, divided by value overall.
6
Figure 2: Left: Documents Page. The left pane shows the available facets (topics and metadata) and the right pane
shows the matching documents (collapsed view.) Right: Expanded Document. An expanded collapsed document is
replaced with this more detailed view, showing the entire document as well as metadata and topic graphs.
pect if she refines her query.
Sarah notices that the News Desk value of
?Sports? matches a large number of documents in
the current view. She adds this filter to the current
facet query, updating the document view.
At the top of the document pane are the cur-
rent view?s ?Aggregate Statistics?, which shows how
many documents match the current query. An ex-
pandable box shows graphs for the current docu-
ments topic distribution and distinctive topics.4
Looking at the topic graph for the current query,
Sarah sees that another topic with sports related
words appears with high probability. She adds it to
the search and updates the document view.
Any document can be tagged with user-created
tags. Tags and their associated documents are dis-
played in the corpus overview on the configuration
page. If a user finds a search query of interest, she
can save and name the search to return to it later.
Sarah sees many documents relevant to the New
York City Marathon. She tags documents of interest
and saves the query for later reference.
2.1 Implementation Details
Our web based tool makes it easy for users to share
results, maintain the system, and make the tool
widely available. The application is built with a
JSP front-end, a Java back-end, and a MongoDB
database for storing the corpus and associated data.
To ensure a fast UI, filters use an in-memory meta-
data and topic index. Searches are cached so incre-
mental search queries are very fast. The UI uses
4Computed as above but with more topics displayed.
Ajax and JQuery UI for dynamic loading and inter-
active elements. We easily hosted more than a dozen
corpora on a single installation.
3 Evaluation
Our primary goal was to investigate whether incor-
porating topic model output along with document
metadata into a faceted browser provided an effec-
tive mechanism for filtering documents. Participants
were presented with four tasks consisting of a ques-
tion to answer using the tool and a paragraph provid-
ing context. The first three tasks tested exploration
(find documents) while the last tested analysis (learn
about article authors). At the end of each task, the
users were directed to a survey on the tool?s useful-
ness. We also logged user actions to further evaluate
how they used the tool.
3.1 Participants and Experimental Setup
Twelve participants (3 female, 9 male) volunteered
after receiving an email from a local mailing list.
They received no compensation for their participa-
tion and they were able to complete the experiment
in their preferred environment at a convenient time
by accessing the tool online. They were provided
with a tool guide and were encouraged to familiarize
themselves with the tool before beginning the tasks;
logs suggest 8 of 12 did exploration before starting.
The study required participants to find informa-
tion from a selection of 10,000 documents from
the New York Times Annotated Corpus (Sandhaus,
2008), which contains a range of metadata.5 All
5The full list of metadata fields that we allowed users to ac-
7
documents in the corpus were published in January
of 1995 and we made no effort at deduplication.
Topics were generated using the Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) implementation
in MALLET (McCallum, 2002). We used 100 top-
ics trained with 1500 Gibbs iterations and hyper-
parameter optimization.
3.2 Quantitative Results
The length of time required to complete individual
tasks ranged from 1 minute and 3 seconds to 24 min-
utes and 54 seconds (average 9 minutes.) 6
Within the scope of each task, each user initi-
ated on average 5.75 searches. The time between
searches was on average 1 minute and 53 seconds.
Of all the searches, 21.4% were new searches and
78.6% built on previous searches when users chose
to expand or narrow the scope of the search. When
users initiated new search queries, they began with
queries on topics 59.3% of the time, with queries on
metadata 37.3% of the time, and queries that used
both topics and metadata 3.4% of the time. This
lends credence to the claim that the ability to access
both metadata and topics is crucial.
We asked users to rate features in terms of their
usefulness on a Likert scale from 1 (not helpful at
all) to 5 (extremely helpful). The most preferred fea-
tures were filtering on topics (mean 4.217, median 5)
and compacted documents (mean 3.848, median 5)
The least preferred were document graphs of topic
usage (mean 1.848, median 1) and aggregate statis-
tics (mean 1.891, median 1).7 The fact that filtering
on topics was the most preferred feature validates
our approach of including topics as a facet. Addi-
tionally, topic names were critical to this success.
3.3 Surveys
Users provided qualitative feedback8 by describing
their approaches to the task, and offering sugges-
cess in the study was: online section, organization, news desk,
date, locations, series name, byline (author), people, title, fea-
ture page, and descriptors.
6These times do not include the 3 instances in which a user
felt unable to complete a task. Also omitted are 11 tasks (from
4 users) for which log files could not provide accurate times.
7Ratings are likely influenced by the specific nature of the
sample user tasks. In tasks that required seeking out metadata,
expanded document views rated higher than their average.
8The survey results presented here consist of one survey per
participant per task, with two exceptions where two participants
tions, the most common of which was an increase
in allowed query complexity, a feature we intend to
enhance. In the current version, all search terms are
combined using AND; 7 of the 12 participants made
requests for a NOT option.
Some users (6 of 12) admitted to using their
browser?s search feature to help complete the tasks.
We chose to forgo a keyword search capability in the
study-ready version of the tool because we wanted
to test the ability of topic information to provide a
way to navigate the content. Given the heavy us-
age of topic searches and the ability of users to com-
plete tasks with or without browser search, we have
demonstrated the usefulness of the topics as a win-
dow into the content. In future versions, we envision
incorporating keyword search capabilities, including
suggested topic filters for searched queries.
As users completed the tasks, their comfort with
the tool increased. One user wrote, ?After the last
task I knew exactly what to do to get my results. I
knew what information would help me find docu-
ments.? Users also began to suggest new ways that
they would like to see topics and metadata com-
bined. Task 4 led one user to say ?It would be in-
teresting to see a page on each author and what top-
ics they mostly covered.? We could provide this in a
general way by showing a page for each metadata at-
tribute that contains relevant topics and other meta-
data. We intend to implement such features.
4 Conclusion
A user evaluation of MetaToMATo, our toolkit for
visualizing text corpora that incorporates both topic
models and metadata, confirms the validity of our
approach to use topic models and metadata in a sin-
gle faceted browser. Users searched with topics a
majority of the time, but also made use of metadata.
This clearly demonstrates a reliance on both, sug-
gesting that users went back and forth as needed.
Additionally, while metadata is traditionally used for
facets, users ranked filtering by topic more highly
than metadata. This suggests a new direction in
which advances in topic models can be used to aid
corpus exploration.
each failed to record one of their four surveys.
8
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
M. Cataldi, L. Di Caro, and C. Schifanella. 2011. Im-
mex: Immersive text documents exploration system.
In Content-Based Multimedia Indexing (CBMI), 2011
9th International Workshop on, pages 1?6. IEEE.
A.J.B. Chaney and D.M. Blei. 2012. Visualizing topic
models. In AAAI.
J. Chuang, C.D. Manning, and J. Heer. 2012. Ter-
mite: visualization techniques for assessing textual
topic models. In Proceedings of the International
Working Conference on Advanced Visual Interfaces,
pages 74?77. ACM.
Christopher Collins, Fernanda B. Vie?gas, and Martin
Wattenberg. 2009. Parallel tag clouds to explore and
analyze faceted text corpora. In Proc. of the IEEE
Symp. on Visual Analytics Science and Technology
(VAST).
M. Correll, M. Witmore, and M. Gleicher. 2011. Explor-
ing collections of tagged text for literary scholarship.
Computer Graphics Forum, 30(3):731?740.
W. Cui, S. Liu, L. Tan, C. Shi, Y. Song, Z. Gao, H. Qu,
and X. Tong. 2011. Textflow: Towards better un-
derstanding of evolving topics in text. Visualiza-
tion and Computer Graphics, IEEE Transactions on,
17(12):2412?2421.
W. Dou, X. Wang, R. Chang, and W. Ribarsky. 2011.
Paralleltopics: A probabilistic approach to exploring
document collections. In Visual Analytics Science and
Technology (VAST), 2011 IEEE Conference on, pages
231?240. IEEE.
Jacob Eisenstein, Duen Horng ?Polo? Chau, Aniket Kit-
tur, and Eric P. Xing. 2012. Topicviz: Interactive topic
exploration in document collections. In CHI.
Jennifer English, Marti Hearst, Rashmi Sinha, Kirsten
Swearingen, and Ka-Ping Yee. 2002. Flexible search
and navigation using faceted metadata. In ACM SIGIR
Conference on Information Retrieval (SIGIR).
Ko Fujimura, Hiroyuki Toda, Takafumi Inoue, Nobuaki
Hiroshima, Ryoji Kataoka, and Masayuki Sugizaki.
2006. Blogranger - a multi-faceted blog search engine.
In World Wide Web (WWW).
Matthew J. Gardner, Joshua Lutes, Jeff Lund, Josh
Hansen, Dan Walker, Eric Ringger, and Kevin Seppi.
2010. The topic browser: An interactive tool for
browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.
T.L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences of the United States of America, 101(Suppl
1):5228?5235.
Marti Hearst. 2006. Clustering versus faceted categories
for information exploration. Communications of the
ACM, 49(4).
Mika Ka?ki. 2005. Findex: search result categories help
users when document ranking fails. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, CHI ?05, pages 131?140, New York,
NY, USA. ACM.
S. Liu, M.X. Zhou, S. Pan, W. Qian, W. Cai, and X. Lian.
2009. Interactive, topic-based visual text summariza-
tion and analysis. In Proceedings of the 18th ACM
conference on Information and knowledge manage-
ment, pages 543?552. ACM.
G. Marchionini. 2006. Exploratory search: from find-
ing to understanding. Communications of the ACM,
49(4):41?46.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
D. Mimno and D. Blei. 2011. Bayesian checking for
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 227?237. Association for Computational Lin-
guistics.
Eyal Oren, Renaud Delbru, and Stefan Decker. 2006.
Extending faceted navigation for rdf data. In Interna-
tional Semantic Web Conference (ISWC).
Evan Sandhaus. 2008. The new york times annotated
corpus.
Christian Seeling and Andreas Becks. 2003. Exploit-
ing metadata for ontology-based visual exploration of
weakly structured text documents. In Proceedings of
the 7th International Conference on Information Visu-
alisation (IV03, pages 0?7695. IEEE Press, ISBN.
Greg Smith, Mary Czerwinski, Brian Meyers, Daniel
Robbins, George Robertson, and Desney S. Tan. 2006.
FacetMap: A Scalable Search and Browse Visualiza-
tion. IEEE Transactions on Visualization and Com-
puter Graphics, 12(5):797?804.
E.M. Talley, D. Newman, D. Mimno, B.W. Herr II,
H.M. Wallach, G.A.P.C. Burns, A.G.M. Leenders, and
A. McCallum. 2011. Database of nih grants using
machine-learned categories and graphical clustering.
Nature Methods, 8(6):443?444.
Ping Yee, Kirsten Swearingen, Kevin Li, and Marti
Hearst. 2003. Faceted metadata for image search and
browsing. In Computer-Human Interaction (CHI).
9
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 444?454,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Nonconvex Global Optimization for Latent-Variable Models?
Matthew R. Gormley Jason Eisner
Department of Computer Science
Johns Hopkins University, Baltimore, MD
{mrg,jason}@cs.jhu.edu
Abstract
Many models in NLP involve latent vari-
ables, such as unknown parses, tags, or
alignments. Finding the optimal model pa-
rameters is then usually a difficult noncon-
vex optimization problem. The usual prac-
tice is to settle for local optimization meth-
ods such as EM or gradient ascent.
We explore how one might instead search
for a global optimum in parameter space,
using branch-and-bound. Our method
would eventually find the global maxi-
mum (up to a user-specified ) if run for
long enough, but at any point can return
a suboptimal solution together with an up-
per bound on the global maximum.
As an illustrative case, we study a gener-
ative model for dependency parsing. We
search for the maximum-likelihood model
parameters and corpus parse, subject to
posterior constraints. We show how to for-
mulate this as a mixed integer quadratic
programming problem with nonlinear con-
straints. We use the Reformulation Lin-
earization Technique to produce convex
relaxations during branch-and-bound. Al-
though these techniques do not yet pro-
vide a practical solution to our instance
of this NP-hard problem, they sometimes
find better solutions than Viterbi EM with
random restarts, in the same time.
1 Introduction
Rich models with latent linguistic variables are
popular in computational linguistics, but in gen-
eral it is not known how to find their optimal pa-
rameters. In this paper, we present some ?new? at-
tacks for this common optimization setting, drawn
from the mathematical programming toolbox.
We focus on the well-studied but unsolved task
of unsupervised dependency parsing (i.e., depen-
?This research was partially funded by the JHU Human
Language Technology Center of Excellence.
-180.2 
-231.0 -254.3 
-387.1 -287.3 -311.1 -467.5 
-298 -342 
!5 " -0.6 -0.6 " !5 
!5 " -2 -2 " !5 !3 " -0.6 -0.6 " !3 
!2 " -0.6 -0.6 " !2 
Branch-and-bound tree: Incumbent solution: 
$ $
M?
m=1
?mf?m = ?400
? = [?0.1,?0.6,?20,?1.3, . . . ]
f = [4, 0, 2, 21, . . .]
i=1
? 4 0
Figure 1: Each node contains a local upper bound
for its subspace, computed by a relaxation. The
node branches on a single model parameter ?m to
partition its subspace. The lower bound, -400, is
given by the best solution seen so far, the incum-
bent. The upper bound, -298, is the min of all re-
maining leaf nodes. The node with a local bound
of -467.5 can be pruned because no solution within
its subspace could be better than the incumbent.
dency grammar induction). This may be a par-
ticularly hard case, but its structure is typical.
Many parameter estimation techniques have been
attempted, including expectation-maximization
(EM) (Klein and Manning, 2004; Spitkovsky et
al., 2010a), contrastive estimation (Smith and Eis-
ner, 2006; Smith, 2006), Viterbi EM (Spitkovsky
et al, 2010b), and variational EM (Naseem et al,
2010; Cohen et al, 2009; Cohen and Smith, 2009).
These are all local search techniques, which im-
prove the parameters by hill-climbing.
The problem with local search is that it gets
stuck in local optima. This is evident for gram-
mar induction. An algorithm such as EM will find
numerous different solutions when randomly ini-
tialized to different points (Charniak, 1993; Smith,
2006). A variety of ways to find better local op-
tima have been explored, including heuristic ini-
tialization of the model parameters (Spitkovsky
et al, 2010a), random restarts (Smith, 2006),
and annealing (Smith and Eisner, 2006; Smith,
2006). Others have achieved accuracy improve-
ments by enforcing linguistically motivated pos-
terior constraints on the parameters (Gillenwater
et al, 2010; Naseem et al, 2010), such as requir-
ing most sentences to have verbs or encouraging
nouns to be children of verbs or prepositions.
We introduce a method that performs global
444
search with certificates of -optimality for both
the corpus parse and the model parameters. Our
search objective is log-likelihood. We can also im-
pose posterior constraints on the latent structure.
As we show, maximizing the joint log-
likelihood of the parses and the parameters can be
formulated as a mathematical program (MP) with
a nonconvex quadratic objective and with integer
linear and nonlinear constraints. Note that this ob-
jective is that of hard (Viterbi) EM?we do not
marginalize over the parses as in classical EM.1
To globally optimize the objective function,
we employ a branch-and-bound algorithm that
searches the continuous space of the model param-
eters by branching on individual parameters (see
Figure 1). Thus, our branch-and-bound tree serves
to recursively subdivide the global parameter hy-
percube. Each node represents a search problem
over one of the resulting boxes (i.e., orthotopes).
The crucial step is to prune nodes high in the
tree by determining that their boxes cannot contain
the global maximum. We compute an upper bound
at each node by solving a relaxed maximization
problem tailored to its box. If this upper bound is
worse than our current best solution, we can prune
the node. If not, we split the box again via another
branching decision and retry on the two halves.
At each node, our relaxation derives a linear
programming problem (LP) that can be efficiently
solved by the dual simplex method. First, we lin-
early relax the constraints that grammar rule prob-
abilities sum to 1?these constraints are nonlin-
ear in our parameters, which are log-probabilities.
Second, we linearize the quadratic objective by ap-
plying the Reformulation Linearization Technique
(RLT) (Sherali and Adams, 1990), a method of
forming tight linear relaxations of various types
of MPs: the reformulation step multiplies together
pairs of the original linear constraints to generate
new quadratic constraints, and then the lineariza-
tion step replaces quadratic terms in the new con-
straints with auxiliary variables.
Finally, if the node is not pruned, we search
for a better incumbent solution under that node
by projecting the solution of the RLT relaxation
back onto the feasible region. In the relaxation, the
model parameters might sum to slightly more than
1This objective might not be a great sacrifice: Spitkovsky
et al (2010b) present evidence that hard EM can outperform
soft EM for grammar induction in a hill-climbing setting. We
use it because it is a quadratic objective. However, maximiz-
ing it remains NP-hard (Cohen and Smith, 2010).
one and the parses can consist of fractional depen-
dency edges. We project in order to compute the
true objective and compare with other solutions.
Our results demonstrate that our method can ob-
tain higher likelihoods than Viterbi EM with ran-
dom restarts. Furthermore, we show how posterior
constraints inspired by Gillenwater et al (2010)
and Naseem et al (2010) can easily be applied
in our framework to obtain competitive accuracies
using a simple model, the Dependency Model with
Valence (Klein and Manning, 2004). We also ob-
tain an -optimal solution on a toy dataset.
We caution that the linear relaxations are very
loose on larger boxes. Since we have many dimen-
sions, the binary branch-and-bound tree may have
to grow quite deep before the boxes become small
enough to prune. This is why nonconvex quadratic
optimization by LP-based branch-and-bound usu-
ally fails with more than 80 variables (Burer and
Vandenbussche, 2009). Even our smallest (toy)
problems have hundreds of variables, so our exper-
imental results mainly just illuminate the method?s
behavior. Nonetheless, we offer the method as
a new tool which, just as for local search, might
be combined with other forms of problem-specific
guidance to produce more practical results.
2 The Constrained Optimization Task
We begin by describing how for our typical model,
the Viterbi EM objective can be formulated as a
mixed integer quadratic programming (MIQP)
problem with nonlinear constraints (Figure 2).
Other locally normalized log-linear generative
models (Berg-Kirkpatrick et al, 2010) would have
a similar formulation. In such models, the log-
likelihood objective is simply a linear function of
the feature counts. However, the objective be-
comes quadratic in unsupervised learning, be-
cause the feature counts are themselves unknown
variables to be optimized. The feature counts are
constrained to be derived from the latent variables
(e.g., parses), which are unknown discrete struc-
tures that must be encoded with integer variables.
The nonlinear constraints ensure that the model
parameters are true log-probabilities.
Concretely, (1) specifies the Viterbi EM objec-
tive: the total log-probability of the best parse
trees under the parameters ?, given by a sum of
log-probabilities ?m of the individual steps needed
to generate the tree, as encoded by the features
fm. The (nonlinear) sum-to-one constraints on the
445
Variables:
?m Log-probability for feature m
fm Corpus-wide feature count for m
esij Indicator of an arc from i to j in tree s
Indices and constants:
m Feature / model parameter index
s Sentence index
c Conditional distribution index
M Number of model parameters
C Number of conditional distributions
Mc cth Set of feature indices that sum to 1.0
S Number of sentences
Ns Number of words in the sth sentence
Objective and constraints:
max
?
m
?mfm (1)
s.t.
?
m?Mc
exp(?m) = 1, ?c (2)
A
[
f
e
]
? b (Model constraints) (3)
?m ? 0, fm, esij ? Z, ?m, s, i, j (4)
Figure 2: Viterbi EM as a mathematical program
probabilities are in (2). The linear constraints in
(3) will ensure that the arc variables for each sen-
tence es encode a valid latent dependency tree,
and that the f variables count up the features of
these trees. The final constraints (4) simply spec-
ify the range of possible values for the model pa-
rameters and their integer count variables.
Our experiments use the dependency model
with valence (DMV) (Klein and Manning, 2004).
This generative model defines a joint distribution
over the sentences and their dependency trees.
We encode the DMV using integer linear con-
straints on the arc variables e and feature counts
f . These will constitute the model constraints in
(3). The constraints must declaratively specify that
the arcs form a valid dependency tree and that the
resulting feature values are as defined by the DMV.
Tree Constraints To ensure that our arc vari-
ables, es, form a dependency tree, we employ the
same single-commodity flow constraints of Mag-
nanti and Wolsey (1994) as adapted by Martins et
al. (2009) for parsing. We also use the projectivity
constraints of Martins et al (2009).
The single-commodity flow constraints simul-
taneously enforce that each node has exactly one
parent, the special root node (position 0) has no in-
coming arcs, and the arcs form a connected graph.
For each sentence, s, the variable ?sij indicates
the amount of flow traversing the arc from i to j in
sentence s. The constraints below specify that the
root node emits Ns units of flow (5), that one unit
of flow is consumed by each each node (6), that
the flow is zero on each disabled arc (7), and that
the arcs are binary variables (8).
Single-commodity flow (Magnanti & Wolsey, 1994)
Ns?
j=1
?s0j = Ns, ?j (5)
Ns?
i=0
?sij ?
Ns?
k=1
?sjk = 1, ?j (6)
?sij ? Nsesij , ?i, j (7)
esij ? {0, 1}, ?i, j (8)
Projectivity is enforced by adding a constraint
(9) for each arc ensuring that no edges will cross
that arc if it is enabled. Xij is the set of arcs (k, l)
that cross the arc (i, j).
Projectivity (Martins et al, 2009)?
(k,l)?Xij
eskl ? Ns(1? esij), ?s, i, j (9)
DMV Feature Counts The DMV generates a
dependency tree recursively as follows. First
the head word of the sentence is generated, t ?
Discrete(?root), where ?root is a subvector of ?.
To generate its children on the left side, we flip
a coin to decide whether an adjacent child is gen-
erated, d ? Bernoulli(?dec.L.0,t). If the coin flip
d comes up continue, we sample the word of that
child as t? ? Discrete(?child.L,t). We continue gen-
erating non-adjacent children in this way, using
coin weights ?dec.L.? 1,t until the coin comes up
stop. We repeat this procedure to generate chil-
dren on the right side, using the model parameters
?dec.R.0,t, ?child.R,t, and ?dec.R.? 1,t. For each new
child, we apply this process recursively to gener-
ate its descendants.
The feature count variables for the DMV are en-
coded in our MP as various sums over the edge
variables. We begin with the root/child feature
counts. The constraint (10) defines the feature
count for model parameter ?root,t as the number
of all enabled arcs connecting the root node to a
word of type t, summing over all sentences s. The
constraint in (11) similarly defines fchild.L,t,t? to be
the number of enabled arcs connecting a parent of
446
type t to a left child of type t?. Wst is the index set
of tokens in sentences s with word type t.
DMV root/child feature counts
froot,t =
Ns?
s=1
?
j?Wst
es0j , ?t (10)
fchild.L,t,t? =
Ns?
s=1
?
j<i
?
[
i?Wst ?
j?Wst?
]
esij , ?t, t? (11)
The decision feature counts require the addi-
tion of an auxiliary count variables f (si)m ? Z in-
dicating how many times decision feature m fired
at some position in the corpus s, i. We then need
only add a constraint that the corpus wide fea-
ture count is the sum of these token-level feature
counts fm =?Ss=1
?Ns
i=1 f
(si)
m , ?m.
Below we define these auxiliary variables for
1 ? s ? S and 1 ? i ? Ns. The helper vari-
able ns,i,l counts the number of enabled arcs to the
left of token i in sentence s. Let t denote the word
type of token i in sentence s. Constraints (11) -
(16) are defined analogously for the right side fea-
ture counts.
DMV decision feature counts
ns,i,l =
i?1?
j=1
esij (12)
ns,i,l/Ns ? f (s,i)dec.L.0,t,cont ? 1 (13)
f (s,i)dec.L.0,t,stop = 1? f
(s,i)
dec.L.0,t,cont (14)
f (s,i)dec.L.? 1,t,stop = f
(s,i)
dec.L.0,t,cont (15)
f (s,i)dec.L.? 1,t,cont = ns,i,l ? f
(s,i)
dec.L.0,t,cont (16)
3 A Branch-and-Bound Algorithm
The mixed integer quadratic program with nonlin-
ear constraints, given in the previous section, max-
imizes the nonconvex Viterbi EM objective and is
NP-hard to solve (Cohen and Smith, 2010). The
standard approach to optimizing this program is
local search by the hard (Viterbi) EM algorithm.
Yet local search can only provide a lower (pes-
simistic) bound on the global maximum.
We propose a branch-and-bound algorithm,
which will iteratively tighten both pessimistic and
optimistic bounds on the optimal solution. This
algorithm may be halted at any time, to obtain the
best current solution and a bound on how much
better the global optimum could be.
A feasible solution is an assignment to all
the variables?both model parameters and corpus
parse?that satisfies all constraints. Our branch-
and-bound algorithm maintains an incumbent so-
lution: the best known feasible solution according
to the objective function. This is updated as better
feasible solutions are found.
Our algorithm implicitly defines a search tree in
which each node corresponds to a region of model
parameter space. Our search procedure begins
with only the root node, which represents the full
model parameter space. At each node we perform
three steps: bounding, projecting, and branching.
In the bounding step, we solve a relaxation of
the original problem to provide an upper bound on
the objective achievable within that node?s subre-
gion. A node is pruned when Lglobal + |Lglobal| ?
Ulocal, where Lglobal is the incumbent score, Ulocal
is the upper bound for the node, and  > 0. This
ensures that its entire subregion will not yield a
-better solution than the current incumbent.
The overall optimistic bound is given by the
worst optimistic bound of all current leaf nodes.
The projecting step, if the node is not pruned,
projects the solution of the relaxation back to the
feasible region, replacing the current incumbent if
this projection provides a better lower bound.
In the branching step, we choose a variable ?m
on which to divide. Each of the child nodes re-
ceives a lower ?minm and upper ?maxm bound for ?m.
The child subspaces partition the parent subspace.
The search tree is defined by a variable order-
ing and the splitting procedure. We do binary
branching on the variable ?m with the highest re-
gret, defined as zm ? ?mfm, where zm is the
auxiliary objective variable we will introduce in
? 4.2. Since ?m is a log-probability, we split its
current range at the midpoint in probability space,
log((exp ?minm + exp ?maxm )/2).
We perform best-first search, ordering the nodes
by the the optimistic bound of their parent. We
also use the LP-guided rule (Martin, 2000; Achter-
berg, 2007, section 6.1) to perform depth-first
plunges in search of better incumbents.
4 Relaxations
The relaxation in the bounding step computes an
optimistic bound for a subspace of the model pa-
rameters. This upper bound would ideally be not
much greater than the true maximum achievable
on that region, but looser upper bounds are gener-
ally faster to compute.
447
We present successive relaxations to the orig-
inal nonconvex mixed integer quadratic program
with nonlinear constraints from (1)?(4). First, we
show how the nonlinear sum-to-one constraints
can be relaxed into linear constraints and tight-
ened. Second, we apply a classic approach to
bound the nonconvex quadratic objective by a lin-
ear concave envelope. Finally, we present our full
relaxation based on the Reformulation Lineariza-
tion Technique (RLT) (Sherali and Adams, 1990).
We solve these LPs by the dual simplex algorithm.
4.1 Relaxing the sum-to-one constraint
In this section, we use cutting planes to create a
linear relaxation for the sum-to-one constraint (2).
When relaxing a constraint, we must ensure that
any assignment of the variables that was feasible
(i.e. respected the constraints) in the original prob-
lem must also be feasible in the relaxation. In most
cases, the relaxation is not perfectly tight and so
will have an enlarged space of feasible solutions.
We begin by weakening constraint (2) to
?
m?Mc
exp(?m) ? 1 (17)
The optimal solution under (17) still satisfies the
original equality constraint (2) because of the
maximization. We now relax (17) by approx-
imating the surface z = ?m?Mc exp(?m) bythe max of N lower-bounding linear functions on
R|Mc|. Instead of requiring z ? 1, we only require
each of these lower bounds to be ? 1, slightly
enlarging the feasible space into a convex poly-
tope. Figure 3a shows the feasible region con-
structed from N=3 linear functions on two log-
probabilities ?1, ?2.
Formally, for each c, we define the ith linear
lower bound (i = 1, . . . , N ) to be the tangent hy-
perplane at some point ??(i)c = [??(i)c,1, . . . , ??(i)c,|Mc|] ?
R|Mc|, where each coordinate is a log-probability
??(i)c,m < 0. We require each of these linear func-
tions to be ? 1:
Sum-to-one Relaxation?
m?Mc
(
?m + 1? ??(i)c,m
)
exp
(
??(i)c,m
)
? 1, ?i, ?c
(18)
4.2 ?Relaxing? the objective
Our true maximization objective?m ?mfm in (1)
is a sum of quadratic terms. If the parameters ?
(a)
0
1
2
3
4
f
sm
?15
?10
?5
?
m
?60
?40
?20
0
20
(b)
Figure 3: In (a), the area under the curve corre-
sponds to those points (?1, ?2) that satisfy (17)
(z ? 1), with equality (2) achieved along the curve
(z = 1). The shaded area shows the enlarged fea-
sible region under the linear relaxation. In (b),
the curved lower surface represents a single prod-
uct term in the objective. The piecewise-linear up-
per surface is its concave envelope (raised by 20
for illustration; in reality they touch).
were fixed, the objective would become linear in
the latent features. Although the parameters are
not fixed, the branch-and-bound algorithm does
box them into a small region, where the quadratic
objective is ?more linear.?
Since it is easy to maximize a concave function,
we will maximize the concave envelope?the con-
cave function that most tightly upper-bounds our
objective over the region. This turns out to be
piecewise linear and can be maximized with an LP
solver. Smaller regions yield tighter bounds.
Each node of the branch-and-bound tree speci-
fies a region via bounds constraints ?minm < ?m <
?maxm , ?m. In addition, we have known bounds
fminm ? fm ? fmaxm , ?m for the count variables.
McCormick (1976) described the concave enve-
lope for a single quadratic term subject to bounds
constraints (Figure 3b). In our case:
?mfm ? min[fmaxm ?m + ?minm fm ? ?minm fmaxm ,
fminm ?m + ?maxm fm ? ?maxm fminm ]
We replace our objective?m ?mfm with
?
m zm,
where we would like to constrain each auxiliary
variable zm to be = ?mfm or (equivalently) ?
?mfm, but instead settle for making it ? the con-
cave envelope?a linear programming problem:
Concave Envelope Objective
max
?
m
zm (19)
s.t. zm ? fmaxm ?m + ?minm fm ? ?minm fmaxm (20)
zm ? fminm ?m + ?maxm fm ? ?maxm fminm (21)
448
4.3 Reformulation Linearization Technique
The Reformulation Linearization Technique
(RLT)2 (Sherali and Adams, 1990) is a method
of forming tighter relaxations of various types of
MPs. The basic method reformulates the problem
by adding products of existing constraints. The
quadratic terms in the objective and in these new
constraints are redefined as auxiliary variables,
thereby linearizing the program.
In this section, we will show how the RLT can
be applied to our grammar induction problem and
contrast it with the concave envelope relaxation
presented in section 4.2.
Consider the original MP in equations (1) -
(4), with the nonlinear sum-to-one constraints in
(2) replaced by our linear constraints proposed in
(18). If we remove the integer constraints in (4),
the result is a quadratic program with purely linear
constraints. Such problems have the form
max xTQx (22)
s.t. Ax ? b (23)
?? < Li ? xi ? Ui <?, ?i (24)
where the variables are x ? Rn, A is an m ? n
matrix, and b ? Rm, and Q is an n? n indefinite3
matrix. Without loss of generality we assume Q is
symmetric. The application of the RLT here was
first considered by Sherali and Tuncbilek (1995).
For convenience of presentation, we repre-
sent both the linear inequality constraints and the
bounds constraints, under a different parameteri-
zation using the matrix G and vector g.
[
(bi ?Aix) ? 0, 1 ? i ? m
(Uk ? xk) ? 0, 1 ? k ? n
(?Lk + xk) ? 0, 1 ? k ? n
]
?
[(gi ?Gix) ? 0,
1 ? i ? m + 2n
]
The reformulation step forms all possible products
of these linear constraints and then adds them to
the original quadratic program.
(gi ?Gix)(gj ?Gjx) ? 0, ?1 ? i ? j ? m+ 2n
In the linearization step, we replace all
quadratic terms in the quadratic objective and new
quadratic constraints with auxiliary variables:
wij ? xixj , ?1 ? i ? j ? n
2The key idea underlying the RLT was originally intro-
duced in Adams and Sherali (1986) for 0-1 quadratic pro-
gramming. It has since been extended to various other set-
tings; see Sherali and Liberti (2008) for a complete survey.
3In the general case, that Q is indefinite causes this pro-
gram to be nonconvex, making this problem NP-hard to solve
(Vavasis, 1991; Pardalos, 1991).
This yields the following RLT relaxation:
RLT Relaxation
max
?
1?i?j?n
Qijwij (25)
s.t. gigj ?
n?
k=1
gjGikxk ?
n?
k=1
giGjkxk
+
n?
k=1
n?
l=1
GikGjlwkl ? 0,
?1 ? i ? j ? m+ 2n (26)
Notice above that we have omitted the original
inequality constraints (23) and bounds (24), be-
cause they are fully enforced by the new RLT con-
straints (26) from the reformulation step (Sherali
and Tuncbilek, 1995). In our experiments, we
keep the original constraints and instead explore
subsets of the RLT constraints.
If the original QP contains equality constraints
of the form Gex = ge, then we can form con-
straints by multiplying this one by each variable
xi. This gives us the following new set of con-
straints, for each equality constraint e: gexi +?n
j=1?Gejwij = 0, ?1 ? i ? n.
Theoretical Properties The new constraints in
eq. (26) will impose the concave envelope con-
straints (20)?(21) (Anstreicher, 2009).
The constraints presented above are consid-
ered to be first-level constraints corresponding to
the first-level variables wij . However, the same
technique can be applied repeatedly to produce
polynomial constraints of higher degree. These
higher level constraints/variables have been shown
to provide increasingly tighter relaxations (Sher-
ali and Adams, 1990) at the cost of a large num-
ber of variables and constraints. In the case where
x ? {0, 1}n the degree-n RLT constraints will re-
strict to the convex hull of the feasible solutions
(Sherali and Adams, 1990).
This is in direct contrast to the concave enve-
lope relaxation presented in section 4.2 which re-
laxes to the convex hull of each quadratic term in-
dependently. This demonstrates the key intuition
of the RLT relaxation: The products of constraints
are implied (and unnecessary) in the original vari-
able space. Yet when we project to a higher-
dimentional space by including the auxiliary vari-
ables, the linearized constraints cut off portions of
the feasible region given by only the concave en-
velope relaxation in eqs. (20)-(21) .
449
4.4 Adding Posterior Constraints
It is a simple extension to impose posterior con-
straints within our framework. Here we empha-
size constraints that are analogous to the universal
linguistic constraints from Naseem et al (2010).
Since we optimize the Viterbi EM objective, we
directly constrain the counts in the single corpus
parse rather than expected counts from a distribu-
tion over parses. Let E be the index set of model
parameters corresponding to edge types from Ta-
ble 1 of Naseem et al (2010), and Ns be the num-
ber of words in the sth sentence. We impose
the constraint that 75% of edges come from E :?
m?E fm ? 0.75
(?S
s=1Ns
)
.
5 Projections
A pessimistic bound, from the projecting step, will
correspond to a feasible but not necessarily opti-
mal solution to the original problem. We propose
several methods for obtaining pessimistic bounds
during the branch-and-bound search, by projecting
and improving the solutions found by the relax-
ation. A solution to the relaxation may be infea-
sible in the original problem for two reasons: the
model parameters might not sum to one, and/or the
parse may contain fractional edges.
Model Parameters For each set of model pa-
rameters Mc that should sum-to-one, we project
the model parameters onto the Mc ? 1 simplex
by one of two methods: (1) normalize the infeasi-
ble parameters or (2) find the point on the simplex
that has minimum Euclidean distance to the infea-
sible parameters using the algorithm of Chen and
Ye (2011). For both methods, we can optionally
apply add-? smoothing before projecting.
Parses Since we are interested in projecting the
fractional parse onto the space of projective span-
ning trees, we can simply employ a dynamic pro-
gramming parsing algorithm (Eisner and Satta,
1999) where the weight of each edge is given as
the fraction of the edge variable.
Only one of these projection techniques is
needed. We then either use parsing to fill in the
optimal parse trees given the projected model pa-
rameters, or use supervised parameter estimation
to fill in the optimal model parameters given the
projected parses. These correspond to the Viterbi
E step and M step, respectively. We can locally
improve the projected solution by continuing with
a few additional iterations of Viterbi EM.
Related models could use very similar projec-
tion techniques. Given a relaxed joint solution to
the parameters and the latent variables, one must
be able to project it to a nearby feasible one, by
projecting either the fractional parameters or the
fractional latent variables into the feasible space
and then solving exactly for the other.
6 Related Work
The goal of this work was to better understand and
address the non-convexity of maximum-likelihood
training with latent variables, especially parses.
Gimpel and Smith (2012) proposed a concave
model for unsupervised dependency parsing us-
ing IBM Model 1. This model did not include a
tree constraint, but instead initialized EM on the
DMV. By contrast, our approach incorporates the
tree constraints directly into our convex relaxation
and embeds the relaxation in a branch-and-bound
algorithm capable of solving the original DMV
maximum-likelihood estimation problem.
Spectral learning constitutes a wholly differ-
ent family of consistent estimators, which achieve
efficiency because they sidestep maximizing the
nonconvex likelihood function. Hsu et al (2009)
introduced a spectral learner for a large class of
HMMs. For supervised parsing, spectral learn-
ing has been used to learn latent variable PCFGs
(Cohen et al, 2012) and hidden-state dependency
grammars (Luque et al, 2012). Alas, there are
not yet any spectral learning methods that recover
latent tree structure, as in grammar induction.
Several integer linear programming (ILP) for-
mulations of dependency parsing (Riedel and
Clarke, 2006; Martins et al, 2009; Riedel et al,
2012) inspired our definition of grammar induc-
tion as a MP. Recent work uses branch-and-bound
for decoding with non-local features (Qian and
Liu, 2013). These differ from our work by treating
the model parameters as constants, thereby yield-
ing a linear objective.
For semi-supervised dependency parsing, Wang
et al (2008) used a convex objective, combin-
ing unsupervised least squares loss and a super-
vised large margin loss, This does not apply to our
unsupervised setting. Branch-and-bound has also
been applied to semi-supervised SVM training, a
nonconvex search problem (Chapelle et al, 2007),
with a relaxation derived from the dual.
450
7 Experiments
We first analyze the behavior of our method on a
toy synthetic dataset. Next, we compare various
parameter settings for branch-and-bound by esti-
mating the total solution time. Finally, we com-
pare our search method to Viterbi EM on a small
subset of the Penn Treebank.
All our experiments use the DMV for unsuper-
vised dependency parsing of part-of-speech (POS)
tag sequences. For Viterbi EM we initialize the pa-
rameters of the model uniformly, breaking parser
ties randomly in the first E-step (Spitkovsky et
al., 2010b). This initializer is state-of-the-art for
Viterbi EM. We also apply add-one smoothing
during each M-step. We use random restarts, and
select the model with the highest likelihood.
We add posterior constraints to Viterbi EM?s E-
step. First, we run a relaxed linear programming
(LP) parser, then project the (possibly fractional)
parses back to the feasible region. If the resulting
parse does not respect the posterior constraints, we
discard it. The posterior constraint in the LP parser
is tighter4 than the one used in the true optimiza-
tion problem, so the projections tends to be feasi-
ble under the true (looser) posterior constraints. In
our experiments, all but one projection respected
the constraints. We solve all LPs with CPLEX.
7.1 Synthetic Data
For our toy example, we generate sentences from a
synthetic DMV over three POS tags (Verb, Noun,
Adjective) with parameters chosen to favor short
sentences with English word order.
In Figure 4 we show that the quality of the root
relaxation increases as we approach the full set of
RLT constraints. That the number of possible RLT
constraints increases quadratically with the length
of the corpus poses a serious challenge. For just
20 sentences from this synthetic model, the RLT
generates 4,056,498 constraints.
For a single run of branch-and-bound, Figure 5
shows the global upper and lower bounds over
time.5 We consider five relaxations, each using
only a subset of the RLT constraints. Max.0k
uses only the concave envelope (20)-(21). Max.1k
uses the concave envelope and also randomly sam-
ples 1,000 other RLT constraints, and so on for
Max.10k and Max.100k. Obj.Filter includes all
480% of edges must come from E as opposed to 75%.
5The initial incumbent solution for branch-and-bound is
obtained by running Viterbi EM with 10 random restarts.
!4
!3
!2
!1
0 ! ! !
!
!
!
!
! ! ! !
0.0 0.2 0.4 0.6 0.8 1.0Proportion of RLT rows included
Up
per
 bo
und
 on
log
!li
kel
iho
od 
at r
oot
Figure 4: The bound quality at the root improves
as the proportion of RLT constraints increases, on
5 synthetic sentences. A random subset of 70%
of the 320,126 possible RLT constraints matches
the relaxation quality of the full set. This bound is
very tight: the relaxations in Figure 5 solve hun-
dreds of nodes before such a bound is achieved.
!12
!10
!8
!6
!4
!2
0
!
!
! !!!
!
! ! !!
20 40 6080
Time (sec)
Bo
un
ds 
on
 lo
g!
lik
eli
ho
od
Bound type
lower
upper
Relaxation
! RLT Obj.Filter
RLT Max.0k
RLT Max.1k
RLT Max.10k
Figure 5: The global upper and lower bounds
improve over time for branch-and-bound using
different subsets of RLT constraints on 5 syn-
thetic sentences. Each solves the problem to -
optimality for  = 0.01. A point marks every 200
nodes processed. (The time axis is log-scaled.)
constraints with a nonzero coefficient for one of
the RLT variables zm from the linearized ob-
jective. The rightmost lines correspond to RLT
Max.10k: despite providing the tightest (local)
bound at each node, it processed only 110 nodes in
the time it took RLT Max.1k to process 1164. RLT
Max.0k achieves the best balance of tight bounds
and speed per node.
7.2 Comparing branch-and-bound strategies
It is prohibitively expensive to repeatedly run our
algorithm to completion with a variety of param-
eter settings. Instead, we estimate the size of the
branch-and-bound tree and the solution time using
a high-variance estimate that is effective for com-
parisons (Lobjois and Lema??tre, 1998).
Given a fixed set of parameters for our algo-
rithm and an -optimality stopping criterion, we
451
RLT Re-
laxation
Avg. ms
per node
# Sam-
ples
Est. #
Nodes
Est. #
Hours
Obj.Filter 63 10000 3.2E+08 4.6E+09
Max.0k 6 10000 1.7E+10 7.8E+10
Max.1k 15 10000 3.5E+08 4.2E+09
Max.10k 161 10000 1.3E+09 3.4E+10
Max.100k 232259 5 1.7E+09 9.7E+13
Table 1: Branch-and-bound node count and com-
pletion time estimates. Each standard deviation
was close in magnitude to the estimate itself. We
ran for 8 hours, stopping at 10,000 samples on 8
synthetic sentences.
can view the branch-and-bound tree T as fixed and
finite in size. We wish to estimate some cost asso-
ciated with the tree C(T ) = ???nodes(T ) f(?).
Letting f(?) = 1 estimates the number of nodes;
if f(?) is the time to solve a node, then we es-
timate the total solution time using the Monte
Carlo method of Knuth (1975). Table 1 gives
these estimates, for the same five RLT relaxations.
Obj.Filter yields the smallest estimated tree size.
7.3 Real Data
In this section, we compare our global search
method to Viterbi EM with random restarts each
with or without posterior constraints. We use 200
sentences of no more than 10 tokens from the WSJ
portion of the Penn Treebank. We reduce the tree-
bank?s gold part-of-speech (POS) tags to a univer-
sal set of 12 tags (Petrov et al, 2012) plus a tag
for auxiliaries, ignoring punctuation. Each search
method is run for 8 hours. We obtain the initial
incumbent solution for branch-and-bound by run-
ning Viterbi EM for 45 minutes. The average time
to solve a node?s relaxation ranges from 3 seconds
for RLT Max.0k to 42 seconds for RLT Max.100k.
Figure 6a shows the log-likelihood of the in-
cumbent solution over time. In our global search
method, like Viterbi EM, the posterior constraints
lead to lower log-likelihoods. RLT Max.0k finds
the highest log-likelihood solution.
Figure 6b compares the unlabeled directed de-
pendency accuracy of the incumbent solution. In
both global and local search, the posterior con-
straints lead to higher accuracies. Viterbi EM
with posterior constraints demonstrates the oscil-
lation of incumbent accuracy: starting at 58.02%
accuracy, it finds several high accuracy solutions
early on (61.02%), but quickly abandons them to
increase likelihood, yielding a final accuracy of
60.65%. RLT Max.0k with posterior constraints
obtains the highest overall accuracy of 61.09% at
(a)
!3300
!3200
!3100
!3000
!2900
!
!
!
!
!
!!!
!
!!
!
!!
!
!
!!
!!
!
!!!!
!!!
! !!!
!
!
100 200 300 400
Time (min)
Lo
g!
lik
eli
ho
od
 (t
rai
n)
Algorithm
! Viterbi EM
RLT Obj.Filter
RLT Max.0k
RLT Max.1k
RLT Max.10k
RLT Max.100k
Posterior Constraints
False
True
(b)
0.35
0.40
0.45
0.50
0.55
0.60
!
!
!
!
!!
!!
!
!!
!!!
!
!
!!!
!
!!!
!!
!
! !
!!!
!
100 200 300 400
Time (min)
Ac
cu
rac
y (
tra
in)
Algorithm
! Viterbi EM
RLT Obj.Filter
RLT Max.0k
RLT Max.1k
RLT Max.10k
RLT Max.100k
Posterior Constraints
False
True
Figure 6: Likelihood (a) and accuracy (b) of in-
cumbent solution so far, on a small real dataset.
306 min and the highest final accuracy 60.73%.
8 Discussion
In principle, our branch-and-bound method can
approach -optimal solutions to Viterbi training of
locally normalized generative models, including
the NP-hard case of grammar induction with the
DMV. The method can also be used with posterior
constraints or a regularized objective.
Future work includes algorithmic improve-
ments for solving the relaxation and the develop-
ment of tighter relaxations. The Dantzig-Wolfe
decomposition (Dantzig and Wolfe, 1960) or La-
grangian Relaxation (Held and Karp, 1970) might
satisfy both of these goals by pushing the inte-
ger tree constraints into a subproblem solved by
a dynamic programming parser. Recent work on
semidefinite relaxations (Anstreicher, 2009) sug-
gests they may provide tighter bounds at the ex-
pense of greater computation time.
Perhaps even more important than tightening
the bounds at each node are search heuristics (e.g.,
surface cues) and priors (e.g., universal grammar)
that guide our global search by deciding which
node to expand next (Chomsky and Lasnik, 1993).
452
References
Tobias Achterberg. 2007. Constraint integer program-
ming. Ph.D. thesis, TU Berlin.
Warren P. Adams and Hanif D. Sherali. 1986. A
tight linearization and an algorithm for zero-one
quadratic programming problems. Management
Science, 32(10):1274?1290, October. ArticleType:
research-article / Full publication date: Oct., 1986 /
Copyright 1986 INFORMS.
Kurt Anstreicher. 2009. Semidefinite programming
versus the reformulation-linearization technique for
nonconvex quadratically constrained quadratic pro-
gramming. Journal of Global Optimization,
43(2):471?484.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
DeNero, John DeNero, and Dan Klein. 2010. Pain-
less unsupervised learning with features. In Proc. of
NAACL, June.
Samuel Burer and Dieter Vandenbussche. 2009. Glob-
ally solving box-constrained nonconvex quadratic
programs with semidefinite-based finite branch-and-
bound. Computational Optimization and Applica-
tions, 43(2):181?195.
Olivier Chapelle, Vikas Sindhwani, and S. Sathiya
Keerthi. 2007. Branch and bound for semi-
supervised support vector machines. In Proc. of
NIPS 19, pages 217?224. MIT Press.
E. Charniak. 1993. Statistical language learning. MIT
press.
Yunmei Chen and Xiaojing Ye. 2011. Projection onto
a simplex. arXiv:1101.6081, January.
Noam Chomsky and Howard Lasnik. 1993. Princi-
ples and parameters theory. In Syntax: An Interna-
tional Handbook of Contemporary Research. Berlin:
de Gruyter.
Shay Cohen and Noah A. Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proc. of HLT-
NAACL, pages 74?82, June.
Shay Cohen and Noah A. Smith. 2010. Viterbi training
for PCFGs: Hardness results and competitiveness of
uniform initialization. In Proc. of ACL, pages 1502?
1511, July.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2009. Lo-
gistic normal priors for unsupervised probabilistic
grammar induction. In Proceedings of NIPS.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable PCFGs. In Proc. of ACL (Volume 1:
Long Papers), pages 223?231. Association for Com-
putational Linguistics, July.
George B. Dantzig and Philip Wolfe. 1960. Decom-
position principle for linear programs. Operations
Research, 8(1):101?111, January.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of ACL, pages 457?
464, June.
Jennifer Gillenwater, Kuzman Ganchev, Joo Graa, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity
in dependency grammar induction. In Proceedings
of the ACL 2010 Conference Short Papers, pages
194?199. Association for Computational Linguis-
tics, July.
K. Gimpel and N. A. Smith. 2012. Concavity and ini-
tialization for unsupervised dependency parsing. In
Proc. of NAACL.
M. Held and R. M. Karp. 1970. The traveling-
salesman problem and minimum spanning trees.
Operations Research, 18(6):1138?1162.
D. Hsu, S. M Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden markov models.
In COLT 2009 - The 22nd Conference on Learning
Theory.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. of ACL, pages
478?485, July.
D. E. Knuth. 1975. Estimating the efficiency of
backtrack programs. Mathematics of computation,
29(129):121?136.
L. Lobjois and M. Lema??tre. 1998. Branch and bound
algorithm selection by performance prediction. In
Proc. of the National Conference on Artificial Intel-
ligence, pages 353?358.
Franco M. Luque, Ariadna Quattoni, Borja Balle, and
Xavier Carreras. 2012. Spectral learning for
non-deterministic dependency parsing. In Proc. of
EACL, pages 409?419, April.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal Trees. Center for Operations Research and
Econometrics.
Alexander Martin. 2000. Integer programs with block
structure. Technical Report SC-99-03, ZIB.
Andre? Martins, Noah A. Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proc. of ACL-IJCNLP,
pages 342?350, August.
Garth P. McCormick. 1976. Computability of global
solutions to factorable nonconvex programs: Part
I?Convex underestimating problems. Mathemati-
cal Programming, 10(1):147?175.
453
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proc. of
EMNLP, pages 1234?1244, October.
P. M. Pardalos. 1991. Global optimization algorithms
for linearly constrained indefinite quadratic prob-
lems. Computers & Mathematics with Applications,
21(6):87?97.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proc. of LREC.
Xian Qian and Yang Liu. 2013. Branch and bound al-
gorithm for dependency parsing with non-local fea-
tures. TACL, 1:37?48.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proc. of EMNLP, pages 129?
137, July.
Sebastian Riedel, David Smith, and Andrew McCal-
lum. 2012. Parse, price and cut?Delayed column
and row generation for graph based parsers. In Proc.
of EMNLP-CoNLL, pages 732?743, July.
Hanif D. Sherali and Warren P. Adams. 1990. A hi-
erarchy of relaxations between the continuous and
convex hull representations for zero-one program-
ming problems. SIAM Journal on Discrete Math-
ematics, 3(3):411?430, August.
H. Sherali and L. Liberti. 2008. Reformulation-
linearization technique for global optimization. En-
cyclopedia of Optimization, 2:3263?3268.
Hanif D. Sherali and Cihan H. Tuncbilek. 1995. A
reformulation-convexification approach for solving
nonconvex quadratic programming problems. Jour-
nal of Global Optimization, 7(1):1?31.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proc. of COLING-ACL, pages 569?576,
July.
N.A. Smith. 2006. Novel estimation methods for unsu-
pervised discovery of latent structure in natural lan-
guage text. Ph.D. thesis, Johns Hopkins University,
Baltimore, MD.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010a. From baby steps to leapfrog: How
Less is more in unsupervised dependency parsing.
In Proc. of HLT-NAACL, pages 751?759. Associa-
tion for Computational Linguistics, June.
Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D Manning. 2010b. Viterbi train-
ing improves unsupervised dependency parsing. In
Proc. of CoNLL, pages 9?17. Association for Com-
putational Linguistics, July.
S. A. Vavasis. 1991. Nonlinear optimization: com-
plexity issues. Oxford University Press, Inc.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2008. Semi-supervised convex training for de-
pendency parsing. In Proc of ACL-HLT, pages
532?540. Association for Computational Linguis-
tics, June.
454
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1177?1187,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Low-Resource Semantic Role Labeling
Matthew R. Gormley
1
Margaret Mitchell
2
Benjamin Van Durme
1
Mark Dredze
1
1
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD 21211
2
Microsoft Research
Redmond, WA 98052
mrg@cs.jhu.edu | memitc@microsoft.com | vandurme@cs.jhu.edu | mdredze@cs.jhu.edu
Abstract
We explore the extent to which high-
resource manual annotations such as tree-
banks are necessary for the task of se-
mantic role labeling (SRL). We examine
how performance changes without syntac-
tic supervision, comparing both joint and
pipelined methods to induce latent syn-
tax. This work highlights a new applica-
tion of unsupervised grammar induction
and demonstrates several approaches to
SRL in the absence of supervised syntax.
Our best models obtain competitive results
in the high-resource setting and state-of-
the-art results in the low resource setting,
reaching 72.48% F1 averaged across lan-
guages. We release our code for this work
along with a larger toolkit for specifying
arbitrary graphical structure.
1
1 Introduction
The goal of semantic role labeling (SRL) is to
identify predicates and arguments and label their
semantic contribution in a sentence. Such labeling
defines who did what to whom, when, where and
how. For example, in the sentence ?The kids ran
the marathon?, ran assigns a role to kids to denote
that they are the runners; and a role to marathon to
denote that it is the race course.
Models for SRL have increasingly come to rely
on an array of NLP tools (e.g., parsers, lem-
matizers) in order to obtain state-of-the-art re-
sults (Bj?orkelund et al, 2009; Zhao et al, 2009).
Each tool is typically trained on hand-annotated
data, thus placing SRL at the end of a very high-
resource NLP pipeline. However, richly annotated
data such as that provided in parsing treebanks is
expensive to produce, and may be tied to specific
domains (e.g., newswire). Many languages do
1
http://www.cs.jhu.edu/
?
mrg/software/
not have such supervised resources (low-resource
languages), which makes exploring SRL cross-
linguistically difficult.
The problem of SRL for low-resource lan-
guages is an important one to solve, as solutions
pave the way for a wide range of applications: Ac-
curate identification of the semantic roles of enti-
ties is a critical step for any application sensitive to
semantics, from information retrieval to machine
translation to question answering.
In this work, we explore models that minimize
the need for high-resource supervision. We ex-
amine approaches in a joint setting where we
marginalize over latent syntax to find the optimal
semantic role assignment; and a pipeline setting
where we first induce an unsupervised grammar.
We find that the joint approach is a viable alterna-
tive for making reasonable semantic role predic-
tions, outperforming the pipeline models. These
models can be effectively trained with access to
only SRL annotations, and mark a state-of-the-art
contribution for low-resource SRL.
To better understand the effect of the low-
resource grammars and features used in these
models, we further include comparisons with (1)
models that use higher-resource versions of the
same features; (2) state-of-the-art high resource
models; and (3) previous work on low-resource
grammar induction. In sum, this paper makes
several experimental and modeling contributions,
summarized below.
Experimental contributions:
? Comparison of pipeline and joint models for
SRL.
? Subtractive experiments that consider the re-
moval of supervised data.
? Analysis of the induced grammars in un-
supervised, distantly-supervised, and joint
training settings.
1177
Modeling contributions:
? Simpler joint CRF for syntactic and semantic
dependency parsing than previously reported.
? New application of unsupervised grammar
induction: low-resource SRL.
? Constrained grammar induction using SRL
for distant-supervision.
? Use of Brown clusters in place of POS tags
for low-resource SRL.
The pipeline models are introduced in ? 3.1 and
jointly-trained models for syntactic and semantic
dependencies (similar in form to Naradowsky et
al. (2012)) are introduced in ? 3.2. In the pipeline
models, we develop a novel approach to unsu-
pervised grammar induction and explore perfor-
mance using SRL as distant supervision. The joint
models use a non-loopy conditional random field
(CRF) with a global factor constraining latent syn-
tactic edge variables to form a tree. Efficient exact
marginal inference is possible by embedding a dy-
namic programming algorithm within belief prop-
agation as in Smith and Eisner (2008).
Even at the expense of no dependency path fea-
tures, the joint models best pipeline-trained mod-
els for state-of-the-art performance in the low-
resource setting (? 4.4). When the models have ac-
cess to observed syntactic trees, they achieve near
state-of-the-art accuracy in the high-resource set-
ting on some languages (? 4.3).
Examining the learning curve of the joint and
pipeline models in two languages demonstrates
that a small number of labeled SRL examples may
be essential for good end-task performance, but
that the choice of a good model for grammar in-
duction has an even greater impact.
2 Related Work
Our work builds upon research in both seman-
tic role labeling and unsupervised grammar in-
duction (Klein and Manning, 2004; Spitkovsky
et al, 2010a). Previous related approaches to se-
mantic role labeling include joint classification of
semantic arguments (Toutanova et al, 2005; Jo-
hansson and Nugues, 2008), latent syntax induc-
tion (Boxwell et al, 2011; Naradowsky et al,
2012), and feature engineering for SRL (Zhao et
al., 2009; Bj?orkelund et al, 2009).
Toutanova et al (2005) introduced one of
the first joint approaches for SRL and demon-
strated that a model that scores the full predicate-
argument structure of a parse tree could lead to
significant error reduction over independent clas-
sifiers for each predicate-argument relation.
Johansson and Nugues (2008) and Llu??s et al
(2013) extend this idea by coupling predictions of
a dependency parser with predictions from a se-
mantic role labeler. In the model from Johans-
son and Nugues (2008), the outputs from an SRL
pipeline are reranked based on the full predicate-
argument structure that they form. The candidate
set of syntactic-semantic structures is reranked us-
ing the probability of the syntactic tree and seman-
tic structure. Llu??s et al (2013) use a joint arc-
factored model that predicts full syntactic paths
along with predicate-argument structures via dual
decomposition.
Boxwell et al (2011) and Naradowsky et al
(2012) observe that syntax may be treated as la-
tent when a treebank is not available. Boxwell
et al (2011) describe a method for training a se-
mantic role labeler by extracting features from a
packed CCG parse chart, where the parse weights
are given by a simple ruleset. Naradowsky et
al. (2012) marginalize over latent syntactic depen-
dency parses.
Both Boxwell et al (2011) and Naradowsky
et al (2012) suggest methods for SRL without
supervised syntax, however, their features come
largely from supervised resources. Even in their
lowest resource setting, Boxwell et al (2011) re-
quire an oracle CCG tag dictionary extracted from
a treebank. Naradowsky et al (2012) limit their
exploration to a small set of basic features, and
included high-resource supervision in the form
of lemmas, POS tags, and morphology available
from the CoNLL 2009 data.
There has not yet been a comparison of tech-
niques for SRL that do not rely on a syntactic
treebank, and no exploration of probabilistic mod-
els for unsupervised grammar induction within an
SRL pipeline that we have been able to find.
Related work for the unsupervised learning of
dependency structures separately from semantic
roles primarily comes from Klein and Manning
(2004), who introduced the Dependency Model
with Valence (DMV). This is a robust generative
model that uses a head-outward process over word
classes, where heads generate arguments.
Spitkovsky et al (2010a) show that Viterbi
(hard) EM training of the DMV with simple uni-
form initialization of the model parameters yields
higher accuracy models than standard soft-EM
1178
  
ParsingModel SemanticDependencyModelCorpusText
Text LabeledWith SemanticRoles
Train Time, Constrained Grammar Induction:Observed Constraints
Figure 1: Pipeline approach to SRL. In this sim-
ple pipeline, the first stage syntactically parses the
corpus, and the second stage predicts semantic
predicate-argument structure for each sentence us-
ing the labels of the first stage as features. In our
low-resource pipelines, we assume that the syntac-
tic parser is given no labeled parses?however, it
may optionally utilize the semantic parses as dis-
tant supervision. Our experiments also consider
?longer? pipelines that include earlier stages: a
morphological analyzer, POS tagger, lemmatizer.
training. In Viterbi EM, the E-step finds the max-
imum likelihood corpus parse given the current
model parameters. The M-step then finds the
maximum likelihood parameters given the corpus
parse. We utilize this approach to produce unsu-
pervised syntactic features for the SRL task.
Grammar induction work has further demon-
strated that distant supervision in the form of
ACE-style relations (Naseem and Barzilay, 2011)
or HTML markup (Spitkovsky et al, 2010b)
can lead to considerable gains. Recent work in
fully unsupervised dependency parsing has sup-
planted these methods with even higher accuracies
(Spitkovsky et al, 2013) by arranging optimiz-
ers into networks that suggest informed restarts
based on previously identified local optima. We do
not reimplement these approaches within the SRL
pipeline here, but provide comparison of these
methods against our grammar induction approach
in isolation in ? 4.5.
In both pipeline and joint models, we use fea-
tures adapted from state-of-the-art approaches to
SRL. This includes Zhao et al (2009) features,
who use feature templates from combinations
of word properties, syntactic positions including
head and children, and semantic properties; and
features from Bj?orkelund et al (2009), who utilize
features on syntactic siblings and the dependency
path concatenated with the direction of each edge.
Features are described further in ? 3.3.
3 Approaches
We consider an array of models, varying:
1. Pipeline vs. joint training (Figures 1 and 2)
2. Types of supervision
3. The objective function at the level of syntax
3.1 Unsupervised Syntax in the Pipeline
Typical SRL systems are trained following a
pipeline where the first component is trained on
supervised data, and each subsequent component
is trained using the 1-best output of the previous
components. A typical pipeline consists of a POS
tagger, dependency parser, and semantic role la-
beler. In this section, we introduce pipelines that
remove the need for a supervised tagger and parser
by training in an unsupervised and distantly super-
vised fashion.
Brown Clusters We use fully unsupervised
Brown clusters (Brown et al, 1992) in place of
POS tags. Brown clusters have been used to good
effect for various NLP tasks such as named entity
recognition (Miller et al, 2004) and dependency
parsing (Koo et al, 2008; Spitkovsky et al, 2011).
The clusters are formed by a greedy hierachi-
cal clustering algorithm that finds an assignment
of words to classes by maximizing the likelihood
of the training data under a latent-class bigram
model. Each word type is assigned to a fine-
grained cluster at a leaf of the hierarchy of clusters.
Each cluster can be uniquely identified by the path
from the root cluster to that leaf. Representing this
path as a bit-string (with 1 indicating a left and 0
indicating a right child) allows a simple coarsen-
ing of the clusters by truncating the bit-strings. We
train 1000 Brown clusters for each of the CoNLL-
2009 languages on Wikipedia text.
2
Unsupervised Grammar Induction Our first
method for grammar induction is fully unsuper-
vised Viterbi EM training of the Dependency
Model with Valence (DMV) (Klein and Manning,
2004), with uniform initialization of the model pa-
rameters. We define the DMV such that it gener-
ates sequences of word classes: either POS tags
or Brown clusters as in Spitkovsky et al (2011).
The DMV is a simple generative model for pro-
jective dependency trees. Children are generated
recursively for each node. Conditioned on the par-
ent class, the direction (right or left), and the cur-
rent valence (first child or not), a coin is flipped to
decide whether to generate another child; the dis-
tribution over child classes is conditioned on only
the parent class and direction.
2
The Wikipedia text was tokenized for Polyglot (Al-Rfou?
et al, 2013): http://bit.ly/embeddings
1179
Constrained Grammar Induction Our second
method, which we will refer to as DMV+C, in-
duces grammar in a distantly supervised fashion
by using a constrained parser in the E-step of
Viterbi EM. Since the parser is part of a pipeline,
we constrain it to respect the downstream SRL an-
notations during training. At test time, the parser
is unconstrained.
Dependency-based semantic role labeling can
be described as a simple structured prediction
problem: the predicted structure is a labeled di-
rected graph, where nodes correspond to words
in the sentence. Each directed edge indicates that
there is a predicate-argument relationship between
the two words; the parent is the predicate and the
child the argument. The label on the edge indi-
cates the type of semantic relationship. Unlike
syntactic dependency parsing, the graph is not re-
quired to be a tree, nor even a connected graph.
Self-loops and crossing arcs are permitted.
The constrained syntactic DMV parser treats
the semantic graph as observed, and constrains the
syntactic parent to be chosen from one of the se-
mantic parents, if there are any. In some cases,
imposing this constraint would not permit any pro-
jective dependency parses?in this case, we ignore
the semantic constraint for that sentence. We parse
with the CKY algorithm (Younger, 1967; Aho and
Ullman, 1972) by utilizing a PCFG corresponding
to the DMV (Cohn et al, 2010). Each chart cell al-
lows only non-terminals compatible with the con-
strained sets. This can be viewed as a variation of
Pereira and Schabes (1992).
Semantic Dependency Model As described
above, semantic role labeling can be cast as a
structured prediction problem where the structure
is a labeled semantic dependency graph. We de-
fine a conditional random field (CRF) (Lafferty et
al., 2001) for this task. Because each word in a
sentence may be in a semantic relationship with
any other word (including itself), a sentence of
length n has n
2
possible edges. We define a single
L+1-ary variable for each edge, whose value can
be any of L semantic labels or a special label indi-
cating there is no predicate-argument relationship
between the two words. In this way, we jointly
perform identification (determining whether a se-
mantic relationship exists) and classification (de-
termining the semantic label). This use of an L+1-
ary variable is in contrast to the model of Narad-
owsky et al (2012), which used a more complex
  
DEPTREE
Dep
1,1
Role
1,1
Role
1,2
Role
1,3
Role
n,n
Dep
1,2
Dep
1,3
Dep
n,n ...
 ...
Figure 2: Factor graph for the joint syntac-
tic/semantic dependency parsing model.
set of binary variables and required a constraint
factor permitting AT-MOST-ONE. We include one
unary factor for each variable.
We optionally include additional variables that
perform word sense disambiguation for each pred-
icate. Each has a unary factor and is completely
disconnected from the semantic edge (similar to
Naradowsky et al (2012)). These variables range
over all the predicate senses observed in the train-
ing data for the lemma of that predicate.
3.2 Joint Syntactic and Semantic Parsing
Model
In Section 3.1, we introduced pipeline-trained
models for SRL, which used grammar induction
to predict unlabeled syntactic parses. In this sec-
tion, we define a simple model for joint syntactic
and semantic dependency parsing.
This model extends the CRF model in Section
3.1 to include the projective syntactic dependency
parse for a sentence. This is done by includ-
ing an additional n
2
binary variables that indicate
whether or not a directed syntactic dependency
edge exists between a pair of words in the sen-
tence. Unlike the semantic dependencies, these
syntactic variables must be coupled so that they
produce a projective dependency parse; this re-
quires an additional global constraint factor to en-
sure that this is the case (Smith and Eisner, 2008).
The constraint factor touches all n
2
syntactic-edge
variables, and multiplies in 1.0 if they form a pro-
jective dependency parse, and 0.0 otherwise. We
couple each syntactic edge variable to its semantic
edge variable with a binary factor. Figure 2 shows
the factor graph for this joint model.
Note that our factor graph does not contain any
loops, thereby permitting efficient exact marginal
inference just as in Naradowsky et al (2012). We
1180
Property Possible values
1 word form all word forms
2 lower case word form all lower-case forms
3 5-char word form prefixes all 5-char form prefixes
4 capitalization True, False
5 top-800 word form top-800 word forms
6 brown cluster 000, 1100, 010110001, ...
7 brown cluster, length 5 length 5 prefixes of brown clusters
8 lemma all word lemmas
9 POS tag NNP, CD, JJ, DT, ...
10 morphological features Gender, Case, Number, ...
(different across languages)
11 dependency label SBJ, NMOD, LOC, ...
12 edge direction Up, Down
Table 1: Word and edge properties in templates.
i, i-1, i+1 noFarChildren(w
i
) linePath(w
p
, w
c
)
parent(w
i
) rightNearSib(w
i
) depPath(w
p
, w
c
)
allChildren(w
i
) leftNearSib(w
i
) depPath(w
p
, w
lca
)
rightNearChild(w
i
) firstVSupp(w
i
) depPath(w
c
, w
lca
)
rightFarChild(w
i
) lastVSupp(w
i
) depPath(w
lca
, w
root
)
leftNearChild(w
i
) firstNSupp(w
i
)
leftFarChild(w
i
) lastNSupp(w
i
)
Table 2: Word positions used in templates. Based
on current word position (i), positions related to
current word w
i
, possible parent, child (w
p
, w
c
),
lowest common ancestor between parent/child
(w
lca
), and syntactic root (w
root
).
train our CRF models by maximizing conditional
log-likelihood using stochastic gradient descent
with an adaptive learning rate (AdaGrad) (Duchi
et al, 2011) over mini-batches.
The unary and binary factors are defined with
exponential family potentials. In the next section,
we consider binary features of the observations
(the sentence and labels from previous pipeline
stages) which are conjoined with the state of the
variables in the factor.
3.3 Features for CRF Models
Our feature design stems from two key ideas.
First, for SRL, it has been observed that fea-
ture bigrams (the concatenation of simple fea-
tures such as a predicate?s POS tag and an ar-
gument?s word) are important for state-of-the-art
(Zhao et al, 2009; Bj?orkelund et al, 2009). Sec-
ond, for syntactic dependency parsing, combining
Brown cluster features with word forms or POS
tags yields high accuracy even with little training
data (Koo et al, 2008).
We create binary indicator features for each
model using feature templates. Our feature tem-
plate definitions build from those used by the top
performing systems in the CoNLL-2009 Shared
Task, Zhao et al (2009) and Bj?orkelund et al
(2009) and from features in syntactic dependency
parsing (McDonald et al, 2005; Koo et al, 2008).
Template Possible values
relative position before, after, on
distance, continuity Z
+
binned distance > 2, 5, 10, 20, 30, or 40
geneological relationship parent, child, ancestor, descendant
path-grams the NN went
Table 3: Additional standalone templates.
Template Creation Feature templates are de-
fined over triples of ?property, positions, order?.
Properties, listed in Table 1, are extracted from
word positions within the sentence, shown in Ta-
ble 2. Single positions for a word w
i
include
its syntactic parent, its leftmost farthest child
(leftFarChild), its rightmost nearest sibling (rightNearSib),
etc. Following Zhao et al (2009), we include the
notion of verb and noun supports and sections of
the dependency path. Also following Zhao et al
(2009), properties from a set of positions can be
put together in three possible orders: as the given
sequence, as a sorted list of unique strings, and re-
moving all duplicated neighbored strings. We con-
sider both template unigrams and bigrams, com-
bining two templates in sequence.
Additional templates we include are the relative
position (Bj?orkelund et al, 2009), geneological re-
lationship, distance (Zhao et al, 2009), and binned
distance (Koo et al, 2008) between two words in
the path. From Llu??s et al (2013), we use 1, 2, 3-
gram path features of words/POS tags (path-grams),
and the number of non-consecutive token pairs in
a predicate-argument path (continuity).
3.4 Feature Selection
Constructing all feature template unigrams and bi-
grams would yield an unwieldy number of fea-
tures. We therefore determine the top N template
bigrams for a dataset and factor a according to an
information gain measure (Martins et al, 2011):
IG
a,m
=
?
f?T
m
?
x
a
p(f, x
a
) log
2
p(f, x
a
)
p(f)p(x
a
)
where T
m
is the mth feature template, f is a par-
ticular instantiation of that template, and x
a
is an
assignment to the variables in factor a. The proba-
bilities are empirical estimates computed from the
training data. This is simply the mutual informa-
tion of the feature template instantiation with the
variable assignment.
This filtering approach was treated as a sim-
ple baseline in Martins et al (2011) to contrast
with increasingly popular gradient based regular-
ization approaches. Unlike the gradient based ap-
1181
proaches, this filtering approach easily scales to
many features since we can decompose the mem-
ory usage over feature templates.
As an additional speedup, we reduce the dimen-
sionality of our feature space to 1 million for each
clique using a common trick referred to as fea-
ture hashing (Weinberger et al, 2009): we map
each feature instantiation to an integer using a hash
function
3
modulo the desired dimentionality.
4 Experiments
We are interested in the effects of varied super-
vision using pipeline and joint training for SRL.
To compare to prior work (i.e., submissions to the
CoNLL-2009 Shared Task), we also consider the
joint task of semantic role labeling and predicate
sense disambiguation. Our experiments are sub-
tractive, beginning with all supervision available
and then successively removing (a) dependency
syntax, (b) morphological features, (c) POS tags,
and (d) lemmas. Dependency syntax is the most
expensive and difficult to obtain of these various
forms of supervision. We explore the importance
of both the labels and structure, and what quantity
of supervision is useful.
4.1 Data
The CoNLL-2009 Shared Task (Haji?c et al, 2009)
dataset contains POS tags, lemmas, morpholog-
ical features, syntactic dependencies, predicate
senses, and semantic roles annotations for 7 lan-
guages: Catalan, Chinese, Czech, English, Ger-
man, Japanese,
4
Spanish. The CoNLL-2005 and
-2008 Shared Task datasets provide English SRL
annotation, and for cross dataset comparability we
consider only verbal predicates (more details in
? 4.4). To compare with prior approaches that use
semantic supervision for grammar induction, we
utilize Section 23 of the WSJ portion of the Penn
Treebank (Marcus et al, 1993).
4.2 Feature Template Sets
Our primary feature set IG
C
consists of 127 tem-
plate unigrams that emphasize coarse properties
(i.e., properties 7, 9, and 11 in Table 1). We also
explore the 31 template unigrams
5
IG
B
described
3
To reduce hash collisions, We use MurmurHash v3
https://code.google.com/p/smhasher.
4
We do not report results on Japanese as that data was
only made freely available to researchers that competed in
CoNLL 2009.
5
Because we do not include a binary factor between pred-
icate sense and semantic role, we do not include sense as a
by Bj?orkelund et al (2009). Each of IG
C
and IG
B
also include 32 template bigrams selected by in-
formation gain on 1000 sentences?we select a
different set of template bigrams for each dataset.
We compare against the language-specific fea-
ture sets detailed in the literature on high-resource
top-performing SRL systems: From Bj?orkelund et
al. (2009), these are feature sets for German, En-
glish, Spanish and Chinese, obtained by weeks of
forward selection (B
de,en,es,zh
); and from Zhao et
al. (2009), these are features for Catalan Z
ca
.
6
4.3 High-resource SRL
We first compare our models trained as a pipeline,
using all available supervision (syntax, morphol-
ogy, POS tags, lemmas) from the CoNLL-2009
data. Table 4(a) shows the results of our model
with gold syntax and a richer feature set than
that of Naradowsky et al (2012), which only
looked at whether a syntactic dependency edge
was present. This highlights an important advan-
tage of the pipeline trained model: the features can
consider any part of the syntax (e.g., arbitrary sub-
trees), whereas the joint model is limited to those
features over which it can efficiently marginalize
(e.g., short dependency paths). This holds true
even in the pipeline setting where no syntactic su-
pervision is available.
Table 4(b) contrasts our high-resource results
for the task of SRL and sense disambiguation
with the top systems in the CoNLL-2009 Shared
Task, giving further insight into the performance
of the simple information gain feature selection
technique. With supervised syntax, our sim-
ple information gain feature selection technique
(? 3.4) performs admirably. However, the orig-
inal unigram Bj?orkelund features (B
de,en,es,zh
),
which were tuned for a high-resource model, ob-
tain higher F1 than our information gain set us-
ing the same features in unigram and bigram tem-
plates (IG
B
). This suggests that further work on
feature selection may improve the results. We
find that IG
B
obtain higher F1 than the original
Bj?orkelund feature sets (B
de,en,es,zh
) in the low-
resource pipeline setting with constrained gram-
mar induction (DMV+C).
feature for argument prediction.
6
This covers all CoNLL languages but Czech, where fea-
ture sets were not made publicly available in either work. In
Czech, we disallowed template bigrams involving path-grams.
1182
(a)
(b)
(c)
SRL Approach Feature Set Dep. Parser Avg. ca cs de en es zh
Pipeline IG
C
Gold 84.98 84.97 87.65 79.14 86.54 84.22 87.35
Pipeline IG
B
Gold 84.74 85.15 86.64 79.50 85.77 84.40 86.95
Naradowsky et al (2012) Gold 72.73 69.59 74.84 66.49 78.55 68.93 77.97
Bj?orkelund et al (2009) Supervised 81.55 80.01 85.41 79.71 85.63 79.91 78.60
Zhao et al (2009) Supervised 80.85 80.32 85.19 75.99 85.44 80.46 77.72
Pipeline IG
C
Supervised 78.03 76.24 83.34 74.19 81.96 76.12 76.35
Pipeline Z
ca
Supervised *77.62 77.62 ? ? ? ? ?
Pipeline B
de,en,es,zh
Supervised *76.49 ? ? 72.17 81.15 76.65 75.99
Pipeline IG
B
Supervised 75.68 74.59 81.61 69.08 78.86 74.51 75.44
Joint IG
C
Marginalized 72.48 71.35 81.03 65.15 76.16 71.03 70.14
Joint IG
B
Marginalized 72.40 71.55 80.04 64.80 75.57 71.21 71.21
Naradowsky et al (2012) Marginalized 71.27 67.99 73.16 67.26 76.12 66.74 76.32
Pipeline IG
C
DMV+C (bc) 70.08 68.21 79.63 62.25 73.81 68.73 67.86
Pipeline Z
ca
DMV+C (bc) *69.67 69.67 ? ? ? ? ?
Pipeline IG
C
DMV (bc) 69.26 68.04 79.58 58.47 74.78 68.36 66.35
Pipeline IG
B
DMV (bc) 66.81 63.31 77.38 59.91 72.02 65.96 62.28
Pipeline IG
B
DMV+C (bc) 65.61 61.89 77.48 58.97 69.11 63.31 62.92
Pipeline B
de,en,es,zh
DMV+C (bc) *63.06 ? ? 57.75 68.32 63.70 62.45
Table 4: Test F1 for SRL and sense disambiguation on CoNLL?09 in high-resource and low-resource
settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are
ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
*Indicates partial averages for the language-specific feature sets (Z
ca
and B
de,en,es,zh
), for which we show results only on the
languages for which the sets were publicly available.
train
test
2008
heads
2005
spans
2005
spans
(oracle
tree)
X PRY?08
2
0
0
5
s
p
a
n
s
84.32 79.44
 B?11 (tdc) ? 71.5
 B?11 (td) ? 65.0
X JN?08
2
0
0
8
h
e
a
d
s
85.93 79.90
 Joint, IG
C
72.9 35.0 72.0
 Joint, IG
B
67.3 37.8 67.1
Table 5: F1 for SRL approaches (without sense
disambiguation) in matched and mismatched
train/test settings for CoNLL 2005 span and 2008
head supervision. We contrast low-resource ()
and high-resource settings (X), where latter uses a
treebank. See ? 4.4 for caveats to this comparison.
4.4 Low-Resource SRL
CoNLL-2009 Table 4(c) includes results for our
low-resource approaches and Naradowsky et al
(2012) on predicting semantic roles as well as
sense. In the low-resource setting of the CoNLL-
2009 Shared task without syntactic supervision,
our joint model (Joint) with marginalized syntax
obtains state-of-the-art results with features IG
C
described in ? 4.2. This model outperforms prior
work (Naradowsky et al, 2012) and our pipeline
model (Pipeline) with contrained (DMV+C) and
unconstrained grammar induction (DMV) trained
on brown clusters (bc).
In the low-resource setting, training and decod-
ing times for the pipeline and joint methods are
similar as computation time tends to be dominated
by feature extraction.
These results begin to answer a key research
question in this work: The joint models outper-
form the pipeline models in the low-resource set-
ting. This holds even when using the same feature
selection process. Further, the best-performing
low-resource features found in this work are those
based on coarse feature templates and selected
by information gain. Templates for these fea-
tures generalize well to the high-resource setting.
However, analysis of the induced grammars in
the pipeline setting suggests that the book is not
closed on the issue. We return to this in ? 4.5.
CoNLL-2008, -2005 To finish out comparisons
with state-of-the-art SRL, we contrast our ap-
proach with that of Boxwell et al (2011), who
evaluate on SRL in isolation (without sense disam-
biguation, as in CoNLL-2009). They report results
on Prop-CCGbank (Boxwell and White, 2008),
which uses the same training/testing splits as the
CoNLL-2005 Shared Task. Their results are there-
fore loosely
7
comparable to results on the CoNLL-
2005 dataset, which we can compare here.
There is an additional complication in com-
paring SRL approaches directly: The CoNLL-
2005 dataset defines arguments as spans instead of
7
The comparison is imperfect for two reasons: first, the
CCGBank contains only 99.44% of the original PTB sen-
tences (Hockenmaier and Steedman, 2007); second, because
PropBank was annotated over CFGs, after converting to CCG
only 99.977% of the argument spans were exact matches
(Boxwell and White, 2008). However, this comparison was
adopted by Boxwell et al (2011), so we use it here.
1183
heads, which runs counter to our head-based syn-
tactic representation. This creates a mismatched
train/test scenario: we must train our model to pre-
dict argument heads, but then test on our models
ability to predict argument spans.
8
We therefore
train our models on the CoNLL-2008 argument
heads,
9
and post-process and convert from heads
to spans using the conversion algorithm available
from Johansson and Nugues (2008).
10
The heads
are either from an MBR tree or an oracle tree. This
gives Boxwell et al (2011) the advantage, since
our syntactic dependency parses are optimized to
pick out semantic argument heads, not spans.
Table 5 presents our results. Boxwell et al
(2011) (B?11) uses additional supervision in the
form of a CCG tag dictionary derived from su-
pervised data with (tdc) and without (tc) a cut-
off. Our model does very poorly on the ?05 span-
based evaluation because the constituent bracket-
ing of the marginalized trees are inaccurate. This
is elucidated by instead evaluating on the ora-
cle spans, where our F1 scores are higher than
Boxwell et al (2011). We also contrast with rela-
vant high-resource methods with span/head con-
versions from Johansson and Nugues (2008): Pun-
yakanok et al (2008) (PRY?08) and Johansson and
Nugues (2008) (JN?08).
Subtractive Study In our subsequent experi-
ments, we study the effectiveness of our models
as the available supervision is decreased. We in-
crementally remove dependency syntax, morpho-
logical features, POS tags, then lemmas. For these
experiments, we utilize the coarse-grained feature
set (IG
C
), which includes Brown clusters.
Across languages, we find the largest drop in
F1 when we remove POS tags; and we find a
gain in F1 when we remove lemmas. This indi-
cates that lemmas, which are a high-resource an-
notation, may not provide a significant benefit for
this task. The effect of removing morphological
features is different across languages, with little
change in performance for Catalan and Spanish,
8
We were unable to obtain the system output of Boxwell
et al (2011) in order to convert their spans to dependencies
and evaluate the other mismatched train/test setting.
9
CoNLL-2005, -2008, and -2009 were derived from Prop-
Bank and share the same source text; -2008 and -2009 use
argument heads.
10
Specifically, we use their Algorithm 2, which produces
the span dominated by each argument, with special handling
of the case when the argument head dominates that of the
predicate. Also following Johansson and Nugues (2008), we
recover the ?05 sentences missing from the ?08 evaluation set.
Rem #FT ca de es
? 127+32 74.46 72.62 74.23
Dep 40+32 67.43 64.24 67.18
Mor 30+32 67.84 59.78 66.94
POS 23+32 64.40 54.68 62.71
Lem 21+32 64.85 54.89 63.80
Table 6: Subtractive experiments. Each row con-
tains the F1 for SRL only (without sense disam-
biguation) where the supervision type of that row
and all above it have been removed. Removed su-
pervision types (Rem) are: syntactic dependencies
(Dep), morphology (Mor), POS tags (POS), and
lemmas (Lem). #FT indicates the number of fea-
ture templates used (unigrams+bigrams).
20
30
40
50
60
70
0 20000 40000 60000
Number of Training Sentences
Lab
eled
 F1
Language / Dependency Parser
Catalan / Marginalized
Catalan / DMV+C
German / Marginalized
German / DMV+C
Figure 3: Learning curve for semantic dependency
supervision in Catalan and German. F1 of SRL
only (without sense disambiguation) shown as the
number of training sentences is increased.
but a drop in performance for German. This may
reflect a difference between the languages, or may
reflect the difference between the annotation of the
languages: both the Catalan and Spanish data orig-
inated from the Ancora project,
11
while the Ger-
man data came from another source.
Figure 3 contains the learning curve for SRL su-
pervision in our lowest resource setting for two
example languages, Catalan and German. This
shows how F1 of SRL changes as we adjust
the number of training examples. We find that
the joint training approach to grammar induction
yields consistently higher SRL performance than
its distantly supervised counterpart.
4.5 Analysis of Grammar Induction
Table 7 shows grammar induction accuracy in
low-resource settings. We find that the gap be-
tween the supervised parser and the unsupervised
methods is quite large, despite the reasonable ac-
curacy both methods achieve for the SRL end task.
11
http://clic.ub.edu/corpus/ancora
1184
Dependency
Parser
Avg. ca cs de en es zh
Supervised* 87.1 89.4 85.3 89.6 88.4 89.2 80.7
DMV (pos) 30.2 45.3 22.7 20.9 32.9 41.9 17.2
DMV (bc) 22.1 18.8 32.8 19.6 22.4 20.5 18.6
DMV+C (pos) 37.5 50.2 34.9 21.5 36.9 49.8 32.0
DMV+C (bc) 40.2 46.3 37.5 28.7 40.6 50.4 37.5
Marginal, IG
C
43.8 50.3 45.8 27.2 44.2 46.3 48.5
Marginal, IG
B
50.2 52.4 43.4 41.3 52.6 55.2 56.2
Table 7: Unlabeled directed dependency accuracy
on CoNLL?09 test set in low-resource settings.
DMV models are trained on either POS tags (pos)
or Brown clusters (bc). *Indicates the supervised parser
outputs provided by the CoNLL?09 Shared Task.
WSJ
?
Distant
Supervision
SAJM?10 44.8 none
SAJ?13 64.4 none
SJA?10 50.4 HTML
NB?11 59.4 ACE05
DMV (bc) 24.8 none
DMV+C (bc) 44.8 SRL
Marginalized, IG
C
48.8 SRL
Marginalized, IG
B
58.9 SRL
Table 8: Comparison of grammar induction ap-
proaches. We contrast the DMV trained with
Viterbi EM+uniform initialization (DMV), our
constrained DMV (DMV+C), and our model?s
MBR decoding of latent syntax (Marginalized)
with other recent work: Spitkovsky et al (2010a)
(SAJM?10), Spitkovsky et al (2010b) (SJA?10),
Naseem and Barzilay (2011) (NB?11), and the CS
model of Spitkovsky et al (2013) (SAJ?13).
This suggests that refining the low-resource gram-
mar induction methods may lead to gains in SRL.
Interestingly, the marginalized grammars best
the DMV grammar induction method; however,
this difference is less pronounced when the DMV
is constrained using SRL labels as distant super-
vision. This could indicate that a better model for
grammar induction would result in better perfor-
mance for SRL. We therefore turn to an analysis of
other approaches to grammar induction in Table 8,
evaluated on the Penn Treebank. We contrast with
methods using distant supervision (Naseem and
Barzilay, 2011; Spitkovsky et al, 2010b) and fully
unsupervised dependency parsing (Spitkovsky et
al., 2013). Following prior work, we exclude
punctuation from evaluation and convert the con-
stituency trees to dependencies.
12
The approach from Spitkovsky et al (2013)
12
Naseem and Barzilay (2011) and our results use the
Penn converter (Pierre and Heiki-Jaan, 2007). Spitkovsky et
al. (2010b; 2013) use Collins (1999) head percolation rules.
(SAJ?13) outperforms all other approaches, in-
cluding our marginalized settings. We therefore
may be able to achieve further gains in the pipeline
model by considering better models of latent syn-
tax, or better search techniques that break out
of local optima. Similarly, improving the non-
convex optimization of our latent-variable CRF
(Marginalized) may offer further gains.
5 Discussion and Future Work
We have compared various approaches for low-
resource semantic role labeling at the state-of-the-
art level. We find that we can outperform prior
work in the low-resource setting by coupling the
selection of feature templates based on informa-
tion gain with a joint model that marginalizes over
latent syntax.
We utilize unlabeled data in both generative and
discriminative models for dependency syntax and
in generative word clustering. Our discriminative
joint models treat latent syntax as a structured-
feature to be optimized for the end-task of SRL,
while our other grammar induction techniques op-
timize for unlabeled data likelihood?optionally
with distant supervision. We observe that careful
use of these unlabeled data resources can improve
performance on the end task.
Our subtractive experiments suggest that lemma
annotations, a high-resource annotation, may not
provide a large benefit for SRL. Our grammar in-
duction analysis indicates that relatively low accu-
racy can still result in reasonable SRL predictions;
still, the models do not outperform those that use
supervised syntax, and we aim to explore how well
the pipeline models in particular improve when we
apply higher accuracy unsupervised grammar in-
duction techniques.
We have utilized well studied datasets in order
to best understand the quality of our models rela-
tive to prior work. In future work, we hope to ex-
plore the effectiveness of our approaches on truly
low resource settings by using crowdsourcing to
develop semantic role datasets in other languages
and domains.
Acknowledgments We thank Richard Johans-
son, Dennis Mehay, and Stephen Boxwell for help
with data. We also thank Jason Naradowsky, Jason
Eisner, and anonymous reviewers for comments
on the paper.
1185
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling.
Prentice-Hall, Inc.
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of the 17th
Conference on Computational Natural Language
Learning (CoNLL 2013). Association for Computa-
tional Linguistics.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task. Association for Computational Linguistics.
Stephen Boxwell and Michael White. 2008. Project-
ing propbank roles onto the CCGbank. In Proceed-
ings of the International Conference on Language
Resources and Evaluation (LREC 2008). European
Language Resources Association.
Stephen Boxwell, Chris Brew, Jason Baldridge, Dennis
Mehay, and Sujith Ravi. 2011. Semantic role label-
ing without treebanks? In Proceedings of the 5th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP). Asian Federation of Natural
Language Processing.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4).
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. The
Journal of Machine Learning Research, 11.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL 2009): Shared Task. Association for
Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008). Association for Computational Lin-
guistics.
Dan Klein and Christopher Manning. 2004. Corpus-
Based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL 2004). Association for Computa-
tional Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT. Association for
Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Ma-
chine Learning (ICML 2001). Morgan Kaufmann.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez.
2013. Joint arc-factored parsing of syntactic and se-
mantic dependencies. Transactions of the Associa-
tion for Computational Linguistics (TACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The Penn Treebank. Com-
putational linguistics, 19(2).
Andre Martins, Noah Smith, Mario Figueiredo, and
Pedro Aguiar. 2011. Structured sparsity in struc-
tured prediction. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011). Association for Compu-
tational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005).
Association for Computational Linguistics.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Susan Dumais, Daniel
Marcu, and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings. Association for Compu-
tational Linguistics.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization of
hidden syntactic structure. In Proceedings of the
2012 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2012). Association
for Computational Linguistics.
Tahira Naseem and Regina Barzilay. 2011. Using
semantic cues to learn syntax. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence
(AAAI 2011). AAAI Press.
1186
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting of
the Association for Computational Linguistics (ACL
1992).
Nugues Pierre and Kalep Heiki-Jaan. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. NODALIDA 2007 Proceedings.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Juraf-
sky, and Christopher D Manning. 2010a. Viterbi
training improves unsupervised dependency parsing.
In Proceedings of the 14th Conference on Computa-
tional Natural Language Learning (CoNLL 2010).
Association for Computational Linguistics.
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010b. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2010). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
tags. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011). Association for Computational Lin-
guistics.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2013). Association for
Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics (ACL 2005). Association for Computational Lin-
guistics.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In L?eon
Bottou and Michael Littman, editors, Proceedings
of the 26th Annual International Conference on Ma-
chine Learning (ICML 2009). Omnipress.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n
3
. Information and
Control, 10(2).
Hai Zhao, Wenliang Chen, Chunyu Kity, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task. Association for Com-
putational Linguistics.
1187
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 9?10,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
Structured Belief Propagation for NLP
Matthew R. Gormley Jason Eisner
Department of Computer Science
Johns Hopkins University, Baltimore, MD
{mrg,jason}@cs.jhu.edu
1 Tutorial Overview
Statistical natural language processing relies on
probabilistic models of linguistic structure. More
complex models can help capture our intuitions
about language, by adding linguistically meaning-
ful interactions and latent variables. However, in-
ference and learning in the models we want often
poses a serious computational challenge.
Belief propagation (BP) and its variants pro-
vide an attractive approximate solution, especially
using recent training methods. These approaches
can handle joint models of interacting compo-
nents, are computationally efficient, and have ex-
tended the state-of-the-art on a number of com-
mon NLP tasks, including dependency parsing,
modeling of morphological paradigms, CCG pars-
ing, phrase extraction, semantic role labeling, and
information extraction (Smith and Eisner, 2008;
Dreyer and Eisner, 2009; Auli and Lopez, 2011;
Burkett and Klein, 2012; Naradowsky et al., 2012;
Stoyanov and Eisner, 2012).
This tutorial delves into BP with an emphasis on
recent advances that enable state-of-the-art perfor-
mance in a variety of tasks. Our goal is to eluci-
date how these approaches can easily be applied
to new problems. We also cover the theory under-
lying them. Our target audience is researchers in
human language technologies; we do not assume
familarity with BP.
In the first three sections, we discuss applica-
tions of BP to NLP problems, the basics of mod-
eling with factor graphs and message passing, and
the theoretical underpinnings of ?what BP is do-
ing? and how it relates to other variational infer-
ence techniques. In the second three sections, we
cover key extensions to the standard BP algorithm
to enable modeling of linguistic structure, efficient
inference, and approximation-aware training. We
survey a variety of software tools and introduce a
new software framework that incorporates many
of the modern approaches covered in this tutorial.
2 Outline
1. Applications [15 min., Eisner]
? Intro: Modeling with factor graphs
? Morphological paradigms
? Dependency and constituency parsing
? Alignment; Phrase extraction
? Relation extraction; Semantic role labeling
? Targeted sentiment
? Joint models for NLP
2. Belief Propagation Basics [40 min., Eisner]
? Messages and beliefs
? Sum-product, max-product, and determin-
istic annealing
? Relation to forward-backward and inside-
outside
? Acyclic vs. loopy graphs
? Synchronous vs. asynchronous propaga-
tion
3. Theory [25 min., Gormley]
? From arc consistency to BP
? From Gibbs sampling to particle BP to BP
? Other message-passing algorithms
? Bethe free energy
? Connection to PFCGs and FSMs
4. Incorporating Structure into Factors and Vari-
ables [30 min., Gormley]
? Embedding dynamic programs (e.g.
inside-outside) within factors
? String-valued and tree-valued variables
5. Message approximation and scheduling [20
min., Eisner]
? Pruning messages
? Variational approximations
? Residual BP and new variants
6. Approximation-aware Training [30 min., Gorm-
ley]
? Empirical risk minimization under approx-
imations (ERMA)
? BP as a computational expression graph
? Automatic differentiation (AD)
7. Software [10 min., Gormley]
9
3 Instructors
Matt Gormley is a PhD student at Johns Hopkins
University working with Mark Dredze and Jason
Eisner. His current research focuses on joint mod-
eling of multiple linguistic strata in learning set-
tings where supervised resources are scarce. He
has authored papers in a variety of areas including
topic modeling, global optimization, semantic role
labeling, and grammar induction.
Jason Eisner is an Associate Professor in Com-
puter Science and Cognitive Science at Johns
Hopkins University, where he has received two
school-wide awards for excellence in teaching.
His 80+ papers have presented many models and
algorithms spanning numerous areas of NLP. His
goal is to develop the probabilistic modeling, in-
ference, and learning techniques needed for a uni-
fied model of all kinds of linguistic structure. In
particular, he and his students introduced struc-
tured belief propagation, which integrates classi-
cal NLP models and their associated dynamic pro-
gramming algorithms, as well as loss-calibrated
training for use with belief propagation.
References
Michael Auli and Adam Lopez. 2011. A compari-
son of loopy belief propagation and dual decompo-
sition for integrated CCG supertagging and parsing.
In Proceedings of ACL.
David Burkett and Dan Klein. 2012. Fast inference in
phrase extraction models with belief propagation. In
Proceedings of NAACL.
Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In Proceedings of
EMNLP.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization
of hidden syntactic structure. In Proceedings of
EMNLP 2012.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
Veselin Stoyanov and Jason Eisner. 2012. Minimum-
risk training of approximate CRF-Based NLP sys-
tems. In Proceedings of NAACL-HLT.
10
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 204?207,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Non-Expert Correction of Automatically Generated Relation Annotations
Matthew R. Gormley?? and Adam Gerber?? and Mary Harper?? and Mark Dredze ??
?Human Language Technology Center of Excellence
?Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21211, USA
?Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742 USA
mrg@cs.jhu.edu,adam.gerber@jhu.edu,mharper@umd.edu,mdredze@cs.jhu.edu
Abstract
We explore a new way to collect human an-
notated relations in text using Amazon Me-
chanical Turk. Given a knowledge base of
relations and a corpus, we identify sentences
which mention both an entity and an attribute
that have some relation in the knowledge base.
Each noisy sentence/relation pair is presented
to multiple turkers, who are asked whether the
sentence expresses the relation. We describe
a design which encourages user efficiency and
aids discovery of cheating. We also present
results on inter-annotator agreement.
1 Introduction
Relation extraction (RE) is the task of determining
the existence and type of relation between two tex-
tual entity mentions. Slot filling, a general form of
relation extraction, includes relations between non-
entities, such as a person and an occupation, age, or
cause of death (McNamee and Dang, 2009).
RE annotated data, such as ACE (2008), is expen-
sive to produce so systems take different approaches
to minimizing data needs. For example, tree kernels
can reduce feature sparsity and generalize across
many examples (GuoDong et al, 2007; Zhou et
al., 2009). Distant supervision automatically gen-
erates noisy training examples from a knowledge
base (KB) without needing annotations (Bunescu
and Mooney, 2007; Mintz et al, 2009). While
this method can quickly generate training data, it
also generates many false examples. We reduce the
noise in such examples by using Amazon Mechani-
cal Turk (MTurk), which has been shown to produce
high quality annotations for a variety of natural lan-
guage processing tasks (Snow et al, 2008).
We use MTurk for annotation of textual relations
to establish an inexpensive and rapid method of cre-
ating data for slot filling. We present a two step an-
notation process: (1) automatic creation of noisy ex-
amples, and (2) human validation of examples.
2 Method
2.1 Automatic generation of noisy examples
To create noisy examples we use a similar approach
to Mintz et al (2009). We extract relations from a
KB in the form of tuples, (e, r, v), where e is an
entity, v is a value, and r is a relation that holds
between them; for example (J.R.R. Tolkien, occu-
pation, author). Our KB is Freebase1, an online
database of structured information, and our corpus
is from the TAC KBP task (McNamee and Dang,
2009)2. For each tuple, we find sentences in a cor-
pus that contain both an exact mention of the entity
e and of the value v. Of course, such sentences may
not attest to the relation r, so the process produces
many incorrect examples.
2.2 Human Intelligence Tasks
A Human Intelligence Task (HIT) is a short paid task
on MTurk. In our HITs, we present the turker with
ten relation examples as sentence/relation pairs. For
each example, the user is asked to select from three
annotation options: the sentence (1) expresses the
relation, (2) does not express the relation, or (3) the
1http://www.freebase.com
2http://projects.ldc.upenn.edu/kbp/
204
1. The sentence expresses the relation.
Sentence: For the past eleven years, James has
lived in Tucson.
Relation: ?Tucson? is the residence of ?James?
2. The sentence does not express the relation.
Sentence: Samuel first met Divya in 1990, while
she was still a student.
Relation: ?Divya? is a spouse of ?Samuel?
3. The relation does not make sense.
Sentence: Soojin was born in January.
Relation: ?January? is the birth place of ?Soojin?
Figure 1: The three annotation options with examples.
relation does not make sense (figure 1.)
Of the ten examples that comprise each HIT,
seven are automatically generated by the method
above. The correct answer is known for the three re-
maining examples; these are included for quality as-
surance (control examples.) The three control exam-
ples are a positive example (expresses the relation,) a
negative example (contradicts the true relation,) and
a nonsense example (relation is nonsensical.)
All control examples derive from a subset of the
automatically generated person examples. Positive
examples were randomly sampled and hand anno-
tated. Negative examples are familial relations in
which we change the relation type so that it would
not be expressed in the sentence. For example,
the relation ?Barack Obama is the parent of Malia
Obama? would be changed to ?Barack Obama is a
sibling of Malia Obama.? To generate nonsense ex-
amples we employ the same method for a different
mapping of relations, which produces relations like
?New Zealand is the gender of John Key.?
2.3 HIT Design
MTurk is a marketplace so users have total freedom
in choosing which HITs to complete. As such, HIT
design should maximize its appeal. We assume that
users find appealing those HITs through which they
may maximize their own monetary gain, while mini-
mizing moment-to-moment frustrations. We empha-
sized clarity and ease of use.
The layout consists of three sections (figure 2).
The leftmost section is a progress list, which shows
the user?s answers and current position; the middle
section contains the current relation example and an-
notation options; the rightmost section (not pictured)
# HITs Cost Time (hours)
Trial 50 $2.75 27
Batch 1 500 $27.50 34
Batch 2 765 $42.08 25
Batch 3 500 $27.50 22
Total 1815 $99.83 108
Table 1: Size, cost and time to complete each HITs batch.
contains instructions. All sections and all UI ele-
ments remain visible and in the same position for the
duration of the HIT, with only the text of the sen-
tence and relation changing according to question
number. Because only a single question is displayed
at a time, we are able to minimize user actions such
as scrolling, clicking small targets, or making large
mouse movements. Additionally, we can monitor
how much time a user spends on each question.
At all times the user is able to consult the instruc-
tions for the task, which include examples of each
annotation option. The user is also reminded of the
technical requirements for the HIT and expectations
for honesty and accuracy. A comment box provides
users with the opportunity to ask questions, make
suggestions, or clarify their responses.
3 Results
We submitted a trial run and three full batches of
HITs. Table 1 summarizes the costs and completion
times for all HITs. The HITs were labeled rapidly
and for a low cost ($0.05 per HIT, i.e., .5? per anno-
tation). Each HIT was assigned to five unique work-
ers. We found that 50% of the 352 different workers
completed 2 or more HITs (figure 3.) Our results
exclude a trial run of 50 hits. Across the 17,650 ex-
amples the mean time spent was 20.77 seconds, with
a standard deviation of 99.96 seconds. The median
time per example was 10.0 seconds.
3.1 Analysis
To evaluate the annotations, two of the authors an-
notated a random sample of 247 (10%) of the 2471
noisy examples. In addition, we analyzed the work-
ers agreement with the control examples.
We used two metrics to assess agreement. The
first metric is pairwise percent agreement (Pair-
wise): the average of the example agreement scores,
where the example agreement score is the percent of
205
Figure 2: An example HIT with instructions excluded.
0 
20 
40 
60 
80 
100 
120 
140 
0 50 100
 
150
 
200
 
250
 
300
 
350
 
# H
ITs 
Workers 
Figure 3: The number of HITs per worker, with columns
sorted left to right.
pairs of annotators that agreed for a particular exam-
ple. The second metric is the exact kappa coefficient
(Exact-?) (Conger, 1980), which takes into account
that agreement can occur by chance. The number of
annotators (R) varies with the test scenario.
Table 2 presents the inter-annotator agreement
scores for various subsets of the examples and com-
binations of annotators. On a sample of examples,
we evaluated agreement between the first and sec-
ond expert annotators (E1/E2) and also the agree-
ment between each expert and the majority vote of
the workers (E1/M and E2/M). The agreement be-
tween the two experts is substantially higher than
their individual agreements with the majority. Yet,
we achieve our goal of reducing noise.
We also analyzed the agreement between the
known control answer and the majority vote of the
workers (C/M). This high level of agreement sup-
ports our belief that the automatically generated neg-
ative and nonsense examples were easier to identify
# Ex. R Exact-? Pairwise
E1/E2 247 2 0.64 0.81
E1/M 247 2 0.29 0.60
E2/M 247 2 0.39 0.70
C/M 1059 2 0.90 0.93
T(sample) 247 5 0.31 0.69
T(control) 1059 5 0.52 0.68
T(all) 3530 5 0.45 0.68
Table 2: Inter-annotator agreement
than noisy negative and nonsense examples. Finally,
we evaluated the agreement between the five work-
ers for different subsets of the data: the sample of
noisy examples (T(sample)), the control examples
only (T(control)), and all examples (T(all)). Table 3
lists the number of examples collected and the agree-
ment scores for all workers for each relation type.
Table 4 shows the divergence of the workers? an-
notations from those of an expert. The high level
of confusability for those examples which the expert
annotated as Not Expressed suggests their inherent
difficulty. The workers labeled more examples as
Expressed than the expert, but both labeled few ex-
amples as Nonsense.
4 Quality Control
We identify spurious responses and unreliable users
in two ways. First, worker responses are compared
to control examples; greater agreement with controls
should indicate greater confidence in the user. We
filtered any worker whose agreement with the con-
trols was less than 0.85 (Control Filtered). The sec-
ond approach uses behavioral data. Because only a
single example is visible at any time, we can mea-
206
Relation # Ex. Exact-? Pairwise
siblings 13 0.67 0.82
children 12 0.57 0.83
gender 80 0.46 0.70
place of death 40 0.43 0.68
parent 12 0.40 0.64
spouse 54 0.37 0.65
title 71 0.30 0.78
residences 228 0.29 0.60
ethnicity 38 0.28 0.54
occupation 551 0.26 0.77
activism 4 0.26 0.55
religion 22 0.23 0.55
place of birth 160 0.20 0.64
nationality 1044 0.19 0.67
schools attended 8 0.16 0.55
employee of 132 0.16 0.70
charges 2 0.14 0.70
Total 2471 0.35 0.69
Table 3: Inter-annotator agreement across relation type.
# Ex. is the number of noisy examples. Exact-? and Pair-
wise agreement are among the five workers.
Worker
E NE Nn Total
E
xp
er
t-
1 E 561 89 20 670
NE 284 248 28 560
Nn 1 1 3 5
Total 846 338 51 1235
Table 4: Confusion matrix of expert-1 and user?s anno-
tations on the sample of noisy examples, for the choices
Expressed (E), Not Expressed (NE), and Nonsense (Nn)
sure how much time a user spends on each exam-
ple. The UI is designed to allow for the extremely
rapid completion of examples and of the HIT in gen-
eral. Thus, a user could complete the HIT in only a
few seconds without even reading any of the exam-
ples. Still other users spend only a moment on all-
but-one question, and then several minutes on the
remaining question. Here, we filter a user answering
three or more questions each in under three seconds
(Time Filtered). We combine these two approaches
(Control and Time), which yields the highest expert-
agreement levels (table 5.)
5 Conclusion
Using non-expert annotators from Amazon Mechan-
ical Turk for the correction of noisy, automatically
E1/M E2/M
Unfiltered 0.28 0.38
Time Filtered 0.32 0.43
Control Filtered 0.34 0.47
Control and Time 0.37 0.48
Table 5: Exact-? scores for three levels of quality control
and a baseline, between each expert and the majority vote
on 231 sampled examples. For a fair comparison, we re-
duced the sample size to include only examples for which
each level of quality control had at least one worker an-
notation remaining.
generated examples is inexpensive and fast. We
achieve good inter-annotator agreement using qual-
ity assurance measures to detect cheating. The result
is thousands of new annotated slot filling example
sentences for 17 person relations.
Acknowledgments
We would like to thank the 352 turkers who made
this work possible.
References
ACE. 2008. Automatic content extraction.
http://projects.ldc.upenn.edu/ace/.
R. Bunescu and R. Mooney. 2007. Learning to extract
relations from the web using minimal supervision. In
Association for Computational Linguistics (ACL).
A.J. Conger. 1980. Integration and generalization of
kappas for multiple raters. Psychological Bulletin,
88(2):322?328.
Z. GuoDong, M. Zhang, D. H Ji, and Z. H. U. QiaoM-
ing. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In
Empirical Methods in Natural Language Processing
(EMNLP).
Paul McNamee and Hoa Dang. 2009. Overview of the
TAC 2009 knowledge base population track. In Text
Analysis Conference (TAC).
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Association for Computational Linguistics
(ACL).
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Empirical
Methods in Natural Language Processing (EMNLP).
G. Zhou, L. Qian, and J. Fan. 2009. Tree kernel-based
semantic relation extraction with rich syntactic and se-
mantic information. Information Sciences.
207
