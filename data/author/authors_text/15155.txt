Integrating compositional semantics into a verb lexicon 
Hoa Trang Dang, Karin Kipper and Martha Palmer 
Department of Computer and Information Sciences 
University of Pennsylvania 
200 South 33rd Street 
Philadelphia, PA 19104, USA 
{htd,kipper, mpahner} @linc.cis.upenn.edu 
Abstract 
We present a class-based approach to building a 
verb lexicon that makes explicit the close asso- 
ciation between syntax and semantics for Levin 
classes. We have used Lexicalized Tree Adjoin- 
ing Grammars to capture the syntax associated with 
each verb class and have augmented the trees to in- 
clude selectional restrictions. In addition, semantic 
predicates are associated with each tree, which al- 
low for a colnpositional interpretation. 
1 Introduction 
The difficulty o1' achieving adequate hand-crafted 
semantic representations has limited the lield of 
natural language t)rocessing to applications that 
can be contained within well-deIined sub-domains. 
Despite many different lexicon development ap- 
proaches (Mel'cuk, 1988; Copestakc and Sanfil- 
ippo, 1993; Lowe et al, 1997), the field has yet 
to develop a clear conseusus on guidelines for a 
colnputational lexicon. One of the most controver- 
sial areas in building such a lexicon is polyselny: 
how senses can be computationally distinguished 
and characterized. We address this problem hy em- 
ploying compositional semantics and the adjunction 
of syntactic l)hrases to support regular verb sense 
extensions. This differs l'rom the Lexical Concep- 
tual Structure (LCS) approach exemplified by Voss 
(1996), which requires a separate LCS representa- 
tion for each possible sense extension. In this pa- 
per we describe the construction of VerbNet, a verb 
lexicon with explicitly stated syntactic and seman- 
tic information for individual exical items, using 
Levin verb classes (Levin, 1993) to systematically 
construct lexical entries. We use Lexicalized Tree 
Adjoining Grammar (LTAG) (Joshi, 1987; Schabes, 
1990) to capture the syntax for each verb class, and 
associate semantic predicates with each tree. 
Althougla similar ideas have been explored for 
w:rb sense extension (Pusteiovsky, 1995; Goldberg, 
1995), our approach of applying LTAG to the prob- 
lem of composing and extending verb senses is 
novel. LTAGs have an extended omain of local- 
ity that captures the arguments of a verb in a local 
manner. The association of semantic predicates to a 
tree yields a complete semantics for the verb. More- 
ovel, the operation of adjunction in LTAGs provides 
a mechanism for extending verb senses. 
2 Levin classes 
Levin verb classes are based on the ability of a verb 
to occur in diathesis alternations, which are pairs 
of syntactic frames that are in some sense meaning 
preserving. The fundalnental ssulnption is that the 
syntactic frames arc a direct reflection of the under- 
lying semantics, ltowever, Levin classes exhibit in- 
consistencies that have hampered researchers' abil- 
ity to reference them directly in applications. Many 
verbs ale listed in multiple classes, some of which 
have confl icting sets of syntactic frames. Dang et al 
(1998) showed that multiple listings could in some 
cases be interpreted as regular sense xtensions, and 
defined intersective Levin classes, which are a more 
line-grained, syntactically and semantically coher- 
ent refinement of basic Levin classes. We represent 
these verb classes and their regular sense xtensions 
in the LTAG forlnalism. 
3 Lexicalized rI?ee Adjoining Grammars 
3.1 Overview of formalism 
Lexicatized Tree Adjoining Granunars consist of a 
finite set of initial and auxiliary elementary hees, 
and two operations to combine them. The min- 
imal, non-recursive linguistic structures of a lan- 
guage, such as a verb and its complements, are cap- 
tured by initial trees. Recursive structures of a lan- 
guage, such as prepositional modifiers which result 
in syntactically embedded VPs, are represented by 
auxiliary trees. 
1011 
Elementary trees are combined by the operations 
of substitution and adjunction. Substitution is a sim- 
ple operation that replaces a leaf of a tree with a new 
tree. Adjunction is a splicing operation that replaces 
an internal node of an elementary tree with an aux- 
iliary tree. Eveu tree is associated with a lexical 
item of the language, called the anchor of the tree. 
Tile tree represents the domain over which the lex- 
ical item can directly specify syntactic onstraints, 
such as subject-verb number agreement, or seman- 
tic constraints, uch as selectional restrictions, all of 
which are implemented as features. 
LTAGs are more powerful than context free gram- 
mars (CFG), allowing localization of so-called un- 
bounded dependencies that cannot be handled by 
CFGs. There are critical benefits to lexical seman- 
tics that are provided by the extended omain of 
locality of the lexicalized trees. Each lexical en- 
try corresponds to a tree. If the lexical item is a 
verb, the conesponding tree is a skeleton for an en- 
tire sentence with the verb already present, anchor- 
ing the tree as a terminal symbol. The other parts 
of the sentence will be substituted or adjoined in at 
appropriate places in the skeleton tree in the course 
of the derivation. The composition of trees during 
parsing is recorded in a derivation tree. The deriva- 
tion tree nodes correspond to lexically anchored el- 
ementary trees, and the arcs are labeled with infor- 
mation about how these trees were combined to pro- 
duce the parse. Since each lexically anchored initial 
tree corresponds to a semantic unit, the derivation 
tree closely resembles a semantic-dependency rep- 
resentation. 
3.2 Semantics for TAGs 
There is a range of previous work in incorporating 
semantics into TAG trees. Stone and Doran (1997) 
describe a system used for generation that simul- 
taneously constructs the semantics and syntax of 
a sentence using LTAGs. Joshi and Vijay-Shanker 
(1999), and Kallmeyer and Joshi (1999), describe 
the semantics of a derivation tree as a set of attach- 
ments of trees. The semantics of these attachments 
is given as a conjunction of formulae in a flat seman- 
tic representation. They provide a specific method- 
ology for composing semantic representations much 
like Candito and Kahane (1998), where the direc- 
tionality of dominance in the derivation tree should 
be interpreted according to the operations used to 
build it. Kallmeyer and Joshi also use a flat semantic 
representation to handle scope phenomena involv- 
ing quantifiers. 
4 Descr ip t ion  of  the verb lexicon 
VerbNet can be viewed in both a static and a dy- 
namic way. Tile static aspect refers to the verb and 
class entries and how they are organized, providing 
the characteristic descriptions of a verb sense or a 
verb class (Kipper et al, 2000). The dynamic as- 
pect of the lexicon constrains the entries to allow 
a compositional interpretation i LTAG derivation 
trees, representing extended verb meanings by in- 
corporating adjuncts. 
Verb classes allow us to capture generalizations 
about verb behavioL Each verb class lists the tlle- 
mafic roles that the predicate-argument structure of 
its members allows, and provides descriptions of 
the syntactic fi'ames conesponding to licensed con- 
structions, with selectional restrictions defined for 
each argument in each frame, l Each frame also 
includes semantic predicates describing the partic- 
ipants at various stages of the event described by 
the frame. 
Verb classes are hierarchically organized, ensur- 
ing that each class is coherent - that is, all its mem- 
bers have common semantic elements and share a 
common set of thematic roles and basic syntactic 
frames. This requires ome manual restructuring of 
the original Levin classes, which is facilitated by us- 
ing intersective Levin classes. 
5 Compositional Semantics 
We use TAG elementary trees for the description 
of allowable frames and associate semantic predi- 
cates with each tree, as was done by Stone and Do- 
ran. The semantic predicates are primitive enough 
so that many may be reused in different rees. By 
using TAGs we get the additional benefit of an ex- 
isting parser that yields derivations and derived trees 
fiom which we can construct the compositional se- 
mantics of a given sentence. 
We decompose each event E into a tripar- 
tite structure in a manner similar to Moens and 
Steedman (1988), introducing a time function for 
each predicate to specify whether the predicate is 
true in the preparatory (d~ring(E)), cuhnination 
(er~d(E)), or consequent (res~ll:(E)) stage of an 
event. 
hfitial trees capture tile semantics of the basic 
senses of verbs in each class. For example, many 
IThese restrictions are more like preferences that generate a 
preferred reading of a sentence. They may be relaxed epend- 
ing on the domain of a particular pplication. 
1012 
S \[ cvcnt=E \] S \[ event=E2 \]
NP.,.sH$ VP \[ cvcnt=E \] NParqo$ VP \[ event=E1 \] 
\[ +aninmtc \] \] \[ +animale \]
V V NI),~,.ql$ 
I \] \[ +animate \]
1"1.11| rU l l  
motion(during(E), Xa,.al ) motion(during(El), Xargl ) 
Figure 1 : Induced action alternation for the Run verbs 
verbs in the Run class can occur in the induced ac- 
tion alternation, in which the subject of the inmmsi- 
tive sentence has the same thematic role as the direct 
object in the transitive sentence. Figure l shows the 
initial trees for the transitive and intransitive vari- 
ants for the Run class, along with their semantic 
predicates. The entity in motion is given by argl, 
associated with the syntactic subject of the intransi- 
tive tree and the direct object of the transitive tree. 
The event denoted by the transitive variant is a com- 
position of two subevents: E1 refers to the event of 
av.ql running, and E2 refers to the event of an entity 
(argO) causing event E l .  
Predicates are associated with not only the verb 
trees, but also the auxiliary trees. We use a flat 
semantic rcpmsentatiou like that of Kalhncycr and 
Joshi, and the semantics of a sentence is the con- 
junction of the semantic predicates of the trees used 
to derive the sentence. Figure 2 shows au auxiliary 
tree for a path prepositional pllrase headed by "to", 
along with its associated semantic predicate. 
When the PP tree for "to the park" is adjoiued into 
the intransitive tree for "John ran", the semantic in- 
terpretation is the conjunction of the two predicates 
motion(during(E),john) A goal(end(E),john, park); 
adjunction into the transitive tree for "Bill ran 
the horse" yields cause(during(E2),bilI, El) A mo- 
tion(during(El), horse) A goal(end(El), horse, park). 
In both cases, the argument X,rs?0.,rgl (john or 
horse) for the anxiliary tree is noulocal and colnes 
from the adjunction site. 2 The arguments are re- 
covered from the derivation tree, following Candito 
and Kahane. When an initial tree is substituted into 
another tree, the dependency mirrors the derivation 
structure, so the variables associated with the sub- 
2X.,..qo,.,.ga is the variable associated with the cntity in mo- 
tion (ar91) in the tree to which tile PP a(Uoins (argO). 
stituting tree can be referenced as arguments in the 
host tree's predicates. When an auxiliary tree is 
adjoined, the dependency for the adjunction is re- 
versed, so that variables associated with the host 
tree can be referenced as arguments in the adjoin- 
ing tree's predicates. 
VP 
VPar:jO* PP 
\[ evc,~t=l~ \] 
I' NP.rql$ 
I 
lo 
qoal (end(E), X,.,.;jo.,,.r11, Xa,.~j1) 
Figure 2: Auxiliary path PP tree 
The tripartite vent structure allows us to express 
the semantics of classes of verbs like change of 
state verbs whose description requires reference to 
a complex event structure. In the case of a verb 
such as "break", it is important to make a distinc- 
tion between the state of the object before the end 
of the action and the new state that results after- 
wards. This event structure also handles the eona- 
tive construction, in which there is an intention of 
a goal during the event, that is not achieved at 
the end of the event. The example of the cona- 
rive construction shown in Figure 3 expresses the 
intention of hitting something. Because the in- 
tention is not satisfied the semantics do not in- 
clude the predicates manner(end(E),fi~rcefuI, X, rgo ) 
A conmct(end(E),X, rgo,Xc~rgO, that express the 
completion of the contact with impact event. 
The ability of verbs to take on extended senses 
in sentences based on their adjuncts is captured in a 
1013 
S \[ event=E \]
NPa~.qO$ VP \[ evcnt=E \]
V NPargl$ 
I 
hit 
manner(during(E), direetedmotion, Xa,..qo )A 
contact(end(E), Xar~O, Xar~l )A 
7naT,,nel' (end(E), f of'ee f '~l, Xar9O ) 
s \[ cvcnt=r~: \] 
NParq0$ VP 
V VP 
I 
hit V PP 
I 
I 
at 
manner (during(E), direct, edmotion, X~r:io )
Figure 3: Syntax and semantics of transitive and conative construction for Hit verbs 
natural way by the TAG operation of adjunction and 
our conjunction of semantic predicates. The orig- 
inal Hit verb class does not include movement of 
the direct object as part of the meaning of hit; only 
sudden contact has to be established. By adjoining 
a path PP such as "across NP", we get an extended 
meaning, and a change in Levin class membership 
to the Throw class. Figure 4 shows the class-specific 
auxiliary tree anchored by the preposition "across" 
together with its semantic predicates, introducing a
motion event that immediately follows (meets) the 
contact event. 
VP \[ evenI:E \] 
VP.rf/o*\[ cvcnt=EargO \] PP 
P NPargl.~ 
I 
aCI'OSS 
meets (E,,..,jo, E) A 
motion(during(E), X~m~,0.,,.91 )A 
via(during(E), X~r,jo.~r~l , Xa,.,j1) 
Figure 4: Auxiliary tree for "across" 
oll the LTAG formalism, for which we already have 
a large English grammar. Palmer et al (1998) de- 
fined compositional semantics for classes of verbs 
implemented in LTAG, representing general seman- 
tic components (e.g., motion, manner) as features 
on the nodes of the trees. Ore" use of separate log- 
ical forms gives a more detailed semantics for the 
sentence, so that for an event involving motion, it is 
possible to know not only that the event has a motion 
semantic omponent, but also which entity is actu- 
ally in motion. This level of detail is necessary for 
applications uch as animatiou of natural anguage 
instructions (Bindiganavale t al., 2000). Another 
important contribution of this work is that by divid- 
ing each event into a tripartite structure, we permit a 
more precise definition of the associated semantics. 
Finally, the operation of adjunction in TAGs pro- 
vides a principled approach to representing the type 
of regular polysemy that has been a major obstacle 
in buildiug verb lexicons. 
Researching whether a TAG grammar for Verb- 
Net can be automatically constructed by using de- 
velopment tools such as Xia et al (1999) or Candito 
(1996) is part of our next step. We also expect o be 
able to factor out some class-specific auxiliary trees 
to be used across several verb classes. 
6 Conclusion 
We have presented a class-based approach to build- 
ing a verb lexicon that makes explicit and imple- 
ments the close association between syntax and se- 
mantics, as postulated by Levin. The power of the 
lexicon comes from its dynamic aspect hat is based 
7 Acknowledgments 
The authors would like to thank the anonymous re- 
viewers for their valuable comments. This researeh 
was partially supported by NSF grants IIS-9800658 
and IIS-9900297 and CAPES grant 0914-95. 
1014 
References 
Ralna Bindiganawde, Willianl Schuler, Jan M. All- 
beck, Nornlan I. Badler, Aravind K. Joshi, and 
Martha Pahner. 2000. Dynamically Altering 
Agent Behaviors Using Natural Language In- 
structions. Fourth International Cot!ference on 
Autonomous Agents, June. 
Marie-Hdl~ne Candito and Sylwtin Kahane. 1998. 
Can the TAG derivation tree represent a senlan- 
tic graph? An answer ill the light of Meaning- 
Text Theory. In Piz)ceedhtgs of the Fourth 77~G+ 
Workshop, pages 21-24, Philadelphia, PA, Au- 
gust. 
Marie-Hdl~ne Candito. 1996. A Principle-Based 
Hierarchical Representation f LTAGs. In Pro- 
ceedings of COLING-96, Copenhagen, Denlnark. 
Aim Copestake and Antonio Sanfilippo. 1993. 
Multilingual exical representation. In Proceed- 
ings ,2.1" the AAAI Spring Symposium: Bttilding 
Lexicons.for Machine Translation, Stanford, Cal- 
ifornia. 
Hoa Trang l)ang, Karin Kipper, Martha Pahner, and 
Joseph Rosenzweig. 1998. hwestigating regu- 
lar sense extensions based on intersective Levin 
classes . In Proceedings of COLING-ACL98, 
Montreal, Canada, August. 
Adele E. Goldberg. 1995. C'onslruclions. A Con- 
struction Grammar Approach 1o k rgument Slrttc- 
lure. University of Chicago Press, Chicago, Ill. 
Aravind K. Joshi and K. Vijay-Shanker. 1999. 
Compositional semantics wilh Lexicalized 
Tlee-ac\[joilfing Granllnar: How nlueh under- 
specification is necessary? In Proceedings of 
the Third International Worksho I) on Conq)tt- 
rational Semantics (IWCS-3), pages 131-145, 
Tilburg, The Netherlands, January. 
Aravind K. Joshi. 1987. An introduction to tree ad- 
joining grannnars. In A. Manaster-Ramer, ditor, 
Mathematics of Language. Jolm 13elljamins, Am- 
sterdaln. 
Laura Kalhneyer and Anwind Joshi. 1999. Under- 
specified selnantics with LTAG. In Proceedhlgs 
of Amsterdam Colloquium on Semantics. 
Karin Kippm, Hoa Trang Dang, and Martha Palmer. 
2000. Class-based construction of a verb lexi- 
COll. Ill Pn)ceedings of the Seventh National Con- 
ference on Art!\[icial httelligence (AAAI-2000), 
Austin, TX, July-August. 
Beth Levin. 1993. English Verb Classes and A Iter- 
nation, A Preliminary hwestigation. Tile Univer- 
sity of Chicago Press. 
J.B. Lowe, C.E Baker, and C.J. Filhnore. 1997. A 
fnnne-semantic approach to semantic annotation. 
Ill Proceedings 1997 Siglex WorksholJ/ANLP97, 
Washington, I).C. 
I. A. Mel'cuk. 1988. Semantic description of lex- 
ical units ill an explanatory combinatorial dic- 
tionary: Basic plilmiples and heuristic criteria. 
International ,lournal of Lexicography, I:3:165- 
188. 
M. Moens and M. Steedman. 1988. Telnporal on- 
tology and tenlporal refel'ence. Computational 
Linguistics, 14:15-38. 
Martha l~allnel ", Joseph Rosenzweig, and Willialn 
Schuler. 1998. Capturing Motion Verb General- 
izations in Sylmhronous TAG. Ill Patrick Saint- 
l)izim, editol, Predicative Forms in Natural Lan- 
guage and in Lexical Knowledge Bases. Kluwer 
Press. 
James Pustejovsky. 1995. The Generative Lexicon. 
MIT Press, Cambridge, Massachusetts, USA. 
Yves Sehabes. 1990. Mathematical nd Computa- 
tional Aspects of Lexicalized Grammars. Ph.D. 
thesis, Compnter Science Department, University 
of Pennsylvania. 
Matthew Stone and Christine Doran. 1997. Sell- 
tence Plalming as l)escription using Tree Adjoin- 
ing Grammar. Ill Proceedings of ACL-EACL '97, 
Madrid, Spain. 
Clare Voss. 1996. lnlerlinxua-l)ased Machine 
7'ranslation o\[" &~atial Expressions. PI'LD. the- 
sis, University of Maryland, Depamnent of Com- 
puter Science. 
Fei Xia, Martha Pahnei, and K. Vijay-Shanker. 
1999. Toward senli-autolnating grannnar 
development. Ill Proceedings of the 51h 
Natural Language Proces'sing Pacific Rim 
3),mposium(NLPRS-99), Beijing, China. 
1015 
Simple Features for Chinese Word Sense Disambiguation
Hoa Trang Dang, Ching-yi Chia, Martha Palmer, and Fu-Dong Chiou
Department of Computer and Information Science
University of Pennsylvania
 
htd,chingyc,mpalmer,chioufd  @unagi.cis.upenn.edu
Abstract
In this paper we report on our experiments on au-
tomatic Word Sense Disambiguation using a max-
imum entropy approach for both English and Chi-
nese verbs. We compare the difficulty of the sense-
tagging tasks in the two languages and investigate
the types of contextual features that are useful for
each language. Our experimental results suggest
that while richer linguistic features are useful for
English WSD, they may not be as beneficial for Chi-
nese.
1 Introduction
Word Sense Disambiguation (WSD) is a central
open problem at the lexical level of Natural Lan-
guage Processing (NLP). Highly ambiguous words
pose continuing problems for NLP applications.
They can lead to irrelevant document retrieval in In-
formation Retrieval systems, and inaccurate transla-
tions in Machine Translation systems (Palmer et al,
2000). For example, the Chinese word (jian4) has
many different senses, one of which can be trans-
lated into English as ?see?, and another as ?show?.
Correctly sense-tagging the Chinese word in context
can prove to be highly beneficial for lexical choice
in Chinese-English machine translation.
Several efforts have been made to develop au-
tomatic WSD systems that can provide accurate
sense tagging (Ide and Veronis, 1998), with a cur-
rent emphasis on creating manually sense-tagged
data for supervised training of statistical WSD sys-
tems, as evidenced by SENSEVAL-1 (Kilgarriff and
Palmer, 2000) and SENSEVAL-2 (Edmonds and
Cotton, 2001). Highly polysemous verbs, which
have several distinct but related senses, pose the
greatest challenge for these systems (Palmer et al,
2001). Predicate-argument information and selec-
tional restrictions are hypothesized to be particu-
larly useful for disambiguating verb senses.
Maximum entropy models can be used to solve
any classification task and have been applied to a
wide range of NLP tasks, including sentence bound-
ary detection, part-of-speech tagging, and parsing
(Ratnaparkhi, 1998). Assigning sense tags to words
in context can be viewed as a classification task sim-
ilar to part-of-speech tagging, except that a separate
set of tags is required for each vocabulary item to be
sense-tagged. Under the maximum entropy frame-
work (Berger et al, 1996), evidence from different
features can be combined with no assumptions of
feature independence. The automatic tagger esti-
mates the conditional probability that a word has
sense  given that it occurs in context  , where 
is a conjunction of features. The estimated prob-
ability is derived from feature weights which are
determined automatically from training data so as
to produce a probability distribution that has max-
imum entropy, under the constraint that it is con-
sistent with observed evidence. With existing tools
for learning maximum entropy models, the bulk of
our work is in defining the types of features to look
for in the data. Our goal is to see if sense-tagging
of verbs can be improved by combining linguistic
features that capture information about predicate-
arguments and selectional restrictions.
In this paper we report on our experiments on au-
tomatic WSD using a maximum entropy approach
for both English and Chinese verbs. We compare
the difficulty of the sense-tagging tasks in the two
languages and investigate the types of contextual
features that are useful for each language. We find
that while richer linguistic features are useful for
English WSD, they do not prove to be as beneficial
for Chinese.
The maximum entropy system performed com-
petitively with the best systems on the English
verbs in SENSEVAL-1 and SENSEVAL-2 (Dang and
Palmer, 2002). However, while SENSEVAL-2 made
it possible to compare many different approaches
over many different languages, data for the Chinese
lexical sample task was not made available in time
for any systems to compete. Instead, we report on
two experiments that we ran using our own lexicon
and two separate Chinese corpora that are very sim-
ilar in style (news articles from the People?s Repub-
lic of China), but have different types and levels of
annotation ? the Penn Chinese Treebank (CTB)(Xia
et al, 2000), and the People?s Daily News (PDN)
corpus from Beijing University. We discuss the util-
ity of different types of annotation for successful au-
tomatic word sense disambiguation.
2 English Experiment
Our maximum entropy WSD system was de-
signed to combine information from many differ-
ent sources, using as much linguistic knowledge
as could be gathered automatically by current NLP
tools. In order to extract the linguistic features nec-
essary for the model, all sentences were first auto-
matically part-of-speech-tagged using a maximum
entropy tagger (Ratnaparkhi, 1998) and parsed us-
ing the Collins parser (Collins, 1997). In addi-
tion, an automatic named entity tagger (Bikel et al,
1997) was run on the sentences to map proper nouns
to a small set of semantic classes.
Chodorow, Leacock and Miller (Chodorow et al,
2000) found that different combinations of topical
and local features were most effective for disam-
biguating different words. Following their work, we
divided the possible model features into topical fea-
tures and several types of local contextual features.
Topical features looked for the presence of key-
words occurring anywhere in the sentence and any
surrounding sentences provided as context (usually
one or two sentences). The set of 200-300 keywords
is specific to each lemma to be disambiguated, and
is determined automatically from training data so
as to minimize the entropy of the probability of the
senses conditioned on the keyword.
The local features for a verb  in a particular sen-
tence tend to look only within the smallest clause
containing  . They include collocational features
requiring no linguistic preprocessing beyond part-
of-speech tagging (1), syntactic features that cap-
ture relations between the verb and its complements
(2-4), and semantic features that incorporate infor-
mation about noun classes for subjects and objects
(5-6):
1. the word  , the part of speech of  , the part
of speech of words at positions -1 and +1 rela-
tive to  , and words at positions -2, -1, +1, +2,
relative to 
2. whether or not the sentence is passive
3. whether there is a subject, direct object, indi-
rect object, or clausal complement (a comple-
ment whose node label is S in the parse tree)
4. the words (if any) in the positions of subject,
direct object, indirect object, particle, preposi-
tional complement (and its object)
5. a Named Entity tag (PERSON, ORGANIZA-
TION, LOCATION) for proper nouns appear-
ing in (4)
6. WordNet synsets and hypernyms for the nouns
appearing in (4)
2.1 English Results
The maximum entropy system?s performance on
the verbs from the evaluation data for SENSEVAL-
1 (Kilgarriff and Rosenzweig, 2000) rivaled that
of the best-performing systems. We looked at the
effect of adding topical features to local features
that either included WordNet class features or used
just lexical and named entity features. In addition,
we experimented to see if performance could be
improved by undoing passivization transformations
to recover underlying subjects and objects. This
was expected to increase the accuracy with which
verb arguments could be identified, helping in cases
where selectional restrictions on arguments played
an important role in differentiating between senses.
The best overall variant of the system for verbs
did not use WordNet class features, but included
topical keywords and passivization transformation,
giving an average verb accuracy of 72.3%. If
only the best combination of feature sets for each
verb is used, then the maximum entropy mod-
els achieve 73.7% accuracy. These results are
not significantly different from the reported results
of the best-performing systems (Yarowsky, 2000).
Our system was competitive with the top perform-
ing systems even though it used only the training
data provided and none of the information from
the dictionary to identify multi-word constructions.
Later experiments show that the ability to correctly
identify multi-word constructions improves perfor-
mance substantially.
We also tested the WSD system on the verbs from
the English lexical sample task for SENSEVAL-2.1
1The verbs were: begin, call, carry, collaborate, develop,
Feature Type (local only) Accuracy Feature Type (local and topical) Accuracy
collocation 48.3 collocation 52.9
+ syntax 53.9 + syntax 54.2
+ syntax + semantics 59.0 + syntax + semantics 60.2
Table 1: Accuracy of maximum entropy system using different subsets of features for SENSEVAL-2 verbs.
In contrast to SENSEVAL-1, senses involving multi-
word constructions could be identified directly from
the sense tags themselves, and the head word and
satellites of multi-word constructions were explic-
itly marked in the training and test data. This addi-
tional annotation made it much easier to incorporate
information about the satellites, without having to
look at the dictionary (whose format may vary from
one task to another). All the best-performing sys-
tems on the English verb lexical sample task filtered
out possible senses based on the marked satellites,
and this improved performance.
Table 1 shows the performance of the system us-
ing different subsets of features. In general, adding
features from richer linguistic sources tended to im-
prove accuracy. Adding syntactic features to collo-
cational features proved most beneficial in the ab-
sence of topical keywords that could detect some
of the complements and arguments that would nor-
mally be picked up by parsing (complementizers,
prepositions, etc.). And while topical information
did not always improve results significantly, syntac-
tic features along with semantic class features al-
ways proved beneficial.
Incorporating topical keywords as well as col-
locational, syntactic, and semantic local features,
our system achieved 60.2% and 70.2% accuracy
using fine-grained and coarse-grained scoring, re-
spectively. This is in comparison to the next best-
performing system, which had fine- and coarse-
grained scores of 57.6% and 67.2% (Palmer et al,
2001). If we had not included a filter that only con-
sidered phrasal senses whenever there were satel-
lites of multi-word constructions marked in the test
data, our fine- and coarse-grained accuracy would
have been reduced to 57.5% and 67.2% (significant
at 	
 ).
3 Chinese Experiments
We chose 28 Chinese words to be sense-tagged.
Each word had multiple verb senses and possibly
draw, dress, drift, drive, face, ferret, find, keep, leave, live,
match, play, pull, replace, see, serve, strike, train, treat, turn,
use, wander, wash, work.
other senses for other parts of speech, with an av-
erage of 6 dictionary senses per word. The first
20 words were chosen by randomly selecting sev-
eral files totaling 5000 words from the 100K-word
Penn Chinese Treebank, and choosing only those
words that had more than one dictionary verb sense
and that occurred more than three times in these
files. The remaining 8 words were chosen by se-
lecting all words that had more than one dictio-
nary verb sense and that occurred more than 25
times in the CTB. The definitions for the words
were based on the CETA (Chinese-English Transla-
tion Assistance) dictionary (Group, 1982) and other
hard-copy dictionaries. Figure 1 shows an exam-
ple dictionary entry for the most common sense of
jian4. For each word, a sense entry in the lexi-
con included the definition in Chinese as well as
in English, the part of speech for the sense, a typ-
ical predicate-argument frame if the sense is for a
verb, and an example sentence. With these defini-
tions, each word was independently sense-tagged by
two native Chinese-speaking annotators in a double-
blind manner. Sense-tagging was done primarily us-
ing raw text, without segmentation, part of speech,
or bracketing information. After finishing sense tag-
ging, the annotators met to compare and to discuss
their results, and to modify the definitions if neces-
sary. The gold standard sense-tagged files were then
made after all this discussion.
In a manner similar to our English approach, we
included topical features as well as collocational,
syntactic, and semantic local features in the maxi-
mum entropy models. Collocational features could
be extracted from data that had been segmented into
words and tagged for part of speech:
 the target word
 the part of speech tag of the target word
 the words (if any) within 2 positions of the tar-
get word
 the part of speech of the words (if any) immedi-
ately preceding and following the target word
 whether the target word follows a verb
<entry id="00007" word=" " pinyin="jian4">
<wordsense id="00007-001">
         <definition id="chinese"> , , </definition>
         <definition id="english">to see, to perceive</definition>
         <pos>VV</pos>
         <pred-arg>NP0 NP1</pred-arg>
         <pred-arg>NP0 NP1 IP</pred-arg>
         <example> <word> </word> </example>
</wordsense>
</entry>
Figure 1: Example sense definition for jian4.
When disambiguating verbs, the following syn-
tactic local features were extracted from data brack-
eted according to the Penn Chinese Treebank guide-
lines:
 whether the verb has a surface subject
 the head noun of the surface subject of the verb
 whether the verb has an object (any phrase la-
beled with ?-OBJ?, such as NP-OBJ, IP-OBJ,
QP-OBJ)
 the phrase label of the object, if any
 the head noun of the object
 whether the verb has a VP complement
 the VP complement, if any
 whether the verb has an IP complement
 whether the verb has two NP complements
 whether the verb is followed by a predicate
(any phrase labeled with ?-PRD?)
Semantic features were generated by assigning a
HowNet2 noun category to each subject and object,
and topical keywords were extracted as for English.
Once all the features were extracted, a maximum
entropy model was trained and tested for each target
word. We used 5-fold cross validation to evaluate
the system on each word. Two methods were used
for partitioning a dataset of size  into five subsets:
Select  consecutive occurrences for each set, or
select every 5th occurrence for a set. In the end,
the choice of partitioning method made little differ-
ence in overall performance, and we report accuracy
as the precision using the latter (stratified) sampling
method.
2http://www.keenage.com/
Feature Type Acc Std Dev
collocation (no part of speech) 86.8 1.0
collocation 93.4 0.5
+ syntax 94.4 0.4
+ syntax + semantics 94.3 0.6
collocation + topic 90.3 1.0
+ syntax + topic 92.6 0.9
+ syntax + semantics + topic 92.8 0.9
Table 2: Overall accuracy of maximum entropy sys-
tem using different subsets of features for Penn Chi-
nese Treebank words (manually segmented, part-of-
speech-tagged, parsed).
3.1 Penn Chinese Treebank
All sentences containing any of the 28 target words
were extracted from the Penn Chinese Treebank,
yielding between 4 and 1143 occurrence (160 av-
erage) for each of the target words. The manual
segmentation, part-of-speech tags, and bracketing
of the CTB were used to extract collocational and
syntactic features.
The overall accuracy of the system on the 28
words in the CTB was 94.4% using local colloca-
tional and syntactic features. This is significantly
better than the baseline of 76.7% obtained by tag-
ging all instances of a word with the most frequent
sense of the word in the CTB. Considering only the
23 words for which more than one sense occurred in
the CTB, overall system accuracy was 93.9%, com-
pared with a baseline of 74.7%. Figure 2 shows the
results broken down by word.
As with the English data, we experimented with
different types of features. Table 2 shows the per-
formance of the system using different subsets of
features. While the system?s accuracy using syntac-
tic features was higher than using only collocational
features (significant at 
 ), the improve-
Word pinying (translation)               Events  Senses  Baseline  Acc.  Std Dev
-------------------------------------------------------------------------------
 biao3 shi4  (to indicate/express)   100     3       63.0      95.0  5.5
   chu1 (to go out/to come out)        34      5       50.0      50.0  11.1
   da2 (to reach a stage/to attain)    181     1       100       100   0.0
   dao3 (to come/to arrive)            219     10      36.5      82.7  7.1
 fa1 zhan3 (to develop/to grow)      437     3       65.2      97.0  1.2
   hui4 (will/be able to)              86      6       58.1      91.9  6.0
   jian4 (to see/to perceive)          4       2       75.0      25.0  38.7
 jie3 jue2 (to solve/to settle)      44      2       79.5      97.7  5.0
 jin4 xing2 (to be in progress)      159     3       89.3      95.6  2.5
   ke3 (may/can)                       57      1       100       100   0.0
   lai2 (to come/to arrive)            148     6       66.2      96.6  2.1
 li4 yong4 (to use/to utilize)       163     2       92.6      98.8  2.4
   rang4 (to let/to allow)             9       1       100       100   0.0
   shi3 (to make/to let)               89      1       100       100   0.0
   shuo1 (to say in spoken words)      306     6       86.9      95.1  2.0
   wan2 (to complete/to finish)        285     2       98.9      100.0 0.0
   wei2/wei4 (to be/to mean)           473     7       32.8      86.1  2.4
   xiang3 (to think/ponder/suppose)    8       3       62.5      50.0  50.0
 yin3 jin4 (to import/to introduce)  62      2       85.5      98.4  3.3
   zai4 (to exist/to be at(in, on))    1143    4       96.9      99.3  0.4
 fa1 xian4 (to discover/to realize)  37      3       59.5      100.0 0.0
 hui1 fu4 (to resume/to restore)     27      3       44.4      77.8  19.8
 kai1 fang4 (to open to investors)   122     5       74.6      96.7  3.0
 ke3 yi3 (may/can)                   32      1       100       100   0.0
 tong1 guo4 (to pass legislation)    81      5       66.7      95.1  2.5
 tou2 ru4 (to input money, etc.)     44      4       40.9      84.1  11.7
   yao4 (must/should/to intend to)     106     6       65.1      62.3  8.9
   yong4 (to use)                      41      2       58.5      100   0.0
-------------------------------------------------------------------------------
Overall                                  4497    3.5     76.7      94.4  0.4
Figure 2: Word, number of instances, number of senses in CTB, baseline accuracy, maximum entropy
accuracy and standard deviation using local collocational and syntactic features.
ment was not as substantial as for English, and this
was despite the fact that the Chinese bracketing was
done manually and should be almost error-free.
Semantic class information from HowNet yielded
no improvement at all. To see if using a differ-
ent ontology would help, we subsequently exper-
imented with the ROCLing conceptual structures
(Mo, 1992). In this case, we also manually added
unknown nouns from the corpus to the ontology and
labeled proper nouns with their conceptual struc-
tures, in order to more closely parallel the named
entity information used in the English experiments.
This resulted in a system accuracy of 95.0% (std.
dev. 0.6), which again is not significantly better than
omitting the noun class information.
3.2 People?s Daily News
Five of the CTB words (chu1, jian4, xiang3, hui1
fu4, yao4) had system performance of less than
80%, probably due to their low frequency in the
CTB corpus. These words were subsequently sense
tagged in the People?s Daily News, a much larger
corpus (about one million words) that has manual
segmentation and part-of-speech, but no bracketing
information.3 Those 5 words included all the words
for which the system performed below the baseline
3The PDN corpus can be found at
http://icl.pku.edu.cn/research/corpus/dwldform1.asp.
The annotation guidelines are not exactly the
same as for the Penn CTB, and can be found at
http://icl.pku.edu.cn/research/corpus/coprus-annotation.htm.
Feature Type Acc Std Dev
collocation (no part of speech) 72.3 2.2
collocation 70.3 2.9
+ syntax 71.7 3.0
+ syntax + semantics 72.7 3.1
collocation + topic 73.3 3.2
+ syntax + topic 72.6 3.9
+ syntax + semantics + topic 72.8 3.7
Table 3: Overall accuracy of maximum entropy sys-
tem using different subsets of features for People?s
Daily News words (automatically segmented, part-
of-speech-tagged, parsed).
Feature Type Acc Std Dev
collocation (no part of speech) 71.4 4.3
collocation 74.7 2.3
collocation + topic 72.1 3.1
Table 4: Overall accuracy of maximum entropy sys-
tem using different subsets of features for People?s
Daily News words (manually segmented, part-of-
speech-tagged).
in the CTB corpus. About 200 sentences for each
word were selected randomly from PDN and sense-
tagged as with the CTB.
We automatically annotated the PDN data to
yield the same types of annotation that had been
available in the CTB. We used a maximum-
matching algorithm and a dictionary compiled from
the CTB (Sproat et al, 1996; Xue, 2001) to do seg-
mentation, and trained a maximum entropy part-of-
speech tagger (Ratnaparkhi, 1998) and TAG-based
parser (Bikel and Chiang, 2000) on the CTB to do
tagging and parsing.4 Then the same feature extrac-
tion and model-training was done for the PDN cor-
pus as for the CTB.
The system performance is much lower for the
PDN than for the CTB, for several reasons. First,
the PDN corpus is more balanced than the CTB,
which contains primarily financial articles. A wider
range of usages of the words was expressed in PDN
than in CTB, making the disambiguation task more
difficult; the average number of senses for the PDN
words was 8.2 (compared to 3.5 for CTB), and the
4On held-out portions of the CTB, the accuracy of the seg-
mentation and part-of-speech tagging are over 95%, and the
accuracy of the parsing is 82%, which are comparable to the
performance of the English preprocessors. The performance
of these preprocessors is naturally expected to degrade when
transferred to a different domain.
baseline accuracy was 58.0% (compared to 76.7%
for CTB). Also, using automatically preprocessed
data for the PDN introduced noise that was not
present for the manually preprocessed CTB. Despite
these differences between PDN and CTB, the trends
in using increasingly richer linguistic preprocessing
are similar. Table 3 shows that adding more features
from richer levels of linguistic annotation yielded
no significant improvement over using only collo-
cational features. In fact, using only lexical collo-
cations from automatic segmentation was sufficient
to produce close to the best results. Table 4 shows
the system performance using the available manual
segmentation and part-of-speech tagging. While us-
ing part-of-speech tags seems to be better than us-
ing only lexical collocations, the difference is not
significant.
4 Conclusion
We have demonstrated the high performance of
maximum entropy models for word sense disam-
biguation in English, and have applied the same ap-
proach successfully to Chinese. While SENSEVAL-
2 showed that methods that work on English also
tend to work on other languages, our experiments
have revealed striking differences in the types of
features that are important for English and Chi-
nese WSD. While parse information seemed crucial
for English WSD, it only played a minor role in
Chinese; in fact, the improvement in Chinese per-
formance contributed by manual parse information
in the CTB disappeared altogether when automatic
parsing was done for the PDN. The fact that brack-
eting was more important for English than Chinese
WSD suggests that predicate-argument information
and selectional restrictions may play a more impor-
tant role in distinguishing English verb senses than
Chinese senses. Or, it may be the case that Chi-
nese verbs tend to be adjacent to their arguments,
so collocational information is sufficient to capture
the same information that would require parsing in
English. This is a question for further study.
The simpler level of linguistic processing re-
quired to achieve relatively high sense-tagging ac-
curacy in Chinese highlights an important differ-
ence between Chinese and English. Chinese is dif-
ferent from English in that much of Chinese linguis-
tic ambiguity occurs at the basic level of word seg-
mentation. Chinese word segmentation is a major
task in itself, and it seems that once this is accom-
plished little more needs to be done for sense dis-
ambiguation. Our experience in English has shown
that the ability to identify multi-word constructions
significantly improves sense-tagging performance.
Multi-character Chinese words, which are identified
by word segmentation, may be the analogy to En-
glish multi-word constructions.
5 Acknowledgments
This work has been supported by National Sci-
ence Foundation Grants, NSF-9800658 and NSF-
9910603, and DARPA grant N66001-00-1-8915 at
the University of Pennsylvania. The authors would
also like to thank the anonymous reviewers for their
valuable comments.
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1).
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the chinese tree-
bank. In Proceedings of the Second Chinese Lan-
guage Processing Workshop, Hong Kong.
Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: A high-
performance learning name-finder. In Proceed-
ings of the Fifth Conference on Applied Natural
Language Processing, Washington, DC.
Martin Chodorow, Claudia Leacock, and George A.
Miller. 2000. A topical/local classifier for word
sense identification. Computers and the Human-
ities, 34(1-2), April. Special Issue on SENSE-
VAL.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the As-
sociation for Computational Linguistics, Madrid,
Spain, July.
Hoa Trang Dang and Martha Palmer. 2002. Com-
bining contextual features for word sense disam-
biguation. In Proceedings of the Workshop on
Word Sense Disambiguation: Recent Successes
and Future Directions, Philadelphia, PA.
Philip Edmonds and Scott Cotton. 2001.
SENSEVAL-2: Overview. In Proceedings
of SENSEVAL-2: Second International Work-
shop on Evaluating Word Sense Disambiguation
Systems, Toulouse, France, July.
Chinese-English Translation Assistance Group.
1982. Chinese Dictionaries: an Extensive Bib-
liography of Dictionaries in Chinese and Other
Languages. Greenwood Publishing Group.
Nancy Ide and Jean Veronis. 1998. Introduction to
the special issue on word sense disambiguation:
The state of the art. Computational Linguistics,
24(1).
Adam Kilgarriff and Martha Palmer. 2000. In-
troduction to the special issue on SENSEVAL.
Computers and the Humanities, 34(1-2), April.
Special Issue on SENSEVAL.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English SENSEVAL. Computers
and the Humanities, 34(1-2), April. Special Issue
on SENSEVAL.
Ruo-Ping Mo. 1992. A conceptual structure that is
suitable for analysing chinese. Technical Report
CKIP-92-04, Academia Sinica, Taipei, Taiwan.
M. Palmer, Chunghye Han, Fei Xia, Dania Egedi,
and Joseph Rosenzweig. 2000. Constraining lex-
ical selection across languages using tags. In
Anne Abeille and Owen Rambow, editors, Tree
Adjoining Grammars: formal, computational
and linguistic aspects. CSLI, Palo Alto, CA.
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. En-
glish tasks: All-words and verb lexical sample.
In Proceedings of SENSEVAL-2: Second Interna-
tional Workshop on Evaluating Word Sense Dis-
ambiguation Systems, Toulouse, France, July.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
Richard Sproat, Chilin Shih, William Gale, and
Nancy Chang. 1996. A stochastic finite-state
word segmentation algorithm for chinese. Com-
putational Linguistics, 22(3).
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu-Dong Chiou,
Shizhe Huang, Tony Kroch, and Mitch Mar-
cus. 2000. Developing guidelines and ensuring
consistency for chinese text annotation. In Pro-
ceedings of the second International Conference
on Language Resources and Evaluation, Athens,
Greece.
Nianwen Xue. 2001. Defining and Automatically
Identifying Words in Chinese. Ph.D. thesis, Uni-
versity of Delaware.
David Yarowsky. 2000. Hierarchical decision lists
for word sense disambiguation. Computers and
the Humanities, 34(1-2), April. Special Issue on
SENSEVAL.
Building a Large-Scale Annotated Chinese Corpus

Nianwen Xue
IRCS, University of Pennsylvania
Suite 400A, 3401 Walnut Street
Philadelphia, PA 19104, USA
xueniwen@linc.cis.upenn.edu
Fu-Dong Chiou and Martha Palmer
CIS, University of Pennsylvania
200 S 33rd Street
Philadelphia, PA 19104, USA
{chioufd,mpalmer}@linc.cis.upenn.edu

Abstract
In this paper we address issues related to
building a large-scale Chinese corpus. We
try to answer four questions: (i) how to
speed up annotation, (ii) how to maintain
high annotation quality, (iii) for what
purposes is the corpus applicable, and
finally (iv) what future work we anticipate.
Introduction
The Penn Chinese Treebank (CTB) is an
ongoing project, with its objective being to
create a segmented Chinese corpus annotated
with POS tags and syntactic brackets. The first
installment of the project (CTB-I) consists of
Xinhua newswire between the years 1994 and
1998, totaling 100,000 words, fully segmented,
POS-tagged and syntactically bracketed and it
has been released to the public via the Penn
Linguistic Data Consortium (LDC). The
preliminary results of this phase of the project
have been reported in Xia et al (2000). Currently
the second installment of the project, the
400,000-word CTB-II is being developed and is
expected to be completed early in the year 2003.
CTB-II will follow the standards set up in the
segmentation (Xia 2000b), POS tagging (Xia
2000a) and bracketing guidelines (Xue and Xia
2000) and it will use articles from Peoples'
Daily, Hong Kong newswire and material
translated into Chinese from other languages in
addition to the Xinhua newswire used in CTB-I
in an effort to diversify the sources.

The availability of CTB-I changed our approach
to CTB-II considerably. Due to the existence of
CTB-I, we were able to train new automatic
Chinese language processing (CLP) tools, which
crucially use annotated corpora as training
material. These tools are then used for
preprocessing in the development of the CTB-II.
We also developed tools to control the quality of
the corpus. In this paper, we will address three
issues in the development of the Chinese
Treebank: annotation speed, annotation accuracy
and usability of the corpus. Specifically, we
attempt to answer four questions: (i) how do we
speed up the annotation process, (ii) how do we
maintain high quality, i.e. annotation accuracy
and inter-annotator consistency during the
annotation process, and (iii) for what purposes is
the corpus applicable, and (iv) what are our
future plans? Although we will touch upon
linguistic problems that are specific to Chinese,
we believe these issues are general enough for
the development of any single language corpus.
1 Annotation Speed
There are three main factors that affect the
annotation speed : annotators? background,
guideline design and more importantly, the
availability of preprocessing tools. We will
discuss how each of these three factors affects
annotation speed.
1.1 Annotator Background
Even with the best sets of guidelines, it is
important that annotators have received
considerable training in linguistics, particularly
in syntax. In both the segmentation/POS tagging
phase and the syntactic bracketing phase,
understanding the structure of the sentences is
essential for correct annotation with reasonable
speed. For example,
 
/de is assigned two part-
of-speech tags, depending on where it occurs in
the sentence. It is tagged as DEC when it marks
the end of the preceding modifying clause and
DEG when it follows a nominal phrase. This
distinction is useful in that it marks different
relations : between the nominal phrase and the
noun head, and between the clause and the noun
head respectively.

1.a.   /NN  /DEG  /NN
        leader         DE          responsibility
        ?leader?s responsibility?
  b. 	 /NT 
 /VV  /DEC  	 /NN  /NN
      recently hold       DE        demonstration
      ?recently held demonstration?

During the bracketing phase, the modifying
clause is further divided into relative clauses and
complement (appositive) clauses. The structures
of these two types of clauses are different, as
illustrated in 2:

2.a. (NP (CP (WHNP-1 *OP*)
                      (CP  (IP  (NP-SBJ (-NONE- *T*-1))
                        (VP (NP-TMP  /NT)     recently
                         (VP  /hold)))               hold
                                /DEC))
         (NP  /NN  Automatic Predicate Argument Analysis of the Penn TreeBank 
 
Martha Palmer, Joseph Rosenzweig and Scott Cotton 
CIS Department, University of Pennsylvania 
{mpalmer,josephr,cotton}@linc.cis.upenn.edu 
 
 
1. INTRODUCTION 
 
One of the primary tasks of Information 
Extraction is recognizing all of the different guises 
in which a particular type of event can appear.  For 
instance, a meeting between two dignitaries can be 
referred to as A meets B or A and B meet, or a 
meeting between A and B took place/was 
held/opened/convened/finished/dragged on or A  
had/presided over a meeting/conference with B 
 
There are several different lexical items that can 
be used to refer to the same type of event, and 
several different predicate argument patterns that can 
be used to specify the participants.  Correctly 
identifying the type of the event and the roles of the 
participants is a critical factor in accurate 
information extraction.  In this paper we refer to the 
specific subtask of participant role identification as 
predicate argument tagging. The type of syntactic 
and semantic information associated with verbs in 
Levin?s Preliminary Classification of English verbs, 
[Levin,93] can be a useful resource for an automatic 
predicate argument tagging system. For instance, the 
?meet? class includes the following members, meet, 
consult, debate and visit, which can all be used to 
refer to the meeting event type described above.  In 
addition, the following types of syntactic frames are 
associated with these verbs: 
 
A met/visited/debated/consulted B 
A met/visited/debated/consulted with B. 
A and B met/visited/debated/consulted 
(with each other). 
 
This type of frame information can be specified 
at the class level, but there is always a certain 
amount of verb-specific information that must still 
be associated with the individual lexical items, such 
as sense distinctions. For the purposes of this paper 
we will only be considering sense distinctions based 
on different predicate argument structures.  We 
begin by giving more information about the Levin 
classes and then describe the system that 
automatically labels the arguments in a predicate 
argument structure.  We end by giving the results of 
evaluating this system versus human annotators 
performing the same task.   Our input to the tagger is 
the Penn TreeBank [Marcus, 94], so the sentences 
already have accurate syntactic parses associated 
with them.   
 
2. LEXICON GUIDELINES 
 
As mentioned above, Levin classes provide the 
theoretical underpinnings for many of our choices 
for basic predicate-argument structures [Levin, 93]. 
Levin verb classes are based on the ability of a verb 
to occur or not occur in pairs of syntactic frames that 
are in some sense meaning preserving (diathesis 
alternations). The distribution of syntactic frames in 
which a verb can appear determines its class 
membership. The sets of syntactic frames associated 
with a particular Levin class are not intended to be 
arbitrary, and they are supposed to reflect underlying 
semantic components that constrain allowable 
arguments. For example, break verbs and cut verbs 
are similar in that they can all occur as transitives 
and in the middle construction, John broke the 
window, Glass breaks easily, John cut the bread, 
This loaf cuts easily. However, only break verbs can 
also occur in the simple intransitive, The window 
broke, *The bread cut. Notice that for all of these 
verbs, the subject of the intransitive, The window 
broke, plays the same role as the object of the 
transitive, John broke the window.  Our goal is to 
capture this by using consistent argument labels, in 
this case Arg1 for the window in both sentences.  So, 
for example, shake and rock would get the following 
annotation: 
 
The earthquake  shook  the building. 
Arg0     REL Arg1 
 
The walls shook; 
Arg1 REL 
 
the building rocked. 
Arg1  REL 
 
VerbNet In a related project funded by NSF, 
NSF-IIS98-00658, we are currently constructing a 
lexicon, VerbNet, that is intended to overcome some 
of the limitations of WordNet, an on-line lexical 
database of English, [Miller, 90], by addressing 
specifically the needs of natural language processing 
applications. This lexicon exploits the systematic 
link between syntax and semantics that motivates the 
Levin classes, and thus provides a clear and regular 
association between syntactic and semantic 
properties of verbs and verb classes, [Dang, et al 98, 
00, Kipper, et al 00]. Specific sets of syntactic 
configurations and appropriate selectional 
restrictions on arguments are associated with 
individual senses. This lexicon gives us a first 
approximation of sense distinctions that are reflected 
in varying predicate argument structures. As such 
these entries provide a suitable foundation for 
directing consistent predicate-argument labeling of 
training data. 
The senses in VerbNet are in turn linked to 
one or more WordNet senses. Since our focus is 
predicate-argument structure, we can rely on 
rigorous and objective sense distinction criteria 
based on syntax. Purely semantic distinctions, 
such as those made in WordNet, are subjective 
and potentially unlimited. Our senses are 
therefore much more coarse-grained than 
WordNet, since WordNet senses are purely 
semantically motivated and often cannot be 
distinguished syntactically. However, some 
senses that share syntactic properties can still be 
distinguished clearly by virtue of different 
selectional restrictions, which we will also be 
exploring in the NSF project. 
 
3. AUTOMATIC EXTRACTION OF 
PREDICATE-ARGUMENT 
RELATIONS FROM PARSED 
CORPORA 
 
The predicate-argument analysis of a parse tree 
from a corpus such as the Treebank corpus is 
performed in three main phases. First, root forms of 
inflected words are identified using a morphological 
analyzer derived from the WordNet stemmer and 
from inflectional information in machine-readable 
dictionaries such as the Project Gutenberg version of 
Webster. Also in this phase, phrasal items such as 
verb-particle constructions, idioms and compound 
nominals are identified. An efficient matching 
algorithm is used which is capable of recognizing 
both continuous and discontinuous phrases, and 
phrases where the order of words is not fixed. The 
matching algorithm makes use of hierarchical 
declarative constraints on the possible realizations of 
phrases in the lexicon, and can exploit syntactic 
contextual cues if a syntactic analysis of the input, 
such as the parse tree structure of the Treebank, is 
present. In the next phase, the explicit antecedents of 
empty constituents are read off from the Treebank 
annotation, and gaps are filled where implicit 
linkages have been left unmarked. This is done by 
heuristic examination of the local syntactic context 
of traces and relative clause heads. If no explicit 
markings are present (for automatically generated 
parses or old-style Treebank parses), they are 
inferred. Estimated accuracy of this phase of the 
algorithm is upwards of 90 percent. 
 
Finally, an efficient tree-template pattern 
matcher is run on the Treebank parse trees, to 
identify syntactic relations that signal a predicate-
argument relationship between lexical items. The 
patterns used are fragmentary tree templates similar 
to the elementary and auxiliary trees of a Tree 
Adjoining Grammar [XTAG, 95]. Each template 
typically corresponds to a predication over one or 
more arguments. There are approximately 200 
templates for: transitive, intransitive and ditransitive 
verbs operating on their subjects, objects and 
indirect objects; prenominal and predicate 
adjectives, operating on the nouns they modify; 
subordinating conjunctions operating on the two 
clauses that they link; prepositions; determiners; and 
so on.  The templates are organized into a compact 
network in which shared substructures need to be 
listed only once, even when they are present in many 
templates. 
 
Templates are matched even if they are not 
contiguous in the tree, as long as the intervening 
material is well-formed. This allows a transitive 
template for example to match a sentence where 
there is an intervening auxiliary verb between the 
subject and the main transitive verb, as in He was 
dropping it. The mechanism for handling such cases 
resembles the adjunction mechanism in Tree 
Adjoining Grammar. 
 
 
Tree grammar template for progressive auxiliary 
verb, licensing discontinuity in main verb tree 
 
 
When a template has been identified, it is 
instantiated with the lexical items that occur in its 
predicate and argument positions. Each template is 
associated with one or more annotated template sets, 
by means of which it is linked to a bundle of 
thematic or semantic features, and to a class of 
lexical items that license the template?s occurrence 
with those features. For instance, if the template is 
an intransitive verb tree, it will be associated both 
with an unergative feature bundle, indicating that its 
subject should have the label Arg0, and also with an 
unaccusative bundle where the subject is marked as 
Arg1. Which of the feature bundles gets used 
depends   on  the   semantic    class of the word   that 
 
 
Recognition of progressive auxiliary tree which 
modifies and splits transitive-verb tree for drop 
in Treebank corpus 
 
appears in the predicate position of the template. If 
the predicate is a causative verb that takes the 
unaccusative alternation, the subject will be assigned 
the Arg1 label. If however it is a verb of creation, for 
example, the subject will be an Arg0. The verb 
semantics that inform the predicate-argument 
extractor are theoretically motivated by the Levin 
classes [Levin, 93], but the actual lexical 
information it uses is not derived from Levin?s work. 
Rather, it draws on information available in the 
WordNet 1.6 database [Miller, 90] and on frame 
codes are derived from the annotation scheme used 
in the Susanne corpus [Sampson, 95]. 
 
For example, one entry for the verb develop 
specifies its WordNet synset membership, and 
indicates its participation in the unaccusative 
alternation with the code o_can_become_s 
 
develop SF:so_N_N+W:svJ3W_W:svIM2+o_can_become_s 
  
The prefix SF: signifies that this is a frame code 
derived from the Susanne corpus. Each frame code 
picks out a lexical class of the words that take it, and 
the frame codes are organized into an inheritance 
network as well. The frame codes in turn are linked 
to annotated template sets, which describe how these 
frames can actually appear in the syntactic 
bracketing format of the TreeBank. In the case of the 
above frame code for an alternating transitive verb, 
two template sets are linked: TG:V_so_N_N for the 
frame with a subject and an object (here notated with 
s and o); and TG:V_s_N+causative, for the 
unaccusative frame. Each of the template sets lists 
tree-grammar templates for all the variations of 
syntactic structure that its corresponding frame may 
take on. A template for the canonical structure of a 
simple declarative sentence involving that frame will 
be present in the set, but additional templates will be 
added for the forms the frame takes in relative 
clauses, questions, or passive constructions. 
 
The features for each set are listed separately 
from the templates, with indications of where they 
should be interpreted within the various template 
structures. Hence the template set 
TG:V_s_N+causative includes the feature 
TGC:subject+print_as=TGPL:arg1 as part of 
its feature bundle. This serves to associate the label 
Arg1 with the subject node in each template in the 
set. When the predicate-argument extractor is able to 
instantiate such a template, thereby connecting its 
subject node with a piece of a TreeBank tree, it 
knows to print that piece of the tree as Arg1 of the 
predicate for that template. If another annotated 
feature set were active instead, for instance in a case 
where the predicate of the template does not belong 
to a verb class which licenses the unaccusative frame 
code and its associated annotated template set 
(TG:V_s_N+causative), the label of the subject 
might be different. 
 
4. EVALUATION 
 
The current implementation of the tagger assigns 
predicate argument structures to all of the 6500 
verbs that occur in the Penn Treebank.  However, 
our evaluation of its accuracy is not yet so 
comprehensive.  Our first preliminary evaluation of 
the performance of the tagger was based on a 5000 
word section of the Penn TreeBank.  The tagger was 
run on this, and the argument labeling was 
subsequently hand corrected by a linguistics 
graduate student, giving an accuracy rate of 81% out 
of 160 predicate argument structures. We have since 
automatically tagged and hand corrected an 
additional 660 predicate argument structures, with 
an accuracy rate of 86%, (556 structures), giving us 
a combined accuracy rate of 83.7%.  There are over 
100 verbs involved in the evaluation.   The number 
of possible frames for the verbs in the second test 
ranges from 13 frames to 30, with the typical 
number being in the teens.  Not all of these frames 
actually appear in the TreeBank data.  
 
These results compare favorably with the results 
reported by Gildea and Jurafsky of 80.7% on their 
development set, (76.9% on the test set.) Their data 
comes from the Framenet project, [Lowe, et al, 97], 
which has been in existence for several years, and 
consisted of over 900 verbs out of 1500 words and 
almost 50,000 sentences.  The Framenet project also 
uses more fine-grained semantic role labels, 
although it should be possible to map from our 
Arg0, Arg1 labels to their labels.  They used 
machine learning techniques applied to human 
annotated data, whereas our tagger does not 
currently use statistics at all, and is primarily rule-
based. Once we have sufficient amounts of data 
annotated we plan to experiment with hybrid 
approaches. 
 
5. ACKNOWLEDGEMENTS 
 
We would like to thank Paul Kingsbury and 
Chris Walker for their annotation efforts, and 
Aravind Joshi, Mitch Marcus, Hoa Dang and 
Christiane Fellbaum for their comments on 
predicate-argument tagging as a task.  This work has 
been funded by DARPA N66001-00-1-8915 and 
NSF 9800658. 
 
6. REFERENCES 
 
[1] Hoa Trang Dang, Karin Kipper, and Martha 
Palmer. Integrating compositional semantics into a 
verb lexicon. In Proceedings of the Eighteenth 
International Conference on Computational 
Linguistics (COLING-2000), Saarbr"ucken, 
Germany, July-August 2000. 
 
[2] Hoa Trang Dang, Karin Kipper, Martha Palmer, 
and Joseph Rosenzweig. Investigating regular sense 
extensions based on intersective levin classes. In 
Proceedings of Coling-ACL98, Montreal, CA, 
August 1998. 
 
[3] Daniel Gildea and Daniel Jurafsky,  Automatic 
Labeling of Semantic Roles, In Proceedings of the 
Association for Computational Linguistics 
Conference, Hong Kong, October, 2000. 
 
[4] Karin Kipper, Hoa Trang Dang, and Martha 
Palmer. Class-based construction of a verb lexicon. 
In Proceedings of the Seventh National Conference 
on Artificial Intelligence (AAAI-2000), Austin, TX, 
July-August 2000. 
 
[5] Beth Levin. English Verb Classes and 
Alternations A Preliminary Investigation. 1993. 
 
[6] J.B. Lowe, C.F. Baker, and C.J. Fillmore. A 
frame-semantic approach to semantic annotation. In 
Proceedings 1997 Siglex Workshop/ANLP97, 
Washington, D.C., 1997. 
 
[7] Mitch Marcus. The penn treebank: A revised 
corpus design for extracting predicate argument 
structure. In Proceedings of the ARPA Human 
Language Technology Workshop, Princeton, NJ, 
March 1994. 
 
[8] G. Miller, R. Beckwith, C. Fellbaum, D. Gross, 
and K. Miller. Five papers on wordnet. Technical 
Report 43, Cognitive Science Laboratory, Princeton 
University, July 1990. 
 
[9] Martha Palmer, Hoa Trang Dang, and Joseph 
Rosenzweig. Sense tagging the penn treebank. In 
Proceedings of the Second Language Resources and 
Evaluation Conference, Athens, Greece. 
 
[10] The XTAG-Group. A Lexicalized Tree 
Adjoining Grammar for English. Technical Report 
IRCS 95-03, University of Pennsylvania, 1995. 
Converting Dependency Structures to Phrase Structures
Fei Xia and Martha Palmer
University of Pennsylvania
Philadelphia, PA 19104, USA
ffxia,mpalmerg@linc.cis.upenn.edu
1. INTRODUCTION
Treebanks are of two types according to their annotation schemata:
phrase-structure Treebanks such as the English Penn Treebank [8]
and dependency Treebanks such as the Czech dependency Tree-
bank [6]. Long before Treebanks were developed and widely used
for natural language processing, there had been much discussion of
comparison between dependency grammars and context-free phrase-
structure grammars [5]. In this paper, we address the relationship
between dependency structures and phrase structures from a practi-
cal perspective; namely, the exploration of different algorithms that
convert dependency structures to phrase structures and the evalua-
tion of their performance against an existing Treebank. This work
not only provides ways to convert Treebanks from one type of rep-
resentation to the other, but also clarifies the differences in repre-
sentational coverage of the two approaches.
2. CONVERTING PHRASE STRUCTURES
TO DEPENDENCY STRUCTURES
The notion of head is important in both phrase structures and
dependency structures. In many linguistic theories such as X-bar
theory and GB theory, each phrase structure has a head that de-
termines the main properties of the phrase and a head has several
levels of projections; whereas in a dependency structure the head
is linked to its dependents. In practice, the head information is ex-
plicitly marked in a dependency Treebank, but not always so in a
phrase-structure Treebank. A common way to find the head in a
phrase structure is to use a head percolation table, as discussed in
[7, 1] among others. For example, the entry (S right S/VP) in the
head percolation table says that the head child1 of an S node is the
first child of the node from the right with the label S or VP.
Once the heads in phrase structures are found, the conversion
from phrase structures to dependency structures is straightforward,
as shown below:
(a) Mark the head child of each node in a phrase structure, using
the head percolation table.
1The head-child of a node XP is the child of the node XP that is
the ancestor of the head of the XP in the phrase structure.
.
(b) In the dependency structure, make the head of each non-
head-child depend on the head of the head-child.
Figure 1 shows a phrase structure in the English Penn Treebank
[8]. In addition to the syntactic labels (such as NP for a noun
phrase), the Treebank also uses function tags (such as SBJ for the
subject) for grammatical functions. In this phrase structure, the root
node has two children: the NP and the VP. The algorithm would
choose the VP as the head-child and the NP as a non-head-child,
and make the head Vinkin of the NP depend on the head join of
the VP in the dependency structure. The dependency structure of
the sentence is shown in Figure 2. A more sophisticated version
of the algorithm (as discussed in [10]) takes two additional tables
(namely, the argument table and the tagset table) as input and pro-
duces dependency structures with the argument/adjunct distinction
(i.e., each dependent is marked in the dependency structure as ei-
ther an argument or an adjunct of the head).
S
VP
NNP
Vinken
MD
will
VP
VB
join
NP
DT NN
the board
as JJ
IN NP
NNP CD
29Nov
DT NN
director
a
NP-SBJ
PP-CLR NP-TMP
nonexecutive
Figure 1: A phrase structure in the Penn Treebank
join
willVinken 
nonexecutivea 
board as 
the
29
Novdirector
Figure 2: A dependency tree for the sentence in Figure 1. Heads
are marked as parents of their dependents in an ordered tree.
It is worth noting that quite often there is no consensus on what
the correct dependency structure for a particular sentence should
be. To build a dependency Treebank, the Treebank annotators must
decide which word depends on which word; for example, they have
to decide whether the subject Vinken in Figure 1 depends on the
modal verb will or the main verb join. In contrast, the annotators
for phrase-structure Treebanks do not have to make such decisions.
The users of phrase-structure Treebanks can modify the head per-
colation tables to get different dependency structures from the same
phrase structure. In other words, phrase structures offer more flex-
ibility than dependency structures with respect to the choices of
heads.
The feasibility of using the head percolation table to identify the
heads in phrase structures depends on the characteristics of the lan-
guage, the Treebank schema, and the definition of the correct de-
pendency structure. For instance, the head percolation table for a
strictly head-final (or head-initial) language is very easy to build,
and the conversion algorithm works very well. For the English
Penn Treebank, which we used in this paper, the conversion algo-
rithm works very well except for the noun phrases with the appos-
itive construction. For example, the conversion algorithm would
choose the appositive the CEO of FNX as the head child of the
phrase John Smith, the CEO of FNX, whereas the correct head child
should be John Smith.
3. CONVERTING DEPENDENCY STRUC-
TURES TO PHRASE STRUCTURES
The main information that is present in phrase structures but not
in dependency structures is the type of syntactic category (e.g., NP,
VP, and S); therefore, to recover syntactic categories, any algorithm
that converts dependency structures to phrase structures needs to
address the following questions:
Projections for each category: for a category X, what kind of
projections can X have?
Projection levels for dependents: Given a category Y depends
on a category X in a dependency structure, how far should Y
project before it attaches to X?s projection?
Attachment positions: Given a category Y depends on a cate-
gory X in a dependency structure, to what position on X?s projec-
tion chain should Y?s projection attach?
In this section, we discuss three conversion algorithms, each of
which gives different answers to these three questions. To make
the comparison easy, we shall apply each algorithm to the depen-
dency structure (d-tree) in Figure 2 and compare the output of the
algorithm with the phrase structure for that sentence in the English
Penn Treebank, as in Figure 1.
Evaluating these algorithms is tricky because just like depen-
dency structures there is often no consensus on what the correct
phrase structure for a sentence should be. In this paper, we mea-
sure the performance of the algorithms by comparing their output
with an existing phrase-structure Treebank (namely, the English
Penn Treebank) because of the following reasons: first, the Tree-
bank is available to the public, and provides an objective although
imperfect standard; second, one goal of the conversion algorithms
is to make it possible to compare the performance of parsers that
produce dependency structures with the ones that produce phrase
structures. Since most state-of-the-art phrase-structure parsers are
evaluated against an existing Treebank, we want to evaluate the
conversion algorithms in the same way; third, a potential appli-
cation of the conversion algorithms is to help construct a phrase-
structure Treebank for one language, given parallel corpora and the
phrase structures in the other language. One way to evaluate the
quality of the resulting Treebank is to compare it with an existing
Treebank.
3.1 Algorithm 1
According to X-bar theory, a category X projects to X?, which
further projects to XP. There are three types of rules, as shown in
Figure 3(a). Algorithm 1, as adopted in [4, 3], strictly follows X-
bar theory and uses the following heuristic rules to build phrase
structures:
X YP(Spec)
XP
X?
X? WP(Mod)
ZP(Arg)X
WY Z
(c) phrase structure
(Arg) (Mod)(Spec)
(1) XP -> YP X?
(2) X? -> X? WP
(3) X? -> X ZP
(a) rules in
X-bar theory
(b) d-tree
Figure 3: Rules in X-bar theory and Algorithm 1 (which is
based on it)
Two levels of projections for any category: any category X has
two levels of projection: X? and XP.
Maximal projections for dependents: a dependent Y always
projects to Y? then YP, and the YP attaches to the head?s projection.
Fixed positions of attachment: Dependents are divided into
three types: specifiers, modifiers, and arguments. Each type of
dependent attaches to a fixed position, as shown in Figure 3(c).
The algorithm would convert the d-tree in Figure 3(b) to the
phrase structure in Figure 3(c). If a head has multiple modifiers,
the algorithm could use either a single X? or stacked X? [3]. Figure
4 shows the phrase structure for the d-tree in Figure 2, where the
algorithm uses a single X? for multiple modifiers of the same head.2
VP
V?
MD
will
VB
join
NP
Vinken
NNP
N? PP
P?
IN
as
NP
V?
nonexecutive
VP
N?
Nov
NNP
N?
NP
N?
NP
CD
29
ADJP
JJ
ADJ?
N?
NN
director
N?
V?
board
N?
NN
NP
DT
DT?
DTP
the
a
DT
DT?
DTP
Figure 4: The phrase structure built by algorithm 1 for the d-
tree in Figure 2
3.2 Algorithm 2
Algorithm 2, as adopted by Collins and his colleagues [2] when
they converted the Czech dependency Treebank [6] into a phrase-
structure Treebank, produces phrase structures that are as flat as
possible. It uses the following heuristic rules to build phrase struc-
tures:
One level of projection for any category: X has only one level
of projection: XP.
2To make the phrase structure more readable, we use N? and NP as
the X? and XP for all kinds of POS tags for nouns (e.g., NNP, NN,
and CD). Verbs and adjectives are treated similarly.
Minimal projections for dependents: A dependent Y does not
project to Y P unless it has its own dependents.
Fixed position of attachment: A dependent is a sister of its
head in the phrase structure.3
The algorithm treats all kinds of dependents equally. It converts
the pattern in Figure 5(a) to the phrase structure in Figure 5(b).
Notice that in Figure 5(b), Y does not project to YP because it does
not have its own dependents. The resulting phrase structure for the
d-tree in Figure 2 is in Figure 6, which is much flatter than the one
produced by Algorithm 1.
Y W
XP
(dep) (dep) (dep)
Z
X
(a) d-tree (b) phrase structure
X ZP WPY
Figure 5: The scheme for Algorithm 2
VP
VB
join
MD NP
DT
PP
IN
nonexecutive
NP
NNP CD
Nov 29
NP
NNJJDT
the
NN
board as
directora
will
NNP
Vinken
Figure 6: The phrase structure built by Algorithm 2 for the
d-tree in Figure 2
3.3 Algorithm 3
The previous two algorithms are linguistically sound. They do
not use any language-specific information, and as a result there are
several major differences between the output of the algorithms and
the phrase structures in an existing Treebank, such as the Penn En-
glish Treebank (PTB).
Projections for each category: Both algorithms assume that the
numbers of projections for all the categories are the same, whereas
in the PTB the number of projections varies from head to head. For
example, in the PTB, determiners do not project, adverbs project
only one level to adverbial phrases, whereas verbs project to VP,
then to S, then to SBAR.4
Projection levels for dependents: Algorithm 1 assumes the
maximal projections for all the dependents, while Algorithm 2 as-
sumes minimal projections; but in the PTB, the level of projection
of a dependent may depend on several factors such as the categories
of the dependent and the head, the position of the dependent with
respect to the head, and the dependency type. For example, when a
3If a dependent Y has its own dependents, it projects to YP and YP
is a sister of the head X; otherwise, Y is a sister of the head X.
4S is similar to IP (IP is the maximal projection of INFL) in GB
theory, so is SBAR to CP (CP is the maximal projection of Comp);
therefore, it could be argued that only VP is a projection of verbs
in the PTB. Nevertheless, because PTB does not mark INFL and
Comp, we treat S and SBAR as projections of verbs.
noun modifies a verb (or VP) such as yesterday in he came yester-
day, the noun always projects to NP, but when a noun N
1
modifiers
another noun N
2
, N
1
projects to NP if N
1
is to the right of N
2
(e.g., in an appositive construction) and it does not project to NP if
N
1
is to the left of N
2
.
Attachment positions: Both algorithms assume that all the de-
pendents of the same dependency type attach at the same level (e.g.,
in Algorithm 1, modifiers are sisters of X?, while in Algorithm 2,
modifiers are sisters of X); but in the PTB, that is not always true.
For example, an ADVP, which depends on a verb, may attach to
either an S or a VP in the phrase structure according to the position
of the ADVP with respect to the verb and the subject of the verb.
Also, in noun phrases, left modifiers (e.g., JJ) are sisters of the head
noun, while the right modifiers (e.g., PP) are sisters of NP.
For some applications, these differences between the Treebank
and the output of the conversion algorithms may not matter much,
and by no means are we implying that an existing Treebank pro-
vides the gold standard for what the phrase structures should be.
Nevertheless, because the goal of this work is to provide an algo-
rithm that has the flexibility to produce phrase structures that are
as close to the ones in an existing Treebank as possible, we pro-
pose a new algorithm with such flexibility. The algorithm distin-
guishes two types of dependents: arguments and modifiers. The
algorithm also makes use of language-specific information in the
form of three tables: the projection table, the argument table, and
the modification table. The projection table specifies the projec-
tions for each category. The argument table (the modification table,
resp.) lists the types of arguments (modifiers, resp) that a head can
take and their positions with respect to the head. For example, the
entry V ! V P ! S in the projection table says that a verb can
project to a verb phrase, which in turn projects to a sentence; the
entry (P 0 1 NP/S) in the argument table indicates that a preposition
can take an argument that is either an NP or an S, and the argument
is to the right of the preposition; the entry (NP DT/JJ PP/S) in the
modification table says that an NP can be modified by a determiner
and/or an adjective from the left, and by a preposition phrase or a
sentence from the right.
Given these tables, we use the following heuristic rules to build
phrase structures:5
One projection chain per category: Each category has a unique
projection chain, as specified in the projection table.
Minimal projection for dependents: A category projects to a
higher level only when necessary.
Lowest attachment position: The projection of a dependent at-
taches to a projection of its head as lowly as possible.
The last two rules require further explanation, as illustrated in
Figure 7. In the figure, the node X has three dependents: Y and Z
are arguments, and W is a modifier of X. Let?s assume that the al-
gorithm has built the phrase structure for each dependent. To form
the phrase structure for the whole d-tree, the algorithm needs to
attach the phrase structures for dependents to the projection chain
X
0
; X
1
; :::X
k of the head X. For an argument such as Z, sup-
pose its projection chain is Z0; Z1; :::Zu and the root of the phrase
structure headed by Z is Zs. The algorithm would find the low-
est position Xh on the head projection chain, such that Z has a
projection Zt that can be an argument of Xh 1 according to the
argument table and Zt is no lower than Zs on the projection chain
for Z. The algorithm then makes Zt a child of Xh in the phrase
structure. Notice that based on the second heuristic rule (i.e., mini-
mal projection for dependents), Zt does not further project to Zu in
5In theory, the last two heuristic rules may conflict each other in
some cases. In those cases, we prefer the third rule over the second.
In practice, such conflicting cases are very rare, if exist.
this case although Zu is a valid projection of Z. The attachment for
modifiers is similar except that the algorithm uses the modification
table instead of the argument table.6
Z
X
Y W
(Arg) (Arg) (Mod)
k
0
j
j-1
h
l
m
n
W
W
W
W
00
h-1
X
X
X
X
X
X
Y
X i
Y
Y
Y
q
Z
Z
0
P
r
s
Zt
Zu
(b) phrase structure(a) d-tree 
Figure 7: The scheme for Algorithm 3
MD
will
VB
join
VP
S
NNP
Vinken
NP
DT NN
boardthe
PP
NP
DT JJ NN
a director
nonexecutive
IN
as
CD
NP
NNP
Nov 29
NP
(a)
(b)
(c)
(d)
(e)
(f)
Figure 8: The phrase structure produced by Algorithm 3
The phrase structure produced by Algorithm 3 for the d-tree in
Figure 2 is in Figure 8. In Figure 8, (a)-(e) are the phrase structures
for five dependents of the head join; (f) is the projection chain for
the head. The arrows indicate the positions of the attachment. No-
tice that to attach (a) to (f), the NNP Vinken needs to further project
to NP because according to the argument table, a VP can take an
NP, but not an NNP, as its argument.
In the PTB, a modifier either sister-adjoins or Chomsky-adjoins
to the modifiee. For example, in Figure 1, the MD will Chomsky-
adjoins whereas the NP Nov. 29 sister-adjoins to the VP node. To
account for that, we distinguish these two types of modifiers in the
modification table and Algorithm 3 is extended so that it would at-
tach Chomsky-adjoining modifiers higher by inserting extra nodes.
To convert the d-tree in Figure 2, the algorithm inserts an extra VP
node in the phrase structure in Figure 8 and attaches the MD will
to the new VP node; the final phrase structure produced by the al-
gorithm is identical to the one in Figure 1.
3.4 Algorithm 1 and 2 as special cases of Al-
gorithm 3
6Note that once Zt becomes a child of Xh, other dependents of
X (such as W) that are on the same side as Z but are further away
from X can attach only to Xh or higher on the projection chain of
X .
Although the three algorithms adopt different heuristic rules to
build phrase structures, the first two algorithms are special cases
of the last algorithm; that is, we can design a distinct set of pro-
jection/argument/modification tables for each of the first two al-
gorithms so that running Algorithm 3 with the associated set of ta-
bles for Algorithm 1 (Algorithm 2, respectively) would produce the
same results as running Algorithm 1 (Algorithm 2, respectively).
For example, to produce the results of Algorithm 2 with the code
for Algorithm 3, the three tables should be created as follows:
(a) In the projection table, each head X has only one projection
XP;
(b) In the argument table, if a category Y can be an argument of
a category X in a d-tree, then include both Y and YP as arguments
of X;
(c) In the modification table, if a category Y can be a modifier
of a category X in a d-tree, then include both Y and YP as sister-
modifiers of XP.
4. EXPERIMENTS
So far, we have described two existing algorithms and proposed
a new algorithm for converting d-trees into phrase structures. As
explained at the beginning of Section 3, we evaluated the perfor-
mance of the algorithms by comparing their output with an exist-
ing Treebank. Because there are no English dependency Treebanks
available, we first ran the algorithm in Section 2 to produce d-trees
from the PTB, then applied these three algorithms to the d-trees
and compared the output with the original phrase structures in the
PTB.7 The process is shown in Figure 9.
tagset table
argument table
head percolation table
modification table
argument table
head projection table
results
new phrase
d-tree
phr-struct => d-tree => phr-struct
(alg1, alg2, alg3)
structures
structures
d-trees compare
structures
in the PTB
phrase
Figure 9: The flow chart of the experiment
The results are shown in Table 1, which use Section 0 of the PTB.
The precision and recall rates are for unlabelled brackets. The last
column shows the ratio of the number of brackets produced by the
algorithms and the number of brackets in the original Treebank.
From the table (especially the last column), it is clear that Algo-
rithm 1 produces many more brackets than the original Treebank,
resulting in a high recall rate but low precision rate. Algorithm 2
produces very flat structures, resulting in a low recall rate and high
precision rate. Algorithm 3 produces roughly the same number of
brackets as the Treebank and has the best recall rate, and its preci-
sion rate is almost as good as that of Algorithm 2.
The differences between the output of the algorithms and the
phrase structures in the PTB come from four sources:
(S1) Annotation errors in the PTB
(S2) Errors in the Treebank-specific tables used by the algorithms
in Sections 2 and 3 (e.g., the head percolation table, the pro-
jection table, the argument table, and the modification table)
7Punctuation marks are not part of the d-trees produced by Lex-
Tract. We wrote a simple program to attach them as high as possi-
ble to the phrase structures produced by the conversion algorithms.
recall prec no-cross ave test/
(%) (%) (%) cross gold
Alg1 81.34 32.81 50.81 0.90 2.48
Alg2 54.24 91.50 94.90 0.10 0.59
Alg3 86.24 88.72 84.33 0.27 0.98
Table 1: Performance of three conversion algorithms on the
Section 0 of the PTB
(S3) The imperfection of the conversion algorithm in Section 2
(which converts phrase structures to d-trees)
(S4) Mismatches between the heuristic rules used by the algorithms
in Section 3 and the annotation schemata adopted by the PTB
To estimate the contribution of (S1)?(S4) to the differences be-
tween the output of Algorithm 3 and the phrase structures in the
PTB, we manually examined the first twenty sentences in Section
0. Out of thirty-one differences in bracketing, seven are due to
(S1), three are due to (S2), seven are due to (S3), and the remaining
fourteen mismatches are due to (S4).
While correcting annotation errors to eliminate (S1) requires more
human effort, it is quite straightforward to correct the errors in the
Treebank-specific tables and therefore eliminate the mismatches
caused by (S2). For (S3), we mentioned in Section 2 that the al-
gorithm chose the wrong heads for the noun phrases with the ap-
positive construction. As for (S4), we found several exceptions
(as shown in Table 2) to the one-projection-chain-per-category as-
sumption (i.e., for each POS tag, there is a unique projection chain),
an assumption which was used by all three algorithms in Section 3.
The performance of the conversion algorithms in Section 2 and 3
could be improved by using additional heuristic rules or statistical
information. For instance, Algorithm 3 in Section 3 could use a
heuristic rule that says that an adjective (JJ) projects to an NP if the
JJ follows the determiner the and the JJ is not followed by a noun
as in the rich are getting richer, and it projects to an ADJP in other
cases. Notice that such heuristic rules are Treebank-dependent.
most likely projection other projection(s)
JJ ! ADJP JJ ! NP
CD ! NP CD ! QP ! NP
VBN ! VP ! S VBN ! VP ! RRC
NN ! NP NN ! NX ! NP
VBG ! VP ! S VBG ! PP
Table 2: Some examples of heads with more than one projection
chain
Empty categories are often explicitly marked in phrase-structures,
but they are not always included in dependency structures. We be-
lieve that including empty categories in dependency structures has
many benefits. First, empty categories are useful for NLP applica-
tions such as machine translation. To translate a sentence from one
language to another, many machine translation systems first create
the dependency structure for the sentence in the source language,
then produce the dependency structure for the target language, and
finally generate a sentence in the target language. If the source
language (e.g., Chinese and Korean) allows argument deletion and
the target language (e.g., English) does not, it is crucial that the
dropped argument (which is a type of empty category) is explic-
itly marked in the source dependency structure, so that the machine
translation systems are aware of the existence of the dropped argu-
ment and can handle the situation accordingly. The second benefit
of including empty categories in dependency structures is that it can
improve the performance of the conversion algorithms in Section
3, because the phrase structures produced by the algorithms would
then have empty categories as well, just like the phrase structures
in the PTB. Third, if a sentence includes a non-projective construc-
tion such as wh-movement in English, and if the dependency tree
did not include an empty category to show the movement, travers-
ing the dependency tree would yield the wrong word order.8
5. CONCLUSION
We have proposed a new algorithm for converting dependency
structures to phrase structures and compared it with two existing
ones. We have shown that our algorithm subsumes the two ex-
isting ones. By using simple heuristic rules and taking as input
certain kinds of Treebank-specific information such as the types of
arguments and modifiers that a head can take, our algorithm pro-
duces phrase structures that are very close to the ones in an anno-
tated phrase-structure Treebank; moreover, the quality of the phrase
structures produced by our algorithm can be further improved when
more Treebank-specific information is used. We also argue for in-
cluding empty categories in the dependency structures.
6. ACKNOWLEDGMENTS
This research reported here was made possible by NSF under
Grant NSF-89-20230-15 and by DARPA as part of the Translingual
Information Detection, Extraction and Summarization (TIDES) pro-
gram under Grant N66001-00-1-8915.
7. REFERENCES
[1] M. Collins. Three Generative, Lexicalised Models for
Statistical Parsing. In Proc. of the 35th ACL, 1997.
[2] M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. A
Statistical Parser for Czech. In Proc. of ACL-1999, pages
505?512, 1999.
[3] M. Covington. An Empirically Motivated Reinterpretation of
Dependency Grammar, 1994. Research Report AI-1994-01.
[4] M. Covington. GB Theory as Dependency Grammar, 1994.
Research Report AI-1992-03.
[5] H. Gaifman. Dependency Systems and Phrase-Structure
Systems. Information and Control, pages 304?337, 1965.
[6] J. Hajic?. Building a Syntactically Annotated Corpus: The
Prague Dependency Treebank, 1998. Issues of Valency and
Meaning (Festschrift for Jarmila Panevova?.
[7] D. M. Magerman. Statistical Decision-Tree Models for
Parsing. In Proc. of the 33rd ACL, 1995.
[8] M. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building
a Large Annotated Corpus of English: the Penn Treebank.
Computational Lingustics, 1993.
[9] O. Rambow and A. K. Joshi. A formal look at dependency
grammars and phrase structure grammars with special
considertaion of word-order phenomena. In L. Wenner,
editor, Recent Trends in Meaning-Text Theory. John
Benjamin, Amsterdam, Philadelphia, 1997.
[10] F. Xia, M. Palmer, and A. Joshi. A Uniform Method of
Grammar Extraction and its Applications. In Proc. of Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora
(EMNLP/VLC), 2000.
8For more discussion of non-projective constructions, see [9].
Facilitating Treebank Annotation Using a Statistical Parser
Fu-Dong Chiou, David Chiang, and Martha Palmer
Dept of Computer and Information Science
University of Pennsylvania
200 S 33rd Street, Philadelphia, PA 19104-6389
{chioufd,dchiang,mpalmer}@linc.cis.upenn.edu
1. INTRODUCTION
Corpora of phrase-structure-annotated text, or treebanks, are use-
ful for supervised training of statistical models for natural language
processing, as well as for corpus linguistics. Their primary draw-
back, however, is that they are very time-consuming to produce. To
alleviate this problem, the standard approach is to make two passes
over the text: first, parse the text automatically, then correct the
parser output by hand.
In this paper we explore three questions:
? How much does an automatic first pass speed up annotation?
? Does this automatic first pass affect the reliability of the final
product?
? What kind of parser is best suited for such an automatic first
pass?
We investigate these questions by an experiment to augment the
Penn Chinese Treebank [15] using a statistical parser developed
by Chiang [3] for English. This experiment differs from previous
efforts in two ways: first, we quantify the increase in annotation
speed provided by the automatic first pass (70?100%); second, we
use a parser developed on one language to augment a corpus in an
unrelated language.
2. THE PARSER
The parsing model described by Chiang [3] is based on stochas-
tic TAG [13, 14]. In this model a parse tree is built up out of tree
fragments (called elementary trees), each of which contains exactly
one lexical item (its anchor).
In the variant of TAG used here, there are three kinds of el-
ementary trees: initial, (predicative) auxiliary, and modifier, and
three corresponding composition operations: substitution, adjunc-
tion, and sister-adjunction. Figure 1 illustrates all three of these op-
erations. The first two come from standard TAG [8]; the third is
borrowed from D-tree grammar [11].
In a stochastic TAG derivation, each elementary tree is gener-
ated with a certain probability which depends on the elementary
tree itself as well as the node it gets attached to. Since every tree is
.
lexicalized, each of these probabilities involves a bilexical depen-
dency, as in many recent statistical parsing models [9, 2, 4].
Since the number of parameters of a stochastic TAG is quite high,
we do two things to make parameter estimation easier. First, we
generate an elementary tree in two steps: the unlexicalized tree,
then a lexical anchor. Second, we smooth the probability estimates
of these two steps by backing off to reduced contexts.
When trained on about 80,000 words of the Penn Chinese Tree-
bank and tested on about 10,000 words of unseen text, this model
obtains 73.9% labeled precision and 72.2% labeled recall [1].
3. METHODOLOGY
For the present experiment the parsing model was trained on
the entire treebank (99,720 words). We then prepared a new set
of 20,202 segmented, POS-tagged words of Xinhua newswire text,
which was blindly divided into 3 sets of equal size (?10 words).
Each set was then annotated in two or three passes, as summa-
rized by the following table:
Set Pass 1 Pass 2 Pass 3
1 ? Annotator A Annotators A&B
2 parser Annotator A Annotators A&B
3 revised parser Annotator A Annotators A&B
Here ?Annotators A&B? means that Annotator B checked the
work of Annotator A, then for each point of disagreement, both an-
notators worked together to arrive at a consensus structure. ?Parser?
is Chiang?s parser, adapted to parse Chinese text as described by
Bikel and Chiang [1].
?Revised parser? is the same parser with additional modifications
suggested by Annotator A after correcting Set 2. These revisions
primarily resulted from a difference between the artificial evalua-
tion metric used by Bikel and Chiang [1] and this real-world task.
The metric used earlier, following common practice, did not take
punctuation or empty elements into account, whereas the present
task ideally requires that they be present and correctly placed. Thus
following changes were made:
? The parser was originally trained on data with the punctua-
tion marks moved, and did not bother to move the punctua-
tion marks back. For Set 3 we simply removed the prepro-
cessing phase which moved the punctuation marks.
? Similarly, the parser was trained on data which had all empty
elements removed. In this case we simply applied a rule-
based postprocessor which inserted null relative pronouns.
? Finally, the parser often produced an NP (or VP) which dom-
inated only a single NP (respectively, VP), whereas such a
NP
NNP
John
S
NP? VP
VB
leaveVP
MD
should
VP?
NP
NN
tomorrow
(
1
)
(
2
)
() (?)
?

2

1
1

2
?
2,1
derivation tree
S
NP
NNP
John
VP
MD
should
VP
VB
leave
NP
NN
tomorrow
derived tree
Figure 1: Grammar and derivation for ?John should leave tomorrow.? 
1
and 
2
are initial trees,  is a (predicative) auxiliary tree,
? is a modifier tree.
structure is not specified by the bracketing guidelines. There-
fore we applied another rule-based postprocessor to remove
these nodes. (This modification would have helped the orig-
inal evaluation as well.)
In short, none of the modifications required major changes to the
parser, but they did improve annotation speed significantly, as we
will see below.
4. RESULTS
The annotation times and rates for Pass 2 are as follows:
Set Pass 1 Time (Pass 2) Rate (Pass 2)
(hours:min) (words/hour)
1 ? 28:01 240
2 parser 16:21 412
3 revised parser 14:06 478
The rate increase for Set 2 over Set 1 was about 70%; for Set 3 over
Set 1, about double. Thus the time saved by the use of an automatic
first pass is substantial.
Assessing the reliability of the final product is somewhat trickier.
Set Pass 1 Accuracy (Pass 1) Accuracy (Pass 2)
LP LR LP LR
1 ? ? ? 99.84 99.76
2 parser 76.73 75.36 99.76 99.65
3 revised parser 82.87 81.42 99.81 99.26
where LP stands for labeled precision and LR stands for labeled
recall. The third column reports the accuracy of Pass 1 (the parser)
using the results of Pass 2 (Annotator A) as a gold standard. The
fourth column reports the accuracy of Pass 2 (Annotator A) using
the results of Pass 3 (Annotators A&B) as a gold standard.
We note several points:
? There is no indication that the addition of an automatic first
pass affected the accuracy of Pass 2. On the other hand, the
near-perfect reported accuracy of Pass 2 suggests that in fact
each pass biased subsequent passes substantially. We need
a more objective measure of reliability, which we leave for
future experiments.
? The parser revisions significantly improved the accuracy of
the parser with respect to the present metric (which is sensi-
tive to punctuation and empty elements). On Set 2 the revised
parser obtained 78.98/77.39% labeled precision/recall, an er-
ror reduction of about 9%.
? Not surprisingly, errors due to large-scale structural ambi-
guities were the most time-consuming to correct by hand. To
take an extreme example, one parse produced by the parser is
shown in Figure 2. It often matches the correct parse (shown
in Figure 3) at the lowest levels but the large-scale errors re-
quire the annotator to make many corrections.
5. DISCUSSION
In summary, although Chiang?s parser was not specifically de-
signed for Chinese, and trained on a moderate amount of data (less
than 100,000 words), the parses it provided were reliable enough
that the annotation rate was effectively doubled.
Now we turn to our third question: what kind of parser is most
suitable for an automatic first pass? Marcus et al [10] describe the
use of the deterministic parser Fidditch [6] as an automatic first
pass for the Penn (English) Treebank. They cite two features of this
parser as strengths:
1. It only produces a single parse per sentence, so that the an-
notator does not have to search through many parses.
2. It produces reliable partial parses, and leaves uncertain struc-
tures unspecified.
The Penn-Helsinki Parsed Corpus of Middle English was con-
structed using a statistical parser developed by Collins [4] as an
automatic first pass. This parser, as well as Chiang?s, retains the
first advantage but not the second. However, we suggest two ways
a statistical parser might be used to speed annotation further:
First, the parser can be made more useful to the annotator. A
statistical parser typically produces a single parse, but can also
(with little additional computation) produce multiple parses. Rat-
naparkhi [12] has found that choosing (by oracle) the best parse out
of the 20 highest-ranked parses boosts labeled recall and precision
(IP (NP (DP (DTYJ)) these
(NP (NN?))) businesses
(VP (VP (ADVP (AD?)) also
(VP (BA?) BA
(IP (NP (QP (CD??y) 36,000
(CLP (M1))) item
(CP (WHNP (-NONE- *OP*))
(CP (IP (VP (VVp?) possess
(NP (NN?) to be one?s own master
(NN#) knowledge
(NN?Y)))) property rights
(DEC{))) DE
(NP (NNb))) technologies
(VP (PP (P5) toward
(NP (DP (DT ??)) other
(NP (NN ?) businesses
(PU)
(NN ??)))) organizations
(VP (VV?#)))))) transfer
(CCZ) and
(VP (VVj?) spread
(IP (VP (PU?)
(VP (VV) create
(NP (NNB?)) income
(QP (CD????7) 4.43 billion
(CLP (M?)))))))) RMB
(PU ))
Figure 2: Parser output. Translation: ?These businesses also transfer and spread the intellectual property rights of 36,000 technolo-
gies to other businesses and organizations, creating an income of 4.43 billion RMB.?
(IP (NP-SBJ (DP (DTYJ)) these
(NP (NN?))) businesses
(VP (ADVP (AD ?)) also
(VP (VP (BA?) BA
(IP-OBJ (NP-SBJ (QP (CD??y) 36,000
(CLP (M1))) item
(CP (WHNP-1 (-NONE- *OP*))
(CP (IP (NP-SBJ (-NONE- *T*-1))
(VP (VVp?) possess
(NP-OBJ (NN ?) to be one?s own master
(NN #) knowledge
(NN ?Y)))) property rights
(DEC{))) DE
(NP (NNb))) technologies
(VP (PP-DIR (P5) toward
(NP (DP (DT??)) other
(NP (NN?) businesses
(PU)
(NN??)))) organizations
(VP (VP (VV ?#)) transfer
(CC Z) and
(VP (VV j?)))))) spread
(PU?)
(VP (VV) create
(NP-OBJ (NN B?)) income
(QP-EXT (CD????7) 4.43 billion
(CLP (M?)))))) RMB
(PU ))
Figure 3: Corrected parse for sentence of Figure 2.
from about 87% to about 93%. This suggests that if the annotator
had access to several of the highest-ranked parses, he or she could
save time by choosing the parse with the best gross structure and
making small-scale corrections.
Would such a change defeat the first advantage above by forcing
the annotator to search through multiple parses? No, because the
parses produced by a statistical parser are ranked. The additional
lower-ranked parses can only be of benefit to the annotator. Indeed,
because the chart contains information about the certainty of each
subparse, a statistical parser might regain the second advantage as
well, provided this information can be suitably presented.
Second, the annotator can be made more useful to the parser by
means of active learning or sample selection [5, 7]. (We are as-
suming now that the parser and annotator will take turns in a train-
parse-correct cycle, as opposed to a simple two-pass scheme.) The
idea behind sample selection is that some sentences are more in-
formative for training a statistical model than others; therefore, if
we have some way of automatically guessing which sentences are
more informative, these sentences are the ones we should hand-
correct first. Thus the parser?s accuracy will increase more quickly,
potentially requiring the annotator to make fewer corrections over-
all.
6. ACKNOWLEDGMENTS
We would like to thank Fei Xia, Mitch Marcus, Aravind Joshi,
Mary Ellen Okurowski and John Kovarik for their helpful com-
ments on the design of the evaluation, Beth Randall for her postpro-
cessing and error-checking code, and Nianwen Xue for serving as
?Annotator B.? This research was funded by DARPA N66001-00-
1-8915, DOD MDA904-97-C-0307, and NSF SBR-89-20230-15.
7. REFERENCES
[1] Daniel M. Bikel and David Chiang. Two statistical parsing
models applied to the Chinese Treebank. In Proceedings of
the Second Chinese Language Processing Workshop, pages
1?6, 2000.
[2] Eugene Charniak. Statistical parsing with a context-free
grammar and word statistics. In Proceedings of the
Fourteenth National Conference on Artificial Intelligence
(AAAI-97), pages 598?603. AAAI Press/MIT Press, 1997.
[3] David Chiang. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting of the Assocation
for Computational Linguistics, pages 456?463, Hong Kong,
2000.
[4] Michael Collins. Three generative lexicalised models for
statistical parsing. In Proceedings of the 35th Annual
Meeting of the Assocation for Computational Linguistics
(ACL-EACL ?97), pages 16?23, Madrid, 1997.
[5] Ido Dagan and Sean P. Engelson. Committee-based sampling
for training probabilistic classifiers. In Proceedings of the
Twelfth International Conference on Machine Learning,
pages 150?157. Morgan Kaufmann, 1995.
[6] Donald Hindle. Acquiring disambiguation rules from text. In
Proceedings of the 27th Annual Meeting of the Association
for Computational Linguistics, 1989.
[7] Rebecca Hwa. Sample selection for statistical grammar
induction. In Proceedings of EMNLP/VLC-2000, pages
45?52, Hong Kong, 2000.
[8] Aravind K. Joshi and Yves Schabes. Tree-adjoining
grammars. In Grzegorz Rosenberg and Arto Salomaa,
editors, Handbook of Formal Languages and Automata,
volume 3, pages 69?124. Springer-Verlag, Heidelberg, 1997.
[9] David M. Magerman. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of the
Assocation for Computational Linguistics, pages 276?283,
Cambridge, MA, 1995.
[10] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics,
19:313?330, 1993.
[11] Owen Rambow, K. Vijay-Shanker, and David Weir. D-tree
grammars. In Proceedings of the 33rd Annual Meeting of the
Assocation for Computational Linguistics, pages 151?158,
Cambridge, MA, 1995.
[12] Adwait Ratnaparkhi. Maximum entropy models for natural
language ambiguity resolution. PhD thesis, University of
Pennsylvania, 1998.
[13] Philip Resnik. Probabilistic tree-adjoining grammar as a
framework for statistical natural language processing. In
Proceedings of the Fourteenth International Conference on
Computational Linguistics (COLING-92), pages 418?424,
Nantes, 1992.
[14] Yves Schabes. Stochastic lexicalized tree-adjoining
grammars. In Proceedings of the Fourteenth International
Conference on Computational Linguistics (COLING-92),
pages 426?432, Nantes, 1992.
[15] Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu-Dong Chiou, Shizhe Huang,
Tony Kroch, and Mitch Marcus. Developing guidelines and
ensuring consistency for Chinese text annotation. In
Proceedings of the Second International Conference on
Language Resources and Evaluation (LREC-2000), Athens,
Greece, 2000.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 933 ? 944, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Towards Robust High Performance  
Word Sense Disambiguation of English Verbs  
Using Rich Linguistic Features 
Jinying Chen and Martha Palmer 
Department of Computer and Information Science, 
University of Pennsylvania, Philadelphia, PA, 19104, USA 
{jinying, mpalmer}@cis.upenn.edu  
Abstract. This paper shows that our WSD system using rich linguistic features 
achieved high accuracy in the classification of English SENSEVAL2 verbs for 
both fine-grained (64.6%) and coarse-grained (73.7%) senses. We describe 
three specific enhancements to our treatment of rich linguistic features and pre-
sent their separate and combined contributions to our system?s performance. 
Further experiments showed that our system had robust performance on test 
data without high quality rich features.  
1   Introduction 
Word sense disambiguation (WSD) has been regarded as essential or necessary in 
many high-level NLP applications that require a certain degree of semantic interpreta-
tion, such as machine translation, information retrieval (IR) and question answering, 
etc. However, previous investigations into the role of WSD in IR have shown that low 
accuracy in WSD negated any possible performance increase from ambiguity resolu-
tion [1,2]. This suggests that improving the performance of WSD systems is crucial 
for applications to attain benefits from WSD. 
Much effort has been aimed at the creation of sense tagged corpora that can be 
used to develop supervised WSD systems with high accuracy. 1  However, highly 
polysemous words with subtle sense distinctions still pose major challenges for auto-
matic systems, as evidenced in SENSEVAL2 [3]. This problem seems more serious 
for verbs, as indicated by the relatively poorer performance achieved by the best sys-
tem in the SENSEVAL2 English lexical sample task for verbs: 56.6% accuracy, in 
contrast with the 64.2% accuracy for all parts-of-speech [4,5]. On the other hand, 
disambiguating verb senses accurately is very important for lexical selection in MT. It 
is also helpful for information retrieval, especially for fact retrieval systems that take 
full-sentence queries as their input. Therefore, this paper will focus on improving the 
accuracy of our supervised WSD system for verbs. 
We are using a linguistically rich approach for verb sense disambiguation. Linguis-
tically rich approaches [5-9] utilize syntactic and/or semantic features, e.g., syntactic 
relations, selectional preferences, and semantic information of NP arguments of verbs, 
                                                          
1
 http://www.senseval.org/ 
934 J. Chen and M. Palmer 
etc. In verb sense disambiguation, Dang and Palmer's work [5] demonstrated that their 
system, which achieved 59.6% accuracy (62.5% in a recent report [10]) in disambigu-
ating the SENSEVAL2 English verbs, benefited substantially from using rich linguis-
tic features that capture information about a verb's lexical semantics.    
On the other hand, the performance of a system using rich linguistic features relies 
heavily on the quality of preprocessing, such as part-of-speech tagging, parsing, fea-
ture extraction and generation, etc. How accurate and how robust can such a system 
be? In particular, we are interested in the following three questions: How much ad-
vantage can we gain from the rich-feature approach by careful extraction and treat-
ment of the rich features? How much will a relatively poor quality of preprocessing 
negatively affect the system's performance? Which strategies can we adopt to allevi-
ate these negative effects?  
To address the first question, we enhance the feature extraction and generation of 
our original system, which was inspired by Dang?s system[10], in three ways. First, to 
increase the recall of the extraction of a verb's subject, we carefully handle relative 
clauses, nonfinite clauses, and verbs within prepositional phrases by using linguistic 
knowledge and heuristics. Second, to treat semantic features of NP arguments of 
verbs and prepositions in a more uniform way, we incorporate a rule-based pronoun 
resolver and also unify the semantic features generated by WordNet [11] and by a 
named entity tagger. Third, we treat sentential complements of verbs in a verb-
specific way. Our evaluation on the SENSEVAL2 English verbs shows that our new 
system achieves 64.6% accuracy, which is significantly better than the best system on 
English verbs in SENSEVAL2 (57.6%) and also outperforms Dang's system (62.5%). 
Further experiments indicate that the three enhancements are all beneficial. They each 
boost the system's performance by 1.0~1.2 percent and the combined gain is 2.6 per-
cent. A similar performance improvement is achieved for coarse-grained senses: 
73.7% vs. Dang's 71.7%. The data analysis of the results suggests that further im-
provements may come from disambiguating WordNet synsets and from using statisti-
cal methods for subject extraction and pronoun resolution.  
We address the last two robustness questions in two more experiments. To investi-
gate how the parser's performance affects our system, we divide the test data into an 
easy set that is similar to the parser's training material and a hard set that is not. The 
evaluation shows that although our system's accuracy is lower on the hard set, it is 
still high (62.2%). In the second experiment, our system is trained with rich features 
and tested on data with linguistically impoverished features. The results show little 
penalty from missing rich features at the test phase. The observations from this ex-
periment also suggest the following strategy for using WSD systems that utilize rich 
linguistic features. When good parsers are not available at the time of application, the 
use of topical features and any available, accurate rich features (e.g., features associ-
ated with the verb's direct object) will alleviate penalties.  
The rest of the paper is organized as follows. We introduce our system and the 
three major enhancements we made in Section 2. In Section 3, we show the evalua-
tion results on SENSEVAL2 English verbs and show how much the three enhance-
ments improve our system's performance. We then discuss the potential improve-
ments of our system in the future. In Section 4, we investigate the robustness of our 
system and propose our strategy for alleviating the negative effects of poor preproc-
essing. We conclude our discussion in Section 5. 
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 935 
2   System Description 
Our original WSD system was inspired by the successful MaxEnt WSD system of 
Dang [5,10]. We used the same machine learning model, Mallet, that implements a 
smoothing maximum entropy (ME) model with a Gaussian prior [12].  An attractive 
property of ME models is that there is no assumption of feature independence [13]. 
Empirical studies have shown that a ME model with a Gaussian prior generally out-
performs ME models with other smoothing methods [14]. In addition to topical and 
collocation features, we also used similar rich syntactic and semantic features, al-
though we implemented them in different ways. Furthermore, we enhanced the treat-
ment of certain rich linguistic features, which we believed would boost the system's 
performance. Before discussing these enhancements, we first briefly describe the 
basic syntactic and semantic features used by our system: 
Syntactic features: 
1. Is the sentence passive, semi-passive2 or active?  
2. Does the target verb have a subject or object? If so, what is the head of its  
         subject or/and object? 
3. Does the target verb have a sentential complement? 
4. Does the target verb have a PP adjunct? If so, what is the preposition and what is 
        the head of the NP argument of the preposition?   
Semantic features: 
1. The Named Entity tags of proper nouns and certain types of common nouns  
2. The WordNet synsets and hypernyms of head nouns of the NP arguments of 
         verbs and prepositions 
To better explore the advantage of using rich syntactic and semantic features, we 
enhanced our original system in three primary aspects: increasing the recall of the 
extraction of a verb's subject; unifying the treatment of semantic features of pronouns, 
common nouns and proper nouns; and providing a verb-specific treatment of senten-
tial complements.  These are each described in more detail below. 
2.1   Increasing Subject Extraction Recall  
To extract a subject, our original system simply checks the left NP siblings of the 
highest VP that contains the target verb and is within the innermost clause (see Figure 
1). This method has high precision but low recall. Typical examples from 
SENSEVAL2 data that are not handled by this approach are shown in (1a-c).3 
(1) a. Relative clauses: For Republicanssbj [SBAR who beganverb this campaign with  
              such high hopes],  ... 
      b. Nonfinite clauses: Isbj didn't ever want [S to seeverb that woman again]. 
      c.Verbs within PP's: Karipo and her womensbj had succeeded [PP in drivingverb 
             a hundred invaders from the isle ...] 
                                                          
2
  Verbs that are past participles and are not preceded by be or have verbs are semi-passive. 
3
  The target verb and its subject or subject candidates are underlined and the innermost clause 
or the PP containing the verb is bracketed.  
936 J. Chen and M. Palmer 
 
Fig. 1. position for verb?s subject 
 
To increase the recall, we refined the procedure of subject extraction by adding 
rules based on linguistic knowledge and bracketing labels that can handle relative 
clauses, nonfinite clauses, and verbs within prepositional phrases (PP's). For example, 
for cases like (1a), if a clause containing the target verb has a bracketing label SBAR 
and an NP parent, and is headed by a relative pronoun such as that, which or who, 
then check its left NP siblings for the verb's subject. For cases like (1b) and (1c), if the 
parent node of a nonfinite clause S or a PP is a VP, then continue searching positions 
outside the S or PP. For the last case, we also use a heuristic, i.e., a check as to 
whether the subject candidate is a person or an organization, to filter out non-person-
and-organization candidate NPs whose parent nodes are not labeled as S or SBAR. 
Many cases like (2a-b) can be handled correctly using this heuristic. 
(2) a. A number of accounts of the events accused the ministrysbj [PP of pullingverb  
               the plug on the UAL deal ...]. 
       b. Mr. Wolfsbj faces a monumental task [PP in pullingverb the company back  
                together again]. 
The above rule-based approach does not handle difficult cases like (3a-b) very well.  
(3) a. Freddy's instinct was [S to keepverb growing by stock mergers and small  
               expenditure of cash ...] 
       b.The arrangement I had with him was [S to workverb four hours a day]. 
With this enhancement, our new system extracts about 35% more subjects than be-
fore.  
S
NP         VP
? 
VP 
target verb
position for subject
Republicans   SBAR 
who        S 
began this campaign ? 
??        NP
(1a)  ?    
ADVP       VP
ever    want         S
to see that woman again  
??       VP
(1b)  ?    
   succeeded       PP 
in driving a hundred ?  
??       VP
(1c)  ?    
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 937 
2.2   Unifying Semantic Features 
In this section we describe the changes to the use of semantic features. In order to 
provide a more uniform treatment for the semantic features of the NP arguments of 
verbs and prepositions, we first merge the semantic features associated with proper 
nouns and common nouns.  We then extend our treatment to include pronouns by 
adding a pronoun resolution module. 
2.2.1   Merging Semantic Features 
Our system used an automatic named entity tagger, IdentiFinderTM [15], to tag proper 
nouns with Person, Organization and Location and common nouns with Date, 
Time, Percent and Money. Additional semantic features are all WordNet synsets and 
hypernyms4 of the head nouns of NP arguments, i.e., the system does not disambigu-
ate different WordNet senses of a head noun.  
To utilize semantic features more efficiently, we refine their treatment. Previously 
there was no overlap between semantic features generated by the named entity tagger 
and by WordNet. For example, a personal proper noun only has a Person tag that has 
no similarity to the WordNet synsets and hypernyms associated with similar common 
nouns such as specialist and doctor, etc. This is likely to be a problem for many WSD 
tasks that usually have small amounts of training data, such as SENSEVAL2. To 
overcome this problem, our new system associates a common noun (or a noun phrase) 
with each Named Entity tag (see 4) and adds the WordNet semantic features of these 
nouns (or noun phrases) to the original semantic feature set. 
(4)  Person ? someone,   Organization ? organization,   Location ? location 
       Time ? time unit,   Date ? time period,   Percent ? percent,   Money ? money 
2.2.2   Adding Pronoun Resolution  
Our original system has no special treatment for pronouns, although a rough count 
shows that about half of the training instances contain pronominal arguments. Lacking 
a high performance automatic pronoun resolution module, we adopt a hybrid approach. 
For personal pronouns, we simply treat them as personal proper nouns. For the rest of 
the pronouns including they, them, it, themselves and itself, which occur in about 13% 
of the training instances, we programmed a rather simple rule-based pronoun resolver. 
In brief, the resolver searches the parse tree for antecedent candidates similarly to 
Hobb's algorithm as exemplified in [16] and uses several syntactic and semantic con-
straints to filter out impossible candidates. The constraints include syntactic constraints 
for anaphora antecedents [16], number agreement, and whether the candidate is a per-
son. The first candidate that survives the filtering is regarded as the antecedent of the 
pronoun and its semantic features are added to the original feature set.  
2.3   Verb-Specific Sentential Complements  
The different types of sentential complements can be very useful for distinguishing 
certain verb senses. (5a-b) shows two sentences containing the verb call in the 
SENSEVAL2 training data. Call has WordNet Sense 1 (name) in (5a) and Sense 3 
                                                          
4
 A unique number defined in WordNet represents each synset or hypernym. 
938 J. Chen and M. Palmer 
(ascribe) in (5b). In both cases, call takes a small clause as its sentential complement, 
i.e., it has the subcategorization frame X call Y Z. The difference is that Z is a Named 
Entity when call is in Sense 1, and Z is usually a common NP or an adjective phrase 
(ADJP) when call is in Sense 3.  
(5)  a. The slender, handsome fellow was calledverb [S Dandy Brandon]. 
    b.The White House is purposely not callingverb [S the meeting a summit] ?  
Another example is shown in (6). The verb keep has WordNet Sense 1 (maintain) 
in (6a) and Sense 2 (continue) in (6b). In Sense 1, keep often takes a small clause and 
has the subcategorization frame X keep Y ADJP. In contrast, keep takes a sentential 
complement the head verb of which is in the present tense when it is in Sense 2. 
(6) a. He shook his head, keptverb [S his face expressionless]. 
      b. We keepverb [S wondering what Mr. Gates wanted to say]. 
Our original system uses a single feature hasSent to represent whether the target 
verb has a sentential complement or not, which cannot capture the rich information 
that is crucial to distinguishing certain verb senses but is deeply embedded in the 
sentential complements, as described above. Therefore, we treat sentential comple-
ments in a more fine-grained, verb-specific way. We resort to WordNet and PropBank 
[17] for the information about verb subcategorization frames. Another advantage of 
this verb-specific treatment is that it can filter out illegal sentential complements gen-
erated by the parser. 
3   System Evaluation 
Since the more recent SENSEVAL3 data were collected over the internet and had a 
relatively low quality of annotation, we decided to evaluate our new system on the 
SENSEVAL2 English verbs. Ratnaparkhi's MaxEnt sentence boundary detector and 
POS tagger [18], Bikel's parsing engine [19], and a named entity tagger, Identi-
FinderTM [15], were used to preprocess the training and test data automatically. 
3.1   Experimental Results  
Table 1 shows the performance of our system (MX-RF) on the 29 verbs with fine-
grained WordNet senses. Columns 2 and 3 show the number of senses and normalized 
sense perplexity5 for each verb in the test data respectively. It also gives the perform-
ance of the best system on English verbs in SENSEVAL2, KUNLP [5], and Dang's 
system [10]. As we see, our system achieves an average accuracy of 64.6%, which is 
significantly better than KUNLP  (57.6%) that only uses linguistically impoverished 
features (topical and collocation features). Our system also outperforms Dang's sys-
tem (62.5%). Recall that the types of rich linguistic features used by our system were 
originally inspired by Dang?s system, although we implemented them in different 
ways. Therefore, we attribute the more success of our new system mainly to the three 
                                                          
5
  It is calculated as the entropy of the sense distribution of a verb in the test data divided by the 
largest possible entropy, i.e., log2 (the number of senses of the verb in the test data). 
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 939 
specific enhancements we made. To our best knowledge, the accuracy our system 
achieved is the best result for this task at present. 
To investigate exactly how much we gain by enhancing the system in the three 
ways discussed in Section 2, we tested our system by removing our refinements (sub-
ject extraction, pronoun coreferences, and verb-specific sentential complements) 
separately and all together. The results (columns 8-11) show that each refinement 
boosts the system's performance by 1.0~1.2 percent and that together they achieve an 
improvement of 2.6 percent. This confirms the utility of these enhancements. 
In addition to fine-grained verb senses, we also evaluated our system on coarse-
grained senses (see Table 2). Previous work [20] suggested that not all NLP applica-
tions need fine-grained sense distinctions; in some cases coarser granularities will 
suffice. Furthermore, it has been demonstrated that annotation with coarser senses is 
much faster and more accurate [21].  The SENSEVAL2 verb senses have been 
grouped by using both syntactic and semantic criteria, with a resulting inter-annotator 
agreement (ITA) of 82% (column 4).  As we expected, the accuracy of our system 
increases by about 9 percent on the coarse-grained senses to 73.7%, which again con-
sistently outperforms Dang's system (71.7%).   
3.2   Discussion 
Compared verb-by-verb, the performance of our system is better than or comparable 
to Dang's on most verbs, except that it has notably lower accuracy on develop, dress 
and serve. It is not obvious why, since although our features are similar to Dang's, the 
implementations are different. Nevertheless, an investigation of the specific features 
our system generated for these three verbs gives us a few clues. The semantic catego-
ries  of  the  direct  objects  of  the three verbs are very diverse, so there are not 
enough instances of similar categories for the model to generalize. Therefore, the 
system performance benefits little from our enhancements.  In fact, our system may 
be more susceptible to noisy data introduced by the pronoun resolver for these three 
verbs. Erroneous antecedents found by the resolver are indistinguishable from the 
actual direct objects that occur rarely in the training data, and therefore they get the 
same treatment from the machine learning algorithm. 
The experimental results and the above data analysis suggest that our system can 
be improved further by increasing the accuracy of subject extraction and pronoun 
resolution. We expect a state-of-the-art pronoun resolution module and a statistical 
subject finder to do better jobs in the future. Our current system does not distinguish 
senses of nouns when using WordNet synsets and hypernyms as semantic features, 
which introduces many irrelevant features (associated with the irrelevant senses). 
The machine learning algorithm sometimes cannot generalize well using these fea-
tures. A potential solution for this problem is to distinguish the senses of the target 
verb and its NP arguments simultaneously. Furthermore, we need to have a better 
generalization, or clustering, of WordNet synsets and hypernyms, especially when 
the subject or object of a verb has semantic versatility.  More performance improve-
ments will bring us closer to our goal of an overall level of accuracy of 80%, espe-
cially with respect to coarse-grained senses, that should finally be more beneficial to 
NLP applications. 
940 J. Chen and M. Palmer 
Table 1. Evaluation of MX-RF on the SENSEVAL2 English verbs, with fine-grained senses 
Verb #of Sen
Sen 
Per-
plex. 
ITA KUNLP Dang 2004
MX-
RF 
MX-RF 
w/o sbj 
extract.
MX-
RF 
w/o 
pron.
MX-RF 
w/o verb 
spec sent-
comp 
MX-
RF w/o 
all 
three 
begin 7 0.63 81.2 81.4 89.3 91.2 90.0 90.4 89.3 88.6 
call 17 0.86 69.3 48.5 54.5 56.8 56.8 55.3 53.8 52.3 
carry 19 0.87 60.7 45.5 39.4 44.7 45.5 40.2 43.2 42.4 
collab-
orate 2 0.47 75.0 90.0 90.0 90.0 90.0 90.0 90.0 90.0 
develop 14 0.82 67.8 42.0 58.0 49.3 49.3 50.7 49.3 49.3 
draw 21 0.95 76.7 34.1 31.7 41.5 39.0 34.1 41.5 36.6 
dress 12 0.79 86.5 71.2 72.9 64.4 64.4 69.5 67.8 64.4 
drift 9 0.89 50.0 53.1 40.6 67.2 51.6 60.9 64.1 48.4 
drive 13 0.84 58.8 54.8 59.5 60.7 60.7 58.3 60.7 58.3 
face 6 0.38 78.6 82.8 83.9 81.2 82.3 83.3 81.2 83.3 
ferret 0 0.00 1.00 100.0 100.0 100.0 100.0 100.0 100.0 100.0 
find 17 0.94 44.3 27.9 36.8 41.2 36.8 36.8 36.8 33.8 
keep 20 0.79 79.1 44.8 61.2 64.2 61.9 65.7 61.2 57.5 
leave 10 0.86 67.2 50.0 60.6 57.6 57.6 54.5 53.0 50.0 
live 9 0.70 79.7 59.7 70.1 69.4 69.4 67.9 69.4 67.9 
match 7 0.79 56.5 52.4 50.0 59.5 61.9 57.1 59.5 57.1 
play 20 0.85 N/A 37.9 53.0 62.1 59.1 62.1 62.1 62.1 
pull 25 0.89 68.1 45.0 50.0 58.3 56.6 58.3 53.3 56.7 
replace 4 0.85 65.9 55.6 60.0 61.1 60.0 55.5 61.1 57.8 
see 13 0.84 70.9 39.1 39.1 44.2 39.9 42.8 41.3 35.5 
serve 11 0.85 90.8 68.6 74.5 68.6 66.7 64.7 68.6 66.7 
strike 20 0.89 76.2 40.7 38.9 51.9 50.0 53.7 51.9 55.6 
train 8 0.87 28.8 58.7 63.5 60.3 60.3 63.5 60.3 63.5 
treat 5 0.88 96.9 56.8 50.0 50.0 50.0 52.3 50.0 56.8 
turn 26 0.93 74.2 37.3 49.3 48.5 44.0 47.8 47.0 46.3 
use 6 0.65 74.3 65.8 71.1 69.7 72.4 68.4 69.7 68.4 
wander 5 0.47 65.0 82.0 80.0 82.0 82.0 82.0 82.0 82.0 
wash 7 0.94 87.5 83.3 66.7 75.0 75.0 75.0 75.0 75.0 
work 18 0.84 N/A 45.0 45.0 53.3 51.7 50.0 53.3 43.3 
average 12 0.77 71.3 57.6 62.5 64.6 63.4 63.6 63.4 62.0 
Table 2. Evaluation of MX-RF on coarse-grained senses of the SENSEVAL2 English verbs 
 # of grp ITA grp Acc. of Dang 2004 Acc. of MX-RF 
Ave. on 29 verbs 5.9 82.0 71.7 73.7 
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 941 
4   System Robustness 
A frequent criticism of systems using rich linguistic features is that they do not port 
well to domains for which accurate preprocessors are not available. In this section we 
discuss two experiments designed to address the following two questions: How much 
will a relatively poor quality of preprocessing negatively affect the system's perform-
ance? Which strategies can we adopt to alleviate these negative effects?  
4.1   Experiment I 
Since the parser is the most critical component of our preprocessing and is more 
likely to have lower performance when it is used in an unfamiliar data set, we investi-
gate how the performance of the parser on different test data sets affects our system. 
We divided the SENSEVAL2 test data into two sets: an easy set and a hard set. The 
test data from the Wall Street Journal (wsj) sections of Penn Treebank (PTB) [22] are 
put into the easy set because they are similar to the parser's training data: 02-21 wsj 
sections. The hard set contains test data from the Brown sections of PTB and BNC 
data. It is expected that the parser and therefore the system will perform better on the 
easy set. We trained our system on the whole SENSEVAL2 training data set and 
evaluated its performance on the easy and hard test sets separately. The results are 
shown in Table 3. 
Table 3. Performance on different test data sets 
Test data set Hard Easy Whole Set
Num. of test inst. 895 911 1806 
Average Acc. 62.2 66.9 64.6 
As we expected, the system's performance on the hard test set is 4.7 percent lower 
than on the easy set. On the other hand, even on the hard set, its accuracy (62.2%) is 
still high and is comparable to Dang's system. It is worth noting that the experiment is 
preliminary because the easy set and the hard set are most likely to be different not 
only on whether they are familiar to the parser but also on the subtlety and distribu-
tions of their senses. Nevertheless, it is evidence of our system's robustness.   
4.2   Experiment I I 
There will be situations where systems trained with rich linguistic features extracted 
from high quality parses will be run on applications where such rich features will not 
be available. It is most likely that systems in such situations will go back to a position 
similar to where rich features are not available in both the training and test phases. 
However, could things get even worse? A machine learning model often tends to 
favor informative features (e.g., rich linguistic features in our case) and fit the distri-
bution of these features well in its training phase. Therefore, it is expected that the 
model will be penalized more heavily when these informative features are used in its 
training phase but are not accessible in its test phase. In this subsection, we discuss a 
942 J. Chen and M. Palmer 
second experiment to test the robustness of our system in such situations and explore 
possible strategies for alleviating penalties.  
We trained our system with rich features of the SENSEVAL2 training data and 
tested its performance on the SENSEVAL2 test data with three different feature sets: 
a rich set containing topical, collocation, syntactic and semantic features 
(top+col+syn+sem), a poor set containing topical and collocation features (top+col) 
and a medium set containing topical and collocation features plus features for direct 
objects (top+col+obj). The reason we include the medium set is that a parser can 
usually find the direct object of verbs. Furthermore, we trained and tested our system 
on SENSEVAL2 data with linguistically impoverished features (top+col) and used 
this result as a control. As shown in Table 4, the system's accuracy drops to the same 
level as the control (58.0% vs. 58.1%) when it is trained with rich features but tested 
with poor features. When the features associated with the verb's direct object are 
added, the system's performance improves (59.1%).  
The experimental results here suggest that our system has not been penalized very 
much when rich linguistic features are only available in its training phase. Intuitively, 
the topical features6 our system uses alleviate the penalty. As expected, when the 
topical features of the test data were excluded, the performance of our system dropped 
to 54.8%. But this will be a common problem for all systems using topical features, 
not only for systems using rich linguistic features.7 These results suggest a strategy 
for using our system and other similar systems in a more robust way. When a state-of-
art parser is not available for the application data, topical features can be used to alle-
viate the penalty. Rich features that can be obtained more easily and reliably, e.g., 
features associated with the direct object of verbs, can also be used whenever they are 
available. 
Table 4. Performance of our system trained and tested on data sets with different features  
  top+col+syn+sem top+col 
top+col+syn+sem 64.6   
top+col 58.0 58.1 
top+col+obj 59.1   
5   Conclusion 
We have shown that our system using rich linguistic features was more successful, 
compared with the previous best systems, in classifying the fine-grained and coarse-
grained SENSEVAL2 verb senses. The three enhancements to the system's treatment 
                                                          
6
  Our system uses all the contextual nouns, verbs, adjectives and adverbs that are not in a stop 
word list as topical features. 
7
  In fact, the performance of our system trained with (top+col) features and tested with only 
collocation features also dropped to 55.8%, in contrast to the control accuracy 58.1%. 
Training set
Test set 
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 943 
of rich linguistic features were beneficial. Further improvements may come from 
disambiguating WordNet synsets and improving the accuracy of subject extraction 
and pronoun resolution. Furthermore, our system was robust when it was applied to 
test data that had a relatively poor quality of rich features. Based on the experimental 
results, we proposed a strategy for using systems with rich features in a more robust 
way. Our goal is to continue to improve the performance of our current WSD system, 
with respect to both fine-grained and coarse-grained senses, so that it becomes in-
creasingly beneficial to NLP applications. 
References 
1. Mark Sanderson: Word sense disambiguation and information retrieval. In Proceedings of 
the 17th Int. ACM SIGIR, Dublin, IE (1994).  
2. Christopher Stokoe, Michael P. Oakes, John Tait: Word sense disambiguation and infor-
mation retrieval revisited. In Proceedings of the 26th annual int. ACM SIGIR conference 
on research and development in information retrieval. Toronto, Canada (2003). 
3. Philip Edmonds and Scott Cotton: SENSEVAL-2: Overview. In Proceedings of 
SENSEVAL-2: 2nd Int. Workshop on Evaluating WSD Systems. ACL-SIGLEX, Tou-
louse, France (2001). 
4. David Yarowsky, Silviu Cucerzan, Radu Florian, Charles Schafer and Richard Wicen-
towski: The Johns hopkins SENSEVAL2 system description. In Proceedings of 
SENSEVAL-2: 2nd Int.Workshop on Evaluating WSD Systems. Toulouse France (2001). 
5. Hoa T. Dang and Martha Palmer: Combining contextual features for word sense disam-
biguation. In Proceedings of the SIGLEX/SENSEVAL Workshop on WSD: Recent Suc-
cesses and Future Directions, in conjunction with ACL-02, Philadelphia (2002).  
6. Mart?nez David, Agirre Enek. and M?rquez Liuis: Syntactic Features for High Precision 
Word Sense Disambiguation. In Proceedings of the 19th International COLING. Taipei 
(2002). 
7. Dekang Lin: Using Syntactic Dependency as Local Context to Resolve Word Sense Am-
biguity In Proceedings of ACL-97, Madrid, Spain (1997). 
8. Yoong Keok Lee and Hwee Tou Ng: An empirical evaluation of knowledge sources and 
learning algorithms for word sense disambiguation. In Proceedings of the Conference on 
Empirical Methods in Natural Language Processing (EMNLP) (2002) pages 41?48. 
9. Rada Mihalcea and Ehsanul Faruque: Sense Learner: Minimally Supervised Word Sense 
Disambiguation for All Words in Open Text. In Proceedings of SENSEVAL-3: Third In-
ternational Workshop on the Evaluation of Systems for the Semantic Analysis of Text, 
Barcelona, Spain (2004). 
10. Hoa T. Dang: Investigations into the role of lexical semantics in word sense disambigua-
tion.  PhD Thesis. University of Pennsylvania (2004). 
11. Christiane Fellbaum: WordNet - an Electronic Lexical Database. The MIT Press, Cam-
bridge, Massachusetts, London, UK (1998). 
12. Andrew K. McCallum: MALLET: A Machine Learning for Language Toolkit.  
http://www.cs. umass.edu/~mccallum/mallet (2002). 
13. Adam L. Berger, Stephen A. Della Piertra, and Vincent J. Della Pietra: A maximum en-
tropy approach to natural language processing. Compuational Linguistics, (1996) 22(1): 
39-71. 
14. Stanley. F. Chen and Ronald Rosenfeld: A Gaussian prior for smoothing maximum en-
tropy models. Technical Report CMU-CS-99-108, CMU (1999). 
944 J. Chen and M. Palmer 
15. Daniel M. Bikel, Richard Schwartz and Ralph M. Weischedel: An algorithm that learns 
what's in a name. Machine Learning, (1999) 34(1-3). Special Issue on Natural Language 
Learning. 
16. Shalom Lappin and Herbert Leass: An algorithm for pronominal anaphora resolution. 
Computational Linguistics, (1994) 20(4): 535-561. 
17. Paul Kingsbury, Martha Palmer, and Mitch Marcus: Adding semantic annotation to the 
Penn Tree-Bank. In Proceedings of HLT 2002, San Diego, CA (2002). 
18. Adwait Ratnaparkhi: Maximum entropy models for natural language ambiguity resolution. 
Ph.D. thesis, University of Pennsylvania (1998). 
19. 19 Daniel M. Bikel: Design of a multi-lingual, parallel-processing statistical parsing en-
gine.In Proceedings of HLT 2002. San Diego, CA (2002). 
20. Paul Buitelaar: Reducing lexical semantic complexity with systematic polysemous classes 
and underspecification. In Poceedings of the ANLP Workshop on Syntactic and Semantic 
Complexity in NLP Systems. Seattle, WA (2000). 
21. Martha Palmer, Olga B. Malaya and Hoa T. Dang: Different sense granularities for differ-
ent appli-cations. In Proceedings of HLT/NAACL-04. Boston (2004). 
22. Mitchell Marcus, Grace Kim, Mary A. Marcinkiewicz, Robert MacIntyre, Mark Ferguson, 
Karen Katz and Britta Schasberger: The Penn Treebank: annotating predicate argument 
structure. In Proceedings of the ARPA'94 HLT Workshop (1994). 
The Proposition Bank: An Annotated
Corpus of Semantic Roles
Martha Palmer
University of Pennsylvania
Daniel Gildea.
University of Rochester
Paul Kingsbury
University of Pennsylvania
The Proposition Bank project takes a practical approach to semantic representation, adding a
layer of predicate-argument information, or semantic role labels, to the syntactic structures of
the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not
represent coreference, quantification, and many other higher-order phenomena, but also broad,
in that it covers every instance of every verb in the corpus and allows representative statistics to
be calculated.
We discuss the criteria used to define the sets of semantic roles used in the annotation process
and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an
automatic system for semantic role tagging trained on the corpus and discuss the effect on its
performance of various types of information, including a comparison of full syntactic parsing
with a flat representation and the contribution of the empty ??trace?? categories of the treebank.
1. Introduction
Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi
1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the
availability of large, hand-annotated training corpora (Marcus, Santorini, and
Marcinkiewicz 1993; Abeille? 2003), have had a major impact on the field of natural
language processing in recent years. However, the syntactic analyses produced by
these parsers are a long way from representing the full meaning of the sentences that
are parsed. As a simple example, in the sentences
(1) John broke the window.
(2) The window broke.
a syntactic analysis will represent the window as the verb?s direct object in the first
sentence and its subject in the second but does not indicate that it plays the same
underlying semantic role in both cases. Note that both sentences are in the active voice
* 2005 Association for Computational Linguistics
 Department of Computer and Information Science, University of Pennsylvania, 3330 Walnut Street,
Philadelphia, PA 19104. Email: mpalmer@cis.upenn.edu.
. Department of Computer Science, University of Rochester, PO Box 270226, Rochester, NY 14627. Email:
gildea@cs.rochester.edu.
Submission received: 9th December 2003; Accepted for publication: 11th July 2004
and that this alternation in subject between transitive and intransitive uses of the verb
does not always occur; for example, in the sentences
(3) The sergeant played taps.
(4) The sergeant played.
the subject has the same semantic role in both uses. The same verb can also undergo
syntactic alternation, as in
(5) Taps played quietly in the background.
and even in transitive uses, the role of the verb?s direct object can differ:
(6) The sergeant played taps.
(7) The sergeant played a beat-up old bugle.
Alternation in the syntactic realization of semantic arguments is widespread,
affecting most English verbs in some way, and the patterns exhibited by specific verbs
vary widely (Levin 1993). The syntactic annotation of the Penn Treebank makes it
possible to identify the subjects and objects of verbs in sentences such as the above
examples. While the treebank provides semantic function tags such as temporal and
locative for certain constituents (generally syntactic adjuncts), it does not distinguish
the different roles played by a verb?s grammatical subject or object in the above
examples. Because the same verb used with the same syntactic subcategorization can
assign different semantic roles, roles cannot be deterministically added to the treebank
by an automatic conversion process with 100% accuracy. Our semantic-role annotation
process begins with a rule-based automatic tagger, the output of which is then hand-
corrected (see section 4 for details).
The Proposition Bank aims to provide a broad-coverage hand-annotated corpus of
such phenomena, enabling the development of better domain-independent language
understanding systems and the quantitative study of how and why these syntactic
alternations take place. We define a set of underlying semantic roles for each verb and
annotate each occurrence in the text of the original Penn Treebank. Each verb?s roles
are numbered, as in the following occurrences of the verb offer from our data:
(8) . . . [
Arg0
the company] to . . . offer [
Arg1
a 15% to 20% stake] [
Arg2
to the public]
(wsj_0345)1
(9) . . . [
Arg0
Sotheby?s] . . . offered [
Arg2
the Dorrance heirs] [
Arg1
a money-back
guarantee] (wsj_1928)
(10) . . . [
Arg1
an amendment] offered [
Arg0
by Rep. Peter DeFazio] . . . (wsj_0107)
(11) . . . [
Arg2
Subcontractors] will be offered [
Arg1
a settlement] . . . (wsj_0187)
We believe that providing this level of semantic representation is important for
applications including information extraction, question answering, and machine
72
1 Example sentences drawn from the treebank corpus are identified by the number of the file in which they
occur. Constructed examples usually feature John.
Computational Linguistics Volume 31, Number 1
73
translation. Over the past decade, most work in the field of information extraction has
shifted from complex rule-based systems designed to handle a wide variety of
semantic phenomena, including quantification, anaphora, aspect, and modality (e.g.,
Alshawi 1992), to more robust finite-state or statistical systems (Hobbs et al 1997;
Miller et al 1998). These newer systems rely on a shallower level of semantic
representation, similar to the level we adopt for the Proposition Bank, but have also
tended to be very domain specific. The systems are trained and evaluated on corpora
annotated for semantic relations pertaining to, for example, corporate acquisitions or
terrorist events. The Proposition Bank (PropBank) takes a similar approach in that we
annotate predicates? semantic roles, while steering clear of the issues involved in
quantification and discourse-level structure. By annotating semantic roles for every
verb in our corpus, we provide a more domain-independent resource, which we hope
will lead to more robust and broad-coverage natural language understanding systems.
The Proposition Bank focuses on the argument structure of verbs and provides a
complete corpus annotated with semantic roles, including roles traditionally viewed as
arguments and as adjuncts. It allows us for the first time to determine the frequency of
syntactic variations in practice, the problems they pose for natural language
understanding, and the strategies to which they may be susceptible.
We begin the article by giving examples of the variation in the syntactic realization
of semantic arguments and drawing connections to previous research into verb alter-
nation behavior. In section 3 we describe our approach to semantic-role annotation,
including the types of roles chosen and the guidelines for the annotators. Section 5
compares our PropBank methodology and choice of semantic-role labels to those of
another semantic annotation project, FrameNet. We conclude the article with a dis-
cussion of several preliminary experiments we have performed using the PropBank
annotations, and discuss the implications for natural language research.
2. Semantic Roles and Syntactic Alternation
Our work in examining verb alternation behavior is inspired by previous research into
the linking between semantic roles and syntactic realization, in particular, the
comprehensive study of Levin (1993). Levin argues that syntactic frames are a direct
reflection of the underlying semantics; the sets of syntactic frames associated with a
particular Levin class reflect underlying semantic components that constrain allowable
arguments. On this principle, Levin defines verb classes based on the ability of
particular verbs to occur or not occur in pairs of syntactic frames that are in some
sense meaning-preserving (diathesis alternations). The classes also tend to share
some semantic component. For example, the break examples above are related by a
transitive/intransitive alternation called the causative/inchoative alternation. Break
and other verbs such as shatter and smash are also characterized by their ability to
appear in the middle construction, as in Glass breaks/shatters/smashes easily. Cut, a
similar change-of-state verb, seems to share in this syntactic behavior and can also
appear in the transitive (causative) as well as the middle construction: John cut the
bread, This loaf cuts easily. However, it cannot also occur in the simple intransitive: The
window broke/*The bread cut. In contrast, cut verbs can occur in the conative? John
valiantly cut/hacked at the frozen loaf, but his knife was too dull to make a dent in it?whereas
break verbs cannot: *John broke at the window. The explanation given is that cut describes
a series of actions directed at achieving the goal of separating some object into pieces.
These actions consist of grasping an instrument with a sharp edge such as a knife and
applying it in a cutting fashion to the object. It is possible for these actions to be
Palmer, Gildea, and Kingsbury The Proposition Bank
performed without the end result being achieved, but such that the cutting manner can
still be recognized, for example, John cut at the loaf. Where break is concerned, the only
thing specified is the resulting change of state, in which the object becomes separated
into pieces.
VerbNet (Kipper, Dang, and Palmer 2000; Kipper, Palmer, and Rambow 2002)
extends Levin?s classes by adding an abstract representation of the syntactic frames for
each class with explicit correspondences between syntactic positions and the semantic
roles they express, as in Agent REL Patient or Patient REL into pieces for break.2 (For other
extensions of Levin, see also Dorr and Jones [2000] and Korhonen, Krymolowsky, and
Marx [2003].) The original Levin classes constitute the first few levels in the hierarchy,
with each class subsequently refined to account for further semantic and syntactic
differences within a class. The argument list consists of thematic labels from a set of 20
such possible labels (Agent, Patient, Theme, Experiencer, etc.). The syntactic frames
represent a mapping of the list of schematic labels to deep-syntactic arguments.
Additional semantic information for the verbs is expressed as a set (i.e., conjunction) of
semantic predicates, such as motion, contact, transfer_info. Currently, all Levin verb
classes have been assigned thematic labels and syntactic frames, and over half the
classes are completely described, including their semantic predicates. In many cases,
the additional information that VerbNet provides for each class has caused it to
subdivide, or use intersections of, Levin?s original classes, adding an additional level
to the hierarchy (Dang et al 1998). We are also extending the coverage by adding new
classes (Korhonen and Briscoe 2004).
Our objective with the Proposition Bank is not a theoretical account of how and
why syntactic alternation takes place, but rather to provide a useful level of repre-
sentation and a corpus of annotated data to enable empirical study of these issues. We
have referred to Levin?s classes wherever possible to ensure that verbs in the same
classes are given consistent role labels. However, there is only a 50% overlap between
verbs in VerbNet and those in the Penn TreeBank II, and PropBank itself does not
define a set of classes, nor does it attempt to formalize the semantics of the roles it
defines.
While lexical resources such as Levin?s classes and VerbNet provide information
about alternation patterns and their semantics, the frequency of these alternations and
their effect on language understanding systems has never been carefully quantified.
While learning syntactic subcategorization frames from corpora has been shown to be
possible with reasonable accuracy (Manning 1993; Brent 1993; Briscoe and Carroll
1997), this work does not address the semantic roles associated with the syntactic
arguments. More recent work has attempted to group verbs into classes based on
alternations, usually taking Levin?s classes as a gold standard (McCarthy 2000; Merlo
and Stevenson 2001; Schulte im Walde 2000; Schulte im Walde and Brew 2002). But
without an annotated corpus of semantic roles, this line of research has not been able
to measure the frequency of alternations directly, or more generally, to ascertain how
well the classes defined by Levin correspond to real-world data.
We believe that a shallow labeled dependency structure provides a feasible level of
annotation which, coupled with minimal coreference links, could provide the
foundation for a major advance in our ability to extract salient relationships from
text. This will in turn improve the performance of basic parsing and generation
74
2 These can be thought of as a notational variant of tree-adjoining grammar elementary trees or tree-
adjoining grammar partial derivations (Kipper, Dang, and Palmer 2000).
Computational Linguistics Volume 31, Number 1
75
components, as well as facilitate advances in text understanding, machine translation,
and fact retrieval.
3. Annotation Scheme: Choosing the Set of Semantic Roles
Because of the difficulty of defining a universal set of semantic or thematic roles
covering all types of predicates, PropBank defines semantic roles on a verb-by-verb
basis. An individual verb?s semantic arguments are numbered, beginning with zero.
For a particular verb, Arg0 is generally the argument exhibiting features of a Pro-
totypical Agent (Dowty 1991), while Arg1 is a Prototypical Patient or Theme. No
consistent generalizations can be made across verbs for the higher-numbered
arguments, though an effort has been made to consistently define roles across mem-
bers of VerbNet classes. In addition to verb-specific numbered roles, PropBank defines
several more general roles that can apply to any verb. The remainder of this section
describes in detail the criteria used in assigning both types of roles.
As examples of verb-specific numbered roles, we give entries for the verbs accept
and kick below. These examples are taken from the guidelines presented to the
annotators and are also available on the Web at http://www.cis.upenn.edu/? cotton/
cgi-bin/pblex_fmt.cgi.
(12) Frameset accept.01 ??take willingly??
Arg0: Acceptor
Arg1: Thing accepted
Arg2: Accepted-from
Arg3: Attribute
Ex:[
Arg0
He] [
ArgM-MOD
would][
ArgM-NEG
n?t] accept [
Arg1
anything of value]
[
Arg2
from those he was writing about]. (wsj_0186)
(13) Frameset kick.01 ??drive or impel with the foot??
Arg0: Kicker
Arg1: Thing kicked
Arg2: Instrument (defaults to foot)
Ex1: [
ArgM-DIS
But] [
Arg0
two big New York banks
i
] seem [
Arg0
*trace*
i
]
to have kicked [
Arg1
those chances] [
ArgM-DIR
away], [
ArgM-TMP
for the
moment], [
Arg2
with the embarrassing failure of Citicorp and
Chase Manhattan Corp. to deliver $7.2 billion in bank financing
for a leveraged buy-out of United Airlines parent UAL Corp].
(wsj_1619)
Ex2: [
Arg0
John
i
] tried [
Arg0
*trace*
i
] to kick [
Arg1
the football], but Mary
pulled it away at the last moment.
A set of roles corresponding to a distinct usage of a verb is called a roleset and can
be associated with a set of syntactic frames indicating allowable syntactic variations in
the expression of that set of roles. The roleset with its associated frames is called a
Palmer, Gildea, and Kingsbury The Proposition Bank
frameset. A polysemous verb may have more than one frameset when the differences
in meaning are distinct enough to require a different set of roles, one for each
frameset. The tagging guidelines include a ??descriptor?? field for each role, such as
??kicker?? or ??instrument,?? which is intended for use during annotation and as
documentation but does not have any theoretical standing. In addition, each frameset
is complemented by a set of examples, which attempt to cover the range of syntactic
alternations afforded by that usage. The collection of frameset entries for a verb is
referred to as the verb?s frames file.
The use of numbered arguments and their mnemonic names was instituted for a
number of reasons. Foremost, the numbered arguments plot a middle course among
many different theoretical viewpoints.3 The numbered arguments can then be mapped
easily and consistently onto any theory of argument structure, such as traditional theta
role (Kipper, Palmer, and Rambow 2002), lexical-conceptual structure (Rambow et al
2003), or Prague tectogrammatics (Hajic?ova and Kuc?erova? 2002).
While most rolesets have two to four numbered roles, as many as six can appear,
in particular for certain verbs of motion:4
(14) Frameset edge.01 ??move slightly??
Arg0: causer of motion Arg3: start point
Arg1: thing in motion Arg4: end point
Arg2: distance moved Arg5: direction
Ex: [
Arg0
Revenue] edged [
Arg5
up] [
Arg2-EXT
3.4%] [
Arg4
to $904 million]
[
Arg3
from $874 million] [
ArgM-TMP
in last year?s third quarter]. (wsj_1210)
Because of the use of Arg0 for agency, there arose a small set of verbs in which an
external force could cause the Agent to execute the action in question. For example, in
the sentence . . . Mr. Dinkins would march his staff out of board meetings and into his private
office . . . (wsj_0765), the staff is unmistakably the marcher, the agentive role. Yet
Mr. Dinkins also has some degree of agency, since he is causing the staff to do the
marching. To capture this, a special tag, ArgA, is used for the agent of an induced
action. This ArgA tag is used only for verbs of volitional motion such as march and
walk, modern uses of volunteer (e.g., Mary volunteered John to clean the garage, or more
likely the passive of that, John was volunteered to clean the garage), and, with some
hesitation, graduate based on usages such as Penn only graduates 35% of its students.
(This usage does not occur as such in the Penn Treebank corpus, although it is evoked
in the sentence No student should be permitted to be graduated from elementary school
without having mastered the 3 R?s at the level that prevailed 20 years ago. (wsj_1286))
In addition to the semantic roles described in the rolesets, verbs can take any of a
set of general, adjunct-like arguments (ArgMs), distinguished by one of the function
tags shown in Table 1. Although they are not considered adjuncts, NEG for verb-level
negation (e.g., John didn?t eat his peas) and MOD for modal verbs (e.g., John would eat
76
3 By following the treebank, however, we are following a very loose government-binding framework.
4 We make no attempt to adhere to any linguistic distinction between arguments and adjuncts. While many
linguists would consider any argument higher than Agr2 or Agr3 to be an adjunct, such arguments occur
frequently enough with their respective verbs, or classes of verbs, that they are assigned a number in
order to ensure consistent annotation.
Computational Linguistics Volume 31, Number 1
77
everything else) are also included in this list to allow every constituent surrounding the
verb to be annotated. DIS is also not an adjunct but is included to ease future discourse
connective annotation.
3.1 Distinguishing Framesets
The criteria for distinguishing framesets are based on both semantics and syntax. Two
verb meanings are distinguished as different framesets if they take different numbers
of arguments. For example, the verb decline has two framesets:
(15) Frameset decline.01 ??go down incrementally??
Arg1: entity going down
Arg2: amount gone down by, EXT
Arg3: start point
Arg4: end point
Ex: . . . [
Arg1
its net income] declining [
Arg2-EXT
42%] [
Arg4
to $121 million]
[
ArgM-TMP
in the first 9 months of 1989]. (wsj_0067)
(16) Frameset decline.02 ??demure, reject??
Arg0: agent
Arg1: rejected thing
Ex: [
Arg0
A spokesman
i
] declined [
Arg1
*trace*
i
to elaborate] (wsj_0038)
However, alternations which preserve verb meanings, such as causative/inchoative or
object deletion, are considered to be one frameset only, as shown in the example (17).
Both the transitive and intransitive uses of the verb open correspond to the same
frameset, with some of the arguments left unspecified:
(17) Frameset open.01 ??cause to open??
Arg0: agent
Arg1: thing opened
Arg2: instrument
Ex1: [
Arg0
John] opened [
Arg1
the door]
Table 1
Subtypes of the ArgM modifier tag.
LOC: location CAU: cause
EXT: extent TMP: time
DIS: discourse connectives PNC: purpose
ADV: general purpose MNR: manner
NEG: negation marker DIR: direction
MOD: modal verb
Palmer, Gildea, and Kingsbury The Proposition Bank
Ex2: [
Arg1
The door] opened
Ex3: [
Arg0
John] opened [
Arg1
the door] [
Arg2
with his foot]
Moreover, differences in the syntactic type of the arguments do not constitute
criteria for distinguishing among framesets. For example, see.01 allows for either an NP
object or a clause object:
(18) Frameset see.01 ??view??
Arg0: viewer
Arg1: thing viewed
Ex1: [
Arg0
John] saw [
Arg1
the President]
Ex2: [
Arg0
John] saw [
Arg1
the President collapse]
Furthermore, verb-particle constructions are treated as separate from the
corresponding simplex verb, whether the meanings are approximately the same or
not. Example (19-21) presents three of the framesets for cut:
(19) Frameset cut.01 ??slice??
Arg0: cutter
Arg1: thing cut
Arg2: medium, source
Arg3: instrument
Ex: [
Arg0
Longer production runs] [
ArgM-MOD
would] cut [
Arg1
inefficiencies
from adjusting machinery between production cycles]. (wsj_0317)
(20) Frameset cut.04 ??cut off = slice??
Arg0: cutter
Arg1: thing cut (off)
Arg2: medium, source
Arg3: instrument
Ex: [
Arg0
The seed companies] cut off [
Arg1
the tassels of each plant].
(wsj_0209)
(21) Frameset cut.05 ??cut back = reduce??
Arg0: cutter
Arg1: thing reduced
Arg2: amount reduced by
78
Computational Linguistics Volume 31, Number 1
79
Arg3: start point
Arg4: end point
Ex: ??Whoa,?? thought John, ?[
Arg0
I
i
]?ve got [
Arg0
*trace*
i
] to start
[
Arg0
*trace*
i
] cutting back [
Arg1
my intake of chocolate].
Note that the verb and particle do not need to be contiguous; (20) above could just as
well be phrased The seed companies cut the tassels of each plant off.
For the WSJ text, there are frames for over 3,300 verbs, with a total of just over
4,500 framesets described, implying an average polysemy of 1.36. Of these verb frames,
only 21.6% (721/3342) have more than one frameset, while less than 100 verbs have
four or more. Each instance of a polysemous verb is marked as to which frameset it
belongs to, with interannotator (ITA) agreement of 94%. The framesets can be viewed
as extremely coarse-grained sense distinctions, with each frameset corresponding to
one or more of the Senseval 2 WordNet 1.7 verb groupings. Each grouping in turn
corresponds to several WordNet 1.7 senses (Palmer, Babko-Malaya, and Dang 2004).
3.2 Secondary Predications
There are two other functional tags which, unlike those listed above, can also be
associated with numbered arguments in the frames files. The first one, EXT (extent),
indicates that a constituent is a numerical argument on its verb, as in climbed 15%
or walked 3 miles. The second, PRD (secondary predication), marks a more subtle
relationship. If one thinks of the arguments of a verb as existing in a dependency tree,
all arguments depend directly on the verb. Each argument is basically independent of
the others. There are those verbs, however, which predict that there is a predicative
relationship between their arguments. A canonical example of this is call in the sense of
??attach a label to,?? as in Mary called John an idiot. In this case there is a relationship
between John and an idiot (at least in Mary?s mind). The PRD tag is associated with the
Arg2 label in the frames file for this frameset, since it is predictable that the Arg2
predicates on the Arg1 John. This helps to disambiguate the crucial difference between
the following two sentences:
predicative reading ditransitive reading
Mary called John a doctor. Mary called John a doctor.5
(LABEL) (SUMMON)
Arg0: Mary Arg0: Mary
Rel: called Rel: called
Arg1: John (item being labeled) Arg2: John (benefactive)
Arg2-PRD: a doctor (attribute) Arg1: a doctor (thing summoned)
It is also possible for ArgMs to predicate on another argument. Since this must be
decided on a case-by-case basis, the PRD function tag is added to the ArgM by the
annotator, as in example (28).
5 This sense could also be stated in the dative: Mary called a doctor for John.
Palmer, Gildea, and Kingsbury The Proposition Bank
3.3 Subsumed Arguments
Because verbs which share a VerbNet class are rarely synonyms, their shared argument
structure occasionally takes on odd characteristics. Of primary interest among these are
the cases in which an argument predicted by one member of a class cannot be attested
by another member of the same class. For a relatively simple example, consider the verb
hit, in VerbNet classes 18.1 and 18.4. This takes three very obvious arguments:
(22) Frameset hit ??strike??
Arg0: hitter
Arg1: thing hit, target
Arg2: instrument of hitting
Ex1: Agentive subject: ??[
Arg0
He
i
] digs in the sand instead of [
Arg0
*trace*
i
]
hitting [
Arg1
the ball], like a farmer,?? said Mr. Yoneyama. (wsj_1303)
Ex2: Instrumental subject: Dealers said [
Arg1
the shares] were hit [
Arg2
by
fears of a slowdown in the U.S. economy]. (wsj_1015)
Ex3: All arguments: [
Arg0
John] hit [
Arg1
the tree] [
Arg2
with a stick].6
VerbNet classes 18.1 and 18.4 are filled with verbs of hitting, such as beat, hammer,
kick, knock, strike, tap, and whack. For some of these the instrument of hitting is
necessarily included in the semantics of the verb itself. For example, kick is essentially
??hit with the foot?? and hammer is exactly ??hit with a hammer.?? For these verbs, then,
the Arg2 might not be available, depending on how strongly the instrument is
incorporated into the verb. Kick, for example, shows 28 instances in the treebank but
only one instance of a (somewhat marginal) instrument:
(23) [
ArgM-DIS
But] [
Arg0
two big New York banks] seem to have kicked [
Arg1
those
chances] [
ArgM-DIR
away], [
ArgM-TMP
for the moment], [
Arg2
with the embarrassing
failure of Citicorp and Chase Manhattan Corp. to deliver $7.2 billion in
bank financing for a leveraged buy-out of United Airlines parent UAL
Corp]. (wsj_1619)
Hammer shows several examples of Arg2s, but these are all metaphorical hammers:
(24) Despite the relatively strong economy, [
Arg1
junk bond prices
i
] did
nothing except go down, [
Arg1
*trace*
i
] hammered [
Arg2
by a seemingly
endless trail of bad news]. (wsj_2428)
Another perhaps more interesting case is that in which two arguments can be
merged into one in certain syntactic situations. Consider the case of meet, which
canonically takes two arguments:
(25) Frameset meet ??come together??
Arg0: one party
80
6 The Wall Street Journal corpus contains no examples with both an agent and an instrument.
Computational Linguistics Volume 31, Number 1
81
Arg1: the other party
Ex: [
Arg0
Argentine negotiator Carlos Carballo] [
ArgM-MOD
will] meet
[
Arg1
with banks this week]. (wsj_0021)
It is perfectly possible, of course, to mention both meeting parties in the same
constituent:
(26) [
Arg0
The economic and foreign ministers of 12 Asian and Pacific
nations] [
ArgM-MOD
will] meet [
ArgM-LOC
in Australia] [
ArgM-TMP
next week]
[
ArgM-PRP
to discuss global trade as well as regional matters such as
transportation and telecommunications]. (wsj_0043)
In these cases there is an assumed or default Arg1 along the lines of ??each other??:
(27) [
Arg0
The economic and foreign ministers of 12 Asian and Pacific
nations] [
ArgM-MOD
will] meet [
Arg1-REC
(with) each other] . . .
Similarly, verbs of attachment (attach, tape, tie, etc.) can express the things being
attached as either one constituent or two:
(28) Frameset connect.01 ??attach??
Arg0: agent, entity causing two objects to be attached
Arg1: patient
Arg2: attached-to
Arg3: instrument
Ex1: The subsidiary also increased reserves by $140 million, however,
and set aside an additional $25 million for [
Arg1
claims] connected
[
Arg2
with Hurricane Hugo]. (wsj_1109)
Ex2: Machines using the 486 are expected to challenge higher-priced
work stations and minicomputers in applications such as [
Arg0
so-called
servers
i
], [
Arg0
which
i
] [
Arg0
*trace*
i
] connect [
Arg1
groups of computers]
[
ArgM-PRD
[together], and in computer-aided design. (wsj_0781)
3.4 Role Labels and Syntactic Trees
The Proposition Bank assigns semantic roles to nodes in the syntactic trees of the Penn
Treebank. Annotators are presented with the roleset descriptions and the syntactic tree
and mark the appropriate nodes in the tree with role labels. The lexical heads of
constituents are not explicitly marked either in the treebank trees or in the semantic
labeling layered on top of them. Annotators cannot change the syntactic parse, but
they are not otherwise restricted in assigning the labels. In certain cases, more than
one node may be assigned the same role. The annotation software does not require that
the nodes being assigned labels be in any syntactic relation to the verb. We discuss
the ways in which we handle the specifics of the treebank syntactic annotation style in
this section.
Palmer, Gildea, and Kingsbury The Proposition Bank
3.4.1 Prepositional Phrases. The treatment of prepositional phrases is complicated by
several factors. On one hand, if a given argument is defined as a ??destination,?? then in
a sentence such as John poured the water into the bottle, the destination of the water is
clearly the bottle, not ??into the bottle.?? The fact that the water is going into the bottle is
inherent in the description ??destination??; the preposition merely adds the specific
information that the water will end up inside the bottle. Thus arguments should
properly be associated with the NP heads of prepositional phrases. On the other hand,
however, ArgMs which are prepositional phrases are annotated at the PP level, not the
NP level. For the sake of consistency, then, numbered arguments are also tagged at the
PP level. This also facilitates the treatment of multiword prepositions such as out of,
according to, and up to but not including.7
(29) [
Arg1
Its net income] declining [
Arg2-EXT
42%] [to
Arg4
$121 million]
[
ArgM-TMP
in the first 9 months of 1989] (wsj_0067)
3.4.2 Traces and Control Verbs. The Penn Treebank contains empty categories known
as traces, which are often coindexed with other constituents in the tree. When a trace is
assigned a role label by an annotator, the coindexed constituent is automatically added
to the annotation, as in
(30) [
Arg0
John
i
] tried [
Arg0
*trace*
i
] to kick [
Arg1
the football], but Mary pulled
it away at the last moment.
Verbs such as cause, force, and persuade, known as object control verbs, pose a
problem for the analysis and annotation of semantic structure. Consider a sentence
such as Commonwealth Edison said the ruling could force it to slash its 1989 earnings by
$1.55 a share. (wsj_0015). The Penn Treebank?s analysis assigns a single sentential (S)
constituent to the entire string it to slash . . . a share, making it a single syntactic
argument to the verb force. In the PropBank annotation, we split the sentential
complement into two semantic roles for the verb force, assigning roles to the noun
phrase and verb phrase but not to the S node which subsumes them:
(31) Frameset cause, force, persuade, etc. ??impelled action??
Arg0: agent
Arg1: impelled agent
Arg2: impelled action
Ex: Commonwealth Edison said [
Arg0
the ruling] [
ArgM-MOD
could] force
[
Arg1
it] [
Arg2-PRD
to slash its 1989 earnings by $1.55 a share]. (wsj_0015)
In such a sentence, the object of the control verb will also be assigned a semantic role
by the subordinate clause?s verb:
(32) Commonwealth Edison said the ruling could force [
Arg0
it] to slash
[
Arg1
its 1989 earnings] by [
Arg2-by
$1.55 a share]. (wsj_0015)
82
7 Note that out of is exactly parallel to into, but one is spelled with a space in the middle and the other isn?t.
Computational Linguistics Volume 31, Number 1
83
While it is the Arg0 of force, it is the Arg1 of slash. Similarly, subject control verbs such as
promise result in the subject of the main clause being assigned two roles, one for each verb:
(33) [
Arg0
Mr. Bush?s legislative package] promises [
Arg2
to cut emissions by
10 million tons?basically in half?by the year 2000]. (wsj_0146)
(34) [
Arg0
Mr. Bush?s legislative package
i
] promises [
Arg0
*trace*
i
] to cut
[
Arg1
emissions] [
Arg2
by 10 million tons?basically in half?]
[
ARGM-TMP
by the year 2000].
We did not find a single case of a subject control verb used with a direct object and an
infinitival clause (e.g., John promised Mary to come) in the Penn Treebank.
The cases above must be contrasted with verbs such as expect, often referred as
exceptional case marking (ECM) verbs, where an infinitival subordinate clause is a
single semantic argument:
(35) Frameset expect ??look forward to, anticipate??
Arg0: expector
Arg1: anticipated event
Ex: Mr. Leinonen said [
Arg0
he] expects [
Arg1
Ford to meet the deadline
easily]. (wsj_0064)
While Ford is given a semantic role for the verb meet, it is not given a role for expect.
3.4.3 Split Constituents. Most verbs of saying (say, tell, ask, report, etc.) have the
property that the verb and its subject can be inserted almost anywhere within another
of the verb?s arguments. While the canonical realization is John said (that) Mary was
going to eat outside at lunchtime today, it is common to say Mary, John said, was going to eat
outside at lunchtime today or Mary was going to eat outside, John said, at lunchtime today. In
this situation, there is no constituent holding the whole of the utterance while not also
holding the verb of saying. We annotate these cases by allowing a single semantic role
to point to the component pieces of the split constituent in order to cover the correct,
discontinuous substring of the sentence.
(36) Frameset say
Arg0: speaker
Arg1: utterance
Arg2: listener
Ex: [
Arg1
By addressing those problems], [
Arg0
Mr. Maxwell] said,
[
Arg1
the new funds have become ??extremely attractive to Japanese
and other investors outside the U.S.??] (wsj_0029)
In the flat structure we have been using for example sentences, this looks like a case of
repeated role labels. Internally, however, there is one role label pointing to multiple
constituents of the tree, shown in Figure 1.
Palmer, Gildea, and Kingsbury The Proposition Bank
4. The Propbank Development Process
Since the Proposition Bank consists of two portions, the lexicon of frames files and the
annotated corpus, the process is similarly divided into framing and annotation.
4.1 Framing
The process of creating the frames files, that is, the collection of framesets for each
lexeme, begins with the examination of a sample of the sentences from the corpus
containing the verb under consideration. These instances are grouped into one or more
major senses, and each major sense is turned into a single frameset. To show all the
possible syntactic realizations of the frameset, many sentences from the corpus are
included in the frames file, in the same format as the examples above. In many cases a
particular realization will not be attested within the Penn Treebank corpus; in these
cases, a constructed sentence is used, usually identified by the presence of the
characters of John and Mary. Care was taken during the framing process to make
synonymous verbs (mostly in the sense of ??sharing a VerbNet Class??) have the same
framing, with the same number of roles and the same descriptors on those roles.
Generally speaking, a given lexeme/sense pair required 10?15 minutes to frame,
although highly polysemous verbs could require longer. With the 4,500+ framesets
currently in place for PropBank, this is clearly a substantial time investment, and the
frames files represent an important resource in their own right. We were able to use
membership in a VerbNet class which already had consistent framing to project
accurate frames files for up to 300 verbs. If the overlap between VerbNet and
PropBank had been more than 50%, this number might have been higher.
4.2 Annotation
We begin the annotation process by running a rule-based argument tagger (Palmer,
Rosenzweig, and Cotton 2001) on the corpus. This tagger incorporates an extensive
lexicon, entirely separate from that used by PropBank, which encodes class-based
84
Figure 1
Split constituents: In this case, a single semantic role label points to multiple nodes in the original
treebank tree.
Computational Linguistics Volume 31, Number 1
85
mappings between grammatical and semantic roles. The rule-based tagger achieved
83% accuracy on pilot data, with many of the errors due to differing assumptions
made in defining the roles for a particular verb. The output of this tagger is then
corrected by hand. Annotators are presented with an interface which gives them access
to both the frameset descriptions and the full syntactic parse of any sentence from the
treebank and allows them to select nodes in the parse tree for labeling as arguments of
the predicate selected. For any verb they are able to examine both the descriptions of
the arguments and the example tagged sentences, much as they have been presented
here. The tagging is done on a verb-by-verb basis, known as lexical sampling, rather
than all-words annotation of running text.
The downside of this approach is that it does not quickly provide a stretch of fully
annotated text, needed for early assessment of the usefulness of the resource (see
subsequent sections). For this reason a domain-specific subcorpus was automatically
extracted from the entirety of the treebank, consisting of texts roughly primarily
concerned with financial reporting and identified by the presence of a dollar sign
anywhere in the text. This ??financial?? subcorpus comprised approximately one-third
of the treebank and served as the initial focus of annotation.
The treebank as a whole contains 3,185 unique verb lemmas, while the financial
subcorpus contains 1,826. These verbs are arrayed in a classic Zipfian distribution,
with a few verbs occurring very often (say, for example, is the most common verb, with
over 10,000 instances in its various inflectional forms) and most verbs occurring two or
fewer times. As with the distribution of the lexical items themselves, the framesets also
display a Zipfian distribution: A small number of verbs have many framesets ( go has
20 when including phrasal variants, and come, get, make, pass, take, and turn each have
more than a dozen) while the majority of verbs (2581/3342) have only one frameset.
For polysemous verbs annotators had to determine which frameset was appropriate
for a given usage in order to assign the correct argument structure, although this
information was explicitly marked only during a separate pass.
Annotations were stored in a stand-off notation, referring to nodes within the Penn
Treebank without actually replicating any of the lexical material or structure of that
corpus. The process of annotation was a two-pass, blind procedure followed by an
adjudication phase to resolve differences between the two initial passes. Both role
labeling decisions and the choice of frameset were adjudicated.
The annotators themselves were drawn from a variety of backgrounds, from
undergraduates to holders of doctorates, including linguists, computer scientists, and
others. Undergraduates have the advantage of being inexpensive but tend to work for
only a few months each, so they require frequent training. Linguists make the best
overall judgments although several of our nonlinguist annotators also had excellent
skills. The learning curve for the annotation task tended to be very steep, with most
annotators becoming comfortable with the process within three days of work. This
contrasts favorably with syntactic annotation, which has a much longer learning curve
(Marcus, personal communication), and indicates one of the advantages of using
a corpus already syntactically parsed as the basis of semantic annotation. Over
30 annotators contributed to the project, some for just a few weeks, some for up to
three years. The framesets were created and annotation disagreements were adju-
dicated by a small team of highly trained linguists: Paul Kingsbury created the frames
files and managed the annotators, and Olga Babko-Malaya checked the frames files for
consistency and did the bulk of the adjudication.
We measured agreement between the two annotations before the adjudication step
using the kappa statistic (Siegel and Castellan 1988), which is defined with respect to
Palmer, Gildea, and Kingsbury The Proposition Bank
the probability of interannotator agreement, P?A?, and the agreement expected by
chance, P?E?:
k ? P?A?  P?E?
1  P?E?
Measuring interannotator agreement for PropBank is complicated by the large num-
ber of possible annotations for each verb. For role identification, we expect agree-
ment between annotators to be much higher than chance, because while any node in
the parse tree can be annotated, the vast majority of arguments are chosen from the
small number of nodes near the verb. In order to isolate the role classification decisions
from this effect and avoid artifically inflating the kappa score, we split role
identification (role vs. nonrole) from role classification (Arg0 vs. Arg1 vs. . . .) and
calculate kappa for each decision separately. Thus, for the role identification kappa,
the interannotator agreement probability P?A? is the number of node observation
agreements divided by the total number of nodes considered, which is the number of
nodes in each parse tree multiplied by the number of predicates annotated in the
sentence. All the PropBank data were annotated by two people, and in calculating
kappa we compare these two annotations, ignoring the specific identities of the
annotators for the predicate (in practice, agreement varied with the training and skill
of individual annotators). For the role classification kappa, we consider only nodes
that were marked as arguments by both annotators and compute kappa over the
choices of possible argument labels. For both role identification and role classification,
we compute kappa for two ways of treating ArgM labels. The first is to treat ArgM
labels as arguments like any other, in which case ArgM-TMP, ArgM-LOC, and so on
are considered separate labels for the role classification kappa. In the second scenario,
we ignore ArgM labels, treating them as unlabeled nodes, and calculate agreement for
identification and classification of numbered arguments only.
Kappa statistics for these various decisions are shown in Table 2. Agreement
on role identification is very high (.99 under both treatments of ArgM), given the large
number of obviously irrelevant nodes. Reassuringly, kappas for the more difficult
role classification task are also high: .93 including all types of ArgM and .96 con-
sidering only numbered arguments. Kappas on the combined identification and
classication decision, calculated over all nodes in the tree, are .91 including all sub-
types of ArgM and .93 over numbered arguments only. Interannotator agreement
among nodes that either annotator identified as an argument was .84, including ArgMs
and .87, excluding ArgMs.
Discrepancies between annotators tended to be less on numbered arguments than
on the selection of function tags, as shown in the confusion matrices of Tables 3 and 4.
86
Table 2
Interannotator agreement.
P?A? P?E? k
Including ArgM Role identification .99 .89 .93
Role classification .95 .27 .93
Combined decision .99 .88 .91
Excluding ArgM Role identification .99 .91 .94
Role classification .98 .41 .96
Combined decision .99 .91 .93
Computational Linguistics Volume 31, Number 1
87
Certain types of functions, particularly those represented by the tags ADV, MNR, and
DIS, can be difficult to distinguish. For example, in the sentence Also, substantially lower
Dutch corporate tax rates helped the company keep its tax outlay flat relative to earnings
growth (wsj_0132), the phrase relative to earnings growth could be interpreted as a
manner adverbial (MNR), describing how the tax outlays were kept flat, or as a
general-purpose adverbial (ADV), merely providing more information on the keeping
event. Similarly, a word such as then can have several functions. It is canonically a
temporal adverb marking time or a sequence of events (. . . the Senate then broadened the
list further . . . (wsj_0101)) but can also mark a consequence of another action (. . . if for
any reason I don?t have the values, then I won?t recommend it. (wsj_0331)) or simply serve as
a placeholder in conversation (It?s possible then that Santa Fe?s real estate . . . could one day
fetch a king?s ransom (wsj_0331)). These three usages require three different taggings
(TMP, ADV, and DIS, respectively) and can easily trip up an annotator.
The financial subcorpus was completely annotated and given a preadjudication
release in June 2002. The fully annotated and adjudicated corpus was completed in
March 2004. Both of these are available through the Linguistic Data Consortium,
although because of the use of the stand-off notation, prior possession of the treebank
is also necessary. The frames files are distributed separately and are available through
the project Web site at http://www.cis.upenn.edu/?ace/.
Table 3
Confusion matrix for argument labels, with ArgM labels collapsed into one category. Entries are
a fraction of total annotations; true zeros are omitted, while other entries are rounded to zero.
Arg0 Arg1 Arg2 Arg3 Arg4 ArgM
Arg0 0.288 0.006 0.001 0.000 0.000
Arg1 0.364 0.006 0.001 0.000 0.002
Arg2 0.074 0.001 0.001 0.003
Arg3 0.013 0.000 0.001
Arg4 0.011 0.000
ArgM 0.228
Table 4
Confusion matrix among subtypes of ArgM, defined in Table 1. Entries are fraction of all ArgM
labels. Entries are a fraction of all ArgM labels; true zeros are omitted, while other entries are
rounded to zero.
ADV CAU DIR DIS EXT LOC MNR MOD NEG PNC TMP
ADV 0.087 0.003 0.001 0.017 0.001 0.004 0.016 0.001 0.000 0.003 0.007
CAU 0.018 0.000 0.000 0.001 0.001 0.002 0.002
DIR 0.014 0.000 0.001 0.001 0.000
DIS 0.055 0.000 0.000 0.002 0.000 0.000 0.000 0.005
EXT 0.007 0.000 0.001 0.000 0.000
LOC 0.106 0.006 0.000 0.000 0.000 0.003
MNR 0.085 0.000 0.000 0.001 0.002
MOD 0.161 0.000 0.000
NEG 0.061 0.001
PNC 0.026 0.000
TMP 0.286
Palmer, Gildea, and Kingsbury The Proposition Bank
5. FrameNet and PropBank
The PropBank project and the FrameNet project at the International Computer Science
Institute (Baker, Fillmore, and Lowe 1998) share the goal of documenting the syntactic
realization of arguments of the predicates of the general English lexicon by annotating
a corpus with semantic roles. Despite the two projects? similarities, their methodol-
ogies are quite different. FrameNet is focused on semantic frames,8 which are defined
as a schematic representation of situations involving various participants, props, and
other conceptual roles (Fillmore 1976). The project methodology has proceeded on a
frame-by-frame basis, that is, by first choosing a semantic frame (e.g., Commerce),
defining the frame and its participants or frame elements (BUYER, GOODS, SELLER,
MONEY), listing the various lexical predicates which invoke the frame (buy, sell, etc.),
and then finding example sentences of each predicate in a corpus (the British National
Corpus was used) and annotating each frame element in each sentence. The example
sentences were chosen primarily to ensure coverage of all the syntactic realizations of
the frame elements, and simple examples of these realizations were preferred over
those involving complex syntactic structure not immediately relevant to the lexical
predicate itself. Only sentences in which the lexical predicate was used ??in frame??
were annotated. A word with multiple distinct senses would generally be analyzed as
belonging to different frames in each sense but may only be found in the FrameNet
corpus in the sense for which a frame has been defined. It is interesting to note that the
semantic frames are a helpful way of generalizing between predicates; words in the
same frame have been found frequently to share the same syntactic argument
structure (Gildea and Jurafsky 2002). A more complete description of the FrameNet
project can be found in Baker, Fillmore, and Lowe (1998) and Johnson et al (2002), and
the ramifications for automatic classification are discussed more thoroughly in Gildea
and Jurafsky (2002).
In contrast with FrameNet, PropBank is aimed at providing data for training
statistical systems and has to provide an annotation for every clause in the Penn
Treebank, no matter how complex or unexpected. Similarly to FrameNet, PropBank
also attempts to label semantically related verbs consistently, relying primarily on
VerbNet classes for determining semantic relatedness. However, there is much less
emphasis on the definition of the semantics of the class that the verbs are associated
with, although for the relevant verbs additional semantic information is provided
through the mapping to VerbNet. The PropBank semantic roles for a given VerbNet
class may not correspond to the semantic elements highlighted by a particular
FrameNet frame, as shown by the examples of Table 5. In this case, FrameNet?s
COMMERCE frame includes roles for Buyer (the receiver of the goods) and Seller (the
receiver of the money) and assigns these roles consistently to two sentences describing
the same event:
FrameNet annotation:
(37) [
Buyer
Chuck] bought [
Goods
a car] [
Seller
from Jerry] [
Payment
for $1000].
(38) [
Seller
Jerry] sold [
Goods
a car] [
Buyer
to Chuck] [
Payment
for $1000].
88
8 The authors apologize for the ambiguity between PropBank?s ??syntactic frames?? and Framenet?s
??semantic frames.?? Syntactic frames refer to syntactic realizations. Semantic frames will appear herein in
boldface.
Computational Linguistics Volume 31, Number 1
89
PropBank annotation:
(39) [
Arg0
Chuck] bought [
Arg1
a car] [
Arg2
from Jerry] [
Arg3
for $1000].
(40) [
Arg0
Jerry] sold [
Arg1
a car] [
Arg2
to Chuck] [
Arg3
for $1000].
PropBank requires an additional level of inference to determine who has possession of
the car in both cases. However, FrameNet does not indicate that the subject in both
sentences is an Agent, represented in PropBank by labeling both subjects as Arg0.9
Note that the subject is not necessarily an agent, as in, for instance, the passive
construction:
FrameNet annotation:
(41) [
Goods
A car] was bought [
Buyer
by Chuck].
(42) [
Goods
A car] was sold [
Buyer
to Chuck] [
Seller
by Jerry].
(43) [
Buyer
Chuck] was sold [
Goods
a car] [
Seller
by Jerry].
PropBank annotation:
(44) [
Arg1
A car] was bought [
Arg0
by Chuck].
(45) [
Arg1
A car] was sold [
Arg2
to Chuck] [
Arg0
by Jerry].
(46) [
Arg2
Chuck] was sold [
Arg1
a car] [
Arg0
by Jerry].
To date, PropBank has addressed only verbs, whereas FrameNet includes nouns
and adjectives.10 PropBank annotation also differs in that it takes place with reference
to the Penn Treebank trees; not only are annotators shown the trees when analyzing
a sentence, they are constrained to assign the semantic labels to portions of the
sentence corresponding to nodes in the tree. Parse trees are not used in FrameNet;
annotators mark the beginning and end points of frame elements in the text and add
Table 5
Comparison of frames.
PropBank FrameNet
buy sell COMMERCE
Arg0: buyer Arg0: seller Buyer
Arg1: thing bought Arg1: thing sold Seller
Arg2: seller Arg2: buyer Payment
Arg3: price paid Arg3: price paid Goods
Arg4: benefactive Arg4: benefactive Rate/Unit
9 FrameNet plans ultimately to represent agency is such examples using multiple inheritance of frames
(Fillmore and Atkins 1998; Fillmore and Baker 2001).
10 New York University is currently in the process of annotating nominalizations in the Penn Treebank
using the PropBank frames files and annotation interface, creating a resource to be known as NomBank.
Palmer, Gildea, and Kingsbury The Proposition Bank
a grammatical function tag expressing the frame element?s syntactic relation to the
predicate.
6. A Quantitative Analysis of the Semantic-Role Labels
The stated aim of PropBank is the training of statistical systems. It also provides a rich
resource for a distributional analysis of semantic features of language that have
hitherto been somewhat inaccessible. We begin this section with an overview of
general characteristics of the syntactic realization of the different semantic-role labels
and then attempt to measure the frequency of syntactic alternations with respect to
verb class membership. We base this analysis on previous work by Merlo and
Stevenson (2001). In the following section we discuss the performance of a system
trained to automatically assign the semantic-role labels.
6.1 Associating Role Labels with Specific Syntactic Constructions
We begin by simply counting the frequency of occurrence of roles in specific syntactic
positions. In all the statistics given in this section, we do not consider past- or present-
participle uses of the predicates, thus excluding any passive-voice sentences. The
syntactic positions used are based on a few heuristic rules: Any NP under an S node in
the treebank is considered a syntactic subject, and any NP under a VP is considered an
object. In all other cases, we use the syntactic category of the argument?s node in the
treebank tree: for example, SBAR for sentential complements and PP for prepositional
phrases. For prepositional phrases, as well as for noun phrases that are the object of
a preposition, we include the preposition as part of our syntactic role: for example,
PP-in, PP-with. Table 6 shows the most frequent semantic roles associated with var-
ious syntactic positions, while Table 7 shows the most frequent syntactic positions for
various roles.
Tables 6 and 7 show overall statistics for the corpus, and some caution is needed in
interpreting the results, as the semantic-role labels are defined on a per-frameset basis
and do not necessarily have corpus-wide definitions. Nonetheless, a number of trends
are apparent. Arg0, when present, is almost always a syntactic subject, while the
subject is Arg0 only 79% of the time. This provides evidence for the notion of a
thematic hierarchy in which the highest-ranking role present in a sentence is given the
90
Table 6
Most frequent semantic roles for each syntactic position.
Position Total Four most common roles (%) Other
roles (%)
Sub 37,364 Arg0 79.0 Arg1 16.8 Arg2 2.4 TMP 1.2 0.6
Obj 21,610 Arg1 84.0 Arg2 9.8 TMP 4.6 Arg3 0.8 0.8
S 10,110 Arg1 76.0 ADV 8.5 Arg2 7.5 PRP 2.4 5.5
NP 7,755 Arg2 34.3 Arg1 23.6 Arg4 18.9 Arg3 12.9 10.4
ADVP 5,920 TMP 30.3 MNR 22.2 DIS 19.8 ADV 10.3 17.4
MD 4,167 MOD 97.4 ArgM 2.3 Arg1 0.2 MNR 0.0 0.0
PP-in 3,134 LOC 46.6 TMP 35.3 MNR 4.6 DIS 3.4 10.1
SBAR 2,671 ADV 36.0 TMP 30.4 Arg1 16.8 PRP 7.6 9.2
RB 1,320 NEG 91.4 ArgM 3.3 DIS 1.6 DIR 1.4 2.3
PP-at 824 EXT 34.7 LOC 27.4 TMP 23.2 MNR 6.1 8.6
Computational Linguistics Volume 31, Number 1
91
honor of subjecthood. Going from syntactic position to semantic role, the numbered
arguments are more predictable than the non-predicate-specific adjunct roles. The two
exceptions are the roles of ??modal?? (MOD) and ??negative?? (NEG), which as previously
discussed are not syntactic adjuncts at all but were simply marked as ArgMs as the
best means of tracking their important semantic contributions. They are almost always
realized as auxiliary verbs and the single adverb (part-of-speech tag RB) not,
respectively.
6.2 Associating Verb Classes with Specific Syntactic Constructions
Turning to the behavior of individual verbs in the PropBank data, it is interesting to
see how much correspondence there is between verb classes proposed in the literature
Table 7
Most frequent syntactic positions for each semantic role.
Roles Total Four most common syntactic positions (%) Other
positions
(%)
Arg1 35,112 Obj 51.7 S 21.9 Subj 17.9 NP 5.2 3.4
Arg0 30,459 Subj 96.9 NP 2.4 S 0.2 Obj 0.2 0.2
Arg2 7,433 NP 35.7 Obj 28.6 Subj 12.1 S 10.2 13.4
TMP 6,846 ADVP 26.2 PP-in 16.2 Obj 14.6 SBAR 11.9 31.1
MOD 4,102 MD 98.9 ADVP 0.8 NN 0.1 RB 0.0 0.1
ADV 3,137 SBAR 30.6 S 27.4 ADVP 19.4 PP-in 3.1 19.5
LOC 2,469 PP-in 59.1 PP-on 10.0 PP-at 9.2 ADVP 6.4 15.4
MNR 2,429 ADVP 54.2 PP-by 9.6 PP-with 7.8 PP-in 5.9 22.5
Arg3 1,762 NP 56.7 Obj 9.7 Subj 8.9 ADJP 7.8 16.9
DIS 1,689 ADVP 69.3 CC 10.6 PP-in 6.2 PP-for 5.4 8.5
Table 8
Semantic roles of verbs? subjects, for the verb classes of Merlo and Stevenson (2001).
Relative frequency of semantic roleVerb Count
Arg0 Arg1 Arg2 ArgA TMP
Unergative
float 14 35.7 64.3
hurry 2 100.0
jump 125 97.6 2.4
leap 11 90.9 9.1
march 8 87.5 12.5
race 4 75.0 25.0
rush 31 6.5 90.3 3.2
vault 1 100.0
wander 3 100.0
glide 1 100.0
hop 34 97.1 2.9
jog 1 100.0
scoot 1 100.0
scurry 2 100.0
skip 5 100.0
tiptoe 2 100.0
Palmer, Gildea, and Kingsbury The Proposition Bank
and the annotations in the corpus. Table 8 shows the PropBank semantic role labels for
the subjects of each verb in each class. Merlo and Stevenson (2001) aim to
automatically classify verbs into one of three categories: unergative, unaccusative,
and object-drop. These three categories, more coarse-grained than the classes of Levin
or VerbNet, are defined by the semantic roles they assign to a verb?s subjects and
objects in both transitive and intransitive sentences, as illustrated by the following
examples:
Unergative: [
Causal Agent
The jockey] raced [
Agent
the horse] past the barn.
[
Agent
The horse] raced past the barn.
92
Table 8
(cont.)
Relative frequency of semantic roleVerb Count
Arg0 Arg1 Arg2 ArgA TMP
Unaccusative
boil 1 100.0
dissolve 4 75.0 25.0
explode 7 100.0
flood 5 80.0 20.0
fracture 1 100.0
melt 4 25.0 50.0 25.0
open 80 72.5 21.2 2.5 3.8
solidify 6 83.3 16.7
collapse 36 94.4 5.6
cool 9 66.7 33.3
widen 29 27.6 72.4
change 148 65.5 33.8 0.7
clear 14 78.6 21.4
divide 1 100.0
simmer 5 100.0
stabilize 33 45.5 54.5
Object-Drop
dance 2 100.0
kick 5 80.0 20.0
knit 1 100.0
paint 4 100.0
play 67 91.0 9.0
reap 10 100.0
wash 4 100.0
yell 5 100.0
borrow 36 100.0
inherit 6 100.0
organize 11 100.0
sketch 1 100.0
clean 4 100.0
pack 7 100.0
study 40 100.0
swallow 5 80.0 20.0
call 199 97.0 1.5 1.0 0.5
Computational Linguistics Volume 31, Number 1
93
Unaccusative: [
Causal Agent
The cook] melted [
Theme
the butter] in the pan.
[
Theme
The butter] melted in the pan.
Object-Drop: [
Agent
The boy] played [
Theme
soccer].
[
Agent
The boy] played.
6.2.1 Predictions. In our data, the closest analogs to Merlo and Stevenson?s three roles
of Causal Agent, Agent, and Theme are ArgA, Arg0, and Arg1, respectively. We
hypothesize that PropBank data will confirm
1. that the subject can take one of two roles (Arg0 or Arg1) for
unaccusative and unergative verbs but only one role (Arg0) for
object-drop verbs;
2. that Arg1s appear more frequently as subjects for intransitive
unaccusatives than they do for intransitive unergatives.
In Table 8 we show counts for the semantic roles of the subjects of the Merlo
and Stevenson verbs which appear in PropBank (80%), regardless of transitivity, in
order to measure whether the data in fact reflect the alternations between syntactic and
semantic roles that the verb classes predict. For each verb, we show counts only for
occurrences tagged as belonging to the first frameset, reflecting the predominant or
unmarked sense.
6.2.2 Results of Prediction 1. The object-drop verbs of Merlo and Stevenson do in fact
show little variability in our corpus, with the subject almost always being Arg0. The
unergative and unaccusative verbs show much more variability in the roles that can
appear in the subject position, as predicted, although some individual verbs always
have Arg0 as subject, presumably as a result of the small number of occurrences.
6.2.3 Results of Prediction 2. As predicted, there is in general a greater preponderance
of Arg1 subjects for unaccusatives than for unergatives, with the striking exception of a
few unergative verbs, such as jump and rush, whose subjects are almost always Arg1.
Jump is being affected by the predominance of a financial-subcorpus sense used for
stock reportage (79 out of 82 sentences), which takes jump as rise dramatically: Jaguar
shares jumped 23 before easing to close at 654, up 6. (wsj_1957) Rush is being affected by a
framing decision, currently being reconsidered, wherein rush was taken to mean cause
to move quickly. Thus the entity in motion is tagged Arg1, as in Congress in Congress would
have rushed to pass a private relief bill. (wsj_0946) The distinction between unergatives
and unaccusatives is not apparent from the PropBank data in this table, since we are
not distinguishing between transitives and intransitives, which is left for future
experiments.
In most cases, the first frameset (numbered 1 in the PropBank frames files) is the
most common, but in a few cases this is not the case because of the domain of the text.
For example, the second frameset for kick, corresponding to the phrasal usage kick in,
meaning begin, accounted for seven instances versus the five instances for frameset 1.
Palmer, Gildea, and Kingsbury The Proposition Bank
The phrasal frameset has a very different pattern, with the subject always corres-
ponding to Arg1, as in
(47) [
Arg1
Several of those post-crash changes] kicked in [
ArgM-TMP
during
Friday?s one-hour collapse] and worked as expected, even though
they didn?t prevent a stunning plunge. (wsj_2417)
Statistics for all framesets of kick are shown in Table 9; the first row in Table 9
corresponds to the entry for kick in the ??Object-Drop?? section of Table 8.
Overall, these results support our hypotheses and also highlight the important
role played by even the relatively coarse-grained sense tagging exemplified by the
framesets.
7. Automatic Determination of Semantic-Role Labels
The stated goal of the PropBank is to provide training data for supervised automatic
role labelers, and the project description cannot be considered complete without a
discussion of PropBank?s suitability for this purpose. One of PropBank?s important
features as a practical resource is that the sentences chosen for annotation are from the
same Wall Street Journal corpus used for the original Penn Treebank project, and thus
hand-checked syntactic parse trees are available for the entire data set. In this section,
we examine the importance of syntactic information for semantic-role labeling by
comparing the performance of a system based on gold-standard parses with one using
automatically generated parser output. We then examine whether it is possible that the
additional information contained in a full parse tree is negated by the errors present in
automatic parser output, by testing a role-labeling system based on a flat or ??chunked??
representation of the input.
Gildea and Jurafsky (2002) describe a statistical system trained on the data from
the FrameNet project to automatically assign semantic roles. The system first passed
sentences through an automatic parser (Collins 1999), extracted syntactic features from
the parses, and estimated probabilities for semantic roles from the syntactic and lexical
features. Both training and test sentences were automatically parsed, as no hand-
annotated parse trees were available for the corpus. While the errors introduced by the
parser no doubt negatively affected the results obtained, there was no direct way of
quantifying this effect. One of the systems evaluated for the Message Understanding
Conference task (Miller et al 1998) made use of an integrated syntactic and semantic
model producing a full parse tree and achieved results comparable to other systems
that did not make use of a complete parse. As in the FrameNet case, the parser was not
94
Table 9
Semantic roles for different frame sets of kick.
Frame set Count Relative frequency of semantic role
Arg0 Arg1 Arg2 ArgA TMP
Unergative
kick.01: drive or impel with the foot 5 80.0 20.0
kick.02: kick in, begin 7 100.0
kick.04: kick off, begin, inaugurate 3 100.0
Computational Linguistics Volume 31, Number 1
95
trained on the corpus for which semantic annotations were available, and the effect of
better, or even perfect, parses could not be measured.
In our first set of experiments, the features and probability model of the Gildea and
Jurafsky (2002) system were applied to the PropBank corpus. The existence of the
hand-annotated treebank parses for the corpus allowed us to measure the
improvement in performance offered by gold-standard parses.
7.1 System Description
Probabilities of a parse constituent belonging to a given semantic role are calculated
from the following features:
The phrase type feature indicates the syntactic type of the phrase expressing the
semantic roles: Examples include noun phrase (NP), verb phrase (VP), and clause (S).
The parse tree path feature is designed to capture the syntactic relation of a
constituent to the predicate.11 It is defined as the path from the predicate through the
parse tree to the constituent in question, represented as a string of parse tree
nonterminals linked by symbols indicating upward or downward movement through
the tree, as shown in Figure 2. Although the path is composed as a string of symbols,
our systems treat the string as an atomic value. The path includes, as the first element
of the string, the part of speech of the predicate, and as the last element, the phrase
type or syntactic category of the sentence constituent marked as an argument.
The position feature simply indicates whether the constituent to be labeled occurs
before or after the predicate. This feature is highly correlated with grammatical
function, since subjects will generally appear before a verb and objects after. This
feature may overcome the shortcomings of reading grammatical function from the
parse tree, as well as errors in the parser output.
The voice feature distinguishes between active and passive verbs and is important
in predicting semantic roles, because direct objects of active verbs correspond to
subjects of passive verbs. An instance of a verb is considered passive if it is tagged as a
past participle (e.g., taken), unless it occurs as a descendent verb phrase headed by any
form of have (e.g., has taken) without an intervening verb phrase headed by any form of
be (e.g., has been taken).
11 While the treebank has a ??subject?? marker on noun phrases, this is the only such grammatical function
tag. The treebank does not explicitly represent which verb?s subject the node is, and the subject tag is not
typically present in automatic parser output.
Figure 2
In this example, the path from the predicate ate to the argument NP He can be represented as
VBjVPjS,NP, with j indicating upward movement in the parse tree and , downward
movement.
Palmer, Gildea, and Kingsbury The Proposition Bank
The headword is a lexical feature and provides information about the semantic
type of the role filler. Headwords of nodes in the parse tree are determined using the
same deterministic set of headword rules used by Collins (1999).
The system attempts to predict argument roles in new data, looking for the
highest-probability assignment of roles ri to all constituents i in the sentence, given the
set of features Fi ? fpti, pathi, posi, vi, hig at each constituent in the parse tree, and the
predicate p:
argmaxr1 : : : n P?r1 : : : njF1 : : : n, p?
We break the probability estimation into two parts, the first being the probability
P?ri j Fi,p? of a constituent?s role given our five features for the constituent and the
predicate p. Because of the sparsity of the data, it is not possible to estimate this
probability from the counts in the training data. Instead, probabilities are estimated
from various subsets of the features and interpolated as a linear combination of the
resulting distributions. The interpolation is performed over the most specific
distributions for which data are available, which can be thought of as choosing the
topmost distributions available from a back-off lattice, shown in Figure 3.
Next, the probabilities P?ri j Fi,p? are combined with the probabilities P?fr1 : : : ngjp?
for a set of roles appearing in a sentence given a predicate, using the following formula:
P?r1 : : : n jF1 : : : n, p? , P?fr1 : : : ngjp?
Y
i
P?ri jFi, p?
P?ri jp?
This approach, described in more detail in Gildea and Jurafsky (2002), allows
interaction among the role assignments for individual constituents while making
certain independence assumptions necessary for efficient probability estimation. In
particular, we assume that sets of roles appear independent of their linear order and
that the features F of a constituent are independent of other constituents? features
given the constituent?s role.
7.1.1 Results. We applied the same system, using the same features, to a preliminary
release of the PropBank data. The data set used contained annotations for 72,109
predicate-argument structures containing 190,815 individual arguments and examples
96
Figure 3
Back-off lattice with more specific distributions towards the top.
Computational Linguistics Volume 31, Number 1
97
from 2,462 lexical predicates (types). In order to provide results comparable with the
statistical parsing literature, annotations from section 23 of the treebank were used as
the test set; all other sections were included in the training set. The preliminary
version of the data used in these experiments was not tagged for WordNet word
sense or PropBank frameset. Thus, the system neither predicts the frameset nor uses it
as a feature.
The system was tested under two conditions, one in which it is given the
constituents which are arguments to the predicate and merely has to predict the
correct role, and one in which it has to both find the arguments in the sentence and
label them correctly. Results are shown in Tables 10 and 11. Results for FrameNet are
based on a test set of 8,167 individual labels from 4,000 predicate-argument structures.
As a guideline for interpreting these results, with 8,167 observations, the threshold for
statistical significance with p < .05 is a 1.0% absolute difference in performance (Gildea
and Jurafsky 2002). For the PropBank data, with a test set of 8,625 individual labels, the
threshold for significance is similar. There are 7,574 labels for which the predicate has
been seen 10 or more times in training (third column of the tables).
Results for PropBank are similar to those for FrameNet, despite the smaller
number of training examples for many of the predicates. The FrameNet data contained
at least 10 examples from each predicate, while 12% of the PropBank data had fewer
than 10 training examples. Removing these examples from the test set gives 82.8%
accuracy with gold-standard parses and 80.9% accuracy with automatic parses.
7.1.2 Adding Traces. The gold-standard parses of the Penn Treebank include several
types of information not typically produced by statistical parsers or included in their
evaluation. Of particular importance are traces, empty syntactic categories which
generally occupy the syntactic position in which another constituent could be
interpreted and include a link to the relevant constituent. Traces are used to indicate
cases of wh-extraction, antecedents of relative clauses, and control verbs exhibiting the
syntactic phenomena of raising and ??equi.?? Traces are intended to provide hints as to
the semantics of individual clauses, and the results in Table 11 show that they do so
effectively. When annotating syntactic trees, the PropBank annotators marked the
traces along with their antecedents as arguments of the relevant verbs. In line 2 of
Table 11, along with all our experiments with automatic parser output, traces were
ignored, and the semantic-role label was assigned to the antecedent in both training
and test data. In line 3 of Table 11, we assume that the system is given trace
information, and in cases of trace chains, the semantic-role label is assigned to the trace
in training and test conditions. Trace information boosts the performance of the system
by roughly 5%. This indicates that systems capable of recovering traces (Johnson 2002;
Dienes and Dubey 2003) could improve semantic-role labeling.
Table 10
Accuracy of semantic-role prediction (in percentages) for known boundaries (the system is given
the constituents to classify).
Accuracy
FrameNet PropBank PropBank > 10 examples
Automatic parses 82.0 79.9 80.9
Gold-standard parses 82.0 82.8
Palmer, Gildea, and Kingsbury The Proposition Bank
As our path feature is a somewhat unusual way of looking at parse trees, its
behavior in the system warrants a closer look. The path feature is most useful as a way
of finding arguments in the unknown boundary condition. Removing the path feature
from the known-boundary system results in only a small degradation in performance,
from 82.0% to 80.1%. One reason for the relatively small impact may be sparseness of
the feature: 7% of paths in the test set are unseen in training data. The most common
values of the feature are shown in Table 12, in which the first two rows correspond to
standard subject and object positions. One reason for sparsity is seen in the third row:
In the treebank, the adjunction of an adverbial phrase or modal verb can cause an
additional VP node to appear in our path feature. We tried two variations of the path
feature to address this problem. The first collapses sequences of nodes with the same
label, for example, combining rows 2 and 3 of Table 12. The second variation uses only
two values for the feature: NP under S (subject position) and NP under VP (object
position). Neither variation improved performance in the known-boundary condition.
As a gauge of how closely the PropBank semantic-role labels correspond to the path
feature overall, we note that by always assigning the most common role for each path
(for example, always assigning Arg0 to the subject position), and using no other
features, we obtain the correct role 64.0% of the time, versus 82.0% for the complete
system. Conditioning on the path and predicate, which allows the subject of different
verbs to receive different labels but does not allow for alternation behavior within a
verb?s argument structure, yields an accuracy rate of 76.6%.
Table 13 shows the performance of the system broken down by the argument types
in the gold standard. Results are shown for the unknown-boundaries condition, using
gold-standard parses and traces (last row, middle two columns of Table 11). The
98
Table 12
Common values (in percentages) for parse tree path in PropBank data, using gold-standard
parses.
Path Frequency
VBjVP,NP 17.6
VBjVPjS,NP 16.4
VBjVPjVPjS,NP 7.8
VBjVP,PP 7.6
VBjVP,PP,NP 7.3
VBjVP,SBAR,S 4.3
VBjVP,S 4.3
VBjVP,ADVP 2.4
Others (n = 1,031) 76.0
Table 11
Accuracy of semantic-role prediction (in percentages) for unknown boundaries (the system must
identify the correct constituents as arguments and give them the correct roles).
FrameNet PropBank PropBank > 10 examples
Precision Recall Precision Recall Precision Recall
Automatic parses 64.6 61.2 68.6 57.8 69.9 61.1
Gold-standard parses 74.3 66.4 76.0 69.9
Gold-standard with traces 80.6 71.6 82.0 74.7
Computational Linguistics Volume 31, Number 1
99
??Labeled Recall?? column shows how often the semantic-role label is correctly iden-
tified, while the ??Unlabeled recall?? column shows how often a constituent with the
given role is correctly identified as being a semantic role, even if it is labeled with the
wrong role. The more central, numbered roles are consistently easier to identify than
the adjunct-like ArgM roles, even when the ArgM roles have preexisting Treebank
function tags.
7.2 The Relation of Syntactic Parsing and Semantic-Role Labeling
Many recent information extraction systems for limited domains have relied on finite-
state systems that do not build a full parse tree for the sentence being analyzed.
Among such systems, Hobbs et al (1997) built finite-state recognizers for various
entities, which were then cascaded to form recognizers for higher-level relations, while
Ray and Craven (2001) used low-level ??chunks?? from a general-purpose syntactic
analyzer as observations in a trained hidden Markov model. Such an approach has a
large advantage in speed, as the extensive search of modern statistical parsers is
avoided. It is also possible that this approach may be more robust to error than parsers.
Our experiments working with a flat, ??chunked?? representation of the input sentence,
described in more detail in Gildea and Palmer (2002), test this finite-state hypothesis.
In the chunked representation, base-level constituent boundaries and labels are pres-
ent, but there are no dependencies between constituents, as shown by the following
sample sentence:
(48) [
NP
Big investment banks] [
VP
refused to step] [
ADVP
up] [
PP
to]
[
NP
the plate] [
VP
to support] [
NP
the beleaguered floor traders] [
PP
by]
[
VP
buying] [
NP
bigblocks] [
PP
of] [
NP
stock], [
NP
traders] [
VP
say]. (wsj_2300)
Our chunks were derived from the treebank trees using the conversion described
by Tjong Kim Sang and Buchholz (2000). Thus, the experiments were carried out using
Table 13
Accuracy of semantic-role prediction for unknown boundaries (the system must identify the
correct constituents as arguments and give them the correct roles).
Role Number Precision Labeled recall Unlabeled recall
Arg0 1,197 94.2% 88.9% 92.2%
Arg1 1,436 95.4 82.5 88.9
Arg2 229 79.0 64.2 77.7
Arg3 61 71.4 49.2 54.1
Arg4 31 91.7 71.0 83.9
ArgM 127 59.6 26.8 52.0
ArgM-ADV 85 59.1 30.6 55.3
ArgM-DIR 49 76.7 46.9 61.2
ArgM-DIS 65 40.0 18.5 55.4
ArgM-EXT 18 81.2 72.2 77.8
ArgM-LOC 95 60.7 38.9 62.1
ArgM-MNR 80 62.7 40.0 63.8
ArgM-MOD 95 77.6 40.0 43.2
ArgM-NEG 40 63.6 17.5 40.0
ArgM-PRD 3 0.0 0.0 33.3
ArgM-PRP 54 70.0 25.9 37.0
ArgM-TMP 325 72.4 45.2 64.6
Palmer, Gildea, and Kingsbury The Proposition Bank
gold-standard rather than automatically derived chunk boundaries, which we believe
will provide an upper bound on the performance of a chunk-based system. Distance in
chunks from the predicate was used in place of the parser-based path feature.
The results in Table 14 show that full parse trees are much more effective than the
chunked representation for labeling semantic roles. This is the case even if we relax
the scoring criteria to count as correct all cases in which the system correctly identifies
the first chunk belonging to an argument (last row of Table 14).
As an example for comparing the behavior of the tree-based and chunk-based
systems, consider the following sentence, with human annotations showing the
arguments of the predicate support:
(49) [
Arg0
Big investment banks] refused to step up to the plate to support
[
Arg1
the beleaguered floor traders] [
MNR
by buying big blocks of stock],
traders say.
Our system based on automatic parser output assigned the following analysis:
(50) Big investment banks refused to step up to the plate to support
[
Arg1
the beleaguered floor traders] [
MNR
by buying big blocks of stock],
traders say.
In this case, the system failed to find the predicate?s Arg0 relation, because it is
syntactically distant from the verb support. The original treebank syntactic tree contains
a trace which would allow one to recover this relation, coindexing the empty subject
position of support with the noun phrase Big investment banks. However, our automatic
parser output does not include such traces. The system based on gold-standard trees
and incorporating trace information produced exactly the correct labels:
(51) [
Arg0
Big investment banks] refused to step up to the plate to support
[
Arg1
the beleaguered floor traders] [
MNR
by buying big blocks of stock],
traders say.
The system based on (gold-standard) chunks assigned the following semantic-role labels:
(52) Big investment banks refused to step up to [
Arg0
the plate] to support
[
Arg1
the beleaguered floor traders] by buying big blocks of stock,
traders say.
Here, as before, the true Arg0 relation is not found, and it would be difficult to imagine
identifying it without building a complete syntactic parse of the sentence. But now,
100
Table 14
Summary of results for unknown-boundary condition.
Precision Recall
Gold parse 74.3% 66.4%
Auto parse 68.6 57.8
Chunk 27.6 22.0
Chunk, relaxed scoring 49.5 35.1
Computational Linguistics Volume 31, Number 1
101
unlike in the tree-based output, the Arg0 label is mistakenly attached to a noun phrase
immediately before the predicate. The Arg1 relation in direct-object position is fairly
easily identifiable in the chunked representation as a noun phrase directly following
the verb. The prepositional phrase expressing the Manner relation, however, is not
identified by the chunk-based system. The tree-based system?s path feature for this
constituent is VBjVP,PP, which identifies the prepositional phrase as attaching to the
verb and increases its probability of being assigned an argument label. The chunk-
based system sees this as a prepositional phrase appearing as the second chunk after
the predicate. Although this may be a typical position for the Manner relation, the fact
that the preposition attaches to the predicate rather than to its direct object is not
represented.
Participants in the 2004 CoNLL semantic-labeling shared task (Carreras and
Ma`rquez 2004) have reported higher results for chunk-based systems, but to date
chunk-based systems have not closed the gap with the state-of-the-art results based on
parser output.
7.2.1 Parsing and Models of Syntax. While treebank parsers such as that of Collins
(1999) return much richer representations than a chunker, they do not include a great
deal of the information present in the original Penn Treebank. Specifically, long-
distance dependencies indicated by traces in the treebank are crucial for seman-
tic interpretation but do not affect the constituent recall and precision metrics most
often used to evaluate parsers and are not included in the output of the standard
parsers.
Gildea and Hockenmaier (2003) present a system for labeling PropBank?s semantic
roles based on a statistical parser for combinatory categorial grammar (CCG)
(Steedman 2000). The parser, described in detail in Hockenmaier and Steedman
(2002), is trained on a version of the Penn Treebank automatically converted to CCG
representations. The conversion process uses the treebank?s trace information to make
underlying syntactic relations explicit. For example, the same CCG-level relation
appears between a verb and its direct object whether the verb is used in a simple
transitive clause, a relative clause, or a question with wh-extraction. Using the CCG-
based parser, Gildea and Hockenmaier (2003) find a 2% absolute improvement over
the Collins parser in identifying core or numbered PropBank arguments. This points to
the shortcomings of evaluating parsers purely on constituent precision and recall; we
feel that a dependency-based evaluation (e.g., Carroll, Briscoe, and Sanfilippo 1998) is
more relevant to real-world applications.
8. Conclusion
The Proposition Bank takes the comprehensive corpus annotation of the Penn
Treebank one step closer to a detailed semantic representation by adding semantic-role
labels. On analyzing the data, the relationships between syntax and semantic
structures are more complex than one might at first expect. Alternations in the
realization of semantic arguments of the type described by Levin (1993) turn out to be
common in practice as well as in theory, even in the limited genre of Wall Street Journal
articles. Even so, by using detailed guidelines for the annotation of each individual
verb, rapid consistent annotation has been achieved, and the corpus is available
through the Linguistic Data Consortium. For information on obtaining the frames file,
please consult http://www.cis.upenn.edu/?ace/.
Palmer, Gildea, and Kingsbury The Proposition Bank
The broad-coverage annotation has proven to be suitable for training automatic
taggers, and in addition to ourselves there is a growing body of researchers engaged in
this task. Chen and Rambow (2003) make use of extracted tree-adjoining grammars.
Most recently, the Gildea and Palmer (2002) scores presented here have been improved
markedly through the use of support-vector machines as well as additional features for
named entity tags, headword POS tags, and verb clusters for back-off (Pradhan et al
2003) and using maximum-entropy classifiers (He and Gildea 2004, Xue and Palmer
2004). This group also used Charniak?s parser instead of Collins?s and tested the
system on TDT data. The performance on a new genre is lower, as would be expected.
Despite the complex relationship between syntactic and semantic structures, we
find that statistical parsers, although computationally expensive, do a good job of
providing information relevant for this level of semantic interpretation. In addition to
the constituent structure, the headword information, produced as a side product, is an
important feature. Automatic parsers, however, still have a long way to go. Our results
using hand-annotated parse trees including traces show that improvements in parsing
should translate directly into more accurate semantic representations.
There has already been a demonstration that a preliminary version of these data
can be used to simplify the effort involved in developing information extraction (IE)
systems. Researchers were able to construct a reasonable IE system by simply mapping
specific Arg labels for a set of verbs to template slots, completely avoiding the
necessity of building explicit regular expression pattern matchers (Surdeanu et al
2003). There is equal hope for advantages for machine translation, and proposition
banks in Chinese (Xue and Palmer 2003) and Korean are already being built, focusing
where possible on parallel data. The general approach ports well to new languages,
with the major effort continuing to go into the creation of frames files for verbs.
There are many directions for future work. Our preliminary linguistic analyses
have merely scratched the surface of what is possible with the current annotation,
and yet it is only a first approximation at capturing the richness of semantic repre-
sentation. Annotation of nominalizations and other noun predicates is currently being
added by New York University, and a Phase II (Babko-Malaya et al) that will include
eventuality variables, nominal references, additional sense tagging, and discourse
connectives is underway.
We have several plans for improving the performance of our automatic semantic-
role labeling. As a first step we are producing a version of PropBank that uses more in-
formative thematic labels based on VerbNet thematic labels (Kipper, Palmer, and
Rambow 2002). We are also working with FrameNet to produce a mapping between
our annotation and theirs which will allow us to merge the two annotated data sets.
Finally, we will explore alternative machine-learning approaches and closer integra-
tion of semantic-role labeling and sense tagging with the parsing process.
102
Acknowledgments
This work was funded by DOD grant
MDA904-00C-2136, NSF grant IIS-9800658,
and the Institute for Research in Cognitive
Science at the University of Pennsylvania
NSF-STC grant SBR-89-20230. Any
opinions, findings, and conclusions or
recommendations expressed in this material
are those of the authors and do not
necessarily reflect the views of the National
Science Foundation. We are indebted to Scott
Cotton, our staff programmer, for his untiring
efforts and his lovely annotation tool, to
Joseph Rosenzweig for the initial automatic
semantic-role labeling of the Penn Treebank,
and to Ben Snyder for programming support
and computing agreement figures. We would
also like to thank Mitch Marcus, Aravind
Joshi, Olga Babko-Malaya, Hoa Trang Dang,
Christiane Fellbaum, and Betsy Klipple for
their extremely useful and insightful
guidance and our many hard-working
Computational Linguistics Volume 31, Number 1
103
annotators, especially Kate Forbes, Ilana
Streit, Ann Delilkin, Brian Hertler, Neville
Ryant, and Jill Flegg, for all of their help.
References
Abeille?, Anne, editor. 2003. Building and Using
Parsed Corpora. Language and Speech
series. Kluwer, Dordrecht.
Alshawi, Hiyan, editor. 1992. The Core
Language Engine. MIT Press, Cambridge,
MA.
Baker, Collin F., Charles J. Fillmore, and John
B. Lowe. 1998. The Berkeley FrameNet
project. In Proceedings of COLING/ACL,
pages 86?90, Montreal.
Bangalore, Srinivas and Aravind K. Joshi.
1999. Supertagging: An approach to almost
parsing. Computational Linguistics,
25(2):237?265.
Brent, Michael R. 1993. From grammar to
lexicon: Unsupervised learning of lexical
syntax. Computational Linguistics, 19(2):
243?262.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Fifth Conference on Applied
Natural Language Processing, pages 356?363,
Washington, DC. ACL.
Carreras, Xavier and LluN?s Ma`rquez.
2004. Introduction to the CoNLL-2004
shared task: Semantic role labeling. In
HLT-NAACL 2004 Workshop: Eighth
Conference on Computational Natural
Language Learning (CoNLL-2004),
pages 89?97, Boston.
Carroll, John, Ted Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation:
A survey and a new proposal. In
Proceedings of the First International
Conference on Language Resources and
Evaluation, pages 447?454, Granada, Spain.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser.
In Proceedings of the First Annual Meeting
of the North American Chapter of the ACL
(NAACL), pages 132?139, Seattle.
Chen, John and Owen Rambow. 2003.
Use of deep linguistic features for the
recognition and labeling of semantic
arguments. In Michael Collins and
Mark Steedman, editors, Proceedings
of the 2003 Conference on Empirical
Methods in Natural Language Processing,
Sapporo, Japan, pages 41?48.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In Proceedings of the International
Conference on Machine Learning (ICML),
Stanford, CA.
Dang, Hoa Trang, Karin Kipper, Martha
Palmer, and Joseph Rosenzweig. 1998.
Investigating regular sense extensions
based on intersective Levin classes. In
COLING/ACL-98, pages 293?299,
Montreal. ACL.
Dienes, Peter and Amit Dubey. 2003.
Antecedent recovery: Experiments with a
trace tagger. In 2003 Conference on Empirical
Methods in Natural Language Processing
(EMNLP), Sapporo, Japan.
Dorr, Bonnie J. and Douglas Jones. 2000.
Acquisition of semantic lexicons: Using
word sense disambiguation to improve
precision. In Evelyn Viegas, editor,
Breadth and Depth of Semantic Lexicons.
Kluwer Academic, Norwell, MA,
pages 79?98.
Dowty, David R. 1991. Thematic proto-roles
and argument selection. Language,
67(3):547?619.
Fillmore, Charles J. 1976. Frame semantics
and the nature of language. In Annals of the
New York Academy of Sciences: Conference on
the Origin and Development of Language and
Speech, volume 280, pages 20?32.
Fillmore, Charles J. and B. T. S. Atkins. 1998.
FrameNet and lexicographic relevance. In
Proceedings of the First International
Conference on Language Resources and
Evaluation, Granada, Spain.
Fillmore, Charles J. and Collin F. Baker. 2001.
Frame semantics for text understanding. In
Proceedings of NAACL WordNet and Other
Lexical Resources Workshop, Pittsburgh, June.
Gildea, Daniel and Julia Hockenmaier. 2003.
Identifying semantic roles using
combinatory categorial grammar. In 2003
Conference on Empirical Methods in Natural
Language Processing (EMNLP), Sapporo,
Japan.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Gildea, Daniel and Martha Palmer. 2002. The
necessity of syntactic parsing for predicate
argument recognition. In Proceedings of the
40th Annual Conference of the Association for
Computational Linguistics (ACL-02),
pages 239?246, Philadelphia.
Hajic?ova?, Eva and Ivona Kuc?erova?. 2002.
Argument/valency structure in PropBank,
LCS Database and Prague Dependency
Treebank: A comparative pilot study. In
Palmer, Gildea, and Kingsbury The Proposition Bank
104
Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC 2002), Las Palmas, Spain,
pages 846?851. ELRA.
He, Shan and Daniel Gildea. 2004. Semantic
roles labeling by maximum entropy model.
Technical Report 847, University of
Rochester.
Hobbs, Jerry R., Douglas Appelt, John Bear,
David Israel, Megumi Kameyama,
Mark E. Stickel, and Mabry Tyson. 1997.
FASTUS: A cascaded finite-state
transducer for extracting information
from natural-language text. In Emmanuel
Roche and Yves Schabes, editors,
Finite-State Language Processing. MIT Press,
Cambridge, MA, pages 383?406.
Hockenmaier, Julia and Mark Steedman.
2002. Generative models for statistical
parsing with combinatory categorial
grammar. In Proceedings of the 40th Annual
Conference of the Association for
Computational Linguistics (ACL-02),
pages 335?342, Philadelphia.
Johnson, Christopher R., Charles J. Fillmore,
Miriam R. L. Petruck, Collin F. Baker,
Michael Ellsworth, Josef Ruppenhofer, and
Esther J. Wood. 2002. FrameNet: Theory
and practice. Version 1.0, available at
http://www.icsi.berkeley.edu/framenet/.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for recovering
empty nodes and their antecedents. In
Proceedings of the 40th Annual Conference of
the Association for Computational Linguistics
(ACL-02), Philadelphia.
Kipper, Karin, Hoa Trang Dang, and Martha
Palmer. 2000. Class-based construction of a
verb lexicon. In Proceedings of the Seventh
National Conference on Artificial Intelligence
(AAAI-2000), Austin, TX, July?August.
Kipper, Karin, Martha Palmer, and Owen
Rambow. 2002. Extending PropBank
with VerbNet semantic predicates.
Paper presented at Workshop on
Applied Interlinguas, AMTA-2002,
Tiburon, California, October.
Korhonen, Anna and Ted Briscoe. 2004.
Extended lexical-semantic classification of
English verbs. In Proceedings of the HLT/
NAACL Workshop on Computational Lexical
Semantics, Boston.
Korhonen, Anna, Yuval Krymolowsky, and
Zvika Marx. 2003. Clustering polysemic
subcategorization frame distributions
semantically. In Proceedings of the 41st
Annual Conference of the Association for
Computational Linguistics (ACL-03),
Sapporo, Japan.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Manning, Christopher D. 1993. Automatic
acquisition of a large subcategorization
dictionary from corpora. In Proceedings
of the 31st Annual Meeting of the Association
for Computational Linguistics, pages 235?242,
Ohio State University, Columbus.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal participation
in role switching alternations. In Proceedings
of the 1st Annual Meeting of the North
American Chapter of the ACL (NAACL),
pages 256?263, Seattle.
Merlo, Paola and Suzanne Stevenson. 2001.
Automatic verb classification based on
statistical distribution of argument
structure. Computational Linguistics,
27(3):373?408.
Miller, Scott, Michael Crystal, Heidi Fox,
Lance Ramshaw, Richard Schwartz,
Rebecca Stone, Ralph Weischedel, and the
Annotation Group. 1998. Algorithms that
learn to extract information?BBN:
Description of the SIFT system as used
for MUC-7. In Proceedings of the Seventh
Message Understanding Conference
(MUC-7), April.
Babko-Malaya, Olga, Martha Palmer,
Nianwen Xue, Aravind Joshi, Seth Kulick,
Proposition Bank II: Delving deeper,
frontiers in corpus annotation, Workshop
in conjunction with HLT/NAACL 2004,
Boston, MA, May 6, 2004.
Palmer, Martha, Olga Babko-Malaya, and
Hoa Trang Dang. 2004. Different sense
granularities for different applications. In
Second Workshop on Scalable Natural
Language Understanding Systems at HLT/
NAACL-04, Boston.
Palmer, Martha, Joseph Rosenzweig, and
Scott Cotton. 2001. Predicate argument
analysis of the Penn Treebank. In
Proceedings of HLT 2001, First International
Conference on Human Language Technology
Research, San Diego, CA, March.
Pradhan, S., K. Hacioglu, W. Ward,
J. Martin, and Daniel Jurafsky. 2003.
Semantic role parsing: Adding
semantic structure to unstructured
text. In Proceedings of the International
Conference on Data Mining (ICDM-2003),
Melbourne, FL.
Computational Linguistics Volume 31, Number 1
105
Rambow, Owen, Bonnie J. Dorr, Karin
Kipper, Ivona Kuc?erova?, and Martha
Palmer. 2003. Automatically deriving
tectogrammatical labels from other
resources: A comparison of semantic
labels across frameworks. Prague
Bulletin of Mathematical Linguistics,
vol. 79?80, pages 23?35.
Ratnaparkhi, Adwait. 1997. A linear
observed time statistical parser based
on maximum entropy models. In
Proceedings of the Second Conference on
Empirical Methods in Natural Language
Processing, pages 1?10, Providence, ACL.
Ray, Soumya and Mark Craven. 2001.
Representing sentence structure in
hidden Markov model for information
extraction. In Seventeenth International Joint
Conference on Artificial Intelligence
(IJCAI-01), Seattle.
Schulte im Walde, Sabine. 2000.
Clustering verbs semantically according
to their alternation behaviour. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING-00), pages 747?753,
Saarbru?cken, Germany.
Schulte im Walde, Sabine and Chris Brew.
2002. Inducing German semantic verb
classes from purely syntactic
subcategorisation information. In
Proceedings of the 40th Annual Conference
of the Association for Computational
Linguistics (ACL-02), pages 223?230,
Philadelphia.
Siegel, Sidney and N. John Castellan Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. 2nd ed. McGraw-Hill,
New York.
Steedman, Mark. 2000. The Syntactic Process.
MIT Press, Cambridge, MA.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate-argument structures
for information extraction. In Proceedings
of the 41st Annual Conference of the
Association for Computational Linguistics
(ACL-03), Sapporo, Japan, pages 8?15.
Tjong Kim Sang, Erik F. and Sabine Buchholz.
2000. Introduction to the CoNLL-2000
shared task: Chunking. In Proceedings
of CoNLL-2000 and LLL-2000, Lisbon,
Portugal.
Xue, Nianwen, and Martha Palmer. 2004.
Calibrating Features for Semantic Role
Labeling, Empirical Methods in Natural
Language Processing Conference, in
conjunction with the 42nd Meeting of the
Association for Computational Linguistics
(ACL-04), Barcelona, Spain, July 21?26.
Xue, Nianwen, and Martha Palmer. 2004.
Annotating the Propositions in the Penn
Chinese Treebank Second SIGHAN
Workshop on Chinese Language
Processing, held in conjunction with
ACL-03, Sapporo, Japan, pages 47?54, July.
Palmer, Gildea, and Kingsbury The Proposition Bank

Proceedings of NAACL HLT 2007, pages 548?555,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Can Semantic Roles Generalize Across Genres?
Szu-ting Yi
Dept of Computer Science
University of Pennsylvania
Philadelphia, PA 19104
Edward Loper
Dept of Computer Science
University of Pennsylvania
Philadelphia, PA 19104
Martha Palmer
Dept of Computer Science
University of Colorado at Boulder
Boulder, CO 80309
Abstract
PropBank has been widely used as train-
ing data for Semantic Role Labeling.
However, because this training data is
taken from the WSJ, the resulting machine
learning models tend to overfit on idiosyn-
crasies of that text?s style, and do not port
well to other genres. In addition, since
PropBank was designed on a verb-by-verb
basis, the argument labels Arg2 - Arg5 get
used for very diverse argument roles with
inconsistent training instances. For exam-
ple, the verb ?make? uses Arg2 for the
?Material? argument; but the verb ?multi-
ply? uses Arg2 for the ?Extent? argument.
As a result, it can be difficult for auto-
matic classifiers to learn to distinguish ar-
guments Arg2-Arg5. We have created a
mapping between PropBank and VerbNet
that provides a VerbNet thematic role la-
bel for each verb-specific PropBank label.
Since VerbNet uses argument labels that
are more consistent across verbs, we are
able to demonstrate that these new labels
are easier to learn.
1 Introduction
Correctly identifying semantic entities and success-
fully disambiguating the relations between them and
their predicates is an important and necessary step
for successful natural language processing applica-
tions, such as text summarization, question answer-
ing, and machine translation. For example, in or-
der to determine that question (1a) is answered by
sentence (1b), but not by sentence (1c), we must de-
termine the relationships between the relevant verbs
(eat and feed) and their arguments.
(1) a. What do lobsters like to eat?
b. Recent studies have shown that lobsters pri-
marily feed on live fish, dig for clams, sea
urchins, and feed on algae and eel-grass.
c. In the early 20th century, Mainers would
only eat lobsters because the fish they
caught was too valuable to eat themselves.
An important part of this task is Semantic Role
Labeling (SRL), where the goal is to locate the con-
stituents which are arguments of a given verb, and to
assign them appropriate semantic roles that describe
how they relate to the verb. Many researchers have
investigated applying machine learning to corpus
specifically annotated with this task in mind, Prop-
Bank, since 2000 (Chen and Rambow, 2003; Gildea
and Hockenmaier, 2003; Hacioglu et al, 2003; Mos-
chitti, 2004; Yi and Palmer, 2004; Pradhan et al,
2005b; Punyakanok et al, 2005; Toutanova et al,
2005). For two years, the CoNLL workshop has
made this problem the shared task (Carreras and
Ma?rquez, 2005). However, there is still little con-
sensus in the linguistic and NLP communities about
what set of role labels are most appropriate. The
Proposition Bank (PropBank) corpus (Palmer et al,
2005) avoids this issue by using theory-agnostic la-
bels (Arg0, Arg1, . . . , Arg5), and by defining those
labels to have verb-specific meanings. Under this
scheme, PropBank can avoid making any claims
548
about how any one verb?s arguments relate to other
verbs? arguments, or about general distinctions be-
tween verb arguments and adjuncts.
However, there are several limitations to this ap-
proach. The first is that it can be difficult to make
inferences and generalizations based on role labels
that are only meaningful with respect to a single
verb. Since each role label is verb-specific, we can
not confidently determine when two different verbs?
arguments have the same role; and since no encoded
meaning is associated with each tag, we can not
make generalizations across verb classes. In con-
trast, the use of a shared set of role labels, such as
thematic roles, would facilitate both inferencing and
generalization.
The second issue with PropBank?s verb-specific
approach is that it can make training automatic se-
mantic role labeling (SRL) systems more difficult.
A vast amount of data would be needed to train the
verb-specific models that are theoretically mandated
by PropBank?s design. Instead, researchers typically
build a single model for the numbered arguments
(Arg0, Arg1, . . . , Arg5). This approach works sur-
prisingly well, mainly because an explicit effort was
made to use arguments Arg0 and Arg1 consistently
across different verbs; and because those two argu-
ment labels account for 85% of all arguments. How-
ever, this approach causes the system to conflate
different argument types, especially with the highly
overloaded arguments Arg2-Arg5. As a result, these
argument labels are quite difficult to learn.
A final difficulty with PropBank?s current ap-
proach is that it limits SRL system robustness in
the face of verb senses, verbs or verb constructions
that were not included in the training data, and the
training data is all Wall Street Journal corpora. If
a PropBank-trained SRL system encounters a novel
verb or verb usage, then there is no way for it to
know which role labels are used for which argument
types, since role labels are defined so specifically.
This is especially problematic for Arg2-5. Similarly,
PropBank-trained SRL systems can have difficulty
generalizing when a known verb is encountered in
a novel construction. These problems can happen
quite frequently if the training data comes from a
different genre than the test data. This issue is re-
flected in the relatively poor performance of most
state-of-the-art SRL systems when tested on a novel
genre, the Brown corpus, during CoNLL 2005. For
example, the SRL system described in (Pradhan et
al., 2005b; Pradhan et al, 2005a) achieves an F-
score of 81% when tested on the same genre as it
is trained on (WSJ); but that score drops to 68.5%
when the same system is tested on a different genre
(the Brown corpus). DARPA-GALE is funding an
ongoing effort to PropBank additional genres, but
better techniques for generalizing the semantic role
labeling task are still needed.
In this paper, we demonstrate an increase in the
generality of our semantic role labeling based on a
mapping that has been developed between PropBank
and another lexical resource, VerbNet. By taking ad-
vantage of VerbNet?s more consistent set of labels,
we can generate more useful role label annotations
with a resulting improvement in SRL performance
on novel genres.
2 Background
2.1 PropBank
PropBank (Palmer et al, 2005) is an annotation of
one million words of the Wall Street Journal por-
tion of the Penn Treebank II (Marcus et al, 1994)
with predicate-argument structures for verbs, using
semantic role labels for each verb argument. In or-
der to remain theory neutral, and to increase anno-
tation speed, role labels were defined on a per-verb-
sense basis. Although the same tags were used for
all verbs, (namely Arg0, Arg1, ..., Arg5), these tags
are meant to have a verb-specific meaning.
Thus, the use of a given argument label should
be consistent across different uses of that verb, in-
cluding syntactic alternations. For example, the
Arg1 (underlined) in ?John broke the window? is the
same window that is annotated as the Arg1 in ?The
window broke?, even though it is the syntactic sub-
ject in one sentence and the syntactic object in the
other. However, there is no guarantee that an argu-
ment label will be used consistently across different
verbs. For example, the Arg2 label is used to des-
ignate the destination of the verb ?bring;? but the
extent of the verb ?rise.? Generally, the arguments
are simply listed in the order of their prominence
for each verb. However, an explicit effort was made
when PropBank was created to use Arg0 for argu-
ments that fulfill Dowty?s criteria for ?prototypical
549
agent,? and Arg1 for arguments that fulfill the cri-
teria for ?prototypical patient.? (Dowty, 1991) As
a result, these two argument labels are significantly
more consistent across verbs than the other three.
But nevertheless, there are still some inter-verb in-
consistencies for even Arg0 and Arg1.
2.2 VerbNet
VerbNet (Schuler, 2005) consists of hierarchically
arranged verb classes, inspired by and extended
from classes of Levin 1993 (Levin, 1993). Each
class and subclass is characterized extensionally by
its set of verbs, and intensionally by a list of the
arguments of those verbs and syntactic and seman-
tic information about the verbs. The argument list
consists of thematic roles (23 in total) and pos-
sible selectional restrictions on the arguments ex-
pressed using binary predicates. The syntactic infor-
mation maps the list of thematic arguments to deep-
syntactic arguments (i.e., normalized for voice alter-
nations, and transformations). The semantic predi-
cates describe the participants during various stages
of the event described by the syntactic frame.
The same thematic role can occur in different
classes, where it will appear in different predicates,
providing a class-specific interpretation of the role.
VerbNet has been extended from the original Levin
classes, and now covers 4526 senses for 3769 verbs.
A primary emphasis for VerbNet is the grouping of
verbs into classes that have a coherent syntactic and
semantic characterization, that will eventually facil-
itate the acquisition of new class members based on
observable syntactic and semantic behavior. The hi-
erarchical structure and small number of thematic
roles is aimed at supporting generalizations.
2.3 Mapping PropBank to VerbNet
Because PropBank includes a large corpus of man-
ually annotated predicate-argument data, it can be
used to train supervised machine learning algo-
rithms, which can in turn provide PropBank-style
annotations for novel or unseen text. However, as
we discussed in the introduction, PropBank?s verb-
specific role labels are somewhat problematic. Fur-
thermore, PropBank lacks much of the information
that is contained in VerbNet, including information
about selectional restrictions, verb semantics, and
inter-verb relationships.
We have therefore created a mapping between
VerbNet and PropBank (Loper et al, 2007), which
will allow us to use the machine learning tech-
niques that have been developed for PropBank anno-
tations to generate more semantically abstract Verb-
Net representations. Additionally, the mapping can
be used to translate PropBank-style numbered ar-
guments (Arg0. . .Arg5) to VerbNet thematic roles
(Agent, Patient, Theme, etc.), which should allow us
to overcome the verb-specific nature of PropBank.
The mapping between VerbNet and PropBank
consists of two parts: a lexical mapping and an in-
stance classifier. The lexical mapping is responsible
for specifying the potential mappings between Prop-
Bank and VerbNet for a given word; but it does not
specify which of those mappings should be used for
any given occurrence of the word. That is the job
of the instance classifier, which looks at the word
in context, and decides which of the mappings is
most appropriate. In essence, the instance classi-
fier is performing word sense disambiguation, de-
ciding which lexeme from each database is correct
for a given occurrence of a word. In order to train
the instance classifier, we semi-automatically anno-
tated each verb in the PropBank corpus with Verb-
Net class information.1 This mapped corpus was
then used to build the instance classifier. More de-
tails about the mapping, and how it was created, can
be found in (Loper et al, 2007).
3 Analysis of the Mapping
In order to confirm our belief that PropBank roles
Arg0 and Arg1 are relatively coherent, while roles
Arg2-5 are much more overloaded, we performed
a preliminary analysis of how argument roles were
mapped. Figure 1 shows how often each PropBank
role was mapped to each VerbNet thematic role, cal-
culated as a fraction of instances in the mapped cor-
pus. From this figure, we can see that Arg0 maps to
agent-like roles, such as ?agent? and ?experiencer,?
over 94% of the time; and Arg1 maps to patient-
like roles, including ?theme,? ?topic,? and ?patient,?
over 82% of the time. In contrast, arguments Arg2-5
get mapped to a much broader variety of roles. It is
also worth noting that the sample size for arguments
1Excepting verbs whose senses are not present in VerbNet
(24.5% of instances).
550
Arg3-5 is quite small in comparison with arguments
Arg0-2, suggesting that any automatically built clas-
sifier for arguments Arg3-5 will suffer severe sparse
data problems for those arguments.
4 Training a SRL system with VerbNet
Roles to Achieve Robustness
An important issue for state-of-the-art automatic
SRL systems is robustness: although they receive
high performance scores when tested on the Wall
Street Journal (WSJ) corpus, that performance drops
significantly when the same systems are tested on a
corpus from another genre. This performance drop
reflects the fact that the WSJ corpus is highly spe-
cialized, and tends to use genre-specific word senses
for many verbs. The 2005 CoNLL shared task has
addressed this issue of robustness by evaluating par-
ticipating systems on a test set extracted from the
Brown corpus, which is very different from the WSJ
corpus that was used for training. The results sug-
gest that there is much work to be done in order to
improve system robustness.
One of the reasons that current SRL systems have
difficulty deciding which role label to assign to a
given argument is that role labels are defined on a
per-verb basis. This is less problematic for Arg0
and Arg1, where a conscious effort was made to be
consistent across verbs; but is a significant problem
for Args[2-5], which tend to have very verb-specific
meanings. This problem is exacerbated even fur-
ther on novel genres, where SRL systems are more
likely to encounter unseen verbs and uses of argu-
ments that were not encountered in the training data.
4.1 Addressing Current SRL Problems via
Lexical Mappings
By exploiting the mapping between PropBank and
VerbNet, we can transform the data to make it more
consistent, and to expand the size and variety of the
training data. In particular, we can use the map-
ping to transform the verb-specific PropBank role
labels into the more general thematic role labels that
are used by VerbNet. Unlike the PropBank labels,
the VerbNet labels are defined consistently across
verbs; and therefore it should be easier for statisti-
cal SRL systems to model them. Furthermore, since
the VerbNet role labels are significantly less verb-
Arg0 (45,579)
Agent 85.4%
Experiencer 7.2%
Theme 2.1%
Cause 1.9%
Actor1 1.8%
Theme1 0.8%
Patient1 0.2%
Location 0.2%
Theme2 0.2%
Product 0.1%
Patient 0.0%
Attribute 0.0%
Arg1 (59,884)
Theme 47.0%
Topic 23.0%
Patient 10.8%
Product 2.9%
Predicate 2.5%
Patient1 2.4%
Stimulus 2.0%
Experiencer 1.9%
Cause 1.8%
Destination 0.9%
Theme2 0.7%
Location 0.7%
Source 0.7%
Theme1 0.6%
Actor2 0.6%
Recipient 0.5%
Agent 0.4%
Attribute 0.2%
Asset 0.2%
Patient2 0.2%
Material 0.2%
Beneficiary 0.0%
Arg2 (11,077)
Recipient 22.3%
Extent 14.7%
Predicate 13.4%
Destination 8.6%
Attribute 7.6%
Location 6.5%
Theme 5.5%
Patient2 5.3%
Source 5.2%
Topic 3.1%
Theme2 2.5%
Product 1.5%
Cause 1.2%
Material 0.8%
Instrument 0.6%
Beneficiary 0.5%
Experiencer 0.3%
Actor2 0.2%
Asset 0.0%
Theme1 0.0%
Arg3 (609)
Asset 38.6%
Source 25.1%
Beneficiary 10.7%
Cause 9.7%
Predicate 9.0%
Location 2.0%
Material 1.8%
Theme1 1.6%
Theme 0.8%
Destination 0.3%
Instrument 0.3%
Arg4 (18)
Beneficiary 61.1%
Product 33.3%
Location 5.6%
Arg5 (17)
Location 100.0%
Figure 1: The frequency with which each PropBank
numbered argument is mapped to each VerbNet the-
matic role in the mapped corpus. The numbers
next to each PropBank argument reflects the num-
ber of occurrences of that numbered argument in the
mapped corpus.
551
dependent than the PropBank roles, the SRL?s mod-
els should generalize better to novel verbs, and to
novel uses of known verbs.
5 SRL Experiments on Linked Lexical
Resources
In order to verify the feasibility of performing se-
mantic role labeling with VerbNet thematic roles, we
re-trained our existing SRL system, which originally
used PropBank role labels, with a new label set that
makes use of VerbNet thematic role information.
5.1 The SRL System
Our SRL system is a Maximum Entropy based
pipelined system which consists of four compo-
nents: Pre-processing, Argument Identification, Ar-
gument Classification, and Post Processing. The
Pre-processing component pipes a sentence through
a syntactic parser and filters out constituents which
are unlikely to be semantic arguments based on a
constituents location in the parse tree. The Argu-
ment Identification component is a binary MaxEnt
classifier, which tags candidate constituents as ar-
guments or non-arguments. The Argument Classifi-
cation component is a multi-class MaxEnt classifier
which assigns a semantic role to each constituent.
The Post Processing component further selects the
final arguments based on global constraints. Our ex-
periments mainly focused on changes to the Argu-
ment Classification stage of the SRL pipeline, and
in particular, on changes to the set of output tags.
For more information on our SRL system, see (Yi
and Palmer, 2004; Yi and Palmer, 2005).
The evaluation of SRL systems is typically ex-
pressed by precision, recall and the F1-measure.
Precision is the number of correct arguments pre-
dicted by a system divided by the total number of
arguments proposed. Recall is the number of cor-
rect arguments divided by the number of the total
number of arguments in the Gold Standard Data. F1
computes the harmonic mean of precision and recall.
5.2 SRL Experiments on Mapped VerbNet
Thematic Roles
Since PropBank arguments Arg0 and Arg1 are al-
ready quite coherent, we left them as-is in the new
label set. But since arguments Arg2-Arg5 are highly
Group 1 Group 2 Group 3 Group 4 Group 5
Recipient Extent Predicate Patient2 Instrument
Destination Asset Attribute Product Cause
Location Theme Experiencer
Source Theme1 Actor2
Material Theme2
Beneficiary Topic
Figure 2: Thematic Role Groupings for the exper-
iments on linked lexical resources; and for Arg2 in
the experiments on arguments with different verb in-
dependency.
overloaded, we replaced them by mapping them
to their corresponding VerbNet thematic role. We
found that mapping directly to individual role labels
created a significant sparse data problem, since the
number of output tags was increased from 6 to 23.
We therefore grouped the VerbNet thematic roles
into five coherent groups of similar thematic roles,
shown in Figure 2.2 Our new tag set therefore in-
cluded the following tags: Arg0 (agent); Arg1 (pa-
tient); Group1 (goal); Group2 (extent); Group3
(predicate/attrib); Group4 (product); and Group5
(instrument/cause).
Training our SRL system using these thematic
role groups, we obtained performance similar to the
original SRL system. However, it is important to
note that these performance figures are not directly
comparable, since the two systems are performing
different tasks: The Original system labels Arg0-
5,ArgA and ArgM and the Mapped system labels
Arg0, Arg1, ArgA, ArgM and Group1-5. In partic-
ular, the role labels generated by the original system
are verb-specific, while the role labels generated by
the new system are less verb-dependent.
5.2.1 Results
For our testing and training, we used the portion
of Penn Treebank II that is covered by the mapping,
and where at least one of Arg2-5 is used. Training
was performed using sections 2-21 of the Treebank
(10,783 instances of argument); and testing was per-
formed on section 23 (859 instances). Table 1 dis-
plays the performance score for the SRL system us-
ing the augmented tag set (?Mapped?). The per-
formance score of the original system (?Original?)
is also listed, for reference; however, as was dis-
2Karin Kipper assisted in creating the groupings.
552
System Precision Recall F1
Original 90.65 85.43 87.97
Mapped 88.85 84.56 86.65
Table 1: Overall SRL System performance using the
PropBank tag set (?Original?) and the augmented
tag set (?Mapped?)
System Precision Recall F1
Original 97.60 83.67 90.10
Mapped 91.70 82.86 87.06
Table 2: SRL System performance evaluated on only
Arg2-5 (Original) or Group1-5 (Mapped).
cussed above, these results are not directly compara-
ble because the two systems are performing different
tasks.
The results indicate that the performance drops
when we train on the new argument labels, espe-
cially on precision when we evaluate the systems
on only Arg2-5/Group1-5 (see Table 2). However,
it is premature to conclude that there is no benefit
from the VerbNet thematic role labels. Firstly, we
have very few mapped Arg3-5 instances (less than
1,000 instances); secondly, we lack test data gen-
erated from a genre other than WSJ to allow us to
evaluate the robustness (generality) of SRL trained
on the new argument labels.
We therefore redesigned our experiments by lim-
iting the scope to mapped instances of Arg1 and
Arg2. By doing this, we should be able to accom-
plish the following: 1) we can map new argument la-
bels back to the original PropBank labels; therefore
we can directly compare results; 2) With the ability
of testing our systems on other test data, we can eval-
uate the influence of the mapping on SRL robust-
ness; 3) We can validate our original hypothesis that
the behavior of Arg1 is primarily verb-independent
while Arg2 is more verb-specific.
5.3 SRL Experiments on Arguments with
Different Verb Independency
We conducted two further sets of experiments: one
to test the effect of the mapping on learning Arg2;
and one to test the effect on learning Arg1. Since
Arg2 is used in very verb-dependent ways, we ex-
pect that mapping it to VerbNet role labels will in-
Group 1 Group 2 Group 3 Group 4 Group 5
Theme Source Patient Agent Topic
Theme1 Location Product Actor2
Theme2 Destination Patient1 Experiencer Group 6
Predicate Recipient Patient2 Cause Asset
Stimulus Beneficiary
Attribute Material
Figure 3: Thematic Role Groupings for Arg1 in the
experiments on arguments with different verb inde-
pendency.
crease our performance. However, since a conscious
effort was made to keep the meaning of Arg1 consis-
tent across verbs, we expect that mapping it to Verb-
Net labels will provide less of an improvement.
Each experiment compares two SRL systems: one
trained using the original PropBank role labels; the
other trained with the argument role under consid-
eration (Arg1 or Arg2) subdivided based on which
VerbNet role label it maps to. In order to prevent
the training data from these subdivided labels from
becoming too sparse (which would impair system
performance) we grouped similar thematic roles to-
gether. For Arg2, we used the same groupings as the
previous experiment, shown in Figure 2. The argu-
ment role groupings we used for Arg1 are shown in
Figure 3.
The training data for both experiments is the por-
tion of Penn Treebank II (sections 02-21) that is cov-
ered by the mapping. We evaluated each experi-
mental system using two test sets: section 23 of the
Penn Treebank II, which represents the same genre
as the training data; and the PropBank-ed portion of
the Brown corpus, which represents a very different
genre.
5.3.1 Results and Discussion
Table 3 describes the results of SRL overall per-
formance tested on the WSJ corpus Section 23; Ta-
ble 4 demonstrates the SRL overall system perfor-
mance tested on the Brown corpus. Systems Arg1-
Original and Arg2-Original are trained using the
original PropBank labels, and show the baseline
performance of our SRL system. Systems Arg1-
Mapped and Arg2-Mapped are trained using Prop-
Bank labels augmented with VerbNet thematic role
groups. In order to allow comparison between the
system using the original PropBank labels and the
systems that augmented those labels with VerbNet
553
System Precision Recall F1
Arg1-Original 89.24 77.32 82.85
Arg1-Mapped 90.00 76.35 82.61
Arg2-Original 73.04 57.44 64.31
Arg2-Mapped 84.11 60.55 70.41
Table 3: SRL System Performance on Arg1 Map-
ping and Arg2 Mapping, tested using the WSJ cor-
pus (section 23). This represents performance on the
same genre as the training corpus.
System Precision Recall F1
Arg1-Original 86.01 71.46 78.07
Arg1-Mapped 88.24 71.15 78.78
Arg2-Original 66.74 52.22 58.59
Arg2-Mapped 81.45 58.45 68.06
Table 4: SRL System Performance on Arg1 Map-
ping and Arg2 Mapping, tested using the PropBank-
ed Brown corpus. This represents performance on a
different genre from the training corpus.
thematic role groups, system performance was eval-
uated based solely on the PropBank role label that
was assigned.
We had hypothesized that with the use of thematic
roles, we would be able to create a more consis-
tent training data set which would result in an im-
provement in system performance. In addition, the
thematic roles would behave more consistently than
the overloaded Args[2-5] across verbs, which should
enhance robustness. However, since in practice we
are also increasing the number of argument labels
an SRL system needs to tag, the system might suf-
fer from data sparseness. Our hope is that the en-
hancement gained from the mapping will outweigh
the loss due to data sparseness.
From Table 3 and Table 4 we see the F1 scores of
Arg1-Original and Arg1-Mapped are statistically in-
different both on the WSJ corpus and the Brown cor-
pus. These results confirm the observation that Arg1
in the PropBank behaves fairly verb-independently
so that the VerbNet mapping does not provide much
benefit. The increase of precision due to a more co-
herent training data set is compensated for by the
loss of recall due to data sparseness.
The results of the Arg2 experiments tell a differ-
Confusion ARG2-Original
Matrix ARG1 ARG2 ARGM
ARG2- ARG0 53 50 -
Mapped ARG1 - 716 -
ARG2 1 - 2
ARG3 - 1 -
ARGM 1 482 -
233 ARG2-Mapped arguments are not labeled by ARG2-
Original
Table 5: Confusion matrix on the 1,539 instances
which ARG2-Mapped tags correctly and ARG2-
Original fails to predict.
ent story. Both precision and recall are improved
significantly, which demonstrates that the Arg2 label
in the PropBank is quite overloaded. The Arg2 map-
ping improves the overall results (F1) on the WSJ
by 6% and on the Brown corpus by almost 10%. As
a more diverse corpus, the Brown corpus provides
many more opportunities for generalizing to new us-
ages. Our new SRL system handles these cases more
robustly, demonstrating the consistency and useful-
ness of the thematic role categories.
5.4 Improved Argument Distinction via
Mapping
The ARG2-Mapped system generalizes well both
on the WSJ corpus and the Brown corpus. In or-
der to explore the improved robustness brought by
the mapping, we extracted and observed the 1,539
instances to which the system ARG2-Mapped as-
signed the correct semantic role label, but which the
system ARG2-Original failed to predict. From the
confusion matrix depicted in Table 5, we discover
the following:
The mapping makes ARG2 more clearly defined,
and as a result there is a better distinction be-
tween ARG2 and other argument labels: Among
the 1,539 instances that ARG2-Original didn?t tag
correctly, 233 instances are not assigned an argu-
ment label, and 1,252 instances ARG2-Original con-
fuse the ARG2 label with another argument label:
the system ARG2-Original assigned the ARG2 la-
bel to 50 ARG0?s, 716 ARG1?s, 1 ARG3 and 482
ARGM?s, and assigned other argument labels to 3
ARG2?s.
554
6 Conclusions
In conclusion, we have described a mapping from
the annotated PropBank corpus to VerbNet verb
classes with associated thematic role labels. We hy-
pothesized that these labels would be more verb-
independent and less overloaded than the PropBank
Args2-5, and would therefore provide more consis-
tent training instances which would generalize better
to new genres. Our preliminary experiments confirm
this hypothesis, with a 6% performance improve-
ment on the WSJ and a 10% performance improve-
ment on the Brown corpus for Arg2.
In future work, we will map the PropBank-ed
Brown corpus to VerbNet as well, which will allow
much more thorough testing of our hypothesis. We
will also examine back-off to verb class membership
as a technique for improving performance on out of
vocabulary verbs. Finally, we plan to explore the ef-
fect of different thematic role groupings on system
performance.
References
Xavier Carreras and Llu??s Ma?rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
In Proceedings of CoNLL.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003,
Sapporo, Japan.
D. R. Dowty. 1991. Thematic proto-roles and argument
selection. Language, 67:574?619.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using Combinatory Categorial Gram-
mar. In 2003 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 57?64,
Sapporo, Japan.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Shallow semantic
parsing using support vector machines. Technical re-
port, The Center for Spoken Language Research at the
University of Colorado (CSLR).
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press.
Edward Loper, Szu-ting Yi, and Martha Palmer. 2007.
Empirical evidence for useful semantic role categories.
In Proceedings of the International Workshop on Com-
putational Linguistics.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn treebank: Annotating predicate argu-
ment structure.
Alessandro Moschitti. 2004. A study on convolution
kernel for shallow semantic parsing. In Proceedings
of the 42-th Conference on Association for Computa-
tional Linguistic (ACL-2004), Barcelona, Spain.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated with
semantic roles. Computational Linguistics, 31(1):71?
106.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, H. Mar-
tin, James, and Daniel Jurafsky. 2005a. Semantic role
chunking combining complementary syntactic views.
In Proceedings of CoNLL-2005.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. In Proceedings
of the Association for Computational Linguistics 43rd
annual meeting (ACL-2005), Ann Arbor, MI.
V. Punyakanok, D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role labeling.
In Proceedings of the 19th International Joint Confer-
ence on Artificial Intelligence (IJCAI-05).
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Kristina Toutanova, Aria Haghighi, and Christopher D.
2005. Joint learning improves semantic role labeling.
In Proceedings of the Association for Computational
Linguistics 43rd annual meeting (ACL-2005), Ann Ar-
bor, MI.
Szu-ting Yi and Martha Palmer. 2004. Pushing the
boundaries of semantic role labeling with svm. In Pro-
ceedings of the International Conference on Natural
Language Processing.
Szu-ting Yi and Martha Palmer. 2005. The integration of
syntactic parsing and semantic role labeling. In Pro-
ceedings of CoNLL-2005.
555
The Necessity of Parsing for Predicate Argument Recognition
Daniel Gildea and Martha Palmer
University of Pennsylvania
dgildea,mpalmer@cis.upenn.edu
Abstract
Broad-coverage corpora annotated with
semantic role, or argument structure, in-
formation are becoming available for the
rst time. Statistical systems have been
trained to automatically label seman-
tic roles from the output of statistical
parsers on unannotated text. In this pa-
per, we quantify the eect of parser accu-
racy on these systems' performance, and
examine the question of whether a at-
ter \chunked" representation of the in-
put can be as eective for the purposes
of semantic role identication.
1 Introduction
Over the past decade, most work in the eld of
information extraction has shifted from complex
rule-based, systems designed to handle a wide
variety of semantic phenomena including quan-
tication, anaphora, aspect and modality (e.g.
Alshawi (1992)), to simpler nite-state or sta-
tistical systems such as Hobbs et al (1997) and
Miller et al (1998). Much of the evaluation of
these systems has been conducted on extracting
relations for specic semantic domains such as
corporate acquisitions or terrorist events in the
framework of the DARPA Message Understand-
ing Conferences.
Recently, attention has turned to creating cor-
pora annotated for argument structure for a
broader range of predicates. The Propbank
project at the University of Pennsylvania (Kings-
bury and Palmer, 2002) and the FrameNet project
at the International Computer Science Institute
(Baker et al, 1998) share the goal of document-
ing the syntactic realization of arguments of the
predicates of the general English lexicon by an-
notating a corpus with semantic roles. Even for
a single predicate, semantic arguments often have
multiple syntactic realizations, as shown by the
following paraphrases:
(1) John will meet with Mary.
John will meet Mary.
John and Mary will meet.
(2) The door opened.
Mary opened the door.
Correctly identifying the semantic roles of the
sentence constituents is a crucial part of interpret-
ing text, and in addition to forming an important
part of the information extraction problem, can
serve as an intermediate step in machine trans-
lation or automatic summarization. In this pa-
per, we examine how the information provided by
modern statistical parsers such as Collins (1997)
and Charniak (1997) contributes to solving this
problem. We measure the eect of parser accu-
racy on semantic role prediction from parse trees,
and determine whether a complete tree is indeed
necessary for accurate role prediction.
Gildea and Jurafsky (2002) describe a statisti-
cal system trained on the data from the FrameNet
project to automatically assign semantic roles.
The system rst passed sentences through an au-
tomatic parser, extracted syntactic features from
the parses, and estimated probabilities for seman-
tic roles from the syntactic and lexical features.
Both training and test sentences were automat-
ically parsed, as no hand-annotated parse trees
were available for the corpus. While the errors
introduced by the parser no doubt negatively af-
fected the results obtained, there was no direct
way of quantifying this eect. Of the systems
evaluated for the Message Understanding Confer-
ence task, Miller et al (1998) made use of an inte-
grated syntactic and semantic model producing a
full parse tree, and achieved results comparable to
other systems that did not make use of a complete
parse. As in the FrameNet case, the parser was
not trained on the corpus for which semantic an-
notations were available, and the eect of better,
or even perfect, parses could not be measured.
One of the dierences between the two semantic
annotation projects is that the sentences chosen
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 239-246.
                         Proceedings of the 40th Annual Meeting of the Association for
for annotation for Propbank are from the same
Wall Street Journal corpus chosen for annotation
for the original Penn Treebank project, and thus
hand-checked syntactic parse trees are available
for the entire dataset. In this paper, we com-
pare the performance of a system based on gold-
standard parses with one using automatically gen-
erated parser output. We also examine whether it
is possible that the additional information con-
tained in a full parse tree is negated by the errors
present in automatic parser output, by testing a
role-labeling system based on a at or \chunked"
representation of the input.
2 The Data
The results in this paper are primarily derived
from the Propbank corpus, and will be compared
to earlier results from the FrameNet corpus. Be-
fore proceeding to the experiments, this section
will briey describe the similarities and dierences
between the two sets of data.
While the goals of the two projects are similar in
many respects, their methodologies are quite dif-
ferent. FrameNet is focused on semantic frames,
which are dened as schematic representation of
situations involving various participants, props,
and other conceptual roles (Fillmore, 1976). The
project methodology has proceeded on a frame-
by-frame basis, that is by rst choosing a semantic
frame, dening the frame and its participants or
frame elements, and listing the various lexical
predicates which invoke the frame, and then nd-
ing example sentences of each predicate in the cor-
pus (the British National Corpus was used) and
annotating each frame element. The example sen-
tences were chosen primarily for coverage of all
the syntactic realizations of the frame elements,
and simple examples of these realizations were
preferred over those involving complex syntactic
structure not immediate relevant to the lexical
predicate itself. From the perspective of an auto-
matic classication system, the overrepresentation
of rare syntactic realizations may cause the system
to perform more poorly than it might on more sta-
tistically representative data. On the other hand,
the exclusion of complex examples may make the
task articially easy. Only sentences where the
lexical predicate was used \in frame" were anno-
tated. A word with multiple distinct senses would
generally be analyzed as belonging to dierent
frames in each sense, but may only be found in the
FrameNet corpus in the sense for which a frame
has been dened. It is interesting to note that the
semantic frames are a helpful way of generalizing
between predicates; words in the same frame have
been found frequently to share the same syntactic
argument structure. A more complete description
of the FrameNet project can be found in (Baker
et al, 1998; Johnson et al, 2001), and the rami-
cations for automatic classication are discussed
more thoroughly in (Gildea and Jurafsky, 2002).
The philosophy of the Propbank project can be
likened to FrameNet without frames. While the
semantic roles of FrameNet are dened at the level
of the frame, in Propbank, roles are dened on a
per-predicate basis. The core arguments of each
predicate are simply numbered, while remaining
arguments are given labels such as \temporal" or
\locative". While the two types of label names are
reminiscent of the traditional argument/adjunct
distinction, this is primarily as a convenience in
dening roles, and no claims are intended as to
optionality or other traditional argument/adjunct
tests. To date, Propbank has addressed only
verbs, where FrameNet includes nouns and ad-
jectives. Propbank's annotation process has pro-
ceeded from the most to least common verbs, and
all examples of each verb from the corpus are an-
notated. Thus, the data for each predicate are
statistically representative of the corpus, as are
the frequencies of the predicates themselves. An-
notation takes place with reference to the Penn
Treebank trees | not only are annotators shown
the trees when analyzing a sentence, they are con-
strained to assign the semantic labels to portions
of the sentence corresponding to nodes in the tree.
Propbank annotators tag all examples of a given
verb, regardless of word sense. The tagging guide-
lines for a verb may contain many \rolesets", cor-
responding to word sense at a relatively coarse-
grained level. The need for multiple rolesets is
determined by the roles themselves, that is, uses
of the verb with dierent arguments are given sep-
arate rolesets. However, the preliminary version
of the data used in the experiments below are
not tagged for word sense, or for the roleset used.
Sense tagging is planned for a second pass through
the data. In many cases the roleset can be deter-
mined from the argument annotations themselves.
However, we did not make any attempt to distin-
guish sense in our experiments, and simply at-
tempted to predict argument labels based on the
identity of the lexical predicate.
3 The Experiments
In previous work using the FrameNet corpus,
Gildea and Jurafsky (2002) developed a system to
predict semantic roles from sentences and their
parse trees as determined by the statistical parser
of Collins (1997). We will briey review their
probability model before adapting the system to
handle unparsed data.
Probabilities of a parse constituent belonging
to a given semantic role were calculated from the
following features:
Phrase Type: This feature indicates the syntac-
tic type of the phrase expressing the semantic
roles: examples include noun phrase (NP),
verb phrase (VP), and clause (S). Phrase
types were derived automatically from parse
trees generated by the parser, as shown in
Figure 1. The parse constituent spanning
each set of words annotated as an argument
was found, and the constituent's nonterminal
label was taken as the phrase type. As an
example of how this feature is useful, in com-
munication frames, the Speaker is likely to
appear as a noun phrase, Topic as a prepo-
sitional phrase or noun phrase, and Medium
as a prepositional phrase, as in: \We talked
about the proposal over the phone." When
no parse constituent was found with bound-
aries matching those of an argument during
testing, the largest constituent beginning at
the argument's left boundary and lying en-
tirely within the element was used to calcu-
late the features.
Parse Tree Path: This feature is designed to
capture the syntactic relation of a constituent
to the predicate. It is dened as the path
from the predicate through the parse tree
to the constituent in question, represented
as a string of parse tree nonterminals linked
by symbols indicating upward or downward
movement through the tree, as shown in Fig-
ure 2. Although the path is composed as a
string of symbols, our systems will treat the
string as an atomic value. The path includes,
as the rst element of the string, the part of
speech of the predicate, and, as the last ele-
ment, the phrase type or syntactic category
of the sentence constituent marked as an ar-
gument.
Position: This feature simply indicates whether
the constituent to be labeled occurs before
or after the predicate dening the semantic
frame. This feature is highly correlated with
grammatical function, since subjects will gen-
erally appear before a verb, and objects after.
This feature may overcome the shortcom-
ings of reading grammatical function from the
parse tree, as well as errors in the parser out-
put.
S
NP VP
NP
He ate some pancakes
PRP
DT NN
VB
Figure 2: In this example, the path from the pred-
icate ate to the argument He can be represented
as VB"VP"S#NP, with " indicating upward move-
ment in the parse tree and # downward movement.
Voice: The distinction between active and pas-
sive verbs plays an important role in the con-
nection between semantic role and grammat-
ical function, since direct objects of active
verbs correspond to subjects of passive verbs.
From the parser output, verbs were classied
as active or passive by building a set of 10
passive-identifying patterns. Each of the pat-
terns requires both a passive auxiliary (some
form of \to be" or \to get") and a past par-
ticiple.
Head Word: Lexical dependencies provide im-
portant information in labeling semantic
roles, as one might expect from their use
in statistical models for parsing. Since the
parser used assigns each constituent a head
word as an integral part of the parsing model,
the head words of the constituents can be
read from the parser output. For example, in
a communication frame, noun phrases headed
by \Bill", \brother", or \he" are more likely
to be the Speaker, while those headed by
\proposal", \story", or \question" are more
likely to be the Topic.
To predict argument roles in new data, we
wish to estimate the probability of each role
given these ve features and the predicate p:
P (rjpt; path; position; voice; hw; p). Due to the
sparsity of the data, it is not possible to estimate
this probability from the counts in the training.
Instead, we estimate probabilities from various
subsets of the features, and interpolate a linear
combination of the resulting distributions. The
interpolation is performed over the most specic
distributions for which data are available, which
can be thought of as choosing the topmost distri-
butions available from a backo lattice, shown in
Figure 3.
He
PRP
NP
heard
VBD
the sound of liquid slurping in a metal container
NP
as
IN
Farrell
NNP
NP
approached
VBD
him
PRP
NP
from
IN
behind
NN
NP
PP
VP
S
SBAR
VP
S
predicate SourceGoalTheme
Figure 1: A sample sentence with parser output (above) and argument structure annotation (below).
Parse constituents corresponding to frame elements are highlighted.
P(r | h)
P(r | pt, voice)
P(r | h, pt, p) P(r | pt, voice, p)
P(r | pt, p)
P(r | p)
P(r | pt, path, p)
P(r | h, p)
Figure 3: Backo lattice with more specic distri-
butions towards the top.
We applied the same system, using the same
features to a preliminary release of the Propbank
data. The dataset used contained annotations for
26,138 predicate-argument structures containing
65,364 individual arguments and containing exam-
ples from 1,527 lexical predicates (types). In order
to provide results comparable with the statistical
parsing literature, annotations from Section 23 of
the Treebank were used as the test set; all other
sections were included in the training set.
The system was tested under two conditions,
one in which it is given the constituents which
are arguments to the predicate and merely has to
predict the correct role, and one in which it has to
both nd the arguments in the sentence and label
them correctly. Results are shown in Tables 1 and
2.
Although results for Propbank are lower than
for FrameNet, this appears to be primarily due to
the smaller number of training examples for each
predicate, rather than the dierence in annotation
style between the two corpora. The FrameNet
data contained at least ten examples from each
predicate, while 17% of the Propbank data had
fewer than ten training examples. Removing these
examples from the test set gives 84.1% accuracy
with gold-standard parses and 80.5% accuracy
with automatic parses.
As our path feature is a somewhat unusual way
of looking at parse trees, its behavior in the sys-
tem warrants a closer look. The path feature is
most useful as a way of nding arguments in the
unknown boundary condition. Removing the path
feature from the known-boundary system results
in only a small degradation in performance, from
82.3% to 81.7%. One reason for the relatively
small impact may be sparseness of the feature |
7% of paths in the test set are unseen in training
data. The most common values of the feature are
shown in Table 3, where the rst two rows cor-
respond to standard subject and object positions.
One reason for sparsity is seen in the third row:
in the Treebank, the adjunction of an adverbial
phrase or modal verb can cause an additional VP
node to appear in our path feature. We tried two
variations of the path feature to address this prob-
lem. The rst collapses sequences of nodes with
Accuracy
FrameNet Propbank Propbank
> 10 ex.
Gold-standard parses 82.8 84.1
Automatic parses 82.0 79.2 80.5
Table 1: Accuracy of semantic role prediction for known boundaries | the system is given the con-
stituents to classify.
FrameNet Propbank Propbank > 10
Precision Recall Precision Recall Precision Recall
Gold-standard parses 71.1 64.4 73.5 71.7
Automatic parses 64.6 61.2 57.7 50.0 59.0 55.4
Table 2: Accuracy of semantic role prediction for unknown boundaries | the system must identify the
correct constituents as arguments and give them the correct roles.
Path Frequency
VB"VP#NP 17:6%
VB"VP"S#NP 16:4
VB"VP"VP"S#NP 7:8
VB"VP#PP 7:6
VB"VP#PP#NP 7:3
VB"VP#SBAR#S 4:3
VB"VP#S 4:3
VB"VP#ADVP 2:4
1031 others 76:0
Table 3: Common values for parse tree path in
Propbank data, using gold-standard parses.
the same label, for example combining rows 2 and
3 of Table 3. The second variation uses only two
values for the feature: NP under S (subject posi-
tion), and NP under VP (object position). Nei-
ther variation improved performance in the known
boundary condition. As a gauge of how closely the
Propbank argument labels correspond to the path
feature overall, we note that by always assigning
the most common role for each path, for example
always assigning ARG0 to the subject position,
and using no other features, we obtain the correct
role 69.4% of the time, vs. 82.3% for the complete
system.
4 Is Parsing Necessary?
Many recent information extraction systems for
limited domains have relied on nite-state systems
that do not build a full parse tree for the sentence
being analyzed. Among such systems, (Hobbs et
al., 1997) built nite-state recognizers for vari-
ous entities, which were then cascaded to form
recognizers for higher-level relations, while (Ray
and Craven, 2001) used low-level \chunks" from
a general-purpose syntactic analyzer as observa-
tions in a trained Hidden Markov Model. Such
an approach has a large advantage in speed, as
the extensive search of modern statistical parsers
is avoided. It is also possible that this approach
may be more robust to error than parsers. Al-
though we expect the attachment decisions made
by a parser to be relevant to determining whether
a constituent of a sentence is an argument of a
particular predicate, and what its relation to the
predicate is, those decisions may be so frequently
incorrect that a much simpler system can do just
as well. In this section we test this hypothesis
by comparing a system which is given only a at,
\chunked" representation of the input sentence to
the parse-tree-based systems described above. In
this representation, base-level constituent bound-
aries and labels are present, but there are no de-
pendencies between constituents, as shown by the
following sample sentence:
(3) [
NP
Big investment banks] [
V P
refused to
step] [
ADV P
up] [
PP
to] [
NP
the plate]
[
V P
to support] [
NP
the beleaguered oor
traders] [
PP
by] [
V P
buying] [
NP
big blocks]
[
PP
of] [
NP
stock] , [
NP
traders] [
V P
say] .
Our chunks were derived from the Tree-
bank trees using the conversion described by
Tjong Kim Sang and Buchholz (2000). Thus,
the experiments were carried out using \gold-
standard" rather than automatically derived
chunk boundaries, which we believe will provide
an upper bound on the performance of a chunk-
based system.
The information provided by the parse tree can
be decomposed into three pieces: the constituent
boundaries, the grammatical relationship between
predicate and argument, expressed by our path
feature, and the head word of each candidate con-
stituent. We will examine the contribution of each
of these information sources, beginning with the
problem of assigning the correct role in the case
where the boundaries of the arguments in the sen-
tence are known, and then turning to the problem
of nding arguments in the sentence.
When the argument boundaries are known, the
grammatical relationship of the the constituent
to the predicate turns out to be of little value.
Removing the path feature from the system de-
scribed above results in only a small degradation
in performance, from 82.3% to 81.7%. While the
path feature serves to distinguish subjects from
objects, the combination of the constituent po-
sition before or after the predicate and the ac-
tive/passive voice feature serves the same purpose.
However, this result still makes use of the parser
output for nding the constituent's head word.
We implemented a simple algorithm to guess the
argument's head word from the chunked output: if
the argument begins at a chunk boundary, taking
the last word of the chunk, and in all other cases,
taking the rst word of the argument. This heuris-
tic matches the head word read from the parse tree
77% of the the time, as it correctly identies the
nal word of simple noun phrases as the head, the
preposition as the head of prepositional phrases,
and the complementizer as the head of sentential
complements. Using this process for determining
head words, the system drops to 77.0% accuracy,
indicating that identifying the relevant head word
from semantic role prediction is in itself an impor-
tant function of the parser. This chunker-based re-
sult is only slightly lower than the 79.2% obtained
using automatic parses in the known boundary
condition. These results for the known boundary
condition are summarized in Table 4.
Path Head Accuracy
gold parse gold parse 82.3
auto parse auto parse 79.2
not used gold parse 81.7
not used chunks 77.0
Table 4: Summary of results for known boundary
condition
We might expect the information provided by
the parser to be more important in identifying the
arguments in the sentence than in assigning them
the correct role. While it is easy to guess whether
a noun phrase is a subject or object given only
its position relative to the predicate, identifying
complex noun phrases and determining whether
they are arguments of a verb may be more di?cult
without the attachment information provided by
the parser.
To test this, we implemented a system in which
the argument labels were assigned to chunks, with
the path feature used by the parse-tree-based sys-
tem replaced by a number expressing the distance
in chunks to the left or right of the predicate.
Of the 3990 arguments in our test set, only
39.8% correspond to a single chunk in the at-
tened sentence representation, giving an upper
bound to the performance of this system. In par-
ticular, sentential complements (which comprise
11% of the data) and prepositional phrases (which
comprise 10%) always correspond to more than
one chunk, and therefore cannot be correctly la-
beled by our system which assigns roles to single
chunks. In fact, this system achieves 27.6% preci-
sion and 22.0% recall.
In order to see how much of the performance
degradation is caused by the di?culty of nding
exact argument boundaries in the chunked rep-
resentation, we can relax the scoring criteria to
count as correct all cases where the system cor-
rectly identies the rst chunk belonging to an
argument. For example, if the system assigns the
correct label to the preposition beginning a prepo-
sitional phrase, the argument will be counted as
correct, even though the system does not nd
the argument's righthand boundary. With this
scoring regime, the chunk-based system performs
at 49.5% precision and 35.1% recall, still signi-
cantly lower than the 57.7% precision/50.0% recall
for exact matches using automatically generated
parses. Results for the unknown boundary condi-
tion are summarized in Table 5.
Precision Recall
gold parse 71.1 64.4
auto parse 57.7 50.0
chunk 27.6 22.0
chunk, relaxed scoring 49.5 35.1
Table 5: Summary of results for unknown bound-
ary condition
As an example for comparing the behavior of
the tree-based and chunk-based systems, consider
the following sentence, with human annotations
showing the arguments of the predicate support:
(4) [
ARG0
Big investment banks] refused to step
up to the plate to support [
ARG1
the
beleaguered oor traders] [
MNR
by buying
big blocks of stock] , traders say .
Our tree-based system assigned the following anal-
ysis:
(5) Big investment banks refused to step up to
the plate to support [
ARG1
the beleaguered
oor traders] [
MNR
by buying big blocks of
stock] , traders say .
In this case, the system failed to nd the predi-
cate's ARG0 relation, because it is syntactically
distant from the verb support. The original Tree-
bank syntactic tree contains a trace which would
allow one to recover this relation, co-indexing the
empty subject position of support with the noun
phrase \Big investment banks". However, our
automatic parser output does not include such
traces, nor does our system make use of them.
The chunk-based system assigns the following ar-
gument labels:
(6) Big investment banks refused to step up to
[
ARG0
the plate] to support [
ARG1
the
beleaguered oor traders] by buying big
blocks of stock , traders say .
Here, as before, the true ARG0 relation is not
found, and it would be di?cult to imagine iden-
tifying it without building a complete syntactic
parse of the sentence. But now, unlike in the
tree-based output, the ARG0 label is mistakenly
attached to a noun phrase immediately before the
predicate. The ARG1 relation in direct object po-
sition is fairly easily identiable in the chunked
representation as a noun phrase directly follow-
ing the verb. The prepositional phrase expressing
the Manner relation, however, is not identied by
the chunk-based system. The tree-based system's
path feature for this constituent is VB"VP#PP,
which identies the prepositional phrase as at-
taching to the verb, and increases its probability
of being assigned an argument label. The chunk-
based system sees this as a prepositional phrase
appearing as the second chunk after the predi-
cate. Although this may be a typical position for
the Manner relation, the fact that the preposition
attaches to the predicate rather than to its direct
object is not represented.
In interpreting these results, it is important to
keep in mind the dierences between this task
and other information extraction datasets. In
comparison to the domain-specic relations eval-
uated by the Message Understanding Conference
(MUC) tasks, we have a wider variety of relations
but fewer training instances for each. The rela-
tions may themselves be less susceptible to nite
state methods. For example, a named-entity sys-
tem which indenties corporation names can go
a long way towards nding the \employment" re-
lation of MUC, and similarly systems for tagging
genes and proteins help a great deal for relations
in the biomedical domain. Both Propbank and
FrameNet tend to include longer arguments with
internal syntactic structure, making parsing deci-
sions more important in nding argument bound-
aries. They also involve abstract relations, with a
wide variety of possible llers for each role.
5 Conclusion
Our chunk-based system takes the last word of
the chunk as its head word for the purposes of
predicting roles, but does not make use of the
identities of the chunk's other words or the inter-
vening words between a chunk and the predicate,
unlike Hidden Markov Model-like systems such as
Bikel et al (1997), McCallum et al (2000) and
Laerty et al (2001). While a more elaborate
nite-state system might do better, it is possible
that additional features would not be helpful given
the small amount of data for each predicate. By
using a gold-standard chunking representation, we
have obtained higher performance over what could
be expected from an entirely automatic system
based on a at representation of the data.
We feel that our results show that statistical
parsers, although computationally expensive, do
a good job of providing relevant information for
semantic interpretation. Not only the constituent
structure but also head word information, pro-
duced as a side product, are important features.
Parsers, however, still have a long way to go.
Our results using hand-annotated parse trees show
that improvements in parsing should translate di-
rectly into better semantic interpretations.
Acknowledgments This work was undertaken
with funding from the Institute for Research in
Cognitive Science at the University of Pennsylva-
nia and from the Propbank project, DoD Grant
MDA904-00C-2136.
References
Hiyan Alshawi, editor. 1992. The Core Language
Engine. MIT Press, Cambridge, MA.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project.
In Proceedings of COLING/ACL, pages 86{90,
Montreal, Canada.
D. M. Bikel, S. Miller, R. Schwartz, and
R. Weischedel. 1997. Nymble: a high-
performance learning name-nder. In Proceed-
ings of the Fifth Conference on Applied Natural
Language Processing.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI-97, pages 598{603, Menlo Park, August.
AAAI Press.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the ACL,
pages 16{23, Madrid, Spain.
Charles J. Fillmore. 1976. Frame semantics
and the nature of language. In Annals of the
New York Academy of Sciences: Conference on
the Origin and Development of Language and
Speech, volume 280, pages 20{32.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Computational
Linguistics, in press.
Jerry R. Hobbs, Douglas Appelt, John Bear,
David Israel, Megumi Kameyama, Mark E.
Stickel, and Mabry Tyson. 1997. FASTUS:
A cascaded nite-state transducer for extract-
ing information from natural-language text. In
Emmanuel Roche and Yves Schabes, editors,
Finite-State Language Processing, pages 383{
406. MIT Press, Cambridge, MA.
Christopher R. Johnson, Charles J. Fillmore,
Esther J. Wood, Josef Ruppenhofer, Mar-
garet Urban, Miriam R. L. Petruk, and
Collin F. Baker. 2001. The FrameNet
project: Tools for lexicon building. Version 0.7,
http://www.icsi.berkeley.edu/~framenet/book.html.
Paul Kingsbury and Martha Palmer. 2002. From
Treebank to PropBank. In Proceedings of the
3rd International Conference on Language Re-
sources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
John Laerty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random elds:
Probabilistic models for segmenting and label-
ing sequence data. In Machine Learning: Pro-
ceedings of the Eighteenth International Confer-
ence (ICML 2001), Stanford, California.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmenta-
tion. In Machine Learning: Proceedings of the
Seventeenth International Conference (ICML
2000), pages 591{598, Stanford, California.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract informa-
tion { BBN: Description of the SIFT system as
used for MUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7),
April.
Soumya Ray and Mark Craven. 2001. Represent-
ing sentence structure in hidden markov model
for information extraction. In Seventeenth In-
ternational Joint Conference on Articial Intel-
ligence (IJCAI-01), Seattle, Washington.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the conll-2000 shared
task: Chunking. In Proceedings of CoNLL-2000
and LLL-2000, Lisbon, Portugal.
Chinese Verb Sense Discrimination Using an EM Clustering Model with Rich 
Linguistic Features 
Jinying Chen, Martha Palmer 
Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA, 19104 
{jinying,mpalmer}@linc.cis.upenn.edu 
 
Abstract 
This paper discusses the application of the 
Expectation-Maximization (EM) clustering 
algorithm to the task of Chinese verb sense 
discrimination. The model utilized rich 
linguistic features that capture predicate-
argument structure information of the target 
verbs. A semantic taxonomy for Chinese 
nouns, which was built semi-automatically 
based on two electronic Chinese semantic 
dictionaries, was used to provide semantic 
features for the model. Purity and normalized 
mutual information were used to evaluate the 
clustering performance on 12 Chinese verbs. 
The experimental results show that the EM 
clustering model can learn sense or sense 
group distinctions for most of the verbs 
successfully. We further enhanced the model 
with certain fine-grained semantic categories 
called lexical sets. Our results indicate that 
these lexical sets improve the model?s 
performance for the three most challenging 
verbs chosen from the first set of experiments. 
1 Introduction 
Highly ambiguous words may lead to irrelevant 
document retrieval and inaccurate lexical choice in 
machine translation (Palmer et al, 2000), which 
suggests that word sense disambiguation (WSD) is 
beneficial and sometimes even necessary in such 
NLP tasks. This paper addresses WSD in Chinese 
through developing an Expectation-Maximization 
(EM) clustering model to learn Chinese verb sense 
distinctions. The major goal is to do sense 
discrimination rather than sense labeling, similar to 
(Sch?tze, 1998). The basic idea is to divide 
instances of a word into several clusters that have 
no sense labels. The instances in the same cluster 
are regarded as having the same meaning. Word 
sense discrimination can be applied to document 
retrieval and similar tasks in information access, 
and to facilitating the building of large annotated 
corpora. In addition, since the clustering model can 
be trained on large unannotated corpora and 
evaluated on a relatively small sense-tagged 
corpus, it can be used to find indicative features for 
sense distinctions through exploring huge amount 
of available unannotated text data.   
The EM clustering algorithm (Hofmann and 
Puzicha, 1998) used here is an unsupervised 
machine learning algorithm that has been applied 
in many NLP tasks, such as inducing a 
semantically labeled lexicon and determining 
lexical choice in machine translation (Rooth et al, 
1998), automatic acquisition of verb semantic 
classes (Schulte im Walde, 2000) and automatic 
semantic labeling (Gildea and Jurafsky, 2002). In 
our task, we equipped the EM clustering model 
with rich linguistic features that capture the 
predicate-argument structure information of verbs 
and restricted the feature set for each verb using 
knowledge from dictionaries. We also semi-
automatically built a semantic taxonomy for 
Chinese nouns based on two Chinese electronic 
semantic dictionaries, the Hownet dictionary1 and 
the Rocling dictionary.2 The 7 top-level categories 
of this taxonomy were used as semantic features 
for the model. Since external knowledge is used to 
obtain the semantic features and guide feature 
selection, the model is not completely 
unsupervised from this perspective; however, it 
does not make use of any annotated training data. 
Two external quality measures, purity and 
normalized mutual information (NMI) (Strehl. 
2002), were used to evaluate the model?s 
performance on 12 Chinese verbs. The 
experimental results show that rich linguistic 
features and the semantic taxonomy are both very 
useful in sense discrimination. The model 
generally performs well in learning sense group 
distinctions for difficult, highly polysemous verbs 
and sense distinctions for other verbs. Enhanced by 
certain fine-grained semantic categories called 
lexical sets (Hanks, 1996), the model?s 
                                                     
1 http://www.keenage.com/. 
2 A Chinese electronic dictionary liscenced from The 
Association for Computational Linguistics and Chinese 
Language Processing (ACLCLP), Nankang, Taipei, 
Taiwan. 
performance improved in a preliminary experiment 
for the three most difficult verbs chosen from the 
first set of experiments. 
The paper is organized as follows: we briefly 
introduce the EM clustering model in Section 2 
and describe the features used by the model in 
Section 3. In Section 4, we introduce a semantic 
taxonomy for Chinese nouns, which is built semi-
automatically for our task but can also be used in 
other NLP tasks such as co-reference resolution 
and relation detection in information extraction. 
We report our experimental results in Section 5 
and conclude our discussion in Section 6. 
2 EM Clustering Model 
The basic idea of our EM clustering approach is 
similar to the probabilistic model of co-occurrence 
described in detail in (Hofmann and Puzicha 
1998). In our model, we treat a set of features { }mfff ,...,, 21 , which are extracted from the 
parsed sentences that contain a target verb, as 
observed variables. These variables are assumed to 
be independent given a hidden variable c, the sense 
of the target verb. Therefore the joint probability of 
the observed variables (features) for each verb 
instance, i.e., each parsed sentence containing the 
target verb, is defined in equation (1), 
? ?
=
=
c
m
i
im cfpcpfffp
1
21 )|()(),...,,(           (1)  
The if ?s are discrete-valued features that can 
take multiple values. A typical feature used in our 
model is shown in (2), 
 
=if
???
???
?
             (2) 
 
At the beginning of training (i.e., clustering), the 
model?s parameters )(cp  and )|( cfp i  are 
randomly initialized.3 Then, the probability of c 
conditioned on the observed features is computed 
in the expectation step (E-step), using equation (3),  
? ?
?
=
==
c
m
i
i
m
i
i
m
cfpcp
cfpcp
fffcp
1
1
21
)|()(
)|()(
),...,,|(~   (3) 
                                                     
3 In our experiments, for verbs with more than 3 
senses, syntactic and semantic restrictions derived from 
dictionary entries are used to constrain the random 
initialization. 
In the maximization step (M-step), )(cp  and 
)|( cfp i  are re-computed by maximizing the log-
likelihood of all the observed data which is 
calculated by using ),...,,|(~ 21 mfffcp  estimated 
in the E-step. The E-step and M-step are repeated 
for a fixed number of rounds, which is set to 20 in 
our experiments,4 or till the amount of change of 
)(cp  and )|( cfp i  is under the threshold 0.001.  
When doing classification, for each verb 
instance, the model calculates the same conditional 
probability as in equation (3) and assigns the 
instance to the cluster with the maximal 
),...,,|( 21 mfffcp . 
3 Features Used in the Model 
The EM clustering model uses a set of linguistic 
features to capture the predicate-argument 
structure information of the target verbs. These 
features are usually more indicative of verb sense 
distinctions than simple features such as words 
next to the target verb or their POS tags. For 
example, the Chinese verb ??| chu1? has a sense 
of produce, the distinction between this sense and 
the verb?s other senses, such as happen and go out, 
largely depends on the semantic category of the 
verb?s direct object. Typical examples are shown 
in (1), 
 
   (1 ?? ? ? ??) a.  /their  /county  /produce  /banana 
          ?Their county produces bananas.? 
 
?? ? ? ?      b. /their  /county  /happen  /big    
? ?          /event /ASP 
          ?A big event happened in their county.? 
 
?? ? ? ?      c.  /their  /county  /go out  ?/door     
? ?          /right away  /be  /mountain 
          ?In their county, you can see mountains as soon  
           as you step out of the doors.? 
 
The verb has the sense produce in (1a) and its 
object should be something producible, such as 
??? /banana?. While in (1b), with the sense 
happen, the verb typically takes an event or event-
like ?? object, such as ? /big event?, 
??? /accident? or ???/problem? etc. In (1c), 
?the verb?s object ? /door? is closely related to 
location, consistent with the sense go out. In 
contrast, simple lexical or POS tag features 
sometimes fail to capture such information, which 
can be seen clearly in (2), 
                                                     
4 In our experiments, we set 20 as the maximal 
number of rounds after trying different numbers of 
rounds (20, 40, 60, 80, 100) in a preliminary 
experiment. 
0  iff  the target verb has no sentential 
         complement 
1  iff  the target verb has a nonfinite  
         sentential complement 
2  iff  the target verb has a finite    
             sentential complement 
?? ?   (2) a. /last year  /produce  ??/banana  3000 
??         / kilogram 
         ?3000 kilograms of bananas were produced last  
          year.?  
       
? ?      b. /in order to /produce   ??/Hainan            
?? ? ??          /best  /DE   /banana 
         ?In order to produce the best bananas in  
          Hainan, ??? 
 
The verb?s object ???/banana?, which is next 
to the verb in (2a), is far away from the verb in 
(2b). For (2b), a classifier only looking at the 
adjacent positions of the target verb tends to be 
misled by the NP right after the verb, i.e., 
???/Hainan?, which is a Province in China and a 
typical object of the verb with the sense go out.    
Five types of features are used in our model: 
1. Semantic category of the subject of the target 
verb 
2. Semantic category of the object of the target 
verb 
3. Transitivity of the target verb 
4. Whether the target verb takes a sentential 
complement and which type of sentential 
complement (finite or nonfinite) it takes 
5. Whether the target verb occurs in a verb 
compound  
We obtain the values for the first two types of 
features (1) and (2) from a semantic taxonomy for 
Chinese nouns, which we will introduce in detail in 
the next section. 
In our implementation, the model uses different 
features for different verbs. The criteria for feature 
selection are from the electronic CETA dictionary 
file 5 and a hard copy English-Chinese dictionary, 
The Warmth Modern Chinese-English Dictionary.6 
For example, the verb ??|chu1? never takes 
sentential complements, thus the fourth type of 
feature is not used for it. It could be supposed that 
we can still have a uniform model, i.e., a model 
using the same set of features for all the target 
verbs, and just let the EM clustering algorithm find 
useful features for different verbs automatically. 
The problem here is that unsupervised learning 
models (i.e., models trained on unlabeled data) are 
more likely to be affected by noisy data than 
supervised ones. Since all the features used in our 
model are extracted from automatically parsed 
sentences that inevitably have preprocessing errors 
such as segmentation, POS tagging and parsing 
errors, using verb-specific sets of features can 
alleviate the problem caused by noisy data to some 
extent. For example, if the model already knows 
                                                     
5 Licensed from the Department of Defense 
6 The Warmth Modern Chinese-English Dictionary, 
Wang-Wen Books Ltd, 1997. 
that a verb like ??|chu1? can never take sentential 
complements (i.e., it does not use the fourth type of 
feature for that verb), it will not be misled by 
erroneous parsing information saying that the verb 
takes sentential complements in certain sentences. 
Since the corresponding feature is not included, the 
noisy data is filtered out. In our EM clustering 
model, all the features selected for a target verb are 
treated in the same way, as described in Section 2. 
4 A Semantic Taxonomy Built Semi-
automatically 
Examples in (1) have shown that the semantic 
category of the object of a verb sometimes is 
crucial in distinguishing certain Chinese verb 
senses. And our previous work on information 
extraction in Chinese (Chen et al, 2004) has 
shown that semantic features, which are more 
general than lexical features but still contain rich 
information about words, can be used to improve a 
model?s capability of handling unknown words, 
thus alleviating potential sparse data problems.  
We have two Chinese electronic semantic 
dictionaries: the Hownet dictionary, which assigns 
26,106 nouns to 346 semantic categories, and the 
Rocling dictionary, which assigns 4,474 nouns to 
110 semantic categories.7 A preliminary 
experimental result suggests that these semantic 
categories might be too fine-grained for the EM 
clustering model (see Section 5.2 for greater 
details). An analysis of the sense distinctions of 
several Chinese verbs also suggests that more 
general categories on top of the Hownet and 
Rocling categories could still be informative and 
most importantly, could enable the model to 
generate meaningful clusters more easily. We 
therefore built a three-level semantic taxonomy 
based on the two semantic dictionaries using both 
automatic methods and manual effort.  
The taxonomy was built in three steps. First, a 
simple mapping algorithm was used to map 
semantic categories defined in Hownet and 
Rocling into 27 top-level WordNet categories.8 
The Hownet or Rocling semantic categories have 
English glosses. For each category gloss, the 
algorithm looks through the hypernyms of its first 
sense in WordNet and chooses the first WordNet 
top-level category it finds. 
                                                     
7 Hownet assigns multiple entries (could be different 
semantic categories) to polysemous words. The Rocling 
dictionary we used only assigns one entry (i.e., one 
semantic category) to each noun.  
8 The 27 categories contain 25 unique beginners for 
noun source files in WordNet, as defined in (Fellbaum, 
1998) and two higher level categories Entity and 
Abstraction. 
The mapping obtained from step 1 needs further 
modification for two reasons. First, the glosses of 
Hownet or Rocling semantic categories usually 
have multiple senses in WordNet. Sometimes, the 
first sense in WordNet for a category gloss is not 
its intended meaning in Hownet or Rocling. In this 
case, the simple algorithm cannot get the correct 
mapping. Second, Hownet and Rocling sometimes 
use adjectives or non-words as category glosses, 
such as animate and LandVehicle etc., which have 
no WordNet nominal hypernyms at all. However, 
those adjectives or non-words usually have 
straightforward meanings and can be easily 
reassigned to an appropriate WordNet category. 
Although not accurate, the automatic mapping in 
step 1 provides a basic framework or skeleton for 
the semantic taxonomy we want to build and 
makes subsequent work easier.  
In step 2, hand correction, we found that we 
could make judgments and necessary adjustments 
on about 80% of the mappings by only looking at 
the category glosses used by Hownet or Rocling, 
such as livestock, money, building and so on. For 
the other 20%, we could make quick decisions by 
looking them up in an electronic table we created. 
For each Hownet or Rocling category, our table 
lists all the nouns assigned to it by the two 
dictionaries. We merged two WordNet categories 
into others and subdivided three categories that 
seemed more coarse-grained than others into 2~5 
subcategories. Step 2 took three days and 35 
intermediate-level categories were generated.  
In step 3, we manually clustered the 35 
intermediate-level categories into 7 top-level 
semantic categories. Figure 1 shows part of the 
taxonomy. 
The EM clustering model uses the 7 top-level 
categories to define the first two types of features 
that were introduced in Section 3. For example, the 
value of a feature kf  is 1 if and only if the object 
NP of the target verb belongs to the semantic 
category Event and is otherwise 0. 
5 Clustering Experiments 
Since we need labeled data to evaluate the 
clustering performance but have limited sense- 
tagged corpora, we applied the clustering model to 
12 Chinese verbs in our experiments. The verbs are 
chosen from 28 annotated verbs in Penn Chinese 
Treebank so that they have at least two verb 
meanings in the corpus and for each of them, the 
number of instances for a single verb sense does 
not exceed 90% of the total number of instances.  
In our task, we generally do not include senses 
for other parts of speech of the selected words, 
such as noun, preposition, conjunction and particle 
etc., since the parser we used has a very high 
accuracy in distinguishing different parts of speech 
of these words (>98% for most of them). However, 
we do include senses for conjunctional and/or 
prepositional usage of two words, ??|dao4? and 
??|wei4?, since our parser cannot distinguish the 
verb usage from the conjunctional or prepositional 
usage for the two words very well. 
Five verbs, the first five listed in Table 1, are 
both highly polysemous and difficult for a 
supervised word sense classifier (Dang et al, 
2002). 9 In our experiments, we manually grouped 
the verb senses for the five verbs. The criteria for 
the grouping are similar to Palmer et al?s (to 
appear) work on English verbs, which considers 
both sense coherence and predicate-argument 
structure distinctions. Figure 2 gives an example of  
                                                     
9 In the supervised task, their accuracies are lower 
than 85%, and four of them are even lower than the 
baselines. 
Entity 
Plant     Artifact 
             Document 
   Food    ?? 
          Money 
 drinks, edible, meals, vegetable, ?  
Location 
Location_Part            
   Location     
    Group  ??  
institution, army, corporation, ? 
 Event
Natural Phenomena 
                Happening
Activity    ?? 
              Process 
chase, cut, pass, split, cheat, ? 
  process, BecomeLess, StateChange, disappear, ?. 
Top level 
 
Intermediate level
 
Hownet/Rocling 
categories 
Figure 1.   Part of the 3-level Semantic Taxonomy for Chinese Nouns (other top-level nodes 
are Time, Human, Animal and State) 
the definition of sense groups. The manually 
defined sense groups are used to evaluate the 
model?s performance on the five verbs. 
The model was trained on an unannotated 
corpus, People?s Daily News (PDN), and tested on 
the manually sense-tagged Chinese Treebank (with 
some additional sense-tagged PDN data).10 We 
parsed the training and test data using a Maximum 
Entropy parser and extracted the features from the 
parsed data automatically. The number of clusters 
used by the model is set to the number of the 
defined senses or sense groups of each target verb. 
For each verb, we ran the EM clustering algorithm 
ten times. Table 2 shows the average performance 
and the standard deviation for each verb. Table 1 
summarizes the data used in the experiments, 
where we also give the normalized sense 
perplexity11 of each verb in the test data. 
5.1 Evaluation Methods 
We use two external quality measures, purity 
and normalized mutual information (NMI) (Strehl. 
2002) to evaluate the clustering performance. 
Assuming a verb has l senses, the clustering model 
assigns n instances of the verb into k clusters, in is 
the size of the ith cluster, jn  is the number of 
instances hand-tagged with the jth sense, and jin is 
the number of instances with the jth sense in the ith 
cluster, purity is defined in equation (4): 
 
?
=
=
k
i
j
ij
n
n
purity
1
max1             (4) 
                                                     
10 The sense-tagged PDN data we used here are the 
same as in (Dang et al, 2002). 
11 It is calculated as the entropy of the sense 
distribution of a verb in the test data divided by the 
largest possible entropy, i.e., log2 (the number of senses 
of the verb in the test data).  
It can be interpreted as classification accuracy 
when for each cluster we treat the majority of 
instances that have the same sense as correctly 
classified. The baseline purity is calculated by 
treating all instances for a target verb in a single 
cluster. The purity measure is very intuitive. In our 
case, since the number of clusters is preset to the 
number of senses, purity for verbs with two senses 
is equal to classification accuracy defined in 
supervised WSD. However, for verbs with more 
than 2 senses, purity is less informative in that a 
clustering model could achieve high purity by 
making the instances of 2 or 3 dominant senses the 
majority instances of all the clusters.  
Mutual information (MI) is more theoretically 
well-founded than purity. Treating the verb sense 
and the cluster as random variables S and C, the 
MI between them is defined in equation (5): 
??
?
= =
=
=
l
j
k
i
j
i
j
i
j
i
cs
nn
nn
n
n
cpsp
cspcspCSMI
1 1
,
log
)()(
),(log),(),(
           (5) 
MI(S,C) characterizes the reduction in  
uncertainty of one random variable S (or C) due to 
knowing the other variable C (or S).  A single 
cluster with all instances for a target verb has a 
zero MI. Random clustering also has a zero MI in 
the limit. In our experiments, we used [0,1]-
normalized mutual information (NMI) (Strehl. 
2002). A shortcoming of this measure, however, is 
that the best possible clustering (upper bound) 
evaluates to less than 1, unless classes are 
balanced. Unfortunately, unbalanced sense 
distribution is the usual case in WSD tasks, which 
makes NMI itself hard to interpret. Therefore, in 
addition to NMI, we also give its upper bound 
(upper-NMI) and the ratio of NMI and its upper 
bound (NMI-ratio) for each verb, as shown in 
columns 6 to 8 in Table 2. 
Senses for ??|dao4?                Sense groups for ??|dao4? 
 
1. to go to, leave for 
2. to come 
3. to arrive 
4. to reach a particular stage, condition, or level 
5. marker for completion of activities (after a verb) 
6. marker for direction of activities (after a verb) 
7. to reach a time point 
8. up to, until (prepositional usage) 
9. up to, until, (from ?) to ? (conjunctional usage) 
1, 2
4,7,8,9
 5 
 3 
6 
Figure 2.  Sense groups for the Chinese verb ??|dao4? 
Verb| Pinyin Sample senses of 
the verb 
# Senses in 
test data 
# Sense 
groups in 
test data 
Sense 
perplexity  
# 
Clusters 
# Training 
instances  
# Test 
instances 
?    |chu1 go out /produce 16 7 0.68 8 399 157 
?    |dao4 come /reach 9 5 0.72 6 1838 186 
?    |jian4 see /show 8 5 0.68 6 117 82 
?    |xiang3 think/suppose 6 4 0.64 6 94 228 
?    |yao4 Should/intend to 8 4 0.65 7 2781 185 
??|biao3shi4 Indicate /express 2  0.93 2 666 97 
??|fa1xian4 discover /realize 2  0.76 2 319 27 
??|fa1zhan3 develop /grow 3  0.69 3 458 130 
??|hui1fu4 resume /restore 4  0.83 4 107 125 
?    |shuo1 say /express by 
written words 
7  0.40 7 2692 307 
??|tou2ru4 to input /plunge into 2  1.00 2 136 23 
?    |wei2_4 to be /in order to 6  0.82 6 547 463 
 
Verb Sense 
perplexity  
Baseline 
Purity (%) 
Purity 
(%) 
Std. Dev. of 
purity (%) 
NMI Upper- 
NMI 
NMI- 
ratio (%) 
Std. Dev. of 
NMI ratio (%) 
? 0.68 52.87 63.31 1.59 0.2954 0.6831 43.24 1.76 
? 0.72 40.32 90.48 1.08 0.4802 0.7200 75.65 0.00 
? 0.68 58.54 72.20 1.61 0.1526 0.6806 22.41 0.66 
? 0.64 68.42 79.39 3.74 0.2366 0.6354 37.24 8.22 
? 0.65 69.19 69.62 0.34 0.0108 0.6550 1.65 0.78 
?? 0.93 64.95 98.04 1.49 0.8670 0.9345 92.77 0.00 
?? 0.76 77.78 97.04 3.87 0.7161 0.7642 93.71 13.26 
?? 0.69 53.13 90.77 0.24 0.4482 0.6918 64.79 2.26 
?? 0.83 45.97 65.32 0.00 0.1288 0.8234 15.64 0.00 
?    0.40 80.13 93.00 0.58 0.3013 0.3958 76.13 4.07 
?? 1.00 52.17 95.65 0.00 0.7827 0.9986 78.38 0.00 
? 0.82 32.61 75.12 0.43 0.4213 0.8213 51.30 2.07 
Average 0.73 58.01 82.50 1.12 0.4088 0.7336 54.41 3.31 
5.2 Experimental Results 
Table 2 summarizes the experimental results for 
the 12 Chinese verbs. As we see, the EM clustering 
model performs well on most of them, except the 
verb ??|yao4?.12 The NMI measure NMI-ratio 
turns out to be more stringent than purity. A high 
purity does not necessarily mean a high NMI-ratio. 
Although intuitively, NMI-ratio should be related 
to sense perplexity and purity, it is hard to 
formalize the relationships between them from the 
results. In fact, the NMI-ratio for a particular verb 
is eventually determined by its concrete sense 
distribution in the test data and the model?s 
clustering behavior for that verb. For example, the 
verbs ??|chu1? and ??|jian4? have the same 
sense perplexity and ??|jian4? has a higher purity 
than ??|chu1? (72.20% vs. 63.31%), but the NMI-
ratio for ??|jian4? is much lower than ??|chu1? 
(22.41% vs. 43.24%). An analysis of the 
                                                     
12 For all the verbs except ??|yao4?, the model?s 
purities outperformed the baseline purities significantly 
(p<0.05, and p<0.001 for 8 of them).  
classification results for ??|jian4? shows that the 
clustering model made the instances of the verb?s 
most dominant sense the majority instances of 
three clusters (of total 5 clusters), which is 
penalized heavily by the NMI measure.    
Rich linguistic features turn out to be very 
effective in learning Chinese verb sense 
distinctions. Except for the two verbs, 
???|fa1xian4? and ???|biao3shi4?, the sense 
distinctions of which can usually be made only by 
syntactic alternations,13 features such as semantic 
features or combinations of semantic features and 
syntactic alternations are very beneficial and 
sometimes even necessary for learning sense 
distinctions of other verbs. For example, the verb 
??|jian4? has one sense see, in which the verb 
typically takes a Human subject and a sentential 
complement, while in another sense show, the verb 
typically takes an Entity subject and a State object. 
An inspection of the classification results shows 
                                                     
13 For example, the verb ???|fa1xian4? takes an 
object in one sense discover and a sentential 
complement in the other sense realize. 
Table 1.   A summary of the training and test data used in the experiments  
Table 2.   The performance of the EM clustering model on 12 Chinese verbs measured
by purity and normalized mutual information (NMI) 
that the EM clustering model has indeed learned 
such combinatory patterns from the training data. 
The experimental results also indicate that the 
semantic taxonomy we built is beneficial for the 
task. For example, the verb ???|tou1ru4? has 
two senses, input and plunge into. It typically takes 
an Event object for the second sense but not for the 
first one. A single feature obtained from our 
semantic taxonomy, which tests whether the verb 
takes an Event object, captures this property neatly 
(achieves purity 95.65% and NMI-ratio 78.38% 
when using 2 clusters). Without the taxonomy, the 
top-level category Event is split into many fine-
grained Hownet or Rocling categories, which 
makes it very difficult for the EM clustering model 
to learn sense distinctions for this verb. In fact, in a 
preliminary experiment only using the Hownet and 
Rocling categories, the model had the same purity 
as the baseline (52.17%) and a low NMI-ratio 
(4.22%) when using 2 clusters. The purity 
improved when using more clusters (70.43% with 
4 clusters and 76.09% with 6), but it was still much 
lower than the purity achieved by using the 
semantic taxonomy and the NMI-ratio dropped 
further (1.19% and 1.20% for the two cases).  
By looking at the classification results, we 
identified three major types of errors. First, 
preprocessing errors create noisy data for the 
model. Second, certain sense distinctions depend 
heavily on global contextual information (cross-
sentence information) that is not captured by our 
model. This problem is especially serious for the 
verb ??|yao4?. For example, without global 
contextual information, the verb can have at least 
three meanings want, need or should in the same 
clause, as shown in (3).   
 
(3) ? ? ??/he    /want/need/should    /at once          
?? ?? ?      /finish reading  /this /book. 
     ?He wants to/needs to/should finish reading this   
     book at once.? 
 
Third, a target verb sometimes has specific types 
of NP arguments or co-occurs with specific types 
of verbs in verb compounds in certain senses. Such 
information is crucial for distinguishing these 
senses from others, but is not captured by the 
general semantic taxonomy used here. We did 
further experiments to investigate how much 
improvement the model could gain by capturing 
such information, as discussed in Section 5.3. 
5.3 Experiments with Lexical Sets 
As discussed by Patrick Hanks (1996), certain 
senses of a verb are often distinguished by very 
narrowly defined semantic classes (called lexical 
sets) that are specific to the meaning of that verb 
sense. For example, in our case, the verb 
???|hui1fu4? has a sense recover in which its 
direct object should be something that can be 
recovered naturally. A typical set of object NPs of 
the verb for this particular sense is partially listed 
in (4), 
(4) Lexical set for naturally recoverable things 
?? ?? ??{ /physical strength, /body, /health,   
?? ??/mental energy, /hearing ??, /feeling, 
???/memory, ??} 
Most words in this lexical set belong to the 
Hownet category attribute and the top-level 
category State in our taxonomy. However, even the 
lower-level category attribute still contains many 
other words irrelevant to the lexical set, some of 
which are even typical objects of the verb for two 
other senses, resume and regain, such as 
???/diplomatic relations? in ???/resume 
??/diplomatic relations? and ???/reputation? 
in ???/regain??/reputation?. Therefore, a 
lexical set like (4) is necessary for distinguishing 
the recover sense from other senses of the verb.  
It has been argued that the extensional definition 
of lexical sets can only be done using corpus 
evidence and it cannot be done fully automatically 
(Hanks, 1997). In our experiments, we use a 
bootstrapping approach to obtain five lexical sets 
semi-automatically for three verbs ??|chu1?, 
??|jian4? and ???|hui1fu4? that have both low 
purity and low NMI-ratio in the first set of 
experiments. 14 We first extracted candidates for 
the lexical sets from the training data. For example, 
we extracted all the direct objects of the verb 
???|hui1fu4? and all the verbs that combined 
with the verb ??|chu1? to form verb compounds 
from the automatically parsed training data. From 
the candidates, we manually selected words to 
form five initial seed sets, each of which contains 
no more than ten words. A simple algorithm was 
used to search for all the words that have the same 
detailed Hownet semantic definitions (semantic 
category plus certain supplementary information) 
as the seed words. We did not use Rocling because 
its semantic definitions are so general that a seed 
word tends to extend to a huge set of irrelevant 
words. Highly relevant words were manually 
selected from all the words found by the searching 
algorithm and added to the initial seed sets. The 
enlarged sets were used as lexical sets. 
The enhanced model first uses the lexical sets to 
obtain the semantic category of the NP arguments 
                                                     
14 We did not include ??|yao4?, since its meaning 
rarely depends on local predicate-argument structure 
information. 
of the three verbs. Only when the search fails does 
the model resort to the general semantic taxonomy. 
The model also uses the lexical sets to determine 
the types of the compound verbs that contain the 
target verb ??|chu1? and uses them as new 
features. 
Table 3 shows the model?s performance on the 
three verbs with or without using lexical sets. As 
we see, lexical sets improves the model?s 
performance on all of them, especially on the verb 
??|chu1?. Although the results are still 
preliminary, they nevertheless provide us hints of 
how much a WSD model for Chinese verbs could 
gain from lexical sets. 
 
w/o  lexical sets (%) with lexical sets (%) Verb Purity NMI-ratio Purity NMI-ratio 
? 63.61 43.24 76.50 52.81 
? 72.20 22.41 77.56 34.63 
?? 65.32 15.64 69.03 19.71 
6 Conclusion 
We have shown that an EM clustering model 
that uses rich linguistic features and a general 
semantic taxonomy for Chinese nouns generally 
performs well in learning sense distinctions for 12 
Chinese verbs. In addition, using lexical sets 
improves the model?s performance on three of the 
most challenging verbs.  
Future work is to extend our coverage and to 
apply the semantic taxonomy and the same types 
of features to supervised WSD in Chinese. Since 
the experimental results suggest that a general 
semantic taxonomy and more constrained lexical 
sets are both beneficial for WSD tasks, we will 
develop automatic methods to build large-scale 
semantic taxonomies and lexical sets for Chinese, 
which reduce human effort as much as possible but 
still ensure high quality of the obtained taxonomies 
or lexical sets. 
7 Acknowledgements 
This work has been supported by an ITIC 
supplement to a National Science Foundation 
Grant, NSF-ITR-EIA-0205448. Any opinions, 
findings, and conclusions or recommendations 
expressed in this material are those of the author(s) 
and do not necessarily reflect the views of the 
National Science Foundation. 
References  
Jinying Chen, Nianwen Xue and Martha Palmer. 
2004. Using a Smoothing Maximum Entropy 
Model for Chinese Nominal Entity Tagging. In 
Proceedings of the 1st Int. Joint Conference on 
Natural Language Processing. Hainan Island, 
China. 
Hoa Trang Dang, Ching-yi Chia, Martha Palmer, 
and Fu-Dong Chiou. 2002. Simple Features for 
Chinese Word Sense Disambiguation. In 
Proceedings of COLING-2002 Nineteenth Int. 
Conference on Computational Linguistics, 
Taipei, Aug.24?Sept.1.  
Christiane Fellbaum. 1998. WordNet ? an 
Electronic Lexical Database. The MIT Press, 
Cambridge, Massachusetts, London. 
Daniel Gildea and Daniel Jurafsky. 2002. 
Automatic Labeling of Semantic Roles. 
Computational Linguistics, 28(3): 245-288, 
2002.  
Patrick Hanks. 1996. Contextual dependencies and 
lexical sets. The Int. Journal of Corpus 
Linguistics, 1:1. 
Patrick Hanks. 1997. Lexical sets: relevance and 
probability. in B. Lewandowska-Tomaszczyk 
and M. Thelen (eds.) Translation and Meaning, 
Part 4, School of Translation and Interpreting, 
Maastricht, The Netherlands. 
Thomas Hofmann and Puzicha Jan. 1998. 
Statistical models for co-occurrence data, MIT 
Artificial Intelligence Lab., Technical Report 
AIM-1625. 
Adam Kilgarriff and Martha Palmer. 2000. 
Introduction to the sepcial issue on SENSEVAL. 
Computers and the Humanities, 34(1-2): 15-48. 
Martha Palmer, Hoa Trang Dang, and Christiane 
Fellbaum. To appear. Making fine-grained and 
coarse-grained sense distinctions, both manually 
and automatically. Natural Language 
Engineering.  
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn 
Carroll, and Franz Beil. 1998. EM-based 
clustering for NLP applications. AIMS Report 
4(3).Institut f?r Maschinelle Sprachverarbeitung.  
Sabine Schulte im Walde. 2000. Clustering verbs 
semantically according to their alternation 
behaviour. In Proceedings of the 18th Int. 
Conference on Computational Linguistics, 747-
753. 
Hinrich Sch?tze. 1998. Automatic Word Sense 
Discrimination. Computational Linguistics, 24 
(1): 97-124. 
Alexander Strehl. 2002. Relationship-based 
Clustering and Cluster Ensembles for High-
dimensional Data Mining. Dissertation. The 
University of Texas at Austin. http://www.lans. 
ece.utexas.edu/~strehl/diss/. 
Table 3.  Clustering performance with and 
without lexical sets for three Chinese verbs
Proceedings of the 43rd Annual Meeting of the ACL, pages 42?49,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Role of Semantic Roles in Disambiguating Verb Senses
Hoa Trang Dang
National Institute of Standards and Technology
Gaithersburg, MD 20899
hoa.dang@nist.gov
Martha Palmer
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
mpalmer@cis.upenn.edu
Abstract
We describe an automatic Word Sense
Disambiguation (WSD) system that dis-
ambiguates verb senses using syntactic
and semantic features that encode infor-
mation about predicate arguments and se-
mantic classes. Our system performs at
the best published accuracy on the English
verbs of Senseval-2. We also experiment
with using the gold-standard predicate-
argument labels from PropBank for dis-
ambiguating fine-grained WordNet senses
and course-grained PropBank framesets,
and show that disambiguation of verb
senses can be further improved with bet-
ter extraction of semantic roles.
1 Introduction
A word can have different meanings depending
on the context in which it is used. Word Sense
Disambiguation (WSD) is the task of determining
the correct meaning (?sense?) of a word in con-
text, and several efforts have been made to develop
automatic WSD systems. Early work on WSD
(Yarowsky, 1995) was successful for easily distin-
guishable homonyms like bank, which have multi-
ple unrelated meanings. While homonyms are fairly
tractable, highly polysemous verbs, which have re-
lated but subtly distinct senses, pose the greatest
challenge for WSD systems (Palmer et al, 2001).
Verbs are syntactically complex, and their syntax
is thought to be determined by their underlying se-
mantics (Grimshaw, 1990; Levin, 1993). Levin verb
classes, for example, are based on the ability of a
verb to occur in pairs of syntactic frames (diathe-
sis alternations); different senses of a verb belong to
different verb classes, which have different sets of
syntactic frames that are supposed to reflect under-
lying semantic components that constrain allowable
arguments. If this is true, then the correct sense of
a verb should be revealed (at least partially) in its
arguments.
In this paper we show that the performance of
automatic WSD systems can be improved by us-
ing richer linguistic features that capture informa-
tion about predicate arguments and their semantic
classes. We describe our approach to automatic
WSD of verbs using maximum entropy models to
combine information from lexical collocations, syn-
tax, and semantic class constraints on verb argu-
ments. The system performs at the best published
accuracy on the English verbs of the Senseval-2
(Palmer et al, 2001) exercise on evaluating au-
tomatic WSD systems. The Senseval-2 verb in-
stances have been manually tagged with their Word-
Net sense and come primarily from the Penn Tree-
bank WSJ. The WSJ corpus has also been manually
annotated for predicate arguments as part of Prop-
Bank (Kingsbury and Palmer, 2002), and the inter-
section of PropBank and Senseval-2 forms a corpus
containing gold-standard annotations of WordNet
senses and PropBank semantic role labels. This pro-
vides a unique opportunity to investigate the role of
predicate arguments in verb sense disambiguation.
We show that our system?s accuracy improves sig-
nificantly by adding features from PropBank, which
explicitly encodes the predicate-argument informa-
42
tion that our original set of syntactic and semantic
class features attempted to capture.
2 Basic automatic system
Our WSD system was built to combine information
from many different sources, using as much linguis-
tic knowledge as could be gathered automatically
by NLP tools. In particular, our goal was to see
the extent to which sense-tagging of verbs could be
improved by adding features that capture informa-
tion about predicate-arguments and selectional re-
strictions.
We used the Mallet toolkit (McCallum, 2002) for
learning maximum entropy models with Gaussian
priors for all our experiments. In order to extract
the linguistic features necessary for the models, all
sentences containing the target word were automat-
ically part-of-speech-tagged using a maximum en-
tropy tagger (Ratnaparkhi, 1998) and parsed using
the Collins parser (Collins, 1997). In addition, an
automatic named entity tagger (Bikel et al, 1997)
was run on the sentences to map proper nouns to a
small set of semantic classes.1
2.1 Topical features
We categorized the possible model features into top-
ical features and several types of local contextual
features. Topical features for a verb in a sentence
look for the presence of keywords occurring any-
where in the sentence and any surrounding sentences
provided as context (usually one or two sentences).
These features are supposed to show the domain in
which the verb is being used, since some verb senses
are used in only certain domains. The set of key-
words is specific to each verb lemma to be disam-
biguated and is determined automatically from train-
ing data so as to minimize the entropy of the proba-
bility of the senses conditioned on the keyword. All
alphabetic characters are converted to lower case.
Words occuring less than twice in the training data
or that are in a stoplist2 of pronouns, prepositions,
and conjunctions are ignored.
1The inclusion or omission of a particular company or prod-
uct implies neither endorsement nor criticism by NIST. Any
opinions, findings, and conclusions expressed are the authors?
own and do not necessarily reflect those of NIST.
2http://www.d.umn.edu/?tpederse/Group01/
WordNet/words.txt
2.2 Local features
The local features for a verb   in a particular sen-
tence tend to look only within the smallest clause
containing   . They include collocational features
requiring no linguistic preprocessing beyond part-
of-speech tagging, syntactic features that capture re-
lations between the verb and its complements, and
semantic features that incorporate information about
noun classes for subjects and objects:
Collocational features: Collocational features re-
fer to ordered sequences of part-of-speech tags or
word tokens immediately surrounding   . They in-
clude:
 unigrams: words   ,   ,  	 ,  
 ,  
 and
parts of speech   ,   ,   ,  
 ,  
 , where
  and   are at position  relative to  
 bigrams:    ,   
 ,  
 
 ;




, 




, 





 trigrams:     ,    
 ,
  
 

,
 
 
 
 ;       ,







, 







, 








Syntactic features: The system uses heuristics to
extract syntactic elements from the parse for the sen-
tence containing   . Let commander VP be the low-
est VP that dominates   and that is not immediately
dominated by another VP, and let head VP be the
lowest VP dominating   (See Figure 1). Then we
define the subject of   to be the leftmost NP sib-
ling of commander VP, and a complement of   to
be a node that is a child of the head VP, excluding
NPs whose head is a number or a noun from a list
of common temporal nouns (?week?, ?tomorrow?,
?Monday?, etc.). The system extracts the following
binary syntactic features:
 Is the sentence passive?
 Is there a subject, direct object (leftmost NP
complement of   ), indirect object (second left-
most NP complement of   ), or clausal comple-
ment (S complement of   )?
 What is the word (if any) that is the particle
or head of the subject, direct object, or indirect
object?
43
SNP
John
(commander) VP
VB
had
(head) VP
VB
pulled
NP
the blanket
PP
across the carpet
S
to create static
Figure 1: Example parse tree for   =?pulled?, from which is extracted the syntactic features: morph=normal
subj dobj sent-comp subj=john dobj=blanket prep=across across-obj=carpet.
 If there is a PP complement, what is the prepo-
sition, and what is the object of the preposition?
Semantic features:
 What is the Named Entity tag (PERSON, OR-
GANIZATION, LOCATION, UNKNOWN)
for each proper noun in the syntactic positions
above?
 What are the possible WordNet synsets and hy-
pernyms for each noun in the syntactic posi-
tions above? (Nouns are not explicitly disam-
biguated; all possible synsets and hypernyms
for the noun are included.)
This set of local features relies on access to syn-
tactic structure as well as semantic class informa-
tion, and attempts to model richer linguistic infor-
mation about predicate arguments. However, the
heuristics for extracting the syntactic features are
able to identify subjects and objects of only simple
clauses. The heuristics also do not differentiate be-
tween arguments and adjuncts; for example, the fea-
ture sent-comp is intended to identify clausal com-
plements such as in (S (NP Mary) (VP (VB called)
(S him a bastard))), but Figure 1 shows how a pur-
pose clause can be mistakenly labeled as a clausal
complement.
2.3 Evaluation
We tested the system on the 1806 test instances of
the 29 verbs from the English lexical sample task for
Senseval-2 (Palmer et al, 2001). Accuracy was de-
fined to be the fraction of the instances for which the
system got the correct sense. All significance testing
between different accuracies was done using a one-
tailed z-test, assuming a binomial distribution of the
successes; differences in accuracy were considered
to be significant if ffProceedings of the 43rd Annual Meeting of the ACL, pages 541?548,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Machine Translation Using Probabilistic 
Synchronous Dependency Insertion Grammars 
 
Yuan Ding Martha Palmer 
Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA 19104, USA 
{yding, mpalmer}@linc.cis.upenn.edu 
 
Abstract 
Syntax-based statistical machine transla-
tion (MT) aims at applying statistical 
models to structured data. In this paper, 
we present a syntax-based statistical ma-
chine translation system based on a prob-
abilistic synchronous dependency 
insertion grammar. Synchronous depend-
ency insertion grammars are a version of 
synchronous grammars defined on de-
pendency trees. We first introduce our 
approach to inducing such a grammar 
from parallel corpora. Second, we de-
scribe the graphical model for the ma-
chine translation task, which can also be 
viewed as a stochastic tree-to-tree trans-
ducer. We introduce a polynomial time 
decoding algorithm for the model. We 
evaluate the outputs of our MT system us-
ing the NIST and Bleu automatic MT 
evaluation software. The result shows that 
our system outperforms the baseline sys-
tem based on the IBM models in both 
translation speed and quality. 
1 Introduction 
Statistical approaches to machine translation, pio-
neered by (Brown et al, 1993), achieved impres-
sive performance by leveraging large amounts of 
parallel corpora. Such approaches, which are es-
sentially stochastic string-to-string transducers, do 
not explicitly model natural language syntax or 
semantics. In reality, pure statistical systems some-
times suffer from ungrammatical outputs, which 
are understandable at the phrasal level but some-
times hard to comprehend as a coherent sentence. 
In recent years, syntax-based statistical machine 
translation, which aims at applying statistical mod-
els to structural data, has begun to emerge. With 
the research advances in natural language parsing, 
especially the broad-coverage parsers trained from 
treebanks, for example (Collins, 1999), the utiliza-
tion of structural analysis of different languages 
has been made possible. Ideally, by combining the 
natural language syntax and machine learning 
methods, a broad-coverage and linguistically well-
motivated statistical MT system can be constructed. 
However, structural divergences between lan-
guages (Dorr, 1994)?which are due to either sys-
tematic differences between languages or loose 
translations in real corpora?pose a major chal-
lenge to syntax-based statistical MT. As a result, 
the syntax based MT systems have to transduce 
between non-isomorphic tree structures. 
(Wu, 1997) introduced a polynomial-time solu-
tion for the alignment problem based on synchro-
nous binary trees. (Alshawi et al, 2000) represents 
each production in parallel dependency trees as a 
finite-state transducer.  Both approaches learn the 
tree representations directly from parallel sen-
tences, and do not make allowances for non-
isomorphic structures.  (Yamada and Knight, 2001, 
2002) modeled translation as a sequence of tree 
operations transforming a syntactic tree into a 
string of the target language.  
When researchers try to use syntax trees in both 
languages, the problem of non-isomorphism must 
be addressed. In theory, stochastic tree transducers 
and some versions of synchronous grammars pro-
vide solutions for the non-isomorphic tree based 
transduction problem and hence possible solutions 
for MT. Synchronous Tree Adjoining Grammars, 
proposed by (Shieber and Schabes, 1990), were 
introduced primarily for semantics but were later 
also proposed for translation. Eisner (2003) pro-
posed viewing the MT problem as a probabilistic 
synchronous tree substitution grammar parsing 
541
problem. Melamed (2003, 2004) formalized the 
MT problem as synchronous parsing based on 
multitext grammars. Graehl and Knight (2004) de-
fined training and decoding algorithms for both 
generalized tree-to-tree and tree-to-string transduc-
ers. All these approaches, though different in for-
malism, model the two languages using tree-based 
transduction rules or a synchronous grammar, pos-
sibly probabilistic, and using multi-lemma elemen-
tary structures as atomic units. The machine 
translation is done either as a stochastic tree-to-tree 
transduction or a synchronous parsing process. 
However, few of the above mentioned formal-
isms have large scale implementations. And to the 
best of our knowledge, the advantages of syntax 
based statistical MT systems over pure statistical 
MT systems have yet to be empirically verified. 
We believe difficulties in inducing a synchro-
nous grammar or a set of tree transduction rules 
from large scale parallel corpora are caused by:  
1. The abilities of synchronous grammars and 
tree transducers to handle non-isomorphism 
are limited. At some level, a synchronous 
derivation process must exist between the 
source and target language sentences.  
2. The training and/or induction of a synchro-
nous grammar or a set of transduction rules 
are usually computationally expensive if all 
the possible operations and elementary struc-
tures are allowed. The exhaustive search for 
all the possible sub-sentential structures in a 
syntax tree of a sentence is NP-complete. 
3. The problem is aggravated by the non-perfect 
training corpora. Loose translations are less of 
a problem for string based approaches than for 
approaches that require syntactic analysis. 
Hajic et al (2002) limited non-isomorphism by 
n-to-m matching of nodes in the two trees.  How-
ever, even after extending this model by allowing 
cloning operations on subtrees, Gildea (2003) 
found that parallel trees over-constrained the 
alignment problem, and achieved better results 
with a tree-to-string model than with a tree-to-tree 
model using two trees. In a different approach, 
Hwa et al (2002) aligned the parallel sentences 
using phrase based statistical MT models and then 
projected the alignments back to the parse trees. 
This motivated us to look for a more efficient 
and effective way to induce a synchronous gram-
mar from parallel corpora and to build an MT sys-
tem that performs competitively with the pure 
statistical MT systems. We chose to build the syn-
chronous grammar on the parallel dependency 
structures of the sentences. The synchronous 
grammar is induced by hierarchical tree partition-
ing operations. The rest of this paper describes the 
system details as follows: Sections 2 and 3 de-
scribe the motivation behind the usage of depend-
ency structures and how a version of synchronous  
dependency grammar is learned. This grammar is 
used as the primary translation knowledge source 
for our system. Section 4 defines the tree-to-tree 
transducer and the graphical model for the stochas-
tic tree-to-tree transduction process and introduces 
a polynomial time decoding algorithm for the 
transducer.  We evaluate our system in section 5 
with the NIST/Bleu automatic MT evaluation 
software and the results are discussed in Section 6. 
2 The Synchronous Grammar 
2.1 Why Dependency Structures? 
According to Fox (2002), dependency representa-
tions have the best inter-lingual phrasal cohesion 
properties. The percentage for head crossings is 
12.62% and that of modifier crossings is 9.22%. 
Furthermore, a grammar based on dependency 
structures has the advantage of being simple in 
formalism yet having CFG equivalent formal gen-
erative capacity (Ding and Palmer, 2004b). 
Dependency structures are inherently lexical-
ized as each node is one word. In comparison, 
phrasal structures (treebank style trees) have two 
node types: terminals store the lexical items and 
non-terminals store word order and phrasal scopes. 
2.2 Synchronous Dependency Insertion Grammars 
Ding and Palmer (2004b) described one version of 
synchronous grammar: Synchronous Dependency 
Insertion Grammars. A Dependency Insertion 
Grammars (DIG) is a generative grammar formal-
ism that captures word order phenomena within the 
dependency representation. In the scenario of two 
languages, the two sentences in the source and tar-
get languages can be modeled as being generated 
from a synchronous derivation process. 
A synchronous derivation process for the two 
syntactic structures of both languages suggests the 
level of cross-lingual isomorphism between the 
two trees (e.g. Synchronous Tree Adjoining 
Grammars (Shieber and Schabes, 1990)). 
542
Apart from other details, a DIG can be viewed 
as a tree substitution grammar defined on depend-
ency trees (as opposed to phrasal structure trees). 
The basic units of the grammar are elementary 
trees (ET), which are sub-sentential dependency 
structures containing one or more lexical items. 
The synchronous version, SDIG, assumes that the 
isomorphism of the two syntactic structures is at 
the ET level, rather than at the word level, hence 
allowing non-isomorphic tree to tree mapping. 
We illustrate how the SDIG works using the 
following pseudo-translation example: 
y [Source] The girl kissed her kitty cat. 
y [Target] The girl gave a kiss to her cat. 
 
Figure 1.
An example
 
Figure 2. 
Tree-to-tree 
transduction
Almost any tree-transduction operations de-
fined on a single node will fail to generate the tar-
get sentence from the source sentence without 
using insertion/deletion operations. However, if we 
view each dependency tree as an assembly of indi-
visible sub-sentential elementary trees (ETs), we 
can find a proper way to transduce the input tree to 
the output tree. An ET is a single ?symbol? in a 
transducer?s language. As shown in Figure 2, each 
circle stands for an ET and thick arrows denote the 
transduction of each ET as a single symbol. 
3 Inducing a Synchronous Dependency 
Insertion Grammar 
As the start to our syntax-based SMT system, the 
SDIG must be learned from the parallel corpora.  
3.1 Cross-lingual Dependency Inconsistencies 
One straightforward way to induce a generative 
grammar is using EM style estimation on the gen-
erative process. Different versions of such training 
algorithms can be found in (Hajic et al, 2002; Eis-
ner 2003; Gildea 2003; Graehl and Knight 2004). 
However, a synchronous derivation process 
cannot handle two types of cross-language map-
pings: crossing-dependencies (parent-descendent 
switch) and broken dependencies (descendent ap-
pears elsewhere), which are illustrated below: 
 
Figure 3. Cross-lingual dependency consistencies 
In the above graph, the two sides are English 
and the foreign dependency trees. Each node in a 
tree stands for a lemma in a dependency tree. The 
arrows denote aligned nodes and those resulting 
inconsistent dependencies are marked with a ?*?.  
Fox (2002) collected the statistics mainly on 
French and English data: in dependency represen-
tations, the percentage of head crossings per 
chance (case [b] in the graph) is 12.62%.  
Using the statistics on cross-lingual dependency 
consistencies from a small word to word aligned 
Chinese-English parallel corpus1, we found that the 
percentage of crossing-dependencies (case [b]) 
between Chinese and English is 4.7% while that of 
broken dependencies (case [c]) is 59.3%. 
The large number of broken dependencies pre-
sents a major challenge for grammar induction 
based on a top-down style EM learning process. 
Such broken and crossing dependencies can be 
modeled by SDIG if they appear inside a pair of 
elementary trees. However, if they appear between 
the elementary trees, they are not compatible with 
the isomorphism assumption on which SDIG is 
based. Nevertheless, the hope is that the fact that 
the training corpus contains a significant percent-
age of dependency inconsistencies does not mean 
that during decoding the target language sentence 
cannot be written in a dependency consistent way. 
3.2 Grammar Induction by Synchronous  
Hierarchical Tree Partitioning 
(Ding and Palmer, 2004a) gave a polynomial time 
solution for learning parallel sub-sentential de-
                                                           
1  Total 826 sentence pairs, 9957 Chinese words, 12660 Eng-
lish words. Data made available by the courtesy of Microsoft 
Research, Asia and IBM T.J. Watson Research. 
543
pendency structures from non-isomorphic depend-
ency trees. Our approach, while similar to (Ding 
and Palmer, 2004a) in that we also iteratively parti-
tion the parallel dependency trees based on a heu-
ristic function, departs (Ding and Palmer, 2004a) 
in three ways: (1) we base the hierarchical tree par-
titioning operations on the categories of the de-
pendency trees; (2) the statistics of the resultant 
tree pairs from the partitioning operation are col-
lected at each iteration rather than at the end of the 
algorithm; (3) we do not re-train the word to word 
probabilities at each iteration. Our grammar induc-
tion algorithm is sketched below: 
Step 0. View each tree as a ?bag of words? and train a 
statistical translation model on all the tree pairs to 
acquire word-to-word translation probabilities. In 
our implementation, the IBM Model 1 (Brown et 
al., 1993) is used. 
Step 1. Let i  denote the current iteration and let 
[ ]C CategorySequence i=  be the current syntac-
tic category set. 
For each tree pair in the corpus, do { 
a) For the tentative synchronous partitioning opera-
tion, use a heuristic function to select the BEST word 
pair * *( , )i je f , where both * *,i je f  are NOT ?chosen?,  
*( )iCategory e C?  and *( )jCategory f C? . 
b) If * *( , )i je f  is found in (a), mark * *,i je f  as ?cho-
sen? and go back to (a), else go to (c). 
c) Execute the synchronous tree partitioning opera-
tion on all the ?chosen? word pairs on the tree pair. 
Hence, several new tree pairs are created. Replace the 
old tree pair with the new tree pairs together with the 
rest of the old tree pair. 
d) Collect the statistics for all the new tree pairs as 
elementary tree pairs. } 
Step 2. 1i i= + . Go to Step 1 for the next iteration. 
At each iteration, one specific set of categories 
of nodes is handled. The category sequence we 
used in the grammar induction is:  
1. Top-NP: the noun phrases that do not have 
another noun phrase as parent or ancestor. 
2. NP: all the noun phrases 
3. VP, IP, S, SBAR:  verb phrases equivalents. 
4. PP, ADJP, ADVP, JJ, RB: all the modifiers 
5. CD: all the numbers. 
We first process top NP chunks because they are 
the most stable between languages. Interestingly, 
NPs are also used as anchor points to learn mono-
lingual paraphrases (Ibrahim et al, 2003). The 
phrasal structure categories can be extracted from 
automatic parsers using methods in (Xia, 2001). 
An illustration is given below (Chinese in pin-
yin form). The placement of the dependency arcs 
reflects the relative word order between a parent 
node and all its immediate children. The collected 
ETs are put into square boxes and the partitioning 
operations taken are marked with dotted arrows. 
y [English]   I have been in Canada since 1947. 
y [Chinese]  Wo 1947 nian yilai  yizhi   zhu  zai  jianada. 
y [Glossary]  I   1947 year since always live in  Canada 
[ ITERATION 1 & 2 ] Partition at word pair  
(?I? and ?wo?) (?Canada? and ?janada?) 
 
[ ITERATION 3 ] (?been? and ?zhu?) are chosen but no 
partition operation is taken because they are roots. 
[ ITERATION 4 ] Partition at word pair  
(?since? and ?yilai?) (?in? and ?zai?) 
 
[ ITERATION 5 ] Partition at ?1947? and ?1947? 
 
[ FINALLY ] Total of 6 resultant ET pairs (figure omitted) 
Figure 4. An Example 
3.3 Heuristics 
Similar to (Ding and Palmer, 2004a), we also use a 
heuristic function in Step 1(a) of the algorithm to 
rank all the word pairs for the tentative tree parti-
544
tioning operation. The heuristic function is based 
on a set of heuristics, most of which are similar to 
those in (Ding and Palmer, 2004a).  
For a word pair ( , )i je f for the tentative parti-
tioning operation, we briefly describe the heuristics: 
y Inside-outside probabilities: We borrow the 
idea from PCFG parsing. This is the probabil-
ity of an English subtree (inside) generating a 
foreign subtree and the probability of the Eng-
lish residual tree (outside) generating a for-
eign residual tree. Here both probabilities are 
based on a ?bag of words? model. 
y Inside-outside penalties: here the probabilities 
of the inside English subtree generating the 
outside foreign residual tree and outside Eng-
lish residual tree generating the inside English 
subtree are used as penalty terms. 
y Entropy: the entropy of the word to word 
translation probability of the English word ie . 
y Part-of-Speech mapping template: whether the 
POS tags of the two words are in the ?highly 
likely to match? POS tag pairs. 
y Word translation probability: P( | )j if e . 
y Rank: the rank of the word to word probabil-
ity of jf  in as a translation of ie  among all 
the foreign words in the current tree. 
The above heuristics are a set of real valued 
numbers. We use a Maximum Entropy model to 
interpolate the heuristics in a log-linear fashion, 
which is different from the error minimization 
training in (Ding and Palmer, 2004a).  ( )0 1P | ( , ), ( , )... ( , )
1 exp ( , )
i j i j n i j
k k i j s
k
y h e f h e f h e f
h e f
Z
? ?? ?= +? ?? ??
  (1) 
where (0,1)y =  as labeled in the training data 
whether the two words are mapped with each other. 
The MaxEnt model is trained using the same 
word level aligned parallel corpus as the one in 
Section 3.1. Although the training corpus isn?t 
large, the fact that we only have a handful of pa-
rameters to fit eased the problem.  
3.4 A Scaled-down SDIG 
It is worth noting that the set of derived parallel 
dependency Elementary Trees is not a full-fledged 
SDIG yet. Many features in the SDIG formalism 
such as arguments, head percolation, etc. are not 
yet filled. We nevertheless use this derived gram-
mar as a Mini-SDIG, assuming the unfilled fea-
tures as empty by default. A full-fledged SDIG 
remains a goal for future research. 
4 The Machine Translation System 
4.1 System Architecture 
As discussed before (see Figure 1 and 2), the archi-
tecture of our syntax based statistical MT system is 
illustrated in Figure 5. Note that this is a non-
deterministic process. The input sentence is first 
parsed using an automatic parser and a dependency 
tree is derived. The rest of the pipeline can be 
viewed as a stochastic tree transducer. The MT 
decoding starts first by decomposing the input de-
pendency tree in to elementary trees. Several dif-
ferent results of the decomposition are possible. 
Each decomposition is indeed a derivation process 
on the foreign side of SDIG. Then the elementary 
trees go through a transfer phase and target ETs are 
combined together into the output. 
 
Figure 5. System architecture 
4.2 The Graphical Model 
The stochastic tree-to-tree transducer we propose 
models MT as a probabilistic optimization process. 
Let f  be the input sentence (foreign language), 
and e  be the output sentence (English). We have 
P( | ) P( )P( | )
P( )
f e ee f
f
= , and the best translation is: 
 * arg max P( | )P( )
e
e f e e=    (2) 
P( | )f e  and P( )e  are also known as the ?trans-
lation model? (TM) and the ?language model? 
(LM). Assuming the decomposition of the foreign 
tree is given, our approach, which is based on ETs, 
uses the graphical model shown in Figure 6. 
In the model, the left side is the input depend-
ency tree (foreign language) and the right side is 
the output dependency tree (English). Each circle 
stands for an ET. The solid lines denote the syntac-
tical dependencies while the dashed arrows denote 
the statistical dependencies. 
545
 Figure 6 
The graphical 
model 
Let T( )x be the dependency tree constructed 
from sentence x . A tree-decomposition function  
D( )t  is defined on a dependency tree t , and out-
puts a certain ET derivation tree of  t , which is 
generated by decomposing t  into ETs. Given t , 
there could be multiple decompositions. Condi-
tioned on decomposition D , we can rewrite (2) as: 
* arg max P( , | )P( )
arg max P( | , )P( | )P( )
e D
e D
e f e D D
f e D e D D
=
=
?
?  (3) 
By definition, the ET derivation trees of the in-
put and output trees should be isomorphic: 
D(T( )) D(T( ))f e? . Let Tran( )u  be a set of possi-
ble translations for the ET u . We have: 
D(T( )), D(T( )), Tran( )
P( | , ) P(T( ) | P(T( ), )
P( | )
u f v e v u
f e D f e D
u v
? ? ?
=
= ?           (4) 
For any ET v  in a given ET derivation tree d , 
let Root( )d  be the root ET of d , and let 
Parent( )v  denote the parent ET of  v . We have: 
( )( )
D(T( )), Root(D(T( ))
P( | ) P(T( ) | )
P Root D(T( )
P( | Parent( ))
v e v e
e D e D
e
v v
? ?
=
= ?
? ??? ?? ??
 (5) 
where, letting root( )v  denote the root word of v , 
( ) ( )( )P | Parent( ) P root( ) | root Parent( )v v v v=  (6) 
The prior probability of tree decomposition is 
defined as: ( )
D(T( ))
P D(T( )) P( )
u f
f u
?
= ?   (7) 
Figure 7 
 Comparing to 
the HMM 
An analogy between our model and a Hidden 
Markov Model (Figure 7) may be helpful. In Eq. 
(4), P( | )u v  is analogous to the emission probably 
P( | )i io s  in an HMM. In Eq. (5), P( | Parent( ))v v  is 
analogous to the transition probability 1P( | )i is s ?  in 
an HMM. While HMM is defined on a sequence 
our model is defined on the derivation tree of ETs. 
4.3 Other Factors 
y Augmenting parallel ET pairs 
In reality, the learned parallel ETs are unlikely to 
cover all the structures that we may encounter in 
decoding. As a unified approach, we augment the 
SDIG by adding all the possible word pairs ( , )j if e   
as a parallel ET pair and using the IBM Model 1 
(Brown et al, 1993) word to word translation 
probability as the ET translation probability. 
y Smoothing the ET translation probabilities. 
The LM probabilities P( | Parent( ))v v  are simply 
estimated using the relative frequencies. In order to 
handle possible noise from the ET pair learning 
process, the ET translation probabilities P ( | )emp u v  
estimated by relative frequencies are smoothed 
using a word level model. For each ET pair ( , )u v , 
we interpolate the empirical probability with the 
?bag of words? probability and then re-normalize: 
size( )
1 1P( | ) P ( , ) P( | )
size( )
ij
emp j iv
e vf u
u v u v f e
Z u ??
= ? ??  (8) 
4.4 Polynomial Time Decoding 
For efficiency reasons, we use maximum approxi-
mation for (3). Instead of summing over all the 
possible decompositions, we only search for the 
best decomposition as follows: 
,
*, * arg max P( | , )P( | )P( )
e D
e D f e D e D D=  (9) 
So bringing equations (4) to (9) together, the 
best translation would maximize: 
( )P( | ) P Root( ) P( | Parent( )) P( )u v e v v u? ?? ? ?? ?? ?? ? ? (10) 
Observing the similarity between our model 
and a HMM, our dynamic programming decoding 
algorithm is in spirit similar to the Viterbi algo-
rithm except that instead of being sequential the 
decoding is done on trees in a top down fashion. 
As to the relative orders of the ETs, we cur-
rently choose not to reorder the children ETs given 
the parent ET because: (1) the permutation of the 
ETs is computationally expensive (2) it is possible 
that we can resort to simple linguistic treatments 
on the output dependency tree to order the ETs. 
Currently, all the ETs are attached to each other 
546
at their root nodes. 
In our implementation, the different decomposi-
tions of the input dependency tree are stored in a 
shared forest structure, utilizing the dynamic pro-
gramming property of the tree structures explicitly. 
Suppose the input sentence has n  words and 
the shared forest representation has m  nodes. 
Suppose for each word, there are maximally k  
different ETs containing it, we have knm ? . Let 
b  be the max breadth factor in the packed forest, it 
can be shown that the decoder visits at most mb  
nodes during execution. Hence, we have: 
)()( kbnOdecodingT ?             (11) 
which is linear to the input size. Combined with a 
polynomial time parsing algorithm, the whole 
decoding process is polynomial time. 
5 Evaluation  
We implemented the above approach for a Chi-
nese-English machine translation system. We used 
an automatic syntactic parser (Bikel, 2002) to pro-
duce the parallel parse trees. The parser was 
trained using the Penn English/Chinese Treebanks. 
We then used the algorithm in (Xia 2001) to con-
vert the phrasal structure trees to dependency trees 
to acquire the parallel dependency trees. The statis-
tics of the datasets we used are shown as follows: 
Dataset Xinhua FBIS NIST 
Sentence# 56263 45212 206 
Chinese word# 1456495 1185297 27.4 average
English word# 1490498 1611932 37.7 average
Usage training training testing 
Figure 8. Evaluation data details 
 The training set consists of Xinhua newswire 
data from LDC and the FBIS data (mostly news), 
both filtered to ensure parallel sentence pair quality. 
We used the development test data from the 2001 
NIST MT evaluation workshop as our test data for 
the MT system performance. In the testing data, 
each input Chinese sentence has 4 English transla-
tions as references. Our MT system was evaluated 
using the n-gram based Bleu (Papineni et al, 2002) 
and NIST machine translation evaluation software. 
We used the NIST software package ?mteval? ver-
sion 11a, configured as case-insensitive. 
In comparison, we deployed the GIZA++ MT 
modeling tool kit, which is an implementation of 
the IBM Models 1 to 4 (Brown et al, 1993; Al-
Onaizan et al, 1999; Och and Ney, 2003). The 
IBM models were trained on the same training data 
as our system. We used the ISI Rewrite decoder 
(Germann et al 2001) to decode the IBM models. 
The results are shown in Figure 9. The score 
types ?I? and ?C? stand for individual and cumula-
tive n-gram scores. The final NIST and Bleu scores 
are marked with bold fonts.  
Systems Score Type 1-gram 2-gram 3-gram 4-gram
NIST 2.562 0.412 0.051 0.008I Bleu 0.714 0.267 0.099 0.040
NIST 2.562 2.974 3.025 3.034
IBM 
Model 4 C Bleu 0.470 0.287 0.175 0.109
NIST 5.130 0.763 0.082 0.013I Bleu 0.688 0.224 0.075 0.029
NIST 5.130 5.892 5.978 5.987
SDIG
C Bleu 0.674 0.384 0.221 0.132
Figure 9. Evaluation Results. 
The evaluation results show that the NIST score 
achieved a 97.3% increase, while the Bleu score 
increased by 21.1%. 
In terms of decoding speed, the Rewrite de-
coder took 8102 seconds to decode the test sen-
tences on a Xeon 1.2GHz 2GB memory machine. 
On the same machine, the SDIG decoder took 3 
seconds to decode, excluding the parsing time. The 
recent advances in parsing have achieved parsers 
with 3( )O n  time complexity without the grammar 
constant (McDonald et al, 2005). It can be ex-
pected that the total decoding time for SDIG can 
be as short as 0.1 second per sentence. 
Neither of the two systems has any specific 
translation components, which are usually present 
in real world systems (E.g. components that trans-
late numbers, dates, names, etc.) It is reasonable to 
expect that the performance of SDIG can be further 
improved with such specific optimizations. 
6 Discussions 
We noticed that the SDIG system outputs tend to 
be longer than those of the IBM Model 4 system, 
and are closer to human translations in length. 
Translation Type Human SDIG IBM-4
Avg. Sent. Len. 37.7 33.6 24.2 
Figure 10. Average Sentence Word Count 
This partly explains why the IBM Model 4 system 
has slightly higher individual n-gram precision 
scores (while the SDIG system outputs are still 
better in terms of absolute matches).  
547
The relative orders between the parent and child 
ETs in the output tree is currently kept the same as 
the orders in the input tree. Admittedly, we bene-
fited from the fact that both Chinese and English 
are SVO languages, and that many of orderings 
between the arguments and adjuncts can be kept 
the same. However, we did notice that this simple 
?ostrich? treatment caused outputs such as ?foreign 
financial institutions the president of?. 
While statistical modeling of children reorder-
ing is one possible remedy for this problem, we 
believe simple linguistic treatment is another, as 
the output of the SDIG system is an English 
dependency tree rather than a string of words. 
7 Conclusions and Future Work 
In this paper we presented a syntax-based statisti-
cal MT system based on a Synchronous Depend-
ency Insertion Grammar and a non-isomorphic 
stochastic tree-to-tree transducer. A graphical 
model for the transducer is defined and a polyno-
mial time decoding algorithm is introduced. The 
results of our current implementation were evalu-
ated using the NIST and Bleu automatic MT 
evaluation software. The evaluation shows that the 
SDIG system outperforms an IBM Model 4 based 
system in both speed and quality. 
Future work includes a full-fledged version of 
SDIG and a more sophisticated MT pipeline with 
possibly a tri-gram language model for decoding. 
References  
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, 
I. D. Melamed, F. Och, D. Purdy, N. A. Smith, and D. 
Yarowsky. 1999. Statistical machine translation. 
Technical report, CLSP, Johns Hopkins University.  
H. Alshawi, S. Bangalore, S. Douglas. 2000. Learning 
dependency translation models as collections of finite 
state head transducers. Comp. Linguistics, 26(1):45-60. 
Daniel M. Bikel. 2002. Design of a multi-lingual, paral-
lel-processing statistical parsing engine. In HLT 2002. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2): 263-311. 
Michael John Collins. 1999. Head-driven Statistical 
Models for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania, Philadelphia. 
Ding and Palmer. 2004a. Automatic Learning of Paral-
lel Dependency Treelet Pairs. In First International 
Joint Conference on NLP (IJCNLP-04). 
Ding and Palmer. 2004b. Synchronous Dependency 
Insertion Grammars: A Grammar Formalism for Syn-
tax Based Statistical MT. Workshop on Recent Ad-
vances in Dependency Grammars, COLING-04. 
Bonnie J. Dorr. 1994. Machine translation divergences: 
A formal description and proposed solution. Compu-
tational Linguistics, 20(4): 597-633. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In ACL-03. (compan-
ion volume), Sapporo, July. 
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP-02. 
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel 
Marcu, and Kenji Yamada. 2001. Fast Decoding and 
Optimal Decoding for Machine Translation. ACL-01. 
Daniel Gildea. 2003. Loosely tree based alignment for 
machine translation. ACL-03, Japan. 
Jonathan Graehl and Kevin Knight. 2004. Training Tree 
Transducers. In NAACL/HLT-2004 
Jan Hajic, et al 2002. Natural language generation in 
the context of machine translation. Summer workshop 
final report, Center for Language and Speech Process-
ing, Johns Hopkins University, Baltimore.  
Rebecca Hwa, Philip S. Resnik, Amy Weinberg, and 
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. ACL-02 
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolin-
gual Corpora. In Proceedings of the Second 
International Workshop on Paraphrasing (IWP 2003) 
Dan Melamed. 2004. Statistical Machine Translation by 
Parsing. In ACL-04, Barcelona, Spain. 
Dan Melamed. 2003. Multitext Grammars and Synchro-
nous Parsers, In NAACL/HLT-2003. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
BLEU: a method for automatic evaluation of machine 
translation. ACL-02, Philadelphia, USA. 
Ryan McDonald, Koby Crammer and Fernando Pereira. 
2005. Online Large-Margin Training of Dependency 
Parsers. ACL-05. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1):19?51. 
S. M. Shieber and Y. Schabes. 1990. Synchronous Tree-
Adjoining Grammars, Proceedings of the 13th 
COLING, pp. 253-258, August 1990. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):3-403. 
Fei Xia. 2001. Automatic grammar generation from two 
different perspectives. PhD thesis, U. of Pennsylvania. 
Kenji Yamada and Kevin Knight. 2001. A syntax based 
statistical translation model. ACL-01, France. 
Kenji Yamada and Kevin Knight. 2002. A decoder for 
syntax-based statistical MT. ACL-02, Philadelphia. 
548
Towards Translingual Information Access 
using Portable Information Extraction 
Michael White, Claire Cardie, Chung-hye Han, Nari Kim, # 
Benoit Lavoie, Martha Palmer, Owen Rainbow,* Juntae Yoon 
CoGenTex, Inc. 
Ithaca, NY, USA 
\[mike,benoit.owen\] 
@cogentex.com 
Institute for Research in 
Cognitive Science 
University of Pennsylvania 
Philadelphia, PA, USA 
chunghye@babel, ling. upenn, edu 
\[ nari, mpalmer, j tyoon } 
@linc. cis.upenn.edu 
Dept. of Computer Science 
Cornell University 
Ithaca, NY, USA 
cardie@cs, cornell, edu 
Abstract 
We report on a small study undertaken to 
demonstrate the feasibility of combining 
portable information extraction with MT in 
order to support translingual information 
access. After describing the proposed 
system's usage scenario and system design, 
we describe our investigation of transferring 
information extraction techniques developed 
for English to Korean. We conclude with a 
brief discussion of related MT issues we plan 
to investigate in future work. 
1 Introduction 
In this paper, we report on a small study 
undertaken to demonstrate the feasibility of 
combining portable information extraction with 
MT in order to support ranslingual information 
access. The goal of our proposed system is to 
better enable analysts to perform information 
filtering tasks on foreign language documents. 
This effort was funded by a SBIR Phase I award 
from the U.S. Army Research Lab, and will be 
pursued further under the DARPA TIDES 
initiative. 
Information extraction (IE) systems are 
designed to extract specific types of information 
from natural language texts. In order to achieve 
acceptable accuracy, IE systems need to be 
tuned for a given topic domain. Since this 
domain tuning can be labor intensive, recent IE 
research has focused on developing learning 
algorithms for training IE system components 
(cf. Cardie, 1997, for a survey). To date, 
however, little work has been done on IE 
systems for languages other than English 
(though cf. MUC-5, 1994, and MUC-7, 1998, 
for Japanese IE systems); and, to our knowledge, 
none of the available techniques for the core task 
of learning information extraction patterns have 
been extended or evaluated for multilingual 
information extraction (though again cf. MUC-7, 
1998, where the use of learning techniques for 
the IE subtasks of named entity recognition and 
coreference r solution are described). 
Given this situation, the primary objective of 
our study was to demonstrate he feasibility of 
using portable--i.e., easily trainable--IE 
technology on Korean documents, focusing on 
techniques for learning information extraction 
patterns. Secondary objectives of the study were 
to elaborate the analyst scenario and system 
design. 
2 Analyst Scenario 
Figure 1 illustrates how an intelligence analyst 
might use the proposed system: 
? The analyst selects one or more Korean 
documents in which to search for 
information (this step not shown). 
# Current affiliation: Konan Technology, Inc., Korea, nari@konantech.co.kr 
* Current affiliation: A'IT Labs-Research, Florham Park, NJ, USA, rambow@research.att.com 
31 
Ouery  
Find Report 
Event: Nest !!lg ........... 
sourcn:l . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ ~ 
sate :  I ................. ' ......................... ' ................. i 
Locat Ion: I~u.'~h K..e~.e..a.; ..................................... j I~ 
Part clpant : I .................................................................. i 
Iseun:i~'North Korea" AND "missiles" i 
I 
Response  to  Ouery  
The reports Indicate 2 meetings held In South Korea on the 
issues of North Korea anti missiles: 
Sources Translated Extracts 
Joon,ap~l A ~ ~  
. . . .  I ,4 ~t ln ,  g ~# ,#~=1# o,1 Apf J l  ~ sLYout tP~ I10t 
,!nerF, orea j /ine? ~t~wn Saoul end Tokyo for  the 
Noes / - -  t~Q I ela~gen?? d~/tu~tlons ~uc/t eg Alottl I  Kofgm'~ 
Trans la t ion  o f  Korean  Source  Repor t  
\[Joongang Dally\] 
Korean. Japanese H in i s ters  Discuss NK Po l i cy  
The tmo ministers ~9rsed that any further launching of a 
missile by North Korean would undermine the security of 
~Northeast Asia and the Korea, the United States and Japan 
should take Joint steps against the North Korean missile 
threat. 
}-long requested that Koeura cork to normalize Japan's 
relations with North Korea. rather than cutting channels 
of dialogue bet#men the two countries. 
Koeura said that i f  North Korea continues Its missile 
testing, the Japanese government will definitely stop 
making contributions to KEDO. 
The tee ministers also tentatively agreed that J~anese 
primo minister Kslzo Obuchl should make a state visit  to 
Korea on or around Nerch 20. 
Korean  Source  Repor t  
E t -~ "~I -D lX i '~  ~oo ~ Cll~o" 
oj_a, xd~. ~ ~.\]Ol D IXF~ ~F ~,~FI,,t ~'-9-, ~.~OF ~t~l.~. ~t l  
ud~Otl ~l . : , r t}  ~\]l~i/ ~ol~.-E.II .?-INto ?,,toiSF.~. ~t.-Ol-~ 8-.~01 
~XlI~II= = ~ZISH LDFPI~_ ~C.F~ uH~C3 ~-~-  ~.1-~..~ OF-..It~ 
~01 ~cF.  
x~.~ ~.~OI ~l,.Lt~ EH~=  ~S lO I  ...~CI.~ ~.~_o~ ~It~,/~F 
a~_tOI LO~O KILL= ~0~OPj ~-~/~1 )H~F ~dXl~ 8~9F 
Figure 1 
The analyst selects one or more scenario 
template, to activate in the query. Each 
scenario template corresponds to a specific 
type of event. Available scenario templates 
might include troop movements, acts of 
violence, meetings and negotiathms, 
protests, etc. In Figure 1, the selected event 
is of type meeting (understood broadly). 
The analyst fills in the available slots of the 
selected scenario template in order to restrict 
the search to the information considered to 
be relevant. In Figure 1, the values specified 
in the scenario template indicate that the 
information to f'md is about meetings having 
as location South Korea and as issue North 
Korea and missiles. The analyst also 
32  
specifies what information s/he wants to be 
reported when information matching the 
query is found. In Figure 1, the selected 
boxes under the Report column indicate that 
all information found satisfying the query 
should be reported except for the meeting 
participants. 1 
? Once the analyst submits the query for 
evaluation, the system searches the input 
documents for information matching the 
query. As a result, a hypertext document is 
generated describing the information 
matching the query as well as the source of 
this information. Note that the query 
contains English keywords that are 
automatically translated into Korean prior to 
matching. The extracted information is 
presented in English after being translated 
from Korean. In Figure 1, the generated 
hypertext response indicates two documents 
in the input set that matched the query 
totally or in part. Each summary in the 
response includes just the translations of the 
extracted information that the analyst 
requested to be reported. 
? For each document extract matching the 
analyst query, the analyst can obtain a 
complete machine translation of the Korean 
document where the match was found, and 
where the matched information is 
highlighted. Working with a human 
translator, the analyst can also verify the 
accuracy of the reported information by 
accessing the documents in their original 
language. 
3 System Design 
Figure 2 shows the high-level design of the 
system. It consists of the following components: 
? The User Interface. The browser-based 
interface is for entering queries and 
displaying the resulting presentations. 
? The Portable Information Extractor (PIE) 
component. The PIE component uses the 
While in this example the exclusion of participant 
information in the resulting report is rather artificial, 
in general a scenario template may contain many 
different ypes of information, not all of which are 
likely to interest an analyst at once. 
Extraction Pattem Library - -  which 
contains the set of extraction patterns 
learned in the lab, one set per scenario 
template - -  to extract specific types of 
information from the input Korean 
documents, once parsed. 
? The Ranker component. This component 
ranks the extracted information returned by 
the PIE component according to how well it 
matches the keyword restrictions in the 
query. The MT component's English-to- 
Korean Transfer Lexicon is used to map the 
English keywords to corresponding Korean 
ones. When the match falls below a user- 
? configurable threshold, the extracted 
information is filtered out. 
? The MT component. The MT component 
(cf. Lavoie et al, 2000) translates the 
extracted Korean phrases or sentences into 
corresponding English ones. 
? The Presentation Generator component. 
This component generates well-organized, 
easy-to-read hypertext presentations by 
organizing and formatting the ranked 
extracted information. It uses existing NLG 
components, including the Exemplars text 
planning framework (White and Caldwell, 
1998) and the RealPro syntactic realizer 
(Lavoie and Rainbow, 1997). 
In our feasibility study, the majority of the effort 
went towards developing the PIE component, 
described in the next section. This component 
was implemented in a general way, i.e. in a way 
that we would expect to work beyond the 
specific training/test corpus described below. In 
contrast, we only implemented initial versions of 
the User Interface, Ranker and Presentation 
Generator components, in order to demonstrate 
the system concept; that is, these initial versions 
were only intended.to work with our training/test 
corpus, and will require considerable further 
development prior to reaching operational status. 
For the MT component, we used an early 
version of the lexical transfer-based system 
currently under development in an ongoing 
SBIR Phase II project (cf. Nasr et al, 1997; 
Palmer et al, 1998; Lavoie et al, 2000), though 
with a limited lexicon specifically for translating 
the slot fillers in our training/test corpus. 
33 
Korean Documents 
Parser 
Tagged l 
Korean Documents ( LexiconK?rean 1
~ Syntactic . . . . . .  Eaglish Grammar Structure (English) RealPro 
English Lexicon / ' S~'ntactic Realizer Sentence (English) 
t Parsed Document ~ ::i~i?~'~vii~i? ' .~:Qi~I~:i~-'-iL \[:!::ili:::.:: ~t r~.  :::::::::::::::::::::::: 
Extracted Information \[ 
(Korean) 
Ordered Extracted 
Information(Korean) 
Parsed Document \] Machine "lYanslation I ( 
~l Component (MT) 
Ordered Extracted 
Information (English) 
User Input Data Presentation (E glish) 
Information Extraction 
Query (English) 1 
i : rla0 Inf0rntauonl 
English-Korean 7 
Transfer Lexicon J
Korean-English 
Transfer Lexicon ) 
T 
Miiiiii ii 
? 
Presentation (English) 
End user Document Processing Knowledge base 
component component 
D (C)OTS component 
\[\]Component created in Phase I 
\[\]Component created or improved in Phase II 
Figure 2 
4 Portable Information Extraction 
4.1 Scenario Template and Training/Fest 
Corpus 
For our Phase I feasibility demonstration, we 
chose a minimal scenario template for meeting 
and negotiation events consisting of one or more 
participant slots plus optional date and location 
slots. 2 We then gathered a small corpus of thirty 
articles by searching for articles containing 
"North Korea" and one or more of about 15 
keywords. The first two sentences (with a few 
exceptions) were then annotated with the slots to 
be extracted, leading to a total of 51 sentences 
containing 47 scenario templates and 89 total 
2 In the end, we did not use the 'issue' slot shown in 
Figure 1, as it contained more complex Idlers than 
those that ypically have been handled in IE systems. 
correct slots. Note that in a couple of cases 
more than one template was given for a single 
long sentence. 
When compared to the MUC scenario 
template task, our extraction task was 
considerably simpler, for the following reasons: 
* The answer keys only contained information 
that could be found within a single sentence, 
i.e. the answer keys did not require merging 
information across entences. 
? The answer keys did not require anaphoric 
references to be resolved, and we did not 
deal with conjuncts eparately. 
? We did not attempt o normalize dates or 
remove appositives from NPs. 
4.2 Extraction Pattern Learning 
For our feasibility study, we chose to follow the 
AutoSlog (Lehnert et al, 1992; Riloff, 1993) 
approach to extraction pattern acquisition. In 
this approach, extraction patterns are acquired 
34 
i. E: 
K: 
<target-np>=<subject> <active voice verb> 
<participant> MET 
<target-np>=<subject> <active voice verb> 
<John-i> MANNASSTA 
<John-nom>'MET 
2. E: 
K: 
<target-np>=<subject> <verb> <infinitive> 
<participant> agreed to MEET 
<target-np>=<subject> <verbl-ki- lo> <verb2> 
<John-un> MANNA-ki- lo hapuyhayssta 
<John-nom> MEET-ki- lo agreed 
(-ki: nominalization ending, -io: an adverbial postposition) 
Figure 3 
via a one-shot general-to-specific learning 
algorithm designed specifically for the 
information extraction task. 3 The learning 
algorithm is straightforward and depends only 
on the existence of a (partial) parser and a small 
set of general inguistic patterns that direct the 
creation of specific patterns. As a training 
corpus, it requires a set of texts with noun 
phrases annotated with the slot type to be 
extracted. 
To adapt the AutoSlog approach to Korean, 
we first devised Korean equivalents of the 
English patterns, two of which are shown in 
Figure 3. It turned out that for our corpus, we 
could collapse some of these patterns, though 
some new ones were also needed. In the end we 
used just nine generic patterns. 
Important issues that arose in adapting the 
approach were (1) greater flexibility in word 
order and heavier reliance on morphological 
cues in Korean, and (2) the predominance of 
light verbs (verbs with little semantic ontent of 
their own) and aspectual verbs in the chosen 
domain. We discuss these issues in the next two 
sections. 
4.3 Korean Parser 
We used Yoon's hybrid statistical Korean parser 
(Yoon et al, 1997, 1999; Yoon, 1999) to process 
the input sentences prior to extraction. The 
parser incorporates a POS tagger and 
3 For TIDES, we plan to use more sophisticated 
learning algorithms, as well as active learning 
techniques, such as those described in Thompson et 
al. (1999). 
morphological nalyzer and yields a dependency 
representation as its output? The use of a 
dependency representation e abled us to handle 
the greater flexibility in word order in Korean. 
To facilitate pattern matching, we wrote a 
simple program to convert he parser's output o 
XML form. During the XML conversion, two 
simple heuristics were applied, one to recover 
implicit subjects, and another to correct a 
recurring misanalysis of noun compounds. 
4.4 Trigger Word Filtering and 
Generalization 
In the newswire corpus we looked at, meeting 
events were rarely described with the verb 
'mannata' ('to meet'). Instead, they were 
usually described with a noun that stands for 
'meeting' and a light or aspectual verb, for 
example, 'hoyuy-lul kacta' ('to have a meeting') 
or 'hoyuy-lul machita' ('to finish a meeting'). 
In order to acquire extraction patterns that made 
appropriate use of such collocations, we decided 
to go beyond the AutoSlog approach and 
explicitly group trigger words (such as 'hoyuy') 
into classes, and to likewise group any 
collocations, such as those involving light verbs 
or aspectual verbs. To fmd collocations for the 
trigger words, we reviewed a Korean lexical co- 
occurrence base which was constructed from a 
corpus of 40 million words (Yoon et al, 1997). 
We then used the resulting specification to filter 
the learned patterns to just those containing the 
4 Overall dependency precision is reported to be 
89.4% (Yoon, 1999). 
35 
. - !  
trigger words or trigger word collocations, as 
well as to generalize the patterns to the word 
class level. Because the number of tr:igger 
words is small, this specification can be done 
quickly, and soon pays off in terms of time 
saved in manually filtering the learned patterns. 
4.5 Results 
In testing our approach, we obtained overall 
results of 79% recall and 67% precision in a 
hold-one-out cross validation test. In a cross 
validation test, one repeatedly divides a corpus 
into different raining and test sets, averaging the 
results; in the hold-one-out version, the system 
is tested on a held-out example after being 
trained on the rest. In the IE setting, the recall 
measure is the number of correct slots found 
divided by the total number of correct slots, 
while the precision measure is the number of 
correct slots found divided by the total number 
of slots found. 
While direct comparisons with the MUC 
conference results cannot be made for the 
reasons we gave above, we nevertheless 
consider these results quite promising, as these 
scores exceed the best scores reported at MUC-6 
on the scenario template task. 5 
Table 1: Hold-One-Out Cross Validation 
Slots Recall Precision 
All 79% 67% 
Participant 75% 84% 
Date/Location 86% 54% 
Table2: Hold-One-OutCross Validat~n 
wi~outGeneralizafion 
Slots Recall Precision 
All 61% 64% 
Participant 57% 81% 
Date/Location 67% 52% 
A breakdown by slot is shown in Table 1. We 
may note that precision is low for date and 
location slots because we used a simplistic 
sentence-level merge, rather than dependencies. 
To measure the impact of our approach to 
generalization, we may compare the results in 
5 
http://www.nist.gov/itl/div894/894.02/related_project 
s/tipster/muc.htm 
Table 1 with those shown in Table 2, where 
generalization is not used. As can be seen, the 
generalization step adds substantially to overall 
recall. 
To illustrate the effect of generalization, 
consider the pattern to extract he subject NP of 
the light verb 'kac (hold)' when paired with an 
object NP headed by the noun 'hyepsang 
(negotiation)'. Since this pattern only occurs 
once in our corpus, the slot is not successfully 
extracted in the cross-validation test without 
generalization. However, since this example 
does fall under the more generalized pattern of 
extracting the subject NP of a verb in the light 
verb class when paired with an object NP 
headed by a noun the 'hoytam-hyepsang' class, 
the slot is successfully extracted in the cross- 
validation test using the generalized patterns. 
Cases like these are the source of the 18% boost 
in recall of participant slots, from 57% to 75%. 
5 Discussion 
Our feasibility study has focused our attention 
on several questions concerning the interaction 
of IE and MT, which we hope to pursue under 
the DARPA TIDES initiative. One question is 
the extent o which slot filler translation is more 
practicable than general-purpose MT; one would 
expect to achieve much higher quality on slot 
fillers, as they are typically relatively brief noun 
phrases, and instantiation of a slot implies a 
degree of semantic lassification. On the other 
hand, one might find that higher quality is 
required in order to take translated phrases out 
of their original context. Another question is 
how to automate the construction of bilingual 
lexicons. An important issue here will be how 
to combine information from different sources, 
given that automatically acquired lexical 
information is apt to be less reliable, though 
domain-specific. 
Acknowledgements 
Our thanks go to Richard Kittredge and Tanya 
Korelsky for helpful comments and advice. This 
work was supported by ARL contract DAAD 17- 
99-C-0005. 
36 
References 
Cardie, C. (1997). Empirical Methods in Information 
Extraction. AI Magazine 18(4):65-79. 
Lavoie, B. and Rambow, O. (1997). RealPro - -  A 
fast, portable sentence realizer. In Proceedings of 
the Conference on Applied Natural Language 
Processing (ANLP'97), Washington, DC. 
Lavoie, B., Korelsky, T., and Rambow, O. (2000). A 
Framework for MT and Multilingual NLG Systems 
Based on Uniform Lexico-Structural Processing. 
To appear in Proceedings of the Sixth Conference 
on Applied Natural Language Processing (ANLP- 
2000), Seattle, WA. 
Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., 
Riloff, E., and Soderland, S. (1992). University of 
Massachusetts: Description of the CIRCUS system 
as used in MUC-4. In Proceedings of the Fourth 
Message Understanding Conference (MUC-4), 
pages 282-288, San Mateo, CA. Morgan 
Kaufmann. 
MUC-5 (1994). Proceedings of the Fifth Message 
Understanding Conference (MUC-5). Morgan 
Kaufmann, San Mateo, CA. 
MUC-7 (1998). Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Morgan 
Kaufmann, San Francisco, CA. 
Nasr, A., Rambow, O., Palmer, M., and Rosenzweig, 
J. (1997). Enriching lexical transfer with cross- 
linguistic semantic features. In Proceedings of the 
lnterlingua Workshop at the MT Summit, San 
Diego, CA. 
Palmer, M., Rambow, O., and Nasr, A. (1998). 
Rapid prototyping of domain-specific machine 
translation systems. In Machine Translation and 
the Information Soup - Proceedings of the Third 
Conference of the Association for Machine 
Translation in the Americas AMTA'98, Springer 
Verlag (Lecture Notes in Artificial Intelligence No. 
1529), Berlin. 
Riloff, E. (1993). Automatically constructing a
dictionary for information exlxaction tasks. In 
Proceedings of the Eleventh National Conference 
on Artificial Intelligence, pages 811-816, 
Washington, DC. AAAI Press / MIT Press. 
Thompson, C. A., Califf, M. E., and Mooney, R. J. 
(1999). Active learning for natural language 
parsing and information extraction. In Proceedings 
of the Sixteenth International Machine Learning 
Conference (1CML-99), Bled, Slovenia. 
White, M. and Caldwell, T. (1998). EXEMPLARS: A 
practical, extensible framework for dynamic text 
generation. In Proceedings of the 8th International 
Workshop on Natural Language Generation, 
Niagara-on-the-Lake, Ontario. 
Yoon, J. (1999). Efficient dependency parsing based 
on three types of chunking and lexical association. 
Submitted. 
Yoon, J., Choi, K.-S., and Song, M. (1999). Three 
types of chunking in Korean and dependency 
analysis based on lexical association. In 
Proceedings of lCCPOL. 
Yoon, J., Kim, S., and Song, M. (1997). New parsing 
method using global association table. In 
Proceedings of the 5th International Workshop on 
Parsing Technology. 
37 
? 
Comparing Lexicalized Treebank Grammars Extracted from 
Chinese, Korean, and English Corpora 
Fei  X ia ,  Chung-hye  Han,  Mar tha  Pa lmer ,  and  Arav ind  Josh i  
University of Pennsylvania 
Phi ladelphia PA 19104, USA 
{fxia, chunghye, mpalmer, j oshi}@linc, cis. upenn, edu 
Abst rac t  
In this paper, we present a method 
for comparing Lexicalized Tree Ad- 
joining Grammars extracted from 
annotated corpora for three lan- 
guages: English, Chinese and Ko- 
rean. This method makes it possi- 
ble to do a quantitative comparison 
between the syntactic structures of 
each language, thereby providing a 
way of testing the Universal Gram- 
mar Hypothesis, the foundation of 
modern linguistic theories. 
1 In t roduct ion  
The comparison of the grammars extracted 
from annotated corpora (i.e., Treebanks) is 
important on both theoretical and engineer- 
ing grounds. Theoretically, it allows us to do 
a quantitative testing of the Universal Gram- 
mar Hypothesis. One of the major concerns 
in modern linguistics is to establish an ex- 
planatory basis for the similarities and varia- 
tions among languages. The working assump- 
tion is that languages of the world share a set 
of universal linguistic principles and the ap- 
parent structural differences attested among 
languages can be explained as variation in 
the way the universal principles are instan- 
tiated. Comparison of the extracted syntac- 
tic trees allows us to quantitatively evaluate 
how similar the syntactic structures of differ- 
ent languages are. From an engineering per- 
spective the extracted grammars and the links 
between the syntactic structures in the gram- 
mars are valuable resources for NLP applica- 
tions, such as parsing, computational lexicon 
development, and machine translation (MT), 
to name a few. 
In this paper we first briefly discuss some 
linguistic characteristics of English, Chinese, 
and Korean, and introduce the Treebanks for 
the three languages. We then describe a 
tool that extracts Lexicalized Tree Adjoin- 
ing Grammars (LTAGs) from Treebanks and 
the results of its application to these three 
Treebanks. Next, we describe our methodol- 
ogy for automatic comparison of the extracted 
Treebank grammars, This consists primar- 
ily of matching syntactic structures (namely, 
templates and sub-templates) in each pair 
of Treebank grammars. The ability to per- 
form this type of comparison for different lan- 
guages has a definite positive impact on the 
possibility of sorting out the universal ver- 
sus language-dependent features of languages. 
Therefore, our grammar extraction tool is not 
only an engineering tool of great value in im- 
proving the efficiency and accuracy of gram- 
mar development, but it is also very useful for 
investigating theoretical linguistics. 
2 Three  Languages  and  Three  
' r reebanks  
In this section, we briefly discuss some lin- 
guistic characteristics of English, Chinese, 
and Korean, and introduce the Treebanks for 
these languages. 
2.1 Three  Languages 
These three languages belong to different lan- 
guage families: English is Germanic, Chinese 
is Sino-Tibetan, and Korean is Altaic (Com- 
rie, 1987). There are several major differences 
between these languages. First, both English 
52 
and Chinese have predominantly subject- 
verb-object (SVO) word order, whereas Ko- 
rean has underlying SOV order. Second, the 
word order in Korean is freer than in English 
and Chinese in the sense that argument NPs 
are freely permutable (subject o certain dis- 
course constraints). Third, Korean and Chi- 
nese freely allow subject and object deletion, 
but English does not. Fourth, Korean has 
richer inflectional morphology than English, 
whereas Chinese has little, if any, inflectional 
morphology. 
2.2 Three Treebanks  
The Treebanks that we used in this paper are 
the English Penn Treebank II (Marcus et al, 
1993), the Chinese Penn Treebank (Xia et 
al., 2000b), and the Korean Penn Treebank 
(Chung-hye Han, 2000). The main param- 
eters of these Treebanks are summarized in 
Table 1.1 The tags in each tagset can be 
classified into one of four types: (1) syntac- 
tic tags for phrase-level annotation, (2) Part- 
Of-Speech (POS) tags for head-level annota- 
tion, (3) function tags for grammatical func- 
tion annotation, and (4) empty category tags 
for dropped arguments, traces, and so on. 
We chose these Treebanks because they all 
use phrase structure annotation and their an- 
notation schemata re similar, which facili- 
tates the comparison between the extracted 
Treebank grammars. Figure 1 shows an an- 
notated sentence from the Penn English Tree- 
bank. 
3 LTAGs  and  Ext rac t ion  
A lgor i thm 
In this section, we give a brief introduction to 
the LTAG formalism and to a system named 
LexTract, which we build to extract LTAGs 
from Treeb~.nks. 
1The reason why the average sentence length for 
Korean is much shorter than those for English and 
Chinese is that a big portion of the corpus for Ko- 
rean Treebank includes dialogues that contain many 
one-word replies, whereas English and Chinese cor- 
pora consist of newspaper articles. 
((S (ppoLOC (IN at) 
(NP (NNP FNX)) 
(NP-SBJ-1 (bINS underwriters)) 
(ADVP (RB stin)) 
(VP (VBP draft) 
(NP (bINS policies)) 
(S-MNR 
(NP-SBJ (-NONE- *-1 )) 
(VP (VBG using) 
(NP 
(NP (iNN fountain) (NNS pens)) 
(CO and) 
(NP (VBG blotting) (NN papers)))))))) 
Figure 1: An example from Penn English 
Treebank 
3.1 LTAG fo rmal i sm 
LTAGs are based on the Tree Adjoining 
Grammar formalism developed by Joshi, 
Levy, and Takahashi (Joshi et al, 1975; Joshi 
and Schabes, 1997). The primitive elements 
of an LTAG are elementary trees (etrees). 
Each etree is associated with a lexical item 
(called the anchor of the tree) on its fron- 
tier. LTAGs possess many desirable proper- 
ties, such as the Extended Domain of Local- 
ity, which allows the encapsulation f all argu- 
ments of the anchor associated with an etree. 
There are two types of etrees: initial trees and 
auxiliary trees. An auxiliary tree represents 
a recursive structure and has a unique leaf 
node, called the foot node, which has the same 
syntactic category as the root node. Leaf 
nodes other than anchor nodes and foot nodes 
are substitution odes. Etrees are combined 
by two operations: substitution and adjunc- 
tion. The resulting structure of the combined 
etrees is called a derived tree. The combina- 
tion process is expressed as a derivation tree. 
Figure 2 shows the etrees, the derived tree, 
and the derivation tree for the sentence un- 
derwriters still draft policies. Foot and sub- 
stitution nodes are marked by ,, and $, re- 
spectively. The dashed and solid lines in the 
derivation tree are for adjunction and substi- 
tution operations, respectively. 
3.2 The Form of  Target Grammars 
Without further constraints, the etrees in 
the target grammar (i.e., the grammar to be 
extracted by LexTract) could be of various 
shapes. LexTract recognizes three types of 
53 
# of POS # ofsyntac- 
tags tic tags 
Language corpus size 
(words) 
English 1,174K 
Chinese 100K 
Korean 30K 
average sen- 
tence length 
23.85 words 
| Ib"~ ' - l l~ , ) _o~ 
34 
17 
of ftmc- # of empty cat- 
tion tags egory tags 
20 12 
26 7 
17 4 
Table 1: Size of the Treebanks and the tagsets used in each Treebank 
#h ~ vP -~,~ #3: S #4: 
I ADVP VP" ", ~, vP / I 
~s I ', . . . . .  ~ /Nns  
vBP ~/  I I ? ' v" , , .  
m~. , r ia .~ ~,ill draR pc u.mm,..~ 
(a) et re~ 
NF VP 
Jail~ draft NN$ 
I 
ptfliei~ 
draf t (#3)  
- - " - - ' "T ' - - - - " - - - -  
undcxwrRcrs(# 1 ) \ policies(#4) 
still(#2) 
(h) dcrivcd trc? (c) dczivatitm trcc 
Figure 2: Etrees, derived tree, and derivation 
tree for underwriters till draft policies 
x TM 
x ~ wq 
y~ X~= w~ X m )?,. cc  I X ~ 
xo z,~ /~  x, 
{ xo z~ x/~'X.z ,~
knti.~l itgm I 
(a) spinc-ctr~ (b) rm~-ctrce (c) c,,nj-etree 
Figure 3: Three types of elementary trees in 
the target grammar 
relation (namely, predicate-argument, modi- 
fication, and coordination relations) between 
the anchor of an etree and other nodes in the 
etree, and imposes the constraint that all the 
etrees to be extracted should fall into exactly 
one of the three patterns in Figure 3. 
The spine-etrees for predicate-argument 
relations. X ? is the head of X m and the 
anchor of the etree. The etree is formed 
by a spine X m ~ X m-1 ~ .. ~ X ? and 
the arguments of X ?. 
The mod-etrees for modification rela- 
tions. The root of the etree has two chil- 
dren, one is a foot node with the label 
Wq,  and the other node X m is a modifier 
of the foot node. X m is further expanded 
into a spine-etree whose head X ? is the 
anchor of the whole mod-etree. 
The conj-etrees for coordination rela- 
tions. In a conj-etree, the children of the 
root are two conjoined constituents and 
a node for a coordination conjunction. 
One conjoined constituent is marked as 
the foot node, and the other is expanded 
into a spine-etree whose head is the an- 
chor of the whole tree. 
Spine-etrees are initial trees, whereas mod- 
etrees and conj-etrees are auxiliary trees. 
3.3  Ext rac t ion  a lgor i thm 
The core of LexTract is an extraction algo- 
rithm that takes a Treebank sentence such as 
the one in Figure 1 and Treebank-specific in-
formation provided by the user of LexTract, 
and produces a set of etrees as in Figure 4 
and a derivation tree. We have described 
LexTract's architecture, its extraction algo- 
rithm, and its applications in (Xia, 1999; Xia 
et al, 2000a). Therefore, we shall not re- 
peat them in this paper other than point- 
ing out that LexTract is completely anguage- 
independent. 
3.4  Exper iments  
The results of running LexTract on English, 
Chinese, and Korean Treebanks are shown in 
Table 2. Templates are etrees with the lexical 
items removed. For instance, #3, #6, and #9 
in Figure 4 are three distinct etrees but they 
share the same template. 
Figure 5 shows the log frequency of tem- 
plates in the English Treebank and percent- 
age of template tokens covered by template 
54 
m 
m ~i \ ] lml~ i J  
template etree word etree types 
types types types per word type 
6926 131,397 49,206 
1140 21,125 10,772 
etree types 
per word token 
CFG rules 
(unlexicalized) 
2.67 34.68 1524 
1.96 9.13 515 
1.45 2.76 : 177 
Table 2: Grammars extracted from three Treebanks 
#1: re2: #3: #4: #5: #6: 
S NP NP VP S NP 
PP S* NNS ADVP VP* NP| VP NNS / ~  NNP 
, . . P ,  I I R', vBf'~-.~ I 
l FNX enderwri,~,~ { { polid~ 
s0ll draft at 
#7: #8: #9: #I0: #l l :  #12: 
VP NP NP NP NP 
VP" S NN NP" N VBG Np~ NIP" CC| NP 
NP VP 
{ ~ finmtain bltXting 
e V~O NP; pen.~ I 
pap~ 
~ing 
Figure 4: The extracted etrees from the fully 
bracketed ttree 
types. 2 In both cases, template types are 
sorted according to their frequencies and plot- 
ted on the X-axis. The figure shows that 
a small subset of template types, which oc- 
curs very frequently in the Treebank and can 
be seen as the core of the Treebank gram- 
mar, covers the majority of template tokens 
in the Treebank. For instance, the most 
frequent template type covers 9.37% of the 
template tokens and the top 100 (500, 1000 
and 1500, respectively) template types cover 
87.1% (96.6%, 98.4% and 99.0%, respectively) 
of the tokens, whereas about half (3440) of 
the template types occur once, accounting for 
only 0.32% of template tokens in total. 
4 Compar ing  Three  Treebank  
Grammars  
In this section, we describe our methodology 
for comparing Treebank gr3.mmars and the 
experimental results. 
4.1 Methodo logy  
To compare Treeb~nb grammars, we need to 
ensure that the Treebank grammars are based 
on the same tagset. To achieve that, we first 
create a new tagset that includes all the tags 
2If a template occurs n times in the corpus, it is 
counted as one template type but n template tokens. 
(a) Frequency (b) Coverage 
Figure 5: Etree template types and template 
tokens in the Penn English Treebank 
(X-axes: (a) and (b) template types 
Y-axes: (a) log frequency of templates; (b) 
percentage of template token covered by tem- 
plate types) 
from the three Treebanks. Then we merge 
some tags in this new tagset into a single tag. 
This step is necessary because certain distinc- 
tions among some tags in one language do not 
exist in another language. For example, the 
English Treebank has distinct tags for verbs 
in past tense, past participals, gerunds, and 
so on; however, no such distinction is mor- 
phologically marked in Chinese and, there- 
fore, the Chinese Treebank uses the same tag 
for verbs regardless of the tense and aspect. 
To make the conversion straightforward for 
verbs, we use a single tag for verbs in the new 
tagset. Next, we replace the tags in the origi- 
nal Treebanks with the tags in the new tagset, 
and then re-run LexTract to build Treebank 
gr~mraars from those Treebanks. 
Now that the Treebank grammars are based 
on the same tagset, we can compare them ac- 
cording to the templates and sub-templates 
that appear in more than one 'rreebank m 
that is, given a pair of Treebank grammars, 
we first calculate how many templates oc- 
cur in both grammars; 3 Next, we decompose 
SIdeally, to get more accurate comparison results, 
we would like to compare trees, rather than templates 
(which are non-lexicalized); however, comparing etrees 
requires bilingual parallel corpora, which we are cur- 
55 
templates: sub-templates: 
~ spine: S -> VP -> V 
NP| ~ subca~ (:NP, V@. NP) 
V@ NP! with root S 
(a) spine-etree template 
VP spine: PP-> P 
VP'~ PP ~ subeat: (P@, NP) 
with root PP 
P@ NP~ rood-pair: (VP*, PP) 
(b) mod-etree t mplate 
I . . . . . . .~  spine: NP->N 
NP* cc~ r~P ~ subeat ~@) with root NP 
lq@ conj-tuple: (NP*, CC, NP) 
(c) conj-etree t mplate 
Figure 6: The decompositions of etree tem- 
plates (In sub-templates, @ marks the anchor 
in subcategorization frame, * marks the mod- 
ifiee in a modifier-modifiee pair.) 
each template into a list of sub-templates (e.g., 
spines and subcategorization frames) and cal- 
culate how many of those sub-templates occur 
in both grammars. A template is decomposed 
as follows: A spine-etree template is decom- 
posed into a spine and a subcategorization 
frame; a mod-etree template is decomposed 
into a spine, a subcategorization frame, and a 
modifier-modifiee pair; a conj-etree template 
is decomposed into a spine, a subcategoriza- 
tion frame, and a coordination tuple. Figure 
6 shows examples of this decomposition for 
each type of template. 
4.2 Exper iments  
After tags in original Treebn.nks being re- 
placed with the tags in the new tagset, the 
numbers of templates in the new Treebank 
gra.mmars decrease by about 50%, as shown 
in the second colnmn of Table 3 (cf. the sec- 
ond column in Table 2). Table 3 also lists the 
numbers of sub-templates, such as spines and 
subcategorization frames, for each grammar. 
Table 4 lists the numbers of template types 
shared by each pair of Treeba.nk gr3.mmars 
and the percentage of the template tokens 
rently building. 
in each Treebank which are covered by these 
common template types. For example, there 
are 237 template types that appear in both 
English and Chinese Treebank grammars. 
These 237 template types account for 80.1% 
of template tokens in the English Treebank, 
and 81.5% of template tokens in the Chi- 
nese Treebank. The table shows that, al- 
though the number of matched templates are 
not very high, they are among the most fre- 
quent emplates and they account for the ma- 
jority of template tokens in the Treebanks. 
For instance, in the (Eng, Ch) pair, the 237 
template types that appear in both gram- 
mars is only 77.5% of all the English template 
types, but they cover 80.1% of template to- 
kens in the English Treebank. If we define the 
core grammar of a language as the set of the 
templates that occur very often in the Tree- 
bnnk, the data suggest hat the majority of 
the core grammars are easily inter-mappable 
structures for these three languages. 
If we compare sub-templates, rather than 
templates, in the Treebank grammars, the 
percentages of matched sub-template tokens 
(as in Table 5) are higher than the percent- 
ages of matched template tokens. This is be- 
cause two distinct templates may share com- 
mon sub-templates. 
4.3 Unmatched templates  
Our previous experiments ( ee Table 4) show 
that the percentages of unmatched template 
tokens in three Treebanks range from 16.0% 
to 43.8%, depending on the language pairs. 
Given a language pair, there are many pos- 
sible reasons why a template appears in one 
Treebank grammar, but not in the other. We 
divide those unmatched templates into two 
categories: spuriously unmatched templates 
and truly unmatched templates. 
Spuriously unmatched templates Spu- 
riously unmatched templates are templates 
that either should have found a matched tem- 
plate in the other gra.mmar or should not have 
been created by LexTract in the first place 
if the Treebanks were complete, uniformly 
annotated, and error-free. A spuriously un- 
matched template xists because of one of the 
56 
templates subtemplates 
spines subcat~ames mod-pairs 
Eng 3139 500 541 332 53 
Ch 547 108 180 152 18 
Kor 271 55 58 53 6 
(Eng,Ch) 
(Eng, Kor) 
(Ch, Kor) 
conj-tuples total 
1426 
458 
172 
Table 3: Treebank grammars with the new tagset 
type (#) 
token (%) 
type (#) 
token (%) 
type (:~) 
token (%) 
matched templates 
(237, 237) 
(80.1, 81.5) 
(83, 83) 
(57.7, 82.8) 
(59,59) 
(57.2, 84.0) 
templates with 
unique tags 
(536, 99) 
(2.8, 12.3) 
(2075, 6) 
(28.1, 0.1) 
(324,6) 
(29.4, 0.1) 
other unmatched 
templates 
(2366, 211) 
(17.1, 6.2) 
(981, 182) 
(14.2, 17.1) 
(164, 206) 
(13.4, 16.0) 
Table 4: Comparisons of templatea in three Treebank grammars 
following reasons: 
(Sl)  T reebank  size: The template is lin- 
guistically sound in both languages, and, 
therefore, should belong to the grarnmars 
for these languages. However, the tem- 
plate appears in only one Treebank gram- 
mar because the other Treebank is too 
small to include such a template. Figure 
7(S1) shows a template that is valid for 
both English and Chinese, but it appears 
only in the English Treebank, not in the 
Chinese Treebank. 
($2) Annotat ion  difference: Treebanks 
may choose different annotations for 
the same constructions; consequentially, 
the templates for those constructions 
look different. Figure 7($2) shows the 
templates used in English and Chinese 
for a VP such as "surged 7 (dollars)". 
In the template for English, the QP 
projects to an NP, but in the template 
for Chinese, it does not. 
($3) Treeb~nk annotat ion  error:  A tem- 
plate in a Treebank may result from an- 
notation errors in that Treebank. If no 
corresponding mistakes are made in the 
other Treebank, the template in the first 
Treebank will not match any template in 
the second 'I~reebank. For instance, in the 
English Treebank the word about in the 
sentence About 5 people showed up is of- 
ten mis-tagged as a preposition, resulting 
in the template in Figure 7($3). Not sur- 
prisingly, that template does not match 
any template in the Chinese Treebank. 
Tru ly  unmatched templates  A truly un- 
matched template is a template that does not 
match any template in the other Treebank 
even if we assume both Treebanks are per- 
fectly annotated. Here, we list three reasons 
why a truly unmatched template xist. 
(T1) Word order:  The word order deter- 
mines the positions of arguments w.r.t. 
their heads, and the positions of modi- 
fiers w.r.t, their modifiees. If two lan- 
guages have different word orders, their 
templates which include arguments of a 
head or a modifier are likely to look dif- 
ferent. For example, Figure 8(T1) show 
the templates for transitive verbs in Chi- 
nese and Korean grammars. The tem- 
plates do not match because of the dif- 
ferent positions of the object of the verb. 
(T2) Unique tags: For each pair of lan- 
guages, some Part-of-speech tags and 
syntactic tags may appear in only one 
language. Therefore, the templates with 
those tags will not match any templates 
in the other language. For instance, in 
Korean the counterparts of preposition 
phrases in English and Chinese are noun 
phrases (with postpositions attaching to 
them, not preposition phrases); there- 
fore, the templates with PP in Chinese, 
57 
(Eng,Ch) 
(Eng, Kor) 
(Ch, Kor) 
Table 
spines subcat frames rood-pairs conj-tuples total 
type (60,60) (92, 92) (83,83) (II,II) (246,246) 
token (94.7,87.2) (94.0, 86.3) (82.6, 80.0) (84.2, 99.1) (91.4, 85.2) 
type (39, 39) (40, 40) (46, 46) (1, 1) (126,126) 
token (70.3, 96.9) (62.1, 96.6) (56.8, 99.5) (9.3, 52.3) (63.4,97.3) 
type (28, 28) (25,25) (29,29) (I, I) (83, 83) 
token I (74.2, 99.2) (63.1, 98.1) (60.2, 93.4) (0.i, 0.4) (66.1, 96.9) 
5: Comparisons of sub-templates in three Treebank grammars 
VP 
VP* CC! VP 
V @ NIL Nl-~ 
English 
vp yP 
A 
VP* NP " VP* 
I QP Qp I 
I cD~ 
CD~ 
English Chinese 
QP 
P@ QP* 
English 
(S 1) Treebank size ($2) annotation difference ($3) annotation crmr 
Figure 7: Examples of spuriously unmatched templates 
such as the left one in Figure 8(T2), do 
not match any template in Korean. 
(T3) Un ique  syntact ic  re lat ions:  Some 
syntactic relations may be present in 
only one of the pair of languages being 
compared. For instance, the template 
in Figure 8(T3) is used for the sentence 
such as "You should go," said John, 
where the subject of the verb said ap- 
pears after the verb. No such template 
exists in Chinese. 
So far, we have listed six possible reasons 
for unmatched templates. Without manually 
examining all the unmatched templates, it is 
difficult to tell how many unmatched tem- 
plates are caused by a particular reason. Nev- 
ertheless, these reasons help us to interpret 
the results in Table 4. For instance, the ta- 
ble shows that Korean grammars cover only 
57.7% of template tokens in the English Tree- 
bank, and 57.2% in the Chinese Treebank, 
whereas the coverages for other language pairs 
are all above 80%. We suspect that this 
difference of coverage is mainly caused by 
(S1), (T1), and (T2). That is, first, Ko- 
rean Treebank is much smaller than the En- 
glish and the Chinese Treebanks, English and 
Chinese Treebanks may have many tree tem- 
plates that simply was not found in the Ko- 
rean Treebank; Second, English and Chinese 
are predominantly head-initial, whereas Ko- 
rean is head-final, therefore, many templates 
in English and Chinese can not find matched 
templates in Korean because of the word or- 
der difference; Third, Korean does not have 
preposition phrases, causing all the templates 
in English and Chinese with PPs become un- 
matched. To measure the effect of the word 
order factor to the matching rate, we re-did 
the experiment in Section 4.2, but this time 
we ignored the word order - -  that is, we treat 
templates as unordered trees. The results are 
given in Table 6. Comparing this table with 
Table 4, we can clearly see that, the percent- 
ages of matched templates increase substan- 
tially for (Eng, Kor) and (Ch, Kor) when the 
word order is ignored. Notice that the match- 
ing percentage for (Eng, Ch) does not change 
as much because the word orders in English 
and Chinese are much similar than the orders 
in English and Korean. 
5 Conclusion 
We have presented a method of quantitatively 
comparing LTAGs extracted from Treebanks. 
Our experimental results show a high pro- 
portion of easily inter-mappable structures, 
giving a positive implications for Universal 
Grammar hypothesis, We have also described 
a number of reasons why a particular tern- 
58 
$ $ 
A 
V~ NPt NPt V~ 
Chinese Korean 
(TI) word order 
vP  
A 
VP* NP VP* 
I 
P@ NPt N@ 
Chinese Korean 
(T2) unique rags 
s 
s(" 's  
NPt 
V~ $ 
! 
? 
English 
(T3) unique relation 
Figure 8: Truly unmatched templates 
(Eng,Ch) 
(Eng, Kor) 
(Ch, Kor) 
matched templates 
type (334, 259) 
token (82.8, 82.2) 
type (222, 167) 
token (66.4, 92.4) 
type (126,125) 
token (68.3, 97.3) 
tag mismatches 
i (536, 99) 
! (2.8, 12.3) 
I (2075, 6) 
! (28.1, 0.1) 
(324,6) 
(29.4, 0.1) 
other mismatches 
(2269, 189) 
(14.4, 5.5) 
(842, 98) 
(5.5, 7.5) 
(97, 140) 
(2.3, 2.6) 
Table 6: Comparisons of templates w/o orders 
plate does not match any template in other 
languages and tested the effect of word order 
on matching percentages. 
There are two natural extensions of this 
work. First, running an alignment algorithm 
on parallel bracketed corpora to produce 
word-to, word mappings. Given such word-to- 
word mappings and our template matching 
algorithm, we can automatically create lexi- 
calized etree-to-etree mappings, which can be 
used for semi-automatic transfer lexicon con- 
struction. Second, LexTract can build deriva- 
tion trees for each sentence in the corpora. By 
comparing derivation trees for parallel sen- 
tences in two languages, instances of struc- 
tural divergences (Dorr, 1993; Dorr, 1994; 
Palmer et al, 1998) can be automatically de- 
tected. 
Re ferences  
Chung-hye Hart. 2000. Bracketing Guide- 
lines for the Penn Korean Treebank (draft). 
www.cis.upenn.edu/xtag/korean.tag. 
Bernard Comrie. 1987. The World's Major Lan- 
guages. Oxford University Press, New York. 
B. J. Dorr. 1993. Machine ~D'anslation: a View 
from the Lexicon. MIT Press, Boston, Mass. 
B. J. Dorr. 1994. Machine translation diver- 
gences: a formal description and proposed so- 
lution. Computational Linguistics, 20(4):597- 
635. 
Aravind Joshi and Yves Schabes. 1997. Tree 
Adjoining Grammars. In A. Salomma and 
G. Rosenberg, editors, Handbook off For- 
mal Languages and Automata. Springer-Verlag, 
Herdelberg. 
Aravind K. Joshi, L. Levy, and M. Takahashi. 
1975. Tree Adjunct Grammars. Journal off 
Computer and System Sciences. 
M. Marcus, B. Santorini, and M. A. 
Marcinkiewicz. 1993.  Building a Large 
Annotated Corpus of English: the Penn 
Treebank. Computational Lingustics. 
Martha Palmer, Owen Rainbow, and Alexis Nasr. 
1998. Rapid Prototyping of Domain-Specific 
Machine Translation System. In Proc. of 
AMTA-1998, Langhorne, PA. 
Fei Xia, Martha Palmer, and Aravind Joshi. 
2000a. A Uniform Method of Grammar Ex- 
traction and its Applications. In Proc. off Joint 
SIGDAT Conference on Empirical Methods in 
Natural Language Processing and Very Large 
Corpora (EMNLP/VLC). 
Fei Xia, Martha Palmer, Nianwen Xue, 
Mary Ellen Okurowski, John Kovarik, Shizhe 
Huang, Tony Kroch, and Mitch Marcus. 
2000b. Developing Guidelines and Ensuring 
Consistency for Chinese Text Annotation. 
In Proc. off the 2nd International Confer- 
ence on Language Resources and Evaluation 
(LREC-2000), Athens, Greece. 
Fei Xia. 1999. Extracting Tree Adjoining Gram- 
mars from Bracketed Corpora. In Proc. off 5th 
Natural Language Processing Pacific Rim Sym- 
posium (NLPRS-99), Beijing, China. 
59 
A Uni form Method of Grammar Extract ion 
and Its Appl icat ions 
Fei  X ia  and Mar tha  Pa lmer  and Arav ind  Josh i  
Department  of Computer  and Information Science 
University of Pennsylvania 
Phi ladelphia PA 19104, USA 
{fxia, mpalmer, j oshi)@linc, cis. upenn, edu 
Abst rac t  
Grammars are core elements of many NLP ap- 
plications. In this paper, we present a system 
that automatically extracts lexicalized gram- 
mars from annotated corpora. The data pro- 
duced by this system have been used in sev- 
eral tasks, such as training NLP tools (such 
as Supertaggers) and estimating the coverage 
of harid-crafted grammars. We report experi- 
mental results on two of those tasks and com- 
pare our approaches with related work. 
1 In t roduct ion  
There are various grammar frameworks pro- 
posed for natural languages. We take Lexi- 
calized Tree-adjoining Grammars (LTAGs) as 
representative of a class of lexicalized gram- 
mars. LTAGs (Joshi et al, 1975) are ap- 
pealing for representing various phenomena 
in natural anguages due to its linguistic and 
computational properties. In the last decade, 
LTAG has been used in several aspects of 
natural language understanding (e.g., pars- 
ing (Schabes, 1990; Srinivas, 1997), semantics 
(Joshi and Vijay-Shanker, 1999; Kallmeyer 
and Joshi, 1999), and discourse (Webber and 
Joshi, 1998)) and a number of NLP applica- 
tions (e.g., machine translation (Palmer et al, 
1998), information retrieval (Chandrasekar 
and Srinivas, 1997), and generation (Stone 
and Doran, 1997; McCoy et al, 1992). This 
paper describes a system that extracts LTAGs 
from annotated corpora (i.e., Treebanks). 
There has been much work done on extract- 
ing Context-Free grammars (CFGs) (Shirai 
et al, 1995; Charniak, 1996; Krotov et al, 
1998). However, extracting LTAGs is more 
complicated than extracting CFGs because 
of the differences between LTAGs and CFGs. 
First, the primitive elements of an LTAG are 
lexicalized tree structures (called elementary 
trees), not context-free rules (which can be 
seen. as trees with depth one). Therefore, an 
LTAG extraction algorithm needs to examine 
a larger portion of a phrase structure to build 
an elementary tree. Second, the composition 
operations in LTAG are substitution (same 
as the one in a CFG) and adjunction. It is 
the operation of adjunction that distinguishes 
LTAG from all other formalisms. Third, un- 
like in CFGs, the parse trees (also known as 
derived trees in the LTAG) and the derivation 
trees (which describe how elementary trees 
are combined to form parse trees) are differ- 
ent in the LTAG formalism in the sense that 
a parse tree can be produced by several dis- 
tinct derivation trees. Therefore, to provide 
training data for statistical LTAG parsers, an 
LTAG extraction algorithm should also build 
derivation trees. 
For each phrase structure in a Treebank, 
our system creates a fully bracketed phrase 
structure, a set of elementary trees and 
a derivation tree. The data produced by 
our system have been used in several NLP 
tasks. We report experimental results on two 
of those applications and compare our ap- 
proaches with related work. 
2 LTAG fo rmal i sm 
The primitive elements of an LTAG are ele- 
mentary trees (etrees). Each etree is associ- 
ated with a lexical item (called the anchor of 
the tree) on its frontier. We choose LTAGs as 
our target grammars (i.e., the grammars to be 
extracted) because LTAGs possess many de- 
sirable properties, such as the Extended Do- 
main of Locality, which allows the encapsula- 
tion of all arguments of \[he anchor associated 
with an etree. There are two types of etrees:. 
initial trees and auxiliary trees. An auxiliary 
tree represents recursive structure and has a 
unique leaf node, called the foot node, which 
has the same syntactic ategory as the root 
node. Leaf nodes other than anchor nodes 
and foot nodes are substitutionnodes. Etrees 
are combined by two operations: substitution 
and adjunction, as in Figure 1 and 2. The 
53 
resulting structure of the combined etrees is 
called a derived tree. The combination pro- 
cess is expressed as a derivation tree. 
Figure 1: The substitution operation 
=>/_ _~ 
Figure 2: The adjunction operation 
Figure 3 shows the etrees, the derived tree, 
and the derivation tree for the sentence un- 
derwriters stil l draft policies. Foot and sub- 
stitution nodes are marked by . ,  and $, re- 
spectively. The dashed and solid lines in the 
derivation tree are for adjunction and substi- 
tution operations, respectively. 
3 System Overv iew 
We have built a system, called LexTract, for 
grammar extraction. The architecture ofLex- 
Tract is shown in Figure 4 (the parts that will 
be discussed in this paper are in bold). The 
core of LexTract is an extraction algorithm 
that takes a Treebank sentence such as the 
one in Figure 5 and produces the trees (el- 
ementary trees, derived trees and derivation 
trees) such as the ones in Figure 3. 
3.1 The Form of Target Grammars  
Without further constraints, the etrees in the 
target grammar could be of various shapes. 
#, .4" - - .  ~ ,  #3. ~: 
l :  " VP -- ' S NP A "- ~I --  NP 
I ADVP VP* ". NP I VP ' 
s~s I " . . . .  "~:~'-. i Ntis 
xa v!31> NP t /  I 
I t i ~ Iicll~i undenviStel'x st i l l  dr'aft Ix) ' 
(a) err??a 
draft(#3) 
I ADVP VP underwd lers (#1 ) ", policies(#4) 
NNS l ~ s t i l l (#2)  
slill ~ NN$ 
I 
(b) derived tree (c) derivation tr~ 
Figure 3: Etrees, derived tree and derivation 
tree for underwriters till draft policies 
i l.~xTract Syslcm i matde:d 
i ! L'rAo~ 
Treebonk ~peclflc tlerlvatim 
t ~ i implaulible 
Figure 4: Architecture of LexTract 
((S (PP-LOC (IN at) 
(NP (NNP FNX))) 
(NP-SBJ-I (NNS underwriters)) 
(ADVP (RiB still)) 
(VP (VBP draft) 
(NP (NNS policies)) 
(S-MNR 
(NP-SBJ (-NONE- *- 1)) 
(VP (VBG using) 
(NP 
(NP (NN fountain) (NNS pens)) 
(CC and) 
(NP (VBG blotting) (NN papers)))))))) 
Figure 5: A Treebank example 
Our system recognizes three types of rela- 
tion (namely, predicate-argument, modifica- 
tion, and coordination relations) between the 
anchor of an etree and other nodes in the etree, 
and imposes the constraint that all the etrees 
to be extracted should fall into exactly one of 
the three patterns in Figure 6. 
? The spine-etrees for predicate-argument 
relations. X ? is the head of X m and the 
anchor of the etree. The etree is formed 
by a spine X m --+ X m-1 -~ .. --+ X ? and 
the arguments of X ?. 
? The mod-etrees for modification rela- 
tions. The root of the etree has two chil- 
dren, one is a foot node with the label 
Wq, and the other node X m is a modifier 
X m 
A 
x '  
X ? zP~, 
lexical item 
X m 
Wq 
A 
Wq" X m ~. .  cclt x ~ 
xO zPl x o z~ I 
lcxical ilerrl I 
Icxicali lem 
(a) spinc..etree (b) mod-etz.~ (c) conj-etree 
Figure 6: Three types of elementary trees in 
the target grammar 
54 
of the foot node. X TM is further expanded 
into a spine-etree whose head X ? is the 
anchor of the whole mod-etree. 
? The conj-etrees for coordination rela- 
tions. In a conj-etree, the children of the 
root are two conjoined constituents and a 
node for a coordination conjunction. One 
conjoined constituent is marked as the 
foot node, and the other is expanded into 
a spine-etree whose head is the anchor of 
the whole tree. 
Spine-etrees are initial trees, whereas mod- 
etrees and conj-etrees are auxiliary trees. 
3.2 Treebank-spec i f i c  n format ion  
The phrase structures in the Treebank (ttrees 
for short) are partially bracketed in the sense 
that arguments and modifiers are not struc- 
turally distinguished. In order to construct 
the etrees, which make such distinction, Lex- 
Tract requires its user to provide additional 
information in the form of three tables: a 
Head Percolation Table, an Argument Table, 
and a Tagset Table. 
A Head Percolation Table has previ- 
ously been used in several statistical parsers 
(Magerman, 1995; Collins, 1997) to find heads 
of phrases. Our strategy for choosing heads is 
similar to the one in (Collins, 1997). An Ar- 
gument Table informs LexTract what types of 
arguments a head can take. The Tagset Table 
specifies what function tags always mark ar- 
guments (adjuncts, heads, respectively). Lex- 
Tract marks each sibling of a head as an argu- 
ment if the sibling can be an argument of the 
head according to the Argument Table and 
none of the function tags of the sister indi- 
cates that it is an adjunct. For example, in 
Figure 5, the head of the root S is the verb 
draft, and the verb has two siblings: the noun 
phrase policies is marked as an argument of 
the verb because from the Argument Table we 
know that verbs in general can take an NP ob- 
ject; the clause is marked as a modifier of the 
verb because, although verbs in general can 
take a sentential argument, the Tagset Table 
informs LexTract that the function tag -MNR 
(manner) always marks a modifier. 
3.3 Overv iew of  the  Ext ract ion  
A lgor i thm 
The extraction process has three steps: First, 
LexTract fully brackets each ttree; Second, 
LexTract decomposes the fully bracketed ttree 
((S (PP-LOC (IN at) 
(NP (NNP FNX))) 
(S (NP-SBJ-I (NNS underwriters)) 
(VP (ADVP (RB still)) 
(VP (VP (VBP draft) 
(NP (NNS policies))) 
(S-MNR 
(NP-SBJ (-NONE- *-1)) 
(VP (VBG using) 
(NP (NP (NN fountain) 
(NP (NNS pens))) 
(CC and) 
(NP (VBG blotting) 
(NP (NN papers)))))))) 
Figure 7: The fully bracketed ttree 
into a set of etrees; Third, LexTract builds the 
derivation tree for the ttree. 
3.3.1 Ful ly  b racket ing  ttrees 
As just mentioned, the ttrees in the Tree- 
bank do not explicitly distinguish arguments 
and modifiers, whereas etrees do. To account 
for this difference, we first fully bracket the 
ttrees by adding intermediate nodes so that 
at each level, one of the following relations 
holds between the head and its siblings: (1) 
head-argument relation, (2) modification re- 
lation, and (3) coordination relation. Lex- 
Tract achieves this by first choosing the head- 
child at each level and distinguishing argu- 
ments from adjuncts with the help of the three 
tables mentioned in Section 3.2, then adding 
intermediate nodes so that the modifiers and 
arguments of a head attach to different levels. 
Figure 7 shows the fully bracketed ttree. The 
nodes inserted by LexTract are in bold face. 
3.3.2 Bu i ld ing  etrees 
In this step, LexTract removes recursive struc- 
tures - which will become mod-etrees or conj- 
etrees - from the fully bracketed ttrees and 
builds spine-etrees for the non-recursive struc- 
tures. Starting from the root of a fully brack- 
eted ttree, LexTract first finds a unique path 
from the root to its head. It then checks each 
node e on the path. If a sibling of e in the ttree 
is marked as a modifier, LexTract marks e and 
e's parent, and builds a mod-etree (or a conj- 
etree if e has another sibling which is a con- 
junction) with e's parent as the root node, e as 
the foot node, and e's siblings as the modifier. 
Next, LexTract creates a spine-etree with the 
remaining unmarked nodes on the path and 
their siblings. Finally, LexTract repeats this 
process for the nodes that are not on the path. 
In Figure 8, which is the same as the one in 
Figure 7 except hat some nodes are numbered 
and split into the top and bottom pairs, 1 the 
1When a pair of etrees are combined ur ing parsing, 
55 
#5 
s2.b -"----...L ?" 
2 
" d I . 
#6 
Figure 8: The extracted et:rees can be seen as 
a decomposition of the fully bracketed ttree 
#1: #2: #3: #4: #5: #6: 
S NP NP VP S NP 
PP S" NNS ADVP VP = NPI VP NNS 
,s NP ~ { I RB VBP Na 
\[ FNX undcrwriten; ~ I pollciex 
still draft a\[ 
#7: #8: #9: #10: #I \[: #12: 
Vp NP NP NP "~ I cc NP 
v .  s NN N.  N~S I ~ VBG NP" NP" CCI NP 
Ne VP \[ NN 
l~,t,lntaln bloitln8 
? 
~per 
uffm8 
Figure 9: The extracted etrees from the fully 
bracketed ttree 
path from the root $1 to the head VBP is 
$1 ~ $2 ~ VP1 ~ VP2 --+ VP3 ~ VBP.  
Along the path the PP  ~ at FNX-  is a 
modifier of $2; therefore, Sl.b, S2.t, and the 
spine-etree rooted at PP  form a mod-etree 
#1. Similarly, the ADVP still is a modifier 
of VP2 and $3 is a modifier of VP3, and the 
corresponding structures form mod-etrees #4 
and #7. On the path from the root to VBP,  
Sl .t  and S2.b are merged (and so are VPi . t  
and VP3.b) to from the spine-etree #5. Re- 
peating this process for other nodes will gen- 
erate other trees such as trees #2, #3 and #6. 
The whole ttree yields twelve etrees as shown 
in Figure 9. 
3.3.3 Bu i ld ing  der ivat ion  t rees  
The fully bracketed ttree is in fact a derived 
tree of the sentence if the sentence is parsed 
with the etrees extracted by LexTract. In ad- 
dition to these etrees and the derived tree, we 
the root of one etree is merged with a node in the other 
etree. Splitting nodes into top and bottom pairs during 
the decomposition f the derived tree is the reverse 
process of merging nodes during parsing. For the sake 
of simplicity, we show the top and the bottom parts of 
a node only when the two parts will end up in different 
etrees. 
also need derivation trees to train statistical 
LTAG parsers. Recall that, in general, given 
a derived tree, the derivation tree that can 
generate the derived tree may not be unique. 
Nevertheless, given the fully bracketed ttree, 
the etrees, and the positions of the etrees in 
the ttree (see Figure 8), the derivation tree 
becomes unique if we choose either one of the 
following: 
? We adopt the traditional definition of 
derivation trees (which allows at most one 
adjunction at any node) and add an ad- 
ditional constraint which says that no ad- 
junction operation is allowed at the foot 
node of any auxiliary tree. 2 
? We adopt the definition of derivation 
trees in (Schabes and Shieber, 1992) 
(which allows multiple adjunction at any 
node) and require all mod-etrees adjoin 
to the etree that they modify. 
The user of LexTract can choose either op- 
tion and inform LexTract about his choice by 
setting a parameter. 3 Figure 10 shows the 
derivation tree based on the second option. 
draft (#5) 
a\[ (#1) underwriters(#3) ~i11(#4) policies(#6) using(#7) 
I I 
FNX(#2) pen(#9) 
fountain(#8) paper(#12) 
and(#10) bloldng(#l I) 
Figure 10: The derivation tree for the sentence 
3.4 Un iqueness  of  decompos i t ion  
To summarize, LexTract is a language- 
independent grammar extraction system, 
which takes Treebank-specific information 
(see Section 3.2) and a ttree T, and creates 
2Without his additional constraint, the derivation 
tree sometimes i  not unique. For example, in Figure 
8, both #4 and #7 modify the etree #5. If adjunc- 
tion were allowed at foot nodes, ~4 could adjoin to 
~7 at VP2.b, and #7 would adjoin to #5 at VPs.b. 
An alternative isfor #4 to adjoin to #5 at VPs.b and 
for ~7 to adjoin to ~4 at VP2.t. The no-adjunction- 
at-foot-node constraint would rule out the latter al- 
ternative and make the derivation tree unique. Note 
that this constraint has been adopted by several hand- 
crafted grammars such as the XTAG grammar for En- 
glish (XTAG-Group, 1998), because it eliminates this 
source of spurious ambiguity. 
SThis decision may affect parsing accuracy of an 
LTAG parser which uses the derivation trees for train- 
ing, but it will not affect he results reported in this 
paper. 
56 
(1) a fully bracketed ttree T*, (2) a set Eset  
of etrees, and (3) a derivation tree D for T*. 
Furthermore, Eset  is the only tree set that 
satisfies all the following conditions: 
(C1)  Decompos i t ion :  The tree set is a de- 
composition of T*, that is, T* would be 
generated if the trees in the set were com- 
bined via the substitution and adjunction 
operations. 
(C2)  LTAG formal i sm:  Each tree in the 
set is a valid etree, according to the LTAG 
formalism. For instance, each tree should 
be lexicalized and the arguments of the 
anchor should be encapsulated in the 
same etree. 
(C3) Target  g rammar :  Each tree in the 
set falls into one of the three types as 
specified in Section 3.1. 
(C4)  T reebank-spec i f i c  in fo rmat ion :  
The head/argument/adjunct distinction 
in the trees is made according to the 
Treebank-specific information provided 
by the user as specified in Section 3.2. 
S 
NP VP 
<1 I 
N V 
I t 
John left 
(T*) 
Ja,. I I 
John left 
(E l) (E2) 
\[ &~m t lea lc~hn \[ Icft 
(E) (E,) (Es) (E6) 
Figure 11: Tree sets for a fully bracketed ttree 
This uniqueness of the tree set may be quite 
surprising at first sight, considering that the 
number of possible decompositions of T* is 
~(2n), where n is the number of nodes in T*. 4 
Instead of giving a proof of the uniqueness, 
4Recall that the process of building etrees has two 
steps. First, LexTract treats each node as a pair of 
the top and bottom parts. The ttree is cut into pieces 
along the boundaries of the top and bottom parts of 
some nodes. The top and the bottom parts of each 
node belong to either two distinct pieces or one piece, 
as a result, there are 2 ~ distinct partitions. Second, 
some non-adjacent pieces in a partition can be glued 
together to form a bigger piece. Therefore, each par- 
tition will result in one or more decompositions of the 
ttree. In total, there are at least 2 n decompositions of 
the ttree. 
we use an example to illustrate how the con- 
ditions (C1)--(C4) rule out all the decompo- 
sitions except the one produced by LexTract. 
In Figure 11, the ttree T* has 5 nodes (i.e., 
S, NP, N, VP, and V). There are 32 distinct 
decompositions for T*, 6 of which are shown 
in the same figure. Out of these 32 decom- 
positions, only five (i.e., E2 - -  E6) are fully 
lexicalized - -  that is, each tree in these tree 
sets is anchored by a lexical item. The rest, 
including El, are not fully lexicalized, and are 
therefore ruled out by the condition (C2). For 
the remaining five etree sets, E2 - -  E4 are 
ruled out by the condition (C3), because ach 
of these tree sets has one tree that violates one 
constraint which says that in a spine-etree an 
argument of the anchor should be a substitu- 
tion node, rather than an internal node. For 
the remaining two, E5 is ruled out by (C4) 
because according to the Head Table provided 
by the user, the head of the S node should be 
V, not N. Therefore, E6, the tree set that is 
produced by LexTract, is the only etree set for 
T* that satisfies (C1)--(C4). 
3.5 The  Exper iments  
We have ran LexTract on the one-million- 
word English Penn Treebank (Marcus et 
al., 1993) and got two Treebank grammars. 
The first one, G1, uses the Treebank's 
tagset. The second Treebank grammar, 
G2, uses a reduced tagset, where some tags 
in the Treebank tagset are merged into a 
single tag. For example, the tags for verbs, 
MD/VB/VBP/VBZ/VBN/VBD/VBG,  are 
merged into a single tag V. The reduced 
tagset is basically the same as the tagset 
used in the XTAG grammar (XTAG-Group, 
1998). G2 is built so that we can compare 
it with the XTAG grammar, as will be 
discussed in the next section. We also ran the 
system on the 100-thousand-word Chinese 
Penn Treebank (Xia et al, 2000b) and on a 
30-thousand-word Korean Penn Treebank. 
The sizes of extracted grammars are shown in 
Table 1. (For more discussion on the Chinese 
and the Korean Treebanks and the compar- 
ison between these Treebank grammars, see 
(Xia et al, 2000a)). The second column of 
the table lists the numbers of unique tem- 
plates in each grammar, where templates are 
etrees with the lexical items removed, s The 
third column shows the numbers of unique 
5For instance, #3, #6 and #9 in Figure 9 are three 
different etrees but they share the same template. An 
etree can be seen as a (word, template) pair. 
57 
etrees. The average numbers of etrees for each 
word type in G1 and G2 are 2.67 and-2.38 
respectively. Because frequent words often 
anchor many etrees, the numbers increase by 
more than 10 times when we consider word 
token, as shown in the fifth and sixth columns 
of the table. G3 and G4 are much smaller 
than G1 and G2 because the Chinese and the 
Korean Treebanks are much smaller than the 
English Treebank. 
In addition to LTAGs, by reading context- 
free rules off the etrees of a Treebank LTAG, 
LexTract also produces CFGs. The numbers 
of unlexicalized context-free rules from G1-- 
G4 are shown in the last column of Table 1. 
Comparing with other CFG extraction algo- 
rithms such as the one in (Krotov et al, 1998), 
the CFGs produced by LexTract have sev- 
eral good properties. For example, they allow 
unary rules and epsilon rules, they are more 
compact and the size of the grammar remains 
monotonic as the Treebank grows. 
Figure 12 shows the log frequency of tem- 
plates and the percentage of template tokens 
covered by template types in G1. 6 In both 
cases, template types are sorted according to 
their frequencies and plotted on the X-axes. 
The figure indicates that a small portion of 
template types, which can be seen as the core 
of the grammar, cover majority of template 
tokens in the Treebank. For example, the first 
100 (500, 1000 and 1500, resp.) templates 
cover 87.1% (96.6~o, 98.4% and 99.0% resp.) 
of the tokens, whereas about half (3411) of 
the templates each occur only once, account- 
ing for only 0.29% of template tokens in total. 
4 App l i ca t ions  o f  LexTract  
In addition to extract LTAGs and CFGs, Lex- 
Tract has been used to perform the following 
tasks: 
? We use the Treebank grammars produced 
by LexTract to evaluate the coverage of 
hand-crafted grammars. 
? We use the (word, template) sequence 
produced by LexTract to re-train Srini- 
vas' Supertaggers (Srinivas, 1997). 
? The derivation trees created by LexTract 
are used to train a statistical LTAG 
parser (Sarkar, 2000). LexTract output 
has also been used to train an LR LTAG 
parser (Prolo, 2000). 
6Similar results hold for G2, G3 and G4. 
? We have used LexTract to retrieve the 
data from Treebanks to test theoret- 
ical linguistic hypotheses uch as the 
Tree-locality Hypothesis (Xia and Bleam, 
20O0). 
? LexTract has a filter that checks the 
plausibility of extracted etrees by decom- 
posing each etree into substructures and 
checking them. Implausible trees are of- 
ten caused by Treebank annotation er- 
rors. Because LexTract maintains the 
mappings between etree nodes and ttree 
nodes, it can detect certain types of an- 
notation errors. We have used LexTract 
for the final cleanup of the Penn Chinese 
Treebank. 
Due to space limitation, in this paper we 
will only discuss the first two tasks. 
4.1 Eva luat ing  the  coverage of 
hand-c ra f ted  grammars  
The XTAG grammar (XTAG-Group, 1998) 
is a hand-crafted large-scale grammar for En- 
glish, which has been developed at University 
of Pennsylvania n the last decade. It has been 
used in many NLP applications uch as gen- 
eration (Stone and Doran, 1997). Evaluating 
the coverage of such a grammar is important 
for both its developers and its users. 
Previous evaluations (Doran et al, 1994; 
Srinivas et al, 1998) of the XTAG grammar 
use raw data (i.e., a set of sentences with- 
out syntactic bracketing). The data are first 
parsed by an LTAG parser and the coverage 
of the grammar is measured as the percent- 
age of sentences in the data that get at least 
one parse, which is not necessarily the correct 
parse. For more discussion on this approach, 
see (Prasad and Sarkar, 2000). 
We propose a new evaluation method that 
takes advantage of Treebanks and LexTract. 
The idea is as follows: given a Treebank T and 
a hand-crafted grammar Gh, the coverage of 
Gh on T can be measured by the overlap of Gh 
and a Treebank grammar Gt that is produced 
by LexTract from T. In this case, we will esti- 
mate the coverage of the XTAG grammar on 
the English Penn Treebank (PTB) using the 
Treebank grammar G2. 
There are obvious differences between these 
two grammars. For example, feature struc- 
tures and multi-anchor etrees are present only 
in the XTAG grammar, whereas frequency in- 
formation is available only in G2. When we 
match templates in two grammars, we disre- 
58 
template etree 
types types 
Eng G1 6926 131,397 
Eng G2 2920 117,356 
Ch G3 1140 21,125 
Kor G4 634 9,787 
word 
types 
49,206 
49,206 
10,772 
6,747 
etree types etree types CFG rules 
per word type i per word token (unlexicalized) 
2.67~ 34.68 1524 
2.38 27.70 675 
1.96 9.13 515 
1.45 2.76 177 
Table 1: Grammars extracted from three Treebanks 
' r  
o.~ 
o.e 
0.7 
o.e 
o.5 
o.4 
o~ 
02  
o.~ 
o 
T~ m T ~  
(a) Frequency of templates (b) Coverage of templates 
Figure 12: Template types and template tokens in G1 
gard the type of information that is present 
only in one grammar. As a result, the map- 
ping between two grammars is not one-to-one. 
XTAG 
G~ 
~equency 
matched unmatched total 
templates templates 
497 507 1004 
215 2705 2920 
82.1% I 17.9% \[100% 
Table 2: Matched templates in two grammars 
Table 2 shows that 497 templates in the 
XTAG grammar and 215 templates in G2 
match, and the latter accounts for 82.1% of 
the template tokens in the PTB.  The remain- 
ing 17.9% template tokens in the PTB do not 
match any template in the XTAG grammar 
because of one of the following reasons: 
(T1) Incorrect templates in G2: These tem- 
plates result from Treebank annotation er- 
rors, and therefore, are not in XTAG. 
(T2) Coordination in XTAG: the templates 
for coordinations in XTAG are generated 
on the fly while parsing (Sarkar and Joshi, 
1996), and are not part of the 1004 templates. 
Therefore, the conj-etrees in G2, which ac- 
count for 3.4% of the template tokens in the 
Treebank, do not match any templates in 
XTAG. 
(T3) Alternative analyses: XTAG and PTB 
sometimes choose different analyses for the 
same phenomenon. For example, the two 
grammars treat reduced relative clauses dif- 
ferently. As a result, the templates used to 
handle those phenomena in these two gram- 
mars do not match according to our defini- 
tion. 
(T4) Construct ions not covered by XTAG: 
Some of such constructions are the unlike 
coordination phrase (UCP), parenthetical 
(PRN), and ellipsis. 
For (T1)--(T3),  the XTAG grammar can 
handle the corresponding constructions al- 
though the templates used in two grammars 
look very different. To find out what construc- 
tions are not covered by XTAG, we manually 
classify 289 of the most frequent unmatched 
templates in G2 according to the reason why 
they are absent from XTAG. These 289 tem- 
plates account for 93.9% of all the unmatched 
template tokens in the Treebank. The results 
are shown in Table 3, where the percentage is
with respect o all the tokens in the Treebank. 
From the table, it is clear that the most com- 
mon reason for mis-matches i (T3). Combin- 
ing the results in Table 2 and 3, we conclude 
that 97.2% of template tokens in the Treebank 
are covered by XTAG, while another 1.7% are 
not. For the remaining 1.1% template tokens, 
we do not know whether or not they are cov- 
ered by XTAG because we have not checked 
the remaining 2416 unmatched templates in 
G2. T 
To summarize, we have just showed that, 
7The number 97.2% is the sum of two numbers: 
the first one is the percentage of matched template to- 
kens (82.1% from Table 2). The secb-nd number is the 
percentage oftemplate tokens which fall under (T1)-- 
(T3), i.e., 16.8%-1.7%=15.1% from Table 3. 
59 
T1 T2 T3 T4 total 
type 51 52 ~93 93 289 
freq 1.1% 3.4% 10.6% 1.7% 16.8% 
Table 3: Classifications of 289 unmatched 
templates 
by comparing templates in the XTAG gram- 
mar with the 'IYeebank grammar produced by 
LexTract, we estimate that the XTAG gram- 
mar covers 97.2% of template tokens in the 
English Treebank. Comparing with previous 
evaluation approach, this :method has several 
advantages. First, the whole process is semi- 
automatic and requires little human effort. 
Second, the coverage can be calculated at ei- 
ther sentence l vel or etree level, which is more 
fine-grained. Third, the method provides a 
list of etrees that can be added to the gram- 
mar to improve its coverage. Fourth, there 
is no need to parse the whole corpus, which 
could have been very time-consuming. 
4.2 Training Super taggers  
A Supertagger (Joshi and Srinivas, 1994; 
Srinivas, 1997) assigns an etree template to 
each word in a sentence. The templates 
are also called Supertags because they in- 
clude more information than Part-of-Speech 
tags. Srinivas implemented the first Supertag- 
ger, and he also built a Lightweight Depen- 
dency Analyzer that assembles the Supertags 
of words to create an almost-parse for the sen- 
tence. Supertaggers have been found useful 
for several applications, such as information 
retrieval (Chandrasekar nd Srinivas, 1997). 
To use a Treebank to train a Supertagger, 
the phrase structures in the Treebank have to 
be converted into (word, Supertag) sequences 
first. Producing such sequences i  exactly one 
of LexTract's main functions, as shown previ- 
ously in Section 3.3.2 and Figure 9. 
Besides LexTract, there are two other at- 
tempts in converting the English Penn Tree- 
bank to train a Supertagger. Srinivas (1997) 
uses heuristics to map structural information 
in the Treebank into Supertags. His method 
is different from LexTract in that the set of 
Supertags in his method is chosen from the 
pre-existing XTAG grammar before the con- 
version starts, whereas LexTract extracts the 
Supertag set from Treebanks. His conversion 
program is also designed for this particular 
Supertag set, and it is not very-easy to port 
it to another Supertag set. A third difference 
is that the Supertags in his converted ata do 
not always fit together, due to the discrep- 
ancy between the XTAG grammar and the  
Treebank annotation and the fact that the 
XTAG grammar does not cover all the tem- 
plates in the Treebank (see Section 4.1). In 
other words, even if the Supertagger is 100% 
accurate, it is possible that the correct parse 
for a sentence can not be produced by com- 
bining those Supertags in the sentence. 
Another work in converting Treebanks into 
LTAGs is described in (Chen and Vijay- 
Shanker, 2000). The method is similar to ours 
in that both work use Head Percolation Tables 
to find the head and both distinguish adjuncts 
from modifiers using syntactic tags and func- 
tional tags. Nevertheless, there are several 
differences: only LexTract explicitly creates 
fully bracketed ttrees, which are identical to 
the derived trees for the sentences. As a re- 
sult, building etrees can be seen as a task of 
decomposing the fully bracketed ttrees. The 
mapping between the nodes in fully bracketed 
ttrees and etrees makes LexTract a useful tool 
for 'IYeebank annotation and error detection. 
The two approaches also differ in how they 
distinguish arguments from adjuncts and how 
they handle coordinations. 
Table 4 lists the tagging accuracy of the 
same trigram Supertagger (Srinivas, 1997) 
trained and tested on the same original PTB 
data. s The difference in tagging accuracy 
is caused by different conversion algorithms 
that convert the original PTB data into the 
(word, template) sequences, which are fed 
to the Supertagger. The results of Chen & 
Vijay-Shanker's method come from their pa- 
per (Chen and Vijay-Shanker, 2000). They 
built eight grammars. We just list two of them 
which seem to be most relevant: C4 uses a re- 
duced tagset while C3 uses the PTB tagset. 
As for Srinivas' results, we did not use the re- 
sults reported in (Srinivas, 1997) and (Chen et 
al., 1999) because they are based on different 
training and testing data. 9 Instead, we re-ran 
SAll use Section 2-21 of the PTB for training, and 
Section 22 or 23 for testing. We choose those sec- 
tions because several state-of-thwart parsers (Collins, 
1997; Ratnaparkhi, 1998; Charniak, 1997) are trained 
on Section 2-21 and tested on Section 23. We include 
the results for Section 22 because (Chen and Vijay- 
Shanker, 2000) is tested on that section. For Srinivas' 
and our grammars, the first line is the results tested on 
Section 23, and the second line is the one for Section 
22. Chen & Vijay-Shauker's results~e for Section 22 
only. 
9He used Section 0-24 minus Section 20 for training 
and the Section 20 for testing. 
60 
his Supertagger using his data on the sections 
that we have chosen. 1? We have calculated 
two baselines for each seg of data. The first 
one tags each word in testing data with the 
most common Supertag w.r.t the word in the 
training data. For an unknown word, just use 
its most common Supertag. For the second 
baseline, we use a trigram POS tagger to tag 
the words first, and then for each word we use 
the most common Supertag w.r.t, the (word, 
POS tag) pair. 
templates 
Srinivas' 483 
our G2 2920 
our G1 6926 
Chen's 2366 - -  
(sect 22) - -  8996 
C4 4911 
C3 8623 
basel base2 
72.59 74.24 
72.14 73.74 
71.45 74.14 
70.54 73.41 
69.70 71.82 
68.79 70.90 
acc 
85.78 
85.53 
84.41 
83.60 
82.21 
81.88 
77.8 - -  
- -  78.9 
78.90 
78.O0 
Table 4: Supertagging results based on three 
different conversion algorithms 
A few observations are in order. First, the 
baselines for Supertagging are lower than the 
one for POS tagging, which is 91%, indicat- 
ing Supertagging is harder than POS tagging. 
Second, the second baseline is slightly bet- 
ter than the first baseline, indicating using 
~?Noticeably, the results we report on Srinivas' data, 
85.78% on Section 23 and 85.53% on Section 22, axe 
lower than 92.2% reported in (Srinivas, 1997) and 
91.37% in (Chen et al, 1999). There axe several 
reasons for the difference. First, the size of training 
data in our report is smaller than the one for his pre- 
vious work; Second, we treat punctuation marks as 
normal words during evaluation because, like other 
words, punctuation marks can anchor etrees, whereas 
he treats the Supertags for punctuation marks as al- 
ways correct. Third, he used some equivalent classes 
during evaluations. If a word is mis-tagged as x, while 
the correct Supertag is y, he considers that not to be 
an error if x and y appear in the same equivalent class. 
We suspect that the reason that those Supertagging er- 
rors axe disregarded is that those errors might not af- 
fect parsing results when the Supertags are combined. 
For example, both adjectives and nouns can modify 
other nouns. The two templates (i.e. Supertags) rep- 
resenting these modification relations look the same 
except for the POS tags of the anchors. If a word 
which should be tagged with one Supertag is mis- 
tagged with the other Supertag, it is likely that the 
wrong Supertag can still fit with other Supertags in 
the sentence and produce the right parse. We did not 
use these quivalent classes in this experiment because 
we are not aware of a systematic way to find all the 
cases in which Supertagging errors do not affect the 
final parsing results. 
POS tags may improve the Supertagging ac- 
curacy, n Third, the Supertagging accuracy 
using G2 is 1.3-1.9% lower than the one using 
Srinivas' data. This is not surprising since the 
size of G2 is 6 times that of Srinivas' grammar. 
Notice that G1 is twice the size of G2 and 
the accuracy using G1 is 2% lower. Fourth, 
higher Supertagging accuracy does not neces- 
sarily means the quality of converted ata are 
better since the underlying rammars differ a 
lot with respect o the size and the coverage. 
A better measure will be the parsing accu- 
racy (i.e., the converted ata should be fed to 
a common LTAG parser and the evaluations 
should be based on parsing results). We are 
currently working on that. Nevertheless, the 
experiments show that the (word, template) 
sequences produced by LexTract are useful for 
training Supertaggers. Our results are slightly 
lower than the ones trained on Srinivas' data, 
but our conversion algorithm has several ap- 
pealing properties: LexTract does not use pre- 
existing Supertag set; LexTract is language- 
independent; he (word, Supertag) sequence 
produced by LexTract fit together. 
5 Conc lus ion  
We have presented a system for grammar ex- 
traction that produces an LTAG from a Tree- 
bank. The output produced by the system 
has been used in many NLP tasks, two of 
which are discussed in the paper. In the first 
task, by comparing the XTAG grammar with 
a Treebank grammar produced by LexTract, 
we estimate that the XTAG grammar covers 
97.2% of template tokens in the English Tree- 
bank. We plan to use the Treebank grammar 
to improve the coverage of the XTAG gram- 
mar. We have also found constructions that 
are covered in the XTAG grammar but do not 
appear in the Treebank. In the second task, 
LexTract converts the Treebank into a format 
that can be used to train Supertaggers, and 
the Supertagging accuracy is compatible to, if 
not better than, the ones based on other con- 
version algorithms. For future work, we plan 
to use derivation trees to train LTAG parsers 
directly and use LexTract to add semantic in- 
formation to the Penn Treebank. 
Re ferences  
R. Chandrasekar nd B. Srinivas. 1997. Glean- 
ing information from the Web: Using Syntax 
to Filter out Irrelevant Information. In Proc. of 
nThe baselines and results on Section 23 for (Chen 
and Vijay-Shanker, 2000) are not available to us. 
61 
AAAI 1997 Spring Symposium on NLP on the 
World Wide Web. 
Eugene Charniak. 1996. Treebank Grammars. In 
Proc. of AAAI-1996. 
Eugene Charniak. 1997. Statistical Parsing with 
a Context-Free Grammar and Word Statistics. 
In Proc. of AAAI-1997. 
John Chen and K. Vijay-Shanker. 2000. Auto- 
mated Extraction of TAGs from the Penn Tree- 
bank. In 6th International Workshop on Pars- 
ing Technologies (IWPT..2000), Italy. 
John Chen, Srinivas Bangalore, and K. Vijay- 
Shanker. 1999. New Models for Improving 
Supertag Disambiguation. In Proc. of EACL- 
1999. 
Mike Collins. 1997. Three Generative, Lexicalised 
Models for Statistical Parsing. In Proc. of the 
35th ACL. 
C. Doran, D. Egedi, B. A. Hockey, B. Srinivas, 
and M. Zaidel. 1994. XTAG System - A Wide 
Coverage Grammar for English. In Proc. of 
COLING-1994, Kyoto, Japan. 
Aravind Joshi and B. Srinivas. 1994. Disambigua- 
tion of Super Parts of Speech (or Supertags): 
Almost Parsing. In Proc. of COLING-1994. 
Aravind Joshi and K. Vijay-Shanker. 1999. Com- 
positional Semantics with LTAG: How Much 
Underspecification Is Necessary? In Proc. of 
3nd International Workshop on Computational 
Semantics. 
Aravind K. Joshi, L. Levy, and M. Takahashi. 
1975. Tree Adjunct Grammars. Journal of 
Computer and System Sciences. 
Laura Kallmeyer and Aravind Joshi. 1999. Un- 
derspecified Semantics with LTAG. 
Alexander Krotov, Mark Hepple, Robert 
Galzauskas, and Yorick Wilks. 1998. Compact- 
ing the Penn Treebank Grammar. In Proc. of 
A CL- COLING. 
David M. Magerman. 1995. Statistical Decision- 
Tree Models for Parsing. In Proc. of the 33rd 
ACL. 
M. Marcus, B. Santorini, and M. A. 
Marcinkiewicz. 1993.  Building a Large 
Annotated Corpus of English: the Penn 
Treebank. Computational Lingustics. 
K. F. McCoy, K. Vijay-Shanker, and G. Yang. 
1992. A Functional Approach to Generation 
with TAG. In Proc. of the 30th A CL. 
Martha Palmer, Owen Rainbow, and Alexis 
Nasr. 1998. Rapid Prototyping of Domain- 
Specific Machine Translation System. In Proc. 
of AMTA-1998, Langhorne, PA. 
Rashmi Prasad and Anoop Sarkar. 2000. Compar- 
ing Test-Suite Based Evaluation and Corpus- 
Based Evaluation of a Wide-Coverage Grammar 
for English. In Proc. of LREC satellite work- 
shop Using Evaluation within HLT Programs: 
Results and Trends, Athen, Greece. 
Carlos A. Prolo. 2000. An Efficient LR Parser 
Generator for TAGs. In 6th International 
Workshop on Parsing Technologies (IWPT 
2000), Italy. 
Adwait Ratnaparkhi. 1998. Maximum Entropy 
Models for Natural Language Ambiguity Resolu- 
tion. Ph.D. thesis, University of Pennsylvania. 
Anoop Sarkar and Aravind Joshi. 1996. Coordi- 
nation in Tree Adjoining Grammars: Formaliza- 
tion and Implementation. In Proc. of the 18th 
COLING, Copenhagen, Denmark. 
Anoop Sarkar. 2000. Practical Experiments in 
Parsing using Tree Adjoining Grammars. In 
Proc. of 5th International Workshop on TAG 
and Related Frameworks (TAG+5). 
The XTAG-Group. 1998. A Lexicalized Tree Ad- 
joining Grammar for English. Technical Report 
IRCS 98-18, University of Pennsylvania. 
Yves Schabes and Stuart Shieber. 1992. An Al- 
ternative Conception of Tree-Adjoining Deriva- 
tion. In Proc. of the 20th Meeting of the Asso- 
ciation for Computational Linguistics. 
Yves Schabes. 1990. Mathematical nd Computa- 
tional Aspects of Lexicalized Grammars. Ph.D. 
thesis, University of Pennsylvania. 
Kiyoaki Shirai, Takenobu Tokunaga, and Hozumi 
Tanaka. 1995.  Automatic Extraction of 
Japanese Grammar from a Bracketed Corpus. 
In Proc. of Natural Language Processing Pacific 
Rim Symposium (NLPRS-1995). 
B. Srinivas, Anoop Sarkar, Christine Doran, and 
Beth Ann Hockey. 1998. Grammar and Parser 
Evaluation in the XTAG Project. In Workshop 
on Evaluation of Parsing Systems, Granada, 
Spain. 
B. Srinivas. 1997. Complexity of Lexical De- 
scriptions and Its Relevance to Partial Parsing. 
Ph.D. thesis, University of Pennsylvania. 
Matthew Stone and Christine Doran. 1997. Sen- 
tence Planning as Description Using Tree Ad- 
joining Grammar. In Proc. of the 35th A CL. 
Bonnie Webber and Aravind Joshi. 1998. Anchor- 
ing a Lexicalized Tree Adjoining Grammar for 
Discourse. In Proc. of A CL-COLING Workshop 
on Discourse Relations and Discourse Markers. 
Fei Xia and Tonia Bleam. 2000. A Corpus-Based 
Evaluation of Syntactic Locality in TAGs. In 
Proc. of 5th International Workshop on TAG 
and Related Frameworks (TAG+5). 
Fei Xia, Chunghye Han, Martha Palmer, and 
Aravind Joshi. 2000a. Comparing Lexicalized 
Treebank Grammars Extracted from Chinese, 
Korean, and English Corpora. In Proc. of the 
2nd GT~inese Language Processing Workshop, 
Hong Kong, China. 
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen 
Okurowski, John Kovarik, Shizhe Huang, Tony 
Kroch, and Mitch Marcus. 2000b. Developing 
Guidelines and Ensuring Consistency for Chi- 
nese Text Annotation. In Proc. of the 2nd In- 
ternational Conference on Language Resources 
and Evaluation (LREC-2000),-Athens, Greece. 
62 
Combining Contextual Features for Word Sense Disambiguation
Hoa Trang Dang and Martha Palmer
Department of Computer and Information Sciences
University of Pennsylvania
Philadelphia, PA, USA, 19104
{htd,mpalmer}linc.cis.upenn.edu
Abstract
In this paper we present a maximum en-
tropy Word Sense Disambiguation system
we developed which performs competi-
tively on SENSEVAL-2 test data for En-
glish verbs. We demonstrate that using
richer linguistic contextual features sig-
nificantly improves tagging accuracy, and
compare the system?s performance with
human annotator performance in light
of both fine-grained and coarse-grained
sense distinctions made by the sense in-
ventory.
1 Introduction
Highly ambiguous words pose continuing problems
for Natural Language Processing (NLP) applica-
tions. They can lead to irrelevant document re-
trieval in IR systems, and inaccurate translations in
Machine Translation systems (Palmer et al, 2000).
While homonyms like bank are fairly tractable, pol-
ysemous words like run, with related but subtly dis-
tinct meanings, present the greatest hurdle for Word
Sense Disambiguation (WSD). SENSEVAL-1 and
SENSEVAL-2 have attempted to provide a frame-
work for evaluating automatic systems by creating
corpora tagged with fixed sense inventories, which
also enables the training of supervised WSD sys-
tems.
In this paper we describe a maximum entropy
WSD system that combines information from many
different sources, using as much linguistic knowl-
edge as can be gathered automatically by current
NLP tools. Maximum entropy models have been
applied to a wide range of classification tasks in
NLP (Ratnaparkhi, 1998). Our maximum entropy
system performed competitively with the best per-
forming systems on the English verb lexical sample
task in SENSEVAL-1 and SENSEVAL-2. We com-
pared the system performance with human annota-
tor performance in light of both fine-grained and
coarse-grained sense distinctions made by WordNet
in SENSEVAL-2, and found that many of the sys-
tem?s errors on fine-grained senses stemmed from
the same sources that caused disagreements between
human annotators. These differences were par-
tially resolved by backing off to more coarse-grained
sense-groups, which are sometimes necessary when
even human annotators cannot make the fine-grained
sense distinctions specified in the dictionary.
2 Related Work
While it is possible to build an automatic sense tag-
ger using only the dictionary definitions, the most
accurate systems tend to take advantage of super-
vised learning. The system with the highest overall
performance in SENSEVAL-1 used Yarowsky?s hier-
archical decision lists (Yarowsky, 2000); while there
is a large set of potential features, only a small num-
ber is actually used to determine the sense of any
given instance of a word. Chodorow, Leacock and
Miller (Chodorow et al, 2000) also achieved high
accuracy using naive bayesian models for WSD,
combining sets of linguistically impoverished fea-
tures that were classified as either topical or local.
Topical features consisted of a bag of open-class
words in a wide window covering the entire con-
text provided; local features were words and parts of
speech within a small window or at particular offsets
                     July 2002, pp. 88-94.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
from the target word. The system was configured to
use only local, only topical, or both local and topical
features for each word, depending on which configu-
ration produced the best result on a held-out portion
of the training data.
Previous experiments (Ng and Lee, 1996) have
explored the relative contribution of different knowl-
edge sources to WSD and have concluded that collo-
cational information is more important than syntac-
tic information. Additionally, Pedersen (Pedersen,
2001; Pedersen, 2000) has pursued the approach
of using simple word bigrams and other linguisti-
cally impoverished feature sets for sense tagging, to
establish upper bounds on the accuracy of feature
sets that do not impose substantial pre-processing
requirements. In contrast, we wish to demonstrate
that such pre-processing significantly improves ac-
curacy for sense-tagging English verbs, because we
believe that they allow us to extract a set of features
that more closely parallels the information humans
use for sense disambiguation.
3 System Description
We developed an automatic WSD system that uses
a maximum entropy framework to combine linguis-
tic contextual features from corpus instances of each
verb to be tagged. Under the maximum entropy
framework (Berger et al, 1996), evidence from dif-
ferent features can be combined with no assump-
tions of feature independence. The automatic tag-
ger estimates the conditional probability that a word
has sense x given that it occurs in context y, where
y is a conjunction of features. The estimated proba-
bility is derived from feature weights which are de-
termined automatically from training data so as to
produce a probability distribution that has maximum
entropy, under the constraint that it is consistent with
observed evidence.
In order to extract the linguistic features neces-
sary for the model, all sentences were first automat-
ically part-of-speech-tagged using a maximum en-
tropy tagger (Ratnaparkhi, 1998) and parsed using
the Collins parser (Collins, 1997). In addition, an
automatic named entity tagger (Bikel et al, 1997)
was run on the sentences to map proper nouns to a
small set of semantic classes. Following work by
Chodorow, Leacock and Miller, we divided the pos-
sible model features into topical and local contex-
tual features. Topical features looked for the pres-
ence of keywords occurring anywhere in the sen-
tence and any surrounding sentences provided as
context (usually one or two sentences). The set
of 200-300 keywords is specific to each lemma to
be disambiguated, and is determined automatically
from training data so as to minimize the entropy of
the probability of the senses conditioned on the key-
word.
The local features for a verb w in a particular sen-
tence tend to look only within the smallest clause
containing w. They include collocational features
requiring no linguistic preprocessing beyond part-
of-speech tagging (1), syntactic features that capture
relations between the verb and its complements (2-
4), and semantic features that incorporate informa-
tion about noun classes for objects (5-6):
1. the word w, the part of speech of w, and words
at positions -2, -1, +1, +2, relative to w
2. whether or not the sentence is passive
3. whether there is a subject, direct object, indi-
rect object, or clausal complement (a comple-
ment whose node label is S in the parse tree)
4. the words (if any) in the positions of subject,
direct object, indirect object, particle, preposi-
tional complement (and its object)
5. a Named Entity tag (PERSON, ORGANIZA-
TION, LOCATION) for proper nouns appear-
ing in (4)
6. WordNet synsets and hypernyms for the nouns
appearing in (4)1
This set of local features relies on access to syntac-
tic structure as well as semantic class information,
and represents our move towards using richer syn-
tactic and semantic knowledge sources to model hu-
man performance.
1Nouns were not disambiguated in any way, and all possible
synsets and hypernyms for the noun were included. No separate
disambiguation of noun complements was done because, given
enough data, the maximum entropy model should assign high
weights to the correct semantic classes of the correct noun sense
if they represent defining selectional restrictions.
4 Evaluation
In this section we describe the system performance
on the verbs from SENSEVAL-1 and SENSEVAL-2.
The system was built after SENSEVAL-1 but before
SENSEVAL-2.2
SENSEVAL-1 SENSEVAL-1 used a DARPA-style
evaluation format where the participants were pro-
vided with hand-annotated training data and test
data. The lexical inventory used was the Hector lex-
icon, developed jointly by DEC and Oxford Univer-
sity Press (Kilgarriff and Rosenzweig, 2000). By
allowing for discussion and revision of confusing
lexical entries during tagging, before the final test
data was tagged, inter-annotator agreement of over
90% was eventually achieved. However, the Hector
lexicon was very small and under proprietary con-
straints, making it an unsuitable candidate for ap-
plications requiring a large-scale, publicly-available
dictionary.
SENSEVAL-2 The subsequent SENSEVAL-2 exer-
cise used a pre-release version of WordNet1.7 which
is much larger than Hector and is more widely used
in NLP applications. The average training set size
for verbs was only about half of that provided in
SENSEVAL-1, while the average polysemy of each
verb was higher3. Smaller training sets and the
use of a large-scale, publicly available dictionary ar-
guably make SENSEVAL-2 a more indicative evalu-
ation of WSD systems in the current NLP environ-
ment than SENSEVAL-1. The role of sense groups
was also explored as a way to address the pop-
ular criticism that WordNet senses are too vague
and fine-grained. During the data preparation for
SENSEVAL-2, previous WordNet groupings of the
verbs were carefully re-examined, and specific se-
mantic criteria were manually associated with each
group. This occasionally resulted in minor revisions
of the original groupings (Fellbaum et al, 2001).
This manual method of creating a more coarse-
grained sense inventory from WordNet contrasts
with automatic methods that rely on existing se-
2The system did not compete officially in SENSEVAL-2 be-
cause it was developed by people who were involved in coordi-
nating the English verbs lexical sample task.
3The average number of senses per verb in the training data
was 11.6 using the Hector dictionary in SENSEVAL-1, and 15.6
using WordNet1.7 in SENSEVAL-2.
mantic links in WordNet (Mihalcea and Moldovan,
2001), which can produce divergent dictionaries.
Our system performs competitively with the
best performing systems in SENSEVAL-1 and
SENSEVAL-2. Measuring accuracy as the recall
score (which is equal to precision in our case be-
cause the system assigns a tag to every instance), we
compare the system?s coarse-grained scores using
the revised groupings versus random groupings, and
demonstrate the coherence and utility of the group-
ings in reconciling apparent tagging disagreements.
4.1 SENSEVAL-1 Results
The maximum entropy WSD system?s perfor-
mance on the verbs from the evaluation data for
SENSEVAL-1 (Kilgarriff and Rosenzweig, 2000) ri-
valed that of the best-performing systems. Table 1
shows the performance of variants of the system us-
ing different subsets of possible features. In addition
to experimenting with different combinations of lo-
cal/topical features, we attempted to undo passiviza-
tion transformations to recover underlying subjects
and objects. This was expected to increase the accu-
racy with which verb arguments could be identified,
helping in cases where selectional restrictions on ar-
guments played an important role in differentiating
between senses.
The best overall variant of the system for verbs
did not use WordNet class features, but included
topical keywords and passivization transformation,
giving an average verb accuracy of 72.3%. This
falls between Chodorow, Leacock, and Miller?s ac-
curacy of 71.0%, and Yarowsky?s 73.4% (74.3%
post-workshop). If only the best combination of fea-
ture sets for each verb is used, then the maximum en-
tropy models achieve 73.7% accuracy. Even though
our system used only the training data provided and
none of the information from the dictionary itself,
it was still competitive with the top performing sys-
tems which also made use of the dictionary to iden-
tify multi-word constructions. As we show later,
using this additional piece of information improves
performance substantially.
In addition to the SENSEVAL-1 verbs, we ran the
system on the SENSEVAL-1 data for shake, which
contains both nouns and verbs. The system sim-
ply excluded verb complement features whenever
the part-of-speech tagger indicated that the word
task lex lex+topic lex+trans+topic wn wn+topic wn+trans+topic
amaze 0.957 0.928 0.942 0.957 0.899 0.913
bet-v 0.709 0.667 0.667 0.718 0.650 0.650
bother 0.866 0.852 0.847 0.837 0.828 0.823
bury 0.468 0.502 0.517 0.572 0.537 0.532
calculate 0.867 0.902 0.904 0.862 0.881 0.872
consume 0.481 0.492 0.508 0.454 0.503 0.454
derive 0.682 0.682 0.691 0.659 0.664 0.696
float-v 0.437 0.441 0.445 0.406 0.445 0.432
invade 0.560 0.522 0.531 0.580 0.551 0.536
promise-v 0.906 0.902 0.902 0.888 0.893 0.893
sack-v 0.972 0.972 0.972 0.966 0.966 0.966
scrap-v 0.812 0.866 0.871 0.796 0.876 0.882
seize 0.653 0.741 0.745 0.660 0.691 0.703
verbs 0.705 0.718 0.723 0.703 0.711 0.709
shake-p 0.744 0.725 0.742 0.767 0.770 0.758
Table 1: Accuracy of different variants of maximum entropy models on SENSEVAL-1 verbs. Only local in-
formation was used, unless indicated by ?+topic,? in which case the topical keyword features were included
in the model; ?wn? indicates that WordNet class features were used, while ?lex? indicates only lexical and
named entity tag features were used for the noun complements; ?+trans? indicates that an attempt was made
to undo passivization transformations.
to be sense-tagged was not a verb. Even on this
mix of nouns and verbs, the system performed
well compared with the best system for shake from
SENSEVAL-1, which had an accuracy of 76.5% on
the same task.
4.2 SENSEVAL-2 Results
We also tested the WSD system on the verbs from
the English lexical sample task for SENSEVAL-2.
In contrast to SENSEVAL-1, senses involving multi-
word constructions could be directly identified from
the sense tags themselves (through the WordNet
sense keys that were used as sense tags), and the
head word and satellites of multi-word construc-
tions were explicitly marked in the training and test
data. This additional annotation made it much eas-
ier for our system to incorporate information about
the satellites, without having to look at the dictio-
nary (whose format may vary from one task to an-
other). The best-performing systems on the English
verb lexical sample task (including our own) filtered
out possible senses based on the marked satellites,
and this improved performance.
Table 2 shows the performance of the system us-
ing different subsets of features. While we found lit-
tle improvement from transforming passivized sen-
tences into a more canonical form to recover under-
lying arguments, there is a clear improvement in per-
formance as richer linguistic information is incorpo-
rated in the model. Adding topical keywords also
helped.
Incorporating topical keywords as well as col-
locational, syntactic, and semantic local features,
our system achieved 59.6% and 69.0% accuracy
using fine-grained and coarse-grained scoring, re-
spectively. This is in comparison to the next best-
performing system, which had fine- and coarse-
grained scores of 57.6% and 67.2% (Palmer et al,
2001). Here we see the benefit from including a filter
that only considered phrasal senses whenever there
were satellites of multi-word constructions marked
in the test data; had we not included this filter, our
fine- and coarse-grained scores would have been
only 56.9% and 66.1%.
Table 3 shows a breakdown of the number of
senses and groups for each verb, the fine-grained
accuracy of the top three official SENSEVAL-2 sys-
tems, fine- and coarse-grained accuracy of our maxi-
Feature Type (local only) Accuracy Feature Type (local and topical) Accuracy
collocation 47.6 collocation 49.8
+ syntax 54.9 + syntax 57.1
+ syntax + transform 55.1 + syntax + transform 57.3
+ syntax + semantics 58.3 + syntax + semantics 59.6
+ syntax + semantics + transform 58.9 + syntax + semantics + transform 59.5
Table 2: Accuracy of maximum entropy system using different subsets of features for SENSEVAL-2 verbs.
Verb Senses Groups SMULS JHU KUNLP MX MX-c ITA ITA-c
begin 8 8 87.5 71.4 81.4 83.2 83.2 81.2 81.4
call 23 16 40.9 43.9 48.5 47.0 63.6 69.3 89.2
carry 27 17 39.4 51.5 45.5 37.9 48.5 60.7 75.3
collaborate 2 2 90.0 90.0 90.0 90.0 90.0 75.0 75.0
develop 15 5 36.2 42.0 42.0 49.3 68.1 67.8 85.2
draw 32 20 31.7 41.5 34.1 36.6 51.2 76.7 82.5
dress 14 8 57.6 59.3 71.2 61.0 89.8 86.5 100.0
drift 9 6 59.4 53.1 53.1 43.8 43.8 50.0 50.0
drive 15 10 52.4 42.9 54.8 59.5 78.6 58.8 71.7
face 7 4 81.7 80.6 82.8 81.7 90.3 78.6 97.4
ferret 1 1 100.0 100.0 100.0 100.0 100.0 100.0 100.0
find 17 10 29.4 26.5 27.9 27.9 39.7 44.3 56.9
keep 27 22 44.8 55.2 44.8 56.7 58.2 79.1 80.1
leave 14 10 47.0 51.5 50.0 62.1 66.7 67.2 80.5
live 10 8 67.2 59.7 59.7 68.7 70.1 79.7 87.2
match 8 4 40.5 52.4 52.4 47.6 69.0 56.5 82.6
play 25 18 50.0 45.5 37.9 50.0 51.5 * *
pull 33 28 48.3 55.0 45.0 53.3 68.3 68.1 72.2
replace 4 2 44.4 57.8 55.6 62.2 93.3 65.9 100.0
see 21 13 37.7 42.0 39.1 47.8 55.1 70.9 75.5
serve 12 7 49.0 54.9 68.6 68.6 72.5 90.8 93.2
strike 26 21 38.9 48.1 40.7 33.3 44.4 76.2 90.5
train 9 4 41.3 54.0 58.7 57.1 69.8 28.8 55.0
treat 6 5 63.6 56.8 56.8 56.8 63.6 96.9 97.5
turn 43 31 35.8 44.8 37.3 44.8 56.7 74.2 89.4
use 7 4 72.4 72.4 65.8 65.8 78.9 74.3 89.4
wander 4 2 74.0 78.0 82.0 82.0 90.0 65.0 90.0
wash 13 10 66.7 58.3 83.3 75.0 75.0 87.5 90.6
work 21 14 43.3 45.0 45.0 41.7 56.7 * *
TOTAL 15.6 10.7 56.3 56.6 57.6 59.6 69.0 71.3 82.0
Table 3: Number of senses and sense groups in training data for each SENSEVAL-2 verb; fine-grained
accuracy of top three competitors (JHU, SMULS, KUNLP) in SENSEVAL-2 English verbs lexical sample
task; fine-grained (MX) and coarse-grained accuracy (MX-c) of maximum entropy system; inter-tagger
agreement for fine-grained senses (ITA) and sense groups (ITA-c). *No inter-tagger agreement figures were
available for ?play? and ?work?.
mum entropy system, and human inter-tagger agree-
ment on fine-grained and coarse-grained senses.
Overall, coarse-grained evaluation using the groups
improved the system?s score by about 10%. This
is consistent with the improvement we found in
inter-tagger agreement for groups over fine-grained
senses (82% instead of 71%). As a base-line, to en-
sure that the improvement did not come simply from
the lower number of tag choices for each verb, we
created random groups. Each verb had the same
number of groups, but with the senses distributed
randomly. We found that these random groups pro-
vided almost no benefit to the inter-annotator agree-
ment figures (74% instead of 71%), confirming the
greater coherence of the manual groupings.
4.3 Analysis of errors
We found that the grouped senses for call substan-
tially improved performance over evaluating with
respect to fine-grained senses; the system achieved
63.6% accuracy with coarse-grained scoring using
the groups, as compared to 47.0% accuracy with
fine-grained scoring. When evaluated against the
fine-grained senses, the system got 35 instances
wrong, but 11 of the ?incorrect? instances were
tagged with senses that were actually in the same
group as the correct sense. This group of senses dif-
fers from others in the ability to take a small clause
as a complement, which is modeled as a feature in
our system. Here we see that the system benefits
from using syntactic features that are linguistically
richer than the features that have been used in the
past.
29% of errors made by the tagger on develop were
due to confusing Sense 1 and Sense 2, which are in
the same group. The two senses describe transitive
verbs that create new entities, characterized as either
?products, or mental or artistic creations: CREATE
(Sense 1)? or ?a new theory of evolution: CREATE
BY MENTAL ACT (Sense 2).? Instances of Sense 1
that were tagged as Sense 2 by the system included:
Researchers said they have developed a genetic en-
gineering technique for creating hybrid plants for
a number of key crops; William Gates and Paul
Allen developed an early language-housekeeper sys-
tem for PCs. Conversely, the following instances of
Sense 2 were tagged as Sense 1 by the tagger: A Pur-
due University team hopes to develop ways to mag-
netically induce cardiac muscle contractions; Kobe
Steel Ltd. adopted Soviet casting technology used it
until it developed its own system. Based on the direct
object of develop, the automatic tagger was hard-
pressed to differentiate between developing a tech-
nique/system (Sense 1) and developing a way/system
(Sense 2).
Analysis of inter-annotator disagreement between
two human annotators doing double-blind tagging
revealed similar confusion between these two senses
of develop; 25% of the human annotator disagree-
ments on develop involved determining which of
these two senses should be applied to phrases like
develop a better way to introduce crystallography
techniques. These instances that were difficult for
the automatic WSD system, were also difficult for
human annotators to differentiate consistently.
These different senses are clearly related, but the
relation is not reflected in their hypernyms, which
emphasize the differences in what is being high-
lighted by each sense, rather than the similarities.
Methods of evaluation that automatically back off
from synset to hypernyms (Lin, 1997) would fail
to credit the system for ?mistagging? an instance
with a closely related sense. Manually created sense
groups, on the other hand, can capture broader, more
underspecified senses which are not explicitly listed
and which do not participate in any of the WordNet
semantic relations.
5 Conclusion
We have demonstrated that our approach to disam-
biguating verb senses using maximum entropy mod-
els to combine as many linguistic knowledge sources
as possible, yields state-of-the-art performance for
English. This may be a language-dependent feature,
as other experiments indicate that additional linguis-
tic pre-processing does not necessarily improve tag-
ging accuracy for languages like Chinese (Dang et
al., 2002).
In examining the instances that proved trouble-
some to both the human taggers and the automatic
system, we found errors that were tied to subtle
sense distinctions which were reconciled by back-
ing off to the more coarse-grained sense groups.
Achieving higher inter-annotator agreement is nec-
essary in order to provide consistent training data
for supervised WSD systems. Lexicographers have
long recognized that many natural occurrences of
polysemous words are embedded in underspecified
contexts and could correspond to more than one spe-
cific sense. Annotators need the option of selecting,
as an alternative to an explicit sense, either a group
of specific senses or a single, broader sense, where
specific meaning nuances are subsumed. Sense
grouping, already present in a limited way in Word-
Net?s verb component, can be guided and enhanced
by the analysis of inter-annotator disagreements and
the development of explicit sense distinction criteria
that such an analysis provides.
6 Acknowledgments
This work has been supported by National Sci-
ence Foundation Grants, NSF-9800658 and NSF-
9910603, and DARPA grant N66001-00-1-8915 at
the University of Pennsylvania. The authors would
also like to thank the anonymous reviewers for their
valuable comments.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1).
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: A high-
performance learning name-finder. In Proceedings of
the Fifth Conference on Applied Natural Language
Processing, Washington, DC.
Martin Chodorow, Claudia Leacock, and George A.
Miller. 2000. A topical/local classifier for word sense
identification. Computers and the Humanities, 34(1-
2), April. Special Issue on SENSEVAL.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, Madrid, Spain, July.
Hoa Trang Dang, Ching yi Chia, Martha Palmer, and Fu-
Dong Chiou. 2002. Simple features for chinese word
sense disambiguation. In Proceedings of Coling-02,
Taipei, Taiwain.
Christiane Fellbaum, Martha Palmer, Hoa Trang Dang,
Lauren Delfs, and Susanne Wolf. 2001. Manual and
automatic semantic annotation with WordNet. In Pro-
ceedings of the Workshop on WordNet and Other Lex-
ical Resources, Pittsburgh, PA.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and
results for English SENSEVAL. Computers and the
Humanities, 34(1-2), April. Special Issue on SENSE-
VAL.
Dekang Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Proceed-
ings of the 35th Annual Meeting of the ACL, Madrid,
Spain.
Rada Mihalcea and Dan I. Moldovan. 2001. Automatic
generation of a coarse grained WordNet. In Proceed-
ings of the Workshop on WordNet and Other Lexical
Resources, Pittsburgh, PA.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association for
Computational Linguistics, Santa Cruz, CA, June.
M. Palmer, Chunghye Han, Fei Xia, Dania Egedi, and
Joseph Rosenzweig. 2000. Constraining lexical selec-
tion across languages using tags. In Anne Abeille and
Owen Rambow, editors, Tree Adjoining Grammars:
formal, computational and linguistic aspects. CSLI,
Palo Alto, CA.
Martha Palmer, Christiane Fellbaum, Scott Cotton, Lau-
ren Delfs, and Hoa Trang Dang. 2001. English
tasks: All-words and verb lexical sample. In Proceed-
ings of SENSEVAL-2: Second International Workshop
on Evaluating Word Sense Disambiguation Systems,
Toulouse, France, July.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive bayesian classifiers for word sense
disambiguation. In Proceedings of the 1st Meeting
of the North American Chapter of the Association for
Computational Linguistics, Seattle, WA.
Ted Pedersen. 2001. A decision tree of bigrams is an
accurate predictor of word sense. In Proceedings of
the 2nd Meeting of the North American Chapter of the
Association for Computational Linguistics, Pittsburgh,
PA.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
David Yarowsky. 2000. Hierarchical decision lists for
word sense disambiguation. Computers and the Hu-
manities, 34(1-2), April. Special Issue on SENSE-
VAL.
Annotating the Propositions in the Penn Chinese Treebank
Nianwen Xue
Dept. of Computer and Info. Science
University of Pennsylvania
Philadelphia, PA 19104, USA
xueniwen@linc.cis.upenn.edu
Martha Palmer
Dept. of Computer and Info. Science
University of Pennsylvania
Philadelphia, PA 19104, USA
mpalmer@linc.cis.upenn.edu
Abstract
In this paper, we describe an approach to
annotate the propositions in the Penn Chi-
nese Treebank. We describe how diathe-
sis alternation patterns can be used to
make coarse sense distinctions for Chi-
nese verbs as a necessary step in anno-
tating the predicate-structure of Chinese
verbs. We then discuss the representation
scheme we use to label the semantic argu-
ments and adjuncts of the predicates. We
discuss several complications for this type
of annotation and describe our solutions.
We then discuss how a lexical database
with predicate-argument structure infor-
mation can be used to ensure consistent
annotation. Finally, we discuss possible
applications for this resource.
1 Introduction
Linguistically interpreted corpora are instrumental
in supervised machine learning paradigms of natu-
ral language processing. The information encoded
in the corpora to a large extent determines what can
be learned by supervised machine learning systems.
Therefore, it is crucial to encode the desired level of
information for its automatic acquisition. The cre-
ation of the Penn English Treebank (Marcus et al,
1993), a syntactically interpreted corpus, played a
crucial role in the advances in natural language pars-
ing technology (Collins, 1997; Collins, 2000; Char-
niak, 2000) for English. The creation of the Penn
Chinese Treebank (Xia et al, 2000) is also begin-
ning to help advance technologies in Chinese syn-
tactic analysis (Chiang, 2000; Bikel and Chiang,
2000). Since the treebanks are generally syntac-
tically oriented (cf. Sinica Treebank (Chen et al,
to appear)), the information encoded there is ?shal-
low?. Important information useful for natural lan-
guage applications is missing. Most notably, signifi-
cant regularities in the predicate-argument structure
of lexical items are not captured. Recent effort in
semantic annotation, the creation of the Penn Propo-
sition Bank (Kingsbury and Palmer, 2002) on top
of the Penn English Treebank is beginning to ad-
dress this issue for English. In this new layer of
annotation, the regularities of the predicates, mostly
verbs, are captured in the predicate-argument struc-
ture. For example, in the sentences ?The Congress
passed the bill? and ?The bill passed?, it is intu-
itively clear that ?the bill? plays the same role in the
two occurrences of the verb ?pass?. Similar regular-
ities also exist in Chinese. For example, in ? /this
/CL /bill /pass /AS? and ? /Congress
/pass /AS /this /CL /bill?, ? /bill?
also plays the same role for the verb ? /pass? even
though it occurs in different syntactic positions (sub-
ject and object respectively).
Capturing such lexical regularities requires a
?deeper? level of annotation than generally provided
in a typical syntactically oriented treebank. It also
requires making sense distinctions at the appropriate
granularity. For example, the regularities demon-
strated for ?pass? does not exist in other senses of
this verb. For example, in ?He passed the exam?
and ?He passed?, the object ?the exam? of the tran-
sitive use of ?pass? does not play the same role as
the subject ?he? of the intransitive use. In fact, the
subject plays the same role in both sentences.
However, how deep the annotation can go is con-
strained by two important factors: how consistently
human annotators can implement this type of anno-
tation (the consistency issue) and whether the an-
notated information is learnable by machine (the
learnability issue). Making fine-grained sense dis-
tinctions, in particular, has been known to be dif-
ficult for human annotators as well as machine-
learning systems (Palmer et al, submitted). It seems
generally true that structural information is more
learnable than non-structural information, as evi-
denced by the higher parsing accuracy and relatively
poor fine-grained WSD accuracy. With this in mind,
we will propose a level of semantic annotation that
still can be captured in structural terms and add this
level of annotation to the Penn Chinese Treebank.
The rest of the paper is organized as follows. In Sec-
tion 2, we will discuss the annotation model in de-
tail and describe our representation scheme. We will
discuss some complications in Section 3 and some
implementation issues in Section 4. Possible appli-
cations of this resource are discussed in Section 5.
We will conclude in Section 6.
2 Annotation Model
In this section we describe a model that annotates
the predicate-argument structure of Chinese pred-
icates. This model captures the lexical regulari-
ties by assuming that different instances of a pred-
icate, usually a verb, have the same predicate argu-
ment structure if they have the same sense. Defin-
ing sense has been one of the most thorny issues
in natural language research (Ide and Vronis, 1998),
and the term ?sense? has been used to mean differ-
ent things, ranging from part-of-speech and homo-
phones, which are easier to define, to slippery fine-
grained semantic distinctions that are hard to make
consistently. Determining the ?right? level of sense
distinction for natural language applications is ul-
timately an empirical issue, with the best level of
sense distinction being the level with the least granu-
larity and yet sufficient for a natural language appli-
cation in question. Without gearing towards one par-
ticular application, our strategy is to use the struc-
tural regularities demonstrated in Section 1 to define
sense. Finer sense distinctions without clear struc-
tural indications are avoided. All instances of a pred-
icate that realize the same set of semantic roles are
assumed to have one sense, with the understanding
that not all of the semantic roles for this verb sense
have to be realized in a given verb instance, and that
the same semantic role may be realized in different
syntactic positions. All the possible syntactic real-
izations of the same set of semantic roles for a verb
sense are then alternations of one another. This
state of affairs has been characterized as diathe-
sis alternation and used to establish cross-predicate
generalizations and classifications (Levin, 1993). It
has been hypothesized and demonstrated that verbs
sharing the same disthesis alternation patterns also
have similar meaning postulates. It is equally plausi-
ble to assume then that verb instances having differ-
ent diathesis alternation patterns also have different
semantic properties and thus different senses.
Using diathesis alternation patterns as a diagnos-
tic test, we can identify the different senses for a
verb. Alternating syntactic frames for a particular
verb sense realizing the same set of semantic roles
(we call this roleset) form a frameset and share sim-
ilar semantic properties. It is easy to see that each
frameset, a set of syntactic frames for a verb, corre-
sponds with one roleset and vice versa. From now
on, we use the term frameset instead of sense for
clarity. Each frameset consists of one or more syn-
tactic frames and each syntactic frame realizes one
or more semantic roles. One frame differs from an-
other in the number and type of arguments its pred-
icate actually takes, and one frameset differs from
another in the total number and type of arguments
its predicate CAN take. This is illustrated graphi-
cally in Figure 1.
Annotating the predicate-argument structure in-
volves mapping the frameset identification informa-
tion for a predicate to an actual predicate instance in
the corpus and assign the semantic roles to its argu-
ments based on the syntactic frame of that predicate
instance. It is hoped that since framesets are defined
through diathesis alternation of syntactic frames, the
distinctions made are still structural in nature and
thus are machine-learnable and can be consistently
annotated by human annotators.
So far our discussion has focused on semantic ar-
Verb
FS                    FS               FS                 ......          FS
FR FR FR......FR j
        i
.....
Argk Arg ArgArg0 1 2
   0        1   2
0 1 2
FS = Frameset       FR = Syntactic Frames       Arg = Arguments
Figure 1: Annotation model
guments, which play a central role in determining
the syntactic frames and framesets. There are other
elements in a proposition: semantic adjuncts. Com-
pared with semantic arguments, semantic adjuncts
do not play a role in defining the syntactic frames
or framesets because they occur in a wide variety of
predicates and as a result are not as discriminative as
semantic arguments. On the other hand, since they
can co-occur with a wide variety of predicates, they
are more generalizable and classifiable than seman-
tic arguments. In the next section, we will describe a
representation scheme that captures this dichotomy.
2.1 Representing arguments and adjuncts
Since the number and type of semantic arguments
for a predicate are unique and thus define the seman-
tic roles for a predicate, we label the arguments for
a predicate with a contiguous sequence of integers,
in the form of argN, where
 
is the integer between
0 and 5. Generally, a predicate has fewer than 6 ar-
guments. Since semantic adjuncts are not subcate-
gorized for by the predicate, we use one label argM
for all semantic adjuncts. ArgN identifies the argu-
ments while argM identifies all adjuncts. An argN
uniquely identifies an argument of a predicate even
if it occupies different syntactic positions in different
predicate instances. Missing arguments of a predi-
cate instance can be inferred by noting the missing
argument labels.
Additionally, we also use secondary tags to gen-
eralize and classify the semantic arguments and ad-
juncts when possible. For example, an adjunct re-
ceiving a  tag if it is a temporal adjunct. The
secondary tags are reserved for semantic adjuncts,
predicates that serve as arguments, as well as certain
arguments for phrasal verbs. The 18 secondary tags
and their descriptions are presented in Table 1.
11 functional tags for semantic adjuncts
ADV adverbial, default tag
BNF beneficiary
CND condition
DIR direction
DGR degree
FRQ frequency
LOC locative
MNR manner
PRP purpose or reason
TMP temporal
TPC topic
1 functional tag for predicate as argument
PRD predicate
6 functional tags for arguments to phrasal verbs
AS , , ,
AT ,
INTO , ,
ONTO
TO ,
TOWARDS ,
Table 1: List of functional tags
3 Complications
In this section we discuss several complications in
annotating the predicate-argument structure as de-
scribed in Section 2. Specifically, we discuss the
phenomenon of ?split arguments? and the annota-
tion of nominalized verbs (or deverbal nouns).
3.1 Split Arguments
What can be characterized as ?split arguments? are
cases where a constituent that occurs as one argu-
ment in one sentence can also be realized as mul-
tiple arguments (generally two) for the same pred-
icate in another sentence, without causing changes
in the meaning of the sentences. This phenomenon
surfaces in several different constructions. One such
construction involves ?possessor raising?, where the
possessor (in a broad sense) raises to a higher posi-
tion. Examples 1a and 1b illustrate this. In 1a, the
possessor originates from the subject position and
raises to the topic1 position, while in 1b, the pos-
sessor originates from the object position and raises
1In Chinese, it is possible to have a topic in addition to the
subject. The topic is higher than the subject and plays an im-
portant role in the sentence (Li and Thompson, 1976).
to the subject position. The exact syntactic analysis
is not important here, and what is important is that
one argument in one sentence becomes two in an-
other. The challenge is then to capture this regularity
when annotating the predicate-argument structure of
the verb.
1. Possessor Raising
a. Subject to Topic
(IP (NP-PN-TPC /China)
(NP-TMP /last year)
(NP-SBJ /import-export
/total volume)
(VP /exceed
(QP-OBJ /325 Billion
(CLP /US. Dollar))))
/exceed
arg0-psr: /China
arg0-pse: /import-export /total volume
arg1: /325 Billion /US. Dollar
(IP (NP-TMP /last year)
(NP-SBJ (DNP (NP-PN /China)
/DE)
(NP /import-export
/volume))
(VP /exceed
(QP-OBJ /325 Billion
(CLP /US. Dollar))))
/exceed
arg0: /China /DE /import-export
/volume
arg1: /325 Billion /US. Dollar
b. Object to Subject
(IP (NP-SBJ (NP-PN /China)
(NP /economy
/expansion))
(VP (ADVP /also)
(ADVP /will)
(VP /slow down
(NP-OBJ /speed)))
/slow down
arg1-psr: /China /economy /expansion
arg1-pse: /speed
(IP (NP-SBJ (DNP (NP (NP-PN /China)
(NP /economy
/expansion))
)
(NP /speed))
(VP (ADVP /also)
(ADVP /will)
(VP /slow down))
/slow down
arg1: /China /economy /expansion
/DE /speed
Another case of ?split arguments? involves the co-
ordinated noun phrases. In 2a, for example, the co-
ordinated structure as a whole is an argument to the
verb ? /sign?. In contrast, in 2b, one piece of the
argument, ? /China? is realized as a noun phrase
introduced by a preposition. There is no apparent
difference in meaning for the two sentences.
2. Coordination vs. Prepositional phrase
a. (IP (NP-PN-SBJ /Burma
/and
/China)
(VP (ADVP /already)
(VP /sign
/ASP
(NP-OBJ /border
/trade
/agreement))))
/sign
arg0: /Burma /and /China
arg1: /border /trade /agreement
b. (IP (NP-PN-SBJ /Burma)
(VP (ADVP /already)
(PP /with
(NP-PN /China))
(VP /sign
/ASP
(NP-OBJ /border
/trade
/agreement))))
/sign
arg0-crd: /Burma
arg0-crd: /China
arg1: /border /trade /agreement
There are two ways to capture this type of regu-
larity. One way is to treat each piece as a separate
argument. The problem is that for coordinated noun
phrases, there can be arbitrarily many coordinated
constituents. So we adopt the alternative approach
of representing the entire constituent as one argu-
ment. When the pieces are separate constituents,
they will receive the same argument label, with dif-
ferent secondary tags indicating they are parts of a
larger constituent. For example, in 1, when pos-
sessor raising occurs, the possessor and possessee
receive the same argument label with different sec-
ondary tags psr and pse. In 2b, both ? /China?
and ? /Burma? receive the label arg0, and the sec-
ondary label crd indicates each one is a part of the
coordinated constituent.
3.2 Nominalizations
Another complication involves nominalizations (or
deverbal nouns) and their co-occurrence with light
and not-so-light verbs. A nominalized verb, while
serving as an argument to another predicate (gen-
erally a verb), also has its own predicate-argument
structure. For example, in 3, the predicate-argument
structure for ? /doubt? should be ? ( ,
)?, where all the arguments of ? /doubt?
are embedded in the NP headed by ? /doubt?.
The complication arises when the nominalized noun
is a complement to another verb, as in 4, where
the subject ? /reader? is an argument to both
the verb ? /produce? and the nominalized verb
? /doubt?. More interestingly, the other argument
? /this /CL /news? is realized as an adjunct to
the verb (introduced by a preposition) even though
it bears no apparent thematic relationship to it.
It might be tempting to treat the verb
? /develop? as a ?light verb? that does not
have its own predicate-argument structure, but this
is questionable because ? /doubt? can also take a
noun that is not a nominalized verb: ? /I /towards
/she /develop /LE /feeling?. In addition,
there is no apparent difference in meaning for
? /develop? between this sentence and 4, so there
is little basis to say these are two different senses of
this verb. So we annotate the predicate-argument
structure of both the verb ? ( , )? and the
nominalized verb ? ( , )?.
3. (IP (NP-SBJ (NP /reader)
(DNP (PP /towards
(NP (DP /this
(CLP /CL))
(NP /news)))
)
(NP /doubt))
(VP /deepen
/LE))
/deepen
arg1: /reader /towards /this /CL
/news
4. (IP (NP-SBJ /reader)
(VP (PP-DIR /towards
(NP (DP /this
(CLP /CL))
(NP /news)))
(ADVP /too)
(VP /will
(VP /develop
(NP-OBJ /doubt)))))
/develop
arg0: /reader
arg1: /doubt
4 Implementation
To implement the annotation model presented in
Section 2, we create a lexical database. Each entry is
a predicate listed with its framesets. The set of pos-
sible semantic roles for each frameset are also listed
with a mnemonic explanation. This explanation is
not part of the formal annotation. It is there to help
human annotators understand the different semantic
roles of this frameset. An annotated example is also
provided to help the human annotator.
As illustrated in Example 5, the verb ? /pass?
has three framesets, and each frameset corresponds
with a different meaning. The different meanings
can be diagnosed with diathesis alternations. For
example, when ? /pass? means ?pass through?,
it allows dropped object. That is, the object does
not have to be syntactically realized. When it means
?pass by vote?, it also has an intransitive use. How-
ever, in this case, the verb demonstrates ?subject of
the intransitive / object of the transitive? alternation.
That is, the subject in the intransitive use refers to
the same entity as the object in the transitive use.
When the verb means ?pass an exam, test, inspec-
tion?, there is also the transitive/intransitive alterna-
tion. Only in this case, the object of the transitive
counterpart is now part of the subject in the intran-
sitive use. This is the argument-split problem dis-
cussed in the last section. The three framesets, rep-
resenting three senses, are illustrated in 5.
5. Verb: /pass
Frameset.01: , /pass through
Roles: arg0(?passer?), arg1(?place?)
Example:
(IP (NP-SBJ /train)
(VP (ADVP /now)
(VP /pass
(NP-OBJ /tunnel))))
.01/pass
arg0: /train
arg1: /tunnel
argM-ADV: /now
(IP (NP-SBJ /train)
(VP (ADVP /now)
(VP /pass)))
.01/pass
arg0: /train
argM-ADV: /now
Frameset.02: , ( , )/pass
(an exam, etc.)
(IP (NP-SBJ (DNP (NP /he)
/DE)
(NP /drug inspection))
(VP (ADVP /not)
(VP /pass)))
.02/pass
arg1: /he /DE /drug inspection
(IP (NP-SBJ (NP /he)
(VP (ADVP /not)
(VP /pass)))
(NP-OBJ /drug inspection))
.02/pass
arg1-psr: /he
arg1-pse: /drug inspection
Frameset.03: /pass (a bill, a law, etc.)
(IP (NP-PN-SBJ /the U.S.
/Congress)
(VP (NP-TMP /recently)
(VP /pass
/ASP
(NP-OBJ /interstate
/banking law))))
.03/pass
arg0: /the U.S.
arg1: /interstate /banking law
(IP (NP-SBJ (ADJP /interstate)
(NP /banking law))
(VP (NP-TMP /recently)
(VP /pass
/ASP)))
.03/pass
arg1: /interstate /banking law
The human annotator can use the information
specified in this entry to annotate all instances of
? /pass? in a corpus. When annotating a predicate
instance, the annotator first determines the syntactic
frame of the predicate instance, and then determine
which frameset this frame instantiates. The frame-
set identification is then attached to this predicate
instance. This can be broadly construed as ?sense-
tagging?, except that this type of sense tagging is
coarser, and the ?senses? are based on structural dis-
tinctions rather than just semantic nuances. A dis-
tinction is made only when the semantic distinc-
tions also coincide with some structural distinctions.
The expectation is that this type of sense tagging is
much amenable to automatic machine-learning ap-
proaches. The annotation does not stop here. The
annotator will go on identifying the arguments and
adjuncts for this predicate instance. For the argu-
ments, the annotator will determine which semantic
role each argument realizes, based on the set of pos-
sible roles for this frameset, and attach the appropri-
ate semantic role label (argN) to it. For adjuncts, the
annotator will determine the type of adjunct this is
and attach a secondary tag to argM.
5 Applications
A resource annotated with predicate-argument struc-
ture can be used for a variety of natural language
applications. For example, this level of abstraction
is useful for Information Extraction. The argument
role labels can be easily mapped to an Information
Extraction template, where each role is mapped to a
piece of information that an IE system is interested
in. Such mapping will not be as straightforward if
it is between surface syntactic entities such as the
subject and IE templates.
This level of abstraction can also provide a plat-
form where lexical transfer can take place. It opens
up the possibility of linking a frameset of a predi-
cate in one language with that of another, rather than
using bilingual (or multilingual) dictionaries where
one word is translated into one or more words in a
different language. This type of lexical transfer has
several advantages. One is that the transfer is made
more precise, in the sense that there will be more
cases where one-to-one mapping is possible. Even
in cases where one-to-one mapping is still not possi-
ble, the identification of the framesets of a predicate
will narrow down the possible lexical choices. For
example, sign.02 in the English Proposition Bank
(Kingsbury and Palmer, 2002) will be linked to ?
.01/enter into an agreement?. This type of linking
rules out ? ? as a possible translation for sign.02,
even though it is a translation for other framesets of
the word sign.
The transfer will also be more precise in another
sense, that is, the predicate-argument structure of a
word instance will be preserved during the trans-
fer process. Knowing the arguments of a predicate
instance can further constrain the lexical choices
and rule out translation candidates whose predicate-
argument structures are incompatible. For example,
if the realized arguments of ?sign.01? of the En-
glish Proposition Bank in a given sentence are the
signer, the document, and the signature, among the
translation candidates ? , ? (? .01/enter into
an agreement? is ruled out as a possibility for this
frameset), only ? ? is possible, because ? ? can
only take two arguments, namely, the signer and the
document.
6. /he /at /this /CL /document /LC /sign
/LE /self /DE /name
?He signed his name on this document.?
One might argue that the syntactic subcategoriza-
tion frame obtained from the syntactic parse tree
can also constrain the lexical choices. For exam-
ple, knowing that ?sign? has a subject, an object
and a prepositional phrase should be enough to rule
out ? ? as a possible translation. This argument
breaks down when there are lexical divergences.
The ?document? argument of ? ? can only be re-
alized as a prepositional phrase in Chinese while
in English it can only be realized the direct object
of ?sign?. If the syntactic subcategorization frame
is used to constrain the lexical choices for ?sign?,
? ? will be incorrectly ruled out as a possible
translation. There will be no such problem if the
more abstract predicate-argument structure is used
for this purpose. Even when the document is re-
alized as a prepositional phrase, it is still the same
argument. Of course, ? /sign? is also a possi-
ble translation. So compared with the surface syn-
tactic frames, the predicate-argument structure con-
strains the lexical choices without incorrectly ruling
out legitimate translation candidates. This is under-
standable because the predicate-structure abstracts
away from the syntactic idiosyncracies of the differ-
ent languages and thus are more transferable across
languages.
7. /he /at /this /CL /document /LC /sign
/he /sign /this /CL /document
?He signed this document.?
Annotating the predicate-argument structure as
described in previous sections will not reduce the
lexical choices to one-to-one mappings in call cases.
For example, ? ? can be translated into ?standard-
ize? or ?unite?, even though there is only one frame-
set for both finer senses of this verb. It is conceiv-
able that one might want to posit two framesets, each
for one finer sense of this verb. This is essentially
a trade-off: either one can conduct deep analysis
of the source language, resolve all sense ambigui-
ties on the source side and have a more straightfor-
ward mapping, or one takes the one-to-many map-
pings and select the correct translation on the tar-
get language side. Hopefully, the annotation of the
predicate-argument provides just the right level of
abstraction and the resource described here, with
each predicate annotated with its arguments and ad-
juncts in context, enables the automatic acquisition
of the predicate-argument structure.
6 Summary
In this paper, we described an approach to annotate
the propositions in the Penn Chinese Treebank. We
described how diathesis alternation patterns can be
used to make coarse sense distinctions for Chinese
verbs as a necessary step in annotating the predicate-
structure of predicates. We also described the repre-
sentation scheme we use to label the semantic argu-
ments and adjuncts of the predicates. We discussed
several complications for this type of annotation and
described our solutions. We then discussed how a
lexical database with predicate-argument structure
information can be used to ensure consistent annota-
tion. Finally, we discussed possible applications for
this resource.
7 Acknowledgement
This work is supported by MDA904-02-C-0412.
References
Daniel M. Bikel and David Chiang. 2000. Two statisti-
cal parsing models applied to the chinese treebank. In
Proceedings of the 2nd Chinese Language Processing
Workshop, Hong Kong, China.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of NAACL-2000.
Keh-Jiann Chen, Chu-Ren Huang, Feng-Yi Chen, Chi-
Ching Luo, Ming-Chung Chang, and Chao-Jan Chen.
to appear. Sinica Treebank: Design Criteria, rep-
resentational issues and immplementation. In Anne
Abeille, editor, Building and Using Syntactically An-
notated Corpora. Kluwer.
David Chiang. 2000. Statisitical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 456-463,
Hong Kong.
Mike Collins. 1997. Three Generative, Lexicalised Mod-
els for Statistical Parsing. In Proc. of ACL-1997.
Mike Collins. 2000. Discriminative Reranking for Natu-
ral Language Parsing. In Proc. of ICML-2000.
N. Ide and J. Vronis. 1998. Word sense disambigua-
tion: The state of the art. Computational Linguistics,
24(1):1?40.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC2002), Las Palmas, Spain.
Beth Levin. 1993. English Verbs and Alternations: A
Preliminary Investigation. Chicago: The Unversity of
Chicago Press.
Charles Li and Sandra Thompson. 1976. Subject and
Topic: A new typology of language. In Charles Li,
editor, Subject and Topic. Academic Press.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of English:
the Penn Treebank. Computational Linguistics.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. submitted. Making fine-grained and coarse-
grained sense distinctions, both manually and auto-
matically. Journal of Natural Language Engineering.
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu-Dong Chiou, Shizhe
Huang, Tony Kroch, and Mitch Marcus. 2000. Devel-
oping Guidelines and Ensuring Consistency for Chi-
nese Text Annotation. In Proc. of the 2nd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2000), Athens, Greece.
 	


 ff Synchronous Dependency Insertion Grammars 
A Grammar Formalism for Syntax Based Statistical MT 
Yuan Ding     and     Martha Palmer 
Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA 19104, USA 
{yding, mpalmer}@linc.cis.upenn.edu 
 
Abstract 
This paper introduces a grammar formalism 
specifically designed for syntax-based sta-
tistical machine translation. The synchro-
nous grammar formalism we propose in 
this paper takes into consideration the per-
vasive structure divergence between lan-
guages, which many other synchronous 
grammars are unable to model. A Depend-
ency Insertion Grammars (DIG) is a gen-
erative grammar formalism that captures 
word order phenomena within the depend-
ency representation. Synchronous Depend-
ency Insertion Grammars (SDIG) is the 
synchronous version of DIG which aims at 
capturing structural divergences across the 
languages. While both DIG and SDIG have 
comparatively simpler mathematical forms, 
we prove that DIG nevertheless has a gen-
eration capacity weakly equivalent to that 
of CFG. By making a comparison to TAG 
and Synchronous TAG, we show how such 
formalisms are linguistically motivated. We 
then introduce a probabilistic extension of 
SDIG. We finally evaluated our current im-
plementation of a simplified version of 
SDIG for syntax based statistical machine 
translation. 
1 Introduction 
Dependency grammars have a long history and 
have played an important role in machine translation 
(MT). The early use of dependency structures in ma-
chine translation tasks mainly fall into the category 
of transfer based MT, where the dependency struc-
ture of the source language is first analyzed, then 
transferred to the target language by using a set of 
transduction rules or a transfer lexicon, and finally 
the linear form of the target language sentence is 
generated. 
While the above approach seems to be plausible, 
the transfer process demands intense human effort in 
creating a working transduction rule set or a transfer 
lexicon, which largely limits the performance and 
application domain of the resultant machine transla-
tion system. 
In the early 1990s, (Brown et. al. 1993) intro-
duced the idea of statistical machine translation, 
where the word to word translation probabilities and 
sentence reordering probabilities are estimated from 
a large set of parallel sentence pairs. By having the 
advantage of leveraging large parallel corpora, the 
statistical MT approach outperforms the traditional 
transfer based approaches in tasks for which ade-
quate parallel corpora is available (Och, 2003). 
However, a major criticism of this approach is that it 
is void of any internal representation for syntax or 
semantics. 
In recent years, hybrid approaches, which aim at 
applying statistical learning to structured data, began 
to emerge. Syntax based statistical MT approaches 
began with (Wu 1997), who introduced a polyno-
mial-time solution for the alignment problem based 
on synchronous binary trees. (Alshawi et al, 2000) 
extended the tree-based approach by representing 
each production in parallel dependency trees as a 
finite-state transducer. (Yamada and Knight, 2001, 
2002) model translation as a sequence of operations 
transforming a syntactic tree in one language into 
the string of the second language. 
The syntax based statistical approaches have 
been faced with the major problem of pervasive 
structural divergence between languages, due to both 
systematic differences between languages (Dorr, 
1994) and the vagaries of loose translations in real 
corpora. While we would like to use syntactic in-
formation in both languages, the problem of non-
isomorphism grows when trees in both languages are 
required to match.  
To allow the syntax based machine translation 
approaches to work as a generative process, certain 
isomorphism assumptions have to be made. Hence a 
reasonable question to ask is: to what extent should 
the grammar formalism, which we choose to repre-
sent syntactic language transfer, assume isomor-
phism between the structures of the two languages? 
(Hajic et al, 2002) allows for limited non-
isomorphism in that n-to-m matching of nodes in the 
two trees is permitted.  However, even after extend-
ing this model by allowing cloning operations on 
subtrees, (Gildea, 2003) found that parallel trees 
over-constrained the alignment problem, and 
achieved better results with a tree-to-string model 
 using one input tree than with a tree-to-tree model 
using two. 
At the same time, grammar theoreticians have 
proposed various generative synchronous grammar 
formalisms for MT, such as Synchronous Context 
Free Grammars (S-CFG) (Wu, 1997) or Synchro-
nous Tree Adjoining Grammars (S-TAG) (Shieber 
and Schabes, 1990). Mathematically, generative 
synchronous grammars share many good properties 
similar to their monolingual counterparts such as 
CFG or TAG (Joshi and Schabes, 1992). If such a 
synchronous grammar could be learnt from parallel 
corpora, the MT task would become a mathemati-
cally clean generative process. 
However, the problem of inducing a synchronous 
grammar from empirical data was never solved. For 
example, Synchronous TAGs, proposed by (Shieber 
and Schabes, 1990), which were introduced primar-
ily for semantics but were later also proposed for 
translation.  From a formal perspective, Syn-TAGs 
characterize the correspondences between languages 
by a set of synchronous elementary tree pairs. While 
examples show that this formalism does capture cer-
tain cross language structural divergences, there is 
not, to our knowledge, any successful statistical 
learning method to learn such a grammar from em-
pirical data. We believe that this is due to the limited 
ability of Synchronous TAG to model structure di-
vergences. This observation will be discussed later 
in Section 5. 
We studied the problem of learning synchronous 
syntactic sub-structures (parallel dependency treelets) 
from unaligned parallel corpora in (Ding and Palmer, 
2004). At the same time, we would like to formalize 
a synchronous grammar for syntax based statistical 
MT. The necessity of a well-defined formalism and 
certain limitations of the current existing formalisms, 
motivate us to design a new synchronous grammar 
formalism which will have the following properties: 
1. Linguistically motivated: it should be able to 
capture most language phenomena, e.g. compli-
cated word orders such as ?wh? movement. 
2. Without the unrealistic word-to-word isomor-
phism assumption: it should be able to capture 
structural variations between the languages. 
3. Mathematically rigorous: it should have a well 
defined formalism and a proven generation ca-
pacity, preferably context free or mildly context 
sensitive. 
4. Generative: it should be ?generative? in a 
mathematical sense. This property is essential 
for the grammar to be used in statistical MT. 
Each production rule should have its own prob-
ability, which will allow us to decompose the 
overall translation probability. 
5. Simple: it should have a minimal number of 
different structures and operations so that it will 
be learnable from the empirical data. 
In the following sections of this paper, we intro-
duce a grammar formalism that satisfies the above 
properties: Synchronous Dependency Insertion 
Grammar (SDIG). Section 2 gives an informal look 
at the desired capabilities of a monolingual version 
Dependency Insertion Grammar (DIG) by address-
ing the problems with previous dependency gram-
mars. Section 3 gives the formal definition of the 
DIG and shows that it is weakly equivalent to Con-
text Free Grammar (CFG). Section 4 shows how 
DIG is linguistically motivated by making a com-
parison between DIG and Tree Adjoining Grammar 
(TAG). Section 5 specifies the Synchronous DIG 
and Section 6 gives the probabilistic extension of 
SDIG. 
2 Issues with Dependency Grammars 
2.1 Dependency Grammars and Statistical MT 
According to (Fox, 2002), dependency represen-
tations have the best phrasal cohesion properties 
across languages. The percentage of head crossings 
per chance is 12.62% and that of modifier crossings 
per chance is 9.22%. Observing this fact, it is rea-
sonable to propose a formalism that handles lan-
guage transfer based on dependency structures. 
What is more, if a formalism based on depend-
ency structures is made possible, it will have the 
nice property of being simple, as expressed in the 
following table: 
 CFG TAG DG 
Node# 2n 2n n 
Lexicalized? NO YES YES 
Node types 2 2 1* 
Operation types 1 2 1* 
(*: will be shown later in this paper) 
Figure 1. 
The simplicity of a grammar is very important for 
statistical modeling, i.e. when it is being learned 
from the corpora and when it is being used in ma-
chine translation decoding, we don?t need to condi-
tion the probabilities on two different node types or 
operations. 
At the same time, dependency grammars are in-
herently lexicalized in that each node is one word. 
Statistical parsers (Collins 1999) showed perform-
ance improvement by using bilexical probabilities, 
i.e. probabilities of word pair occurrences. This is 
what dependency grammars model explicitly. 
 2.2 A Generative Grammar? 
Why do we want the grammar for statistical MT 
to be generative? First of all, generative models have 
long been studied in the machine learning commu-
nity, which will provide us with mathematically rig-
orous algorithms for training and decoding. Second, 
CFG, the most popular formalism in describing 
natural language phenomena, is generative. Certain 
ideas and algorithms can be borrowed from CFG if 
we make the formalism generative. 
While there has been much previous work in 
formalizing dependency grammars and in its appli-
cation to the parsing task, until recently (Joshi and 
Rambow, 2003), little attention has been given to the 
issue of making the proposed dependency grammar 
generative. And in machine translation tasks, al-
though using dependency structures is an old idea, 
little effort has been made to propose a formal 
grammar which views the composition and decom-
position of dependency trees as a generative process 
from a formal perspective. 
There are two reasons for this fact: (1) The 
?pure? dependency trees do not have nonterminals. 
The standard solution to this problem was intro-
duced as early as (Gaifman 1965), where he pro-
posed adding syntactic categories to each node on 
the dependency tree. (2) However, there is a deeper 
problem with dependency grammar formalisms, as 
observed by (Rambow and Joshi 1997). In the de-
pendency representation, it is hard to handle com-
plex word order phenomena without resorting to 
global word order rules, which makes the grammar 
no longer generative. This will be explored in the 
next subsection (2.3).  
2.3 Non-projectivity 
Non-projectivity has long been a major obstacle 
for anyone who wants to formalize dependency 
grammar. When we draw projection lines from the 
nodes in the dependency trees to a linear representa-
tion of the sentence, if we cannot do so without hav-
ing one or more projection lines going across at least 
one of the arcs of the dependency tree, we say the 
dependency tree is non-projective. 
A typical example for non-projectivity is ?wh? 
movement, which is illustrated below.  
 
Figure 2. 
 
Our solution for this problem is given in section 
4 and in the next section we will first give the formal 
definition of the monolingual Dependency Insertion 
Grammar. 
3 The DIG Formalism 
3.1 Elementary Trees 
Formally, the Dependency Insertion Grammar is 
defined as a six tuple ),,,,,( RSBALC . C  is a set 
of syntactic categories and L  is a set of lexical 
items. A  is a set of Type-A trees and B  is a set of 
Type-B trees (defined later). S  is a set of the start-
ing categories of the sentences. R  is a set of word 
order rules local to each node of the trees. 
Each node in the DIG has three fields:  
A Node consists of: 
1. One lexical item 
2. One corresponding category 
3. One local word order rule.  
We define two types of elementary trees in DIG: 
Type-A trees and Type-B trees. Both types of trees 
have one or more nodes. One of the nodes in an 
elementary tree is designated as the head of the ele-
mentary tree. 
Type-A trees are also called ?root lexicalized 
trees?. They roughly correspond to the ?  trees in 
TAG. Type-A trees have the following properties: 
Properties of a Type-A elementary tree: 
1. The root is lexicalized. 
2. The root is designated as the head of the 
tree 
3. Any lexicalized node can take a set of 
unlexicalized nodes as its arguments. 
4. The local word order rule specifies the 
relative order between the current node 
and all its immediate children, including 
the unlexicalized arguments. 
Here is an example of a Type-A elementary tree 
for the verb ?like?. Note that the head node is 
marked with (@).  
Please note that the placement of the dependency 
arcs reflects the relative order between the parent 
and all its immediate children. 
Figure 3 
Type-B trees are also called ?root unlexicalized 
trees?. They roughly correspond to ?  trees in TAG 
and have the following properties: 
 Properties of a Type-B elementary tree: 
1. The root is the ONLY unlexicalized node 
2. One of the lexicalized nodes is desig-
nated as the head of the tree 
3. Similar to Type-A trees, each node also 
have a word order rule that specifies the 
relative order between the current node 
and all its immediate children. 
Here is and example of a Type-B elementary tree for 
the adverb ?really? 
 
Figure 4 
3.2 The Unification Operation 
We define only one type of operation: unification 
for any DIG derivation: 
Unification Operation: 
When an unlexicalized node and a head 
node have the same categories, they can 
be merged into one node. 
This specifies that an unlexicalized node cannot 
be unified with a non-head node, which guarantees 
limited complexity when a unification operation 
takes place.  
After unification,  
1. If the resulting tree is a Type-A tree, its root 
becomes the new root; 
2. If the resulting tree is a Type-B tree, the root 
node involved in the unification operation be-
comes the new root. 
Here is one example for the unification operation 
which adjoins the adverb ?really? to the verb ?like?: 
 
Figure 5 
Note that for the above unification operation the 
dependency tree on the right hand side is just one of 
the possible resultant dependency trees. The strings 
generated by the set of possible resultant depend-
ency trees should all be viewed as the language 
)(DIGL  generated by the DIG grammar. 
Also note that the definition of DIG is preserved 
through the unification operation, as we have: 
1. (Type-A) (unify) (Type A)  =  (Type-A) 
2. (Type-A) (unify) (Type B)  =  (Type-A) 
3. (Type-B) (unify) (Type B)  =  (Type-B) 
3.3 Comparison to Other Approaches 
There are two major differences between our de-
pendency grammar formalism and that of (Joshi and 
Rambow, 2003): 
1. We only define one unification operation, 
whereas (Joshi and Rambow, 2003) defined two 
operations: substitution and adjunction. 
2. We introduce the concept of ?heads? in the DIG 
so that the derivation complexity is significantly 
smaller. 
3.4 Proof of Weak Equivalence between DIG 
and CFG 
We prove the weak equivalence between DIG and 
CFG by first showing that the language that a DIG 
generates is a subset of one that a CFG generates, 
i.e. )()( CFGLDIGL ? . And then we show the 
opposite is also true: )()( DIGLCFGL ? . 
3.4.1 )()( CFGLDIGL ?  
The proof is given constructively. First, for each 
Type-A tree, we ?insert? a ?waiting for Type-B tree? 
argument at each possible slot underneath it with the 
category B. This process is shown below: 
 
Figure 6 
Then we ?flatten? the Type-A tree to its linear 
form according to the local word order rule, which 
decides the relative ordering between the parent and 
all its children at each of the nodes. And we get: 
}.{}{
}.{}{}.{}.{ 100
Hnji
HHH
CBNTwCNTw
CBNTwCNTwCBNTCANT
LL
L?  
y nww L0 is the strings of lexical items 
y }.{ HCANT  is the nonterminal created for  
this Type-A tree, and HC is the category of the 
head (root). 
y }{ jCNT  is the nonterminal for each category 
y }.{ HCBNT  is the nonterminal for each ?Type-
B site? 
Similarly, for each Type-B tree we can create 
?Type-B site? under its head node. So we have: 
nHiHR wCBNTwCBNTwCRBNT }.{}.{}.{ 0 LL?  
Then we create the production to take arguments: 
}.{}{ CANTCNT ?  
And the production rules to take Type-B trees: 
}.{}.{}.{ CBNTCRBNTCBNT ?  
}.{}.{}.{ CRBNTCBNTCBNT ?  
 Hence, a DIG can be converted to a CFG. 
3.4.2 )()( DIGLCFGL ?  
It is known that a context free grammar can be con-
verted to Greibach Normal Form, where each pro-
duction will have the form: 
*aVA ? , where V  is the set of nonterminals 
We simply construct a corresponding Type-A 
dependency tree as follows: 
 
Figure 7
 
4 Compare DIG to TAG 
A Tree Adjoining Grammars is defined as a five 
tuple ),,,,( SAINT? , where ?  is a set of terminals, 
NT  is a set of nonterminals, I  is a finite set of fi-
nite initial trees (?  trees), A  is a finite set of auxil-
iary trees ( ?  trees), and S  is a set of starting 
symbols. The TAG formalism defines two opera-
tions, substitution and adjunction. 
A TAG derives a phrase-structure tree, called the 
?derived tree? and at the same time, in each step of 
the derivation process, two elementary trees are 
connected through either the substitution or adjunc-
tion operation. Hence, we have a ?derivation tree? 
which represents the syntactic and/or logical relation 
between the elementary trees. Since each elementary 
tree of TAG has exactly one lexical node, we can 
view the derivation tree as a ?Deep Syntactic Repre-
sentation? (DSynR). This representation closely re-
sembles the dependency structure of the sentence. 
Here we show how DIG models different opera-
tions of TAG and hence handles word order phe-
nomena gracefully.  
We categorize the TAG operations into three dif-
ferent types: substitution, non-predicative adjunction 
and predicative adjunction. 
z Substitution 
We model the TAG substitution operation by 
having the embedded tree replaces the non-terminal 
that is in accordance with its root. An example for 
this type is the substitution of NP. 
 
Figure 8a Substitution in TAG 
 
Figure 8b Substitution through DIG unification 
z Non-predicative Adjunction 
In TAG, this type of operation includes all ad-
junctions when the embedded tree does not contain a 
predicate, i.e. the root of the embedded tree is not an 
S. For example, the trees for adverbs are with root 
VP and are adjoined to non-terminal VPs in the ma-
trix tree. 
 
Figure 9a Non-predicative Adjunction in TAG 
Like[V]@
[N]John[N]really[adv]@
[V] Like[V]@
[N]John[N] really[adv]  
Figure 9b Non-predicative Adjunction through DIG 
unification 
z Predicative Adjunction 
This type of operation adjoins an embedded tree 
which contains a predicate, i.e. with a root S, to the 
matrix tree. A typical example is the sentence: Who 
does John think Mary likes?  
This example is non-projective and has ?wh? 
movement. In the TAG sense, the tree for ?does 
John think? is adjoined to the matrix tree for ?Who 
Mary likes?. This category of operation has some 
interesting properties. The dependency relation of 
the embedded tree and the matrix tree is inverted. 
This means that if tree T1 is adjoined to T2, in non-
predicative adjunction, T1 depends on T2, but in 
predicative adjunction, T2 depends on T1. In the 
above example, the tree with ?like? depends on the 
tree with ?think?. 
 
Figure 10a ?Wh? movement through TAG  
(predicative) adjunction operation 
 Our solution is quite simple: when we are con-
structing the grammar, we invert the arc that points 
to a predicative clause. Despite the fact that the re-
sulting dependency trees have certain arcs inverted, 
we will still be able to use localized word order rules 
and derive the desired sentence with the simple uni-
fication operation. As shown below: 
 
Figure 10b ?Wh? movement through unification 
Since TAG is mildly context sensitive, and we 
have shown in Section 3 that DIG is context free, we 
are not claiming the two grammars are weakly or 
strongly equivalent. Also, please note DIG does not 
handle all the non-projectivity issues due to its CFG 
equivalent generation capacity. 
5 Synchronous DIG 
5.1 Definition 
(Wu, 1997) introduced synchronous binary trees 
and (Shieber, 1990) introduced synchronous tree 
adjoining grammars, both of which view the transla-
tion process as a synchronous derivation process of 
parallel trees. Similarly, with our DIG formalism, 
we can construct a Synchronous DIG by synchroniz-
ing both structures and operations in both languages 
and ensuring synchronous derivations. 
Properties of SDIG: 
1. The roots of both trees of the source and 
target languages are aligned, and have the 
same category 
2. All the unlexicalized nodes of both trees 
are aligned and have the same category. 
3. The two heads of both trees are aligned 
and have the same category. 
Synchronous Unification Operation: 
By the above properties of SDIG, we can 
show that unification operations are synchro-
nized in both languages. Hence we can have 
synchronous unification operations. 
5.2 Isomorphism Assumption 
So how is SDIG different from other synchro-
nous grammar formalisms?  
As we know, a synchronous grammar derives 
both source and target languages through a series of 
synchronous derivation steps. For any tree-based 
synchronous grammar, the synchronous derivation 
would create two derivation trees for both languages 
which have isomorphic structure. Thus a synchro-
nous grammar assumes certain isomorphism be-
tween the two languages which we refer to as the 
?isomorphism assumption?. 
Now we examine the isomorphism assumptions 
in S-CFG and S-TAG: 
y For S-CFG, the substitutions for all the non-
terminals need to be synchronous. Hence the 
isomorphism assumption for S-CFG is isomor-
phic phrasal structure. 
y For S-TAG, all the substitution and adjunction 
operations need to be synchronous, and the 
derivation trees of both languages are isomor-
phic. The derivation tree for TAG is roughly 
equivalent to a dependency tree. Hence the 
isomorphism assumption for S-TAG is an iso-
morphic dependency structure. 
As shown by real translation tasks, both of those 
assumptions would fail due to structural divergences 
between languages. 
On the other hand SDIG does NOT assume word 
level isomorphism or isomorphic dependency trees. 
Since in the SDIG sense, the parallel dependency 
trees are in fact the ?derived? form rather than the 
?derivation? form. In other words, SDIG assumes 
the isomorphism lies deeper than the dependency 
structure. It is ?the derivation tree of DIG? that is 
isomorphic. 
The following ?pseudo-translation? example il-
lustrates how SDIG captures structural divergence 
between the languages. Suppose we want to translate: 
y [Source] The girl kissed her kitty cat. 
y [Target] The girl gave a kiss to her cat. 
 
 
Figure 11 
Note that both S-CFG and S-TAG won?t be able 
to handle such structural divergence. However, 
when we view each of the two sentences as derived 
from three elementary trees in DIG, we can have a 
synchronous derivation, as shown below: 
 6 The Probabilistic Extension to SDIG and 
Statistical MT 
The major reason to construct an SDIG is to have 
a generative model for syntax based statistical MT. 
By relying on the assumption that the derivation tree 
of DIG represents the probability dependency graph, 
we can build a graphical model which captures the 
following two statistical dependencies: 
1. Probabilities of Elementary Tree unification (in 
the target language) 
2. Probabilities of Elementary Tree transfer (be-
tween languages), i.e. the probability of two 
elementary trees being paired 
ET-f3
ET-f1
ET-f2
ET-f4
ET-e3
ET-e1
ET-e2
ET-e4  
Figure 12 
The above graph shows two isomorphic deriva-
tion trees for two languages. ET stands for elemen-
tary trees and dotted arcs denote the conditional 
dependence assumptions). Under the above model, 
the best translation is: )()|(maxarg* ePefPe
e
= ; 
And ?=
i
ii eETfETPefP ))(|)(()|( ; also we 
have ( )?=
i
ii eETParenteETPeP ))((|)()( . 
Hence, we can have PSDIG (probabilistic syn-
chronous Dependency Insertion Grammar). Given 
the dynamic programming property of the above 
graphical model, an efficient polynomial time 
Viterbi decoding algorithm can be constructed. 
7 Current Implementation 
To test our idea, we implemented the above syn-
chronous grammar formalism in a Chinese-English 
machine translation system. The actual implementa-
tion of the synchronous grammar used in the system 
is a scaled-down version of the SDIG introduced 
above, where all the word categories are treated as 
one. The reason for this simplification is that word 
category mappings across languages are not straight-
forward. Defining the word categories so that they 
can be consistent between the languages is a major 
goal for our future research. 
The uni-category version of the SDIG is induced 
using the algorithm in (Ding and Palmer, 2004), 
which is a statistical approach to extracting parallel 
dependency structures from large scale parallel cor-
pora. An example is given in Figure 12. We can 
construct the parallel dependency trees as shown in 
Figure 13a. The expected output of the above ap-
proach is shown in Figure 13b. (e) stands for an 
empty node trace. 
y [English]  I have been here since 1947. 
y [Chinese] Wo 1947  nian   yilai   yizhi     zhu   zai  zheli. 
              I             year   since  always  live   in     here 
 
Figure 
13a.  
Input 
 
Figure 13b. Output 
(5 parallel elementary tree pairs) 
We build a decoder for the model in Section 6 for 
our machine translation system. The decoder is 
based on a polynomial time decoding algorithm for 
fast non-isomorphic tree-to-tree transduction (Un-
published by the time of this paper). 
We use an automatic syntactic parser (Collins, 
1999; Bikel, 2002) to produce the parallel unaligned 
syntactic structures. The parser was trained using the 
Penn English/Chinese Treebanks. We then used the 
algorithm in (Xia 2001) to convert the phrasal struc-
ture trees into dependency trees. 
The following table shows the statistics of the 
datasets we used. (Genre, number of sentence pairs, 
number of Chinese/English words, type and usage). 
Dataset Xinhua FBIS NIST 
Genre News News News 
Sent# 56263 21003 206 
Chn W# 1456495 522953 26.3 average 
Eng W# 1490498 658478 32.5 average 
Type unaligned unaligned multi-reference
Usage training training testing 
Figure 14 
The training set consists of Xinhua newswire 
data from LDC and the FBIS data. We filtered both 
datasets to ensure parallel sentence pair quality. We 
used the development test data from the 2001 NIST 
MT evaluation workshop as our test data for the MT 
system performance. In the testing data, each input 
Chinese sentence has 4 English translations as refer-
ences, so that the result of the MT system can be 
evaluated using Bleu and NIST machine translation 
evaluation software. 
  1-gram 2-gram 3-gram 4-gram
NIST: 4.3753 4.9773 5.0579 5.0791
BLEU: 0.5926 0.3417 0.2060 0.1353
Figure 15 
The above table shows the cumulative Bleu and 
NIST n-gram scores for our current implementation; 
with the final Bleu score 0.1353 with average input 
sentence length of 26.3 words.  
In comparison, in (Yamada and Knight, 2002), 
which was a phrasal structure based statistical MT 
system for Chinese to English translation, the Bleu 
score reported for short sentences (less than 14 
words) is 0.099 to 0.102.  
Please note that the Bleu/NIST scorers, while 
based on n-gram matching, do not model syntax dur-
ing evaluation, which means a direct comparison 
between a syntax based MT system and a string 
based statistical MT system using the above scorer 
would favor the string based systems. 
We believe that our results can be improved us-
ing a more sophisticated machine translation pipe-
line which has separate components that handle 
specific language phenomena such as named entities. 
Larger training corpora can also be helpful. 
8 Conclusion 
Finally, let us review whether the proposed SDIG 
formalism has achieved the goals we setup in Sec-
tion 1 of this paper for a grammar formalism for Sta-
tistical MT applications: 
1. Linguistically motivated: DIG captures word-
order phenomena within the CFG domain. 
2. SDIG dropped the unrealistic word-to-word 
isomorphism assumption and is able to capture 
structural divergences. 
3. DIG is weakly equivalent to CFG. 
4. DIG and SDIG are generative grammars. 
5. They have both simple formalisms, only one 
type of node, and one type of operation. 
9 Future Work 
We observe from our testing results that the cur-
rent simplified uni-category version of SDIG suffers 
from various grammatical errors, both in grammar 
induction and decoding, therefore our future work 
should focus on word category consistency between 
the languages so that a full-fledged version of SDIG 
can be used.  
10 Acknowledgements 
Our thanks go Aravind Joshi, Owen Rambow, 
Dekai Wu and all the anonymous reviewers of the 
previous versions of the paper, who gave us invalu-
able advices, suggestions and feedbacks.  
 
References 
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computational 
Linguistics, 26(1): 45-60. 
Daniel M. Bikel. 2002. Design of a multi-lingual, paral-
lel-processing statistical parsing engine. In Proceedings 
of HLT 2002. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della 
Pietra, and Robert L. Mercer. 1993. The mathematics of 
statistical machine translation: parameter estimation. 
Computational Linguistics, 19(2): 263-311. 
Michael John Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia. 
Yuan Ding and Martha Palmer. 2004. Automatic Learn-
ing of Parallel Dependency Treelet Pairs, in Proceed-
ings of The First International Joint Conference on 
Natural Language Processing (IJCNLP-04). 
Bonnie J. Dorr. 1994. Machine translation divergences: A 
formal description and proposed solution. Computa-
tional Linguistics, 20(4): 597-633. 
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP-02, pages 
304-311 
Daniel Gildea. 2003. Loosely tree based alignment for 
machine translation. In Proceedings of ACL-03 
Jan Hajic, et al 2002. Natural language generation in the 
context of machine translation. Summer workshop final 
report, Center for Language and Speech Processing, 
Johns Hopkins University, Baltimore.  
Aravind Joshi and Owen Rambow. 2003. A formalism of 
dependency grammar based on Tree Adjoining Gram-
mar. In Proceedings of the first international confer-
ence on meaning text theory (MTT 2003), June 2003. 
Aravind K. Joshi and Yves Schabes. Tree-adjoining 
grammars and lexicalized grammars. In Maurice Nivat 
and Andreas Podelski, editors, Tree Automata and Lan-
guages. Elsevier Science, 1992. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
ACL-03), pages 160-167. 
Owen Rambow and Aravind Joshi. 1997. A formal look 
at dependency grammars and phrase structures. In Leo 
Wanner, editor, Recent Trends in Meaning-Text Theory, 
pages 167-190.  
S. M. Shieber and Y. Schabes. 1990. Synchronous Tree-
Adjoining Grammars, Proceedings of the 13th COLING, 
pp. 253-258, August 1990. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):3-403. 
Fei Xia. 2001. Automatic grammar generation from two 
different perspectives. Ph.D. thesis, University of Penn-
sylvania, Philadelphia. 
Kenji Yamada and Kevin Knight. 2001. A syntax based 
statistical translation model. In Proceedings of ACL-01 
Kenji Yamada and Kevin Knight. 2002. A decoder for 
syntax-based statistical MT. In Proceedings of ACL-02 
Putting Meaning into Your Trees  
 
 
Martha Palmer 
University of Pennsylvania 
mpalmer@cis.upenn.edu 
The meaning of a sentence is an essential aspect of 
natural language understanding, yet an elusive one, 
since there is no accepted methodology for determin-
ing it. There is not even a consensus on criteria for 
distinguishing word senses. Clearly a more robust 
technology is needed that uses data-driven techniques.  
These techniques typically rely on supervised machine 
learning, so a critical goal is the definition of a level of 
semantic representation (sense tags and semantic role 
labels) that could be consistently annotated on a large 
scale.  We have been training automatic WSD systems 
on the English sense-tagged training data based on 
WordNet that we supplied to SENSEVAL2 (Dang & 
Palmer, 2002).  A pervasive problem with sense tag-
ging is finding a sense inventory with clear criteria for 
sense distinctions.  WordNet is often criticized for its 
subtle and fine-grained sense distinctions.  Perhaps 
more consistent and coarse-grained sense distinctions 
would be more suitable for natural language process-
ing applications.  Grouping the highly polysemous 
verb senses in WordNet (on average reducing the >16 
senses per verb to 8) provides an important first step a 
more flexible granularity for WordNet senses that im-
proves both inter-annotator agreement (71% to 82%) 
and system performance (60.2% to 69%) (Dang & 
Palmer, 2002).  The Frameset sense tags associated 
with the PropBank, as discussed below, provide an 
even more coarse-grained and easily replicable level 
of sense distinctions. 
 
Based on a consensus of colleagues participating in the 
ACE (Automatic Content Extraction) program, we 
have developed a Proposition Bank, or PropBank, 
which provides semantic role labels for verbs and par-
ticipial modifiers for the 1M word Penn Treebank II 
corpus (Marcus, 1994).  VerbNet classes have proved 
invaluable for defining the appropriate semantic roles 
in this endeavor (Dang, et. al., 1998).  For example, 
John is the Agent or Arg0 of John broke the window, 
IBM is the Theme or Arg1 of IBM rose 1.2 points. In 
addition, for just over 700 of the most polysemous 
verbs in the Penn TreeBank, we have defined two or 
more Framesets ? major sense distinctions based on 
differing sets of semantic roles (Palmer, et al submit-
ted).  These Framesets overlap closely (95%) with our 
manual groupings of the SENSEVAL2 verb senses, 
and thus they can be combined to provide an hierarchi-
cal set of sense distinctions.  The PropBank is complete 
and a beta-release version was made publicly available 
through LDC in February for use in the CoNLL-04 
shared task.  There is a complementary lexicography 
project at Berkeley, Chuck Fillmore?s FRAMENET, 
which provides representative annotated samples rather 
than broad-coverage annotation, and there are current 
plans to combine these resources and train automatic 
labelers for English and Chinese. The automatic se-
mantic role labelers we are building use features that 
are very similar to our WSD system features, and we 
find that semantic role label features improve WSD 
while sense tag features improve semantic role labeling 
(Gildea & Palmer, 2002). 
 
References 
 
Dang, H.T., Kipper, K.,  Palmer, M., Rosenzweig, J. 
(1998) Investigating regular sense extensions 
based on intersective Levin classes. Coling/ACL-
98, pp 293-300, Montreal CA, August 11-17, 
1998. 
 
Dang, H. T. and Palmer, M., (2002). Combining Con-
textual Features for Word Sense Disambiguation. 
In Proceedings of the Workshop on Word Sense 
Disambiguation: Recent Successes and Future 
Directions, Philadelphia, Pa. 
 
Gildea, D and Palmer, M., (2002) The Necessity of 
Parsing for Predicate-Argument Recognition, 
ACL-02, Philadelphia, PA, July 7-12. 
 
Marcus, M, (1994), The Penn TreeBank: A revised 
corpus design for extracting predicate argument 
structure, In Proceedings of the ARPA Human 
Language Technology Workshop, Princeton, NJ.   
 
Palmer, M., Gildea, D., Kingsbury, P. (submitted) The 
Proposition Bank: An Annotated Corpus of Se-
mantic Roles, Computational Linguistics. 
Using prepositions to extend a verb lexicon
Karin Kipper, Benjamin Snyder, Martha Palmer
University of Pennsylvania
200 South 33rd Street
Philadelphia, PA 19104 USA
fkipper,bsnyder3,mpalmerg@linc.cis.upenn.edu
Abstract
This paper presents a detailed account of
prepositional mismatch between our hand-
crafted verb lexicon and a semantically an-
notated corpus. The analysis of these mis-
matches allows us to rene the lexicon and
to create a more robust resource capable
of better semantic predictions based on the
verb-preposition relations.
1 Introduction
There is currently much interest in training super-
vised systems to perform shallow semantic annota-
tion tasks such as word sense tagging and semantic
role labeling. These systems are typically trained on
annotated corpora such as the Penn Treebank [Mar-
cus1994], and perform best when they are tested on
data from the same genre. A more long-term goal
is to develop systems that will perform equally well
on diverse genres, and that will also be able to per-
form additional, more complex, semantic annotation
tasks. With this end in mind, we have been man-
ually developing a large-scale, general purpose hi-
erarchical verb lexicon that, in addition to links to
WordNet senses [Miller1985, Fellbaum1998], has ex-
plicit and detailed syntactic and semantic informa-
tion associated with each entry. Much of the syn-
tactic information is derived from the Levin verb
classes, although the classication has been extended
and modied. Sets of syntactic frames are associated
with each verb class, and specic prepositions are
often listed as well. We are interested in evaluating
how well our lexicon predicts syntactic frames in nat-
urally occurring data. This will give us an estimate
of its likely usefulness in extending the coverage of
systems trained on one genre to other genres.
This paper presents a comparison between our hi-
erarchical verb lexicon, VerbNet [Kipper et al2000,
Dang et al2000], and a corpus annotated seman-
tically with predicate-argument structure, Prop-
Bank [Kingsbury and Palmer2002]. We briey de-
scribe an experiment which established a baseline for
the syntactic coverage of the verb lexicon and more
extensively we compare and discuss the preposition
mismatches found while doing this evaluation. We
used this experiment, which used almost 50,000 verb
instances, to measure how well the linguistic intu-
itions motivating our verb lexicon are attested to in
the actual data. It allowed us to determine which
of the expected syntactic frames and specic prepo-
sitions occur and which do not, and also look for
unexpected occurrences. Although prepositions are
generally described as restrictions on syntax, their
signicance goes far beyond that of a syntactic re-
striction. Verb-preposition relations can also allow
us to make predictions about the semantic contents
of a verb-frame.
The mapping between the two resources was done
by assigning verb classes to the dierent senses in
PropBank and by assigning the thematic roles used
to describe VerbNet classes to argument roles of
PropBank. The criteria used for matches includes
both a notion of exact frame match where the en-
countered preposition was explicitly listed in the
frame, as well as a more relaxed notion of frame
match that allows alternative prepositions. We found
that under the former, our lexicon correctly predicts
over 78% of all the syntactic frames found in Prop-
Bank, while under the latter criterion, the results go
up to 81%. This dierential hints at the di?culty of
accounting for semantically signicant prepositions
in sentences. We believe that it is precisely because
the preposition-semantics relationship is so complex
that properly accounting for it will lead to a more
robust natural language resource.
The remainder of this paper is organized as follows.
Sections 2 and 3 present the lexical resources used for
the experiment. Section 4 discusses the evaluation
of VerbNet against PropBank and Section 5 shows
examples of preposition mismatches between the two
resources.
2 VerbNet's components
VerbNet is an on-line broad-coverage domain-
independent lexical resource with syntactic descrip-
tions for over 4,100 verbs organized into classes ac-
cording to the Levin classication [Levin1993]. It is
a general purpose verb lexicon created initially with
the task of instructing a virtual character in a simu-
lated environment in mind [Badler et al1999,Bindi-
ganavale et al2000].
VerbNet extends Levin's classication by pro-
viding explicit syntactic and semantic information
about the verbs it describes. In addition, the lex-
icon is organized hierarchically so that all verbs in
a class (or subclass) share these syntactic descrip-
tions and have common semantics. Each verb class is
completely described by the set of its members (each
verb has links to the appropriate senses in WordNet,
thematic roles for the predicate-argument structure
of the members, selectional restrictions on these ar-
guments to express preferred argument types, and
frames. Each frame consists of a brief description,
an example, a syntactic description corresponding to
one of Levin's alternations, and a set of semantic
predicates. In addition, each predicate has a time
function to show at what stage of the event the pred-
icate holds true, in a manner similar to the event
decomposition of Moens and Steedman (1988) . In
order for the members of each class to be coherent
with respect to the thematic roles, selectional restric-
tions, syntactic frames, and semantics they allow, we
rened the original Levin classes and added 74 new
subclasses.
VerbNet's broad-coverage, with explicit syntax
and semantics, attempts to address several gaps
present in other resources. WordNet was designed
mainly as a semantic network, and contains little
syntactic information. VerbNet, in contrast, includes
explicit predicate argument structures for verbs in
their classes, as well as a way to systematically ex-
tend those senses based on the semantics of each
class. FrameNet [Baker et al1998] and VerbNet both
contain the notion of verb groupings. The group-
ings in FrameNet however are based solely on the
semantic roles shared by the members of a class.
These members do not need to have the same set of
syntactic frames, and lack explicit semantics other
than what is provided by the semantic labels. Un-
like VerbNet, which uses a small set of thematic roles
for all classes, FrameNet uses frame elements which
are particular to a lexical item or to small groups of
frames. Besides, one of the benets of constructing
a general lexicon like VerbNet is that it allows one
to extend the coverage of resources tied to specic
corpora.
The syntactic frames in VerbNet describe the sur-
face realization for constructions such as transitive,
intransitive, prepositional phrases, resultatives, and
a large set of Levin's alternations. A syntactic frame
consists of the thematic roles, the verb, and other
lexical items which may be required for a particu-
lar construction or alternation. Additional restric-
tions may be further imposed on the thematic roles
(quotation, plural, innitival, etc.). Illustrations of
syntactic frames are shown in examples 1, 2, and 3.
(1) Agent V Patient
(John hit the ball)
(2) Agent V at Patient
(John hit at the window)
(3) Agent V Patient[+plural] together
(John hit the sticks together)
VerbNet alo includes a hierarchy of prepositions,
with 57 entries, derived from an extended version
of work described in Sparck-Jones and Boguraev
(1987). This restriction is necessary in order to spec-
ify which prepositions are possible in a particular
frame since many of Levin's alternations require spe-
cic prepositions such as `as' or `with/against'. A
partial and somewhat simplied hierarchy is shown
in Figure 1. This gure shows the spatial preposi-
tions hierarchy divided into path and locative prepo-
sitions. Path prepositions are further subdivided into
source, direction, and destination prepositions. A
syntactic frame with Prep[+src] as a constraint will
allow only those specic prepositions (from, out, out
of, etc) that are part of the spatial, path, source hi-
erarchy.
The semantic information for the verbs in Verb-
Net is expressed as a conjunction of semantic pred-
icates, any of which may be negated. These seman-
tic predicates fall into four categories: general pred-
icates such as motion and cause which are widely
used across classes; variable predicates whose mean-
ing is assumed to be in a one-to-one relation with a
set of words in the language; predicates that are spe-
cic to certain classes; and predicates for multiple
events which are used to express relations between
events. The semantic predicates can take arguments
over the verb complements, as well as over implicit
existentially quantied event variables.
Relations between verbs (or between verb classes)
such as antonymy and entailment present in Word-
spatial
path
src
from
out
out of
. . .
dir
across
along
around
down
. . .
dest
dest-conf
into
onto
dest-dir
for
at
to
towards
loc
about
above
against
. . .
Figure 1: Partial hierarchy of prepositions of the verb lexicon
Net can be predicted upon verication of the pred-
icates used. Relations between verbs (and verb
classes) such as the ones predicted in FrameNet, can
also be veried by the semantic predicates, for in-
stance all of the Communication classes have the
same predicates of cause and transfer info. Aspect in
VerbNet is captured by the time function argument
present in the predicates.
3 PropBank
The PropBank project [Kingsbury and Palmer2002]
is annotating the Penn Treebank with predicate-
argument structures. Semantic roles are dened for
each verb in PropBank. These roles are meant to be
theory neutral and are simply numbered. Verb senses
are distinguished by dierent Framesets, with a sep-
arate set of numbered roles, called a roleset, dened
for each Frameset. An example of the Framesets for
the verb leave can be seen in Figure 2. Arg0 is usually
associated with Agent and Arg1 is usually similar to
Theme or Patient. However, argument labels are not
necessarily signicant across dierent verb meanings
or across dierent verbs.
Roleset leave.01 \move away from":
Arg0: entity leaving
Arg1: place left
Arg3: attribute
Ex: [
ARG0
The move] [
rel
left] [
ARG1
the
companies] [
ARG3 as
as outside bidders.]
Roleset leave.02 \give":
Arg0: giver
Arg1: thing given
Arg2: beneciary
Ex: [
ARG0
John] [
rel
left] [
ARG1
cookies]
[
ARG2 for
for Mary]
Figure 2: Framesets for the verb leave in PropBank
4 Matching syntactic coverage
between the two resources
In order to test the syntactic coverage of VerbNet, we
performed an experiment to identify which syntactic
frames found in the PropBank corpus are represented
in our verb lexicon. As expected, we uncovered syn-
tactic frames and prepositions not initially predicted
in our resource which may now be added.
For this evaluation 49,073 PropBank annotated in-
stances were used, which translated into 1,678 verb
entries in VerbNet. Since the notion of a Prop-
Bank Frameset and a VerbNet class are not perfectly
equivalent, an individual Frameset may be mapped
to multiple classes. In order to put the two re-
sources in correspondence we created mappings be-
tween the Framesets and our verb classes, as well as
mappings between the argument labels in the roleset
of a Frameset to the thematic roles in our classes.
The process of assigning a verb class to a Frameset
was performed manually during the creation of new
PropBank frames. The thematic role assignment, on
the other hand, is a semi-automatic process which
nds the best match for the argument labels, based
on their descriptors, to the set of thematic roles of
VerbNet.
To verify whether a particular syntactic frame
found in PropBank was present in our lexicon, we
translated the PropBank annotated sentence into
VerbNet-style frames. An example of this transla-
tion for the verb leave is given below. Example sen-
tence (4) is taken from the corpus, its PropBank an-
notation can be seen in (5), and the VerbNet-style
frame is shown in (6). In this example, the verb
leave is mapped to two VerbNet classes 51.2 (Leave
class), and 13.3 (Future-having class), with dierent
roles mapped to the argument labels in each of these
classes.
(4) wsj/05/wsj 0568.mrg 12 4:
The tax payments will leave Unisys with $ 225
million *U* in loss carry-forwards that *T*-1 will
cut tax payments in future quarters .
(5) [
ARG0
The tax payments] [
rel
leave] [
ARG2
Unisys]
[
ARG1 with
with $ 225 million]
(6) (a) leave-51.2: Theme V NP Prep(with) Source
(b) future having-13.3: Agent V Recipient
Prep(with) Theme
In this instance, only the latter of the two con-
structed frames matches a frame in VerbNet. In ef-
fect, this serves as a sort of sense disambiguation, as
the leave entry in class 51.2 has the sense \to exit,"
while the entry in class 13.3 has a sense similar to
the verb \to give." In fact the sense of \leave" in the
sentence is the latter, and the single matched frame
conrms this.
In general, we used several criteria when attempt-
ing to match a constructed frame to a frame in Verb-
Net. Two of these criteria are of primary interest for
this paper:
1. the exact frame description was present in Verb-
Net (henceforth called \exact match", or a
match under the strict criterion);
2. the frame description is present in VerbNet but
there is a preposition mismatch (henceforth re-
ferred as a \relaxed match").
For instance, if the translated corpus sentence
is Agent V Prep(as) Theme, but VerbNet predicts
Agent V Prep(for) Theme for verbs in the class, this
annotation would be considered a relaxed match,
but not an exact match. VerbNet predicts 78% of
frames found in PropBank under the strict criterion
and 81% of those frames under the relaxed criterion.
More details of this experiment are described in Kip-
per et al (2004) .
5 Using prepositions from the
corpus to rene verb classes
By comparing our theoretically motivated sets of
syntactic frames for an individual verb with the ac-
tual data, we can evaluate both the coverage of our
lexicon and its theoretical underpinnings. There are
many questions to be addressed with respect to cov-
erage: Do the predicted syntactic frames occur? Do
the predicted prepositions occur? Do other, unpre-
dicted prepositions occur as well? Depending on the
answers to these questions, prepositions (or syntactic
frames) may be inserted into or deleted from specic
classes and entire classes may be restructured.
Our verb lexicon matches over 78% of all the syn-
tactic frames found in PropBank. However, when
restricting the frames found in PropBank to those
without prepositions, the resulting match rate is al-
most 81%. This dierence hints at the di?culty
of accounting for semantically signicant preposi-
tions in sentences, and a proper account of this
preposition-semantic relationship seems essential to
us in order to build a more robust lexical resource.
5.1 Prepositions in the Corpus
Verb occurrences are partitioned according to
whether a preposition occurs or not in the instance
frame, and according to how well the constructed
frame matches a VerbNet frame. Almost 4/5 of the
verb instances studied do not contain a signicant
preposition in their PropBank annotation (and con-
sequently their constructed frames do not include
any prepositions).
1
On these instances, we obtained
a 81% match rate under the strict criterion.
1
We consider a preposition \signicant" if the prepo-
sition object is a PropBank argument with a mapping to
a thematic role, excluding preposition \by".
Of the 49,073 verb instances we are looking at,
9,304 instances had a signicant preposition, with
constructed frames including one or more preposi-
tional items. For those we obtain match rates of
65% and 76% (depending on whether preposition
mismatches were allowed or not).
The dierence between the 81% match rate of the
frames without prepositions and the 65%-76% match
rate in the frames with prepositions is substantial
enough to lead us to believe that a close examina-
tion of the sentences containing a preposition and
their comparison to VerbNet frames would allow us
to improve the coherence of our verb classes.
5.2 Prepositional Mismatch
For the instances with signicant prepositional
items, 65% (6,033 instances) have constructed frames
with an exact match to VerbNet. Of the remaining
3,271 instances, 1,015 are relaxed matches, and 2,256
do not bear any matches to VerbNet frames.
We focused on those verb instances which would
have matched a VerbNet frame if only a dierent
preposition had been used in the sentence or if the
VerbNet frame had included a wider range of prepo-
sitions. In addition to the 1,015 instances, we looked
at 652 verb instances, all of which share the follow-
ing two properties: (i) that the verb in question is
contained in multiple VerbNet classes, and (ii) that
although the constructed frame matches one of those
VerbNet classes exactly, there is at least one other
class where it matches only under the relaxed crite-
rion (when the value of the preposition is ignored).
These instances are important because the value of
the preposition in these cases can help decide which
is the most appropriate VerbNet class for that in-
stance. This information could then be used for
coarse-grained automatic sense tagging { either to
establish a PropBank Frameset or a set of WordNet
senses for those instances, since verbs instances in
our verb lexicon are mapped to that resource.
These 1,667 verb instances (1,015 preposition mis-
matches + 652 exact matches) comprise 285 unique
verbs and are mapped to a total of 97 verb classes.
5.3 Explanation of Mismatch
After a close examination of these 1,667 instances,
we veried that the mismatches can be explained and
divided into the following cases:
1. cases where a preposition should be added to a
VerbNet class (in some of these cases, a rene-
ment of the class into more specic subclasses is
needed, since not all members take the included
preposition);
2. cases where the particular usage of the verb is
not captured by any VerbNet entry (this is the
case with metaphorical uses of certain verbs);
3. incorrect mappings between PropBank and
VerbNet;
2
4. cases where the PropBank annotation is incon-
sistent;
5. cases where the particular instance belongs to
another VerbNet class (which are expected since
the PropBank data used does not yet provide
sense tags).
As an example, in the PropBank annotated corpus
we nd the sentence:
\Lotus Development Corp. feeds its evaluations
into a computer...",
The verb to feed is present in four VerbNet classes.
The frame resulting from translating the PropBank
annotation to a VerbNet-style frame Agent V Theme
Prep(into) Recipient bears a resemblance to a frame
present in one of the classes (Give-13.1, syntactic
frame Agent V Theme Prep(to) Recipient). This is
a case where a VerbNet class requires renements
(with addition of new subclasses) to account for
prepositions unique to a subset of the verbs in the
class. It is an open question whether such rene-
ments, taken to completion, would result in sub-
classes that are so ne-grained they have a mem-
bership of one. If so, it may be more appropriate to
add verb-specic preposition preferences to existing
classes.
Another example is the following use of \build" in
the PropBank corpus:
\...to build their resumes through good grades and
leadership roles ..."
This sentence yields the frame Agent V Product
Prep(through) Material after translating the Prop-
Bank annotation to a VerbNet-style frame. This
frame bears a relaxed match to the Agent V Product
Prep(from, out of) Material syntactic frame found
in the Build-26.1 class. In VerbNet, the phrase
\..through good grades ..." is considered an adjunct
and therefore not relevant for the syntactic frame.
In PropBank, however, this phrase is annotated as
an argument (Arg2), which maps to the \Material"
thematic role in VerbNet. This example shows, as ex-
pected, mismatches between argument and adjuncts
in the two resources.
As a nal example, consider the following use of
the verb lease:
2
We asserted an error of 6.7% for the automatic map-
pings in a random sample of the data.
\The company said it was leasing the site of the
renery from Aruba."
Two frames are constructed for this verb instance,
one for each of the VerbNet classes to which the
PropBank lease Frameset is mapped. Its member-
ship in class Get-13.5.1, and class Give-13.1 respec-
tively yield the following two VerbNet-style frames:
(a) 13.1: Agent V Theme Prep(from) Recipient
(b) 13.5.1: Agent V Theme Prep(from) Source.
The rst frame bears a relaxed match to a frame
in its class (Agent V Theme Prep(to) Recipient)
whereas the second is an exact match to a frame
in the second class. In this instance, the preposition
`selects' the appropriate VerbNet class.
3
In fact, we
expect this to happen in all the 652 instances with ex-
act matches, since in those instances, the constructed
frame bears an exact match to one VerbNet class, but
a relaxed match to another. The dierent Framesets
of a verb are typically mapped to distinct sets of
VerbNet classes. If the preposition present in the
sentence matches frames in only a subset of those
VerbNet classes, then we are able to rule out cer-
tain Framesets as putative senses of the instance in
a sense tagging task.
6 Conclusion
We presented a detailed account of how prepositions
taken from a semantically annotated corpus can be
used to extend and rene a hand-crafted resource
with syntactic and semantic information for English
verbs. That the role of prepositions should not be
neglected can be clearly seen from the dierential in
match rates between those sentences with preposi-
tions and those without. The signicance of prepo-
sitions and their relation with verbs is of the utmost
importance for a robust verb lexicon, not only as
a syntactic restrictor, but also as a predictor of se-
mantic content. On the basis of these experiments
we are adding 132 new subclasses to VerbNet's ini-
tial 191 classes and 74 subclasses, going far beyond
basic Levin Classes.
One of the payos of constructing a general lexicon
like VerbNet is that it allows one to extend the cov-
erage of resources tied to specic corpora (e.g. Prop-
Bank, FrameNet). Currently we are in the process of
adding mappings between our verbs and FrameNet
verbs and mappings between our syntactic frames
and Xtag [XTAG Research Group2001] trees. These
3
It was pointed out that a possible interpretation is
that \from Aruba" is linked to the \renery" argument,
in which case this instance would be translated as Agent
V Theme and therefore have a perfect match to the Give-
13.1 class.
mappings will allow us to more deeply investigate
verb behavior.
Acknowledgments
This work was partially supported by NSF Grant
9900297, DARPA Tides Grant N66001-00-1-891 and
ACE Grant MDA904-00-C-2136.
References
Norman I. Badler, Martha Palmer, and Rama Bindi-
ganavale. 1999. Animation control for real-time
virtual humans. Communications of the ACM,
42(7):65{73.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project. In
Proceedings of the 17th International Conference
on Computational Linguistics (COLING/ACL-
98), pages 86{90, Montreal. ACL.
Rama Bindiganavale, William Schuler, Jan M. All-
beck, Norman I. Badler, Aravind K. Joshi, and
Martha Palmer. 2000. Dynamically Altering
Agent Behaviors Using Natural Language Instruc-
tions. Fourth International Conference on Au-
tonomous Agents, June.
Hoa Trang Dang, Karin Kipper, and Martha Palmer.
2000. Integrating compositional semantics into
a verb lexicon. In Proceedings of the Eighteenth
International Conference on Computational Lin-
guistics (COLING-2000), Saarbrucken, Germany,
July-August.
Christiane Fellbaum, editor. 1998. WordNet: An
Eletronic Lexical Database. Language, Speech and
Communications. MIT Press, Cambridge, Mas-
sachusetts.
Karen Sparck Jones and Branimir Boguraev. 1987.
A note on a study of cases. American Journal of
Computational Linguistics, 13((1-2)):65{68.
Paul Kingsbury and Martha Palmer. 2002. From
treebank to propbank. In Proceedings of the 3rd
International Conference on Language Resources
and Evaluation (LREC-2002), Las Palmas, Ca-
nary Islands, Spain.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of the Seventh National Conference
on Articial Intelligence (AAAI-2000), Austin,
TX, July-August.
Karin Kipper, Benjamin Snyder, and Martha
Palmer. 2004. Extending a verb-lexicon using a
semantically annotated corpus. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-04), Lisbon, Por-
tugal.
Beth Levin. 1993. English Verb Classes and Alterna-
tion, A Preliminary Investigation. The University
of Chicago Press.
Mitch Marcus. 1994. The penn treebank: A revised
corpus design for extracting predicate-argument
structure. In Proceedings of the ARPA Human
Language Technology Workshop, Princeton, NJ,
March.
George Miller. 1985. Wordnet: A dictionary
browser. In Proceedings of the First International
Conference on Information in Data, Waterloo,
Ontario.
M. Moens and M. Steedman. 1988. Temporal On-
tology and Temporal Reference. Computational
Linguistics, 14:15{38.
XTAG Research Group. 2001. A lexicalized tree
adjoining grammar for english. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
Proposition Bank II:  Delving Deeper 
 
 
Olga Babko-Malaya, Martha Palmer, Nianwen Xue, Aravind Joshi1, Seth Kulick 
University of Pennsylvania 
{malayao/mpalmer/xueniwen/joshi/skulick}@linc.cis.upenn.edu 
                                                          
1 Associated with Penn Discourse Treebank (PDTB). Other members of the project are Eleni Miltsakaki, Rashmi Prasad, 
(Univ. of PA) and Bonnie Webber (Univ. of Edinburgh) 
 
 
Abstract 
The PropBank project is creating a corpus of 
text annotated with information about basic 
semantic propositions. PropBank I (Kingsbury 
& Palmer, 2002) added a layer of predicate-
argument information, or semantic roles, to 
the syntactic structures of the English Penn 
Treebank.   This paper presents an overview 
of the second phase of PropBank Annotation, 
PropBank II, which is being applied to Eng-
lish and Chinese, and includes (Neodavid-
sonian) eventuality variables, nominal 
references, sense tagging, and connections to 
the Penn Discourse Treebank (PDTB), a pro-
ject for annotating discourse connectives and 
their arguments. 
1 Introduction 
An important question is the degree to which current 
statistical NLP systems can be made more domain-
independent without prohibitive costs, either in terms of 
engineering or annotation.  The Proposition Bank is 
designed as a broad-coverage resource to facilitate the 
development of more general systems.  It focuses on the 
argument structure of verbs, and provides a complete 
corpus annotated with semantic roles, including partici-
pants traditionally viewed as arguments and ad-
juncts.  Correctly identifying the semantic roles of the 
sentence constituents is a crucial part of interpreting 
text, and in addition to forming a component of the in-
formation extraction problem, can serve as an interme-
diate step in machine translation or automatic 
summarization. 
 
The Proposition Bank project takes a practical approach 
to semantic representation, adding a layer of predicate-
argument information, or semantic roles, to the syntactic 
structures of the Penn Treebank.  The resulting resource 
can be thought of as shallow, in that it does not repre-
sent co-reference, quantification, and many other 
higher-order phenomena, but also broad, in that it cov-
ers every verb in the corpus and allows representative 
statistics to be calculated. The semantic annotation pro-
vided by PropBank is only a first approximation at cap-
turing the full richness of semantic representation. 
Additional annotation of nominalizations and other 
noun predicates has already begun at NYU. This paper 
presents an overview of the second phase of PropBank 
Annotation, PropBank II, which is being applied to Eng-
lish and Chinese and includes (Neodavidsonian) eventu-
ality variables, nominal references, sense tagging, and 
discourse connectives.   
2 PropBank I 
PropBank (Kingsbury & Palmer, 2002) is an annotation 
of the Wall Street Journal portion of the Penn Treebank 
II (Marcus, 1994) with `predicate-argument' structures, 
using sense tags for highly polysemous words and se-
mantic role labels for each argument. An important goal 
is to provide consistent semantic role labels across dif-
ferent syntactic realizations of the same verb, as in the 
window in [ARG0 John] broke [ARG1 the window] and 
[ARG1 The window] broke. PropBank can provide fre-
quency counts for (statistical) analysis or generation 
components in a machine translation system, but pro-
vides only a shallow semantic analysis in that the anno-
tation is close to the syntactic structure and each verb is 
its own predicate. 
 
In PropBank, semantic roles are defined on a verb-by-
verb basis.  An individual verb's semantic arguments are 
simply numbered, beginning with 0.  Polysemous verbs 
have several Framesets, corresponding to a relatively 
coarse notion of word senses, with a separate set of 
numbered roles, a roleset, defined for each Frameset.  
For instance, leave has both a DEPART Frameset ([ARG0 
John] left [ARG1 the room]) and a GIVE Frameset, ([ARG0 
I] left [ARG1 my pearls] [ARG2 to my daughter-in-law] 
[ARGM-LOC in my will].)   While most Framesets have 
three or four numbered roles, as many as six can appear, 
in particular for certain verbs of motion. Verbs can take 
any of a set of general, adjunct-like arguments 
(ARGMs), such as LOC (location), TMP (time), DIS 
(discourse connectives), PRP (purpose) or DIR (direc-
tion).  Negations (NEG) and modals (MOD) are also 
marked. 
 
The same annotation philosophy has been extended to 
the Penn Chinese Proposition Bank (Xue and Palmer, 
2003). The Chinese PropBank annotation is performed 
on a smaller (250k words) and yet growing corpus an-
notated with syntactic structures (Xue et al2004). The 
same syntactic alternations that form the basis for the 
English PropBank annotation also exist in robust quanti-
ties in Chinese, even though it may not be the case that 
the same exact verbs (meaning verbs that are close 
translations of one another) have the exact same range 
of syntactic realization for Chinese and English.  For 
example, in (1), "xin-nian/New Year  zhao-dai-
hui/reception" plays the same role in (a) and (b), which 
is the event or activity held,  even though it occurs in 
different syntactic positions. Assigning the same argu-
ment label, Arg1, to both instances, captures this regu-
larity. It is worth noting that the predicate ?ju-
xing/hold" does not have passive morphology in (1a), 
despite of what its English translation suggests. Like the 
English PropBank, the adjunct-like elements receive 
more general labels like TMP or LOC, as also illustrated 
in (1). The tag set for Chinese and English PropBanks 
are to a large extent similar and more details can be 
found in (Xue and Palmer, 2003). 
  
(1) a. [ARG1 xin-nian/New Year zhao-dai-
hui/reception] [ARGM-TMP jin-tian/today] [ARGM-
LOC zai/at diao-yu-tai/Diaoyutai guo-bin-guan/state 
guest house ju-xing/hold]  
"The New Year reception was held in Diaoyutai State 
Guest House today." 
 
          b. [ARG0 tang-jia-xuan/Tang Jiaxuan] [ARGM-
TMP jin-tian/today] [ARGM-LOC zai/at diao-yu-
tai/Diaoyutai guo-bin-guan/state guest house] ju-
xing/hold [arg1 xin-nian/New Year zhao-dai-
hui/reception] 
"Tang Jiaxuan was holding the New Year Reception in 
Diaoyutai State Guest House today." 
 
For polysemous verbs that take different sets of seman-
tic roles, we also distinguish different Framesets. (2) 
and (3) illustrate the different Framesets of "tong-
guo/pass", which correspond loosely with major senses 
of the verb.  The Frameset in (2) roughly means "pass 
by voting" while the Frameset illustrated by (3) means 
"pass through". The different Framesets are generally 
reflected in the different alternation patterns, which can 
serve as a cue for statistical systems performing Frame-
set disambiguation. (2) is similar to the causa-
tive/inchoative alternation (Levin, 1993). In contrast, (3) 
shows object drop. 
 
(2) a. [ARG0 guo-hui/Congress] zui-jin/recently tong-
guo/pass le/ASP [ARG1 zhou-ji/interstate yin-hang-
fa/banking law] 
    "The U.S. Congress recently passed the inter-state 
banking law." 
      b. [ARG1 zhou-ji/interstate yin-hang-fa/banking 
law] zui-jin/recently tong-guo/pass le/ASP 
       "The inter-state banking law passed recently." 
 
(3) a. [ARG0 huo-che/train] zheng-zai/now tong-
guo/pass [ARG1 sui-dao/tunnel] 
       "The train is passing through the tunne." 
        b. [ARG0 huo-che/train]  zheng-zai/now  tong-
guo/pass. 
       "The train is passing." 
 
There are also some notable differences between Chi-
nese PropBank and English PropBank. In general, the 
verbs in the Chinese PropBank are less polysemous, 
with the vast majority of the verbs having just one 
Frameset. On the other hand, the Chinese PropBank has 
more verbs (including static verbs which are generally 
translated into adjectives in English) normalized by the 
corpus size.  
3 Adding Event Variables to PropBank 
Event variables provide a rich analytical tool for analyz-
ing verb meaning. Positing that there is an event vari-
able allows for a straightforward representation of the 
logical form of adverbial modifiers, the capturing of 
pronominal reference to events, and the representation 
of nouns that refer to events. For example, event vari-
ables make it possible to have direct reference to an 
event with a noun phrase, as in (4a) destruction, and to 
refer back to an event with a pronoun (as illustrated in 
(4b) That): 
 
(4) a. The destruction of Pompeii happened in the 1st 
century. 
       b. Brutus stabbed Caesar. That was a pivotal event 
in history. 
 
PropBank I annotations can be translated straightfor-
wardly into logical representations with event variables, 
as illustrated in (5), with relations being defined as 
predicates of events, and Args and ArgMs representing 
relations between event variables and corresponding 
phrases.   
 
(5) a. Mr. Bush met him privately, in the White House,          
on Thursday. 
 
     b. PropBank annotation 
  Rel:  met                      
       Arg0: Mr. Bush  
       ArgM-MNR: privately 
       ArgM-LOC: in the White House 
       ArgM-TMP: on Thursday 
 
     c. Logical representation with an event variable  
 ?e meeting(e) & Arg0(e, Mr. Bush) & Arg1(e, he) 
& MNR(e, privately) & LOC(e, ?in the White 
House?) & TIME(e, ?on Thursday?) 
 
As the representation in (5c) shows, we adopt Neo-
davidsonian analysis of events, which follows Parsons 
(1990) in treating arguments on a par with modifiers in 
the event structure. An alternative analysis is the origi-
nal Davidsonian analysis of events (Davidson 1967), 
where the arguments of the verb are analyzed as its 
logical arguments. 
  
Our choice of a Neodavidsonian representation is moti-
vated by its predictions with respect to obligatoriness of 
arguments. Under the Davidsonian approach, arguments 
are logical arguments of the verb and thus must be im-
plied by the meaning of the sentence, either explicitly or 
implicitly (i.e. existentially quantified). On the other 
hand, it has been a crucial assumption in PropBank that 
not all roles must necessarily be present in each sen-
tence. For example, the Frameset for the verb serve, 
shown in (6a) has three roles: Arg0, Arg1, and Arg2. 
Actual usages of the verb, on the other hand, do not 
require the presence of all three roles. For example, the 
sentence in (6b), as its PropBank annotation in (6c) 
shows, does not include Arg1. 
 
(6)  a.  serve.01 "act, work":            
Arg0:worker 
Arg1:job, project 
Arg2:employer 
 
     b.  Each new trading roadblock is likely to be beaten     
by institutions seeking better ways *trace* to serve 
their high-volume clients.  
 
c. Arg0:  *trace* -> institutions 
     REL:    serve 
     Arg2:   their high-volume clients 
 
As the representations in (7) illustrate, only the Neo-
davidsonian representation gives the correct interpreta-
tion of this sentence. 
 
(7) Davidsonian representation: 
 ?e ?z serve(e, institutions, z, their high-volume 
clients) 
 
Neodavidsonian representation: 
     ?e serve(e)&Arg0(e, institutions)&Arg2(e, their 
high-volume clients) 
 
Assuming a Neodavidsonian representation, we can 
analyze all Args and certain types of modifiers as predi-
cates of events.  The types of ArgMs that can be ana-
lyzed as predicates of event variables are shown below: 
 
? MNR:   to manage businesses profitably 
? TMP:    to run the company for 23 years 
? LOC:    to use the notes on the test 
? DIR:     to jump up 
? CAU:    because of ? 
? PRP:      in order to ? 
 
Whereas for the most part, translating these adverbials 
into modifiers of event variables does not require man-
ual annotation, certain constructions need human revi-
sion. For example, in the sentence in (8a) the temporal 
ArgM ?for the past five years? does not modify the event 
variable e introduced by the verb manage, as our auto-
matic translation would predict. The revised analysis of 
this sentence, given in (8b), follows Krifka 1989, who 
proposed that negated sentences refer to maximal events 
? events that have everything that happened during their 
running time as a part. Annotation of this sentence 
would thus require us to introduce an additional event 
variable, the maximal event e?, which has a duration 
?for the past five years? and has no event of unions 
managing wage increases as part. 
 
(8)  a. For the past five years, unions have not managed 
to win wage increases. 
       b. ?e? TMP(e?, ?for the past five years?) &  
??e(e<e? & managing(e) & Arg0(e, unions) & 
Arg1(e, ?win wage increases?)) 
 
Further annotation involves linking empty categories in 
PropBank to event variables in cases of control, as illus-
trated in (9), where event variables can be viewed as the 
appropriate antecedents for PRO, marked as ?*? below: 
    
(9) The car collided with a lorry, * killing both drivers. 
 
And, finally, we will consider tagging variables accord-
ing to the aspectual class of the eventuality they denote, 
such as states or events. Events, such as John built a 
house, involve some kind of change and usually imply 
that some condition, which obtains when the event be-
gins, is terminated by the event. States, on the other 
hand, do not involve any change and hold for varying 
amounts of time. It does not make sense to ask how long 
a state took (as opposed to events), and whether the 
state is culminated or finished.  
 
This distinction between states and events plays an im-
portant role for the temporal analysis of discourse, as 
the following examples (from Kamp and Reyle 1993) 
illustrate: 
 
(10) a. A man entered the White Hart. Bill served him a 
beer. 
       b. I arrived at the Olivers? cottage on Friday night. 
It was not a propitious beginning to my visit. She 
was ill and he in a foul mood. 
 
If a non-initial sentence denotes an event, then it is typi-
cally understood as following the event described by the 
preceding sentence. For example, in (10a), the event of 
Bill serving a beer is understood as taking place after 
the event of ?a man entering the White Hart? was com-
pleted.  On the other hand, states are interpreted as tem-
porally overlapping with the time of the preceding 
sentence, as illustrated in (10b). The sentences she was 
ill and he was in a foul mood seem to describe a state of 
affairs obtaining at the time of the speaker?s arrival.  
 
As this example illustrates, there are different types of 
temporal relations between eventualities (as we will call 
both events and states) and adverbials that modify them, 
such as temporal overlap and temporal containment. 
Furthermore, these relations crucially depend on the 
aspectual properties of the sentence. Translation of PB 
annotations to logical representations with eventuality 
variables and tagging these variables according to their 
aspectual type would thus make it possible to provide an 
analysis of temporal relations. This analysis should also 
be compatible with a higher level of annotation of tem-
poral structure (e.g. Ferro et al 2001).   
4 Annotation of Nominal Coreference 
Our approach to coreference annotation is based on the 
recognition of the different types of relationships that 
might be called "coreference".  The most straightfor-
ward case is that of two semantically definite NPs that 
refer to identical entities, as in (11).  Anaphoric rela-
tions (very broadly defined) are those in which one NP 
(or possessive adjective) has no referential value of its 
own but depends on an antecedent for its interpretation. 
In some cases this can be relatively simple, as in (12), in 
which the pronoun He takes John Smith as its antece-
dent.  However, in some cases, as in (13), the antecedent 
may not even be a referring expression, or can, as in 
(14), refer to an entity that may or may not exist, with 
the non-existent a car being the antecedent of it.  The 
anaphor does not have to be an NP, as in (15), in which 
the possessive their, which takes many companies as its 
antecedent, is an adjective. 
 
(11) John Smith of Company X arrived yesterday.  Mr. 
Smith said that..." 
(12) John Smith of Company X arrived yesterday.  He 
said that..." 
(13) No team spoke about its system. 
(14) I want to buy a car.  I need it to go to work. 
(15) Many companies raised their payouts by more than 
10%. 
 
Another level of complexity is raised by NPs that are 
not anaphors, in that they have their own reference (per-
haps abstract or nonexistent), but are not in an identity 
relationship with an antecedent, but rather describe a 
property of that antecedent.  Typical cases of this are 
predicate nominals, as in (16), or appositives, as in (17), 
and other cases as in (18). 
 
(16) Larry is a university lecturer. 
(17) Larry, the chair of his department, became presi-
dent. 
(18) The stock price fell from $4.02 to $3.85 
 
As has been discussed (e.g., van Deemter & Kibble, 
2001), such cases have fundamentally different proper-
ties than either the identity relationships of (11) or the 
anaphoric relationships of (12)-(15).   
 
Annotation of nominal co-reference is being done in 
two passes. The first pass involves annotation of true 
co-reference between semantically definite NPs`. The 
issue here is to consider what the semantically definite 
nouns are.  Initially, they are defined as proper nouns 
(named entities), either as NPs (America) or prenominal 
adjectives (American politicians).   
 
(19) The last time the S&P 500 yield dropped below 3% 
was in the summer of 1987... There have been only 
seven other times when the yield on the S&P 500 
dropped....   
 
It is reasonable to expand this to definite descriptions, 
so that in (19), the S&P 500 yield and the yield on the 
S&P 500 are marked as coreferring.  However, some 
definite NPs can refer to clauses, not NPs, such as The 
pattern in (20), and we will not do such cases of clausal 
antecedents on the first pass. 
 
(20) The index fell 40% in 1975 and jumped 80% in 
1976.  The pattern is an unusual one. 
 
Anaphoric relations are being done on a "need-to-
annotate" basis.   For each anaphoric NP or possessive 
adjective, the annotator needs to determine its antece-
dent.  As discussed, this is a different type of relation 
than identity, and this distinction will be noted in the 
annotation. The issue here is what we consider an ana-
phoric element to be.  We consider all cases of pro-
nouns, possessives, reflexives, and NPs with that/those 
to be potential cases of anaphors (again, broadly de-
fined). However, as with definite NPs, we only mark 
those that have an NP antecedent, and not clausal ante-
cedents.  For example, in (21), it refers to the current 
3.3% reading, and so would be marked as being in an 
antecedent-anaphor relation. In (22), it refers to having 
the dividend increases, which is not an NP, and so 
would not be marked as being in an anaphor relation in 
the first pass. Similar considerations apply to potential 
anaphors like those NP, that NP, etc. 
 
 (21) ...the current 3.3% reading isn't as troublesome as 
it might have been. 
(22) Having the dividend increases is a supportive ele-
ment in the market outlook, but I don't think it's a 
main consideration". 
 
Note that placing the burden on the anaphors to deter-
mine what gets marked as being in an anaphor-
antecedent leaves it open as to what the antecedent 
might be, other than the requirement just mentioned of it 
being an NP.  Not only might it be non-referring NPs as 
in  (13) or (14), it could even be a generic, as in (23), in 
which books is the antecedent for they. 
 
(23) I like books.  They make me smile. 
 
The second pass will tackle the more difficult issues: 
 
1. Descriptive NPs, as in (16)-(18).  While the informa-
tion provided by these cases would be extremely valu-
able for information extraction and other systems, there 
are some uncertain issues here, mostly focusing on how 
such descriptors describe the antecedent at different 
moments in time and/or space.  The crucial question is 
therefore what to take the descriptor to be.   
 
(24) Henry Higgins might become the president of 
Dreamy Detergents. 
 
For example, in (18), it can't be just $4.02 and $3.85, 
since this does not include information about *when* 
the stock price had such values. The same issue arises 
for (17).  As van Deemter & Kibble point out, such 
cases can interact with issues of modality in uncertain 
ways, as illustrated in (24).  Just saying that in (24) the 
president of Dreamy Detergents is in the same type of 
relationship with Henry Higgins as a university lecturer 
is with Larry in (16) would be very misleading. 
 
2. Clausal antecedents - Here we will handle cases of it 
and other anaphor elements and definite NPs referring 
to non-NPs as antecedents, as in (21).  This will most 
likely be done by referring to the eventuality variable 
associated with the antecedent. 
5 Linking to the Penn Discourse Treebank 
(PDTB)  
The Penn Discourse Treebank (PDTB) is currently be-
ing built by the PDTB team at the University of Penn-
sylvania, providing the next appropriate level of  
annotation: the annotation of the predicate argument 
structure of connectives (Miltsakaki et al2004a/b). The 
PDTB project is based on the idea that discourse con-
nectives can be thought of as predicates with their asso-
ciated argument structure. This perspective of discourse 
is based on a series of papers extending lexicalized tree-
adjoining grammar (LTAG) to discourse (DLTAG), 
beginning with Webber and Joshi (1998).2  This level of 
annotation is quite complex for a variety of reasons, 
such as the lack of available literature describing dis-
course connectives and frequent occurrences of empty 
(lexically null) connectives between two sentences that 
cannot be ignored. Also, unlike the predicates at the 
sentence level, some of the discourse connectives, espe-
cially discourse adverbials, take their arguments ana-
phorically and not structurally, requiring an intimate 
association with event variable representation.  
 
The long-range goal of the PDTB project is to develop a 
large scale and reliably annotated corpus that will en-
code coherence relations associated with discourse con-
nectives, including their argument structure and 
anaphoric links, thus exposing a clearly defined level of 
discourse structure and supporting the extraction of a 
range of inferences associated with discourse connec-
tives. This annotation will reference the Penn Treebank 
(PTB) annotations as well as PropBank. 
 
In PDTB, a variety of connectives are considered, such 
as subordinate and coordinate conjunctions, adverbial 
connectives and implicit connectives amounting to a 
total of approximately 20,000 annotations; 10,000 im-
                                                          
2 The PDTB annotations are deliberately kept independ-
ent of DLTAG framework for two reasons: (1) to make the 
annotated corpus widely useful to researchers working in 
different frameworks and (2) to make the annotation task 
easier, thereby increasing interannotator reliability. 
plicit connectives and 10,000 annotations of the 250 
explicit connectives identified in the corpus (for details 
see (Miltsakaki et al2004a and Miltsakaki et al2004b).  
Current annotations in PDTB are performed by four 
annotators. Individual annotation proceeds one connec-
tive at a time. This way, the annotators quickly gain 
experience with that connective and develop a better 
understanding of its predicate-argument characteristics. 
For the annotation of implicit connectives, the annota-
tors are required to provide an explicit connective that 
best expressed the inferred relation. 
 
The PDTB is expected to be released by November 
2005. The final version of the corpus  will also contain 
characterizations of the semantic roles associated with 
the arguments of each type of connective as well as 
links to PropBank. 
6.   Annotation of Word Senses 
The critical question with respect to sense tagging in-
volves the choice of senses.  In other words, which 
sense inventory, and which level of granularity with 
respect to that sense inventory?  The PropBank Frames 
Files for the verbs include coarse-grained sense distinc-
tions based primarily on usages of a verb that have dif-
ferent numbers of predicate-arguments. These are 
termed Framesets ? referring to the set of roles for each 
one and the corresponding set of syntactic frames.  We 
are currently sense-tagging the annotated predicates for 
lemmas with multiple Framesets, which can be done 
quickly and accurately with an inter-annotator agree-
ment of over 90%.  The distinctions made by the 
Framesets are very coarse, and each one would map to 
several standard dictionary entries for the lemma in 
question.  More fine-grained sense distinctions could be 
useful for Automatic Content Extraction, yet it remains 
to be determined exactly which distinctions are neces-
sary and what methodology should be followed to pro-
vide additional word sense annotation. 
 
Palmer et al(2004b) present an hierarchical approach to 
verb senses, where different levels of sense distinctions, 
from PropBank Framesets to WordNet senses, form a 
continuum of granularity. At the intermediate level of 
sense hierarchy we are considering manual groupings of 
the SENSEVAL-2 verb senses (Palmer, et.al., 2004a), 
developed in a separate project. Given a large disagree-
ment rate between annotators (average inter-annotator 
agreement rate for Senseval-2 verbs was only 71%), 
verbs were grouped by two or more people into sets of 
closely related senses, with grouping differences being 
reconciled, and the sense groups were used for coarse-
grained scoring of the systems. These groupings of 
WordNet senses were shown to reconcile a substantial 
portion of the manual and automatic tagging disagree-
ments, showing that many of these disagreements are 
fairly subtle.  Using the groups as a more coarse-grained 
set of sense distinctions improved ITA and system 
scores by almost 10%, to 82% and 69%, respectively 
(Palmer, et. al. 2004a). 
 
We have been investigating whether or not the groups 
can provide an intermediate level of hierarchy in be-
tween the PropBank Framesets and the WN 1.7 senses.  
Based on our existing WN 1.7 tags and Frameset tags of 
the Senseval2 verbs in the Penn Treebank, 95% of the 
verb instances map directly from sense groups to 
Framesets, with each Frameset typically corresponding 
to two or more sense groups. Using the PropBank 
coarse-grained senses as a starting place, and WordNet 
sense tagging for over 1000 verbs produced automati-
cally through mapping VerbNet to PropBank (Kipper, 
et. al., 2004), we have the makings of a large scale tag-
ging experiment on the Penn Treebank.  This will en-
able investigations into the applicability of clearly 
defined criteria for sense distinctions at varying levels 
of granularity, and produce a large, 1M word corpus of 
sense-tagged text for training WSD systems 
 
The hierarchical approach to verb senses, as utilized by 
most standard dictionaries as well as Hector (Atkins, 
?93), and as applied to SENSEVAL-2, presents obvious 
advantages for the problem of Word Sense Disambigua-
tion. The human annotation task is simplified, since 
there are fewer choices at each level and clearer distinc-
tions between them.  The automated systems can com-
bine training data from closely related senses to 
overcome the sparse data problem, and both humans 
and systems can back off to a more coarse-grained 
choice when fine-grained choices prove too difficult.  
 
Conclusion 
This paper has presented an overview of the second 
phase of PropBank Annotation, PropBank II, which is 
being applied to English and Chinese. It  includes (Neo-
davidsonian) eventuality variables, nominal references, 
an hierarchical approach to sense tagging, and connec-
tions to the Penn Discourse Treebank (PDTB), a project 
for annotating discourse connectives and their argu-
ments. 
References 
 
Atkins, S. (1993) Tools for computer-aided corpus lexi-
cography: The Hector Project.  Actu Linguistica Hunguricu, 
41:5-72. 
 
Carlson, L., Marcu, D. and Okurowski, M. E. (2002). 
Building a Discourse-Tagged Corpus in the Framework of 
Rhetorical Structure Theory. In Current Directions in Dis-
course and Dialogue, Jan van Kuppevelt and Ronnie Smith 
eds., Kluwer Academic Publishers. To appear. 
 
Davidson, D. 1967.  The Logical Form of Action Sen-
tences. In The Logic of Decision and Action, ed. Nicholas 
Rescher.  81--95. Pittsburgh: University of Pittsburgh 
Press.  Republished in Donald Davidson, Essays on Actions 
and Events,Oxford University Press, Oxford, 1980. 
 
Edmonds, P. and Cotton, S. 2001. SENSEVAL-2: Over-
view. In Proceedings of SENSEVAL-2: Second International 
Workshop on Evaluating Word Sense Disambiguation Sys-
tems, ACL-SIGLEX, Toulouse, France. 
 
Ferro L, I. Mani, B. Sundheim and G.Wilson 2001 TIDES 
Temporal Annotation Guidelines, MITRE Technical Report, 
MTR 01W0000041.  
 
Kamp, H. and U.Reyle. 1993. From Discourse to Logic, 
Kluwer, Dordrecht. 
 
Kingsbury, P. and Palmer, M, (2002), From TreeBank to 
PropBank, Third International Conference on Language  Re-
sources and Evaluation, LREC-02, Las Palmas, Canary Is-
lands, Spain, May 28- June 3. 
 
Kilgarriff, A. and Palmer, M.. 2000. Introduction to the 
special issue on Senseval, Computers and the Humanities, 
34(1-2):1-13. 
 
Kipper K., B. Snyder, and M. Palmer. (to appear, 2004) 
"Extending a verb-lexicon using a semantically annotated 
corpus". Proceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC-04). Lisbon, 
Portugal, 2004. 
 
Krifka, M. 1989. Nominalreferenz und Zeitkonstitution. 
M?nchen, Wilhelm Fink Verlag 
 
Levin, B. 1993. English Verb Classes and Alternations: a 
Preliminary Investigation. Chicago: The University of Chi-
cago Press. 
 
Mann, W. and S. Thompson. 1986. ?Relational Proposi-
tions in Discourse?, Discourse Processes 9, 57-90. 
Marcu, D. 2000. The Theory and Practice of Discourse 
Parsing and Summarization. The MIT Press. 
Miltsakaki, E., R. Prasad, A. Joshi and B. Webber. 2004a. 
The Penn Discourse Treebank. In Proceedings of the 4th In-
ternational Conference on Language Resources and Evalua-
tion (LREC 2004), Lisbon. 
 
Miltsakaki, E., R. Prasad, A. Joshi and B. Webber. 2004b. 
Annotation of Discourse Connectives and Their Arguments, in 
Proceedings of the HLT-EACL Workshop on Frontiers in 
Corpus Annotation, Boston, Massachussetts. 
      Palmer, M., Dang, H. T, and Fellbaum, C., 2004a.  Making 
fine-grained and coarse-grained sense distinctions, both manu-
ally and automatically, under revision for Natural Language 
Engineering. 
Palmer, M., Babko-Malaya, O., Dang, H. T., 2004b. Dif-
ferent Sense Granularities for Different Applications, to ap-
pear in the Scalable Natural Language Understanding 
Workshop, held in conjunction with HLT/NAACL-04, May, 
2004. 
 
Parsons, T. 1990.  Events in the Semantics of Eng-
lish.  Cambridge, MA: MIT Press. 
  
van Deemter, K. and R. Kibble. 2000. ?On Coreferring: 
Coreference in MUC and Related Annotation Schemes?, 
Computational Linguistics 26:629-637. 
 
Webber B. and A. Joshi. 1998. Anchoring a lexicalized 
tree-adjoining grammar for discourse. In ACL/COLING 
Workshop on Discourse Relations and Discourse Markers, 
Montreal, Canada, pp. 41-48. 
 
 
Xue, N. and Palmer, M. 2003. Annotating the Propositions 
in the Penn Chinese Treebank. In the Proceedings of the Sec-
ond SIGHAN Workshop on Chinese Language Processing.  
Sapporo, Japan. 
  
Xue, Nianwen, Xia, Fei, Chiou, Fu-dong and Palmer, 
Martha. 2004. The Penn Chinese Treebank: phrase structure 
annotation of a large corpus. Natural Language Engineering, 
10(4):1-30, June 2004.  
 
 
Different Sense Granularities for Different Applications 
 
 
Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang 
University of Pennsylvania 
{mpalmer/malayao/htd}@linc.cis.upenn.edu 
 
 
Abstract 
This paper describes an hierarchical approach 
to WordNet sense distinctions that provides 
different types of automatic Word Sense Dis-
ambiguation (WSD) systems, which perform 
at varying levels of accuracy.  For tasks where 
fine-grained sense distinctions may not be es-
sential, an accurate coarse-grained WSD sys-
tem may be sufficient. The paper discusses the 
criteria behind the three different levels of 
sense granularity, as well as the machine learn-
ing approach used by the WSD system. 
1 Introduction 
The difficulty of finding consistent criteria for making 
sense distinctions has been thoroughly attested to in the 
literature (Kilgarriff, ?97, Hanks, ?00).  Difficulties have 
been found with truth-theoretical criteria, linguistic crite-
ria and definitional criteria (Sparck-Jones, ?86, Geer-
aerts, ?93).  In spite of the proliferation of dictionaries, 
there is no methodology by which two lexicographers 
working independently are guaranteed to derive the same 
set of distinctions for a given word, with objects and 
events vying for which is the most difficult to character-
ize (Cruse, ?86, Apresjan, ?74, Pustejovsky, ?91, ?95).   
 
On the other hand, accurate Word Sense Disambiguation 
(WSD) could significantly improve the precision of In-
formation Retrieval by ensuring that the senses of verbs 
in the retrieved documents match the sense of the verb in 
the query.  For example, the two queries What do you 
call a successful movie? and Whom do you call for a 
successful movie? submitted to AskJeeves both retrieve 
the same set of documents, even though they are asking 
quite different questions, referencing very different 
senses of call.  The documents retrieved are also not very 
relevant, again because they do not distinguish which 
matches contain relevant senses and which do not. 
 
Tips on Being a Successful Movie Vampire ... I shall 
call the police. 
 
Successful Casting Call & Shoot for ``Clash of Em-
pires'' ... thank everyone for their participation in the 
making of yesterday's movie. 
 
Demme's casting is also highly entertaining, although I 
wouldn't go so far as to call it successful. This  movie's 
resemblance to its predecessor is pretty vague... 
 
VHS Movies: Successful Cold Call Selling: Over 100 
New Ideas, Scripts, and Examples from the Nation's 
Foremost Sales Trainer. 
 
The two senses of call in the two queries can be easily 
distinguished by their differing predicate-argument 
structures.  They are also separate senses in WordNet, 
but WordNet has an additional 26 senses for call, and the 
current best performance of an automatic Word Sense 
Disambiguation system this type of polysemous verb is 
only 60.2% (Dang and Palmer, 2002).  Is it possible that 
sense distinctions that are less fine-grained than Word-
Net?s distinctions could be made more reliably, and 
could still benefit this type of NLP application?   
 
The idea of underspecification as a solution to WSD has 
been proposed in Buitelaar 2000 (among others), who 
pointed out that for some applications, such as document 
categorization, information retrieval, and information 
extraction it may be sufficient to know if a given word 
belongs to a certain class of WordNet senses or under-
specified sense. On the other hand, there is evidence that 
machine translation of languages as diverse as Chinese 
and English will require all of the fine-grained sense 
distinctions that WordNet is capable of providing, and 
even more (Ng, et al2003, Palmer, et. al., to appear).   
 
An hierarchical approach to verb senses, of the type dis-
cussed in this paper, presents obvious advantages for the 
problem of word sense disambiguation. The human an-
notation task is simplified, since there are fewer choices 
at each level and clearer distinctions between them.  The 
automated systems can combine training data from 
closely related senses to overcome the sparse data prob-
lem, and both humans and systems can back off to a 
more coarse-grained choice when fine-grained choices 
prove too difficult. 
 
The approach to verb senses presented in this paper as-
sumes three different levels of sense distinctions: Prop-
Bank Framesets, WordNet groupings, and WordNet 
senses.  In a project for the semantic annotation of predi-
cate-argument structure, PropBank, we have made 
coarse-grained sense distinctions for the 700 most 
polysemous verbs in the Penn TreeBank (Kingsbury and 
Palmer, ?02).  These distinctions are based primarily on 
different subcategorization frames that require different 
argument label annotations. In a separate project, as dis-
cussed in Palmer et al2004, we have grouped 
SENSEVAL-2 verb senses (which came from WordNet 
1.7). These manual groupings were shown to reconcile a 
substantial portion of the manual and automatic tagging 
disagreements, showing that many of these disagree-
ments are fairly subtle (Palmer, et.al., ?04).   
 
The tree levels of sense distinctions form a continuum of 
granularity. Our criterion for the Framesets, being pri-
marily syntactic, is also the most clear cut. These distinc-
tions are based primarily on usages of a verb that have 
different numbers of predicate-arguments, however they 
also separate verb senses on semantic grounds, if these 
senses are not closely related. Sense groupings provide 
an intermediate level of hierarchy, where groups are 
distinguished by more fine-grained criteria.  Both 
Frameset and grouping distinctions can be made consis-
tently by humans and systems (over 90% accuracy for 
Framesets and 82% for groupings) and are surprisingly 
compatible; 95% of our groups map directly onto a sin-
gle PropBank sense.   
 
2  Background 
2.1 Propbank 
PropBank [Kingsbury & Palmer, 2002] is an annotation 
of the Wall Street Journal portion of the Penn Treebank 
II [Marcus, 1994] with dependency structures (or `predi-
cate-argument' structures), using sense tags for highly 
polysemous words and semantic role labels for each de-
pendency. An important goal is to provide consistent 
semantic role labels across different syntactic realiza-
tions of the same verb, as in the window in [ARG0 John] 
broke [ARG1 the window] and [ARG1 The window] broke. 
PropBank can provide frequency counts for (statistical) 
analysis or generation components in a machine transla-
tion system, but provides only a shallow semantic analy-
sis in that the annotation is close to the syntactic 
structure and each verb is its own predicate. 
 
In addition to the annotated corpus, PropBank provides a 
lexicon that lists, for each broad meaning of each anno-
tated verb, its Frameset, i.e., the possible arguments in 
the predicate and their labels and all possible syntactic 
realizations.  The notion of ``meaning'' used is fairly 
coarse-grained, and it is typically motivated from differ-
ing syntactic behavior.  The Frameset alo includes a 
``descriptor'' field for each role which is intended for use 
during annotation and as documentation, but which does 
not have any theoretical standing. The collection of 
Frameset entries for a verb is referred to as the verb's 
frame.  As an example of a PropBank entry, we give the 
frame for the verb leave below.  Currently, there are 
frames for over 3,000 verbs, with a total of just over 
4,300 Framesets described.  Of these 3,000 verb frames, 
only a small percentage 21.8 % (700) have more than 
one Frameset, with less than 100 verbs with 4 or more.  
The process of sense-tagging the PropBank corpus with 
the Frameset tags has just been completed. 
 
The criteria used for the Framesets are primarily syntac-
tic and clear cut. The guiding principle is that two verb 
meanings are distinguished as different framesets if they 
have distinct subcategorization frames. For example, the 
verb ?leave? has 2 framesets with the following frames, 
illustrated by the examples in (1) and (2): 
 
Frameset 1:  move away from 
Arg0:entity leaving 
Arg1:place left 
 
Frameset 2:  give  
Arg0:giver / leaver 
Arg1:thing given 
Arg2:benefactive / given-to 
 
(1) John left the room. 
(2) Mary left her daughter-in-law her pearls in her will 
 
2.2 WordNet  Sense Groupings 
In a separate project, as part of Senseval tagging exer-
cises, we have developed a lexicon with another level of 
coarse-grained distinctions, as described below. 
 
The Senseval-1 workshop (Kilgarriff and Palmer, 2000) 
provided convincing evidence that supervised automatic 
systems can perform word sense disambiguation (WSD) 
satisfactorily, given clear, consistent sense distinctions 
and suitable training data.  However, the Hector lexicon 
that was used as the sense inventory was very small and 
under proprietary constraints, and the question remained 
whether it was possible to have a publicly available, 
broad-coverage lexical resource for English and other 
languages, with the requisite clear, consistent sense dis-
tinctions. 
 
Subsequently, the Senseval-2 (Edmonds and Cotton, 
2001) exercise was run, which included WSD tasks for 
10 languages.  A concerted effort was made to use exist-
ing WordNets as sense inventories because of their 
widespread popularity and availability. Each language 
had a choice between the lexical sample task and the all-
words task.  The most polysemous words in the English 
Lexical Sample task are the 29 verbs, with an average 
polysemy of 16.28 senses using the pre-release version 
of WordNet 1.7.  Double blind annotation by two lin-
guistically trained annotators was performed on corpus 
instances, with a third linguist adjudicating between in-
ter-annotator differences to create the ?Gold Standard.?  
The average inter-annotator agreement rate was only 
71%, which is comparable to the 73% agreement for all 
words in SemCor, with a much lower average polysemy. 
However, a comparison of system performance on words 
of similar polysemy in Senseval-1 and Senseval-2 
showed very little difference in accuracy (Palmer et al, 
submitted).  In spite of the lower inter-annotator agree-
ment figures for Senseval-2, the double blind annotation 
and adjudication provided a reliable enough filter to en-
sure consistently tagged data with WordNet senses.  
Even so, the high polysemy of the WordNet 1.7 entries 
on average poses a challenge for automatic word sense 
disambiguation.  In addition, WordNet only gives a flat 
listing of alternative senses, unlike most standard dic-
tionaries which are more structured and often provide 
hierarchical entries. To address this lack, the verbs were 
grouped by two or more people, with differences being 
reconciled, and the sense groups were used for coarse-
grained scoring of the systems. 
 
The criteria used for groupings included syntactic and 
semantic ones. Syntactic structure performed two dis-
tinct functions in our groupings. Recognizable alterna-
tions with similar corresponding predicate-argument 
structures were often a factor in choosing to group 
senses together, as in the Levin classes and PropBank, 
whereas distinct subcategorization frames were also of-
ten a factor in putting senses in separate groups.  Fur-
thermore, senses were grouped together if they were 
more specialized versions of a general sense.  The se-
mantic criteria for grouping senses separately included 
differences in semantic classes of arguments (abstract 
versus concrete, animal versus human, animacy versus 
inanimacy, different instrument types...), differences in 
the number and type of arguments (often reflected in the 
subcategorization frame as discussed above), differences 
in entailments (whether an argument refers to a created 
entity or a resultant state), differences in the type of 
event (abstract, concrete, mental, emotional...), whether 
there is a specialized subject domain, etc.   
 
Senseval-2 verb inter-annotator disagreements were re-
duced by more than a third when evaluated against the 
groups, from 29% to 18%, and by over half in a separate 
study, from 28% to 12%.    A similar number of random 
groups provided almost no benefit to the inter-annotator 
agreement figures (74% instead of 71%), confirming the 
greater coherence of the manual groupings. 
3 Mapping of Sense Groups to Framesets  
Groupings of senses for Senseval-2, as discussed above, 
use both syntactic and semantic criteria.  Propbank, on 
the other hand, uses mostly syntactic cues to divide verb 
senses into framesets. As a result, framesets are more 
general than sense-groups and usually incorporate sev-
eral sense groups. We have been investigating whether 
or not the groups developed for SENSEVAL-2 can provide 
an intermediate level of hierarchy in between the Prop-
Bank Framesets and the WN 1.7 senses, and our initial 
results are promising.  Based on our existing WN 1.7 
tags and frameset tags of the Senseval2 verbs in the Penn 
TreeBank, 95% of the verb instances map directly from 
sense groups to framesets, with each frameset typically 
corresponding to two or more sense groups, as illustrated 
by the tables 1-4 for the verbs ?serve?, ?leave?, ?pull?, and 
?see?1  below. 
 
As the tables 1-4 illustrate, the criteria used to split the 
Framesets into groups are as follows:  
  
 1) Syntactic Frames. Most verb senses which allow syn-
tactic alternations (such as transitive/inchoative, unspeci-
fied object deletion, etc) are analyzed as one sense 
group. However, in some cases, as illustrated by the verb 
leave, intransitive and transitive uses are distinguished as 
different sense groups: 
 
Group 1: DEPART (Ship leaves at midnight) 
Group 2: LEAVE BEHIND (She left a mess.) 
 
The DEPART sense of the verb can be used transitively if 
the object specifies the place of departure. The LEAVE 
BEHIND sense is more general and allows syntactic varia-
tion as well as different semantic types of NPs. In Prop-
Bank, these groups are unified as one frameset (Frameset 
1 MOVE AWAY FROM). 
                                                          
1 All these verbs have one or more additional framesets, which 
correspond to one group or sense, and therefore are not in-
cluded here 
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 1:   
WN1 (function) 
WN3(contribute to) 
WN12 (answer) 
 
His freedom served him well 
The scandal served to increase his popularity 
Nothing else will serve 
GROUP 2:   
WN2 (do duty) 
WN13 (do military service) 
 
She served in Congress 
She served in Vietnam 
GROUP 5:    
WN7 (devote one?s efforts) 
WN10 (attend to) 
 
She served the art of music 
May I serve you? 
serve 01:  Act, work 
 
Roles: 
Arg0:worker 
Arg1:job,  project 
Arg2:employer 
 
GROUP 3:   
WN4 (be used by) 
WN8 (serve well)                
WN14 (service) 
 
 
The garage served to shelter horses 
Art serves commerce 
Male animals serve the females for breeding 
purposes 
 
Table 1. Frameset  serve 01.
            
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 2:   
WN2 (leave behind)  
 WN12 (be survived by)  
 WN14  (forget) 
 
She left a mess 
He left six children I left my keys 
GROUP 1:   
WN1  (go away) 
 
WN5 (exit, go out) 
WN8 (depart) 
 
The ship leaves at midnight 
Leave the room 
The teenager left home 
GROUP 3:   
WN3 (to act)   
WN7 (result in) 
 
The inflation left them penniless 
Her blood left a stain on the napkin 
SINGLETON  
WN4 (leave behind) 
 
Leave it as is 
leave 02: Move away 
from  
 
Roles:  
Arg0:entity leaving 
Arg1:thing left 
Arg2 :attribute / sec-
ondary predication 
 
SINGLETON  
WN6 (allow for, provide) 
 
Leave lots of time for the trip 
 
Table 2. Frameset leave 02. 
 
 
 
2. Optional Arguments.  In PropBank verbs of manner 
of motion and verbs of directed motion are usually 
grouped into one frameset. For example, one of the 
framesets of the verb pull (TRY) TO CAUSE MOTION 
unifies the following two group senses: 
Group 1: MOVE ALONG (pull a sled) 
Group 2: MOVE INTO A CERTAIN DIRECTION (The van 
pulled up) 
    
Although the frame for the frameset 1 of the verb pull 
has a ?direction? argument, this argument does not 
have to be present (or implied), and verbs with this 
frame can also be understood as verbs of manner of 
motion in PropBank. 
 
3) Syntactic variation of arguments. Syntactic varia-
tion in objects can also be used to distinguish sense 
groups, but are not taken into consideration for distin-
guishing framesets.  Here both noun phrases and sen-
 
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 1:  
WN1 (draw)  
WN4(apply force)  
WN9 (cause to move) 
WN10 (operate) 
WN13 (hit) 
 
Pull a sled 
Pull the rope 
A declining dollar pulled  down the export figures 
Pull the oars 
Pull the ball 
GROUP 2:   
WN2 (attract) 
WN12 (rip) 
 
The ad pulled in many potential customers 
 Pull the cooked chicken into strips 
GROUP 3:   
WN3 (move) 
WN7 (steer) 
 
The car pulls to the right 
 Pull the car over 
pull.01:  try to cause motion 
 
Roles:  
Arg0:puller 
Arg1:thing pulled 
Arg2: direction or predication 
Arg3:extent, distance moved 
  
 
GROUP 4:   
WN6 (pull out) 
WN15 (extract) 
WN17(take away) 
 
The mugger pulled a knife on his victim 
 Pull weeds 
 Pull the old soup cans from the shelf 
 
Table 3. Frameset pull 01. 
 
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 1:  
WN1 (perceive by sight)  
WN7 (watch)  
WN19 (observe as if with an eye) 
 WN20 (examine)       
 
Can you the bird? 
See a movie 
The camera saw the burglary 
I must see your passport 
GROUP 3:   
WN3 (witness) 
 WN6 (learn) 
 
I want to see the results 
I see that you have been promoted 
GROUP 4:   
WN5 (consider) 
WN24 (interpret) 
 
I don?t see the situation quite as negatively 
What message do you see in this letter? 
GROUP 5:   
WN8 (determine) 
WN10 (check) 
WN14 (attend) 
 
See whether it works 
See that the curtains are closed 
Could you see about lunch? 
see.01: view  
 
Roles:  
Arg0:viewer 
Arg1:thing viewed 
Arg2:secondary attribute 
 
GROUP 6:   
WN11 (see a professional) 
WN15 (receive as a guest) 
 
You should see a lawyer 
The doctor will see you now 
 
Table 4. Frameset see 01. 
 
tential complements are contained in the same frame-
set.  These could also be distinguished by the type of 
event, a physical perception vs. an abstract or mental 
perception, but these would also not distinguished by 
PropBank. 
   
Group 1: PERCEIVE BY SIGHT (Can you see the bird?) 
Group 5: DETERMINE, CHECK (See whether it works) 
 
4) Semantic classes of arguments. Differences in se-
mantic classes of arguments, such as ANIMACY versus 
INANIMACY, are also not considered for distinguishing 
framesets. The verb serve, for example, has the follow-
ing group senses, the second of which requires an 
ANIMATE agent, which are unified as one frameset in 
PropBank: 
 
Group 1: FUNCTION (His freedom served him well) 
Group 2: WORK (He served in Congress)   
 
Most of the criteria which are used to split Framesets 
into groupings, as the tables above illustrate, are se-
mantic. These distinctions, although more fine-grained 
than Framesets, are still more easily distinguished than 
WordNet senses. 
 
Mismatches between Framesets and groupings usually 
occur for the following two reasons. First, some senses 
can be missing in the PropBank, if they do not occur in 
the corpus.  Second, given that PropBank is an annota-
tion of the Wall Street Journal, it often distinguishes 
obscure financial senses of the verb as separate senses.  
4 Experiments with Automatic WSD  
We have also been investigating the suitability of these 
distinctions for training automatic Word Sense 
Disambiguation systems.  The system that we used to 
tag verbs with their frameset is the same maximum 
entropy system as that of Dang and Palmer (2002), 
including both topical and local features. Topical 
features looked for the presence of keywords occurring 
anywhere in the sentence and any surrounding 
sentences provided as context (usually one or two 
sentences).  The set of keywords is specific to each 
lemma to be disambiguated, and is determined 
automatically from training data so as to minimize the 
entropy of the probability of the senses conditioned on 
the keyword.  
The local features for a verb w in a particular sentence 
tend to look only within the smallest clause containing 
w.  They include collocational features requiring no 
linguistic prepro essing beyond part-of-speech tagging 
(1), syntactic features that capture relations 
between the verb and its complements (2-4), and se-
mantic features that incorporate information about 
noun classes for objects (5-6): 
1) the word w, the part of speech of w, and 
words at positions -2, -1, +1, +2, relative to w 
2) whether or not the sentence is passive 
3) whether there is a subject, direct object, indi-
rect object, or clausal complement (a comple-
ment whose node label is S in the parse tree) 
4) the words (if any) in the positions of subject, 
direct object, indirect object, particle, preposi-
tional complement (and its object) 
5) a Named Entity tag (PERSON, 
ORGANIZATION, LOCATION) for proper 
nouns appearing in (4). 
6) all possible WordNet synsets and hypernyms 
for the nouns appearing in (4). 
The system performed well on the English verbs in 
Senseval-2, achieving an accuracy of 60.2% when tag-
ging verbs with their fine-grained WordNet senses, and 
70.2% when tagging with the more coarse-grained 
sense groups. 
 
 
Verb Framesets Instances Accuracy
call 11 522 0.835 
carry 4 195 0.933 
develop 2 240 0.938 
draw 3 94 0.926 
dress 3 15 0.800 
drive 2 99 0.808 
keep 5 136 0.919 
leave 3 147 0.762 
live 4 125 0.888 
play 5 98 0.806 
pull 6 88 0.784 
see 2 187 0.995 
serve 2 150 0.967 
strike 10 59 0.610 
train 2 17 0.941 
treat 2 51 0.863 
turn 14 141 0.638 
use 2 820 0.988 
wash 2 8 0.875 
work 7 398 0.955 
Table 5.  Frameset tagging results 
 
For frameset tagging, we collected a total of 3590 in-
stances of 20 verbs in the PropBank corpus that had 
been annotated with their framesets.  The verbs all had 
more than one possible frameset and were a subset of 
the ones used for the English lexical sample task of 
Senseval-2.  Local features for frameset taging were 
extracted using the gold-standard part-of-speech tags 
and bracketing of the Penn Treebank.  Table 5 shows 
the number of framesets, the number of instances, and 
the system accuracy for each verb using 10-fold cross-
validation. The overall accuracy of our automatic 
frameset tagging was 90.0%, compared to a baseline 
accuracy of 73.5% if verbs are tagged with their most 
frequent frameset. While the data is only a subset of 
that used in Senseval-2, it is clear that framesets can be 
much more reliably tagged than fine-grained WordNet 
senses and even sense groups. 
Conclusion 
This paper described an hierarchical approach to 
WordNet sense distinctions that provided different 
types of automatic Word Sense Disambiguation (WSD) 
systems, which perform at varying levels of accuracy. 
We have described three different levels of sense 
granularity, with PropBank Framesets being the most 
syntactic, the most coarse-grained, and most easily 
reproduced.  A set of manual groupings devised for 
Senseval2 provides a middle level of granularity that 
mediates between Framesets and WordNet.   For tasks 
where fine-grained sense distinctions may not be essen-
tial such as an AskJeeves information retrieval task, an 
accurate coarse-grained WSD system such as our 
Frameset tagger may be sufficient. There is evidence, 
however, that machine translation of languages as di-
verse as Chinese and English might require all of the 
fine-grained sense distinctions of WordNet, and even 
more (Ng, et al2003, Palmer, et. al., to appear).   
References 
 
Apresjan, J. D. .(1974) Regular polysemy, Linguistics, 
142:5?32. 
 
Atkins, S. (1993) Tools for computer-aided corpus 
lexicography: The Hector Project.  Actu Linguis-
tica Hunguricu, 41:5-72. 
 
Buitelaar, P.P (2000). Reducing Lexical Semantic 
Complexity with Systematic Polysemous Classes 
and Underspecification. In Poceedings of the 
ANLP Workshop on Syntactic and Semantic Com-
plexity in NLP Systems. Seattle, WA. 
 
Cruse, D. A., (1986), Lexical Semantics, Cambridge 
University Press, Cambridge, UK, 1986. 
 
Dang, H. T. and Palmer, M., (2002). Combining Con-
textual Features for Word Sense Disambiguation. 
In Proceedings of the Workshop on Word Sense 
Disambiguation: Recent Successes and Future Di-
rections, Philadelphia, Pa. 
 
Edmonds, P. and Cotton, S. (2001). SENSEVAL-2: 
Overview. In Proceedings of SENSEVAL-2: Sec-
ond International Workshop on Evaluating Word 
Sense Disambiguation Systems, ACL-SIGLEX, 
Toulouse, France. 
 
Hanks, P., (2000), Do word meanings exist? Com-
puters and the Humanities, Special Issue on 
SENSEVAL, 34(1-2). 
 
Geeraerts, D., (1993), Vagueness's puzzles, polysemy's 
vagaries, Cognitive Linguistics, 4. 
 
Kilgarriff, A., (1997), I don't believe in word senses, 
Computers and the Humanities, 31(2). 
 
Kilgarriff, A. and Palmer, M., (2000), Introduction to 
the special issue on Senseval, Computers and the 
Humanities, 34(1-2):1-13. 
 
Kingsbury, P., and Palmer, M, (2002), From TreeBank 
to PropBank, Third International Conference on 
Language  Resources and Evaluation, LREC-02, 
Las Palmas, Canary Islands, Spain, May 28- June 
3. 
 
Marcus, M, (1994), The Penn TreeBank: A revised 
corpus design for extracting predicate argument 
structure, In Proceedings of the ARPA Human 
Language Technology Workshop, Princeton, NJ.   
 
Ng, H. T., & Wang, B., & Chan, Y. S. (2003). 
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In the Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics (ACL-03). Sapporo, 
Japan, July. 
Palmer, M., Dang, H. T., and Fellbaum, C., (to appear, 
2004), Making fine-grained and coarse-grained 
sense distinctions, both manually and automati-
cally, under revision for Natural Language Engi-
neering. 
Pustejovsky, J. (1991) The Generative Lexicon,  in 
Computational Linguistics 17(4).   
Pustejovsky, J. (1995) The Generative Lexicon, Cam-
bridge, MIT Press, Mass. 
 
Integrated Annotation for Biomedical Information Extraction
Seth Kulick and Ann Bies and Mark Liberman and Mark Mandel
and Ryan McDonald and Martha Palmer and Andrew Schein and Lyle Ungar
University of Pennsylvania
Philadelphia, PA 19104
 
skulick,bies,myl  @linc.cis.upenn.edu,
mamandel@unagi.cis.upenn.edu,
 
ryantm,mpalmer,ais,ungar  @cis.upenn.edu
Scott Winters and Pete White
Division of Oncology,
Children?s Hospital of Philadelphia
Philadelphia, Pa 19104
 
winters,white  @genome.chop.edu
Abstract
We describe an approach to two areas of
biomedical information extraction, drug devel-
opment and cancer genomics. We have devel-
oped a framework which includes corpus anno-
tation integrated at multiple levels: a Treebank
containing syntactic structure, a Propbank con-
taining predicate-argument structure, and an-
notation of entities and relations among the en-
tities. Crucial to this approach is the proper
characterization of entities as relation compo-
nents, which allows the integration of the entity
annotation with the syntactic structure while
retaining the capacity to annotate and extract
more complex events. We are training statis-
tical taggers using this annotation for such ex-
traction as well as using them for improving the
annotation process.
1 Introduction
Work over the last few years in literature data mining
for biology has progressed from linguistically unsophisti-
cated models to the adaptation of Natural Language Pro-
cessing (NLP) techniques that use full parsers (Park et
al., 2001; Yakushiji et al, 2001) and coreference to ex-
tract relations that span multiple sentences (Pustejovsky
et al, 2002; Hahn et al, 2002) (For an overview, see
(Hirschman et al, 2002)). In this work we describe an ap-
proach to two areas of biomedical information extraction,
drug development and cancer genomics, that is based on
developing a corpus that integrates different levels of se-
mantic and syntactic annotation. This corpus will be a
resource for training machine learning algorithms useful
for information extraction and retrieval and other data-
mining applications. We are currently annotating only
abstracts, although in the future we plan to expand this to
full-text articles. We also plan to make publicly available
the corpus and associated statistical taggers.
We are collaborating with researchers in the Division
of Oncology at The Children?s Hospital of Philadelphia,
with the goal of automatically mining the corpus of can-
cer literature for those associations that link specified
variations in individual genes with known malignancies.
In particular we are interested in extracting three entities
(Gene, Variation Event, and Malignancy) in the follow-
ing relationship: Gene X with genomic Variation Event
Y is correlated with Malignancy Z. For example, WT1 is
deleted in Wilms Tumor #5. Such statements found in the
literature represent individual gene-variation-malignancy
observables. A collection of such observables serves
two important functions. First, it summarizes known
relationships between genes, variation events, and ma-
lignancies in the cancer literature. As such, it can be
used to augment information available from curated pub-
lic databases, as well as serve as an independent test for
accuracy and completeness of such repositories. Second,
it allows inferences to be made about gene, variation, and
malignancy associations that may not be explicitly stated
in the literature, both at the fact and entity instance lev-
els. Such inferences provide testable hypotheses and thus
future research targets.
The other major area of focus, in collaboration with
researchers in the Knowledge Integration and Discov-
ery Systems group at GlaxoSmithKline (GSK), is the ex-
traction of information about enzymes, focusing initially
on compounds that affect the activity of the cytochrome
P450 (CYP) family of proteins. For example, the goal is
to see a phrase like
Amiodarone weakly inhibited CYP2C9,
CYP2D6, and CYP3A4-mediated activities
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 61-68.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
with Ki values of 45.1?271.6 
and extract the facts
amiodarone inhibits CYP2C9 with
Ki=45.1-271.6
amiodarone inhibits CYP2D6 with
Ki=45.1-271.6
amiodarone inhibits CYP3A4 with
Ki=45.1-271.6
Previous work at GSK has used search algorithms that
are based on pattern matching rules filling template slots.
The rules rely on identifying the relevant passages by first
identifying compound names and then associating them
with a limited number of relational terms such as inhibit
or inactivate. This is similar to other work in biomedical
extraction projects (Hirschman et al, 2002).
Creating good pattern-action rules for an IE problem is
far from simple. There are many complexities in the dif-
ferent ways that a relation can be expressed in language,
such as syntactic alternations and the heavy use of co-
ordination. While sufficiently complex patterns can deal
with these issues, it requires a good amount of time and
effort to build such hand-crafted rules, particularly since
such rules are developed for each specific problem. A
corpus that is annotated with sufficient syntactic and se-
mantic structure offers the promise of training taggers for
quicker and easier information extraction.
The corpus that we are developing for the two differ-
ent application demands consists of three levels of anno-
tation: the entities and relations among the entities for the
oncology or CYP domain, syntactic structure (Treebank),
and predicate-argument structure (Propbank). This is a
novel approach from the point-of-view of NLP since pre-
vious efforts at Treebanking and Propbanking have been
independent of the special status of any entities, and pre-
vious efforts at entity annotation have been independent
of corresponding layers of syntactic and semantic struc-
ture. The decomposition of larger entities into compo-
nents of a relation, worthwhile by itself on conceptual
grounds for entity definition, also allows the component
entities to be mapped to the syntactic structure. These
entities can be viewed as semantic types associated with
syntactic constituents, and so our expectation is that au-
tomated analyses of these related levels will interact in a
mutually reinforcing and beneficial way for development
of statistical taggers. Development of such statistical tag-
gers is proceeding in parallel with the annotation effort,
and these taggers help in the annotation process, as well
as being steps towards automatic extraction.
In this paper we focus on the aspects of this project
that have been developed and are in production, while
also trying to give enough of the overall vision to place
the work that has been done in context. Section 2 dis-
cusses some of the main issues around the development
of the guidelines for entity annotation, for both the on-
cology and inhibition domains. Section 3 first discusses
the overall plan for the different levels of annotation, and
then focuses on the integration of the two levels currently
in production, entity annotation and syntactic structure.
Section 4 describes the flow of the annotation process,
including the development of the statistical taggers men-
tioned above. Section 5 is the conclusion.
2 Guidelines for Entity Annotation
Annotation has been proceeding for both the oncology
and the inhibition domains. Here we give a summary of
the main features of the annotation guidelines that have
been developed. We have been influenced in this by pre-
vious work in annotation for biomedical information ex-
traction (Ohta et al, 2002; Gaizauskas et al, 2003). How-
ever, we differ in the domains we are annotating and the
design philosophy for the entity guidelines. For exam-
ple, we have been concentrating on explicit concepts for
entities like genes rather than developing a wide-range
ontology for the various physical instantiations.
2.1 Oncology Domain
Gene Entity For the sake of this project the defini-
tion for ?Gene Entity? has two significant characteristics.
First, ?Gene? refers to a composite entity as opposed to
the strict biological definition. As has been noted by oth-
ers, there are often ambiguities in the usage of the en-
tity names. For example, it is sometimes unclear as to
whether it is the gene or protein being referenced, or the
same name might refer to the gene or the protein at dif-
ferent locations in the same document. Our approach to
this problem is influenced by the named entity annota-
tion in the Automatic Content Extraction (ACE) project
(Consortium, 2002), in which ?geopolitical? entities can
have different roles, such as ?location? or ?organization?.
Analogously, we consider a ?gene? to be a composite en-
tity that can have different roles throughout a document.
Standardization of ?Gene? references between different
texts and between gene synonyms is handled by exter-
nally referencing each instance to a standard ontology
(Ashburner et al, 2000).
In the context of this project, ?Gene? refers to a con-
ceptual entity as opposed to the specific manifestation of
a gene (i.e. an allele or nucleotide sequence). Therefore,
we consider genes to be abstract concepts identifying ge-
nomic regions often associated with a function, such as
MYC or TrkB; we do not consider actual instances of
such genes within the gene-entity domain. Since we are
interested in the association between Gene-entities and
malignancies, for this project genes are of interest to us
when they have an associated variation event. Therefore,
the combination of Gene entities and Variation events
provides us with an evoked entity representing the spe-
cific instance of a gene.
Variation Events as Relations Variations comprise a
relationship between the following entities: Type (e.g.
point mutation, translocation, or inversion), Location
(e.g. codon 14, 1p36.1, or base pair 278), Original-State
(e.g. Alanine), and Altered-State (e.g. Thymine). These
four components represent the key elements necessary
to describe any genomic variation event. Variations are
often underspecified in the literature, frequently having
only two or three of these specifications. Characterizing
individual variations as a relation among such compo-
nents provides us with a great deal of flexibility: 1) it al-
lows us to capture the complete variation event even when
specific components are broadly spaced in the text, often
spanning multiple sentences or even paragraphs; 2) it pro-
vides us with a convenient means of tracking anaphora
between detailed descriptions (e.g. a point mutation at
codon 14 and summary references (e.g. this variation);
and 3) it provides a single structure capable of capturing
the breadth of variation specifications (e.g. A-  T point
mutation at base pair 47, A48-  G or t(11;14)(q13;32)).
Malignancy The guidelines for malignancy annotation
are under development. We are planning to define it in a
manner analogous to variation, whereby a Malignancy is
composed of various attribute types (such as developmen-
tal stage, behavior, topographic site, and morphology).
2.2 CYP Domain
In the CYP Inhibition annotation task we are tagging
three types of entities:
1. CYP450 enzymes (cyp)
2. other substances (subst)
3. quantitative measurements (quant)
Each category has its own questions and uncertain-
ties. Names like CYP2C19 and cytochrome P450 en-
zymes proclaim their membership, but there are many
aliases and synonyms that do not proclaim themselves,
such as 17,20-lyase. We are compiling a list of such
names.
Other substances is a potentially huge and vaguely-
delimited set, which in the current corpus includes grape-
fruit juice and red wine as well as more obviously bio-
chemical entities like polyunsaturated fatty acids and ery-
thromycin. The quantitative measurements we are di-
rectly interested in are those directly related to inhibition,
such as IC50 and K(i). We tag the name of the measure-
ment, the numerical value, and the unit. For example, in
the phrase ...was inhibited by troleandomycin (ED50 = 1
microM), ED50 is the name, 1 the value, and microM the
unit. We are also tagging other measurements, since it
is easy to do and may provide valuable information for
future IE work.
3 Integrated Annotation
As has been noted in the literature on biomedical IE (e.g.,
(Pustejovsky et al, 2002; Yakushiji et al, 2001)), the
same relation can take a number of syntactic forms. For
example, the family of words based on inhibit occurs
commonly in MEDLINE abstracts about CYP enzymes
(as in the example in the introduction) in patterns like A
inhibited B, A inhibited the catalytic activity of B, inhibi-
tion of B by A, etc.
Such alternations have led to the use of pattern-
matching rules (often hand-written) to match all the rele-
vant configurations and fill in template slots based on the
resulting pattern matches. As discussed in the introduc-
tion, dealing with such complications in patterns can take
much time and effort.
Our approach instead is to build an annotated corpus
in which the predicate-argument information is annotated
on top of the parsing annotations in the Treebank, the re-
sulting corpus being called a ?proposition bank? or Prop-
bank. This newly annotated corpus is then used for train-
ing processors that will automatically extract such struc-
tures from new examples.
In a Propbank for biomedical text, the types of in-
hibit examples listed above would consistently have their
compounds labeled as Arg0 and their enzymes labeled as
Arg1, for nominalized forms such as A is an inhibitor of
B, A caused inhibition of B, inhibition of B by A, as well
the standard A inhibits B. We would also be able to la-
bel adjuncts consistently, such as the with prepositional
phrase in CYP3A4 activity was decreased by L, S and F
with IC(50) values of about 200 mM. In accordance with
other Calibratable verbs such as rise, fall, decline, etc.,
this phrase would be labeled as an Arg2-EXTENT, re-
gardless of its syntactic role.
A Propbank has been built on top of the Penn Tree-
bank, and has been used to train ?semantic taggers?, for
extracting argument roles for the predicates of interest,
regardless of the particular syntactic context.1
Such semantic taggers have been developed by using
machine learning techniques trained on the Penn Prop-
bank (Surdeanu et al, 2003; Gildea and Palmer, 2002;
Kingsbury and Palmer, 2002). However, the Penn Tree-
bank and Propbank involve the annotation of Wall Street
Journal text. This text, being a financial domain, differs
in significant ways from the biomedical text, and so it is
1The Penn Propbank is complemented by NYU?s Nom-
bank project (Meyers, October 2003), which includes tagging
of nominal predicate structure. This is particular relevant for
the biomedical domain, given the heavy use of nominals such
mutation and inhibition.
necessary for this approach to have a corpus of biomed-
ical texts such as MEDLINE articles annotated for both
syntactic structure (Treebanking) and shallow semantic
structure (Propbanking).
In this project, the syntactic and semantic annotation is
being done on a corpus which is also being annotated for
entities, as described in Section 2. Since semantic tag-
gers of the sort described above result in semantic roles
assigned to syntactic tree constituents, it is desirable to
have the entities correspond to syntactic constituents so
that the semantic roles are assigned to entities. The en-
tity information can function as type information and be
taken advantage of by learning algorithms to help charac-
terize the properties of the terms filling specified roles in
a given predicate.
This integration of these three different annotation lev-
els, including the entities, is being done for the first time2,
and we discuss here three main challenges to this corre-
spondence between entities and constituents: (1) entities
that are large enough to cut across multiple constituents,
(2) entities within prenominal modifiers, and (3) coordi-
nation.3
Relations and Large Entities One major area of con-
cern is the possibility of entities that contain more than
one syntactic constituent and do not match any node in
the syntax tree. For example, as discussed in Section 2, a
variation event includes material on a variation?s type, lo-
cation, and state, and can cut not only across constituents,
but even sentences and paragraphs. A simple example is
point mutations at codon 12, containing both the nominal
(the type of mutation) and following NP (the location).
Note that while in isolation this could also be considered
one syntactic constituent, the NP and PP together, the ac-
tual context is ...point mutations at codon 12 in duode-
nal lavage fluid.... Since all PPs are attached at the same
level, at codon 12 and in duodenal lavage fluid are sis-
ters, and so there is no constituent consisting of just point
mutations at codon 12.
Casting the variation event as a relation between dif-
ferent component entities allows the component entities
to correspond to tree constituents, while retaining the ca-
pacity to annotate and search for more complex events.
In this case, one component entity point mutations cor-
2An influential precursor to this integration is the system de-
scribed in (Miller et al, 1996). Our work is in much the same
spirit, although the representation of the predicate-argument
structure via Propbank and the linkage to the entities is quite
different, as well as of course the domain of annotation.
3There are cases where the entities are so minimal that they
are contained within a NP, not including the determiner, such as
CpG site in the NP a CpG site. entities. We are not as concerned
about these cases since we expect that such entity information
properly contained within a base NP can be associated with the
full base NP.
responds to a (base) NP node, and at codon 12 is corre-
sponds to the PP node that is the NP?s sister. At the same
time, the relation annotation contains the information re-
lating these two constituents.
Similarly, while the malignancy entity definition is cur-
rently under development, as mentioned in Section 2.1, a
guiding principle is that it will also be treated as a relation
and broken down into component entities. While this also
has conceptual benefits for the annotation guidelines, it
has the fortunate effect of making such otherwise syntax-
unfriendly malignancies as colorectal adenomas contain-
ing early cancer and acute myelomonocytic leukemia in
remission amenable for mapping the component parts to
syntactic nodes.
Entities within Prenominal Modifiers While we are
for the most part following the Penn Treebank guide-
lines (Bies et al, 1995), we are modifying them in two
important aspects. One concerns the prenominal mod-
ifiers, which in the Penn Treebank were left flat, with
no structure, but in this biomedical domain contain much
of the information - e.g., cancer-associated autoimmune
antigen. Not only would this have had no annotation
for structure, but even more bizarrely, cancer-associated
would have been a single token in the Penn Treebank,
thus making it impossible to capture the information as
to what is associated with what. We have developed new
guidelines to assign structure to prenominal entities such
as breast cancer, as well as changed the tokenization
guidelines to break up tokens such as cancer-associated.
Coordination We have also modified the treebank an-
notation to account for the well-known problem of enti-
ties that are discontinuous within a coordination structure
- e.g., K- and H-ras, where the entities are K-ras and H-
ras. Our annotation tool allows for discontinuous entities,
so that both K-ras and H-ras are annotated as genes.
Under standard Penn Treebank guidelines for tokeniza-
tion and syntactic structure, this would receive the flat
structure
NP
K- and H-ras
in which there is no way to directly associate the entity
K-ras with a constituent node.
We have modified the treebank guidelines so that K-ras
and H-ras are both constituents, with the ras part of K-ras
represented with an empty category co-indexed with ras
in H-ras:4.
4This is related to the approach to coordination in the GE-
NIA project.
NP
NP
K - NX-1
*P*
and NP
H - NX-1
ras
4 Annotation Process
We are currently annotating MEDLINE abstracts for both
the oncology and CYP domains. The flowchart for the
annotation process is shown in Figure 1. Tokenization,
POS-tagging, entity annotation (both domains), and tree-
banking are in full production. Propbank annotation and
the merging of the entities and treebanking remain to be
integrated into the current workflow. The table in Fig-
ure 2 shows the number of abstracts completed for each
annotation area.
The annotation sequence begins with tokenization and
part-of-speech annotating. While both aspects are simi-
lar to those used for the Penn Treebank, there are some
differences, partly alluded to in Section 3. Tokens are
somewhat more fine-grained than in the Penn Treebank,
so that H-ras, e.g., would consist of three tokens: H, -,
and ras.
Tokenized and part-of-speech annotated files are then
sent to the entity annotators, either for oncology or CYP,
depending on which domain the abstract has been chosen
for. The entities described in Section 2 are annotated at
this step. We are using WordFreak, a Java-based linguis-
tic annotation tool5, for annotation of tokenization, POS,
and entities. Figure 3 is a screen shot of the oncology do-
main annotation, here showing a variation relation being
created out of component entities for type and location.
In parallel with the entity annotation, a file is tree-
banked - i.e., annotated for its syntactic structure. Note
that this is done independently of the entity annotation.
This is because the treebanking guidelines are relatively
stable (once they were adjusted for the biomedical do-
main as described in Section 3), while the entity defini-
tions can require a significant period of study before sta-
bilizing, and with the parallel treatment the treebanking
can proceed without waiting for the entity annotation.
However, this does mean that to produce the desired
integrated annotation, the entity and treebanking annota-
tions need to be merged into one representation. The con-
sideration of the issues described in Section 3 has been
carried out for the purpose of allowing this integration
of the treebanking and entity annotation. This has been
completed for some pilot documents, but the full merging
remains to be integrated into the workflow system.
5http://www.sf.net/projects/wordfreak
As mentioned in the introduction, statistical taggers
are being developed in parallel with the annotation effort.
While such taggers are part of the final goal of the project,
providing the building blocks for extracting entities and
relations, they are also useful in the annotation process
itself, so that the annotators only need to perform correc-
tion of automatically tagged data, instead of starting from
scratch.
Until recently (Feb. 10), the part-of-speech annotation
was done by hand-correcting the results of tagging the
data with a part-of-speech tagger trained on a modified
form of the Penn Treebank.6 The tagger is a maximum-
entropy model utilizing the opennlp package available
at http://www.sf.net/projects/opennlp . It
has now been retrained using 315 files (122 from the
oncology domain, 193 from the cyp domain). Figure 4
shows the improvement of the new vs. the old POS tag-
ger on the same 294 files that have been hand-corrected.
These results are based on testing files that have already
been tokenized, and thus are an evaluation only of the
POS tagger and not the tokenizer. While not directly
comparable to results such as (Tateisi and Tsujii, 2004),
due to the different tag sets and tokenization, they are in
the same general range.7
The oncology and cyp entity annotation, as well as the
treebanking are still being done fully manually, although
that will change in the near future. Initial results for a tag-
ger to identify the various components of a variation re-
lation are promising, although not yet integrated into an-
notation process. The tagger is based on the implementa-
tion of Conditional Random Fields (Lafferty et al, 2001)
in the Mallet toolkit (McCallum, 2002). Briefly, Condi-
tional Random Fields are log-linear models that rely on
weighted features to make predictions on the input. Fea-
tures used by our system include standard pattern match-
ing and word features as well as some expert-created reg-
ular expression features8. Using 10-fold cross-validation
on 264 labelled abstracts containing 551 types, 1064 lo-
6Roughly, Penn Treebank tokens were split at hyphens, with
the individual components then sent through a Penn Treebank-
trained POS tagger, to create training data for another POS tag-
ger. For example (JJ York-based) is treated as (NNP
York) (HYPH -) (JJ based). While this works rea-
sonably well for tokenization, the POS tagger suffered severely
from being trained on a corpus with such different properties.
7The tokenizer has also been retrained and the new tokenizer
is being used for annotation, although although we do not have
the evaluation results here.
8e.g., chr|chromosome [1-9]|1[0-9]|2[0-
2]|X|Y p|q
Merged Entity/
Treebank Annotation
Tokenization
Entity Annotation
POS Annotation
Treebank/Propbank
Annotation
Figure 1: Annotation Flow
Annotation Task Start Date Annotated Documents
Part-of-Speech Tagging 8/22/03 422
Entity Tagging 9/12/03 414
Treebanking 1/8/04 127
Figure 2: Current Annotation Production Results
Figure 3: Relation Annotation in WordFreak
Tagger Training Material Token Instances
Old Sections 00-15 Penn Treebank 773832
New 315 abstracts 103159
Tagger Overall Accuracy Number Token Instances Accuracy on Accuracy on
Unseen in Training Data Unseen Seen
Old 88.53% 14542 58.80% 95.53%
New 97.33% 4096 85.05% 98.02%
(Testing Material: 294 abstracts from the oncology domain, with 76324 token instances.)
Figure 4: Evaluation of Part-of-Speech Taggers
cations and 557 states, we obtained the following results:
Entity Precision Recall F-measure
Type 0.80 0.72 0.76
Location 0.85 0.73 0.79
State 0.90 0.80 0.85
Overall 0.86 0.75 0.80
An entity is considered correctly identified if and only
if it matches the human labeling by both category (type,
location or state) and span (from position a to position b).
At this stage we have not distinguished between initial
and final states.
While it is difficult to compare taggers that tag
different types of entities (e.g., (Friedman et al, 2001;
Gaizauskas et al, 2003)), CRFs have been utilized for
state-of-the-art results in NP-chunking and gene and
protein tagging (Sha and Pereira, 2003; McDonald
and Pereira, 2004) Currently, we are beginning to
investigate methods to identify relations over the varia-
tion components that are extracted using the entity tagger.
5 Conclusion
We have described here an integrated annotation ap-
proach for two areas of biomedical information extrac-
tion. We discussed several issues that have arisen for this
integration of annotation layers. Much effort has been
spent on the entity definitions and how they relate to the
higher-level concepts which are desired for extraction.
There are promising initial results for training taggers to
extract these entities.
Next steps in the project include: (1) continued anno-
tation of the layers we are currently doing, (2) integra-
tion of the level of predicate-argument annotation, and
(3) further development of the statistical taggers, includ-
ing taggers for identifying relations over their component
entities.
Acknowledgements
The project described in this paper is based at the In-
stitute for Research in Cognitive Science at the Uni-
versity of Pennsylvania and is supported by grant EIA-
0205448 from the National Science Foundation?s Infor-
mation Technology Research (ITR) program.
We would like to thank Aravind Joshi, Jeremy
Lacivita, Paula Matuszek, Tom Morton, and Fernando
Pereira for their comments.
References
M. Ashburner, C.A. Ball, J.A. Blake, D. Botstein, H. But-
ler, J.M. Cherry, A.P. Davis, K. Dolinski, S.S. Dwight,
J.T. Eppig, M.A. Harris, D.P. Hill, L. Issel-Tarver,
A. Kasarskis, S. Lewis, J.C. Matese, J.E. Richardson,
M. Ringwald, G.M. Rubin, and G. Sherlock. 2000.
Gene ontology: Tool for the unification of biology.
Nature Genetics, 25(1):25?29.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
Style, Penn Treebank Project. Tech report MS-CIS-
95-06, University of Pennsylvania, Philadelphia, PA.
Linguistic Data Consortium. 2002. Entity de-
tection and tracking - phase 1 - EDT and
metonymy annotation guidelines version 2.5
20021205. http://www.ldc.upenn.edu/Projects/ACE
/PHASE2/Annotation/.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. Genies: a
natural-language processing system for the extraction
of molecular pathways from journal articles. ISMB
(Supplement of Bioinformatics), pages 74?82.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Bioinformatics applications of information
extraction from journal articles. Journal of Bioinfor-
matics, 19(1):135?143.
Daniel Gildea and Martha Palmer. 2002. The Necessity
of Syntactic Parsing for Predicate Argument Recogni-
tion. In Proc. of ACL-2002.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: The
MEDSYNDIKATE text mining system. In Proceed-
ings of the Pacific Rim Symposium on Biocomputing,
pages 338?349.
Lynette Hirschman, Jong C. Park, Junichi Tsuji, Limsoon
Wong, and Cathy H. Wu. 2002. Accomplishments and
challenges in literature data mining for biology. Bioin-
formatics Review, 18(12):1553?1561.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC2002), Las Palmas, Spain.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2004. Identify-
ing gene and protein mentions in text using conditional
random fields. In A Critical Assessment of Text Min-
ing Methods in Molecular Biology workshop. To be
presented.
Adam Meyers. October, 2003. Nombank. Talk at Auto-
matic Content Extraction (ACE) PI Meeting, Alexan-
dria, VA.
Scott Miller, David Stallard, Robert Bobrow, and Richard
Schwartz. 1996. A fully statistical approach to
natural language interfaces. In Aravind Joshi and
Martha Palmer, editors, Proceedings of the Thirty-
Fourth Annual Meeting of the Association for Compu-
tational Linguistics, pages 55?61, San Francisco. Mor-
gan Kaufmann Publishers.
Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, and Jun?ici
Tsuji. 2002. The GENIA corpus: An annotated corpus
in molecular biology domain. In Proceedings of the
10th International Conference on Intelligent Systems
for Molecular Biology.
J. Park, H. Kim, and J. Kim. 2001. Bidirectional in-
cremental parsing for automatic pathway identification
with combinatory categorial grammar. In Proceedings
of the Pacific Rim Symposium on Biocomputing, pages
396?407.
J. Pustejovsky, J. Castano, and J. Zhang. 2002. Robust
relational parsing over biomedical literature: Extract-
ing inhibit relations. In Proceedings of the Pacific Rim
Symposium on Biocomputing, pages 362?373.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceeds of Human
Language Technology-NAACL 2003.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Yuka Tateisi and Jun-ichi Tsujii. 2004. Part-of-speech
annotation of biology research abstracts. In Proceed-
ings of LREC04. To be presented.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001.
Event extraction from biomedical papers using a full
parser. In Proceedings of the Pacific Rim Symposium
on Biocomputing, pages 408?419.
 
	

	Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 61?67,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Parallel Proposition Bank II for Chinese and English?
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jinying Chen, Benjamin Snyder
Department of Computer and Information Science
University of Pennsylvania
{mpalmer/xueniwen/malayao/Jinying/bsnyder3}@linc.cis.upenn.edu
Abstract
The Proposition Bank (PropBank) project
is aimed at creating a corpus of text an-
notated with information about seman-
tic propositions. The second phase of
the project, PropBank II adds additional
levels of semantic annotation which in-
clude eventuality variables, co-reference,
coarse-grained sense tags, and discourse
connectives. This paper presents the re-
sults of the parallel PropBank II project,
which adds these richer layers of semantic
annotation to the first 100K of the Chinese
Treebank and its English translation. Our
preliminary analysis supports the hypoth-
esis that this additional annotation recon-
ciles many of the surface differences be-
tween the two languages.
1 Introduction
There is a pressing need for a consensus on a task-
oriented level of semantic representation that can en-
able the development of powerful new semantic ana-
lyzers in the same way that the Penn Treebank (Mar-
cus et al, 1993) enabled the development of sta-
tistical syntactic parsers (Collins, 1999; Charniak,
2001). We believe that shallow semantics expressed
as a dependency structure, i.e., predicate-argument
structure, for verbs, participial modifiers, and nom-
inalizations provides a feasible level of annotation
that would be of great benefit. This annotation, cou-
pled with word senses, minimal co-reference links,
?This work is funded by the NSF via Grant EIA02-05448 .
event identifiers, and discourse and temporal rela-
tions, could provide the foundation for a major ad-
vance in our ability to automatically extract salient
relationships from text. This will in turn facilitate
breakthroughs in message understanding, machine
translation, fact retrieval, and information retrieval.
The Proposition Bank project is a major step towards
providing this type of annotation. It takes a prac-
tical approach to semantic representation, adding a
layer of predicate argument information, or seman-
tic roles, to the syntactic structures of the Penn Tree-
bank (Palmer et al, 2005). The Frame Files that
provide guidance to the annotators constitute a rich
English lexicon with explicit ties between syntac-
tic realizations and coarse-grained senses, Frame-
sets. PropBank Framesets are distinguished primar-
ily by syntactic criteria such as differences in sub-
categorization frames, and can be seen as the top-
level of an hierarchy of sense distinctions. Group-
ings of fine-grained WordNet senses, such as those
developed for Senseval2 (Palmer et al, to appear)
provide an intermediate level, where groups are dis-
tinguished by either syntactic or semantic criteria.
WordNet senses constitute the bottom level. The
PropBank Frameset distinctions, which can be made
consistently by humans and systems (over 90% ac-
curacy for both), are surprisingly compatible with
the groupings; 95% of the groups map directly onto
a single PropBank frameset sense (Palmer et al,
2004).
The semantic annotation provided by PropBank
is only a first approximation at capturing the full
richness of semantic representation. Additional an-
notation of nominalizations and other noun pred-
61
icates has already begun at NYU. This paper de-
scribes the results of PropBank II, a project to pro-
vide richer semantic annotation to structures that
have already been propbanked, specifically, eventu-
ality ID.s, coreference, coarse-grained sense tags,
and discourse connectives. Of special interest to the
machine translation community is our finding, pre-
sented in this paper, that PropBank II annotation rec-
onciles many of the surface differences of the two
languages.
2 PropBank I
PropBank (Palmer et al, 2005) is an annotation of
the Wall Street Journal portion of the Penn Treebank
II (Marcus et al, 1994) with ?predicate-argument?
structures, using sense tags for highly polysemous
words and semantic role labels for each argument.
An important goal is to provide consistent seman-
tic role labels across different syntactic realizations
of the same verb, as in the window in [ARG0 John]
broke [ARG1 the window] and [ARG1 The window]
broke. PropBank can provide frequency counts for
(statistical) analysis or generation components in
a machine translation system, but provides only a
shallow semantic analysis in that the annotation is
close to the syntactic structure and each verb is its
own predicate.
In PropBank, semantic roles are defined on a
verb-by-verb basis. An individual verb?s seman-
tic arguments are simply numbered, beginning with
0. Polysemous verbs have several framesets, cor-
responding to a relatively coarse notion of word
senses, with a separate set of numbered roles, a role-
set, defined for each Frameset. For instance, leave
has both a DEPART Frameset ([ARG0 John] left
[ARG1 the room]) and a GIVE Frameset, ([ARG0
I] left [ARG1 my pearls] [ARG2 to my daughter-in-
law] [ARGM-LOC in my will].) While most Frame-
sets have three or four numbered roles, as many
as six can appear, in particular for certain verbs of
motion. Verbs can take any of a set of general,
adjunct-like arguments (ARGMs), such as LOC (lo-
cation), TMP (time), DIS (discourse connectives),
PRP (purpose) or DIR (direction). Negations (NEG)
and modals (MOD) are also marked.
There are several other annotation projects,
FrameNet (Baker et al, 1998), Salsa (Ellsworth et
al., 2004), and the Prague Tectogrammatics (Haji-
cova and Kucerova, 2002), that share similar goals.
Berkeley.s FrameNet project, (Baker et al, 1998;
Fillmore and Atkins, 1998; Johnson et al, 2002)
is committed to producing rich semantic frames on
which the annotation is based, but it is less con-
cerned with annotating complete texts, concentrat-
ing instead on annotating a set of examples for each
predicator (including verbs, nouns and adjectives),
and attempting to describe the network of relations
among the semantic frames. For instance, the buyer
of a buy event and the seller of a sell event would
both be Arg0.s (Agents) in PropBank, while in
FrameNet one is the BUYER and the other is the
SELLER. The Salsa project (Ellsworth et al, 2004)
in Germany is producing a German lexicon based
on the FrameNet semantic frames and annotating a
large German newswire corpus. PropBank style an-
notation is being used for verbs which do not yet
have FrameNet frames defined.
The PropBank annotation philosophy has been
extended to the Penn Chinese Proposition Bank
(Xue and Palmer, 2003). The Chinese PropBank an-
notation is performed on a smaller (250k words) and
yet growing corpus annotated with syntactic struc-
tures (Xue et al, To appear). The same syntac-
tic alternations that form the basis for the English
PropBank annotation also exist in robust quantities
in Chinese, even though it may not be the case that
the same exact verbs (meaning verbs that are close
translations of one another) have the exact same
range of syntactic realization for Chinese and En-
glish. For example, in (1), ?#c/New Year???/
reception? plays the same role in (a) and (b), which
is the event or activity held, even though it occurs in
different syntactic positions. Assigning the same ar-
gument label, Arg1, to both instances, captures this
regularity. It is worth noting that the predicate /?
1/hold? does not have passive morphology in (1a),
despite what its English translation suggests. Like
the English PropBank, the adjunct-like elements re-
ceive more general labels like TMP or LOC, as also
illustrated in (1). The functional tags for Chinese
and English PropBanks are to a large extent similar
and more details can be found in (Xue and Palmer,
2003).
(1) a. [ARG1 #c/New Year ???/reception] [ARGM-
TMP 8 U/today] [ARGM-LOC 3/at M ~
62
/DiaoyutaiIU,/state guest house ?1/hold]
?The New Year reception was held in Diao-yutai
State Guest House today.?
b. [ARG0 /[^/Tang Jiaxuan] [ARGM-TMP 8
U/today] [ARGM-LOC 3/at M~/Diaoyutai I
U,/state guest house] ?1/ hold [arg1 #c/New
Year???/reception]
?Tang Jiaxuan was holding the New Year reception in
Diaoyutai State Guest House today.?
3 A Parallel PropBank II
As discussed above, PropBank II adds richer se-
mantic annotation to the PropBank I predicate ar-
gument structures, notably eventuality variables,
co-references, coarse-grained sense tags (Babko-
Malaya et al, 2004; Babko-Malaya and Palmer,
2005), and discourse connectives (Xue, To appear)
To create our parallel PropBank II, we began with
the first 100K words of the Chinese Treebank which
had already been propbanked, and which we had
had translated into English. The English transla-
tion was first treebanked and then propbanked, and
we are now in the process of adding the PropBank
II annotation to both the English and the Chinese
propbanks. We will discuss our progress on each of
the three individual components of PropBank II in
turn, bringing out translation issues along the way
that have been highlighted by the additional anno-
tation. In general we find that this level of abstrac-
tion facilitates the alignment of the source and tar-
get language descriptions: event ID.s and event
coreferences simplify the mappings between verbal
and nominal events; English coarse-grained sense
tags correspond to unique Chinese lemmas; and dis-
course connectives correspond well.
3.1 Eventuality variables
Positing eventuality1 variables provides a straight-
forward way to represent the semantics of adver-
bial modifiers of events and capture nominal and
pronominal references to events. Given that the ar-
guments and adjuncts for the verbs are already an-
notated in Propbank I, adding eventuality variables
is for the most part straightforward. The example
in (2) illustrates a Propbank I annotation, which is
identified with a unique event id in Propbank II.
1The term ?eventuality? is used here to refer to events and
states.
(2) a. Mr. Bush met him privately in the White House on
Thursday.
b. Propbank I: Rel: met, Arg0: Mr. Bush, Arg1: him,
ArgM-MNR: privately, ArgM-LOC: in the White
House, ArgM-TMP: on Thursday.
c. Propbank II: ?e meeting(e) & Arg0(e,Mr. Bush) &
Arg1(e, him) & MNR (e, privately) & LOC(e, in the
White House) & TMP (e, on Thursday).
Annotation of event variables starts by auto-
matically associating all Propbank I annotations
with potential event ids. Since not all annotations
actually denote eventualities, we manually filter
out selected classes of verbs. We further attempt
to identify all nouns and nominals which describe
eventualities as well as all sentential arguments of
the verbs which refer to events. And, finally, part
of the PropBank II annotation involves tagging of
event coreference for pronouns as well as empty
categories. All these tasks are discussed in more
detail below.
Identifying event modifiers. The actual annota-
tion starts from the presumption that all verbs are
events or states and nouns are not. All the verbs in
the corpus are automatically assigned a unique event
identifier and the manual part of the task becomes (i)
identification of verbs or verb senses that do not de-
note eventualities, (ii) identification of nouns that do
denote events. For example, in (3), begin is an as-
pectual verb that does not introduce an event vari-
able, but rather modifies the verb -take., as is
supported by the fact that it is translated as an ad-
verb ??/initially? in the corresponding Chinese sen-
tence.
(3) ?:/key u?/develop /DE ??/medicine ?/and )
?/biology E?/technology, #/new E?/technology,
#/new ?/material, O ? ?/computer 9/and A
^/application, 1/photo >/electric ?Nz/integration
/etc. ?/industry ?/already ?/initially ?/take 5
/shape.
/Key developments in industries such as medicine,
biotechnology, new materials, computer and its applica-
tions, protoelectric integration, etc. have begun to take
shape.0
Nominalizations as events Although most nouns
do not introduce eventualities, some do and these
nouns are generally nominalizations2 . This is true
2The problem of identifying nouns which denote events is
addressed as part of the sense-tagging tagging. Detailed discus-
sion can be found in (Babko-Malaya and Palmer, 2005).
63
for both English and Chinese, as is illustrated in (4).
Both /u?/develop0and /\/deepening0are
nominalized verbs that denote events. Having a par-
allel propbank annotated with event variables allows
us to see how events are lined up in the two lan-
guages and how their lexical realizations can vary.
The nominalized verbs in Chinese can be translated
into verbs or their nominalizations, as is shown in
the alternative translations of the Chinese original
in (4). What makes this particular example even
more interesting is the fact that the adjective mod-
ifier of the events, /??/continued0, can ac-
tually be realized as an aspectual verb in English.
The semantic representations of the Propbank II an-
notation, however, are preserved: both the aspec-
tual verb /continue0in English and the adjective
/??/continued0in Chinese are modifiers of the
events denoted by /u?/development0and /
\/deepening0.
(4) ? X/with ? I/China ? L/economy /DE ?
?/continuedu?/development ?/and?/to	/outside
m?/open/DE??/continued\/deepen ,
/As China.s economy continues to develop and
its practice of opening to the outside continues to
deepen,0
/With the continued development of China.s economy
and the continued deepening of its practice of opening to
the outside,0
Event Coreference Another aspect of the event
variable annotation involves identifying pronominal
expressions that corefer with events. These pronom-
inal expressions may be overt, as in the Chinese ex-
ample in (5), while others correspond to null pro-
nouns, marked as pro3. in the Treebank annotations,
as in (6):
(5) ?/additionally, ? ?/export ??/commodity (
/structure UY/continue ` z/optimize, c/last
year ? ?/industry ? ? ?/finished product ?
?/export /quota ?/account for I/entire country
? ?/export o /quantity /DE ' ?/proportion
?/reach z??l??:8/85.6 percent, ?/this ?
?/clearly L?/indicate ?I/China ??/industry 
?/product/DE?E/produce Y?/level'/compared
with L /past k/have 
/LE ?/very ?/big J
p/improvement.
/Moreover, the structure of export com-modities
continues to optimize, and last year.s export volume
of manufactured products ac-counts for 85.6 percent of
3The small *pro* and big *PRO* distinction made in the
Chinese Treebank is exploratory in nature. The idea is that it is
easier to erase this distinction if it turns out to be implausible or
infeasible than to add it if it turns out to be important.
the whole countries.export, *pro* clearly indicating
that China.s industrial product manufacturing level has
improved.0
(6) ?
/these ?J/achievement ?/among k/have ?
z n? l/138 ?/item Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 237?240, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Integration of Syntactic Parsing and Semantic Role Labeling
Szu-ting Yi
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104 USA
szuting@linc.cis.upenn.edu
Martha Palmer
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104 USA
mpalmer@linc.cis.upenn.edu
Abstract
This paper describes a system for the
CoNLL-2005 Shared Task on Semantic
Role Labeling. We trained two parsers
with the training corpus in which the se-
mantic argument information is attached
to the constituent labels, we then used the
resulting parse trees as the input of the
pipelined SRL system. We present our re-
sults of combining the output of various
SRL systems using different parsers.
1 Introduction
Semantic parsing, identifying and classifying the se-
mantic entities in context and the relations between
them, potentially has great impact on its downstream
applications, such as text summarization, question
answering, and machine translation. As a result, se-
mantic parsing could be an important intermediate
step for natural language comprehension. In this pa-
per, we investigate the task of Semantic Role Label-
ing (SRL): Given a verb in a sentence, the goal is to
locate the constituents which are arguments of the
verb, and assign them appropriate semantic roles,
such as, Agent, Patient, and Theme.
Previous SRL systems have explored the effects
of using different lexical features, and experimented
on different machine learning algorithms. (Gildea
and Palmer, 2002; Pradhan et al, 2005; Punyakanok
et al, 2004) However, these SRL systems generally
extract features from sentences processed by a syn-
tactic parser or other shallow parsing components,
such as a chunker and a clause identifier. As a result,
the performance of the SRL systems relies heavily
on those syntax-analysis tools.
In order to improve the fundamental performance
of an SRL system, we trained parsers with training
data containing not only syntactic constituent infor-
mation but also semantic argument information. The
new parsers generate more correct constituents than
that trained on pure syntactic information. Because
the new parser generate different constituents than a
pure syntactic parser, we also explore the possibility
of combining the output of several parsers with the
help of a voting post-processing component.
This paper is organized as follows: Section 2
demonstrates the components of our SRL system.
We elaborate the importance of training a new parser
and outline our approach in Section 3 and Section 4.
Finally, Section 5 reports and discusses the results.
2 Semantic Role Labeling: the
Architecture
Our SRL system has 5 phases: Parsing, Pruning, Ar-
gument Identification, Argument Classification, and
Post Processing. The Argument Identification and
Classification components are trained with Sec 02-
21 of the Penn Treebank corpus.
2.1 Parsing
Previous SRL systems usually use a pure syntactic
parser, such as (Charniak, 2000; Collins, 1999), to
retrieve possible constituents. Once the boundary of
a constituent is defined, there is no way to change
it in later phases. Therefore the quality of the syn-
tactic parser has a major impact on the final per-
237
formance of an SRL system, and the percentage of
correct constituents that is generated by the syntac-
tic parser also defines the recall upper bound of an
SRL system. In order to attack this problem, in addi-
tion to Charniak?s parser (Charniak, 2000), our sys-
tem combine two parser which are trained on both
syntactic constituent information and semantic argu-
ment information. (See Section 3)
2.2 Pruning
Given a parse tree, a pruning component filters out
the constituents which are unlikely to be semantic
arguments in order to facilitate the training of the Ar-
gument Identification component. Our system uses
the heuristic rules introduced by (Xue and Palmer,
2004). The heuristics first spot the verb and then ex-
tract all the sister nodes along the verb spine of the
parse tree. We expand the coverage by also extract-
ing all the immediate children of an S, ADVP, PP
and NP node. This stage generally prunes off about
80% of the constituents given by a parser. For our
newly trained parsers, we also extract constituents
which have a secondary constituent label indicating
the constituent in question is an argument.
2.3 Argument Identification and Classification
We have as our Argument Identification component
a binary maximum-entropy classifier to determine
whether a constituent is an argument or not. If
a constituent is tagged as an argument, the Argu-
ment Classification component, which is a multi-
class maximum-entropy classifier, would assign it
a semantic role. The implementation of both the
Argument Identification and Classification compo-
nents makes use of the Mallet package1.
The lexical features we use to train these two
components are taken from (Xue and Palmer, 2004).
We trained the Argument Identification compo-
nent with the following single features: the path
from the constituent to the verb, the head word of
the constituent and its POS tag, and the distance
between the verb and the constituent, and feature
combinations: the verb and the phrasal type of the
constituent, the verb and the head word of the con-
stituent. If the parent node of the constituent is a PP
node, then we also include the head word of the PP
1http://mallet.cs.umass.edu
node and the feature combination of the verb and the
head word of the PP node.
In addition to the features listed above, the Ar-
gument Classification component also contains the
following features: the verb, the first and the last
content word of the constituent, the phrasal type
of the left sibling and the parent node, voice (pas-
sive or active), position of the constituent relative to
the verb, the subcategorization frame, and the syn-
tactic frame which describes the sequential pattern
of the noun phrases and the verb in the sentence.
2.4 Post Processing
The post processing component merges adjacent dis-
continuous arguments and marks the R-arguments
based on the content word and phrase type of the ar-
gument. Also it filters out arguments according to
the following constraints:
1. There are no overlapping arguments.
2. There are no repeating core arguments.
In order to combine the different systems, we also
include a voting scheme. The algorithm is straight-
forward: Suppose there are N participating systems,
we pick arguments with N votes, N-1 votes ..., and
finally 1 vote. The way to break a tie is based on
the confidence level of the argument given by the
system. Whenever we pick an argument, we need
to check whether this argument conflicts with pre-
viously selected arguments based on the constraints
described above.
3 Training a Parser with Semantic
Argument Information
A good start is always important, especially for a
successful SRL system. Instead of passively accept-
ing candidate constituents from the upstream syn-
tactic parser, an SRL system needs to interact with
the parser in order to obtain improved performance.
This motivated our first attempt which is to integrate
syntactic parsing and semantic parsing as a single
step, and hopefully as a result we would be able to
discard the SRL pipeline. The idea is to augment
the Penn Treebank (Marcus et al, 1994) constituent
labels with the semantic role labels from the Prop-
Bank (Palmer et al, 2005), and generate a rich train-
ing corpus. For example, if an NP is also an ar-
238
gument ARG0 of a verb in the given sentence, we
change the constituent label NP into NP-ARG0. A
parser therefore is trained on this new corpus and
should be able to serve as an SRL system at the same
time as predicting a parse.
However, this ideal approach is not feasible.
Given the fact that there are many different semantic
role labels and the same constituent can be different
arguments of different verbs in the same sentence,
the number of constituent labels will soon grow out
of control and make the parser training computation-
ally infeasible. Not to mention that anchor verb in-
formation has not yet been added to the constituent
label, and general data sparseness. As a compro-
mise, we decided to integrate only Argument Iden-
tification with syntactic parsing. We generated the
training corpus by simply marking the constituents
which are also semantic arguments.
4 Parsing Experiments
We trained a maximum-entropy parser based
on (Ratnaparkhi, 1999) using the OpenNLP pack-
age 2. We started our experiments with this specific
parsing implementation because of its excellent flex-
ibility that allows us to test different features. Be-
sides, this parser contains four clear parse tree build-
ing stages: TAG, CHUNK, BUILD, and CHECK.
This parsing structure offers us an isolated working
environment for each stage that helps us confine nec-
essary implementation modifications and trace down
implementation errors.
4.1 Data Preparation
Following standard practice, we use Sec 02-21 of
the Penn Treebank and the PropBank as our training
corpus. The constituent labels defined in the Penn
Treebank consist of a primary label and several sec-
ondary labels. A primary label represents the major
syntactic function carried by the constituent, for in-
stance, NP indicates a noun phrase and PP indicates
a prepositional phrase. A secondary label, starting
with ?-?, represents either a grammatical function of
a constituent or a semantic function of an adjunct.
For example, NP-SBJ means the noun phrase is a
surface subject of the sentence; PP-LOC means the
prepositional phrase is a location. Although the sec-
2http://sourceforge.net/projects/opennlp/
ondary labels give us much to encourage informa-
tion, because of data sparseness problem and train-
ing efficiency, we stripped off all the secondary la-
bels from the Penn Treebank.
After stripping off the secondary labels from the
Penn Treebank, we augment the constituent labels
with the semantic argument information from the
PropBank. We adopted four different labels, -AN,
-ANC, -AM, and -AMC. If the constituent in the
Penn Treebank is a core argument, which means
the constituent has one of the labels of ARG0-5 and
ARGA in the PropBank, we attach -AN to the con-
stituent label. The label -ANC means the constituent
is a discontinuous core argument. Similarly, -AM
indicates an adjunct-like argument, ARGM, and -
AMC indicates a discontinuous ARM.
For example, the sentence from Sec 02, [ARG0
The luxury auto maker] [ARGM-TMP last year]
sold [ARG1 1,214 cars] [ARGM-LOC in the U.S.],
would appear in the following format in our train-
ing corpus: (S (NP-AN (DT The) (NN luxury) (NN
auto) (NN maker) ) (NP-AM (JJ last) (NN year) )
(VP (VBD sold) (NP-AN (CD 1,214) (NNS cars) )
(PP -AM (IN in) (NP (DT the) (NNP U.S.) ) ) ) )
4.2 The 2 Different Parsers
Since the core arguments and the ARGMs in the
PropBank loosely correspond to the complements
and adjuncts in the linguistics literature, we are in-
terested in investigating their individual effect on
parsing performance. We trained two parsers. An
AN-parser was trained on the Penn Treebank cor-
pus augmented with two semantic argument labels:
-AN, and -ANC. Another AM-parser was trained on
labels -AM, and -AMC.
5 Results and Discussion
Table 1 shows the results after combining various
SRL systems using different parsers. In order to ex-
plore the effects of combining, we include the over-
all performance on the development dataset of indi-
vidual SRL systems in Table 2.
The performance of Semantic Role Labeling
(SRL) is determined by the quality of the syntactic
information provided to the system. In this paper,
we investigate that for the SRL task whether it is
more suitable to use a parser trained with data con-
239
Precision Recall F  
Development 75.70% 69.99% 72.73
Test WSJ 77.51% 72.97% 75.17
Test Brown 67.88% 59.03% 63.14
Test WSJ+Brown 76.31% 71.10% 73.61
Test WSJ Precision Recall F  
Overall 77.51% 72.97% 75.17
A0 85.14% 77.32% 81.04
A1 77.61% 75.16% 76.37
A2 68.18% 62.16% 65.03
A3 66.91% 52.60% 58.90
A4 77.08% 72.55% 74.75
A5 100.00% 40.00% 57.14
AM-ADV 59.73% 51.58% 55.36
AM-CAU 67.86% 52.05% 58.91
AM-DIR 65.67% 51.76% 57.89
AM-DIS 80.39% 76.88% 78.59
AM-EXT 78.95% 46.88% 58.82
AM-LOC 57.43% 55.37% 56.38
AM-MNR 54.37% 56.10% 55.22
AM-MOD 96.64% 94.01% 95.31
AM-NEG 96.88% 94.35% 95.59
AM-PNC 41.38% 41.74% 41.56
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 77.13% 74.15% 75.61
R-A0 86.82% 85.27% 86.04
R-A1 67.72% 82.05% 74.20
R-A2 46.15% 37.50% 41.38
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 100.00% 42.86% 60.00
R-AM-MNR 33.33% 33.33% 33.33
R-AM-TMP 78.57% 63.46% 70.21
R-C-A1 0.00% 0.00% 0.00
V 97.35% 95.54% 96.44
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
taining both syntactic bracketing and semantic ar-
gument boundary information than a pure syntactic
one.
The results of the SRL systems using the AM-
or AN- parsers are not significantly better than that
using the Charniak?s parser. This might due to the
simple training mechanism of the base parsing al-
gorithm which the AM- and AN- parsers exploit. It
also suggests our future work to apply the approach
to more sophisticated parsing frameworks. By then,
We show that we can boost the final performance
by combining different SRL systems using different
parsers, given that the combination algorithm is ca-
Precision Recall F  
AN-parser 71.31% 63.68% 67.28
AM-parser 74.09% 65.11% 69.31
Charniak 76.31% 64.62% 69.98
All 3 combined 75.70% 69.99% 72.73
Table 2: Overall results on the development set of
individual SRL systems.
pable of maintaining the quality of the final argu-
ments.
6 Acknowledgments
We thank Tom Morton for providing detailed expla-
nation for any of our parsing related inquiries.
References
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL-2000.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. PhD Dissertation, Uni-
versity of Pennsylvania.
Daniel Gildea and Martha Palmer. 2002. The Neces-
sity of Parsing for Predicate Argument Recognition.
In Proceedings of ACL 2002, Philadelphia, USA.
Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz,
et al 1994. The Penn Treebank: Annotating Predicate
Argument Structure. In Proceedings of ARPA Speech
and Natural Language Workshop.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An Annotated Corpus of Se-
mantic Roles. Computational Linguistics, 31(1).
Pradhan, S., Hacioglu, K., Krugler, V., Ward, W., Martin,
J., and Jurafsky, D. 2005. Support Vector Learning for
Semantic Argument Classification. To appear in Ma-
chine Learning journal, Special issue on Speech and
Natural Language Processing.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic Role Labeling via Integer Linear Program-
ming Inference. In Proceedings of COLING.
Adwait Ratnaparkhi. 1999. Learning to Parse Natural
Language with Maximum Entropy Models. Machine
Learning, 34, 151?175.
Nianwen Xue and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceedings
of EMNLP.
240
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 120?127,
New York, June 2006. c?2006 Association for Computational Linguistics
An Empirical Study of the Behavior of Active Learning for Word Sense 
Disambiguation 
1 Jinying Chen, 1 Andrew Schein, 1 Lyle Ungar, 2 Martha Palmer 
1 Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA, 19104 
{jinying,ais,ungar}@cis.upenn.edu 
2 Linguistic Department  
University of Colorado 
Boulder, CO, 80309 
Martha.Palmer@colorado.edu 
Abstract 
This paper shows that two uncertainty-
based active learning methods, combined 
with a maximum entropy model, work 
well on learning English verb senses. 
Data analysis on the learning process, 
based on both instance and feature levels, 
suggests that a careful treatment of feature 
extraction is important for the active 
learning to be useful for WSD. The 
overfitting phenomena that occurred 
during the active learning process are 
identified as classic overfitting in machine 
learning based on the data analysis. 
1 Introduction 
Corpus-based methods for word sense 
disambiguation (WSD) have gained popularity in 
recent years. As evidenced by the SENSEVAL 
exercises (http://www.senseval.org), machine 
learning models supervised by sense-tagged 
training corpora tend to perform better on the 
lexical sample tasks than unsupervised methods. 
However, WSD tasks typically have very limited 
amounts of training data due to the fact that 
creating large-scale high-quality sense-tagged 
corpora is difficult and time-consuming. Therefore, 
the lack of sufficient labeled training data has 
become a major hurdle to improving the 
performance of supervised WSD.  
A promising method for solving this problem 
could be the use of active learning. Researchers 
use active learning methods to minimize the 
labeling of examples by human annotators. A 
decrease in overall labeling occurs because active 
learners (the machine learning models used in 
active learning) pick more informative examples 
for the target word (a word whose senses need to 
be learned) than those that would be picked 
randomly. Active learning requires human labeling 
of the newly selected training data to ensure high 
quality. 
We focus here on pool-based active learning 
where there is an abundant supply of unlabeled 
data, but where the labeling process is expensive.  
In NLP problems such as text classification (Lewis 
and Gale, 1994; McCallum and Nigam, 1998), 
statistical parsing (Tang et al, 2002), information 
extraction (Thompson et al, 1999), and named 
entity recognition (Shen et al, 2004), pool-based 
active learning has produced promising results.  
This paper presents our experiments in applying 
two active learning methods, a min-margin based 
method and a Shannon-entropy based one, to the 
task of the disambiguation of English verb senses. 
The contribution of our work is not only in 
demonstrating that these methods work well for the 
active learning of coarse-grained verb senses, but 
also analyzing the behavior of the active learning 
process on two levels: the instance level and the 
feature level. The analysis suggests that a careful 
treatment of feature design and feature generation 
is important for a successful application of active 
learning to WSD. We also accounted for the 
overfitting phenomena that occurred in the learning 
process based on our data analysis.  
The rest of the paper is organized as follows. In 
Section 2, we introduce two uncertainty sampling 
methods used in our active learning experiments 
and review related work in using active learning 
for WSD. We then present our active learning 
experiments on coarse-grained English verb senses 
in Section 3 and analyze the active learning 
120
process in Section 4. Section 5 presents 
conclusions of our study.        
2 Active Learning Algorithms 
The methods evaluated in this work fit into a 
common framework described by Algorithm 1 (see 
Table 1). The key difference between alternative 
active learning methods is how they assess the 
value of labeling individual examples, i.e., the 
methods they use for ranking and selecting the 
candidate examples for labeling. The framework is 
wide open to the type of ranking rule employed. 
Usually, the ranking rule incorporates the model 
trained on the currently labeled data.  This is the 
reason for the requirement of a partial training set 
when the algorithm begins. 
                                Algorithm 1 
Require: initial training set, pool of unlabeled examples 
  Repeat 
Select T random examples from pool 
      Rank T examples according to active learning rule 
     Present the top-ranked example to oracle for labeling 
     Augment the training set with the new example 
  Until Training set reaches desirable size 
Table 1. A Generalized Active Learning Loop 
 
In our experiments we look at two variants of 
the uncertainty sampling heuristic: entropy 
sampling and margin sampling. Uncertainty 
sampling is a term invented by Lewis and Gale 
(Lewis and Gale, 1994) to describe a heuristic 
where a probabilistic classifier picks examples for 
which the model?s current predictions are least 
certain. The intuitive justification for this approach 
is that regions where the model is uncertain 
indicate a decision boundary, and clarifying the 
position of decision boundaries is the goal of 
learning classifiers. Schein (2005) demonstrates 
the two methods run quickly and compete 
favorably against alternatives when combined with 
the logistic regression classifier. 
2.1 Entropy Sampling 
A key question is how to measure uncertainty.  
Different methods of measuring uncertainty will 
lead to different variants of uncertainty sampling.  
We will look at two such measures.  As a 
convenient notation we use q (a vector) to 
represent the trained model?s predictions, with cq  
equal to the predicted probability of class c .  One 
method is to pick the example whose prediction 
vector q displays the greatest Shannon entropy: 
??
c
cc qq log    (1) 
Such a rule means ranking candidate examples 
in Algorithm 1 by Equation 1.  
2.2 Margin Sampling 
An alternative method picks the example with the 
smallest margin: the difference between the largest 
two values in the vector q (Abe and Mamitsuka, 
1998). In other words, if c and 'c are the two most 
likely categories for example nx , the margin is 
measured as follows: 
)|'Pr()|Pr( nnn xcxcM ?=   (2) 
In this case Algorithm 1 would rank examples 
by increasing values of margin, with the smallest 
value at the top of the ranking. 
Using either method of uncertainty sampling, 
the computational cost of picking an example from 
T candidates is: O(TD) where D is the number of 
model parameters.   
2.3 Related Work 
To our best knowledge, there have been very few 
attempts to apply active learning to WSD in the 
literature (Fujii and Inui, 1999; Chklovski and 
Mihalcea, 2002; Dang, 2004). Fujii and Inui (1999) 
developed an example sampling method for their 
example-based WSD system in the active learning 
of verb senses in a pool-based setting. Unlike the 
uncertainty sampling methods (such as the two 
methods we used), their method did not select 
examples for which the system had the minimal 
certainty. Rather, it selected the examples such that 
after training using those examples the system 
would be most certain about its predictions on the 
rest of the unlabeled examples in the next iteration. 
This sample selection criterion was enforced by 
calculating a training utility function. The method 
performed well on the active learning of Japanese 
verb senses. However, the efficient computation of 
the training utility function relied on the nature of 
the example-based learning method, which made 
their example sampling method difficult to export 
to other types of machine learning models. 
Open Mind Word Expert (Chklovski and 
Mihalcea, 2002) was a real application of active 
learning for WSD. It collected sense-annotated 
examples from the general public through the Web 
to create the training data for the SENSEVAL-3 
lexical sample tasks. The system used the 
121
disagreement of two classifiers (which employed 
different sets of features) on sense labels to 
evaluate the difficulty of the unlabeled examples 
and ask the web users to tag the difficult examples 
it selected. There was no formal evaluation for this 
active learning system.  
Dang (2004) used an uncertainty sampling 
method to get additional training data for her WSD 
system. At each iteration the system selected a 
small set of examples for which it had the lowest 
confidence and asked the human annotators to tag 
these examples. The experimental results on 5 
English verbs with fine-grained senses (from 
WordNet 1.7) were a little surprising in that active 
learning performed no better than random 
sampling. The proposed explanation was that the 
quality of the manually sense-tagged data was 
limited by an inconsistent or unclear sense 
inventory for the fine-grained senses. 
3 Active Learning Experiments 
3.1 Experimental Setting 
We experimented with the two uncertainty 
sampling methods on 5 English verbs that had 
coarse-grained senses (see Table 2), as described 
below. By using coarse-grained senses, we limit 
the impact of noisy data due to unclear sense 
boundaries and therefore can get a clearer 
observation of the effects of the active learning 
methods themselves.  
verb # of 
sen. 
baseline 
acc. (%) 
Size of data for 
active learning 
Size of 
test data  
Add 3 91.4 400 100 
Do 7 76.9 500 200 
Feel 3 83.6 400 90 
See 7 59.7 500 200 
Work 9 68.3 400 150 
Table 2. The number of senses, the baseline 
accuracy, the number of instances used for active 
learning and for held-out evaluation for each verb 
 
The coarse-grained senses are produced by 
grouping together the original WordNet senses 
using syntactic and semantic criteria (Palmer et al, 
2006). Double-blind tagging is applied to 50 
instances of the target word. If the ITA < 90%, the 
sense entry is revised by adding examples and 
explanations of distinguishing criteria. 
Table 2 summarizes the statistics of the data. 
The baseline accuracy was computed by using the 
?most frequent sense? heuristic to assign sense 
labels to verb instances (examples). The data used 
in active learning (Column 4 in Table 2) include 
two parts: an initial labeled training set and a pool 
of unlabeled training data. We experimented with 
sizes 20, 50 and 100 for the initial training set. The 
pool of unlabeled data had actually been annotated 
in advance, as in most pool-based active learning 
experiments. Each time an example was selected 
from the pool by the active learner, its label was 
returned to the learner. This simulates the process 
of asking human annotators to tag the selected 
unlabeled example at each time. The advantage of 
using such a simulation is that we can experiment 
with different settings (different sizes of the initial 
training set and different sampling methods).  
The data sets used for active learning and for 
held-out evaluation were randomly sampled from a 
large data pool for each round of the active 
learning experiment. We ran ten rounds of the 
experiments for each verb and averaged the 
learning curves for the ten rounds. 
In the experiments, we used random sampling 
(picking up an unlabeled example randomly at 
each time) as a lower bound. Another control 
(ultimate-maxent) was the learner?s performance 
on the test set when it was trained on a set of 
labeled data that were randomly sampled from a 
large data pool and equaled the amount of data 
used in the whole active learning process (e.g., 400 
training data for the verb add).  
The machine learning model we used for active 
learning was a regularized maximum entropy 
(MaxEnt) model (McCallum, 2002). The features 
used for disambiguating the verb senses included 
topical, collocation, syntactic (e.g., the subject, 
object, and preposition phrases taken by a target 
verb), and semantic (e.g., the WordNet synsets and 
hypernyms of the head nouns of a verb?s NP 
arguments) features (Chen and Palmer, 2005). 
3.2 Experimental Results 
Due to space limits, Figure 1 only shows the 
learning curves for 4 verbs do, feel, see, and work 
(size of the initial training set = 20). The curve for 
the verb add is similar to that for feel. These curves 
clearly show that the two uncertainty sampling 
methods, the entropy-based (called entropy-maxent 
in the figure) and the margin-based (called 
min_margin-maxent), work very well for active 
learning of the senses of these verbs. 
 
122
Figure 1 Active learning for four verbs  
Both methods outperformed the random 
sampling method in that they reached the upper-
bound accuracy earlier and had smoother learning 
curves. For the four verbs add, do, feel and see, 
their learning curves reached the upper bound at 
about 200~300 iterations, which means 1/2 or 1/3 
of the annotation effort can be saved for these 
verbs by using active learning, while still achieving 
the same level of performance as supervised WSD 
without using active learning. Given the large-
scale annotation effort currently underway in the 
OntoNotes project (Hovy et al, 2006), this could 
provide considerable savings in annotation effort 
and speed up the process of providing sufficient 
data for a large vocabulary. The OntoNotes project 
has now provided coarse-grained entries for over 
350 verbs, with corresponding double?blind 
annotation and adjudication in progress.  As this 
adjudicated data becomes available, we will be 
able to train our system accordingly. Preliminary 
results for 22 of these coarse-grained verbs (with 
an average grouping polysemy of 4.5) give us an 
average accuracy of 86.3%. This will also provide 
opportunities for more experiments with active 
learning, where there are enough instances.  Active 
learning could also be beneficial in porting these 
supervised taggers to new genres with different 
sense distributions. 
We also experimented with different sizes of 
the initial training set (20, 50 and 100) and found 
no significant differences in the performance at 
different settings. That means, for these 5 verbs, 
only 20 labeled training instances will be enough 
to initiate an efficient active learning process.        
From Figure 1, we can see that the two 
uncertainty sampling methods generally perform 
equally well except that for the verb do, the min-
margin method is slightly better than the entropy 
method at the beginning of active learning. This 
may not be so surprising, considering that the two 
methods are equal for two-class classification tasks 
(see Equations 1 and 2 for their definition) and the 
verbs used in our experiments have coarse-grained 
senses and often have only 2 or 3 major senses.   
An interesting phenomenon observed from 
these learning curves is that for the two verbs add 
and feel, the active learner reached the upper 
bound very soon (at about 100 iterations) and then 
even breached the upper bound. However, when 
the training set was extended, the learner?s 
performance dropped and eventually returned to 
123
the same level of the upper bound. We discuss the 
phenomenon below.  
4 Analysis of the Learning Process 
In addition to verifying the usefulness of active 
learning for WSD, we are also interested in a 
deeper analysis of the learning process. For 
example, why does the active learner?s 
performance drop sometimes during the learning 
process? What are the characteristics of beneficial 
features that help to boost the learner?s accuracy? 
How do we account for the overfitting phenomena 
that occurred during the active learning for the 
verbs add and feel? We analyzed the effect of both 
instances and features throughout the course of 
active learning using min-margin-based sampling.  
4.1 Instance-level Analysis  
Intuitively, if the learner?s performance drops after 
a new example is added to the training set, it is 
likely that something has gone wrong with the new 
example. To find out such bad examples, we 
define a measure credit_inst for instance i as: 
??
=
+
=
?
m
r
ll
n
l
AccAcclisel
m 1
1
1
)(),(1   (3) 
where Accl and Accl+1 are the classification 
accuracies of the active learner at the lth and 
(l+1)th iterations. n is the total number of 
iterations of active learning and m is the number of 
rounds of active learning (m=10 in our case). 
),( lisel is 1 iff instance i is selected by the active 
learner at the lth iteration and is 0 if otherwise. 
An example is a bad example if and only if it 
satisfies the following conditions: 
a)  its credit_inst value is negative 
b) it increases the learner?s performance, if it 
does, less often than it decreases the 
performance in the 10 rounds. 
We ranked the bad examples by their 
credit_inst values and their frequency of 
decreasing the learner?s performance in the 10 
rounds. Table 3 shows the top five bad examples 
for feel and work. There are several reasons why 
the bad examples may hurt the learner?s 
performance. Column 3 of Table 3 proposes 
reasons for many of our bad examples. We 
categorized these reasons into three major types. 
I. The major senses of a target verb depend 
heavily on the semantic categories of its NP 
arguments but WordNet sometimes fails to provide 
the appropriate semantic categories (features) for 
the head nouns of these NP arguments. For 
example, feel in the board apparently felt no 
pressure has Sense 1 (experience). In Sense 1, feel 
typically takes an animate subject. However, 
board, the head word of the verb?s subject in the 
above sentence has no animate meanings defined 
in WordNet. Even worse, the major meaning of 
board, i.e., artifact, is typical for the subject of feel 
in Sense 2 (touch, grope). Similar semantic type 
mismatches hold for the last four bad examples of 
the verb work in Table 3.  
II. The contexts of the target verb are difficult 
for our feature exaction module to analyze. For 
example, the antecedent for the pronoun subject 
they in the first example of work in Table 3 should 
be ringers, an agent subject that is typical for 
Sense 1 (exert oneself in an activity). However, the 
feature exaction module found the wrong 
antecedent changes that is an unlikely fit for the 
intended verb sense. In the fourth example for feel, 
the feature extraction module cannot handle the 
expletive ?it? (a dummy subject) in ?it was felt 
that?, therefore, it cannot identify the typical 
syntactic pattern for Sense 3 (find, conclude), i.e., 
subject+feel+relative clause. 
III. Sometimes, deep semantic and discourse 
analyses are needed to get the correct meaning of 
the target verb. For example, in the third example 
of feel, ??, he or she feels age creeping up?, it is 
difficult to tell whether the verb has Sense 1 
(experience) or Sense 3 (find) without an 
understanding of the meaning of the relative clause 
and without looking at a broader discourse context. 
The syntactic pattern identified by our feature 
extraction module, subject+feel+relative clause, 
favors Sense 3 (find), which leads to an inaccurate 
interpretation for this case. 
Recall that the motivation behind uncertainty 
samplers is to find examples near decision 
boundaries and use them to clarify the position of 
these boundaries. Active learning often does find 
informative examples, either ones from the less 
common senses or ones close to the boundary 
between the different senses. However, active 
learning also identifies example sentences that are 
difficult to analyze. The failure of our feature 
extraction module, the lack of appropriate semantic 
categories for certain NP arguments in WordNet, 
the lack of deep analysis (semantic and discourse 
analysis) of the context of the target verb can all 
124
         Table 3 Data analysis of the top-ranked bad examples found for two verbs 
produce misleading features. Therefore, in order to 
make active learning useful for its applications, 
both identifying difficult examples and getting 
good features for these examples are equally 
important. In other words, a careful treatment of 
feature design and feature generation is necessary 
for a successful application of active learning. 
There is a positive side to identifying such 
?bad? examples; one can have human annotators 
look at the features generated from the sentences 
(as we did above), and use this to improve the data 
or the classifier. Note that this is exactly what we 
did above: the identification of bad sentences was 
automatic, and they could then be reannotated or 
removed from the training set or the feature 
extraction module needs to be refined to generate 
informative features for these sentences. 
Not all sentences have obvious interpretations; 
hence the two question marks in Table 3. An 
example can be bad for many reasons: conflicting 
features (indicative of different senses), misleading 
features (indicative of non-intended senses), or just 
containing random features that are incorrectly 
incorporated into the model. We will return to this 
point in our discussion of the overfitting 
phenomena for active learning in Section 4.3. 
4.2 Feature-level Analysis 
The purpose of our feature-level analysis is to 
identify informative features for verb senses. The 
learning curve of the active learner may provide 
some clues. The basic idea is, if the learner?s 
performance increases after adding a new example, 
it is likely that the good example contains good 
features that contribute to the clarification of sense 
boundaries. However, the feature-level analysis is 
much less straightforward than the instance-level 
analysis since we cannot simply say the features 
that are active (present) in this good example are 
all good. Rather, an example often contains both 
good and bad features, and many other features 
that are somehow neutral or uninformative. The 
interaction or balance between these features 
determines the final outcome. On the other hand, a 
statistics based analysis may help us to find 
features that tend to be good or bad. For this 
analysis, we define a measure credit_feat for 
feature i as: 
feel Proposed reasons for bad examples Senses 
Some days the coaches make you feel as though you 
are part of a large herd of animals . 
? S1: experience 
And , with no other offers on the table , the board 
apparently felt no pressure to act on it.  
subject: board, no ?animate? meaning in 
WordNet  
S1: experience 
Sometimes a burst of aggressiveness will sweep over a 
man -- or his wife -- because he or she feels age 
creeping up.  
syntactic pattern: sbj+feel+relative clause 
headed by that, a typical pattern for Sense 
3 (find) rather than Sense 1 (experience)  
S1: experience 
At this stage it was felt I was perhaps more pertinent as 
chief. executive . 
syntactic pattern: sbj+feel+relative clause, 
typical for Sense 3 (find) but has not been 
detected by the feature exaction module 
S3: find, conclude
I felt better Tuesday evening when I woke up. ? S1: experience 
Work    
When their changes are completed, and after they have 
worked up a sweat, ringers often ?? 
subject: they, the feature exaction module 
found the wrong antecedent (changes 
rather than ringers) for they 
S1: exert oneself 
in an activity 
Others grab books, records , photo albums , sofas and 
chairs , working frantically in the fear that an 
aftershock will jolt the house again . 
subject: others (means people here), no 
definition in WordNet 
S1: exert oneself 
in an activity 
Security Pacific 's factoring business works with 
companies in the apparel, textile and food industries ?
subject: business, no ?animate? meaning 
in WordNet 
S1: exert oneself 
in an activity 
? ; blacks could work there , but they had to leave at 
night . 
subject: blacks, no ?animate? meaning in 
WordNet 
S1: exert oneself 
in an activity 
? has been replaced by alginates (gelatin-like material 
) that work quickly and accurately and with least 
discomfort to a child . 
subject: alginates, unknown by WordNet S2: perform, 
function, behave 
125
??
=
+
=
?
m
r l
ll
n
l act
AccAccliactive
m 1
1
1
1)(),(1         (4) 
where ),( liactive is 1 iff feature i is active in the 
example selected by the active learner at the lth 
iteration and is 0 if otherwise. actl is the total 
number of active features in the example selected 
at the lth iteration. n and m have the same 
definition as in Equation 3.  
A feature is regarded as good if its credit_feat 
value is positive. We ranked the good features by 
their credit_feat values.  By looking at the top-
ranked good features for the verb work (due to 
space limitations, we omit the table data), we 
identify two types of typically good features.  
The first type of good feature occurs frequently 
in the data and has a frequency distribution over 
the senses similar to the data distribution over the 
senses. Such features include those denoting that 
the target verb takes a subject (subj), is not used in 
a passive mode (morph_normal), does not take a 
direct object (intransitive), occurs in present tense 
(word_work, pos_vb, word_works, pos_vbz), and 
semantic features denoting an abstract subject 
(subjsyn_16993 1) or an entity subject (subjsyn_ 
1742), etc. We call such features background 
features. They help the machine learning model 
learn the appropriate sense distribution of the data. 
In other words, a learning model only using such 
features will be equal to the ?most frequent sense? 
heuristic used in WSD.  
Another type of good feature occurs less 
frequently and has a frequency distribution over 
senses that mismatches with the sense distribution 
of the data. Such features include those denoting 
that the target verb takes an inanimate subject 
(subj_it), takes a particle out (prt_out), is followed 
directly by the word out (word+1_out), or occurs at 
the end of the sentence. Such features are 
indicative of less frequent verb senses  that still 
occur fairly frequently in the data. For example, 
taking an inanimate subject (subj_it) is a strong 
clue for Sense 2 (perform, function, behave) of the 
verb work. Occurring at the end of the sentence is 
also indicative of Sense 2 since when work is used 
in Sense 1 (exert oneself in an activity), it tends to 
take adjuncts to modify the activity as in He is 
working hard to bring up his grade. 
                                                          
1 Those features are from the WordNet. The numbers are 
WordNet ids of synsets and hypernyms. 
There are some features that don?t fall into the 
above two categories, such as the topical feature 
tp_know and the collocation feature pos-2_nn. 
There are no obvious reasons why they are good 
for the learning process, although it is possible that 
the combination of two or more such features 
could make a clear sense distinction. However, this 
hypothesis cannot be verified by our current 
statistics-based analysis. It is also worth noting that 
our current feature analysis is post-experimental 
(i.e., based on the results). In the future, we will try 
automatic feature selection methods that can be 
used in the training phase to select useful features 
and/or their combinations.  
We have similar results for the feature analysis 
of the other four verbs. 
4.3 Account for the Overfitting Phenomena 
Recall that in the instance-level analysis in Section 
4.1, we found that some examples hurt the learning 
performance during active learning but for no 
obvious reasons (the two examples marked by ? in 
Table 3). We found that these two examples 
occurred in the overfitting region for feel. By 
looking at the bad examples (using the same 
definition for bad example as in Section 4.1) that 
occurred in the overfitting region for both feel and 
add, we identified two major properties of these 
examples. First, most of them occurred only once 
as bad examples (19 out 23 for add and 40 out of 
63 for feel). Second, many of the examples had no 
obvious reasons for their badness. 
Based on the above observations, we believe 
that the overfitting phenomena that occurred for 
the two verbs during active learning is typical of 
classic overfitting, which is consistent with a 
"death by a thousand mosquito bites" of rare bad 
features, and consistent with there often being (to 
mix a metaphor) no "smoking gun" of a bad 
feature/instance that is added in, especially in the 
region far away from the starting point of active 
learning. 
5 Conclusions 
We have shown that active learning can lead to 
substantial reductions (often by half) in the number 
of observations that need to be labeled to achieve a 
given accuracy in word sense disambiguation, 
compared to labeling randomly selected instances. 
In a follow-up experiment, we also compared a 
larger number of different active learning methods. 
126
The results suggest that for tasks like word sense 
disambiguation where maximum entropy methods 
are used as the base learning models, the minimum 
margin active criterion for active learning gives 
superior results to more comprehensive 
competitors including bagging and two variants of 
query by committee (Schein, 2005). By also taking 
into account the high running efficiency of the 
min-margin method, it is a very promising active 
learning method for WSD. 
We did an analysis on the learning process on 
two levels: instance-level and feature-level. The 
analysis suggests that a careful treatment of feature 
design and feature generation is very important for 
the active learner to take advantage of the difficult 
examples it finds during the learning process. The 
feature-level analysis identifies some 
characteristics of good features. It is worth noting 
that the good features identified are not particularly 
tied to active learning, and could also be obtained 
by a more standard feature selection method rather 
than by looking at how the features provide 
benefits as they are added in.   
For a couple of the verbs examined, we found 
that active learning gives higher prediction 
accuracy midway through the training than one 
gets after training on the entire corpus.  Analysis 
suggests that this is not due to bad examples being 
added to the training set. It appears that the widely 
used maximum entropy model with Gaussian 
priors is overfitting: the model by including too 
many features and thus fitting noise as well as 
signal.  Using different strengths of the Gaussian 
prior does not solve the problem. If a very strong 
prior is used, then poorer accuracy is obtained. We 
believe that using appropriate feature selection 
would cause the phenomenon to vanish. 
Acknowledgements 
This work was supported by National Science 
Foundation Grant NSF-0415923, Word Sense 
Disambiguation, the DTO-AQUAINT NBCHC-
040036 grant under the University of Illinois 
subcontract to University of Pennsylvania 2003-
07911-01 and the GALE program of the Defense 
Advanced Research Projects Agency, Contract No. 
HR0011-06-C-0022. Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the authors and do not 
necessarily reflect the views of the National 
Science Foundation, the DTO, or DARPA.  
References 
Naoki Abe and Hiroshi Mamitsuka. 1998. Query 
learning strategies using boosting and bagging. In 
Proc. of ICML1998, pages 1?10. 
Jinying Chen and Martha Palmer. 2005. Towards 
Robust High Performance Word Sense 
Disambiguation of English Verbs Using Rich 
Linguistic Features, In Proc. of IJCNLP2005, Oct., 
Jeju, Republic of Korea. 
Tim Chklovski and Rada Mihalcea, Building a Sense 
Tagged Corpus with Open Mind Word Expert, in 
Proceedings of the ACL 2002 Workshop on "Word 
Sense Disambiguation: Recent Successes and Future 
Directions", Philadelphia, July 2002. 
Hoa T. Dang. 2004. Investigations into the role of 
lexical semantics in word sense disambiguation.  PhD 
Thesis. University of Pennsylvania. 
Atsushi Fujii, Takenobu Tokunaga, Kentaro Inui, 
Hozumi Tanaka. 1998. Selective sampling for 
example-based word sense disambiguation, 
Computational Linguistics, v.24 n.4, p.573-597, Dec.  
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance 
Ramshaw and Ralph Weischedel. OntoNotes: The 
90% Solution. Accepted by HLT-NAACL06. Short 
paper. 
David D. Lewis and William A. Gale. 1994. A 
sequential algorithm for training text classifiers. In W. 
Bruce Croft and Cornelis J. van Rijsbergen, editors, 
Proceedings of SIGIR-94, Dublin, IE. 
Andrew K. McCallum. 2002. MALLET: A Machine 
Learning for Language Toolkit.  http://www.cs. 
umass.edu/~mccallum/mallet. 
Andew McCallum and Kamal Nigam. 1998. Employing 
EM in pool-based active learning for text 
classification. In Proc. of ICML ?98. 
Martha Palmer, Hoa Trang Dang and Christiane 
Fellbaum. (to appear, 2006). Making fine-grained and 
coarse-grained sense distinctions, both manually and 
automatically. Natural Language Engineering. 
Andrew I. Schein. 2005. Active Learning for Logistic 
Regression. Ph.D. Thesis. Univ. of Pennsylvania. 
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou and Chew 
Lim Tan. 2004 Multi-criteria-based active learning 
for named entity recognition, In Proc. of ACL04, 
Barcelona, Spain. 
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proc. of ACL 2002. 
Cynthia A. Thompson, Mary Elaine Califf, and 
Raymond J. Mooney. 1999. Active learning for 
natural language parsing and information extraction. 
In Proc. of ICML-99. 
127
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 57?60,
New York, June 2006. c?2006 Association for Computational Linguistics
OntoNotes: The 90% Solution 
 
 
Eduard Hovy Mitchell Marcus Martha Palmer Lance Ramshaw Ralph Weischedel 
USC/ICI Comp & Info Science ICS and Linguistics BBN Technologies BBN Technologies 
4676 Admiralty U. of Pennsylvania U. of Colorado 10 Moulton St. 10 Moulton St. 
Marina d. R., CA Philadelphia, PA Boulder, CO Cambridge, MA Cambridge, MA 
hovy 
@isi.edu 
mitch  
@cis.upenn.edu 
martha.palmer 
@colorado.edu 
lance.ramshaw 
@bbn.com 
weischedel 
@bbn.com 
 
 
 
 
Abstract* 
We describe the OntoNotes methodology and its 
result, a large multilingual richly-annotated corpus 
constructed at 90% interannotator agreement. An 
initial portion (300K words of English newswire 
and 250K words of Chinese newswire) will be 
made available to the community during 2007. 
1 Introduction 
Many natural language processing applications 
could benefit from a richer model of text meaning 
than the bag-of-words and n-gram models that cur-
rently predominate. Until now, however, no such 
model has been identified that can be annotated 
dependably and rapidly. We have developed a 
methodology for producing such a corpus at 90% 
inter-annotator agreement, and will release com-
pleted segments beginning in early 2007. 
The OntoNotes project focuses on a domain in-
dependent representation of literal meaning that 
includes predicate structure, word sense, ontology 
linking, and coreference. Pilot studies have shown 
that these can all be annotated rapidly and with 
better than 90% consistency. Once a substantial 
and accurate training corpus is available, trained 
algorithms can be developed to predict these struc-
tures in new documents. 
                                                        
*
 This work was supported under the GALE program of the 
Defense Advanced Research Projects Agency, Contract No. 
HR0011-06-C-0022. 
This process begins with parse (TreeBank) and 
propositional (PropBank) structures, which provide 
normalization over predicates and their arguments.  
Word sense ambiguities are then resolved, with 
each word sense also linked to the appropriate 
node in the Omega ontology. Coreference is also 
annotated, allowing the entity mentions that are 
propositional arguments to be resolved in context. 
Annotation will cover multiple languages (Eng-
lish, Chinese, and Arabic) and multiple genres 
(newswire, broadcast news, news groups, weblogs, 
etc.), to create a resource that is broadly applicable. 
2 Treebanking 
The Penn Treebank (Marcus et al, 1993) is anno-
tated with information to make predicate-argument 
structure easy to decode, including function tags 
and markers of ?empty? categories that represent 
displaced constituents.  To expedite later stages of 
annotation, we have developed a parsing system 
(Gabbard et al, 2006) that recovers both of these 
latter annotations, the first we know of.  A first-
stage parser matches the Collins (2003) parser on 
which it is based on the Parseval metric, while si-
multaneously achieving near state-of-the-art per-
formance on recovering function tags (F-measure 
89.0). A second stage, a seven stage pipeline of 
maximum entropy learners and voted perceptrons, 
achieves state-of-the-art performance (F-measure 
74.7) on the recovery of empty categories by com-
bining a linguistically-informed architecture and a 
rich feature set with the power of modern machine 
learning methods. 
57
3 PropBanking  
The Penn Proposition Bank, funded by ACE 
(DOD), focuses on the argument structure of verbs, 
and provides a corpus annotated with semantic 
roles, including participants traditionally viewed as 
arguments and adjuncts.  The 1M word Penn Tree-
bank II Wall Street Journal corpus has been suc-
cessfully annotated with semantic argument 
structures for verbs and is now available via the 
Penn Linguistic Data Consortium as PropBank I 
(Palmer et al, 2005).   Links from the argument 
labels in the Frames Files to FrameNet frame ele-
ments and VerbNet thematic roles are being added.  
This style of annotation has also been successfully 
applied to other genres and languages. 
4 Word Sense  
Word sense ambiguity is a continuing major ob-
stacle to accurate information extraction, summari-
zation and machine translation.  The subtle fine-
grained sense distinctions in WordNet have not 
lent themselves to high agreement between human 
annotators or high automatic tagging performance. 
Building on results in grouping fine-grained 
WordNet senses into more coarse-grained senses 
that led to improved inter-annotator agreement 
(ITA) and system performance (Palmer et al, 
2004; Palmer et al, 2006), we have  developed a 
process for rapid sense inventory creation and an-
notation that includes critical links between the 
grouped word senses and the Omega ontology 
(Philpot et al, 2005; see Section 5 below). 
This process is based on recognizing that sense 
distinctions can be represented by linguists in an 
hierarchical structure, similar to a decision tree, 
that is rooted in very coarse-grained distinctions 
which become increasingly fine-grained until 
reaching WordNet senses at the leaves.  Sets of 
senses under specific nodes of the tree are grouped 
together into single entries, along with the syntac-
tic and semantic criteria for their groupings, to be 
presented to the annotators.   
As shown in Figure 1, a 50-sentence sample of 
instances is annotated and immediately checked for 
inter-annotator agreement.  ITA scores below 90% 
lead to a revision and clarification of the groupings 
by the linguist. It is only after the groupings have 
passed the ITA hurdle that each individual group is 
linked to a conceptual node in the ontology. In ad-
dition to higher accuracy, we find at least a three-
fold increase in annotator productivity. 
 
Figure 1. Annotation Procedure 
As part of OntoNotes we are annotating the 
most frequent noun and verb senses in a 300K 
subset of the PropBank, and will have this data 
available for release in early 2007.  
4.1 Verbs 
Our initial goal is to annotate the 700 most fre-
quently occurring verbs in our data, which are 
typically also the most polysemous; so far 300 
verbs have been grouped and 150 double anno-
tated. Subcategorization frames and semantic 
classes of arguments play major roles in determin-
ing the groupings, as illustrated by the grouping for 
the 22 WN 2.1 senses for drive in Figure 2.  In ad-
word
Check against ontology (1 person)
not OK
Annotate test (2 people)
Results: agreement 
and confusion matrix
Sense partitioning, creating definitions, 
commentary, etc. (2 or 3 people)
Adjudication (1 person)
OK 
not OK
Sa
ve
 
fo
r 
fu
ll
a
n
n
o
ta
tio
n
GI: operating or traveling via a vehi-
cle 
NP (Agent) drive NP, NP drive PP 
WN1: ?Can you drive a truck??, WN2: ?drive to school,?, WN3: ?drive her to 
school,?, WN12: ?this truck drives well,? WN13: ?he drives a taxi,?,WN14: ?The car 
drove around the corner,?, WN:16: ?drive the turnpike to work,?  
G2: force to a position or stance 
NP drive NP/PP/infinitival 
WN4: ?He drives me mad.,? WN6: ?drive back the invaders,? WN7: ?She finally 
drove him to change jobs,? WN8: ?drive a nail,? WN15: ?drive the herd,? WN22: 
?drive the game.? 
G3:  to exert energy on behalf of 
something NP drive NP/infinitival 
WN5: ?Her passion drives her,? WN10: ?He is driving away at his thesis.? 
G4: cause object to move rapidly by 
striking it NP drive NP 
WN9: ?drive the ball into the outfield ,? WN17 ?drive a golf ball,? WN18 ?drive a 
ball? 
Figure 2. A Portion of the Grouping of WordNet Senses for "drive? 
58
dition to improved annotator productivity and ac-
curacy, we predict a corresponding improvement 
in word sense disambiguation performance.  Train-
ing on this new data, Chen and Palmer (2005) re-
port 86.3% accuracy for verbs using a smoothed 
maximum entropy model and rich linguistic fea-
tures, which is 10% higher than their earlier, state-
of-the art performance on ungrouped, fine-grained 
senses. 
4.2 Nouns 
We follow a similar procedure for the annotation 
of nouns.  The same individual who groups Word-
Net verb senses also creates noun senses, starting 
with WordNet and other dictionaries.  We aim to 
double-annotate the 1100 most frequent polyse-
mous nouns in the initial corpus by the end of 
2006, while maximizing overlap with the sentences 
containing annotated verbs.   
Certain nouns carry predicate structure; these 
include nominalizations (whose structure obvi-
ously is derived from their verbal form) and vari-
ous types of relational nouns (like father, 
President, and believer, that express relations be-
tween entities, often stated using of).  We have 
identified a limited set of these whose structural 
relations can be semi-automatically annotated with 
high accuracy.   
5 Ontology  
In standard dictionaries, the senses for each word 
are simply listed.   In order to allow access to addi-
tional useful information, such as subsumption, 
property inheritance, predicate frames from other 
sources, links to instances, and so on, our goal is to 
link the senses to an ontology.  This requires de-
composing the hierarchical structure into subtrees 
which can then be inserted at the appropriate con-
ceptual node in the ontology. 
The OntoNotes terms are represented in the 
110,000-node Omega ontology (Philpot et al, 
2005), under continued construction and extension 
at ISI.  Omega, which has been used for MT, 
summarization, and database alignment, has been 
assembled semi-automatically by merging a vari-
ety of sources, including Princeton?s WordNet, 
New Mexico State University?s Mikrokosmos, and 
a variety of Upper Models, including DOLCE 
(Gangemi et al, 2002), SUMO (Niles and Pease, 
2001), and ISI?s Upper Model, which are in the 
process of being reconciled.  The verb frames from 
PropBank, FrameNet, WordNet, and Lexical Con-
ceptual Structures (Dorr and Habash, 2001) have 
all been included and cross-linked.   
In work planned for later this year, verb and 
noun sense groupings will be manually inserted 
into Omega, replacing the current (primarily 
WordNet-derived) contents. For example, of the 
verb groups for drive in the table above, G1 and 
G4 will be placed into the area of ?controlled mo-
tion?, while G2 will then sort with ?attitudes?.   
6 Coreference  
The coreference annotation in OntoNotes connects 
coreferring instances of specific referring expres-
sions, meaning primarily NPs that introduce or 
access a discourse entity. For example, ?Elco In-
dustries, Inc.?, ?the Rockford, Ill. Maker of fasten-
ers?, and ?it? could all corefer. (Non-specific 
references like ?officials? in ?Later, officials re-
ported?? are not included, since coreference for 
them is frequently unclear.) In addition, proper 
premodifiers and verb phrases can be marked when 
coreferent with an NP, such as linking, ?when the 
company withdrew from the bidding? to ?the with-
drawal of New England Electric?.  
Unlike the coreference task as defined in the 
ACE program, attributives are not generally 
marked. For example, the ?veterinarian? NP would 
not be marked in ?Baxter Black is a large animal 
veterinarian?. Adjectival modifiers like ?Ameri-
can? in ?the American embassy? are also not sub-
ject to coreference. 
Appositives are annotated as a special kind of 
coreference, so that later processing will be able to 
supply and interpret the implicit copula link. 
All of the coreference annotation is being dou-
bly annotated and adjudicated. In our initial Eng-
lish batch, the average agreement scores between 
each annotator and the adjudicated results were 
91.8% for normal coreference and 94.2% for ap-
positives. 
7 Related and Future Work  
PropBank I (Palmer et al, 2005), developed at 
UPenn, captures predicate argument structure for 
verbs; NomBank provides predicate argument 
structure for nominalizations and other noun predi-
cates (Meyers et al, 2004).  PropBank II annota-
59
tion (eventuality ID?s, coarse-grained sense tags, 
nominal coreference and selected discourse con-
nectives) is being applied to a small (100K) paral-
lel Chinese/English corpus (Babko-Malaya et al, 
2004).  The OntoNotes representation extends 
these annotations, and allows eventual inclusion of 
additional shallow semantic representations for 
other phenomena, including temporal and spatial 
relations, numerical expressions, deixis, etc. One 
of the principal aims of OntoNotes is to enable 
automated semantic analysis.  The best current al-
gorithm for semantic role labeling for PropBank 
style annotation (Pradhan et al, 2005) achieves an 
F-measure of 81.0 using an SVM. OntoNotes will 
provide a large amount of new training data for 
similar efforts.   
Existing work in the same realm falls into two 
classes: the development of resources for specific 
phenomena or the annotation of corpora. An ex-
ample of the former is Berkeley?s FrameNet pro-
ject (Baker et al, 1998), which produces rich 
semantic frames, annotating a set of examples for 
each predicator (including verbs, nouns and adjec-
tives), and describing the network of relations 
among the semantic frames.  An example of the 
latter type is the Salsa project (Burchardt et al, 
2004), which produced a German lexicon based on 
the FrameNet semantic frames and annotated a 
large German newswire corpus.  A second exam-
ple, the Prague Dependency Treebank (Hajic et al, 
2001), has annotated a large Czech corpus with 
several levels of (tectogrammatical) representation, 
including parts of speech, syntax, and topic/focus 
information structure. Finally, the IL-Annotation 
project (Reeder et al, 2004) focused on the repre-
sentations required to support a series of increas-
ingly semantic phenomena across seven languages 
(Arabic, Hindi, English, Spanish, Korean, Japanese  
and French). In intent and in many details, 
OntoNotes is compatible with all these efforts, 
which may one day all participate in a larger multi-
lingual corpus integration effort.   
References  
O. Babko-Malaya, M. Palmer, N. Xue, A. Joshi, and S. Ku-
lick. 2004. Proposition Bank II: Delving Deeper, Frontiers 
in Corpus Annotation, Workshop, HLT/NAACL  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berke-
ley FrameNet Project. In Proceedings of COLING/ACL, 
pages 86-90. 
J. Chen and M. Palmer.  2005.  Towards Robust High Per-
formance Word Sense Disambiguation of English Verbs 
Using Rich Linguistic Features. In Proceedings of 
IJCNLP2005, pp. 933-944. 
B. Dorr and N. Habash.  2001.  Lexical Conceptual Structure 
Lexicons. In Calzolari et al ISLE-IST-1999-10647-WP2-
WP3, Survey of Major Approaches Towards Bilin-
gual/Multilingual Lexicons.  
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado, and M. 
Pinkal. 2006. Consistency and Coverage: Challenges for 
exhaustive semantic annotation. In Proceedings of DGfS-
06. 
C. Fellbaum (ed.). 1998. WordNet: An On-line Lexical Data-
base and Some of its Applications. MIT Press. 
R. Gabbard, M. Marcus, and S. Kulick. Fully Parsing the Penn 
Treebank. In Proceedings of HLT/NAACL 2006.  
A. Gangemi, N. Guarino, C. Masolo, A. Oltramari, and L. 
Schneider. 2002. Sweetening Ontologies with DOLCE. In 
Proceedings of EKAW  pp. 166-181. 
J. Hajic, B. Vidov?-Hladk?, and P. Pajas.  2001: The Prague 
Dependency Treebank: Annotation Structure and Support. 
Proceeding of the IRCS Workshop on Linguistic Data-
bases, pp. 105?114. 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. 
Building a Large Annotated Corpus of English: The Penn 
Treebank. Computational Linguistics 19: 313-330. 
A. Meyers, R. Reeves, C Macleod, R. Szekely, V. Zielinska, 
B. Young, and R. Grishman. 2004. The NomBank Project: 
An Interim Report. Frontiers in Corpus Annotation, Work-
shop in conjunction with HLT/NAACL. 
I. Niles and A. Pease.  2001.  Towards a Standard Upper On-
tology.  Proceedings of the International Conference on 
Formal Ontology in Information Systems (FOIS-2001). 
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004. Differ-
ent Sense Granularities for Different Applications, 2nd 
Workshop on Scalable Natural Language Understanding 
Systems, at HLT/NAACL-04,  
M. Palmer, H. Dang and C. Fellbaum. 2006. Making Fine-
grained and Coarse-grained Sense Distinctions, Both 
Manually and Automatically, Journal of Natural Language 
Engineering, to appear. 
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposi-
tion Bank: A Corpus Annotated with Semantic Roles, 
Computational Linguistics, 31(1). 
A. Philpot, E.. Hovy, and P. Pantel. 2005. The Omega Ontol-
ogy. Proceedings of the ONTOLEX Workshop at IJCNLP 
 S. Pradhan, W. Ward, K. Hacioglu, J. Martin, D. Jurafsky.  
2005.  Semantic Role Labeling Using Different Syntactic 
Views.  Proceedings of the ACL.  
F. Reeder, B. Dorr, D. Farwell, N. Habash, S. Helmreich, E.H. 
Hovy, L. Levin, T. Mitamura, K. Miller, O. Rambow, A. 
Siddharthan. 2004.  Interlingual Annotation for MT Devel-
opment. Proceedings of AMTA.  
60
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 25?30,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Language Modeling to Select Useful Annotation Data
Dmitriy Dligach
Department of  
Computer Science 
University of Colorado  
at Boulder 
Dmitriy.Dligach
@colorado.edu
Martha Palmer
Department of Linguistics 
University of Colorado  
at Boulder 
Martha.Palmer
@colorado.edu
 
 
Abstract
An annotation project typically has an abun-
dant supply of unlabeled data that can be 
drawn from some corpus, but because the 
labeling process is expensive, it is helpful to 
pre-screen the pool of the candidate instances 
based on some criterion of future usefulness. 
In many cases, that criterion is to improve the 
presence of the rare classes in the data to be 
annotated. We propose a novel method for 
solving this problem and show that it com-
pares favorably to a random sampling baseline 
and a clustering algorithm. 
1 Introduction
A data set is imbalanced when the distribution 
of classes in it is dominated by a single class. In 
Word Sense Disambiguation (WSD), the classes 
are word senses. The problem of imbalanced data 
is painfully familiar to WSD researchers: word 
senses are particularly well known for their skewed 
distributions that are also highly domain and cor-
pus dependent. Most polysemous words have a 
sense that occurs in a disproportionately high 
number of cases and another sense that is seen very 
infrequently. For example, the OntoNotes (Hovy et 
al., 2006) sense inventory defines two senses for 
the verb to add.  Of all the instances of this verb in 
the OntoNotes sense-tagged corpus, 93% are the 
instances of the predominant sense (not the arith-
metic sense!). Another fact: there are 4,554 total 
senses in the OntoNotes sense inventory for 1,713 
recently released verbs. Only 3,498 of them are 
present in the actual annotated data. More than 
1,000 senses (23%) are so rare that they are miss-
ing from the corpus altogether. More than a third 
of the released verbs are missing representative 
instances of at least one sense. In fact many of the 
verbs are pseudo-monosemous: even though the 
sense inventory defines multiple senses, only the 
most frequent sense is present in the actual anno-
tated data. For example, only 1 out of 8 senses of 
to rip is present in the data. 
The skewed nature of sense distributions is a 
fact of life. At the same time, a large-scale annota-
tion project like OntoNotes, whose goal is the crea-
tion of a comprehensive linguistic resource, cannot 
simply ignore it. That a sense is rare in a corpus 
does not mean that it is less important to annotate a 
sufficient number of instances of that sense: in a 
different domain it can be more common and not 
having enough annotated instances of that sense 
could jeopardize the success of an automatic cross-
domain WSD system. For example, sense 8 of to
rip ("to import an audio file directly from CD") is 
extremely popular on the web but it does not exist 
at all in the OntoNotes data. Only the traditional 
sense of to swap exists in the data but not the com-
puter science sense ("to move a piece of program 
into memory"), while the latter can conceivably be 
significantly more popular in technical domains.  
In general, class imbalance complicates super-
vised learning. This contention certainly holds for 
WSD. As an illustration, consider the verb to call, 
for which the OntoNotes sense inventory defines 
11 senses. Senses 3 and 5 are the most frequent: 
together they constitute 84% of the data. To inves-
tigate which classes are problematic for a classifi-
25
er, we conducted 50 supervised learning experi-
ments. In each experiment one instance of this verb 
was selected at random and used for testing while 
the rest was used for training a maximum entropy 
model. The resulting confusion matrix shows that 
the model correctly classified most of the instances 
of the two predominant senses while misclassify-
ing the other classes. The vast majority of the er-
rors came from confusing other senses with sense 5 
which is the most frequent sense of to call. Clearly, 
the data imbalance problem has a significant nega-
tive effect on performance. 
Let us now envision the following realistic sce-
nario: An annotation project receives funds to 
sense-tag a set of verbs in a corpus. It may be the 
case that some annotated data is already available 
for these verbs and the goal is to improve sense 
coverage, or no annotated data is available at all.  
But it turns out there are only enough funds to an-
notate a portion (e.g. half) of the total instances. 
The question arises how to pre-select the instances 
from the corpus in a way that would ensure that all 
the senses are as well represented as possible. Be-
cause some senses of these verbs are very rare, the 
pool of instances pre-selected for the annotation 
should include as many as possible instances of the 
rare senses. Random sampling ? the simplest ap-
proach ? will clearly not work: the pre-selected 
data will contain roughly the same proportion of 
the rare sense instances as the original set.  
If random sampling is not the answer, the data 
must be selected in some non-uniform way, i.e. 
using selective sampling. Active learning (e.g. 
Chen et al, 2006) is one approach to this problem. 
Some evidence is available (Zhu and Hovy, 2007) 
that active learning outperforms random sampling 
in finding the instances of rare senses. However, 
active learning has several shortcomings: (1) it re-
quires some annotated data to start the process; (2) 
it is problematic when the initial training set only 
contains the data for a single class (e.g. the pseudo-
monosemous verbs); (3) it is not always efficient in 
practice: In the OntoNotes project, the data is an-
notated by two human taggers and the disagree-
ments are adjudicated by the third. In classic active 
learning a single instance is labeled on each itera-
tion  This means the human taggers would have to 
wait on each other to tag the instance, on the adju-
dicator for the resolution of a possible disagree-
ment, and finally on the system which still needs to 
be-retrained to select the next instance to be la-
beled, a time sink much greater than tagging addi-
tional instances; (4) finally, active learning may 
not be an option if the data selected needs to be 
manually pre-processed (e.g. sentence segmented, 
tokenized, and treebanked ? as was the case with 
some of the OntoNotes data). In this setting, on 
each iteration of the algorithm, the taggers have to 
also wait for the selected instance to be manually 
pre-processed before they can label it. 
Thus, it would be significantly more convenient 
if all the data to be annotated could be pre-selected 
in advance. In this paper we turn to two unsuper-
vised methods which have the potential to achieve 
that goal. We propose a simple language modeling-
based sampling method (abbreviated as LMS) that 
increases the likelihood of seeing rare senses in the 
pre-selected data. The basic approach is as follow: 
using language modeling we can rank the instances 
of the ambiguous verb according to their probabili-
ty of occurrence in the corpus. Because the in-
stances of the rare senses are less frequent than the 
instances of the predominant sense, we can expect 
that there will be a higher than usual concentration 
of the rare sense instances among the instances that 
have low probabilities. The method is completely 
unsupervised and the only resource that it requires 
is a Language Modeling toolkit such as SRILM 
(Stolcke, 2002), which we used in our experiments. 
We compare this method with a random sampling 
baseline and semi-supervised clustering, which can 
serve the same purpose. We show that our method 
outperforms both of the competing approaches. We 
review the relevant literature in section 2, explain 
the details of LMS in section 3, evaluate LMS in 
section 4, discuss the results in section 5, and de-
scribe our plans for future work in section 6. 
2 Relevant Work
The problem of imbalanced data has recently re-
ceived much attention in the machine learning 
community. Rare classes can be of higher impor-
tance than frequent classes, as in medical diagnosis 
when one is interested in correctly identifying a 
rare disease. Network intrusion detection faces a 
similar problem: a malicious activity, although of 
crucial importance, is a very rare event compared 
to the large volumes of routine network traffic. At 
the same time, imbalanced data poses difficulties 
for an automatic learner in that rare classes have a 
much higher misclassification rate than common 
26
ones (Weiss, 1995; Japkowicz, 2001). Learning 
from imbalanced sets can also be problematic if the 
data is noisy: given a sufficiently high level of 
background noise, a learner may not distinguish 
between true exceptions (i.e. rare cases) and noise 
(Kubat and Matwin, 1997; Weiss, 2004). 
In the realm of supervised learning, cost-
sensitive learning has been recommended as a so-
lution to the problem of learning from imbalanced 
data (e.g. Weiss, 2004). However, the costs of mis-
classifying the senses are highly domain specific 
and hard to estimate. Several studies recently ap-
peared that attempted to apply active learning prin-
ciples to rare category detection (Pelleg and 
Moore, 2004; He and Carbonell, 2007). In addition 
to the issues with active learning outlined in the 
introduction, the algorithm described in (He and 
Carbonell, 2007) requires the knowledge of the 
priors, which is hard to obtain for word senses.  
WSD has a long history of experiments with 
unsupervised learning (e.g. Schutze, 1998; Puran-
dare and Peterson, 2004). McCarthy et al (2004) 
propose a method for automatically identifying the 
predominant sense in a given domain. Erk (2006) 
describes an application of an outlier detection al-
gorithm to the task of identifying the instances of 
unknown senses. Our task differs from the latter 
two works in that it is aimed at finding the in-
stances of the rare senses. 
Finally, the idea of LMS is similar to the tech-
niques for sentence selection based on rare n-gram 
co-occurrences used in machine translation (Eck et 
al., 2005) and syntactic parsing (Hwa, 2004). 
3 Language Modeling for Data Selection
Our method is outlined in Figure 1: 
 
Input 
A large corpus that contains T candidate instances 
from which S instances are to be selected for anno-
tation 
Basic Steps 
1. Compute the language model for the corpus 
2. Compute the probability distribution over the T 
candidate instances of the target verb  
3. Rank the T candidate instances by their proba-
bilities 
4. Form a cluster by selecting S instances with the 
lowest probability 
 
Figure 1. Basic steps of LMS 
 
Let us now clarify a few practical points. Al-
though an instance of the target verb can be 
represented as the entire sentence containing the 
verb, from the experiments with automatic WSD 
(e.g. Dligach and Palmer, 2008), it is known that 
having access to just a few words in the neighbor-
hood of the target verb is sufficient in many cases 
to predict the sense. For the purpose of LMS we 
represent an instance as the chunk of text centered 
upon the target verb plus the surrounding words on 
both sides within a three-word window. Although 
the size of the window around the target verb is 
fixed, the actual number of words in each chunk 
may vary when the target verb is close to the be-
ginning or the end of sentence. Therefore, we need 
some form of length normalization. We normalize 
the log probability of each chunk by the actual 
number of words to make sure we do not favor 
shorter chunks (SRILM operates in log space). The 
resulting metric is related to perplexity: for a se-
quence of words W = w1w2 ? wN  the perplexity is 
N
NwwwPWPP
1
21 )...()(
?
=  
The log of perplexity is 
)]...(log[1)](log[ 21 NwwwPNWPP ?=
 
Thus, the quantity we use for ranking is nega-
tive perplexity. 
4 Evaluation
For the evaluation, we selected two-sense verbs 
from the OntoNotes data that have at least 100 in-
stances and where the share of the rare sense is less 
than 20%. There were 11 such verbs (2,230 in-
stances total) with the average share of the rare 
sense 11%.  
Our task consists of clustering the instances of a 
verb into two clusters, one of which is expected to 
have a higher concentration of the rare senses than 
the other. Since the rare sense cluster is of primary 
interest to us, we report two metrics: (1) precision: 
the ratio of the number of instances of the rare 
sense in the cluster and the total number of in-
stances in the cluster; (2) recall: the ratio of the 
number of instances of the rare sense in the cluster 
and the total number of the rare sense instances in 
both clusters. Note that precision is not of primary 
importance for this task because the goal is not to 
reliably identify the instances of the rare sense but 
27
rather to group them into a cluster where the rare 
senses will have a higher concentration than in the 
original set of the candidate instances. At the same 
time achieving high recall is important since we 
want to ensure that most, if not all, of the rare 
senses that were present among the candidate in-
stances are captured in the rare sense cluster.  
4.1 Plausibility of LMS
The goal of our first set of experiments is to illu-
strate the plausibility of LMS. Due to space con-
straints, we examine only two verbs: compare and 
add. The remaining experiments will focus on a 
more comprehensive evaluation that will involve 
all 11 verbs. We computed the normalized log 
probability for each instance of a verb. We then 
ordered these candidate instances by their norma-
lized log probability and computed the recall of the 
rare sense at various levels of the size of the rare 
sense cluster. We express the size of the rare sense 
cluster as a share of the total number of instances. 
We depict recall vs. cluster size with a dotted 
curve. The graphs are in Figures 2 and 3.  
 
Figure 2. Rare sense recall for compare 
 
Figure 3. Rare sense recall for add 
The diagonal line on these figures corresponds 
to the random sampling baseline. A successful 
LMS would correspond to the dotted curve lying 
above the random sampling baseline, which hap-
pens to be the case for both of these verbs. For 
compare we can capture all of the rare sense in-
stances in a cluster containing less than half of the 
candidate instances. While verbs like compare re-
flect the best-case scenario, the technique we pro-
posed still works for the other verbs although not 
always as well. For example, for add we can recall 
more than 70% of the rare sense instances in a 
cluster that contains only half of all instances. This 
is more than 20 percentage points better than the 
random sampling baseline where the recall of the 
rare sense instances would be approximately 50%. 
4.2 LMS vs. Random Sampling Baseline
In this experiment we evaluated the performance 
of LMS for all 11 verbs. For each verb, we ranked 
the instances by their normalized log probability 
and placed the bottom half in the rare sense cluster. 
The results are in Table 2. The second column 
shows the share of the rare sense instances in the 
entire corpus for each verb. Thus, it represents the 
precision that would be obtained by random sam-
pling. The recall for random sampling in this set-
ting would be 0.5.   
Ten verbs outperformed the random sampling 
baseline both with respect to precision and recall 
(although recall is much more important for this 
task) and one verb performed as well. On average 
these verbs showed a recall figure that was 22 per-
centage points better than random sampling. Two 
of the 11 verbs (compare and point) were able to 
recall all of the rare sense instances. 
 
Verb Rare Inst Precision Recall 
account 0.12 0.21 0.93 
add 0.07 0.10 0.73 
admit 0.18 0.18 0.50 
allow 0.06 0.07 0.62 
compare 0.08 0.16 1.00 
explain 0.10 0.12 0.60 
maintain 0.11 0.11 0.53 
point 0.15 0.29 1.00 
receive 0.07 0.08 0.60 
remain 0.15 0.20 0.65 
worry 0.15 0.22 0.73 
average 0.11 0.16 0.72 
 
Table 2. LMS results for 11 verbs 
28
4.3 LMS vs. K-means Clustering
Since LMS is a form of clustering one way to eva-
luate its performance is by comparing it with an 
established clustering algorithm such as K-means 
(Hastie et al, 2001). There are several issues re-
lated to this evaluation. First, K-means produces 
clusters and which cluster represents which class is 
a moot question. Since for the purpose of the eval-
uation we need to know which cluster is most 
closely associated with a rare sense, we turn K-
means into a semi-supervised algorithm by seeding 
the clusters. This puts LMS at a slight disadvan-
tage since LMS is a completely unsupervised algo-
rithm, while the new version of K-means will 
require an annotated instance of each sense. How-
ever, this disadvantage is not very significant: in a 
real-world application, the examples from a dictio-
nary can be used to seed the clusters. For the pur-
pose of this experiment, we simulated the 
examples from a dictionary by simply taking the 
seeds from the pool of the annotated instances we 
identified for the evaluation. K-means is known to 
be highly sensitive to the choice of the initial 
seeds. Therefore, to make the comparison fair, we 
perform the clustering ten times and pick the seeds 
at random for each iteration. The results are aver-
aged. 
Second, K-means generates clusters of a fixed 
size while the size of the LMS-produced clusters 
can be easily varied. This advantage of the LMS 
method has to be sacrificed to compare its perfor-
mance to K-means. We compare LMS to K-means 
by counting the number of instances that K-means 
placed in the cluster that represents the rare sense 
and selecting the same number of instances that 
have the lowest normalized probability. Thus, we 
end up with the two methods producing clusters of 
the same size (with k-means dictating the cluster 
size).  
Third, K-means operates on vectors and there-
fore the instances of the target verb need to be 
represented as vectors. We replicate lexical, syn-
tactic, and semantic features from a verb sense dis-
ambiguation system that showed state-of-the-art 
performance on the OntoNotes data (Dligach and 
Palmer, 2008).  
The results of the performance comparison are 
shown in Table 3. The fourth column shows the 
relative size of the K-means cluster that was 
seeded with the rare sense. Therefore it also de-
fines the share of the instances with the lowest 
normalized log probability that are to be included 
in the LMS-produced rare sense clusters. On aver-
age, LMS showed 3% better recall than K-means 
clustering.  
 
K-means LMS
verb precision recall size precision recall
account 0.21 1.00 0.58 0.20 1.00
add 0.06 0.54 0.50 0.10 0.73
admit 0.21 0.31 0.29 0.09 0.15
allow 0.08 0.36 0.31 0.06 0.31
compare 0.22 0.42 0.18 0.19 0.43
explain 0.16 0.61 0.44 0.14 0.60
maintain 0.13 0.91 0.80 0.11 0.82
point 0.27 0.66 0.42 0.31 0.89
receive 0.11 0.68 0.72 0.08 0.80
remain 0.10 0.41 0.44 0.21 0.61
worry 0.81 0.51 0.13 0.38 0.33
average 0.21 0.58 0.44 0.17 0.61
 
Table 3. LMS vs. K-means 
5 Discussion and Conclusion
In this paper we proposed a novel method we 
termed LMS for pre-selecting instances for annota-
tion. This method is based on computing the prob-
ability distribution over the instances and selecting 
the ones that have the lowest probability. The ex-
pectation is that instances selected in this fashion 
will capture more of the instances of the rare 
classes than would have been captured by random 
sampling. We evaluated LMS by comparing it to 
random sampling and showed that LMS outper-
forms it. We also demonstrated that LMS com-
pares favorably to K-means clustering. This is 
despite the fact that the cluster sizes were dictated 
by K-means and that K-means had at its disposal 
much richer linguistic representations and some 
annotated data.  
Thus, we conclude that LMS is a promising me-
thod for data selection. It is simple to use since one 
only needs the basic functionality that any lan-
guage modeling toolkit offers. It is flexible in that 
the number of the instances to be selected can be 
specified by the user, unlike, for example, when 
clustering using k-means. 
29
6 Future Work
First, we would like to investigate the effect of se-
lective sampling methods (including LMS) on the 
performance of WSD models learned from the se-
lected data. Next, we plan to apply LMS for Do-
main adaptation. Unlike the scenario we dealt with 
in this paper, the language model would have to be 
learned from and applied to different corpora: it 
would be trained on the source corpus and used to 
compute probabilities for the instances in the target 
corpus that needs to be adapted. We will also expe-
riment with various outlier detection techniques to 
determine their applicability to data selection. 
Another promising direction is a simplified active 
learning approach in which a classifier is trained 
on the labeled data and applied to unlabeled data; 
the instances with a low classifier's confidence are 
selected for annotation (i.e. this is active learning 
conducted over a single iteration). This approach is 
more practical than the standard active learning for 
the reasons mentioned in Section 1 and should be 
compared to LMS. Finally, we will explore the 
utility of LMS-selected data as the initial training 
set for active learning (especially in the cases of 
the pseudo-monosemous verbs). 
Acknowledgments
 
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078, 
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No. 
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team.  Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not 
necessarily reflect the views of the National 
Science Foundation. 
References
 
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha 
Palmer. 2006. An Empirical Study of the Behavior of 
Active Learning for Word Sense Disambiguation. In 
Proceedings of the HLT-NAACL.  
Dmitriy Dligach and Martha Palmer. 2008. Novel Se-
mantic Features for Verb Sense Disambiguation. In 
Proceedings of ACL-HLT.  
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. 
Low Cost Portability for Statistical Machine Transla-
tion Based on N-gram Frequency and TF-IDF. Pro-
ceedings of IWSLT 2005. 
Katrin Erk. Unknown Word Sense Detection as Outlier 
Detection. 2006. In Proceedings of HLT-NAACL. 
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 
The Elements of Statistical Learning. Data Mining, 
Inference, and Prediction. 2001. Springer.  
Jingrui He and Jaime Carbonell. 2007. Nearest-
Neighbor-Based Active Learning for Rare Category 
Detection. NIPS. 
Hovy, E.H., M. Marcus, M. Palmer, S. Pradhan, L. 
Ramshaw, and R. Weischedel. 2006. OntoNotes: The 
90% Solution. In Proceedings of the HLT-NAACL. 
Eduard Hovy and Jingbo Zhu. 2007. Active Learning 
for Word Sense Disambiguation with Methods for 
Addressing the Class Imbalance Problem. In Pro-
ceedings of EMNLP.  
Rebecca Hwa. 2004. Sample Selection for Statistical 
Parsing. Computational Linguistics. Volume 30. Is-
sue 3. 
Natalie Japkowicz. 2001. Concept Learning in the Pres-
ence of Between-Class and Within-Class Imbalances. 
Proceedings of the Fourteenth Conference of the Ca-
nadian Society for Computational Studies of Intelli-
gence, Springer-Verlag. 
Miroslav Kubat and Stan Matwin. 1997. Addressing the 
curse of imbalanced training sets: one-sided selec-
tion. In Proceedings of the Fourteenth International 
Conference on Machine Learning. 
Diana McCarthy, Rob Koeling, Julie Weeds, and John 
Carroll. 2004. Finding Predominant Word Senses in 
Untagged Text. In Proceedings of 42nd Annual 
Meeting of Association for Computational Linguis-
tics. 
Dan Pelleg and Andrew Moore. 2004. Active Learning 
for Anomaly and Rare-Category Detection. NIPS. 
Amruta Purandare and Ted Pedersen. Word Sense Dis-
crimination by Clustering Contexts in Vector and 
Similarity Spaces. 2004. In Proceedings of the Con-
ference on CoNLL. 
Hinrich Schutze. 1998 Automatic Word Sense Discrim-
ination. Computational Linguistics.  
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. Spoken 
Language Processing, Denver, Colorado. 
Gary M. Weiss. 1995. Learning with Rare Cases and 
Small Disjuncts. Proceedings of the Twelfth Interna-
tional Conference on Machine Learning, Morgan 
Kaufmann. 
Gary M. Weiss. 2004. Mining with Rarity: A Unifying 
Framework. SIGKDD Explorations, special issue on 
learning from imbalanced datasets. 
30
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 921?928,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Aligning Features with Sense Distinction Dimensions  
1 Nianwen Xue,  2 Jinying Chen,  3 Martha Palmer 
1CSLR and 3Department of Linguistics 
University of Colorado 
Boulder, CO, 80309 
{Nianwen.Xue,Martha.Palmer}@colorado.edu 
2 Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA, 19104 
jinying@cis.upenn.edu 
 
Abstract 
In this paper we present word sense 
disambiguation (WSD) experiments on 
ten highly polysemous verbs in Chinese, 
where significant performance 
improvements are achieved using rich 
linguistic features. Our system performs 
significantly better, and in some cases 
substantially better, than the baseline on 
all ten verbs. Our results also 
demonstrate that features extracted from 
the output of an automatic Chinese 
semantic role labeling system in general 
benefited the WSD system, even though 
the amount of improvement was not 
consistent across the verbs. For a few 
verbs, semantic role information actually 
hurt WSD performance. The 
inconsistency of feature performance is a 
general characteristic of the WSD task, as 
has been observed by others. We argue 
that this result can be explained by the 
fact that word senses are partitioned 
along different dimensions for different 
verbs and the features therefore need to 
be tailored to particular verbs in order to 
achieve adequate accuracy on verb sense 
disambiguation. 
1 Introduction 
Word sense disambiguation, the determination of 
the correct sense of a polysemous word from a 
number of possible senses based on the context 
in which it occurs, is a continuing obstacle to 
high performance natural language processing 
applications. There are several well-documented 
factors that make accurate WSD particularly 
challenging. The first has to do with how senses 
are defined. The English data used for the 
SENSEVAL exercises, arguably the most widely 
used data to train and test WSD systems, are 
annotated based on very fine-grained distinctions 
defined in WordNet (Fellbaum, 1998), with 
human inter-annotator agreement at a little over 
seventy percent and the top-ranked systems? 
performances falling between 60%~70%  
(Palmer, et al, 2001; Mihalcea et al, 2004). The 
second source of difficulty for accurate WSD 
comes from how senses are distributed. It is 
often the case that a polysemous word has a 
dominant sense or several dominant senses that 
occur with high frequency and not enough 
instances can be found for its low frequency 
senses in the currently publicly available data. 
There are on-going efforts to address these 
issues. For example, the sense annotation 
component of the OntoNotes project (Hovy, et 
al., 2006) attempts to create a large-scale coarse-
grained sense-annotated corpus with senses 
defined based on explicit linguistic criteria. 
These problems will be alleviated when 
resources like this are available to the general 
NLP community. There have already been 
experiments that show such coarse-grained 
senses lead to substantial improvement in system 
performance (Palmer et al 2006).  
The goal of our experiments is to explore the 
implications of a related and yet separate 
problem, specifically the extent to which the 
linguistic criteria used to define senses are 
related to what features need to be used in 
machine-learning systems. There are already 
published results that show WSD for different 
syntactic categories may need different types of 
features. For example, Yarowsky and Florian 
(2002), in their experiments on SENSEVAL2 
English data, showed that sense distinctions of 
verbs relied more on linguistically motivated 
features than other parts-of-speech. In this paper, 
921
we will go one step further and show that even 
for words of the same syntactic category senses 
are often defined along different dimensions 
based on different criteria. One direct implication 
of this observation for supervised machine-
learning approaches to WSD is that the features 
have to be customized for different word 
categories, or even for different words of the 
same category. This supports previous arguments 
for word-specific feature design and parametric 
modeling for WSD tasks (Chen and Palmer, 
2005; Hoste et al 2002). We report experiments 
on ten highly polysemous Chinese verbs and 
show that features are not uniformly useful for 
all words.  
The rest of the paper is organized as follows. 
In Section 2, we describe our WSD system, 
focusing on the features we used. We also briefly 
compare the features we use for Chinese with 
those used in a similar English WSD system. In 
Section 3, we present our experimental results 
and show that although rich linguistic features 
and features derived from a Chinese Semantic 
Role Labeling improve the WSD accuracy, the 
improvement is not uniform across all verbs. We 
show that this lack of consistency is due to the 
different dimensions along which the features are 
defined. In Section 4, we discuss related work. 
Finally Section 5 concludes this paper and 
describes future directions.  
2 WSD System for Chinese Verbs 
Our WSD system uses a smoothed maximum 
entropy (MaxEnt) model with a Gaussian prior 
(McCallum, 2002) for learning Chinese verb 
senses. The primary reason is that the MaxEnt 
model provides a natural way for combining 
different features without the assumption of 
feature independence. Furthermore, smoothing 
the MaxEnt model with a Gaussian prior is better 
than other smoothing methods at alleviating the 
overfitting problem caused by low frequency 
features (Chen et al, 1999). This model has been 
applied successfully for English WSD (Dang, 
2004; Chen and Palmer, 2005). 
The features used by our Chinese WSD 
system include: 
Collocation Features 
- Previous and next word  (relative to the target 
verb), w-1 and w1 and their parts-of-speech p-1 
and p1 
 
Syntactic Features 
- Whether the target verb takes a direct object 
(i.e., in a transitive use)  
- Whether the verb takes a sentential 
complement 
- Whether the verb, if it consists of a single 
character, occurs at the last position of a 
compound verb 
 
Semantic Features 
- The semantic role information about the verbs 
- The semantic categories for the verb?s NP 
arguments from a general Chinese noun 
Taxonomy 
  
All of these features require some level of 
preprocessing of the Chinese raw text, which 
comes without word boundaries. To extract the 
collocation features the raw text needs to be 
segmented and POS-tagged; to extract the 
syntactic and semantic features, the Chinese text 
needs to be parsed. We use an integrated parser 
that does segmentation, POS-tagging and parsing 
in one step. Since part of the sense-tagged data 
comes from the Chinese Treebank that the parser 
is trained on, we divide the Chinese Treebank 
into nine equal-sized portions and parse each 
portion with a parsing model trained on the other 
eight portions so that the parser has not seen any 
of the data it parses. The data that is not from the 
Chinese Treebank is parsed with a parsing model 
trained on the entire Chinese Treebank. The 
parser produces a segmented, POS-tagged and 
parsed version of the same text to facilitate the 
extraction of the different types of features. The 
extraction of the semantic role labels as features 
requires the use of a semantic role tagger, which 
we describe in greater detail in Section 2.2. 
In addition to using the semantic role labeling 
information, we also extract another type of 
semantic features from the verb?s NP arguments. 
These features are top-level semantic categories 
from a three-level general taxonomy for Chinese 
nouns, which was created semi-automatically 
based on two Chinese semantic dictionaries 
(Chen and Palmer, 2004). 
2.1 A Comparison with  Our English WSD 
System 
Similar to our English WSD system, which 
achieved the best published results on 
SENSEVAL2 English verbs for both fine-
grained and coarse-grained senses (Chen and 
Palmer, 2005), our Chinese WSD system uses 
the same smoothed MaxEnt machine learning 
model and linguistically motivated features for 
Chinese verb sense disambiguation. However, 
the features used in the two systems differ 
922
somewhat  due to the different properties of  the 
two languages . 
For example, our English system uses the 
inflected form and the part-of-speech tag of the 
target verb as feature. For Chinese we no longer 
use such features since Chinese words, unlike 
English ones, do not contain morphology that 
marks tense. 
The collocation features used by our English 
system include bi-grams and tri-grams of the 
words that occur within two positions before or 
after the target verb and their part-of-speech tags. 
In contrast, our Chinese system extracts 
collocation features from a narrower, three-word 
window, with one word immediately before and 
after the target verb. This decision was made 
based on two observations about the Chinese 
language. First, certain single-character Chinese 
verbs, such as the verbs ??|chu?, ??|kai? and 
??|cheng? in our experiments, often form a 
compound with  a verb to its immediate left. That 
verb is often a good indicator of the sense of this 
verb. An example is given in (1):   
 
(1) ??    ?       ??   ? 
       Liaoning  already     show      completion         
      
    ???                    ??          ?? ? 
     multidimensional   development        trend  
 
 ?Liaoning Province has shown the trend of 
multidimensional development.? 
 
Being the last  word of a verb compound is a 
strong indicator for Sense 8 of the verb ??
|chu1? (used after a verb to indicate direction or 
aspect), as in ???|cheng2xian4?|chu1?. 
Second, unlike English common nouns that 
often require determiners such as the, a or an, 
Chinese common nouns can stand alone. 
Therefore, the direct object of a verb often 
occurs right after the verb in Chinese, as shown 
in (2). 
 
?2???         ??         ??      ??           ?  
    mobilize     people      tighten   waistband  collect 
 
    ?   ?  ?? (direct object)? 
    funds   build highway 
 
?Mobilize people to tighten their waistbands (i.e., 
save money) in order to collect funds to build 
highways.? 
  
Based on these observations, we use words 
surrounding  the target verb and their part-of-
speech tags  as collocation features. A further 
investigation on the different sizes of the context 
window (3,5,7,9,11) showed that  increasing the 
window size decreased our system?s accuracy.   
2.2 Features Based on Automatic Semantic 
Role Tagging 
In a recent paper on the WSD of English verbs, 
Dang and Palmer (2005) showed that semantic 
role information significantly improves the WSD 
accuracy of English verbs for both fine-grained 
and coarse-grained senses. However, this result 
assumes the human annotation of the Penn 
English Propbank (Palmer et al 2005). It seems 
worthwhile to investigate whether the semantic 
role information produced by a fully automatic 
Semantic Role tagger can improve the WSD 
accuracy on verbs, and test the hypothesis that 
the senses of a verb  have a high correlation to 
the arguments it takes. To that end, we assigned 
semantic role labels to the arguments  of the 
target verb with a fully automatic semantic role 
tagger (Xue and Palmer, 2005) trained on the 
Chinese Propbank (CPB) (Xue and Palmer, 
2003), a corpus annotated with semantic role 
labels that are similiar in style to the Penn 
English Propbank. In this annotation, core 
arguments such as agent or theme are labeled 
with numbered arguments such as Arg0 and Arg1, 
up to Arg5 while adjunct-like elements are 
assigned functional tags such as TMP (for 
temporal), MNR, prefixed by ArgM. The 
Semantic Role tagger takes as input syntactic 
parses produced by the parser described above as 
input and produces a list of arguments for each 
of  the sense-tagged target verbs and assigns 
argument labels to them. Features are extracted 
from both the core arguments and adjuncts of the 
target verb. In addition to providing the sematnic 
role labels (e.g., Arg0 and Arg1) of the extracted 
core arguments, the Semantic Role tagger also 
provides Hownet (Dong and Dong, 1991) 
semantic categories associated with these 
arguments. (3) shows the arguments for the 
target verb ???  identified by the Semantic Role 
tagger: 
 
(3)  [ArgM-MNR ??    ?    ?     ?      ?] ,    
                  through  three   year   hard   work,   
  
 [arg0 ?        ?]    [rel ?]    ?             
            whole   county   dig         finish   
 
  [Arg1 ?      ??]      ?       ? ? 
        deep    well         three  classifier  
923
 
?The whole county finished digging three deep 
wells through 3 years of hard work.? 
 
Based on the output of the Semantic Role tagger 
and the Chinese noun taxonomy (as described  in 
Section 2.1), the following features are extracted: 
 
SRL+lex               SRL+HowNet     SRL+Taxonomy 
ARG1-??  ARG1-??         ARG1_location  
ARG0-? ARG0-??         ARG0_location 
ARGM|MNR-?? ARGM|MNR-?? ARGM|MNR 
 
In this example, semantic role related features 
include: (1) the head word of the core arguments 
(ARG1-?? and ARG0-?) and the adjunct 
(ARGM|MNR-??); (2) the HowNet semantic 
category for the head word (ARG1-??, ARG0-
??, ARGM|MNR-??); (3) the semantic role 
label of the adjunct (ARGM|MNR); and (4) the 
top level semantic category from the taxonomy 
of Chinese nouns for the head word of the NP 
arguments (ARG1_location and ARG0_location).       
3 Experimental Results 
The data we used for our experiments are 
developed as part of the OntoNotes project 
(Hovy et al, 2006) and they come from a variety 
of sources. Part of the data is from the Chinese 
Treebank (Xue et al 2005), which has a 
combination of Xinhua news and Sinorama 
News Magazine. Since some verbs have an 
insufficient number of instances for any 
meaningful experiments, we also annotated 
portions of the People?s Daily corpus, developed 
by Peking University. We chose not to use the 
Chinese WSD dataset used in Senseval 3 1 
because we are mainly interested in investigating 
how the features used in WSD are related to the 
criteria used to define the senses of Chinese 
verbs. The Chinese Senseval dataset includes 
both nouns and verbs. In addition, the criteria 
used to define their senses are not made explicit 
and therefore are not clear to us.  
Table 1 summarizes the corpus statistics and 
the experimental results for the 10 highly 
polysemous Chinese verbs used in our 
experiments. The results were obtained by using 
5-fold cross validation. The top five verbs are 
verbs that were identified as difficult verbs in 
Dang et als (2002) experiments. The first three 
columns show the verbs (and their pinyin), the 
number of instances and the number of senses for 
                                                 
1 http://www.senseval.org/senseval3 
each verb in the data. The fourth column shows 
the sense entropy for each verb in its test data, as 
calculated in Equation 1. 
)(log)(
1
i
n
i
i sensePsenseP?
=
?            (1) 
Where n is the number of senses of a verb in our 
data; )( isenseP is the probability of the ith sense 
of the verb, which is estimated based on the 
frequency count of the verb?s senses in the data. 
Sense entropy generally reflects the frequency 
distribution of senses in the corpus. A verb with 
an evenly distributed sense distribution tends to 
have a high entropy value. However, a verb can 
also have a high sense entropy simply because it 
is highly polysemous (say, has 20 or more senses) 
even though the sense distribution may be 
skewed, with one or two dominant senses. To 
separate the effects of the number of senses, we 
also use a normalized sense entropy metric (the 
sixth column in Table 1), as calculated in 
Equation 2.  
)1(log1
)(log)(
1
1
n
P
n
sensePsenseP
n
i
i
n
i
i
?
?
=
=
?
?
        (2) 
Here a large sense number n corresponds to a 
high value for the normalization factor 
)1(log1
1 n
P
n
n
i
?
=
? . Therefore, normalized sense 
entropy can indicate sense frequency distribution 
more precisely than sense entropy.  
Table 1 (Columns 7 to 10) also shows the 
experimental results. As we can see, on average, 
our system achieved about 19% improvement 
(absolute gain) in accuracy compared to the most 
frequent sense baseline. Its performance is 
consistently better than the baseline for all 10 
verbs. 
3.1 Corpus Statistics and Disambiguation 
Accuracy 
The data in Table 1 shows that verbs with a high 
normalized sense entropy have the low frequency 
baselines. Furthermore, this relation is stronger 
than that between un-normalized sense entropy 
and the baseline. However, sense entropy is a 
better predictor for system performance than 
normalized sense entropy. The reason is intuitive: 
unlike the baseline, the automatic WSD system, 
trained on the training data, does not only rely on 
sense frequency information to predict senses.  
924
  
# of 
instance 
# of 
sense 
sense 
entropy
norm. 
sense 
entropy baseline all feat all-SRL 
?|chu 271 11 1.12 0.47 74.54 79.70 78.59 
??|huifu 113 3 0.93 0.84 50.44 69.91 72.57 
?|jian 167 7 1.01 0.52 72.46 82.63 82.03 
?|xiang 231 6 1.00 0.56 65.80 76.19 77.49 
?|yao 254 9 1.56 0.71 33.46 46.46 49.21 
?|cheng 161 8 1.38 0.67 43.48 73.29 72.67 
?|da 313 21 2.29 0.75 20.77 45.05 32.59 
?|kai 382 18 2.31 0.80 19.37 50.00 39.27 
??|tongguo 384 4 0.97 0.70 55.73 81.51 79.17 
??|fazhan 1141 7 0.88 0.45 74.76 79.58 77.56 
average   9.4     51.08 70.18 67.13 
total 3417             
The number of senses has a direct impact on how 
many training instances exist for each verb sense. 
As a consequence, it is more difficult for the 
system to make good generalizations from the 
limited training data that is available for highly 
polysemous verbs. Therefore, sense entropy, 
which is based on both sense frequency 
distribution and polysemy is more appropriate 
for predicting system accuracy. A related 
observation is that the system gain (compared 
with the baseline) is bigger for verbs with a high 
normalized sense entropy, such as ???|huifu?, 
??|da?, ??|kai?, and ???|tongguo?, than for 
other verbs; and the system gain is very small for 
verbs with low normalized sense entropy and a 
relatively large number of senses, such as ??
|chu? and ???|fazhan?, since they already have 
high baselines. 
3.2 The Effect of Semantic Role Features 
When Semantic Role information is used in 
features, the system?s performance on average 
improves 3.05%, from 67.13% to 70.18% 
compared with when the features derived from 
the Semantic Role information is not used.  If we 
look at the system?s performance on individual 
verbs, the results show that adding Semantic 
Role information as features improves the 
accuracy of 7 of the 10 verbs. For the remaining 
3 verbs, adding semantic role information 
actually hurts the system?s performance. We 
believe this apparent inconsistency can be 
explained by looking at how senses are defined 
for the different verbs.  The two verbs that 
present the most challenge to the system, are 
??|da? and  ??|yao? While Semantic Role 
features substantially improve the accuracy of 
??|da?, they actually hurt the accuracy of ??
|yao?. For ??|yao?, its three most frequent 
senses account for 86% of its total instances (232 
out of 270) and they are the ?intend to?, ?must, 
should? and ?need? senses: 
 
(4) Three most frequent senses of ??|yao? 
(a)  ??        ??       ?       ??? ?? ? 
    two sides  indicate  intend  further   cooperation                  
 
?The two sides indicated that they intended to step up 
their cooperation.? 
 
(b) ?     ?  ? ? ??  ?            ?? ? 
      road  very  slippery,     everybody  should  careful     
 
?The road is slippery. Everybody should be careful.? 
 
(c) ??  ?                   ?       ?       ? 
 Suzhou Steel Works  every   year    need   depend 
      
???            ??         ?? ? 
 
 the Great Canal      transport    raw material 
 
?Suzhou Steel Works needs to depend on the Great 
Canal to transport raw material.?  
 
Two of the senses, ?must? and ?need?, are 
used as auxiliary verbs. As such, they do not take 
arguments in the same way non-auxiliary verbs 
do. For example, they do not take noun phrases 
as arguments. As a result, the Semantic Role 
tagger, which assigns argument labels to head 
words of noun phrases or clauses, cannot 
produce a meaningful argument for an auxiliary 
verb. For the ?intend to? sense, even if it is not 
Table 1 Corpus Statistics and Experimental Results for the 10 Chinese Verbs 
925
an auxiliary verb, it still does not take a noun 
phrase as an object. Instead, its object is a verb 
phrase or a clause, depending on the analysis.  
The correct head word of its argument should be 
the lower verb, which apparently is not a useful 
discriminative feature either. 
In contrast, the senses of ??|da? are generally 
defined based on their arguments. The three most 
frequent senses of ? ? |da? are ?call by 
telephone?, ?play? and ?fight? and they account 
for 40% of the ??|da? instances. Some examples 
are provided in (5) 
 
(5) Top three senses of ??|da? 
 
(a) ?      ??     ??                          ?     
       you   have     queue in long line     call    
        
?? ??            ?        ?? ?           ? 
       public   phone  DE     experience    ma    
 
?Do you have the experience of queuing in a line 
and waiting to make a call with a public phone?? 
 
(b) ??      ??     ?? ??           
a few     on duty    personnel      sit      
 
? ?            ?  ??   ? 
one      circle     play    poker  
 
?A few of the personnel on duty were sitting in a 
circle and playing poker.? 
 
(c) ?? ?   ??  ??  ?? 
mobilize    whole society  power   fight 
 
       ??   ??  ? ? 
       helping the poor crucial battle 
 
??mobilize the power of the whole society and 
fight the crucial battle of helping the poor.? 
 
The senses of ??|da? are to a large extent 
determined by its PATIENT (or Arg1) argument, 
which is generally realized in the object position. 
The Arg1 argument usually forms highly 
coherent lexical classes. For example, the Arg1 
of the ?call? sense can be ? ? ?
|dianghua/phone, ???|shouji/cellphone?, 
etc. its Arg1 argument can be ? ? ?
|langqiu/basketball?, ? ?? |qiaopai/bridge?, 
???|youxi/game?, etc for the "play" sense,  
Finally , for its sense ?fight?, the Arg1 argument 
can be ???|gongjian/crucial ?|zhan/battle?, 
??? |xiangzhang/street warfare?, ?????
youjizhan/guerilla warfare?, etc.. It?s not 
surprising that recognizing the arguments of 
??|da? is crucial in determining its sense.  
The accuracy for both verbs is still very low, 
but for very different reasons. In the case of 
??|yao4?, the challenge is identifying 
discriminative features that may not be found in 
the narrow local context. These could for 
instance include discourse features. In the case of 
??|da?, one important reason why the accuracy 
is still low is because ??|da? is highly 
polysemous and has over forty senses. Given its 
large number of senses, the majority of its senses 
do not have enough instances to train a 
reasonable model. We believe that more data will 
improve its WSD accuracy.  
There are other dimensions along which verb 
senses are defined in addition to whether or not a 
verb is an auxiliary verb and what type of 
auxiliary verb it is, and what types of arguments 
it takes. One sense of ??|chu? is a verb particle 
that indicates the direction or aspect of the main 
verb that generally immediately precedes it. In 
this case the most important feature for 
identifying this sense is the collocation feature.  
 
Our experimental results seem to lend support 
to a WSD approach where features are tailored to 
each target word, or at least each class of words, 
based on a careful analysis of the dimensions 
along which senses are defined. Automatic 
feature selection (Blum and Langley, 1997) 
could also prove useful in providing this type of 
tailoring. An issue that immediately arises is the 
feasibility of this approach. At least for Chinese, 
the task is not too daunting, as the number of 
highly polysemous verbs is small. Our estimation 
based on a 250K-word chunk of the Chinese 
Treebank and a large electronic dictionary in our 
possession shows only 6% or 384 verb types 
having four or more definitions in the dictionary.  
Even for these verbs, the majority of them are 
not difficult to disambiguate, based on work by 
Dang et al (2002). Only a small number of these 
verbs truly need customized features. 
4 Related work 
There is a large body of literature on WSD and 
here we only discuss a few that are most relevant 
to our work. Dang and Palmer (2005) also use 
predicate-argument information as features in 
their work on English verbs, but their argument 
labels are not produced by an automatic SRL 
system. Rather, their semantic role labels are 
directly extracted from a human annotated 
926
corpus, the English Proposition Bank (Palmer et 
al, 2005), citing the inadequate accuracy of 
automatic semantic role labeling systems. In 
contrast, we used a fully antomated SRL system 
trained on the Chinese Propbank. Nevertheless, 
their results show, as ours do, that the use of 
semantic role labels as features improves the 
WSD accuracy of verbs.  
There are relatively few attempts to use 
linguistically motivated features for Chinese 
word sense disambiguation. Niu et al(2004) 
applied a Naive Bayesian model to Chinese 
WSD and experimented with different window 
sizes for extracting local and topical features and 
different types of local features (e.g., bigram 
templates, local words with position or parts-of-
speech information). One basic finding of their 
experiments is that simply increasing the window 
size for extracting local features or enriching the 
set of local features does not improve 
disambiguation performance. This is consistent 
with our usage of a small size window for 
extracting bigram collocation features. Li et al 
(2005) used sense-tagged true bigram 
collocations 2  as features. These features were 
obtained from a collocation extraction system 
that used lexical co-occurrence statistics to 
extract candidate collocations and then selected 
true collocations by using syntactic dependencies 
(Xu et al, 2003). In their experiments on 
Chinese nouns and verbs extracted from the 
People?s Daily News and the SENSEVAL3 data 
set,  the Naive Bayesian classifier using true 
collocation features generally performed better 
than that using simple bigram collocation 
features (i.e., bigram co-occurence features). It is 
worth noting that the true collocations overlap to 
a large degree with rich syntactic information 
used here such as the subject and direct object of 
a target verb. Therefore, their experiments show 
evidence that rich linguistic information benefits 
WSD on Chinese, consistent with our results. 
Our work is more closely related to the work 
of Dang et al(2002), who conducted 
experiments on 28 verbs and achieved an 
accuracy of 94.2%. However the high accuarcy is 
largely due to  the fact that their verbs are 
randomly chosen from the Chinese Treebank and 
some of them are not even polysemous (having a 
single sense). Extracting features from the gold 
                                                 
2 In their definition, a collocation is a recurrent and 
conventional fixed expression of words that holds 
syntactic and semantic relations. 
 
standard parses also contributed to the high 
accuracy, although not by much. For 5 of their 28 
verbs, their initial experimental results did not 
break the most frequent sense baseline. They 
annotated additional data on those five verbs and 
their system trained on this new data did 
outperfom the baseline. However, they 
concluded that the contribution of linguistic 
motivated features, such as features extracted 
from a syntactic parse, is insignificant, a finding 
they attributed to unique properties of Chinese 
given that the same syntactic features 
significantly improves the WSD accuracy. Our 
experimental results show that this conclusion is 
premature, without a detailed analysis of the 
senses for the individual verbs.  
5 Conclusion and future work 
We presented experiments with ten highly 
polysemous Chinese verbs and showed that a 
previous conclusion that rich linguistic features 
are not useful for the WSD of Chinese verbs is 
premature. We demonstrated that rich linguistic 
features, specifically features based on syntactic 
and semantic role information, are useful for the 
WSD of Chinese verbs. We believe that the 
WSD systems can benefit even more from rich 
linguistic features as the performance of other 
NLP tools such as parsers and Semantic Role 
Taggers improves. Our experimental results also 
lend support to the position that feature design 
for WSD should be linked tightly to the study of 
the criteria that sense distinctions are based on. 
This position calls for the customization of 
features for individual verbs based on 
understanding of the dimensions along which 
sense distinctions are made and a closer marriage 
between machine learning and linguistics. We 
believe this represents a rich area of exploration 
and we intend to experiment with more verbs 
with further customization of features, including 
experimenting with automatic feature selection. 
Acknowledgement 
This work was supported by National Science 
Foundation Grant NSF-0415923, Word Sense 
Disambiguation, the DTO-AQUAINT NBCHC-
040036 grant under the University of Illinois 
subcontract to University of Pennsylvania 2003-
07911-01 and the GALE program of the Defense 
Advanced Research Projects Agency, Contract 
No. HR0011-06-C-0022. Any opinions, findings, 
and conclusions or recommendations expressed 
in this material are those of the authors and do 
927
not necessarily reflect the views of the National 
Science Foundation, the DTO, or DARPA. 
References 
Avrim L. Blum and Pat Langley. 1997. Selection of 
relevant features and examples in machine learning. 
Artificial Intelligence, 97:245-271, 1997. 
Jinying Chen and Martha Palmer. 2004. Chinese Verb 
Sense Discrimination Using an EM Clustering 
Model with Rich Linguistic Features, In Proc. of 
the 42nd Annual meeting of the Assoication for 
Computational Linguistics, ACL-04. July 21-24, 
Barcelona, Spain 
Jinying Chen and Martha Palmer. 2005. Towards 
Robust High Performance Word Sense 
Disambiguation of English Verbs Using Rich 
Linguistic Features. In Proc. of the 2nd 
International Joint Conference on Natural 
Language Processing. Jeju Island, Korea, in press.    
Stanley. F. Chen and Ronald Rosenfeld. 1999. A 
Gaussian Prior for Smoothing Maximum Entropy 
Modals. Technical Report CMU-CS-99-108, CMU. 
Hoa T. Dang, Ching-yi Chia, Martha Palmer and Fu-
Dong Chiou. 2002. Simple Features for Chinese 
Word Sense Disambiguation. In Proceedings of 
COLING-2002, the Nineteenth Int. Conference on 
Computational Linguistics, Taipei, Aug.24?Sept.1. 
Hoa T. Dang. 2004. Investigations into the role of 
lexical semantics in word sense disambiguation.  
PhD Thesis. University of Pennsylvania.  
Hoa Dang and Martha Palmer. 2005. The role of 
semantic roles in  disambiguating verb senses. In 
Proceedings of ACL-05, Ann Arbor, Michigan. 
Zhendong Dong and Qiang Dong, HowNet. 1991. 
http://www.keenage.com. 
Christiane Fellbaum, ed. 1998. WordNet: An 
Electronic Lexical Database. Cambridge, MA: 
MIT Press. 
Veronique Hoste, Iris Hendrickx, Walter Daelemans, 
and Antal van den Bosch. 2002. Parameter 
optimization for machine-learning of word sense 
disambiguation. NLE, Special Issue on Word Sense 
Disambiguation Systems, 8(4):311?325.  
Eduard Hovy, Mtchchell Marcus, Martha Palmer, 
Lance Ramshaw and Ralph Weischedel. 2006. 
OntoNotes: the 90% solution. In Proceedings of the 
HLT-NAACL 2006, New York City. 
Wanyin Li, Qin Lu and Wenjie Li. 2005. Integrating 
Collocation Features in Chinese Word Sense 
Disambiguation. In Proceedings of the Fourth 
Sighan Workshop on Chinese Language Processing.  
pp: 87-94. Jeju, Korea.  
Andrew K. McCallum: MALLET: A Machine 
Learning for Language Toolkit.  http://www.cs. 
umass.edu/~mccallum/mallet (2002). 
Rada Mihalcea, Timothy Chklovski and Adam 
Kilgarriff. 2004. The Senseval-3 English lexical 
sample task. In Proceedings of Senseval-3: The 
Third International Workshop on the Evaluation of 
Systems for the Semantic Analysis of Text. 
Barcelona, Spain. July. 
Zheng-Yu Niu, Dong-Hong Ji and Chew Lim Tan, 
Optimizing Feature Set for Chinese Word Sense 
Disambiguation. 2004. In Proceedings of the 3rd 
International Workshop on the Evaluation of 
Systems for the Semantic Analysis of Text 
(SENSEVAL-3). Barcelona, Spain. 
Martha Palmer, Christiane Fellbaum, Scott Cotton, 
Lauren Delfs, and Hoa Trang Dang. 2001. English 
tasks: All-words and verb lexical sample. 
Proceedings of Senseval-2: Second International 
Workshop on Evaluating Word Sense 
Disambiguation Systems, Toulouse, France, 21-24. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles, Computational Linguistics, 
31(1): 71?106. 
Martha Palmer, Christiane Fellbaum and Hoa Trang 
Dang. (to appear, 2006). Making fine-grained and 
coarse-grained sense distinctions, both manually 
and automatically. Natural Language Engineering. 
Ruifeng Xu , Qin Lu, and Yin Li. 2003. An automatic 
Chinese Collocation Extraction Algorithm Based 
On Lexical Statistics. In Proceedings of the 
NLPKE Workshop. Beijing, China. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
Structure Annotation of a Large Corpus. Natural 
Language Engineering, 11(2):207-238. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
Propositions in the Penn Chinese Treebank, In 
Proceedings of the 2nd SIGHAN Workshop on 
Chinese Language Processing, in conjunction with 
ACL'03. Sapporo, Japan. 
Nianwen Xue and Martha Palmer. 2005. Automatic 
Semantic Role Labeling for Chinese Verbs. In 
Proceedings of the 19th International Joint 
Conference on Artificial Intelligence. Edinburgh, 
Scotland. 
David Yarowsky and Radu Florian. 2002. Evaluating 
sense disambiguation across diverse parameter 
spaces. Journal of Natural Language Engineering,  
8(4): 293?310. 
 
928
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 29?32,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Novel Semantic Features for Verb Sense Disambiguation 
Dmitriy Dligach 
The Center for Computational 
Language and Education  
Research 
1777 Exposition Drive 
Boulder, Colorado 80301  
Dmitriy.Dligach 
@colorado.edu 
Martha Palmer 
Department of Linguistics 
University of Colorado  
at Boulder 
295 UCB 
Boulder, Colorado 80309 
Martha.Palmer 
@colorado.edu 
 
 
Abstract 
We propose a novel method for extracting 
semantic information about a verb's arguments 
and apply it to Verb Sense Disambiguation 
(VSD). We contrast this method with two 
popular approaches to retrieving this informa-
tion and show that it improves the perform-
ance of our VSD system and outperforms the 
other two approaches  
1 Introduction 
The task of Verb Sense Disambiguation (VSD) 
consists in automatically assigning a sense to a 
verb (target verb) given its context. In a supervised 
setting, a VSD system is usually trained on a set of 
pre-labeled examples; the goal of this system is to 
tag unseen examples with a sense from some sense 
inventory. 
 
An automatic VSD system usually has at its 
disposal a diverse set of features among which the 
semantic features play an important role: verb 
sense distinctions often depend on the distinctions 
in the semantics of the target verb's arguments 
(Hanks, 1996). Therefore, some method of captur-
ing the semantic knowledge about the verb's argu-
ments is crucial to the success of a VSD system.  
 
The approaches to obtaining this kind of 
knowledge can be based on extracting it from elec-
tronic dictionaries such as WordNet (Fellbaum, 
1998), using Named Entity (NE) tags, or a combi-
nation of both (Chen, 2005). In this paper, we pro-
pose a novel method for obtaining semantic 
knowledge about words and show how it can be 
applied to VSD. We contrast this method with the 
other two approaches and compare their perform-
ances in a series of experiments.  
2 Lexical and Syntactic Features 
We view VSD as a supervised learning problem, 
solving which requires three groups of features: 
lexical, syntactic, and semantic. Lexical features 
include all open class words; we extract them from 
the target sentence and the two surrounding sen-
tences. We also use as features two words on the 
right and on the left of the target verb as well as 
their POS tags. We extract syntactic features from 
constituency parses; they indicate whether the tar-
get verb has a subject/object and what their head 
words and POS tags are, whether the target verb is 
in a passive or active form, whether the target verb 
has a subordinate clause, and whether the target 
verb has a PP adjunct. Additionally, we implement 
several new syntactic features, which have not 
been used in VSD before: the path through the 
parse tree from the target verb to the verb's argu-
ments and the subcategorization frame, as used in 
semantic role labeling. 
3 Semantic Features 
Consider the verb prepare for which our sense in-
ventory defines two senses: (1) to put together, 
assemble (e.g. He is going to prepare breakfast for 
the whole crowd ; I haven't prepared my lecture 
29
yet); (2) to make ready (e.g. She prepared the chil-
dren for school every morning). Knowing the se-
mantic class of the objects breakfast, lecture and 
children is the decisive factor in distinguishing the 
two senses and facilitates better generalization 
from the training data. One way to obtain this 
knowledge is from WordNet (WN) or from the 
output of a NE-tagger. However, both approaches 
suffer from the same limitation: they collapse mul-
tiple semantic properties of nouns into a finite 
number of predefined static classes. E.g., the most 
immediate hypernym of breakfast in WN is meal, 
while the most immediate hypernym of lecture is 
address, which makes these two nouns unrelated. 
Yet, breakfast and lecture are both social events 
which share some semantic properties: they both 
can be attended, hosted, delivered, given, held, 
organized etc. To discover these class-like descrip-
tions of nouns, one can observe which verbs take 
these nouns as objects. E.g. breakfast can serve as 
the object of serve, host, attend, and cook  which 
are all indicative of breakfast's semantic proper-
ties. 
 
Given a noun, we can dynamically retrieve 
other verbs that take that noun as an object from a 
dependency-parsed corpus; we call this kind of 
data Dynamic Dependency Neighbors  (DDNs) 
because it is obtained dynamically and based on 
the dependency relations in the neighborhood of 
the noun of interest. The top 501 DDNs can be 
viewed as a reliable inventory of semantic proper-
ties of the noun. To collect this data, we utilized 
two resources: (1) MaltParser (Nivre, 2007) ? a 
high-efficiency dependency parser; (2) English 
Gigaword ? a large corpus of 5.7M news articles. 
We preprocessed Gigaword with MaltParser, ex-
tracted all pairs of nouns and verbs that were 
parsed as participants of the object-verb relation, 
and counted the frequency of occurrence of all the 
unique pa irs. Finally, we indexed the resulting re-
cords of the form <frequency, verb, object> using 
the Lucene2 indexing engine. 
 
As an example, consider four nouns: dinner, 
breakfast, lecture, child. When used as the objects 
of prepare, the first three of them correspond to the 
instances of the sense 1 of prepare; the fourth one 
                                                                 
1 In future, we will try to optimize this parameter 
2 Available at http://lucene.apache.org/ 
corresponds to an instance of the sense 2. With the 
help of our index, we can retrieve their DDNs. 
There is a considerable overlap among the DDNs 
of the first three nouns and a much smaller overlap 
between child  and the first three nouns. E.g., din-
ner and breakfast have 34 DDNs in common, 
while dinner and child  only share 14. 
 
Once we have set up the framework for the ex-
traction of DDNs, the algorithm for applying them 
to VSD is straightforward: (1) find the noun object 
of the ambiguous verb (2) extract the DDNs for 
that noun (3) sort the DDNs by frequency and keep 
the top 50 (4) include these DDNs in the feature 
vector so that each of the extracted verbs becomes 
a separate feature. 
4 Relevant Work 
At the core of our work lies the notion of distrib u-
tional similarity (Harris, 1968), which states that 
similar words occur in similar contexts. In various 
sources, the notion of context ranges from bag-of-
words-like approaches to more structured ones in 
which syntax plays a role. Schutze (1998) used 
bag-of-words contexts for sense discrimination. 
Hindle (1990) grouped nouns into thesaurus-like 
lists based on the similarity of their syntactic con-
texts. Our approach is similar with the difference 
that we do not group noun arguments into finite 
categories, but instead leave the category bounda-
ries blurry and allow overlaps. 
 
The DDNs are essentially a form of world 
knowledge which we extract automatically and 
apply to VSD. Other researches attacked the prob-
lem of unsupervised extraction of world knowl-
edge: Schubert (2003) reports a method for 
extracting general facts about the world from tree-
banked Brown corpus. Lin and Pantel in (2001) 
describe their DIRT system for extraction of para-
phrase-like inference rules. 
5 Evaluation 
We selected a subset of the verbs annotated in the 
OntoNotes project (Chen, 2007) that had at least 
50 instances. The resulting data set consisted of 
46,577 instances of 217 verbs. The predominant 
sense baseline for this data is 68%. We used 
30
libsvm3 for classification. We computed the accu-
racy and error rate using 5-fold cross-validation.  
5.1 Experiments with a limited set of features 
The main objective of this experiment was to iso-
late the effect of the novel semantic features we 
proposed in this paper, i.e. the DDN features. To-
ward that goal, we stripped our system of all the 
features but the most essential ones to investigate 
whether the DDN features would have a clearly 
positive or negative impact on the system perform-
ance. Lexical features are the most essential to our 
system: a model that includes only the lexical fea-
tures achieves an accuracy of 80.22, while the ac-
curacy of our full-blown VSD system is 82.88%4. 
Since the DDN features have no effect when the 
object is not present, we identified 18,930 in-
stances where the target verb had an object (about 
41% of all instances) and used only them in the 
experiment. 
 
We built three models that included (1) the 
lexical features only (2) the lexical and the DDN 
features (3) the lexical and the object features. The 
object features consist of the head word of the NP 
object and the head word's POS tag. The object is 
included since extracting the DDN features re-
quires knowledge of the object; therefore the per-
formance of a model that only includes lexical 
features cannot be considered a fair baseline for 
studying the effect of the DDN features. Results 
are in Table 4. 
 
Features Included in 
Model 
Accuracy, % Error Rate, % 
Lexical 78.95 21.05 
Lexical + Object  79.34 20.66 
Lexical + DDN 82.40 17.60 
 
Table 4. Experiments with object instances 
 
As we see, the model that includes the DDN 
features performs more than 3 percentage points 
better than the model that only includes the object 
features (approximately 15% reduction in error 
rate). Also, based on the comparison of the per-
formance of the "lexical features only" and the 
"lexical + DDN" models, we can claim that the 
                                                                 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
4 Given this high baseline, we include error rate when report-
ing the results of the experiments as it is more informative 
knowledge of the DDNs provides richer semantic 
knowledge than just the knowledge of the object's 
head word. 
5.2 Integrating the DDN features into a full-
fledged VSD system 
The objective of this experiment was to investigate 
whether the DDN features improve the perform-
ance of a full-fledged VSD system. We built two 
models which consisted of (1) the entire set of fea-
tures (2) all the features of the first model exclud-
ing the DDN features. The entire data set (46K 
instances) participated in the experiment. Results 
are in Table 5. 
 
Features Included in 
Model 
Accuracy, % Error Rate, % 
All Features ? DDN 82.38 17.62 
All Features 82.88 17.12 
 
Table 5. Performance of the full-fledged VSD system 
 
The DDN features improved performance by 
0.5% (3% drop in error rate). The difference be-
tween the accuracies is statistically significant 
(p=0.05).   
5.3 Relative Contribution of Various Seman-
tic Fe atures 
The goal of this experiment was to study the rela-
tive contribution of various semantic features to 
the performance of our VSD system. We built five 
models each of which, in addition to the lexical 
and syntactic features, included only certain 
type(s) of semantic feature: (1) WN (2) NE (3) 
WN and NE (4) DDN (5) no semantic features 
(baseline). All 46K instances participated in the 
experiment. The results are shown in Table 6. 
 
Features Included in Model Accuracy, 
% 
Error Rate, 
% 
Lexical + Syntactic 81.82 18.18 
Lexical + Syntactic + WN 82.34 17.60 
Lexical + Syntactic + NE 82.01 17.99 
Lexical + Syntactic + WN + NE 82.38 17.62 
Lexical + Syntactic + DDN 82.97 17.03 
 
Table 6. Relative Contribution of Semantic Features 
 
The DDN features outperform the other two 
types of semantic features used separately and in 
conjunction. The difference in performance is sta-
tistically significant (p=0.05). 
31
6 Discussion and Conclusion 
As we saw, the novel semantic features we pro-
posed are beneficial to the task of VSD: they re-
sulted in a decrease in error rate from 3% to 15%, 
depending on the particular experiment. We also 
discovered that the DDN features contributed twice 
as much as the other two types of semantic features 
combined: adding the WN and NE features to the 
baseline resulted in about a 3% decrease in error 
rate, while adding the DDN features caused a more 
than 6% drop. 
 
Our results suggest that DDNs duplicate the ef-
fect of WN and NE: our system achieved the same 
performance when all three types of semantic fea-
tures were used and when we discarded WN and 
NE features and kept only the DDNs. This finding 
is important because such resources as WN and 
NE-taggers are domain and language specific 
while the DDNs have the advantage of being ob-
tainable from a large collection of texts in the do-
main or language of interest. Thus, the DDNs can 
become a crucial part of building a robust VSD 
system for a resource-poor domain or language, 
given a high-accuracy parser. 
7 Future Work 
In this paper we only experimented with verbs' 
objects, however the concept of DDNs can be eas-
ily extended to other arguments of the target verb. 
Also, we only utilized the object-verb relation in 
the dependency parses, but the range of potentially 
useful relations does not have to be limited only to 
it. Finally, we used as features the 50 most fre-
quent verbs that took the noun argument as an ob-
ject. However, the raw frequency is certainly not 
the only way to rank the verbs; we plan on explor-
ing other metrics such as Mutual Information. 
 
Acknowledgements 
 
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078, 
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No. 
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team.  Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not 
necessarily reflect the views of the National Sc i-
ence Foundation.  We also thank our colleagues 
Rodney Nielsen and Philipp Wetzler for parsing 
English Gigaword with MaltParser. 
 
References 
 
Jinying Chen, Dmitriy Dligach and Martha Palmer. 
2007. Towards Large-scale High-Performance Eng-
lish Verb Sense Disambiguation by Using Linguisti-
cally Motivated Features. In International 
Conference on Semantic Computing. Issue , 17-19. 
 
Jinying Chen and Martha Palmer. 2005. Towards Ro-
bust High Performance Word Sense Disambiguation 
of English Verbs Using Rich Linguistic Features. In 
Proceedings of the 2nd International Joint Confer-
ence on Natural Language Processing, Korea. 
 
Christiane Fellbaum. 1998. WordNet - an Electronic 
Lexical Database. The MIT Press, Cambridge, Mas-
sachusetts, London, UK. 
  
Patrick Hanks, 1996. Contextual Dependencies and 
Lexical Sets. In The Int. Journal of Corpus Linguis-
tics, 1:1 
 
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. New York. Wiley. 
 
Donald Hindle. 1990. Noun Classification from Predi-
cate-Argument Structures. In Proceedings of the 28th 
Annual Meeting of Association for Computational 
Linguistics. Pages 268-275 
 
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery 
of Inference Rules from Text. In Proceedings of 
ACM Conference on Knowledge Discovery and Data 
Mining. pp. 323-328. San Francisco, CA. 
 
Joakim Nivre, Johan Hall, Jens Nilsson, et. al. Malt -
Parser: A language-independent system for data-
driven dependency parsing. 2007. In Natural Lan-
guage Engineering, 13(2), 95-135. 
 
Lenhart Schubert and Matthew Tong, Extracting and 
evaluating general world knowledge from the Brown 
corpus. 2003. In Proc. of the HLT/NAACL Workshop 
on Text Meaning, May 31, Edmonton, Alberta, Can-
ada. 
 
Hinrich Schutze. 1998. Automatic Word Sense Dis-
crimination. In Computational Linguistics, 24(1):97-
123 
32
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 241?244,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extracting a Representation from Text for Semantic Analysis  Rodney D. Nielsen1,2, Wayne Ward1,2, James H. Martin1, and Martha Palmer1 1 Center for Computational Language and Education Research, University of Colorado, Boulder 2 Boulder Language Technologies, 2960 Center Green Ct., Boulder, CO 80301 Rodney.Nielsen, Wayne.Ward, James.Martin, Martha.Palmer@Colorado.edu       Abstract We present a novel fine-grained semantic rep-resentation of text and an approach to con-structing it. This representation is largely extractable by today?s technologies and facili-tates more detailed semantic analysis. We dis-cuss the requirements driving the representation, suggest how it might be of value in the automated tutoring domain, and provide evidence of its validity. 1 Introduction This paper presents a new semantic representation intended to allow more detailed assessment of stu-dent responses to questions from an intelligent tu-toring system (ITS). Assessment within current ITSs generally provides little more than an indica-tion that the student?s response expressed the target knowledge or it did not. Furthermore, virtually all ITSs are developed in a very domain-specific way, with each new question requiring the handcrafting of new semantic extraction frames, parsers, logic representations, or knowledge-based ontologies (c.f., Jordan et al, 2004). This is also true of re-search in the area of scoring constructed response questions (e.g., Leacock, 2004). The goal of the representation described here is to facilitate domain-independent assessment of student responses to questions in the context of a known reference answer and to perform this as-sessment at a level of detail that will enable more effective ITS dialog. We have two key criteria for this representation: 1) it must be at a level that fa-cilitates detailed assessment of the learner?s under-standing, indicating exactly where and in what manner the answer did not meet expectations and 
2) the representation and assessment should be learnable by an automated system ? they should not require the handcrafting of domain-specific representations of any kind.  Rather than have a single expressed versus un-expressed assessment of the reference answer as a whole, we instead break the reference answer down into what we consider to be approximately its lowest level compositional facets. This roughly translates to the set of triples composed of labeled (typed) dependencies in a dependency parse of the reference answer. Breaking the reference answer down into fine-grained facets permits a more fo-cused assessment of the student?s response, but a simple yes or no entailment at the facet level still lacks semantic expressiveness with regard to the relation between the student?s answer and the facet in question, (e.g., did the student contradict the facet or completely fail to address it?) Therefore, it is also necessary to break the annotation labels into finer levels in order to specify more clearly the relationship between the student?s answer and the reference answer facet. The emphasis of this paper is on this fine-grained facet-based representation ? considerations in defining it, the process of extract-ing it, and the benefit of using it. 2 Representing the Target Knowledge We acquired grade 3-6 responses to 287 questions from the Assessing Science Knowledge (ASK) project (Lawrence Hall of Science, 2006). The re-sponses, which range in length from moderately short verb phrases to several sentences, cover all 16 diverse Full Option Science System teaching and learning modules spanning life science, physi-cal science, earth and space science, scientific rea-soning, and technology. We generated a corpus by transcribing a random sample (approx. 15400) of the students? handwritten responses. 
241
2.1 Knowledge Representation The ASK assessments included a reference answer for each constructed response question. These ref-erence answers were manually decomposed into fine-grained facets, roughly extracted from the re-lations in a syntactic dependency parse and a shal-low semantic parse. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al, 2006). Figure 1 illustrates the process of deriving the constituent facets that comprise the representation of the final reference answer. We begin by deter-mining the dependency parse following the style of MaltParser (Nivre et al, 2006). This dependency parse was then modified in several ways. The ra-tionale for the modifications, which we elaborate below, is to increase the semantic content of facets. These more expressive facets are used later to gen-erate features for the assessment classification task. These types of modifications to the parser output address known limitations of current statistical parser outputs, and are reminiscent of the modifi-cations advocated by Briscoe and Carroll for more effective parser evaluation, (Briscoe, et. al, 2002). Example 1 illustrates the reference answer facets derived from the final dependencies in Figure 1, along with their glosses.  Figure 1. Reference answer representation revisions (1) The brass ring would not stick to the nail because the ring is not iron. (1a)  NMod(ring, brass)  (1a?) The ring is brass. (1b)  Theme_not(stick, ring) (1b?) The ring does not stick. (1c)  Destination_to_not(stick, nail) (1c?) Something does not stick to the nail. (1d)  Be_not(ring, iron) (1d?) The ring is not iron. (1e)  Cause_because(1b-c, 1d) (1e?) 1b and 1c are caused by 1d. Various linguistic theories take a different stance on what term should be the governor in a 
number of phrase types, particularly noun phrases. In this regard, the manual parses here varied from the style of MaltParser by raising lexical items to governor status when they contextually carried more significant semantics. In our example, the verb stick is made the governor of would, whose modifiers are reattached to stick. Similarly, the noun phrases the pattern of pigments and the bunch of leaves typically result in identical dependency parses. However, the word pattern is considered the governor of pigments; whereas, conversely the word leaves is treated as the governor of bunch because it carries more semantics. Then, terms that were not crucial to the student answer, frequently auxiliary verbs, were removed (e.g., the modal would and determiners in our example). Next, we incorporate prepositions into the de-pendency type labels following (Lin and Pantel, 2001). This results in the two dependencies vmod(stick, to) and pmod(to, nail), each of which carries little semantic value over its key lexical item, stick and nail, being combined into the sin-gle, more expressive dependency vmod_to(stick, nail), ultimately vmod is replaced with destination, as described below. Likewise, the dependencies connected by because are consolidated and be-cause is integrated into the new dependency type.  Next, copulas and a few similar verbs are also incorporated into the dependency types. The verb?s predicate is reattached to its subject, which be-comes the governor, and the dependency is labeled with the verb?s root. In our example, the two se-mantically impoverished dependencies sub(is, ring) and prd(is, iron) are combined to form the more meaningful dependency be(ring, iron). Then terms of negation are similarly incorporated into the dependency types. Finally, wherever a shallow semantic parse would identify a predicate argument structure, we used the thematic role labels in VerbNet (Kipper et al, 2000) between the predicate and the argu-ment?s headword, rather than the MaltParser de-pendency tags. This also involved adding new structural dependencies that a typical dependency parser would not generate. For example, in the sen-tence As it freezes the water will expand and crack the glass, typically the dependency between crack and its subject water is not generated since it would lead to a non-projective tree, but it does play the role of Agent in a semantic parse. In a small number of instances, these labels were also at-
242
tached to noun modifiers, most notably the Loca-tion label. For example, given the reference answer fragment The water on the floor had a much larger surface area, one of the facets extracted was Loca-tion_on(water, floor). We refer to facets that express relations between higher-level propositions as inter-propositional facets. An example of such a facet is (1e) above, connecting the proposition the brass ring did not stick to the nail to the proposition the ring is not iron. In addition to specifying the headwords of inter-propositional facets (stick and is, in 1e), we also note up to two key facets from each of the propositions that the relation is connecting (b, c, and d in example 1). Reference answer facets that are assumed to be understood by the learner a pri-ori, (e.g., because they are part of the question), are also annotated to indicate this.  There were a total of 2878 reference answer fac-ets, resulting in a mean of 10 facets per answer (median 8). Facets that were assumed to be under-stood a priori by students accounted for 33% of all facets and inter-propositional facets accounted for 11%. The results of automated annotation of stu-dent answers (section 3) focus on the facets that are not assumed to be understood a priori (67% of all facets); of these, 12% are inter-propositional.  A total of 36 different facet relation types were utilized. The majority, 21, are VerbNet thematic roles. Direction, Manner, and Purpose are Prop-Bank adjunctive argument labels (Palmer et al, 2005). Quantifier, Means, Cause-to-Know and copulas were added to the preceding roles. Finally, anything that did not fit into the above categories retained its dependency parse type: VMod (Verb Modifier), NMod (Noun Modifier), AMod (Adjec-tive or Adverb Modifier), and Root (Root was used when a single word in the answer, typically yes, no, agree, disagree, A-D, etc., stood alone without a significant relation to the remainder of the refer-ence answer; this occurred only 21 times, account-ing for fewer than 1% of the reference answer facets). The seven highest frequency relations are NMod, Theme, Cause, Be, Patient, AMod, and Location, which together account for 70% of the reference answer facet relations 2.2 Student Answer Annotation For each student answer, we annotated each reference answer facet to indicate whether and how 
the student addressed that facet. We settled on the five annotation categories in Table 1. These labels and the annotation process are detailed in (Nielsen et al, 2008b).  Understood: Reference answer facets directly ex-pressed or whose understanding is inferred Contradiction: Reference answer facets contradicted by negation, antonymous expressions, pragmatics, etc. Self-Contra: Reference answer facets that are both con-tradicted and implied (self contradictions) Diff-Arg: Reference answer facets whose core relation is expressed, but it has a different modifier or argument Unaddressed: Reference answer facets that are not ad-dressed at all by the student?s answer Table 1. Facet Annotation Labels 3 Automated Classification As partial validation of this knowledge representa-tion, we present results of an automatic assessment of our student answers. We start with the hand generated reference answer facets. We generate automatic parses for the reference answers and the student answers and automatically modify these parses to match our desired representation. Then for each reference answer facet, we extract features indicative of the student?s understanding of that facet. Finally, we train a machine learning classi-fier on training data and use it to classify unseen test examples, assigning a Table 1 label for each reference answer facet. We used a variety of linguistic features that as-sess the facets? similarity via lexical entailment probabilities following (Glickman et al, 2005), part of speech tags and lexical stem matches. They include information extracted from modified de-pendency parses such as relevant relation types and path edit distances. Revised dependency parses are used to align the terms and facet-level information for feature extraction. Remaining details can be found in (Nielsen et al, 2008a) and are not central to the semantic representation focus of this paper. Current classification accuracy, assigning a Table 1 label to each reference answer facet to indicate the student?s expressed understanding, is 79% within domain (assessing unseen answers to ques-tions associated with the training data) and 69% out of domain (assessing answers to questions re-garding entirely different science subjects). These results are 26% and 15% over the majority class baselines, respectively, and 21% and 6% over lexi-
243
cal entailment baselines based on Glickman et al (2005). 4 Discussion and Future Work Analyzing the results of reference facet extraction, there are many interesting open linguistic issues in this area. This includes the need for a more sophisticated treatment of adjectives, conjunctions, plurals and quantifiers, all of which are known to be beyond the abilities of state of the art parsers. Analyzing the dependency parses of 51 of the student answers, about 24% had errors that could easily lead to problems in assessment. Over half of these errors resulted from inopportune sentence segmentation due to run-on student sentences con-joined by and (e.g., the parse of a shorter string makes a higher pitch and a longer string makes a lower pitch, errantly conjoined a higher pitch and a longer string as the subject of makes a lower pitch, leaving a shorter string makes without an object). We are working on approaches to mitigate this problem.  In the long term, when the ITS generates its own questions and reference answers, the system will have to construct its own reference answer facets. The automatic construction of reference answer facets must deal with all of the issues described in this paper and is a significant area of future research. Other key areas of future research involve integrating the representation described here into an ITS and evaluating its impact. 5 Conclusion We presented a novel fine-grained semantic repre-sentation and evaluated it in the context of auto-mated tutoring. A significant contribution of this representation is that it will facilitate more precise tutor feedback, targeted to the specific facet of the reference answer and pertaining to the specific level of understanding expressed by the student. This representation could also be useful in areas such as question answering or document summari-zation, where a series of entailed facets could be composed to form a full answer or summary. The representation?s validity is partially demon-strated in the ability of annotators to reliably anno-tate inferences at this facet level, achieving substantial agreement (86%, Kappa=0.72) and by promising results in automatic assessment of stu-
dent answers at this facet level (up to 26% over baseline), particularly given that, in addition to the manual reference answer facet representation, an automatically extracted approximation of the rep-resentation was a key factor in the features utilized by the classifier.  The domain independent approach described here enables systems that can easily scale up to new content and learning environments, avoiding the need for lesson planners or technologists to create extensive new rules or classifiers for each new question the system must handle. This is an obligatory first step to the long-term goal of creat-ing ITSs that can truly engage children in natural unrestricted dialog, such as is required to perform high quality student directed Socratic tutoring. Acknowledgments This work was partially funded by Award Number 0551723 from the National Science Foundation. References  Briscoe, E., Carroll, J., Graham, J., and Copestake, A. 2002. Relational evaluation schemes. In Proc. of the Beyond PARSEVAL Workshop at LREC. Gildea, D. and Jurafsky, D. 2002. Automatic labeling of semantic roles. Computational Linguistics. Glickman, O, Dagan, I, and Koppel, M. 2005. Web Based Probabilistic Textual Entailment. In Proc RTE. Jordan, P, Makatchev, M, VanLehn, K. 2004. Combin-ing competing language understanding approaches in an intelligent tutoring system. In Proc ITS. Kipper, K, Dang, H, and Palmer, M. 2000. Class-Based Construction of a Verb Lexicon. In Proc. AAAI. Lawrence Hall of Science 2006. Assessing Science Knowledge (ASK), UC Berkeley, NSF-0242510 Leacock, C. 2004. Scoring free-response automatically: A case study of a large-scale Assessment. Examens. Lin, D & Pantel, P. 2001. Discovery of inference rules for Question Answering. In Natl. Lang. Engineering. Nielsen, R, Ward, W, and Martin, JH. 2008a. Learning to Assess Low-level Conceptual Understanding. In Proc. FLAIRS. Nielsen, R, Ward, W, Martin, JH and Palmer, P. 2008b. Annotating Students? Understanding of Science Con-cepts. In Proc. LREC. Nivre, J, Hall, J, Nilsson, J, Eryigit, G and Marinov, S. 2006. Labeled Pseudo-Projective Dependency Pars-ing with Support Vector Machines. In Proc. CoNLL. Palmer, M, Gildea, D, & Kingsbury, P. 2005. The proposition bank: An annotated corpus of semantic roles. In Computational Linguistics. 
244
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 70?77,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Issues in Synchronizing the English Treebank and PropBank 
 
Olga Babko-Malayaa, Ann Biesa, Ann Taylorb, Szuting Yia, Martha Palmerc,  
Mitch Marcusa, Seth Kulicka and Libin Shena 
aUniversity of Pennsylvania, bUniversity of York, cUniversity of Colorado 
{malayao,bies}@ldc.upenn.edu, {szuting,mitch,skulick,libin}@linc.cis.upenn.edu,
at9@york.ac.uk, Martha.Palmer@colorado.edu
 
Abstract 
The PropBank primarily adds semantic 
role labels to the syntactic constituents in 
the parsed trees of the Treebank. The 
goal is for automatic semantic role label-
ing to be able to use the domain of local-
ity of a predicate in order to find its ar-
guments. In principle, this is exactly what 
is wanted, but in practice the PropBank 
annotators often make choices that do not 
actually conform to the Treebank parses. 
As a result, the syntactic features ex-
tracted by automatic semantic role label-
ing systems are often inconsistent and 
contradictory. This paper discusses in de-
tail the types of mismatches between the 
syntactic bracketing and the semantic 
role labeling that can be found, and our 
plans for reconciling them. 
1 Introduction 
The PropBank corpus annotates the entire Penn 
Treebank with predicate argument structures by 
adding semantic role labels to the syntactic 
constituents of the Penn Treebank.  
Theoretically, it is straightforward for PropBank 
annotators to locate possible arguments based on 
the syntactic structure given by a parse tree, and 
mark the located constituent with its argument 
label. We would expect a one-to-one mapping 
between syntactic constituents and semantic 
arguments. However, in practice, PropBank 
annotators often make choices that do not 
actually conform to the Penn Treebank parses. 
The discrepancies between the PropBank and 
the Penn Treebank obstruct the study of the syn-
tax and semantics interface and pose an immedi-
ate problem to an automatic semantic role label-
ing system. A semantic role labeling system is 
trained on many syntactic features extracted from 
the parse trees, and the discrepancies make the 
training data inconsistent and contradictory. In 
this paper we discuss in detail the types of mis-
matches between the syntactic bracketing and the 
semantic role labeling that can be found, and our 
plans for reconciling them. We also investigate 
the sources of the disagreements, which types of 
disagreements can be resolved automatically, 
which types require manual adjudication, and for 
which types an agreement between syntactic and 
semantic representations cannot be reached. 
1.1 Treebank  
The Penn Treebank annotates text for syntactic 
structure, including syntactic argument structure 
and rough semantic information. Treebank anno-
tation involves two tasks: part-of-speech tagging 
and syntactic annotation. 
The first task is to provide a part-of-speech tag 
for every token. Particularly relevant for Prop-
Bank work, verbs in any form (active, passive, 
gerund, infinitive, etc.) are marked with a verbal 
part of speech (VBP, VBN, VBG, VB, etc.). 
(Marcus, et al 1993; Santorini 1990) 
The syntactic annotation task consists of 
marking constituent boundaries, inserting empty 
categories (traces of movement, PRO, pro), 
showing the relationships between constituents 
(argument/adjunct structures), and specifying a 
particular subset of adverbial roles. (Marcus, et 
al. 1994; Bies, et al 1995) 
Constituent boundaries are shown through 
syntactic node labels in the trees. In the simplest 
case, a node will contain an entire constituent, 
complete with any associated arguments or 
modifiers. However, in structures involving syn-
tactic movement, sub-constituents may be dis-
placed. In these cases, Treebank annotation 
represents the original position with a trace and 
shows the relationship as co-indexing. In (1) be-
low, for example, the direct object of entail is 
shown with the trace *T*, which is coindexed to 
the WHNP node of the question word what. 
 
(1) (SBARQ (WHNP-1 (WP What ))
(SQ (VBZ does )
(NP-SBJ (JJ industrial )
(NN emigration ))
(VP (VB entail)
(NP *T*-1)))
(. ?))
70
In (2), the relative clause modifying a journal-
ist has been separated from that NP by the prepo-
sitional phrase to al Riyadh, which is an argu-
ment of the verb sent. The position where the 
relative clause originated or ?belongs? is shown 
by the trace *ICH*, which is coindexed to the 
SBAR node containing the relative clause con-
stituent. 
 
(2)(S (NP-SBJ You)  
(VP sent
(NP (NP a journalist)
(SBAR *ICH*-2))
(PP-DIR to
(NP al Riyadh))
(SBAR-2
(WHNP-3 who)
(S (NP-SBJ *T*-3)
(VP served
(NP (NP the name)
(PP of
(NP Lebanon)))
(ADVP-MNR
magnificently))))))
 
Empty subjects which are not traces of move-
ment, such as PRO and pro, are shown as * (see 
the null subject of the infinite clause in (4) be-
low). These null subjects are coindexed with a 
governing NP if the syntax allows. The null sub-
ject of an infinitive clause complement to a noun 
is, however, not coindexed with another node in 
the tree in the syntax. This coindexing is shown 
as a semantic coindexing in the PropBank anno-
tation. 
The distinction between syntactic arguments 
and adjuncts of the verb or verb phrase is made 
through the use of functional dashtags rather than 
with a structural difference. Both arguments and 
adjuncts are children of the VP node. No distinc-
tion is made between VP-level modification and 
S-level modification. All constituents that appear 
before the verb are children of S and sisters of 
VP; all constituents that appear after the verb are 
children of VP.  
Syntactic arguments of the verb are NP-SBJ, 
NP (no dashtag), SBAR (either ?NOM-SBJ or no 
dashtag), S (either ?NOM-SBJ or no dashtag),  
-DTV, -CLR (closely/clearly related), -DIR with 
directional verbs. 
Adjuncts or modifiers of the verb or sentence 
are any constituent with any other adverbial 
dashtag, PP (no dashtag), ADVP (no dashtag). 
Adverbial constituents are marked with a more 
specific functional dashtag if they belong to one 
of the more specific types in the annotation sys-
tem (temporal ?TMP, locative ?LOC, manner  
?MNR, purpose ?PRP, etc.). 
Inside NPs, the argument/adjunct distinction is 
shown structurally. Argument constituents (S and 
SBAR only) are children of NP, sister to the head 
noun. Adjunct constituents are sister to the NP 
that contains the head noun, child of the NP that 
contains both:  
 
(NP (NP head)
(PP adjunct)) 
1.2 PropBank   
PropBank is an annotation of predicate-argument 
structures on top of syntactically parsed, or Tree-
banked, structures. (Palmer, et al 2005; Babko-
Malaya, 2005). More specifically, PropBank 
annotation involves three tasks: argument 
labeling, annotation of modifiers, and creating 
co-reference chains for empty categories.  
The first goal is to provide consistent argu-
ment labels across different syntactic realizations 
of the same verb, as in   
 
(3) [ARG0 John] broke [ARG1 the window]   
 [ARG1 The window] broke.  
 
As this example shows, semantic arguments 
are tagged with numbered argument labels, such 
as Arg0, Arg1, Arg2, where these labels are de-
fined on a verb by verb basis.  
The second task of the PropBank annotation 
involves assigning functional tags to all modifi-
ers of the verb, such as MNR (manner), LOC 
(locative), TMP (temporal), DIS (discourse con-
nectives), PRP (purpose) or DIR (direction) and 
others. 
And, finally, PropBank annotation involves 
finding antecedents for ?empty? arguments of the 
verbs, as in (4). The subject of the verb leave in 
this example is represented as an empty category 
[*] in Treebank. In PropBank, all empty catego-
ries which could be co-referred with a NP within 
the same sentence are linked in ?co-reference? 
chains:  
 
(4) I made a decision [*] to leave 
 
Rel:    leave,   
Arg0: [*] -> I 
 
As the following sections show, all three tasks 
of PropBank annotation result in structures 
which differ in certain respects from the corre-
sponding Treebank structures. Section 2 presents 
71
our approach to reconciling the differences be-
tween Treebank and PropBank with respect to 
the third task, which links empty categories with 
their antecedents. Section 3 introduces mis-
matches between syntactic constituency in Tree-
bank and PropBank. Mismatches between modi-
fier labels are not addressed in this paper and are 
left for future work. 
2 Coreference and syntactic chains  
PropBank chains include all syntactic chains 
(represented in the Treebank) plus other cases of 
nominal semantic coreference, including those  
in which the coreferring NP is not a syntactic 
antecedent. For example, according to PropBank 
guidelines, if a trace is coindexed with a NP in 
Treebank, then the chain should be reconstructed: 
 
(5) What-1 do you like [*T*-1]? 
 
Original PropBank annotation: 
Rel: like 
Arg0: you 
Arg1: [*T*] -> What 
 
Such chains usually include traces of A and A? 
movement and PRO for subject and object con-
trol. On the other hand, not all instances of PROs 
have syntactic antecedents. As the following ex-
ample illustrates, subjects of infinitival verbs and 
gerunds might have antecedents within the same 
sentence, which cannot be linked as a syntactic 
chain. 
 
(6) On the issue of abortion , Marshall Coleman 
wants  to take away your right  [*] to choose 
and give it to the politicians .  
 
ARG0:          [*] -> your 
REL:           choose 
 
Given that the goal of PropBank is to find all 
semantic arguments of the verbs, the links be-
tween empty categories and their coreferring NPs 
are important, independent of whether they are 
syntactically coindexed or not. In order to recon-
cile the differences between Treebank and Prop-
Bank annotations, we decided to revise Prop-
Bank annotation and view it as a 3 stage process. 
First, PropBank annotators should not recon-
struct syntactic chains, but rather tag empty cate-
gories as arguments. For example, under the new 
approach annotators would simply tag the trace 
as the Arg1 argument in (7): 
(7) What-1 do you like [*T*-1]? 
 
Revised PropBank annotation: 
Rel: like 
Arg0: you 
Arg1: [*T*]  
  
As the second stage, syntactic chains will be re-
constructed automatically, based on the 
coindexation provided by Treebank (note that the 
trace is coindexed with the NP What in (7)). And, 
finally, coreference annotation will be done on 
top of the resulting resource, with the goal of 
finding antecedents for the remaining empty 
categories, including empty subjects of infinitival 
verbs and gerunds.   
One of the advantages of this approach is that 
it allows us to distinguish different types of 
chains, such as syntactic chains (i.e., chains 
which are derived as the result of syntactic 
movement, or control coreference), direct 
coreference chains (as illustrated by the example 
in (6)), and semantic type links for other ?indi-
rect? types of links between an empty category 
and its antecedent.  
Syntactic chains are annotated in Treebank, 
and are reconstructed automatically in PropBank. 
The annotation of direct coreference chains is 
done manually on top of Treebank, and is re-
stricted to empty categories that are not 
coindexed with any NP in Treebank. And, finally, 
as we show next, a semantic type link is used for 
relative clauses and a coindex link for verbs of 
saying. 
A semantic type link is used when the antece-
dent and the empty category do not refer to the 
same entity, but do have a certain kind of rela-
tionship. For example, consider the relative 
clause in (8):  
 
(8) Answers that we?d like to have 
 
Treebank annotation: 
(NP (NP answers)
(SBAR (WHNP-6 which)
(S (NP-SBJ-3 we)
(VP 'd
(VP like
(S (NP-SBJ *-3)
(VP to
(VP have
(NP *T*-6)
))))))))
 
In Treebank, the object of the verb have is a trace, 
which is coindexed with the relative pronoun. In 
72
the original PropBank annotation, a further link 
is provided, which specifies the relative pronoun 
as being of ?semantic type? answers.  
 
(9) Original PropBank annotation: 
Arg1:    [NP *T*-6] -> which -> answers 
 rel:         have 
 Arg0:     [NP-SBJ *-3] -> we 
 
This additional link between which and answers 
is important for many applications that make use 
of preferences for semantic types of verb argu-
ments, such as Word Sense Disambiguation 
(Chen & Palmer 2005). In the new annotation 
scheme, annotators will first label traces as ar-
guments: 
 
(10) Revised PropBank annotation (stage 1): 
Rel:  have 
Arg1: [*T*-6]  
Arg0: [NP-SBJ *-3] 
 
As the next stage, the trace [*T*-6] will be 
linked to the relative pronoun automatically (in 
addition to the chain [NP-SBJ *-3] -> we being 
automatically reconstructed). As the third stage, 
PropBank annotators will link which to answers. 
However, this chain will be labeled as a ?seman-
tic type? to distinguish it from direct coreference 
chains and to indicate that there is no identity 
relation between the coindexed elements. 
Verbs of saying illustrate another case of links 
rather than coreference chains. In many sen-
tences with direct speech, the clause which intro-
duces a verb of saying is ?embedded? into the 
utterance. Syntactically this presents a problem 
for both Treebank and Propbank annotation. In 
Treebank, the original annotation style required a 
trace coindexed to the highest S node as the ar-
gument of the verb of saying, indicating syntactic 
movement. 
 
(11) Among other things, they said  [*T*-1] , Mr. 
Azoff would develop musical acts for a new 
record label . 
 
Treebank annotation: 
(S-1 (PP Among
(NP other things))
(PRN ,
(S (NP-SBJ they)
(VP said
(SBAR 0
(S *T*-1))))
,)
(NP-SBJ Mr. Azoff)
(VP would
(VP develop
(NP (NP musical acts)
(PP for
(NP a new record
label)))))
.)
In PropBank, the different pieces of the utterance, 
including the trace under the verb said, were 
concatenated 
 
(12) Original PropBank annotation: 
ARG1:      [ Among other things] [ Mr. 
Azoff] [ would develop musical acts for a 
new record label] [ [*T*-1]] 
ARG0:       they 
rel:        said 
 
Under the new approach, in stage one, Tree-
bank annotation will introduce not a trace of the 
S clause, but rather *?*, an empty category indi-
cating ellipsis. In stage three, PropBank annota-
tors will link this null element to the S node, but 
the resulting chain will not be viewed as  ?direct? 
coreference. A special tag will be used for this 
link, in order to distinguish it from other types of 
chains. 
 
(13) Revised PropBank  annotation: 
ARG1:      [*?*] (-> S) 
ARG0:       they 
rel:        said 
3 Differences in syntactic constituency  
3.1 Extractions of mismatches between 
PropBank and Treebank 
In order to make the necessary changes to both 
the Treebank and the PropBank, we have to first 
find all instances of mismatches. We have used 
two methods to do this: 1) examining the argu-
ment locations; 2) examining the discontinuous 
arguments. 
 
Argument Locations  In a parse tree which ex-
presses the syntactic structure of a sentence, a 
semantic argument occupies specific syntactic 
locations: it appears in a subject position, a verb 
complement location or an adjunct location. 
Relative to the predicate, its argument is either a 
sister node, or a sister node of the predicate?s 
ancestor. We extracted cases of PropBank argu-
ments which do not attach to the predicate spine, 
and filtered out VP coordination cases. For ex-
ample, the following case is a problematic one 
because the argument PP node is embedded too 
73
deeply in an NP node and hence it cannot find a 
connection with the main predicate verb lifted. 
This is an example of a PropBank annotation 
error. 
 
(14) (VP (VBD[rel] lifted) 
(NP us) )
(NP-EXT
(NP a good 12-inches)
(PP-LOC[ARGM-LOC] above
(NP the water level))))
 
However, the following case is not problem-
atic because we consider the ArgM PP to be a 
sister node of the predicate verb given the VP 
coordination structure:  
 
(15) (VP (VP (VB[rel] buy)  
(NP the basket of ? )
(PP in whichever market ?))
(CC and)
(VP (VBP sell)
(NP them)
(PP[ARGM] in the more
expensive market)))
 
Discontinuous Arguments happen when Prop-
Bank annotators need to concatenate several 
Treebank constituents to form an argument.  Dis-
continuous arguments often represent different 
opinions between PropBank and Treebank anno-
tators regarding the interpretations of the sen-
tence structure. 
For example, in the following case, the Prop-
Bank concatenates the NP and the PP to be the 
Arg1. In this case, the disagreement on PP at-
tachment is simply a Treebank annotation error. 
 
(16) The region lacks necessary mechanisms for 
handling the aid and accounting items. 
 
Treebank annotation: 
(VP lacks
(NP necessary mechanisms)
(PP for
(NP handing the aid?)))
 
PropBank annotation: 
REL: lacks 
Arg1: [NP necessary mechanisms][PP for 
handling the aid and accounting items] 
 
All of these examples have been classified into 
the following categories: (1) attachment ambi-
guities, (2) different policy decisions, and (3) 
cases where one-to-one mapping cannot be pre-
served. 
3.2 Attachment ambiguities  
Many cases of mismatches between Treebank 
and PropBank constituents are the result of am-
biguous interpretations. The most common ex-
amples are cases of modifier attachment ambi-
guities, including PP attachment. In cases of am-
biguous interpretations, we are trying to separate 
cases which can be resolved automatically from 
those which require manual adjudication. 
 
PP-Attachment  The most typical case of PP 
attachment annotation disagreement is shown in 
(17).  
 
(17) She wrote a letter for Mary. 
 
Treebank annotation: 
(VP wrote
(NP (NP a letter)
(PP for
(NP Mary))))
 
PropBank annotation: 
REL: write 
Arg1: a letter 
Arg2: for Mary 
 
In (17), the PP ?for Mary? is attached to the 
verb in PropBank and to the NP in Treebank. 
This disagreement may have been influenced by 
the set of roles of the verb ?write?, which in-
cludes a beneficiary as its argument.  
 
(18) Frameset write:  Arg0: writer 
   Arg1: thing written 
   Arg2: beneficiary 
 
Examples of this type cannot be automatically 
resolved and require manual adjudication. 
Adverb Attachment  Some cases of modifier 
attachment ambiguities, on the other hand, could 
be automatically resolved. Many cases of mis-
matches are of the type shown in (19), where a 
directional adverbial follows the verb. In Tree-
bank, this adverbial is analyzed as part of an 
ADVP which is the argument of the verb in 
question. However, in PropBank, it is annotated 
as a separate ArgM-DIR.  
(19) Everything is going back to Korea or Japan. 
 
 
74
Treebank annotation:  
(S (NP-SBJ (NN Everything) )
(VP (VBZ is)
(VP (VBG[rel] going)
(ADVP-DIR
(RB[ARGM-DIR] back)
(PP[ARG2] (TO to)
(NP (NNP Korea)
(CC and)
(NNP Japan)
))))) (. .))
 
Original PropBank annotation: 
Rel: going 
ArgM-DIR: back 
Arg2: to Korea or Japan 
 
For examples of this type, we have decided to 
automatically reconcile PropBank annotations to 
be consistent with Treebank, as shown in (20). 
 
(20) Revised PropBank annotation: 
Rel:  going 
Arg2: back to Korea or Japan 
3.3 Sentential complements 
Another area of significant mismatch between 
Treebank and PropBank annotation involves sen-
tential complements, both infinitival clauses and 
small clauses. In general, Treebank annotation 
allows many more verbs to take sentential com-
plements than PropBank annotation. 
For example, the Treebank annotation of the 
sentence in (21) gives the verb keep a sentential 
complement which has their markets active un-
der the S as the subject of the complement clause. 
PropBank annotation, on the other hand, does not 
mark the clause but rather labels each subcon-
stituent as a separate argument. 
 
(21)  ?keep their markets active 
 
Treebank annotation: 
(VP keep
(S (NP-SBJ their markets)
(ADJP-PRD active)))
 
PropBank annotation: 
REL: keep 
Arg1: their markets 
Arg2: active 
 
In Propbank, an important criterion for decid-
ing whether a verb takes an S argument, or de-
composes it into two arguments (usually tagged 
as Arg1 and Arg2) is based on the semantic in-
terpretation of the argument, e.g. whether the 
argument can be interpreted as an event or pro-
position. 
For example, causative verbs (e.g. make, get), 
verbs of perception (see, hear), and intensional 
verbs (want, need, believe), among others, are 
analyzed as taking an S clause, which is inter-
preted as an event in the case of causative verbs 
and verbs of perception, and as a proposition in 
the case of intensional verbs. On the other hand, 
?label? verbs (name, call, entitle, label, etc.), do 
not select for an event or proposition and are 
analyzed as having 3 arguments: Arg0, Arg1, 
and Arg2. 
Treebank criteria for distinguishing arguments, 
on the other hand, were based on syntactic 
considerations, which did not always match with 
Propbank. For example, in Treebank, evidence of 
the syntactic category of argument that a verb 
can take is used as part of the decision process 
about whether to allow the verb to take a small 
clause. Verbs that take finite or non-finite (verbal) 
clausal arguments, are also treated as taking 
small clauses. The verb find takes a finite clausal 
complement as in We found that the book was 
important and also a non-finite clausal comple-
ment as in We found the book to be important. 
Therefore, find is also treated as taking a small 
clause complement as in We found the book 
important.  
 
(22) (S (NP-SBJ We) 
(VP found
(S (NP-SBJ the book)
(ADJP-PRD important))))
 
The obligatory nature of the secondary predi-
cate in this construction also informed the deci-
sion to use a small clause with a verb like find. In 
(22), for example, important is an obligatory part 
of the sentence, and removing it makes the sen-
tence ungrammatical with this sense of find (?We 
found the book? can only be grammatical with a 
different sense of find, essentially ?We located 
the book?). 
With verbs that take infinitival clausal com-
plements, however, the distinction between a 
single S argument and an NP object together 
with an S argument is more difficult to make. 
The original Treebank policy was to follow the 
criteria and the list of verbs taking both an NP 
object and an infinitival S argument given in 
Quirk, et al (1985).  
Resultative constructions are frequently a 
source of mismatch between Treebank annota-
75
tion as a small clause and PropBank annotation 
with Arg1 and Arg2. Treebank treated a number 
of resultative as small clauses, although certain 
verbs received resultative structure annotation, 
such as the one in (23). 
 
(23) (S (NP-SBJ They) 
(VP painted
(NP-1 the apartment)
(S-CLR (NP-SBJ *-1)
(ADJP-PRD orange))))
 
In all the mismatches in the area of sentential 
complementation, Treebank policy tends to 
overgeneralize S-clauses, whereas Propbank 
leans toward breaking down clauses into separate 
arguments.  
This type of mismatch is being resolved on a 
verb-by-verb basis. Propbank will reanalyze 
some of the verbs (like consider and find), which 
have been analyzed as having 3 arguments, as 
taking an S argument. Treebank, on the other 
hand, will change the analysis of label verbs like 
call, from a small clause analysis to a structure 
with two complements. 
Our proposed structure for label verbs, for ex-
ample, is in (24). 
 
(24) (S (NP-SBJ[Arg0] his parents) 
(VP (VBD called)
(NP-1[Arg1] him)
(S-CLR[Arg2]
(NP-SBJ *-1)
(NP-PRD John))))
 
This structure will accommodate both Treebank 
and PropBank requirements for label verbs. 
4 Where Syntax and Semantics do not 
match  
Finally, there are some examples where the dif-
ferences seem to be impossible to resolve with-
out sacrificing some important features of Prop-
Bank or Treebank annotation. 
4.1 Phrasal verbs   
PropBank has around 550 phrasal verbs like 
keep up, touch on, used to and others, which are 
analyzed as separate predicates in PropBank. 
These verbs have their own set of semantic roles, 
which is different from the set of roles of the cor-
responding ?non-phrasal? verbs, and therefore 
they require a separate PropBank entry. In Tree-
bank, on the other hand, phrasal verbs are not 
distinguished. If the second part of the phrasal 
verb is labeled as a verb+particle combination in 
the Treebank, the PropBank annotators concate-
nate it with the verb as the REL. If Treebank la-
bels the second part of the ?phrasal verb? as part 
of a prepositional phrase, there is no way to re-
solve the inconsistency.   
 
(25) But Japanese institutional investors are used 
to quarterly or semiannual payments on their in-
vestments, so ?  
 
Treebank annotation: 
(VBN used)
(PP (TO to)
(NP quarterly or ?
on their investments))
 
PropBank annotation: 
      Arg1: quarterly or ? on their investments 
 Rel: used to (?used to? is a separate predi-
cate in PropBank) 
4.2 Conjunction  
In PropBank, conjoined NPs and clauses are 
usually analyzed as one argument, parallel to 
Treebank. For example, in John and Mary came, 
the NP John and Mary is a constituent in Tree-
bank and it is also marked as Arg0 in PropBank. 
However, there are a few cases where one of the 
conjuncts is modified, and PropBank policy is to 
mark these modifiers as ArgMs. For example, in 
the following NP, the temporal ArgM now modi-
fies a verb, but it only applies to the second con-
junct.  
 
(26) 
(NP (NNP Richard)
(NNP Thornburgh) )
(, ,)
(SBAR
(WHNP-164 (WP who))
(S
(NP-SBJ-1 (-NONE- *T*-164))
(VP
(VBD went)
(PRT (RP on) )
(S
(NP-SBJ (-NONE- *-1))
(VP (TO to)
(VP (VB[rel] become)
(NP-PRD
(NP[ARG2]
(NP (NN governor))
(PP (IN of)
(NP
(NNP
Pennsylvania))))
76
(CC and)
(PRN (, ,)
(ADVP-TMP (RB now))
(, ,) )
(NP[ARG2] (NNP U.S.)
(NNP Attorney)
(NNP General))
)))))))
 
In PropBank, cases like this can be decom-
posed into two propositions: 
 
(27) Prop1:      rel: become    
                Arg1: attorney general         
                Arg0: [-NONE- *-1]                       
         
   Prop2: rel:  become    
  ArgM-TMP: now   
  Arg0: [-NONE- *-1] 
Arg1: a governor               
 
In Treebank, the conjoined NP is necessarily 
analyzed as one constituent. In order to maintain 
the one-to-one mapping between PropBank and 
Treebank, PropBank annotation would have to 
be revised in order to allow the sentence to have 
one proposition with a conjoined phrase as an 
argument. Fortunately, these types of cases do 
not occur frequently in the corpus. 
4.3 Gapping 
Another place where the one-to-one mapping 
is difficult to preserve is with gapping construc-
tions. Treebank annotation does not annotate the 
gap, given that gaps might correspond to differ-
ent syntactic categories or may not even be a 
constituent. The policy of Treebank, therefore, is 
simply to provide a coindexation link between 
the corresponding constituents:  
 
(28) Mary-1 likes chocolates-2 and  
 Jane=1 ? flowers=2 
 
This policy obviously presents a problem for 
one-to-one mapping, since Propbank annotators 
tag Jane and flowers as the arguments of an im-
plied second likes relation, which is not present 
in the sentence. 
5 Summary 
In this paper we have considered several types 
of mismatches between the annotations of the 
English Treebank and the PropBank: coreference 
and syntactic chains, differences in syntactic 
constituency, and cases in which syntax and se-
mantics do not match. We have found that for the 
most part, such mismatches arise because Tree-
bank decisions are based primarily  on syntactic 
considerations while PropBank decisions give 
more weight  to semantic representation.. 
In order to reconcile these differences we have 
revised the annotation policies of both the Prop-
Bank and Treebank in appropriate ways. A 
fourth source of mismatches is simply annotation 
error in either the Treebank or PropBank. Look-
ing at the mismatches in general has allowed us 
to find these errors, and will facilitate their cor-
rection.  
References 
Olga Babko-Malaya. 2005. PropBank Annotation 
Guidelines. http://www.cis.upenn.edu/~mpalmer/ 
project_pages/PBguidelines.pdf 
Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-
Intyre. 1995. Bracketing Guidelines for Treebank 
II Style. Penn Treebank Project, University of 
Pennsylvania, Department of Computer and Infor-
mation Science Technical Report MS-CIS-95-06. 
Jinying Chen and Martha Palmer. 2005. Towards Ro-
bust High Performance Word Sense Disambigua-
tion of English Verbs Using Rich Linguistic Fea-
tures. In Proceedings of the 2nd International Joint 
Conference on Natural Language Processing, 
IJCNLP2005, pp. 933-944. Oct. 11-13, Jeju Island, 
Republic of Korea. 
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz & B. Schas-
berger, 1994. The Penn Treebank: Annotating 
predicate argument structure. Proceedings of the 
Human Language Technology Workshop, San 
Francisco. 
M. Marcus, B. Santorini and M.A. Marcinkiewicz, 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics. 
Martha Palmer, Dan Gildea, and Paul Kingsbury. 
2005. The proposition bank: An annotated corpus 
of semantic roles. Computational Linguistics, 
31(1). 
R. Quirk, S. Greenbaum, G. Leech and J. Svartvik. 
1985. A Comprehensive Grammar of the English 
Language. Longman, London. 
B. Santorini. 1990. Part-of-speech tagging guidelines 
for the Penn Treebank Project. University of Penn-
sylvania, Department of Computer and Information 
Science Technical Report MS-CIS-90-47. 
77
Proceedings of the Linguistic Annotation Workshop, pages 49?52,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
Criteria for the Manual Grouping of Verb Senses  Cecily Jill Duffield, Jena D. Hwang, Susan Windisch Brown,  Dmitriy Dligach, Sarah E.Vieweg, Jenny Davis, Martha Palmer Departments of Linguistics and Computer Science University of Colorado Boulder, C0 80039-0295, USA {cecily.duffield, hwangd, susan.brown, dmitry.dligach,  sarah.vieweg, jennifer.davis, martha.palmer}@colorado.edu  
Abstract  In this paper, we argue that clustering WordNet senses into more coarse-grained groupings results in higher inter-annotator agreement and increased system performance. Clustering of verb senses involves examining syntactic and semantic features of verbs and arguments on a case-by-case basis rather than applying a strict methodology. Determining appropriate criteria for clustering is based primarily on the needs of annotators.  1  Credits  We gratefully acknowledge the support of the National Science Foundation Grant NSF-0415923, Word Sense Disambiguation, and the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.  2  Introduction  Word sense ambiguity poses significant obstacles to accurate and efficient information extraction and automatic translation. Successful disambiguation of polysemous words in NLP applications depends on determining an appropriate level of granularity of sense distinctions, perhaps more so for distinguishing between multiple senses of verbs than for any other grammatical category. WordNet, an important and widely used lexical resource, uses fine-grained distinctions that provide subtle information about the particular usages of various 
lexical items (Felbaum, 1998). When used as a resource for annotation of various genres of text, this fine level of granularity has not been conducive to high rates of inter-annotator agreement (ITA) or high automatic tagging performance. Annotation of verb senses as described by coarse-grained Proposition Bank framesets may result in higher ITA scores, but the blurring of distinctions between verb senses with similar argument structures may fail to alleviate the problems posed by ambiguity. Our goal in this project is to create verb sense distinctions at a middle level of granularity that allow us to capture as much information as possible from a lexical item while still attaining high ITA scores and high system performance in automatic sense disambiguation. We have demonstrated that clear sense distinctions improve annotator productivity and accuracy. System performance typically lags around 10% behind ITA rates. ITA scores of at least 90% for a majority of our sense-groupings result in the expected corresponding improvement in system performance. Training on this new data, Chen et al, (2006) report 86.7% accuracy for verbs using a smoothed maximum entropy model and rich linguistic features. (Also Semeval071) They also report state-of-the-art performance on fine-grained senses, but the results are more than 16% lower. We begin by describing the overall process.  3  The Grouping and Annotation Process  The process for building our database with the appropriate level of verb sense distinctions                                                 1 Task 17,  http://nlp.cs.swarthmore.edu/semeval/.  
49
  
involves two steps: sense grouping and annotation (Figure 1). During our sense grouping process, linguists (henceforth, ?groupers?) cluster fine-grained sense distinctions listed in WordNet 2.1 into more coarse-grained groupings. These rough clusters of WordNet entries are based on speaker intuition. Other resources, including PropBank, VerbNet (based on Levin?s verb classes (Levin, 1993)), and online dictionaries are consulted in further refining the distinctions between senses (Palmer, et. al., 2005, Kipper et al, 2006). To aid annotators in understanding the distinctions, sense groupings are ordered according to saliency and frequency. Detailed information, including syntactic frames and semantic features, is provided as commentary for the groupings. We also provide the annotators with simple example sentences from WordNet as well as syntactically complex and ambiguous attested usages from Google search results. These examples are intended to guide annotators faced with similar challenges in the data to be tagged.  Completed verb sense groupings are sent through sample-annotation and tagged by two annotators. Groupings that receive an ITA score of 90% or above are then used to annotate all instances of that verb in our corpora in actual-annotation. Groupings that receive less than 90% ITA scores are regrouped (Hovy et al, 2006). Revisions are made based on a second grouper?s evaluation of the original grouping, as well as patterns of annotator disagreement. Verb groupings receiving ITA scores of 85% or above are sent through actual-annotation. Verbs scoring below 85% are regrouped by a third grouper, and in some cases, by the entire grouping team. It is sometimes impossible to get ITA scores over 85% for high   
frequency verbs that also have high entropy. These have to be carefully adjudicated to produce a gold standard. Revised verbs are then evaluated and either deemed ready for actual-annotation or are sent for a third and final round of sample-annotation. Verbs subject to the re-annotation process are tagged by different annotators. Data from actual-annotation is examined by an adjudicator who resolves remaining disagreements between annotators. The adjudicated data is then used as the gold standard for automatic annotation. The final versions of the sense groupings are mapped to VerbNet and FrameNet and linked to the Omega Ontology (Philpot et al, 2005).  Verbs are selected based on frequency of appearance in the WSJ corpus. As the most frequent verbs are also the most polysemous, the number of sense distinctions per verb as well as the number of instances to be tagged decreases as the project continues. The 740 most frequent verbs in the WSJ corpus were grouped in order of frequency. They have an average polysemy of 7 senses in WordNet; our sense groups have reduced the polysemy to 3.75 senses. Of these, 307 verb groupings have undergone regrouping to some extent. A total of 670 verbs have completed actual-annotation and adjudication. The next 660 verbs have been divided into rough semantic domains based on VerbNet classes, and grouping will proceed according to these semantic domains rather than by verb frequency. As groupers create sense groupings for new verbs, old verb sense groupings in the same semantic domain are consulted. This organization allows for more consistent grouping methodologies, as well as more efficiency in integrating our sense groupings into the Ontology.  
 Figure 1:  The grouping and annotation process.  
50
  
4  Grouping Methodology  Various criteria are considered when disambiguating senses and creating sense groupings for the verbs, including frequent lexical usages and collocations, syntactic features and alternations, and semantic features, similarly to Senseval2 (Palmer, et. al. 2006). Because these criteria do not apply uniformly to every verb, groupers take various approaches when creating sense groupings. Groupers recognize that there are many alternate ways to cluster senses at this level of granularity; each grouping represents only one possible clustering as a middle ground between PropBank and WordNet senses for each verb. Our highest priority is to then create clear distinctions among sense groupings that will be easily understood by the annotators and consequently result in high ITA scores. Initial clustering is based on groupers? intuitions of the most salient categories. Many verb groupings, such as that for the verb kill, provide little detailed syntactic or semantic analysis and yet have received high ITA scores. The success of these intuitive sense groupings is not due to lack of polysemy; kill has 15 WordNet senses and 2 multi-word expressions clustered into 9 sense groupings, yet it received 94% ITA in first round sample-annotation.  While annotators have little trouble tagging text with verb senses that fall neatly into intuitive categories, many verbs have fine-grained WordNet senses that fall on a continuum between two distinct lexical usages. In such cases, syntactic and semantic aspects of the verb and its arguments help groupers cluster senses in such a way that annotators can make consistent decisions in tagging the text. 
Syntactic criteria: Annotators have found syntactic frames, such as those defining VerbNet classes, to be useful in understanding boundaries between sense groupings. For example, split was originally grouped with consideration for the units resulting from a splitting event (i.e. whether a whole unit had been split into incomplete portions of the whole, or into smaller, but complete, individual units.)  This grouping proved difficult for annotators to distinguish, with an ITA of 42%. Using the causative/inchoative alternation for verbs in the ?break-45.1? class to regroup resulted in higher consistency among annotators, increasing the ITA score to 95%. Semantic criteria: When senses of a verb have similar syntactic frames, and usages fall along a continuum between these senses, semantic features of the arguments, or less often, of the verb itself, can clarify these senses and help groupers draw clear distinctions between them. Argument features that are considered when creating sense groupings include [+/-attribute], [+/-patient], and [+/-locative]. It is most common for groupers to mark these features on nominal arguments, but a prepositional phrase may also be described in semantic terms. Semantic features of the verb that are considered include aspectual features, as illustrated by the use of [+/-punctual] in sense groupings for make (Figure 2). However, it may be argued that this feature is unnecessary for annotators to be able to distinguish between the sense groupings, as the prepositional phrase in sense 9 is a more salient feature for annotators. Other features of the verb that were used earlier in the project include concrete/abstract, continuative, stative, and others. However, these features proved less useful than those Sense group Description and Commentary WordNet 2.1 senses Examples 8 Attain or reach something desired NP1[+agent] MAKE[+punctual] NP2[desired goal, destination, state] This sense implies the goal has been met. Includes: MAKE IT 
make 13, 22, 38 - He made the basketball team. - We barely made the plane. - I made the opening act in plenty of time. - Can you believe it? We made it!  
9 Move toward or away from a location NP1[+agent] MAKE[-punctual] (pronoun+way) PP/INFP  
make 30, 37 make off 1 make way 1  
- As the enemy approached our town, we made for the hills. - He made his way carefully across the icy parking lot. - They made off with the jewels. Figure 2: Sense groupings 8 and 9 for ?make.? Senses are distinguished in part by aspectual features marked on the verb.  
51
 described above, and annotators not familiar with linguistic theory found them to be confusing. Therefore, they are now rarely used to label sense groupings. Such concepts, when used, are more likely to be described in prose commentary for the sake of the annotators. Certain compositional features of verbs have also proven to be confusing for annotators. In several cases, attempts to distinguish sense groupings based on manner and path have resulted in increased annotator disagreement. In the first attempt at grouping roll, syntactic and semantic information, as well as prose commentary, was presented to help annotators distinguish the manner and path sense groupings. Despite this, the admissibility of certain prepositions in both senses (?The baby rolled over,? vs ?She rolled over to the wall,?) may have blurred the distinction. In two rounds of sample-annotation, the greatest number of disagreements occurred with respect to these two senses for roll, which were then merged in the final version of the sense groupings.  5  Conclusion  Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al, 2004; Palmer et al, 2007), we have developed a process for rapid sense inventory creation and annotation of verbs that also provides critical links between the grouped word senses and the ontology (Philpot et al, 2005). This process is based on recognizing that sense distinctions can be represented by linguists in a hierarchical structure, that is rooted in very coarse-grained distinctions which become increasingly fine-grained until reaching WordNet (or similar) senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntactic and semantic criteria for their groupings, to be presented to the annotators. Criteria are applied on a case-by-case basis, considering syntactic and semantic features as consistently as possible when grouping verbs in similar semantic domains as defined by VerbNet. By using this approach when creating sense groupings, we are 
able to provide annotators with clear and reliable descriptions of senses, resulting in improved accuracy and performance.  References Chen, J., A. Schein, L. Ungar and M. Palmer. 2006. An Empirical Study of the Behavior of Word Sense Disambiguation. Proceedings of HLT-NAACL 2006. New York, NY. Fellbaum, C. (ed.) 1998. WordNet: An On-line Lexical Database and Some of its Applications. MIT Press, Cambridge, MA. Kipper, K., A. Korhonen, N. Ryant, and M. Palmer. 2006. Extensive Classifications of English Verbs. Proceedings of the 12th EURALEX International Congress. Turin, Italy. Levin, B. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago, IL. OntoNotes, 2006. Hovy, E.H., M. Marcus, M. Palmer, S. Pradhan, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% Solution. Short paper. Proceedings of HLT-NAACL 2006. New York, NY. Palmer, M., O. Babko-Malaya, and H.T. Dang. 2004. Different Sense Granularities for Different Applications. Proceedings of the 2nd Workshop on Scalable Natural Language Understanding Systems (HLT-NAACL 2004). Boston, MA.  Palmer, M., Dang, H.T., and Fellbaum, C., Making Fine-grained and Coarse-grained sense distinctions, both manually and automatically, Journal of Natural Language Engineering (to appear, 2007). Palmer, M., Gildea, D., Kingsbury, P., The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics Journal, 31:1, 2005. Philpot, A., E.H. Hovy, and P. Pantel. 2005. The Omega Ontology. Proceedings of the ONTOLEX Workshop at the International Conference on Natural Language Processing (IJCNLP). Jeju Island, Korea.  
52
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 87?92,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 17: English Lexical Sample, SRL and All Words
Sameer S. Pradhan
BBN Technologies,
Cambridge, MA 02138
Edward Loper
University of Pennsylvania,
Philadelphia, PA 19104
Dmitriy Dligach and Martha Palmer
University of Colorado,
Boulder, CO 80303
Abstract
This paper describes our experience in
preparing the data and evaluating the results
for three subtasks of SemEval-2007 Task-17
? Lexical Sample, Semantic Role Labeling
(SRL) and All-Words respectively. We tab-
ulate and analyze the results of participating
systems.
1 Introduction
Correctly disambiguating words (WSD), and cor-
rectly identifying the semantic relationships be-
tween those words (SRL), is an important step for
building successful natural language processing ap-
plications, such as text summarization, question an-
swering, and machine translation. SemEval-2007
Task-17 (English Lexical Sample, SRL and All-
Words) focuses on both of these challenges, WSD
and SRL, using annotated English text taken from
the Wall Street Journal and the Brown Corpus.
It includes three subtasks: i) the traditional All-
Words task comprising fine-grained word sense dis-
ambiguation using a 3,500 word section of the Wall
Street Journal, annotated with WordNet 2.1 sense
tags, ii) a Lexical Sample task for coarse-grained
word sense disambiguation on a selected set of lex-
emes, and iii) Semantic Role Labeling, using two
different types of arguments, on the same subset of
lexemes.
2 Word Sense Disambiguation
2.1 English fine-grained All-Words
In this task we measure the ability of systems to
identify the correct fine-grained WordNet 2.1 word
sense for all the verbs and head words of their argu-
ments.
2.1.1 Data Preparation
We began by selecting three articles
wsj 0105.mrg (on homelessness), wsj 0186.mrg
(about a book on corruption), and wsj 0239.mrg
(about hot-air ballooning) from a section of the WSJ
corpus that has been Treebanked and PropBanked.
All instances of verbs were identified using the
Treebank part-of-speech tags, and also the head-
words of their noun arguments (using the PropBank
and standard headword rules). The locations of the
sentences containing them as well as the locations
of the verbs and the nouns within these sentences
were recorded for subsequent sense-annotation. A
total of 465 lemmas were selected from about 3500
words of text.
We use a tool called STAMP written by Ben-
jamin Snyder for sense-annotation of these in-
stances. STAMP accepts a list of pointers to the in-
stances that need to be annotated. These pointers
consist of the name of the file where the instance
is located, the sentence number of the instance, and
finally, the word number of the ambiguous word
within that sentence. These pointers were obtained
as described in the previous paragraph. STAMP also
requires a sense inventory, which must be stored in
XML format. This sense inventory was obtained by
querying WordNet 2.1 and storing the output as a
87
set of XML files (one for each word to be anno-
tated) prior to tagging. STAMP works by displaying
to the user the sentence to be annotated with the tar-
get word highlighted along with the previous and the
following sentences and the senses from the sense
inventory. The user can select one of the senses and
move on to the next instance.
Two linguistics students annotated the words with
WordNet 2.1 senses. Our annotators examined each
instance upon which they disagreed and resolved
their disagreements. Finally, we converted the re-
sulting data to the Senseval format. For this dataset,
we got an inter-annotator agreement (ITA) of 72%
on verbs and 86% for nouns.
2.1.2 Results
A total of 14 systems were evaluated on the All
Words task. These results are shown in Table 1.
We used the standard Senseval scorer ? scorer21
to score the systems. All the F-scores2 in this table
as well as other tables in this paper are accompanied
by a 95% confidence interval calculated using the
bootstrap resampling procedure.
2.2 OntoNotes English Lexical Sample WSD
It is quite well accepted at this point that it is dif-
ficult to achieve high inter-annotator agreement on
the fine-grained WordNet style senses, and with-
out a corpus with high annotator agreement, auto-
matic learning methods cannot perform at a level
that would be acceptable for a downstream applica-
tion. OntoNotes (Hovy et al, 2006) is a project that
has annotated several layers of semantic information
? including word senses, at a high inter-annotator
agreement of over 90%. Therefore we decided to
use this data for the lexical sample task.
2.2.1 Data
All the data for this task comes from the 1M word
WSJ Treebank. For the convenience of the partici-
pants who wanted to use syntactic parse information
as features using an off-the-shelf syntactic parser,
we decided to compose the training data of Sections
02-21. For the test sets, we use data from Sections
1http://www.cse.unt.edu/?rada/senseval/senseval3/scoring/
2
scorer2 reports Precision and Recall scores for each system. For a sys-
tem that attempts all the words, both Precision and Recall are the same. Since a
few systems had missing answers, they got different Precision and Recall scores.
Therefore, for ranking purposes, we consolidated them into an F-score.
Train Test Total
Verb 8988 2292 11280
Noun 13293 2559 15852
Total 22281 4851
Table 2: The number of instances for Verbs and
Nouns in the Train and Test sets for the Lexical Sam-
ple WSD task.
01, 22, 23 and 24. Fortunately, the distribution of
words was amenable to an acceptable number of in-
stances for each lemma in the test set. We selected
a total of 100 lemmas (65 verbs and 35 nouns) con-
sidering the degree of polysemy and total instances
that were annotated. The average ITA for these is
over 90%.
The training and test set composition is described
in Table 2. The distribution across all the verbs and
nouns is displayed in Table 4
2.2.2 Results
A total of 13 systems were evaluated on the Lexi-
cal Sample task. Table 3 shows the Precision/Recall
for all these systems. The same scoring software was
used to score this task as well.
2.2.3 Discussion
For the all words task, the baseline performance
using the most frequent WordNet sense for the lem-
mas is 51.4. The top-performing system was a su-
pervised system that used a Maximum Entropy clas-
sifier, and got a Precision/Recall of 59.1% ? about 8
points higher than the baseline. Since the coarse and
fine-grained disambiguation tasks have been part of
the two previous Senseval competitions, and we hap-
pen to have access to that data, we can take this op-
portunity to look at the disambiguation performance
trend. Although different test sets were used for ev-
ery evaluation, we can get a rough indication of the
trend. For the fine-grained All Words sense tagging
task, which has always used WordNet, the system
performance has ranged from our 59% to 65.2 (Sen-
seval3, (Decadt et al, 2004)) to 69% (Seneval2,
(Chklovski and Mihalcea, 2002)). Because of time
constraints on the data preparation, this year?s task
has proportionally more verbs and fewer nouns than
previous All-Words English tasks, which may ac-
count for the lower scores.
As expected, the Lexical Sample task using coarse
88
Rank Participant System ID Classifier F
1 Stephen Tratz <stephen.tratz@pnl.gov> PNNL MaxEnt 59.1?4.5
2 Hwee Tou Ng <nght@comp.nus.edu.sg> NUS-PT SVM 58.7?4.5
3 Rada Mihalcea <rada@cs.unt.edu> UNT-Yahoo Memory-based 58.3?4.5
4 Cai Junfu <caijunfu@gmail.com> NUS-ML naive Bayes 57.6?4.5
5 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM kNN 54.4?4.5
6 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-2 kNN 54.0?4.5
7 Jonathan Chang <jcone@princeton.edu> PU-BCD Exponential Model 53.9?4.5
8 Radu ION <radu@racai.ro> RACAI Unsupervised 52.7?4.5
9 Most Frequent WordNet Sense Baseline N/A 51.4?4.5
10 Davide Buscaldi <dbuscaldi@dsic.upv.es> UPV-WSD Unsupervised 46.9?4.5
11 Sudip Kumar Naskar <sudip.naskar@gmail.com> JU-SKNSB Unsupervised 40.2?4.5
12 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-1 Unsupervised 39.9?4.5
14 Rafael Berlanga <berlanga@uji.es> tkb-uo Unsupervised 32.5?4.5
15 Jordan Boyd-Graber <jbg@princeton.edu> PUTOP Unsupervised 13.2?4.5
Table 1: System Performance for the All-Words task.
Rank Participant System Classifier F
1 Cai Junfu <caijunfu@gmail.com> NUS-ML SVM 88.7?1.2
2 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM SVD+kNN 86.9?1.2
3 Zheng-Yu Niu <niu zy@hotmail.com> I2R Supervised 86.4?1.2
4 Lucia Specia <lspecia@gmail.com> USP-IBM-2 SVM 85.7?1.2
5 Lucia Specia <lspecia@gmail.com> USP-IBM-1 ILP 85.1?1.2
5 Deniz Yuret <dyuret@ku.edu.tr> KU Semi-supervised 85.1?1.2
6 Saarikoski <harri.saarikoski@helsinki.fi> OE naive Bayes, SVM 83.8?1.2
7 University of Technology Brno VUTBR naive Bayes 80.3?1.2
8 Ana Zelaia <ana.zelaia@ehu.es> UBC-ZAS SVD+kNN 79.9?1.2
9 Carlo Strapparava <strappa@itc.it> ITC-irst SVM 79.6?1.2
10 Most frequent sense in training Baseline N/A 78.0?1.2
11 Toby Hawker <toby@it.usyd.edu.au> USYD SVM 74.3?1.2
12 Siddharth Patwardhan <sidd@cs.utah.edu> UMND1 Unsupervised 53.8?1.2
13 Saif Mohammad <smm@cs.toronto.edu> Tor Unsupervised 52.1?1.2
- Toby Hawker <toby@it.usyd.edu.au> USYD? SVM 89.1?1.2
- Carlo Strapparava <strappa@itc.it> ITC? SVM 89.1?1.2
Table 3: System Performance for the OntoNotes Lexical Sample task. Systems marked with an * were
post-competition bug-fix submissions.
grained senses provides consistently higher per-
formance than previous more fine-grained Lexical
Sample Tasks. The high scores here were foreshad-
owed in an evaluation involving a subset of the data
last summer (Chen et al, 2006). Note that the best
system performance is now closely approaching the
ITA for this data of over 90%. Table 4 shows the
performance of the top 8 systems on all the indi-
vidual verbs and nouns in the test set. Owing to
space constraints we have removed some lemmas
that have perfect or almost perfect accuracies. At the
right are mentioned the average, minimum and max-
imum performances of the teams per lemma, and at
the bottom are the average scores per lemma (with-
out considering the lemma frequencies) and broken
down by verbs and nouns. A gap of about 10 points
between the verb and noun performance seems to
indicate that in general the verbs were more difficult
than the nouns. However, this might just be owing
to this particular test sample having more verbs with
higher perplexities, and maybe even ones that are
indeed difficult to disambiguate ? in spite of high
human agreement. The hope is that better knowl-
edge sources can overcome the gap still existing be-
tween the system performance and human agree-
ment. Overall, however, this data indicates that the
approach suggested by (Palmer, 2000) and that is be-
ing adopted in the ongoing OntoNotes project (Hovy
et al, 2006) does result in higher system perfor-
mance. Whether or not the more coarse-grained
senses are effective in improving natural language
processing applications remains to be seen.
89
Lemma S s T t 1 2 3 4 5 6 7 8 Average Min Max
turn.v 13 8 340 62 58 61 40 55 52 53 27 44 49 27 61
go.v 12 6 244 61 64 69 38 66 43 46 31 39 49 31 69
come.v 10 9 186 43 49 46 56 60 37 23 23 49 43 23 60
set.v 9 5 174 42 62 50 52 57 50 57 36 50 52 36 62
hold.v 8 7 129 24 58 46 50 54 54 38 50 67 52 38 67
raise.v 7 6 147 34 50 44 29 26 44 26 24 12 32 12 50
work.v 7 5 230 43 74 65 65 65 72 67 46 65 65 46 74
keep.v 7 6 260 80 56 54 52 64 56 52 48 51 54 48 64
start.v 6 4 214 38 53 50 47 55 45 42 37 45 47 37 55
lead.v 6 6 165 39 69 69 85 69 51 69 36 46 62 36 85
see.v 6 5 158 54 56 54 46 54 57 52 48 48 52 46 57
ask.v 6 3 348 58 84 72 72 78 76 52 67 66 71 52 84
find.v 5 3 174 28 93 93 86 89 82 82 75 86 86 75 93
fix.v 5 3 32 2 50 50 50 50 50 0 0 50 38 0 50
buy.v 5 3 164 46 83 80 80 83 78 76 70 76 78 70 83
begin.v 4 2 114 48 83 65 75 69 79 56 50 56 67 50 83
kill.v 4 1 111 16 88 88 88 88 88 88 88 81 87 81 88
join.v 4 4 68 18 44 50 50 39 56 57 39 44 47 39 57
end.v 4 3 135 21 90 86 86 90 62 87 86 67 82 62 90
do.v 4 2 207 61 92 90 90 93 93 90 85 84 90 84 93
examine.v 3 2 26 3 100 100 67 100 100 67 100 33 83 33 100
report.v 3 2 128 35 89 91 91 91 91 91 91 86 90 86 91
regard.v 3 3 40 14 93 93 86 86 64 86 57 93 82 57 93
recall.v 3 1 49 15 100 100 87 87 93 87 87 87 91 87 100
prove.v 3 2 49 22 90 88 82 80 90 86 70 74 82 70 90
claim.v 3 2 54 15 67 73 80 80 80 80 80 87 78 67 87
build.v 3 3 119 46 74 67 74 61 54 74 61 72 67 54 74
feel.v 3 3 347 51 71 69 69 74 76 69 61 71 70 61 76
care.v 3 3 69 7 43 43 43 43 100 29 57 57 52 29 100
contribute.v 2 2 35 18 67 72 72 67 50 61 50 67 63 50 72
maintain.v 2 2 61 10 80 80 70 100 80 90 90 80 84 70 100
complain.v 2 1 32 14 93 86 86 86 86 86 86 79 86 79 93
propose.v 2 2 34 14 100 86 100 86 100 93 79 79 90 79 100
promise.v 2 2 50 8 88 88 75 88 75 75 62 88 80 62 88
produce.v 2 2 115 44 82 82 77 73 75 75 77 80 78 73 82
prepare.v 2 2 54 18 94 83 89 89 83 86 83 83 86 83 94
explain.v 2 2 85 18 94 89 94 89 94 89 89 94 92 89 94
believe.v 2 2 202 55 87 78 78 86 84 78 74 80 81 74 87
occur.v 2 2 47 22 86 73 91 96 86 96 86 82 87 73 96
grant.v 2 2 19 5 100 80 80 80 40 80 60 80 75 40 100
enjoy.v 2 2 56 14 50 57 57 50 64 57 50 57 55 50 64
need.v 2 2 195 56 89 82 86 89 86 78 70 70 81 70 89
disclose.v 1 1 55 14 93 93 93 93 93 93 93 93 93 93 93
point.n 9 6 469 150 91 91 89 91 92 87 84 79 88 79 92
position.n 7 6 268 45 78 78 78 53 56 65 58 64 66 53 78
defense.n 7 7 120 21 57 48 52 43 48 29 48 48 46 29 57
carrier.n 7 3 111 21 71 71 71 71 67 71 71 62 70 62 71
order.n 7 4 346 57 93 95 93 91 93 92 90 91 92 90 95
exchange.n 5 3 363 61 92 90 92 85 90 88 82 79 87 79 92
system.n 5 3 450 70 79 73 66 67 59 63 63 61 66 59 79
source.n 5 5 152 35 86 80 80 63 83 68 60 29 69 29 86
space.n 5 2 67 14 93 100 93 93 93 86 86 71 89 71 100
base.n 5 4 92 20 75 80 75 50 65 40 50 75 64 40 80
authority.n 4 3 90 21 86 86 81 62 71 33 71 81 71 33 86
people.n 4 4 754 115 96 96 95 96 95 90 91 91 94 90 96
chance.n 4 3 91 15 60 67 60 60 67 73 20 73 60 20 73
part.n 4 3 481 71 90 90 92 97 90 74 66 66 83 66 97
hour.n 4 2 187 48 83 85 92 83 77 90 58 92 83 58 92
development.n 3 3 180 29 100 79 86 79 76 62 79 62 78 62 100
president.n 3 3 879 177 98 97 98 97 93 96 97 85 95 85 98
network.n 3 3 152 55 91 87 98 89 84 88 87 82 88 82 98
future.n 3 3 350 146 97 96 94 97 83 98 89 85 92 83 98
effect.n 3 2 178 30 97 93 80 93 80 90 77 83 87 77 97
state.n 3 3 617 72 85 86 86 83 82 79 83 82 83 79 86
power.n 3 3 251 47 92 87 87 81 77 77 77 74 81 74 92
bill.n 3 3 404 102 98 99 98 96 90 96 96 22 87 22 99
area.n 3 3 326 37 89 73 65 68 84 70 68 65 73 65 89
job.n 3 3 188 39 85 80 77 90 80 82 69 82 80 69 90
management.n 2 2 284 45 89 78 87 73 98 76 67 64 79 64 98
condition.n 2 2 132 34 91 82 82 56 76 78 74 76 77 56 91
policy.n 2 2 331 39 95 97 97 87 95 97 90 64 90 64 97
rate.n 2 2 1009 145 90 88 92 81 92 89 88 91 89 81 92
drug.n 2 2 205 46 94 94 96 78 94 94 87 78 89 78 96
Average Overall 86 83 83 82 82 79 76 77
Verbs 78 75 73 76 73 70 65 70
Nouns 89 87 86 81 83 80 77 76
Table 4: All Supervised system performance per predicate. (Column legend ? S=number of senses in training; s=number senses appearing more than 3 times;
T=instances in training; t=instances in test.; The numbers indicate system ranks.)
90
3 Semantic Role Labeling
Subtask 2 evaluates Semantic Role Labeling (SRL)
systems, where the goal is to locate the constituents
which are arguments of a given verb, and to assign
them appropriate semantic roles that describe how
they relate to the verb. SRL systems are an impor-
tant building block for many larger semantic sys-
tems. For example, in order to determine that ques-
tion (1a) is answered by sentence (1b), but not by
sentence (1c), we must determine the relationships
between the relevant verbs (eat and feed) and their
arguments.
(1) a. What do lobsters like to eat?
b. Recent studies have shown that lobsters pri-
marily feed on live fish, dig for clams, sea
urchins, and feed on algae and eel-grass.
c. In the early 20th century, Mainers would
only eat lobsters because the fish they
caught was too valuable to eat themselves.
Traditionally, SRL systems have been trained on
either the PropBank corpus (Palmer et al, 2005)
? for two years, the CoNLL workshop (Carreras
and Ma`rquez, 2004; Carreras and Ma`rquez, 2005)
has made this their shared task, or the FrameNet
corpus ? Senseval-3 used this for their shared task
(Litkowski, 2004). However, there is still little con-
sensus in the linguistics and NLP communities about
what set of role labels are most appropriate. The
PropBank corpus avoids this issue by using theory-
agnostic labels (ARG0, ARG1, . . . , ARG5), and
by defining those labels to have only verb-specific
meanings. Under this scheme, PropBank can avoid
making any claims about how any one verb?s ar-
guments relate to other verbs? arguments, or about
general distinctions between verb arguments and ad-
juncts.
However, there are several limitations to this ap-
proach. The first is that it can be difficult to make
inferences and generalizations based on role labels
that are only meaningful with respect to a single
verb. Since each role label is verb-specific, we can
not confidently determine when two different verbs?
arguments have the same role; and since no encoded
meaning is associated with each tag, we can not
make generalizations across verb classes. In con-
trast, the use of a shared set of role labels, such
System Type Precision Recall F
UBC-UPC Open 84.51 82.24 83.36?0.5
UBC-UPC Closed 85.04 82.07 83.52?0.5
RTV Closed 81.82 70.37 75.66?0.6
Without ?say?
UBC-UPC Open 78.57 74.70 76.60?0.8
UBC-UPC Closed 78.67 73.94 76.23?0.8
RTV Closed 74.15 57.85 65.00?0.9
Table 5: System performance on PropBank argu-
ments.
as VerbNet roles, would facilitate both inferencing
and generalization. VerbNet has more traditional la-
bels such as Agent, Patient, Theme, Beneficiary, etc.
(Kipper et al, 2006).
Therefore, we chose to annotate the corpus us-
ing two different role label sets: the PropBank role
set and the VerbNet role set. VerbNet roles were
generated using the SemLink mapping (Loper et al,
2007), which provides a mapping between Prop-
Bank and VerbNet role labels. In a small number of
cases, no VerbNet role was available (e.g., because
VerbNet did not contain the appropriate sense of the
verb). In those cases, the PropBank role label was
used instead.
We proposed two levels of participation in this
task: i) Closed ? the systems could use only the an-
notated data provided and nothing else. ii) Open ?
where systems could use PropBank data from Sec-
tions 02-21, as well as any other resource for training
their labelers.
3.1 Data
We selected 50 verbs from the 65 in the lexical sam-
ple task for the SRL task. The partitioning into train
and test set was done in the same fashion as for the
lexical sample task. Since PropBank does not tag
any noun predicates, none of the 35 nouns from the
lexical sample task were part of this data.
3.2 Results
For each system, we calculated the precision, re-
call, and F-measure for both role label sets. Scores
were calculated using the srl-eval.pl script from
the CoNLL-2005 scoring package (Carreras and
Ma`rquez, 2005). Only two teams chose to perform
the SRL subtask. The performance of these two
teams is shown in Table 5 and Table 6.
91
System Type Precision Recall F
UBC-UPC Open 85.31 82.08 83.66?0.5
UBC-UPC Closed 85.31 82.08 83.66?0.5
RTV Closed 81.58 70.16 75.44?0.6
Without ?say?
UBC-UPC Open 79.23 73.88 76.46?0.8
UBC-UPC Closed 79.23 73.88 76.46?0.8
RTV Closed 73.63 57.44 64.53?0.9
Table 6: System performance on VerbNet roles.
3.3 Discussion
Given that only two systems participated in the task,
it is difficult to form any strong conclusions. It
should be noted that since there was no additional
VerbNet role data to be used by the Open system, the
performance of that on PropBank arguments as well
as VerbNet roles is exactly identical. It can be seen
that there is almost no difference between the perfor-
mance of the Open and Closed systems for tagging
PropBank arguments. The reason for this is the fact
that all the instances of the lemma under consider-
ation was selected from the Propbank corpus, and
probably the number of training instances for each
lemma as well as the fact that the predicate is such
an important feature combine to make the difference
negligible. We also realized that more than half of
the test instances were contributed by the predicate
?say? ? the performance over whose arguments is in
the high 90s. To remove the effect of ?say? we also
computed the performances after excluding exam-
ples of ?say? from the test set. These numbers are
shown in the bottom half of the two tables. These
results are not directly comparable to the CoNLL-
2005 shared task since: i) this test set comprises
Sections 01, 22, 23 and 24 as opposed to just Sec-
tion 23, and ii) this test set comprises data for only
50 predicates as opposed to all the verb predicates in
the CoNLL-2005 shared task.
4 Conclusions
The results in the previous discussion seem to con-
firm the hypothesis that there is a predictable corre-
lation between human annotator agreement and sys-
tem performance. Given high enough ITA rates we
can can hope to build sense disambiguation systems
that perform at a level that might be of use to a con-
suming natural language processing application. It
is also encouraging that the more informative Verb-
Net roles which have better/direct applicability in
downstream systems, can also be predicted with al-
most the same degree of accuracy as the PropBank
arguments from which they are mapped.
5 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022;
National Science Foundation Grant NSF-0415923,
Word Sense Disambiguation; the DTO-AQUAINT
NBCHC040036 grant under the University of
Illinois subcontract to University of Pennsylvania
2003-07911-01; and NSF-ITR-0325646:
Domain-Independent Semantic Interpretation.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL-2004.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. In
Proceedings of CoNLL-2005.
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer.
2006. An empirical study of the behavior of active learning for
word sense disambiguation. In Proceedings of HLT/NAACL.
Timothy Chklovski and Rada Mihalcea. 2002. Building a
sense tagged corpus with open mind word expert. In
Proceedings of ACL-02 Workshop on WSD.
Bart Decadt, Ve?ronique Hoste, Walter Daelemans, and Antal
Van den Bosch. 2004. GAMBL, genetic algorithm
optimization of memory-based wsd. In Senseval-3.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90%
solution. In Proceedings of HLT/NAACL, June.
Karin Kipper, Anna Korhonen, Neville Ryant, and Martha
Palmer. 2006. Extending VerbNet with novel verb classes. In
LREC-06.
Ken Litkowski. 2004. Senseval-3 task: Automatic labeling of
semantic roles. In Proceedings of Senseval-3.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between propbank and
verbnet. In Proceedings of the IWCS-7.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
proposition bank: A corpus annotated with semantic roles.
Computational Linguistics, 31(1):71?106.
Martha Palmer. 2000. Consistent criteria for sense
distinctions. Computers and the Humanities, 34(1-1):217?222.
92
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 93?98,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semeval 2007 Task 18: Arabic Semantic Labeling
Mona Diab
Columbia University
mdiab@cs.columbia.edu
Christiane Fellbaum
Princeton University
fellbaum@clarity.princeton.edu
Musa Alkhalifa
University of Barcelona
musa@thera-clic.com
Aous Mansouri
University of Colorado, Boulder
aous.mansouri@colorado.edu
Sabri Elkateb
University of Manchester
Sabri.Elkateb@manchester.ac.uk
Martha Palmer
University of Colorado, Boulder
martha.palmer@colorado.edu
Abstract
In this paper, we present the details of the
Arabic Semantic Labeling task. We describe
some of the features of Arabic that are rele-
vant for the task. The task comprises two
subtasks: Arabic word sense disambiguation
and Arabic semantic role labeling. The task
focuses on modern standard Arabic.
1 Introduction
Recent years have witnessed a surge in available re-
sources for the Arabic language.1 The computa-
tional linguistics community is just about starting
to exploit these resources toward several interesting
scientific and engineering goals. The Arabic lan-
guage is interesting from a computational linguistic
perspective. It is significantly different from English
hence creating a challenge for existing technology to
be easily portable to Arabic. The Arabic language is
inherently complex due to its rich morphology and
relative free word order. Moreover, with the exis-
tence of several interesting varieties, the spoken ver-
naculars, we are witnessing the emergence of written
dialectal Arabic everyday on the web, however there
are no set standards for these varieties.
We have seen many successful strides towards
functional systems for Arabic enabling technolo-
gies, but we are yet to read about large Arabic NLP
applications such as Machine Translation and Infor-
mation Extraction that are on par with performance
on the English language. The problem is not the ex-
istence of data, but rather the existence of data an-
notated with the relevant level of information that
1Author 1 is supported by DARPA contract Contract No.
HR0011-06-C-0023. Authors 2, 3 and 4 are supported by the
US Central Intelligence Service.
is useful for NLP. This task attempts a step towards
the goal of creating resources that could be useful
for such applications.
In this task, we presented practitioners in the field
with challenge of labeling Arabic text with seman-
tic labels. The labels constitute two levels of gran-
ularity: sense labels and semantic role labels. We
specifically chose data that overlapped such that we
would have the same data annotated for different
types of semantics, lexical and structural. The over-
all task of Arabic Semantic Labeling was subdivided
into 4 sub-tasks: Arabic word sense disambiguation
(AWSD), English to Arabic WSD task (EAWSD),
argument detection within the context of semantic
role labeling, and argument semantic role classifica-
tion.
Such a set of tasks would not have been feasible
without the existence of several crucial resources:
the Arabic Treebank (ATB) (Maamouri et al,
2004), the Arabic WordNet (AWN) (Elkateb et
al., 2006), and the Pilot Arabic Propbank
(APB).2
This paper is laid out as follows: Section 2 will
describe some facts about the Arabic language; Sec-
tion 3 will present the overall description of the
tasks; Section 4 describes the word sense disam-
biguation task; Section 5 describes the semantic role
labeling task.
2 The Arabic Language
In the context of our tasks, we only deal with MSA.3
Arabic is a Semitic language. It is known for its
templatic morphology where words are made up of
2Funded by DARPA subcontract to BBN Inc. to University
of Colorado, LDC-UPenn and Columbia University.
3In this paper we use MSA and Arabic interchangeably.
93
roots and affixes. Clitics agglutinate to words. For
instance, the surface word  	
  wbHsnAthm4
?and by their virtues[fem.]?, can be split into the con-
junction w ?and?, preposition b ?by?, the stem HsnAt
?virtues [fem.]?, and possessive pronoun hm ?their?.
Arabic is different from English from both the mor-
phological and syntactic perspectives which make it
a challenging language to the existing NLP technol-
ogy that is too tailored to the English language.
From the morphological standpoint, Arabic ex-
hibits rich morphology. Similar to English, Ara-
bic verbs are marked explicitly for tense, voice and
person, however in addition, Arabic marks verbs
with mood (subjunctive, indicative and jussive) in-
formation. For nominals (nouns, adjectives, proper
names), Arabic marks case (accusative, genitive and
nominative), number, gender and definiteness fea-
tures. Depending on the genre of the text at hand,
not all of those features are explicitly marked on nat-
urally occurring text.
Arabic writing is known for being underspecified
for short vowels. Some of the case, mood and voice
features are marked only using short vowels. Hence,
if the genre of the text were religious such as the
Quran or the Bible, or pedagogical such as children?s
books in Arabic, it would be fully specified for all
the short vowels to enhance readability and disam-
biguation.
From the syntactic standpoint, Arabic, different
from English, is considered a pro-drop language,
where the subject of a verb may be implicitly en-
coded in the verb morphology. Hence, we observe
sentences such as       Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 106?111,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 10:
Linking Events and Their Participants in Discourse
Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{josefr,csporled}@coli.uni-sb.de
Roser Morante
CNTS
University of Antwerp
Roser.Morante@ua.ac.be
Collin Baker
ICSI
Berkeley, CA 94704
collin@icsi.berkeley.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
In this paper, we describe the SemEval-2010
shared task on ?Linking Events and Their Par-
ticipants in Discourse?. This task is a variant
of the classical semantic role labelling task.
The novel aspect is that we focus on linking
local semantic argument structures across sen-
tence boundaries. Specifically, the task aims at
linking locally uninstantiated roles to their co-
referents in the wider discourse context (if such
co-referents exist). This task is potentially ben-
eficial for a number of NLP applications and
we hope that it will not only attract researchers
from the semantic role labelling community
but also from co-reference resolution and infor-
mation extraction.
1 Introduction
Semantic role labelling (SRL) has been defined as
a sentence-level natural-language processing task in
which semantic roles are assigned to the syntactic
arguments of a predicate (Gildea and Jurafsky, 2002).
Semantic roles describe the function of the partici-
pants in an event. Identifying the semantic roles of
the predicates in a text allows knowing who did what
to whom when where how, etc.
SRL has attracted much attention in recent years,
as witnessed by several shared tasks in Sense-
val/SemEval (Ma`rquez et al, 2007; Litkowski, 2004;
Baker et al, 2007; Diab et al, 2007), and CoNLL
(Carreras and Ma`rquez, 2004; Carreras and Ma`rquez,
2005; Surdeanu et al, 2008). The state-of-the-art
in semantic role labelling has now advanced so
much that a number of studies have shown that au-
tomatically inferred semantic argument structures
can lead to tangible performance gains in NLP ap-
plications such as information extraction (Surdeanu
et al, 2003), question answering (Shen and Lapata,
2007) or recognising textual entailment (Burchardt
and Frank, 2006).
However, semantic role labelling as it is currently
defined also misses a lot of information that would
be beneficial for NLP applications that deal with
text understanding (in the broadest sense), such as
information extraction, summarisation, or question
answering. The reason for this is that SRL has tra-
ditionally been viewed as a sentence-internal task.
Hence, relations between different local semantic ar-
gument structures are disregarded and this leads to a
loss of important semantic information.
This view of SRL as a sentence-internal task is
partly due to the fact that large-scale manual anno-
tation projects such as FrameNet1 and PropBank2
typically present their annotations lexicographically
by lemma rather than by source text. Furthermore,
in the case of FrameNet, the annotation effort did
not start out with the goal of exhaustive corpus an-
notation but instead focused on isolated instances of
the target words sampled from a very large corpus,
which did not allow for a view of the data as ?full-text
annotation?.
It is clear that there is an interplay between local
argument structure and the surrounding discourse
(Fillmore, 1977). In early work, Palmer et al (1986)
discussed filling null complements from context by
using knowledge about individual predicates and ten-
1http://framenet.icsi.berkeley.edu/
2http://verbs.colorado.edu/?mpalmer/
projects/ace.html
106
dencies of referential chaining across sentences. But
so far there have been few attempts to find links
between argument structures across clause and sen-
tence boundaries explicitly on the basis of semantic
relations between the predicates involved. Two no-
table exceptions are Fillmore and Baker (2001) and
Burchardt et al (2005). Fillmore and Baker (2001)
analyse a short newspaper article and discuss how
frame semantics could benefit discourse processing
but without making concrete suggestions of how to
model this. Burchardt et al (2005) provide a detailed
analysis of the links between the local semantic argu-
ment structures in a short text; however their system
is not fully implemented either.
In the shared task, we intend to make a first step
towards taking SRL beyond the domain of individual
sentences by linking local semantic argument struc-
tures to the wider discourse context. In particular, we
address the problem of finding fillers for roles which
are neither instantiated as direct dependents of our
target predicates nor displaced through long-distance
dependency or coinstantatiation constructions. Of-
ten a referent for an uninstantiated role can be found
in the wider context, i.e. in preceding or following
sentences. An example is given in (1), where the
CHARGES role (ARG2 in PropBank) of cleared is left
empty but can be linked to murder in the previous
sentence.
(1) In a lengthy court case the defendant was
tried for murder. In the end, he was cleared.
Another very rich example is provided by (2),
where, for instance, the experiencer and the object of
jealousy are not overtly expressed as syntactic depen-
dents of the noun jealousy but can be inferred to be
Watson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anything
of art but that is mere jealousy because our
views upon the subject differ.
NIs are also very frequent in clinical reports.
For example, in (3) the EXPERIENCER role of
?cough?, ?tachypnea?, and ?breathing? can be linked
to ?twenty-two month old?. Text mining systems in
the biomedical domain focus on extracting relations
between biomedical entities and information about
patients. It is important that these systems extract
information as accurately as possible. Thus, finding
co-referents for NIs is also very relevant for improv-
ing results on mining relations in biomedical texts.
(3) Twenty-two month old with history of recur-
rent right middle lobe infiltrate. Increased
cough, tachypnea, and work of breathing.
In the following sections we describe the task in
more detail. We start by providing some background
on null instantiations (Section 2). Section 3 gives an
overview of the task, followed by a description of
how we intend to create the data (Section 4). Sec-
tion 5 provides a short description of how null in-
stantiations could be resolved automatically given
the provided data. Finally, Section 6 discusses the
evaluation measures and we wrap up in Section 7.
2 Background on Null Instantiation
The theory of null complementation used here is the
one adopted by FrameNet, which derives from the
work of Fillmore (1986).3 Briefly, omissions of core
arguments of predicates are categorised along two
dimensions, the licensor and the interpretation they
receive. The idea of a licensor refers to the fact that
either a particular lexical item or a particular gram-
matical construction must be present for the omission
of a frame element (FE) to occur. For instance, the
omission of the agent in (4) is licensed by the passive
construction.
(4) No doubt, mistakes were made 0Protagonist.
The omission is a constructional omission because
it can apply to any predicate with an appropriate
semantics that allows it to combine with the passive
construction. On the other hand, the omission in (5)
is lexically specific: the verb arrive allows the Goal
to be unspecified but the verb reach, also a member
of the Arriving frame, does not.
(5) We arrived 0Goal at 8pm.
The above two examples also illustrate the second
major dimension of variation. Whereas, in (4) the
protagonist making the mistake is only existentially
bound within the discourse (instance of indefinite null
3Palmer et al?s (1986) treatment of uninstantiated ?essential
roles? is very similar (see also Palmer (1990)).
107
instantiation, INI), the Goal location in (5) is an entity
that must be accessible to speaker and hearer from
the discourse or its context (definite null instantiation,
DNI). Finally note that the licensing construction or
lexical item fully and reliably determines the interpre-
tation. Missing by-phrases always have an indefinite
interpretation and whenever arrive omits the Goal
lexically, the Goal has to be interpreted as definite,
as it is in (5).
The import of this classification to the task here
is that we will concentrate on cases of DNI whether
they are licensed lexically or constructionally.
3 Task Description
We plan to run the task in the following two modes:
Full Task For the full task we supply a test set in
which the target words are marked and labelled with
the correct sense (i.e. frame).4 The participants then
have to:
1. find the overt semantic arguments of the target
(role recognition)
2. label them with the correct role (role labelling)
3. recognize definite null instantiations and find
links to antecedents in the wider context (NI
linking)
NIs only In the second mode, participants will be
supplied with a test set which is annotated with gold
standard local semantic argument structure.5 The
task is then restricted to recognizing that a core role
is missing, ascertaining that it must have a definite
interpretation and finding a filler for it (i.e., sub-task
3 from the full task).
The full task and the null instantiation linking task
will be evaluated separately. By setting up a SRL
task, we expect to attract participants from the es-
tablished SRL community. Furthermore, by allow-
ing participants to only address the second task, we
4We supply the correct sense to ensure that all systems use
the same role inventory for each target (i.e., the role inventory
associated with the gold standard sense). This makes it easier
to evaluate the systems consistently with respect to role assign-
ments and null instantiation linking, which is our main focus.
5The training set is identical for both set-ups and will contain
the full annotation, i.e., frames, semantic roles and their fillers,
and referents of null instantiations in the wider context (see
Section 4 for details).
hope to also attract researchers from areas such as co-
reference resolution or information extraction who do
not want to implement a complete SRL system. We
also plan to provide the data with both FrameNet and
PropBank style annotations to encourage researchers
from both areas to take part.
4 Data
The data will come from one of Arthur Conan
Doyle?s fiction works. We chose fiction rather than
news because we believe that fiction texts with
a linear narrative generally contain more context-
resolvable null instantiations. They also tend to be
longer and have a simpler structure than news texts
which typically revisit the same facts repeatedly at
different levels of detail (in the so-called ?inverted
pyramid? structure) and which mix event reports with
commentary and evaluation, thus sequencing mate-
rial that is understood as running in parallel. Fiction
texts should lend themselves more readily to a first at-
tempt at integrating discourse structure into semantic
role labeling. We chose Conan Doyle?s work because
most of his books are not subject to copyright restric-
tions anymore, which allows us to freely release the
annotated data.
We plan to make the data sets available with both
FrameNet and PropBank semantic argument anno-
tation, so that participants can choose which frame-
work they want to work in. The annotations will
originally be made using FrameNet-style and will
later be mapped semi-automatically to PropBank an-
notations. The data set for the FrameNet version of
the task will be built at Saarland University, in close
co-operation with the FrameNet team in Berkeley.
We aim for the same density of annotation as is ex-
hibited by FrameNet?s existing full-text annotation6
and are currently investigating whether the semantic
argument annotation can be done semi-automatically,
e.g., by starting the annotation with a run of the Shal-
maneser role labeller (Erk and Pado?, 2006), whose
output is then corrected and expanded manually. To
ensure a high annotation quality, at least part of the
data will be annotated by two annotators and then
manually adjudicated. We also provide detailed an-
notation guidelines (largely following the FrameNet
6http://framenet.icsi.berkeley.edu/
index.php?option=com_wrapper&Itemid=84
108
guidelines) and any open questions are discussed in
a weekly annotation meeting.
For the annotation of null instantiations and their
links to the surrounding discourse we have to create
new guidelines as this is a novel annotation task. We
will adopt ideas from the annotation of co-reference
information, linking locally unrealised roles to all
mentions of the referents in the surrounding dis-
course, where available. We will mark only identity
relations but not part-whole or bridging relations be-
tween referents. The set of unrealised roles under
consideration includes only the core arguments but
not adjuncts (peripheral or extra-thematic roles in
FrameNet?s terminology). Possible antecedents are
not restricted to noun phrases but include all con-
stituents that can be (local) role fillers for some pred-
icate plus complete sentences (which can sometimes
fill roles such as MESSAGE).
The data-set for PropBank will be created by map-
ping the FrameNet annotations onto PropBank and
NomBank labels. For verbal targets, we use the Sem-
link7 mappings. For nominal targets, there is no
existing hand-checked mapping between FrameNet
and NomBank but we will explore a way of build-
ing a FrameNet - NomBank mapping at least for
eventive nouns indirectly with the help of Semlink.
This would take advantage of the fact that PropBank
verbs and eventive NomBank nouns both have a map-
ping to VerbNet classes, which are referenced also by
Semlink. Time permitting, non-eventive nouns could
be mapped manually. For FrameNet targets of other
parts of speech, in particular adjectives and prepo-
sitions, no equivalent PropBank-style counterparts
will be available. The result of the automatic map-
pings will be partly hand-checked. The annotations
resolving null instantiations need no adjustment.
We intend to annotate at least two data sets of
around 4,000 words. One set for testing and one for
training. Because we realise that the training set will
not be large enough to train a semantic role labelling
system on it, we permit the participants to boost the
training data for the SRL task by making use of the
existing FrameNet and PropBank corpora.8
7http://verbs.colorado.edu/semlink/
8This may require some genre adaption but we believe this is
feasible.
5 Resolving Null Instantiations
We conceive of null instantiation resolution as a three
step problem. First, one needs to determine whether a
core role is missing. This involves looking up which
core roles are overtly expressed and which are not.
In the second step, one needs to determine what
licenses an omission and what its interpretation is.
To do this, one can use rules and heuristics based on
various syntactic and lexical facts of English. As an
example of a relevant syntactic fact, consider that sub-
jects in English can only be omitted when licensed by
a construction. One such construction is the impera-
tive (e.g. Please, sit down). Since this construction
also specifies that the missing referent must be the
addressee of the speaker of the imperative, it is clear
what referent one has to try to find.
As for using lexical knowledge, consider omis-
sions of the Goods FE of the verb steal in the Theft
frame. FrameNet annotation shows that whenever
the Goods FE of steal is missing it is interpreted in-
definitely, suggesting that a new instance of the FE
being missing should have the same interpretation.
More evidence to the same effect can be derived us-
ing Ruppenhofer?s (2004) observation that the inter-
pretation of a lexically licensed omission is definite
if the overt instances of the FE have mostly definite
form (i.e. have definite determiners such as that, the ,
this), and indefinite if they are mostly indefinite (i.e.
have bare or indefinite determiners such as a(n) or
some). The morphology of overt instances of an FE
could be inspected in the FrameNet data, or if the
predicate has only one sense or a very dominant one,
then the frequencies could even be estimated from
unannotated corpora.
The third step is linking definite omissions to ref-
erents in the context. This linking problem could be
modelled as a co-reference resolution task. While
the work of Palmer et al (1986) relied on special
lexicons, one might instead want to learn information
about the semantic content of different role fillers
and then assess for each of the potential referents in
the discourse context whether their semantic content
is close enough to the expected content of the null
instantiated role.
Information about the likely fillers of a role can
be obtained from annotated data sets (e.g., FrameNet
or PropBank). For instance, typical fillers of the
109
CHARGES role of clear might be murder, accusa-
tions, allegations, fraud etc. The semantic content of
the role could then be represented in a vector space
model, using additional unannotated data to build
meaning vectors for the attested role fillers. Meaning
vectors for potential role fillers in the context of the
null instantiation could be built in a similar fashion.
The likelihood of a potential filler filling the target
role can then be modelled as the distance between the
meaning vector of the filler and the role in the vec-
tor space model (see Pado? et al (2008) for a similar
approach for semi-automatic SRL).
We envisage that the manually annotated null in-
stantiated data can be used to learn additionally
heuristics for the filler resolution task, such as in-
formation about the average distance between a null
instantiation and its most recent co-referent.
6 Evaluation
As mentioned above we allow participants to address
either the full role recognition and labelling task plus
the linking of null instantiations or to make use of
the gold standard semantic argument structure and
look only at the null instantiations. We also permit
systems to perform either FrameNet or PropBank
style SRL. Hence, systems can be entered for four
subtasks which will be evaluated separately:
? full task, FrameNet
? null instantiations, FrameNet
? full task, PropBank
? null instantiations, PropBank
The focus for the proposed task is on the null in-
stantiation linking, however, for completeness, we
also evaluate the standard SRL task. For role recogni-
tion and labelling we use a standard evaluation set-up,
i.e., for role recognition we will evaluate the accuracy
with respect to the manually created gold standard,
for role labelling we will evaluate precision, recall,
and F-Score.
The null instantiation linkings are evaluated
slightly differently. In the gold standard, we will iden-
tify referents for null instantiations in the discourse
context. In some cases, more than one referent might
be appropriate, e.g., because the omitted argument
refers to an entity that is mentioned multiple times
in the context. In this case, a system should be given
credit if the null instantiation is linked to any of these
expressions. To achieve this we create equivalence
sets for the referents of null instantiations. If the null
instantiation is linked to any item in the equivalence
set, the link is counted as a true positive. We can then
define NI linking precision as the number of all true
positive links divided by the number of links made by
a system, and NI linking recall as the number of true
positive links divided by the number of links between
a null instantiation and its equivalence set in the gold
standard. NI linking F-Score is then the harmonic
mean between NI linking precision and recall.
Since it may sometimes be difficult to determine
the correct extend of the filler of an NI, we score
an automatic annotation as correct if it includes the
head of the gold standard filler in the predicted filler.
However, in order to not favour systems which link
NIs to excessively large spans of text to maximise the
likelihood of linking to a correct referent, we intro-
duce a second evaluation measure, which computes
the overlap (Dice coefficient) between the words in
the predicted filler (P) of a null instantiation and the
words in the gold standard one (G):
NI linking overlap = 2|P ?G||P |+ |G| (6)
Example (7) illustrates this point. The verb won in
the second sentence evokes the Finish competition
frame whose COMPETITION role is null instantiated.
From the context it is clear that the competition role
is semantically filled by their first TV debate (head:
debate) and last night?s debate (head: debate) in
the previous sentences. These two expressions make
up the equivalence set for the COMPETITION role in
the last sentence. Any system that would predict a
linkage to a filler that covers the head of either of
these two expressions would score a true positive for
this NI. However, a system that linked to last night?s
debate would have an NI linking overlap of 1 (i.e.,
2*3/(3+3)) while a system linking the whole second
sentence Last night?s debate was eagerly anticipated
to the NI would have an NI linking overlap of 0.67
(i.e., 2*3/(6+3))
(7) US presidential rivals Republican John
McCain and Democrat Barack Obama have
yesterday evening attacked each other over
110
foreign policy and the economy, in [their
first TV debate]Competition. [Last night?s
debate]Competition was eagerly anticipated.
Two national flash polls suggest that
[Obama]Competitor wonFinish competition
0Competition.
7 Conclusion
In this paper, we described the SemEval-2010 shared
task on ?Linking Events and Their Participants in
Discourse?. With this task, we intend to take a first
step towards viewing semantic role labelling not as a
sentence internal problem but as a task which should
really take the discourse context into account. Specif-
ically, we focus on finding referents for roles which
are null instantiated in the local context. This is po-
tentially useful for various NLP applications. We
believe that the task is timely and interesting for a
number of researchers not only from the semantic
role labelling community but also from fields such as
co-reference resolution or information extraction.
While our task focuses specifically on finding links
between null instantiated roles and the discourse con-
text, we hope that in setting it up, we can stimulate re-
search on the interaction between discourse structure
and semantic argument structure in general. Possible
future editions of the task could then focus on addi-
tional connections between local semantic argument
structures (e.g., linking argument structures that refer
to the same event).
8 Acknowledgements
Josef Ruppenhofer and Caroline Sporleder are supported
by the German Research Foundation DFG (under grant
PI 154/9-3 and the Cluster of Excellence Multimodal
Computing and Interaction (MMCI), respectively). Roser
Morante?s research is funded by the GOA project BIO-
GRAPH of the University of Antwerp.
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction. In
Proceedings of SemEval-07.
A. Burchardt and A. Frank. 2006. Approximating textual
entailment with LFG and framenet frames. In Pro-
ceedings of the Second Recognising Textual Entailment
Workshop.
A. Burchardt, A. Frank, and M. Pinkal. 2005. Building
text meaning representations from contextually related
frames ? A case study. In Proceedings of IWCS-6.
X. Carreras and Ll. Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL-04, pages 89?97.
X. Carreras and Ll. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-05, pages 152?164.
M. Diab, M. Alkhalifa, S. ElKateb, C. Fellbaum, A. Man-
souri, and M. Palmer. 2007. SemEval-2007 Task 18:
Arabic semantic labeling. In Proc. of SemEval-07.
K. Erk and S. Pado?. 2006. Shalmaneser - a flexible
toolbox for semantic role assignment. In Proceedings
of LREC-06.
C.J. Fillmore and C.F. Baker. 2001. Frame semantics for
text understanding. In Proc. of the NAACL-01 Work-
shop on WordNet and Other Lexical Resources.
C.J. Fillmore. 1977. Scenes-and-frames semantics, lin-
guistic structures processing. In Antonio Zampolli,
editor, Fundamental Studies in Computer Science, No.
59, pages 55?88. North Holland Publishing.
C.J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the Twelfth Annual Meet-
ing of the Berkeley Liguistics Society.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?
288.
K. Litkowski. 2004. SENSEVAL-3 Task: Automatic
labeling of semantic roles. In Proc. of SENSEVAL-3.
L. Ma`rquez, L. Villarejo, M. A. Mart?`, and M. Taule`. 2007.
SemEval-2007 Task 09: Multilevel semantic annotation
of Catalan and Spanish. In Proceedings of SemEval-07.
S. Pado?, M. Pennacchiotti, and C. Sporleder. 2008. Se-
mantic role assignment for event nominalisations by
leveraging verbal data. In Proceedings of Coling-2008.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, and J. Dowding. 1986. Recovering
implicit information. In Proceedings of ACL-1986.
M. Palmer. 1990. Semantic Processing for Finite Do-
mains. CUP, Cambridge, England.
J. Ruppenhofer. 2004. The interaction of valence and
information structure. Ph.d., University of California,
Berkeley, CA.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In Proc. of EMNLP-07.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate arguments structures for infor-
mation extraction. In Proceedings of ACL-2003.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proceedings of CoNLL-2008, pages 159?177.
111
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 121?124,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Using Parallel Propbanks to enhance Word-alignments
Jinho D. Choi
Dept. of Computer Science
Univ. of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Dept. of Linguistics
Univ. of Colorado at Boulder
mpalmer@colorado.edu
Nianwen Xue
Dept. of Computer Science
Brandeis University
xuen@brandeis.edu
Abstract
This short paper describes the use of the
linguistic annotation available in paral-
lel PropBanks (Chinese and English) for
the enhancement of automatically derived
word alignments. Specifically, we sug-
gest ways to refine and expand word
alignments for verb-predicates by using
predicate-argument structures. Evalua-
tions demonstrate improved alignment ac-
curacies that vary by corpus type.
1 Introduction
Since verbs tend to be the roots of dependency re-
lations in a sentence (Palmer et al, 2005), when it
comes down to translations, finding correct map-
pings between verbs in a source and a target lan-
guage is very important. Many machine transla-
tion systems (Fraser and Marcu, 2007) use word-
alignment tools such as GIZA++ (Och and Ney,
2003) to retrieve word mappings between a source
and a target language. Although GIZA++ gives
well-structured alignments, it has limitations in
several ways. First, it is hard to verify if align-
ments generated by GIZA++ are correct. Second,
GIZA++ may not find alignments for low-frequent
words. Third, GIZA++ does not account for any
semantic information.
In this paper, we suggest a couple of ways to
enhance word-alignments for predicating expres-
sions such as verbs1. We restricted the source
and the target language to Chinese and English,
respectively. The goal is to use the linguistic
annotation available in parallel PropBanks (Xue
and Palmer, 2009) to refine and expand automatic
word-alignments. First, we check if the alignment
for each Chinese predicate, generated by GIZA++,
is also a predicate in English (Section 3). If it is,
we verify if the alignment is correct by matching
1Throughout the paper, all predicates refer to verbs.
their arguments (Section 4.1). If it is not, we find
an English predicate that has the maximum argu-
ment matching with the Chinese predicate (Sec-
tion 4.2). Finally, we evaluate the potential of the
enhanced word-alignments for providing a signif-
icant improvement over the GIZA++ baseline.
2 Parallel Corpus
We used the ?English Chinese Translation Tree-
bank? (ECTB), a parallel English-Chinese cor-
pus. In addition to the treebank syntactic struc-
ture, the corpus has also been annotated with
semantic role labels in the standard PropBank
style of Arg0, Arg1, etc., based on verb specific
frame file definitions (Xue and Palmer, 2009).
The corpus is divided into two parts: the Xin-
hua Chinese newswire with literal English trans-
lations (4,363 parallel sentences) and the Sino-
rama Chinese news magazine with non-literal En-
glish translations (12,600 parallel sentences). We
experimented with the two parts separately to
see how literal and non-literal translations affect
word-alignments.
3 Predicate Matching
For preprocessing, we ran GIZA++ on ECTB to
get word-alignments between Chinese and En-
glish. Then, for each Chinese predicate, we
checked if it is aligned to an English predicate by
using the gold-standard parallel Propbanks. Ta-
ble 1 shows how many Chinese predicates were
aligned to what kind of English words.
Only (45.3%-Xinhua, 19.1%-Sinorama) of Chi-
nese predicates were aligned to words that are
predicates in English. It is true that not all Chi-
nese verbs are supposed to be translated to verbs
in English, but that does not account for the num-
bers in Table 1. We therefore assume that there
are opportunities to enhance word-alignments for
Chinese and English predicates.
121
Alignment Xinhua Sinorama
Ch.pred? En.pred 5,842 7,643
Ch.pred? En.be 386 1,229
Ch.pred? En.else 2,489 8,726
Ch.pred? En.none 4,178 22,488
Total 12,895 40,086
Table 1: Results of predicate matching (Ch: Chi-
nese, En: English, pred: predicates, be: be-verbs,
else: non-verbs, none: no word). The numbers in-
dicate the amount of verb-tokens, not verb-types.
4 Argument Matching
For Chinese predicates aligned to English predi-
cates, we can verify the alignments by ?Top-down
argument matching?: given Chinese and English
predicates that are aligned, check if their argu-
ments are also aligned (arguments are found from
parallel Propbanks). The intuition is that if the
predicates are correctly aligned across the lan-
guages, their arguments should be aligned as well.
For Chinese predicates not aligned to any En-
glish words, we can find their potential English
alignments by ?Bottom-up argument matching?:
given a set of arguments for a such Chinese predi-
cate, find some English predicate whose set of ar-
guments has the most words aligned to words in
the Chinese arguments. If the words in the argu-
ments are mostly aligned (above a certain thresh-
old) across the languages, we suspect that the
predicates should be aligned as well.
4.1 Top-down Argument Matching (T-D)
Given a Chinese predicate pc aligned to an English
predicate pe, let Sc and Se be a set of arguments
for pc and pe, respectively. For each cai ? Sc, we
match it with some eaj ? Se that has the most
words aligned to words in cai. If such eaj ex-
ists, we count the number of aligned words, say
|cai ? eaj |; otherwise, the count is 0. Once the
matchings are done, we average the proportions
of the counts and if the average is above a certain
threshold, we consider the alignment is correct.
Let us look at the example in Table 2. Af-
ter the preprocessing, a Chinese predicate ????
is aligned to an English predicate ?set up? by
GIZA++. ???? has two arguments, Ch.Arg0 and
Ch.Arg1, retrieved from the Chinese Propbank.
For each Chinese argument, we search for some
argument of ?set? (from the English Propbank) that
? Chinese Sentence ?
: ?????????????????????
- Predicate: ??.01? set up
- Ch.Arg0: ????? those municipalities
- Ch.Arg1: ??????????
? fourteen border economic cooperation zones
? English Sentence ?
: At the same time it also sanctioned those municipalities
to set up fourteen border economic cooperation zones
- Predicate: set.03 (set up)
- En.Arg0: those municipalities
- En.Arg1: fourteen border economic cooperation zones
Table 2: Parallel sentences labelled with their se-
mantic roles
has the most words aligned. For instance, words
in Ch.Arg0, ??? ???, are aligned to ?those
municipalities? by GIZA++ so Ch.Arg0 finds
En.Arg0 as the one maximizes word-interscetions
(similar for Ch.Arg1 and En.Arg1). In this case,
the argument matchings for all pairs of arguments
are 100%, so we consider the alignment is correct.
Table 3 shows the average argument matching
scores for all pairs of Chinese and English predi-
cates. For each pair of predicates, ?macro-average?
measures the proportion of word-intersections for
each pair of Chinese and English arguments (with
the most words aligned) and averages the pro-
portions whereas ?micro-average? counts word-
intersections for all pairs of arguments (each pair
with the most words aligned) and divides it by the
total number of words in Chinese arguments.
? Sc = a set of Chinese arguments, cai ? Sc
? Se = a set of English arguments, eaj ? Se
? Macro average argument matching score
= 1|Sc|
?
?cai
(argmax(|cai ? eaj |)|cai| )
? Micro average argument matching score
=
?
?cai argmax(|cai ? eaj |)?
?cai |cai|
Xinhua Sinorama
Macro Avg. 80.55% 53.56%
Micro Avg. 83.91% 52.62%
Table 3: Average argument matching scores for
top-down argument matching
122
It is not surprising that Xinhua?s scores are
higher because the English sentences in Xinhua
are more literally translated than ones in Sinorama
so that it is easier to find correct alignments in Xin-
hua.
4.2 Bottom-Up Argument Matching (B-U)
A large portion of Chinese predicates are aligned
to no English words. For such Chinese predicate,
say pc, we check to see if there exists an English
predicate within the parallel sentence, say pe, that
is not aligned to any Chinese word and gives the
maximum micro-average score (Section 4.1) com-
pare to all other predicates in the English sen-
tence. If the micro-average score is above a certain
threshold, we align pc to pe.
The thresholds we used are 0.7 and 0.8. Thresh-
olds below 0.7 assumes too many alignments that
are incorrect and ones above 0.8 assumes too few
alignments to be useful. Table 4 shows the average
argument matching scores for alignments found by
bottom-up argument matching.
Xinhua Sinorama
Thresh. 0.7 0.8 0.7 0.8
Macro 80.74 83.99 77.70 82.86
Micro 82.63 86.46 79.45 85.07
Table 4: Average argument matching scores in
percentile for bottom-up argument matching
5 Evaluations
Evaluations are done by a Chinese-English bilin-
gual. We used a different English-Chinese paral-
lel corpus for evaluations. There are 100 paral-
lel sentences, 365 Chinese verb-tokens, and 273
Chinese verb-types in the corpus. We tested
word-alignments, refined and expanded by our ap-
proaches, on verb-types rather than verb-tokens
to avoid over-emphasizing multiple appearances
of a single type. Furthermore, we tested word-
alignments from Xinhua and Sinorama separately
to see how literal and non-literal translations affect
the outcomes.
5.1 Refining word-alignment
We used three kinds of measurements for compar-
isons: term coverage, term expansion, and align-
ment accuracy. ?Term coverage? shows how many
source terms (Chinese verb-types) are covered by
word-alignments found in each corpus. Out of
273 Chinese verb-types in the test corpus, (79-
Xinhua, 129-Sinorama) were covered by word-
alignments generated by GIZA++. ?Term expan-
sion? shows how many target terms (English verb-
types) are suggested for each of the covered source
terms. There are on average (1.77-Xinhua, 2.29-
Sinorama) English verb-types suggested for each
covered Chinese verb-type. ?Alignment accuracy?
shows how many of the suggested target terms are
correct. Among the suggested English verb-types,
(83.35%-Xinhua, 57.76%-Sinorama) were correct
on average.
The goal is to improve the alignment accu-
racy with minimum reduction of the term cov-
erage and expansion. To accomplish the goal,
we set a threshold for the T-D?s macro-average
score: for Chinese predicates aligned to English
predicates, we kept only alignments whose macro-
average scores meet or exceed a certain threshold.
The thresholds we chose are 0.4 and 0.5; lower
thresholds did not have much effect and higher
thresholds threw out too many alignments. Table 5
shows the results of three measurements with re-
spect to the thresholds (Note that all these align-
ments were generated by GIZA++).
Xinhua Sinorama
TH TC ATE AAA TC ATE AAA
0.0 79 1.77 83.35 129 2.29 57.76
0.4 76 1.72 83.54 93 1.8 65.88
0.5 76 1.68 83.71 62 1.58 78.09
Table 5: Results for alignment refinement (TH:
threshold, TC: term coverage, ATE: average term
expansion, AAA: average alignment accuracy in
percentage). The highest score for each measure-
ment is marked as bold.
As you can see, thresholds did not have much
effect on alignments found in Xinhua. This is
understandable because the translations in Xin-
hua are so literal that it was relatively easy for
GIZA++ to find correct alignments; in other
words, the alignments generated by GIZA++ were
already very accurate. However, for alignments
found in Sinorama, the average alignment accu-
racy increases radically as the threshold increases.
This implies that it is possible to refine word-
alignments found in a corpus containing many
non-literal translations by using T-D.
Notice that the term coverage for Sinorama de-
creases as the threshold increases. Considering
123
how much improvement it made for the average
alignment accuracy, we suspect that it filtered out
mostly ones that were incorrect alignments.
5.2 Expanding word-alignment
We used B-U to expand word-alignments for Chi-
nese predicates aligned to no English words. We
decided not to expand alignments for Chinese
predicates aligned to non-verb English words be-
cause GIZA++ generated alignments are more ac-
curate than ones found by B-U in general.
There are (22-Xinhua, 20-Sinorama) additional
verb-types covered by the expanded-alignments.
Note that these alignments are already filtered by
the micro-average score (Section 4.2). To refine
the alignments even more, we set a threshold on
the macro-average score as well. The thresholds
we used for the macro-average score are 0.6 and
0.7. Table 6 shows the results of the expanded-
alignments found in Xinhua and Sinorama.
Mac - 0.7 Mac - 0.8
TC ATE AAA TC ATE AAA
Mic Xinhua
0.0 22 4.27 50.38 20 3.35 57.50
0.6 21 3.9 54.76 18 3.39 63.89
0.7 19 3.47 55.26 17 3.12 61.76
Mic Sinorama
0.0 37 3.59 18.01 29 3.14 14.95
0.6 31 3.06 15.11 27 2.93 14.46
0.7 21 2.81 11.99 25 2.6 11.82
Table 6: Results for expanded-alignments found in
Xinhua and Sinorama (Mac: threshold on macro-
average score, Mic: threshold on micro-average
score)
The average alignment accuracy for Xinhua is
encouraging; it shows that B-U can expand word-
alignments for a corpus with literal translations.
The average alignment accuracy for Sinorama is
surprisingly low; it shows that B-U cannot func-
tion effectively given non-literal translations.
6 Summary and Future Works
We have demonstrated the potential for using par-
allel Propbanks to improve statistical verb transla-
tions from Chinese to English. Our B-U approach
shows promise for expanding the term-coverage
of GIZA++ alignments that are based on literal
translations. In contrast, our T-D is most effec-
tive with non-literal translations for verifying the
alignment accuracy, which has been proven diffi-
cult for GIZA++.
This is still a preliminary work but in the fu-
ture, we will try to enhance word-alignments
by using automatically labelled Propbanks, Nom-
banks (Meyers et al, 2004), Named-entity tag-
ging, and test the enhancement on bigger corpora.
Furthermore, we will also evaluate the integration
of our enhanced alignments with statistical ma-
chine translation systems.
Acknowledgments
Special thanks to Daniel Gildea, Ding Liu
(University of Rochester) who provided word-
alignments, Wei Wang (Information Sciences In-
stitute at University of Southern California) who
provided the test-corpus, and Hua Zhong (Uni-
versity of Colorado at Boulder) who performed
the evaluations. We gratefully acknowledge
the support of the National Science Foundation
Grants IIS-0325646, Domain Independent Seman-
tic Parsing, CISE-CRI-0551615, Towards a Com-
prehensive Linguistic Annotation, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-
C-0022, subcontract from BBN, Inc. Any contents
expressed in this material are those of the authors
and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The nombank project: An interim report. In HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24?31.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the chinese treebank. Natural Lan-
guage Engineering, 15(1):143?172.
124
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 186?189,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Multi-Representational and Multi-Layered  Treebank for Hindi/Urdu     Rajesh Bhatt  U. of Massachusetts                      Amherst, MA, USA                                    bhatt@linguist.umass.edu  Owen Rambow      Columbia University     New York, NY, USA   rambow@ccls.columbia.edu           
Bhuvana Narasimhan  U. of Colorado                      Boulder, CO, USA                                    narasimb@colorado.edu  Dipti Misra Sharma    Int?l Institute of Info. Technology  Hyderabad, India   dipti@iiit.ac.in                                           
Martha Palmer U. of Colorado Boulder, CO, USA mpalmer@colorado.edu  Fei Xia     University of Washington Seattle, WA, USA fxia@u.washington.edu 
Abstract 
This paper describes the simultaneous develop-ment of dependency structure and phrase structure treebanks for Hindi and Urdu, as well as a Prop-Bank.  The dependency structure and the Prop-Bank are manually annotated, and then the phrase structure treebank is produced automatically.  To ensure successful conversion the development of the guidelines for all three representations are care-fully coordinated.  1 Introduction Annotated corpora have played an increasingly important role in the training of supervised natu-ral language processing components. Today, treebanks have been constructed for many lan-guages, including Arabic, Chinese, Czech, Eng-lish, French, German, Korean, Spanish, and Turkish.  This paper describes the creation of a Hindi/Urdu multi-representational and multi-layered treebank.  Multi-layered means that we design the annotation process from the outset to include both a syntactic annotation and a lexical semantic annotation such as the English Prop-Bank (Palmer et al 2005). Multi-representational means that we distinguish con-ceptually what is being represented from how it is represented; for example, in a case of long-distance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent).  Having made this choice, we can determine how to represent it: For exam-ple, we can use a discontinuous constituent 
(crossing arcs), or we can use a trace and co-indexation.      Flexibility of representation is important be-cause the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al 1999) the automatic mapping from depend-ency to phrase-structure was a major area of re-search. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic represen-tations that applications can make immediate use of (McDonald et al 2005, CoNLL 2006 Shared Task).  We first provide a comparison of de-pendency structure and phrase structure in Sec-tion 2.  Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our con-version approach. 2 Two Kinds of Syntactic Structure  Two different approaches to describing syntactic structure, dependency structure (DS) (Mel??uk 1979) and phrase structure (PS) (Chomsky, 1981), have in a sense divided the field in two, with parallel efforts on both sides.  Formally, in a PS tree, all and only the leaf nodes are labeled 
186
with words from the sentence (or empty catego-ries), while the interior nodes are labeled with nonterminal labels. In a dependency tree, all nodes are labeled with words from the sentence (or empty categories). Linguistically, a PS groups consecutive words hierarchically into phrases (or constituents), and each phrase is as-signed a syntactic label. In a DS, syntactic de-pendency (i.e., the relation between a syntactic head and its arguments and adjuncts) is the pri-mary syntactic relation represented. The notion of constituent is only derived.   In a dependency representation, a node stands for itself, for the lexical category (or ?preterminal?) spanning only the word itself (e.g., N), and for its maximal projection spanning the node and all words in the subtree it anchors (e.g., NP). Thus, intermediate projections which cover only some of the dependents of a word (such as N? or VP) do not directly correspond to anything in a de-pendency representation. Attachments at the dif-ferent levels of projection are therefore not dis-tinguished in a dependency tree. This has certain ramifications for annotation.  Conisder for ex-ample scope in conjunctions.  The two readings of young men and women can be distinguished (are the women young as well or not?). If a de-pendency representation represents conjunction by treating the conjunction as a dependent to the first conjunct, then the two readings do not re-ceive different syntactic representations, unless a scope feature is introduced for the adjective.  Suppose y depends on x in a DS, we need to ad-dress the following questions in order to devise a DS-to-PS conversion algorithm that builds the corresponding phrase structure: 1) What kinds of projections do x and y have? 2) How far should y project before it attaches to x's projection? 3) What position on x's projection chain should y's projec-tion attach to?  These questions are answered by the annotation manual of the target PS represen-tation ? there are many possible answers. If the source dependency representation contains the right kind of information (for example, the scope of adjectives in conjunctions), and if the target phrase structure representation is well docu-mented, then we can devise a conversion algo-rithm.  Another important issue is that of ?non-projectivity? which is used to represent discon-tinuous constituents. Non-projectivity is common in dependency-based syntactic theories, but rare in phrase structure-based theories.  The next sec-
tion highlights our most salient representation choices in Treebank design. 3 Treebank Design Our goal is the delivery of a treebank that is multi-representational: it will have a syntactic dependency version and a phrase structure ver-sion. Another recent trend in treebanking is the addition of deeper, semantic levels of annotation on top of the syntactic annotations of the PTB, for example PropBank (Palmer et al 2005).  A multi-layered approach is also found in the Pra-gue Dependency Treebank (Haji? et al 2001), or in treebanks based on LFG (King et al 2003) or HPSG (Oepen et al 2002). A lesson learned here is that the addition of deeper, more semantic lev-els may be complicated if the syntactic annota-tion was not designed with the possibility of mul-tiple layers of annotation in mind. We therefore also propose a treebank that is from the start multi-layered: we will include a PropBank-style predicate-argument annotation in the release. Crucially, the lexical subcategorization frames that are made explicit during the process of prop-banking should always inform the syntactic structure of the treebanking effort. In addition, some of the distinctions made by PS that are not naturally present in DS, such as unaccusativity and null arguments, are more naturally made dur-ing PropBank annotation. Our current approach anticipates that the addition of the PropBank an-notation to the DS will provide a rich enough structure for accurate PS conversion.   In order to ensure successful conversion from DS to PS, we are simultaneously developing three sets of guidelines for Hindi: dependency struc-ture, phrase structure, and PropBank. While al-lowing DS and PS guidelines to be based on dif-ferent, independently motivated principles (see Section 4), we have been going through a com-prehensive list of constructions in Hindi, care-fully exploring any potentially problematic is-sues.  Specifically, we make sure that both DS and PS represent the same syntactic facts (what is represented): we know that if PS makes a dis-tinction that neither DS nor PropBank make, then we cannot possibly convert automatically. Fur-thermore, we coordinate the guidelines for DS and PS with respect to the examples chosen to support the conversion process.  These examples form a conversion test suite.  
187
4 Syntactic Annotation Choices  4.1 Dependency Structure Guidelines  Our dependency analysis is based on the Pan-inian grammatical model (Bharati et al1999, Sharma et al 2007). The model offers a syntac-tico-semantic level of linguistic knowledge with an especially transparent relationship between the syntax and the semantics.  The sentence is treated as a series of modifier-modified relations which has a primary modified (generally the main verb). The appropriate syntactic cues (rela-tion markers) help in identifying various rela-tions.  The relations are of two types ? karaka and others. 'Karakas' are the roles of various par-ticipants in an action (arguments). For a noun to hold a karaka relation with a verb, it is important that they (noun and verb) have a direct syntactic relation. Relations other than 'karaka' such as purpose, reason, and possession are also captured using the relational concepts of the model (ad-juncts). These argument labels are very similar in spirit to the verb specific semantic role labels used by PropBank, which have already been suc-cessfully mapped to richer semantic role labels from VerbNet and FrameNet. This suggests that much of the task of PropBanking can be done as part of the dependency annotation. 4.2 Phrase Structure Guidelines Our PS guidelines are inspired by the Principles-and-Parameters methodology, as instantiated by the theoretical developments starting with Gov-ernment and Binding Theory (Chomsky 1981). We assume binary branching. There are three theoretical commitments/design considerations that underlie the guidelines. First, any minimal clause distinguishes at most two positions struc-turally (the core arguments). These positions can be identified as the specifier of VP and the com-plement of V. With a transitive predicate, these positions are occupied by distinct NPs while with an unaccusative or passive, the same NP occu-pies both positions. All other NPs are represented as adjuncts. Second, we represent any displace-ment of core arguments from their canonical po-sitions, irrespective of whether a clause boundary is crossed, via traces. The displacement of other arguments is only represented if a clause bound-ary is crossed. Third, syntactic relationships such as agreement and case always require c-command but do not necessarily require a [speci-fier, head] configuration. Within these con-straints, we always choose the simplest structure 
compatible with the word order. We work with a very limited set of category labels (NP, AP, AdvP, VP, CP) assuming that finer distinctions between different kinds of verbal functional heads can be made via features.  4.3 Two Constructions in Hindi We give examples for two constructions in Hindi and show the DS and PS for each. Simple Transitive Clauses:  (1) raam-ne   khiir              khaayii    ram-erg   rice-pudding     ate    ?Ram ate rice-pudding.? The two main arguments of the Hindi verb in Figure 1(b) have dependency types k1 and k2.  They correspond roughly to subject and object, and they are the only arguments that can agree with the verb.  In the PS, Figure 1(a), the two arguments that correspond to k1 and k2 have fixed positions in the phrase structure as ex-plained in Section 4.2. 
 Figure 1: PS and DS for transitive clause in (1).  Unaccusative verbs: (2) darwaazaa  khul   rahaa           hai       door.M        open  Prog.MSg   be.Prs.Sg      ?The door is opening.?  Here, the issue is that the DS guidelines treats unaccusatives like other intransitives, with the surface argument simply annotated as k1.  In contrast, PS shows a derivation in which the sub-ject originates in object position.   
 Figure 2: PS and DS for the unaccusative  in  (2). 5 Conversion Process  The DS-to-PS conversion process has three steps. First, for each (DS, PS) pair appearing in the conversion test suite, we run a consistency 
188
checking algorithm to determine whether the DS and the PS are consistent. The inconsistent cases are studied manually and if the inconsistency cannot be resolved by changing the analyses used in the guidelines, a new DS that is consis-tent with the PS is proposed. We call this new dependency structure ?DScons? (?cons? for ?con-sistency?; DScons is the same as DS for the con-sistent cases). Because the DS and PS guidelines are carefully coordinated, we expect the incon-sistent cases to be rare and well-motivated. Sec-ond, conversion rules are extracted automatically from these (DScons, PS) pairs. Last, given a new DS, a PS is created by applying conversion rules. Note that non-projective DSs will be converted to projective DScons.  (For an alternate account of handling non-projective DSs, see Kuhlman and M?hl (2007).)  A preliminary study on the Eng-lish Penn Treebank showed promising results and error analyses indicated that most conversion errors were caused by ambiguous DS patterns in the conversion rules. This implies that including sufficient information in the input DS could re-duce ambiguity, significantly improving the per-formance of the conversion algorithm. The de-tails of the conversion algorithm and the experi-mental results are described in (Xia et al, 2009). 6 Conclusion We presented our approach to the joint develop-ment of DS and PS treebanks and a PropBank for Hindi/Urdu.  Since from the inception of the pro-ject we have planned manual annotation of DS and automatic conversion to PS, we are develop-ing the annotation guidelines for all structures in parallel.  A series of linguistic constructions with specific examples are being carefully examined for any DS annotation decisions that might result in inconsistency between DS and PS and/or mul-tiple conversion rules with identical DS patterns. Our preliminary studies yield promising results, indicating that coordinating the design of DS/PS and PropBank guidelines and running the con-version algorithm in the early stages is essential to the success of building a multi-representational and multi-layered treebank.   Acknowledgments This work is supported by NSF grants CNS-0751089, CNS-0751171, CNS-0751202, and CNS-0751213.    
References  A. Bharati, V. Chaitanya and R. Sangal. 1999. Natu-ral Language Processesing: A Paninian Per-spective, Prentice Hall of India, New Delhi. N. Chomsky. 1981. Lectures on Government and Binding: The Pisa Lectures. Holland: Foris Pub-lications.  M. Collins, Jan Haji?, L. Ramshaw and C. Tillmann. 1999. A Statistical Parser for Czech. In the Proc of ACL-1999, pages 505-512. J. Haji?, E. Hajicova, M. Holub, P. Pajas, P. Sgall, B. Vidova-Hladka, and V. Reznickova. 2001. The Current Status of the Prague Dependency Tree-bank. Lecture Notes in Artificial Intelligence (LNAI) 2166, pp 11?20, NY. T. H. King, R. Crouch, S. Riezler, M. Dalrymple and R. Kaplan. 2003. The PARC700 Dependency Bank. In Proc. of the 4th Int? Workshop on Linguistically Interpreted Corpora (LINC-2003), Budapest, Hungary. D. Klein and C. D. Manning. 2003. Accurate Unlexi-calized Parsing. In the Proc of ACL-2003,.Japan M. Kuhlmann and M. M?hl. 2007. Mildly context-sensitive dependency language. In the Proc of ACL 2007. Prague, Czech Republic. R. McDonald, F. Pereira, K. Ribarov and J. Haji?. 2005.  Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proc. of HLT-EMNLP 2005.  I. Mel?uk. 1979. Studies in Dependency Syntax. Karoma Publishers, Inc. S. Oepen, K. Toutanova, S. M. Shieber, C. D. Man-ning, D. Flickinger, and T. Brants, 2002. The LinGO Redwoods Treebank: Motivation and Pre-liminary Applications. In Proc. of COLING, 2002. Taipei, Taiwan. M. Palmer, D. Gildea, P. Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Seman-tic Roles.Computational Linguistics, 31(1):71-106. D. M. Sharma, R. Sangal, L. Bai, R. Begam, and K.V. Ramakrishnamacharyulu. 2007. AnnCorra : TreeBanks for Indian Languages, Annotation Guidelines (manuscript), IIIT, Hyderabad, India.  F. Xia, O. Rambow, R. Bhatt, M. Palmer and D. Sharma, 2009. Towards a Multi-Representational Treebank.  In Proc. of the 7th Int?lWorkshop on Treebanks and Linguistic Theories (TLT-7).  
189
Proceedings of the 8th International Conference on Computational Semantics, page 2,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Invited Talk
Knowing a word(sense) by its company
Martha Palmer
Department of Linguistics
University of Colorado
Boulder, Colorado, USA
Martha.Palmer@colorado.edu
Abstract
Supervised word sense disambiguation requires training corpora that
have been tagged with word senses, and these word senses typically come
from a pre-existing sense inventory. Space limitations imposed by dictio-
nary publishers have biased the field towards lists of discrete senses for an
individual lexeme. This approach does not capture information about relat-
edness of individual senses. How important is this information to knowing
which sense distinctions are critical for particular types of NLP applications?
How much does sense relatedness affect automatic word sense disambigua-
tion performance? Recent psycholinguistic evidence seems to indicate that
closely related word senses may be represented in the mental lexicon much
like a single sense, whereas distantly related senses may be represented more
like discrete entities. These results suggest that, for the purposes of WSD,
closely related word senses can be clustered together into a more general
sense with little meaning loss. This talk will describe the relatedness of verb
senses and its impact on NLP applications and WSD components as well as
recent psycholinguistic research results.
2
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1438?1442,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
The VerbCorner Project: Toward an Empirically-Based Semantic
Decomposition of Verbs
Joshua K. Hartshorne
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
77 Massachusetts Avenue
Cambridge, MA 02139, USA
jkhartshorne@gmail.com
Claire Bonial, Martha Palmer
Department of Linguistics
University of Colorado at Boulder
Hellems 290, 295 UCB
Boulder, CO 80309, USA
{CBonial, MPalmer}@colorado.edu
Abstract
This research describes efforts to use crowd-
sourcing to improve the validity of the seman-
tic predicates in VerbNet, a lexicon of about
6300 English verbs. The current semantic
predicates can be thought of semantic prim-
itives, into which the concepts denoted by a
verb can be decomposed. For example, the
verb spray (of the Spray class), involves the
predicates MOTION, NOT, and LOCATION,
where the event can be decomposed into an
AGENT causing a THEME that was originally
not in a particular location to now be in that
location. Although VerbNet?s predicates are
theoretically well-motivated, systematic em-
pirical data is scarce. This paper describes a
recently-launched attempt to address this issue
with a series of human judgment tasks, posed
to subjects in the form of games.
1 Introduction
One key application of Natural Language Processing
(NLP) is meaning extraction. Of particular impor-
tance is propositional meaning: To understand ?Jes-
sica sprayed paint on the wall,? it is not enough to
know who Jessica is, what paint is, and where the
wall is, but that, by the end of the event, some quan-
tity of paint that was not previously on the wall now
is. One must extract not only meanings for individ-
ual words but also the relations between them.
One option is to learn these relations in a largely
bottom-up, data-driven fashion (Chklovski and Pan-
tel, 2004; Poon and Domingos, 2009). For instance,
Poon and Domingos (2009) first extracts depen-
dency trees, converts those into quasi-logical form,
recursively induces lambda expressions from them,
and uses clustering to derive progressively abstract
knowledge.
An alternative is to take a human-inspired ap-
proach, mapping the linguistic input onto the kinds
of representations that linguistic and psychologi-
cal research suggests are the representations em-
ployed by humans. While the exact characteriza-
tion of meaning (and by extension, thought) remains
an area of active research in the cognitive sciences
(Margolis and Laurence, 1999), decades of research
in linguistics and psychology suggests that much of
the meaning of a sentence ? as well as its syntactic
structure ? can be accounted for by invoking a small
number of highly abstract semantic features (usu-
ally represented as predicates), such as causation,
agency, basic topological relations, and directed mo-
tion (Ambridge et al, 2013; Croft, 2012; Jackend-
off, 1990; Levin and Rappaport Hovav, 2005; Peset-
sky, 1995; Pinker, 1989). For instance, a given verb
can appear in some syntactic frames (Sally broke the
vase. Sally broke the vase with the hammer. The vase
broke.) and not others (*Sally broke the vase to the
floor. *Sally broke John the vase.). When verbs are
classified according to the syntactic frames they can
appear in, most if not all the verbs in a class involve
the same set of abstract semantic features.1
Interestingly, roughly these same features (causa-
tion, etc.) have been singled out by developmental
psychologists as part of ?core knowledge? ? a set of
early-learned or perhaps innate concepts upon which
1Whether all verbs in a class share the same abstract pred-
icates or merely most is an area of active research (Levin and
Rappaport Hovav, 2005).
1438
the rest of cognition is built (Spelke and Kinzler,
2007). Thus these semantic features/predicates may
be not only crucial to describing linguistic mean-
ing but may be central organizing principles for a
human?s (reasonably successful) thinking about and
conceptualization of the world. As such, they pro-
vide a potentially rewarding target for NLP.
2 VerbNet
2.1 Overview and Structure
Perhaps the most comprehensive implementation
of this approach appears in VerbNet (Kipper et al,
2008; based on Levin, 1993). VerbNet classifies
verbs based on the syntactic frames they can appear
in, providing a semantic description of each frame
for each class. An example entry is shown below:
Syntactic Frame NP V NP PP.DESTINATION
Example Jessica sprayed the wall.
Syntax AGENT V THEME {+LOC|+DEST CONF}
DESTINATION
Semantics MOTION(DURING(E), THEME)
NOT(PREP(START(E), THEME, DESTINATION))
PREP(END(E), THEME, DESTINATION)
CAUSE(AGENT, E)
The ?Syntactic Frame? provides a flat syntactic
parse. ?Syntax? provides semantic role labels for
each of the NPs and PPs, which are invoked in ?Se-
mantics?. VerbNet decomposes the semantics of
this sentence into four separate predicates: 1) the
THEME (the paint) moves doing the event E; 2) at
the start of the event E, the THEME (the paint) is
not at the DESTINATION (on the wall), whereas 3)
at the end of the event E, the THEME (the paint) is
at the DESTINATION (on the wall), and; 4) the event
is caused by the AGENT (Sally). Note that this cap-
tures only the core aspects of semantics shared by all
verbs in the class; differences between verbs in the
same class (e.g., spray vs. splash) are omitted.
Importantly, the semantics of the sentence is de-
pendent on both the matrix verb (paint) and the syn-
tactic frame. Famously, when inserted in the slightly
different frame NP V NP.DESTINATION PP.THEME
? ?Sally sprayed the wall with paint? ? ?spray? en-
tails that destination (the wall) is now fully painted,
an entailment that does not follow in the example
above (Pinker, 1989).
2.2 Uses and Limitations
VerbNet has been used in a variety of NLP appli-
cations, such as semantic role labeling (Swier and
Stevenson, 2004), inferencing (Zaenen et al, 2008),
verb classification (Joanis et al, 2008), and informa-
tion extraction (Maynard, Funk, and Peters, 2009).
While such applications have been successful thus
far, an important constraint on how well VerbNet-
based NLP applications can be expected to perform
is the accuracy of the semantics encoded in Verb-
Net. Here, several issues arise. Leaving aside mis-
categorized verbs and other inaccuracies, as noted
above VerbNet assumes that all verbs in the same
class share the same core predicates, which may or
may not be empirically justified. Given the number
of semantic predicates (146),2 verb entries (6580),
and unique verb lemmas (6284) it is not feasible for
a single research team to check, particularly since af-
ter a certain number of verbs, intuitions become less
clear. In any case, it may not be ideal to rely solely
on the intuitions of invested researchers, whose in-
tuitions about subtle judgments may be clouded by
theoretical commitments (Gibson and Federenko,
2013); the only way to ensure this is not the case
is through independent validation. Unfortunately, of
the 280 verb classes in VerbNet, this has been done
for only a few (cf Ambridge et al, 2013).
3 VerbCorner
The VerbCorner project was designed to address
these issues by crowd-sourcing the semantic judg-
ments online (gameswithwords.org/VerbCorner/).
Several previous projects have successfully crowd-
sourced linguistic annotations, such as Phrase De-
tectives, where volunteers have contributed 2.5 mil-
lion judgments on anaphoric relations (Poesio et al,
2012). Below, we outline the VerbCorner project
and describe one specific annotation task in detail.
3.1 Developing Semantic Annotation Tasks
Collecting accurate judgments on subtle questions
from naive participants with limited metalinguistic
2Note that these vary in applicability from those specific to
a small number of verbs (CHARACTERIZE, CONSPIRE) to those
frequently invoked (BEGIN, EXIST).
1439
skills is difficult. Rare is the non-linguist who can
immediately answer the question, ?Does the verb
?throw,? when used transitively, entail a change of
location on the part of its THEME?? Thus, we began
by developing tasks that isolate semantic features in
a way accessible to untrained annotators.
We converted the metalinguistic judgments
(?Does this verb entail this abstract predicate??) into
real-world problems, which previous research sug-
gests should be easier (Cosmides and Tooby, 1992).
Each judgment tasks involved a fanciful backstory.
For instance, in ?Simon Says Freeze?, a task de-
signed to elicit judgments about movement, the
Galactic Overlord (Simon) decrees ?Galactic Stay
Where You Are Day,? during which nobody is al-
lowed to move from their current location. Partici-
pants read descriptions of events and decide whether
anyone violated the rule. In ?Explode on Contact?,
designed to elicit judgments about physical contact,
objects and people explode when they touch one an-
other. The participant reads descriptions of events
and decides whether anything has exploded.3
Each task was piloted until inter-coder reliability
was acceptably high and the modal response nearly
always corresponded with researcher intuitions. As
such, these tasks cannot be used to establish whether
researcher intuitions for the pilot stimuli are correct
(this would be circular); however, there is no guar-
antee that agreement with the researcher will gener-
alize to new items (the pilot stimuli cover a trivial
proportion of all verbs in VerbNet).
3.2 Crowd-sourcing Semantic Judgments
The pilot experiments showed that it is possible to
elicit reliable semantic judgments corresponding to
VerbNet predicates from naive participants (see sec-
tion 3.3). At the project website, volunteers choose
one of the tasks from a list and begin tagging sen-
tences. The sentences are sampled smartly, avoid-
ing sentences already tagged by that volunteer and
biased in favor of of the sentences with the fewest
3Note that each task is designed to elicit judgments about
entailments ? things that must be true rather than are merely
likely to be true. If John greeted Bill, they might have come
into contact (e.g., by shaking hands), but perhaps they did not.
Previous work suggests that it is entailments that matter, partic-
ularly for explaining the syntactic behavior of verbs (Levin and
Rappaport Hovav, 2005)
judgments so far. Rather than assessing annotator
quality through gold standard trials with known an-
swers (which wastes data ? the answers to these tri-
als are known), approximately 150 sentences were
chosen to be ?over-sampled.? As the volunteer tags
sentences, approximately one out of every five are
from this over-sampled set until that volunteer has
tagged all of them. This guarantees that any given
volunteer will have tried some sentences targeted
by many other volunteers, allowing inter-annotator
agreement to be used to assess annotator quality.
Following the example of Zooniverse (zooni-
verse.org), a popular ?Citizen Science? platform,
volunteers are encouraged but required to register
(requiring registration prior to seeing the tasks was
found to be a significant barrier to entry). Regis-
tration allows collecting linguistic and educational
background from the volunteer, and also makes it
possible to track the same volunteer across sessions.
Multiple gamification elements were incorporated
into VerbCorner in order to recruit and motivate vol-
unteers. Each task has a leaderboard, where the
volunteer can see his/her rank out of all volunteers
in terms of number of contributions made. In ad-
dition, there is a general leaderboard, which sums
across tasks. Volunteers can earn badges, displayed
on their homepage, for answering certain numbers
of questions in each task. Finally, at random inter-
vals bonus points are awarded, with the explanation
for the bonus points tailored to the task?s backstory.
VerbCorner was launched on May 21, 2013. After
six weeks, 555 volunteers had provided at least one
annotation, for a total of 39,274 annotations, demon-
strating the feasibility of collecting large numbers of
annotations through this method.
3.3 Case Study: Equilibrium
?Equilibrium? was designed to elicit judgments
about application of force, frequently argued to be
a core semantic feature in the sense discussed above
(Pinker, 1989). The backstory involves the ?Zen Di-
mension,? in which nobody is allowed to exert force
on anything else. The participant reads descriptions
of events (Sally sprayed paint onto the wall) and de-
cides whether they would be allowable in the Zen
Dimension ? and, in particular, which participants
in the event are illegally applying force.
In order to minimize unwanted effects of world
1440
knowledge, the verb?s arguments are replaced with
nonsense words or randomly chosen proper names
(Sally sprayed the dax onto the blicket). In the
context of the story, this is explained as necessary
anonymization: You are a government official de-
termining whether certain activities are allowable,
and ensuring anonymity is an important safeguard
against favoritism and corruption. An alternative
wouod be to use multiple different content words,
randomly chosen for each annotator. However, this
greatly increases the number of annotators needed
and quickly becomes infeasible.
3.3.1 Pilot Results
The task was piloted on 138 sentences, which com-
prised all possible syntactic frames for three verbs
from each of five verb classes in VerbNet. After
two rounds of piloting (between the first and second,
wording in the backstory was adjusted for clarity
based on pilot subject feedback and results), Kripp?s
alpha reached .76 for 8 annotators, which represents
a reasonably high level of inter-annotator agreement.
Importantly, the modal response matched the intu-
itions of the researchers in 137 of 138 cases.4
3.3.2 Preliminary VerbCorner Results
?Equillibrium? was one of the first tasks posted on
VerbCorner, with data currently being collected on
12 of the 280 VerbNet classes, for a total of 5,171
sentences. As of writing, 414 users have submitted
14,294 judgments. Individual annotators annotated
anywhere from 1 to 195 sentences (mean=8, me-
dian=4). While most sentences have relatively few
judgments, each of the 194 over-sampled sentences
has between 15 and 20 judgments.5
Comparing the modal response with the re-
searchers? intuitions resulted in a match for 184 of
194 sentences. In general, where the modal response
4The remaining case was ?The crose smashed sondily.? for
which four pilot subjects thought involved the crose applying
force ? matching researcher intuition ? and four thought did
not involve any application of force, perhaps interpreting the
sentence was a passive.
5These are the same 15 verbs used in the piloting. The num-
ber of sentences is larger in order to test a wider range of pos-
sible arguments. In particular, wherever appropriate, separate
sentences were constructed using animate and inanimate argu-
ments. Compare Sally sprayed the dax onto Mary and Sally
sprayed the dax onto the blicket.
did not match researcher intuitions, the modal re-
sponse was itself not popular, comprising an aver-
age of 53% of responses, compared with an aver-
age of 77% where the modal response matched re-
searcher intuitions. Thus, these appear to be cases of
disagreement, either because the correct intuition re-
quires more work to obtain or because of differences
across idiolects (at the moment, there is no obvious
pattern as to which sentences caused difficulty, but
the sample size is small). Thus, follow-up investi-
gation of sentences with little inter-coder agreement
may be warranted.
4 Conclusion and Future Work
Data-collection is ongoing. VerbNet identifies ap-
proximately 150 different semantic predicates. An-
notating every verb in each of its syntactic frames for
each semantic predicate would take many millions
of judgments. However, most of the semantic predi-
cates employed in VerbNet are very narrow in scope
and only apply to a few classes. Thus, we have be-
gun with broad predicates that are thought to apply
to many verbs and are adding progressively narrower
predicates as work progresses. At the current rate,
we should complete annotation for the half-dozen
most frequent semantic predicates in the space of a
year.
Future work will explore using an individual
annotator?s history across trials to weight that
user?s contributions, something that VerbCorner was
specifically designed to allow (see above). How to
assess annotator quality without gold standard data
is an active area of research (Passonneau and Car-
penter, 2013; Rzhetsky, Shatkay and Wilbur, 2009;
Whitehill et al, 2009). For instance, Whitehill and
colleagues (2009) provide an algorithm for jointly
estimating both annotator quality and annotation
difficulty (including the latter is important because
some annotators will have low agreement with oth-
ers due to their poor luck in being assigned difficult-
to-annotate sentences). This algorithm is shown to
outperform using the modal response.
Note that this necessarily biases against annota-
tors with few responses. In our case study above, ex-
cluding annotators who contributed small numbers
of annotations led to progressively worse match to
researcher intuition, suggesting that the loss in data
1441
caused by excluding these annotations may not be
worth the increased confidence in annotation quality.
Future research will be needed to assess this trade-
off.
The above work shows the feasibility of crowd-
sourcing VerbNet semantic entailments, as has been
shown for a handful of other linguistic judgments
(Artignan, Hascoet and Lafourcade, 2009; Poesio et
al., 2012; Venhuizen et al, 2013). There are many
domains in which gold standard human judgments
are scarce; crowd-sourcing has considerable poten-
tial at addressing this need.
References
B. Ambridge, J. M. Pine, C. F. Rowland, F. Chang, and
A. Bidgood. 2013. The retreat from overgeneral-
ization in child language acquisition: Word learning,
morphology, and verb argument structure. Wiley In-
terdisciplinary Reviews: Cognitive Science. 4:47-62.
G. Artignan, M. Hascoet, and M. Lafourcade. 2009.
Mutliscale visual analysis of lexical networks. Pro-
ceedings of the 13th International Conference on In-
formation Visualisation. Barcelona, Spain.
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining
the Web for fine-grained semantic relations. Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing (EMNLP). Barcelona, Spain.
L. Cosmides and J. Tooby. 1992. Cognitive adaptations
for social exchange. in The Adapted Mind. (J. Barkow,
L. Cosmides, and J. Tooby, Eds.) Oxford University
Press, Oxford, UK.
W. Croft. 2012. Verbs: Aspect and Argument Structure.
Oxford University Press, Oxford, UK.
D. R. Dowty. 1991. Thematic proto-roles and argument
selection. Language. 67:547-619.
E. Gibson and E. Fedorenko. 2013. The need for quanti-
tative methods in syntax and semantics research. Lan-
guage and Cognitive Processes. 28(1-2):88?124.
R. Jackendoff. 1990. Semantic Structures. The MIT
Press, Cambridge, MA.
E. Joanis, S. Stevenson, and D. James. 2008. A general
feature space for automatic verb classification. Natu-
ral Language Engineering. 14(3):337-367.
K. Kipper, A. Korhonen, N. Ryant and M. Palmer. 2008.
A large-scale classification of English verbs. Lan-
guage Resources and Evaluation Journal, 42:21?40
E. Margolis and S. Laurence 1999. Concepts: Core
Readings. The MIT Press, Cambridge, MA.
B. Levin. 1993. English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago.
B. Levin and M. Rappaport Hovav. 2005. Argument
Realization. Cambridge University Press, Cambridge,
UK.
D. Maynard, A. Funk, and W. Peters. 2009. Using
lexico-syntactic ontology design patterns for ontology
creation and population. Proceedings of Workshop on
Ontology Patterns (WOP 2009). Washington, DC
R. J. Passonneau and B. Carpenter 2013. The benefits
of a model of annotation. 7th Linguistic Annotation
Workshop and Interoperability with Discourse. Sofia,
Bulgaria.
D. Pesetsky. 1995. Zero Syntax: Experiencers and Cas-
cades. The MIT Press, Cambridge, MA.
S. Pinker. 1989. Learnability and Cognition. The MIT
Press, Cambridge, MA.
M. Poesio, J. Camberlain, U. Kruschwitz, L. Robaldo,
and L. Ducceschi. 2012. The Phrase Detective Multi-
lingual Corpus, Release 0.1. Proceedings of the Col-
laborative Resource Development and Delivery Work-
shop. Istanbul, Turkey
H. Poon and P. Domingos. 2009. Unsupervised seman-
tic parsing. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
Singapore.
A. Rzhetsky, H. Shatkay, and W. J. Wilbur. 2009. How
to get the most out of your curation effort. PLoS Com-
putational Biology, 5(5):1?13.
E. S. Spelke and K. D. Kinzler. 2007. Core knowledge.
Developmental Science, 10(1):89?96.
R. Swier and S. Stevenson. 2004. Unsupervised seman-
tic role labeling. Proceedings of the Generative Lexi-
con Conference, GenLex-09. Pisa, Italy.
N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013.
Gamification for word sense labeling. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013). Potsdam, Germany
J. Whitehill, P. Ruvolo, T. F. Wu, J. Bergsma. and J.
Movellan. 2009. Whose vote should count more: Op-
timal integration of labels from labelers of unknown
expertise. Advances in Neural Information Processing
Systems, 22. Vancouver, Canada
A. Zaenen, C. Condoravdi, and D G. Bobrow. 2008. The
encoding of lexical implications in VN. Proceedings
of LREC 2008. Morocco
1442
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 58?67,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Inducing Example-based Semantic Frames
from a Massive Amount of Verb Uses
Daisuke Kawahara
?
Daniel W. Peterson
?
Octavian Popescu
?
Martha Palmer
?
?
Kyoto University, Kyoto, Japan
?
University of Colorado at Boulder, Boulder, CO, USA
?
Fondazione Bruno Kessler, Trento, Italy
dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu, popescu@fbk.eu
Abstract
We present an unsupervised method for in-
ducing semantic frames from verb uses in
giga-word corpora. Our semantic frames
are verb-specific example-based frames
that are distinguished according to their
senses. We use the Chinese Restau-
rant Process to automatically induce these
frames from a massive amount of verb in-
stances. In our experiments, we acquire
broad-coverage semantic frames from two
giga-word corpora, the larger comprising
20 billion words. Our experimental results
indicate the effectiveness of our approach.
1 Introduction
Semantic frames are indispensable knowledge for
semantic analysis or text understanding. In the
last decade, semantic frames, such as FrameNet
(Baker et al., 1998) and PropBank (Palmer et al.,
2005), have been manually elaborated. These
resources are effectively exploited in many nat-
ural language processing (NLP) tasks, includ-
ing not only semantic parsing but also ma-
chine translation (Boas, 2002), information ex-
traction (Surdeanu et al., 2003), question answer-
ing (Narayanan and Harabagiu, 2004), paraphrase
acquisition (Ellsworth and Janin, 2007) and recog-
nition of textual entailment (Burchardt and Frank,
2006).
There have been many attempts to automati-
cally acquire frame knowledge from raw corpora
with the goal of either adding frequency informa-
tion to an existing resource or of inducing simi-
lar frames for other languages. Most of these ap-
proaches, however, focus on syntactic frames, i.e.,
subcategorization frames (e.g., (Manning, 1993;
Briscoe and Carroll, 1997; Korhonen et al., 2006;
Lippincott et al., 2012; Reichart and Korhonen,
2013)). Since subcategorization frames represent
argument patterns of verbs and are purely syn-
tactic, expressions that have the same subcatego-
rization frame can have different meanings (e.g.,
metaphors). Semantics-oriented NLP applications
based on frames, such as paraphrase acquisition
and machine translation, require consistency in the
meaning of each frame, and thus these subcatego-
rization frames are not suitable for these semantic
tasks.
Recently, there have been a few studies on au-
tomatically acquiring semantic frames (Materna,
2012; Materna, 2013). Materna induced seman-
tic frames (called LDA-Frames) from triples of
(subject, verb, object) in the British National
Corpus (BNC) based on Latent Dirichlet Allo-
cation (LDA) and the Dirichlet Process. LDA-
Frames capture limited linguistic phenomena of
these triples, and are defined across verbs based
on probabilistic topic distributions.
This paper presents a method for automati-
cally building verb-specific semantic frames from
a large raw corpus. Our semantic frames are verb-
specific like PropBank and semantically distin-
guished. A frame has several syntactic case slots,
each of which consists of words that are eligible to
fill the slot. For example, let us show three seman-
tic frames of the verb ?observe?:
1
observe:1
nsubj:{we, author, ...} dobj:{effect, result, ...}
prep in:{study, case, ...} ...
observe:2
nsubj:{teacher, we, ...} dobj:{child, student, ...}
prep in:{classroom, school, ...} ...
observe:3
nsubj:{child, people, ...} dobj:{bird, animal, ...}
prep at:{range, time, ...} ...
1
In this paper, we use the dependency relation names
of the Stanford collapsed dependencies (de Marneffe et al.,
2006) as the notations of case slots. For instance, ?nsubj?
means a nominal subject, ?dobj? means a direct object, ?iboj?
means an indirect object, ?ccomp? means a clausal comple-
ment and ?prep *? means a preposition.
58
Frequencies, which are not shown in the above ex-
amples, are attached to each semantic frame, case
slot and word, and can be effectively exploited for
the applications of these semantic frames. The fre-
quencies of words in each case slot become good
sources of selectional preferences.
Our novel contributions are summarized as fol-
lows:
? induction of semantic frames based on the
Chinese Restaurant Process (Aldous, 1985)
from only automatic parses of a web-scale
corpus,
? exploitation of the assumption of one sense
per collocation (Yarowsky, 1993) to make the
computation feasible,
? providing broad-coverage knowledge for se-
lectional preferences, and
? evaluating induced semantic frames by us-
ing an existing annotated corpus with verb
classes.
2 Related Work
The most closely related work to our semantic
frames are LDA-Frames, which are probabilistic
semantic frames automatically induced from a raw
corpus (Materna, 2012; Materna, 2013). He used a
model based on LDA and the Dirichlet Process to
cluster verb instances of a triple (subject, verb, ob-
ject) to produce semantic frames and slots. Both
of these are represented as a probabilistic distri-
bution of words across verbs. He applied this
method to the BNC and acquired 427 frames and
144 slots (Materna, 2013). These frames are over-
generalized across verbs and might be difficult
to provide with fine-grained selectional prefer-
ences. In addition, Grenager and Manning (2006)
proposed a method for inducing PropBank-style
frames from Stanford typed dependencies ex-
tracted from raw corpora. Although these frames
are based on typed dependencies and more seman-
tic than subcategorization frames, they are not dis-
tinguished in terms of the senses of words filling a
case slot.
There are hand-crafted semantic frames in the
lexicons of FrameNet (Baker et al., 1998) and
PropBank (Palmer et al., 2005). Corpus Pattern
Analysis (CPA) frames (Hanks, 2012) are another
manually created repository of patterns for verbs.
Each pattern represents a prototypical word usage
as extracted by lexicographers from the BNC. Cre-
ating CPA is time consuming, but our proposed
method may be employed to assist in the creation
of this type of resource, as shown in Section 4.4.
Our task can be regarded as clustering of verb
instances. In this respect, the models of Parisien
and Stevenson are related to our method (Parisien
and Stevenson, 2009; Parisien and Stevenson,
2010). Parisien and Stevenson (2009) proposed
a Dirichlet Process model for clustering usages
of the verb ?get.? Later, Parisien and Stevenson
(2010) proposed a Hierarchical Dirichlet Process
model for jointly clustering argument structures
(i.e., subcategorization frames) and verb classes.
However, their argument structures are not seman-
tic but syntactic, and also they did not evaluate the
resulting frames. There have also been related ap-
proaches to clustering verb types (Vlachos et al.,
2009; Sun and Korhonen, 2009; Falk et al., 2012;
Reichart and Korhonen, 2013). These methods in-
duce verb clusters in which multiple verbs partic-
ipate, and do not consider the polysemy of verbs.
Our objective is different from theirs.
Another line of related work is unsupervised
semantic parsing or semantic role labeling (Poon
and Domingos, 2009; Lang and Lapata, 2010;
Lang and Lapata, 2011a; Lang and Lapata, 2011b;
Titov and Klementiev, 2011; Titov and Klemen-
tiev, 2012). These approaches basically clus-
ter predicates and their arguments to distinguish
predicate senses and semantic roles of arguments.
Modi et al. (2012) extended the model of Titov and
Klementiev (2012) to jointly induce semantic roles
and frames using the Chinese Restaurant Process,
which is also used in our approach. However,
they did not aim at building a lexicon of semantic
frames, but at distinguishing verbs that have dif-
ferent senses in a relatively small annotated cor-
pus. Applying this method to a large corpus could
produce a frame lexicon, but its scalability would
be a big problem.
For other languages than English, Kawahara
and Kurohashi (2006a) proposed a method for au-
tomatically compiling Japanese semantic frames
from a large web corpus. They applied con-
ventional agglomerative clustering to predicate-
argument structures using word/frame similarity
based on a manually-crafted thesaurus. Since
Japanese is head-final and has case-marking post-
positions, it seems easier to build semantic frames
with it than with other languages such as English.
They also achieved an improvement in depen-
dency parsing and predicate-argument structure
59
analysis by using their resulting frames (Kawahara
and Kurohashi, 2006b).
3 Method for Inducing Semantic Frames
Our objective is to automatically induce verb-
specific example-based semantic frames. Each se-
mantic frame consists of a partial set of syntactic
slots: nsubj, dobj, iobj, ccomp and prep *. Each
slot consists of words with frequencies, which
could provide broad-coverage selectional prefer-
ences.
Frames for a verb should be semantically distin-
guished. That is to say, each frame should consist
of predicate-argument structures that have consis-
tent usages or meanings.
Our procedure to automatically generate seman-
tic frames from verb usages is as follows:
1. apply dependency parsing to a raw corpus
and extract predicate-argument structures for
each verb from the automatic parses,
2. merge the predicate-argument structures that
have presumably the same meaning based on
the assumption of one sense per collocation
to get a set of initial frames, and
3. apply clustering to the initial frames based
on the Chinese Restaurant Process to produce
the final semantic frames.
Each of these steps is described in the following
sections in detail.
3.1 Extracting Predicate-argument
Structures from a Raw Corpus
We first apply dependency parsing to a large raw
corpus. We use the Stanford parser with Stanford
dependencies (de Marneffe et al., 2006).
2
Col-
lapsed dependencies are adopted to directly extract
prepositional phrases.
Then, we extract predicate-argument structures
from the dependency parses. Dependents that have
the following dependency relations to a verb are
extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp,
prep ?
Here, we do not distinguish adjuncts from argu-
ments. All extracted dependents of a verb are han-
dled as arguments. This distinction is left for fu-
ture work, but this will be performed using slot
2
http://nlp.stanford.edu/software/lex-parser.shtml
Sentences:
They observed the effects of ...
This statistical ability to observe an effect ...
We did not observe a residual effect of ...
He could observe the results at the same time ...
My first opportunity to observe the results of ...
You can observe beautiful birds ...
Children may then observe birds ...
.
.
.
Predicate-argument structures:
nsubj:they observe dobj:effect
observe dobj:effect
nsubj:we observe dobj:effect
nsubj:he observe dobj:result prep at:time
observe dobj:result
nsubj:you observe dobj:bird
nsubj:child observe dobj:bird
.
.
.
Initial frames:
nsubj:{they, we, ...} observe dobj:{effect}
nsubj:{he, ...} observe dobj:{result} prep at:{time}
nsubj:{you, child, ...} observe dobj:{bird}
.
.
.
Figure 1: Examples of predicate-argument struc-
tures and initial frames for the verb ?observe.?
frequencies in the applications of semantic frames
or the method proposed by Abend and Rappoport
(2010).
We apply the following processes to extracted
predicate-argument structures:
? A verb and an argument are lemmatized, and
only the head of an argument is preserved for
compound nouns.
? Phrasal verbs are also distinguished from
non-phrasal verbs. For example, ?look up?
has independent frames from ?look.?
? The passive voice of a verb is distinguished
from the active voice, and thus these have in-
dependent frames. Passive voice is detected
using the part-of-speech tag ?VBN? (past
participle). The alignment between frames of
active and passive voices will be done after
the induction of frames using the model of
Sasano et al. (2013) in the future.
? ?xcomp? (open clausal complement) is re-
named to ?ccomp? (clausal complement) and
?xsubj? (controlling subject) is renamed to
?nsubj? (nominal subject). This is because
60
these usages as predicate-argument structures
are not different.
? A capitalized argument with the part-of
speech ?NNP? (singular proper noun) or
?NNPS? (plural proper noun) is general-
ized to ?name?. Similarly, an argument of
?ccomp? is generalized to ?comp? since the
content of a clausal complement is not impor-
tant.
Extracted predicate-argument structures are
collected for each verb and the subsequent pro-
cesses are applied to the predicate-argument struc-
tures of each verb. Figure 1 shows examples of
predicate-argument structures for ?observe.?
3.2 Constructing Initial Frames from
Predicate-argument Structures
A straightforward way to produce semantic frames
is to cluster the extracted predicate-argument
structures directly. Since our objective is to com-
pile broad-coverage semantic frames, a massive
amount of predicate-argument structures should
be fed into the clustering. It would take prohibitive
computational costs to conduct the sampling pro-
cedure, which is described in the next section.
To make the computation feasible, we merge the
predicate-argument structures that have the same
or similar meaning to get initial frames. These ini-
tial frames are the input of the subsequent cluster-
ing process. For this merge, we assume one sense
per collocation (Yarowsky, 1993) for predicate-
argument structures.
For each predicate-argument structure of a verb,
we couple the verb and an argument to make a unit
for sense disambiguation. We select an argument
in the following order by considering the degree of
effect on the verb sense:
3
dobj, ccomp, nsubj, prep ?, iobj.
This selection of a predominant argument order
above is justified by relative comparisons of the
discriminative power of the different slots for CPA
frames (Popescu, 2013). If a predicate-argument
structure does not have any of the above slots, it is
discarded.
Then, the predicate-argument structures that
have the same verb and argument pair (slot and
3
If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.
word, e.g., ?dobj:effect?) are merged into an ini-
tial frame (Figure 1). After this process, we dis-
card minor initial frames that occur fewer than 10
times.
For example, we have 732,292 instances
(predicate-argument structures) for the verb ?ob-
serve? in the web corpus that is used in our exper-
iment (its details are described in Section 4.1). As
the result of this merging process, we obtain 6,530
initial frames, which become an input for the clus-
tering. This means that this process accelerates the
speed of clustering more than 100 times.
The precision of this process will be evaluated
in Section 4.3.
3.3 Clustering using Chinese Restaurant
Process
We cluster initial frames for each verb to produce
final semantic frames using the Chinese Restau-
rant Process (Aldous, 1985). We regard each ini-
tial frame as an instance in the usual clustering of
the Chinese Restaurant Process.
We calculate the posterior probability of a se-
mantic frame f
j
given an initial frame v
i
as fol-
lows:
P (f
j
|v
i
) ?
{
n(f
j
)
N+?
? P (v
i
|f
j
) f
j
?= new
?
N+?
? P (v
i
|f
j
) f
j
= new,
(1)
where N is the number of initial frames for the
target verb and n(f
j
) is the current number of ini-
tial frames assigned to the semantic frame f
j
. ?
is a hyper-parameter that determines how likely
it is for a new semantic frame to be created. In
this equation, the first term is the Dirichlet process
prior and the second term is the likelihood of v
i
.
P (v
i
|f
j
) is defined based on the Dirichlet-
Multinomial distribution as follows:
P (v
i
|f
j
) =
?
w?V
P (w|f
j
)
count(v
i
,w)
, (2)
where V is the vocabulary in all case slots cooc-
curring with the verb. It is distinguished by
the case slot, and thus consists of pairs of slots
and words, e.g., ?nsubj:child? and ?dobj:bird.?
count(v
i
, w) is the number of w in the initial
frame v
i
.
P (w|f
j
) is defined as follows:
P (w|f
j
) =
count(f
j
, w) + ?
?
t?V
count(f
j
, t) + |V | ? ?
, (3)
61
where count(f
j
, w) is the current number of w in
the frame f
j
, and ? is a hyper-parameter of Dirich-
let distribution. For a new semantic frame, this
probability is uniform (1/|V |).
We use Gibbs sampling to realize this cluster-
ing.
4 Experiments and Evaluations
4.1 Experimental Settings
We use two kinds of large-scale corpora: a web
corpus and the English Gigaword corpus.
To prepare a web corpus, we first crawled the
web. We extracted sentences from each web
page that seems to be written in English based
on the encoding information. Then, we selected
sentences that consist of at most 40 words, and
removed duplicated sentences. From this pro-
cess, we obtained a corpus of one billion sen-
tences, totaling approximately 20 billion words.
We focused on verbs whose frequency was more
than 1,000. There were 19,649 verbs, includ-
ing phrasal verbs, and separating passive and ac-
tive constructions. We extracted 2,032,774,982
predicate-argument structures.
We also used the English Gigaword corpus
(LDC2011T07; English Gigaword Fifth Edition)
to induce semantic frames. This corpus consists
of approximately 180 million sentences, which to-
taling four billion words. There were 7,356 verbs
after applying the same frequency threshold as the
web corpus. We extracted 423,778,278 predicate-
argument structures from this corpus.
We set the hyper-parameters ? in (1) and ? in
(3) to 1.0. The frame assignments for all the com-
ponents were initialized randomly. We took 100
samples for each initial frame and selected the
frame assignment that has the highest probability.
These parameters were determined according to a
preliminary experiment to manually examine the
quality of resulting frames.
4.2 Experimental Results
We executed the per-verb clustering tasks on a PC
cluster. It finished within a few hours for most
verbs, but it took a couple of days for very frequent
verbs, such as ?get? and ?say.? The clustering pro-
duced an average number of semantic frames per
verb of 15.2 for the web corpus and 18.5 for the
Gigaword corpus. Examples of induced semantic
frames from the web corpus are shown in Table 1.
slot instances
nsubj i:5850, we:5201, he:3796, you:3669, ...
dobj what:7091, people:2272, this:2262, ...
observe:1
prep in way:254, world:204, life:194, ...
.
.
.
nsubj we:11135, you:1321, i:1317, ...
dobj change:5091, difference:2719, ...
observe:2
prep in study:622, case:382, cell:362, ...
.
.
.
nsubj student:3921, i:2240, we:2174, ...
dobj child:2323, class:2184, student:2025, ...
observe:3
prep in classroom:555, action:509, ...
.
.
.
nsubj we:44833, i:6873, order:4051, ...
dobj card:28835, payment:22569, ...
accept:1
prep for payment:1166, convenience:1147, ...
.
.
.
nsubj i:10568, we:9300, you:5106, ...
dobj that:14180, this:12061, it:7756, ...
accept:2
prep as part:1879, fact:1085, truth:926, ...
.
.
.
nsubj people:7459, he:6696, we:5515, ...
dobj christ:13766, jesus:6528, it:5612, ...
accept:3
prep as savior:5591, lord:597, one:469, ...
.
.
.
Table 1: Examples of resulting frames for the verb
?observe? and ?accept? induced from the web cor-
pus. The number following an instance word rep-
resents its frequency.
4.3 Evaluation of Induced Semantic Frames
We evaluate precision and coverage of induced se-
mantic frames. To measure the precision of in-
duced semantic frames, we adopt the purity met-
ric, which is usually used to evaluate clustering re-
sults. However, the problem is that it is impossible
to assign gold-standard classes to the huge num-
ber of instances. To automatically measure the
purity of the induced semantic frames, we make
use of the SemLink corpus (Loper et al., 2007), in
which VerbNet classes (Kipper-Schuler, 2005) and
PropBank/FrameNet frames are assigned to each
instance. We make a test set that contains 157 pol-
ysemous verbs that occur 10 or more times in the
SemLink corpus (sections 02-21 of the Wall Street
Journal). We first add these instances to the in-
stances from a raw corpus and apply clustering to
these merged instances. Then, we compare the in-
duced semantic frames of the SemLink instances
with their gold-standard classes. We adopt Verb-
Net classes and PropBank frames as gold-standard
classes.
For each group of verb-specific semantic
frames, we measure the purity of the frames as the
percentage of SemLink instances belonging to the
majority gold class in their respective cluster. Let
62
PU CO F
1
Mac Mic Mac Mic Mac Mic
against One frame 0.799 0.802 0.917 0.952 0.854 0.870
VerbNet Initial frames 0.985 0.982 0.755 0.812 0.855 0.889
Induced sem frames 0.900 0.901 0.886 0.928 0.893 0.914
against One frame 0.901 0.872 ? ? 0.909 0.910
PropBank Initial frames 0.994 0.993 ? ? 0.858 0.893
Induced sem frames 0.965 0.949 ? ? 0.924 0.939
Table 2: Evaluation results of semantic frames from the web corpus against VerbNet classes and Prop-
Bank frames. ?Mac? means a macro average and ?Mic? means a micro average.
PU CO F
1
Mac Mic Mac Mic Mac Mic
against One frame 0.799 0.804 0.855 0.920 0.826 0.858
VerbNet Initial frames 0.985 0.981 0.666 0.758 0.795 0.855
Induced sem frames 0.916 0.909 0.796 0.880 0.852 0.894
against One frame 0.901 0.874 ? ? 0.877 0.896
PropBank Initial frames 0.994 0.993 ? ? 0.798 0.859
Induced sem frames 0.968 0.953 ? ? 0.874 0.915
Table 3: Evaluation results of semantic frames from the Gigaword corpus against VerbNet classes and
PropBank frames. ?Mac? means a macro average and ?Mic? means a micro average.
N denote the total number of SemLink instances
of the target verb, G
j
the set of instances belong-
ing to the j-th gold class and F
i
the set of instances
belonging to the i-th frame. The purity (PU) can
then be written as follows:
PU =
1
N
?
i
max
j
|G
j
? F
i
|. (4)
For example, a frame of the verb ?observe? con-
tains 11 SemLink instances, and eight out of them
belong to the class SAY-37.7, which is the ma-
jority class among these 11 instances. PU is cal-
culated by summing up such counts over all the
frames of this verb.
Usually, inverse purity or collocation is used
to measure the recall of normal clustering tasks.
However, these recall measures do not fit our task.
This is because it is not a real error to have similar
separate frames. Instead, we want to avoid hav-
ing so many frames that we cannot provide broad-
coverage selectional preferences due to sparsity.
To judge this aspect, we measure coverage.
The coverage (CO) measures to what extent
predicate-argument structures of the target verb in
a test set are included in one of frames of the verb.
We use the predicate-argument structures of the
above 157 verbs from the SemLink corpus, which
are the same ones used in the evaluation of PU.
We judge a predicate-argument structure as cor-
rect if all of its argument words (of the target slot
described in Section 3.1) are included in the corre-
sponding slot of a frame. If the clustering gets bet-
ter, the value of CO will get higher, because merg-
ing instances by clustering alleviates data sparsity.
These per-verb scores are aggregated into an
overall score by averaging over all verbs. We use
two ways of averaging: a macro average and a mi-
cro average. The macro average is a simple av-
erage of scores for individual verbs. The micro
average is obtained by weighting the scores for in-
dividual verbs proportional to the number of in-
stances for that verb. Finally, we use the harmonic
mean (F
1
) of purity and coverage as a single mea-
sure of clustering quality.
For comparison, we adopt the following two
baseline methods:
One frame a frame into which all the instances
for a verb are merged
Initial frames the initial frames without cluster-
ing (described in Section 3.2)
Table 2 and Table 3 list evaluation results for
semantic frames induced from the web corpus and
the Gigaword corpus, respectively.
4
Note that CO
does not consider gold-standard classes, and thus
the values of CO are the same for the VerbNet
4
We did not adopt inverse purity, but its values for the
induced semantic frames range from 0.42 to 0.49.
63
and PropBank evaluations. The induced frames
outperformed the two baseline methods in terms
of F
1
in most cases. While the coverage of the
web frames was higher than that of the Giga-
word frames, as expected, the purity of the web
frames was slightly lower than that of the Giga-
word frames. This degradation might be caused
by the noise in the web corpus.
The purity of the initial frames was around
98%-99%, which means that there were few cases
that the one-sense-per-collocation assumption was
violated.
Modi et al. (2012) reported a purity of 77.9%
for the assignment of FrameNet frames to the
FrameNet corpus. We also conducted the above
purity evaluation against FrameNet frames for 140
verbs.
5
We obtained a macro average of 92.9%
and a micro average of 89.2% for the web frames,
and a macro average of 93.2% and a micro average
of 89.8% for the Gigaword frames. It is difficult
to directly compare these results with Modi et al.
(2012), but our frame assignments seem to have
higher accuracy.
4.4 Evaluation against CPA Frames
Corpus Pattern Analysis (CPA) is a technique for
linking word usage to prototypical syntagmatic
patterns.
6
The resource was built manually by in-
vestigating examples in the BNC, and the set of
corpus examples used to induce each pattern is
given. For example, the following three patterns
describe the usage of the verb ?accommodate.?
[Human 1] accommodate [Human 2]
[Building] accommodate [Eventuality]
[Human] accommodate [Self] to [Eventuality]
In this paper, we use CPA to evaluate the quality
of the automatically induced frames. By compar-
ing the induced frames to CPA patterns, we can
evaluate the correctness and relevance of this ap-
proach from a human point of view. To do that,
we associate semantic features to the set of words
in each slot in the frames, using SUMO (Niles
and Pease, 2001). For example, take the follow-
ing frame for the verb ?accomplish?:
accomplish:1
nsubj:{you, leader, employee, ...}
dobj:{developing, progress, objective, ...}.
5
Since FrameNet frames are not assigned to all the verbs
of SemLink, the number of verbs is different from the evalu-
ations against VerbNet and PropBank.
6
http://deb.fi.muni.cz/pdev/
all K-means
Entropy (E) 0.790 0.516
Recovery Rate (RC) 0.347 0.630
Purity (P ) 0.462 0.696
Table 4: CPA Evaluation.
Using SUMO, we map this frame to the following:
nsubj: [Human]
dobj: [SubjectiveAssessmentAttribute],
which corresponds to pattern 3 for ?accomplish?
in CPA.
We also associate SUMO attributes to the CPA
patterns with more than 10 examples (716 verbs).
There are many patterns of SUMO attributes for
any CPA frame or induced frame, since each
filler word in a particular slot can have more
than one SUMO attribute. We filter out the
non-discriminative SUMO attributes following the
technique described in Popescu (2013). Using
this, we obtain SUMO attributes for both CPA
clusters and induced frames, and we can use the
standard entropy-based measures to evaluate the
match between the two types of patterns: E ? en-
tropy, RC ? recovery rate, and P ? purity (Li et
al., 2004):
E =
K
?
j=1
m
j
m
? e
j
, RC = 1 ?
K,L
?
j,i=1
p
ij
m
i
, (5)
P =
K
?
j=1
m
j
m
? p
j
, p
j
= max
i
p
ij
, (6)
e
j
=
L
?
i=1
p
ij
log
2
p
ij
, p
ij
=
m
ij
m
i
, (7)
where m
j
is the number of induced frames corre-
sponding to topic j, m
ij
is the number of induced
frames in cluster j and annotated with the CPA
pattern i, m is the total number of induced frames,
L is the number of CPA patterns, and K is the
number of induced frames.
We also consider a K-means clustering process,
with K set as 2 or 3 depending on the number of
SUMO-attributed patterns. The K-means evalu-
ation is carried out considering only the centroid
of the cluster, which corresponds to the prototypi-
cal induced semantic frame with SUMO attributes.
We compute E, RC and P using formulae (5) -
(7) for each verb and then compute the macro av-
erage, considering all the frames and only the K-
means centroids, respectively. The results for the
induced web frames are displayed in Table 4.
64
The evaluation method presented here over-
comes some of the drawbacks of the previous ap-
proaches (Materna, 2012; Materna, 2013). First,
we did not limit the evaluation to the most frequent
patterns. Second, the mapping was carried out au-
tomatically and not by hand. The results above
compare favorably with the previous approaches,
especially considering that no filtering procedures
were applied to the induced frames. We anticipate
that the results based on the prototypical induced
frames with SUMO attributes would be competi-
tive. Our post-analysis revealed that the entropy
can be lowered further if an automatic filtering
based on frequencies is applied.
4.5 Evaluation of the Quality of Selectional
Preferences
We also investigated the quality of selectional
preferences within the induced semantic frames.
The only publicly available test data for selectional
preferences, to our knowledge, is from Chambers
and Jurafsky (2010). This data consists of quadru-
ples (verb, relation, word, confounder) and does
not contain their context.
7
A typical way for using our semantic frames is
to select an appropriate frame for an input sen-
tence and judge the eligibility of the word uses
against the selected frame. However, due to the
lack of context for the above data, it is difficult to
select a corresponding semantic frame for a test
quadruple and thus the induced semantic frames
cannot be naturally applied to this data. To in-
vestigate the potential for selectional preferences
of the semantic frames, we approximately match
a quadruple with each of the semantic frames of
the verb and select the frame that has the highest
probability as follows:
P (w) = max
i
P (w|v, rel, f
i
), (8)
where w is the word or confounder, v is the verb,
rel is the relation and f
i
is a semantic frame. By
comparing the probabilities of the word and the
confounder, we select either of them according to
the higher probability. For tie breaking in the case
that no frames are found for the verb or both the
word and confounder are not found in the case slot,
we randomly select either of them in the same way
as Chambers and Jurafsky (2010).
We use the ?neighbor frequency? set, which is
the most difficult among the three sets included
7
A document ID of the English Gigaword corpus is avail-
able, but it is difficult to recover the context of each instance
from this information.
in the data. It contains 6,767 quadruples and the
relations consist of three classes: subject, object
and preposition, which has no distinction of ac-
tual prepositions. To link these relations with our
case slots, we manually aligned the subject with
the nsubj (nominal subject) slot, the object with
the dobj (direct object) slot and the preposition
with prep * (all the prepositions) slots. For the
preposition relation, we choose the highest prob-
ability among all the preposition slots in a frame.
To match the generalized ?name? with the word in
a quadruple, we change the word to ?name? if it is
capitalized and not a capitalized personal pronoun.
Our semantic frames from the Gigaword corpus
achieved an accuracy of 81.7%
8
and those from
the web corpus achieved an accuracy of 80.2%.
This slight deterioration seems to come from the
noise in the web corpus. The best performance
in Chambers and Jurafsky (2010) is 81.7% on
this ?neighbor frequency? set, which was achieved
by conditional probabilities with the Erk (2007)?s
smoothing method calculated from the English Gi-
gaword corpus. Our approach for selectional pref-
erences does not use smoothing like Erk (2007),
but it achieved equivalent performance to the pre-
vious work. If we applied our semantic frames to a
verb instance with its context, a more precise judg-
ment of selectional preferences would be possible
with appropriate frame selection.
5 Conclusion
This paper has described an unsupervised method
for inducing semantic frames from instances of
each verb in giga-word corpora. This method is
clustering based on the Chinese Restaurant Pro-
cess. The resulting frame data are open to the pub-
lic and also can be searched by inputting a verb via
our web interface.
9
As applications of the resulting frames, we plan
to integrate them into syntactic parsing, semantic
role labeling and verb sense disambiguation. For
instance, Kawahara and Kurohashi (2006b) im-
proved accuracy of dependency parsing based on
Japanese semantic frames automatically induced
from a large raw corpus. It is valuable and promis-
ing to apply our semantic frames to these NLP
tasks.
8
Since the dataset was created from the NYT 2001 portion
of the English Gigaword Corpus, we built semantic frames
again from the Gigaword corpus except this part.
9
http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
65
Acknowledgments
This work was supported by Kyoto University
John Mung Program and JST CREST. We grate-
fully acknowledge the support of the National Sci-
ence Foundation Grant NSF 1116782 - RI: Small:
A Bayesian Approach to Dynamic Lexical Re-
sources for Flexible Language Processing. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
References
Omri Abend and Ari Rappoport. 2010. Fully unsuper-
vised core-adjunct argument classification. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 226?236.
David Aldous. 1985. Exchangeability and related top-
ics.
?
Ecole d?
?
Et?e de Probabilit?es de Saint-Flour XIII
?1983, pages 1?198.
Collin Baker, Charles J. Fillmore, and John Lowe.
1998. The Berkeley FrameNet Project. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 86?90.
Hans C. Boas. 2002. Bilingual framenet dictionaries
for machine translation. In Proceedings of the 3rd
International Conference on Language Resources
and Evaluation, pages 1364?1371.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th Conference on Applied Natural
Language Processing, pages 356?363.
Aljoscha Burchardt and Anette Frank. 2006. Approx-
imating textual entailment with LFG and FrameNet
frames. In Proceedings of the 2nd PASCAL Recog-
nizing Textual Entailment Workshop, pages 92?97.
Nathanael Chambers and Daniel Jurafsky. 2010. Im-
proving the use of pseudo-words for evaluating se-
lectional preferences. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 445?453.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?
454.
Michael Ellsworth and Adam Janin. 2007. Mu-
taphrase: Paraphrasing with framenet. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 143?150.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 216?223.
Ingrid Falk, Claire Gardent, and Jean-Charles Lamirel.
2012. Classifying french verbs using french and en-
glish lexical resources. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 854?863.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 1?
8.
Patrick Hanks. 2012. How people use words to make
meanings: Semantic types meet valencies. Input,
Process and Product: Developments in Teaching
and Language Corpora, pages 54?69.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using high-
performance computing. In Proceedings of the 5th
International Conference on Language Resources
and Evaluation, pages 1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, pages 176?183.
Karin Kipper-Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 345?352.
Joel Lang and Mirella Lapata. 2010. Unsuper-
vised induction of semantic roles. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 939?947.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1117?1126.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
1320?1331.
Tao Li, Sheng Ma, and Mitsunori Ogihara. 2004.
Entropy-based criterion in categorical clustering. In
Proceedings of the 21st International Conference on
Machine Learning, volume 4, pages 536?543.
66
Thomas Lippincott, Anna Korhonen, and Diarmuid
?
O S?eaghdha. 2012. Learning syntactic verb frames
using graphical models. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 420?429.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics.
Christopher Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
In Proceedings of the 31st Annual Meeting of the As-
sociation for Computational Linguistics, pages 235?
242.
Ji?r?? Materna. 2012. LDA-Frames: An unsupervised
approach to generating semantic frames. In Alexan-
der Gelbukh, editor, Proceedings of the 13th Inter-
national Conference CICLing 2012, Part I, volume
7181 of Lecture Notes in Computer Science, pages
376?387. Springer Berlin / Heidelberg.
Ji?r?? Materna. 2013. Parameter estimation for LDA-
Frames. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 482?486.
Ashutosh Modi, Ivan Titov, and Alexandre Klementiev.
2012. Unsupervised induction of frame-semantic
representations. In Proceedings of the NAACL-HLT
Workshop on the Induction of Linguistic Structure,
pages 1?7.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 693?701.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In Proceedings of the International
Conference on Formal Ontology in Information Sys-
tems, pages 2?9.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Christopher Parisien and Suzanne Stevenson. 2009.
Modelling the acquisition of verb polysemy in chil-
dren. In Proceedings of the CogSci2009 Workshop
on Distributional Semantics beyond Concrete Con-
cepts, pages 17?22.
Christopher Parisien and Suzanne Stevenson. 2010.
Learning verb alternations in a usage-based
Bayesian model. In Proceedings of the 32nd annual
meeting of the Cognitive Science Society.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10.
Octavian Popescu. 2013. Learning corpus patterns us-
ing finite state automata. In Proceedings of the 10th
International Conference on Computational Seman-
tics, pages 191?203.
Roi Reichart and Anna Korhonen. 2013. Improved
lexical acquisition through DPP-based verb cluster-
ing. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 862?872.
Ryohei Sasano, Daisuke Kawahara, Sadao Kurohashi,
and Manabu Okumura. 2013. Automatic knowl-
edge acquisition for case alternation between the
passive and active voices in Japanese. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1213?1223.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 638?647.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8?15.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1445?1455.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12?22.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geomet-
rical Models of Natural Language Semantics, pages
74?82.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the Workshop on Human Language
Technology, pages 266?271.
67
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 13?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Multilingual Propbank Annotation Tools:
Cornerstone and Jubilee
Jinho D. Choi, Claire Bonial, Martha Palmer
University of Colorado at Boulder, USA
{choijd;bonial;mpalmer}@colorado.edu
Abstract
This paper demonstrates two annotation
tools related to Propbank: Cornerstone
and Jubilee. Propbank is a corpus in
which the arguments of each verb pred-
icate are annotated with their semantic
roles. Propbank annotation also requires
the choice of a sense id for each predicate,
defined in the corresponding frameset file.
Jubilee expedites the annotation process
by displaying several resources of syntactic
and semantic information simultaneously;
easy access to each of these resources al-
lows the annotator to quickly absorb and
apply the necessary syntactic and semantic
information pertinent to each predicate for
consistent and efficient annotation. Cor-
nerstone is a user-friendly xml editor, cus-
tomized to allow frame authors to create
and edit frameset files. Both tools have
been successfully adapted to many Prop-
bank projects; they run platform indepen-
dently, are light enough to run as X11 ap-
plications and support multiple languages
such as Arabic, Chinese, English, Hindi
and Korean.
1 Introduction
Propbank is a corpus in which the arguments of
each verb predicate are annotated with their se-
mantic roles (Palmer et al, 2005). Propbank an-
notation also requires the choice of a sense id for
each predicate. Thus, for each predicate in the
Propbank, there exists a corresponding frame-
set file encompassing one or more senses of the
predicate. All frameset files are written in xml,
which is somewhat difficult to read and edit. Al-
though there already exist many xml editors,
most of them require some degree of knowledge
of xml, and none of them are specifically cus-
tomized for frameset files. This motivated the
development of our own frameset editor, Cor-
nerstone.
Jubilee is a Propbank instance editor. For
each verb predicate, we create a Propbank in-
stance that consists of the predicate?s sense id
and its arguments labeled with semantic roles.
Previously the allocation of tasks, the annota-
tion of argument labels and the frameset tagging
were all done as separate tasks. With Jubilee,
the entire annotation procedure can be done us-
ing one tool that simultaneously provides rich
syntactic information as well as comprehensive
semantic information.
Both Cornerstone and Jubilee are developed
in Java (Jdk 6.0), so they run on any plat-
form where the Java virtual machine is installed.
They are light enough to run as X11 applica-
tions. This aspect is important because Prop-
bank data are usually stored in a server, so
annotators need to update them remotely (via
ssh). One of the biggest advantages of using
these tools is that they accommodate several
languages; in fact, the tools have been used
for Propbank projects in Arabic (M.Diab et al,
2008), Chinese (Xue and Palmer, 2009), En-
glish (Palmer et al, 2005) and Hindi, and have
been tested in Korean (Han et al, 2002).
This demo paper details how to create Prop-
bank framesets in Cornerstone, and how to an-
notate Propbank instances using Jubilee. There
are two modes in which to run Cornerstone:
multi-lemma and uni-lemma mode. In multi-
lemma mode, a predicate can have multiple lem-
13
mas, whereas a predicate can have only one
lemma in uni-lemma mode. Jubilee also has
two modes: normal and gold mode. In normal
mode, annotators are allowed to view and edit
only tasks that have been claimed by themselves
or by one other annotator. In gold mode, adju-
dicators are allowed to view and edit all tasks
that have undergone at least single-annotation.
2 How to obtain the tools
Cornerstone and Jubilee are available as an open
source project on Google code.1 The webpage
gives detailed instructions of how to download,
install and launch the tools (Choi et al, 2009a;
Choi et al, 2009b).
3 Description of Cornerstone
3.1 Multi-lemma mode
Languages such as English and Hindi are ex-
pected to run in multi-lemma mode, due to the
nature of their verb predicates. In multi-lemma
mode, a predicate can have multiple lemmas
(e.g., ?run?, ?run out?, ?run up?). The xml struc-
ture of the frameset files for such langauges is
defined in a dtd file, frameset.dtd.
Figure 1 shows what appears when you open
a frameset file, run.xml, in multi-lemma mode.
The window consists of four panes: the frame-
set pane, predicate pane, roleset pane and roles
pane. The frameset pane contains a frameset
note reserved for information that pertains to all
predicate lemmas and rolesets within the frame-
set file. The predicate pane contains one or more
tabs titled by predicate lemmas that may in-
clude verb particle constructions. The roleset
pane contains tabs titled by roleset ids (e.g.,
run.01, run.02, corresponding to different senses
of the predicate) for the currently selected predi-
cate lemma (e.g., ?run?). The roles pane includes
one or more roles, which represent arguments
that the predicate requires or commonly takes
in usage.
3.2 Uni-lemma mode
Languages such as Arabic and Chinese are ex-
pected to run in uni-lemma mode. Unlike multi-
1http://code.google.com/p/propbank/
Figure 1: Open run.xml in multi-lemma mode
lemma mode, which allows a predicate to have
multiple lemmas, uni-lemma mode allows only
one lemma for a predicate. The xml structure
of the frameset files for such langauges is defined
in a dtd file, verb.dtd.
Figure 2: Open HAfaZ.xml in uni-lemma mode
Figure 2 shows what appears when you open a
frameset file, HAfaZ.xml, in uni-lemma mode.
The window consists of four panes: the verb
pane, frameset pane, frame pane and roles pane.
The verb pane contains a verb comment field
for information helpful to annotators about the
verb, as well as the attribute field, ID, which in-
dicates the predicate lemma of the verb, repre-
sented either in the Roman alphabet or charac-
ters in other languages. The frameset pane con-
tains several tabs titled by frameset ids (corre-
sponding to verb senses) for the predicate. The
frame pane contains a frame comment for op-
14
tional information about the frame and the map-
ping pane, which includes mappings between
syntactic constituents and semantic arguments.
The roles pane consists of a set of arguments
that the predicate requires or commonly takes.
4 Description of Jubilee
4.1 Normal mode
Annotators are expected to run Jubilee in
normal mode. In normal mode, annotators
are allowed to view and edit only tasks claimed
by themselves or one other annotator when
the max-number of annotators allowed is two.
Jubilee gives the option of assigning a different
max-number of annotators as well.
When you run Jubilee in normal mode, you
will see an open-dialog (Figure 3). There are
three components in the open-dialog. The
combo-box at the top shows a list of all Prop-
bank projects. Once you select a project (e.g.,
english.sample), both [New Tasks] and [My
Tasks] will be updated. [New Task] shows a
list of tasks that have either not been claimed,
or claimed by only one other annotator. [My
Tasks] shows a list of tasks that have been
claimed by the current annotator.
Figure 3: Open-dialog
Once you choose a task and click the [Enter]
button, Jubilee?s main window will be prompted
(Figure 4). There are three views available in
the main window: the treebank view, frame-
set view and argument view. By default, the
treebank view shows the first tree (in the Penn
Treebank format (Marcus et al, 1993)) in the
selected task. The frameset view displays role-
sets and allows the annotator to choose the sense
of the predicate with respect to the current tree.
The argument view contains buttons represent-
ing each of the Propbank argument labels.
Figure 4: Jubilee?s main window
4.2 Gold mode
Adjudicators are expected to run Jubilee in gold
mode. In gold mode, adjudicators are allowed to
view and edit all tasks that have undergone at
least single-annotation. When you run Jubilee
in gold mode, you will see the same open-dialog
as you saw in Figure. 3. The [New Tasks] shows
a list of tasks that have not been adjudicated,
and the [My Tasks] shows a list of tasks that
have been adjudicated. Gold mode does not al-
low adjudicators to open tasks that have not
been at least single-annotated.
5 Demonstrations
5.1 Cornerstone
We will begin by demonstrating how to view
frameset files in both multi-lemma and uni-
lemma mode. In each mode, we will open an
existing frameset file, compare its interface with
the actual xml file, and show how intuitive it is
to interact with the tool. Next, we will demon-
strate how to create and edit a new frameset file
either from scratch or using an existing frameset
file. This demonstration will reflect several ad-
vantages of using the tool. First, the xml struc-
ture is completely transparent to the frame au-
thors, so that no knowledge of xml is required to
manage the frameset files. Second, the tool au-
tomates some of the routine work for the frame
authors (e.g., assigning a new roleset/frameset
id) and gives lists of options to be chosen (e.g.,
15
a list of function tags) so that frameset creation,
and the entire annotation procedure in turn, be-
come much faster. Third, the tool checks for the
completion of required fields and formatting er-
rors so that frame authors do not have to check
them manually. Finally, the tool automatically
saves the changes so the work is never lost.
5.2 Jubilee
For the treebank view, we will compare Jubilee?s
graphical representation of the trees with the
parenthetical representation of former tools: the
clear visual representation of the phrase struc-
ture helps the annotator to better understand
the syntax of the instance and to annotate the
appropriate node within the correct span. For
the frameset view, we will detail what kind of
semantic information it provides as you choose
different rolesets. This will highlight how Ju-
bilee?s support of roleset id annotation not only
speeds up the annotation process, but also en-
sures consistent annotation because the roleset
information provides a guideline for the correct
annotation of a particular verb sense. For the
argument view, we will illustrate how to anno-
tate Propbank arguments and use the opera-
tors for concatenations and links; thereby also
demonstrating that having each of these labels
clearly visible helps the annotator to remember
and evaluate the appropriateness of each possi-
ble argument label. Finally, we will show how
intuitive it is to adjudicate the annotations in
gold mode.
6 Future work
Both Cornerstone and Jubilee have been suc-
cessfully adapted to Propbank projects in sev-
eral universities such as Brandeis University, the
University of Colorado at Boulder, and the Uni-
versity of Illinois at Urbana-Champaign. We
will continuously develop the tools by improv-
ing their functionalities through user-testing and
feedback, and also by applying them to more
languages.
Acknowledgments
Special thanks are due to Prof. Nianwen Xue of
Brandeis University for his very helpful insights
as well as Scott Cotton, the developer of RATS
and Tom Morton, the developer of WordFreak,
both previously used for PropBank annotation.
We gratefully acknowledge the support of the
National Science Foundation Grants CISE-CRI-
0551615, Towards a Comprehensive Linguistic
Annotation and CISE-CRI 0709167, Collabo-
rative: A Multi-Representational and Multi-
Layered Treebank for Hindi/Urdu, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-
C-0022, subcontract from BBN, Inc. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views
of the National Science Foundation.
References
Jinho D. Choi, Claire Bonial, and Martha Palmer.
2009a. Cornerstone: Propbank frameset editor
guideline (version 1.3). Technical report, Institute
of Cognitive Science, the University of Colorado at
Boulder.
Jinho D. Choi, Claire Bonial, and Martha Palmer.
2009b. Jubilee: Propbank instance editor guide-
line (version 2.1). Technical report, Institute of
Cognitive Science, the University of Colorado at
Boulder.
C. Han, N. Han, E. Ko, and M. Palmer. 2002. Ko-
rean treebank: Development and evaluation. In
Proceedings of the 3rd International Conference on
Language Resources and Evaluation.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
putational Linguistics, 19(2):313?330.
M.Diab, A.Mansouri, M.Palmer, O.Babko-Malaya,
W Zaghouani, A.Bies, and M.Maamouri. 2008.
A pilot arabic propbank. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Nianwen Xue and Martha Palmer. 2009. Adding
semantic roles to the chinese treebank. Natural
Language Engineering, 15(1):143?172.
16
Tutorials, NAACL-HLT 2013, pages 10?12,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Semantic Role Labeling
Martha Palmer?, Ivan Titov?, Shumin Wu?
?University of Colorado
?Saarland University
Martha.Palmer@colorado.edu
titovian,wushumin@gmail.com
1 Overview
This tutorial will describe semantic role labeling, the assignment of semantic roles
to eventuality participants in an attempt to approximate a semantic representation
of an utterance. The linguistic background and motivation for the definition of
semantic roles will be presented, as well as the basic approach to semantic role
annotation of large amounts of corpora. Recent extensions to this approach that
encompass light verb constructions and predicative adjectives will be included,
with reference to their impact on English, Arabic, Hindi and Chinese. Current
proposed extensions such as Abstract Meaning Representations and richer event
representations will also be touched on.
Details of machine learning approaches will be provided, beginning with fully
supervised approaches that use the annotated corpora as training material. The
importance of syntactic parse information and the contributions of different feature
choices, including tree kernels, will be discussed, as well as the advantages and
disadvantages of particular machine learning algorithms and approaches such as
joint inference. Appropriate considerations for evaluation will be presented as well
as successful uses of semantic role labeling in NLP applications.
We will also cover techniques for exploiting unlabeled corpora and transfer-
ring models across languages. These include methods, which project annotations
across languages using parallel data, induce representations solely from unlabeled
corpora (unsupervised methods) or exploit a combination of a small amount of hu-
man annotation and a large unlabeled corpus (semi-supervised techniques). We
will discuss methods based on different machine learning paradigms, including
generative Bayesian models, graph-based algorithms and bootstrapping style tech-
niques.
10
2 Outline
I. Introduction, background and annotation
? Motivation ? who did what to whom
? Linguistic Background
? Basic Annotation approach
? Recent extensions
? Language Specific issues with English, Arabic, Hindi and Chinese
? Semlink ? Mapping between PropBank, VerbNet and FrameNet.
? The next step ? Events and Abstract Meaning Representations
II. Supervised Machine Learning for SRL
? Identification and Classification
? Features (tree kernel, English vs. Chinese)
? Choice of ML method and feature combinations (kernel vs feature space)
? Joint Inference
? Impact of Parsing
? Evaluation
? Applications (including multi-lingual)
III. Semi-supervised and Unsupervised Approaches
? Cross-lingual annotation projection methods and direct transfer of SRL mod-
els across languages
? Semi-supervised learning methods
? Unsupervised induction
? Adding supervision and linguistic priors to unsupervised methods
11
3 Speaker Bios
Martha Palmer1 is a Professor of Linguistics and Computer Science, and a Fel-
low of the Institute of Cognitive Science at the University of Colorado. Her current
research is aimed at building domain-independent and language independent tech-
niques for semantic interpretation based on linguistically annotated data, such as
Proposition Banks. She has been the PI on NSF, NIH and DARPA projects for
linguistic annotation (syntax, semantics and pragmatics) of English, Chinese, Ko-
rean, Arabic and Hindi. She has been a member of the Advisory Committee for
the DARPA TIDES program, Chair of SIGLEX, Chair of SIGHAN, a past Presi-
dent of the Association for Computational Linguistics, and is a Co-Editor of JNLE
and of LiLT and is on the CL Editorial Board. She received her Ph.D. in Artificial
Intelligence from the University of Edinburgh in 1985.
Ivan Titov2 joined the Saarland University as a junior faculty and head of a
research group in November 2009, following a postdoc at the University of Illi-
nois at Urbana-Champaign. He received his Ph.D. in Computer Science from the
University of Geneva in 2008 and his master?s degree in Applied Mathematics
and Informatics from the St. Petersburg State Polytechnic University (Russia) in
2003. His research interests are in statistical natural language processing (models
of syntax, semantics and sentiment) and machine learning (structured prediction
methods, latent variable models, Bayesian methods).
Shumin Wu is a Computer Science PhD student (advised by Dr. Martha
Palmer) at the University of Colorado. His current research is aimed at developing
and applying semantic mapping (aligning and jointly inferring predicate-argument
structures between languages) to Chinese dropped-pronoun recovery/alignment,
automatic verb class induction, and other applications relevant to machine transla-
tion.
1http://verbs.colorado.edu/?mpalmer/
2http://people.mmci.uni-saarland.de/?titov/
12
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 6?10,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Good Seed Makes a Good Crop:
Accelerating Active Learning Using Language Modeling
Dmitriy Dligach
Department of Computer Science
University of Colorado at Boulder
Dmitriy.Dligach@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
Martha.Palmer@colorado.edu
Abstract
Active Learning (AL) is typically initialized
with a small seed of examples selected ran-
domly. However, when the distribution of
classes in the data is skewed, some classes
may be missed, resulting in a slow learning
progress. Our contribution is twofold: (1) we
show that an unsupervised language modeling
based technique is effective in selecting rare
class examples, and (2) we use this technique
for seeding AL and demonstrate that it leads
to a higher learning rate. The evaluation is
conducted in the context of word sense disam-
biguation.
1 Introduction
Active learning (AL) (Settles, 2009) has become a
popular research field due to its potential benefits: it
can lead to drastic reductions in the amount of anno-
tation that is necessary for training a highly accurate
statistical classifier. Unlike in a random sampling
approach, where unlabeled data is selected for anno-
tation randomly, AL delegates the selection of un-
labeled data to the classifier. In a typical AL setup,
a classifier is trained on a small sample of the data
(usually selected randomly), known as the seed ex-
amples. The classifier is subsequently applied to a
pool of unlabeled data with the purpose of selecting
additional examples that the classifier views as infor-
mative. The selected data is annotated and the cycle
is repeated, allowing the learner to quickly refine the
decision boundary between the classes.
Unfortunately, AL is susceptible to a shortcom-
ing known as the missed cluster effect (Schu?tze et
al., 2006) and its special case called the missed class
effect (Tomanek et al, 2009). The missed cluster ef-
fect is a consequence of the fact that seed examples
influence the direction the learner takes in its ex-
ploration of the instance space. Whenever the seed
does not contain the examples of a certain cluster
that is representative of a group of examples in the
data, the learner may become overconfident about
the class membership of this cluster (particularly if it
lies far from the decision boundary). As a result, the
learner spends a lot of time exploring one region of
the instance space at the expense of missing another.
This problem can become especially severe, when
the class distribution in the data is skewed: a ran-
domly selected seed may not adequately represent
all the classes or even miss certain classes altogether.
Consider a binary classification task where rare class
examples constitute 5% of the data (a frequent sce-
nario in e.g. word sense disambiguation). If 10
examples are chosen randomly for seeding AL, the
probability that none of the rare class examples will
make it to the seed is 60% 1. Thus, there is a high
probability that AL would stall, selecting only the
examples of the predominant class over the course
of many iterations. At the same time, if we had a
way to ensure that examples of the rare class were
present in the seed, AL would be able to select the
examples of both classes, efficiently clarifying the
decision boundary and ultimately producing an ac-
curate classifier.
Tomanek et al (2009) simulated these scenarios
using manually constructed seed sets. They demon-
strated that seeding AL with a data set that is artifi-
cially enriched with rare class examples indeed leads
to a higher learning rate comparing to randomly
1Calculated using Binomial distribution
6
sampled and predominant class enriched seeds. In
this paper, we propose a simple automatic approach
for selecting the seeds that are rich in the examples
of the rare class. We then demonstrate that this ap-
proach to seed selection accelerates AL. Finally, we
analyze the mechanism of this acceleration.
2 Approach
Language Model (LM) Sampling is a simple unsu-
pervised technique for selecting unlabeled data that
is enriched with rare class examples. LM sampling
involves training a LM on a corpus of unlabeled can-
didate examples and selecting the examples with low
LM probability. Dligach and Palmer (2009) used
this technique in the context of word sense disam-
biguation and showed that rare sense examples tend
to concentrate among the examples with low prob-
ability. Unfortunately these authors provided a lim-
ited evaluation of this technique: they looked at its
effectiveness only at a single selection size. We pro-
vide a more convincing evaluation in which the ef-
fectiveness of this approach is examined for all sizes
of the selected data.
Seed Selection for AL is typically done ran-
domly. However, for datasets with a skewed dis-
tribution of classes, rare class examples may end
up being underrepresented. We propose to use LM
sampling for seed selection, which captures more
examples of rare classes than random selection, thus
leading to a faster learning progress.
3 Evaluation
3.1 Data
For our evaluation, we needed a dataset that is
characterized by a skewed class distribution. This
phenomenon is pervasive in word sense data. A
large word sense annotated corpus has recently
been released by the OntoNotes (Hovy et al, 2006;
Weischedel et al, 2009) project. For clarity of eval-
uation, we identify a set of verbs that satisfy three
criteria: (1) the number of senses is two, (2) the
number of annotated examples is at least 100, (3) the
proportion of the rare sense is at most 20%. The fol-
lowing 25 verbs satisfy these criteria: account, add,
admit, allow, announce, approve, compare, demand,
exist, expand, expect, explain, focus, include, invest,
issue, point, promote, protect, receive, remain, re-
place, strengthen, wait, wonder. The average num-
ber of examples for these verbs is 232. In supervised
word sense disambiguation, a single model per word
is typically trained and that is the approach we take.
Thus, we conduct our evaluation using 25 different
data sets. We report the averages across these 25
data sets. In our evaluation, we use a state-of-the-
art word sense disambiguation system (Dligach and
Palmer, 2008), that utilizes rich linguistic features to
capture the contexts of ambiguous words.
3.2 Rare Sense Retrieval
The success of our approach to seeding AL hinges
on the ability of LM sampling to discover rare class
examples better than random sampling. In this ex-
periment, we demonstrate that LM sampling outper-
forms random sampling for every selection size. For
each verb we conduct an experiment in which we
select the instances of this verb using both methods.
We measure the recall of the rare sense, which we
calculate as the ratio of the number of selected rare
sense examples to the total number of rare sense ex-
amples for this verb.
We train a LM (Stolcke, 2002) on the corpora
from which OntoNotes data originates: the Wall
Street Journal, English Broadcast News, English
Conversation, and the Brown corpus. For each verb,
we compute the LM probability for each instance of
this verb and sort the instances by probability. In
the course of the experiment, we select one example
with the smallest probability and move it to the set
of selected examples. We then measure the recall of
the rare sense for the selected examples. We con-
tinue in this fashion until all the examples have been
selected. We use random sampling as a baseline,
which is obtained by continuously selecting a single
example randomly. We continue until all the exam-
ples have been selected. At the end of the exper-
iment, we have produced two recall curves, which
measure the recall of the rare sense retrieval for this
verb at various sizes of selected data. Due to the
lack of space, we do not show the plots that display
these curves for individual verbs. Instead, in Figure
1 we display the curves that are averaged across all
verbs. At every selection size, LM sampling results
in a higher recall of the rare sense. The average dif-
ference across all selection sizes is 11%.
7
Figure 1: Average recall of rare sense retrieval for LM
and random sampling by relative size of training set
3.3 Classic and Selectively Seeded AL
In this experiment, we seed AL using LM sampling
and compare how this selectively seeded AL per-
forms in comparison with classic (randomly-seeded)
AL. Our experimental setup is typical for an active
learning study. We split the set of annotated exam-
ples for a verb into 90% and 10% parts. The 90%
part is used as a pool of unlabeled data. The 10%
part is used as a test set. We begin classic AL by
randomly selecting 10% of the examples from the
pool to use as seeds. We train a maximum entropy
model (Le, 2004) using these seeds. We then repeat-
edly apply the model to the remaining examples in
the pool: on each iteration of AL, we draw a sin-
gle most informative example from the pool. The
informativeness is estimated using prediction mar-
gin (Schein and Ungar, 2007), which is computed as
|P (c1|x) ? P (c2|x)|, where c1 and c2 are the two
most probable classes of example x according to the
model. The selected example is moved to the train-
ing set. On each iteration, we also keep track of how
accurately the current model classifies the held out
test set.
In parallel, we conduct a selectively seeded AL
experiment that is identical to the classic one but
with one crucial difference: instead of selecting the
seed examples randomly, we select them using LM
sampling by identifying 10% of the examples from
the pool with the smallest LM probability. We also
produce a random sampling curve to be used as a
baseline. At the end of this experiment we have ob-
tained three learning curves: for classic AL, for se-
lectively seeded AL, and for the random sampling
baseline. The final learning curves for each verb are
produced by averaging the learning curves from ten
different trials.
Figure 2 presents the average accuracy of selec-
tively seeded AL (top curve), classic AL (middle
curve) and the random sampling baseline (bottom
curve) at various fractions of the total size of the
training set. The size of zero corresponds to a train-
ing set consisting only of the seed examples. The
size of one corresponds to a training set consisting
of all the examples in the pool labeled. The accuracy
at a given size was averaged across all 25 verbs.
It is clear that LM-seeded AL accelerates learn-
ing: it reaches the same performance as classic AL
with less training data. LM-seeded AL also reaches
a higher classification accuracy (if stopped at its
peak). We will analyze this somewhat surprising be-
havior in the next section. The difference between
the classic and LM-seeded curves is statistically sig-
nificant (p = 0.0174) 2.
Figure 2: Randomly and LM-seeded AL. Random sam-
pling baseline is also shown.
3.4 Why LM Seeding Produces Better Results
For random sampling, the system achieves its best
accuracy, 94.4%, when the entire pool of unlabeled
examples is labeled. The goal of a typical AL study
is to demonstrate that the same accuracy can be
2We compute the average area under the curve for each type
of AL and use Wilcoxon signed rank test to test whether the
difference between the averages is significant.
8
achieved with less labeled data. For example, in our
case, classic AL reaches the best random sampling
accuracy with only about 5% of the data. However,
it is interesting to notice that LM-seeded AL actually
reaches a higher accuracy, 95%, during early stages
of learning (at 15% of the total training set size). We
believe this phenomenon takes place due to overfit-
ting the predominant class: as the model receives
new data (and therefore more and more examples of
the predominant class), it begins to mislabel more
and more examples of the rare class. A similar idea
has been expressed in literature (Weiss, 1995; Kubat
and Matwin, 1997; Japkowicz, 2001; Weiss, 2004;
Chen et al, 2006), however it has never been veri-
fied in the context of AL.
To verify our hypothesis, we conduct an experi-
ment. The experimental setup is the same as in sec-
tion 3.3. However, instead of measuring the accu-
racy on the test set, we resort to different metrics
that reflect how accurately the classifier labels the in-
stances of the rare class in the held out test set. These
metrics are the recall and precision for the rare class.
Recall is the ratio of the correctly labeled examples
of the rare class and the total number of instances of
the rare class. Precision is the ratio of the correctly
labeled examples of the rare class and the number of
instances labeled as that class. Results are in Figures
3 and 4.
Figure 3: Rare sense classification recall
Observe that for LM-seeded AL, the recall peaks
at first and begins to decline later. Thus the clas-
sifier makes progressively more errors on the rare
class as more labeled examples are being received.
Figure 4: Rare sense classification precision
This is consistent with our hypothesis that the clas-
sifier overfits the predominant class. When all the
data is labeled, the recall decreases from about 13%
to only 7%, an almost 50% drop. The reason that
the system achieved a higher level of recall at first is
due to the fact that AL was seeded with LM selected
data, which has a higher content of rare classes (as
we demonstrated in the first experiment). The avail-
ability of the extra examples of the rare class allows
the classifier to label the instances of this class in
the test set more accurately, which in turn boosts the
overall accuracy.
4 Conclusion and Future Work
We introduced a novel approach to seeding AL, in
which the seeds are selected from the examples with
low LM probability. This approach selects more rare
class examples than random sampling, resulting in
more rapid learning and, more importantly, leading
to a classifier that performs better on rare class ex-
amples. As a consequence of this, the overall classi-
fication accuracy is higher than that for classic AL.
Our plans for future work include improving our
LM by incorporating syntactic information such as
POS tags. This should result in better performance
on the rare classes, which is currently still low.
We also plan to experiment with other unsupervised
techniques, such as clustering and outlier detection,
that can lead to better retrieval of rare classes. Fi-
nally, we plan to investigate the applicability of our
approach to a multi-class scenario.
9
Acknowledgements
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambiguation,
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-C-0022, a subcontract from the BBN-AGILE
Team. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha
Palmer. 2006. An empirical study of the behavior
of active learning for word sense disambiguation. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 120?127, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
HLT ?08: Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies, pages 29?32, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Dmitriy Dligach and Martha. Palmer. 2009. Using lan-
guage modeling to select useful annotation data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Student Research Workshop and
Doctoral Consortium, pages 25?30. Association for
Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57?60, Morristown, NJ, USA. Association for
Computational Linguistics.
Nathalie Japkowicz. 2001. Concept-learning in the pres-
ence of between-class and within-class imbalances. In
AI ?01: Proceedings of the 14th Biennial Conference
of the Canadian Society on Computational Studies
of Intelligence, pages 67?77, London, UK. Springer-
Verlag.
M. Kubat and S. Matwin. 1997. Addressing the curse of
imbalanced training sets: one-sided selection. In Pro-
ceedings of the Fourteenth International Conference
on Machine Learning, pages 179?186. Citeseer.
Zhang Le, 2004. Maximum Entropy Modeling Toolkit for
Python and C++.
A.I. Schein and L.H. Ungar. 2007. Active learning for
logistic regression: an evaluation. Machine Learning,
68(3):235?265.
H. Schu?tze, E. Velipasaoglu, and J.O. Pedersen. 2006.
Performance thresholding in practical text classifica-
tion. In Proceedings of the 15th ACM international
conference on Information and knowledge manage-
ment, pages 662?671. ACM.
Burr Settles. 2009. Active learning literature survey. In
Computer Sciences Technical Report 1648 University
of Wisconsin-Madison.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, Denver, Colorado., pages
901?904.
Katrin Tomanek, Florian Laws, Udo Hahn, and Hinrich
Schu?tze. 2009. On proper unit selection in active
learning: co-selection effects for named entity recog-
nition. In HLT ?09: Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 9?17, Morristown, NJ, USA.
Association for Computational Linguistics.
R. Weischedel, E. Hovy, M. Marcus, M. Palmer,
R Belvin, S Pradan, L. Ramshaw, and N. Xue, 2009.
OntoNotes: A Large Training Corpus for Enhanced
Processing, chapter in Global Automatic Language
Exploitation, pages 54?63. Springer Verglag.
G.M. Weiss. 1995. Learning with rare cases and small
disjuncts. In Proceedings of the Twelfth International
Conference on Machine Learning, pages 558?565.
Citeseer.
G.M. Weiss. 2004. Mining with rarity: a unifying
framework. ACM SIGKDD Explorations Newsletter,
6(1):7?19.
10
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 687?692,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Getting the Most out of Transition-based Dependency Parsing
Jinho D. Choi
Department of Computer Science
University of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
mpalmer@colorado.edu
Abstract
This paper suggests two ways of improving
transition-based, non-projective dependency
parsing. First, we add a transition to an exist-
ing non-projective parsing algorithm, so it can
perform either projective or non-projective
parsing as needed. Second, we present a boot-
strapping technique that narrows down dis-
crepancies between gold-standard and auto-
matic parses used as features. The new ad-
dition to the algorithm shows a clear advan-
tage in parsing speed. The bootstrapping
technique gives a significant improvement to
parsing accuracy, showing near state-of-the-
art performance with respect to other parsing
approaches evaluated on the same data set.
1 Introduction
Dependency parsing has recently gained consider-
able interest because it is simple and fast, yet pro-
vides useful information for many NLP tasks (Shen
et al, 2008; Councill et al, 2010). There are two
main dependency parsing approaches (Nivre and
McDonald, 2008). One is a transition-based ap-
proach that greedily searches for local optima (high-
est scoring transitions) and uses parse history as fea-
tures to predict the next transition (Nivre, 2003).
The other is a graph-based approach that searches
for a global optimum (highest scoring tree) from
a complete graph in which vertices represent word
tokens and edges (directed and weighted) represent
dependency relations (McDonald et al, 2005).
Lately, the usefulness of the transition-based ap-
proach has drawn more attention because it gener-
ally performs noticeably faster than the graph-based
approach (Cer et al, 2010). The transition-based ap-
proach has a worst-case parsing complexity of O(n)
for projective, and O(n2) for non-projective pars-
ing (Nivre, 2008). The complexity is lower for pro-
jective parsing because it can deterministically drop
certain tokens from the search space whereas that
is not advisable for non-projective parsing. Despite
this fact, it is possible to perform non-projective
parsing in linear time in practice (Nivre, 2009). This
is because the amount of non-projective dependen-
cies is much smaller than the amount of projective
dependencies, so a parser can perform projective
parsing for most cases and perform non-projective
parsing only when it is needed. One other advan-
tage of the transition-based approach is that it can
use parse history as features to make the next pre-
diction. This parse information helps to improve
parsing accuracy without hurting parsing complex-
ity (Nivre, 2006). Most current transition-based ap-
proaches use gold-standard parses as features dur-
ing training; however, this is not necessarily what
parsers encounter during decoding. Thus, it is desir-
able to minimize the gap between gold-standard and
automatic parses for the best results.
This paper improves the engineering of different
aspects of transition-based, non-projective depen-
dency parsing. To reduce the search space, we add a
transition to an existing non-projective parsing algo-
rithm. To narrow down the discrepancies between
gold-standard and automatic parses, we present a
bootstrapping technique. The new addition to the
algorithm shows a clear advantage in parsing speed.
The bootstrapping technique gives a significant im-
provement to parsing accuracy.
687
LEFT-POPL
( [?1|i], ?2, [j|?], E ) ? ( ?1 , ?2, [j|?], E ? {i
L
? j} )
?i 6= 0, j. i 6?? j ? @k ? ?. i? k
LEFT-ARCL
( [?1|i], ?2 , [j|?], E )? ( ?1 , [i|?2], [j|?], E ? {i
L
? j} )
?i 6= 0, j. i 6?? j
RIGHT-ARCL
( [?1|i], ?2 , [j|?], E )? ( ?1 , [i|?2], [j|?], E ? {i
L
? j} )
?i, j. i 6?? j
SHIFT
( ?1 , ?2, [j|?], E ) ? ( [?1 ? ?2|j], [ ] , ? , E )
DT: ?1 = [ ], NT: @k ? ?1. k ? j ? k ? j
NO-ARC
( [?1|i], ?2 , [j|?], E )? ( ?1 , [i|?2], [j|?], E )
default transition
Table 1: Transitions in our algorithm. For each row, the first line shows a transition and the second line shows
preconditions of the transition.
2 Reducing search space
Our algorithm is based on Choi-Nicolov?s approach
to Nivre?s list-based algorithm (Nivre, 2008). The
main difference between these two approaches is in
their implementation of the SHIFT transition. Choi-
Nicolov?s approach divides the SHIFT transition into
two, deterministic and non-deterministic SHIFT?s,
and trains the non-deterministic SHIFT with a classi-
fier so it can be predicted during decoding. Choi and
Nicolov (2009) showed that this implementation re-
duces the parsing complexity from O(n2) to linear
time in practice (a worst-case complexity is O(n2)).
We suggest another transition-based parsing ap-
proach that reduces the search space even more.
The idea is to merge transitions in Choi-Nicolov?s
non-projective algorithm with transitions in Nivre?s
projective algorithm (Nivre, 2003). Nivre?s projec-
tive algorithm has a worst-case complexity of O(n),
which is faster than any non-projective parsing al-
gorithm. Since the number of non-projective depen-
dencies is much smaller than the number of projec-
tive dependencies (Nivre and Nilsson, 2005), it is
not efficient to perform non-projective parsing for
all cases. Ideally, it is better to perform projective
parsing for most cases and perform non-projective
parsing only when it is needed. In this algorithm, we
add another transition to Choi-Nicolov?s approach,
LEFT-POP, similar to the LEFT-ARC transition in
Nivre?s projective algorithm. By adding this tran-
sition, an oracle can now choose either projective or
non-projective parsing depending on parsing states.1
1We also tried adding the RIGHT-ARC transition from
Nivre?s projective algorithm, which did not improve parsing
performance for our experiments.
Note that Nivre (2009) has a similar idea of per-
forming projective and non-projective parsing selec-
tively. That algorithm uses a SWAP transition to
reorder tokens related to non-projective dependen-
cies, and runs in linear time in practice (a worst-case
complexity is still O(n2)). Our algorithm is distin-
guished in that it does not require such reordering.
Table 1 shows transitions used in our algorithm.
All parsing states are represented as tuples (?1, ?2,
?, E), where ?1, ?2, and ? are lists of word tokens.
E is a set of labeled edges representing previously
identified dependencies. L is a dependency label and
i, j, k represent indices of their corresponding word
tokens. The initial state is ([0], [ ], [1,. . . ,n], ?). The
0 identifier corresponds to an initial token, w0, intro-
duced as the root of the sentence. The final state is
(?1, ?2, [ ], E), i.e., the algorithm terminates when
all tokens in ? are consumed.
The algorithm uses five kinds of transitions. All
transitions are performed by comparing the last to-
ken in ?1, wi, and the first token in ?, wj . Both
LEFT-POPL and LEFT-ARCL are performed when
wj is the head of wi with a dependency relation L.
The difference is that LEFT-POP removes wi from
?1 after the transition, assuming that the token is no
longer needed in later parsing states, whereas LEFT-
ARC keeps the token so it can be the head of some
token wj<k?n in ?. This wi ? wk relation causes
a non-projective dependency. RIGHT-ARCL is per-
formed whenwi is the head ofwj with a dependency
relation L. SHIFT is performed when ?1 is empty
(DT) or there is no token in ?1 that is either the head
or a dependent ofwj (NT). NO-ARC is there to move
tokens around so each token in ? can be compared
to all (or some) tokens prior to it.
688
It
1
was
2
in
3
my
4
interest
5
to
6
Root
0
see
7
you
8
SBJ
ROOT
PRD NMOD
PMOD
IM
NMOD
OBJ
Transition ?1 ?2 ? E
0 [0] [ ] [1|?] ?
1 SHIFT (NT) [?1|1] [ ] [2|?]
2 LEFT-ARC [0] [1] [2|?] E ? {1?SBJ? 2}
3 RIGHT-ARC [ ] [0|?2] [2|?] E ? {0 ?ROOT? 2}
4 SHIFT (DT) [?1|2] [ ] [3|?]
5 RIGHT-ARC [?1|1] [2] [3|?] E ? {2 ?PRD? 3}
6 SHIFT (NT) [?1|3] [ ] [4|?]
7 SHIFT (NT) [?1|4] [ ] [5|?]
8 LEFT-POP [?1|3] [ ] [5|?] E ? {4?NMOD? 5}
9 RIGHT-ARC [?1|2] [3] [5|?] E ? {3 ?PMOD? 5}
10 SHIFT (NT) [?1|5] [ ] [6|?]
11 NO-ARC [?1|3] [5] [6|?]
12 NO-ARC [?1|2] [3|?2] [6|?]
13 NO-ARC [?1|1] [2|?2] [6|?]
14 RIGHT-ARC [0] [1|?2] [6|?] E ? {1 ?NMOD? 6}
15 SHIFT (NT) [?1|6] [ ] [7|?]
16 RIGHT-ARC [?1|5] [6] [7|?] E ? {6 ?IM? 7}
17 SHIFT (NT) [?1|7] [ ] [8|?]
18 RIGHT-ARC [?1|6] [7] [8|?] E ? {7 ?OBJ? 8}
19 SHIFT (NT) [?1|8] [ ] [ ]
Table 2: Parsing states for the example sentence. After LEFT-POP is performed (#8), [w4 = my] is removed from the
search space and no longer considered in the later parsing states (e.g., between #10 and #11).
During training, the algorithm checks for the pre-
conditions of all transitions and generates training
instances with corresponding labels. During decod-
ing, the oracle decides which transition to perform
based on the parsing states. With the addition of
LEFT-POP, the oracle can choose either projective
or non-projective parsing by selecting LEFT-POP or
LEFT-ARC, respectively. Our experiments show that
this additional transition improves both parsing ac-
curacy and speed. The advantage derives from im-
proving the efficiency of the choice mechanism; it is
now simply a transition choice and requires no addi-
tional processing.
3 Bootstrapping automatic parses
Transition-based parsing has the advantage of using
parse history as features to make the next prediction.
In our algorithm, when wi and wj are compared,
subtree and head information of these tokens is par-
tially provided by previous parsing states. Graph-
based parsing can also take advantage of using parse
information. This is done by performing ?higher-
order parsing?, which is shown to improve parsing
accuracy but also increase parsing complexity (Car-
reras, 2007; Koo and Collins, 2010).2 Transition-
based parsing is attractive because it can use parse
information without increasing complexity (Nivre,
2006). The qualification is that parse information
provided by gold-standard trees during training is
not necessarily the same kind of information pro-
vided by automatically parsed trees during decod-
ing. This can confuse a statistical model trained only
on the gold-standard trees.
To reduce the gap between gold-standard and au-
tomatic parses, we use bootstrapping on automatic
parses. First, we train a statistical model using gold-
2Second-order, non-projective, graph-based dependency
parsing is NP-hard without performing approximation.
689
standard trees. Then, we parse the training data us-
ing the statistical model. During parsing, we ex-
tract features for each parsing state, consisting of
automatic parse information, and generate a train-
ing instance by joining the features with the gold-
standard label. The gold-standard label is achieved
by comparing the dependency relation between wi
and wj in the gold-standard tree. When the parsing
is done, we train a different model using the training
instances induced by the previous model. We repeat
the procedure until a stopping criteria is met.
The stopping criteria is determined by performing
cross-validation. For each stage, we perform cross-
validation to check if the average parsing accuracy
on the current cross-validation set is higher than the
one from the previous stage. We stop the procedure
when the parsing accuracy on cross-validation sets
starts decreasing. Our experiments show that this
simple bootstrapping technique gives a significant
improvement to parsing accuracy.
4 Related work
Daume? et al (2009) presented an algorithm, called
SEARN, for integrating search and learning to solve
complex structured prediction problems. Our boot-
strapping technique can be viewed as a simplified
version of SEARN. During training, SEARN itera-
tively creates a set of new cost-sensitive examples
using a known policy. In our case, the new examples
are instances containing automatic parses induced
by the previous model. Our technique is simpli-
fied because the new examples are not cost-sensitive.
Furthermore, SEARN interpolates the current policy
with the previous policy whereas we do not per-
form such interpolation. During decoding, SEARN
generates a sequence of decisions and makes a fi-
nal prediction. In our case, the decisions are pre-
dicted dependency relations and the final prediction
is a dependency tree. SEARN has been successfully
adapted to several NLP tasks such as named entity
recognition, syntactic chunking, and POS tagging.
To the best of our knowledge, this is the first time
that this idea has been applied to transition-based
parsing and shown promising results.
Zhang and Clark (2008) suggested a transition-
based projective parsing algorithm that keeps B dif-
ferent sequences of parsing states and chooses the
one with the best score. They use beam search and
show a worst-case parsing complexity ofO(n) given
a fixed beam size. Similarly to ours, their learn-
ing mechanism using the structured perceptron al-
gorithm involves training on automatically derived
parsing states that closely resemble potential states
encountered during decoding.
5 Experiments
5.1 Corpora and learning algorithm
All models are trained and tested on English and
Czech data using automatic lemmas, POS tags,
and feats, as distributed by the CoNLL?09 shared
task (Hajic? et al, 2009). We use Liblinear L2-L1
SVM for learning (L2 regularization, L1 loss; Hsieh
et al (2008)). For our experiments, we use the fol-
lowing learning parameters: c = 0.1 (cost), e = 0.1
(termination criterion), B = 0 (bias).
5.2 Accuracy comparisons
First, we evaluate the impact of the LEFT-POP tran-
sition we add to Choi-Nicolov?s approach. To make
a fair comparison, we implemented both approaches
and built models using the exact same feature set.
The ?CN? and ?Our? rows in Table 3 show accuracies
achieved by Choi-Nicolov?s and our approaches, re-
spectively. Our approach shows higher accuracies
for all categories. Next, we evaluate the impact of
our bootstrapping technique. The ?Our+? row shows
accuracies achieved by our algorithm using the boot-
strapping technique. The improvement from ?Our?
to ?Our+? is statistically significant for all categories
(McNemar, p < .0001). The improvment is even
more significant in a language like Czech for which
parsers generally perform more poorly.
English Czech
LAS UAS LAS UAS
CN 88.54 90.57 78.12 83.29
Our 88.62 90.66 78.30 83.47
Our+ 89.15? 91.18? 80.24? 85.24?
Merlo 88.79 (3) - 80.38 (1) -
Bohnet 89.88 (1) - 80.11 (2) -
Table 3: Accuracy comparisons between different pars-
ing approaches (LAS/UAS: labeled/unlabeled attachment
score). ? indicates a statistically significant improvement.
(#) indicates an overall rank of the system in CoNLL?09.
690
Finally, we compare our work against other state-of-
the-art systems. For the CoNLL?09 shared task, Ges-
mundo et al (2009) introduced the best transition-
based system using synchronous syntactic-semantic
parsing (?Merlo?), and Bohnet (2009) introduced the
best graph-based system using a maximum span-
ning tree algorithm (?Bohnet?). Our approach shows
quite comparable results with these systems.3
5.3 Speed comparisons
Figure 1 shows average parsing speeds for each
sentence group in both English and Czech eval-
uation sets (Table 4). ?Nivre? is Nivre?s swap
algorithm (Nivre, 2009), of which we use the
implementation from MaltParser (maltparser.
org). The other approaches are implemented in
our open source project, called ClearParser (code.
google.com/p/clearparser). Note that fea-
tures used in MaltParser have not been optimized
for these evaluation sets. All experiments are tested
on an Intel Xeon 2.57GHz machine. For general-
ization, we run five trials for each parser, cut off
the top and bottom speeds, and average the middle
three. The loading times for machine learning mod-
els are excluded because they are independent from
the parsing algorithms. The average parsing speeds
are 2.86, 2.69, and 2.29 (in milliseconds) for Nivre,
CN, and Our+, respectively. Our approach shows
linear growth all along, even for the sentence groups
where some approaches start showing curves.
   0 10 20 30 40 50 60 70
   
   2
6
10
14
18
22
Sentence length
Par
sing
 spe
ed 
(in 
ms)
Our+
CN
Nivre
Figure 1: Average parsing speeds with respect to sentence
groups in Table 4.
3Later, ?Merlo? and ?Bohnet? introduced more advanced
systems, showing some improvements over their previous ap-
proaches (Titov et al, 2009; Bohnet, 2010).
< 10 < 20 < 30 < 40 < 50 < 60 < 70
1,415 2,289 1,714 815 285 72 18
Table 4: # of sentences in each group, extracted from both
English/Czech evaluation sets. ?< n? implies a group
containing sentences whose lengths are less than n.
We also measured average parsing speeds for ?Our?,
which showed a very similar growth to ?Our+?. The
average parsing speed of ?Our? was 2.20 ms; it per-
formed slightly faster than ?Our+? because it skipped
more nodes by performing more non-deterministic
SHIFT?s, which may or may not have been correct
decisions for the corresponding parsing states.
It is worth mentioning that the curve shown by
?Nivre? might be caused by implementation details
regarding feature extraction, which we included as
part of parsing. To abstract away from these im-
plementation details and focus purely on the algo-
rithms, we would need to compare the actual num-
ber of transitions performed by each parser, which
will be explored in future work.
6 Conclusion and future work
We present two ways of improving transition-based,
non-projective dependency parsing. The additional
transition gives improvements to both parsing speed
and accuracy, showing a linear time parsing speed
with respect to sentence length. The bootstrapping
technique gives a significant improvement to parsing
accuracy, showing near state-of-the-art performance
with respect to other parsing approaches. In the fu-
ture, we will test the robustness of these approaches
in more languages.
Acknowledgments
We gratefully acknowledge the support of the Na-
tional Science Foundation Grants CISE-IIS-RI-0910992,
Richer Representations for Machine Translation, a sub-
contract from the Mayo Clinic and Harvard Children?s
Hospital based on a grant from the ONC, 90TR0002/01,
Strategic Health Advanced Research Project Area 4: Nat-
ural Language Processing, and a grant from the Defense
Advanced Research Projects Agency (DARPA/IPTO) un-
der the GALE program, DARPA/CMO Contract No.
HR0011-06-C-0022, subcontract from BBN, Inc. Any
opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do
not necessarily reflect the views of the National Science
Foundation.
691
References
Bernd Bohnet. 2009. Efficient parsing of syntactic and
semantic dependency structures. In Proceedings of the
13th Conference on Computational Natural Language
Learning: Shared Task (CoNLL?09), pages 67?72.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In The 23rd In-
ternational Conference on Computational Linguistics
(COLING?10).
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL?07
(CoNLL?07), pages 957?961.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing
to stanford dependencies: Trade-offs between speed
and accuracy. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC?10).
Jinho D. Choi and Nicolas Nicolov. 2009. K-best, lo-
cally pruned, transition-based dependency parsing us-
ing robust risk minimization. In Recent Advances in
Natural Language Processing V, pages 205?216. John
Benjamins.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: Learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing (NeSp-NLP?10), pages 51?59.
Hal Daume?, Iii, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75(3):297?325.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning: Shared
Task (CoNLL?09), pages 37?42.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL?09): Shared Task, pages 1?18.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In Proceed-
ings of the 25th international conference on Machine
learning (ICML?08), pages 408?415.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL?10).
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP?05), pages 523?530.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL:HLT?08), pages 950?958.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 99?106.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT?03), pages 23?25.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP?09),
pages 351?359.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL:HLT?08), pages 577?585.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic depen-
dencies. In Proceedings of the 21st International
Joint Conference on Artificial Intelligence (IJCAI?09),
pages 1562?1567.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing using
beam-search. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?08), pages 562?571.
692
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 263?272,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Verb Classification using Distributional Similarity
in Syntactic and Semantic Structures
Danilo Croce
University of Tor Vergata
00133 Roma, Italy
croce@info.uniroma2.it
Alessandro Moschitti
University of Trento
38123 Povo (TN), Italy
moschitti@disi.unitn.it
Roberto Basili
University of Tor Vergata
00133 Roma, Italy
basili@info.uniroma2.it
Martha Palmer
University of Colorado at Boulder
Boulder, CO 80302, USA
mpalmer@colorado.edu
Abstract
In this paper, we propose innovative repre-
sentations for automatic classification of verbs
according to mainstream linguistic theories,
namely VerbNet and FrameNet. First, syntac-
tic and semantic structures capturing essential
lexical and syntactic properties of verbs are
defined. Then, we design advanced similarity
functions between such structures, i.e., seman-
tic tree kernel functions, for exploiting distri-
butional and grammatical information in Sup-
port Vector Machines. The extensive empir-
ical analysis on VerbNet class and frame de-
tection shows that our models capture mean-
ingful syntactic/semantic structures, which al-
lows for improving the state-of-the-art.
1 Introduction
Verb classification is a fundamental topic of com-
putational linguistics research given its importance
for understanding the role of verbs in conveying se-
mantics of natural language (NL). Additionally, gen-
eralization based on verb classification is central to
many NL applications, ranging from shallow seman-
tic parsing to semantic search or information extrac-
tion. Currently, a lot of interest has been paid to
two verb categorization schemes: VerbNet (Schuler,
2005) and FrameNet (Baker et al, 1998), which
has also fostered production of many automatic ap-
proaches to predicate argument extraction.
Such work has shown that syntax is necessary
for helping to predict the roles of verb arguments
and consequently their verb sense (Gildea and Juras-
fky, 2002; Pradhan et al, 2005; Gildea and Palmer,
2002). However, the definition of models for opti-
mally combining lexical and syntactic constraints is
still far for being accomplished. In particular, the ex-
haustive design and experimentation of lexical and
syntactic features for learning verb classification ap-
pears to be computationally problematic. For exam-
ple, the verb order can belongs to the two VerbNet
classes:
? The class 60.1, i.e., order someone to do some-
thing as shown in: The Illinois Supreme Court or-
dered the commission to audit Commonwealth Edi-
son ?s construction expenses and refund any unrea-
sonable expenses .
? The class 13.5.1: order or request something like
in: ... Michelle blabs about it to a sandwich man
while ordering lunch over the phone .
Clearly, the syntactic realization can be used to dis-
cern the cases above but it would not be enough to
correctly classify the following verb occurrence: ..
ordered the lunch to be delivered .. in Verb class
13.5.1. For such a case, selectional restrictions are
needed. These have also been shown to be use-
ful for semantic role classification (Zapirain et al,
2010). Note that their coding in learning algorithms
is rather complex: we need to take into account syn-
tactic structures, which may require an exponential
number of syntactic features (i.e., all their possible
substructures). Moreover, these have to be enriched
with lexical information to trig lexical preference.
In this paper, we tackle the problem above
by studying innovative representations for auto-
matic verb classification according to VerbNet and
FrameNet. We define syntactic and semantic struc-
tures capturing essential lexical and syntactic prop-
erties of verbs. Then, we apply similarity between
263
such structures, i.e., kernel functions, which can also
exploit distributional lexical semantics, to train au-
tomatic classifiers. The basic idea of such functions
is to compute the similarity between two verbs in
terms of all the possible substructures of their syn-
tactic frames. We define and automatically extract
a lexicalized approximation of the latter. Then, we
apply kernel functions that jointly model structural
and lexical similarity so that syntactic properties are
combined with generalized lexemes. The nice prop-
erty of kernel functions is that they can be used
in place of the scalar product of feature vectors to
train algorithms such as Support Vector Machines
(SVMs). This way SVMs can learn the association
between syntactic (sub-) structures whose lexical ar-
guments are generalized and target verb classes, i.e.,
they can also learn selectional restrictions.
We carried out extensive experiments on verb
class and frame detection which showed that our
models greatly improve on the state-of-the-art (up
to about 13% of relative error reduction). Such re-
sults are nicely assessed by manually inspecting the
most important substructures used by the classifiers
as they largely correlate with syntactic frames de-
fined in VerbNet.
In the rest of the paper, Sec. 2 reports on related
work, Sec. 3 and Sec. 4 describe previous and our
models for syntactic and semantic similarity, respec-
tively, Sec. 5 illustrates our experiments, Sec. 6 dis-
cusses the output of the models in terms of error
analysis and important structures and finally Sec. 7
derives the conclusions.
2 Related work
Our target task is verb classification but at the same
time our models exploit distributional models as
well as structural kernels. The next three subsec-
tions report related work in such areas.
Verb Classification. The introductory verb classi-
fication example has intuitively shown the complex-
ity of defining a comprehensive feature representa-
tion. Hereafter, we report on analysis carried out in
previous work.
It has been often observed that verb senses tend
to show different selectional constraints in a specific
argument position and the above verb order is a clear
example. In the direct object position of the example
sentence for the first sense 60.1 of order, we found
commission in the role PATIENT of the predicate. It
clearly satisfies the +ANIMATE/+ORGANIZATION
restriction on the PATIENT role. This is not true
for the direct object dependency of the alternative
sense 13.5.1, which usually expresses the THEME
role, with unrestricted type selection. When prop-
erly generalized, the direct object information has
thus been shown highly predictive about verb sense
distinctions.
In (Brown et al, 2011), the so called dynamic
dependency neighborhoods (DDN), i.e., the set of
verbs that are typically collocated with a direct ob-
ject, are shown to be more helpful than lexical in-
formation (e.g., WordNet). The set of typical verbs
taking a noun n as a direct object is in fact a strong
characterization for semantic similarity, as all the
nounsm similar to n tend to collocate with the same
verbs. This is true also for other syntactic depen-
dencies, among which the direct object dependency
is possibly the strongest cue (as shown for example
in (Dligach and Palmer, 2008)).
In order to generalize the above DDN feature, dis-
tributional models are ideal, as they are designed
to model all the collocations of a given noun, ac-
cording to large scale corpus analysis. Their abil-
ity to capture lexical similarity is well established in
WSD tasks (e.g. (Schutze, 1998)), thesauri harvest-
ing (Lin, 1998), semantic role labeling (Croce et al,
2010)) as well as information retrieval (e.g. (Furnas
et al, 1988)).
Distributional Models (DMs). These models fol-
low the distributional hypothesis (Firth, 1957) and
characterize lexical meanings in terms of context of
use, (Wittgenstein, 1953). By inducing geometrical
notions of vectors and norms through corpus analy-
sis, they provide a topological definition of seman-
tic similarity, i.e., distance in a space. DMs can
capture the similarity between words such as dele-
gation, deputation or company and commission. In
case of sense 60.1 of the verb order, DMs can be
used to suggest that the role PATIENT can be inher-
ited by all these words, as suitable Organisations.
In supervised language learning, when few exam-
ples are available, DMs support cost-effective lexi-
cal generalizations, often outperforming knowledge
based resources (such as WordNet, as in (Pantel et
al., 2007)). Obviously, the choice of the context
264
type determines the type of targeted semantic prop-
erties. Wider contexts (e.g., entire documents) are
shown to suggest topical relations. Smaller con-
texts tend to capture more specific semantic as-
pects, e.g. the syntactic behavior, and better capture
paradigmatic relations, such as synonymy. In partic-
ular, word space models, as described in (Sahlgren,
2006), define contexts as the words appearing in a
n-sized window, centered around a target word. Co-
occurrence counts are thus collected in a words-by-
words matrix, where each element records the num-
ber of times two words co-occur within a single win-
dow of word tokens. Moreover, robust weighting
schemas are used to smooth counts against too fre-
quent co-occurrence pairs: Pointwise Mutual Infor-
mation (PMI) scores (Turney and Pantel, 2010) are
commonly adopted.
Structural Kernels. Tree and sequence kernels
have been successfully used in many NLP applica-
tions, e.g., parse reranking and adaptation, (Collins
and Duffy, 2002; Shen et al, 2003; Toutanova et
al., 2004; Kudo et al, 2005; Titov and Hender-
son, 2006), chunking and dependency parsing, e.g.,
(Kudo and Matsumoto, 2003; Daume? III and Marcu,
2004), named entity recognition, (Cumby and Roth,
2003), text categorization, e.g., (Cancedda et al,
2003; Gliozzo et al, 2005), and relation extraction,
e.g., (Zelenko et al, 2002; Bunescu and Mooney,
2005; Zhang et al, 2006).
Recently, DMs have been also proposed in in-
tegrated syntactic-semantic structures that feed ad-
vanced learning functions, such as the semantic
tree kernels discussed in (Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b; Mehdad et
al., 2010; Croce et al, 2011).
3 Structural Similarity Functions
In this paper we model verb classifiers by exploiting
previous technology for kernel methods. In particu-
lar, we design new models for verb classification by
adopting algorithms for structural similarity, known
as Smoothed Partial Tree Kernels (SPTKs) (Croce et
al., 2011). We define new innovative structures and
similarity functions based on LSA.
The main idea of SPTK is rather simple: (i) mea-
suring the similarity between two trees in terms of
the number of shared subtrees; and (ii) such number
also includes similar fragments whose lexical nodes
are just related (so they can be different). The con-
tribution of (ii) is proportional to the lexical similar-
ity of the tree lexical nodes, where the latter can be
evaluated according to distributional models or also
lexical resources, e.g., WordNet.
In the following, we define our models based on
previous work on LSA and SPTKs.
3.1 LSA as lexical similarity model
Robust representations can be obtained through
intelligent dimensionality reduction methods. In
LSA the original word-by-context matrix M is de-
composed through Singular Value Decomposition
(SVD) (Landauer and Dumais, 1997; Golub and Ka-
han, 1965) into the product of three new matrices:
U , S, and V so that S is diagonal and M = USV T .
M is then approximated by Mk = UkSkV Tk , where
only the first k columns of U and V are used,
corresponding to the first k greatest singular val-
ues. This approximation supplies a way to project
a generic term wi into the k-dimensional space us-
ing W = UkS
1/2
k , where each row corresponds to
the representation vectors ~wi. The original statisti-
cal information about M is captured by the new k-
dimensional space, which preserves the global struc-
ture while removing low-variant dimensions, i.e.,
distribution noise. Given two words w1 and w2,
the term similarity function ? is estimated as the
cosine similarity between the corresponding projec-
tions ~w1, ~w2 in the LSA space, i.e ?(w1, w2) =
~w1? ~w2
? ~w1?? ~w2?
. This is known as Latent Semantic Ker-
nel (LSK), proposed in (Cristianini et al, 2001),
as it defines a positive semi-definite Gram matrix
G = ?(w1, w2) ?w1, w2 (Shawe-Taylor and Cris-
tianini, 2004). ? is thus a valid kernel and can be
combined with other kernels, as discussed in the
next session.
3.2 Tree Kernels driven by Semantic Similarity
To our knowledge, two main types of tree kernels
exploit lexical similarity: the syntactic semantic tree
kernel defined in (Bloehdorn and Moschitti, 2007a)
applied to constituency trees and the smoothed
partial tree kernels (SPTKs) defined in (Croce et
al., 2011), which generalizes the former. We report
the definition of the latter as we modified it for our
purposes. SPTK computes the number of common
substructures between two trees T1 and T2 without
explicitly considering the whole fragment space. Its
265
SVP
S
-
NP-1
NN
commission::n
DT
the::d
VBD
TARGET-order::v
NP-SBJ
NNP
court::n
NNP
supreme::n
NNP
illinois::n
DT
the::d
Figure 1: Constituency Tree (CT) representation of verbs.
ROOT
OPRD
IM
VB
audit::v
TO
to::t
OBJ
NN
commission::n
NMOD
DT
the::d
VBD
TARGET-order::v
SBJ
NNP
court::n
NMOD
NNP
supreme::n
NMOD
NNP
illinois::n
NMOD
DT
the::d
Figure 2: Representation of verbs according to the Grammatical Relation Centered Tree (GRCT)
general equations are reported hereafter:
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), (1)
where NT1 and NT2 are the sets of the T1?s and T2?s
nodes, respectively and ?(n1, n2) is equal to the
number of common fragments rooted in the n1 and
n2 nodes1. The ? function determines the richness
of the kernel space and thus induces different tree
kernels, for example, the syntactic tree kernel (STK)
(Collins and Duffy, 2002) or the partial tree kernel
(PTK) (Moschitti, 2006).
The algorithm for SPTK?s ? is the follow-
ing: if n1 and n2 are leaves then ??(n1, n2) =
???(n1, n2); else
??(n1, n2) = ??(n1, n2)?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(
~I1)+d(~I2)
l(~I1)?
j=1
??(cn1(~I1j), cn2(~I2j))
)
, (2)
where (1) ? is any similarity between nodes, e.g., be-
tween their lexical labels; (2) ?, ? ? [0, 1] are decay
factors; (3) cn1(h) is the h
th child of the node n1;
(4) ~I1 and ~I2 are two sequences of indexes, i.e., ~I =
(i1, i2, .., l(I)), with 1 ? i1 < i2 < .. < il(I); and (5)
d(~I1) = ~I1l(~I1)?
~I11+1 and d(~I2) = ~I2l(~I2)?
~I21+1.
Note that, as shown in (Croce et al, 2011), the av-
erage running time of SPTK is sub-quadratic in the
number of the tree nodes. In the next section we
show how we exploit the class of SPTKs, for verb
classification.
1To have a similarity score between 0 and 1, a normalization
in the kernel space, i.e. TK(T1,T2)?
TK(T1,T1)?TK(T2,T2)
is applied.
4 Verb Classification Models
The design of SPTK-based algorithms for our verb
classification requires the modeling of two differ-
ent aspects: (i) a tree representation for the verbs;
and (ii) the lexical similarity suitable for the task.
We also modified SPTK to apply different similarity
functions to different nodes to introduce flexibility.
4.1 Verb Structural Representation
The implicit feature space generated by structural
kernels and the corresponding notion of similarity
between verbs obviously depends on the input struc-
tures. In the cases of STK, PTK and SPTK different
tree representations lead to engineering more or less
expressive linguistic feature spaces.
With the aim of capturing syntactic features, we
started from two different parsing paradigms: phrase
and dependency structures. For example, for repre-
senting the first example of the introduction, we can
use the constituency tree (CT) in Figure 1, where the
target verb node is enriched with the TARGET label.
Here, we apply tree pruning to reduce the computa-
tional complexity of tree kernels as it is proportional
to the number of nodes in the input trees. Accord-
ingly, we only keep the subtree dominated by the
target VP by pruning from it all the S-nodes along
with their subtrees (i.e, all nested sentences are re-
moved). To further improve generalization, we lem-
matize lexical nodes and add generalized POS-Tags,
i.e., noun (n::), verb (v::), adjective (::a), determiner
(::d) and so on, to them. This is useful for constrain-
ing similarity to be only contributed by lexical pairs
of the same grammatical category.
266
TARGET-order::v
VBDROOTto::t
TOOPRDaudit::v
VBIM
commission::n
NNOBJthe::d
DTNMOD
court::n
NNPSBJsupreme::n
NNPNMOD
illinois::n
NNPNMOD
the::d
DTNMOD
Figure 3: Representation of verbs according to the Lexical Centered Tree (LCT)
To encode dependency structure information in a
tree (so that we can use it in tree kernels), we use
(i) lexemes as nodes of our tree, (ii) their dependen-
cies as edges between the nodes and (iii) the depen-
dency labels, e.g., grammatical functions (GR), and
POS-Tags, again as tree nodes. We designed two
different tree types: (i) in the first type, GR are cen-
tral nodes from which dependencies are drawn and
all the other features of the central node, i.e., lexi-
cal surface form and its POS-Tag, are added as ad-
ditional children. An example of the GR Centered
Tree (GRCT) is shown in Figure 2, where the POS-
Tags and lexemes are children of GR nodes. (ii) The
second type of tree uses lexicals as central nodes on
which both GR and POS-Tag are added as the right-
most children. For example, Figure 3 shows an ex-
ample of a Lexical Centered Tree (LCT). For both
trees, the pruning strategy only preserves the verb
node, its direct ancestors (father and siblings) and
its descendants up to two levels (i.e., direct children
and grandchildren of the verb node). Note that, our
dependency tree can capture the semantic head of
the verbal argument along with the main syntactic
construct, e.g., to audit.
4.2 Generalized node similarity for SPTK
We have defined the new similarity ?? to be used in
Eq. 2, which makes SPTK more effective as shown
by Alg. 1. ?? takes two nodes n1 and n2 and applies
a different similarity for each node type. The latter is
derived by ? and can be: GR (i.e., SYNT), POS-Tag
(i.e., POS) or a lexical (i.e., LEX) type. In our exper-
iment, we assign 0/1 similarity for SYNT and POS
nodes according to string matching. For LEX type,
we apply a lexical similarity learned with LSA to
only pairs of lexicals associated with the same POS-
Tag. It should be noted that the type-based similarity
allows for potentially applying a different similarity
for each node. Indeed, we also tested an amplifica-
tion factor, namely, leaf weight (lw), which ampli-
fies the matching values of the leaf nodes.
Algorithm 1 ?? (n1, n2, lw)
?? ? 0,
if ?(n1) = ?(n2) = SYNT ? label(n1) = label(n2) then
?? ? 1
end if
if ?(n1) = ?(n2) = POS ? label(n1) = label(n2) then
?? ? 1
end if
if ?(n1) = ?(n2) = LEX ? pos(n1) = pos(n2) then
?? ? ?LEX(n1, n2)
end if
if leaf(n1) ? leaf(n2) then
?? ? ?? ? lw
end if
return ??
5 Experiments
In these experiments, we tested the impact of our dif-
ferent verb representations using different kernels,
similarities and parameters. We also compared with
simple bag-of-words (BOW) models and the state-
of-the-art.
5.1 General experimental setup
We consider two different corpora: one for VerbNet
and the other for FrameNet. For the former, we used
the same verb classification setting of (Brown et al,
2011). Sentences are drawn from the Semlink cor-
pus (Loper et al, 2007), which consists of the Prop-
Banked Penn Treebank portions of the Wall Street
Journal. It contains 113K verb instances, 97K of
which are verbs represented in at least one VerbNet
class. Semlink includes 495 verbs, whose instances
are labeled with more than one class (including one
single VerbNet class or none). We used all instances
of the corpus for a total of 45,584 instances for 180
verb classes. When instances labeled with the none
class are not included, the number of examples be-
comes 23,719.
The second corpus refers to FrameNet frame clas-
sification. The training and test data are drawn from
the FrameNet 1.5 corpus2, which consists of 135K
sentences annotated according the frame semantics
2http://framenet.icsi.berkeley.edu
267
(Baker et al, 1998). We selected the subset of
frames containing more than 100 sentences anno-
tated with a verbal predicate for a total of 62,813
sentences in 187 frames (i.e., very close to the Verb-
Net datasets). For both the datasets, we used 70% of
instances for training and 30% for testing.
Our verb (multi) classifier is designed with
the one-vs-all (Rifkin and Klautau, 2004) multi-
classification schema. This uses a set of binary
SVM classifiers, one for each verb class (frame) i.
The sentences whose verb is labeled with the class
i are positive examples for the classifier i. The sen-
tences whose verbs are compatible with the class i
but evoking a different class or labeled with none
(no current verb class applies) are added as negative
examples. In the classification phase the binary clas-
sifiers are applied by (i) only considering classes that
are compatible with the target verbs; and (ii) select-
ing the class associated with the maximum positive
SVM margin. If all classifiers provide a negative
score the example is labeled with none.
To learn the binary classifiers of the schema
above, we coded our modified SPTK in SVM-Light-
TK3 (Moschitti, 2006). The parameterization of
each classifier is carried on a held-out set (30% of
the training) and is concerned with the setting of the
trade-off parameter (option -c) and the leaf weight
(lw) (see Alg. 1), which is used to linearly scale
the contribution of the leaf nodes. In contrast, the
cost-factor parameter of SVM-Light-TK is set as the
ratio between the number of negative and positive
examples for attempting to have a balanced Preci-
sion/Recall.
Regarding SPTK setting, we used the lexical simi-
larity ? defined in Sec. 3.1. In more detail, LSA was
applied to ukWak (Baroni et al, 2009), which is a
large scale document collection made up of 2 billion
tokens. M is constructed by applying POS tagging to
build rows with pairs ?lemma, ::POS? (lemma::POS
in brief). The contexts of such items are the columns
of M and are short windows of size [?3,+3], cen-
tered on the items. This allows for better captur-
ing syntactic properties of words. The most frequent
20,000 items are selected along with their 20k con-
texts. The entries of M are the point-wise mutual
3(Structural kernels in SVMLight (Joachims, 2000)) avail-
able at http://disi.unitn.it/moschitti/Tree-Kernel.htm
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 83.83% 8 84.57% 8 84.46%
GRCT - 84.83% 8 85.15% 8 85.28%
LCT - 77.73% 0.1 86.03% 0.2 86.72%
Br. et Al. 84.64%
BOW 79.08%
SK 82.08%
Table 1: VerbNet accuracy with the none class
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
GRCT - 92.67% 6 92.97% 0.4 93.54%
LCT - 90.28% 6 92.99% 0.3 93.78%
BOW 91.13%
SK 91.84%
Table 2: FrameNet accuracy without the none class
information between them. SVD reduction is then
applied to M, with a dimensionality cut of l = 250.
For generating the CT, GRCT and LCT struc-
tures, we used the constituency trees generated by
the Charniak parser (Charniak, 2000) and the de-
pendency structures generated by the LTH syntactic
parser (described in (Johansson and Nugues, 2008)).
The classification performance is measured with
accuracy (i.e., the percentage of correct classifica-
tion). We also derive statistical significance of the
results by using the model described in (Yeh, 2000)
and implemented in (Pado?, 2006).
5.2 VerbNet and FrameNet Classification
Results
To assess the performance of our settings, we also
derive a simple baseline based on the bag-of-words
(BOW) model. For it, we represent an instance of
a verb in a sentence using all words of the sentence
(by creating a special feature for the predicate word).
We also used sequence kernels (SK), i.e., PTK ap-
plied to a tree composed of a fake root and only one
level of sentence words. For efficiency reasons4, we
only consider the 10 words before and after the pred-
icate with subsequence features of length up to 5.
Table 1 reports the accuracy of different mod-
els for VerbNet classification. It should be noted
that: first, SK produces a much higher accuracy than
BOW, i.e., 82.08 vs. 79.08. On one hand, this is
4The average running time of the SK is much higher than the
one of PTK. When a tree is composed by only one level PTK
collapses to SK.
268
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 91.14% 8 91.66% 6 91.66%
GRCT - 91.71% 8 92.38% 4 92.33%
LCT - 89.20% 0.2 92.54% 0.1 92.55%
BOW 88.16%
SK 89.86%
Table 3: VerbNet accuracy without the none class
generally in contrast with standard text categoriza-
tion tasks, for which n-gram models show accuracy
comparable to the simpler BOW. On the other hand,
it simply confirms that verb classification requires
the dependency information between words (i.e., at
least the sequential structure information provided
by SK).
Second, SK is 2.56 percent points below the state-
of-the-art achieved in (Brown et al, 2011) (BR), i.e,
82.08 vs. 84.64. In contrast, STK applied to our rep-
resentation (CT, GRCT and LCT) produces compa-
rable accuracy, e.g., 84.83, confirming that syntactic
representation is needed to reach the state-of-the-art.
Third, PTK, which produces more general struc-
tures, improves over BR by almost 1.5 (statistically
significant result) when using our dependency struc-
tures GRCT and LCT. CT does not produce the same
improvement since it does not allow PTK to directly
compare the lexical structure (lexemes are all leaf
nodes in CT and to connect some pairs of them very
large trees are needed).
Finally, the best model of SPTK (i.e, using LCT)
improves over the best PTK (i.e., using LCT) by al-
most 1 point (statistically significant result): this dif-
ference is only given by lexical similarity. SPTK im-
proves on the state-of-the-art by about 2.08 absolute
percent points, which, given the high accuracy of the
baseline, corresponds to 13.5% of relative error re-
duction.
We carried out similar experiments for frame clas-
sification. One interesting difference is that SK im-
proves BOW by only 0.70, i.e., 4 times less than in
the VerbNet setting. This suggests that word order
around the predicate is more important for deriving
the VerbNet class than the FrameNet frame. Ad-
ditionally, LCT or GRCT seems to be invariant for
both PTK and SPTK whereas the lexical similarity
still produces a relevant improvement on PTK, i.e.,
13% of relative error reduction, for an absolute accu-
racy of 93.78%. The latter improves over the state-
50% 
60% 
70% 
80% 
90% 
0% 20% 40% 60% 80% 100% 
Accu
racy 
Percentage of train examples 
SPTK 
BOW 
Brown et al 
Figure 4: Learning curves: VerbNet accuracy with the
none Class
of-the-art, i.e., 92.63% derived in (Giuglea and Mos-
chitti, 2006), by using STK on CT on 133 frames.
We also carried out experiments to understand
the role of the none class. Table 3 reports on the
VerbNet classification without its instances. This is
of course an unrealistic setting as it would assume
that the current VerbNet release already includes all
senses for English verbs. In the table, we note that
the overall accuracy highly increases and the differ-
ence between models reduces. The similarities play
no role anymore. This may suggest that SPTK can
help in complex settings, where verb class character-
ization is more difficult. Another important role of
SPTK models is their ability to generalize. To test
this aspect, Figure 4 illustrates the learning curves
of SPTK with respect to BOW and the accuracy
achieved by BR (with a constant line). It is impres-
sive to note that with only 40% of the data SPTK can
reach the state-of-the-art.
6 Model Analysis and Discussion
We carried out analysis of system errors and its in-
duced features. These can be examined by apply-
ing the reverse engineering tool5 proposed in (Pighin
and Moschitti, 2010; Pighin and Moschitti, 2009a;
Pighin and Moschitti, 2009b), which extracts the
most important features for the classification model.
Many mistakes are related to false positives and neg-
atives of the none class (about 72% of the errors).
This class also causes data imbalance. Most errors
are also due to lack of lexical information available
to the SPTK kernel: (i) in 30% of the errors, the
argument heads were proper nouns for which the
lexical generalization provided by the DMs was not
5http://danielepighin.net/cms/software/flink
269
VerbNet class 13.5.1
(IM(VB(target))(OBJ))
(VC(VB(target))(OBJ))
(VC(VBG(target))(OBJ))
(OPRD(TO)(IM(VB(target))(OBJ)))
(PMOD(VBG(target))(OBJ))
(VB(target))
(VC(VBN(target)))
(PRP(TO)(IM(VB(target))(OBJ)))
(IM(VB(target))(OBJ)(ADV(IN)(PMOD)))
(OPRD(TO)(IM(VB(target))(OBJ)(ADV(IN)(PMOD))))
VerbNet class 60
(VC(VB(target))(OBJ))
(NMOD(VBG(target))(OPRD))
(VC(VBN(target))(OPRD))
(NMOD(VBN(target))(OPRD))
(PMOD(VBG(target))(OBJ))
(ROOT(SBJ)(VBD(target))(OBJ)(P(,)))
(VC(VB(target))(OPRD))
(ROOT(SBJ)(VBZ(target))(OBJ)(P(,)))
(NMOD(SBJ(WDT))(VBZ(target))(OPRD))
(NMOD(SBJ)(VBZ(target))(OPRD(SBJ)(TO)(IM)))
Table 4: GRCT fragments
available; and (ii) in 76% of the errors only 2 or less
argument heads are included in the extracted tree,
therefore tree kernels cannot exploit enough lexical
information to disambiguate verb senses. Addition-
ally, ambiguity characterizes errors where the sys-
tem is linguistically consistent but the learned selec-
tional preferences are not sufficient to separate verb
senses. These errors are mainly due to the lack of
contextual information. While error analysis sug-
gests that further improvement is possible (e.g. by
exploiting proper nouns), the type of generalizations
currently achieved by SPTK are rather effective. Ta-
ble 4 and 5 report the tree structures characterizing
the most informative training examples of the two
senses of the verb order, i.e. the VerbNet classes
13.5.1 (make a request for something) and 60 (give
instructions to or direct somebody to do something
with authority).
In line with the method discussed in (Pighin and
Moschitti, 2009b), these fragments are extracted as
they appear in most of the support vectors selected
during SVM training. As easily seen, the two classes
are captured by rather different patterns. The typ-
ical accusative form with an explicit direct object
emerges as characterizing the sense 13.5.1, denot-
ing the THEME role. All fragments of the sense 60
emphasize instead the sentential complement of the
verb that in fact expresses the standard PROPOSI-
TION role in VerbNet. Notice that tree fragments
correspond to syntactic patterns. The a posteriori
VerbNet class 13.5.1
(VP(VB(target))(NP))
(VP(VBG(target))(NP))
(VP(VBD(target))(NP))
(VP(TO)(VP(VB(target))(NP)))
(S(NP-SBJ)(VP(VBP(target))(NP)))
VerbNet class 60
(VBN(target))
(VP(VBD(target))(S))
(VP(VBZ(target))(S))
(VBP(target))
(VP(VBD(target))(NP-1)(S(NP-SBJ)(VP)))
Table 5: CT fragments
analysis of the learned models (i.e. the underlying
support vectors) confirm very interesting grammati-
cal generalizations, i.e. the capability of tree kernels
to implicitly trigger useful linguistic inductions for
complex semantic tasks. When SPTK are adopted,
verb arguments can be lexically generalized into
word classes, i.e., clusters of argument heads (e.g.
commission vs. delegation, or gift vs. present). Au-
tomatic generation of such classes is an interesting
direction for future research.
7 Conclusion
We have proposed new approaches to characterize
verb classes in learning algorithms. The key idea is
the use of structural representation of verbs based on
syntactic dependencies and the use of structural ker-
nels to measure similarity between such representa-
tions. The advantage of kernel methods is that they
can be directly used in some learning algorithms,
e.g., SVMs, to train verb classifiers. Very interest-
ingly, we can encode distributional lexical similar-
ity in the similarity function acting over syntactic
structures and this allows for generalizing selection
restrictions through a sort of (supervised) syntactic
and semantic co-clustering.
The verb classification results show a large im-
provement over the state-of-the-art for both Verb-
Net and FrameNet, with a relative error reduction
of about 13.5% and 16.0%, respectively. In the fu-
ture, we plan to exploit the models learned from
FrameNet and VerbNet to carry out automatic map-
ping of verbs from one theory to the other.
Acknowledgements This research is partially sup-
ported by the European Community?s Seventh Frame-
work Programme (FP7/2007-2013) under grant numbers
247758 (ETERNALS), 288024 (LIMOSINE) and 231126
(LIVINGKNOWLEDGE). Many thanks to the reviewers
for their valuable suggestions.
270
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a collec-
tion of very large linguistically processed web-crawled
corpora. LRE, 43(3):209?226.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Gianni Amati, Claudio Carpineto, and
Gianni Romano, editors, Proceedings of ECIR, vol-
ume 4425 of Lecture Notes in Computer Science,
pages 307?318. Springer, APR.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In CIKM?07: Proceedings of the sixteenth ACM con-
ference on Conference on information and knowledge
management, pages 861?864, New York, NY, USA.
ACM.
Susan Windisch Brown, Dmitriy Dligach, and Martha
Palmer. 2011. Verbnet class assignment as a wsd task.
In Proceedings of the Ninth International Conference
on Computational Semantics, IWCS ?11, pages 85?94,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2001. Latent semantic kernels. In Carla Brodley and
Andrea Danyluk, editors, Proceedings of ICML-01,
18th International Conference on Machine Learning,
pages 66?73, Williams College, US. Morgan Kauf-
mann Publishers, San Francisco, US.
Danilo Croce, Cristina Giannone, Paolo Annesi, and
Roberto Basili. 2010. Towards open-domain semantic
role labeling. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 237?246, Uppsala, Sweden, July. Association
for Computational Linguistics.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured Lexical Similarity via Convolution
Kernels on Dependency Trees. In Proceedings of
EMNLP 2011.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
ACL (Short Papers), pages 29?32. The Association for
Computer Linguistics.
J. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis. Philological
Society, Oxford. reprinted in Palmer, F. (ed. 1968) Se-
lected Papers of J. R. Firth, Longman, Harlow.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proc. of SIGIR ?88, New York, USA.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA.
Ana-Maria Giuglea and Alessandro Moschitti. 2006. Se-
mantic role labeling via framenet, verbnet and prop-
bank. In Proceedings of ACL, pages 929?936, Sydney,
Australia, July.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
G. Golub and W. Kahan. 1965. Calculating the singular
values and pseudo-inverse of a matrix. Journal of the
Society for Industrial and Applied Mathematics: Se-
ries B, Numerical Analysis.
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proceedings of
ICML?00.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of
CoNLL 2008, pages 183?187.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Tom Landauer and Sue Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis theory
271
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar word. In Proceedings of COLING-ACL, Mon-
treal, Canada.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between prop-
bank and verbnet. In In Proceedings of the 7th Inter-
national Workshop on Computational Linguistics.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In HLT-NAACL,
pages 1020?1028.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. Isp:
Learning inferential selectional preferences. In Pro-
ceedings of HLT/NAACL 2007.
Daniele Pighin and Alessandro Moschitti. 2009a. Ef-
ficient linearization of tree kernel functions. In Pro-
ceedings of CoNLL?09.
Daniele Pighin and Alessandro Moschitti. 2009b. Re-
verse engineering of tree kernel feature spaces. In Pro-
ceedings of EMNLP, pages 111?120, Singapore, Au-
gust. Association for Computational Linguistics.
Daniele Pighin and Alessandro Moschitti. 2010. On
reverse feature engineering of syntactic tree kernels.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, CoNLL ?10,
pages 223?233, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. Journal of Machine Learning
Research, 5:101?141.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylyania.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97?
123.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Empirical Methods for Natural Language Processing
(EMNLP), pages 89?96, Sapporo, Japan.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Blackwells, Oxford.
Alexander S. Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In COLING,
pages 947?953.
Ben?at Zapirain, Eneko Agirre, Llu??s Ma`rquez, and Mi-
hai Surdeanu. 2010. Improving semantic role classi-
fication with selectional preferences. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 373?376,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
272
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 363?367,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection
Jinho D. Choi
Department of Computer Science
University of Colorado Boulder
choijd@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado Boulder
mpalmer@colorado.edu
Abstract
This paper presents a novel way of improv-
ing POS tagging on heterogeneous data. First,
two separate models are trained (generalized
and domain-specific) from the same data set
by controlling lexical items with different doc-
ument frequencies. During decoding, one of
the models is selected dynamically given the
cosine similarity between each sentence and
the training data. This dynamic model selec-
tion approach, coupled with a one-pass, left-
to-right POS tagging algorithm, is evaluated
on corpora from seven different genres. Even
with this simple tagging algorithm, our sys-
tem shows comparable results against other
state-of-the-art systems, and gives higher ac-
curacies when evaluated on a mixture of the
data. Furthermore, our system is able to tag
about 32K tokens per second. We believe that
this model selection approach can be applied
to more sophisticated tagging algorithms and
improve their robustness even further.
1 Introduction
When it comes to POS tagging, two things must be
checked. First, a POS tagger needs to be tested for
its robustness in handling heterogeneous data.1 Sta-
tistical POS taggers perform very well when their
training and testing data are from the same source,
achieving over 97% tagging accuracy (Toutanova et
al., 2003; Gime?nez and Ma`rquez, 2004; Shen et
al., 2007). However, the performance degrades in-
creasingly as the discrepancy between the training
1We use the term ?heterogeneous data? as a mixture of data
collected from several different sources.
and testing data gets larger. Thus, to ensure robust-
ness, a tagger needs to be evaluated on several dif-
ferent kinds of data. Second, a POS tagger should be
tested for its speed. POS tagging is often performed
as a pre-processing step to other tasks (e.g., pars-
ing, chunking) and it should not be a bottleneck for
those tasks. Moreover, recent NLP tasks deal with
very large-scale data where tagging speed is critical.
To improve robustness, we first train two separate
models; one is optimized for a general domain and
the other is optimized for a domain specific to the
training data. During decoding, we dynamically se-
lect one of the models by measuring similarities be-
tween input sentences and the training data. Our hy-
pothesis is that the domain-specific and generalized
models perform better for sentences similar and not
similar to the training data, respectively. In this pa-
per, we describe how to build both models using the
same training data and select an appropriate model
given input sentences during decoding. Each model
uses a one-pass, left-to-right POS tagging algorithm.
Even with the simple tagging algorithm, our system
gives results that are comparable to two other state-
of-the-art systems when coupled with this dynamic
model selection approach. Furthermore, our system
shows noticeably faster tagging speed compared to
the other two systems.
For our experiments, we use corpora from seven
different genres (Weischedel et al, 2011; Nielsen et
al., 2010). This allows us to check the performance
of each system on different kinds of data when run
individually or selectively. To the best of our knowl-
edge, this is the first time that a POS tagger has been
evaluated on such a wide variety of data in English.
363
2 Approach
2.1 Training generalized and domain-specific
models using document frequency
Consider training data as a collection of documents
where each document contains sentences focusing
on a similar topic. For instance, in the Wall Street
Journal corpus, a document can be an individual file
or all files within each section.2 To build a gener-
alized model, lexical features (e.g., n-gram word-
forms) that are too specific to individual documents
should be avoided so that a classifier can place more
weight on features common to all documents.
To filter out these document-specific features, a
threshold is set for the document frequency of each
lowercase simplified word-form (LSW) in the train-
ing data. A simplified word-form (SW) is derived by
applying the following regular expressions sequen-
tially to the original word-form, w. ?replaceAll? is a
function that replaces all matches of the regular ex-
pression in w (the 1st parameter) with the specific
string (the 2nd parameter). In a simplified word, all
numerical expressions are replaced with 0.
1. w.replaceAll(\d%, 0) (e.g., 1% ? 0)
2. w.replaceAll(\$\d, 0) (e.g., $1 ? 0)
3. w.replaceAll(?\.\d, 0) (e.g., .1 ? 0)
4. w.replaceAll(\d(,|:|-|\/|\.)\d, 0)
(e.g., 1,2|1:2|1-2|1/2|1.2 ? 0)
5. w.replaceAll(\d+, 0) (e.g., 1234 ? 0)
A LSW is a decapitalized SW. Given a set of LSW?s
whose document frequencies are greater than a cer-
tain threshold, a model is trained by using only lexi-
cal features associated with these LSW?s. For a gen-
eralized model, we use a threshold of 2, meaning
that only lexical features whose LSW?s occur in at
least 3 documents of the training data are used. For
a domain-specific model, we use a threshold of 1.
The generalized and domain-specific models are
trained separately; their learning parameters are op-
timized by running n-fold cross-validation where n
is the total number of documents in the training data
and grid search on Liblinear parameters c and B (see
Section 2.4 for more details about the parameters).
2For our experiments, we treat each section of the Wall
Street Journal as one document.
2.2 Dynamic model selection during decoding
Once both generalized and domain-specific models
are trained, alternative approaches can be adapted
for decoding. One is to run both models and merge
their outputs. This approach can produce output that
is potentially more accurate than output from either
model, but takes longer to decode because the merg-
ing cannot be processed until both models are fin-
ished. Instead, we take an alternative approach, that
is to select one of the models dynamically given the
input sentence. If the model selection is done ef-
ficiently, this approach runs as fast as running just
one model, yet can give more robust performance.
The premise of this dynamic model selection is
that the domain-specific model performs better for
input sentences similar to its training space, whereas
the generalized model performs better for ones that
are dissimilar. To measure similarity, a set of SW?s,
say T , used for training the domain-specific model
is collected. During decoding, a set of SW?s in each
sentence, say S, is collected. If the cosine similarity
between T and S is greater than a certain threshold,
the domain-specific model is selected for decoding;
otherwise, the generalized model is selected.
0.0710 0.02 0.04
190
0
40
80
120
160
Cosine Similarity
Occ
urre
nce
5%
Figure 1: Cosine similarity distribution: the y-axis shows
the number of occurrences for each cosine similarity dur-
ing cross-validation.
The threshold is derived automatically by running
cross-validation; for each fold, both models are run
simultaneously and cosine similarities of sentences
on which the domain-specific model performs bet-
ter are extracted. Figure 1 shows the distribution
of cosine similarities extracted during our cross-
validation. Given the cosine similarity distribution,
the similarity at the first 5% area (in this case, 0.025)
is taken as the threshold.
364
2.3 Tagging algorithm and features
Each model uses a one-pass, left-to-right POS tag-
ging algorithm. The motivation is to analyze how
dynamic model selection works with a simple algo-
rithm first and then apply it to more sophisticated
ones later (e.g., bidirectional tagging algorithm).
Our feature set (Table 1) is inspired by Gime?nez
and Ma`rquez (2004) although ambiguity classes are
derived selectively for our case. Given a word-form,
we count how often each POS tag is used with the
form and keep only ones above a certain threshold.
For both generalized and domain-specific models, a
threshold of 0.7 is used, which keeps only POS tags
used with their forms over 70% of the time. From
our experiments, we find this to be more useful than
expanding ambiguity classes with lower thresholds.
Lexical
fi?{0,1,2,3}, (mi?2,i?1), (mi?1,i), (mi?1,i+1),
(mi,i+1), (mi+1,i+2), (mi?2,i?1,i), (mi?1,i,i+1),
(mi,i+1,i+2), (mi?2,i?1,i+1), (mi?1,i+1,i+2)
POS
pi?{3,2,1}, ai+{0,1,2,3}, (pi?2,i?1), (ai+1,i+2),
(pi?1, ai+1), (pi?2, pi?1, ai), (pi?2, pi?1, ai+1),
(pi?1, ai, ai+1), (pi?1, ai+1, ai+2)
Affix c:1, c:2, c:3, cn:, cn?1:, cn?2:, cn?3:
Binary
initial uppercase, all uppercase/lowercase,
contains 1/2+ capital(s) not at the beginning,
contains a (period/number/hyphen)
Table 1: Feature templates. i: the index of the current
word, f : SW, m: LSW, p: POS, a: ambiguity class, c?:
character sequence in wi (e.g., c:2: the 1st and 2nd char-
acters of wi, cn?1:: the n-1?th and n?th characters of wi).
See Gime?nez and Ma`rquez (2004) for more details.
2.4 Machine learning
Liblinear L2-regularization, L1-loss support vector
classification is used for our experiments (Hsieh et
al., 2008). From several rounds of cross-validation,
learning parameters of (c = 0.2, e = 0.1, B = 0.4) and
(c = 0.1, e = 0.1, B = 0.9) are found for the gener-
alized and domain-specific models, respectively (c:
cost, e: termination criterion, B: bias).
3 Related work
Toutanova et al (2003) introduced a POS tagging
algorithm using bidirectional dependency networks,
and showed the best contemporary results. Gime?nez
and Ma`rquez (2004) used one-pass, left-to-right
and right-to-left combined tagging algorithm and
achieved near state-of-the-art results. Shen et al
(2007) presented a tagging approach using guided
learning for bidirectional sequence classification and
showed current state-of-the-art results.3
Our individual models (generalized and domain-
specific) are similar to Gime?nez and Ma`rquez (2004)
in that we use a subset of their features and take one-
pass, left-to-right tagging approach, which is a sim-
pler version of theirs. However, we use Liblinear for
learning, which trains much faster than their classi-
fier, Support Vector Machines.
4 Experiments
4.1 Corpora
For training, sections 2-21 of the Wall Street Jour-
nal (WSJ) from OntoNotes v4.0 (Weischedel et al,
2011) are used. The entire training data consists of
30,060 sentences with 731,677 tokens. For evalua-
tion, corpora from seven different genres are used:
the MSNBC broadcasting conversation (BC), the
CNN broadcasting news (BN), the Sinorama news
magazine (MZ), the WSJ newswire (NW), and the
GALE web-text (WB), all from OntoNotes v4.0. Ad-
ditionally, the Mipacq clinical notes (CN) and the
Medpedia articles (MD) are used for evaluation of
medical domains (Nielsen et al, 2010). Table 2
shows distributions of these evaluation sets.
4.2 Accuracy comparisons
Our models are compared with two other state-of-
the-art systems, the Stanford tagger (Toutanova et
al., 2003) and the SVMTool (Gime?nez and Ma`rquez,
2004). Both systems are trained with the same train-
ing data and use configurations optimized for their
best reported results. Tables 3 and 4 show tagging
accuracies of all tokens and unknown tokens, re-
spectively. Our individual models (Models D and
G) give comparable results to the other systems.
Model G performs better than Model D for BC, CN,
and MD, which are very different from the WSJ.
This implies that the generalized model shows its
strength in tagging data that differs from the train-
ing data. The dynamic model selection approach
(Model S) shows the most robust results across gen-
res, although Models D and G still can perform
3Some semi-supervised and domain-adaptation approaches
using external data had shown better performance (Daume III,
2007; Spoustova? et al, 2009; S?gaard, 2011).
365
BC BN CN MD MZ NW WB Total
Source MSNBC CNN Mipacq Medpedia Sinorama WSJ ENG -
Sentences 2,076 1,969 3,170 1,850 1,409 1,640 1,738 13,852
All tokens 31,704 31,328 35,721 34,022 32,120 39,590 34,707 239,192
Unknown tokens 3,077 1,284 6,077 4,755 2,663 983 2,609 21,448
Table 2: Distributions of evaluation sets. The Total column indicates a mixture of data from all genres.
BC BN CN MD MZ NW WB Total
Model D 91.81 95.27 87.36 90.74 93.91 97.45 93.93 92.97
Model G 92.65 94.82 88.24 91.46 93.24 97.11 93.51 93.05
Model S 92.26 95.13 88.18 91.34 93.88 97.46 93.90 93.21
G over D 50.63 36.67 68.80 40.22 21.43 9.51 36.02 41.74
Stanford 87.71 95.50 88.49 90.86 92.80 97.42 94.01 92.50
SVMTool 87.82 95.13 87.86 90.54 92.94 97.31 93.99 92.32
Table 3: Tagging accuracies of all tokens (in %). Models D and G indicate domain-specific and generalized models,
respectively and Model S indicates the dynamic model selection approach. ?G over D? shows how often Model G is
selected over Model D using the dynamic selection (in %).
BC BN CN MD MZ NW WB Total
Model S 60.97 77.73 68.69 67.30 75.97 88.40 76.27 70.54
Stanford 19.24 87.31 71.20 64.82 66.28 88.40 78.15 64.32
SVMTool 19.08 78.35 66.51 62.94 65.23 86.88 76.47 47.65
Table 4: Tagging accuracies of unknown tokens (in %).
better for individual genres (except for NW, where
Model S performs better than any other model).
For both all and unknown token experiments,
Model S performs better than the other systems
when evaluated on a mixture of the data (the Total
column). The differences are statistically significant
for both experiments (McNemar?s test, p < .0001).
The Stanford tagger gives significantly better results
for unknown tokens in BN; we suspect that this is
where their bidirectional tagging algorithm has an
advantage over our simple left-to-right algorithm.
4.3 Speed comparisons
Tagging speeds are measured by running each sys-
tem on the mixture of all data. Our system and the
Stanford system are both written in Java; the Stan-
ford tagger provides APIs that allow us to make fair
comparisons between the two systems. The SVM-
Tool is written in Perl, so there is a systematic dif-
ference between the SVMTool and our system.
Table 5 shows speed comparisons between these
systems. All experiments are evaluated on an In-
tel Xeon 2.57GHz machine. Our system tags about
32K tokens per second (0.03 milliseconds per to-
ken), which includes run-time for both POS tagging
and model selection.
Stanford SVMTool Model S
tokens / sec. 421 1,163 31,914
Table 5: Tagging speeds.
5 Conclusion
We present a dynamic model selection approach that
improves the robustness of POS tagging on hetero-
geneous data. We believe that this approach can
be applied to more sophisticated algorithms and im-
prove their robustness even further. Our system also
shows noticeably faster tagging speed against two
other state-of-the-art systems. For future work, we
will experiment with more diverse training and test-
ing data and also more sophisticated algorithms.
Acknowledgments
This work was supported by the SHARP program
funded by ONC: 90TR0002/01. The content is
solely the responsibility of the authors and does not
necessarily represent the official views of the ONC.
366
References
Hal Daume III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL?07, pages 256?263.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vec-
tor Machines. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
LREC?04.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A Dual Coordi-
nate Descent Method for Large-scale Linear SVM. In
Proceedings of the 25th international conference on
Machine learning, ICML?08, pages 408?415.
Rodney D. Nielsen, James Masanz, Philip Ogren, Wayne
Ward, James H. Martin, Guergana Savova, and Martha
Palmer. 2010. An architecture for complex clinical
question answering. In Proceedings of the 1st ACM
International Health Informatics Symposium, IHI?10,
pages 395?399.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided Learning for Bidirectional Sequence Classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL?07, pages 760?767.
Anders S?gaard. 2011. Semi-supervised condensed
nearest neighbor for part-of-speech tagging. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, ACL?11, pages 48?52.
Drahom??ra ?johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised Train-
ing for the Averaged Perceptron POS Tagger. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL?09, pages 763?771.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of the Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
NAACL?03, pages 173?180.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue. 2011. OntoNotes: A
Large Training Corpus for Enhanced Processing. In
Joseph Olive, Caitlin Christianson, and John McCary,
editors, Handbook of Natural Language Processing
and Machine Translation. Springer.
367
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1030?1040,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Step-wise Usage-based Method for Inducing
Polysemy-aware Verb Classes
Daisuke Kawahara
?
Daniel W. Peterson
?
Martha Palmer
?
?
Kyoto University, Kyoto, Japan
?
University of Colorado at Boulder, Boulder, CO, USA
dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu
Abstract
We present an unsupervised method for in-
ducing verb classes from verb uses in giga-
word corpora. Our method consists of
two clustering steps: verb-specific seman-
tic frames are first induced by clustering
verb uses in a corpus and then verb classes
are induced by clustering these frames.
By taking this step-wise approach, we can
not only generate verb classes based on a
massive amount of verb uses in a scalable
manner, but also deal with verb polysemy,
which is bypassed by most of the previous
studies on verb clustering. In our exper-
iments, we acquire semantic frames and
verb classes from two giga-word corpora,
the larger comprising 20 billion words.
The effectiveness of our approach is veri-
fied through quantitative evaluations based
on polysemy-aware gold-standard data.
1 Introduction
A verb plays a primary role in conveying the
meaning of a sentence. Capturing the sense of a
verb is essential for natural language processing
(NLP), and thus lexical resources for verbs play
an important role in NLP.
Verb classes are one such lexical resource.
Manually-crafted verb classes have been devel-
oped, such as Levin?s classes (Levin, 1993) and
their extension, VerbNet (Kipper-Schuler, 2005),
in which verbs are organized into classes on the
basis of their syntactic and semantic behavior.
Such verb classes have been used in many NLP ap-
plications that need to consider semantics in par-
ticular, such as word sense disambiguation (Dang,
2004), semantic parsing (Swier and Stevenson,
2005; Shi andMihalcea, 2005) and discourse pars-
ing (Subba and Di Eugenio, 2009).
There have also been many attempts to auto-
matically acquire verb classes with the goal of ei-
ther adding frequency information to an existing
resource or of inducing similar verb classes for
other languages. Most of these approaches assume
that all target verbs are monosemous (Stevenson
and Joanis, 2003; Schulte im Walde, 2006; Joa-
nis et al, 2008; Li and Brew, 2008; Sun et al,
2008; Sun and Korhonen, 2009; Vlachos et al,
2009; Parisien and Stevenson, 2010; Parisien and
Stevenson, 2011; Falk et al, 2012; Lippincott et
al., 2012; Reichart and Korhonen, 2013; Sun et al,
2013). This monosemous assumption, however, is
not realistic because many frequent verbs actually
have multiple senses. Moreover, to the best of our
knowledge, none of the following approaches at-
tempt to quantitatively evaluate soft clusterings of
verb classes induced by polysemy-aware unsuper-
vised approaches (Korhonen et al, 2003; Lapata
and Brew, 2004; Li and Brew, 2007; Schulte im
Walde et al, 2008).
In this paper, we propose an unsupervised
method for inducing verb classes that is aware
of verb polysemy. Our method consists of two
clustering steps: verb-specific semantic frames are
first induced by clustering verb uses in a cor-
pus and then verb classes are induced by clus-
tering these frames. By taking this step-wise ap-
proach, we can not only induce verb classes with
frequency information from a massive amount of
verb uses in a scalable manner, but also deal with
verb polysemy.
Our novel contributions are summarized as fol-
lows:
? induce both semantic frames and verb classes
from a massive amount of verb uses by a scal-
able method,
? explicitly deal with verb polysemy,
? discover effective features for each of the
clustering steps, and
? quantitatively evaluate a soft clustering of
verbs.
1030
!"#$%&'(#)*#+%!"#%#,#-!( %%%$&.%&'(#)*#%$&.)%-"/0+%
-"/0+)#1%&'(#)*#+%'/)+( %%%2#%&'(#)*#+%1/-#%'/)+(%
3#%&'(#)*#+%!"#%)#(.0! %%%%2#%&'(#)*#+%445%6#&60#%
7 % % % % %%%7%
7 % % % % %%%7!
&'(#)*#84%
9!"#$:%"#:%7;%&'(#)*#%
%%%%%%%9#,#-!:%)#(.0!:%7;!
&'(#)*#8<%
9$&.:%2#:%7;%&'(#)*#%
%%%%%%9-"/0+:%6#&60#:%7;!
&'(#)*#8=%
92#:%-"/0+:%7;%&'(#)*#%
%%%%%%%%%9'/)+:%2/0+0/>#:%7;!
2?!-"8@%
9A:%2#:%7;%2?!-"%
%%%%%%%%%9'/)+:%?1/B?0:%7;!
7!
/1*#(CD?!#E=@FG! (/D"!E=5F<!
2#%2?!-"%&.)%'/)+(%
A%2?!-"#+%!"#%B&*/#%
H#%2/00%2?!-"%!"#%D?B#%
7%
7!
7!
I#)'%-0?((#(8!
J#B?1C-%>)?B#(8!
I#)'%.(#(8!
Figure 1: Overview of our two-step approach. Verb-specific semantic frames are first induced from verb
uses (lower part) and then verb classes are induced from the semantic frames (upper part). The labels of
verb classes are manually assigned here for better understanding.
2 Related Work
As stated in Section 1, most of the previous studies
on verb clustering assume that verbs are monose-
mous. A typical method in these studies is to rep-
resent each verb as a single data point and apply
classification (e.g., Joanis et al (2008)) or clus-
tering (e.g., Sun and Korhonen (2009)) to these
data points. As a representation for a data point,
distributions of subcategorization frames are often
used, and other semantic features (e.g., selectional
preferences) are sometimes added to improve the
performance.
Among these studies on monosemous verb clus-
tering (i.e., predominant class induction), there
have been several Bayesian methods. Vlachos
et al (2009) proposed a Dirichlet process mix-
ture model (DPMM; Neal (2000)) to cluster verbs
based on subcategorization frame distributions.
They evaluated their result with a gold-standard
test set, where a single class is assigned to a verb.
Parisien and Stevenson (2010) proposed a hierar-
chical Dirichlet process (HDP; Teh et al (2006))
model to jointly learn argument structures (sub-
categorization frames) and verb classes by using
syntactic features. Parisien and Stevenson (2011)
extended their model by adding semantic features.
They tried to account for verb learning by children
and did not evaluate the resultant verb classes.
Modi et al (2012) extended the model of Titov
and Klementiev (2012), which is an unsupervised
model for inducing semantic roles, to jointly in-
duce semantic roles and frames across verbs using
the Chinese Restaurant Process (Aldous, 1985).
All of the above methods considered verbs to be
monosemous and did not deal with verb polysemy.
Our approach also uses Bayesian methods, but is
designed to capture verb polysemy.
We summarize a few studies that consider poly-
semy of verbs in the rest of this section.
Miyao and Tsujii (2009) proposed a supervised
method that can handle verb polysemy. Their
method represents a verb?s syntactic and seman-
tic features, and learns a log-linear model from
the SemLink corpus (Loper et al, 2007). Boleda
et al (2007) also proposed a supervised method
for Catalan adjectives considering the polysemy of
adjectives.
The most closely related work to our polysemy-
aware task of unsupervised verb class induction is
the work of Korhonen et al (2003), who used dis-
tributions of subcategorization frames to cluster
verbs. They adopted the Nearest Neighbor (NN)
and Information Bottleneck (IB) methods for clus-
tering. In particular, they tried to consider verb
polysemy by using the IB method, which is a soft
clustering method (Tishby et al, 1999). However,
the verb itself is still represented as a single data
point. After performing soft clustering, they noted
that most verbs fell into a single class, and they
decided to assign a single class to each verb by
hardening the clustering. They considered multi-
ple classes only in the gold-standard data used for
their evaluations. We also evaluate our induced
verb classes on this gold-standard data, which was
created on the basis of Levin?s classes (Levin,
1993).
Lapata and Brew (2004) and Li and Brew
(2007) proposed probabilistic models for calculat-
ing prior probabilities of verb classes for a verb.
These models are approximated to condition not
1031
on verbs but on subcategorization frames. As
mentioned in Li and Brew (2007), it is desirable
to extend the model to depend on verbs to fur-
ther improve accuracy. They conducted several
evaluations including predominant class induction
and token-level verb sense disambiguation, but did
not evaluate multiple classes output by their mod-
els. Schulte im Walde et al (2008) also applied
probabilistic soft clustering to verbs by incorporat-
ing subcategorization frames and selectional pref-
erences based on WordNet. This model is based
on the Expectation-Maximization algorithm and
the Minimum Description Length principle. Since
they focused on the incorporation of selectional
preferences, they did not evaluate verb classes but
evaluated only selectional preferences using a lan-
guage model-based measure.
Materna proposed LDA-frames, which are de-
fined across verbs and can be considered to be
a kind of verb class (Materna, 2012; Materna,
2013). LDA-frames are probabilistic semantic
frames automatically induced from a raw corpus.
He used a model based on latent Dirichlet alo-
cation (LDA; Blei et al (2003)) and the Dirichlet
process to cluster verb instances of a triple (sub-
ject, verb, object) to produce semantic frames and
roles. Both of these are represented as a proba-
bilistic distribution of words across verbs. He ap-
plied this method to the BNC and acquired 1,200
frames and 400 roles (Materna, 2012). He did not
evaluate the resulting frames as verb classes.
In sum, there have been no studies that quantita-
tively evaluate polysemous verb classes automati-
cally induced by unsupervised methods.
3 Our Approach
3.1 Overview
Our objective is to automatically learn semantic
frames and verb classes from a massive amount
of verb uses following usage-based approaches.
Although Bayesian approaches are a possible so-
lution to simultaneously induce frames and verb
classes from a corpus as used in previous stud-
ies, it has prohibitive computational cost. For in-
stance, Parisien and Stevenson applied HDP only
to a small-scale child speech corpus that contains
170K verb uses to jointly induce subcategoriza-
tion frames and verb classes (Parisien and Steven-
son, 2010; Parisien and Stevenson, 2011). Ma-
terna applied an LDA-based method to the BNC,
which contains 1.4M verb uses, to induce seman-
tic frames across verbs that can be considered to
be verb classes (Materna, 2012; Materna, 2013).
However, it would take three months for this ex-
periment using this 100 million word corpus.
1
Al-
though it is best to use the largest possible cor-
pus for this kind of knowledge acquisition tasks
(Sasano et al, 2009), it is infeasible to scale to
giga-word corpora using such joint models.
In this paper, we propose a two-step approach
for inducing semantic frames and verb classes.
First, we make multiple data points for each verb
to deal with verb polysemy (cf. polysemy-aware
previous studies still represented a verb as one
data point (Korhonen et al, 2003; Miyao and Tsu-
jii, 2009)). To do that, we induce verb-specific
semantic frames by clustering verb uses. Then,
we induce verb classes by clustering these verb-
specific semantic frames across verbs. An interest-
ing point here is that we can use exactly the same
method for these two clustering steps.
Our procedure to automatically induce verb
classes from verb uses is summarized as follows:
1. induce verb-specific semantic frames by clus-
tering predicate-argument structures for each
verb extracted from automatic parses as
shown in the lower part of Figure 1, and
2. induce verb classes by clustering the induced
semantic frames across verbs as shown in the
upper part of Figure 1.
Each of these two steps is described in the follow-
ing sections in detail.
3.2 Inducing Verb-specific Semantic Frames
We induce verb-specific semantic frames from
verb uses based on the method of Kawahara et al
(2014). Our semantic frames consist of case slots,
each of which consists of word instances that can
be filled. The procedure for inducing these seman-
tic frames is as follows:
1. apply dependency parsing to a raw corpus
and extract predicate-argument structures for
each verb from the automatic parses,
2. merge the predicate-argument structures that
have presumably the same meaning based
on the assumption of one sense per colloca-
tion (Yarowsky, 1993) to get a set of initial
frames, and
1
In our replication experiment, it took a week to perform
70 iterations usingMaterna?s code and an Intel Xeon E5-2680
(2.7GHz) CPU. To reach 1,000 iterations, which are reported
to be optimum, it would take three months.
1032
3. apply clustering to the initial frames based
on the Chinese Restaurant Process (Al-
dous, 1985) to produce verb-specific seman-
tic frames.
These three steps are briefly described below.
3.2.1 Extracting Predicate-argument
Structures from a Raw Corpus
We apply dependency parsing to a large raw cor-
pus. We use the Stanford parser with Stanford
dependencies (de Marneffe et al, 2006).
2
Col-
lapsed dependencies are adopted to directly extract
prepositional phrases.
Then, we extract predicate-argument structures
from the dependency parses. Dependents that have
the following dependency relations to a verb are
extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp,
prep ?
In this process, the verb and arguments are lem-
matized, and only the head of an argument is pre-
served for compound nouns.
Predicate-argument structures are collected for
each verb and the subsequent processes are ap-
plied to the predicate-argument structures of each
verb.
3.2.2 Constructing Initial Frames from
Predicate-argument Structures
To make the computation feasible, we merge the
predicate-argument structures that have the same
or similar meaning to get initial frames. These ini-
tial frames are the input of the subsequent cluster-
ing process. For this merge, we assume one sense
per collocation (Yarowsky, 1993) for predicate-
argument structures.
For each predicate-argument structure of a verb,
we couple the verb and an argument to make a unit
for sense disambiguation. We select an argument
in the following order by considering the degree of
effect on the verb sense:
3
dobj, ccomp, nsubj, prep ?, iobj.
Then, the predicate-argument structures that
have the same verb and argument pair (slot and
word, e.g., ?dobj:effect?) are merged into an ini-
tial frame. After this process, we discard minor
initial frames that occur fewer than 10 times.
2
http://nlp.stanford.edu/software/lex-parser.shtml
3
If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.
3.2.3 Clustering Method
We cluster initial frames for each verb to pro-
duce semantic frames using the Chinese Restau-
rant Process (Aldous, 1985), regarding each initial
frame as an instance.
We calculate the posterior probability of a clus-
ter c
j
given an initial frame f
i
as follows:
P (c
j
|f
i
) ?
{
n(c
j
)
N+?
? P (f
i
|c
j
) c
j
?= new
?
N+?
? P (f
i
|c
j
) c
j
= new,
(1)
whereN is the number of initial frames for the tar-
get verb and n(c
j
) is the current number of initial
frames assigned to the cluster c
j
. ? is a hyper-
parameter that determines how likely it is for a
new cluster to be created. In this equation, the first
term is the Dirichlet process prior and the second
term is the likelihood of f
i
.
P (f
i
|c
j
) is defined based on the Dirichlet-
Multinomial distribution as follows:
P (f
i
|c
j
) =
?
w?V
P (w|c
j
)
count(f
i
,w)
, (2)
where V is the vocabulary in all case slots cooc-
curring with the verb and count(f
i
, w) is the num-
ber of w in the initial frame f
i
. The original
method in Kawahara et al (2014) defined w as
pairs of slots and words, e.g., ?nsubj:child? and
?dobj:bird,? but does not consider slot-only fea-
tures, e.g., ?nsubj? and ?dobj,? which ignore lex-
ical information. Here we experiment with both
representations and compare the results.
P (w|c
j
) is defined as follows:
P (w|c
j
) =
count(c
j
, w) + ?
?
t?V
count(c
j
, t) + |V | ? ?
, (3)
where count(c
j
, w) is the current number of w
in the cluster c
j
, and ? is a hyper-parameter of
Dirichlet distribution. For a new cluster, this prob-
ability is uniform (1/|V |).
We regard each output cluster as a semantic
frame, by merging the initial frames in a clus-
ter into a semantic frame. In this way, semantic
frames for each verb are acquired.
We use Gibbs sampling to realize this cluster-
ing.
3.3 Inducing Verb Classes from Semantic
Frames
To induce verb classes across verbs, we apply
clustering to the induced verb-specific semantic
1033
frames. We can use exactly the same clustering
method as described in Section 3.2.3 by using se-
mantic frames for multiple verbs as an input in-
stead of initial frames for a single verb. This is
because an initial frame has the same structure as
a semantic frame, which is produced by merging
initial frames. We regard each output cluster as a
verb class this time.
For the features, w, in equation (2), we try the
two representations again: slot-only features and
slot-word pair features. The representation using
only slots corresponds to the consideration of only
syntactic argument patterns. The other representa-
tion using the slot-word pairs means that semantic
similarity based on word overlap is naturally con-
sidered by looking at lexical information. We will
compare in our experiments four possible combi-
nations: two feature representations for each of the
two clustering steps.
4 Experiments and Evaluations
We first describe our experimental settings and de-
fine evaluation metrics to evaluate induced soft
clusterings of verb classes. Then, we con-
duct type-level multi-class evaluations, type-level
single-class evaluations and token-level multi-
class evaluations. These two levels of evaluations
are performed by considering the work of Reichart
et al (2010) on clustering evaluation. Finally, we
discuss the results of our full experiments.
4.1 Experimental Settings
We use two kinds of large-scale corpora: a web
corpus and the English Gigaword corpus.
To prepare a web corpus, we extracted sen-
tences from crawled web pages that are judged to
be written in English based on the encoding infor-
mation. Then, we selected sentences that consist
of at most 40 words, and removed duplicated sen-
tences. From this process, we obtained a corpus of
one billion sentences, totaling approximately 20
billion words. We focused on verbs whose fre-
quency in the web corpus was more than 1,000.
There were 19,649 verbs, including phrasal verbs,
and separating passive and active constructions.
We extracted 2,032,774,982 predicate-argument
structures.
We also used the English Gigaword corpus
(LDC2011T07; English Gigaword Fifth Edition).
This corpus consists of approximately 180 mil-
lion sentences, which totaling four billion words.
There were 7,356 verbs after applying the same
frequency threshold as the web corpus. We ex-
tracted 423,778,278 predicate-argument structures
from this corpus.
We set the hyper-parameters ? in (1) and ? in
(3) to 1.0. The cluster assignments for all the com-
ponents were initialized randomly. We took 100
samples for each input frame and selected the clus-
ter assignment that has the highest probability.
4.2 Evaluation Metrics
To measure the precision and recall of a cluster-
ing, modified purity and inverse purity (also called
collocation or weighted class accuracy) are com-
monly used in previous studies on verb clustering
(e.g., Sun and Korhonen (2009)). However, since
these measures are only applicable to a hard clus-
tering, it is necessary to extend them to be applica-
ble to a soft clustering, because in our task a verb
can belong to multiple clusters or classes.
4
We
propose a normalized version of modified purity
and inverse purity. This kind of normalization for
soft clusterings was performed for other evalua-
tion metrics as in Springorum et al (2013).
To measure the precision of a clustering, a nor-
malized version of modified purity is defined as
follows. Suppose K is the set of automatically in-
duced clusters and G is the set of gold classes. Let
K
i
be the verb vector of the i-th cluster and G
j
be
the verb vector of the j-th gold class. Each com-
ponent of these vectors is a normalized frequency,
which equals a cluster/class attribute probability
given a verb. Where there is no frequency in-
formation available for class distribution, such as
the gold-standard data described in Section 4.3,
we use a uniform distribution across the verb?s
classes. The core idea of purity is that each clus-
ter K
i
is associated with its most prevalent gold
class. In addition, to penalize clusters that consist
of only one verb, such singleton clusters in K are
considered as errors, as is usual with modified pu-
rity. The normalized modified purity (nmPU) can
then be written as follows:
nmPU =
1
N
?
i s.t. |K
i
|>1
max
j
?
K
i
(K
i
? G
j
), (4)
?
K
i
(K
i
? G
j
) =
?
v?K
i
?G
j
c
iv
, (5)
4
Korhonen et al (2003) evaluated hard clusterings based
on a gold standard with multiple classes per verb. They re-
ported only precision measures including modified purity,
and avoided extending the evaluation metrics for soft clus-
terings.
1034
verb classes verb classes
place 9 drop 9, 45, 004, 47,
51, A54, A30dye 24, 21, 41
focus 31, 45 bake 26, 45
stare 30 persuade 002
lay 9 sparkle 43
build 26, 45 pour 9, 43, 26, 57,
13, 31force 002, 11
glow 43 invent 26, 27
Table 1: An excerpt of the gold-standard verb
classes for several verbs from Korhonen et al
(2003). The classes starting with ?0? were de-
rived from the LCS database, those starting with
?A? were defined by Korhonen et al, and the other
classes were from Levin?s classes. A bolded class
is the predominant class for each verb.
where N denotes the total number of verbs, |K
i
|
denotes the number of positive components in
K
i
, and c
iv
denotes the v-th component of K
i
.
?
K
i
(K
i
? G
j
) means the total mass of the set of
verbs in K
i
?G
j
, given by summing up the values
in K
i
. In case of evaluating a hard clustering, this
is equal to |K
i
? G
j
| because all the values of c
iv
are equal to 1.
As usual, the following normalized inverse pu-
rity (niPU) is used to measure the recall of a clus-
tering:
niPU =
1
N
?
j
max
i
?
G
j
(K
i
? G
j
). (6)
Finally, we use the harmonic mean (F
1
) of nmPU
and niPU as a single measure of clustering quality.
4.3 Type-level Multi-class Evaluations
We first evaluate our induced verb classes on the
test set created by Korhonen et al (2003) (Table 1
of their paper) which was created by considering
verb polysemy on the basis of Levin?s classes and
the LCS database (Dorr, 1997). It consists of 62
classes and 110 verbs, out of which 35 verbs are
monosemous and 75 verbs are polysemous. The
average number of verb classes per verb is 2.24.
An excerpt from this data is shown in Table 1.
As our baselines, we adopt two previously pro-
posed methods. We first implemented a soft clus-
tering method for verb class induction proposed by
Korhonen et al (2003). They used the information
bottleneck (IB) method for assigning probabilities
of classes to each verb. Note that Korhonen et al
(2003) actually hardened the clusterings and left
method K nmPU niPU F
1
IB (k=35, t=0.10) 35.0 53.59 51.44 52.44
IB (k=35, t=0.05) 35.0 53.67 52.62 53.10
IB (k=35, t=0.02) 35.0 54.42 54.43 54.40
IB (k=35, t=0.01) 35.0 54.60 55.54 55.04
IB (k=42, t=0.10) 41.6 55.42 49.46 52.24
IB (k=42, t=0.05) 41.8 55.55 49.97 52.59
IB (k=42, t=0.02) 42.0 56.19 51.24 53.58
IB (k=42, t=0.01) 42.0 56.80 51.92 54.24
LDA-frames (t=0.10) 100 47.52 56.83 51.76
LDA-frames (t=0.05) 165 50.46 67.94 57.91
LDA-frames (t=0.02) 306 49.98 75.50 60.14
LDA-frames (t=0.01) 458 49.55 82.71 61.97
Gigaword/S-S 272.8 63.46 67.66 65.49
Gigaword/S-SW 36.4 31.49 95.70 47.38
Gigaword/SW-S 186.2 63.52 64.18 63.84
Gigaword/SW-SW 30.0 36.27 94.66 52.40
web/S-S 363.6 61.32 78.64 68.90
web/S-SW 52.2 35.80 99.30 52.62
web/SW-S 212.2 66.26 77.38 71.39
web/SW-SW 55.0 36.70 96.25 53.13
Table 2: Type-level multi-class evaluations. K rep-
resents the (average) number of induced classes.
?S? denotes the use of slot-only features and ?SW?
denotes the use of slot-word pair features. For ex-
ample, ?SW-S? means that slot-word pair features
are used for semantic frame induction and slot-
only features are used for verb class induction.
the evaluations of soft clusterings for their future
work. For input data, we employ VALEX (Ko-
rhonen et al, 2006), which is a publicly-available
large-scale subcategorization lexicon.
5
By follow-
ing the method of Korhonen et al (2003), preposi-
tional phrases (pp) are parameterized for two fre-
quent subcategorization frames (NP and NP PP),
and the unfiltered raw frequencies of subcatego-
rization frames are used as features to represent
a verb. It is necessary to specify the number of
clusters, k, for the IB method beforehand, and
we adopt 35 and 42 clusters according to their re-
ported high accuracies. To output multiple classes
for each verb, we set a threshold, t, for class at-
tribute probabilities. That is, classes that have a
higher class attribute probability than the thresh-
old are output for each verb. We report the results
of the following threshold values: 0.01, 0.02, 0.05
and 0.10.
The other baseline is LDA-frames (Materna,
2012). We use the induced LDA-frames that are
5
http://ilexir.co.uk/applications/valex/
1035
predominant class eval multiple class eval
method K mPU iPU F
1
mPU niPU F
1
NN 24 46.36 52.73 49.34 52.73 46.85 49.62
IB (k=35) 34.8 42.73 51.82 46.82 51.64 46.83 49.09
IB (k=42) 41.0 47.45 50.91 49.11 55.27 45.45 49.87
LDA-frames 53 30.00 47.27 36.71 41.82 44.28 43.01
Gigaword/S 9.6 25.64 71.27 37.70 32.91 64.71 43.62
Gigaword/SW 10.6 30.36 71.09 42.25 39.82 66.92 49.70
web/S 20.4 42.73 61.46 50.31 54.91 57.12 55.86
web/SW 11.8 34.36 71.82 46.40 49.09 67.01 56.50
Table 3: Type-level single-class evaluations against predominant/multiple classes. K represents the (av-
erage) number of induced classes.
available on the web site.
6
This frame data was in-
duced from the BNC and consists of 1,200 frames
and 400 semantic roles. Again, we set a threshold
for frame attribute probabilities.
We report results using our methods with four
feature combinations (slot-only (S) and slot-word
pair (SW) features each used for both the frame-
generation and verb-class clustering steps) for
both the Gigaword and web corpora. Table 2 lists
evaluation results for the baseline methods and our
methods.
7
The results of the IB baseline and our
methods are obtained by averaging five runs.
We can see that ?web/SW-S? achieved the best
performance and obtained a higher F
1
than the
baselines by more than nine points. ?Web/SW-
S? uses the combination of slot-word pair fea-
tures for clustering verb-specific frames and slot-
only features for clustering across verbs. Inter-
estingly, this result indicates that slot distributions
are more effective than lexical information in slot-
word pairs for inducing verb classes similar to the
gold standard. This result is consistent with ex-
pectations, given a gold standard based on Levin?s
verb classes, which are organized according to the
syntactic behavior of verbs. The use of slot-word
pairs for verb class induction generally merged too
many frames into each class, apparently due to ac-
cidental word overlaps across verbs.
The verb classes induced from the web corpus
achieved a higher F
1
than those from the Gigaword
corpus. This can be attributed to the larger size of
the web corpus. The employment of this kind of
huge corpus is enabled by our scalable method.
6
http://nlp.fi.muni.cz/projekty/lda-frames/
7
Although we do not think that the classes with very small
attribute probabilities are meaningful, the F
1
scores for lower
thresholds than 0.01 converged to about 66 in the case of
LDA-frames.
4.4 Type-level Single-class Evaluations
against Predominant/Multiple Classes
Since we focus on the handling of verb polysemy,
predominant class induction for each verb is not
our main objective. However, we wish to compare
our method with previous work on the induction of
a predominant (monosemous) class for each verb.
To output a single class for each verb by us-
ing our proposed method, we skip the induction
of verb-specific semantic frames and instead cre-
ate a single frame for each verb by merging all
predicate-argument structures of the verb. Then,
we apply clustering to these frames across verbs.
For clustering features, we again compare two rep-
resentations: slot-only features (S) and slot-word
pair features (SW).
We evaluate the single-class output for each
verb based on the predominant gold-standard
classes, which are defined for each verb in the
test set of Korhonen et al (2003). This data con-
tains 110 verbs and 33 classes. We evaluate these
single-class outputs in the same manner as Korho-
nen et al (2003), using the gold standard with mul-
tiple classes, which we also use for our multi-class
evaluations.
As we did with the multi-class evaluations, we
adopt modified purity (mPU), inverse purity (iPU)
and their harmonic mean (F
1
) as the metrics for the
evaluation with predominant classes. It is not nec-
essary to normalize these metrics when we treat
verbs as monosemous, and evaluate against the
predominant sense. When we evaluate against the
multiple classes in the gold standard, we do nor-
malize the inverse purity.
For baselines, we once more adopt the Nearest
Neighbor (NN) and Information Bottleneck (IB)
methods proposed by Korhonen et al (2003), and
LDA-frames proposed by Materna (2012). The
1036
clusterings with the NN and IB methods are ob-
tained by using the VALEX subcategorization lex-
icon. To harden the clusterings of the IB method
and the LDA-frames, the class with the highest
probability is selected for each verb. This hard-
ening process is exactly the same as Korhonen et
al. (2003). Note that our results of the NN and IB
methods are different from those reported in their
paper since the data source is different.
8
Table 3 lists accuracies of baseline methods and
our methods. Our proposed method using the web
corpus achieved comparable performance with the
baseline methods on the predominant class evalu-
ation and outperformed them on the multiple class
evaluation. More sophisticated methods for pre-
dominant class induction, such as the method of
Sun and Korhonen (2009) using selectional pref-
erences, could produce better single-class outputs,
but have difficulty in producing polysemy-aware
verb classes.
From the result, we can see that the induced
verb classes based on slot-only features did not
achieve a higher F
1
than those based on slot-word
pair features in many cases. This result is differ-
ent from that of multi-class evaluations in Section
4.3. We speculate that slot distributions are not so
different among verbs when all uses of a verb are
merged into one frame, and thus their discrimina-
tion power is lower than that in the intermediate
construction of semantic frames.
4.5 Token-level Multi-class Evaluations
We conduct token-level multi-class evaluations us-
ing 119 verbs, which appear 100 or more times in
sections 02-21 of the SemLink WSJ corpus. These
119 verbs cover 102 VerbNet classes, and 48 of
them are polysemous in the sense of being in more
than one VerbNet class. Each instance of these 119
verbs in this corpus belongs to one of 102 Verb-
Net classes. We first add these instances to the
instances from a raw corpus and apply the two-
step clustering to these merged instances. Then,
we compare the induced verb classes of the Sem-
Link instances with their gold-standard VerbNet
classes. We report the values of modified purity
(mPU), inverse purity (iPU) and their harmonic
mean (F
1
). It is not necessary to normalize these
metrics because the clustering of these instances is
hard.
8
Korhonen et al (2003) reported that the highest modified
purity was 49% against predominant classes and 60% against
multiple classes.
method K mPU iPU F
1
Gigaword/S-NIL ? 93.43 20.06 33.03
Gigaword/SW-NIL ? 94.45 41.07 57.25
Gigaword/S-S 512.2 75.06 45.26 56.47
Gigaword/SW-S 260.6 73.98 56.45 64.04
web/S-NIL ? 93.70 32.96 48.76
web/SW-NIL ? 94.51 44.95 60.92
web/S-S 500.0 72.25 52.48 60.79
web/SW-S 255.2 72.65 61.00 66.31
Table 4: Token-level evaluations against VerbNet
classes. K represents the average number of in-
duced classes.
For clustering features, we compare two fea-
ture combinations: ?S-S? and ?SW-S,? which
achieved high performance in the type-level multi-
class evaluations (Section 4.3). The results of
these methods are obtained by averaging five runs.
For a baseline, we use verb-specific semantic
frames without clustering across verbs (?S-NIL?
and ?SW-NIL?), where these frames are consid-
ered to be verb classes but not shared across verbs.
Table 4 lists accuracies of these methods for the
two corpora. We can see that ?SW-S? achieved
a higher F
1
than ?S-S? and the baselines without
verb class induction (?S-NIL? and ?SW-NIL?).
Modi et al (2012) induced semantic frames
across verbs using the monosemous assumption
and reported an F
1
of 44.7% (77.9% PU and
31.4% iPU) for the assignment of FrameNet
frames to the FrameNet corpus. We also con-
ducted the above evaluation against FrameNet
frames for 75 verbs.
9
We achieved an F
1
of
62.79% (66.97% mPU and 59.09% iPU) for
?web/SW-S,? and an F
1
of 60.06% (65.58% mPU
and 55.39% iPU) for ?Gigaword/SW-S.? It is dif-
ficult to directly compare these results with Modi
et al (2012), but our induced verb classes seem to
have higher F
1
accuracy.
4.6 Full Experiments and Discussions
We finally induce verb classes from the semantic
frames of 1,667 verbs, which appear at least once
in sections 02-21 of the WSJ corpus. Based on
the best results in the above evaluations, we in-
duced semantic frames using slot-word pair fea-
tures, and then induced verb classes using slot-
only features. We ended with 38,481 semantic
frames and 699 verb classes from the Gigaword
9
Since FrameNet frames are not assigned to all verbs of
SemLink, the number of verbs is different from the evalua-
tions against VerbNet classes.
1037
class semantic frames
Class 1 rave:1, talk:1
Class 2 need:2, say:2
Class 3 smell:1, sound:1
Class 4 concentrate:1, focus:1
Class 5 express:2, inquire:62, voice:1
Class 6 revolve:1, snake:2, wrap:2
Class 7 hand:1, hand:3, hand:4
Class 8 depend:1, rely:1, rely:3
Class 9 collaborate:1, compete:2, work:1
Class 10 coach:3, teach:3, teach:4
Class 11 dance:1, react:1, stick:1
Class 12 advise:8, express:4, quiz:10, voice:2
Class 13 give:18, grant:6, offer:11, offer:12
Class 14 keep:14, keep:18, stay:4, stay:488
Class 15 cuff:5, fasten:2, tie:1, tie:4
Class 16 arrange:3, book:4, make:27, reserve:5
Class 17 deport:6, differ:1, fluctuate:1, vary:1
Class 18 peek:1, peek:3, peer:1, peer:7, ...
Class 19 groan:1, growl:1, hiss:1, moan:1, purr:1
Class 20 inform:1, notify:2, remind:1, beware:1, ...
Table 5: Examples of induced verb classes. Un-
derlined semantic frames are shown in Table 6.
corpus, and 61,903 semantic frames and 840 verb
classes from the web corpus. It took two days to
induce verb classes from the Gigaword corpus and
three days from the web corpus.
Examples of verb classes and semantic frames
induced from the web corpus are shown in Table
5 and Table 6. While there are many classes with
consistent meanings, such as ?Class 4? and ?Class
16,? some classes have mixed meanings. For in-
stance, ?Class 2? consists of the semantic frames
?need:2? and ?say:2.? These frames were merged
due to the high syntactic similarity of constituting
slot distributions, which are comprised of a sub-
ject and a sentential complement. To improve the
quality of verb classes, it is necessary to develop
a clustering model that can consider syntactic and
lexical similarity in a balanced way.
5 Conclusion
We presented a step-wise unsupervised method
for inducing verb classes from instances in giga-
word corpora. This method first clusters predicate-
argument structures to induce verb-specific se-
mantic frames and then clusters these semantic
frames across verbs to induce verb classes. Both
clustering steps are performed with exactly the
same method, which is based on the Chinese
Restaurant Process. The resulting semantic frames
and verb classes are open to the public and also can
be searched via our web interface.
10
10
http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
slot instance words
nsubj you:2150273, i:7678, we:4599, ...
need:2
ccomp ?s?:2193321
nsubj she:1705781, he:20693, i:9422, ...
say:2
ccomp ?s?:1829616
nsubj i:11100, he:10323, we:6373, ...
dobj me:30646, you:27678, us:21642, ...
inform:1
prep of decision:846, this:759, situation:688, ...
.
.
.
nsubj we:7505, you:3439, i:1035, ...
dobj you:18604, us:7281, them:3649, ...
notify:2
prep of change:1540, problem:496, status:386, ...
.
.
.
Table 6: Examples of induced semantic frames.
The number following an instance word denotes
its frequency and ?s? denotes a sentential comple-
ment.
From the results, we can see that the combi-
nation of the slot-word pair features for cluster-
ing verb-specific frames and the slot-only features
for clustering across verbs is the most effective
and outperforms the baselines by approximately
10 points. This indicates that slot distributions
are more effective than lexical information in slot-
word pairs for the induction of verb classes, when
Levin-style classes are used for evaluation. This
is consistent with Levin?s principle of organizing
verb classes according to the syntactic behavior of
verbs.
As applications of the resulting semantic frames
and verb classes, we plan to integrate them into
syntactic parsing, semantic role labeling and verb
sense disambiguation. For instance, Kawahara
and Kurohashi (2006) improved accuracy of de-
pendency parsing based on Japanese semantic
frames automatically induced from a raw corpus.
It is also valuable and promising to apply the in-
duced verb classes to NLP applications as used in
metaphor identification (Shutova et al, 2010) and
argumentative zoning (Guo et al, 2011).
Acknowledgments
This work was supported by Kyoto University
John Mung Program and JST CREST. We also
gratefully acknowledge the support of the National
Science Foundation Grant NSF-IIS-1116782, A
Bayesian Approach to Dynamic Lexical Re-
sources for Flexible Language Processing. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
1038
References
David Aldous. 1985. Exchangeability and related top-
ics.
?
Ecole d?
?
Et?e de Probabilit?es de Saint-Flour XIII
?1983, pages 1?198.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. the Journal of
Machine Learning Research, 3:993?1022.
Gemma Boleda, Sabine Schulte im Walde, and Toni
Badia. 2007. Modelling polysemy in adjective
classes by multi-label classification. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 171?180.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?
454.
Bonnie J. Dorr. 1997. Large-scale dictionary con-
struction for foreign language tutoring and inter-
lingual machine translation. Machine Translation,
12(4):271?322.
Ingrid Falk, Claire Gardent, and Jean-Charles Lamirel.
2012. Classifying French verbs using French and
English lexical resources. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics, pages 854?863.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 273?283.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic
verb classification. Natural Language Engineering,
14(3):337?367.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, pages 176?183.
Daisuke Kawahara, Daniel W. Peterson, Octavian
Popescu, and Martha Palmer. 2014. Inducing
example-based semantic frames from a massive
amount of verb uses. In Proceedings of the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Karin Kipper-Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Anna Korhonen, Yuval Krymolowski, and Zvika
Marx. 2003. Clustering polysemic subcategoriza-
tion frame distributions semantically. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 64?71.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 345?352.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?73.
Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. The University
of Chicago Press.
Jianguo Li and Chris Brew. 2007. Disambiguating
Levin verbs using untagged data. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing.
Jianguo Li and Chris Brew. 2008. Which are the best
features for automatic verb classification. In Pro-
ceedings of ACL-08: HLT, pages 434?442.
Thomas Lippincott, Anna Korhonen, and Diarmuid
?
O S?eaghdha. 2012. Learning syntactic verb frames
using graphical models. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 420?429.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics.
Ji?r?? Materna. 2012. LDA-frames: An unsupervised ap-
proach to generating semantic frames. In Proceed-
ings of the 13th International Conference CICLing
2012, Part I, pages 376?387.
Ji?r?? Materna. 2013. Parameter estimation for LDA-
frames. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 482?486.
Yusuke Miyao and Jun?ichi Tsujii. 2009. Supervised
learning of a probabilistic lexicon of verb semantic
classes. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1328?1337.
Ashutosh Modi, Ivan Titov, and Alexandre Klementiev.
2012. Unsupervised induction of frame-semantic
representations. In Proceedings of the NAACL-HLT
Workshop on the Induction of Linguistic Structure,
pages 1?7.
1039
Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal
of computational and graphical statistics, 9(2):249?
265.
Christopher Parisien and Suzanne Stevenson. 2010.
Learning verb alternations in a usage-based
Bayesian model. In Proceedings of the 32nd Annual
Meeting of the Cognitive Science Society.
Christopher Parisien and Suzanne Stevenson. 2011.
Generalizing between form and meaning using
learned verb classes. In Proceedings of the 33rd An-
nual Meeting of the Cognitive Science Society.
Roi Reichart and Anna Korhonen. 2013. Improved
lexical acquisition through DPP-based verb cluster-
ing. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 862?872.
Roi Reichart, Omri Abend, and Ari Rappoport. 2010.
Type level clustering evaluation: New measures and
a POS induction case study. In Proceedings of the
14th Conference on Computational Natural Lan-
guage Learning, pages 77?87.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2009. The effect of corpus size on case frame
acquisition for discourse analysis. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
521?529.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining
EM training and the MDL principle for an automatic
verb classification incorporating selectional prefer-
ences. In Proceedings of ACL-08: HLT, pages 496?
504.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159?194.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Computational
Linguistics and Intelligent Text Processing, pages
100?111. Springer.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
1002?1010.
Sylvia Springorum, Sabine Schulte im Walde, and Ja-
son Utt. 2013. Detecting polysemy in hard and
soft cluster analyses of German preposition vector
spaces. In Proceedings of the 6th International Joint
Conference on Natural Language Processing, pages
632?640.
Suzanne Stevenson and Eric Joanis. 2003. Semi-
supervised verb class discovery using noisy features.
In Proceedings of the 7th Conference on Natural
Language Learning, pages 71?78.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 566?574.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 638?647.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Automatic classification of English verbs us-
ing rich syntactic features. In Proceedings of the
3rd International Joint Conference on Natural Lan-
guage Processing, pages 769?774.
Lin Sun, Diana McCarthy, and Anna Korhonen. 2013.
Diathesis alternation approximation for verb clus-
tering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, Short Papers, pages 736?741.
Robert Swier and Suzanne Stevenson. 2005. Exploit-
ing a verb lexicon in automatic semantic role la-
belling. In Proceedings of Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
883?890.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101(476).
Naftali Tishby, Fernando C. Pereira, and William
Bialek. 1999. The information bottleneck method.
In Proceedings of the 37th Annual Allerton Confer-
ence on Communication, Control and Computing,
pages 368?377.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12?22.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
Dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geometri-
cal Models of Natural Language Semantics, pages
74?82.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the Workshop on Human Language
Technology, pages 266?271.
1040
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 397?402,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
The VerbCorner Project: Findings from Phase 1 of Crowd-Sourcing a
Semantic Decomposition of Verbs
Joshua K. Hartshorne
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
77 Massachusetts Avenue
Cambridge, MA 02139, USA
jkhartshorne@gmail.com
Claire Bonial, Martha Palmer
Department of Linguistics
University of Colorado at Boulder
Hellems 290, 295 UCB
Boulder, CO 80309, USA
{CBonial, MPalmer}@colorado.edu
Abstract
Any given verb can appear in some syntac-
tic frames (Sally broke the vase, The vase
broke) but not others (*Sally broke at the
vase, *Sally broke the vase to John). There
is now considerable evidence that the syn-
tactic behaviors of some verbs can be pre-
dicted by their meanings, and many cur-
rent theories posit that this is true for most
if not all verbs. If true, this fact would
have striking implications for theories and
models of language acquisition, as well as
numerous applications in natural language
processing. However, empirical investiga-
tions to date have focused on a small num-
ber of verbs. We report on early results
from VerbCorner, a crowd-sourced project
extending this work to a large, representa-
tive sample of English verbs.
1 Introduction
Verbs vary in terms of which syntactic frames they
can appear in (Table 1). In principle, this could be
an unpredictable fact about the verb that must be
acquired, much like the phonological form of the
verb.
However, most theorists posit that there is a sys-
tematic relationship between the semantics of a
verb and the syntactic frames in which it can ap-
pear (Levin and Hovav, 2005). For instance, it
is argued that verbs like break, which describe a
Frame hit like break
NP V NP x x x
NP V - - x
NP that S - x -
NP V at NP x - -
Table 1: Some of the syntactic frames available for
hit, like, and break.
caused change of state, can appear in both the NP
V NP form (Sally broke the vase) and the NP
V form (The vase broke). Verbs such as hit and
like do not describe a change of state and so can-
not appear in both forms.
1
Similarly, only verbs
that describe propositional attitudes, such as like,
can take a that complement (John liked that Sally
broke the vase).
1.1 The Semantic Consistency Hypothesis
This account has a natural consequence, which we
dub the Semantic Consistency Hypothesis: There
is some set of semantic features such that verbs
that share the same syntactic behavior are identi-
cal along those semantic features.
2
Note that on
certain accounts, this is a strong tendency rather
than a strict necessity (e.g., Goldberg, 1995).
It is widely recognized that a principled re-
lationship between syntax and semantics would
have broad implications. It is frequently invoked
in theories of language acquisition. For instance,
Pinker (1984, 1989) has described how this cor-
respondence could solve long-standing puzzles
about how children learn syntax in the first place.
Conversely, Gleitman (1990) has shown such a
syntax-semantics relationship could solve signif-
icant problems in vocabulary acquisition. In fact,
both researchers argue that a principled relation-
ship between syntax and semantics is necessary
for language to be learnable at all.
In computational linguistics and natural lan-
guage processing, some form of the Semantic
Consistency Hypothesis is often included in lin-
guistic resources and utilized in applications. We
1
Note that this is a simplification in that there are non-
causal verbs that appear in both the NP V NP frame and the
NP V frame. For details, see (Levin, 1993).
2
There is a long tradition of partitioning semantics into
those aspects of meaning which are ?grammatically relevant?
and those which are not. We refer the interested reader to
Pinker (1989), Jackendoff (1990), and Levin & Rappaport
Hovav (2005).
397
describe in detail one such resource, VerbNet,
which is highly relevant to our investigation.
1.2 VerbNet
VerbNet (Kipper et al, 2008; based on Levin,
1993) lists over 6,000 verbs, categorized into 280
classes according to the syntactic frames they can
appear in. That is, all verbs in the same class ap-
pear in the same set of syntactic frames. Impor-
tantly, in addition to characterizing the syntactic
frames associated with each class, VerbNet alo
characterizes the semantics of each class.
For instance, class 9.7, which comprises a
couple dozen verbs, allows 7 different syntactic
frames. The entry for one frame is shown below:
Syntactic Frame NP V NP PP.DESTINATION
Example Jessica sprayed the wall.
Syntax AGENT V THEME {+LOC|+DEST CONF}
DESTINATION
Semantics MOTION(DURING(E), THEME)
NOT(PREP(START(E), THEME, DESTINATION))
PREP(END(E), THEME, DESTINATION)
CAUSE(AGENT, E)
Importantly, the semantics listed here is not just
for the verb spray but applies to all verbs from the
Spray Class whenever they appear in that syntac-
tic frame ? that is, VerbNet assumes the Semantic
Consistency Hypothesis.
VerbNet and its semantic features have been
used in a variety of NLP applications, such as se-
mantic role labeling (Swier and Stevenson, 2004),
inferencing (Zaenen et al, 2008), verb classifica-
tion (Joanis et al, 2008), and information extrac-
tion (Maynard et al, 2009). It has also been em-
ployed in models of language acquisition (Parisien
and Stevenson, 2011; Barak et al, 2012). In gen-
eral, there has been interest in the NLP literature
in using these syntactially-relevant semantic fea-
tures for shallow semantic parsing (e.g., Giuglea
and Moschitti, 2006).
2 Empirical Status of the Semantic
Consistency Hypothesis
Given the prominence of the Semantic Consis-
tency Hypothesis in both theory and practice, one
might expect that it was on firm empirical foot-
ing. That is, ideally there would be some database
of semantic judgments for a comprehensive set
of verbs from each syntactic class. In princi-
ple, these judgments would come from naive an-
notators, since researchers? intuitions about sub-
tle judgments may be unconsciously clouded by
theoretical commitments (Gibson and Fedorenko,
2013). The Semantic Consistency Hypothesis
would be supported if, within that database, predi-
cates with the same syntactic properties were sys-
tematically related semantically.
No such database exists, whether consisting of
the judgments of linguists or naive annotators.
Most theoretical studies report researcher judg-
ments for only a handful of examples; how many
additional examples were considered by the re-
searcher goes unreported. In any case, to our
knowledge, of the 280 syntactic verb classes listed
by VerbNet, only a handful have been studied in
any detail.
The strongest evidence comes from experimen-
tal work on several so-called alternations (the pas-
sive, causative, locative, and dative alternations).
Here, there does appear to be a systematic seman-
tic distinction between the two syntactic frames in
each alternation, at least most of the time. This
has been tested with a reasonable sample of the
relevant verbs and also in both children and adults
(Ambridge et al, 2013; Pinker, 1989). However,
the relevant verbs make up a tiny fraction of all
English verbs, and even for these verbs, the syn-
tactic frames in question represent only a fraction
of the syntactic frames available to those verbs.
This is not an accidental oversight. The limit-
ing factor is scale: with many thousands of verbs
and over a hundred commonly-discussed seman-
tic features and syntactic frames, it is not feasi-
ble for a single researcher, or even team of re-
searchers, to check which verbs appear in which
syntactic frames and carry which semantic en-
tailments. Collecting data from naive subjects is
even more laborious, particularly since the aver-
age Man on the Street is not necessarily equipped
with metalinguistic concepts like caused change of
state and propositional attitude. The VerbCorner
Project is aimed at filling that empirical gap.
3 VerbCorner
The VerbCorner Project
3
is devoted to collecting
semantic judgments for a comprehensive set of
verbs along a comprehensive set of theoretically-
relevant semantic dimension. These data can be
used to test the Semantic Consistency Hypothesis.
3
http://gameswithwords.org/VerbCorner/
398
Independent of the validity of that hypothesis, the
semantic judgments themselves should prove use-
ful for any study of linguistic meaning or related
application.
We address the issue of scale through crowd-
sourcing: Recruiting large numbers of volunteers,
each of whom may provide only a few annota-
tions. Several previous projects have success-
fully crowd-sourced linguistic annotations, such
as Phrase Detectives, where volunteers have con-
tributed 2.5 million judgments on anaphoric rela-
tions (Poesio et al, 2012).
3.1 Integration with VerbNet
One significant challenge for any such project is
first classifying verbs according to the syntactic
frames they can appear in. Thus, at least initially,
we are focusing on the 6,000+ verbs already cata-
loged in VerbNet. As such, the VerbCorner Project
is also verifying and validating the semantics cur-
rently encoded in VerbNet. VerbNet will be edited
as necessary based on the empirical results.
Integration with VerbNet has additional bene-
fits, since VerbNet itself is integrated with a vari-
ety of linguistic resources, such as PropBank and
Penn TreeBank. This amplifies the impact of any
VerbCorner-inspired changes to VerbNet.
3.2 The Tasks
We selected semantic features of interest based on
those most commonly cited in the linguistics lit-
erature, with a particular focus on those that ? ac-
cording to VerbNet ? apply to many predicates.
Previous research has shown that humans find
it easier to reason about real-world scenarios than
make abstract judgments (Cosmides and Tooby,
1992). Thus, for each feature (e.g., MOVEMENT),
we converted the metalinguistic judgment (?Does
this verb entail movement on the part of some en-
tity??) into a real-world problem.
For example, in ?Simon Says Freeze,? a task
designed to elicit judgments about movement, the
Galactic Overlord (Simon) decrees ?Galactic Stay
Where You Are Day,? during which nobody is al-
lowed to move from their current location. Par-
ticipants read descriptions of events and decide
whether anyone violated the rule.
In ?Explode on Contact,? designed to elicit
judgments about physical contact, objects and
people explode when they touch one another. The
participant reads descriptions of events and de-
cides whether anything has exploded.
Note that each task is designed to elicit judg-
ments about entailments ? things that must be true
rather than are merely likely to be true. If John
greeted Bill, they might have come into contact
(e.g., by shaking hands), but perhaps they did not.
Previous work suggests that it is the semantic en-
tailments that matter, particularly for explaining
the syntactic behavior of verbs (Levin, 1993).
3.3 The Items
The exact semantics associated with a verb may
depend on its syntactic frame. Thus Sally rolled
the ball entails that somebody applied force to the
ball (namely: Sally), whereas The ball rolled does
not. Thus, we investigate the semantics of each
verb in each syntactic frame available to it (as de-
scribed by VerbNet). Below, the term item is the
unit of annotation: a verb in a frame.
In order to minimize unwanted effects of world
knowledge, the verb?s arguments are replaced with
nonsense words or randomly chosen proper names
(Sally sprayed the dax onto the blicket). The use
of novel words is explained by the story for each
task.
3.4 The Phases
Given the sheer scale of the project, data-
collection is expected to take several years at least.
Thus, data-collection has been broken up into a se-
ries of phases. Each phase focuses on a small num-
ber of classes and/or semantic entailments. This
ensures that there are meaningful intermediate re-
sults that can be disseminated prior to the comple-
tion of the entire project. This manuscript reports
the results of Phase 1.
4 Results
The full data and annotations will be released in
the near future and may be available now by re-
quest. Below, we summarize the main findings
thus far.
4.1 Description of Phase 1
In Phase 1 of the project, we focused on 11 verb
classes (Table 3) comprising 641 verbs and seven
different semantic entailments (Table 2). While
six of these entailments were chosen from among
those features widely believed to be relevant for
syntax, one was not: A Good World, which inves-
tigated evaluation (Is the event described by the
verb positive or negative?). Although evaluation
399
Task Semantic Feature Anns. Anns./Item Mode Consistency
Entropy PHYSICAL CHANGE 23,875 7 86% 95%
Equilibrium APPLICATION OF FORCE 27,128 8 79% 95%
Explode on Contact PHYSICAL CONTACT 23,590 7 93% 95%
Fickle Folk CHANGE OF MENTAL STATE 16,466 5 81% 96%
Philosophical Zombie Hunter MENTAL STATE 24,592 7 80% 89%
Simon Says Freeze LOCATION CHANGE 24,245 7 83% 88%
A Good World EVALUATION 22,668 7 72% 74%
Table 2: Respectively: Task, semantic feature tested, number of annotations, mean number of annotations
per item, mean percentage of participants choosing the modal response, consistency within class.
of events is an important component of human
psychology, to our knowledge no researcher has
suggested that it is relevant for syntax. As such,
this task provides a lower bound for how much se-
mantic consistency one might expect within a syn-
tactic verb class.
In all, we collected 162,564 judgments from
1,983 volunteers (Table 2).
4.2 Inter-annotator Agreement
Each task had been iteratively piloted and re-
designed until inter-annotator reliability was ac-
ceptable, as described in a previous publication.
However, these pilot studies involved a small num-
ber of items which were coded by all annota-
tors. How good was the reliability in the crowd-
sourcing context?
Because we recruited large numbers of an-
notators, most of whom annotated only a few
items, typical measures of inter-annotator agree-
ment such as Cohen?s kappa are not easily calcu-
lated. Instead, for each item, we calculated the
most common (modal) response. We then con-
sidered what proportion of all annotations were
accounted for by the modal response: a mean of
100% would indicate that there was no disagree-
ment among annotators for any item.
As can be seen in Table 2, for every task, the
modal response covered the bulk responses, rang-
ing from a low of 72% for EVALUATION to a high
of 93% for PHYSICAL CONTACT. Since there
were typically 4 or more possible answers per
item, inter-annotator agreement was well above
chance. This represents good performance given
that the annotators were entirely untrained.
In many cases, annotator disagreement seems
to be driven by syntactic constructions that are
only marginally grammatical. For instance, inter-
annotator agreement was typically low for class
63. VerbNet suggests two syntactic frames for
class 63, one of which (NP V THAT S) appears to
be marginal (?I control that Mary eats). In fact,
annotators frequently flagged these items as un-
grammatical, which is a valuable result in itself for
improving VerbNet.
Class Examples PChange Force Contact MChange Mental LChange
12 yank, press - x d - - d
18.1 hit, squash d x d - - d
29.5 believe, conjecture - - - - d -
31.1 amuse, frighten - - - x d -
31.2 like, fear - - - - x -
45.1 break, crack x d d - - d
51.3.1 bounce, roll - d d - - d
51.3.2 run, slink - d - - - d
51.6 chase, follow - - - - - d
61 attempt, try - - - - - -
63 control, enforce - - - - - -
Table 3: VerbNet classes investigated in Phase 1, with presence of semantic entailments as indicated by
data. x = feature present; - = feature absent; d = depends on syntactic frame.
400
4.3 Testing the Semantic Consistency
Hypothesis
4.3.1 Calculating consistency
We next investigated whether our results support
the Semantic Consistency Hypothesis. As noted
above, the question is not whether all verbs in the
same syntactic class share the same semantic en-
tailments. Even a single verb may have different
semantic entailments when placed in different syn-
tactic frames. Thus, calculating consistency of a
class must take differing frames into account.
There are many sophisticated rubrics for calcu-
lating consistency. However, for expository pur-
poses here, we use one that is intuitive and easy
to interpret. First, we determined the annotation
for each item (i.e., each verb/frame combination)
by majority vote. We then considered how many
verbs in each class had the same annotation in any
given syntactic frame.
For example, suppose a class had 10 verbs and
2 frames. In the first frame, 8 verbs received the
same annotation and 2 received others. The con-
sistency for this class/frame combination is 80%.
In the second frame, 6 verbs received the same
annotation and 4 verbs received others. The con-
sistency for this class/frame combination is 60%.
The consistency for the class as a whole is the av-
erage across frames: 70%.
4.3.2 Results
Mean consistency averaged across classes is
shown for each task in Table 2. As expected,
consistency was lowest for EVALUATION, which
is not expected to necessarily correlate with syn-
tax. Interestingly, consistency for EVALUATION
was nonetheless well above floor. This is per-
haps not surprising: two sentences that have the
same values for PHYSICAL CHANGE, APPLICA-
TION OF FORCE, PHYSICAL CONTACT, CHANGE
OF MENTAL STATE, MENTAL STATE, and LO-
CATION CHANGE are, on average, also likely to
be both good or both bad.
Consistency was much higher for the other
tasks, and in fact was close to ceiling for most of
them. It remains to be seen whether the items that
deviate from the mode represent true differences in
semantics or reflect merely noise. One way of ad-
dressing this question is to collect additional anno-
tations for those items that deviate from the mode.
4.4 Verb semantics
For each syntactic frame in each class, we deter-
mined the most common annotation. This is sum-
marized in Table 3. The semantic annotation de-
pended on syntactic frame nearly 1/4 of the time.
4
These frequently matched VerbNet?s seman-
tics, though not always. For instance, annota-
tors judged that class 18.1 verbs in the NP V NP
PP.INSTRUMENT entailed movement on the part
of the instrument (Sally hit the ball with the stick)
? something not reflected in VerbNet.
5 Conclusion and Future Work
Results of Phase 1 provide support for the Seman-
tic Consistency Hypothesis, at least as a strong
bias. More work will be needed to determine the
strength of that bias. The findings are largely con-
sistent with VerbNet?s semantics, but changes are
indicated in some cases.
We find that inter-annotator agreement is suf-
ficiently high that annotation can be done effec-
tively using the modal response with an average
of 6-7 responses per item. We are currently in-
vestigating whether we can achieve better reliabil-
ity with fewer responses per item by taking into
account an individual annotator?s history across
items, as recent work suggests is possible (Passon-
neau and Carpenter, 2013; Rzhetsky et al, 2009;
Whitehill et al, 2009).
Thus, crowd-sourcing VerbNet semantic entail-
ments appears to be both feasible and productive.
Data-collection continues. Phase 2, which added
over 10 new verb classes, is complete. Phase 3,
which includes both new classes and new entail-
ments, has been launched.
Acknowledgments
We gratefully acknowledge the support of the
National Science Foundation Grant NSF-IIS-
1116782, DARPA Machine Reading FA8750-09-
C-0179, and funding from the Ruth L. Kirschstein
National Research Service Award. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
4
Note that this table was calculated based on whether the
semantic feature was present or not. In many cases, the data
was significantly richer. For instance, for APPLICATION OF
FORCE, annotators determined which participant in the event
was applying the force.
401
References
Ben Ambridge, Julian Pine, Caroline Rowland,
Franklin Chang, and Amy Bidgood. 2013. The re-
treat from overgeneralization in child language ac-
quisition: word learning, morphology and verb ar-
gument structure. Wiley Interdisciplinary Reviews:
Cognitive Science, 4(1):47?62.
Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2012. Modeling the acquisition of mental
state verbs. In Proceedings of the 3rd Workshop on
Cognitive Modeling and Computational Linguistics,
pages 1?10. Association for Computational Linguis-
tics.
Leda Cosmides and John Tooby. 1992. Cognitive
adaptations for social exchange. The Adapted Mind,
pages 163?228.
Edward Gibson and Evelina Fedorenko. 2013. The
need for quantitative methods in syntax and seman-
tics research. Language and Cognitive Processes,
28(1-2):88?124.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Shallow semantic parsing based on framenet, verb-
net and propbank. In Proceedings of the 217th
European Conference on Artificial Intelligence,
pages 563?567, Amsterdam, The Netherlands, The
Netherlands. IOS Press.
Lila Gleitman. 1990. The structural sources of verb
meanings. Language Acquisition, 1(1):3?55.
Adele E. Goldberg. 1995. Constructions: A Construc-
tion Grammar approach to argument structure. Uni-
versity of Chicago Press.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic
verb classification. Natural Language Engineering,
14(3):337?367.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Cambridge University Press.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A preliminary Investigation. University of
Chicago press.
Diana Maynard, Adam Funk, and Wim Peters. 2009.
Using lexico-syntactic ontology design patterns for
ontology creation and population. In Proc. of the
Workshop on Ontology Patterns.
Christopher Parisien and Suzanne Stevenson. 2011.
Generalizing between form and meaning using
learned verb classes. In Proceedings of the 33rd An-
nual Meeting of the Cognitive Science Society. Cite-
seer.
Rebecca J Passonneau and Bob Carpenter. 2013. The
benefits of a model of annotation. In Proceedings of
the 7th Linguistic Annotation Workshop and Inter-
operability with Discourse, pages 187?195.
Steven Pinker. 1984. Language Learnability and Lan-
guage Development. Harvard University Press.
Steven Pinker. 1989. Learnability and Cognition: The
Acquisition of Argument Structure. MIT Press.
Massimo Poesio, Jon Chamberlain, Udo Kruschwitz,
Livio Robaldo, and Luca Ducceschi. 2012. The
phrase detective multilingual corpus, release 0.1. In
Collaborative Resource Development and Delivery
Workshop Programme, page 34.
Andrey Rzhetsky, Hagit Shatkay, and W John Wilbur.
2009. How to get the most out of your curation ef-
fort. PLoS Computational Biology, 5(5):1?13.
Robert S Swier and Suzanne Stevenson. 2004. Un-
supervised semantic role labeling. In Proceedings
of the Generative Lexicon Conference, volume 95,
page 102.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier R Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In Advances in
Neural Information Processing Systems, volume 22,
pages 2035?2043.
Annie Zaenen, Daniel G Bobrow, and Cleo Condo-
ravdi. 2008. The encoding of lexical implications in
verbnet: Predicates of change of locations. In Lan-
guage Resources Evaluation Conference.
402
Temporal Annotation in the Clinical Domain
William F. Styler IV1, Steven Bethard2, Sean Finan3, Martha Palmer1,
Sameer Pradhan3, Piet C de Groen4, Brad Erickson4, Timothy Miller3,
Chen Lin3, Guergana Savova3 and James Pustejovsky5
1 Department of Linguistics, University of Colorado at Boulder
2 Department of Computer and Information Sciences, University of Alabama at Birmingham
3 Children?s Hospital Boston Informatics Program and Harvard Medical School
4 Mayo Clinic College of Medicine, Mayo Clinic, Rochester, MN
5 Department of Computer Science, Brandeis University
Abstract
This article discusses the requirements of
a formal specification for the annotation of
temporal information in clinical narratives.
We discuss the implementation and extension
of ISO-TimeML for annotating a corpus of
clinical notes, known as the THYME cor-
pus. To reflect the information task and the
heavily inference-based reasoning demands
in the domain, a new annotation guideline
has been developed, ?the THYME Guidelines
to ISO-TimeML (THYME-TimeML)?. To
clarify what relations merit annotation, we
distinguish between linguistically-derived and
inferentially-derived temporal orderings in the
text. We also apply a top performing Temp-
Eval 2013 system against this new resource to
measure the difficulty of adapting systems to
the clinical domain. The corpus is available to
the community and has been proposed for use
in a SemEval 2015 task.
1 Introduction
There is a long-standing interest in temporal reason-
ing within the biomedical community (Savova et al.,
2009; Hripcsak et al., 2009; Meystre et al., 2008;
Bramsen et al., 2006; Combi et al., 1997; Keravnou,
1997; Dolin, 1995; Irvine et al., 2008; Sullivan et
al., 2008). This interest extends to the automatic ex-
traction and interpretation of temporal information
from medical texts, such as electronic discharge sum-
maries and patient case summaries. Making effective
use of temporal information from such narratives is
a crucial step in the intelligent analysis of informat-
ics for medical researchers, while an awareness of
temporal information (both implicit and explicit) in a
text is also necessary for many data mining tasks.
It has also been demonstrated that the temporal in-
formation in clinical narratives can be usefully mined
to provide information for some higher-level tempo-
ral reasoning (Zhao et al., 2005). Robust temporal
understanding of such narratives, however, has been
difficult to achieve, due to the complexity of deter-
mining temporal relations among events, the diver-
sity of temporal expressions, and the interaction with
broader computational linguistic issues.
Recent work on Electronic Health Records (EHRs)
points to new ways to exploit and mine the informa-
tion contained therein (Savova et al., 2009; Roberts
et al., 2009; Zheng et al., 2011; Turchin et al., 2009).
We target two main use cases for extracted data. First,
we hope to enable interactive displays and summaries
of the patient?s records to the physician at the time of
visit, making a comprehensive review of the patient?s
history both faster and less prone to oversights. Sec-
ond, we hope to enable temporally-aware secondary
research across large databases of medical records
(e.g., ?What percentage of patients who undergo pro-
cedure X develop side-effect Y within Z months??).
Both of these applications require the extraction of
time and date associations for critical events and the
relative ordering of events during the patient?s period
of care, all from the various records which make up a
patient?s EHR. Although we have these two specific
applications in mind, the schema we have developed
is generalizable and could potentially be embedded
in a wide variety of biomedical use cases.
Narrative texts in EHRs are temporally rich doc-
uments that frequently contain assertions about the
timing of medical events, such as visits, laboratory
values, symptoms, signs, diagnoses, and procedures
(Bramsen et al., 2006; Hripcsak et al., 2009; Zhou
et al., 2008). Temporal representation and reason-
ing in the medical record are difficult due to: (1) the
diversity of time expressions; (2) the complexity of
determining temporal relations among events (which
are often left to inference); (3) the difficulty of han-
dling the temporal granularity of an event; and (4)
143
Transactions of the Association for Computational Linguistics, 2 (2014) 143?154. Action Editor: Ellen Riloff.
Submitted 9/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
general issues in natural language processing (e.g.,
ambiguity, anaphora, ellipsis, conjunction). As a re-
sult, the signals used for reconstructing a timeline can
be both domain-specific and complex, and are often
left implicit, requiring significant domain knowledge
to accurately detect and interpret.
In this paper, we discuss the demands on accurately
annotating such temporal information in clinical
notes. We describe an implementation and extension
of ISO-TimeML (Pustejovsky et al., 2010), devel-
oped specifically for the clinical domain, which we
refer to as the ?THYME Guidelines to ISO-TimeML?
(?THYME-TimeML?), where THYME stands for
?Temporal Histories of Your Medical Events?. A sim-
plified version of these guidelines formed the basis
for the 2012 i2b2 medical-domain temporal relation
challenge (Sun et al., 2013a).
This is being developed in the context of the
THYME project, whose goal is to both create ro-
bust gold standards for semantic information in clini-
cal notes, as well as to develop state-of-the-art algo-
rithms to train and test on this dataset.
Deriving timelines from news text requires the con-
crete realization of context-dependent assumptions
about temporal intervals, orderings and organization,
underlying the explicit signals marked in the text
(Pustejovsky and Stubbs, 2011). Deriving patient
history timelines from clinical notes also involves
these types of assumptions, but there are special de-
mands imposed by the characteristics of the clinical
narrative. Due to both medical shorthand practices
and general domain knowledge, many event-event
relations are not signaled in the text at all, and rely
on a shared understanding and common conceptual
models of the progressions of medical procedures
available only to readers familiar with language use
in the medical community.
Identifying these implicit relations and temporal
properties puts a heavy burden on the annotation
process. As such, in the THYME-TimeML guideline,
considerable effort has gone into both describing and
proscribing the annotation of temporal orderings that
are inferable only through domain-specific temporal
knowledge.
Although the THYME guidelines describe a num-
ber of departures from the ISO-TimeML standard for
expediency and ease of annotation, this paper will
focus on those differences specifically motivated by
the needs of the clinical domain, and on the conse-
quences for systems built to extract temporal data in
both the clinical and general domain.
2 The Nature of Clinical Documents
In the THYME corpus, we have been examining
1,254 de-identified1 notes from a large healthcare
practice (the Mayo Clinic), representing two distinct
fields within oncology: brain cancer, and colon can-
cer. To date, we have principally examined two dif-
ferent general types of clinical narrative in our EHRs:
clinical notes and pathology reports.
Clinical notes are records of physician interactions
with a patient, and often include multiple, clearly
delineated sections detailing different aspects of the
patient?s care and present illness. These notes are
fairly generic across institutions and specialities, and
although some terms and inferences may be specific
to a particular type of practice (such as oncology),
they share a uniform structure and pattern. The ?His-
tory of Present Illness?, for example, summarizes the
course of the patient?s chief complaint, as well as the
interventions and diagnostics which have been thus
far attempted. In other sections, the doctor may out-
line her current plan for the patient?s treatment, then
later describe the patient?s specific medical history,
allergies, care directives, and so forth.
Most critically for temporal reasoning, each clin-
ical note reflects a single time in the patient?s treat-
ment history at which all of the doctor?s statements
are accurate (the DOCTIME), and each section tends
to describe events of a particular timeframe. For
example, ?History of Present illness? predominantly
describes events occuring before DOCTIME, whereas
?Medications? provides a snapshot at DOCTIME and
?Ongoing Care Orders? discusses events which have
not yet occurred.2
Clinical notes contain rich temporal information
and background, moving fluidly from prior treat-
ments and symptoms to present conditions to future
interventions. They are also often rich with hypo-
thetical statements (?if the tumor recurs, we can...?),
each of which can form its own separate timeline.
By constrast, pathology notes are quite different.
Such notes are generated by a medical pathologist
1Although most patient information was removed, dates
and temporal information were not modified according to this
project?s specific data use agreement.
2One complication is the propensity of doctors and automated
systems to later update sections in a note without changing the
timestamp or metadata. We have added a SECTIONTIME to keep
these updated sections from affecting our overall timeline.
144
upon receipt and analysis of specimens (ranging from
tissue samples from biopsy to excised portions of
tumor or organs). Pathology notes provide crucial
information to the patient?s doctor confirming the
malignancy (cancer) in samples, describing surgi-
cal margins (which indicate whether a tumor was
completely excised), and classifying and ?staging? a
tumor, describing the severity and spread of the can-
cer. Because the information in such notes pertains
to samples taken at a single moment in time, they are
temporally sparse, seldom referring to events before
or after the examination of the specimen. However,
they contain critical information about the state of
the patient?s illness and about the cancer itself, and
must be interpreted to understand the history of the
patient?s illness.
Most importantly, in all EHRs, we must contend
with the results of a fundamental tension in mod-
ern medical records: hyper-detailed records provide
a crucial defense against malpractice litigation, but
including such detail takes enormous time, which
doctors seldom have. Given that these notes are writ-
ten by and for medical professionals (who form a
relatively insular speech community), a great many
non-standard expressions, abbreviations, and assump-
tions of shared knowledge are used, which are simul-
taneously concise and detail-rich for others who have
similar backgrounds.
These time-saving devices can range from tempo-
rally loaded acronyms (e.g., ?qid?, Latin for quater in
die, ?four times daily?), to assumed orderings (a diag-
nostic test for a disorder is assumed to come before
the procedure which treats it), and even to completely
implicit events and temporal details. For example,
consider the sentence in (1).
(1) Colonoscopy 3/12/10, nodule biopsies negative
We must understand that during the colonoscopy,
the doctor obtained biopsies of nodules, which were
packaged and sent to a pathologist, who reviewed
them and determined them to be ?negative? (non-
cancerous).
In such documents, we must recover as much tem-
poral detail as possible, even though it may be ex-
pressed in a way which is not easily understood out-
side of the medical community, let alone by linguists
or automated systems. We must also be aware of the
legal relevance of some events (e.g., ?We discussed
the possible side effects?), even when they may not
seem relevant to the patient?s actual care.
Finally, each specialty and note type has separate
conventions. Within colon cancer notes, the Amer-
ican Joint Committee on Cancer (AJCC) Staging
Codes (e.g., T4N1, indicating the nature of the tumor,
lymph node and metastasis involvement) are metic-
ulously recorded, but are largely absent in the brain
cancer notes which make up the second corpus in
our project. So, although clinical notes share many
similarities, annotators without sufficient domain ex-
pertise may require additional training to adapt to the
inferences and nuances of a new clinical subdomain.
3 Interpreting ?Event? and Temporal
Expressions in the Clinical Domain
Much prior work has been done on standardizing
the annotation of events and temporal expressions
in text. The most widely used approach is the ISO-
TimeML specification (Pustejovsky et al., 2010), an
ISO standard that provides a common framework for
annotating and analyzing time, events, and event rela-
tions. As defined by ISO-TimeML, an EVENT refers
to anything that can be said ?to obtain or hold true, to
happen or to occur?. This is a broad notion of event,
consistent with Bach?s use of the term ?eventuality?
(Bach, 1986) as well as the notion of fluents in AI
(McCarthy, 2002).
Because the goals of the THYME project involve
automatically identifying the clinical timeline for
a patient from clincal records, the scope of what
should be admitted into the domain of events is inter-
preted more broadly than in ISO-TimeML3. Within
the THYME-TimeML guideline, an EVENT is any-
thing relevant to the clinical timeline, i.e., anything
that would show up on a detailed timeline of the pa-
tient?s care or life. The best single-word syntactic
head for the EVENT is then used as its span. For
example, a diagnosis would certainly appear on such
a timeline, as would a tumor, illness, or procedure.
On the other hand, entities that persist throughout
the relevant temporal period of the clinical timeline
(endurants in ontological circles) would not be con-
sidered as event-like. This includes the patient, other
humans mentioned (the patient?s mother-in-law or
the doctor), organizations (the emergency room),
non-anatomical objects (the patient?s car), or indi-
vidual parts of the patient?s anatomy (an arm is not
an EVENT unless missing or otherwise notable).
To meet our explicit goals, the THYME-TimeML
guideline introduces two additional levels of interpre-
3Our use of the term ?EVENT? corresponds with the less
specific ISO-TimeML term ?Eventuality?
145
tation beyond that specified by ISO-TimeML: (i) a
well-defined task; and (ii) a clearly identified domain.
By focusing on the creation of a clinical timeline
from clinical narrative, the guideline imposes con-
straints that cannot be assumed for a broadly defined
and domain independent annotation schema.
Some EVENTs annotated under our guideline are
considered meaningful and eventive mostly by virtue
of a specific clinical or legal value. For example,
AJCC Staging Codes (discussed in Section 2) are
eventive only in the sense of the code being assigned
to a tumor at a given moment in the patient?s care.
However, they are of such critical importance and
informative value to doctors that we have chosen to
annotate them specifically so that they will show up
on the patient?s timeline in a clinical setting.
Similarly, because of legal pressures to establish in-
formed consent and patient knowledge of risk, entire
paragraphs of clinical notes are dedicated to docu-
menting the doctor?s discussion of risks, plans, and
alternative strategies. As such, we annotate verbs of
discussion (?We talked about the risks of this drug?),
consent (?She agreed with the current plan?), and
comprehension (?Mrs. Larsen repeated the potential
side effects back to me?), even though they are more
relevant to legal defense than medical treatment.
It is also because of this grounding in clinical lan-
guage that entities and other non-events are often
interpreted in terms of their associated eventive prop-
erties. There are two major types for which this is a
significant shift in semantic interpretation:
(2) a Medication as Event:
Orders: Lariam twice daily.
b Disorder as Event:
Tumor of the left lung.
In both these cases, entities which are not typically
marked as events are identified as such, because they
contribute significant information to the clinical time-
line being constructed. In (2a), for example, the
TIMEX3 ?twice daily? is interpreted as scoping over
the eventuality of the patient taking the medication,
not the prescription event. In sentence (2b), the ?tu-
mor? is interpreted as a stative eventuality of the
patient having a tumor located within an anatomical
region, rather than an entity within an entity.
Within the medical domain, these eventive inter-
pretations of medications, growths and status codes
are unambiguous and consistent. Doctors in clini-
cal notes (unlike in biomedical research texts) do
not discuss medications without an associated (im-
plicit) administering EVENT (though some mentions
may be hypothetical, generic or negated). Similarly,
mentions of symptoms or disorders reflect occur-
rences in a patient?s life, rather than abstract entities.
With these interpretations in mind, we can safely in-
fer, for instance, that all UMLS (Unified Medical
Language System, (Bodenreider, 2004)) entities of
the types Disorder, Chemical/Drug, Procedure and
Sign/Symptom will be EVENTs.
In general, in the medical domain, it is essential to
read ?between the lines? of the shorthand expressions
used by the doctors, and recognize implicit events
that are being referred to by specific anatomical sites
or medications.
4 Modifications to ISO-TimeML for the
Clinical Domain
Overall, we have found that the specification required
for temporal annotation in the clinical domain does
not require substantial modification from existing
specifications for the general domain. The clinical
domain includes no shortage of inferences, short-
hands, and unusual use of language, but the structure
of the underlying timeline is not unique.
As a result of this, we have been able to adopt most
of the framework from ISO-TimeML, adapting the
guidelines where needed, as well as reframing the
focus of what gets annotated. This is reflected in a
comprehensive guideline, incorporating the specific
patterns and uses of events and temporal expressions
as seen in clinical data. This approach allows the
resulting annotations to be interoperable with exist-
ing solutions, while still accommodating the major
differences in the nature of the texts. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org4
Our extensions of the ISO-TimeML specification
to the clinical domain are intended to address specific
constructions, meanings, and phenomena in medical
texts. Our schema differs from ISO-TimeML in a
few notable ways.
EVENT Properties We have both simplified the
ISO-TimeML coding of EVENTs, and extended it to
meet the needs of the clinical domain and the specific
language goals of the clinical narrative.
4Access to the corpus will require a data use agreement.
More information about this process is available from the corpus
website.
146
Consider, for example, how modal subordination is
handled in ISO-TimeML. This involves the semantic
characterization of an event as ?likely?, ?possible?, or
as presented by observation, evidence, or hearsay. All
of these are accounted for compositionally in ISO-
TimeML within the SLINK (Subordinating Link)
relation (Pustejovsky et al., 2005). While accept-
ing ISO-TimeML?s definition of event modality, we
have simplified the annotation task within the cur-
rent guideline, so that EVENTs now carry attributes
for ?contextual modality?, ?contextual aspect? and
?permanence?.
Contextual modality allows the values ACTUAL,
HYPOTHETICAL, HEDGED, and GENERIC. ACTUAL
covers EVENTs which have actually happened, e.g.,
?We?ve noted a tumor?. HYPOTHETICAL covers con-
ditionals and possibilities, e.g., ?If she develops a
tumor?. HEDGED is for situations where doctors
proffer a diagnosis, but do so cautiously, to avoid
legal liability for an incorrect diagnosis or for over-
looking a correct one. For example:
(3) a. The signal in the MRI is not inconsistent
with a tumor in the spleen.
b. The rash appears to be measles, awaiting
antibody test to confirm.
These HEDGED EVENTs are more real than a hypo-
thetical diagnosis, and likely merit inclusion on a
timeline as part of the diagnostic history, but must
not be conflated with confirmed fact. These (and
other forms of uncertainty in the medical domain)
are discussed extensively in (Vincze et al., 2008). In
contrast, GENERIC EVENTs do not refer to the pa-
tient?s illness or treatment, but instead discuss illness
or treatment in general (often in the patient?s specific
demographic). For example:
(4) In other patients without significant comor-
bidity that can tolerate adjuvant chemother-
apy, there is a benefit to systemic adjuvant
chemotherapy.
These sections would be true if pasted into any pa-
tient?s note, and are often identical chunks of text
repeatedly used to justify a course of action or treat-
ment as well as to defend against liability.
Contextual Aspect (to distinguish from grammati-
cal aspect), allows the clinically-necessary category,
INTERMITTENT. This serves to distinguish intermit-
tent EVENTs (such as vomiting or seizures) from
constant, more stative EVENTs (such as fever or sore-
ness). For example, the bolded EVENT in (5a) would
be marked as INTERMITTENT, while that in (5b)
would not:
(5) a She has been vomiting since June.
b She has had swelling since June.
In the first case, we assume that her vomiting has
been intermittent, i.e., there were several points since
June in which she was not actively vomiting. In the
second case, unless made otherwise explicit (?she has
had occasional swelling?), we assume that swelling
was a constant state. This property is also used when
a particular instance of an EVENT is intermittent,
even though it generally would not be:
(6) Since starting her new regime, she has had occa-
sional bouts of fever, but is feeling much better.
The permanence attribute has two values, FINITE
and PERMANENT. Permanence is a property of dis-
eases themselves, roughly corresponding to the med-
ical concept of ?chronic? vs. ?acute? disease, which
marks whether a disease is persistent following diag-
nosis. For example, a (currently) uncurable disease
like Multiple Sclerosis would be classed as PERMA-
NENT, and thus, once mentioned in a patient?s note,
will be assumed to persist through the end of the
patient?s timeline. This is compared with FINITE
disorders like ?Influenza? or ?fever?, which, if not
mentioned in subsequent notes, should be considered
cured and no longer belongs on the patient?s time-
line. Because it requires domain-specific knowledge,
although present in the specification, Permanence
is not currently annotated. However, annotators are
trained on the basic idea and told about subsequent
axiomatic assignment. The addition of this property
to our schema is designed to relieve annotators of any
feeling of obligation to express this inferred informa-
tion in some other way.
TIMEX3 Types Temporal expressions (TIMEX3s)
in the clinical domain function the same as in the gen-
eral linguistic community, with two notable excep-
tions. ISO-TimeML SETs (statements of frequency)
occur quite frequently in the medical domain, par-
ticularly with regard to medications and treatments.
Medication sections within notes often contain long
lists of medications, each with a particular associated
set (?Claritin 30mg twice daily?), and further tempo-
ral specification is not uncommon (e.g., ?three times
per day at meals?, ?once a week at bedtime?).
The second major change for the medical domain
is a new type of TIMEX3 which we call PREPOS-
TEXP. This covers temporally complex terms like
147
?preoperative?, ?postoperative?, and ?intraoperative?.
These temporal expressions designate a span of time
bordered, usually only on one side, by the incorpo-
rated event (an operation, in the previous EVENTs).
In many cases, the referent is clear:
(7) She underwent hemicolectomy last week, and
had some postoperative bleeding.
Here we understand that ?postoperative? refers to
?the period of time following the hemicolectomy?. In
these cases, the PREPOSTEXP makes explicit a tempo-
ral link between the bleeding and the hemicolectomy.
In other cases, no clear referent is present:
(8) Patient shows some post-procedure scarring.
In these situations, where no procedure is mentioned
(or the reference is never explicitly resolved), we
treat the PREPOSTEXP as a narrative container (see
Section 5), covering the span of time following the
unnamed procedure.
Finally, it is worth noting that the process of nor-
malizing those TIMEX3s is significantly more com-
plex relative to the general domain, because many
temporal expressions are anchored not to dates or
times, but to other EVENTs (whose dates are often
not mentioned or not known by the physician). As
we move towards a complete system, we are working
to expand the ISO-TimeML system for TIMEX3 nor-
malization to allow some value to be assigned to a
phrase like ?in the months after her hemicolectomy?
when no referent date is present. ISO-TimeML, in
discussion with ISO TC 37SC 4, plans to reference
to such TIMEX3s in a future release of the standard.
5 Temporal Ordering and Narrative
Containers
The semantic content and informational impact of
a timeline is encoded in the ordering relations that
are identified between the temporal and event expres-
sions present in clinical notes. ISO-TimeML speci-
fies the standard thirteen ?Allen relations? from the
interval calculus (Allen, 1983), which it refers to as
TLINK values. For unguided, general-purpose annota-
tion, the number of relations that could be annotated
grows quadratically with the number of events and
times, and the task quickly becomes unmanageable.
There are, however, strategies that we can adopt to
make this labeling task more tractable. Temporal
ordering relations in text are of three kinds:
1. Relations between two events
2. Relations between two times
3. Relations between a time and an event.
ISO-TimeML, as a formal specification of the tem-
poral information conveyed in language, makes no
distinction between these ordering types. Humans,
however, do make distinctions, based on local tempo-
ral markers and the discourse relations established in
a narrative (Miltsakaki et al., 2004; Poesio, 2004).
Because of the difficulty of humans capturing ev-
ery relationship present in the note (and the disagree-
ment which arises when annotators attempt to do so),
it is vital that the annotation guidelines describe an
approach that reduces the number of relations that
must be considered, but still results in maximally in-
formative temporal links. We have found that many
of the weaknesses in prior annotation approaches
stem from interaction between two competing goals:
? The guideline should specify certain types of an-
notations that should be performed;
? The guideline should not force annotations to be
performed when they need not be.
Failing in the first goal will result in under-annotation
and the neglect of relations which provide necessary
information for inference and analysis. Failure in the
second goal results in over-annotation, creating com-
plex webs of temporal relations which yield mostly
inferable information, but which complicate annota-
tion and adjudication considerably.
Our method of addressing both goals in tempo-
ral relations annotation is that of the narrative con-
tainer, discussed in Pustejovsky and Stubbs (2011).
A narrative container can be thought of as a temporal
bucket into which an EVENT or series of EVENTs
may fall, or a natural cluster of EVENTs around a
given time or situation. These narrative containers
are often represented (or ?anchored?) by dates or
other temporal expressions (within which a variety
of different EVENTs occur), although they can also
be anchored to more abstract concepts (?recovery?
which might involve a variety of EVENTs) or even
durative EVENTs (many other EVENTs can occur dur-
ing a surgery). Rather than marking every possible
TLINK between each EVENT, we instead try to link
all EVENTs to their narrative containers, and then
link those containers so that the contained EVENTs
can be linked by inference.
First, annotators assign each event to one of four
broad narrative containers: before the DOCTIME, be-
fore and overlapping the DOCTIME, just overlapping
the DOCTIME or after the DOCTIME. This narrative
148
container is identified by the EVENT attribute Doc-
TimeRel. After the assignment of DocTimeRel, the
remainder of the narrative container relations must
be specified using temporal links (TLINKs). There
are five different temporal relations used for such
TLINKs: BEFORE, OVERLAP, BEGINS-ON, ENDS-ON
and CONTAINS5. Due to our narrative container ap-
proach, CONTAINS is the most frequent relation by a
large margin.
EVENTs serving as narrative container anchors are
not tagged as containers per-se. Instead, annotators
use the narrative container idea to help them visu-
alize the temporal relations within a document, and
then make a series of CONTAINS TLINK annotations
which establish EVENTs and TIMEX3s as anchors,
and specify their contents. If the annotators do their
jobs correctly, properly implementing DocTimeRel
and creating accurate TLINKs, a good understanding
of the narrative containers present in a document will
naturally emerge from the annotated text.
The major advantage introduced with narrative
containers is this: a narrative event is placed within a
bounding temporal interval which is explicitly men-
tioned in the text. This allows EVENTs within sep-
arate containers to be linked by post-hoc inference,
temporal reasoning, and domain knowledge, rather
than by explicit (and time-consuming) one-by-one
temporal relations annotation.
A secondary advantage is that this approach works
nicely with the general structure of story-telling in
both the general and clinical domains, and provides a
compelling and useful metaphor for interpreting time-
lines. Often, especially in clinical histories, doctors
will cluster discussions of symptoms, interventions
and diagnoses around a given date (e.g. a whole para-
graph starting ?June 2009:?), a specific hospitaliza-
tion (?During her January stay at Mercy?), or a given
illness or treatment (?While she underwent Chemo?).
Even when specific EVENTs are not explicitly or-
dered within a cluster (often because the order can be
easily inferred with domain knowledge), it is often
quite easy to place the EVENTs into containers, and
just a few TLINKs can order the containers relative to
one another with enough detail to create a clinically
useful understanding of the overall timeline.
Narrative containers also allow the inference of re-
lations between sub-events within nested containers:
5This is a subset of the ISO-TimeML TLINK types, excluding
those seldom occurring in medical records, like ?simultaneous?
as well as inverse relations like ?during? or ?after?.
(9) December 19th: The patient underwent an MRI
and EKG as well as emergency surgery. Dur-
ing the surgery, the patient experienced mild
tachycardia, and she also bled significantly
during the initial incision.
1. December 19th CONTAINS MRI
2. December 19th CONTAINS EKG
3. December 19th CONTAINS surgery
a. surgery CONTAINS tachycardia
b. surgery CONTAINS incision
c. incision CONTAINS bled
Through our container nesting, we can automatically
infer that ?bled? occurred on December 19th (because
?19th? CONTAINS ?surgery? which CONTAINS ?inci-
sion? which CONTAINS ?bled?). This also allows the
capture of EVENT/sub-event relations, and the rapid
expression of complex temporal interactions.
6 Explicit vs. Inferable Annotation
Given a specification language, there are essentially
two ways of introducing the elements into the docu-
ment (data source) being annotated:6
? Manual annotation: Elements are introduced into
the document directly by the human annotator fol-
lowing the guideline.
? Automatic (inferred) annotation: Elements are cre-
ated by applying an automated procedure that in-
troduces new elements that are derivable from the
human annotations.
As such, there is a complex interaction between spec-
ification and guideline, and we focus on how the
clinical annotation task has helped shape and refine
the annotation guidelines. It is important to note that
an annotation guideline does not necessarily force
the markup of certain elements in a text, even though
the specification language (and the eventual goal of
the project) might require those annotations to exist.
In some cases, these added annotations are derived
logically from human annotations. Explicitly marked
temporal relations can be used to infer others that are
not marked but exist implicitly through closure. For
instance, given EVENTs A, B and C and TLINKs ?A
BEFORE B? and ?B BEFORE C?, the TLINK ?A BE-
FORE C? can be automatically inferred. Repeatedly
applying such inference rules allows all inferable
6We ignore the application of automatic techniques, such as
classifiers trained on external datasets, as our focus here is on
the preparation of the gold standard used for such classifiers.
149
TLINKs to be generated (Verhagen, 2005). We can
use this idea of closure to show our annotators which
annotations need not be marked explicitly, saving
time and effort. We have also incorporated these clo-
sure rules into our inter-annotator agreement (IAA)
calculation for temporal relations, described further
in Section 7.2.
The automatic application of rules following the
annotation of the text is not limited to the marking
of logically inferable relations or EVENTs. In the
clinical domain, the combination of within-group
shared knowledge and pressure towards concise writ-
ing leads to a number of common, inferred relations.
Take, for example, the sentence:
(10) Jan 2013: Colonoscopy, biopsies. Pathology
showed adenocarcinoma, resected at Mercy.
Diagnosis T3N1 Adenocarcinoma.
In this sentence, only the CONTAINS relations be-
tween ?Jan 2013? and the EVENTs (in bold) are
explicitly stated. However, based on the known
progression-of-care for colon cancer, we can infer
that the colonoscopy occurs first, biopsies occur dur-
ing the colonoscopy, pathology happens afterwards,
a diagnosis (here, adenocarcinoma) is returned after
pathology, and resection of the tumor occurs after
diagnosis. The presence of the AJCC staging infor-
mation in the final sentence (along with the confir-
mation of the adenocarcinoma diagnosis) implies a
post-surgical pathology exam of the resected spec-
imen, as the AJCC staging information cannot be
determined without this additional examination.
These inferences come naturally to domain ex-
perts but are largely inaccessible to people outside
the medical community without considerable anno-
tator training. Making explicit our understanding of
these ?understood orderings? is crucial; although they
are not marked by human annotators in our schema,
the annotators often found it initially frustrating to
leave these (purely inferential) relations unstated. Al-
though many of our (primarily linguistically trained)
annotators learned to see these patterns, we chose to
exclude them from the manual task since newer an-
notators with varying degrees of domain knowledge
may struggle if asked to manually annotate them.
Similar unspoken-but-understood orderings are
found throughout the clinical domain. As mentioned
in Section 3, both Permanence and Contextual As-
pect:Intermittent are properties of symptoms and dis-
eases themselves, rather than of the patient?s particu-
lar situation. As such, these properties could easily
Annotation Type Raw Count
EVENT 15,769
TIMEX3 1,426
LINK 7935
Total 25,130
Table 1: Raw Frequency of Annotation Types
TLINK Type Raw Count % of TLINKs
CONTAINS 5,112 64.42%
OVERLAP 1,205 15.19%
BEFORE 1,004 12.65%
BEGINS-ON 488 6.15%
ENDS-ON 126 1.59%
Total 7,935 100.00%
Table 2: Relative Frequency of TLINK types
be identified and marked across a medical ontology,
and then be automatically assigned to EVENTs rec-
ognized as specific medical named entities.
Finally, due to the peculiarities of EHR systems,
some annotations must be done programatically. Ex-
act dates of patient visit (or of pathology/radiology
consult) are often recorded as metadata on the EHR
itself, rather than within the text, making the canoni-
cal DOCTIME (or time of automatic section modifi-
cations) difficult to access in de-identified plaintext
data, but easy to find automatically.
7 Results
We report results on the annotations from the here-
released subset of the THYME colon cancer corpus,
which includes clinical notes and pathology reports
for 35 patients diagnosed with colon cancer for a
total of 107 documents. Each note was annotated
by a pair of graduate or undergraduate students in
Linguistics at the University of Colorado, then adju-
dicated by a domain expert. These clinical narratives
were sampled from the EHRs of a major healthcare
center (the Mayo Clinic). They were deidentified for
all patient-sensitive information; however, original
dates were retained.
7.1 Descriptive Statistics
Table 1 presents the raw counts for events, temporal
expressions and links in the adjudicated gold anno-
tations. Table 2 presents the number and percentage
of TLINKs by type in the adjudicated relations gold
annotations.
150
Annotation Type F1-Score Alpha
EVENT 0.8038 0.7899
TIMEX3 0.8047 0.6705
LINK: Participants only 0.5012 0.4999
LINK: Participants+type 0.4506 0.4503
LINK: CONTAINS 0.5630 0.5626
Table 3: IAA (F1-Score and Alpha) by annotation type
EVENT Property F1-Score Alpha
DocTimeRel 0.7189 0.6889
Cont.Aspect 0.9947 0.9930
Cont.Modality 0.9547 0.9420
Table 4: IAA (F1-Score and Alpha) for EVENT properties
7.2 Inter-annotator Agreement
We report inter-annotator agreement (IAA) results
on the THYME corpus. Each note was annotated by
two independent annotators. The final gold standard
was produced after disagreement adjudication by a
third annotator was performed.
We computed the IAA as F1-score and Krippen-
dorff?s Alpha (Krippendorff, 2012) by applying clo-
sure, using explicitly marked temporal relations to
identify others that are not marked but exist implicitly.
In the computation of the IAA, inferred-only TLINKs
do not contribute to the score, matched or unmatched.
For instance, if both annotators mark A BEFORE B
and B BEFORE C, to prevent artificially inflating the
agreement score, the inferred A BEFORE C is ignored.
Likewise, if one annotator marked A BEFORE B and
B BEFORE C and the other annotator did not, the
inferred A BEFORE C is not counted. However, if
one annotator did explicitly mark A BEFORE C, then
an equivalent inferred TLINK would be used to match
it. EVENT and TIMEX3 IAA was generated based
on exact and overlapping spans, respectively. These
results are reported in Table 3.
The THYME corpus also differs from ISO-
TimeML in terms of EVENT properties, with the
addition of DocTimeRel, ContextualModality and
ContextualAspect. IAA for these properties is in
Table 4.
7.3 Baseline Systems
To get an idea of how much work will be neces-
sary to adapt existing temporal information extrac-
tion systems to the clinical domain, we took the freely
available ClearTK-TimeML system (Bethard, 2013),
TempEval 2013 THYME Corpus
P R F1 P R F1
TIMEX3 83.2 71.7 77.0 59.3 42.8 49.7
EVENT 81.4 76.4 78.8 78.9 23.9 36.6
DocTimeRel - - - 47.4 47.4 47.4
LINK7 28.6 30.9 26.6 22.7 18.6 20.4
EVENT-TIMEX3 - - - 32.3 60.7 42.1
EVENT-EVENT - - - 7.0 3.0 4.2
Table 5: Performance of ClearTK-TimeML models, as
reported in the TempEval 2013 competition, and as applied
to the THYME Corpus development set.
which was among the top performing systems in
TempEval 2013 (UzZaman et al., 2013), and eval-
uated its performance on the THYME corpus.
ClearTK-TimeML uses support vector machine
classifiers trained on the TempEval 2013 training
data, employing a small set of features including
character patterns, tokens, stems, part-of-speech tags,
nearby nodes in the constituency tree, and a small
time word gazetteer. For EVENTs and TIMEX3s,
the ClearTK-TimeML system could be applied di-
rectly to the THYME corpus. For DocTimeRels, the
relation for an EVENT was taken from the TLINK
between that EVENT and the document creation time,
after mapping INCLUDES to OVERLAP. EVENTs
with no such TLINK were assumed to have a Doc-
TimeRel of OVERLAP. For other temporal relations,
INCLUDES was mapped to CONTAINS.
Results of this system on TempEval 2013 and the
THYME corpus are shown in Table 5. For time ex-
pressions, performance when moving to the clinical
data degrades about 25%, from F1 of 77.0 to 49.7.
For events, the degradation is much larger, about
40%, from 78.8 to 36.6, most likely because of the
large number of clinical symptoms, diseases, disor-
ders, etc. which have never been observed by the
system during training. Temporal relations are a bit
more difficult to compare because TempEval lumped
DocTimeRel and other temporal relations together
and had several differences in their evaluation met-
ric7. However, we at least can see that performance
of the ClearTK-TimeML system on temporal rela-
tions is low on clinical text, achieving only F1 of
20.4.
These results suggest that clinical narratives do
7The TempEval 2013 evaluation metric penalized systems
for parts of the text that were not examined by annotators, and
used different variants of closure-based precision and recall.
151
indeed present new challenges for temporal informa-
tion extraction systems, and that having access to
domain specific training data will be crucial for ac-
curate extraction in the clinical domain. At the same
time, it is encouraging that we were able to apply
existing ISO-TimeML-based systems to our corpus,
despite the several extensions to ISO-TimeML that
were necessary for clinical narratives.
8 Discussion
CONTAINS plays a large role in the THYME cor-
pus, representing 66% of TLINK annotations made,
compared with only 14.6% for OVERLAP, the second
most frequent type. We also see that BEFORE links
are relatively less common than OVERLAP and CON-
TAINS, illustrating that much of the temporal ordering
on the timeline is accomplished by using many ver-
tical links (CONTAINS, OVERLAP) to build contain-
ers, and few horizontal links (BEFORE, BEGINS-ON,
ENDS-ON) to order them.
IAA on EVENTs and Temporal Expressions is
strong, although differentiating implicit EVENTs
(which should not be marked) from explicit, mark-
able EVENTs remains one of the biggest sources of
disagreement. When compared to the data from the
2012 i2b2 challenge (Sun et al., 2013b), our IAA
figures are quite similar. Even with our more com-
plex schema, we achieved an F1-score of 0.8038 for
EVENTs (compared to the i2b2 score of 0.87 for par-
tial match). For TIMEX3s, our F1-score was 0.8047,
compared to an F1-score of 0.89 for i2b2.
TLINKing medical EVENTs remains a very diffi-
cult task. By using our narrative container approach
to constrain the number of necessary annotations and
by eliminating often-confusing inverse relations (like
?after? and ?during?) (neither of which were done for
the i2b2 data), we were able to significantly improve
on the i2b2 TLINK span agreement F1-score of 0.39,
achieving an agreement score of 0.5012 for all LINKs
across our corpus. The majority of remaining an-
notator disagreement comes from different opinions
about whether any two EVENTs require an explicit
TLINK between them or an inferred one, rather than
what type of TLINK it would be (e.g. BEFORE vs.
CONTAINS). Although our results are still signifi-
cantly higher than the results reported for i2b2, and
in line with previously reported general news figures,
we are not satisfied. Improving IAA is an important
goal for future work, and with further training, speci-
fication, experience, and standardization, we hope to
clarify contexts for explicit TLINKS.
News-trained temporal information extraction sys-
tems see a significant drop in performance when ap-
plied to the clinical texts of the THYME corpus. But
as the corpus is an extension of ISO-TimeML, future
work will be able to train ISO-TimeML compliant
systems on the annotations of the THYME corpus to
reduce or eliminate this performance gap.
Some applications that our work may enable in-
clude (1) better understanding of event semantics,
such as whether a disease is chronic or acute and
its usual natural history, (2) typical event duration
for these events, (3) the interaction of general and
domain-specific events and their importance in the fi-
nal timeline, and, more generally, (4) the importance
of rough temporality and narrative containers as a
step towards finer-grained timelines.
We have several avenues of ongoing and future
work. First, we are working to demonstrate the utility
of the THYME corpus for training machine learning
models. We have designed support vector machine
models with constituency tree kernels that were able
to reach an F1-score of 0.737 on an EVENT-TIMEX3
narrative container identification task (Miller et al.,
2013), and we are working on training models to
identify events, times and the remaining types of
temporal relations. Second, as per our motivating
use cases, we are working to integrate this annotation
data with timeline visualization tools and to use these
annotations in quality-of-care research. For example,
we are using temporal reasoning built on this work to
investigate the liver toxicity of methotrexate across
a large corpus of EHRs (Lin et al., under review)].
Finally, we plan to explore the application of our
notion of an event (anything that should be visible on
a domain-appropriate timeline) to other domains. It
should transfer naturally to clinical notes about other
(non-cancer) conditions, and even to other types of
clinical notes, as certain basic events should always
be included in a patient?s timeline. Applying our
notion of event to more distant domains, such as legal
opinions, would require first identifying a consensus
within the domain about which events must appear
on a timeline.
9 Conclusion
Much of the information in clinical notes critical to
the construction of a detailed timeline is left implicit
by the concise shorthand used by doctors. Many
events are referred to only by a term such as ?tu-
152
mor?, while properties of the event itself, such as
?intermittent?, may not be specified. In addition, the
ordering of events on a timeline is often left to the
reader to infer, based on domain-specific knowledge.
It is incumbent upon the annotation guideline to in-
dicate that only informative event orderings should
be annotated, while leaving domain-specific order-
ings to post-annotation inference. This document
has detailed our approach to adapting the existing
ISO-TimeML standard to this recovery of implicit
information, and defining guidelines that support an-
notation within this complex domain. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org, and the full
corpus has been proposed for use in a SemEval 2015
shared task.
Acknowledgments
The project described is supported by Grant Num-
ber R01LM010090 and U54LM008748 from the Na-
tional Library Of Medicine. The content is solely the
responsibility of the authors and does not necessarily
represent the official views of the National Library
Of Medicine or the National Institutes of Health.
We would also like to thank Dr. Piet C. de Groen
and Dr. Brad Erickson at the Mayo Clinic, as well as
Dr. William F. Styler III, for their contributions to the
schema and to our understanding of the intricacies of
clinical language.
References
James F Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Emmon Bach. 1986. The algebra of events. Linguistics
and philosophy, 9(1):5?16.
Steven Bethard. 2013. Cleartk-timeml: A minimalist ap-
proach to tempeval 2013. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
10?14, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): integrating biomedical
terminology. Nucleic acids research, 32(Database
issue):D267?D270, January.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Finding temporal order
in discharge summaries. In AMIA Annual Symposium
Proceedings, volume 2006, page 81. American Medical
Informatics Association.
Carlo Combi, Yuval Shahar, et al. 1997. Temporal reason-
ing and temporal data maintenance in medicine: issues
and challenges. Computers in biology and medicine,
27(5):353?368.
Robert H Dolin. 1995. Modeling the temporal complex-
ities of symptoms. Journal of the American Medical
Informatics Association, 2(5):323?331.
George Hripcsak, Nicholas D Soulakis, Li Li, Frances P
Morrison, Albert M Lai, Carol Friedman, Neil S Cal-
man, and Farzad Mostashari. 2009. Syndromic surveil-
lance using ambulatory electronic health records. Jour-
nal of the American Medical Informatics Association,
16(3):354?361.
Ann K Irvine, Stephanie W Haas, and Tessa Sullivan.
2008. Tn-ties: A system for extracting temporal infor-
mation from emergency department triage notes. In
AMIA Annual Symposium proceedings, volume 2008,
page 328. American Medical Informatics Association.
Elpida T Keravnou. 1997. Temporal abstraction of med-
ical data: Deriving periodicity. In Intelligent Data
Analysis in Medicine and Pharmacology, pages 61?79.
Springer.
Klaus H. Krippendorff. 2012. Content Analysis: An
Introduction to Its Methodology. SAGE Publications,
Inc, third edition edition, April.
Chen Lin, Elizabeth Karlson, Dmitriy Dligach, Mon-
ica Ramirez, Timothy Miller, Huan Mo, Natalie
Braggs, Andrew Cagan, Joshua Denny, and Guer-
gana. Savova. under review. Automatic identification
of methotrexade-induced liver toxicity in rheumatoid
arthritis patients from the electronic medical records.
Journal of the Medical Informatics Association.
John McCarthy. 2002. Actions and other events in sit-
uation calculus. In Proceedings of the International
conference on Principles of Knowledge Representation
and Reasoning, pages 615?628. Morgan Kaufmann
Publishers; 1998.
Ste?phane M Meystre, Guergana K Savova, Karin C Kipper-
Schuler, John F Hurdle, et al. 2008. Extracting infor-
mation from textual documents in the electronic health
record: a review of recent research. Yearb Med Inform,
35:128?44.
Timothy Miller, Steven Bethard, Dmitriy Dligach, Sameer
Pradhan, Chen Lin, and Guergana Savova. 2013. Dis-
covering temporal narrative containers in clinical text.
In Proceedings of the 2013 Workshop on Biomedical
Natural Langua ge Processing, pages 18?26, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
153
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bon-
nie Webber. 2004. The penn discourse treebank. In In
Proceedings of LREC 2004.
Massimo Poesio. 2004. Discourse annotation and seman-
tic annotation in the gnome corpus. In In Proceedings
of the ACL Workshop on Discourse Annotation.
James Pustejovsky and Amber Stubbs. 2011. Increasing
informativeness in temporal annotation. In Proceedings
of the 5th Linguistic Annotation Workshop, pages 152?
160. Association for Computational Linguistics.
James Pustejovsky, Robert Knippen, Jessica Littman, and
Roser Sauri. 2005. Temporal and event information in
natural language text. Language Resources and Evalu-
ation, 39(2-3):123?164.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent
Romary. 2010. Iso-timeml: An international standard
for semantic annotation. In Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
Angus Roberts, Robert Gaizauskas, Mark Hepple, George
Demetriou, Yikun Guo, and Ian Roberts. 2009. Build-
ing a semantically annotated corpus of clinical texts.
Journal of biomedical informatics, 42(5):950?966.
Guergana Savova, Steven Bethard, Will Styler, James Mar-
tin, Martha Palmer, James Masanz, and Wayne Ward.
2009. Towards temporal relation discovery from the
clinical narrative. In AMIA Annual Symposium Pro-
ceedings, volume 2009, page 568. American Medical
Informatics Association.
Tessa Sullivan, Ann Irvine, and Stephanie W Haas. 2008.
It?s all relative: usage of relative temporal expressions
in triage notes. Proceedings of the American Society
for Information Science and Technology, 45(1):1?8.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013a.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013b.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association, 20(5):806?813.
Alexander Turchin, Maria Shubina, Eugene Breydo,
Merri L Pendergrass, and Jonathan S Einbinder. 2009.
Comparison of information content of structured and
narrative text data sources on the example of medica-
tion intensification. Journal of the American Medical
Informatics Association, 16(3):362?370.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013. Semeval-2013 task 1: Tempeval-3: Evaluating
time expressions, events, and temporal relations. In Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 1?9, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2):211?241.
Veronika Vincze, Gyrgy Szarvas, Richrd Farkas, Gyrgy
Mra, and Jnos Csirik. 2008. The bioscope corpus:
biomedical texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics, 9(Suppl 11):1?
9.
Ying Zhao, George Karypis, and Usama M. Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discovery,
10:141?168.
Jiaping Zheng, Wendy W Chapman, Rebecca S Crowley,
and Guergana K Savova. 2011. Coreference resolution:
A review of general methodologies and applications in
the clinical domain. Journal of biomedical informatics,
44(6):1113?1122.
Li Zhou, Simon Parsons, and George Hripcsak. 2008. The
evaluation of a temporal reasoning system in processing
clinical discharge summaries. Journal of the American
Medical Informatics Association, 15(1):99?106.
154
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 45?50,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 10:
Linking Events and Their Participants in Discourse
Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{josefr,csporled}@coli.uni-sb.de
Roser Morante
CNTS
University of Antwerp
Roser.Morante@ua.ac.be
Collin Baker
ICSI
Berkeley, CA 94704
collin@icsi.berkeley.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
We describe the SemEval-2010 shared
task on ?Linking Events and Their Partic-
ipants in Discourse?. This task is an ex-
tension to the classical semantic role label-
ing task. While semantic role labeling is
traditionally viewed as a sentence-internal
task, local semantic argument structures
clearly interact with each other in a larger
context, e.g., by sharing references to spe-
cific discourse entities or events. In the
shared task we looked at one particular as-
pect of cross-sentence links between ar-
gument structures, namely linking locally
uninstantiated roles to their co-referents
in the wider discourse context (if such
co-referents exist). This task is poten-
tially beneficial for a number of NLP ap-
plications, such as information extraction,
question answering or text summarization.
1 Introduction
Semantic role labeling (SRL) has been defined as
a sentence-level natural-language processing task
in which semantic roles are assigned to the syntac-
tic arguments of a predicate (Gildea and Jurafsky,
2002). Semantic roles describe the function of the
participants in an event. Identifying the seman-
tic roles of the predicates in a text allows knowing
who did what to whom when where how, etc.
However, semantic role labeling as it is cur-
rently defined misses a lot of information due to
the fact that it is viewed as a sentence-internal
task. Hence, relations between different local se-
mantic argument structures are disregarded. This
view of SRL as a sentence-internal task is partly
due to the fact that large-scale manual annotation
projects such as FrameNet
1
and PropBank
2
typ-
ically present their annotations lexicographically
by lemma rather than by source text.
It is clear that there is an interplay between lo-
cal argument structure and the surrounding dis-
course (Fillmore, 1977). In early work, Palmer et
al. (1986) discussed filling null complements from
context by using knowledge about individual pred-
icates and tendencies of referential chaining across
sentences. But so far there have been few attempts
to find links between argument structures across
clause and sentence boundaries explicitly on the
basis of semantic relations between the predicates
involved. Two notable exceptions are Fillmore and
Baker (2001) and Burchardt et al (2005). Fillmore
and Baker (2001) analyse a short newspaper arti-
cle and discuss how frame semantics could benefit
discourse processing but without making concrete
suggestions of how to model this. Burchardt et al
(2005) provide a detailed analysis of the links be-
tween the local semantic argument structures in a
short text; however their system is not fully imple-
mented either.
With the shared task, we aimed to make a first
step towards taking SRL beyond the domain of
individual sentences by linking local semantic ar-
gument structures to the wider discourse context.
The task addresses the problem of finding fillers
for roles which are neither instantiated as direct
dependents of our target predicates nor displaced
through long-distance dependency or coinstantia-
tion constructions. Often a referent for an unin-
stantiated role can be found in the wider context,
i.e. in preceding or following sentences. An ex-
ample is given in (1), where the CHARGES role
1
http://framenet.icsi.berkeley.edu/
2
http://verbs.colorado.edu/
?
mpalmer/
projects/ace.html
45
(ARG2 in PropBank) of cleared is left empty but
can be linked to murder in the previous sentence.
(1) In a lengthy court case the defendant was
tried for murder. In the end, he was
cleared.
Another very rich example is provided by (2),
where, for instance, the experiencer and the ob-
ject of jealousy are not overtly expressed as depen-
dents of the noun jealousy but can be inferred to be
Watson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anything
of art but that is mere jealousy because our
views upon the subject differ.
This paper is organized as follows. In Section 2
we define how the concept of Null Instantiation
is understood in the task. Section 3 describes the
tasks to be performed, and Section 4, how they
are evaluated. Section 5 presents the participant
systems, and Section 6, their results. Finally, in
Section 7, we put forward some conclusions.
2 Null Instantiations
The theory of null complementation used here is
the one adopted by FrameNet, which derives from
the work of Fillmore (1986).
3
Briefly, omissions
of core arguments of predicates are categorized
along two dimensions, the licensor and the in-
terpretation they receive. The idea of a licensor
refers to the fact that either a particular lexical item
or a particular grammatical construction must be
present for the omission of a frame element (FE)
to occur. For instance, the omission of the agent in
(3) is licensed by the passive construction.
(3) No doubt, mistakes were made
0
Protagonist
.
The omission is a constructional omission be-
cause it can apply to any predicate with an appro-
priate semantics that allows it to combine with the
passive construction. On the other hand, the omis-
sion in (4) is lexically specific: the verb arrive al-
lows the Goal to be unspecified but the verb reach,
also a member of the Arriving frame, does not.
(4) We arrived 0
Goal
at 8pm.
3
Palmer et al?s (1986) treatment of uninstantiated ?essen-
tial roles? is very similar (see also Palmer (1990)).
The above two examples also illustrate the sec-
ond major dimension of variation. Whereas, in (3)
the protagonist making the mistake is only existen-
tially bound within the discourse (instance of in-
definite null instantiation, INI), the Goal location
in (4) is an entity that must be accessible to speaker
and hearer from the discourse or its context (def-
inite null instantiation, DNI). Finally, note that
the licensing construction or lexical item fully and
reliably determines the interpretation. Whereas
missing by-phrases have always an indefinite in-
terpretation, whenever arrive omits the Goal lexi-
cally, the Goal has to be interpreted as definite, as
it is in (4).
The import of this classification to the task here
is that we will concentrate on cases of DNI, be
they licensed lexically or constructionally.
3 Description of the Task
3.1 Tasks
We originally intended to offer the participants a
choice of two different tasks: a full task, in which
the test set was only annotated with gold stan-
dard word senses (i.e., frames) for the target words
and the participants had to perform role recogni-
tion/labeling and null instantiation linking, and a
NI only task, in which the test set was already
annotated with gold standard semantic argument
structures and the participants only had to recog-
nize definite null instantiations and find links to
antecedents in the wider context (NI linking).
However, it turned out that the basic semantic
role labeling task was already quite challenging
for our data set. Previous shared tasks have shown
that frame-semantic SRL of running text is a hard
problem (Baker et al, 2007), partly due to the fact
that running text is bound to contain many frames
for which no or little annotated training data are
available. In our case the difficulty was increased
because our data came from a new genre and do-
main (i.e., crime fiction, see Section 3.2). Hence,
we decided to add standard SRL, i.e., role recogni-
tion and labeling, as a third task (SRL only). This
task did not involve NI linking.
3.2 Data
The participants were allowed to make use of a va-
riety of data sources. We provided a training set
annotated with semantic argument structure and
null instantiation information. The annotations
were originally made using FrameNet-style and
46
later mapped semi-automatically to PropBank an-
notations, so that participants could choose which
framework they wanted to work in. The data for-
mats we used were TIGER/SALSA XML (Erk
and Pad?o, 2004) (FrameNet-style) and a modified
CoNLL-format (PropBank-style). As it turned
out, all participants chose to work on FrameNet-
style annotations, so we will not describe the Prop-
Bank annotation in this paper (see Ruppenhofer et
al. (2009) for more details).
FrameNet-style annotation of full text is ex-
tremely time-consuming. Since we also had to an-
notate null instantiations and co-reference chains
(for evaluation purposes, see Section 4), we could
only make available a limited amount of data.
Hence, we allowed participants to make use of ad-
ditional data, in particular the FrameNet and Prop-
Bank releases.
4
We envisaged that the participants
would want to use these additional data sets to
train SRL systems for the full task and to learn
something about typical fillers for different roles
in order to solve the NI linking task. The anno-
tated data sets we made available were meant to
provide additional information, e.g., about the typ-
ical distance between an NI and its filler and about
how to distinguish DNIs and INIs.
We annotated texts from two of Arthur Conan
Doyle?s fiction works. The text that served as
training data was taken from ?The Adventure of
Wisteria Lodge?. Of this lengthy, two-part story
we annotated the second part, titled ?The Tiger of
San Pedro?. The test set was made up of the last
two chapters of ?The Hound of the Baskervilles?.
We chose fiction rather than news because we be-
lieve that fiction texts with a linear narrative gen-
erally contain more context-resolvable NIs. They
also tend to be longer and have a simpler structure
than news texts, which typically revisit the same
facts repeatedly at different levels of detail (in the
so-called ?inverted pyramid? structure) and which
mix event reports with commentary and evalua-
tion, thus sequencing material that is understood
as running in parallel. Fiction texts should lend
themselves more readily to a first attempt at inte-
grating discourse structure into semantic role la-
beling. We chose Conan Doyle?s work because
most of his books are not subject to copyright any-
more, which allows us to freely release the anno-
tated data. Note, however, that this choice of data
4
For FrameNet we provided an intermediate release,
FrameNet 1.4 alpha, which contained more frames and lexi-
cal units than release 1.3.
means that our texts come from a different domain
and genre than many of the examples in FrameNet
and PropBank as well as making use of a some-
what older variety of English.
5
Table 1 provides basic statistics of the data sets.
The training data had 3.1 frames per sentence and
the test data 3.2, which is lower than the 8.8 frames
per sentence in the test data of the 2007 SemEval
task on Frame Semantic Structure Extraction.
6
We
think this is mainly the result of switching to a do-
main different from the bulk of what FrameNet
has made available in the way of full-text anno-
tation. In doing so, we encountered many new
frames and lexical units for which we could not
ourselves create the necessary frames and pro-
vide lexicographic annotations. The statistics also
show that null-instantiation is relatively common:
in the training data, about 18.7% of all FEs are
omitted, and in the test set, about 18.4%. Of the
DNIs, 80.9% had an antecedent in the training
data, and 74.2% in the test data.
To ensure a high quality of the annotations, both
data sets were annotated by more than one person
and then adjudicated. The training set was an-
notated independently by two experienced anno-
tators and then adjudicated by the same two peo-
ple. The test set was annotated by three annota-
tors and then adjudicated by the two experienced
annotators. Throughout the annotation and adju-
dication process, we discussed difficult cases and
also maintained a wiki. Additionally, we created a
software tool that checked the consistency of our
annotations against the frame, frame element and
FE-relation specifications of FrameNet and alerted
annotators to problems with their annotations. The
average agreement (F-score) for frame assignment
for pairs of annotators on the two chapters in the
test set ranges from 0.7385 to 0.7870. The agree-
ment of individual annotators with the adjudicated
gold standard ranges from 0.666 to 0.798. Given
that the gold standard for the two chapters features
228 and 229 different frame types, respectively,
this level of agreement seems quite good.
5
While PropBank provides annotations for the Penn Tree-
bank and is thus news-based, the lexicographic annotations
in FrameNet are extracted from the BNC, a balanced cor-
pus. The FrameNet full-text annotations, however, only cover
three domains: news, travel guides, and nuclear proliferation
reports.
6
The statistics in Table 1 and all our discussion of the
data includes only instances of semantic frames and ignores
the instances of the Coreference, Support, and Relativization
frames, which we labeled on the data as auxiliary informa-
tion.
47
data set sentences tokens frame inst. frame types overt FEs DNIs (resolved) INIs
train 438 7,941 1,370 317 2,526 303 (245) 277
test 525 9,131 1,703 452 3,141 349 (259) 361
Table 1: Statistics for the provided data sets
For the annotation of NIs and their links to the
surrounding discourse we created new guidelines
as this was a novel annotation task. We adopted
ideas from the annotation of co-reference informa-
tion, linking locally unrealized roles to all men-
tions of the referents in the surrounding discourse,
where available. We marked only identity rela-
tions but not part-whole or bridging relations be-
tween referents. The set of unrealized roles un-
der consideration includes only the core arguments
but not adjuncts (peripheral or extra-thematic roles
in FrameNet?s terminology). Possible antecedents
are not restricted to noun phrases but include all
constituents that can be (local) role fillers for
some predicate plus complete sentences (which
can sometimes fill roles such as MESSAGE).
4 Evaluation
As noted above, we allowed participants to ad-
dress three different tasks: SRL only, NI only,
full task. For role recognition and labeling we
used a standard evaluation set-up, i.e., accuracy for
role labeling and precision, recall, F-Score for role
recognition.
The NI linkings were evaluated slightly differ-
ently. In the gold standard, we identified refer-
ents for null instantiations in the discourse con-
text. In some cases, more than one referent might
be appropriate, e.g., because the omitted argument
refers to an entity that is mentioned multiple times
in the context. In this case, a system is given credit
if the NI is linked to any of these expressions. To
achieve this we create equivalence sets for the ref-
erents of NIs (by annotating coreference chains).
If the NI is linked to any item in the equivalence
set, the link is counted as a true positive. We can
then define NI linking precision as the number
of all true positive links divided by the number of
links made by a system, and NI linking recall as
the number of true positive links divided by the
number of links between an NI and its equivalence
set in the gold standard. NI linking F-Score is
then the harmonic mean between NI linking preci-
sion and recall.
Since it may sometimes be difficult to deter-
mine the correct extent of the filler of an NI, we
score an automatic annotation as correct if it in-
cludes the head of the gold standard filler in the
predicted filler. However, in order to not favor sys-
tems which link NIs to very large spans of text to
maximize the likelihood of linking to a correct ref-
erent, we introduce a second evaluation measure,
which computes the overlap (Dice coefficient) be-
tween the words in the predicted filler (P) of an NI
and the words in the gold standard one (G):
NI linking overlap =
2|P ?G|
|P | + |G|
(5)
Example (6) illustrates this point. The verb
won in the second sentence evokes the Fin-
ish competition frame whose COMPETITION role
is omitted. From the context it is clear that the
competition role is semantically filled by their first
TV debate (head: debate) and last night?s debate
(head: debate) in the previous sentences. These
two expressions form the equivalence set for the
COMPETITION role in the last sentence. Any sys-
tem that would predict a linkage to a filler that
covers the head of either of these two expressions
would score a true positive for this NI. However,
a system that linked to last night?s debate would
have an NI linking overlap of 1 (i.e., 2*3/(3+3))
while a system linking the whole second sentence
Last night?s debate was eagerly anticipated to the
NI would have an overlap of 0.67 (i.e., 2*3/(6+3))
(6) US presidential rivals Republican John
McCain and Democrat Barack Obama
have yesterday evening attacked each
other over foreign policy and the econ-
omy, in [their first TV debate]
Competition
.
[Last night?s debate]
Competition
was ea-
gerly anticipated. Two national flash
polls suggest that [Obama]
Competitor
won
Finish competition
0
Competition
.
5 Participating Systems
While a fair number of people expressed an inter-
est in the task and 26 groups or individuals down-
loaded the data sets, only three groups submitted
48
results for evaluation. Feedback from the teams
that downloaded the data suggests that this was
due to coinciding deadlines and to the difficulty
and novelty of the task. Only the SEMAFOR
group addressed the full task, using a pipeline of
argument recognition followed by NI identifica-
tion and resolution. Two groups (GETARUNS++
and SEMAFOR) tackled the NI only task, and
also two groups, the SRL only task (CLR and SE-
MAFOR
7
).
All participating systems were built upon ex-
isting systems for semantic processing which
were modified for the task. Two of the groups,
GETARUNS++ and CLR, employed relatively
deep semantic processing, while the third, SE-
MAFOR, employed a shallower probabilistic sys-
tem. Different approaches were taken for NI link-
ing. The SEMAFOR group modeled NI linking as
a variant of role recognition and labeling by ex-
tending the set of potential arguments beyond the
locally available arguments to also include noun
phrases from the previous sentence. The system
then uses, among other information, distributional
semantic similarity between the heads of potential
arguments and role fillers in the training data. The
GETARUNS++ group applied an existing system
for deep semantic processing, anaphora resolution
and recognition of textual entailment, to the task.
The system analyzes the sentences and assigns its
own set of labels, which are subsequently mapped
to frame semantic categories. For more details of
the participating systems please consult the sepa-
rate system papers.
6 Results and Analysis
6.1 SRL Task
Argument Recognition Label
Prec. Rec. F1 Acc.
SHA 0.6332 0.3884 0.4812 0.3471
SEM 0.6528 0.4674 0.5448 0.4184
CLR 0.6702 0.1121 0.1921 0.1093
Table 2: Shalmaneser (SHA), SEMAFOR (SEM)
and CLR performance on the SRL task (across
both chapters)
The results on the SRL task are shown in Table
2. To get a better sense of how good the perfor-
mance of the submitted systems was on this task,
7
For SEMAFOR, this was the first step of their pipeline.
we applied the Shalmaneser statistical semantic
parser (Erk and Pad?o, 2006) to our test data and
report the results. Note, however, that we used a
Shalmaneser trained only on FrameNet version 1.3
which is different from the version 1.4 alpha that
was used in the task, so its results are lower than
what can be expected with release 1.4 alpha.
We observe that although the SEMAFOR and
the CLR systems score a higher precision than
Shalmaneser for argument recognition, the SE-
MAFOR system scores considerably higher recall
than Shalmaneser, whereas the CLR system scores
a much lower recall.
6.2 NI Task
Tackling the resolution of NIs proved to be a dif-
ficult problem due to a variety of factors. First,
the NI sub-task was completely new and involves
several steps of linguistic processing. It also is
inherently difficult in that a given FE is not al-
ways omitted with the same interpretation. For
instance, the Content FE of the Awareness frame
evoked by know is interpreted as indefinite in
the blog headline More babbling about what it
means to know but as definite in a discourse
like Don?t tell me you didn?t know!. Second,
prior to this SemEval task there was no full-text
training data available that contained annotations
with all the kinds of information that is relevant
to the task, namely overt FEs, null-instantiated
FEs, resolutions of null-instantiations, and coref-
erence. Third, the data we used also represented
a switch to a new domain compared to existing
FrameNet full-text annotation, which comes from
newspapers, travel guides, and the nuclear pro-
liferation domain. Our most frequent frame was
Observable bodyparts, whereas it is Weapons in
FrameNet full-text. Fourth, it was not well un-
derstood at the beginning of the task that, in cer-
tain cases, FrameNet?s null-instantiation annota-
tions for a given FE cannot be treated in isolation
of the annotations of other FEs. Specifically, null-
instantiation annotations interact with the set of re-
lations between core FEs that FrameNet uses in its
analyses. As an example, consider the CoreSet re-
lation, which specifies that from a set of core FEs
at least one must be instantiated overtly, though
more of them can be. As long as one of the FEs
in the set is expressed overtly, null-instantiation is
not annotated for the other FEs in the set. For
instance, in the Statement frame, the two FEs
49
Topic and Message are in one CoreSet and the
two FEs Speaker and Medium are in another. If
a frame instance occurs with an overt Speaker and
an overt Topic, the Medium and Message FEs are
not marked as null-instantiated. Automatic sys-
tems that treat each core FE separately, may pro-
pose DNI annotations for Medium and Message,
resulting in false positives.
Therefore, we think that the evaluation that we
initially defined was too demanding for a novel
task. It would have been better to give sepa-
rate scores for 1) ability to recognize when a core
FE has to be treated as null-instantiated; 2) abil-
ity to distinguish INI and DNI; and 3) ability to
find antecedents. The systems did have to tackle
these steps anyway and an analysis of the sys-
tem output shows that they did so with different
success. The two chapters of our test data con-
tained a total of 710 null instantiations, of which
349 were DNI and 361 INI. The SEMAFOR sys-
tem recognized 63.4% (450/710) of the cases of
NI, while the GETARUNS++ system found only
8.0% (57/710). The distinction between DNI and
INI proved very difficult, too. Of the NIs that
the SEMAFOR system correctly identified, 54.7%
(246/450) received the correct interpretation type
(DNI or INI). For GETARUNS++, the percentage
is higher at 64.2% (35/57), but also based on fewer
proposed classifications. A simple majority-class
baseline gives a 50.8% accuracy. Interestingly, the
SEMAFOR system labeled many more INIs than
DNIs, thus often misclassifying DNIs as INI. The
GETARUNS++ system applied both labels about
equally often.
7 Conclusion
In this paper we described the SemEval-2010
shared task on ?Linking Events and Their Partic-
ipants in Discourse?. The task is novel, in that it
tackles a semantic cross-clausal phenomenon that
has not been treated before in a task, namely, link-
ing locally uninstantiated roles to their coreferents
at the text level. In that sense the task represents
a first step towards taking SRL beyond the sen-
tence level. A new corpus of fiction texts has been
annotated for the task with several types of seman-
tic information: semantic argument structure, co-
reference chains and NIs. The results scored by
the systems in the NI task and the feedback from
participant teams shows that the task was more dif-
ficult than initially estimated and that the evalua-
tion should have focused on more specific aspects
of the NI phenomenon, rather than on the com-
pleteness of the task. Future work will focus on
modeling the task taking this into account.
Acknowledgements
Josef Ruppenhofer and Caroline Sporleder are supported
by the German Research Foundation DFG (under grant PI
154/9-3 and the Cluster of Excellence Multimodal Comput-
ing and Interaction (MMCI), respectively). Roser Morante?s
research is funded by the GOA project BIOGRAPH of the
University of Antwerp. We would like to thank Jinho Choi,
Markus Dr?ager, Lisa Fuchs, Philip John Gorinski, Russell
Lee-Goldman, Ines Rehbein, and Corinna Schorr for their
help with preparing the data and/or implementing software
for the task. Thanks also to the SemEval-2010 Chairs Katrin
Erk and Carlo Strapparava for their support during the task
organization period.
References
C. Baker, M. Ellsworth, K. Erk. 2007. SemEval-2007
Task 19: Frame semantic structure extraction. In
Proceedings of SemEval-07.
A. Burchardt, A. Frank, M. Pinkal. 2005. Building text
meaning representations from contextually related
frames ? A case study. In Proceedings of IWCS-6.
K. Erk, S. Pad?o. 2004. A powerful and versatile XML
format for representing role-semantic annotation. In
Proceedings of LREC-2004.
K. Erk, S. Pad?o. 2006. Shalmaneser - a flexible tool-
box for semantic role assignment. In Proceedings of
LREC-06.
C. Fillmore, C. Baker. 2001. Frame semantics for text
understanding. In Proc. of the NAACL-01 Workshop
on WordNet and Other Lexical Resources.
C. Fillmore. 1977. Scenes-and-frames semantics, lin-
guistic structures processing. In A. Zampolli, ed.,
Fundamental Studies in Computer Science, No. 59,
55?88. North Holland Publishing.
C. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the Twelfth Annual
Meeting of the Berkeley Liguistics Society.
D. Gildea, D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, J. Dowding. 1986. Recovering im-
plicit information. In Proceedings of ACL-1986.
M. Palmer. 1990. Semantic Processing for Finite Do-
mains. CUP, Cambridge, England.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker,
M. Palmer. 2009. Semeval-2010 task 10: Linking
events and their participants in discourse. In The
NAACL-HLT 2009 Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions
(SEW-09).
50
Natural Language ~-~I Planner PAR graphics 
Figure 3: General architecture of the animation system 
The planner uses information from the general 
schema, such as pre-conditions and post-assertions, 
as well as information derived from the agents' ca- 
pabilities and the objects properties to fill in these 
gaps in several ways: 
? to select he way (activity) in which the instruc- 
tion is performed (enter by walking, by swim- 
ming, etc.); 
? to determine the prepartory actions that must 
be completed before the instruction is carried 
out, (for example, in order for an agent o open 
the door, the door has to be reachable and that 
may involve a locomotion process); 
? to decompose the action into smaller units (put 
the glass on the table, involves getting the glass, 
planning a route to the table, etc.) 
The output of the planner for the input instruction 
is a complete description of the actions involved, in- 
cluding participants, preparatory specifications, ter- 
mination conditions, manner, duration, etc. Partic- 
ipants bring with them a list of inherent properties 
of the agent (e.g. agent capabilities) or physical ob- 
jects (e.g., object configurations) and other charac- 
teristics, such as 'how to open' for an object such 
as a door. This complete description refers to a set 
of animation PARS which can be immediately ani- 
mated. 
In this way, a PAR schema for the action enter 
may actually translate into an animation PAR for 
walking into a certain area. One way to differenti- 
ate between action PAR schemas and instantiated 
animation PARs is to consider what it is possible to 
motion capture 4 (by attaching sensors to a moving 
human figure). For example, the enter action and 
the put action are quite general and underspecified 
and could not be motion captured. However, char- 
acteristic activities such as walking and swimming 
could be. For further details about the animation 
PARs and the animation system see (Badler et al, 
1999) and (Bindiganavaie et al, 2000). 
4 PAR as an IL  
The PAR representation for an action can be seen as 
a general template. PAR schemas include, as part 
of the basic sub-categorization frame, properties of 
4There are several other ways to generate motions, for 
example, through inverse kinematics, dynamics and key- 
framing. 
the action that can occur linguistically either as the 
main verb or as adjuncts to the main verb phrase. 
This captures problems of divergences, uch as the 
ones described by Talmy (Talmy, 1991), for verb- 
framed versus satellite-framed languages. 
New information may come from a sentence in 
natural anguage that modifies the action's inherent 
properties, uch as in John hit the ball slowly, where 
'slowly' is not part of the initial representation of
the action 'hit'. This new information is added to 
the PAR schema. 
Verb- versus Satel l i te-framed languages 
Verb-Framed Languages (VFL) map the motion 
(path or path + ground location) onto the verb, 
and the manner either onto a satellite or an ad- 
junct, while Satellite-Framed Languages (SFL) map 
the motion into the satellite, and the manner onto 
the main verb. 
English and other Germanic languages are consid- 
ered satellite-framed languages, expressing the path 
in the satellite; Spanish, among other Romance lan- 
guages, is a verb-framed language and expresses the 
path in the main verb. The pairs of sentences (1) 
and (2) from Talmy (1991) show examples of these 
divergences. In (1), in English, the exit of the bot- 
tle is expressed by the preposition out, in Spanish 
the same concept is incorporated in the main verb 
salir (to exit). In (2), the concept of blowing out 
the candle is represented differently in English and 
Spanish. 
(1) The bottle .floated out 
La boteUa sali6 flotando 
(the bottle exited floating) 
(2) I blew out the candle 
Apagud la vela sopldndola 
(I extinguish the candle blowing) 
4.1 Mot ion  
In order to capture generalizations about motion ac- 
tions, we have a generalized PAR schema for mo- 
tion, and our hierarchy includes different ypes of 
motion actions such as inherently directed motion 
and manner of motion actions that inherit from the 
more general schema, as shown in Figure 4. Directed 
motion actions, such as enter and exit, don't bring 
with them the manner by which the action is carried 
out but they have a inherent ermination condition. 
For example, 'enter a room' may be done by walk- 
ing, crawling or flying depending on the agents' ca- 
14 
motion/(par: motion) 
directed_motion manner_raotion 
enter/(term: in (0B J) ) exit/(term: out (0B J) ) crawl/(act : crawl) f loat/(act ::f loat) 
Figure 4: PAR schema hierarchy for motion actions 
pabilities, but it should end when the agent is in the 
room. In contrast, manner of motion verbs express 
the action explicitly and don't have an intrinsic ter- 
mination condition. 
Motion is a type of framing event where the path 
is in the main verb for VFLs and in the satellite for 
SFLs. In (3), we see the English sentence expressing 
the 'enter' idea in the preposition into whereas the 
Spanish sentence expresses it in the main verb entrar 
(to enter). 
(3) The bottle floated into the cave 
La botella entr5 flotando a la cueva 
(the bottle entered floating the cave) 
The PAR schemas don't distinguish the represen- 
tation for these sentences, because there is a sin- 
gle schema which includes both the manner and the 
path without specifying how they are realiized lin- 
guistically. Mappings from the lexical items to the 
schemas or to constraints in the schemas can be seen 
in Figure 5. 5 Independent of which is the source lan- 
guage, the PAR schema selected is motion, the ac- 
tivity field, which determines how the action is per- 
formed (in this case, by floating), is filled by float 
(the main verb in English, or the adjunct in Span- 
ish). The termination condition, which says that 
action ends when the agent is in the object, is added 
from the preposition in English and is part of the 
semantics of the main verb to enter in Spanish. 
EN float/\[par:motion,activity:float\] 
into/\[term:in(AG,OBJ)\] 
SP entrar/\[par:motion,term:in(AG,OBJ)\] 
flotar/\[activity :float\] 
Figure 5: Entries for the example sentences in (3) 
Because all of the necessary elements for a trans- 
lation are specified in this representation, it is up 
5A lexical item may have several mappings toreflect its 
semantics: For instance, float in English can be used also in 
the non-motion sense, in which case there will be two entries 
to capture that distinction. 
MOTION PAR 
activity : float 
agent : 
participants : object : 
bottle \] 
cave 
termination_cond : in(bott le,  cave) 
Figure 6: A (simplified) PAR schema for the sen- 
tences in (3) 
to the language specific component to transform it
into a surface structure that satisfies the grammati- 
cal principles of the destination language. 
Comparison with other work 
Our approach now diverges considerably from the 
approach outlined in Palmer et al (1998) which 
discusses the use of Feature-Based Tree Adjoining 
Grammars, (Joshi, 1985; Vijay-Shanker and Joshi, 
1991) to capture generalizations about manner-of- 
motion verbs. They do not propose an interlin- 
gua but use a transfer-based mechanism expressed 
in Synchronous Tree Adjoining Grammars to cap- 
ture divergences of VFL and SFL through the use 
of semantic features and links between the gram- 
mars. The problem of whether or not a preposi- 
tional phrase constitutes an argument to a verb or 
an adjunct (described by Palmer et al) does not 
constitute a problem in our representation, since all 
the information is recovered in the same template 
for the action to be animated. 
The PAR approach is much more similar to 
the Lexical Conceptual Structures (LCS) approach, 
(Jackendoff, 1972; Jackendoff, 1990), used as an in- 
terlingua representation (Doff, 1993). Based on the 
assumption that motion and manner of motion are 
conflated in a matrix verb like swim, the use of LCS 
allows separation of the concepts of motion, direc- 
tion, and manner of motion in the sentence John 
swam across the lake. Each one of these concepts i  
15 
represented separately in the interlingua represen- 
tation, as GO, PATH and MANNER, respectively. 
Our approach allows for a similar representation a d 
the end result is the same, namely that the event of 
swimming across the lake is characterized by sepa- 
rate semantic omponents, which can be expressed 
by the main schema nd by the activity field. In ad- 
dition, our representation also incorporates details 
about the action such as applicability conditions, 
preparatory specifications, termination conditions, 
and adverbial modifiers. It is not clear to us how 
the LCS approach could be used to effect the same 
commonality of representation. 
4.2 Ins t rument  
The importance of the additional information such 
as the termination conditions can be more clearly 
illustrated with a different set of examples. Another 
class of actions that presents interesting divergences 
involves instruments where the instrument is used 
as the main verb or as an adjunct depending on the 
language. The sentence pair in (4) shows this di- 
vergence for English and Portuguese. Because Por- 
tuguese does not have a verb for to spoon, it uses 
a more general verb colocar (to put) as the main 
verb and expresses the instrument in a prepositional 
phrase. Unlike directed motion actions, a put with 
hand-held instrument action (e.g., spoon, scoop, la- 
dle, etc.) leaves the activity field unspecified in both 
languages. The specific action is generated by taking 
the instrument into account. A simplified schema is 
shown in Figure 7. 
(4) Mary spoons chocolate over the ice cream 
Mary coloca chocolate sobre o sorvete coma 
colher 
(Mary puts chocolate over the ice cream with 
a spoon) 
PUT3 PAR 
activity : - 
participants : 
agent: Mary 
objects: chocolate, 
icecresm, 
spoon 
preparatory_spec : get(Mary, spoon) 
termination_cond : over(chocolate, icecream) 
Figure 7: Representation f the sentences in (4) 
Notice that the only connection between to spoon 
and its Portuguese translation would be the termi- 
nation condition where the object of the verb, choco- 
late, has a new location which is over the ice cream. 
5 Conc lus ion  
We have discussed a parameterized representation 
of actions grounded by the needs of animation of 
instructions in a simulated environment. In order 
to support he animation of these instructions, our 
representation makes explicit many details that are 
often underspecified in the language, such as start 
and end states and changes in the environment that 
happen as a result of the action. 
Sometimes the start and end state information 
provides critical information for accurate translation 
but it is not always necessary. Machine translation 
can often simply preserve ambiguities in the transla- 
tion without resolving them. In our application we 
cannot afford this luxury. An interesting question 
to pursue for future work will be whether or not we 
can determine which PAR slots are not needed for 
machine translation purposes. 
Generalizations based on action classes provide 
the basis for an interlingua pproach that captures 
the semantics of actions without committing to any 
language-dependent specification. This framework 
offers a strong foundation for handling the range 
of phenomena presented by the machine translation 
task. 
The structure of our PAR schemas incorpo- 
rate into a single template the kind of divergence 
presented in verb-framed and satellite-framed lan- 
guages. Although not shown in this paper, this 
representation can also capture idioms and non- 
compositional constructions since the animations of 
actions - and therefore the PARs that control them 
- must be equivalent for the same actions described 
in different languages. 
Currently, we are also investigating the possibility 
of building these action representations from a class- 
based verb lexicon which has explicit syntactic and 
semantic information (Kipper et al, 2000). 
Acknowledgments  
The authors would like to thank the Actionary 
group, Hoa Trang Dang, and the anonymous review- 
ers for their valuable comments. This work was par- 
tially supported by NSF Grant 9900297. 
References  
Norman I. Badler, Martha Palmer, and Rama Bindi- 
ganavale. 1999. Animation control for real-time 
virtual humans. Communications off the ACM, 
42(7):65-73. 
Norman I. Badler, Rarna Bindiganavale, Jan All- 
beck, William Schuler, Liwei Zhao, and Martha 
Palmer, 2000. Embodied Conversational Agents, 
chapter Parameterized Action Representation for 
Virtual Human Agents. MIT Press. to appear. 
Rama Bindiganavale, William Schuler, Jan M. All- 
beck, Norman I. Badler, Aravind K. Joshi, and 
16 
Martha Palmer. 2000. Dynamically altering agent 
behaviors using natural language instructions. 
Fourth International Conference on Autonomous 
Agents, June. 
Hoa Trang Dang, Karin Kipper, Martha Palmer, 
and Joseph Rosenzweig. 1998. Investigating reg- 
ular sense extensions based on intersective l vin 
classes. In Proceedings of COLING-A CL98, pages 
293-299, Montreal, CA, August. 
Bonnie J. Dorr. 1993. Machine Translation: A View 
from the Lexicon. MIT Press, Boston, MA. 
R. Jackendoff. 1972. Semantic Interpretation in 
Generative Grammar. MIT Press, Cambridge, 
Massachusetts. 
R. Jackendoff. 1990. Semantic Structures. MIT 
Press, Boston, Mass. 
Aravind K. Joshi. 1985. How much context sensi- 
tivity is necessary for characterizing structural de- 
scriptions:. Tree adjoining rammars. In L. Kart- 
tunen D. Dowry and A. Zwicky, editors, Nat- 
ural language parsing: Psychological, computa- 
tional and theoretical perspectives, pages 206-250. 
Cambridge University Press, Cambridge, U.K. 
Aravind K. Joshi. 1987. An introduction to tree ad- 
joining grammars. In A. Manaster-Ramer, ditor, 
Mathematics of Language. John Benjamins, Ams- 
terdam. 
Karin Kipper, Hoa Trang Dang, and Martha 
Palmer. 2000. Class-based construction of a verb 
lexicon. In submitted to AAAL 
Beth Levin. 1993. English Verb Classes and Alter- 
nation, A Preliminary Investigation. The Univer- 
sity of Chicago Press. 
Martha Palmer, Joseph Rosenzweig, and William 
Schuler. 1998. Capturing Motion Verb General- 
izations with Synchronous TAG. In Patrick St. 
Dizier, editor, Predicative Forms in NLP. Kluwer 
Press. 
William Schuler. 1999. Preserving semantic depen- 
dencies in synchronous tree adjoining grammar. 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics (ACL 
'99). 
Stuart M. Shieber and Yves Schabes. 1990. Syn- 
chronous tree adjoining rammars. In Proceedings 
of the 13th International Conference on Compu- 
tational Linguistics (COLING '90), Helsinki, Fin- 
land, August. 
Stuart M. Shieber. 1994. Restricting the weak- 
generative capability of synchronous tree adjoin- 
ing grammars. Computational Intelligence, 10(4). 
Leonard Talmy. 1991. Path to realization-via as- 
pect and result. In Proceedings of the 17th Annual 
Meeting of the Berkeley Linguistic Society, pages 
480-519. 
K. Vijay-Shanker and Aravind Joshi. 1991. Uni- 
fication based tree adjoining grammars. In 
J. Wedekind, editor, Unification-based Grammars. 
MIT Press, Cambridge, Massachusetts. 
17 
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 5?12,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
  
Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and Coreference 
James Pustejovsky, Adam Meyers, Martha Palmer, Massimo Poesio 
 
Abstract 
Many recent annotation efforts for English 
have focused on pieces of the larger problem 
of semantic annotation, rather than initially 
producing a single unified representation. 
This paper discusses the issues involved in 
merging four of these efforts into a unified 
linguistic structure: PropBank, NomBank, the 
Discourse Treebank and Coreference 
Annotation undertaken at the University of 
Essex. We discuss resolving overlapping and 
conflicting annotation as well as how the 
various annotation schemes can reinforce 
each other to produce a representation that is 
greater than the sum of its parts. 
 
1. Introduction 
 
The creation of the Penn Treebank (Marcus et al 
1993) and the word sense-annotated SEMCOR 
(Fellbaum, 1997) have shown how even limited 
amounts of annotated data can result in major 
improvements in complex natural language 
understanding systems. These annotated corpora 
have led to high-level improvements for parsing 
and word sense disambiguation (WSD), on the 
same scale as previously occurred for Part of 
Speech tagging by the annotation of the Brown 
corpus and, more recently, the British National 
Corpus (BNC) (Burnard, 2000). However, the 
creation of semantically annotated corpora has 
lagged dramatically behind the creation of other 
linguistic resources: in part due to the perceived 
cost, in part due to an assumed lack of theoretical 
agreement on basic semantic judgments, in part, 
finally, due to the understandable unwillingness 
of  research groups to get involved in such an 
undertaking. As a result, the need for such 
resources has become urgent.   
 
Many recent annotation efforts for English have 
focused on pieces of the larger problem of 
semantic annotation, rather than producing a 
single unified representation like Head-driven 
Phrase Structure Grammar (Pollard and Sag 
1994) or the Prague Dependency Tecto-
gramatical Representation (Hajicova & Kucer-
ova, 2002). PropBank (Palmer et al 2005) 
annotates predicate argument structure anchored 
by verbs. NomBank (Meyers, et. al., 2004a) 
annotates predicate argument structure anchored 
by nouns.  TimeBank (Pustejovsky et al 2003) 
annotates the temporal features of propositions 
and the temporal relations between propositions. 
The Penn Discourse Treebank (Miltsakaki et al
2004a/b) treats discourse connectives as 
predicates and the sentences being joined as 
arguments. Researchers at Essex were 
responsible for the coreference markup scheme 
developed in MATE (Poesio et al 1999; Poesio, 
2004a) and have annotated corpora using this 
scheme including a subset of the Penn Treebank 
(Poesio and Vieira, 1998), and the GNOME 
corpus (Poesio, 2004a).  This paper discusses the 
issues involved in creating a Unified Linguistic 
Annotation (ULA) by merging annotation of 
examples using the schemata from these efforts. 
Crucially, all individual annotations can be kept 
separate in order to make it easy to produce 
alternative annotations of a specific type of 
semantic information without need to modify the 
annotation at the other levels. Embarking on 
separate annotation efforts has the advantage of 
allowing researchers to focus on the difficult 
issues in each area of semantic annotation and 
the disadvantage of inducing a certain amount of 
tunnel vision or task-centricity ? annotators 
working on a narrow task tend to see all 
phenomena in light of the task they are working 
on, ignoring other factors. However, merging 
these annotation efforts allows these biases to be 
dealt with. The result, we believe, could be a 
more detailed semantic account than possible if 
the ULA had been the initial annotation effort 
rather than the result of merging. 
 
There is a growing community consensus that 
general annotation, relying on linguistic cues, 
and in particular lexical cues, will produce an 
enduring resource that is useful, replicable and 
portable.  We provide the beginnings of one such 
level derived from several distinct annotation 
efforts. This level could provide the foundation 
for a major advance in our ability to 
automatically extract salient relationships from 
text. This will in turn facilitate breakthroughs in 
message understanding, machine translation, fact 
retrieval, and information retrieval. 
 
2. The Component Annotation Schemata 
 
We describe below existing independent 
annotation efforts, each one of which is focused 
on a specific aspect of the semantic 
representation task: semantic role labeling, 
5
  
coreference, discourse relations, temporal 
relations, etc.  They have reached a level of 
maturity that warrants a concerted attempt to 
merge them into a single, unified representation, 
ULA.  There are several technical and theoretical 
issues that will need to be resolved in order to 
bring these different layers together seamlessly.  
Most of these approaches have annotated the 
same type of data, Wall Street Journal text, so it 
is also important to demonstrate that the 
annotation can be extended to other genres such 
as spoken language.  The demonstration of 
success for the extensions would be the training 
of accurate statistical semantic taggers. 
 
PropBank: The Penn Proposition Bank focuses 
on the argument structure of verbs, and provides 
a corpus annotated with semantic roles, 
including participants traditionally viewed as 
arguments and adjuncts.  An important goal is to 
provide consistent semantic role labels across 
different syntactic realizations of the same verb, 
as in the window in [ARG0 John] broke [ARG1 
the window] and [ARG1 The window] broke. 
Arg0 and Arg1 are used rather than the more 
traditional Agent and Patient to keep the 
annotation as theory-neutral as possible, and to 
facilitate mapping to richer representations.  The 
1M word Penn Treebank II Wall Street Journal 
corpus has been successfully annotated with 
semantic argument structures for verbs and is 
now available via the Penn Linguistic Data 
Consortium as PropBank I (Palmer, et. al., 2005).   
Coarse-grained sense tags, based on groupings of 
WordNet senses, are being added, as well as 
links from the argument labels in the Frames 
Files to FrameNet frame elements.  There are 
close parallels to other semantic role labeling 
projects, such as FrameNet (Baker, et. al., 1998; 
Fillmore & Atkins, 1998; Fillmore & Baker, 
2001), Salsa (Ellsworth, et.al, 2004), Prague 
Tectogrammatics (Hajicova & Kucerova, 2002) 
and IAMTC, (Helmreich, et. al., 2004) 
 
NomBank: The NYU NomBank project can be 
considered part of the larger PropBank effort and 
is designed to provide argument structure for 
instances of about 5000 common nouns in the 
Penn Treebank II corpus (Meyers, et. al., 2004a).  
PropBank argument types and related verb 
Frames Files are used to provide a commonality 
of annotation.  This enables the development of 
systems that can recognize regularizations of 
lexically and syntactically related sentence 
structures, whether they occur as verb phrases or 
noun phrases. For example, given an IE system 
tuned to a hiring scenario (MUC-6, 1995), 
NomBank and PropBank annotation facilitate  
generalization over patterns. PropBank and 
NomBank would both support a single IE pattern 
stating that the object (ARG1) of appoint is John 
and the subject (ARG0) is IBM, allowing a 
system to detect that IBM hired John from each 
of the following strings: IBM appointed John, 
John was appointed by IBM, IBM's appointment 
of John, the appointment of John by IBM and 
John is the current IBM appointee.  
 
Coreference: Coreference involves the detection 
of subsequent mentions of invoked entities, as in 
George Bush,? he?.  Researchers at Essex (UK) 
were responsible for the coreference markup 
scheme developed in MATE (Poesio et al 1999; 
Poesio, 2004a), partially implemented in the 
annotation tool MMAX and now proposed as an 
ISO standard; and have been responsible for the 
creation of two small, but commonly used 
anaphorically annotated corpora ? the Vieira / 
Poesio subset of the Penn Treebank (Poesio and 
Vieira, 1998), and the GNOME corpus (Poesio, 
2004a).   Parallel coreference annotation efforts 
funded by ACE have resulted in similar 
guidelines, exemplified by BBN?s recent 
annotation of Named Entities, common nouns 
and pronouns.   These two approaches provide a 
suitable springboard for an attempt at achieving a 
community consensus on coreference. 
 
Discourse Treebank:  The Penn Discourse 
Treebank (PDTB) (Miltsakaki et al2004a/b) is 
based on the idea that discourse connectives are 
predicates with associated argument structure 
(for details see (Miltsakaki et al2004a, 
Miltsakaki et al2004b). The long-range goal is 
to develop a large scale and reliably annotated 
corpus that will encode coherence relations 
associated with discourse connectives, including 
their argument structure and anaphoric links, 
thus exposing a clearly defined level of discourse 
structure and supporting the extraction of a range 
of inferences associated with discourse 
connectives. This annotation references the Penn 
Treebank annotations as well as PropBank, and 
currently only considers Wall Street Journal text. 
 
TimeBank: The Brandeis TimeBank corpus, 
funded by ARDA, focuses on the annotation of 
all major aspects in natural language text 
associated with temporal and event information 
(Day, et al 2003, Pustejovsky, et al 2004). 
Specifically, this involves three areas of the 
annotation: temporal expressions, event-denoting 
6
  
expressions, and the links that express either an 
anchoring of an event to a time or an ordering of 
one event relative to another. Identifying events 
and their temporal anchorings is a critical aspect  
of reasoning, and without a robust ability to 
identify and extract events and their temporal 
anchoring from a text, the real aboutness of the 
article can be missed.  The core of TimeBank is a 
set of 200 news reports documents, consisting of 
WSJ, DUC, and ACE articles, each annotated to 
TimeML 1.2 specification. It is currently being 
extended to AQUAINT articles. The corpus is 
available from the timeml.org website. 
 
3. Unifying Linguistic Annotations 
  
Since September, 2004, researchers representing 
several different sites and annotation projects 
have begun collaborating to produce a detailed 
semantic annotation of two difficult sentences. 
These researchers aim to produce a single unified 
representation with some consensus from the 
NLP community. This effort has given rise to 
both a listserv email list and this workshop: 
http://nlp.cs.nyu.edu/meyers/pie-in-the-sky.html, 
http://nlp.cs.nyu.edu/meyers/frontiers/2005.html 
The merging operations discussed here would 
seem crucial to the furthering of this effort. 
 
3.1 The Initial Pie in the Sky Example 
 
The following two consecutive sentences have 
been annotated for Pie in the Sky.  
 
Two Sentences From ACE Corpus File 
NBC20001019.1830.0181 
 
? but Yemen's president says the FBI has told 
him the explosive material could only have 
come from the U.S., Israel or two Arab 
countries. 
? and to a former federal bomb investigator, 
that description suggests a powerful 
military-style plastic explosive c-4 that can 
be cut or molded into different shapes. 
 
Although the full Pie-in-the-Sky analysis 
includes information from many different 
annotation projects, the Dependency Structure in 
Figure 1 includes only those components that 
relate to PropBank, NomBank, Discourse 
annotation, coreference and TimeBank. Several 
parts of this representation require further 
explanation. Most of these are signified by the 
special arcs, arc labels, and nodes. Dashed lines 
represent transparent arcs, such as the transparent 
dependency between the argument (ARG1) of 
modal can and the or. Or is transparent in that it 
allows this dependency to pass through it to cut 
and mold. There are two small arc loops -- 
investigator is its own ARG0 and description is 
its own ARG1. Investigator is a relational noun 
in NomBank. There is assumed to be an 
underlying relation between the Investigator 
(ARG0), the beneficiary or employer (the ARG2) 
and the item investigated (ARG1). Similarly, 
description acts as its own ARG1 (the thing 
described). There are four special coreference arc 
labels: ARG0-CF, ARG-ANAPH, EVENT-
ANAPH and ARG1-SBJ-CF. At the target of 
these arcs are pointers referring to phrases from 
the previous sentence or previous discourse. The 
first three of these labels are on arcs with the 
noun description as their source. The ARG0-CF 
label indicates that the phrase Yemen's president 
(**1**) is the ARG0, the one who is doing the 
describing. The EVENT-ANAPH label points to 
a previous mention of the describing event, 
namely the clause: The FBI told him the 
explosive material? (**3**). However, as noted 
above, the NP headed by description represents 
the thing described in addition to the action. The 
ARG-ANAPH label points to the thing that the 
FBI told him the explosive material can only 
come from ? (**2**). The ARG1-SBJ-CF label 
links the NP from the discourse what the bomb 
was made from as the subject with the NP 
headed by explosive as its predicate, much the 
same as it would in a copular construction such 
as: What the bomb was made from is the 
explosive C-4. Similarly, the arc ARG1-APP 
marks C-4 as an apposite, also predicated to the 
NP headed by explosive. Finally, the thick arcs 
labeled SLINK-MOD represent TimeML SLINK 
relations between eventuality variables, i.e.,  the 
cut and molded events are modally subordinate 
to the suggests proposition. The merged 
representation aims to be compatible with the 
projects from which it derives, each of which 
analyzes a different aspect of linguistic analysis. 
Indeed most of the dependency labels are based 
on the annotation schemes of those projects. 
 
We have also provided the individual PropBank, 
NomBank and TimeBank annotations below in 
textual form, in order to highlight potential 
points of interaction. 
 
PropBank:  and [Arg2 to a former federal bomb 
investigator], [Arg0 that description]  
[Rel_suggest.01 suggests]  [Arg1 [Arg1 a powerful 
military-style plastic explosive c-4] that 
7
  
 [ArgM-MOD can] be [Rel_cut.01 cut] or  [Rel_mold.01 
molded] [ArgM-RESULT into different shapes]]. 
 
NomBank: and to a former [Arg2 federal] [Arg1 
bomb] [Rel investigator], that description 
suggests a powerful [Arg2 military] - [Rel style] 
plastic [Arg1 explosive] c-4 that can be cut 
or molded into different shapes. 
 
TimeML: and to a former federal bomb 
investigator, that description [Event = ei1 
suggests]  a powerful military-style plastic 
explosive c-4 that  can be [Event = ei2 modal=?can? cut] 
or  [Event = ei3 modal=?can? molded]  into different 
shapes. <SLINK eventInstanceID = ei1 
subordinatedEventID = ei2 relType = ?Modal?/> 
<SLINK eventInstanceID = ei1 
subordinatedEventID = ei3 relType = ?Modal?/> 
 
  
Figure 1. Dependency Analysis of Sentence 2  
 
Note that the subordinating Events indicated by 
the TimeML SLINKS refer to the predicate 
argument structures labeled by PropBank, and 
that the ArgM-MODal also labeled by PropBank 
contains modality information also crucial to the 
SLINKS. While the grammatical modal on cut 
and mold is captured as an attribute value on the 
event tag, the governing event predicate suggest 
introduces a modal subordination to its internal 
argument, along with its relative clause. While 
this markup is possible in TimeML, it is difficult 
to standardize (or automate, algorithmically) 
since arguments are not marked up unless they 
are event denoting.  
 
3.2 A More Complex Example 
 
To better illustrate the interaction between 
annotation levels, and the importance of merging 
information resident in one level but not 
necessarily in another, consider the sentence 
below which has more complex temporal 
properties than the Pie-in-the-Sky sentences and 
its dependency analysis (Figure 2). 
 
 According to reports, sea trials for a patrol boat 
developed by Kazakhstan are being conducted 
and the formal launch is planned for the 
beginning of April this year.  
 
 
Figure 2.  Dependency Analysis of a Sentence 
with Interesting Temporal Properties 
 
The graph above incorporates these distinct 
annotations into a merged representation, much 
like the previous analysis. This sentence has 
more TimeML annotation than the previous 
sentence.  Note the loops of arcs which show that 
According to plays two roles in the sentence: (1) 
it heads a constituent that is the ARGM-ADV of 
the verbs conducted and planned; (2) it indicates 
that the information in this entire sentence is 
attributed to the reports. This loop is problematic 
in some sense because the adverbial appears to 
modify a constituent that includes itself. In 
actuality, however, one would expect that the 
ARGM-ADV role modifies the sentence minus 
the adverbial, the constituent that you would get 
if you ignore the transparent arc from ARGM-
8
  
ADV to the rest of the sentence.  Alternatively, a 
merging decision may elect to delete the ARGM-
ADV arcs, once the more specific predicate 
argument structure of the sentence adverbial 
annotation is available. 
 
The PropBank annotation for this sentence 
would label arguments for develop, conduct and 
plan, as given below. 
 
 [ArgM-ADV According to reports], [Arg1sea trials for  
[Arg1 a patrol boat] [Rel_develop.02 developed] [Arg0 
by Kazakhstan]] are being  
[Rel_conduct.01 conducted]  and [Arg1 the formal 
launch] is [Rel_plan.01 planned]  
[ArgM-TMP for the beginning of April this year].  
 
NomBank would add arguments for report, trial, 
launch and beginning as follows: 
 
 According to [Rel_report.01 reports], [Arg1 [ArgM-LOC 
sea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 a 
patrol boat] developed by Kazakhstan] are being 
conducted and the [ArgM-MNR formal] [Rel_launch.01 
launch] is planned for the [[REL_beginning.01 
beginning] [ARG1 of April this year]].  
 
TimeML, however, focuses on the anchoring of 
events to explicit temporal expressions (or 
document creation dates) through TLINKs, as 
well as subordinating relations, such as those 
introduced by modals, intensional predicates, 
and other event-selecting predicates, through 
SLINKs. For discussion, only part of the 
complete annotation is shown below.  
  
According to [Event = ei1  reports], sea [Event = ei3  
trials] for a boat [Event = ei4  developed]  by 
Kazakhstan are being [Event = ei5  conducted] and 
the formal [Event = ei6  launch] 
 is  [Event = ei7  planned] for the [Timex3= t1  beginning 
of April] [Timex3= t2 this year]. 
<SLINK eventID=?ei1? subordinatedEvent=?ei5, 
ei7? relType=EVIDENTIAL/> 
<TLINK eventID=?ei4? relatedToEvent =?ei3? 
relType=BEFORE/> 
<TLINK eventID=?ei6? relatedToTime=?t1? 
relType=IS_INCLUDED /> 
<SLINK eventID=?ei7? 
subordinatedEvent=?ei6? relType=?MODAL?/> 
<TLINK eventID=?ei5? relatedToEvent=?ei3? 
relType=IDENTITY/> 
 
Predicates such as plan and nominals such as 
report are lexically encoded to introduce 
SLINKs with a specific semantic relation, in this 
case, a ?MODAL? relType,. This effectively 
introduces an intensional context over the 
subordinated events. 
 
These examples illustrate the type of semantic 
representation we are trying to achieve.  It is 
clear that our various layers already capture 
many of the intended relationships, but they do 
not do so in a unified, coherent fashion.  Our 
goal is to develop both a framework and a 
process for annotation that allows the individual 
pieces to be automatically assembled into a 
coherent whole.   
 
4.0 Merging Annotations  
 
4.1 First Order Merging of Annotation 
We begin by discussing issues that arise in 
defining a single format for a merged 
representation of PropBank, NomBank and 
Coreference, the core predicate argument 
structures and  referents for the arguments.   One 
possible representation format would be to 
convert each annotation into features and values 
to be added to a larger feature structure. 1 The 
resulting feature structure would combine stand 
alone and offset annotation ? it would include 
actual words and features from the text as well as 
special features that point to the actual text 
(character offsets) and, perhaps, syntactic trees 
(offsets along the lines of PropBank/NomBank). 
Alternative global annotation schemes include 
annotation graphs (Cieri & Bird, 2001), and 
MATE (Carletta, et. al., 1999).  There are many 
areas in which the boundaries between these 
annotations have not been clearly defined, such 
as the treatment of support constructions and 
light verbs, as discussed below.  Determining the 
most suitable format for the merged 
representation should be a top priority. 
 
4.2 Resolving Annotation Overlap 
There are many possible interactions between 
different types of annotation: aspectual verbs 
have argument labels in PropBank, but are also 
important roles for temporal relations.  Support 
                                                 
 
1 The Feature Structure has many advantages as a target 
representation including: (1) it is easy to add lots of detailed 
features; and (2) the mathematical properties of Feature 
Structures are well understood, i.e., there are well-defined 
rule-writing languages, subsumption and unification 
relations, etc. defined for Feature Structures (Carpenter, 
1992) The downside is that a very informative Feature 
Structure is difficult for a human to read.  
 
9
  
constructions also have argument labels, and the 
question arises as to whether these should be 
associated with the support verb or the 
predicative nominal.  Given the sentence They 
gave the chefs a standing ovation, a PropBank 
component will assign role labels to arguments 
of give; a NomBank component will assign 
argument structure to ovation that labels the 
same participants. If the representations are 
equivalent, the question arises as to which of 
them (or both) should be included in the merged 
representation. The following graph  (Figure 3) 
is a combined PropBank and NomBank analysis 
of this sentence. "They" is the ARG0 of both 
"give" and "ovation"; "the chefs" is the ARG2 of 
"give", but the "ARG1" of ovation; "ovation" is 
the ARG1 of "give" and "give" is a support verb 
for "ovation". For this case, a reasonable choice 
might be to preserve the argument structure from 
both NomBank and PropBank, and to do the 
same for other predicative nominals that have 
give (or receive, obtain, request?) as a support 
verb, e.g., (give a kiss/hug/squeeze, give a 
lecture/speech, give a promotion, etc.).   For 
other support constructions, such as take a walk, 
have a headache and make a mistake, the noun is 
really the main predicate and it is questionable 
whether the verbal argument structure carries  
gave
chefsthe
They
a ovationstanding
NP
NP
S
ARG0
REL
ARG2
ARG1
NP
ARG1 REL
ARG0SUPPORT
 
Figure 3. Merged PropBank/NomBank representation 
of They gave the chefs a standing ovation. 
much information, e.g., there are no selection 
restrictions between light verbs and their subject 
(ARG0) -- these are inherited from the noun. 
Thus make a mistake selects a different type of 
subject than make a gain, e.g., people and 
organizations make mistakes, but stock prices 
make gains. For these constructions, the merged 
representation might not need to include the 
(ARG0) relation between the subject of the 
sentence and make, and future propbanking 
efforts might do well to ignore the shared 
arguments of such instances and leave them for 
NomBank. However, the merged representation 
would inherit PropBank?s annotation of some 
other light verb features including: negation, e.g., 
They did not take a walk; modality, e.g., They 
might take a walk; and sentence adverbials, e.g., 
They probably will take a walk. 
 
4.3 Resolving Annotation Conflicts 
Interactions between linguistic phenomena can 
aid in quality control, and conflicts found during 
the deliberate merging of different annotations 
provides an opportunity to correct and fine-tune 
the original layers. For example, predicate 
argument structure (PropBank and NomBank) 
annotation sometimes assumes different 
constituent structure than the Penn Treebank. We 
have noticed some tendencies that help resolve 
these conflicts, e.g., prenominal noun 
constituents as in Indianapolis 500, which forms 
a single argument in NomBank, is correctly 
predicted to be a constituent, even though the 
Penn Treebank II assumes a flatter structure.  
 
Similarly, idioms and multiword expressions 
often cause problems for both PropBank and 
NomBank. PropBank annotators tend to view 
argument structure in terms of verbs and 
NomBank annotators tend to view argument 
structure in terms of nouns. Thus many examples 
that, perhaps, should be viewed as idioms are 
viewed as special senses of either verbs or nouns. 
Having idioms detected and marked before 
propbanking and nombanking could greatly 
improve efficiency.   
 
Annotation accuracy is often evaluated in terms 
of inter-annotation consistency. Task definitions 
may need to err on the side of being more 
inclusive in order to simplify the annotators task. 
For example, the NomBank project assumes the 
following definition of a support verb (Meyers, 
et.al., 2004b):  ?? a verb which takes at least 
two arguments NP1 and XP2 such that XP2 is an 
argument of the head of NP1. For example, in 
John took a walk, a support verb (took) shares 
one of its arguments (John) with the head of its 
other argument (walk).? The easiest way to 
apply this definition is without exception, so it 
will include idiomatic expressions such as keep 
tabs on, take place, pull strings. Indeed, the 
dividing line between support constructions and 
idioms is difficult to draw (Meyers 2004b).   
PropBank annotators are also quite comfortable 
with associating general meanings to the main 
verbs of idiomatic expressions and labeling their 
10
  
argument roles, as in cases like bring home the 
bacon and mince words with. Since idioms often 
have interpretations that are metaphorical 
extensions of their literal meaning, this is not 
necessarily incorrect.  It may be helpful to have 
the literal dependencies and the idiomatic 
reading both represented. The fact that both 
types of meaning are available is evidenced by 
jokes, irony, and puns.  
 
With respect to idioms and light verbs, TimeML 
can be viewed as a mediator between PropBank 
and NomBank. In TimeML, light verbs and the 
nominalizations accompanying them are marked 
with two separate EVENT tags. This guarantees 
an annotation independent of textual linearity 
and therefore ensures a parallel treatment for 
different textual configurations. In (a) the light 
verb construction "make an allusion" is 
constituted of a verb and an NP headed by an 
event-denoting noun, whereas in (b) the nominal 
precedes a VP, which in addition contains a 
second N:  
(a) Max [made an allusion] to the crime.  
(b) Several anti-war [demonstrations have taken 
place] around the globe. 
Both verbal and nominal heads are tagged 
because they both contribute relevant 
information to characterizing the nature of the 
event. The nominal element plays a role in the 
more semantically based task of event 
classification. On the other hand, the information 
in the verbal component is important at two 
different levels: it provides the grammatical 
features typically associated with verbal 
morphology, such as tense and aspect, and at the 
same time it may help in disambiguating cases 
like take/give a class, make/take a phone call. 
The two tagged events are marked as identical by 
a TLINK introduced for that purpose. The 
TimeML annotation for the example in (a) is 
provided below.  
Max [Event = ei1  made] an [Event = ei2  allusion] to 
the crime.  
<TLINK eventID="ei1"relatedToEvent="ei2" 
relType=IDENTITY> 
Some cases of support in NomBank could also 
be annotated as "bridging" anaphora. Consider 
the sentence: The pieces make up the whole. 
It is unclear whether make up is a support verb 
linking whole as the ARG1 of pieces or if pieces 
is linked to whole by bridging anaphora.  
There are also clearer cases. In Nastase, a rival 
player defeated Jimmy Connors in the third 
round, the word rival and Jimmy Connors are 
clearly linked by bridging. However, a wayward 
NomBank annotator might construct a support 
chain (player + defeated) to link rival with its 
ARG1 Jimmy Connors.  In such a case, a 
merging of annotation could reveal annotation 
errors. In contrast, a NomBank annotator would 
be correct in linking John as an argument of walk 
in John took a series of walks (the support chain 
took + series consists of a support verb and a 
transparent noun), but this may not be obvious to 
the non-NomBanker. Thus the merging of 
annotation may result in the more consistent 
specifications for all.  
 
In our view, this process of annotating all layers 
of information and then merging them in a 
supervised manner, taking note of the conflicts, 
is a necessary prerequisite to defining more 
clearly the boundaries between the different 
types of annotation and determining how they 
should fit together.  Other areas of annotation 
interaction include: (1) NomBank  and 
Coreference, e.g. deriving that John teaches 
Mary from John is Mary's teacher involves: (a) 
recognizing that teacher is an argument 
nominalization such that the teacher is the ARG0 
of teach (the one who teaches); and (b) marking 
John and teacher as being linked by predication 
(in this case, an instance of type coreference); 
and (2) Time and Modality -  when a fact used to 
be true, there are two time components: one in 
which the fact is true and one in which it is false. 
Clearly more areas of interaction will emerge as 
more annotation becomes available and as the 
merging of annotation proceeds.  
 
5. Summary 
 
We proposed a way of taking advantage of the 
current practice of separating aspects of semantic 
analysis of text into small manageable pieces. 
We propose merging these pieces, initially in a 
careful, supervised way, and hypothesize that the 
result could be a more detailed semantic analysis 
than was previously available. This paper 
discusses some of the reasons that the merging 
process should be supervised. We primarily gave 
examples involving the interaction of PropBank, 
NomBank and TimeML. However, as the 
merging process continues, we anticipate other 
conflicts that will require resolution. 
 
References 
 
C. F. Baker, F. Collin, C. J. Fillmore, and J. B.  
Lowe (1998), The Berkeley FrameNet 
project. In Proc. of COLING/ACL-98,  86--90 
11
  
O. Babko-Malaya, M. Palmer, X. Nianwen, S.  
Kulick, A. Joshi (2004), Propbank II, 
Delving Deeper, In Proc.  of HLT-NAACL 
Workshop: Frontiers in Corpus Annotation. 
R. Carpenter (1992), The Logic of Typed  
Feature Structures. Cambridge Univ. Press. 
J. Carletta and A. Isard (1999), The MATE  
Annotation Workbench: User Requirements. 
In Proc. of the ACL Workshop: Towards 
Standards and Tools for Discourse Tagging. 
Univ. of Maryland, 11-17 
C. Cieri and S. Bird (2001), Annotation Graphs  
and Servers and Multi-Modal Resources: 
Infrastructure for  Interdisciplinary Education, 
Research and Development Proc. of the ACL 
Workshop on Sharing Tools and Resources 
for Research  and Education, 23-30 
D. Day,  L. Ferro, R. Gaizauskas, P. Hanks, M.  
Lazo, J. Pustejovsky, R. Saur?, A. See, A. 
Setzer, and B. Sundheim (2003), The 
TIMEBANK Corpus. Corpus Linguistics. 
M. Ellsworth, K. Erk, P. Kingsbury and S. Pado  
(2004), PropBank, SALSA, and FrameNet: 
How Design Determines Product, in Proc. of  
LREC 2004 Workshop: Building Lexical 
Resources from Semantically Annotated 
Corpora.  
C. Fellbaum (1997), WordNet: An Electronic  
Lexical Database, MIT Press.. 
C. J. Fillmore and B. T. S. Atkins (1998), 
FrameNet and lexicographic relevance. In the 
Proc. of the First International Conference 
on Language Resources and Evaluation.  
C. J. Fillmore and C. F. Baker (2001), Frame  
semantics for text understanding. In Proc. of 
NAACL WordNet and Other Lexical 
Resources Workshop. 
E. Hajivcova and I. Kuvcerov'a (2002).  
Argument/Valency Structure in PropBank, 
LCS Database and Prague Dependency 
Treebank: A Comparative Pilot Study. In the 
Proc. of the Third International Conference 
on Language Resources and Evaluation 
(LREC 2002),  846--851. 
S. Helmreich, D. Farwell, B. Dorr, N. Habash, L. 
    Levin, T. Mitamura, F. Reeder, K. Miller, E. 
     Hovy, O. Rambow and A. Siddharthan,(2004), 
     Interlingual Annotation of Multilingual Text 
     Corpora, Proc. of the HLT-EACL Workshop 
     on Frontiers in Corpus Annotation. 
A, Meyers, R. Reeves, C. Macleod, R, Szekely,  
V. Zielinska, B. Young, and R. Grishman  
(2004a), The NomBank Project: An Interim 
Report, Proc. of HLT-EACL Workshop: 
Frontiers in Corpus Annotation. 
A. Meyers, R. Reeves, and C. Macleod (2004b),  
NP-External Arguments: A Study of 
Argument Sharing in English. In The ACL 
2004 Workshop on Multiword Expressions: 
Integrating Processing. 
E. Miltsakaki, R. Prasad, A. Joshi and B. Webber. 
 (2004a), The Penn Discourse Treebank. In 
Proc. 4th International Conference on 
Language Resources and Evaluation (LREC 
2004). 
E. Miltsakaki, R. Prasad, A. Joshi and B. Webber  
(2004b), Annotation of Discourse 
Connectives and their Arguments, in Proc. of 
HLT-NAACL Workshop: Frontiers in Corpus 
Annotation 
M.  Marcus, B. Santorini, and M. Marcinkiewicz  
(1993), Building a large annotated corpus of 
english: The penn treebank. Computational 
Linguistics, 19:313--330. 
M. Palmer, D. Gildea, P. Kingsbury (2005), The  
Proposition Bank: A Corpus Annotated with 
Semantic Roles, Computational Linguistics 
Journal, 31:1. 
M. Poesio (2004a), The MATE/GNOME  
Scheme for Anaphoric Annotation, Revisited, 
Proc. of SIGDIAL 
M. Poesio (2004b), Discourse Annotation and  
Semantic Annotation in the GNOME Corpus, 
Proc. of ACL Workshop on Discourse 
Annotation. 
M. Poesio and M. Alexandrov-Kabadjov (2004), 
A general-purpose, off-the-shelf system for 
anaphora resol.. Proc. of LREC. 
M. Poesio, F. Bruneseaux, and L. Romary  
(1999), The MATE meta-scheme for 
coreference in dialogues in multiple language, 
Proc. of the ACL Workshop on Standards for 
Discourse Tagging.  
M. Poesio and R. Vieira (1998), A corpus-based  
investigation of definite description use. 
Computational Linguistics, 24(2). 
C. Pollard and I. A. Sag (1994), Head-driven  
phrase structure grammar. Univ. of Chicago 
Press. 
J. Pustejovsky, R. Saur?, J. Casta?o, D. R. 
 Radev, R. Gaizauskas, A. Setzer, B. 
Sundheim and G. Katz (2004), Representing 
Temporal and Event Knowledge for QA 
Systems. In Mark T. Maybury (ed.), New 
Directions in Question Answering, MIT Press. 
J. Pustejovsky,  B. Ingria, R. Saur?, J. Casta?o, J.  
Littman, R. Gaizauskas, A. Setzer, G. Katz, 
and I. Mani (2003), The Specification 
Language TimeML. In I. Mani, J. 
Pustejovsky, and R. Gaizauskas, editors, The 
Language of Time: A Reader. Oxford Univ. 
Press. 
12
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, page 1
Manchester, August 2008
The Relevance of a Cognitive Model of the Mental
Lexicon to Automatic Word Sense Disambiguation
Martha Palmer and Susan Brown
University of Colorado at Boulder
Department of Linguistics
Hellems 290, 295 UCB
Boulder, CO 80309-0295, USA
Supervised word sense disambiguation requires training corpora that have been tagged with word senses,
and these word senses typically come from a pre-existing sense inventory. Space limitations imposed by
dictionary publishers have biased the field towards lists of discrete senses for an individual lexeme.
Although some dictionaries use hierarchical entries to emphasize relations between senses, many do not.
WordNet, which has been the default choice of NLP researchers for sense tagging because of its broad
coverage and easy accibility, does not have hierarchical entries. Could the relations between senses that
are captured by a hierarchy be useful to NLP systems? Concerns have also been raised about whether or
not WordNet?s word senses are unnecessarily fine-grained. WSD systems are obviously more successful
in distinguishing coarse-grained senses than fine-grained ones (Navigli, 2006), but important information
could be lost if fine-grained distinctions are ignored. Recent psycholinguistic evidence seems to indicate
that closely related word senses may be represented in the mental lexicon much like a single sense,
whereas distantly related senses may be represented more like discrete entities (Brown, 2008). These
results suggest that, for the purposes of WSD, closely related word senses can be clustered together into
a more general sense with little meaning loss. This talk will describe this psycholinguistic research and
its current implications for automatic word sense disambiguation, as well as plans for future research and
its possible impact.
c
? 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.
1
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 23?24,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Twitter in Mass Emergency:  What NLP Techniques Can Contribute  William J. Corvey1, Sarah Vieweg2, Travis Rood1 & Martha Palmer1 1Department of Linguistics, 2ATLAS Institute University of Colorado Boulder, CO 80309 William.Corvey, Sarah.Vieweg, Travis.Rood, Martha.Palmer@colorado.edu 
Abstract 
We detail methods for entity span identifica-tion and entity class annotation of Twitter communications that take place during times of mass emergency. We present our motiva-tion, method and preliminary results. 
1 Introduction During times of mass emergency, many turn to Twitter to gather and disperse relevant, timely in-formation (Starbird et al 2010; Vieweg et al 2010). However, the sheer amount of information now communicated via Twitter during these time- and safety-critical situations can make it difficult for individuals to locate personally meaningful and actionable information. In this paper, we discuss natural language processing (NLP) techniques de-signed for Twitter data that will lead to the location and extraction of specific information during times of mass emergency.  2 Twitter Use in Mass Emergency Twitter communications are comprised of 140-character messages called ?tweets.?  During times of mass emergency, Twitter users send detailed information that may help those affected to better make critical decisions.   Our goal is to develop techniques to automatically identify crucial pieces of information in these tweets. This process will lead to the automatic ex-traction of information that helps people under-stand the situation ?on the ground? during mass emergencies. Relevant information would include such things as warnings, road closures, and evacuations among other timely information.  
3 The Annotation Process A foundational level of linguistic annotation for many natural language processing tasks is Named Entity (or nominal entity) tagging (Bikel 1999).  Typical labeled entities that were included in the Automatic Content Extraction (ACE) guidelines (LDC 2004) are: Person, Location, Organization, and Facility, the four maximal entity classes. Our preliminary annotation task consists of identifying the syntactic span and entity class for these four types of entities in a pilot set of Twitter data (200 tweets from a data set generated during the 2009 Oklahoma grassfires). In future annotation, the ontology will be expanded to include event and relation annotations, as well as additional sub-classes of the entities now examined.  Annotations are done using Knowtator (Ogren 2006), a tool built within the Prot?g? framework (http://protege.stanford.edu/).  The ontology devel-opment is data-driven; as such it is likely that cer-tain ACE annotations will never emerge and other annotations (such as disaster-relevant materials) will be necessary additions.   Three annotators undertook pilot annotation as part of the construction of preliminary annotation guidelines; the top pairwise ITA score is reported below. Twitter data makes reference to numerous entity spans that are of specific interest to this an-notation task, such as road intersections and multi-word named entities. The example below, from the pilot annotation set, shows a relatively simple span delineation.  [PERSON Velma area residents]: [PERSON Officials] say to take [FACILTY Old Hwy 7] to [FACILTY Speedy G] to safely evacuate. [LOCATION Stephens Co Fair-grounds] in [LOCATION Duncan] for shel-ter  
23
Because of the varying length of entities, annota-tors cannot be given simple rules for deciding the spans for annotations. This difficulty is reflected in markedly lower rates for span identification inter-annotator agreement (IAA) rates than for simple class assignment.   4 Preliminary Results  IAA calculations were performed using the Know-tator IAA functionality. When annotations are re-quired to be both the same span and class, the pilot annotation yielded an F-score of 56.27 (An addi-tional 4% have exact span matches but different classes). However, when annotations are required to have the same class assignment but only over-lapping spans, this F-score rises to 72.85. While Facility and Location are the most commonly con-fused classes, span-matching remains a difficult issue for all entity classes.  5 Discussion While these ITA rates are significantly lower than published results from previous ACE annotation efforts (LDC 2004), we believe that the crisis communications domain, particularly with regard to Twitter analysis, provides challenges not en-countered in newswire, broadcast transcripts, or newspaper data. First, determining the maximal span of interest for a given class assignment is non-trivial. The constraint of 140 characters neces-sarily results in very limited syntactic and semantic contexts, making spans and entity class assign-ments much harder to determine.   A large source of disagreement was on the treat-ment of coordinated or listed noun phrases. In cer-tain contexts, each entity (cities below) requires its own span (e.g. ?Firestorms in Oklahoma. [Midwest City], [Lake Draper]. Some houses lost?), whereas in other contexts we find multiple entities per span (e.g. ?Midwest City to evacuate between SE 15th and Rena and Anderson and Hiwassee also [Tur-tlewood, Wingsong, and Oakwood additions]?).  Equally, class assignment cannot be a mechanistic process or accomplished by reference to lists, as it is important to distinguish between cases where terms have been elided due to limited space and cases where no elision has taken place. For in-stance, the entity ?Attorney General? (as opposed 
to ?Attorney General?s Office?) might be anno-tated ?Person? or ?Organization? depending on con-text, or simply ambiguous, i.e. lacking sufficient context. It is primarily these unclear cases of class assignment that will require careful discussion in the annotation guidelines and in future mappings to an ontology.  In summary, this pilot study represents a new ap-plication of ACE annotation practices to a uniquely challenging domain. We outline issues that place special demands on annotators and future direc-tions for ongoing research. We are confident that as we refine our guidelines and provide more cues and examples for the annotators that the determina-tion of spans and entity classes will improve. Acknowledgments This work is supported by the US National Science Foundation IIS-0546315 and IIS-0910586 but does not represent the views of the NSF. This work was conducted using the Prot?g? resource, supported by grant LM007885 from the US NLM. References  Daniel M. Bikel, Richard Schwartz and Ralph M. Weischedel. 1999. An Algorithm that Learns What?s in a Name. In: the Machine Learning Journal Special Issue on Natural Language Learning. George Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. (2004). The Automatic Content Extraction (ACE) Program ? Tasks, Data, and Evaluation. In: Proceedings of Con-ference on Language Resources and Evaluation (LREC 2004). Kate Starbird, Leysia Palen, Amanda L. Hughes and Sarah Vieweg. 2010. Chatter on The Red: What Hazards Threat Reveals About the Social Life of Mi-croblogged Information. In: Proc. CSCW 2010. ACM Press. LDC, 2004, Automatic Content Extraction [www.ldc.upenn.edu/Projects/ACE/] Philip Ogren. 2006. Knowtator: A Prot?g? plug-in for annotated corpus construction. In : Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology 2006. ACM Press. Sarah Vieweg, Amanda L. Hughes, Kate Starbird and Leysia Palen. 2010. Microblogging During Two Natural Hazards Events: What Twitter May Contrib-ute to Situational Awareness. In: Proc. CHI 2010. ACM Press. 
24
Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 1?8,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
 
Towards a Domain Independent Semantics: Enhancing Semantic Representation with Construction Grammar   Jena D. Hwang1,2 Rodney D. Nielsen1 Martha Palmer1,2   1Ctr. for Computational Language and Education Research University of Colorado at Boulder Boulder, CO 80302 2Department of Linguistics University of Colorado at Boulder Boulder, CO 80302 {hwangd,rodney.nielsen,martha.palmer}@colorado.edu     Abstract 
In Construction Grammar, structurally patterned units called constructions are assigned meaning in the same way that words are ?  via convention rather than composition. That is, rather than piecing semantics together from individual lexical items, Construction Grammar proposes that semantics can be assigned at the construction level. In this paper, we investigate whether a classifier can be taught to identify these constructions and consider the hypothesis that identifying construction types can improve the semantic interpretation of previously unseen predicate uses. Our results show that not only can the constructions be automatically identified with high accuracy, but the classifier also performs just as well with out-of-vocabulary predicates.  1 Introduction The root of many challenges in natural language processing applications is the fact that humans can convey a single piece of information in numerous and creative ways. Syntactic variations (e.g. I gave him my book. vs. I gave my book to him.), the use of synonyms (e.g. She bought a used car. vs. She purchased a pre-owned automobile.) and numerous other variations can complicate the semantic analysis and the automatic understanding of a text.  Consider the following sentence.  (1) They hissed him out of the university  While (1) is clearly understandable for humans, to automatically discern the meaning of hissed in this 
instance would take more than learning that the verb hiss is defined as ?make a sharp hissing sound? (WordNet 3.0). Knowing that hiss can also mean ?a show of contempt? is helpful. However, it would also require the understanding that the sentence describes a causative event if we are to interpret this sentence as meaning something like ?They caused him to leave the university by means of hissing or contempt?. The problem of novel words, expressions and usages are especially significant because discriminative learning methods used for automatic text classification do not perform as well when tested on text with a feature distribution that is different from what was seen in the training data. This is recognized to be a critical issue in domain adaptation (Ben-David et. al, 2006). Whether we seek to account for words or usages that are infrequent in the training data or to adapt a trained classifier to a new domain of text that includes new vocabulary or new forms of expressions, success in overcoming these challenges partly lies in the successful identification and the use of features that generalize over linguistic variation.  In this paper we borrow from the theories presented by Construction Grammar (CxG) to explore the development of general features that may help account for the linguistic variability and creativity we see in the data.  Specifically, we investigate whether a classifier can be taught to identify constructions as described by CxG and gauge their value in interpreting novel words. The development of approaches to effectively capture such novel semantics will enhance applications requiring richer representations of language understanding such as machine 
1
 translation, information retrieval, and text summarization. Consider, for instance, the following machine translation into Spanish by the Google translate (http://translate.google.com/):   They hissed him out of the university. ? Silbaban fuera de la universidad. Tr. They were whistling outside the university.1  The translation has absolutely no implication that a group of people did something to cause another person to leave the university. However, when the verb is changed to a verb that is seen to frequently appear in a caused motion interpretation (e.g. throw), the results are correct:  They threw him out of the university. ? Lo sacaron de la universidad. Tr. They took him out of the university.  Thus, if we could facilitate a caused motion interpretation by bootstrapping semantics from constructions (e.g. ?X ___ Y out of Z? implies caused motion), we could enable accurate translations that otherwise would not be possible. 2 Current Approaches In natural language processing (NLP), the issue of semantic analysis in the presence of lexical and syntactic variability is often perceived as the purview of either word sense disambiguation (WSD) or semantic role labeling (SRL) or both. In the case of WSD, the above issue is often tackled through the use of large corpora tagged with sense information to train a classifier to recognize the different shades of meaning of a semantically ambiguous word (Ng and Lee, 2006; Agirre and Edmonds, 2006).  In the case of SRL, the goal is to identify each of the arguments of the predicate and label them according to their semantic relationship to the predicate (Gildea and Jurafsky, 2002).   There are several corpora available for training WSD classifiers such as WordNet?s SemCor (Miller 1995; Fellbaum 1998) and the GALE OntoNotes data (Hovy et. al., 2006). However, most, if not all, of these corpora include only a small fraction of all English predicates. Since WSD systems train separate classifiers for each                                                 1 We have hand translated the Google translation back to English for comparison. 
predicate, if a particular predicate does not exist in the sparse training data, a system cannot create an accurate semantic interpretation. Even if the predicate is present, the appropriate sense might not be. In such a case, the WSD will again be unable to contribute to a correct overall semantic interpretation. This is the case in example (1), where even the extremely fine-grained sense distinctions provided by WordNet do not include a sense of hiss that is consistent with the caused motion interpretation rendered in the example. Available for SRL tasks are efforts such as PropBank (Palmer et al, 2005) and FrameNet (Fillmore et al, 2003) that have developed semantic role labels (based on differing approaches) and have labeled large corpora for training and testing of SRL systems. PropBank (PB) identifies and labels the semantic arguments of the verb on a verb-by-verb basis, creating a separate frameset that includes verb specific semantic roles to account for each subcategorization frame of the verb. Much like PB, FrameNet (FN) identifies and labels semantic roles, known as Frame Elements, around a relational target, usually a verb.2 But unlike PB, Frame Elements less verb specific, but rather are defined in terms of semantic structures called frames evoked by the verb. That is, one or more verbs can be associated with a single semantic frame. Currently FN has over 2000 distinct Frame Elements.  The lexical resource VerbNet (Kipper-Schuler, 2005) details semantic classes of verbs, where a class is composed of verbs that have similar syntactic realizations, following work by Levin (1993). Verbs are grouped by their syntactic realization or frames, and each frame is associated with a meaning. For example, the verbs loan and rent are grouped together in class 13.1 with roughly a ?give? meaning, and the verbs deposit and situate are grouped into 9.1 with roughly a ?put? meaning.  Although differing in the nature of their tasks, WSD and SRL systems both treat lexical items as the source of meaning in a clause. In WSD, for every sense we need a new entry in our dictionary to be able to interpret the sentence. With SRL, we                                                 2 PropBank labels Arg0 and Arg1, for the most part, correspond to Dowty?s Prototypical Agent and Prototypical Patient, respectively, providing important generalizations. 
2
 need the semantic role labels that describe the predicate argument relationships in order to extract the meaning.  In either case, we are still left with the same issue ? if the meaning lies in the lexical items, how do we interpret unseen words and novel lexical usages? As shown in the CoNLL-2005 shared task (Carreras and Marquez, 2005), system performance numbers drop significantly when a classifier, trained on the Wall Street Journal (WSJ) corpus, is tested on the Brown corpus. This is largely due to the ?highly ambiguous and unseen predicates (i.e. predicates that do not have training examples)? (Giuglea and Moschitti, 2006). 3 Construction Grammar This issue of scalability and generalizability across genres could possibly be improved by linking semantics more directly with syntax, as theorized by Construction Grammar (CxG) (Fillmore et. al., 1988; Golderg, 1995; Kay, 2002; Michaelis, 2004; Goldberg, 2006). This theory suggests that the meaning of a sentence arises not only from the lexical items but also from the patterned structures or constructions they sit in. The meaning of a given phrase, a sentence, or an utterance, then, arises from the combination of lexical items and the syntactic structure in which they are found, including any patterned structural configurations (e.g. patterns of idiomatic expressions such as ?The Xer, the Yer? ? The bigger, the better) or recurring structural elements (e.g. function words such as determiners, particles, conjunctions, and prepositions). That is, instead of focusing solely on the semantic label of words, as is done in SRL and in many traditional theories in Linguistics, CxG brings more into focus the interplay of lexical items and syntactic forms or structural patterns as the source of meaning.  3.1 Application of Construction Grammar Thus, rather than just assigning labels at the level of lexical items and predicate arguments as a way of piecing together the meaning of a sentence, we follow the central premise of CxG. Specifically, that semantics can be and should be interpreted at the level of the larger structural configuration.  Consider the following three sentences, each having the same syntactic structure, each taken 
from different genres of writing available on the web.  Blogger arrested - blog him out of jail! [Blog] Someone mind controlled me off the cliff. [Gaming] He clocked the first pitch into center field. [Baseball]  Each of these sentences makes use of words, especially the verb, in ways particular to their genre. Even if we are unfamiliar with the specific jargon used, as a human we can infer the general meaning intended by each of the three sentences: a person X causes an entity Y to move in the path specified by the prepositional phrase (e.g. third sentence: ?A player causes something to land in the center field.?).   In a similar way, if we can assign a meaning of caused motion at the sentence level and an automatic learner can be trained to accurately identify the construction, then even when presented with an unseen word, a useful semantic analysis is still possible. 3.2 Caused-Motion Construction For this effort, we focused on the caused-motion construction, which can be defined as having the coarse-grained syntactic structure of Subject Noun Phrase followed by a verb that takes both a Noun Phrase Object and a Prepositional Phrase: (NP-SBJ (V NP PP)); and the semantic meaning ?the agent, NP-SBJ, directly causes the patient, NP, to move along the path specified by the PP? (Goldberg 1995). This construction is exemplified by the following sentences from (Goldberg 1995):  (2) Frank sneezed the tissue off the table. (3) Mary urged Bill into the house. (4) Fred stuffed the papers in the envelope. (5) Sally threw a ball to him.  However, not all syntactic structures of the form (NP-SBJ (V NP PP)) belong to the caused-motion construction. Consider the following sentences.  (6) I considered Ben as one of my brothers. (7) Jen took the highway into Pennsylvania. (8) We saw the bird in the shopping mall. (9) Mary kicked the ball to my relief.  In (6) and (9), the PPs do not specify a location, a direction or a path. In (8), the PP is a location; 
3
 however, the PP indicates the location in which the ?seeing? event happened, not a path along which ?we? caused ?the bird? to move.  Though the PP in (7) expresses a path, it is not a path in which Jen causes ?the highway? to move. 3.3 Goals As an initial step in determining the usefulness of construction grammar for interpreting semantics in computational linguistics, we present the results of our study aimed at ascertaining if a classifier can be taught to identify caused-motion constructions. We also report on our investigations into which features were most useful in the classification of caused-motion constructions.  4 Data & Experiments The data for this study was pulled from the WSJ part of Penn Treebank II (Marcus et al, 1994). From this corpus, all sentences with the syntactic form (NP-SBJ (V NP PP)) were selected. The selection allowed for intervening adverbial phrases (e.g. ?Sally threw a ball quickly to him?) and additional prepositional phrases (e.g. ?Sally threw a ball to him on Tuesday? or ?Sally threw a ball in anger into the scorer?s table?). A total of 14.7k instances3 were identified in this manner. To reduce the size of the corpus to be labeled to a target of 1800 instances, we removed, firstly, instances containing traces as parsed by the TreeBank. These included passive usages (e.g. ?Coffee was shipped from Colombia by Gracie?) and instances with traces in the object NP or PP including questions and relative clauses (e.g. ?What did Gracie ship from Colombia??). In construction grammar, however, traces do not exist, since grammar is a set of patterns of varying degrees of complexity. Thus CxG would characterize passives, questions structures, and relative clauses as having their own respective phrasal constructions, which combine with the caused-motion construction. In order to ensure sufficient training data with the standard form of the caused-motion construction as defined in Goldberg 1995 and 2006 (see Section 3.2), we                                                 3 We use the term instances over sentences since a sentence can have more than one instance. For example, the sentence ?I gave the ball to Bill, and he kicked it to the wall.? is composed of 2 instances. 
chose to remove these usages.  Secondly, we removed the instances of sentences that can be deterministically categorized as non-caused motion constructions: instances containing ADV, EXT, PRD, VOC, or TMP type object NPs (e.g.?Cindy drove five hours from Dallas?, ?You listen, boy, to what I say!?). Because we can automatically identify this category, keeping these examples in our data would have resulted in even higher performance. We also considered the possibility of reducing the size by removing certain classes of verbs such as verbs of communication (e.g. reply, bark), psychological state (e.g. amuse, admire), or existence (e.g. be, exist). While it is reasonable to say that these verb types are highly unlikely to appear in a caused-motion construction, if we were to remove sets of verbs based on their likely behavior, we would also be excluding interesting usages such as ?The stand-up comedian amused me into a state of total enjoyment.? or ?The leader barked a command into a radio.? After filtering these sentences, 8700 remained. From the remaining instances, we selected 1800 instances at random for the experiments presented. 4.1 Labels and Classifier The 1800 instances were hand-labeled with one of the following two labels:   - Caused-Motion (CM)  - Non Caused-Motion (NON-CM)  The CM label included both literal usages (e.g. ?Well-wishers stuck little ANC flags in their hair.?) and non-literal usages (e.g. ?Producers shepherded ?Flashdance? through several scripts.?) of caused-motion. After the annotation, the corpus was randomly divided into two sets: 75% for training data and 25% for testing data. The distribution of the labels in the test data is 33.3% CM and 66.7% NON-CM. The distribution in the training set is 31.8% CM and 68.2% NON-CM. For our experiments, we used a Support Vector Machine (SVM) classifier with a linear kernel. In particular we made use of the LIBSVM (Chang and Lin, 2001) as training and testing software. 
4
 4.2 Baseline Features The baseline consisted of a single conceptual feature - the lemmatized, case-normalized verb. We chose the verb as a baseline feature because it is generally accepted to be the core lexical item in a sentence, which governs the syntactic structure and semantic constituents around it. This is especially evidenced in the Penn Treebank where NP nodes are assigned with syntactic labels according to the position in the tree relative to the verb (e.g. Subject). In VerbNet and PropBank, the semantic labels are assigned to the constituents around the verb, each according to its semantic relationship with the verb.   This verb feature was encoded as 478 binary features (one for each unique verb in the dataset), where the feature value corresponding to the instance?s verb was 1 and all others were 0. 4.3 Additional Features In the present experiments, we utilize gold-standard values for two of the PP features for a proof of feasibility. Future work will evaluate the effect of automatically extracting these features. In addition to the baseline verb feature (feature 1), our full feature set consisted of 8 additional types for a total of 334 features. Examples used in the feature descriptions are pulled from our data.  PP features:  2. Preposition (76 features) The preposition heading the prepositional phrase (e.g. ?Producers shepherded ?Flashdance? [[through]P several scripts]PP.?) was encoded as 76 binary features, one per preposition type in the training data. For instances with multiple PPs, preposition features were extracted from each of the PPs. 3. Function Tag on PP (11 features) Penn Treebank encodes grammatical, adverbial, and other related information on the PP?s POS tag (e.g. ?PP-LOC?). The function tag on the prepositional phrase was encoded as 10 binary features plus an extra feature for PPs without function tags. Again, for instances with multiple PPs, each corresponding function tag feature was set to 1. 4. Complement Category to P (19 features) Normally a PP node consists of a P and a NP. 
However, there are some cases where the complement of the P can be of a different syntactic category (e.g. ?So, view permanent insurance [[for]P [what it is]SBAR]PP.?). Thus, the phrasal category tags (e.g. NP, SBAR) of the preposition?s sister nodes were encoded as 19 binary features. For instances with multiple PPs, all sister nodes of the prepositions were collected.  VerbNet features: The following features were automatically extracted from VerbNet classes with frames matching the target syntactic structure, namely ?NP V NP PP?.  5. VerbNet Classes (123 features) The verbs in the data were associated with one or more of the above VerbNet classes according to their membership. The VerbNet classes were then encoded as 122 binary features with one additional feature for verbs that were not found to be members of any of these classes. If a verb belongs to multiple matching classes, each corresponding feature was set. 6. VerbNet PP Type (27 features) VerbNet frames associate the PP with a description (e.g. ?NP V NP PP.location?). The types were encoded as 26 binary features, plus an extra feature for PPs without a description. The features represented the union of all PP types (i.e. if a VerbNet class included multiple PPs, each of the corresponding features was assigned a value of 1). If a verb was associated with multiple VerbNet classes, the features were set according to the union over both the corresponding classes and their set of PP types.  Named Entity features: These features were automatically annotated using BBN?s IdentiFinder (Bikel, 1999). The feature counts for the subject NP and object NP differ strictly due to what entities were represented in the data. For example, the entity type ?DISEASE? was found in an object NP position but not in a subject NP. 7. NEs for Subject NP (23 features) The union of all named entities under the NP-SBJ node was encoded as 23 binary features.  8. NEs for Object NP (27 features) The union of all named entities under the object NP node was encoded as 27 binary features.  9. NEs for PP?s Object (28 features) The union 
5
 of all named entities under the NP under the PP node was encoded as 28 binary features. 5 Results For the baseline system, the model was built from the training data using a linear kernel and a cost parameter of C=1 (LIBSVM default value). When using the full feature set, the model was also built from the training data using a linear kernel, but the cost parameter was C=0.5, the best value from 10-fold cross validation on the training data.  In Table 1, we report the precision (P), recall (R), F1 score, and accuracy (A) for identifying caused-motion constructions4.  Features P% R% F A% Baseline* Set 78.0 52.0 0.624 79.1 Full Set 87.2 86.0 0.866 91.1 Table 1: System Performance (*verb feature baseline) The results show that the addition of the features presented in section 4.3 resulted in a significant increase in both precision and recall, which in turn boosted the F score from 0.624 to 0.857, an increase of 0.233.  6 Feature Performance In order to determine the usefulness of the individual features in the classification of caused-motion, we evaluated the features in two ways. In one (Table 2), we compared the performance of each of the features to a majority class baseline (i.e. 66.7% accuracy). A useful feature was expected to show an increase over this baseline with statistical significance. Significance of each feature?s performance was evaluated via a chi-squared test (p<0.05).  Our results show that the features 3, 1, 2 and 5 performed significantly better over the majority class baseline. The features 4, 7 and 8 were unable to distinguish between the caused-motion constructions and the non caused-motion usages.                                                 4 As we can see in Table 1, the accuracy is higher than precision or recall. This is because precision and recall are calculated with regard to identifying caused-motion constructions, whereas accuracy is based on identifying both caused-motion and non-caused motion constructions. Since it?s easier to get better performance on the majority class (NON-CM), the overall accuracy is higher.  
Their precision values could not be calculated due to the fact that these features resulted in zero positive (CM) classification.  In a second study, we evaluated the performance of the system when each feature was removed individually from the full set of features (Table 3). The removal of a useful feature was expected to show a statistically significant drop in performance compared to that of the full feature set.  Significance in this performance degradation when compared against the full set of features was evaluated via chi-squared test (p<0.05). Here, features 3, 8 and 1, when removed, showed a statistically significant performance drop. The rest of the features were not shown to have a statistically significant effect on the performance. Our results show that the preposition feature is the single most predictive feature and the feature that has the most significant effect in the full feature set. These results are encouraging: unlike the purely lexical features like the named entity features (6, 7, and 8) that are dependent on the particular expression used in the sentence, 
Table 2:  Effect of each feature on the performance in classification of the caused-motion construction, in the order of decreasing F-score. Features that performed statistically higher than the majority class baseline are marked with an * in the last column.  
# Removed Feature P% R% F A%  3 Preposition 76.9 73.3 0.751 83.8 * 8 NEs for Object NP 84.6 80.7 0.826 88.7 * 1 Verb 85.9 81.3 0.836 89.3 * 2 Function Tag on PP 85.2 84.7 0.849 90.0  9 NEs for PP?s Object 87.5 84.0 0.857 90.7  7 NEs for Subject NP 87.0 84.7 0.858 90.7  5 VerbNet Classes 86.0 86.0 0.860 90.7  4 Comp. Cat. of P 86.7 86.7 0.867 91.1  6 VerbNet PP Type 87.8 86.0 0.869 91.3  Table 3: System performance when the specified feature is removed from the full set of features, in the order of increasing F-score. Significant performance degradation, when compared against the full feature set performance (Table 1) was labeled with an * in the last column. 
# Included Feature P% R% F A%  3 Preposition 82.4 65.3 0.729 83.8 * 1 Verb  78.0 52.0 0.624 79.1 * 2 Function Tag on PP 82.6 38.0 0.521 76.7 * 5 VerbNet Classes 73.5 33.3 0.459 73.8 * 6 VerbNet PP Type 59.6 33.3 0.427 70.2  9 NEs for PP?s Object 71.4 6.7 0.122 68.0  4 Comp. Cat. of P   0.0  66.7  7 NEs for Subject NP  0.0  66.7  8 NEs for Object NP  0.0  66.7  
6
 prepositions are function words. Like syntactic elements, these function words also contribute to the patterned structures of a construction as discussed in Section 3. Furthermore, unlike the semantics of features that are dependent on content words that are subject to lexical variability, prepositions are limited in their lexical variability, which make them good general features that scale well across different semantic domains. In addition to the preposition feature, the verb feature was found to affect performance at a statistically significant level in both cases. Based on the numerous studies in the past that have shown the usefulness of the verb as a feature, this is not an unexpected result. Interestingly, our results seem to indicate interactions between features. This can be seen in two different instances. First, while feature 8 (NEs for Object NP) alone was not found to be a predictive feature, when removed, it resulted in a statistically significant drop in performance compared to that of the full feature set. The opposite effect can be seen with the VerbNet Classes feature. While it showed a statistically significant boost in performance when introduced into the system by itself, when dropped from the full feature set, the drop in the system performance was not found to be significant. This seems to indicate that NEs for Object NP and the VerbNet Classes features have strong interactions with one or more of the other features. We will continue investigating these interactions in future work. 7 Out-of-Vocabulary Verbs Additionally, we separately examined the performance on the test set verbs that were not seen in the training data (i.e. out-of-vocabulary/OOV items).  Just over a fifth of the instances (92 out of 450 constructions) in the test data had unseen verbs, with a total of 83 unique verb types. The results show that there was no decrease in the accuracy or F-score. In fact, there was a chance increase, not statistically significant, in a two-sample t-test (t=1.13; p>0.2).  We carried out the same feature studies for the OOV verbs, as detailed in section 6 (Tables 4 and 5). The performance in both of the studies reflected the results seen in Tables 2 and 3, with one expected exception. The verb feature was, of course, found to be of no value to the predictor. 
What is interesting here is that the verb feature did perform at a significant level for the full test data. By this observation, it would be expected that the overall performance on the OOV verbs would be negatively affected since there is no available verb information. However, this was not the case. 8 Discussion and Conclusion  The results presented show that a classifier can be trained to automatically identify the semantics of constructions; at least for the caused-motion construction, and that it can do this with high accuracy. Furthermore, we have determined that the preposition feature is the most useful feature when identifying caused-motion constructions. Moreover, in considering our results in light of the performance of the SRL systems (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005), where unseen predicates result in significant performance degradation, we found in contrast that using CxG to inform semantics resulted in equally high performance on the out-of-vocabulary predicates. This serves as evidence that semantic 
Table 4: Effect of each feature on the performance in classification of the caused-motion construction with OOV verbs, in the order of decreasing F-score. The precision values could not be calculated for the performance of the features 1,4,7, and 8 due to the fact that these features resulted in zero positive classifications. 
# Removed Feature P% R% F A% 3 Preposition 63 76 0.69 90 2 Function Tag on PP 83 80 0.82 82 6 VerbNet PP Type 84 84 0.84 67 5 VerbNet Classes 84 84 0.84 73 9 NEs for PP?s Object 84 84 0.84 74 1 Verb  0  73 4 Comp. Cat. of P  0  73 7 NEs for Subject NP  0  73 8 NEs for Object NP  0  73 
# Removed Feature P% R% F A% 3 Preposition 63 76 0.69 82 8 NEs for Object NP 83 80 0.82 90 2 Function Tag on PP 84 84 0.84 91 5 VerbNet Classes 84 84 0.84 91 7 NEs for Subject NP 84 84 0.84 91 1 Verb 88 88 0.88 93 4 Comp. Cat. of P 88 88 0.88 93 6 VerbNet PP Type 92 88 0.90 95 9 NEs for PP?s Object 92 88 0.90 95 Table 5: System performance when the specified feature is removed from the full set of features in the classification of constructions with OOV items, in the order of increasing F-score. 
7
 analysis of novel lexical combinations and unseen verbs can be improved by enriching semantics with a construction-level analysis. 9 Future Work There are several directions to go from here. First, in this paper we have kept our study within the scope of caused-motion constructions. We intend to introduce more types of constructions and include more syntactic variation in our data.  We will also add more annotated instances. Secondly, we examine the impact of the introduction of additional features, such as a bag-of-words feature. In particular, we will include semantic features based on FrameNet to the VerbNet semantic features we are already using.  This will be more feasible once the SemLink semantic role labeler for FrameNet becomes available (Palmer, 2009). Finally, we plan to include a more detailed analysis of the feature interactions, and examine the benefit that a construction grammar perspective might add to our semantic analysis. Acknowledgements We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No. HR0011-06-C-0022, subcontract from BBN, Inc. We are also grateful to Laura Michaelis for helpful discussions and comments. References  Agirre, Eneko and Philip Edmonds. 2006. Introduction. In Word Sense Disambiguation: Algorithms and Applications, Agirre and Edmonds (eds.), Springer. Ben-David, Shai, Blitzer, John, Crammer, Koby  Pereira, Fernando. 2006. 'Analysis of representations for domain adaptation', in NIPS. Bikel, D., Schwartz, R., Weischedel, R.  1999.  An algorithm that learns what?s in a name.  Machine Learning: Special Issue on NL Learning, 34, 1-3. Carreras, Xavier and Lluis Marquez. 2005. Introduction to the CoNLL- 2005 shared task: Semantic role labeling. Procs of CoNLL- 2005.  Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm Gildea, Daniel and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics 28:3, 245-288. 
Fillmore, Charles J., Christopher R. Johnson and Miriam R.L. Petruck (2003) Background to Framenet, International Journal of Lexicography, Vol 16.3: 235-250. Fillmore, Charles, Paul Kay and Catherine O'Connor (1988). Regularity and Idiomaticity in Grammatical Constructions: The Case of let alne. Language 64: 501-38. Giuglea, Ana-Maria and Alessandro Moschitti. 2006. Shallow semantic parsing based on FrameNet, Verb-Net and PropBank. In Proceedings of the 17th European Conference on Artificial Intelligence, Riva del Garda, Italy. Goldberg, Adele E. 2006. Constructions at work. The nature of generalization in language. Oxford: Oxford University Press Goldberg, Adele. E. 1995. Constructions: A construction grammar approach to argument structure. Chicago: University of Chicago Press. Hovy, Edward H., Mitch Marcus, Martha Palmer, Sameer Pradhan, Lance Ramshaw, and Ralph M. Weischedel. 2006. OntoNotes: The 90% Solution. Short paper. Proceedings of the Human Language Technology / North American Association of Computational Linguistics conference (HLT-NAACL 2006). pp. 57-60, New York, NY. Kay, Paul. 2002. English Subjectless Tag Sentences. Language 78: 453-81. Kipper-Schuler, Karin. 2005. VerbNet: A broad coverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylvania. Levin, Beth. 1993. English Verb Classes and Alternations: A Preliminary Investigation, University of Chicago Press, Chicago, IL. Michaelis, Laura A. (2004). Type Shifting in Construction Grammar: An Integrated Approach to Aspectual Coercion. Cognitive Linguistics 15: 1-67. Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, California, 40?47.  Marcus, Mitchell P, Santorini, Beatrice, Marcinkiewicz, Mary A. (1994) "Building a large annotated corpus of English: the Penn Treebank" Computational Linguistics 19: 313-330. Palmer, Martha. "Semlink: Linking PropBank, VerbNet and FrameNet." Proceedings of the Generative Lexicon Conference. Sept. 2009, Pisa, Italy: GenLex-09, 2009. Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71?106. 
8
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 64?72,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
To Annotate More Accurately or to Annotate More
Dmitriy Dligach
Department of Computer Science
University of Colorado at Boulder
Dmitriy.Dligach@colorado.edu
Rodney D. Nielsen
The Center for Computational Language
and Education Research
University of Colorado at Boulder
Rodney.Nielsen@colorado.edu
Martha Palmer
Department of Linguistics
Department of Computer Science
University of Colorado at Boulder
Martha.Palmer@colorado.edu
Abstract
The common accepted wisdom is that
blind double annotation followed by adju-
dication of disagreements is necessary to
create training and test corpora that result
in the best possible performance. We pro-
vide evidence that this is unlikely to be the
case. Rather, the greatest value for your
annotation dollar lies in single annotating
more data.
1 Introduction
In recent years, supervised learning has become
the dominant paradigm in Natural Language Pro-
cessing (NLP), thus making the creation of hand-
annotated corpora a critically important task. A
corpus where each instance is annotated by a sin-
gle tagger unavoidably contains errors. To im-
prove the quality of the data, an annotation project
may choose to annotate each instance twice and
adjudicate the disagreements, thus producing the
(largely) error-free gold standard. For example,
OntoNotes (Hovy et al, 2006), a large-scale an-
notation project, chose this option.
However, given a virtually unlimited supply of
unlabeled data and limited funding ? a typical set
of constraints in NLP ? an annotation project must
always face the realization that for the cost of dou-
ble annotation, more than twice as much data can
be single annotated. The philosophy behind this
alternative says that modern machine learning al-
gorithms can still generalize well in the presence
of noise, especially when given larger amounts of
training data.
Currently, the commonly accepted wisdom
sides with the view that says that blind double
annotation followed by adjudication of disagree-
ments is necessary to create annotated corpora that
leads to the best possible performance. We pro-
vide empirical evidence that this is unlikely to be
the case. Rather, the greatest value for your an-
notation dollar lies in single annotating more data.
There may, however, be other considerations that
still argue in favor of double annotation.
In this paper, we also consider the arguments of
Beigman and Klebanov (2009), who suggest that
data should be multiply annotated and then filtered
to discard all of the examples where the annota-
tors do not have perfect agreement. We provide
evidence that single annotating more data for the
same cost is likely to result in better system per-
formance.
This paper proceeds as follows: first, we out-
line our evaluation framework in Section 2. Next,
we compare the single annotation and adjudica-
tion scenarios in Section 3. Then, we compare
the annotation scenario of Beigman and Klebanov
(2009) with the single annotation scenario in Sec-
tion 4. After that, we discuss the results and future
work in section 5. Finally, we draw the conclusion
in Section 6.
2 Evaluation
2.1 Data
For evaluation we utilize the word sense data an-
notated by the OntoNotes project. The OntoNotes
data was chosen because it utilizes full double-
blind annotation by human annotators and the dis-
agreements are adjudicated by a third (more expe-
64
rienced) annotator. This allows us to
? Evaluate single annotation results by using
the labels assigned by the first tagger
? Evaluate double annotation results by using
the labels assigned by the second tagger
? Evaluate adjudication results by using the la-
bels assigned by the the adjudicator to the in-
stances where the two annotators disagreed
? Measure the performance under various sce-
narios against the double annotated and adju-
dicated gold standard data
We selected the 215 most frequent verbs in the
OntoNotes data. To make the size of the dataset
more manageable, we randomly selected 500 ex-
amples of each of the 15 most frequent verbs. For
the remaining 200 verbs, we utilized all the an-
notated examples. The resulting dataset contained
66,228 instances of the 215 most frequent verbs.
Table 1 shows various important characteristics of
this dataset averaged across the 215 verbs.
Inter-tagger agreement 86%
Annotator1-gold standard agreement 93%
Share of the most frequent sense 70%
Number of classes (senses) per verb 4.74
Table 1: Data used in evaluation at a glance
2.2 Cost of Annotation
Because for this set of experiments we care pri-
marily about the cost effectiveness of the annota-
tion dollars, we need to know how much it costs
to blind annotate instances and how much it costs
to adjudicate disagreements in instances. There is
an upfront cost associated with any annotation ef-
fort to organize the project, design an annotation
scheme, set up the environment, create annotation
guidelines, hire and train the annotators, etc. We
will assume, for the sake of this paper, that this
cost is fixed and is the same regardless of whether
the data is single annotated or the data is double
annotated and disagreements adjudicated.
In this paper, we focus on a scenario where there
is essentially no difference in cost to collect ad-
ditional data to be annotated, as is often the case
(e.g., there is virtually no additional cost to down-
load 2.5 versus 1.0 million words of text from the
web). However, this is not always the case (e.g.,
collecting speech can be costly).
To calculate a cost per annotated instance for
blind annotation, we take the total expenses asso-
ciated with the annotators in this group less train-
ing costs and any costs not directly associated with
annotation and divide by the total number of blind
instance annotations. This value, $0.0833, is the
per instance cost used for single annotation. We
calculated the cost for adjudicating instances sim-
ilarly, based on the expenses associated with the
adjudication group. The adjudication cost is an ad-
ditional $0.1000 per instance adjudicated. The per
instance cost for double blind, adjudicated data is
then computed as double the cost for single an-
notation plus the per instance cost of adjudication
multiplied by the percent of disagreement, 14%,
which is $0.1805.
We leave an analysis of the extent to which the
up front costs are truly fixed and whether they can
be altered to result in more value for the dollar to
future work.
2.3 Automatic Word Sense Disambiguation
For the experiments we conduct in this study, we
needed a word sense disambiguation (WSD) sys-
tem. Our WSD system is modeled after the state-
of-the-art verb WSD system described in (Dligach
and Palmer, 2008). We will briefly outline it here.
We view WSD as a supervised learning prob-
lem. Each instance of the target verb is represented
as a vector of binary features that indicate the pres-
ence (or absence) of the corresponding features in
the neighborhood of the target verb. We utilize
all of the linguistic features that were shown to be
useful for disambiguating verb senses in (Chen et
al., 2007).
To extract the lexical features we POS-tag
the sentence containing the target verb and the
two surrounding sentences using MXPost soft-
ware (Ratnaparkhi, 1998). All open class words
(nouns, verbs, adjectives, and adverbs) in these
sentences are included in our feature set. In addi-
tion to that, we use as features two words on each
side of the target verb as well as their POS tags.
To extract the syntactic features we parse the
sentence containing the target verb with Bikel?s
constituency parser and utilize a set of rules to
identify the features in Table 2.
Our semantic features represent the semantic
classes of the target verb?s syntactic arguments
65
Feature Explanation
Subject and object - Presence of subject and
object
- Head word of subject
and object NPs
- POS tag of the head
word of subject and
object NPs
Voice - Passive or Active
PP adjunct - Presence of PP adjunct
- Preposition word
- Head word of the
preposition?s NP
argument
Subordinate clause - Presence of subordinate
clause
Path - Parse tree path from
target verb to neighboring
words
- Parse tree path from
target verb to subject and
object
- Parse tree path from
target verb to subordinate
clause
Subcat frame - Phrase structure rule
expanding the target
verb?s parent node in
parse tree
Table 2: Syntactic features
such as subject and object. The semantic classes
are approximated as
? WordNet (Fellbaum, 1998) hypernyms
? NE tags derived from the output of Identi-
Finder (Bikel et al, 1999)
? Dynamic dependency neighbors (Dligach
and Palmer, 2008), which are extracted in an
unsupervised way from a dependency-parsed
corpus
Our WSD system uses the Libsvm software
package (Chang and Lin, 2001) for classification.
We accepted the default options (C = 1 and lin-
ear kernel) when training our classifiers. As is the
case with most WSD systems, we train a separate
model per verb.
3 Experiment One
The results of experiment one show that in these
circumstances, better performance is achieved by
single annotating more data than by deploing re-
sources towards ensuring that the data is annotated
more accurately through an adjudication process.
3.1 Experimental Design
We conduct a number of experiments to compare
the effect of single annotated versus adjudicated
data on the accuracy of a state of the art WSD sys-
tem. Since OntoNotes does not have a specified
test set, for each word, we used repeated random
partitioning of the data with 10 trials and 10% into
the test set and the remaining 90% comprising the
training set.
We then train an SVM classifier on varying frac-
tions of the data, based on the number of examples
that could be annotated per dollar. Specifically,
in increments of $1.00, we calculate the number
of examples that can be single annotated and the
number that can be double blind annotated and ad-
judicated with that amount of money.
The number of examples computed for single
annotation is selected at random from the train-
ing data. Then the adjudicated examples are se-
lected at random from this subset. Selecting from
the same subset of data approaches pair statisti-
cal testing and results in a more accurate statistical
comparison of the models produced.
Classifiers are trained on this data using the la-
bels from the first round of annotation as the single
annotation labels and the final adjudicated labels
for the smaller subset. This procedure is repeated
ten times and the average results are reported.
For a given verb, each classifier created
throughout this process is tested on the same dou-
ble annotated and adjudicated held-out test set.
3.2 Results
Figure 1 shows a plot of the accuracy of the clas-
sifiers relative to the annotation investment for a
typical verb, to call. As can be seen, the accu-
racy is always higher when training on the larger
amount of single annotated data than when train-
ing on the amount of adjudicated data that had the
equivalent cost of annotation.
Figures 2 and 3 present results averaged over
all 215 verbs in the dataset. First, figure 2 shows
the average accuracy over all verbs by amount in-
vested. These accuracy curves are not smooth be-
66
Figure 1: Performance of single annotated vs. ad-
judicated data by amount invested for to call
cause the verbs all have a different number of total
instances. At various annotation cost values, all of
the instances of one or more verbs will have been
annotated. Hence, the accuracy values might jump
or drop by a larger amount than seen elsewhere in
the graph.
Toward the higher dollar amounts the curve is
dominated by fewer and fewer verbs. We only
display the dollar investments of up to $60 due to
the fact that only five verbs have more than $60?s
worth of instances in the training set.
Figure 2: Average performance of single anno-
tated vs. adjudicated data by amount invested
The average difference in accuracy for Figure 2
across all amounts of investment is 1.64%.
Figure 3 presents the average accuracy relative
to the percent of the total cost to single annotate
all of the instances for a verb. The accuracy at a
given percent of total investment was interpolated
for each verb using linear interpolation and then
averaged over all of the verbs.
Figure 3: Average performance of single anno-
tated vs. adjudicated data by fraction of total in-
vestment
The average difference in accuracy for Figure 3
across each percent of investment is 2.10%.
Figure 4 presents essentially the same informa-
tion as Figure 2, but as a reduction in error rate for
single annotation relative to full adjudication.
Figure 4: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 2
The relative reduction in error rate averaged
over all investment amounts in Figure 2 is 7.77%.
Figure 5 presents the information in Figure 3
as a reduction in error rate for single annotation
relative to full adjudication.
The average relative reduction in error rate over
the fractions of total investment in Figure 5 is
9.32%.
67
Figure 5: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 3
3.3 Discussion
First, it is worth noting that, when the amount of
annotated data is the same for both scenarios, ad-
judicated data leads to slightly better performance
than single annotated data. For example, consider
Figure 3. The accuracy at 100% of the total invest-
ment for the double annotation and adjudication
scenario is 81.13%. The same number of exam-
ples can be single annotated for 0.0833 / 0.1805 =
0.4615 of this dollar investment (using the costs
from Section 2.2). The system trained on that
amount of single annotated data shows a lower ac-
curacy, 80.21%. Thus, in this case, the adjudica-
tion scenario brings about a performance improve-
ment of about 1%.
However, the main thesis of this paper is that in-
stead of double annotating and adjudicating, it is
often better to single annotate more data because
it is a more cost-effective way to achieve a higher
performance. The results of our experiments sup-
port this thesis. At every dollar amount invested,
our supervised WSD system performs better when
trained on single annotated data comparing to dou-
ble annotated and adjudicated data.
The maximum annotation investment amount
for each verb is the cost of single annotating all
of its instances. When the system is trained on
the amount of double annotated data possible at
this investment, its accuracy is 81.13% (Figure 3).
When trained on single annotated data, the system
attains the same accuracy much earlier, at approxi-
mately 60% of the total investment. When trained
on the entire available single annotated data, the
system reaches an accuracy of 82.99%, nearly a
10% relative reduction in error rate over the same
system trained on the adjudicated data obtained for
the same cost.
Averaged over the 215 verbs, the single anno-
tation scenario outperformed adjudication at every
dollar amount investigated.
4 Experiment Two
In this experiment, we consider the arguments of
Beigman and Klebanov (2009). They suggest that
data should be at least double annotated and then
filtered to discard all of the examples where there
were any annotator disagreements.
The main points of their argument are as fol-
lows. They first consider the data to be dividable
into two types, easy (to annotate) cases and hard
cases. Then they correctly note that some anno-
tators could have a systematic bias (i.e., could fa-
vor one label over others in certain types of hard
cases), which would in turn bias the learning of
the classifier. They show that it is theoretically
possible that a band of misclassified hard cases
running parallel to the true separating hyperplane
could mistakenly shift the decision boundary past
up to
?
N easy cases.
We suggest that it is extremely unlikely that a
consequential number of easy cases would exist
nearer to the class boundary than the hard cases.
The hard cases are in fact generally considered to
define the separating hyperplane.
In this experiment, our goal is to determine how
the accuracy of classifiers trained on data labeled
according to Beigman and Klebanov?s discard dis-
agreements strategy compares empirically to the
accuracy resulting from single annotated data. As
in the previous experiment, this analysis is per-
formed relative to the investment in the annotation
effort.
4.1 Experimental Design
We follow essentially the same experimental de-
sign described in section 3.1, using the same state
of the art verb WSD system. We conduct a num-
ber of experiments to compare the effect of single
annotated versus double annotated data. We uti-
lized the same training and test sets as the previous
experiment and similarly trained an SVM on frac-
tions of the data representing increments of $1.00
investments.
As before, the number of examples designated
68
for single annotation is selected at random from
the training data and half of that subset is selected
as the training set for the double annotated data.
Again, selecting from the same subset of data re-
sults in a more accurate statistical comparison of
the models produced.
Classifiers for each annotation scenario are
trained on the labels from the first round of an-
notation, but examples where the second annota-
tor disagreed are thrown out of the double anno-
tated data. This results in slightly less than half as
much data in the double annotation scenario based
on the disagreement rate. Again, the procedure is
repeated ten times and the average results are re-
ported.
For a given verb, each classifier created
throughout this process is tested on the same dou-
ble annotated and adjudicated held-out test set.
4.2 Results
Figure 6 shows a plot of the accuracy of the classi-
fiers relative to the annotation investment for a typ-
ical verb, to call. As can be seen, the accuracy for
a specific investment performing single annotation
is always higher than it is for the same investment
in double annotated data.
Figure 6: Performance of single annotated vs.
double annotated data with disagreements dis-
carded by amount invested for to call
Figures 7 and 8 present results averaged over
all 215 verbs in the dataset. First, figure 7 shows
the average accuracy over all verbs by amount
invested. Again, these accuracy curves are not
smooth because the verbs all have a different num-
ber of total instances. Hence, the accuracy val-
ues might jump or drop by a larger amount at the
points where a given verb is no longer included in
the average.
Toward the higher dollar amounts the curve is
dominated by fewer and fewer verbs. As before,
we only display the results for investments of up
to $60.
The average difference in accuracy for Figure 7
across all amounts of investment is 2.32%.
Figure 8 presents the average accuracy relative
to the percent of the total cost to single annotate
all of the instances for a verb. The accuracy at
a given percent of total investment was interpo-
lated for each verb and then averaged over all of
the verbs.
Figure 7: Average performance of single anno-
tated vs. double annotated data with disagree-
ments discarded by amount invested
Figure 8: Average performance of single anno-
tated vs. adjudicated data by fraction of total in-
vestment
The average difference in accuracy for Figure 8
across all amounts of investment is 2.51%.
69
Figures 9 and 10 present this information as a
reduction in error rate for single annotation rela-
tive to full adjudication.
Figure 9: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 7
The relative reduction in error rate averaged
over all investment amounts in Figure 9 is 10.88%.
Figure 10: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 8
The average relative reduction in error rate over
the fractions of total investment in Figure 10 is
10.97%.
4.3 Discussion
At every amount of investment, our supervised
WSD system performs better when trained on sin-
gle annotated data comparing to double annotated
data with discarded cases of disagreements.
The maximum annotation investment amount
for each verb is the cost of single annotating all
of its instances. When the system is trained on
the amount of double annotated data possible at
this investment, its accuracy is 80.78% (Figure 8).
When trained on single annotated data, the system
reaches the same accuracy much earlier, at approx-
imately 52% of the total investment. When trained
on the entire available single annotated data, the
system attains an accuracy of 82.99%, an 11.5%
relative reduction in error rate compared to the
same system trained on the double annotated data
obtained for the same cost.
The average accuracy of the single annotation
scenario outperforms the double annotated with
disagreements discarded scenario at every dollar
amount investigated.
While this empirical investigation only looked
at verb WSD, it was performed using 215 distinct
verb type datasets. These verbs each have con-
textual features that are essentially unique to that
verb type and consequently, 215 distinct classi-
fiers, one per verb type, are trained. Hence, these
could loosely be considered 215 distinct annota-
tion and classification tasks.
The fact that for the 215 classification tasks the
single annotation scenario on average performed
better than the discard disagreements scenario of
Beigman and Klebanov (2009) strongly suggests
that, while it is theoretically possible for annota-
tion bias to, in turn, bias a classifier?s learning, it
is more likely that you will achieve better results
by training on the single annotated data.
It is still an open issue whether it is generally
best to adjudicate disagreements in the test set or
to throw them out as suggested by (Beigman Kle-
banov and Beigman, 2009).
5 Discussion and Future Work
We investigated 215 WSD classification tasks,
comparing performance under three annotation
scenarios each with the equivalent annotation cost,
single annotation, double annotation with dis-
agreements adjudicated, and double annotation
with disagreements discarded. Averaging over the
215 classification tasks, the system trained on sin-
gle annotated data achieved 10.0% and 11.5% rel-
ative reduction in error rates compared to training
on the equivalent investment in adjudicated and
disagreements discarded data, respectively. While
we believe these results will generalize to other an-
notation tasks, this is still an open question to be
determined by future work.
70
There are probably similar issues in what were
considered fixed costs for the purposes of this pa-
per. For example, it may be possible to train fewer
annotators, and invest the savings into annotating
more data. Perhaps more appropriately, it may be
feasible to simply cut back on the amount of train-
ing provided per annotator and instead annotate
more data.
On the other hand, when the unlabeled data
is not freely obtainable, double annotation may
be more suitable as a route to improving system
performance. There may also be factors other
than cost-effectiveness which make double anno-
tation desirable. Many projects point to their ITA
rates and corresponding kappa values as a mea-
sure of annotation quality, and of the reliability of
the annotators (Artstein and Poesio, 2008). The
OntoNotes project used ITA rates as a way of eval-
uating the clarity of the sense inventory that was
being developed in parallel with the annotation.
Lexical entries that resulted in low ITA rates were
revised, usually improving the ITA rate. Calculat-
ing these rates requires double-blind annotation.
Annotators who consistently produced ITA rates
lower than average were also removed from the
project. Therefore, caution is advised in determin-
ing when to dispense with double annotation in fa-
vor of more cost effective single annotation.
Double annotation can also be used to shed light
on other research questions that, for example, re-
quire knowing which instances are ?hard.? That
knowledge may help with designing additional,
richer annotation layers or with cognitive science
investigations into human representations of lan-
guage.
Our results suggest that systems would likely
benefit more from the larger training datasets that
single annotation makes possible than from the
less noisy datasets resulting from adjudication.
Regardless of whether single or double annota-
tion with adjudication is used, there will always be
noise. Hence, we see the further investigation of
algorithms that generalize despite the presence of
noise to be critical to the future of computational
linguistics. Humans are able to learn in the pres-
ence of noise, and our systems must follow suit.
6 Conclusion
Double annotated data contains less noise than
single annotated data and thus improves the per-
formance of supervised machine learning systems
that are trained on a specific amount of data. How-
ever, double annotation is expensive and the alter-
native of single annotating more data instead is on
the table for many annotation projects.
In this paper we compared the performance of
a supervised machine learning system trained on
double annotated data versus single annotated data
obtainable for the same cost. Our results clearly
demonstrate that single annotating more data can
be a more cost-effective way to improve the sys-
tem performance in the many cases where the un-
labeled data is freely available and there are no
other considerations that necessitate double anno-
tation.
7 Acknowledgements
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In ACL-IJCNLP
?09: Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1, pages 280?287, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
put. Linguist., 35(4):495?503.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3):211?231.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
71
J. Chen and M. Palmer. 2005. Towards robust high
performance word sense disambiguation of english
verbs using rich linguistic features. pages 933?944.
Springer.
Jinying Chen, Dmitriy Dligach, and Martha Palmer.
2007. Towards large-scale high-performance en-
glish verb sense disambiguation by using linguisti-
cally motivated features. In ICSC ?07: Proceed-
ings of the International Conference on Semantic
Computing, pages 378?388, Washington, DC, USA.
IEEE Computer Society.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
HLT ?08: Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies, pages 29?32, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Christiane Fellbaum. 1998. WordNet: An electronic
lexical database. MIT press Cambridge, MA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57?60, Morristown, NJ, USA. Association for
Computational Linguistics.
A. Ratnaparkhi. 1998. Maximum entropy models for
natural language ambiguity resolution. Ph.D. the-
sis, University of Pennsylvania.
72
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 82?90,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PropBank Annotation of Multilingual Light Verb Constructions 
 
 
Jena D. Hwang1, Archna Bhatia3, Clare Bonial1, Aous Mansouri1,  
Ashwini Vaidya1, Nianwen Xue2, and Martha Palmer1 
1Department of Linguistics, University of Colorado at Boulder, Boulder CO 80309 
2Department of Computer Science, Brandeis University, Waltham MA 02453 
3Department of Linguistics, University of Illinois at Urbana-Champaign, Urbana IL 61801 
{hwangd,claire.bonial,aous.mansouri,ashwini.vaidya,martha.palmer} 
@colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu 
 
  
 
Abstract 
In this paper, we have addressed the task 
of PropBank annotation of light verb 
constructions, which like multi-word 
expressions pose special problems. To 
arrive at a solution, we have evaluated 3 
different possible methods of annotation. 
The final method involves three passes: 
(1) manual identification of a light verb 
construction, (2) annotation based on the 
light verb construction?s Frame File, and 
(3) a deterministic merging of the first 
two passes. We also discuss how in 
various languages the light verb 
constructions are identified and can be 
distinguished from the non-light verb 
word groupings.  
1 Introduction  
One of the aims in natural language processing, 
specifically the task of semantic role labeling 
(SRL), is to correctly identify and extract the 
different semantic relationships between words 
in a given text. In such tasks, verbs are 
considered important, as they are responsible for 
assigning and controlling the semantic roles of 
the arguments and adjuncts around it. Thus, the 
goal of the SRL task is to identify the arguments 
of the predicate and label them according to their 
semantic relationship to the predicate (Gildea 
and Jurafsky, 2002; Pradhan et al, 2003).  
To this end, PropBank (Palmer et. al., 2005) 
has developed semantic role labels and labeled 
large corpora for training and testing of 
supervised systems. PropBank identifies and 
labels the semantic arguments of the verb on a 
verb-by-verb basis, creating a separate Frame 
File that includes verb specific semantic roles to 
account for each subcategorization frame of the 
verb. It has been shown that training supervised 
systems with PropBank?s semantic roles for 
shallow semantic analysis yield good results (see 
CoNLL 2005 and 2008).  
However, semantic role labeling tasks are 
often complicated by multiword expressions 
(MWEs) such as idiomatic expressions (e.g., 
?Stop pulling my leg!?), verb particle 
constructions (e.g., ?You must get over your 
shyness.?), light verb constructions (e.g., ?take a 
walk?, ?give a lecture?), and other complex 
predicates (e.g., V+V predicates such as Hindi?s 
???? ??? nikal gayaa, lit. ?exit went?, means 
?left? or ?departed?). MWEs that involve verbs 
are especially challenging because the 
subcategorization frame of the predicate is no 
longer solely dependent on the verb alone. 
Rather, in many of these cases the argument 
structure is assigned by the union of two 
predicating elements. Thus, it is important that 
the manual annotation of semantic roles, which 
will be used by automatic SRL systems, define 
and label these MWEs in a consistent and 
effective manner. 
In this paper we focus on the PropBank 
annotation of light verb constructions (LVCs). 
We have developed a multilingual schema for 
annotating LVCs that takes into consideration the 
similarities and differences shared by the 
construction as it appears in English, Arabic, 
Chinese, and Hindi. We also discuss in some 
detail the practical challenges involved in the 
crosslinguistic analysis of LVCs, which we hope 
will bring us a step closer to a unified 
crosslinguistic analysis.    
Since NomBank, as a companion to 
PropBank, provides corresponding semantic role 
82
labels for noun predicates (Meyers et al, 2004), 
we would like to take advantage of NomBank?s 
existing nominalization Frame Files and 
annotations as much as possible.  A question that 
we must therefore address is, ?Are 
nominalization argument structures exactly the 
same whether or not they occur within an LVC?? 
as will be discussed in section 6.1. 
2 Identifying Light Verb Constructions 
Linguistically LVCs are considered a type of a 
complex predicate. Many studies from differing 
angles and frameworks have characterized 
complex predicates as a fusion of two or more 
predicative elements. For example, Rosen (1997) 
treats complex structures as complementation 
structures, where the argument structure of 
elements in a complex predicate are fused 
together.  Goldberg (1993) takes a constructional 
approach to complex predicates and arrives at an 
analysis that is comparable to viewing complex 
predicates as a single lexical item. Similarly, 
Mohanan (1997) assumes different levels of 
linguistic representation for complex predicates 
in which the elements, such as the noun and the 
light verb, functionally combine to give a single 
clausal nucleus. Alsina (1997) and Butt (1997) 
suggest that complex predicates may be formed 
by syntactically independent elements whose 
argument structures are brought together by a 
predicate composition mechanism.  
While there is no clear-cut definition of LVCs, 
let alne the whole range of complex predicates, 
for the purposes of this study, we have adapted 
our approach largely from Butt?s (2004) criteria 
for defining LVCs. LVCs are characterized by a 
light verb and a predicating complement 
(henceforth, true predicate) that ?combine to 
predicate as a single element.? (Ibid.) In LVC, 
the verb is considered semantically bleached in 
such a way that the verb does not hold its full 
predicating power. Thus, the light verb plus its 
true predicate can often be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression. For example, 
the light verb ?gave? and the predicate ?lecture? 
in ?gave a lecture?, together form a single 
predicating unit such that it can be paraphrased 
by ?lectured?. 
True predicates in LVCs can be a noun (the 
object of the verb or the object of the preposition 
in a prepositional phrase), an adjective, or a verb. 
One light verb plus true predicate combination 
found commonly across all our PropBank 
languages (i.e., English, Arabic, Chinese, and 
Hindi) is the noun as the object of the verb as in 
?Sara took [a stroll] along the beach?. In Hindi, 
true predicates can be adjectives or verbs, in 
addition to the nouns. 
??? ? ??? [?????]  ???         (Adjective) 
to-me  you [nice]  seem 
lit. ?You seem nice to me? 
'You (are) liked to me (=I like you).' 
?????  ?? ???  [??] ????   (Verb) 
I-ERG everything  [do] took 
lit. ?I took do everything? 
'I have done everything.' 
As for Arabic, the LVCs come in verb+noun 
pairings. However, they surface in two syntactic 
forms. It can either be the object of the verb just 
like in English: 
 
 ???? ????]?????? ] ????? ??  
gave.he Georges [lecture] PREP Lebanon 
lit.'Georges gave a lecture about Lebanon' 
?Georges lectured about Lebanon? 
or the complement can be the object of a 
preposition: 
 
 ??????]????? ]????? ????? 
conduct.I [PREP-visit] our.saint Ilias 
lit. ?I will conduct with visit Saint Ilias?s? 
?I will visit Saint Ilias?s? 
3 Standard PropBank  
Annotation Procedure 
The PropBank annotation process can be broken 
down into two major steps: creation of the Frame 
Files for verbs occurring in the data and 
annotation of the data using the Frame Files. 
During the creation of the Frame Files, the 
usages of the verbs in the data are examined by 
linguists (henceforth, ?framers?). Based on these 
observations, the framers create a Frame File for 
each verb containing one or more framesets, 
which correspond to coarse-grained senses of the 
predicate lemma. Each frameset specifies the 
PropBank labels (i.e., ARG0, ARG1,?ARG5) 
corresponding to the argument structure of the 
verb. Additionally, illustrative examples are 
included for each frameset, which will later be 
referenced by the annotators. These examples 
also include the use of the ARGM labels. 
Thus, the framesets are based on the 
examination of the data, the framers? linguistic 
knowledge and native-speaker intuition. At 
83
times, we also make use of the syntactic and 
semantic behavior of the verb as described by 
certain lexical resources. These resources include 
VerbNet (Kipper et. al., 2006) and FrameNet 
(Baker et. al., 1998) for English, a number of 
monolingual and bilingual dictionaries for 
Arabic, and Hindi WordNet and DS Parses 
(Palmer et. al., 2009) for Hindi. Additionally, if 
available, we consult existing framesets of words 
with similar meanings across different languages. 
The data awaiting annotation are passed onto 
the annotators for a double-blind annotation 
process using the previously created framesets. 
The double annotated data is then adjudicated by 
a third annotator, during which time the 
differences of the two annotations are resolved to 
produce the Gold Standard. 
Two major guiding considerations during the 
framing and annotating process are data 
consistency and annotator productivity. During 
the frameset creation process, verbs that share 
similar semantic and syntactic characteristics are 
framed similarly. During the annotation process, 
the data is organized by verbs so that each verb is 
tackled all at once. In doing so, we firstly ensure 
that the framesets of similar verbs, and in turn, 
the annotation of the verbs, will both be 
consistent across the data. Secondly, by tackling 
annotation on verb-by-verb basis, the annotators 
are able to concentrate on a single verb at a time, 
making the process easier and faster for the 
annotators. 
4 Annotating LVC 
A similar process must be followed when 
annotating light verb constructions The first step 
is to create consistent Frame Files for light verbs. 
Then in order to make the annotation process 
produce consistent data at a reasonable speed, we 
have decided to carry out the light verb 
annotation in three passes (Table 1):  (1) annotate 
the light verb, (2) annotate the true predicate, and 
(3) merge the two annotations into one. 
The first pass involves the identification of the 
light verb. The most important parts of this step 
are to identify a verb as having bleached 
meaning, thereafter assign a generic light verb 
frameset and identify the true predicating 
expression of the sentence, which would be 
marked with ARG-PRX (i.e., ARGument-
PRedicating eXpression). For English, for 
example, annotators were instructed to use Butt?s 
(2004) criteria as described in Section 2. These 
criteria required that annotators be able to 
recognize whether or not the complement of a 
potential light verb was itself a predicating 
element. To make this occasionally difficult 
judgment, annotators used a simple heuristic test 
of whether or not the complement was headed by 
an element that has a verbal counterpart.  If so, 
the light verb frameset was selected. 
The second pass involves the annotation of the 
sentence with the true predicate as the relation. 
During this pass, the true predicate is annotated 
with an appropriate frameset. In the third pass, 
the arguments and the modifiers of the two 
previous passes are reconciled and merged into a 
single annotation. In order to reduce the number 
of hand annotation, it is preferable for this last 
pass, the Pass 3, to be done automatically. 
Since the nature of the light verb is different 
from that of other verbs as described in Section 
2, the advantage of doing the annotation of the 
light verb and the true predicate on separate 
passes is that in the light verb pass the annotators 
will be able to quickly dispose of the verb as a 
light verb and in the second pass, they will be 
allowed to solely focus on the annotation of the 
light verb?s true predicate. 
The descriptions of how the arguments and 
modifiers of the light verbs and their true 
predicates are annotated are mentioned in Table 
1, but notably, none of the examples in it 
currently include the annotation of arguments 
 Pass 1: Pass 2: Pass 3: 
 Light Verb Annotation True Predicate Annotation Merge of Pass1&2 Annotation 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
and 
Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the light verb are annotated 
- Arguments and modifiers of 
the true predicate are annotated 
- Arguments and modifiers 
found in the two passes are 
merged, preferably 
automatically. 
Frameset Light verb frameset True predicate?s frameset LVC?s frameset 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG-MNR: brisk  
REL: walk 
REL: took walk 
ARG-MNR: brisk 
Table 1. Preliminary Annotation Scheme 
84
and modifiers.  This is intentional, as coming to 
an agreement concerning the details of what 
exactly each of the three passes looks like while 
meeting the needs of the four PropBank 
languages is quite challenging. Thus, for the rest 
of the paper we will discuss the strengths and 
weaknesses of the two trial methods of 
annotation we have considered and discarded in 
Section 5, as well as the final annotation scheme 
we chose in Section 6. 
5 Trials 
5.1 Method 1 
As our first attempt, the annotation of argument 
and adjuncts was articulated in the following 
manner (Table 2). 
Pass 1: Pass 2: 
Light verb True predicate 
- Predicating expression 
is labeled ARG-PRX 
- Annotate the Subject 
argument of the light 
verb as the Arg0. 
- Annotate the rest of the 
arguments and modifiers 
of the light verb with 
ARGM labels. 
- Annotate arguments 
and modifiers of the 
true predicate within 
its domain of locality. 
Generic light verb Frame 
File 
True predicate?s 
Frame File 
?-RKQ WRRN D EULVN ZDON WKURXJK WKH SDUN? 
ARG0: John 
REL: took 
ARG-PRX: a brisk walk 
ARG-DIR: through the park 
ARG-MNR: brisk  
REL: walk 
Table 2. Method 1 for annotation for Passes 1 and 2. 
Revised information is in italics. 
In Pass 1, in addition to annotating the 
predicating expression of the light verb with 
ARG-PRX, the subject argument was marked 
with an ARG0. The choice of ARG0, which 
corresponds to a proto-typical agent, was guided 
by the observation that English LVCs tend to 
lend a component of agentivity to the subject 
even in cases where the true predicate would not 
necessarily assign an agent as its subject. The 
rest of the arguments and modifiers were labeled 
with corresponding ARGM (i.e., modifier) 
labels. The assumption here is that the arguments 
of the light verb will also be the arguments of the 
true predicate.   
In Pass 2, then, the annotation of the 
arguments of the true predicate was restricted to 
its domain of locality (i.e., the span of the ARG-
PRX as marked in Pass1). That is, in the example 
?John took a brisk walk through the park?, the 
labeled spans for the true predicate would be 
limited to the NP ?a brisk walk? and neither 
?John? nor through the park? would be annotated 
as the arguments of the true predicate ?walk?. 
Frame Files: This method would require three 
Frame Files: a generic light verb Frame File, a 
true predicate Frame File, and an LVC Frame 
File. The Frame File for the light verb would not 
be specific to the form of the light verb (e.g., 
same frame for take and make). Rather, it would 
indicate a skeletal argument structure in order to 
reduce the amount of Frame Files made, 
including only Arg0 as its argument1.  
5.2 Weakness of Method 1 
This method has one glaring problem: the 
assumption that the semantic roles of the 
arguments as assigned by the light verb 
uniformly coincide with those assigned by the 
true predicate does not always hold. Consider the 
following English sentence2. 
whether Wu Shu-Chen would make another 
[appearance] in court was subject to observation 
In this example, ?Wu Shu-Chen? is the agent 
argument (Arg0) of the light verb ?make? and is 
the theme or patient argument (Arg1) of a typical  
?appearance? event. Also consider the following 
example from Hindi.  
It is possible that in a light verb construction, 
the light verb actually modifies the standard 
underlying semantics of a nominalization like 
appearance.  In any event, we cannot assume that 
the expected argument labels for the light verb 
and for the standard interpretation of the 
nominalization will always coincide. Thus, we 
could say that Pass 2?s true predicate annotation 
is only partial and is not representative of the 
complete argument structure. In particular, we 
are left with a very difficult merging problem, 
because the argument labels of the two separate 
passes conflict as seen in the above examples. 
5.3 Method 2 
In order to remedy the problem of conflicting 
argument labels, we revised Method 1?s Pass 2 
annotation scheme. This is shown in Table 3. 
Pass 1 remains unchanged from Method 1. 
In this method, both the light verb and the true 
predicate of the sentence receive complete sets of 
                                                          
1 This is why the rest of the argument/modifiers would be 
annotated using ARGM modifier labels. 
2  The light verb is in boldface, the true predicate is in bold 
and square brackets, and the argument/adjunct under 
consideration is underlined. 
85
argument and modifier labels. In Pass 2, the 
limitation of annotating within the domain of 
locality is removed. That is, the arguments and 
modifiers inside and outside the true predicate?s 
domain of control are annotated with respect to 
their semantic relationship to the true predicate 
(e.g., in the English example of Section 5.2, ?Wu 
Shu-Chen? would be considered ARG1 of 
?appearance?).  
Frame Files: This method would also require 
three Frame Files. The major difference is that 
with this method the Frame File for the true 
predicate includes arguments that are sisters to 
the light verb.  
5.4 Weaknesses of Method 2 
If in Method 1 we have committed the error of 
semantic unfaithfulness due to omission, in 
Method 2 we are faced with the problem of 
including too much. In the following sentence, 
consider the role of the underlined adjunct: 
A New York audience ? gave it a big round 
of applause when the music started to play. 
By the annotation in Method 2, the underlined 
temporal adjunct ?when the music started to 
play? is labeled as both the argument of ?give? 
and of ?applause?. The question here is does the 
argument apply to both the giving and the 
applauding event? In other words, does the 
adjunct play an equal role in both passes?  
 Since it could be easily said that the temporal 
phrase applies to both the applauding and the 
giving of the applause events, this example may 
not be particularly compelling. However, what if 
a syntactic complement of the light verb is a 
semantic argument of the true predicate and the 
true predicate only? This is seen more frequently 
in the cases where the light verb is less bleached 
than in the case of ?give? above. Consider the 
following Arabic example. 
 
 ????? ??]???????? ] ????? ????????? ??????? ??????  
took.we PREP DEF-consideration PREP 
prepertations.our possibility sustain.their losses 
?We took into [consideration] during our prepa-
rations the possibility of them sustaining losses? 
 
Here, even though the constituent ?of them 
sustaining losses? is the syntactic complement of 
the verb ?to take;? semantically, it modifies only 
the nominal object of the PP ?consideration.?  
There are similar phenomena in Chinese light 
verb constructions. Syntactic modifiers of the 
light verb are semantic arguments of the true 
predicate, which is usually a nominalization that 
serves as its complement.  
 
?? ?  ?    ? ? ??    [??]    ?? ? 
we now regarding this CL issue [conduct] discussion. 
lit.?We are conducting a discussion on this issue.? 
 ?We are discussing this issue.? 
 
The prepositional phrase ????? ?regarding 
this issue? is a sister to the light verb but 
semantically it is an argument of the nominalized 
predicate ?? ?discussion?. 
The logical next question would be: does the 
annotation of the arguments, adjuncts and 
modifiers have to be all or nothing? It could 
conceivably be possible to assign a selected set 
of arguments at the light verb or true predicate 
level. For example, in the Chinese sentence, the 
modifier ?regarding this CL issue?, though a 
syntactic adjunct to the light verb, could be left 
out from the semantic annotation in Pass 1 and 
included only in the Pass 2. 
However, the objection to this treatment 
comes from a more practical need. As mentioned 
above, in order to keep the manual annotation to 
a minimum, it would be necessary to keep Pass 3 
completely deterministic. As is, with the 
unmodified Method 2, there would be the need to 
choose between Pass 1 or Pass 2 annotation to 
when doing the automatic Pass 3. If we modify 
Method 2 by annotating only a selected set of 
syntactic arguments for the light verb or the true 
predicate, then this issue is exacerbated. In such 
a case there we would have to develop with strict 
rules for which arguments of which pass should 
be included in Pass 3. Pass 3 would no longer be 
automatic, and should be done manually.  
Pass 2: 
True predicate 
- Annotate the Subject argument of the light verb 
with the appropriate role of the true predicate 
- Annotate arguments and modifiers of the true 
predicate without limitation as to the domain of 
locality. 
True predicate?s Frame File 
?+H PDGH DQRWKHU DSSHDUDQFH DW WKH SDUW\? 
ARG1: He 
ARG-ADV: another 
REL: appearance 
ARG-DIR: at court 
Table 3. Method 2 for annotation for Pass 2. Pass 
1 as presented in Table 2 remains unchanged. 
Revised information for Pass 2 is in italics 
 
86
6 Final Annotation Scheme 
6.1 Semantic Fidelity 
Many of the objections so far to Methods 1 and 2 
have centered on the issue of semantic fidelity 
during the annotation of each of the two passes. 
The debate of whether both passes should be 
annotated and to what extent has practical 
implications for the third Pass, as described 
above. However, more importantly it comes 
down to whether or not the semantics of the final 
light verb plus true predicate combination is 
indeed distinct from the semantics of its parts 
(i.e. light verb and true predicate, separately). 
This may be a fascinating linguistic question, but 
it is not something our annotators can be 
debating for each and every instance.   
Instead, we argue that the semantic argument 
structure of the light verb plus true predicate 
combination can in practice be different from 
that of the expressions taken independently as 
has been proposed by various studies (Butt, 
2004; Rosen, 1997; Grimshaw & Mester, 1988). 
Thus, we resolve the cases in which the 
differences in argument roles as assigned by the 
light verb and the nominalization (Section 5.2) 
by handling the argument structure of the 
standard nominalization separately from that of 
the nominalization participating in the LVC. In 
the example ?Chen made another appearance in 
court?, we annotate ?Chen? as the Agent (ARG0) 
of the full predicate ?[make] [appearance]?, 
which is different from the argument structure of 
the standard nominalization which would label 
?Chen? to be the Patient argument (ARG1). 
6.2 Method 3: Final Method 
Our final method of light verb annotation reflects 
the notion that the noun, verb, or adjective as a 
true predicate within an LVC can have a 
different argument structure from that of the 
word alone. Table 4 shows the final annotation 
scheme for light verb construction.  
During Pass 1, the LVCs and their predicating 
expressions are identified in the data. Instances 
identified as LVCs in Pass 1 are then manually 
annotated during Pass 2, annotating the 
arguments and adjuncts of the light verb and the 
true predicate with roles that reflect their 
semantic relationships to the light verb plus true 
predicate. In practice, Pass 1 becomes a way of 
simply manually identifying the light verb 
usages. It is in Pass 2 that we make the final 
choice of argument labels for all of the 
arguments. Thus in Pass 3, the light verb and the 
true predicate lemmas from Pass 1 and 2 are 
joined into a single unit (e.g., in the example 
found in Table 4, the light verb ?took? would be 
joined with the true predicate ?walk? into 
?took+walk?) 3. In this final method, Pass 3 can 
be achieved completely deterministically. 
The major difference in this annotation 
scheme from that of Methods 1 and 2 is that 
instead of annotating in terms of the semantics of 
the bare noun, adjective or verb, the argument 
structure is determined for the entire predicate or 
the full event: semantics of the light verb plus the 
true predicate. This means that for the sentences 
where the argument roles of the verb and the 
nominalization disagree like ?Chen? in ?Chen 
                                                          
3 The order of Pass 2 and Pass 3 as presented in Table 4 is 
arguably a product of how the annotation tools for 
PropBank are set up for Arabic, Chinese, and English. That 
is, the order of the Pass 2 and Pass 3 could potentially be 
flipped provided that the tools and procedures of annotation 
support it, as is the case for Hindi PropBank. After the LVC 
and ARG-PRX are identified in Pass 1, the light verb and 
the true predicate can be deterministically joined into a 
single relation in Pass 2, leaving the manual annotation of 
LVC for Pass 3.  The advantage of this alternative ordering 
is that because the annotation of LVC is done around light 
verb plus the true predicate as a single relation, rather than 
the true predicate alone as in Table 4, the argument 
annotation may in actuality be more intuitive for annotators 
even with less training. 
 Pass 1: Pass 2:  Pass 3: 
 Light Verb Identification LVC Annotation Deterministic relation merge 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
& Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the LVCs are annotated 
- Arguments and modifiers 
are taken from Pass 2 
Frame File <no Frame File needed> LVC?s Frame File LVC?s Frame File 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG0: John 
ARG-MNR: brisk  
REL: walk 
ARGM-DIR: through the park 
ARG0: John 
ARG-MNR: brisk  
REL: [took][walk] 
ARGM-DIR: through the park 
Table 4. Final Annotation Scheme 
87
made another4 appearance in court?, we label the 
argument with the role that is consistent with the 
entire predicate (i.e. Agent, ARG0).  
Frame Files: The final advantage to this 
method is that only one Frame File is needed. 
Since Pass 1 is an identification round, no Frame 
File is required. A single Frame File for LVC 
that includes the argument structure with respect 
to the light verb plus true predicate combination 
will suffice for Pass 2 and Pass 3. 
7 Distinguishing LVCs from MWEs 
As we have discussed in Section 2, we adapted 
our approach from Butt?s (2004) definition of 
LVCs. That is, an LVC is characterized by a 
semantically bleached light verb and a true 
predicate. These elements combine as a single 
predicating unit, in such a way that the light verb 
plus its true predicate can be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression (e.g. 
?lectured? for ?gave a lecture?). Also, as 
discussed in Section 6.1, our approach advocates 
the notion that the semantic argument structure 
of the light verb plus true predicate is different 
from that of the expressions taken independently 
(as also proposed by Butt, 2004; Rosen, 1997; 
Grimshaw & Mester, 1988 among others). 
While these definitions are appropriate for the 
PropBank annotation task as we have presented 
it, there are still cases that merit closer attention. 
Even English with a rather limited set of verbs 
that are commonly cited as LVCs, includes a 
problematic mixture of what could arguably be 
termed either LVCs or idiomatic expressions: 
?make exception?, ?take charge?. This difficulty 
in part is the effect of frequency and 
entrenchment of particular constructions.  The 
light verbs themselves do not diminish in form 
over time in a manner similar to auxiliaries (Butt, 
2004), although the complements of common 
LVCs can change over time such that it is no 
longer clear that the complement is a predicating 
element.   
In the case of English, the expressions ?take 
charge? may be more commonly found today as a 
LVC than independently in its verbal form.  As 
we discovered with our annotators, native 
English speakers are uncomfortable using the 
verb ?charge? (i.e. to burden with a 
                                                          
4 The adjective ?another? is annotated as the modifier of the 
full predicate ?[make][appearance]? as it can be interpreted 
to mean that the make appearance event happened a 
previous appearance has been made. 
responsibility) as an independent matrix verb. A 
similar phenomenon can be seen in Arabic, 
where the predicate ??? ???? lit. ?release name? 
exemplifies a prototypical LVC that means ?to 
name?. However, in our data we see cases in 
which the complement is missing, while the 
semantics of the LVC remains intact: 
 ???? ???? ?? ??????? ??????  
CONJ REL be released.he PREP-him/it  
DEF-sector DEF-public 
lit ?Or what is released to it ?the public sector?? 
?Or what is called/named ?the public sector.?? 
This raises the question of: when does a 
construction that may have once been an LVC 
become more properly defined as an idiomatic 
expression due to such entrenchment?  Idiomatic 
expressions can potentially be distinguished from 
LVCs through judgments of how fixed or 
syntactically variable a construction is, and on 
the basis of how semantically transparent or 
decomposable the construction is (Nunberg et. 
al., 1994). However, sometimes the dividing line 
is hard to draw.  
A similar problem arises in determining 
whether a construction is a case of an LVC or 
simply a usage with a distinct sense of the verb. 
Take, for example, the following Arabic 
sentence. 
 ?????? ????? 
   take.he DEF-food 
lit. ?(he) took food? 
?he ate? 
Here, the Arabic word ???? ?food? is the noun 
derivation of the root shared by the verb ???? ?to 
eat?, in such a way that the sentence could be 
rephrased as ???? ?(he) ate?. This example falls 
neatly into the LVC category. However, further 
examples suggest that the example is a case of a 
distinct sense of ?to take orally? where the 
restrictions on the object are that the theme must 
be something that can be taken by mouth: 
?????? ????? 
take.he DEF-medicine 
?he took medicine? 
?????? ????? 
take.he DEF-soup 
?he took soup? 
Finally, determining the appropriate criteria to 
distinguish between a truly semantically 
bleached verb and verbs that seem to be 
participating in complex predication but 
contribute more to the semantics of the 
construction is a challenge for all languages. For 
example, in English data, there are potential 
LVCs with verbs that are not often thought of as 
light verbs, such as ?produce an alteration? and 
88
?issue a complaint?.  Although most English 
speakers would agree that the verbs in these 
constructions do not contribute to the semantics 
of the construction (e.g. ?issue a complaint? can 
be paraphrased to ?to complain?), there are 
similar constructions such as ?register a 
complaint,? wherein the verb cannot be 
considered light. For the purposes of annotation, 
where it is necessary for annotators to understand 
clear criteria for distinguishing light verbs, such 
cases are highly problematic because there is no 
deterministic way to measure the extent to which 
the verbal element contributes to the semantics 
of the construction.  In turn, there is not a good 
way to distinguish some of these borderline 
verbs from their normal, heavy usages.  
Such problems can be resolved by establishing 
language-specific semantic or syntactic tests that 
can be used for taking care of the borderline 
cases of LVCs. However, there is one other 
plausible manner we have identified that could 
help in detecting such atypical LVCs. This can 
be done by focusing on the argument structures 
of predicating complements rather than focusing 
on the verbs themselves.  Grimshaw & Mester 
(1988) suggest that the formation of LVCs 
involves argument transfer from the predicating 
complement to the verb, which is semantically 
bleached and thematically incomplete and 
assigns no thematic roles itself.  Similarly, 
Stevenson et al (2004) suggest that the 
acceptability of a potential LVC depends on the 
semantic properties of the complement.  Thus, 
atypical LVCs, such as the English construction 
?issue a complaint,? can potentially be detected 
during the annotation of eventive nouns, planned 
for all PropBank languages.  
This process will make our treatment of LVCs 
more comprehensive. Used with our language-
specific semantic and syntactic criteria relating to 
both the verb and the predicating complement, it 
will help us to more effectively capture as many 
types of LVCs as possible, including those of the 
V+ADJ and V+V varieties. 
8 Usefulness of our Approach 
Two basic approaches have previously been 
taken to handle all types of MWEs, including 
LVCs in natural language processing 
applications. The first is to treat MWEs quite 
simply as fixed expressions or long strings of 
words with spaces in between; the second is to 
treat MWEs as purely compositional (Sag et al, 
2002). The words-with-spaces approach is 
adequate for handling fixed idiomatic 
expressions, but issues of lexical proliferation 
and flexibility quickly arise when this approach 
is applied to light verbs, which are syntactically 
flexible and can number in the tens of thousands 
for a given language (Stevenson et al, 2004; Sag 
et al, 2002).  Nonetheless, large-scale lexical 
resources such as FrameNet (Baker et al, 1998) 
and WordNet (Fellbaum, 1999) continue to 
expand with entries that are MWEs.   
The purely compositional approach is also 
problematic for light verbs because it is 
notoriously difficult to predict which light verbs 
can grammatically combine with other 
predicating elements; thus, this approach leads to 
problems of overgeneration (Sag et al, 2002).  In 
order to overcome this problem, Stevenson et al 
(2004) attempted to determine which 
nominalizations could form a valid complement 
to the English light verbs take, give and make, 
using Levin?s (1993) verb classes to group 
similar nominalizations.  This approach was 
rather successful for take and give, but 
inconclusive for the verb make.  
Our approach can help to develop a resource 
that is useful whether one takes a words-with-
spaces approach or a compositional approach. 
Specifically, for those implementing a words-
with-spaces approach, the resulting PropBank 
annotation can serve as a lexical resource listing 
for LVCs. For those interested in implementing a 
compositional approach the PropBank annotation 
can serve to assist in predicting likely 
combinations. Moreover, information in the 
PropBank Frame Files can be used to generalize 
across classes of nouns that can occur with a 
given light verb with the help of lexical resources 
such as WordNet (Fellbaum, 1998), FrameNet 
(Baker et. al., 1998), and VerbNet (Kipper-
Schuler, 2005) (in a manner similar to the 
approach of Stevenson et al (2004)). 
Acknowledgements 
We also gratefully acknowledge the support of the 
National Science Foundation Grant CISE-CRI 
0709167, Collaborative: A Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu, and a 
grant from the Defense Advanced Research Projects 
Agency (DARPA/IPTO) under the GALE program, 
DARPA/CMO Contract No HR0011-06-C-0022, 
subcontract from BBN, Inc.  
Any opinions, findings, and conclusions or 
recommendations expressed in this material are those 
of the authors and do not necessarily reflect the views 
of the National Science Foundation. 
89
Reference 
Alsina, A. 1997. Causatives in Bantu and Romance. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 203-246. 
Baker, Collin F., Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of the 17th International Conference 
on Computational Linguistics (COLING/ACL-98), 
pages 86?90, Montreal. ACL. 
Butt, M. 2004.  The Light Verb Jungle. In G. Aygen, 
C. Bowern & C. Quinn eds.  Papers from the 
GSAS/Dudley House Workshop on Light Verbs. 
Cambridge, Harvard Working Papers in 
Linguistics, p. 1-50.   
Butt, M. 1997. Complex Predicates in Urdu. In A. 
Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 107-149. 
Fellbaum, Christine, ed.: 1998, WordNet: An 
Electronic Lexical Database, Cambridge, MA: 
MIT Press.  
Grimshaw, J., and A. Mester. 1988. Light verbs and 
?-marking. Linguistic Inquiry 19(2):205?232. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics 28:3, 245-288. 
Goldberg, Adele E. 2003.  ?Words by Default: 
Inheritance and the Persian Complex Predicate 
Construction.? In E. Francis and L. Michaelis 
(eds). Mismatch: Form-Function Incongruity and 
the Architecture of Grammar. CSLI Publications.  
84-112. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad 
coverage, comprehensive verb lexicon. Ph.D. 
thesis, University of Pennsylvania. 
Levin, B. 1993. English Verb Classes and 
Alternations: A Preliminary Investigation. 
Chicago: Chicago Univ. Press.  
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, and R. Grishman. 2004. The 
NomBank Project: An interim report. In 
Proceedings of the HLT-NAACL 2004 Workshop: 
Frontiers in Corpus Annotation, pages 24- 31, 
Boston, MA. pages 430?437, Barcelona, Spain. 
Mohanan, T. 1997. Multidimensionality of 
Representation: NV Complex Predicates in Hindi. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 431-471. 
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, 
Owen Rambow, Dipti Misra Sharma, Fei Xia, 
Hindi Syntax: Annotating Dependency, Lexical 
Predicate-Argument Structure, and Phrase 
Structure, In the Proceedings of the 7th 
International Conference on Natural Language 
Processing, ICON-2009, Hyderabad, India, Dec 
14-17, 2009 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 
31(1):71?106. 
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin, Daniel Jurafsky. 
2004. Shallow Semantic Parsing using Support 
Vector Machines. University of Colorado 
Technical Report: TR-CSLR 2003-03. 
Rosen, C. 1997. Auxiliation and Serialization: On 
Discerning the Difference. In A. Alsina, J. 
Bresnan, and P. Sells eds. Complex Predicates. 
Stanford, California: CSLI Publications, p. 175-
202. 
Sag, I., Baldwin, T. Bond, F., Copestake, A., 
Flickinger, D. 2002.  Multiword expressions: A 
pain in the neck for NLP.  In Proceedings of teh 
Third International Conference on Intelligent Text 
processing and Computatinal Linguistics 
(CICLING 2002), p. 1-15, Mexico City, Mexico. 
ACL. 
Stevenson, S., Fazly, A., and North, R. (2004). 
Statistical measures of the semi-productivity of 
light verb constructions. In Proceedings of the 
ACL-04 Workshop on Multiword Expressions: 
Integrating Processing, p. 1?8. 
 
90
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 91?99,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Retrieving Correct Semantic Boundaries in Dependency Structure
Jinho D. Choi
Department of Computer Science
University of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
This paper describes the retrieval of cor-
rect semantic boundaries for predicate-
argument structures annotated by depen-
dency structure. Unlike phrase structure,
in which arguments are annotated at the
phrase level, dependency structure does
not have phrases so the argument labels are
associated with head words instead: the
subtree of each head word is assumed to
include the same set of words as the an-
notated phrase does in phrase structure.
However, at least in English, retrieving
such subtrees does not always guarantee
retrieval of the correct phrase boundaries.
In this paper, we present heuristics that
retrieve correct phrase boundaries for se-
mantic arguments, called semantic bound-
aries, from dependency trees. By apply-
ing heuristics, we achieved an F1-score
of 99.54% for correct representation of
semantic boundaries. Furthermore, error
analysis showed that some of the errors
could also be considered correct, depend-
ing on the interpretation of the annotation.
1 Introduction
Dependency structure has recently gained wide in-
terest because it is simple yet provides useful in-
formation for many NLP tasks such as sentiment
analysis (Kessler and Nicolov, 2009) or machine
translation (Gildea, 2004). Although dependency
structure is a kind of syntactic structure, it is quite
different from phrase structure: phrase structure
gives phrase information by grouping constituents
whereas dependency structure gives dependency
relations between pairs of words. Many depen-
dency relations (e.g., subject, object) have high
correlations with semantic roles (e.g., agent, pa-
tient), which makes dependency structure suit-
able for representing semantic information such as
predicate-argument structure.
In 2009, the Conference on Computational Nat-
ural Language Learning (CoNLL) opened a shared
task: the participants were supposed to take de-
pendency trees as input and produce semantic role
labels as output (Hajic? et al, 2009). The depen-
dency trees were automatically converted from the
Penn Treebank (Marcus et al, 1993), which con-
sists of phrase structure trees, using some heuris-
tics (cf. Section 3). The semantic roles were ex-
tracted from the Propbank (Palmer et al, 2005).
Since Propbank arguments were originally anno-
tated at the phrase level using the Penn Treebank
and the phrase information got lost during the con-
version to the dependency trees, arguments are an-
notated on head words instead of phrases in depen-
dency trees; the subtree of each head word is as-
sumed to include the same set of words as the an-
notated phrase does in phrase structure. Figure 1
shows a dependency tree that has been converted
from the corresponding phrase structure tree.
S
NP1
DT
The
NNS
results
VP
VBP
appear
PP1
IN
in
NP
NP
NN
today
POS
?s
NN
news
The results appear in today 's newsroot
NMOD SBJ LOC NMOD
NMOD
ROOT PMOD
Figure 1: Phrase vs. dependency structure
91
In the phrase structure tree, arguments of the verb
predicate appear are annotated on the phrases:
NP1 as ARG0 and PP1 as ARGM-LOC. In the de-
pendency tree, the arguments are annotated on the
head words instead: results as the ARG0 and in as
the ARGM-LOC. In this example, both PP1 and the
subtree of in consist of the same set of words {in,
today, ?s, news} (as is the case for NP1 and the
subtree of results); therefore, the phrase bound-
aries for the semantic arguments, called semantic
boundaries, are retrieved correctly from the depen-
dency tree.
Retrieving the subtrees of head words usually
gives correct semantic boundaries; however, there
are cases where the strategy does not work. For
example, if the verb predicate is a gerund or a past-
participle, it is possible that the predicate becomes
a syntactic child of the head word annotated as a
semantic argument of the predicate. In Figure 2,
the head word plant is annotated as ARG1 of the
verb predicate owned, where owned is a child of
plant in the dependency tree. Thus, retrieving the
subtree of plant would include the predicate it-
self, which is not the correct semantic boundary
for the argument (the correct boundary would be
only {The, plant}).
The plant owned by Mark
NMOD NMOD LGS PMOD
Figure 2: Past-participle example
For such cases, we need some alternative for re-
trieving the correct semantic boundaries. This is
an important issue that has not yet been thoroughly
addressed. In this paper, we first show how to con-
vert the Penn Treebank style phrase structure to
dependency structure. We then describe how to
annotate the Propbank arguments, already anno-
tated in the phrase structure, on head words in the
dependency structure. Finally, we present heuris-
tics that correctly retrieve semantic boundaries in
most cases. For our experiments, we used the en-
tire Penn Treebank (Wall Street Journal). Our ex-
periments show that it is possible to achieve an F1-
score of 99.54% for correct representation of the
semantic boundaries.
2 Related work
Ekeklint and Nivre (2007) tried to retrieve seman-
tic boundaries by adding extra arcs to dependency
trees, so the structure is no longer a tree but a
graph. They experimented with the same cor-
pus, the Penn Treebank, but used a different de-
pendency conversion tool, Penn2Malt.1 Our work
is distinguished from theirs because we keep the
tree structure but use heuristics to find the bound-
aries. Johansson (2008) also tried to find seman-
tic boundaries for evaluation of his semantic role
labeling system using dependency structure. He
used heuristics that apply to general cases whereas
we add more detailed heuristics for specific cases.
3 Converting phrase structure to
dependency structure
We used the same tool as the one used for the
CoNLL?09 shared task to automatically convert
the phrase structure trees in the Penn Treebank
to the dependency trees (Johansson and Nugues,
2007). The script gives several options for the con-
version; we mostly used the default values except
for the following options:2
? splitSlash=false: do not split slashes. This
option is taken so the dependency trees pre-
serve the same number of word-tokens as the
original phrase structure trees.
? noSecEdges=true: ignore secondary edges
if present. This option is taken so all sib-
lings of verb predicates in phrase structure
become children of the verbs in dependency
structure regardless of empty categories. Fig-
ure 3 shows the converted dependency tree,
which is produced when the secondary edge
(*ICH*) is not ignored, and Figure 4 shows
the one produced by ignoring the secondary
edge. This option is useful because NP? and
PP-2? are annotated as separate arguments of
the verb predicate paid in Propbank (NP? as
ARG1 and PP-2? as ARGM-MNR).
S
NP-1
He
VP
VBD
was
VP
VBN
paid
NP
*-1
NP*
NP
.. salary
PP
*ICH*-2
PP-2?
with ..
1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html
2http://nlp.cs.lth.se/software/treebank converter/
92
paidHe wasroot
SBJ NMOD
ROOT NMOD
$342Ka salary with
VC NMOD
NMOD
$280Ka bonus
OBJ
NMOD
NMOD
Figure 3: When the secondary edge is not ignored
paidHe wasroot
SBJ NMOD
ROOT NMOD
$342Ka salary with
VC NMOD
NMOD
$280Ka bonus
OBJ
NMOD
ADV
Figure 4: When the secondary edge is ignored
Total 49,208 dependency trees were converted
from the Penn Treebank. Although it was pos-
sible to apply different values for other options,
we found them not helpful in finding correct se-
mantic boundaries of Propbank arguments. Note
that some of non-projective dependencies are re-
moved by ignoring the secondary edges. However,
it did not make all dependency trees projective;
our methods can be applied for either projective
or non-projective dependency trees.
4 Adding semantic roles to dependency
structure
4.1 Finding the head words
For each argument in the Propbank annotated on
a phrase, we extracted the set of words belonging
to the phrase. Let this set be Sp. In Figure 1, PP1
is the ARGM-LOC of appear so Sp is {in, today,
?s, news}. Next, we found a set of head words,
say Sd, whose subtrees cover all words in Sp (e.g.,
Sd = {in} in Figure 1). It would be ideal if there
existed one head word whose subtree covers all
words in Sp, but this is not always the case. It is
possible that Sd needs more than one head word to
cover all the words in Sp.
Figure 5 shows an algorithm that finds a set of
head words Sd whose subtrees cover all words in
Sp. For each word w in Sp, the algorithm checks
if w?s subtree gives the maximum coverage (if w?s
subtree contains more words than any other sub-
tree); if it does, the algorithm adds w to Sd, re-
moves all words in w?s subtree from Sp, then re-
peats the search. The search ends when all words
in Sp are covered by some subtree of a head word
in Sd. Notice that the algorithm searches for the
minimum number of head words by matching the
maximum coverages.
Input: Sp = a set of words for each argument
in the Propbank
Output: Sd = a set of head words whose
subtrees cover all words in Sp
Algorithm:getHeadWords(Sp)1
Sd = {}2
while Sp 6= ? do3
max = None4
foreach w ? Sp do5
if |subtree(w)| > |subtree(max)|6
then
max = w7
end8
Sd.add(max)9
Sp.removeAll(subtree(max))10
end11
return Sd12
Figure 5: Finding the min-set of head words
The algorithm guarantees to find the min-set Sd
whose subtrees cover all words in Sp. This gives
100% recall for Sd compared to Sp; however, the
precision is not guaranteed to be as perfect. Sec-
tion 5 illustrates heuristics that remove the over-
generated words so we could improve the preci-
sion as well.
4.2 Ignoring empty categories
As described in Figures 3 and 4, dependency trees
do not include any empty categories (e.g., null
elements, traces, PRO?s): the empty categories
are dropped during the conversion to the depen-
dency trees. In the Penn Treebank, 11.5% of the
Propbank arguments are annotated on empty cat-
egories. Although this is a fair amount, we de-
cided to ignore them for now since dependency
structure is not naturally designed to handle empty
categories. Nonetheless, we are in the process of
finding ways of automatically adding empty cate-
gories to dependency trees so we can deal with the
remaining of 11.5% Propbank arguments.
4.3 Handling disjoint arguments
Some Propbank arguments are disjoint in the
phrase structure so that they cannot be represented
as single head words in dependency trees. For ex-
ample in Figure 6, both NP-1? and S? are ARG1 of
the verb predicate continued but there is no head
word for the dependency tree that can represent
both phrases. The algorithm in Figure 5 naturally
93
handles this kind of disjoint arguments. Although
words in Sp are not entirely consecutive ({Yields,
on, mutual, funds, to, slide}), it iteratively finds
both head words correctly: Yields and to.
S
NP-1?
NP
Yields
PP
IN
on
NP
mutual funds
VP
VBD
continued
S?
NP
*-1
VP
TO
to
VP
slide
Yields on mutual toroot
NMOD OPRDNMOD
PMOD
ROOT
SBJ
funds continued slide
IM
Figure 6: Disjoint argument example
5 Retrieving fine-grained semantic
boundaries
There are a total of 292,073 Propbank arguments
in the Penn Treebank, and only 88% of them map
to correct semantic boundaries from the depen-
dency trees by taking the subtrees of head words.
The errors are typically caused by including more
words than required: the recall is still 100% for the
error cases whereas the precision is not. Among
several error cases, the most critical one is caused
by verb predicates whose semantic arguments are
the parents of themselves in the dependency trees
(cf. Figure 2). In this section, we present heuris-
tics to handle such cases so we can achieve preci-
sion nearly as good as the recall.
5.1 Modals
In the current dependency structure, modals (e.g.,
will, can, do) become the heads of the main verbs.
In Figure 7, will is the head of the verb predicate
remain in the dependency tree; however, it is also
an argument (ARGM-MOD) of the verb in Prop-
bank. This can be resolved by retrieving only the
head word, but not the subtree. Thus, only will is
retrieved as the ARGM-MOD of remain.
Modals can be followed by conjuncts that are
also modals. In this case, the entire coordination
is retrieved as ARGM-MOD (e.g., {may, or, may,
not} in Figure 8).
They will remain on the list
SBJ
root
VC PRD NMOD
PRD
ROOT
Figure 7: Modal example 1
He may or read the bookroot
SBJ COORD ADV NMOD
OBJ
ROOT
may not
CONJ
COORD
Figure 8: Modal example 2
5.2 Negations
Negations (e.g., not, no longer) are annotated as
ARGM-NEG in Propbank. In most cases, nega-
tions do not have any child in dependency trees,
so retrieving only the negations themselves gives
the correct semantic boundaries for ARGM-NEG,
but there are exceptions. One is where a negation
comes after a conjunction; in which case, the nega-
tion becomes the parent of the main verb. In Fig-
ure 9, not is the parent of the verb predicate copy
although it is the ARGM-NEG of the verb.
You may come but notroot
SBJ COORD
ROOT
to read
PRP
copy
VC IM CONJ COORD
Figure 9: Negation example 1
The other case is where a negation is modified by
some adverb; in which case, the adverb should
also be retrieved as well as the negation. In Fig-
ure 10, both no and longer should be retrieved as
the ARGM-NEG of the verb predicate oppose.
They no longer the legislationroot
SBJ
NMOD
OBJ
oppose
AMOD
TMP
ROOT
Figure 10: Negation example 2
5.3 Overlapping arguments
Propbank does not allow overlapping arguments.
For each predicate, if a word is included in one
argument, it cannot be included in any other argu-
ment of the predicate. In Figure 11, burdens and
in the region are annotated as ARG1 and ARGM-
LOC of the verb predicate share, respectively. The
arguments were originally annotated as two sepa-
rate phrases in the phrase structure tree; however,
94
in became the child of burdens during the conver-
sion, so the subtree of burdens includes the subtree
of in, which causes overlapping arguments.
S
NP
U.S.
VP
VBZ
encourages
S
NP
Japan
VP
TO
to
VP
VB
share
NP
NP
burdens
PP
in ..
U.S. encourages Japan
in
root
share
LOC
OPRD
to burdens
the region
NMOD
PMOD
OBJIMOBJSBJ
ROOT
Figure 11: Overlapping argument example 1
When this happens, we reconstruct the depen-
dency tree so in becomes the child of share instead
of burdens (Figure 12). By doing so, taking the
subtrees of burdens and in no longer causes over-
lapping arguments.3
U.S. encourages Japan
in
root
share
OPRD
to burdens
the region
NMOD
PMOD
OBJIMOBJSBJ
ROOT
LOC
Figure 12: Overlapping argument example 2
5.4 Verb predicates whose semantic
arguments are their syntactic heads
There are several cases where semantic arguments
of verb predicates become the syntactic heads of
the verbs. The modals and negations in the previ-
ous sections are special cases where the seman-
tic boundaries can be retrieved correctly with-
out compromising recall. The following sec-
tions describe other cases, such as relative clauses
(Section 5.4.2), gerunds and past-participles (Sec-
tion 5.4.3), that may cause a slight decrease in re-
call by finding more fine-grained semantic bound-
aries. In these cases, the subtree of the verb predi-
cates are excluded from the semantic arguments.
3This can be considered as a Treebank/Propbank dis-
agreement, which is further discussed in Sectino 6.2.
5.4.1 Verb chains
Three kinds of verb chains exist in the current
dependency structure: auxiliary verbs (including
modals and be-verbs), infinitive markers, and con-
junctions. As discussed in Section 5.1, verb chains
become the parents of their main verbs in depen-
dency trees. This indicates that when the subtree
of the main verb is to be excluded from semantic
arguments, the verb chain needs to be excluded as
well. This usually happens when the main verbs
are used within relative clauses. In addition, more
heuristics are needed for retrieving correct seman-
tic boundaries for relative clauses, which are fur-
ther discussed in Section 5.4.2.
The following figures show examples of each
kind of verb chain. It is possible that multiple verb
chains are joined with one main verb. In this case,
we find the top-most verb chain and exclude its
entire subtree from the semantic argument. In Fig-
ure 13, part is annotated as ARG1 of the verb pred-
icate gone, chained with the auxiliary verb be, and
again chained with the modal may. Since may is
the top-most verb chain, we exclude its subtree so
only a part is retrieved as the ARG1 of gone.
a part that
be
NMOD
may gone
PRDVCDEPNMOD
Figure 13: Auxiliary verb example
Figure 14 shows the case of infinitive markers.
those is annotated as ARG0 of the verb predicate
leave, which is first chained with the infinitive
marker to then chained with the verb required. By
excluding the subtree of required, only those is re-
trieved as the ARG0 of leave.
rules are
tough
root those
ROOT
on
required
SBJ
to
AMOD
leave
PRD PMOD APPO OPRD IM
Figure 14: Infinitive marker example
Figure 15 shows the case of conjunctions. people
is annotated as ARG0 of the verb predicate exceed,
which is first chained with or then chained with
meet. By excluding the subtree of meet, only peo-
ple is retrieved as the ARG0 of exceed.
When a verb predicate is followed by an ob-
ject complement (OPRD), the subtree of the object
complement is not excluded from the semantic ar-
gument. In Figure 16, distribution is annotated as
95
people
who meet
exceed
NMOD
or the
DEP NMOD
OBJ
expectation
CONJCOORD
Figure 15: Conjunction example
ARG1 of the verb predicate expected. By excluding
the subtree of expected, the object complement to
occur would be excluded as well; however, Prop-
bank annotation requires keeping the object com-
plement as the part of the argument. Thus, a dis-
tribution to occur is retrieved as the ARG1 of ex-
pected.
a distribution expected to occur
NMOD IMOPRDAPPO
Figure 16: Object complement example
5.4.2 Relative clauses
When a verb predicate is within a relative clause,
Propbank annotates both the relativizer (if present)
and its antecedent as part of the argument. For ex-
ample in Figure 15, people is annotated as ARG0
of both meet and exceed. By excluding the subtree
of meet, the relativizer who is also excluded from
the semantic argument, which is different from the
original Propbank annotation. In this case, we
keep the relativizer as part of the ARG0; thus, peo-
ple who is retrieved as the ARG0 (similarly, a part
that is retrieved as the ARG0 of gone in Figure 13).
It is possible that a relativizer is headed by a
preposition. In Figure 17, climate is annotated as
ARGM-LOC of the verb predicate made and the
relativizer which is headed by the preposition in.
In this case, both the relativizer and the preposi-
tion are included in the semantic argument. Thus,
the climate in which becomes the ARGM-LOC of
made.
the
climate in decisionsthe was
PMOD
madewhich
NMOD NMOD
LOC
DEP
VC
Figure 17: Relativizer example
5.4.3 Gerunds and past-participles
In English, when gerunds and past-participles are
used without the presence of be-verbs, they often
function as noun modifiers. Propbank still treats
them as verb predicates; however, these verbs be-
come children of the nouns they modify in the de-
pendency structure, so the heuristics discussed in
Section 5.4 and 5.4.1 need to be applied to find the
correct semantic boundaries. Furthermore, since
these are special kinds of verbs, they require even
more rigorous pruning.
When a head word, annotated to be a seman-
tic argument of a verb predicate, comes after the
verb, every word prior to the verb predicate needs
to be excluded from the semantic argument. In
Figure 18, group is annotated as ARG0 of the
verb predicate publishing, so all words prior to the
predicate (the Dutch) need to be excluded. Thus,
only group is retrieved as the ARG0 of publishing.
the Dutch publishing group
NMOD
NMOD
NMOD
Figure 18: Gerund example
When the head word comes before the verb pred-
icate, the subtree of the head word, excluding the
subtree of the verb predicate, is retrieved as the se-
mantic argument. In Figure 19, correspondence is
annotated as ARG1 of the verb predicate mailed,
so the subtree of correspondence, excluding the
subtree of mailed, is retrieved to be the argument.
Thus, correspondence about incomplete 8300s be-
comes the ARG1 of mailed.
correspondence mailed about
NMOD
NMOD
incomplete 8300s
NMOD
PMOD
Figure 19: Past-participle example 1
When the subtree of the verb predicate is imme-
diately followed by comma-like punctuation (e.g.,
comma, colon, semi-colon, etc.) and the head
word comes before the predicate, every word after
the punctuation is excluded from the semantic ar-
gument. In Figure 20, fellow is annotated as ARG1
of the verb predicate named, so both the subtree
of the verb (named John) and every word after the
comma (, who stayed for years) are excluded from
the semantic argument. Thus, only a fellow is re-
trieved as the ARG1 of named.
5.5 Punctuation
For evaluation, we built a model that excludes
punctuation from semantic boundaries for two rea-
sons. First, it is often not clear how punctuation
96
a named John who stayedfellow , for years
NMOD APPO OPRD
P
DEP TMP PMOD
NMOD
Figure 20: Past-participle example 2
needs to be annotated in either Treebank or Prop-
bank; because of that, annotation for punctuation
is not entirely consistent, which makes it hard to
evaluate. Second, although punctuation gives use-
ful information for obtaining semantic boundaries,
it is not crucial for semantic roles. In fact, some
of the state-of-art semantic role labeling systems,
such as ASSERT (Pradhan et al, 2004), give an
option for omitting punctuation from the output.
For these reasons, our final model ignores punctu-
ation for semantic boundaries.
6 Evaluations
6.1 Model comparisons
The following list describes six models used for
the experiments. Model I is the baseline approach
that retrieves all words in the subtrees of head
words as semantic boundaries. Model II to VI use
the heuristics discussed in the previous sections.
Each model inherits all the heuristics from the pre-
vious model and adds new heuristics; therefore,
each model is expected to perform better than the
previous model.
? I - all words in the subtrees (baseline)
? II - modals + negations (Sections 5.1, 5.2)
? III - overlapping arguments (Section 5.3)
? IV - verb chains + relative clauses (Sec-
tions 5.4.1, 5.4.2)
? V - gerunds + past-participles (Section 5.4.3)
? VI - excluding punctuations (Section 5.5)
The following list shows measurements used for
the evaluations. gold(arg) is the gold-standard
set of words for the argument arg. sys(arg) is
the set of words for arg produced by our system.
c(arg1, arg2) returns 1 if arg1 is equal to arg2;
otherwise, returns 0. T is the total number of ar-
guments in the Propbank.
Accuracy =
1
T
?
?
?arg
c(gold(arg), sys(arg))
Precision =
1
T
?
?
?arg
|gold(arg) ? sys(arg)|
|sys(arg)|
Recall =
1
T
?
?
?arg
|gold(arg) ? sys(arg)|
|gold(arg)|
F1 =
2 ? Precision ?Recall
Precision + Recall
Table 1 shows the results from the models us-
ing the measurements. As expected, each model
shows improvement over the previous one in
terms of accuracy and F1-score. The F1-score
of Model VI shows improvement that is statisti-
cally significant compared to Model I using t-test
(t = 149.00, p < 0.0001). The result from the
final model is encouraging because it enables us
to take full advantage of dependency structure for
semantic role labeling. Without finding the correct
semantic boundaries, even if a semantic role label-
ing system did an excellent job finding the right
head words, we would not be able to find the ac-
tual chunks for the arguments. By using our ap-
proach, finding the correct semantic boundaries is
no longer an issue for using dependency structure
for automatic semantic role labeling.
Model Accuracy Precision Recall F1
I 88.00 92.51 100 96.11
II 91.84 95.77 100 97.84
III 92.17 97.08 100 98.52
IV 95.89 98.51 99.95 99.23
V 97.00 98.94 99.95 99.44
VI 98.20 99.14 99.95 99.54
Table 1: Model comparisons (in percentage)
6.2 Error analysis
Although each model consistently shows improve-
ment on the precision, the recall is reduced a bit for
some models. Specifically, the recalls for Mod-
els II and III are not 100% but rather 99.9994%
and 99.996%, respectively. We manually checked
all errors for Models II and III and found that they
are caused by inconsistent annotations in the gold-
standard. For Model II, Propbank annotation for
ARGM-MOD was not done consistently with con-
97
junctions. For example in Figure 8, instead of an-
notating may or may not as the ARGM-MOD, some
annotations include only may and may not but not
the conjunction or. Since our system consistently
included the conjunctions, they appeared to be dif-
ferent from the gold-standard, but are not errors.
For Model III, Treebank annotation was not
done consistently for adverbs modifying nega-
tions. For example in Figure 10, longer is some-
times (but rarely) annotated as an adjective where
it is supposed to be an adverb. Furthermore,
longer sometimes becomes a child of the verb
predicate oppose (instead of being the child of no).
Such annotations made our system exclude longer
as a part of ARGM-NEG, but it would have found
them correctly if the trees were annotated consis-
tently.
There are a few cases that caused errors in Mod-
els IV and V. The most critical one is caused by PP
(prepositional phrase) attachment. In Figure 21,
enthusiasm is annotated as ARG1 of the verb pred-
icate showed, so our system retrieved the subtree
of enthusiasm, excluding the subtree of showed,
as the semantic boundary for the ARG1 (e.g., the
enthusiasm). However, Propbank originally an-
notated both the enthusiasm and for stocks as the
ARG1 in the phrase structure tree (so the preposi-
tional phrase got lost in our system).
the investors showed forenthusiasm stocks
NMOD
NMOD
SBJ ADV PMOD
Figure 21: PP-attachment example 1
This happens when there is a disagreement be-
tween Treebank and Propbank annotations: the
Treebank annotation attached the PP (for stocks)
to the verb (showed) whereas the Propbank anno-
tation attached the PP to the noun (enthusiasm).
This is a potential error in the Treebank. In this
case, we can trust the Propbank annotation and re-
construct the tree so the Treebank and Propbank
annotations agree with each other. After the re-
construction, the dependency tree would look like
one in Figure 22.
the investors showed forenthusiasm stocks
NMOD
NMOD
SBJ PMOD
ADV
Figure 22: PP-attachment example 2
7 Conclusion and future work
We have discussed how to convert phrase struc-
ture trees to dependency trees, how to find the
minimum-set of head words for Propbank argu-
ments in dependency structure, and heuristics for
retrieving fine-grained semantic boundaries. By
using our approach, we correctly retrieved the se-
mantic boundaries of 98.2% of the Propbank ar-
guments (F1-score of 99.54%). Furthermore, the
heuristics can be used to fix some of the incon-
sistencies in both Treebank and Propbank annota-
tions. Moreover, they suggest ways of reconstruct-
ing dependency structure so that it can fit better
with semantic roles.
Retrieving correct semantic boundaries is im-
portant for tasks like machine translation where
not only the head words but also all other words
matter to complete the task (Choi et al, 2009).
In the future, we are going to apply our approach
to other corpora and see how well the heuristics
work. In addition, we will try to find ways of auto-
matically adding empty categories to dependency
structure so we can deal with the full set of Prop-
bank arguments.
Acknowledgments
Special thanks are due to Professor Joakim Nivre
of Uppsala University and Claire Bonial of the
University of Colorado at Boulder for very helpful
insights. We gratefully acknowledge the support
of the National Science Foundation Grants CISE-
CRI-0551615, Towards a Comprehensive Lin-
guistic Annotation and CISE-CRI 0709167, Col-
laborative: A Multi-Representational and Multi-
Layered Treebank for Hindi/Urdu, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-C-
0022, subcontract from BBN, Inc. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
References
Jinho D. Choi, Martha Palmer, and Nianwen Xue.
2009. Using parallel propbanks to enhance word-
alignments. In Proceedings of ACL-IJCNLP work-
shop on Linguistic Annotation (LAW?09), pages
121?124.
98
Susanne Ekeklint and Joakim Nivre. 2007. A
dependency-based conversion of propbank. In
Proceedings of NODALIDA workshop on Building
Frame Semantics Resources for Scandinavian and
Baltic Languages (FRAME?07), pages 19?25.
Daniel Gildea. 2004. Dependencies vs. constituents
for tree-based alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?04), pages 214?221.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL?09), pages 1?18.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of the 16th Nordic Conference
of Computational Linguistics (NODALIDA?07).
Richard Johansson. 2008. Dependency-based Seman-
tic Analysis of Natural-language Text. Ph.D. thesis,
Lund University.
Jason S. Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking of
linguistic configurations. In Proceedings of the 3rd
International AAAI Conference on Weblogs and So-
cial Media (ICWSM?09).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low semantic parsing using support vector machines.
In Proceedings of the Human Language Technology
Conference/North American chapter of the Associ-
ation for Computational Linguistics annual meeting
(HLT/NAACL?04).
99
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 207?211,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
An Overview of the CRAFT Concept Annotation Guidelines
Michael Bada
Lawrence E. Hunter
University of Colorado Denver 
Anschutz Medical Campus
Aurora, CO, USA
mike.bada@ucdenver.edu
larry.hunter@ucdenver.edu
Miriam Eckert
Martha Palmer
University of Colorado Boulder
Boulder, CO, USA
miriam_eckert@jdpa.com
martha.palmer@colorado.edu
Abstract
We present our concept-annotation guidelines 
for an large multi-institutional effort to create 
a gold-standard manually annotated corpus of 
full-text  biomedical  journal  articles.   We are 
semantically annotating these documents with 
the full term sets of eight large biomedical on-
tologies and controlled terminologies ranging 
from approximately 1,000 to millions of terms, 
and, using these guidelines, we have been able 
to  perform  this  extremely  challenging  task 
with  a  high  degree  of  interannotator  agree-
ment.  The guidelines have been designed to 
be able to be used with any terminology em-
ployed to semantically annotate concept men-
tions in text and are available for external use.
1 Introduction
Manually  annotated  gold-standard  corpora  are 
becoming increasingly  critical  for  the  develop-
ment  of  advanced NLP systems.   At  the  same 
time, the use of ontologies as formal representa-
tions of domain-specific knowledge is being seen 
in  a  wide range of  applications,  particularly  in 
the biomedical domain.  We are synergistically 
creating a gold-standard corpus called the  Col-
orado  Richly  Annotated  Full-Text  (CRAFT) 
Corpus  that  pushes  the  boundaries  of  both  of 
these  prominent  types  of  resources.   For  this 
project, we are manually annotating a collection 
of 97 full-text biomedical journal articles com-
prising a total  of  more than 750,000 words,  as 
opposed to the sentences or abstracts upon which 
other gold-standard corpora have focused.  Addi-
tionally,  while  most  other  related corpora  have 
used  small  annotation  schemas  consisting  of  a 
few to several  dozen classes for  their  semantic 
annotation,  we  are  employing  the  full  sets  of 
terms, ranging from approximately one thousand 
to several tens of thousands of terms, of select 
ontologies  of  the  Open  Biomedical  Ontologies 
(OBO)  Consortium,  the  most  prominent  set  of 
biomedical  ontologies  (Smith  et  al.,  2007),  as 
well as several other significant large biomedical 
controlled terminologies.  The terms of these on-
tologies  and  terminologies,  which  serve  as  the 
classes of the semantic annotation schema for the 
this  corpus,  are  continually  under  development 
by biomedical researchers and knowledge engi-
neers  and  are  widely  used  throughout  the  bio-
medical  field,  as  opposed  to  other  annotation 
schemas that are often idiosyncratic and not like-
ly reusable for other tasks.  Furthermore, though 
these ontologies have been used for a variety of 
NLP tasks, they have not been used in their en-
tirety toward gold-standard markup of text.
With regard to the CRAFT Corpus project, we 
have  previously  written  of  desiderata  in  using 
large ontologies and terminologies for semantic 
annotation of natural-language documents (Bada 
and Hunter, 2009a) and of semantic issues in the 
use of  one of the  ontologies  we are  using,  the 
Gene Ontology (Bada and Hunter,  2009b).   In 
this  paper,  we present  a  brief  overview  of  the 
concept1 annotation guidelines we are using for 
this corpus and the motivations behind our choic-
es.  With these guidelines,  our annotators have 
routinely  achieved  90+%  agreement  with  the 
project lead on all but the one most challenging 
terminological annotation passes, which current-
ly is more than 80%.  The guidelines were de-
signed  to  be  reusable  regardless  of  the 
1 Throughout this document, ?concept?, ?class?, and 
?term? are used interchangeably.
207
ontology/terminology  being  used  for  semantic 
annotation, and we have indeed used them with 
minimal exceptions for concept annotation of our 
corpus  using  eight  orthogonal  large  ontologies 
and terminologies.
2 Overview of the CRAFT Corpus
The CRAFT Corpus is a collection of 97 full-text 
biomedical  journal  articles  that  is  being  richly 
annotated  both  syntactically  and  semantically 
and  is  designed  to  be  an  open  community  re-
source for the development of advanced bioNLP 
systems.  The 97 articles of the corpus comprise 
the  intersection  of articles  that  are  open-access 
and that have been used as evidential sources for 
Gene Ontology (GO) annotations  of  genes and 
gene  products  of  the  laboratory  mouse  by  our 
collaborators who serve as the official GO cura-
tors  of  the  preeminent  mouse  database.   (The 
GO, the flagship OBO, is an ontology composed 
of three  subontologies  representing the specific 
molecular  functions  (MF)  of  genes  and  gene 
products,  the  higher-level  biological  processes 
(BP) in which they participate, and the cellular 
components (CC) in  which they localize  (Ash-
burner et al, 2000).  GO annotations, which are 
entirely different from the annotations we discuss 
in the work presented here, are created by label-
ing genes and gene products of organisms with 
GO terms.)
These articles in their entirety are being syn-
tactically  annotated  by  sentence  segmentation, 
tokenization,  part-of-speech  tagging,  and  tree-
banking.  The articles'  nouns and noun phrases 
are also being coreferentially  annotated (Cohen 
et al, 2010).  Though these branches constitute a 
significant  amount  of  the  annotations  of  the 
project, they are outside the scope of this paper. 
Furthermore, we are working on creating asser-
tional  annotations  between the  concept  annota-
tions via relations.
Six ontologies of the OBO library and two ad-
ditional  controlled  terminologies  have  thus  far 
been selected for concept annotation of these ar-
ticles on the bases that these are relatively well-
constructed knowledge representations, are wide-
ly used by bioinformaticians and/or biomedical 
researchers, and represent concepts needed to ex-
tract  significant  biomedical  assertions  from the 
literature.   In  addition  to  the  three  aforemen-
tioned GO ontologies,  the  OBOs that  were  se-
lected for concept annotation are the Cell Type 
Ontology (CL),  which represents types of cells 
(Bard et al, 2005); the Chemical Entities of Bio-
logical Interest (ChEBI) ontology, which repre-
sents  types  of  small  molecules,  parts  of  mole-
cules,  atoms,  and  subatomic  particles  (Degt-
yarenko et al, 2008); and the Sequence Ontolo-
gy  (SO),  which  represents  types  of  biological 
macromolecules  and their  components  (Eilbeck 
et al, 2005).  In addition to these ontologies, we 
are also annotating the articles with the terms of 
the NCBI Taxonomy, the most widely used Lin-
naean hierarchy of biological organisms, and the 
unique identifiers  of the Entrez Gene database, 
the  preeminent  resource  for  species-specific 
genes (Sayers et al, 2009).
The  annotation  methodology,  not  presented 
here due to lack of space, has been presented in a 
previous publication (Bada and Hunter, 2009a).
3 Overview of the CRAFT Concept An-
notation Guidelines
Concept  annotation entails  annotating text  with 
concepts, i.e., classes or terms from ontologies or 
terminologies.   (We  use  this  more  expansive 
term as opposed to named-entity annotation since 
several of the terminologies we are using contain 
terms  representing  processes  and  functions, 
which  are  annotated  just  as  terms  representing 
entities are.)  Every mention (including abbrevia-
tions and misspellings) of every explicitly repre-
sented concept of the ontology or terminology is 
annotated, and the text selected must be as se-
mantically close as possible?essentially seman-
tically equivalent?to  the term with with which 
it is annotated.   Thus (as shown later), a mention 
of platelets is semantically annotated with a term 
representing  platelets  as  opposed  to  the  more 
common case of annotating with a more general 
term  (e.g., representing  cells)  selected  from  a 
much smaller annotation schema.
For each concept annotation, any selected text 
span must be adjacent on each of its boundaries 
to an appropriate delimiter.  A whitespace char-
acter most often serves as a delimiter:
Ex. 1. localization: :of: :annexin: :A7: 
:in: :platelets: :and: :red: :blood: :cells 
[PMID:129252382]
(Colons  indicate  possible  boundaries  of 
annotations.)  Any  punctuation  mark  can  also 
serve as a delimiter indicating a boundary of an 
annotation:
2 For each example, the PubMed ID of the biomedi-
cal article from which it is extracted is shown.
208
Ex. 2. To examine this:,: we analyzed the 
ability of red blood cells derived from the 
annexin A7 mice :(:anxA7:-:/:-:): to form 
exovesicles:.: [PMID:12925238]
Finally,  beginnings and ends of documents can 
serve as boundaries of annotations.
It  is  important  to note that  letters  (including 
non-Latin letters) and numbers can never serve 
as  delimiters.   Practically,  this  means  that  an 
annotation text span can never begin or end in 
between two letters,  between  two numbers,  or 
between a number and a letter.  These delimiters 
were chosen so that the annotator would not be 
burdened  with  the  very  difficult  and  time-
consuming  task  of  having  to  figure  out  what 
every  letter  of  every  abbreviation  represented 
and whether they should be annotated; similarly, 
this avoids evaluation of any arbitrary part of any 
word (e.g.,  whether the  "cyto"  of  "cytological" 
should be annotated with the term cell3).  This 
choice  of  delimiters  sometimes  prevents  the 
annotator from creating an annotation that he or 
she may wish to create,  but  in  our experience, 
this  is  a  relatively  rare  occurrence,  and  it  is  a 
small  price  to  pay  for  greatly  simplifying  an 
already  extremely  large  and  difficult  task. 
Furthermore, it is a straighforward rule for both 
human and computational annotators to follow.
One primary motivation behind our strategy of 
annotating only explicitly represented concepts is 
the  capture  of  the  exact  semantics  of  textual 
mentions; conversely, annotating a textual men-
tion with a more general term (e.g.,  annotating 
?platelet? with cell) entails loss of knowledge. 
A second motivation is that of making this task 
of semantic annotation doable: The alternative of 
annotating every mention of the concepts within 
the domain of a given terminology including all 
concepts within the domain that are not explicitly 
represented in the terminology rapidly becomes 
an  overwhelming task  with  even  a  moderately 
sized terminology.  For example, using this alter-
native strategy to annotate all mentions of ChEBI 
chemical concepts explicitly represented or not, 
if an annotator came across a mention of a chem-
ical not represented in the ontology,  e.g., iodix-
anol,  assuming he  were  not  intimately  familiar 
with the structure and function of iodixanol, he 
would have to first research this.  From among 
the thousands of structural terms, he would have 
to annotate this mention with all relevant terms 
3 Names of ontological concepts are rendered in 
fixed?width?type throughout this document.
pertaining  to  its  structure  such  as  amides, 
polyols,  aromatic ? compounds,  and 
organoiodine?compounds since this com-
pound  contains  the  corresponding  chemical 
groups that define these types of molecules (and 
none of these terms subsumes another).  Further-
more, he would have to evaluate annotating with 
all  relevant  terms from among the  hundreds of 
ChEBI  functional  terms  (e.g.,  xenobiotic, 
base,  chromophore,  cofactor).   This 
enormous amount of work becomes even more 
difficult when working with concepts that are not 
as precisely defined as, for example, the chemi-
cal structure terms.
Text spans that can be considered for annota-
tion are dictated by syntax, and the text that is se-
lected must be semantically equivalent to a term 
in the ontology/terminology.  For example, for a 
noun,  any modifying  adjective  or  prepositional 
phrase can be considered for inclusion in the an-
notation  if  its  inclusion  results  in  a  semantic 
match to a concept in the ontology/terminology.
Fig. 1. Part of the GO BP cellular?
lipid?metabolic?process hierar-
chy.
Ex. 3: Skeletal muscle is a major site to 
regulate whole-body fatty-acid and glu-
cose metabolism. [PMID:15328533]
In Ex. 3, ?metabolism? along with its premodify-
ing ?fatty-acid?  (but  not  with its  premodifying 
?whole-body?)  are  selected  for  one  annotation, 
as this is a semantic match to the GO term fat?
ty?acid?metabolic?process.   Deter-
miners and quantifiers are never included in con-
cept annotation.  Note that this is an example of a 
discontinuous annotation?an annotation consist-
ing of two or more discontinuous spans of text, 
which is unambiguously represented as standoff.
The use of one or more terminologies in the 
semantic markup of text may result  in overlap-
ping and nesting annotations.  Overlapping refers 
to the overlapping of the selected text of an an-
notation, in  part  or  in  whole, with the selected 
text of another annotation.  Nesting is a type of 
overlapping in which the selected text of an an-
notation is a proper subset of the selected text of 
209
another another.  A nested annotation is created 
only if it is to be annotated with a term that is not 
a superclass of the term used in the nesting anno-
tation.  This is a trivial evaluation if the terms for 
the nesting and nested annotations are from dif-
ferent terminologies,  as one cannot be a super-
class of the other; if the terms are from the same 
terminology, one may or may not be a superclass 
of the other.  There are no corresponding restric-
tions  for  overlapping  annotations  that  are  not 
nesting/nested annotations.
The full CRAFT Corpus annotation guidelines 
can  be  viewed  at  http://bionlp-corpora.source-
forge.net/CRAFT/CRAFT_concept_annotation_
guidelines.pdf and are available for use by others 
under a specified Creative Commons license.
4 Results
To date, we have created more than 107,000 con-
cept annotations; these are broken down by ter-
minology in Table 1.
Terminology # Annotations # Articles
ChEBI 15,313 97
CL 8,290 97
Entrez Gene* 5,618 29
GO BP* 22,101 91
GO CC 7,247 97
GO MF* 5,563 91
NCBI Taxonomy 11,202 97
SO 32,502 97
Total 107,836 -
Table 1. Current counts of annotations 
and articles; * indicates an ongoing pass.
To  illustrate  the  utility  of  our  guidelines,  we 
present the IAAs for six terminological passes of 
the corpus.  As seen in Figs. 2 and 3, the annota-
tors  quickly reach and with few exceptions re-
main at a 90+% IAA level for all of the termino-
logical passes except for the extremely challeng-
ing (and ongoing) GO BP & MF pass, currently 
at  a  typical  80-85%.  As presented previously, 
most of these data points are single-blind statis-
tics; however, as a control, a small number were 
annotated  double-blind,  including  three  articles 
annotated with the SO, which resulted in an IAA 
of 89.9%, compared with a single-blind IAA of 
90.4%  for  the  previous  week,  suggesting  that 
these single-blind IAAs are unlikely to be signif-
icantly biased.
Fig. 2. IAA vs. number of training ses-
sions for annotation of the corpus with 
ChEBI, GO BP & MF, and GO CC.
Fig. 3. IAA vs. number of training ses-
sions for annotation of the corpus with 
SO, CL, and NCBI Taxonomy.
5 Conclusions
We have succinctly presented our concept-anno-
tation  guidelines,  with  which  we  routinely 
achieve high IAAs in the semantic annotation of 
full-text  biomedical  journal  articles.   The  deci-
sions behind these guidelines were made to max-
imally facilitate both manual and programmatic 
annotation of text with the full term sets of termi-
nologies, particularly large ones.  Foremost, the 
decision to annotate a part of the text with a term 
is based on whether this text is a direct semantic 
match to an explicitly represented term, and the 
specific  selection  of text  is  cleanly dictated by 
syntactic rules.   Additionally,  to greatly reduce 
the workload of our human annotators, a nested 
annotation is created only if the term to be used 
is not a superclass of the term used to annotate 
the  nesting  concept  mention.  These  guidelines 
were designed to be used with any ontology or 
terminology and are available for others to use. 
Acknowledgments
The authors gratefully acknowledge their support 
by NIH 5G08M009639 and 5T15 LM009451.
210
References 
Ashburner M, Ball CA, Blake JA, Botstein D, Butler 
H, Cherry JM, Davis AP, Dolinski K, Dwight SS, Ep-
pig JT, Harris MA, Hill DP, Issel-Tarver L, Kasarskis 
A, Lewis S, Matese JC, Richardson JE, Ringwald M, 
Rubin GM, Sherlock G.  2000. Gene Ontology: tool 
for the unification of biology. Nat Genetics, 25:25-29.
Bada, M. and Hunter, L. 2009a.  Using Large Termi-
nologies to Semantically Annotate Concept Mentions 
in Natural-Language Documents.  Proceedings of the 
International Conference on Knowledge Capture (K-
CAP) Semantic Authoring, Annotation and Knowl-
edge Markup (SAAKM) Workshop 2009, Redondo 
Beach, CA, USA.
Bada, M. and Hunter, L. 2009b.  Using the Gene On-
tology to Annotate Biomedical Journal Articles.  Pro-
ceedings of the International Conference on Biomedi-
cal Ontology (ICBO) 2009, Buffalo, NY, USA.
Bard, J., Rhee, S. Y., and Ashburner, M. 2005. An on-
tology for cell types.  Genome Biology, 6(2), R21.
Cohen, K. B., Lanfranchi, A., Corvey, W., Baumgart-
ner, Jr., W. A., Roeder, C., Ogren, P. V., Palmer, M., 
and Hunter, L. E. 2010. Annotation of all coreference 
in biomedical text: Guideline selection and adapta-
tion.  Proceedings of the 7th Language Resources and 
Evaluation Conference (LREC) Workshop on Build-
ing and Evaluating Resources for Biomedical Text 
Mining (BioTxtM), Valletta, Malta.
Degtyarenko, K., de Matos, P., Ennis, M., Hastings, 
J., Zbinden, M., McNaught, A., Alc?ntara, R., Dar-
sow, M., Guedj, M., and Ashburner, M. 2008. ChEBI: 
a database and ontology for chemical entities of bio-
logical interest.  Nucleic Acids Research, 36, Data-
base Issue:D344-D350.
Eilbeck, K., Lewis, S. E., Mungall, C. J., Yandell, M., 
Stein, L., Durbin, R., and Ashburner, M. 2005.  The 
Sequence Ontology: a tool for the unification of 
genome annotations. Genome Biology 6, R44.
Sayers, E. W., Barrett, T., Benson, D. A., Bryant, S. 
H., Canese, K., Chetvernin, V., Church, D. M., 
DiCuccio, M., Edgar, R., Federhen, S., Feolo, M., 
Geer, L. Y., Helmberg, W., Kapustin, Y., Landsman, 
D., Lipman, D. J., Madden, T. L., Maglott, D. R., 
Miller, V., Mizrachi, I., Ostell, J., Pruitt, K. D., 
Schuler, G. D., Sequeira, E., Sherry, S. T., Shumway, 
M., Sirotkin, K., Souvarov, A., Starchenko, G., 
Tatusova, T. A., Wagner, L., Yaschenko, E., and Ye, 
J. 2009.  Database resources of the National Center 
for Biotechnology Information.  Nucleic Acids Re-
search, 37, Database Issue:D5-15.
211
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 222?226,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Revised Arabic PropBank 
Wajdi Zaghouani? , Mona Diab? , Aous Mansouri?, 
Sameer Pradhan? and Martha Palmer? 
 
?Linguistic Data Consortium, ?Columbia University,  
?University of Colorado, ?BBN Technologies 
 
wajdiz@ldc.upenn.edu, mdiab@ccls.columbia.edu, aous.mansouri@colorado.edu,  
pradhan@bbn.com, martha.palmer@colorado.edu 
 
Abstract 
The revised Arabic PropBank (APB) reflects 
a number of changes to the data and the proc-
ess of PropBanking. Several changes stem 
from Treebank revisions. An automatic proc-
ess was put in place to map existing annota-
tion to the new trees. We have revised the 
original 493 Frame Files from the Pilot APB 
and added 1462 new files for a total of 1955 
Frame Files with 2446 framesets. In addition 
to a heightened attention to sense distinctions 
this cycle includes a greater attempt to ad-
dress complicated predicates such as light 
verb constructions and multi-word expres-
sions. New tools facilitate the data tagging 
and also simplify frame creation. 
1 Introduction 
Recent years have witnessed a surge in available 
automated resources for the Arabic language. 1 
These resources can now be exploited by the 
computational linguistics community with the 
aim of improving the automatic processing of 
Arabic. This paper discusses semantic labeling. 
  
Shallow approaches to semantic processing are 
making large advances in the direction of effi-
ciently and effectively deriving application rele-
vant explicit semantic information from text 
(Pradhan et al, 2003; Gildea and Palmer, 2002; 
Pradhan et al, 2004; Gildea and Jurafsky, 2002; 
Xue and Palmer, 2004; Chen and Rambow, 
2003; Carreras and Marquez, 2005; Moschitti, 
2004; Moschitti et al, 2005; Diab et al, 2008). 
Indeed, the existence of semantically annotated 
resources in English such as FrameNet (Baker et 
al., 1998) and PropBank (Kingsbury and Palmer, 
2003; Palmer et al, 2005) corpora have marked a 
surge in efficient approaches to automatic se-
                                               
1 In this paper, we use Arabic to refer to Modern Standard 
Arabic (MSA). 
mantic labeling of the English language. For ex-
ample, in the English sentence, ?John enjoys 
movies?, the predicate is ?enjoys? and the first 
argument, the subject, is ?John?, and the second 
argument, the object, is ?movies?. ?John? would 
be labeled as the agent/experiencer and ?movies? 
would be the theme/content. According to Prop-
Bank, ?John? is labeled Arg0 (or enjoyer) and 
?movies? is labeled Arg1 (or thing enjoyed). Cru-
cially, that independent of the labeling formalism 
adopted, the labels do not vary in different syn-
tactic constructions, which is why proposition 
annotation is different from syntactic Treebank 
annotation. For instance, if the example above 
was in the passive voice, ?Movies are enjoyed by 
John?, ?movies? is still the Theme/Content (Arg1) 
and (thing enjoyed), while ?John? remains the 
Agent/Experiencer (Arg0) and (enjoyer). Like-
wise for the example ?John opened the door? vs. 
?The door opened?, in both of these examples 
?the door? is the Theme (Arg1). In addition to 
English, there are PropBank efforts in Chinese 
(Xue et al, 2009), Korean (Palmer et al 2006) 
and Hindi (Palmer et al, 2009), as well as Fra-
meNet annotations in Chinese, German, Japa-
nese, Spanish and other languages (Hans 2009). 
Being able to automatically apply this level of 
analysis to Arabic is clearly a desirable goal, and 
indeed, we began a pilot Arabic PropBank effort 
several years ago (Palmer et al, 2008). 
  
In this paper, we present recent work on adapting 
the original pilot Arabic Proposition Bank (APB) 
annotation to the recent changes that have been 
made to the Arabic Treebank (Maamouri et al, 
2008). These changes have presented both lin-
guistic and engineering challenges as described 
in the following sections. In Section 2 we discuss 
major linguistics changes in the Arabic Treebank 
annotation, and any impact they might have for 
the APB effort. In Section 3 we discuss the engi-
neering ramifications of adding and deleting 
nodes from parse trees, which necessitates mov-
222
ing all of the APB label pointers to new tree lo-
cations. Finally, in Section 4 we discuss the cur-
rent APB annotation pipeline, which takes into 
account all of these changes. We conclude with a 
statement of our current goals for the project.  
2 Arabic Treebank Revision and APB 
The Arabic syntactic Treebank Part 3 v3.1 was 
revised according to the new Arabic Treebank 
Annotation Guidelines. Major changes have af-
fected the NP structure and the classification of 
verbs with clausal arguments, as well as im-
provements to the annotation in general.2  
  
The Arabic Treebank (ATB) is at the core of the 
APB annotations. The current revisions have re-
sulted in a more consistent treebank that is closer 
in its analyses to traditional Arabic grammar. 
The ATB was revised for two levels of linguistic 
representation, namely morphological informa-
tion and syntactic structure. Both of these 
changes have implications for APB annotations.  
 
The new ATB introduced more consistency in 
the application of morphological features to POS 
tags, hence almost all relevant words in the ATB 
have full morphological features of number, 
gender, case, mood, and definiteness associated 
with them. This more comprehensive application 
has implications on agreement markers between 
nouns and their modifiers and predicative verbs 
and their arguments, allowing for more consis-
tent semantic analysis in the APB. 
  
In particular, the new ATB explicitly marks the 
gerunds in Arabic known as maSAdir (singular 
maSdar.) MaSAdirs, now annotated as VN, are 
typically predicative nouns that take arguments 
that should receive semantic roles. The nouns 
marked as VN are embedded in a new kind of 
syntactic S structure headed by a VN and having 
subject and object arguments similar to verbal 
arguments. This syntactic structure, namely S-
NOM, was present in previous editions/versions 
of the ATB but it was headed by a regular noun, 
hence it was difficult to find. This explicit VN 
annotation allows the APB effort to take these 
new categories into account as predicates. For 
instance [????]VN [-??]ARG0 [????? ?????]ARG1, 
transliterated as takab~udi-,  meaning 'suffered' 
                                               
2 For a complete description of the new Treebank annotation 
guidelines, see (Arabic Treebank Morphological and Syn-
tactic Annotation Guidelines 2008) at 
http://projects.ldc.upenn.edu/ArabicTreebank/. 
is an example of predicative nominal together 
with its semantically annotated arguments ARG0 
transliterated as -him, meaning 'they' and ARG1 
transliterated as xasA}ira kabiyrap, meaning 
'heavy losses'. 
 
Other changes in the ATB include idafa con-
structions (a means of expressing possession) 
and the addition of a pseudo-verb POS tag for a 
particular group of particles traditionally known 
as ?the sisters of  ??? <in~a 'indeed' ?. These have 
very little impact on the APB annotation. 
3 Revised Treebank processing 
One of the challenges that we faced during the 
process of revising the APB was the transfer of 
the already existing annotation to the newly re-
vised trees -- especially since APB data encoding 
is tightly coupled with the explicit tree structure. 
Some of the ATB changes that affected APB 
projection from the old pilot effort to the new 
trees are listed as follows:  
i. Changes to the tree structure 
ii. Changes to the number of tokens -- both 
modification (insertion and deletion) of 
traces and modification to some tokeni-
zation 
iii. Changes in parts of speech 
iv. Changes to sentence breaks 
The APB modifications are performed within the 
OntoNotes project (Hovy et al 2006), we have 
direct access to the OntoNotes DB Tool, which 
we extended to facilitate a smooth transition. The 
tool is modified to perform a three-step mapping 
process: 
 
a) De-reference the existing (tree) node-level 
annotations to the respective token spans; 
 
b) Align the original token spans to the best pos-
sible token spans in the revised trees. This was 
usually straight forward, but sometimes the to-
kenization affected the boundaries of a span in 
which case careful heuristics had to be employed 
to find the correct mapping. We incorporated the 
standard "diff" utility into the API. A simple 
space separated token-based diff would not com-
pletely align cases where the tokenization had 
been changed in the new tree. For these cases we 
had to back-off to a character based alignment to 
recover the alignments. This two-pass strategy 
works better than using character-based align-
223
ment as a default since the diff tool does not have 
any specific domain-level constraints and gets 
spurious alignments; 
 
c) Create the PropBank (tree) node-pointers for 
the revised spans. 
 
As expected, this process is not completely 
automatic. There are cases where we can deter-
ministically transfer the annotations to the new 
trees, and other cases (especially ones that in-
volve decision making based on newly added 
traces) where we cannot. We automatically trans-
ferred all the annotation that could be done de-
terministically, and flagged all the others for hu-
man review. These cases were grouped into mul-
tiple categories for the convenience of the anno-
tators. Some of the part of speech changes in-
validated some existing annotations, and created 
new predicates to annotate. In the first case, we 
simply dropped the existing annotations on the 
affected nodes, and in the latter we just created 
new pointers to be annotated. We could auto-
matically map roughly 50% of the annotations. 
The rest are being manually reviewed. 
4 Annotation Tools and Pipeline 
4.1 Annotation process 
APB consists of two major portions: the lexicon 
resource of Frame Files and the annotated cor-
pus. Hence, the process is divided into framing 
and annotation (Palmer et al, 2005). 
 
Currently, we have four linguists (framers) creat-
ing predicate Frame Files. Using the frame crea-
tion tool Cornerstone, a Frame File is created for 
a specific lemma found in the Arabic Treebank. 
The information in the Frame File must include 
the lemma and at least one frameset.  
 
Previously, senses were lumped together into a 
single frame if they shared the same argument 
structure. In this effort, however, we are attempt-
ing to be more sensitive to the different senses 
and consequently each unique sense has its own 
frameset. A frameset contains an English defini-
tion, the argument structure for the frameset, a 
set of (parsed) Arabic examples as an illustration, 
and it may include Arabic synonyms to further 
help the annotators with sense disambiguation.  
 
Figure 1 illustrates the Frameset for the verb 
????? } isotamaE 'to listen' 
 
Predicate: {isotamaE ????? 
Roleset id: f1, to listen 
Arg0: entity listening 
Arg1: thing listened 
 
Figure 1. The frameset of the verb {isotamaE 
         
 
Rel: {isotamaE, ????? 
Arg0: -NONE- * 
Gloss: He 
Arg1: ??? ??????? 
Gloss: to their demands 
Example: ?????  ??? ??????? 
 
Figure 2. An example annotation for a sentence 
containing the verb {isotamaE 
 
In addition to the framers, we also have five na-
tive Arabic speakers as annotators on the team, 
using the annotation tool Jubilee (described be-
low). Treebanked sentences from the ATB are 
clearly displayed in Jubilee, as well as the raw 
text for that sentence at the bottom of the screen. 
The verb that needs to be tagged is clearly 
marked on the tree for the annotators. A drop-
down menu is available for the annotators to use 
so that they may choose a particular frameset for 
that specific instance. Once a frameset is chosen 
the argument structure will be displayed for them 
to see. As a visual aid, the annotators may also 
click on the ?example? button in order to see the 
examples for that particular frameset. Finally, the 
complements of the predicate are tagged directly 
on the tree, and the annotators may move on to 
the next sentence. Figure 2 illustrates a sample 
annotation. 
 
Once the data has been double-blind annotated, 
the adjudication process begins. An adjudicator, 
a member of the framing team, provides the Gold 
Standard annotation by going over the tagged 
instances to settle any differences in the choices. 
Occasionally a verb will be mis-lemmatized (e.g. 
the instance may actually be ????? sah~al 'to cause 
to become easy' but it is lemmatized under ????? 
sahul-u 'to be easy' which looks identical without 
vocalization.) At this point the lemmas are cor-
rected and sent back to the annotators to tag be-
fore the adjudicators can complete their work. 
 
The framers and annotators meet regularly at 
least every fortnight. These meetings are impor-
tant for the framers since they may need to con-
vey to the annotators any changes or issues with 
the frames, syntactic matters, or anything else 
that may require extra training or preparation for 
224
the annotators. It is important to note that while 
the framers are linguists, the annotators are not. 
This means that the annotators must be instructed 
on a number of things including, but not limited 
to, how to read trees, and what forms a constitu-
ent, as well as how to get familiar with the tools 
in order to start annotating the data. Therefore, 
little touches, such as the addition of Arabic 
synonyms to the framesets (especially since not 
all of the annotators have the same level of flu-
ency in English), or confronting specific linguis-
tic phenomena via multiple modalities are a nec-
essary part of the process. To these meetings, the 
annotators mostly bring their questions and con-
cerns about the data they are working on. We 
rely heavily on the annotator?s language skills. 
They take note of whether a frame appears to be 
incorrect, is missing an argument, or is missing a 
sense. And since they go through every instance 
in the data, annotators are instrumental for point-
ing out any errors the ATB. Since everything is 
discussed together as a group people frequently 
benefit from the conversations and issues that are 
raised. These bi-monthly meetings not only help 
maintain a certain level of quality control but 
establish a feeling of cohesion in the group. 
 
The APB has decided to thoroughly tackle light 
verb constructions and multi-word expressions as 
part of an effort to facilitate mapping between 
the different languages that are being Prop-
Banked. In the process of setting this up a num-
ber of challenges have surfaced which include: 
how can we cross-linguistically approach these 
phenomena in a (semi) integrated manner, how 
to identify one construction from the other, figur-
ing out a language specific reliable diagnostic 
test, and whether we deal with these construc-
tions as a whole unit or as separate parts; and 
how? (Hwang, et al, 2010) 
4.2 Tools 
Frameset files are created in an XML format. 
During the Pilot Propbank project these files 
were created manually by editing the XML file 
related to a particular predicate. This proved to 
be time consuming and prone to many formatting 
errors. The Frame File creation for the revised 
APB is now performed with the recently devel-
oped Cornerstone tool (Choi et al, 2010a), which 
is a PropBank frameset editor that allows the 
creation and editing of Propbank framesets with-
out requiring any prior knowledge of XML. 
Moreover, the annotation is now performed by 
Jubilee, a new annotation tool, which has im-
proved the annotation process by displaying sev-
eral types of relevant syntactic and semantic in-
formation at the same time. Having everything 
displayed helps the annotator quickly absorb and 
apply the necessary syntactic and semantic in-
formation pertinent to each predicate for consis-
tent and efficient annotation (Choi et al, 
20010b). Both tools are available as Open Source 
tools on Google code.3 
4.3 Current Annotation Status and Goals 
We have currently created 1955 verb predicate 
Frame Files which correspond to 2446 framesets, 
since one verb predicate Frame File can contain 
one or more framesets. We will reconcile the 
previous Arabic PropBank with the new Tree-
bank and create an additional 3000 Frame files to 
cover the rest of the ATB3 verb types.  
5 Conclusion  
This paper describes the recently revived and 
revised APB. The changes in the ATB have af-
fected the APB in two fundamentally different 
ways. More fine-grained POS tags facilitate the 
tasks of labeling predicate argument structures. 
However, all of the tokenization changes have 
rendered the old pointers obsolete, and new 
pointers to the new constituent boundaries have 
to be supplied. This task is underway, as well as 
the task of creating several thousand additional 
Frame Files to complete predicate coverage of 
ATB3. 
 
Acknowledgments 
 
We gratefully acknowledge a grant from the De-
fense Advanced Research Projects Agency 
(DARPA/IPTO) under the GALE program, 
DARPA/CMO Contract No. HR0011-06-C-
0022, subcontract from BBN, Inc. We also thank 
Abdel-Aati Hawwary and Maha Saliba Foster 
and our annotators for their invaluable contribu-
tions to this project. 
References  
Boas, Hans C. 2009. Multilingual FrameNets. In 
Computational Lexicography: Methods and Appli-
cations. Berlin: Mouton de Gruyter. pp. x+352 
Carreras, Xavier & Llu?s M?rquez. 2005. Introduction 
to the CoNLL-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI, USA. 
                                               
3 http://code.google.com/p/propbank/ 
225
Chen, John & Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Choi, Jinho D., Claire Bonial, & Martha Palmer. 
2010a. Propbank Instance Annotation Guidelines 
Using a Dedicated Editor,Cornerstone. In Proceed-
ings of the 7th International Conference on Lan-
guage Resources and Evaluation 
(LREC'10),Valletta, Malta. 
Choi, Jinho D., Claire Bonial, & Martha Palmer. 
2010b. Propbank Instance Annotation Guidelines 
Using a Dedicated Editor,Jubilee. In Proceedings 
of the 7th International Conference on Language 
Resources and Evaluation (LREC'10),Valletta, 
Malta. 
Diab, Mona, Alessandro Moschitti, & Daniele Pighin. 
2008. Semantic Role Labeling Systems for Arabic 
using Kernel Methods. In Proceedings of ACL. As-
sociation for Computational Linguistics, Colum-
bus, OH, USA. 
Gildea, Daniel & Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Gildea, Daniel & Martha Palmer. 2002. The necessity 
of parsing for predicate argument recognition. In 
Proceedings of the 40th Annual Conference of the 
Association for Computational Linguistics (ACL-
02), Philadelphia, PA, USA. 
Gusfield, Dan. 1997. Algorithms on Strings, Trees 
and Sequences. Cambridge University Press, 
Cambridge, UK. 
Habash, Nizar & Owen Rambow. 2007. Arabic dia-
critization through full morphological tagging. In 
HLT-NAACL 2007; Companion Volume, Short Pa-
pers, Association for Computational Linguistics, 
pages 53?56, Rochester, NY, USA. 
Hovy, Eduard, Mitchell Marcus, Martha Palmer, 
Lance Ramshaw & Ralph Weischedel. 2006. 
OntoNotes: The 90% Solution. In Proceedings of 
HLT-NAACL 2006, New York, USA. 
Hwang, Jena D., Archna Bhatia, Clare Bonial, Aous 
Mansouri, Ashwini Vaidya, Nianwen Xue & Mar-
tha Palmer. 2010. PropBank Annotation of Multi-
lingual Light Verb Constructions. In Proceedings 
of the LAW-ACL 2010. Uppsala, Sweden. 
Maamouri, Mohamed, Ann Bies, Seth Kulick. 2008. 
Enhanced Annotation and Parsing of the Arabic 
Treebank. In Proceedings of INFOS 2008, Cairo, 
Egypt. 
M?rquez, Llu?s. 2009. Semantic Role Labeling. Past, 
Present and Future . TALP Research Center. Tech-
nical University of Catalonia. Tutorial at ACL-
IJCNLP 2009. 
Moschitti, Alessandro. 2004. A study on convolution 
kernels for shallow semantic parsing. In proceed-
ings of the 42th Conference on Association for 
Computational Linguistic (ACL-2004), Barcelona, 
Spain.  
Moschitti, Alessandro, Ana-Maria Giuglea, Bonaven-
tura Coppola, & Roberto Basili. 2005. Hierarchical 
semantic role labeling. In Proceedings of CoNLL-
2005, Ann Arbor, MI, USA. 
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona 
Diab, Mohamed Maamouri, Aous Mansouri, Wajdi 
Zaghouani. 2008. A Pilot Arabic Propbank. In 
Proceedings of LREC 2008, Marrakech, Morocco. 
Palmer, Martha, Rajesh Bhatt, Bhuvana Narasimhan, 
Owen Rambow, Dipti Misra Sharma, & Fei Xia. 
2009. Hindi Syntax: Annotating Dependency, 
Lexical Predicate-Argument Structure, and Phrase 
Structure. In The 7th International Conference on 
Natural Language Processing (ICON-2009), Hy-
derabad, India. 
Palmer, Martha, Daniel Gildea, & Paul Kingsbury.  
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 31, 
1 (Mar. 2005), 71-106. 
Palmer, Martha, Shijong Ryu, Jinyoung Choi, Sinwon 
Yoon, & Yeongmi Jeon. 2006. LDC Catalog 
LDC2006T03. 
Pradhan, Sameer, Kadri Hacioglu, Wayne Ward, 
James H. Martin, & Daniel Jurafsky. 2003. Seman-
tic role parsing: Adding semantic structure to un-
structured text. In Proceedings of ICDM-2003, 
Melbourne, USA. 
Pradhan, Sameer S., Wayne H Ward, Kadri Hacioglu, 
James H Martin, & Dan Jurafsky. 2004. Shallow 
semantic parsing using support vector machines. In 
Susan Dumais, Daniel Marcu, & Salim Roukos, 
editors, HLT-NAACL 2004: Main Proceedings, 
pages 233?240, Boston, MA, USA. 
Xue, Nianwen & Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
& Dekai Wu, editors, Proceedings of ACL-EMNLP 
2004, pages 88?94, Barcelona, Spain.  
Xue, Nianwen & Martha Palmer. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural 
Language Engineering, 15 Jan. 2009, 143-172. 
226
	ABBCBBDEFFABAAB

BAFDFDBDISCUSS: A dialogue move taxonomy layered over semantic
representations
Lee Becker1 Wayne H. Ward1,2 Sarel van Vuuren1 Martha Palmer1
{lee.becker, martha.palmer, sarel.vanvuuren}@colorado.edu,
wward@bltek.com
1University of Colorado at Boulder, 2Boulder Language Technologies
Abstract
In this paper we describe DISCUSS, a dialogue move taxonomy layered over semantic represen-
tations. We designed this scheme to enable development of computational models of tutorial dia-
logues and to provide an intermediate representation suitable for question and tutorial act generation.
As such, DISCUSS captures semantic and pragmatic elements across four dimensions: Dialogue Act,
Rhetorical Form, Predicate Type, Semantic Roles. Together these dimensions provide a summary of
an utterance?s propositional content and how it may change the underlying information state of the
conversation. This taxonomy builds on previous work in both general dialogue act taxonomies as
well as work in tutorial act and tutorial question categorization. The types and values found within
our taxonomy are based on preliminary observations and on-going annotation from our corpus of
multimodal tutorial dialogues for elementary school science education.
1 Introduction
Past successes with conversational Intelligent Tutoring Systems (ITS) (Graesser et al, 2001), have helped
to demonstrate the efficacy of computer-led, tutorial dialogue. However, ITS will not reach their full
potential until they can overcome current limitations in spoken dialogue technologies. Producing systems
capable of leading open-ended, Socratic-style tutorials will likely require more sophisticated models to
automate analysis and generation of dialogue. A well defined tutorial dialogue annotation scheme can
serve as a stepping stone towards these goals. Such a scheme should account for differences in tutoring
style and question scaffolding techniques and should capture the subtle distinctions between different
question types. To do this, requires a representation that connects a turn?s communicative and rhetorical
functions to its underlying semantic content.
While efforts such as DAMSL (Core and Allen, 1997) and DIT++ (Bunt, 2009) have helped to make
dialogue act annotation more uniform and applicable to a wider audience, and while tutoring-specific
initiatives (Tsovaltzi and Karagjosova, 2004; Buckley and Wolska, 2008) have helped to bring dialogue
acts to tutorial dialogue, the move granularity in these schemas is too coarse to capture the differences
in tutorial questioning styles exhibited in our corpus of Socratic-style tutorial dialogues. Conversely,
question type categories (Graesser and Person, 1994; Nielsen et al, 2008) have been designed with
education in mind, but they largely ignore how the student and tutor may work together to construct
meaning. The DISCOUNT scheme?s (Pilkington, 1999) combination of dialogue acts and rhetorical
functions enabled it to better capture tutoring moves, but its omission of shallow semantics prevents it
from capturing how content influences behavior.
Our long-term goals of automatic dialogue characterization, tutorial move prediction and question
generation led us to design our own dialogue representation called DISCUSS (Dialogue Scheme for
Unifying Speech and Semantics). Design of this dialogue move taxonomy was based on preliminary
observations from our corpus of tutorial dialogues, and was influenced by the aforementioned research.
We hope that undertaking this ambitious endeavor to capture not only a turn?s pragmatic interpretation,
310
but also its rhetorical and semantic functions will enable us to better model the complexity of open-ended,
tutorial dialogue.
The remainder of the this paper is organized as follows. In the next section we describe our tutorial
dialogue setting and our data. Section 3 discusses the organization of the DISCUSS annotation scheme.
Section 4 briefly explains the current status of our annotation. Lastly section 5 outlines our future plans
and conclusions.
2 Tutorial Dialogue Setting and Data
My Science Tutor (MyST) (Ward et al, 2010) is a conversational virtual tutor designed to improve
science learning and understanding for students in grades 3-5. Students using MyST investigate and
discuss science through natural spoken dialogues and multimedia interactions with a virtual tutor named
Marni. The MyST dialogue design and tutoring style is based on a pedagogy called Questioning the
Author (QtA) (Beck et al, 1996), wherein the teacher facilitates discovery by challenging students with
open-ended questions and by directly keying in on ideas expressed in the student?s language.
To gather data for MyST system coverage and dialogue analysis, we ran Wizard-of-Oz (WoZ) exper-
iments that allowed a human tutor to be inserted into the interaction loop. Project tutors trained in QtA
served as Wizards and were responsible for accepting and overriding system actions. Over the past three
years we have accumulated over five-hundred, 15-minute WoZ sessions across four modules Magnetism
and Electricity, Measurement, Variables, and Water, each with 16 lessons. Student speech from these
sessions was professionally transcribed at the word level.
3 The DISCUSS Annotation Scheme
The Dialogue Scheme for Unifying Speech and Semantics (DISCUSS) is a multifaceted dialogue move
taxonomy intended to capture both the pragmatic and semantic interpretations of an utterance. A DIS-
CUSS move is a tuple composed of values from four dimensions: Dialogue Act, Rhetorical Form, Pred-
icate Type, and Semantic Roles. Together these dimensions convey the communicative action, surface
form, and meaning of an utterance independent of the original utterance text.
We designed DISCUSS to serve as an intermediate representation that will enable future work in
dialogue session characterization, dialogue strategy optimization, and automatic question generation. To
facilitate these goals, we have endeavored to create a taxonomy that is both descriptive and curriculum-
independent while allowing for expansion as necessary. A complete listing of all the DISCUSS moves
and dimensions can be found in our forthcoming technical report.
In the following subsection we will describe the different DISCUSS move categories. Descriptions
of the Semantic Role and Predicate Type are found in the subsection about semantic dimensions, while
discussion about the dialogue act and rhetorical form has been placed in the pragmatic dimensions
subsection. Throughout the rest of this paper we denote DISCUSS tuples using the following notation:
Dialogue Act/Rhetorical Form/Predicate Type ?Semantic Role?.
3.1 Move Categories
DISCUSS moves are dictated by the dialogue act dimension and may belong to one of three broad cate-
gories: Dialogue Control, Information Exchange, and Attention Management. Dialogue Control moves
are largely concerned with maintaining and enabling the flow of information. This includes dialogue
acts such as Acknowledge, Open, Close, Repeat, and RequestRepeat. The Information Exchange moves
relay content (often lesson-specific) between speakers using moves such as Assert, Ask, Answer, Mark,
Revoice. For tutorial dialogue the bulk of student-tutor interactions reside in this category. Lastly, At-
tention Management moves indicate how a speaker exercises initiative over other speakers or topics.
Dialogue acts found in the attention category are Focus, Defer, Elicit, and Direct.
311
3.2 Semantic Dimensions
The semantic dimensions define the objects, events, properties and relations contained within an utter-
ance. The semantic roles at the lowest level of the DISCUSS hierarchy directly capture the propositional
entities. Predicate Types summarize the interactions between all of the semantic roles found within an
utterance.
Semantic Roles: The MyST system models a lesson?s key concepts as propositions which are real-
ized as semantic frames. For MyST natural language understanding, these frames serve as the top-level
nodes for a manually written semantic grammar used by the Phoenix parser (Ward, 1994). Two example
concepts/frames and Phoenix parses are shown below. Although these semantic frames form the basis
of MyST dialogues, for DISCUSS annotation we sought a more domain-independent representation that
would generalize across a wide range of subjects. We began with VerbNet (Schuler, 2005) for defining
our set of semantic roles because of its intuitive balance between descriptiveness and portability. While
we used a majority of the labels as is, we found that the definition of some roles needed to be modified
or extended to properly cover our set of concepts. For example, many concepts that express proportion-
ality relationships can not be easily represented using predicate argument structure, and are more easily
decomposed into cause and effect roles. We also added the catch-all keyword label to reflect terms that
may relate to the proposition, but are not part of the core representation.
For our annotation project, rather than manually tagging all of the utterances with VerbNet labels, we
created a mapping layer between the Phoenix frame roles and the VerbNet roles. The table below shows
two frames along with their role mappings. We envision that in future projects, the hand-tuned semantic
grammars could be replaced with a statistically trained semantic role labeler.
Frame: BatteryFunction Frame: MagnetsAttract
Description: The DCell is the source of elec-
tricity.
Description: Magnets attract to certain ob-
jects.
?Instrument?: [Battery] ?Instrument?: [Magnet]
?Predicate?: [Source] ?Predicate?: [Attract]
?Theme?: [Electricity] ?Theme?: [Object]
Predicate Type: Simply knowing an utterance?s propositional content is insufficient for inferring
what was stated. Consider the two exchanges shown in the table below. The mixture of semantic roles
in both students? responses are identical. Additionally, we can not differentiate between the exchanges
based solely on dialogue act or rhetorical form. We need additional information to know the first scenario
seeks to elicit discussion about observations while the second scenario focuses on procedures. One can
also imagine such information would be useful for identifying communication breakdowns. For example,
responding with a description of a procedure to a request about a process may indicate that the student
did not understand the question or that the student is unwilling or unable to address the question.
T12: Tell me about what?s going on here in this picture.
Ask/Describe/Observation
S13: The wires connect the battery and the light bulb and then then light bulb lights up.
Answer/Describe/Observation
?Instrument?.wires ?Predicate?.connect ?Theme1?.battery ?Theme2?.light bulb ?Effect?.bulb
lights up
T7: Tell me about how you got the bulb to light up.
Ask/Describe/Procedure
S8: To make the light go we connected the wires to the battery and the bulb.
Answer/Describe/Procedure
?Effect?.light go ?Predicate?.connected ?Instrument?.wires ?Theme1?.battery ?Theme2?.bulb
To address this need, we created the Predicate Type based partly on the rhetorical predicates used in
the DISCOUNT (Pilkington, 1999) scheme. While DISCOUNT included discourse relations in the set
of predicate types, we restrict predicate types to those that encapsulate or summarize the collection of
semantic roles in an utterance. Example predicate types include procedure, observation and purpose. A
complete list of predicate types can be found in our forthcoming technical report.
312
3.3 Pragmatic Dimensions
The pragmatic dimensions are composed of the dialogue act dimension and the rhetorical form dimen-
sion. The dialogue act expresses the communicative function of a move and is the most general dimen-
sion in DISCUSS. The rhetorical form expresses attributes of the utterance?s surface realization and can
be thought of as refining the intent of the coarser dialogue act.
Dialogue Act: The dialogue act dimension is the top-level dimension in DISCUSS with the values
of all other dimensions depending on the value of this dimension. Like with the majority of dialogue
act taxonomies, DISCUSS dialogue acts have a grounding in speech act theory with a focus on what
action the utterance performs. While most of the dialogue acts in the Dialogue Control and Informa-
tion Exchange move categories have direct corollaries to those found in other taxonomies like DIT++ or
DAMSL, we needed to supplement them with two frequently used Questioning the Author discussion
moves: marking and revoicing. In marking, the tutor highlights parts of the student?s language to em-
phasize important points and to steer the conversation towards key concepts. Revoicing serves a similar
purpose, but instead of highlighting, the tutor rephrases student speech to clarify ideas they may have
been struggling with. Examples of these acts are shown below.
S5: that when you stick a magnet to a rusty nail and then you stick it to a paper clip it sticks
Answer/Describe/Process
T6: I think I heard you say something about magnets sticking or attracting. Tell me more about that.
Mark/None/None, Ask/Elaborate/Process
S33: well when you scrub the the paperclip to the magnet the paperclip is starting to be a magnet
Answer/Describe/Process
T34: very good, so if the magnet gets close to the paperclip it picks it up
Feedback/Positive/None, Revoice/None/None
Dialogue acts in the Attention Management move category also reflect many of the actions regularly
seen in tutorial dialogue. Focus and Defer acts are often used to move to or away from lesson-specific
topics. In our corpus Direct is typically used to give instructions related to the multimedia (e.g. ?Click
on the box? or ?Look at this animation.?).
Rhetorical Form: The DISCUSS Rhetorical Form dimension provides another mechanism for dif-
ferentiating between utterances with identical semantic content. While the dialogue act dimension is
useful for providing an utterance?s pragmatic interpretation and for determining what sequences are li-
censed, by itself it provides no indication of how a speaker is advancing the topic under discussion.
Additional information is needed to create an utterance?s surface form. Consider the two transactions
in the table below. The semantic parses in both scenarios would be identical, however the tutor?s ques-
tions and the resulting student response serve very different functions. In the first, the tutor is asking
for a description and in the second, identification. Selection of the DISCUSS rhetorical forms found in
the Information Exchange move category were inspired by the sixteen top-level tags used in Rhetori-
cal Structure Theory (RST) (Mann and Thompson, 1988). While RST uses a rhetorical relation to link
clauses and to show the development of an argument, DISCUSS uses the rhetorical form to refine the
dialogue act. A sequence of dialogue acts paired with rhetorical forms can show progressions in the
dialogue and tutoring process such as a shift from open-ended to directed questioning.
T1: Can you tell which one is the battery? T1: Can you describe what is going on with the battery?
Ask/Describe/Visual Ask/Identify/None
S2: The battery is putting out electricity. S2: The battery is the one putting out the electricity.
Answer/Describe/Process Answer/Identify/None
4 Annotation Status
We are still in the early stages of this ambitious annotation project. We currently have approximately
60 transcripts singly-annotated with DISCUSS moves. Each of these transcripts represents roughly 15
minutes of conversation and 50 turns on average. The DISCUSS taxonomy is a work in progress. Though
313
we have created the tags for each dimension based on a wide body of prior research and on preliminary
studies of our transcripts, we expect that future analysis of our annotation reliability and consistency will
likely lead us to add, modify, and combine tags. We anticipate that DISCUSS?s multidimensional nature
will likely raise issues for inter-annotator reliability, and the ability to add multiple tags per turn will
further complicate the process of evaluating agreement.
5 Future Work and Conclusions
We plan to use our corpus of DISCUSS annotated tutorial dialogues to build dialogue models for a variety
of applications including assessment of tutorial quality and dialogue move prediction. This annotation
will allow us to investigate what features of tutorial dialogue correlate with increased learning gains and
what types of questions encourage greater student interaction. Data-driven dialogue characterization will
also allow us to explore how tutorial tactics vary across domains and tutors. We envision this work as an
important first step towards automatic question generation.
In this paper we introduced the DISCUSS dialogue move taxonomy. This scheme overlays dialogue
act and rhetorical annotation over semantic representations. We believe this combination of pragmatic
interpretations and semantic representations provide an intermediate representation rich enough to an-
alyze the interactions in a complex task-oriented domain like tutorial dialogue. Furthermore, we think
DISCUSS moves can succinctly summarize the actions of a speaker?s turn, while still providing suffi-
cient information for natural language generation of dialogue moves.
Acknowledgments This work was supported by grants from the NSF (DRL-0733322, DRL-0733323) and the IES (R3053070434).
Any findings, recommendations, or conclusions are those of the author and do not necessarily represent the views of NSF or
IES.
References
Beck, I. L., M. G. McKeown, J. Worthy, C. A. Sandora, and L. Kucan (1996). Questioning the author: A year-long classroom
implementation to engage students with text. The Elementary School Journal 96(4), 387?416.
Buckley, M. and M. Wolska (2008). A classification of dialogue actions in tutorial dialogue. In Proc. COLING, pp. 73?80.
ACL.
Bunt, H. (2009). The dit++ taxonomy for functional dialogue markup. In Proc. EDAML 2009.
Core, M. and J. Allen (1997). Coding dialogs with the damsl annotation scheme. In AAAI Fall Symposium on Comm. Action in
Humans and Machines, pp. 28?35.
Graesser, A., X. Hu, S. Susarla, D. Harter, N. Person, M. Louwerse, B. Olde, and the Tutoring Research Group (2001).
Autotutor: An intelligent tutor and conversational tutoring scaffold. In Proc. AIED?01, pp. 47?49.
Graesser, A. and N. Person (1994). Question asking during tutoring. American Educational Research Journal 31, 104?137.
Mann, W. C. and S. A. Thompson (1988). Rhetorical structure theory: Toward a functional theory of text organization. Text 8(3),
243?281.
Nielsen, R. D., J. Buckingham, G. Knoll, B. Marsh, and L. Palen (2008, September). A taxonomy of questions for question
generation. In Proc. WS on the Question Generation STEC.
Pilkington, R. M. (1999). Analysing educational discourse: The discount scheme. Technical Report 99/2, Computer Based
Learning Unit, University of Leeds.
Schuler, K. K. (2005). VerbNet: A broad-coverage, comprehensive verb lexicon. Ph. D. thesis, University of Pennsylvania.
Tsovaltzi, D. and E. Karagjosova (2004). A view on dialogue move taxonomies for tutorial dialogues. In Proc. SIGDial, pp.
35?38. ACL.
Ward, W. (1994). Extracting information from spontaneous speech. In Proc. ICSLP.
Ward, W., R. Cole, D. Bolanos, C. Buchenroth-Martin, E. Svirsky, S. Van Vuuren, T. Weston, J. Zheng, and L. Becker (2010).
My science tutor: A conversational multi-media virtual tutor for elementary school science. ACM TSLP: Special Issue on
Speech and Language Processing of Children?s Speech for Child-machine Interaction Applications.
314
Proceedings of the Fifth Law Workshop (LAW V), pages 21?29,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Analysis of the Hindi Proposition Bank using Dependency Structure
Ashwini Vaidya Jinho D. Choi Martha Palmer Bhuvana Narasimhan
Institute of Cognitive Science
University of Colorado at Boulder
{vaidyaa,choijd,mpalmer,narasimb}@colorado.edu
Abstract
This paper makes two contributions. First, we
describe the Hindi Proposition Bank that con-
tains annotations of predicate argument struc-
tures of verb predicates. Unlike PropBanks
in most other languages, the Hind PropBank
is annotated on top of dependency structure,
the Hindi Dependency Treebank. We explore
the similarities between dependency and pred-
icate argument structures, so the PropBank an-
notation can be faster and more accurate. Sec-
ond, we present a probabilistic rule-based sys-
tem that maps syntactic dependents to seman-
tic arguments. With simple rules, we classify
about 47% of the entire PropBank arguments
with over 90% confidence. These preliminary
results are promising; they show how well
these two frameworks are correlated. This can
also be used to speed up our annotations.
1 Introduction
Proposition Bank (from now on, PropBank) is a cor-
pus in which the arguments of each verb predicate
are annotated with their semantic roles (Palmer et
al., 2005). PropBank annotation has been carried
out in several languages; most of them are annotated
on top of Penn Treebank style phrase structure (Xue
and Palmer, 2003; Palmer et al, 2008). However, a
different grammatical analysis has been used for the
Hindi PropBank annotation, dependency structure,
which may be particularly suited for the analysis of
flexible word order languages such as Hindi.
As a syntactic corpus, we use the Hindi Depen-
dency Treebank (Bhatt et al, 2009). Using de-
pendency structure has some advantages. First, se-
mantic arguments1 can be marked explicitly on the
syntactic trees, so annotations of the predicate ar-
gument structure can be more consistent with the
dependency structure. Second, the Hindi Depen-
dency Treebank provides a rich set of dependency
relations that capture the syntactic-semantic infor-
mation. This facilitates mappings between syntac-
tic dependents and semantic arguments. A success-
ful mapping would reduce the annotation effort, im-
prove the inter-annotator agreement, and guide a full
fledged semantic role labeling task.
In this paper, we briefly describe our annotation
work on the Hindi PropBank, and suggest mappings
between syntactic and semantic arguments based on
linguistic intuitions. We also present a probabilistic
rule-based system that uses three types of rules to
arrive at mappings between syntactic and semantic
arguments. Our experiments show some promising
results; these mappings illustrate how well those two
frameworks are correlated, and can also be used to
speed up the PropBank annotation.
2 Description of the Hindi PropBank
2.1 Background
The Hindi PropBank is part of a multi-dimensional
and multi-layered resource creation effort for the
Hindi-Urdu language (Bhatt et al, 2009). This
multi-layered corpus includes both dependency an-
notation as well as lexical semantic information in
the form of PropBank. The corpus also produces
phrase structure representations in addition to de-
1The term ?semantic argument? is used to indicate all num-
bered arguments as well as modifiers in PropBank.
21
pendency structure. The Hindi Dependency Tree-
bank has created an annotation scheme for Hindi
by adapting labels from Panini?s Sanskrit gram-
mar (also known as CPG: Computational Paninian
Grammar; see Begum et al (2008)). Previous work
has demonstrated that the English PropBank tagset
is quite similar to English dependency trees anno-
tated with the Paninian labels (Vaidya et al, 2009).
PropBank has also been mapped to other depen-
dency schemes such as Functional Generative De-
scription (Cinkova, 2006).
2.2 Hindi Dependency Treebank
The Hindi Dependency Treebank (HDT) includes
morphological, part-of-speech and chunking infor-
mation as well as dependency relations. These are
represented in the Shakti Standard Format (SSF; see
Bharati et al (2007)). The dependency labels de-
pict relations between chunks, which are ?minimal
phrases consisting of correlated, inseparable enti-
ties? (Bharati et al, 2006), so they are not neces-
sarily individual words. The annotation of chunks
also assumes that intra-chunk dependencies can be
extracted automatically (Husain et al, 2010).
The dependency tagset consists of about 43 labels,
which can be grouped into three categories: depen-
dency relation labels, modifier labels, and labels for
non-dependencies (Bharati et al, 2009). PropBank
is mainly concerned with those labels depicting de-
pendencies in the domain of locality of verb predi-
cates. The dependency relation labels are based on
the notion of ?karaka?, defined as ?the role played by
a participant in an action?. The karaka labels, k1-5,
are centered around the verb?s meaning. There are
other labels such as rt (purpose) or k7t (location)
that are independent of the verb?s meaning.
2.3 Annotating the Hindi PropBank
The Hindi PropBank (HPB) contains the labeling of
semantic roles, which are defined on a verb-by-verb
basis. The description at the verb-specific level is
fine-grained; e.g., ?hitter? and ?hittee?. These verb-
specific roles are then grouped into broader cate-
gories using numbered arguments (ARG#). Each
verb can also have modifiers not specific to the verb
(ARGM*). The annotation process takes place in two
stages: the creation of frameset files for individual
verb types, and the annotation of predicate argu-
ment structures for each verb instance. As annota-
tion tools, we use Cornerstone and Jubilee (Choi et
al., 2010a; Choi et al, 2010b). The annotation is
done on the HDT; following the dependency anno-
tation, PropBank annotates each verb?s syntactic de-
pendents as their semantic arguments at the chunk
level. Chunked trees are conveniently displayed for
annotators in Jubilee. PropBank annotations gener-
ated in Jubilee can also be easily projected onto the
SSF format of the original dependency trees.
The HPB currently consists of 24 labels including
both numbered arguments and modifiers (Table 1).
In certain respects, the HPB labels make some dis-
tinctions that are not made in some other language
such as English. For instance, ARG2 is subdivided
into labels with function tags, in order to avoid
ARG2 from being semantically overloaded (Yi,
2007). ARGC and ARGA mark the arguments of mor-
phological causatives in Hindi, which is different
from the ARG0 notion of ?causer?. We also intro-
duce two labels to represent the complex predicate
constructions: ARGM-VLV and ARGM-PRX.
Label Description
ARG0 agent, causer, experiencer
ARG1 patient, theme, undergoer
ARG2 beneficiary
ARG3 instrument
ARG2-ATR attribute ARG2-GOL goal
ARG2-LOC location ARG2-SOU source
ARGC causer
ARGA secondary causer
ARGM-VLV verb-verb construction
ARGM-PRX noun-verb construction2
ARGM-ADV adverb ARGM-CAU cause
ARGM-DIR direction ARGM-DIS discourse
ARGM-EXT extent ARGM-LOC location
ARGM-MNR manner ARGM-MNS means
ARGM-MOD modal ARGM-NEG negation
ARGM-PRP purpose ARGM-TMP temporal
Table 1: Hindi PropBank labels.
2.4 Empty arguments in the Hindi PropBank
The HDT and HPB layers have different ways of
handling empty categories (Bhatia et al, 2010).
HPB inserts empty arguments such as PRO (empty
subject of a non-finite clause), RELPRO (empty
22
relative pronoun), pro (pro-drop argument), and
gap-pro (gapped argument). HPB annotates syn-
tactic relations between its semantic roles, notably
co-indexation of the empty argument PRO as well as
gap-pro. The example in Figure 1 shows that Mo-
han and PRO are co-indexed; thus, Mohan becomes
ARG0 of read via the empty argument PRO. There is
no dependency link between PRO and read because
PRO is inserted only in the PropBank layer.
Mohan wanted to read a book
????_?
PRO
?? ???
????
Mohan_ERG
k1
vmod
ARG1
ARG0
ARG0
????
PRO book read want
k2
ARG1
Figure 1: Empty argument example. The upper and lower
edges indicate HDT and HPB labels, respectively.
3 Comparisons between syntactic and
semantic arguments
In this section, we describe the mappings between
HDT and HPB labels based on our linguistic intu-
itions. We show that there are several broad similar-
ities between two tagsets. These mappings form the
basis for our linguistically motivated rules in Sec-
tion 4.2.3. In section 5.5, we analyze whether the
intuitions discussed in this section are borne out by
the results of our probabilistic rule-based system.
3.1 Numbered arguments
The numbered arguments correspond to ARG0-3,
including function tags associated with ARG2. In
PropBank, ARG0 and ARG1 are conceived as
framework-independent labels, closely associated
with Dowty?s Proto-roles (Palmer et al, 2010). For
instance, ARG0 corresponds to the agent, causer, or
experiencer, whether it is realized as the subject of
an active construction or as the object of an adjunct
(by phrase) of the corresponding passive. In this re-
spect, ARG0 and ARG1 are very similar to k1 and
k2 in HDT, which are annotated based on their se-
mantic roles, not their grammatical relation. On the
other hand, HDT treats the following sentences sim-
ilarly, whereas PropBank does not:
? The boy broke the window.
? The window broke.
The boy and the window are both considered k1 for
HDT, whereas PropBank labels the boy as ARG0 and
The window as ARG1. The window is not consid-
ered a primary causer as the verb is unaccusative for
Propbank. For HDT, the notion of unaccusativity is
not taken into consideration. This is an important
distinction that needs to be considered while carry-
ing out the mapping. k1 is thus ambiguous between
ARG0 and ARG1. Also, HDT makes a distinction
between Experiencer subjects of certain verbs, label-
ing them as k4a. As PropBank does not make such
a distinction, k4a maps to ARG0. The Experiencer
subject information is included in the corresponding
frameset files of the verbs. The mappings to ARG0
and ARG1 would be accurate only if they make use
of specific verb information. The mappings for other
numbered arguments as well as ARGC and ARGA are
given in Table 2.
HDT label HPB label
k1 (karta); k4a (experiencer) Arg0
k2 (karma) Arg1
k4 (beneficiary) Arg2
k1s (attribute) Arg2-ATR
k5 (source) Arg2-SOU
k2p (goal) Arg2-GOL
k3 (instrument) Arg3
mk1 (causer) ArgC
pk1 (secondary causer) ArgA
Table 2: Mappings to the HPB numbered arguments.
Note that in HDT annotation practice, k3 and k5
tend to be interpreted in a broad fashion such that
they map not only to ARG3 and ARG2-SOU, but also
to ARGM-MNS and ARGM-LOC (Vaidya and Husain,
2011). Hence, a one-to-one mapping for these la-
bels is not possible. Furthermore, the occurrence of
morphological causatives (ARGC and ARGA) is fairly
low so that we may not be able to test the accuracy
of these mappings with the current data.
3.2 Modifiers
The modifiers in PropBank are quite similar in their
definitions to certain HDT labels. We expect a fairly
high mapping accuracy, especially as these are not
verb-specific. Table 3 shows mappings between
23
HDT labels and HPB modifiers. A problematic map-
ping could be ARGM-MNR, which is quite coarse-
grained in PropBank, applying not only to adverbs
of manner, but also to infinitival adjunct clauses.
HDT label HPB label
sent-adv (epistemic adv) ArgM-ADV
rh (cause/reason) ArgM-CAU
rd (direction) ArgM-DIR
rad (discourse) ArgM-DIS
k7p (location) ArgM-LOC
adv (manner adv) ArgM-MNR
rt (purpose) ArgM-PRP
k7t (time) ArgM-TMP
Table 3: Mappings to the HPB modifiers.
3.3 Simple and complex predicates
HPB distinguishes annotations between simple and
complex predicates. Simple predicates consist of
only a single verb whereas complex predicates con-
sist of a light verb and a pre-verbal element. The
complex predicates are identified with a special label
ARGM-PRX (ARGument-PRedicating eXpresstion),
which is being used for all light verb annotations
in PropBank (Hwang et al, 2010). Figure 2 shows
an example of the predicating noun mention anno-
tated as ARGM-PRX, used with come. The predicat-
ing noun also has its own argument, matter of, in-
dicated with the HDT label r6-k1. The HDT has
two labels, r6-k1 and r6-k2, for the arguments of
the predicating noun. Hence, the argument span for
complex predicates includes not only direct depen-
dents of the verb but also dependents of the noun.
??????_?_????? ??????_?? ????_?? ?? ?_??
hearing_of_during Wed._of matter_of mention_to
k7t
k7t
pof
ARGM-PRX
ARG1
ARGM-TMP
come
???
r6-k1
ARGM-TMP
During the hearing on Wednesday, the matter was mentioned
Figure 2: Complex predicate example.
The ARGM-PRX label usually overlaps with the
HDT label pof, indicating a ?part of units? as pre-
verbal elements in complex predicates. However, in
certain cases, HPB has its own analysis for noun-
verb complex predicates. Hence, not all the nom-
inals labeled pof are labeled as ARGM-PRX. In
the example in Figure 3, the noun chunk important
progress is not considered to be an ARGM-PRX by
HPB (in this example, we have pragati hona; (lit)
progess be; to progress). The nominal for PropBank
is in fact ARG1 of the verb be, rather than a com-
posite on the verb. Additional evidence for this is
that neither the nominal nor the light verb seem to
project arguments of their own.
Important progress has been made in this work
??_???_? ?_?
k7p
pof
ARG1
ARGM-LOC
?????? ??_?? ??
this_work_LOC important_progress be_PRES
Figure 3: HDT vs. HPB on complex predicates.
4 Automatic mapping of HDT to HPB
Mapping between syntactic and semantic structures
has been attempted in other languages. The Penn
English and Chinese Treebanks consist of several se-
mantic roles (e.g., locative, temporal) annotated on
top of Penn Treebank style phrase structure (Marcus
et al, 1994; Xue and Palmer, 2009). The Chinese
PropBank specifies mappings between syntactic and
semantic arguments in frameset files (e.g., SBJ ?
ARG0) that can be used for automatic mapping (Xue
and Palmer, 2003). However, these Chinese map-
pings are limited to certain types of syntactic argu-
ments (mostly subjects and objects). Moreover, se-
mantic annotations on the Treebanks are done inde-
pendently from PropBank annotations, which causes
disagreement between the two structures.
Dependency structure transparently encodes rela-
tions between predicates and their arguments, which
facilitates mappings between syntactic and seman-
tic arguments. Hajic?ova? and Kuc?erova? (2002) tried
to project PropBank semantic roles onto the Prague
Dependency Treebank, and showed that the projec-
tion is not trivial. The same may be true to our case;
however, our goal is not to achieve complete map-
pings between syntactic and semantic arguments,
24
but to find a useful set of mappings that can speed
up our annotation. These mappings will be applied
to our future data as a pre-annotation stage, so that
annotators do not need to annotate arguments that
have already been automatically labeled by our sys-
tem. Thus, it is important to find mappings with high
precision and reasonably good recall.
In this section, we present a probabilistic rule-
based system that identifies and classifies semantic
arguments in the HPB using syntactic dependents in
the HDT. This is still preliminary work; our system
is expected to improve as we annotate more data and
do more error analysis.
4.1 Argument identification
Identifying semantic arguments of each verb pred-
icate is relatively easy given the dependency Tree-
bank. For each verb predicate, we consider all syn-
tactic dependents of the predicate as its semantic
arguments (Figure 4). For complex predicates, we
consider the syntactic dependents of both the verb
and the predicating noun (cf. Section 3.3).
?? ???? ? ?? ??_ ? ?? ??? ??_ ??
Kishori Haridwar_from Delhi come_be
k1
k5
k2p
ARG2-GOL
ARG2-SOU
ARG0
Kishori came from Haridwar to Delhi
Figure 4: Simple predicate example.
With our heuristics, we get a precision of 99.11%,
a recall of 95.50%, and an F1-score of 97.27% for
argument identification. Such a high precision is
expected as the annotation guidelines for HDT and
HPB generally follow the same principles of iden-
tifying syntactic and semantic arguments of a verb.
About 4.5% of semantic arguments are not identi-
fied by our method. Table 4 shows distributions of
the most frequent non-identified arguments.
Label Dist. Label Dist. Label Dist.
ARG0 3.21 ARG1 0.90 ARG2? 0.09
Table 4: Distributions of non-identified arguments caused
by PropBank empty categories (in %).
Most of the non-identified argument are antecedents
of PropBank empty arguments. As shown in Fig-
ure 1, the PropBank empty argument has no depen-
dency link to the verb predicate. Identifying such
arguments requires a task of empty category reso-
lution, which will be explored as future work. Fur-
thermore, we do not try to identify PropBank empty
arguments for now, which will also be explored later.
4.2 Argument classification
Given the identified semantic arguments, we classify
their semantic roles. Argument classification is done
by using three types of rules. Deterministic rules are
heuristics that are straightforward given dependency
structure. Empirically-derived rules are generated
by measuring statistics of dependency features in as-
sociation with semantic roles. Finally, linguistically-
motivated rules are derived from our linguistic intu-
itions. Each type of rule has its own strength; how
to combine them is the art we need to explore.
4.2.1 Deterministic rule
Only one deterministic rule is used in our system.
When an identified argument has a pof dependency
relation with its predicate, we classify the argu-
ment as ARGM-PRX. This emphasizes the advan-
tage of using our dependency structure: classifying
ARGM-PRX cannot be done automatically in most
other languages where there is no information pro-
vided for light verb constructions. This determin-
istic rule is applied before any other type of rule.
Therefore, we do not generate further rules to clas-
sify the ARGM-PRX label.
4.2.2 Empirically-derived rules
Three kinds of features are used for the generation of
empirically-derived rules: predicate ID, predicate?s
voice type, and argument?s dependency label. The
predicate ID is either the lemma or the roleset ID
of the predicate. Predicate lemmas are already pro-
vided in HDT. When we use predicate lemmas, we
assume no manual annotation of PropBank. Thus,
rules generated from predicate lemmas can be ap-
plied to any future data without modification. When
we use roleset ID?s, we assume that sense annota-
tions are already done. PropBank includes anno-
tations of coarse verb senses, called roleset ID?s,
that differentiate each verb predicate with different
25
senses (Palmer et al, 2005). A verb predicate can
form several argument structures with respect to dif-
ferent senses. Using roleset ID?s, we generate more
fine-grained rules that are specific to those senses.
The predicate?s voice type is either ?active? or
?passive?, also provided in HDT. There are not many
instances of passive construction in our current data,
which makes it difficult to generate rules general
enough for future data. However, even with the lack
of training instances, we find some advantage of us-
ing the voice feature in our experiments. Finally, the
argument?s dependency label is the dependency la-
bel of an identified argument with respect to its pred-
icate. This feature is straightforward for the case of
simple predicates. For complex predicates, we use
the dependency labels of arguments with respect to
their syntactic heads, which can be pre-verbal ele-
ments. Note that rules generated with complex pred-
icates contain slightly different features for predicate
lemmas as well; instead of using predicate lemmas,
we use joined tags of the predicate lemmas and the
lemmas of pre-verbal elements.
ID V Drel PBrel #
come a k1 ARG0 1
come a k5 ARG2-SOU 1
come a k2p ARG2-GOL 1
come mention a k7t ARGM-TMP 2
come mention a r6-k1 ARG1 1
Table 5: Rules generated by the examples in Figures 4 and
2. The ID, V, and Drel columns show predicate ID, predicate?s
voice type, and argument?s dependency label. The PBrel col-
umn shows the PropBank label of each argument. The # column
shows the total count of each feature tuple being associated with
the PropBank label. ?a? stands for active voice.
Table 5 shows a set of rules generated by the exam-
ples in Figures 4 (come) and 2 (come mention). No
rule is generated for ARGM-PRX because the label
is already covered by our deterministic rule (Sec-
tion 4.2.1). When roleset ID?s are used in place of
the predicate ID, come and come mention are re-
placed with A.03 and A.01, respectively. These
rules can be formulated as a function rule such that:
rule(id, v, drel) = argmax i P (pbreli)
where P (pbreli) is a probability of the predicted
PropBank label pbreli, given a tuple of features
(id, v, drel). The probability is measured by es-
timating a maximum likelihood of each PropBank
label being associated with the feature tuple. For
example, a feature tuple (come, active, k1) can be
associated with two PropBank labels, ARG0 and
ARG1, with counts of 8 and 2, respectively. In this
case, the maximum likelihoods of ARG0 and ARG1
being associated with the feature tuple is 0.8 and 0.2;
thus rule(come, active, k1) = ARG0.
Since we do not want to apply rules with low con-
fidence, we set a threshold to P (pbrel), so predic-
tions with low probabilities can be filtered out. Find-
ing the right threshold is a task of handling the pre-
cision/recall trade-off. For our experiments, we ran
10-fold cross-validation to find the best threshold.
4.2.3 Linguistically-motivated rules
Linguistically-motivated rules are applied to argu-
ments that the deterministic rule and empirically-
derived rules cannot classify. These rules capture
general correlations between syntactic and seman-
tic arguments for each predicate, so they are not as
fine-grained as empirically-derived rules, but can be
helpful for predicates not seen in the training data.
The rules are manually generated by our annota-
tors and specified in frameset files. Table 6 shows
linguistically-motivated rules for the predicate ?A
(come)?, specified in the frameset file, ?A-v.xml?.3
Roleset Usage Rule
A.01 to come
k1 ? ARG1
k2p ? ARG2-GOL
A.03 to arrive
k1 ? ARG1
k2p ? ARG2-GOL
k5 ? ARG2-SOU
A.02 light verb No rule provided
Table 6: Rules for the predicate ?A (come)?.
The predicate ?A? has three verb senses and each
sense specifies a different set of rules. For instance,
the first rule of A.01 maps a syntactic dependent
with the dependency label k1 to a semantic ar-
gument with the semantic label ARG1. Note that
frameset files include rules only for numbered ar-
guments. Most of these rules should already be in-
cluded in the empirically-derived rules as we gain
3See Choi et al (2010a) for details about frameset files.
26
more training data; however, for an early stage of
annotation, these rules provide useful information.
5 Experiments
5.1 Corpus
All our experiments use a subset of the Hindi Depen-
dency Treebank, distributed by the ICON?10 con-
test (Husain et al, 2010). Our corpus contains about
32,300 word tokens and 2,005 verb predicates, in
which 546 of them are complex predicates. Each
verb predicate is annotated with a verse sense speci-
fied in its corresponding frameset file. There are 160
frameset files created for the verb predicates. The
number may seem small compared to the number
of verb predicates. This is because we do not cre-
ate separate frameset files for light verb construc-
tions, which comprise about 27% of the predicate
instances (see the example in Table 6).
All verb predicates are annotated with argument
structures using PropBank labels. A total of 5,375
arguments are annotated. Since there is a relatively
small set of data, we do not make a separate set for
evaluations. Instead, we run 10-fold cross-validation
to evaluate our rule-based system.
5.2 Evaluation of deterministic rule
First, we evaluate how well our deterministic rule
classifies the ARGM-PRX label. Using the determin-
istic rule, we get a 94.46% precision and a 100%
recall on ARGM-PRX. The 100% recall is expected;
the precision implies that about 5.5% of the time,
light verb annotations in the HPB do not agree with
the complex predicate annotations (pof relation) in
the HDT (cf. Section 3.3). More analysis needs to
be done to improve the precision of this rule.
5.3 Evaluation of empirically-derived rules
Next, we evaluate our empirically-derived rules with
respect to the different thresholds set for P (pbreli).
In general, the higher the threshold is, the higher
and lower the precision and recall become, respec-
tively. Figure 5 shows comparisons between preci-
sion and recall with respect to different thresholds.
Notice that a threshold of 1.0, meaning that using
only rules with 100% confidence, does not give the
highest precision. This is because the model with
this high of a threshold overfits to the training data.
Rules that work well in the training data do not nec-
essarily work as well on the test data.
   0 0.2 0.4 0.6 0.8 1
   
   30
40
50
60
70
80
Threshold
Acc
ura
cy (
in %
)
R
F1
P
0.93
Figure 5: Accuracies achieved by the empirically derived
rules using (lemma, voice, label) features. P, R, and F1
stand for precisions, recalls, and F1-scores, respectively.
We need to find a threshold that gives a high preci-
sion (so annotators do not get confused by the au-
tomatic output) while maintaining a good recall (so
annotations can go faster). With a threshold of 0.93
using features (lemma, voice, dependency label), we
get a precision of 90.37%, a recall of 44.52%, and
an F1-score of 59.65%. Table 7 shows accuracies
for all PropBank labels achieved by a threshold of
0.92 using roleset ID?s instead of predicate?s lem-
mas. Although the overall precision stays about the
same, we get a noticeable improvement in the over-
all recall using roleset ID?s. Note that some labels
are missing in Table 7. This is because either they
do not occur in our current data (ARGC and ARGA)
or we have not started annotating them properly yet
(ARGM-MOD and ARGM-NEG).
5.4 Evaluation of linguistically-motivated rules
Finally, we evaluate the impact of the linguistically-
motivated rules. Table 8 shows accuracies achieved
by the linguistically motivated rules applied after the
empirically derived rules. As expected, the linguis-
tically motivated rules improve the recall of ARGN
significantly, but bring a slight decrease in the pre-
cision. This shows that our linguistic intuitions are
generally on the right track. We may combine some
of the empirically derived rules with linguistically
motivated rules together in the frameset files so an-
notators can take advantage of both kinds of rules in
the future.
27
Dist. P R F1
ALL 100.00 90.59 47.92 62.69
ARG0 17.50 95.83 67.27 79.05
ARG1 27.28 94.47 61.62 74.59
ARG2 3.42 81.48 37.93 51.76
ARG2-ATR 2.54 94.55 40.31 56.52
ARG2-GOL 1.61 64.29 21.95 32.73
ARG2-LOC 0.87 90.91 22.73 36.36
ARG2-SOU 0.83 78.26 42.86 55.38
ARG3 0.08 0.00 0.00 0.00
ARGM-ADV 3.50 31.82 3.93 7.00
ARGM-CAU 1.44 50.00 5.48 9.88
ARGM-DIR 0.43 100.00 18.18 30.77
ARGM-DIS 1.63 26.67 4.82 8.16
ARGM-EXT 1.42 0.00 0.00 0.00
ARGM-LOC 10.77 83.80 27.42 41.32
ARGM-MNR 6.00 57.14 9.18 15.82
ARGM-MNS 0.79 77.78 17.50 28.57
ARGM-PRP 2.15 65.52 17.43 27.54
ARGM-PRX 10.75 94.46 100.00 97.15
ARGM-TMP 7.01 74.63 14.04 23.64
Table 7: Labeling accuracies achieved by the empirically de-
rived rules using (roleset ID, voice, label) features and a thresh-
old of 0.92. The accuracy for ARGM-PRX is achieved by the
deterministic rule. The Dist. column shows a distribution of
each label.
Dist. P R F1
ALL 100.00 89.80 55.28 68.44
ARGN 54.12 91.87 72.36 80.96
ARGM 45.88 85.31 35.14 49.77
ARGN w/o LM 93.63 58.76 72.21
Table 8: Labeling accuracies achieved by the linguistically
motivated rules. The ARGN and ARGM rows show statistics of
all numbered arguments and modifiers combined, respectively.
The ?ARGN w/o LM? row shows accuracies of ARGN achieved
only by the empirically derived rules.
5.5 Error anlaysis
The precision and recall results for ARG0 and ARG1,
are better than expected, despite the complexity of
the mapping (Section 3.1). This is because they oc-
cur most often in the corpus, so enough rules can
be extracted. The other numbered arguments are
closely related to particular types of verbs (e.g., mo-
tion verbs for ARG2-GOL|SOU). Our linguistically
motivated rules are more effective for these types
of HPB labels. We would expect the modifiers to
be mapped independently of the verb, but our ex-
periments show that the presence of the verb lemma
feature enhances the performance of modifiers. Al-
though section 3.2 expects one-to-one mappings for
modifiers, it is not the case in practice.
We observe that the interpretation of labels in an-
notation practice is important. For example, our sys-
tem performs poorly for ARGM-ADV because the la-
bel is used for various sentential modifiers and can
be mapped to as many as four HDT labels. On the
other hand, HPB makes some fine-grained distinc-
tions. For instance, means and causes are distin-
guished using ARGM-CAU and ARGM-MNS labels, a
distinction that HDT does not make. In the example
in Figure 6, we find that aptitude with is assigned to
ARGM-MNS, but gets the cause label rh in HDT.
Rajyapal can call upon any party with his aptitude
???????
Rajyapal
???
his
?? ??_ ?
aptitude_with
?? ??_ ??
any_EMPH
????_ ??
party_DAT
????_ ????_ ?
call_can_be
Figure 6: Means vs. cause example.
6 Conclusion and future work
We provide an analysis of the Hindi PropBank anno-
tated on the Hindi Dependency Treebank. There is
an interesting correlation between dependency and
predicate argument structures. By analyzing the
similarities between the two structures, we find rules
that can be used for automatic mapping of syntactic
and semantic arguments, and achieve over 90% con-
fidence for almost half of the data. These rules will
be applied to our future data, which will make the
annotation faster and possibly more accurate.
We plan to use different sets of rules generated by
different thresholds to see which rule set leads to the
most effective annotation. We also plan to develop
a statistical semantic role labeling system in Hindi,
once we have enough training data. In addition, we
will explore the possibility of using existing lexical
resource such as WordNet (Narayan et al, 2002) to
improve our system.
Acknowledgements
This work is supported by NSF grants CNS- 0751089, CNS-
0751171, CNS-0751202, and CNS-0751213. Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect
the views of the National Science Foundation.
28
References
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for indian languages. In
In Proceedings of the 3rd International Joint Confer-
ence on Natural Language Processing, IJCNLP?08.
Akshar Bharati, Dipti Misra Sharma, Lakshmi Bai, and
Rajeev Sangal. 2006. AnnCorra: Guidelines for POS
and Chunk Annotation for Indian Languages. Techni-
cal report, IIIT Hyderabad.
Akshar Bharati, Rajeev Sangal, and Dipti Misra Sharma.
2007. Ssf: Shakti standard format guide. Technical
report, IIIT Hyderabad.
Akshara Bharati, Dipti Misra Sharma, Samar Husain,
Lakshmi Bai, Rafiya Begam, and Rajeev Sangal.
2009. Anncorra : Treebanks for indian languages,
guidelines for annotating hindi treebank. Technical re-
port, IIIT Hyderabad.
Archna Bhatia, Rajesh Bhatt, Bhuvana Narasimhan,
Martha Palmer, Owen Rambow, Dipti Misra Sharma,
Michael Tepper, Ashwini Vaidya, and Fei Xia. 2010.
Empty categories in a hindi treebank. In Proceedings
of the 7th International Conference on Language Re-
sources and Evaluation (LREC?10), pages 1863?1870.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Sharma, and Fei Xia. 2009. A
Multi-Representational and Multi-Layered Treebank
for Hindi/Urdu. In In the Proceedings of the Third Lin-
guistic Annotation Workshop held in conjunction with
ACL-IJCNLP 2009.
Jinho D. Choi, Claire Bonial, and Martha Palmer. 2010a.
Propbank frameset annotation guidelines using a ded-
icated editor, cornerstone. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation, LREC?10, pages 3650?3653.
Jinho D. Choi, Claire Bonial, and Martha Palmer. 2010b.
Propbank instance annotation guidelines using a ded-
icated editor, jubilee. In Proceedings of the 7th In-
ternational Conference on Language Resources and
Evaluation, LREC?10, pages 1871?1875.
Silvie Cinkova. 2006. From PropBank to EngVALLEX:
Adapting PropBank-Lexicon to the Valency Theory of
Functional Generative Description. In Proceedings
of the fifth International conference on Language Re-
sources and Evaluation (LREC 2006), Genova, Italy.
Eva Hajic?ova? and Ivona Kuc?erova?. 2002. Argu-
ment/valency structure in propbank, lcs database and
prague dependency treebank: A comparative pi-
lot study. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation,
LREC?02, pages 846?851.
Samar Husain, Prashanth Mannem, Bharat Ram Ambati,
and Phani Gadde. 2010. The ICON-2010 tools contest
on Indian language dependency parsing. In Proceed-
ings of ICON-2010 Tools Contest on Indian Language
Dependency Parsing, ICON?10, pages 1?8.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and Martha
Palmer. 2010. PropBank Annotation of Multilingual
Light Verb Constructions. In Proceedings of the Lin-
guistic Annotation Workshop at ACL 2010.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert Macintyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop, pages
114?119.
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande,
and Pushpak Bhattacharyya. 2002. An experience
in building the indo wordnet - a wordnet for hindi.
In Proceedings of the 1st International Conference on
Global WordNet.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohamed Maamouri, Aous Mansouri, and Wa-
jdi Zaghouani. 2008. A pilot arabic propbank. In Pro-
ceedings of the 6th International Language Resources
and Evaluation, LREC?08, pages 28?30.
Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010.
Semantic role labeling. In Graeme Hirst, editor, Syn-
thesis Lectures on Human Language Technologies.
Morgan and Claypool.
Ashwini Vaidya and Samar Husain. 2011. A classifica-
tion of dependencies in the Hindi/Urdu Treebank. In
Presented at the Workshop on South Asian Syntax and
Semantics, Amherst, MA.
Ashwini Vaidya, Samar Husain, and Prashanth Mannem.
2009. A karaka based dependency scheme for En-
glish. In Proceedings of the CICLing-2009, Mexico
City, Mexico.
Nianwen Xue and Martha Palmer. 2003. Annotating the
propositions in the penn chinese treebank. In Proceed-
ings of the 2nd SIGHAN workshop on Chinese lan-
guage processing, SIGHAN?03, pages 47?54.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the chinese treebank. Natural Language
Engineering, 15(1):143?172.
Szu-Ting Yi. 2007. Automatic Semantic Role Labeling.
Ph.D. thesis, University of Pennsylvania.
29
Proceedings of the Fifth Law Workshop (LAW V), pages 65?73,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Reducing the Need for Double Annotation
Dmitriy Dligach
Department of Computer Science
University of Colorado at Boulder
Dmitriy.Dligach@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
Martha.Palmer@colorado.edu
Abstract
The quality of annotated data is crucial for
supervised learning. To eliminate errors in
single annotated data, a second round of an-
notation is often used. However, is it abso-
lutely necessary to double annotate every ex-
ample? We show that it is possible to reduce
the amount of the second round of annotation
by more than half without sacrificing the per-
formance.
1 Introduction
Supervised learning has become the dominant
paradigm in NLP in recent years thus making the
creation of high-quality annotated corpora a top pri-
ority in the field. A corpus where each instance is
annotated by a single annotator unavoidably con-
tains errors. To improve the quality of the data, one
may choose to annotate each instance twice and ad-
judicate the disagreements thus producing the gold
standard. For example, the OntoNotes (Hovy et al,
2006) project opted for this approach.
However, is it absolutely necessary to double an-
notate every example? In this paper, we demonstrate
that it is possible to double annotate only a subset of
the single annotated data and still achieve the same
level of performance as with full double annotation.
We accomplish this task by using the single anno-
tated data to guide the selection of the instances to
be double annotated.
We propose several algorithms that accept sin-
gle annotated data as input. The algorithms select
a subset of this data that they recommend for an-
other round of annotation and adjudication. The sin-
gle annotated data our algorithms work with can po-
tentially come from any source. For example, it can
be the single annotated output of active learning or
the data that had been randomly sampled from some
corpus and single annotated. Our approach is ap-
plicable whenever a second round of annotation is
being considered to improve the quality of the data.
Our approach is similar in spirit to active learn-
ing but more practical in a double annotation multi-
tagger environment. We evaluate this approach on
OntoNotes word sense data. Our best algorithm de-
tects 75% of the errors, while the random sampling
baseline only detects less than a half of that amount.
We also show that this algorithm can lead to a 54%
reduction in the amount of annotation needed for the
second round of annotation.
The rest of this paper is structured as follows: we
discuss the relevant work in section 2, we explain
our approach in section 3, we evaluate our approach
in section 4, we discuss the results and draw a con-
clusion in section 5, and finally, we talk about our
plans for future work in section 6.
2 Related Work
Active Learning (Settles, 2009; Olsson, 2009) has
been the traditional avenue for reducing the amount
of annotation. However, in practice, serial active
learning is difficult in a multi-tagger environment
(Settles, 2009) when many annotators are working
in parallel (e.g. OntoNotes employs tens of tag-
gers). At the same time, several papers recently ap-
peared that used OntoNotes data for active learning
experiments (Chen et al, 2006; Zhu, 2007; Zhong et
al., 2008). These works all utilized OntoNotes gold
standard labels, which were obtained via double an-
notation and adjudication. The implicit assumption,
therefore, was that the same process of double anno-
65
tation and adjudication could be reproduced in the
process of active learning. However, this assumption
is not very realistic and in practice, these approaches
may not bring about the kind of annotation cost re-
duction that they report. For example, an instance
would have to be annotated by two taggers (and each
disagreement adjudicated) on each iteration before
the system can be retrained and the next instance se-
lected. Active learning tends to select ambiguous ex-
amples (especially at early stages), which are likely
to cause an unusually high number of disagreements
between taggers. The necessity of frequent manual
adjudication would slow down the overall process.
Thus, if the scenarios of (Chen et al, 2006; Zhu,
2007; Zhong et al, 2008) were used in practice, the
taggers would have to wait on each other, on the ad-
judicator, and on the retraining, before the system
can select the next example. The cost of annotator
waiting time may undermine the savings in annota-
tion cost.
The rationale for our work arises from these dif-
ficulties: because active learning is not practical
in a double annotation scenario, the data is single
annotated first (with the instances selected via ac-
tive learning, random sampling or some other tech-
nique). After that, our algorithms can be applied to
select a subset of the single annotated data for the
second round of annotation and adjudication. Our
algorithms select the data for repeated labeling in a
single batch, which means the selection can be done
off-line. This should greatly simplify the application
of our approach in a real life annotation project.
Our work also borrows from the error detection
literature. Researchers have explored error detec-
tion for manually tagged corpora in the context
of pos-tagging (Eskin, 2000; Kve?ton? and Oliva,
2002; Nova?k and Raz??mova?, 2009), dependency
parsing (Dickinson, 2009), and text-classification
(Fukumoto and Suzuki, 2004). The approaches to
error detection include anomaly detection (Eskin,
2000), finding inconsistent annotations (van Hal-
teren, 2000; Kve?ton? and Oliva, 2002; Nova?k and
Raz??mova?, 2009), and using the weights assigned
by learning algorithms such as boosting (Abney et
al., 1999; Luo et al, 2005) and SVM (Nakagawa
and Matsumoto, 2002; Fukumoto and Suzuki, 2004)
by exploiting the fact that errors tend to concentrate
among the examples with large weights. Some of
these works eliminate the errors (Luo et al, 2005).
Others correct them automatically (Eskin, 2000;
Kve?ton? and Oliva, 2002; Fukumoto and Suzuki,
2004; Dickinson, 2009) or manually (Kve?ton? and
Oliva, 2002). Several authors also demonstrate en-
suing performance improvements (Fukumoto and
Suzuki, 2004; Luo et al, 2005; Dickinson, 2009).
All of these researchers experimented with single
annotated data such as Penn Treebank (Marcus et al,
1993) and they were often unable to hand-examine
all the data their algorithms marked as errors be-
cause of the large size of their data sets. Instead,
to demonstrate the effectiveness of their approaches,
they examined a selected subset of the detected ex-
amples (e.g. (Abney et al, 1999; Eskin, 2000; Nak-
agawa and Matsumoto, 2002; Nova?k and Raz??mova?,
2009)). In this paper, we experiment with fully dou-
ble annotated and adjudicated data, which allows us
to evaluate the effectiveness of our approach more
precisely. A sizable body of work exists on us-
ing noisy labeling obtained from low-cost annota-
tion services such as Amazon?s Mechanical Turk
(Snow et al, 2008; Sheng et al, 2008; Hsueh et
al., 2009). Hsueh et al (2009) identify several cri-
teria for selecting high-quality annotations such as
noise level, sentiment ambiguity, and lexical uncer-
tainty. (Sheng et al, 2008) address the relationships
between various repeated labeling strategies and the
quality of the resulting models. They also propose
a set of techniques for selective repeated labeling
which are based on the principles of active learn-
ing and an estimate of uncertainty derived from each
example?s label multiset. These authors focus on
the scenario where multiple (greater than two) labels
can be obtained cheaply. This is not the case with the
data we experiment with: OntoNotes data is double
annotated by expensive human experts. Also, unfor-
tunately, Sheng et al simulate multiple labeling (the
noise is introduced randomly). However, human an-
notators may have a non-random annotation bias re-
sulting from misreading or misinterpreting the direc-
tions, or from genuine ambiguities. The data we use
in our experiments is annotated by humans.
3 Algorithms
In the approach to double annotation we are propos-
ing, the reduction in annotation effort is achieved by
66
double annotating only the examples selected by our
algorithms instead of double annotating the entire
data set. If we can find most or all the errors made
during the first round of labeling and show that dou-
ble annotating only these instances does not sacri-
fice performance, we will consider the outcome of
this study positive. We propose three algorithms for
selecting a subset of the single annotated data for the
second round of annotation.
Our machine tagger algorithm draws on error de-
tection research. Single annotated data unavoidably
contains errors. The main assumption this algorithm
makes is that a machine learning classifier can form
a theory about how the data should be labeled from
a portion of the single annotated data. The classifier
can be subsequently applied to the rest of the data to
find the examples that contradict this theory. In other
words, the algorithm is geared toward detecting in-
consistent labeling within the single annotated data.
The machine tagger algorithm can also be viewed as
using a machine learning classifier to simulate the
second human annotator. The machine tagger al-
gorithm accepts single annotated data as input and
returns the instances that it believes are labeled in-
consistently.
Our ambiguity detector algorithm is inspired by
uncertainty sampling (Lewis and Gale, 1994), a kind
of active learning in which the model selects the
instances for which its prediction is least certain.
Some instances in the data are intrinsically ambigu-
ous. The main assumption the ambiguity detector
algorithm makes is that a machine learning classifier
trained using a portion of the single annotated data
can be used to detect ambiguous examples in the
rest of the single annotated data. The algorithm is
geared toward finding hard-to-classify instances that
are likely to cause problems for the human annota-
tor. The ambiguity detector algorithm accepts single
annotated data as input and returns the instances that
are potentially ambiguous and thus are likely to be
controversial among different annotators.
It is important to notice that the machine tagger
and ambiguity detector algorithms target two differ-
ent types of errors in the data: the former detects
inconsistent labeling that may be due to inconsistent
views among taggers (in a case when the single an-
notated data is labeled by more than one person) or
the same tagger tagging inconsistently. The latter
finds the examples that are likely to result in dis-
agreements when labeled multiple times due to their
intrinsic ambiguity. Therefore, our goal is not to
compare the performance of the machine tagger and
ambiguity detector algorithms, but rather to provide
a viable solution for reducing the amount of annota-
tion on the second round by detecting as much noise
in the data as possible. Toward that goal we also
consider a hybrid approach, which is a combination
of the first two.
Still, we expect some amount of overlap in the
examples detected by the two approaches. For ex-
ample, the ambiguous instances selected by the sec-
ond algorithm may also turn out to be the ones that
the first one will identify because they are harder
to classify (both by human annotators and machine
learning classifiers). The three algorithms we exper-
iment with are therefore (1) the machine tagger, (2)
the ambiguity detector, and (3) the hybrid of the two.
We will now provide more details about how each of
them is implemented.
3.1 General Framework
All three algorithms accept single annotated data as
input. They output a subset of this data that they rec-
ommend for repeated labeling. All algorithms be-
gin by splitting the single annotated data into N sets
of equal size. They proceed by training a classifier
on N ? 1 sets and applying it to the remaining set,
which we will call the pool1. The cycle repeats N
times in the style of N -fold cross-validation. Upon
completion, each single annotated instance has been
examined by the algorithm. A subset of the single
annotated data is selected for the second round of an-
notation based on various criteria. These criteria are
what sets the algorithms apart. Because of the time
constraints, for the experiments we describe in this
paper, we set N to 10. A larger value will increase
the running time but may also result in an improved
performance.
1Notice that the term pool in active learning research typi-
cally refers to the collection of unlabeled data from which the
examples to be labeled are selected. In our case, this term ap-
plies to the data that is already labeled and the goal is to select
data for repeated labeling.
67
3.2 Machine Tagger Algorithm
The main goal of the machine tagger algorithm is
finding inconsistent labeling in the data. This al-
gorithm operates by training a discriminative clas-
sifier and making a prediction for each instance in
the pool. Whenever this prediction disagrees with
the human-assigned label, the instance is selected
for repeated labeling.
For classification we choose a support vector ma-
chine (SVM) classifier because we need a high-
accuracy classifier. The state-of-the art system we
use for our experiments is SVM-based (Dligach and
Palmer, 2008). The specific classification software
we utilize is LibSVM (Chang and Lin, 2001). We
accept the default settings (C = 1 and linear ker-
nel).
3.3 Ambiguity Detector Algorithm
The ambiguity detector algorithm trains a proba-
bilistic classifier and makes a prediction for each
instance in the pool. However, unlike the previous
algorithm, the objective in this case is to find the in-
stances that are potentially hard to annotate due to
their ambiguity. The instances that lie close to the
decision boundary are intrinsically ambiguous and
therefore harder to annotate. We hypothesize that a
human tagger is more likely to make a mistake when
annotating these instances.
We can estimate the proximity to the class bound-
ary using a classifier confidence metric such as the
prediction margin, which is a simple metric often
used in active learning (e.g. (Chen et al, 2006)). For
an instance x, we compute the prediction margin as
follows:
Margin(x) = |P (c1|x)? P (c2|x)| (1)
Where c1 and c2 are the two most probable classes
of x according to the model. We rank the single
annotated instances by their prediction margin and
select selectsize instances with the smallest margin.
The selectsize setting can be manipulated to increase
the recall. We experiment with the settings of select-
size of 20% and larger.
While SVM classifiers can be adapted to produce
a calibrated posterior probability (Platt and Platt,
1999), for simplicity, we use a maximum entropy
classifier, which is an intrinsically probabilistic clas-
sifier and thus has the advantage of being able to
output the probability distribution over the class la-
bels right off-the-shelf. The specific classification
software we utilize is the python maximum entropy
modeling toolkit (Le, 2004) with the default options.
3.4 Hybrid Algorithm
We hypothesize that both the machine tagger and
ambiguity detector algorithms we just described se-
lect the instances that are appropriate for the second
round of human annotation. The hybrid algorithm
simply unions the instances selected by these two
algorithms. As a result, the amount of data selected
by this algorithm is expected to be larger than the
amount selected by each individual algorithm.
4 Evaluation
For evaluation we use the word sense data annotated
by the OntoNotes project. The OntoNotes data was
chosen because it is fully double-blind annotated by
human annotators and the disagreements are adjudi-
cated by a third (more experienced) annotator. This
type of data allows us to: (1) Simulate single anno-
tation by using the labels assigned by the first an-
notator, (2) Simulate the second round of annotation
for selected examples by using the labels assigned
by the second annotator, (3) Evaluate how well our
algorithms capture the errors made by the first anno-
tator, and (4) Measure the performance of the cor-
rected data against the performance of the double
annotated and adjudicated gold standard.
We randomly split the gold standard data into ten
parts of equal size. Nine parts are used as a pool
of data from which a subset is selected for repeated
labeling. The rest is used as a test set. Before pass-
ing the pool to the algorithm, we ?single annotate?
it (i.e. relabel with the labels assigned by the first
annotator). The test set alays stays double anno-
tated and adjudicated to make sure the performance
is evaluated against the gold standard labels. The cy-
cle is repeated ten times and the results are averaged.
Since our goal is finding errors in single anno-
tated data, a brief explanation of what we count as
an error is appropriate. In this evaluation, the er-
rors are the disagreements between the first anno-
tator and the gold standard. The fact that our data
68
Sense Definition Sample Context
Accept as true without
verification
I assume his train was
late
Take on a feature, po-
sition, responsibility,
right
When will the new
President assume of-
fice?
Take someone?s soul
into heaven
This is the day when
Mary was assumed
into heaven
Table 1: Senses of to assume
is double annotated allows us to be reasonably sure
that most of the errors made by the first annotator
were caught (as disagreements with the second an-
notator) and resolved. Even though other errors may
still exist in the data (e.g. when the two annotators
made the same mistake), we assume that there are
very few of them and we ignore them for the pur-
pose of this study.
4.1 Task
The task we are using for evaluating our approach
is word sense disambiguation (WSD). Resolution of
lexical ambiguities has for a long time been viewed
as an important problem in natural language pro-
cessing that tests our ability to capture and represent
semantic knowledge and and learn from linguistic
data. More specifically, we experiment with verbs.
There are fewer verbs in English than nouns but the
verbs are more polysemous, which makes the task
of disambiguating verbs harder. As an example, we
list the senses of one of the participating verbs, to
assume, in Table 1.
The goal of WSD is predicting the sense of an am-
biguous word given its context. For example, given
a sentence When will the new President assume of-
fice?, the task consists of determining that the verb
assume in this sentence is used in the Take on a fea-
ture, position, responsibility, right, etc. sense.
4.2 Data
We selected the 215 most frequent verbs in the
OntoNotes data and discarded the 15 most frequent
ones to make the size of the dataset more manage-
able (the 15 most frequent verbs have roughly as
many examples as the next 200 frequent verbs). We
Inter-annotator agreement 86%
Annotator1-gold standard agreement 93%
Share of the most frequent sense 71%
Number of classes (senses) per verb 4.44
Table 2: Evaluation data at a glance
ended up with a dataset containing 58,728 instances
of 200 frequent verbs. Table 2 shows various impor-
tant characteristics of this dataset averaged across
the 200 verbs.
Observe that even though the annotator1-gold
standard agreement is high, it is not perfect: about
7% of the instances are the errors the first annota-
tor made. These are the instances we are target-
ing. OntoNotes double annotated all the instances
to eliminate the errors. Our goal is finding them au-
tomatically.
4.3 System
Our word sense disambiguation system (Dligach and
Palmer, 2008) includes three groups of features.
Lexical features include open class words from the
target sentence and the two surrounding sentences;
two words on both sides of the target verb and their
POS tags. Syntactic features are based on con-
stituency parses of the target sentence and include
the information about whether the target verb has a
subject/object, what their head words and POS tags
are, whether the target verb has a subordinate clause,
and whether the target verb has a PP adjunct. The
semantic features include the information about the
semantic class of the subject and the object of the
target verb. The system uses Libsvm (Chang and
Lin, 2001) software for classification. We train a
single model per verb and average the results across
all 200 verbs.
4.4 Performance Metrics
Our objective is finding errors in single annotated
data. One way to quantify the success of error de-
tection is by means of precision and recall. We com-
pute precision as the ratio of the number of errors
in the data that the algorithm selected and the to-
tal number of instances the algorithm selected. We
compute recall as the ratio of the number of errors
in the data that the algorithm selected to the total
69
number of errors in the data. To compute baseline
precision and recall for an algorithm, we count how
many instances it selected and randomly draw the
same number of instances from the single annotated
data. We then compute precision and recall for the
randomly selected data.
We also evaluate each algorithm in terms of clas-
sification accuracy. For each algorithm, we measure
the accuracy on the test set when the model is trained
on: (1) Single annotated data only, (2) Single anno-
tated data with a random subset of it double anno-
tated2 (of the same size as the data selected by the
algorithm), (3) Single annotated data with the in-
stances selected by the algorithm double annotated,
and (4) Single annotated data with all instances dou-
ble annotated.
4.5 Error Detection Performance
In this experiment we evaluate how well the three
algorithms detect the errors. We split the data for
each word into 90% and 10% parts as described at
the beginning of section 4. We relabel the 90% part
with the labels assigned by the first tagger and use it
as a pool in which we detect the errors. We pass the
pool to each algorithm and compute the precision
and recall of errors in the data the algorithm returns.
We also measure the random baseline performance
by drawing the same number of examples randomly
and computing the precision and recall. The results
are in the top portion of Table 3.
Consider the second column, which shows the
performance of the machine tagger algorithm. The
algorithm identified as errors 16.93% of the total
number of examples that we passed to it. These se-
lected examples contained 60.32% of the total num-
ber of errors found in the data. Of the selected ex-
amples, 23.81% were in fact errors. By drawing the
same number of examples (16.93%) randomly we
recall only 16.79% of the single annotation errors.
The share of errors in the randomly drawn examples
is 6.82%. Thus, the machine tagger outperforms the
random baseline both with respect to precision and
recall.
The ambiguity detector algorithm selected 20% of
the examples with the highest value of the prediction
2Random sampling is often used as a baseline in the active
learning literature (Settles, 2009; Olsson, 2009).
margin and beat the random baseline both with re-
spect to precision and recall. The hybrid algorithm
also beat the random baselines. It recalled 75% of
errors but at the expense of selecting a larger set of
examples, 30.48%. This is the case because it selects
both the data selected by the machine tagger and the
ambiguity detector. The size selected, 30.48%, is
smaller than the sum, 16.93% + 20.01%, because
there is some overlap between the instances selected
by the first two algorithms.
4.6 Model Performance
In this experiment we investigate whether double
annotating and adjudicating selected instances im-
proves the accuracy of the models. We use the same
pool/test split (90%-10%) as was used in the previ-
ous experiment. The results are in the bottom por-
tion of Table 3.
Let us first validate empirically an assumption this
paper makes: we have been assuming that full dou-
ble annotation is justified because it helps to correct
the errors the first annotator made, which in turn
leads to a better performance. If this assumption
does not hold, our task is pointless. In general re-
peated labeling does not always lead to better per-
formance (Sheng et al, 2008), but it does in our
case. We train a model using only the single an-
notated data and test it. We then train a model using
the double annotated and adjudicated version of the
same data and evaluate its performance.
As expected, the models trained on fully double
annotated data perform better. The performance of
the fully double annotated data, 84.15%, is the ceil-
ing performance we can expect to obtain if we detect
all the errors made by the first annotator. The perfor-
mance of the single annotated data, 82.84%, is the
hard baseline. Thus, double annotating is beneficial,
especially if one can avoid double annotating every-
thing by identifying the single annotated instances
where an error is suspected.
All three algorithms beat both the hard and the
random baselines. For example, by double annotat-
ing the examples the hybrid algorithm selected we
achieve an accuracy of 83.82%, which is close to the
full double annotation accuracy, 84.15%. By double
annotating the same number of randomly selected
instances, we reach a lower accuracy, 83.36%. The
differences are statistically significant for all three
70
Metric Machine Tagger, % Ambiguity Detector, % Hybrid, %
Actual size selected 16.93 20.01 30.48
Error detection precision 23.81 10.61 14.70
Error detection recall 60.32 37.94 75.14
Baseline error detection precision 6.82 6.63 6.86
Baseline error detection recall 16.79 19.61 29.06
Single annotation only accuracy 82.84 82.84 82.84
Single + random double accuracy 83.23 83.09 83.36
Single + selected double accuracy 83.58 83.42 83.82
Full double annotation accuracy 84.15 84.15 84.15
Table 3: Results of performance evaluation. Error detection performance is shown at the top part of the table. Model
performance is shown at the bottom.
algorithms (p < 0.05).
Even though the accuracy gains over the random
baseline are modest in absolute terms, the reader
should keep in mind that the maximum possible ac-
curacy gain is 84.15% - 82.84% = 1.31% (when all
the data is double annotated). The hybrid algorithm
came closer to the target accuracy than the other
two algorithms because of a higher recall of errors,
75.14%, but at the expense of selecting almost twice
as much data as, for example, the machine tagger
algorithm.
4.7 Reaching Double Annotation Accuracy
The hybrid algorithm performed better than the
baselines but it still fell short of reaching the accu-
racy our system achieves when trained on fully dou-
ble annotated data. However, we have a simple way
of increasing the recall of error detection. One way
to do it is by increasing the number of instances with
the smallest prediction margin the ambiguity detec-
tor algorithm selects, which in turn will increase the
recall of the hybrid algorithm. In this series of exper-
iments we measure the performance of the hybrid al-
gorithm at various settings of the selection size. The
goal is to keep increasing the recall of errors until the
performance is close to the double annotation accu-
racy.
Again, we split the data for each word into 90%
and 10% parts. We relabel the 90% part with the
labels assigned by the first tagger and pass it to the
hybrid algorithm. We vary the selection size setting
between 20% and 50%. At each setting, we com-
pute the precision and recall of errors in the data
the algorithm returns as well as in the random base-
line. We also measure the performance of the mod-
els trained on on the single annotated data with its
randomly and algorithm-selected subsets double an-
notated. The results are in Table 4.
As we see at the top portion of the Table 4, as we
select more and more examples with a small predic-
tion margin, the recall of errors grows. For exam-
ple, at the 30% setting, the hybrid algorithm selects
37.91% of the total number of single annotated ex-
amples, which contain 80.42% of all errors in the
single annotated data (more than twice as much as
the random baseline).
As can be seen at the bottom portion of the Ta-
ble 4, with increased recall of errors, the accuracy
on the test set alo grows and nears the double an-
notation accuracy. At the 40% setting, the algorithm
selects 45.80% of the single annotated instances and
the accuracy with these instances double annotated
reaches 84.06% which is not statistically different
(p < 0.05) from the double annotation accuracy.
5 Discussion and Conclusion
We proposed several simple algorithms for reducing
the amount of the second round of annotation. The
algorithms operate by detecting annotation errors
along with hard-to-annotate and potentially error-
prone instances in single annotated data. We evalu-
ate the algorithms using OntoNotes word sense data.
Because OntoNotes data is double annotated and ad-
judicated we were able to evaluate the error detec-
tion performance of the algorithms as well as their
accuracy on the gold standard test set. All three al-
71
Metric Selection Size
20% 30% 40% 50%
Actual size selected 30.46 37.91 45.80 54.12
Error detection precision 14.63 12.81 11.40 10.28
Error detection recall 75.65 80.42 83.95 87.37
Baseline error detection precision 6.80 6.71 6.78 6.77
Baseline error detection recall 29.86 36.23 45.63 53.30
Single annotation only accuracy 83.04 83.04 83.04 83.04
Single + random double accuracy 83.47 83.49 83.63 83.81
Single + selected double accuracy 83.95 83.99 84.06 84.10
Full double annotation accuracy 84.18 84.18 84.18 84.18
Table 4: Performance at various sizes of selected data.
gorithms outperformed the random sampling base-
line both with respect to error recall and model per-
formance.
By progressively increasing the recall of errors,
we showed that the hybrid algorithm can be used
to replace full double annotation. The hybrid algo-
rithm reached accuracy that is not statistically dif-
ferent from the full double annotation accuracy with
approximately 46% of data double annotated. Thus,
it can potentially save 54% of the second pass of an-
notation effort without sacrificing performance.
While we evaluated the proposed algorithms only
on word sense data, the evaluation was performed
using 200 distinct word type datasets. These words
each have contextual features that are essentially
unique to that word type and consequently, 200
distinct classifiers, one per word type, are trained.
Hence, these could loosely be considered 200 dis-
tinct annotation and classification tasks. Thus, it is
likely that the proposed algorithms will be widely
applicable whenever a second round of annotation
is being contemplated to improve the quality of the
data.
6 Future Work
Toward the same goal of reducing the cost of the sec-
ond round of double annotation, we will explore sev-
eral research directions. We will investigate the util-
ity of more complex error detection algorithms such
as the ones described in (Eskin, 2000) and (Naka-
gawa and Matsumoto, 2002). Currently our algo-
rithms select the instances to be double annotated
in one batch. However it is possible to frame the
selection more like batch active learning, where the
next batch is selected only after the previous one is
annotated, which may result in further reductions in
annotation costs.
Acknowledgements
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambiguation,
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-C-0022, a subcontract from the BBN-AGILE
Team. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Steven Abney, Robert E. Schapire, and Yoram Singer.
1999. Boosting applied to tagging and pp attachment.
In Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 38?45.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha
Palmer. 2006. An empirical study of the behavior
of active learning for word sense disambiguation. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 120?127, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
72
Markus Dickinson. 2009. Correcting dependency anno-
tation errors. In EACL ?09: Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 193?201,
Morristown, NJ, USA. Association for Computational
Linguistics.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
HLT ?08: Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies, pages 29?32, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Eleazar Eskin. 2000. Detecting errors within a corpus
using anomaly detection. In Proceedings of the 1st
North American chapter of the Association for Com-
putational Linguistics conference, pages 148?153, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Fumiyo Fukumoto and Yoshimi Suzuki. 2004. Correct-
ing category errors in text classification. In COLING
?04: Proceedings of the 20th international conference
on Computational Linguistics, page 868, Morristown,
NJ, USA. Association for Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57?60, Morristown, NJ, USA. Association for
Computational Linguistics.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In HLT ?09: Proceedings
of the NAACL HLT 2009 Workshop on Active Learning
for Natural Language Processing, pages 27?35, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Pavel Kve?ton? and Karel Oliva. 2002. (semi-)automatic
detection of errors in pos-tagged corpora. In Proceed-
ings of the 19th international conference on Compu-
tational linguistics, pages 1?7, Morristown, NJ, USA.
Association for Computational Linguistics.
Zhang Le, 2004. Maximum Entropy Modeling Toolkit for
Python and C++.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In SIGIR ?94:
Proceedings of the 17th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 3?12, New York, NY, USA.
Springer-Verlag New York, Inc.
Dingsheng Luo, Xinhao Wang, Xihong Wu, and
Huisheng Chi. 2005. Learning outliers to refine a cor-
pus for chinese webpage categorization. In ICNC (1),
pages 167?178.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Comput. Linguist.,
19(2):313?330.
Tetsuji Nakagawa and Yuji Matsumoto. 2002. Detect-
ing errors in corpora using support vector machines.
In Proceedings of the 19th international conference
on Computational linguistics, pages 1?7, Morristown,
NJ, USA. Association for Computational Linguistics.
Va?clav Nova?k and Magda Raz??mova?. 2009. Unsu-
pervised detection of annotation inconsistencies using
apriori algorithm. In ACL-IJCNLP ?09: Proceedings
of the Third Linguistic Annotation Workshop, pages
138?141, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. In Technical Report, Swedish Institute of
Computer Science.
John C. Platt and John C. Platt. 1999. Probabilistic out-
puts for support vector machines and comparisons to
regularized likelihood methods. In Advances in Large
Margin Classifiers, pages 61?74. MIT Press.
Burr Settles. 2009. Active learning literature survey. In
Computer Sciences Technical Report 1648 University
of Wisconsin-Madison.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-
tis. 2008. Get another label? improving data qual-
ity and data mining using multiple, noisy labelers. In
KDD ?08: Proceeding of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 614?622, New York, NY, USA.
ACM.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 254?263, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Hans van Halteren. 2000. The detection of inconsistency
in manually tagged text. In Proceedings of LINC-00,
Luxembourg.
Z. Zhong, H.T. Ng, and Y.S. Chan. 2008. Word sense
disambiguation using OntoNotes: An empirical study.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1002?
1010. Association for Computational Linguistics.
Jingbo Zhu. 2007. Active learning for word sense disam-
biguation with methods for addressing the class imbal-
ance problem. In In Proceedings of ACL, pages 783?
790.
73
Proceedings of the Fifth Law Workshop (LAW V), pages 82?91,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A scaleable automated quality assurance technique for semantic
representations and proposition banks
K. Bretonnel Cohen
Computational Bioscience Program
U. of Colorado School of Medicine
Department of Linguistics
University of Colorado at Boulder
kevin.cohen@gmail.com
Lawrence E. Hunter
Computational Bioscience Program
U. of Colorado School of Medicine
larry.hunter@ucdenver.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
This paper presents an evaluation of an auto-
mated quality assurance technique for a type
of semantic representation known as a pred-
icate argument structure. These representa-
tions are crucial to the development of an im-
portant class of corpus known as a proposi-
tion bank. Previous work (Cohen and Hunter,
2006) proposed and tested an analytical tech-
nique based on a simple discovery proce-
dure inspired by classic structural linguistic
methodology. Cohen and Hunter applied the
technique manually to a small set of repre-
sentations. Here we test the feasibility of au-
tomating the technique, as well as the ability
of the technique to scale to a set of seman-
tic representations and to a corpus many times
larger than that used by Cohen and Hunter.
We conclude that the technique is completely
automatable, uncovers missing sense distinc-
tions and other bad semantic representations,
and does scale well, performing at an accu-
racy of 69% for identifying bad representa-
tions. We also report on the implications of
our findings for the correctness of the seman-
tic representations in PropBank.
1 Introduction
It has recently been suggested that in addition to
more, bigger, and better resources, we need a sci-
ence of creating them (Palmer et al, Download date
December 17 2010).
The corpus linguistics community has arguably
been developing at least a nascent science of anno-
tation for years, represented by publications such as
(Leech, 1993; Ide and Brew, 2000; Wynne, 2005;
Cohen et al, 2005a; Cohen et al, 2005b) that ad-
dress architectural, sampling, and procedural issues,
as well as publications such as (Hripcsak and Roth-
schild, 2005; Artstein and Poesio, 2008) that address
issues in inter-annotator agreement. However, there
is not yet a significant body of work on the subject
of quality assurance for corpora, or for that matter,
for many other types of linguistic resources. (Mey-
ers et al, 2004) describe three error-checking mea-
sures used in the construction of NomBank, and the
use of inter-annotator agreement as a quality control
measure for corpus construction is discussed at some
length in (Marcus et al, 1993; Palmer et al, 2005).
However, discussion of quality control for corpora is
otherwise limited or nonexistent.
With the exception of the inter-annotator-
agreement-oriented work mentioned above, none of
this work is quantitative. This is a problem if our
goal is the development of a true science of annota-
tion.
Work on quality assurance for computational lex-
ical resources other than ontologies is especially
lacking. However, the body of work on quality as-
surance for ontologies (Kohler et al, 2006; Ceusters
et al, 2004; Cimino et al, 2003; Cimino, 1998;
Cimino, 2001; Ogren et al, 2004) is worth consider-
ing in the context of this paper. One common theme
in that work is that even manually curated lexical re-
sources contain some percentage of errors.
The small size of the numbers of errors uncovered
in some of these studies should not be taken as a
significance-reducing factor for the development of
quality assurance measures for lexical resources?
82
rather, the opposite: as lexical resources become
larger, it becomes correspondingly more difficult to
locate errors in them. Finding problems in a very
errorful resource is easy; finding them in a mostly
correct resource is an entirely different challenge.
We present here an evaluation of a methodol-
ogy for quality assurance for a particular type of
lexical resource: the class of semantic representa-
tion known as a predicate argument structure (PAS).
Predicate argument structures are important in the
context of resource development in part because
they are the fundamental annotation target of the
class of corpus known as a proposition bank. Much
of the significance claim for this work comes from
the significance of proposition banks themselves in
recent research on natural language processing and
computational lexical semantics. The impact of
proposition banks on work in these fields is sug-
gested by the large number of citations of just the
three publications (Kingsbury and Palmer, 2002;
Kingsbury et al, 2002; Palmer et al, 2005)?at the
time of writing, 290, 220, and 567, respectively. Ad-
ditional indications of the impact of PropBank on
the field of natural language processing include its
use as the data source for two shared tasks ((Car-
reras and Ma`rquez, 2005)).
The methodology consists of looking for argu-
ments that never coo?ccur with each other. In struc-
tural linguistics, this property of non-coo?ccurrence
is known as complementary distribution. Comple-
mentary distribution occurs when two linguistic el-
ements never occur in the same environment. In
this case, the environment is defined as any sen-
tence containing a given predicate. Earlier work
showed a proof-of-concept application to a small set
of rolesets (defined below) representing the potential
PAS of 34 biomedical predicates (Cohen and Hunter
2006). The only inputs to the method are a set of
rolesets and a corpus annotated with respect to those
rolesets. Here, we evaluate the ability of the tech-
nique to scale to a set of semantic representations
137 times larger (4,654 in PropBank versus 34 in
Cohen and Hunter?s pilot project) and to a corpus
about 1500 times larger (1M words in PropBank ver-
sus about 680 in Cohen and Hunter?s pilot project)
than that considered in previous work. We also use
a set of independent judges to assess the technique,
where in the earlier work, the results were only as-
sessed by one of the authors.
Novel aspects of the current study include:
? Investigating the feasibility of automating the
previously manual process
? Scaling up the size of the set of semantic repre-
sentations evaluated
? Scaling up the size of the corpus against which
the representations are evaluated
? Using independent judges to assess the predic-
tions of the method
1.1 Definitions
For clarity, we define the terms roleset, frame file,
and predicate here. A roleset is a 2-tuple of a sense
for a predicate, identified by a combination of a
lemma and a number?e.g., love.01?and a set of in-
dividual thematic roles for that predicate?e.g., Arg0
lover and Arg1 loved. A frame file is the set of all
rolesets for a single lemma?e.g., for love, the role-
sets are love.01 (the sense whose antonym is hate)
and love.02, the ?semi-modal? sense in whether it
be melancholy or gay, I love to recall it (Austen,
1811). Finally, we refer to sense-labelled predicates
(e.g. love.01) as predicates in the remainder of the
paper.
PropBank rolesets contain two sorts of thematic
roles: (core) arguments and (non-core) adjuncts. Ar-
guments are considered central to the semantics of
the predicate, e.g. the Arg0 lover of love.01. Ad-
juncts are not central to the semantics and can occur
with many predicates; examples of adjuncts include
negation, temporal expressions, and locations.
In this paper, the arity of a roleset is determined
by its count of arguments, disregarding adjuncts.
1.2 The relationship between observed
argument distributions and various
characteristics of the corpus
This work is predicated on the hypothesis that argu-
ment distributions are affected by goodness of the fit
between the argument set and the actual semantics
of the predicate. However, the argument distribu-
tions that are observed in a specific data set can be
affected by other factors, as well. These include at
least:
? Inflectional and derivational forms attested in
the corpus
83
? Sublanguage characteristics
? Incidence of the predicate in the corpus
A likely cause of derivational effects on observed
distributions is nominalization processes. Nomi-
nalization is well known for being associated with
the omission of agentive arguments (Koptjevskaja-
Tamm, 1993). A genre in which nominalization is
frequent might therefore show fewer coo?ccurrences
of Arg0s with other arguments. Since PropBank
does not include annotations of nominalizations, this
phenomenon had no effect on this particular study.
Sublanguage characteristics might also affect ob-
served distributions. The sublanguage of recipes
has been noted to exhibit rampant deletions of def-
inite object noun phrases both in French and in En-
glish, as has the sublanguage of technical manuals
in English. (Neither of these sublanguages have
been noted to occur in the PropBank corpus. The
sublanguage of stock reports, however, presumably
does occur in the corpus; this sublanguage has been
noted to exhibit distributional subtleties of predi-
cates and their arguments that might be relevant to
the accuracy of the semantic representations in Prop-
Bank, but the distributional facts do not seem to in-
clude variability in argument coo?ccurrence so much
as patterns of argument/predicate coo?ccurrence (Kit-
tredge, 1982).)
Finally, incidence of the predicate in the corpus
could affect the observed distribution, and in partic-
ular, the range of argument coo?ccurrences that are
attested: the lower the number of observations of a
predicate, the lower the chance of observing any two
arguments together, and as the number of arguments
in a roleset increases, the higher the chance of failing
to see any pair together. That is, for a roleset with
an arity of three and an incidence of n occurrences
in a corpus, the likelihood of never seeing any two
of the three arguments together is much lower than
for a roleset with an arity of six and an incidence of
n occurrences in the corpus. The number of obser-
vations required in order to be able to draw conclu-
sions about the observed argument distributions with
some degree of confidence is an empirical question;
prior work (Cohen and Hunter 2006) suggests that
as few as ten tokens can be sufficient to uncover er-
roneous representations for rolesets with an arity of
four or less, although that number of observations
of one roleset with an arity of four showed multiple
non-coo?ccurring arguments that were not obviously
indicative of problems with the representation (i.e.,
a false positive finding).
Besides the effects of these aspects of the corpus
contents on the observed distributions, there are also
a number of theoretical and practical issues in the
design and construction of the corpus (as distinct
from the rolesets, or the distributional characteris-
tics of the contents) which have nontrivial implica-
tions for the methodology being evaluated here. In
particular, the implications of the argument/adjunct
distinction, of the choice of syntactic representation,
and of annotation errors are all discussed in Sec-
tion 4. Note that we are aware that corpus-based
studies generally yield new lexical items and us-
ages any time a new corpus is introduced, so we
do not make the naive assumption that PropBank
will give complete coverage of all coo?ccurring argu-
ments, and in fact our evaluation procedure took this
into account explicitly, as described in Section 2.3.
2 Materials and Methods
2.1 Materials
We used Rev. 1.0 of the PropBank I corpus, and the
associated framesets in the frames directory.
2.2 Methods
2.2.1 Determining the distribution of
arguments for a roleset
In determining the possible coo?ccurring argument
pairs for a roleset, we considered only arguments,
not adjuncts. As we discuss in Section 4.1, this
is a non-trivial decision with potential implications
for the ability of the algorithm to detect problem-
atic representations in general, and with implications
for PropBank in particular. The rationale behind the
choice to consider only arguments is that our goal
is to evaluate the representation of the semantics of
the predicates, and that by definition, the PropBank
arguments are essential to defining that semantics,
while by definition, the adjuncts are not.
In the first processing step, for each roleset, we
used the corresponding framefile as input and gen-
erated a look-up table of the possible argument
pairs for that predicate. For example, the predi-
cate post.01 has the three arguments Arg0, Arg1, and
84
Arg2; we generated the set {<Arg0, Arg1>, <Arg0,
Arg2>, <Arg1, Arg2>} for it.
In the second processing step, we iterated over all
annotations in the PropBank corpus, and for each to-
ken of each predicate, we extracted the complete set
of arguments that occurred in association with that
token. We then constructed the set of coo?ccurring ar-
guments for that annotation, and used it to increment
the counts of each potential argument pair for the
predicate in question. For example, the PropBank
annotation for Oils and fats also did well, posting
a 5.3% sales increase (wsj/06/wsj 0663.mrg)
contains an Arg0 and an Arg1, so we incremented
the count for that argument pair by 1; it contains no
other argument pairs, so we did not increment the
counts for <Arg0, Arg2> or <Arg1, Arg2>.
The output of this step was a table with the count
of occurrence of every potential pair of arguments
for every roleset; members of pairs whose count was
zero were then output as arguments in complemen-
tary distribution. For example, for post.01, the pairs
<Arg0, Arg2> and <Arg1, Arg2> never occurred,
even as traces, so the arguments Arg0 and Arg2 are
in complementary distribution for this predicate, as
are the arguments Arg1 and Arg2.
To manipulate the data, we used Scott Cotton?s
Java API, with some extensions, which we docu-
mented in the API?s Javadoc.
2.3 Determining the goodness of rolesets
exhibiting complementary distribution
In (Cohen and Hunter, 2006), determinations of the
goodness of rolesets were made by pointing out the
distributional data to the corpus creators, showing
them the corresponding data, and reaching consen-
sus with them about the appropriate fixes to the rep-
resentations. For this larger-scale project, one of the
goals was to obtain goodness judgements from com-
pletely independent third parties.
Towards that end, two judges with experience in
working with PropBank were assigned to judge the
predictions of the algorithm. Judge 1 had two years
of experience, and Judge 2 had four years of expe-
rience. The judges were then given a typology of
classification to assign to the predicates: good, bad,
and conditionally bad. The definitions of these cate-
gories, with the topology of the typology, were:
? Good: This label is assigned to predicates that
the algorithm predicted to have bad representa-
tions, but that are actually good. They are false
positives for the method.
? Not good: (This label was not actually as-
signed, but rather was used to group the fol-
lowing two categories.)
? Bad: This label is assigned to predicates
that the algorithm predicted to have bad
representations and that the judges agreed
were bad. They are true positives for the
method.
? Conditionally bad: This label is assigned
to predicates that the algorithm predicted
to have bad representations and that the
judges agreed were bad based on the ev-
idence available in PropBank, but that the
judges thought might be good based on
native speaker intiution or other evidence.
In all of these cases, the judges did suggest
changes to the representations, and they
were counted as not good, per the typol-
ogy, and are also true positives.
Judges were also asked to indicate whether bad
representations should be fixed by splitting predi-
cates into more word senses, or by eliminating or
merging one or more arguments.
We then took the lists of all predicted bad predi-
cates that appeared at least 50, 100, or 200 times in
the PropBank corpus. These were combined into a
single list of 107 predicates and randomized. The
judges then split the list into halves, and each judge
examined half of the list. Additionally, 31 predi-
cates, or 29% of the data set, were randomly selected
for double annotation by both judges to assess inter-
judge agreement. Judges were shown both the predi-
cates themselves and the sets of non-coo?ccurring ar-
guments for each predicate.
3 Results
3.1 Accuracy
The overall results were that out of 107 predicates,
33 were judged GOOD, i.e. were false positives.
44 were judged BAD and 30 were judged CONDI-
TIONAL, i.e. were true positives. This yields a ratio
of 2.24 of true positives to false positives: the pro-
85
Table 1: Ratios of BAD plus CONDITIONAL to GOOD
for the pooled judgements as broken down by arity
Arity Ratio
3 1.29
4 1.47
5 4.0
6 8.0
7 None found
cedure returns about two true positives for every one
false positive. Expressed in terms of accuracy, this
corresponds to 69% for correctly labelling true pos-
itives.
We broke down the data by (1) arity of the role-
set, and (2) minimum number of observations of a
role set. This allowed us to test whether predictive
power decreased as arity increased, and to test the
dependency of the algorithm on the minimum num-
ber of observations; we suspected that it might be
less accurate the fewer the number of observations.
Table 1 shows the ratios of true positives to false
positives, broken down by arity. The data confirms
that the algorithm is effective at finding bad repre-
sentations, with the number of true positives out-
numbering the number of false positives at every
arity. This data is also important because it allows
us to test a hypothesis: is it the case that predictive
power becomes worse as arity increases? As the ta-
ble shows, the ratio of true positives to false posi-
tives actually increases as the arity of the predicate
increases. Therefore, the data is consistent with the
hypothesis that not only does the predictive power of
the algorithm not lessen as arity increases, but rather
it actually becomes greater.
Table 2 shows the ratios of true positives to false
positives again, this time broken down by minimum
number of occurrences of the predicates. Again, the
data confirms that the algorithm is effective at find-
ing bad representations?it returns more bad repre-
sentations than good representations at every level of
minimum number of observations. This data is also
important because it allows us to test the hypothe-
sis of whether or not predictive power of the algo-
rithm decreases with the minimum number of obser-
vations. As we hypothesized, it does show that the
predictive power decreases as the minimum number
Table 2: Ratios of BAD plus CONDITIONAL to GOOD
for the pooled judgements as broken down by minimum
number of observations
ratio
Minimum 50 1.88
Minimum 100 2.63
Minimum 200 2.63
of observations decreases, with the ratio of true pos-
itives to false positives dropping from 2.63 with a
minimum of 200 or 100 observations to 1.88 with a
minimum of 50 observations. However, the ratio of
true positives to false positives remains close to 2:1
at every level.
3.2 Suggested fixes to the representations
Of the 74 true positives, the judges felt that 17 of
the bad representations should be fixed by splitting
the predicate into multiple senses. For the 57 re-
maining true positives, the judges felt that an argu-
ment should be removed from the representation or
converted to an adjunct. This demonstrates that the
method is applicable both to the problem of reveal-
ing missing sense distinctions and to the problem of
identifying bad arguments.
3.3 Scalability
The running time was less than one and a half min-
utes for all 4,654 rolesets on the 1-million-word cor-
pus.
3.4 Inter-judge agreement
A subset of 31 predicates was double-annotated by
the two judges to examine inter-judge agreement.
The judges then examined the cases on which they
initially disagreed, and came to a consensus where
possible. Initially, the judges agreed in 63.3% of the
cases, which is above chance but not the 80% agree-
ment that we would like to see. The judges then went
through a reconciliation process. They were able to
come to a consensus in all cases.
3.5 Putting the results in context
To help put these results in context, we give here the
distribution of arities in the PropBank rolesets and
the minimum number of observations of each in the
PropBank corpus.
86
Table 3: Distribution of arities by percentage and by
count in the 4,654 PropBank rolesets.
Arity percentage (count)
0 0.28% (13)
1 (Arg0) 155
1 (Arg1) 146
1 (all) 6.5% (301)
2 45.14% (2,101)
3 37.02% (1,723)
4 7.05% (328)
5 3.5% (163)
6 0.5% (24)
7 0.0002% (1)
Total 100% (4,654)
Table 3 shows the distribution of arities in the
PropBank rolesets. It distinguishes between non-
ergatives and ergatives (although for the purpose
of calculating percentages, they are combined into
one single-arity group). The mode is an arity of 2:
45.14% of all rolesets (2,101/4,654) have an arity of
2. 3 is a close second, with 37.02% (1,723/4,654).
(The single roleset with an arity of seven is notch.02,
with a gloss of ?move incrementally.?)
Table 4 gives summary statistics for the occur-
rence of complementary distribution, showing the
distribution of rolesets in which there were at least
one argument pair in complementary distribution
and of the total number of argument pairs in comple-
mentary distribution. Since (as noted in Section 1.2)
the incidence of a predicate has a potential effect
on the incidence of argument pairs in apparent com-
plementary distribution, we display the counts sepa-
rately for four cut-offs for the minimum number of
observations of the predicate: 200, 100, 50, and 10.
To further explicate the operation of the discovery
procedure, we give here some examples of rolesets
that were found to have arguments in complemen-
tary distribution.
3.5.1 accept.01
Accept.01 is the only roleset for the lemma ac-
cept. Its sense is take willingly. It has four argu-
ments:
? Arg0 acceptor
Table 4: Summary statistics: counts of predicates with
at least one argument pair in complementary distribution
and of total argument pairs in complementary distribution
for four different minimum numbers of observations of
the predicates.
Minimum observations Predicates Argument pairs
200 29 69
100 58 125
50 107 268
10 328 882
? Arg1 thing accepted
? Arg2 accepted-from
? Arg3 attribute
The predicate occurs 149 times in the corpus. The
algorithm found Arg2 and Arg3 to be in complemen-
tary distribution.
Manual investigation showed the following distri-
butional characteristics for the predicate and its ar-
guments:
? (Arg0 or Arg1) and Arg2: 5 tokens
? (Arg0 or Arg1) and Arg3: 8 tokens
? Arg2 with neither Arg0 nor Arg1: 0 tokens
? Arg3 with neither Arg0 nor Arg1: 0 tokens
? Arg0 or Arg1 with neither Arg2 nor Arg 3: 136
tokens
Examination of the 5 tokens in which Arg2
coo?ccurred with Arg0 or Arg1 and the 8 tokens
in which Arg3 coo?ccurred with Arg0 or Arg1 sug-
gested an explanation for the complementary distri-
bution of arguments Arg2 and Arg3. When Arg2
appeared, the sense of the verb seemed to be one
of physical transfer: Arg2 coo?ccurred with Arg1s
like substantial gifts (wsj 0051.mrg) and a $3
million payment (wsj 2071.mrg). In contrast,
when Arg3 appeared, the sense was not one of
physical transfer, but of some more metaphorical
sense?Arg3 coo?ccurred with Arg1s like the war
(wsj 0946.mrg) and Friday?s dizzying 190-point
plunge (wsj 2276.mrg). There is no accept.02;
creating one with a 3-argument roleset including the
current Arg3 seems warranted. Keeping the Arg3
for accept.01 might be warranted, as well, but prob-
ably as an adjunct (to account for usages like John
accepted it as a gift.)
87
3.5.2 affect.01
Affect.01 is one of two senses for the lemma af-
fect. Its sense is have an effect on. It has three argu-
ments:
? Arg0 thing affecting
? Arg1 thing affected
? Arg2 instrument
The predicate occurs 149 times in the corpus. The
algorithm found Arg0 and Arg2, as well as Arg1 and
Arg2, to be in complementary distribution.
Manual investigation revealed that in fact, Arg2
never appears in the corpus at all. Presumably, ei-
ther Arg0 and Arg2 should be merged, or?more
likely?Arg2 should not be an argument, but rather
an adjunct.
3.6 Incidental findings
3.6.1 Mistakes uncovered in frame files
In the process of calculating the set of possible
argument pairs for each predicate in the PropBank
frame files, we found a roleset that erroneously had
two Arg1s. The predicate in question was pro-
scribe.01. The roles in the frame file were:
? Arg0 causer
? Arg1 thing proscribed
? Arg1 proscribed from
It was clear from the annotations in the exam-
ple sentence that the ?second? Arg1 was intended to
be an Arg2: [The First AmendmentArg0] proscribes
[the governmentArg1] from [passing laws abridging
the right to free speechArg2].
3.6.2 Unlicensed arguments used in the corpus
We found eighteen tokens in the corpus that were
annotated with argument structures that were not li-
censed by the roleset for the corresponding predi-
cate. For example, the predicate zip.01 has only
a single argument in its semantic representation?
Arg0, described as entity in motion. However, the
corpus contains a token of zip.01 that is annotated
with an Arg0 and an Arg1.
4 Discussion/Conclusions
4.1 The effect of the argument/adjunct
distinction
The validity and usefulness of the distinction be-
tween arguments and adjuncts is an ongoing con-
troversy in biomedical computational lexical se-
mantics. The BioProp project (Chou et al, 2006;
Tsai et al, 2006) makes considerable use of ad-
juncts, essentially identically to PropBank; however,
most biomedical PAS-oriented projects have rela-
tively larger numbers of arguments and lesser use
of adjuncts (Wattarujeekrit et al, 2004; Kogan et al,
2005; Shah et al, 2005) than PropBank. Overall,
one would predict fewer non-coo?ccurring arguments
with a set of representations that made a stronger
distinction between arguments and adjuncts; over-
all arity of rolesets would be smaller (see above for
the effect of arity on the number of observations re-
quired for a predicate), and the arguments for such a
representation might be more ?core? to the seman-
tics of the predicate, and might therefore be less
likely to not occur overall, and therefore less likely
to not coo?ccur.
4.2 The effect of syntactic representation on
observed argument distributions
The original work by Cohen and Hunter assumed a
very simple, and very surface, syntactic representa-
tion. In particular, there was no representation of
traces. In contrast, PropBank is built on Treebank
II, which does include representation of traces, and
arguments can, in fact, be filled by traces. This could
be expected to reduce the number of tokens of appar-
ently absent arguments, and thereby the number of
non-coo?occurring arguments. This doesn?t seem to
have had a strong enough effect to interfere with the
ability of the method to uncover errors.
4.3 The effect of arity
The mode for distribution of arities in the Prop-
Bank framefiles was 2 (see Table 3). In contrast, the
modes for distribution of rolesets with at least one
argument pair in complementary distribution across
arities and for distribution of argument pairs in com-
plementary distribution across arities was 4 or 5
for the full range of minimum observations of the
predicates from 200 to 10 (data omitted for space).
88
This supports the initial assumption that higher-arity
predicates are more likely to have argument pairs in
complementary distribution?see Section 1.2 above.
One aspect of a granular analysis of the data is
worth pointing out with respect to the effects of ar-
ity: as a validation check, note that for all arities,
the number of predicates and the number of argu-
ment pairs rises as the minimum required number of
tokens of the predicate in the corpus goes down.
4.4 Conclusions
The goals of this study were to investigate the au-
tomatability and scalability of a technique for PAS
quality assurance that had previously only been
shown to work for a small lexical resource and
a small corpus, and to use it to characterize the
quality of the shallow semantic representations in
the PropBank framefiles. The evaluation procedure
was found to be automatable: the process of find-
ing argument pairs in complementary distribution is
achievable by running a single Java application. In
addition, the use of a common representation for ar-
gument sets in a framefile and argument sets in a
PropBank annotation enabled the fortuitous discov-
ery of a number of problems in the framefiles and in
the corpus (see Section 3.6) as a side-effect of appli-
cation of the technique.
The process was also found to scale well, with
a running time of less than one and a half minutes
for a set of 4,654 rolesets and a 1-million-word cor-
pus on a moderately priced laptop; additionally, the
resource maintainer?s efforts can easily be focussed
towards the most likely and the most prevalent error
sources by adjusting the minimum number of obser-
vations required before reporting a case of comple-
mentary distribution. The process was also found to
be able to identify missing sense distinctions and to
identify bad arguments.
In addition to our findings regarding the quality
assurance technique, a granular breakdown of the
errors found by the algorithm by arity and mini-
mum number of observations (data not shown due to
space) allows us to estimate the number of errors in
the PropBank framefiles. A reasonable upper-bound
estimate for the number of errorful rolesets is the
number of predicates that were observed at least 10
times and were found to have at least one pair of ar-
guments in complementary distribution (the bottom
row of Table 4), adjusted by the accuracy of the tech-
nique that we reported in Section 3.1, i.e. 0.69. This
yields a worst-case scenario of (0.69*328)/4,654
rolesets, or 4.9% of the rolesets in PropBank, be-
ing in need of revision. The best-case scenario
would assume that we can only draw conclusions
about the predicates with high numbers of observa-
tions and high arity, again adjusted downward for
the accuracy of the technique; taking 5 or more argu-
ments as high arity, this yields a best-case scenario
of (0.69*17)/4,654 rolesets, or 0.3% of the rolesets
in PropBank, being in need of revision. A different
sort of worst-case scenario assumes that the major
problem in maintaining a proposition bank is not fix-
ing inadequate representations, but finding them. On
this assumption, the problematic representations are
the ones with small numbers of tokens and low ar-
ity. Taking 3 or fewer arguments as low arity yields a
worst-case scenario of 99/4,654 rolesets (no adjust-
ment for accuracy required), or 2.13% of the rolesets
in PropBank, being essentially uncharacterizable as
to the goodness of their semantic representation1.
Besides its obvious role in quality assurance for
proposition banks, there may be other uses for this
technique, as well. The output of the technique may
also be useful in sense grouping and splitting and in
detecting metaphorical uses of verbs (e.g. the accept
example). As the PropBank model is extended to an
increasingly large set of languages (currently Ara-
bic, Basque, Catalan, Chinese, Hindi, Korean, and
Russian), the need for a quality assurance mecha-
nism for proposition banks?both to ensure the qual-
ity of their contents, and to assure funding agencies
that they are evaluatable?will only grow larger.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Jane Austen. 1811. Sense and Sensibility.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: semantic role label-
1The situation is arguably actually somewhat worse than
this, since it does not take into account predicates which occur
fewer than ten times in the corpus; however, there is a reason-
able counter-argument that those predicates are too rare for any
individual roleset to have a large impact on the overall goodness
of the resource.
89
ing. In Proceedings of the 9th conference on computa-
tional natural language learning, pages 152?164.
Werner Ceusters, Barry Smith, Anand Kumar, and
Christoffel Dhaen. 2004. Mistakes in medical on-
tologies: where do they come from and how can they
be detected? In D.M. Pisanelli, editor, Ontologies in
medicine: proceedings of the workshop on medical on-
tologies, pages 145?163. IOS Press.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,
Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu. 2006.
A semi-automatic method for annotating a biomedi-
cal proposition bank. In Proceedings of the workshop
on frontiers in linguistically annotated corpora 2006,
pages 5?12. Association for Computational Linguis-
tics.
J.J. Cimino, H. Min, and Y. Perl. 2003. Consistency
across the hierarchies of the UMLS Semantic Network
and Metathesaurus. Journal of Biomedical Informat-
ics, 36:450?461.
James J. Cimino. 1998. Auditing the Unified Medical
Language System with semantic methods. Journal of
the American Medical Informatics Association, 5:41?
51.
James J. Cimino. 2001. Battling Scylla and Charybdis:
the search for redundancy and ambiguity in the 2001
UMLS Metathesaurus. In Proc. AMIA annual sympo-
sium, pages 120?124.
K. Bretonnel Cohen and Lawrence Hunter. 2006. A
critical revew of PASBio?s argument structures for
biomedical verbs. BMC Bioinformatics, 7(Suppl. 3).
K. B. Cohen, Lynne Fox, Philip V. Ogren, and Lawrence
Hunter. 2005a. Corpus design for biomedical natural
language processing. In Proceedings of the ACL-ISMB
workshop on linking biological literature, ontologies
and databases, pages 38?45. Association for Compu-
tational Linguistics.
K. Bretonnel Cohen, Lynne Fox, Philip V. Ogren, and
Lawrence Hunter. 2005b. Empirical data on corpus
design and usage in biomedical natural language pro-
cessing. In AMIA 2005 symposium proceedings, pages
156?160.
George Hripcsak and Adam S. Rothschild. 2005. Agree-
ment, the F-measure, and reliability in information re-
trieval. Journal of the American Medical Informatics
Association, 12(3):296?298.
Nancy Ide and Chris Brew. 2000. Requirements, tools,
and architectures for annotated corpora. In Proc. data
architectures and software support for large corpora,
pages 1?5.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the LREC.
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference.
Richard Kittredge. 1982. Variation and homogene-
ity of sublanguages. In Richard Kittredge and John
Lehrberger, editors, Sublanguage: studies of language
in restricted semantic domains, pages 107?137.
Yacov Kogan, Nigel Collier, Serguei Pakhomov, and
Michael Krauthammer. 2005. Towards semantic role
labeling & IE in the medical literature. In AMIA 2005
Symposium Proceedings, pages 410?414.
Jacob Kohler, Katherine Munn, Alexander Ruegg, An-
dre Skusa, and Barry Smith. 2006. Quality control
for terms and definitions in ontologies and taxonomies.
BMC Bioinformatics, 7(1).
Maria Koptjevskaja-Tamm. 1993. Nominalizations.
Routledge.
Geoffrey Leech. 1993. Corpus annotation schemes. Lit-
erary and linguistic computing, pages 275?281.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. Annotating noun argument structure
for NomBank. In Proceedings of Language Resources
and Evaluation, LREC.
Philip V. Ogren, K. Bretonnel Cohen, George K.
Acquaah-Mensah, Jens Eberlein, and Lawrence
Hunter. 2004. The compositional structure of Gene
Ontology terms. Pacific Symposium on Biocomputing,
pages 214?225.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: an annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Stephanie Strassel, and Randee Tangi.
Download date December 17, 2010. Historical devel-
opment and future directions in data resource develop-
ment. In MINDS 2006?2007.
Parantu K. Shah, Lars J. Jensen, Ste?phanie Boue?, and
Peer Bork. 2005. Extraction of transcript diversity
from scientific literature. PLoS Computational Biol-
ogy, 1(1):67?73.
Richard Tzong-Han Tsai, Wen-Chi Chou, Yu-Chun Lin,
Cheng-Lung Sung, Wei Ku, Ying-Shan Su, Ting-Yi
Sung, and Wen-Lian Hsu. 2006. BIOSMILE: adapt-
ing semantic role labeling for biomedical verbs: an
exponential model coupled with automatically gener-
ated template features. In Proceedings of the BioNLP
Workshop on Linking Natural Language Processing
and Biology, pages 57?64. Association for Computa-
tional Linguistics.
90
Tuangthong Wattarujeekrit, Parantu K. Shah, and Nigel
Collier. 2004. PASBio: predicate-argument structures
for event extraction in molecular biology. BMC Bioin-
formatics, 5(155).
Martin Wynne, editor. 2005. Developing linguistic cor-
pora: a guide to good practice. David Brown Book
Company.
91
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), page 1,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Going Beyond Shallow Semantics
(invited talk)
Martha Palmer
University of Colorado at Boulder
Martha.Palmer@colorado.edu
Abstract
Shallow semantic analyzers, such as semantic role labeling and sense tagging, are increasing in ac-
curacy and becoming commonplace. However, they only provide limited and local representations of
local words and individual predicate-argument structures. This talk will address some of the current
challenges in producing deeper, connected representations of eventualities. Available resources, such
as VerbNet, FrameNet and TimeBank, that can assist in this process will also be discussed, as well as
some of their limitations.
Speaker?s Bio
Martha Palmer is a Full Professor at the University of Colorado with joint appointments in Linguistics
and Computer Science and is an Institute of Cognitive Science Faculty Fellow. She recently won a
Boulder Faculty Assembly 2010 Research Award. Beginning with her dissertation work at Edinburgh
and her first job as a Research Scientist at Unisys, her research has been focused on trying to capture
the meanings of words in representations that the computer can use to build up meanings of complex
sentences and documents. These representations can in turn be used to improve the computer?s ability
to perform question answering, information retrieval, and machine translation. Current approaches rely
on techniques for applying supervised machine learning algorithms, which use vast amounts of anno-
tated training data. Therefore, she and her students, both at Colorado and previously at the University of
Pennsylvania, are engaged in providing data with word sense tags and semantic role labels for English,
Chinese, Arabic, and Hindi, funded by DARPA and NSF. They also use machine learning algorithms
to develop automatic sense taggers and semantic role labelers, and to extract bilingual lexicons from
parallel corpora. A more recent focus is the application of these methods to biomedical journal articles
and clinical notes, funded by NIH. She is a co-editor for both the Journal of Natural Language Engi-
neering and LiLT, Linguistic Issues in Language Technology. She is a past President of the Association
for Computational Linguistics, past Chair of SIGLEX and SIGHAN, and is currently the Director of
the 2011 Linguistics Institute to be held in Boulder, Colorado.
1
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 37?45,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Transition-based Semantic Role Labeling
Using Predicate Argument Clustering
Jinho D. Choi
Department of Computer Science
University of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
mpalmer@colorado.edu
Abstract
This paper suggests two ways of improving
semantic role labeling (SRL). First, we intro-
duce a novel transition-based SRL algorithm
that gives a quite different approach to SRL.
Our algorithm is inspired by shift-reduce pars-
ing and brings the advantages of the transition-
based approach to SRL. Second, we present
a self-learning clustering technique that effec-
tively improves labeling accuracy in the test
domain. For better generalization of the sta-
tistical models, we cluster verb predicates by
comparing their predicate argument structures
and apply the clustering information to the
final labeling decisions. All approaches are
evaluated on the CoNLL?09 English data. The
new algorithm shows comparable results to
another state-of-the-art system. The cluster-
ing technique improves labeling accuracy for
both in-domain and out-of-domain tasks.
1 Introduction
Semantic role labeling (SRL) has sparked much in-
terest in NLP (Shen and Lapata, 2007; Liu and
Gildea, 2010). Lately, dependency-based SRL has
shown advantages over constituent-based SRL (Jo-
hansson and Nugues, 2008). Two main benefits can
be found. First, dependency parsing is much faster
than constituent parsing, whereas constituent pars-
ing is usually considered to be a bottleneck to SRL in
terms of execution time. Second, dependency struc-
ture is more similar to predicate argument struc-
ture than phrase structure because it specifically de-
fines relations between a predicate and its arguments
with labeled arcs. Unlike constituent-based SRL
that maps phrases to semantic roles, dependency-
based SRL maps headwords to semantic roles be-
cause there is no phrasal node in dependency struc-
ture. This may lead to a concern about getting the
actual semantic chunks back, but Choi and Palmer
(2010) have shown that it is possible to recover the
original chunks from the headwords with minimal
loss, using a certain type of dependency structure.
Traditionally, either constituent or dependency-
based, semantic role labeling is done in two steps,
argument identification and classification (Gildea
and Jurafsky, 2002). This is from a general be-
lief that each step requires a different set of fea-
tures (Xue and Palmer, 2004), and training these
steps in a pipeline takes less time than training them
as a joint-inference task. However, recent machine
learning algorithms can deal with large scale vector
spaces without taking too much training time (Hsieh
et al, 2008). Furthermore, from our experience in
dependency parsing, handling these steps together
improves accuracy in identification as well as clas-
sification (unlabeled and labeled attachment scores
in dependency parsing). This motivates the develop-
ment of a new semantic role labeling algorithm that
treats these two steps as a joint inference task.
Our algorithm is inspired by shift-reduce pars-
ing (Nivre, 2008). The algorithm uses several transi-
tions to identify predicates and their arguments with
semantic roles. One big advantage of the transition-
based approach is that it can use previously identi-
fied arguments as features to predict the next argu-
ment. We apply this technique to our approach and
achieve comparable results to another state-of-the-
art system evaluated on the same data sets.
37
NO-PRED
( ?1 , ?2, j, ?3, [i|?4], A )? ( [?1|j], ?2, i, ?3, ?4 , A )
?j. oracle(j) 6= predicate
SHIFT
( ?1 , ?2, j, [i|?3], ?4, A )? ( [?2|j], [ ] , i, [ ] , ?3, A )
?j. oracle(j) = predicate ? ?1 = [ ] ? ?4 = [ ]
NO-ARC?
( [?1|i], ?2 , j, ?3, ?4, A )? ( ?1 , [i|?2], j, ?3, ?4, A )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {i 6? j}
NO-ARC?
( ?1, ?2, j, ?3 , [i|?4], A )? ( ?1, ?2, j, [?3|i], ?4 , A )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {j 6? i}
LEFT-ARC?L
( [?1|i], ?2 , j, ?3, ?4, A )? ( ?1 , [i|?2], j, ?3, ?4, A ? {i
L
? j} )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {i
L
? j}
RIGHT-ARC?L
( ?1, ?2, j, ?3 , [i|?4], A )? ( ?1, ?2, j, [?3|i], ?4 , A ? {j
L
? i} )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {j
L
? i}
Table 1: Transitions in our bidirectional top-down search algorithm. For each row, the first line shows a transition and
the second line shows preconditions of the transition.
For better generalization of the statistical models,
we apply a self-learning clustering technique. We
first cluster predicates in test data using automati-
cally generated predicate argument structures, then
cluster predicates in training data by using the previ-
ously found clusters as seeds. Our experiments show
that this technique improves labeling accuracy for
both in-domain and out-of-domain tasks.
2 Transition-based semantic role labeling
Dependency-based semantic role labeling can be
viewed as a special kind of dependency parsing in
the sense that both try to find relations between
word pairs. However, they are distinguished in two
major ways. First, unlike dependency parsing that
tries to find some kind of relation between any word
pair, semantic role labeling restricts its search only
to top-down relations between predicate and argu-
ment pairs. Second, dependency parsing requires
one head for each word, so the final output is a tree,
whereas semantic role labeling allows multiple pred-
icates for each argument. Thus, not all dependency
parsing algorithms, such as a maximum spanning
tree algorithm (Mcdonald and Pereira, 2006), can be
naively applied to semantic role labeling.
Some transition-based dependency parsing algo-
rithms have been adapted to semantic role labeling
and shown good results (Henderson et al, 2008;
Titov et al, 2009). However, these algorithms are
originally designed for dependency parsing, so are
not necessarily customized for semantic role label-
ing. Here, we present a novel transition-based algo-
rithm dedicated to semantic role labeling. The key
difference between this algorithm and most other
transition-based algorithms is in its directionality.
Given an identified predicate, this algorithm tries to
find top-down relations between the predicate and
the words on both left and right-hand sides, whereas
other transition-based algorithms would consider
words on either the left or the right-hand side, but
not both. This bidirectional top-down search makes
more sense for semantic role labeling because predi-
cates are always assumed to be the heads of their ar-
guments, an assumption that cannot be generalized
to dependency parsing, and arguments can appear
either side of the predicate.
Table 1 shows transitions used in our algorithm.
All parsing states are represented as tuples (?1, ?2,
p, ?3, ?4, A), where ?1..4 are lists of word indices
and p is either a word index of the current predi-
cate candidate or @ indicating no predicate candi-
date. ?1,4 contain indices to be compared with p and
?2,3 contain indices already compared with p. A is a
set of labeled arcs representing previously identified
arguments with respect to their predicates. ? and
? indicate parsing directions. L is a semantic role
label, and i, j represent indices of their correspond-
ing word tokens. The initial state is ([ ], [ ], 1, [ ],
[2, . . . , n], ?), where w1 and wn are the first and the
last words in a sentence, respectively. The final state
is (?1, ?2, @, [ ], [ ],A), i.e., the algorithm terminates
when there is no more predicate candidate left.
38
John
1
wants
2
to
3
buy
4
a
5
car
6
Root
0
SBJ
ROOT
OPRD
OBJ
IM
NMOD
A0 A1
A0 A1
Figure 1: An example of a dependency tree with semantic roles. The upper and lower arcs stand for syntactic and
semantic dependencies, respectively. SBJ, OBJ, OPRD, IM, NMOD stand for a subject, object, object predicative,
infinitive marker, and noun-modifier. A0, A1 stand for ARG0, ARG1 in PropBank (Palmer et al, 2005).
Transition ?1 ?2 p ?3 ?4 A
0 [ ] [ ] 1 [ ] [2..6] ?
1 NO-PRED [1] [ ] 2 [ ] [3..6]
2 LEFT-ARC [ ] [1] 2 [ ] [3..6] A ? {1?A0? 2}
3 RIGHT-ARC [ ] [1] 2 [3] [4..6] A ? {2 ?A1? 3}
4 NO-ARC [ ] [1] 2 [3..4] [5..6]
5 NO-ARC [ ] [1] 2 [3..5] [6]
6 NO-ARC [ ] [1] 2 [3..6] [ ]
7 SHIFT [1..2] [ ] 3 [ ] [4..6]
8 NO-PRED [1..3] [ ] 4 [ ] [5..6]
9 NO-ARC [1..2] [3] 4 [ ] [5..6]
10 NO-ARC [1] [2..3] 4 [ ] [5..6]
11 LEFT-ARC [ ] [1..3] 4 [ ] [5..6] A ? {1?A0? 4}
12 NO-ARC [ ] [1..3] 4 [5] [6]
13 RIGHT-ARC [ ] [1..3] 4 [5..6] [ ] A ? {4 ?A1? 6}
14 SHIFT [1..4] [ ] 5 [ ] [6]
15 NO-PRED [1..5] [ ] 6 [ ] [ ]
16 NO-PRED [1..6] [ ] @ [ ] [ ]
Table 2: Parsing states generated by our algorithm for the example in Figure 1.
The algorithm uses six kinds of transitions. NO-
PRED is performed when an oracle identifies wj as
not a predicate. All other transitions are performed
when wj is identified as a predicate. SHIFT is per-
formed when both ?1 and ?4 are empty, meaning
that there are no more argument candidates left for
the predicate wj . NO-ARC is performed when wi
is identified as not an argument of wj . LEFT-ARCL
and RIGHT-ARCL are performed when wi is identi-
fied as an argument of wj with a label L. These tran-
sitions can be performed in any order as long as their
preconditions are satisfied. For our experiments, we
use the following generalized sequence:
[ (NO-PRED)? ? (LEFT-ARC?L |NO-ARC
?)? ?
(RIGHT-ARC?L |NO-ARC
?)? ? SHIFT ]?
Notice that this algorithm does not take separate
steps for argument identification and classification.
By adding the NO-ARC transitions, we successfully
merge these two steps together without decrease in
labeling accuracy.1 Since each word can be a predi-
cate candidate and each predicate considers all other
words as argument candidates, a worst-case com-
plexity of the algorithm is O(n2). To reduce the
complexity, Zhao et al (2009) reformulated a prun-
ing algorithm introduced by Xue and Palmer (2004)
for dependency structure by considering only direct
dependents of a predicate and its ancestors as ar-
gument candidates. This pruning algorithm can be
easily applied to our algorithm: the oracle can pre-
filter such dependents and uses the information to
perform NO-ARC transitions without consulting sta-
tistical models.
1We also experimented with the traditional approach of
building separate classifiers for identification and classification,
which did not lead to better performance in our case.
39
Table 2 shows parsing states generated by our al-
gorithm. Our experiments show that this algorithm
gives comparable results against another state-of-
the-art system.
3 Predicate argument clustering
Some studies showed that verb clustering informa-
tion could improve performance in semantic role la-
beling (Gildea and Jurafsky, 2002; Pradhan et al,
2008). This is because semantic role labelers usually
perform worse on verbs not seen during training, for
which the clustering information can provide useful
features. Most previous studies used either bag-of-
words or syntactic structure to cluster verbs; how-
ever, this may or may not capture the nature of predi-
cate argument structure, which is more semantically
oriented. Thus, it is preferable to cluster verbs by
their predicate argument structures to get optimized
features for semantic role labeling.
In this section, we present a self-learning clus-
tering technique that effectively improves labeling
accuracy in the test domain. First, we perform se-
mantic role labeling on the test data using the algo-
rithm in Section 2. Next, we cluster verbs in the test
data using predicate argument structures generated
by our semantic role labeler (Section 3.2). Then, we
cluster verbs in the training data using the verb clus-
ters we found in the test data (Section 3.3). Finally,
we re-run our semantic role labeler on the test data
using the clustering information. Our experiments
show that this technique gives improvement to la-
beling accuracy for both in and out-of domain tasks.
3.1 Projecting predicate argument structure
into vector space
Before clustering, we need to project the predicate
argument structure of each verb into vector space.
Two kinds of features are used to represent these
vectors: semantic role labels and joined tags of
semantic role labels and their corresponding word
lemmas. Figure 2 shows vector representations of
predicate argument structures of verbs, want and
buy, in Figure 1.
Initially, all existing and non-existing features are
assigned with a value of 1 and 0, respectively. How-
ever, assigning equal values to all existing features
is not necessarily fair because some features have
want 1 1 1 1 00s 0s
buy 1 1 1 0 10s 0s
A0 A1 john:A0 to:A1 car:A1... ...Verb
Figure 2: Projecting the predicate argument structure of
each verb into vector space.
higher confidence, or are more important than the
others; e.g., ARG0 and ARG1 are generally predicted
with higher confidence than modifiers, nouns give
more important information than some other gram-
matical categories, etc. Instead, we assign each ex-
isting feature with a value computed by the follow-
ing equations:
s(lj |vi) =
1
1 + exp(?score(lj |vi))
s(mj , lj) =
{
1 (wj 6= noun)
exp( count(mj ,lj)?
?k count(mk,lk)
)
vi is the current verb, lj is the j?th label of vi, and
mj is lj?s corresponding lemma. score(lj |vi) is a
score of lj being a correct argument label of vi; this
is always 1 for training data and is provided by our
statistical models for test data. Thus, s(lj |vi) is an
approximated probability of lj being a correct argu-
ment label of vi, estimated by the logistic function.
s(mj , lj) is equal to 1 if wj is not a noun. If wj is
a noun, it gets a value ? 1 given a maximum likeli-
hood of mj being co-occurred with lj .2
With the vector representation, we can apply any
kind of clustering algorithm (Hofmann and Puzicha,
1998; Kamvar et al, 2002). For our experiments,
we use k-best hierarchical clustering for test data,
and k-means clustering for training data.
3.2 Clustering verbs in test data
Given automatically generated predicate argument
structures in the test data, we apply k-best hierar-
chical clustering; that is, a relaxation of classical hi-
erarchical agglomerative clustering (from now on,
HAC; Ward (1963)), to find verb clusters. Unlike
HAC that merges a pair of clusters at each iteration,
k-best hierarchical clustering merges k-best pairs at
2Assigning different weights for nouns resulted in more
meaningful clusters in our experiments. We will explore addi-
tional grammatical category specific weighting schemes in fu-
ture work.
40
each iteration (Lo et al, 2009). Instead of merging a
fixed number of k-clusters, we use a threshold to dy-
namically determine the top k-clusters. Our studies
indicate that this technique produces almost as fine-
grained clusters as HAC, yet converges much faster.
Our algorithm for k-best hierarchical clustering is
presented in Algorithm 1. thup is a threshold that de-
termines which k-best pairs are to be merged (in our
case, kup = 0.8). sim(ci, cj) is a similarity between
clusters ci and cj . For our experiments, we use co-
sine similarity with average-linkage. It is possible
that other kinds of similarity metrics would work
better, which we will explore as future work. Con-
ditions in line 15 ensure that each cluster is merged
with at most one other cluster at each iteration, and
conditions in line 17 force at least one cluster to
be merged with one other cluster at each iteration.
Thus, the algorithm is guaranteed to terminate after
at most (n? 1) iterations.
When the algorithm terminates, it returns a set of
one cluster with different hierarchical levels. For
our experiments, we set another threshold, thlow, for
early break-out: if there is no cluster pair whose sim-
ilarity is greater than thlow, we terminate the algo-
rithm (in our case, thlow = 0.7). A cluster set gen-
erated by this early break-out contains several unit
clusters that are not merged with any other cluster.
All of these unit clusters are discarded from the set
to improve set quality. This is reasonable because
our goal is not to cluster all verbs but to find a useful
set of verb clusters that can be mapped to verbs in
training data, which can lead to better performance
in semantic role labeling.
3.3 Clustering verbs in training data
Given the verb clusters we found in the test data,
we search for verbs that are similar to these clusters
in the training data. K-means clustering (Hartigan,
1975) is a natural choice for this case because we
already know k-number of center clusters to begin
with. Each verb in the training data is compared with
all verb clusters in the test data, and merged with the
cluster that gives the highest similarity. To maintain
the quality of the clusters, we use the same thresh-
old, thlow, to filter out verbs in the training data that
are not similar enough to any verb cluster in the test
data. By doing so, we keep only verbs that are more
likely to be helpful for semantic role labeling.
input : C = [c1, .., cn]: ci is a unit cluster.
thup ? R: threshold.
output: C? = [c1, .., cm]: cj is a unit or merged
cluster, where m ? n.
begin1
while |C| > 1 do2
L? list()3
for i ? [1, |C| ? 1] do4
for j ? [i+ 1, |C|] do5
t? (i, j, sim(ci, cj))6
L.add(t)7
end8
end9
descendingSortBySimilarity(L)10
S ? set()11
for k ? [1, |L|] do12
t? L.get(k)13
i? t(0); j ? t(1); sim? t(2)14
if i ? S or j ? S then15
continue16
if k = 1 or sim > thup then17
C.add(ci ? cj); S.add(i, j)18
C.remove(ci, cj)19
else20
break21
end22
end23
end24
end25
Algorithm 1: k-best hierarchical clustering.
4 Features
4.1 Baseline features
For a baseline approach, we use features similar to
ones used by Johansson and Nugues (2008). All fea-
tures are assumed to have dependency structures as
input. Table 3 shows n-gram feature templates used
for our experiments (f: form, m: lemma, p: POS tag,
d: dependency label). warg andwpred are the current
argument and predicate candidates. hd(w) stands for
the head of w, lm(w), rm(w) stand for the leftmost,
rightmost dependents of w, and ls(w), rs(w) stand
for the left-nearest, right-nearest siblings of w, with
respect to the dependency structures. Some of these
features can be presented as a joined feature; e.g., a
combination of warg?s POS tag and lemma.
41
Word tokens Features
warg, wpred f,m,p,d
warg?1, hd, lm, rm, ls, rs (warg) m,p
wpred?1, hd, lm, rm (wpred) m,p
Table 3: N -gram feature templates.
Besides the n-gram features, we use several struc-
tural features such as dependency label set, subcat-
egorization, POS path, dependency path, and depen-
dency depth. Dependency label set features are de-
rived by collecting all dependency labels of wpred?s
direct dependents. Unlike Johansson and Nugues,
we decompose subcategorization features into two
parts: one representing the left-hand side and the
other representing the right-hand side dependencies
of wpred. For the predicate wants in Figure 3, we
generate ??SBJ and ????OPRD as separate subcategoriza-
tion features.
wants
PRP:John TO:to
VB:buy
SBJ OPRD
IM
Figure 3: Dependency structure used for subcategoriza-
tion, path, and depth features.
We also decompose path features into two parts:
given the lowest common ancestor (LCA) of warg
and wpred, we generate path features from warg to
the LCA and from the LCA to wpred, separately.
For example, the predicate buy and the argument
John in Figure 3 have a LCA at wants, so we gen-
erate two sets of path features, {?PRP, ?TO?VB}
with POS tags, and {?SBJ, ?OPRD?IM} with depen-
dency labels. Such decompositions allow more gen-
eralization of those features; even if one part is not
matched to the current parsing state, the other part
can still participate as a feature. Throughout our
experiments, these generalized features give slightly
higher labeling accuracy than ungeneralized features
although they form a smaller feature space.
In addition, we apply dependency path features to
wpred?s highest verb chain, which often shares ar-
guments with the predicate (e.g., John is a shared
argument of the predicate buy and its highest verb
chain wants). To retrieve the highest verb chain, we
apply a simple heuristic presented below. The func-
tion getHighestVerbChain takes a predicate,
pred, as input and returns its highest verb chain,
vNode, as output. If there is no verb chain for the
predicate, it returns null instead. Note that this
heuristic is designed to work with dependency rela-
tions and labels described by the CoNLL?09 shared
task (Hajic? et al, 2009).
func getHighestVerbChain(pred)
vNode = pred;
regex = "CONJ|COORD|IM|OPRD|VC";
while (regex.matches(vNode.deprel))
vNode = vNode.head;
if (vNode != pred) return vNode;
else return null;
Dependency depth features are a reduced form of
path features. Instead of specifying POS tags or de-
pendency labels, we indicate paths with their depths.
For instance, John and buy in Figure 3 have a depen-
dency depth feature of ?1?2, which implies that the
depth between John and its LCA (wants) is 1, and
the depth between the LCA and buy is 2.
Finally, we use four kinds of binary features: if
warg is a syntactic head of wpred, if wpred is a syn-
tactic head ofwarg, ifwpred is a syntactic ancestor of
warg, and if wpred?s verb chain has a subject. Each
feature gets a value of 1 if true; otherwise, it gets a
value of 0.
4.2 Dynamic and clustering features
All dynamic features are derived by using previ-
ously identified arguments. Two kinds of dynamic
features are used for our experiments. One is a la-
bel of the very last predicted numbered argument of
wpred. For instance, the parsing state 3 in Table 2
uses a label A0 as a feature to make its prediction,
wants
A1
? to, and the parsing states 4 to 6 use a label
A1 as a feature to make their predictions, NO-ARC?s.
With this feature, the oracle can narrow down the
scope of expected arguments of wpred. The other is
a previously identified argument label of warg. The
existence of this feature implies that warg is already
identified as an argument of some other predicate.
For instance, when warg = John and wpred = buy in
Table 2, a label A0 is used as a feature to make the
prediction, John
A0
? buy, because John is already
identified as an A0 of wants.
42
Finally, we use wpred?s cluster ID as a feature. The
dynamic and clustering features combine a very
small portion of the entire feature set, but still give a
fair improvement to labeling accuracy.
5 Experiments
5.1 Corpora
All models are trained on Wall Street Journal sec-
tions 2-21 and developed on section 24 using auto-
matically generated lemmas and POS tags, as dis-
tributed by the CoNLL?09 shared task (Hajic? et al,
2009). CoNLL?09 data contains semantic roles for
both verb and noun predicates, for which we use
only ones related to verb predicates. Furthermore,
we do not include predicate sense classification as a
part of our task, which is rather a task of word sense
disambiguation than semantic role labeling.
For in-domain and out-of-domain evaluations,
WSJ section 23 and the Brown corpus are used, also
distributed by CoNLL?09. To retrieve automatically
generated dependency trees as input to our semantic
role labeler, we train our open source dependency
parser, called ClearParser3, on the training set and
run the parser on the evaluation sets. ClearParser
uses a transition-based dependency parsing algo-
rithm that gives near state-of-the-art results (Choi
and Palmer, 2011), and mirrors our SRL algorithm.
5.2 Statistical models
We use Liblinear L2-L1 SVM for learning; a linear
classification algorithm using L2 regularization and
L1 loss function. This algorithm is designed to han-
dle large scale data: it assumes the data to be lin-
early separable so does not use any kind of kernel
space (Hsieh et al, 2008). As a result, it significantly
reduces training time compared to typical SVM, yet
performs accurately. For our experiments, we use
the following learning parameters: c = 0.1 (cost),
e = 0.2 (termination criterion), B = 0 (bias).
Since predicate identification is already provided
in the CoNLL?09 data, we do not train NO-PRED.
SHIFT does not need to be trained in general be-
cause the preconditions of SHIFT can be checked
deterministically without consulting statistical mod-
els. NO-ARC? and LEFT-ARC?L are trained to-
gether using the one-vs-all method as are NO-ARC?
3http://code.google.com/p/clearparser/
and RIGHT-ARC?L . Even with multi-classifications,
it takes less than two minutes for the entire training
using Liblinear.
5.3 Accuracy comparisons
Tables 4 and 5 show accuracy comparisons between
three models evaluated on the WSJ and Brown cor-
pora, respectively. ?Baseline? uses the features de-
scribed in Section 4.1. ?+Dynamic? uses all baseline
features and the dynamic features described in Sec-
tion 4.2. ?+Cluster? uses all previous features and the
clustering feature. Even though our baseline system
already has high performance, each model shows an
improvement over its previous model (very slight
for ?+Cluster?). The improvement is greater for the
out-of-domain task, implying that the dynamic and
clustering features help more on new domains. The
differences between ?Baseline? and ?+Dynamic? are
statistically significant for both in and out-of domain
tasks (Wilcoxon signed-rank test, treating each sen-
tence as an individual event, p ? 0.025).
Task P R F1
Baseline
AI 92.57 88.44 90.46
AI+AC 87.20 83.31 85.21
+Dynamic
AI 92.38 88.76 90.54
AI+AC 87.33 83.91 85.59?
+Cluster
AI 92.62 88.90 90.72
AI+AC 87.43 83.92 85.64
JN (2008) AI+AC 88.46 83.55 85.93
Table 4: Labeling accuracies evaluated on the WSJ (P:
precision, R: recall, F1: F1-score, all in %). ?AI? and
?AC? stand for argument identification and argument clas-
sification, respectively.
Task P R F1
Baseline
AI 90.96 81.57 86.01
AI+AC 77.11 69.14 72.91
+Dynamic
AI 90.90 82.25 86.36
AI+AC 77.41 70.05 73.55?
+Cluster
AI 90.87 82.43 86.44
AI+AC 77.47 70.28 73.70
JN (2008) AI+AC 77.67 69.63 73.43
Table 5: Labeling accuracies evaluated on the Brown.
We also compare our results against another state-
of-the-art system. Unfortunately, no other system
43
has been evaluated with our exact environmental set-
tings. However, Johansson and Nugues (2008), who
showed state-of-the-art performance in CoNLL?08,
evaluated their system with settings very similar to
ours. Their task was exactly the same as ours;
given predicate identification, they evaluated their
dependency-based semantic role labeler for argu-
ment identification and classification on the WSJ
and Brown corpora, distributed by the CoNLL?05
shared task (Carreras and Ma`rquez, 2005). Since
the CoNLL?05 data was not dependency-based, they
applied heuristics to build dependency-based predi-
cate argument structures. Their converted data may
appear to be a bit different from the CoNLL?09 data
we use (e.g., hyphenated words are tokenized by the
hyphens in CoNLL?09 data whereas they are not in
CoNLL?05 data), but semantic role annotations on
headwords should look very similar.
Johansson and Nugues?s results are presented as
JN (2008) in Tables 4 and 5. Our final system shows
comparable results against this system. These re-
sults are meaningful in two ways. First, JN used a
graph-based dependency parsing algorithm that gave
higher parsing accuracy for these test sets than the
transition-based dependency parsing algorithm used
in ClearParser (about 0.9% better in labeled attach-
ment score). Even with poorer parse output, our SRL
system performed as well as theirs. Furthermore,
our system used only one set of features, which
makes the feature engineering easier than JN?s ap-
proach that used different sets of features for argu-
ment identification and classification.
6 Conclusion and future work
This paper makes two contributions. First, we in-
troduce a transition-based semantic role labeling al-
gorithm that shows comparable performance against
another state-of-the-art system. The new algorithm
takes advantage of using previous predictions as fea-
tures to make the next predictions. Second, we
suggest a self-learning clustering technique that im-
proves labeling accuracy slightly in both the do-
mains. The clustering technique shows potential for
improving performance in other new domains.
These preliminary results are promising; however,
there is still much room for improvement. Since our
algorithm is transition-based, many existing tech-
niques such as k-best ranking (Zhang and Clark,
2008) or dynamic programming (Huang and Sagae,
2010) designed to improve transition-based parsing
can be applied. We can also apply different kinds of
clustering algorithms to improve the quality of the
verb clusters. Furthermore, more features, such as
named entity tags or dependency labels, can be used
to form a better representation of feature vectors for
the clustering.
One of the strongest motivations for designing our
transition-based SRL system is to develop a joint-
inference system between dependency parsing and
semantic role labeling. Since we have already de-
veloped a dependency parser, ClearParser, based
on a parallel transition-based approach, it will be
straightforward to integrate this SRL system with the
parser. We will also explore the possiblity of adding
empty categories during semantic role labeling.
7 Related work
Nivre (2008) introduced several transition-based de-
pendency parsing algorithms that have been widely
used. Johansson and Nugues (2008) and Zhao
et al (2009) presented dependency-based semantic
role labelers showing state-of-the-art performance
for the CoNLL?08 and ?09 shared tasks in English.
Scheible (2010) clustered predicate argument struc-
tures using EM training and the MDL principle.
Wagner et al (2009) used predicate argument clus-
tering to improve verb sense disambiguation.
Acknowledgments
We gratefully acknowledge the support of the
National Science Foundation Grants CISE-IIS-
RI-0910992, Richer Representations for Machine
Translation, a subcontract from the Mayo Clinic and
Harvard Children?s Hospital based on a grant from
the ONC, 90TR0002/01, Strategic Health Advanced
Research Project Area 4: Natural Language Pro-
cessing, and a grant from the Defense Advanced
Research Projects Agency (DARPA/IPTO) under
the GALE program, DARPA/CMO Contract No.
HR0011-06-C-0022, subcontract from BBN, Inc.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
44
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
conll-2005 shared task: semantic role labeling. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
J. D. Choi and M. Palmer. 2010. Retrieving correct se-
mantic boundaries in dependency structure. In Pro-
ceedings of ACL workshop on Linguistic Annotation.
J. D. Choi and M. Palmer. 2011. Getting the most out
of transition-based dependency parsing. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. S?te?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The conll-2009 shared task: Syntac-
tic and semantic dependencies in multiple languages.
In Proceedings of the 13th Conference on Computa-
tional Natural Language Learning: Shared Task.
J. A. Hartigan. 1975. Clustering Algorithms. New York:
John Wiley & Sons.
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008.
A latent variable model of synchronous parsing for
syntactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natural
Language Learning.
T. Hofmann and J. Puzicha. 1998. Statistical models for
co-occurrence data. Technical report, Massachusetts
Institute of Technology.
C. Hsieh, K. Chang, C. Lin, S. S. Keerthi, and S. Sun-
dararajan. 2008. A dual coordinate descent method
for large-scale linear svm. In Proceedings of the 25th
international conference on Machine learning.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
R. Johansson and P. Nugues. 2008. Dependency-based
semantic role labeling of PropBank. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing.
S. D. Kamvar, D. Klein, and C. D. Manning. 2002. Inter-
preting and extending classical agglomerative cluster-
ing algorithms using a model-based approach. In Pro-
ceedings of the 9th International Conference on Ma-
chine Learning.
D. Liu and D. Gildea. 2010. Semantic role features for
machine translation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics.
C. Lo, J. Luo, and M. Shieh. 2009. Hardware/software
codesign of resource constrained real-time systems. In
Proceedings of the 5th International Conference on In-
formation Assurance and Security.
R. Mcdonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of the Annual Meeting of the European Amer-
ican Chapter of the Association for Computational
Linguistics.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
S. Pradhan, W. Ward, and J. H. Martin. 2008. Towards
robust semantic role labeling. Computational Linguis-
tics: Special Issue on Semantic Role Labeling, 34(2).
C. Scheible. 2010. An evaluation of predicate argument
clustering using pseudo-disambiguation. In Proceed-
ings of the 7th conference on International Language
Resources and Evaluation.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing and on Computational Natural Lan-
guage Learning.
I. Titov, J. Henderson, P. Merlo, and G. Musillo. 2009.
Online graph planarisation for synchronous parsing of
semantic and syntactic dependencies. In Proceedings
of the 21st International Joint Conference on Artificial
Intelligence.
W. Wagner, H. Schmid, and S. Schulte im Walde.
2009. Verb sense disambiguation using a predicate-
argument-clustering model. In Proceedings of the
CogSci Workshop on Distributional Semantics beyond
Concrete Concepts.
J. H. Ward. 1963. Hierarchical grouping to optimize an
objective function. Journal of the American Statistical
Association, 58(301).
N. Xue and M. Palmer. 2004. Calibrating features for se-
mantic role labeling. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Y. Zhang and S. Clark. 2008. A tale of two parsers: in-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
H. Zhao, W. Chen, and C. Kit. 2009. Semantic depen-
dency parsing of NomBank and PropBank: An effi-
cient integrated approach via a large-scale feature se-
lection. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
45
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 72?80,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Incorporating Coercive Constructions into a Verb Lexicon 
Claire Bonial*, Susan Windisch Brown*, Jena D. Hwang*, Christopher Parisien**, 
Martha Palmer* and Suzanne Stevenson** 
*Department of Linguistics, University of Colorado at Boulder 
**Department of Computer Science, University of Toronto 
{Claire.Bonial, Susan.Brown, hwangd, Martha.Palmer}@colorado.edu 
{chris, suzanne}@cs.toronto.edu 
 
 
Abstract 
We take the first steps towards augmenting a lexical 
resource, VerbNet, with probabilistic information 
about coercive constructions. We focus on CAUSED-
MOTION as an example construction occurring with 
verbs for which it is a typical usage or for which it 
must be interpreted as extending the event semantics 
through coercion, which occurs productively and adds 
substantially to the relational semantics of a verb. 
However, through annotation we find that VerbNet 
fails to accurately capture all usages of the 
construction. We use unsupervised methods to 
estimate  probabilistic measures from corpus data for 
predicting usage of the construction across verb 
classes in the lexicon and evaluate against VerbNet. 
We discuss how these methods will form the basis for 
enhancements for VerbNet supporting more accurate 
analysis of the relational semantics of a verb across 
productive usages. 
1 Introduction  
Automatic semantic analysis has been very successful 
when taking a supervised learning approach on data 
labeled with sense tags and semantic roles (e.g., see 
M?rquez et al, 2008). Underlying these recent successes 
are lexical resources, such as PropBank (Palmer et al, 
2005), VerbNet (Kipper et al, 2008), and FrameNet 
(Baker et al, 1998; Fillmore et al, 2002), which encode 
the relational semantics of numerous lexical items, 
especially verbs. However, because authors and speakers 
use verbs productively in previously unseen ways, 
semantic analysis systems must not be limited to direct 
extrapolation from previously seen usages licensed by 
static lexical resources (cf. Pustejovsky & Jezek, 2008). 
To achieve more accurate semantic analyses, we must 
augment such resources with knowledge of the 
extensibility of verbs. 
Central to verb extensibility is the process of semantic 
and syntactic coercion. Coercion allows a verb to be used 
in ?atypical? contexts that extend its relational semantics, 
thereby enabling expression of a novel concept, or simply 
more fluid expression of a complex concept. For 
example, consider a strictly intransitive action verb such 
as blink. This verb may instead be used in a construction 
with an object, as in She blinked the snow off her lashes, 
leading to an interpretation of the verb in which the object 
is causally affected and changes location (the CAUSED-
MOTION construction; Goldberg, 1995). This type of 
constructional coercion is common in language and 
underlies much extensibility of verb usages. 
Understanding such coercive processes thus has 
significant impact on how we should represent 
knowledge about verbs in a lexical resource. 
Importantly, constructional coercion is not an all-or-
nothing process ? a word must be semantically and 
syntactically compatible in some respects with a context 
in order for its use to be extended to that context, but the 
restrictions on compatibility are not hard-and-fast rules 
(Langacker, 1987; Kay & Fillmore, 1999; Goldberg, 
2006; Goldberg, to appear). Gradience of compatibility 
plays an important role in coercion, suggesting that a 
probabilistic approach may be necessary for encoding 
knowledge of constructional coercion in a verb lexicon 
(cf. Lapata & Lascarides, 2003). 
Our hypothesis here is that, due to this gradient process 
of productivity, existing verb lexicons do not adequately 
capture the actual patterns of use of extensible 
constructions. In this paper, we focus on the CAUSED-
MOTION (CM) construction as an initial test case. We first 
annotate the classes of an extensive verb lexicon, 
VerbNet, as to whether the CM construction is allowed 
for all, some, or none of the verbs in the class, noting 
additionally whether it is a typical or coerced usage. We 
find that many of the classes that allow the construction 
for at least some verbs do not include the CM frame in 
their definition, indicating a significant shortcoming in the 
relational knowledge encoded in the lexicon. Next, we 
72
develop probabilistic measures for determining to what 
degree a class is likely to admit the CM construction. We 
then test our measures over corpus data, manually 
annotated for use of the CM construction. Finally, we 
present preliminary work on automatic techniques for 
calculating the proposed measures in an unsupervised 
way, to avoid the need for expensive manual annotation. 
This work forms the preliminary steps toward empirically 
augmenting VerbNet?s predictive capabilities concerning 
the event semantics of verbs in coercible constructions. 
2 Extensible Constructions and VerbNet 
Construction grammar has much insight to offer on the 
topic of productivity and on the resulting statistical 
patterns and gradience of usages (e.g., Langacker, 1987; 
Kay & Fillmore, 1999; Goldberg, 2006). A construction 
is formally defined to be any pairing of linguistic form 
(e.g., a syntactic frame) and meaning. Words can be used 
in constructions to the extent that their lexical semantics is 
compatible with ? or can be coerced to be compatible 
with ? the semantic constraints on the construction. 
It is this notion of constructional coercion, and degree 
of coercibility, that accounts for the richness of usages 
that go beyond those thought of as typical or definitional 
for a verb: by coercing a verb not normally associated 
with a particular frame to occur in it, the meaning of the 
event can take on additional properties not considered a 
core part of the verb?s semantics. For example, in the case 
of the sentence discussed above, She blinked the snow off 
her lashes, it is not the verb but rather the CM 
construction itself that licenses the direct object and adds 
the notion of ?motion causally affecting the object? to the 
event semantics. Amongst other examples of well-known 
constructional coercions are: (1) The CAUSE-RECEIVE 
construction has the syntactic form of NP-V-NP-NP. For 
example, in Bob painted Sally a picture, the simple 
transitive verb paint gains the CAUSE TO RECEIVE sense, 
in which Sally is the recipient and the picture is the 
transferred item. (2) The WAY construction has the form 
of NP-V-[POSS way]-PP. For example, in Frank found 
his way to New York, the construction allows the verb 
find to gain a motion reading (i.e., ?Frank traveled to New 
York?) that would not otherwise be allowed (e.g., *Frank 
found to New York).  
Recognizing such extensions to the relational 
semantics of verbs is very important for accurate 
semantic interpretation in NLP. However, precise 
specifications for capturing the notion of coercible 
constructions, such as are needed for a computational 
resource, have heretofore been lacking. 
2.1 VerbNet & Knowledge of Constructions 
Computational verb lexicons are key to supporting NLP 
systems aimed at semantic interpretation. Verbs express 
the semantics of an event being described as well as the 
relational information among participants in that event, 
and project the syntactic structures that encode that 
information. Verbs are also highly variable, displaying a 
rich range of semantic and syntactic behavior. 
Verb classifications help NLP systems to deal with 
this complexity by organizing verbs into groups that 
share core semantic and syntactic properties. For 
example, VerbNet (derived from Levin?s [1993] work, 
Kipper et al, 2008) is widely used for a number of 
semantic processing tasks, including semantic role 
labeling (Swier and Stevenson, 2004), the creation of 
semantic parse trees (Shi and Mihalcea, 2005), and 
implicit argument resolution (Gerber and Chai, 2010). 
The detailed semantic predicates listed with each 
VerbNet class also have the potential to contribute to text-
specific semantic representations and, thereby, to tasks 
requiring inferencing (Zaenen et al, 2008; Palmer et al, 
2009). 
VerbNet identifies semantic roles and syntactic 
patterns characteristic of the verbs in each class makes 
explicit the connections between the syntactic patterns 
and the underlying semantic relations that can be inferred 
for all members of the class. Each syntactic frame in a 
class has a corresponding semantic representation that 
details the semantic relations between event participants 
across the course of the event. For example, one of the 
characteristic patterns listed for the Pour class is a 
CAUSED-MOTION pattern, which accounts for sentences 
like She poured water from the pitcher into the bowl. This 
is represented in VerbNet as follows: 
Syntactic representation: 
NP V NP PP PP 
Agent V Theme Source Location 
Semantic representation: 
MOTION (DURING(E), THEME)  
NOT (PREP (START(E), THEME, LOCATION)) 
PREP (START(E), THEME, SOURCE) 
PREP (END(E), THEME, LOCATION) 
CAUSE (AGENT, E) 
This representation details connections between the 
syntax and semantics using the semantic roles as links, 
indicating that the Agent is the Subject NP and has 
CAUSED the Event, and that the Theme is the Object NP 
and has a new LOCATION at the end of the event. These 
types of inferences provide the foundation for deep 
semantic analysis of text.  
73
However, the specifications in VerbNet (as in other 
predicate lexicons, such as FrameNet, Baker et al, 1998; 
Fillmore et al, 2002) are seen as definitional ? they are 
restricted to the core usages of the verbs that are valid for 
all verbs in the class. However, as noted above, people 
often use verbs productively, in ways that go beyond the 
boundaries of the verb class structure. It is important to 
correctly identify these productive usages when they 
occur, since they may be explicitly adding crucial 
inferences. If a construction is not recognized in the form 
of a syntactic frame in VerbNet, such inferences are not 
possible, greatly reducing VerbNet?s utility and coverage. 
For example, creative uses of a verb, such as She blinked 
the snow off her lashes, would have no corresponding 
frame in blink?s class, the Hiccup class.  It contains one 
intransitive frame: 
 NP V 
Agent V 
  
 
BODY_PROCESS (E, AGENT) 
INVOLUNTARY (E, AGENT) 
 
Sentences that coerce the meaning of blink to fit with a 
CM event would currently be misanalysed. One option 
might be to augment the Hiccup class with the CM frame 
from the Pour class, which would ensure that such 
sentences would be analyzed more accurately. However, 
given the productive nature of constructional coercion 
and its widespread applicability, the approach of adding 
any possible pattern to each class is not appropriate: this 
would undermine the definitional distinctions between 
classes and greatly lessen their usefulness.  
Complicating the issue is the phenomenon of regular 
sense extensions (Dang et al, 1998), where what once 
may have been coercion has become entrenched and is 
now seen as a different sense of the verb. For example, 
the verbs in the Push class express the general meaning of 
exerting force on an object, such as She pushed on the 
wall. Often, the exertion of force moves the object, which 
can be expressed in a CM construction such as She 
pushed the box across the room. VerbNet accounts for 
this regular sense extension by including most of the Push 
verbs in the Carry class as well, which has the CM 
construction as one of its frames. Deciding when to 
include a verb in another class based on regular sense 
extensions, when to add a frame for a construction to a 
class, or when to reject the frame as a defining part of a 
class, is made difficult by the graded nature of matches 
between verbs and a construction. Our goal is to maintain 
the advantages of the class structure of VerbNet while 
enhancing it with a graded view of the applicability of a 
construction for each class. Noting the applicability of a 
construction will enable the inclusion of its appropriate 
semantic predicates, and the inferencing over them, 
which are currently not supported. 
3 Our Proposal: Constructional Profiles 
We aim to augment VerbNet with knowledge of 
constructions that are likely to be used extensibly with a 
range of verbs. Such extensible constructions will be core 
usages for some classes (such as the CM for the Pour 
class, as noted above) but will be less characteristic of the 
fundamental semantics of other verb classes (such as CM 
for the Hiccup class). We propose to identify such a 
construction and its varying roles in the different classes 
by using relevant statistics over usages of verbs in a 
corpus ? what we call a constructional profile. 
A constructional profile is a probabilistic assessment 
of the usage of a particular construction by the verbs in a 
class. We developed the following three measures to 
capture the relevant behavior, with the goal of providing 
both type- and token-based views of the behavior of a 
verb class with respect to a target construction: 
P1 Ptype(X|C): probability that a verb type in class C is 
attested in construction X 
P1 gives a type-based assessment, indicating how 
widespread the use of the construction is across the 
verb types in the class. For example, if 8 out of 10 
members of a class appear with the construction, we 
might estimate P1 as 0.8. 
P2 Ptoken(X|C): probability that the instances of a typical 
verb in class C occur in construction X 
P2 gives a token-based assessment, indicating, for a 
typical verb in the class, the relative amount of usage of 
the construction among all usages of the verb. For 
example, to estimate this, we might average across all 
verbs in the class, the percentage of tokens in this 
construction. 
P3: Ptoken(X|X-verbs-in-C): same as P2 but considering 
only verbs that have been attested in construction X 
P3 is the same as P2, but looking only at those verbs in 
the class that have an attested usage of the construction, 
removing verbs without attested usages. 
We hypothesize that these measures will have high 
values for those classes for which the construction should 
be definitional; very low values for those classes that are 
not compatible with the construction; and varying values 
for those classes that allow coerced usages to a greater or 
lesser extent. 
Although these probabilities are intuitively very 
simple, estimating them from corpus data poses a 
significant challenge. Since a construction is a pairing of 
form with meaning, recognizing the use of a particular 
74
construction is not simply a matter of determining the 
syntactic pattern of the usage; rather, certain semantic 
properties and relations must co-occur with the syntactic 
pattern. Earlier work has shown that a supervised learning 
method was able to discriminate potential usages of the 
CM construction given training sentences manually 
labeled as either CM or not (Hwang et al, 2010). Here, 
we aim instead to identify usages of the CM construction, 
but without requiring an expensive manual annotation 
effort. That is, we seek an unsupervised method for 
estimating the probabilities in P1?P3 above. 
We approach this goal in steps as follows. First, we 
examine all the classes in VerbNet to see which allow the 
CM construction (Section 4). This anno-tation reveals 
shortcomings in VerbNet?s representa-tion (classes that 
allow the CM construction but do not list it) and also 
provides a gold standard with which to evaluate our 
method of identifying an exten-sible construction using 
our constructional profiles. Second, we use the manually 
annotated CM construction data from Hwang et al 
(2010) to estimate probabilities P1?P3 using maximum 
likelihood formulations (Section 5). An analysis of the 
predictive power of these constructional profile measures 
shows a good match with the distinctions made in the 
human annotation of the classes. Thus, our annotation 
based constructional profile measures show promise for 
identifying relevant behaviors of the construction across 
the classes. Third, we explore automatic methods for 
estimating the constructional profile measures without the 
need for manual annotations (Section 6). We use a 
hierarchical Bayesian model that learns verb classes from 
corpus data to provide unsupervised estimates of the 
constructional profiles, which also exhibit the relevant 
distinctions across the classes. 
4 Annotating the VerbNet Resource  
We begin with a manual examination of the resource and 
a thorough annotation of the status of each class with 
respect to the CM construction. This effort reveals a 
number of shortcomings in VerbNet, and the need for 
developing methods that can support the extension of 
VerbNet to better reflect the coercive uses of 
constructions across the classes. The annotation described 
here also forms the basis for the evaluation in the 
following sections of our new probabilistic measures, by 
motivating hypotheses about the expected patterns of use 
of the CM construction across the classes. 
4.1 Annotation Guidelines and Results 
The first goal of our manual annotation of VerbNet 
classes was to determine which classes currently 
represent CM in one of their frames. To this end, we 
identified which classes contain the following frame:  
NP [Agent/Cause]-V-NP [Patient/Theme]- 
PP [Source/Destination/Recipient/Location]  
These frames correspond to classes such as Slide, with its 
frame NP-V-NP-PP.Destination: Carla slid the books to 
the floor. We also examined classes with the patterns NP-
V-NP-PP.Oblique, NP-V-NP-PP. Theme2, and NP-V-
NP-PP.Patient2. In these classes, annotators had to judge 
whether the final PP was compatible with CM. For 
example, the Breathe class contains the frame NP-V-
NP.Theme-PP.Oblique, The dragon breathed fire on 
Mary, which is compatible with CM; whereas the same 
basic frame in the Other_cos class is not: NP V NP 
PP.Oblique, The summer sun tanned her skin to a golden 
bronze. 
In addition, we annotated which classes were 
potentially compatible with CM for either all verbs in the 
class or only some verbs. The "some" classification has 
the drawback that it may be applied to classes with very 
different proportions of compatible verbs; while suitable 
for our exploratory work here, we plan to make finer 
distinctions in the future. A secondary determination was 
whether or not the class was compatible with CM as part 
of its core semantics, or if it was compatible with CM 
because it was coercible into the construction. A verb was 
considered ?compatible with CM? and ?not coerced? if 
the verb could be used in the CM construction and its 
semantics, as reflected in VerbNet?s semantic predicates, 
involved a CAUSE predicate in combination with another 
predicate such as CONTACT, TRANSFER, (EN)FORCE, 
EMIT, TAKE_IN (predicates potentially involving 
movement along some path). For example, although CM 
is not already included as a frame for the Bend class 
containing the verb fold, the semantics of this class 
include CAUSE and CONTACT, and the verb can be used 
in a CM construction: She folded the note into her 
journal. Therefore, this class would have been considered 
?compatible with CM? but ?not coerced?. Conversely, a 
verb was considered ?compatible with CM? and 
?coerced? if the verb could be used in the CM 
construction, yet its semantics, again as reflected in 
VerbNet, did not involve CAUSE and MOVEMENT 
ALONG A PATH (e.g., the verb wiggle of the 
Body_internal_motion class: She wiggled her foot out of 
the boot). 
In summary, as presented in the table below, we 
annotated each class according to whether (1) the CM 
construction was already represented in VerbNet for this 
class, (2) the construction was possible for all, some, or 
75
none of the verbs in that class, and (3) the verbs of any 
class compatible with CM were coerced into the 
construction or not. The classification for (3) was made 
regardless of whether ?all? verbs or only ?some? were 
compatible with CM. This determination was made 
uniformly for a class: there were no classes in which only 
certain CM-compatible verbs were considered ?coerced?.  
VN class example  
[# of classes like this] 
CM in 
VN 
CM is 
possible 
CM is 
coerced 
Banish [50] Yes All No 
Nonverbal_Expression [2] Yes All Yes 
Cheat [6] Yes Some No 
Exhale [18] No All No 
Hiccup [30] No All Yes 
Fill [46] No Some No 
Wish [54] No Some Yes 
Matter [64] No None N/A 
Notably, we identified 206 classes where at least some of 
the verbs in that class are compatible with the CM 
construction; however, VerbNet currently only 
recognizes the CM construction in 58 classes. There were 
several classes of interest: First, although it may seem 
unusual that CM is represented in 6 classes where we 
found that only ?some? verbs were compatible with CM 
(e.g., Cheat class), these were cases where only more 
restricted subclasses are compatible with CM, and this 
syntactic frame is listed for that subclass. This suggests 
subclasses may provide a more precise characterization 
of which verbs are compatible with a construction.  
Secondly, we identified 18 classes in which all verbs 
were compatible with CM without coercion; thus, these 
classes could likely be improved by the addition of the 
CM syntactic frame. Additionally, we found 30 classes in 
which all verbs are coercible into the CM construction; 
however, the actual likelihood of a verb in those classes 
occurring in a CM construction remains to be 
investigated in the following sections. Like those classes 
where it was determined that only ?some? verbs are 
compatible with CM, usefully incorporating the CM 
construction into classes that require coercion relies on 
accurately determining the probability that verbs in those 
classes will actually appear in the CM construction.  
For those classes in which ?all? verbs are compatible 
with CM, our intuition was that some aspect of the verb?s 
semantics either inherently includes or allows the verb to 
be coerced into the CM construction. Conversely, for 
those classes in which no verbs are compatible with CM, 
presumably some aspect of the verb?s semantics is 
logically incompatible with CM. Although pinpointing 
precisely what aspect of a verb?s semantics makes it 
compatible with CM may not be possible, we can 
investigate whether or not our intuitions are supported by 
examining the actual frequencies of CM constructions for 
given verbs or a given class.  
4.2 Hypotheses  
Using these annotations, we were able to develop two 
simple hypotheses. 
Hypothesis 1: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for those classes in which all verbs were found to 
be compatible with CM; lower for classes in which only 
some verbs were found to be compatible; and lowest for 
classes in which no verbs were found to be compatible. 
Hypothesis 2: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for verbs that fall into classes where CM is not 
considered coerced (for either some or all of the verbs in 
the class); lower for verbs that fall into classes in which 
the CM construction only works through coercion (for 
either some or all of the verbs in the class); and lowest for 
verbs that fall into classes in which no verbs are 
compatible with CM.  
To investigate Hypothesis 1, we grouped the annotated 
classes according to whether all, some, or no verbs in the 
class are compatible with CM: 
 Class example # of classes 
Allowed by All Bring, Carry 106 
Allowed by Some Appoint, Lodge 100 
Allowed by None Try, Own 64 
To investigate Hypothesis 2, we did a second grouping 
of the classes according to whether CM is not coerced, 
CM is coerced, or CM is simply not compatible with the 
class. This second grouping did not distinguish whether 
CM was compatible with ?all? or ?some? of the verbs in 
a given class. 
 Class example # of classes 
Not Coerced Put, Throw 120 
Coerced Floss, Wink 86 
Not Compatible Differ 64 
5 Evaluation using Constructional Profiles 
5.1 Annotated data description 
Our research uses the data annotated for Hwang et al 
(2010), in which 1800 instances in the form NP-V-NP-
PP were identified in the Wall Street Journal portion of 
the Penn Treebank II (Marcus et al, 1994). Each instance 
76
of the data was single annotated with one of the two 
labels: CM or non-CM. The annotation guidelines were 
based on the CM analysis of Goldberg (1995). 
Our analysis began with the same data but adopted a 
slightly narrower definition of CM. We diverged from 
the Hwang et al (2010) study in the following two ways: 
(1) sentences where the object NP is an item that is 
created by the event denoted by the verb were not 
considered CM (e.g., Mr. Pilson scribbled a frighteningly 
large figure on a slip of paper, where the figure is created 
through the scribbling event); and (2) sentences in which 
movement is prevented were not considered CM (e.g., 
He kept her at arm?s length). In agreement with Hwang 
et al, our annotation included both metaphorical senses 
(e.g., [It] cast a shadow over world oil markets) and 
literal senses (e.g., The company moved the employees to 
New York) of CM. Our annotation using the narrower 
guidelines resulted in 85.8% agreement with the original 
annotation.1  The distribution of labels in our data is 
21.8% for CM and 78.2% for NON-CM. 
5.2 Annotated data description 
Using statistics over the manually annotated data, we 
calculate maximum likelihood estimates of the three 
constructional profile measures introduced in Section 3, 
as follows. First, let the probability that a verb v is used in 
the CM construction be estimated as: 
P(CM|v,C) = 
#(CM usages of     ) 
#(CM+non-CM usages of    ) 
That is, P(CM|v,C) is estimated as the relative frequency 
of the CM construction for v out of all annotated usages 
of v that are labeled as class C. Now let CCM be all verbs v 
in C with at least one usage annotated as CM; i.e.: 
    *      |  (  |   )    + 
Then we calculate estimates of P1?P3 as: 
P1: Ptype(CM|C) = |CCM |/|C| 
This measure indicates how widespread the use of CM is 
across the verb types in the class. 
P2: Ptoken(CM|C) =,?  (  |   )   - | |?  
The average over all verbs v in C of P(CM|v,C) 
This indicates the relative amount of usage of CM among 
all usages of the verbs in the class.  
P3: Ptoken(CM|v,C) = [?  (  |   ))- |   |       
The average over all verbs v in CCM of P(CM|v,C) 
P3 narrows the P2 measure to only those verbs in the 
                                                          
1We found that 34.0% of the disagreements were directly due to 
the changes in annotation resulting from our two new criteria. 
class for which there is an attested usage of CM. 
5.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
measures P1-P3 for the groups of VerbNet classes as 
defined in section 4.2. For each group listed, we report 
the averages of P1-P3 over all classes in the group where 
at least one verb in the class occurred in the data 
manually annotated for CM usage. 
 P1 P2 P3 
CM Allowed by All 0.413 0.323 0.437 
CM Allowed by Some 0.087 0.078 0.224 
CM Not Allowed 0.055 0.055 0.083 
As seen here, the constructional profile measures over 
CM in the data corroborate our Hypothesis 1 (Section 
4.2). All three measures on average are highest for the 
classes that fall into the ?all allowed? group, next highest 
for those in the ?some allowed? group, and lowest for the 
?not allowed? classes.  
 P1 P2 P3 
CM Non-Coerced 0.354 0.274 0.418 
CM Coerced 0.091 0.091 0.185 
CM Not Allowed2 0.056 0.056 0.083 
Furthermore, the second table here confirms our 
expectations for Hypothesis 2 (Section 4.2). Again, all 
three measures on average are highest for classes that fall 
into the ?non-coerced? group, next highest for classes in 
the ?coerced? group (in which the construction is 
achievable only through coercion), and lowest for the 
?not allowed? group.  
Thus, our two hypotheses are borne out, showing that 
our constructional profile measures, when estimated over 
manually annotated data, can be useful in capturing 
important distinctions among classes of verbs with regard 
to their usage in an extensible construction such as CM. 
6 Automatic Creation of Constructional 
Profiles Using a Bayesian Model  
Manually annotating a corpus for usages of a con-
struction can be prohibitively expensive, so we also 
investigate the use of automatic methods to estimate 
constructional profile measures. By using a hierarchi-cal 
Bayesian model (HBM) that acquires latent prob-abilistic 
verb classes from corpus data, we provide unsupervised 
                                                          
2 Note the non-zero values result from actual CM verb usages in 
the data belonging to classes believed to be not compatible with 
CM by VerbNet expert annotators. 
77
estimates of the constructional profiles. 
6.1 Overview of Model and Data 
We use the HBM of Parisien & Stevenson (2011), a 
model that automatically acquires probabilistic 
knowledge about verb argument structure and verb 
classes from large-scale corpora. The model is based on a 
large body of research in nonparametric Bayesian topic 
modeling (e.g., Teh et al, 2004), a robust method of 
discovering syntactic and semantic structure in very large 
datasets. For each verb encountered in a corpus, the 
model provides an estimate of the verb?s expected overall 
pattern of usage. By using latent probabilistic verb classes 
to influence these expected usage patterns, the model can, 
for example, estimate the probability that a verb like blink 
might occur in a CM construction, even if no such 
attested usages appear in the corpus. 
In this preliminary study, we use the corpus data from 
Parisien & Stevenson (2011), since the model has been 
trained and evaluated on this data. As that study was 
aimed at modeling facts of child language acquisition, it 
uses child-directed speech from the Thomas corpus 
(Lieven et al, 2009), part of the CHILDES database 
(MacWhinney, 2000). In this preliminary study, we use 
their development dataset containing approx. 170,000 
verb usages, covering approx. 1,400 verb types. (We 
reserve the test set for future experiments.) For each verb 
usage in the input, a number of features are automatically 
extracted that indicate the number and type of syntactic 
arguments occurring with the verb and general semantic 
properties of the verb. The semantic features are drawn 
from the set of VerbNet semantic predicates, such as 
CAUSE, MOTION, and CONTACT. These are automatically 
extracted from all classes compatible with the verb (with 
no sense disambiguation). 
6.2 Measures for Constructional Profiles 
Using the argument structure constructions, verb usage 
patterns and classes learned by the model, we estimate 
the three constructional profile measures in Section 3, as 
follows. First, we note that since the constructions 
acquired by the model are probabilistic in nature, a 
particular CM instance may be a partial match to more 
than one of the model?s constructions.  
For each verb in the input, we consider the likelihood 
of use of the CM construction to be the likelihood of a 
contrived frame intended to capture the important 
properties of a CM usage. FCM is a usage taking a direct 
object and a prepositional phrase, and including the 
semantic features CAUSE and MOTION, with all other 
semantic features left unspecified. For a given verb v, we 
estimate the likelihood of this CM usage, over all 
constructions in the model, as follows: 
 (   | )  ? (   | ) (
 
 | ) 
Here, P(FCM |k) is the likelihood of the CM usage FCM 
being an instance of the probabilistic construction k, and 
P(k|v) is the likelihood that verb v occurs with 
construction k. These component probabilities are 
estimated using the probability distributions acquired by 
the model and averaged over 100 samples from the 
Markov Chain Monte Carlo simulation, as described in 
Parisien & Stevenson (2011). 
Now, we let CCM be the set of verbs in VerbNet class 
C where the expected likelihood of a CM usage is non-
negligible (akin to the set of verbs with attested usage in 
Section 5.2): 
CCM = {v C | P(FCM|v)>? } 
where ? is a small threshold, here 0.0001. Note that since 
v is not disambiguated for class in our data, all usages of v 
contribute to this estimate. 
The estimates of P1-P3 are comparable to those in 
Section 5.2. The difference is that since we are un-able to 
disambiguate individual usages of the verbs, each usage 
of v is considered to belong to all possible classes C of 
which v is a member. P1 is estimated as before; P2 and 
P3 are averages of P(FCM|v). 
6.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
estimates P1-P3 for the groups of VerbNet classes as 
given in Section 4.2. For each group listed, we report the 
averages of P1-P3 over all classes in the group where at 
least one of the verbs in the class occurred in the training 
input to the model. 
 P1 P2 P3 
All allowed 0.569 0.0180 0.0250 
Some allowed 0.449 0.0106 0.0192 
Not allowed 0.363 0.0044 0.0079 
These profile measures align with the hypotheses in 
Section 4.2 and with the measures based on manually 
annotated data in Section 5.2. The estimates are high-est 
for classes where all verbs permit the CM con-struction, 
second highest for classes where only some permit it, and 
lowest for classes that do not permit it. 
 P1 P2 P3 
CM non-coerced 0.546 0.0178 0.0260 
CM coerced 0.458 0.0095 0.0167 
CM not allowed 0.363 0.0044 0.0079 
78
Again, the overall patterns of the profile measures align 
with Sections 4.2 and 5.2. The profile estimates are 
highest for classes annotated to be non-coerced usages of 
CM, second highest for coerced classes, and lowest for 
?not allowed?.  
The measures show the overall differences among 
classes in the different groups (for both groupings) ? i.e., 
the average behavior among classes in the different 
groups varies as we predicted.  This indicates that the 
measures are tapping into aspects of construction usage 
that are relevant to making the desired distinctions in 
VerbNet, and validates the use of automatic 
techniques.  However, there is a substantial amount of 
variability in these measures across the classes, so we also 
consider how well the estimates can predict the 
appropriate group for individual classes. That is, can we 
automatically predict whether the CM construction can 
be used by all, some, or none of the verbs in a given verb 
class, and can we predict whether such usages are 
coerced? 
We consider the P3 measure as it provides the best 
separation among the class groupings. The tables below 
report precision (P), recall (R) and F-measures (F) for 
each group, where ?all? and ?some? have been collapsed. 
For exploratory purposes, we pick P3 = 0.006 as the 
value that optimizes F-measures of this classification. 
Future work will explore more principled means for 
setting these thresholds. 
 P R F 
CM allowed 0.880 0.742 0.806 
CM not allowed 0.407 0.636 0.497 
Only a 2-way distinction can be made reliably for the 
allowed grouping. The F-score of over 80% for the 
?allowed? label is very promising. The low precision for 
the ?not allowed? case suggests that the model can?t 
generalize sufficiently due to sparse data. 
 P R F 
CM non-coerced 0.691 0.491 0.574 
CM coerced 0.461 0.417 0.438 
CM not allowed 0.406 0.709 0.517 
We use thresholds of P3 = 0.021 to separate non-coerced 
from coerced classes, and P3 = 0.007 to separate coerced 
from not allowed classes. The model estimates show 
moderate success in distinguishing classes with coerced 
vs. non-coerced usage of the CM construction. However, 
our measures simply cannot distinguish non-occurrence 
due to semantic incompatibility from non-occurrence due 
to chance, given the expected low frequency of a novel 
coerced use of a construction.  To separate the allowed 
cases into whether they are coerced or not requires a 
more detailed assessment of the semantic compatibility of 
the class, which means looking at finer-grained features 
of verb usages that are indicative of the semantic 
predicates compatible with the particular construction.  
Moreover, this kind of assessment likely needs to be 
applied on a verb-specific (and not just class-specific) 
level, in order to identify those verbs out of a potentially 
coercible class that are indeed coercible (i.e., identifying 
the coercible verbs in a class labeled as "some allowed"). 
7 Conclusion 
Our investigation demonstrates that VerbNet does not 
currently represent the CM construction for all verbs or 
verb classes that are compatible with this construction, 
and the existing static representation of verbs is 
inadequate for analyzing extensions of verb meaning 
brought about by coercion. The utility of VerbNet would 
be greatly enhanced by an improved representation of 
constructions: specifically, the incorporation of 
probabilities that verbs in a given (sub)class would occur 
in a particular construction, and whether this constitutes a 
regular sense extension. This addition to VerbNet would 
increase the resource?s coverage of syntactic frames that 
are compatible with a given verb, and therefore enable 
appropriate inferences when coercion occurs. We have 
made preliminary steps towards developing this 
probabilistic distribution over both verb instances and 
classes, based on a large corpus. Unsupervised methods 
for estimating the probabilities achieve an F-score of over 
80% in distinguishing the classes that allow the target 
construction. However, making distinctions among 
coerced and non-coerced cases will require us to go 
beyond these class-based probabilities to finer-grained, 
corpus-based assessments of a verb?s semantic 
compatibility with a coercible construction.  
To move beyond these preliminary findings, we must 
therefore shift our focus to the behavior of individual 
verbs. Additionally, to reduce the impact of errors 
resulting from low-frequency verbs and classes, we plan 
to expand our research to more data, specifically the 
OntoNotes TreeBank data (Weischedel et al, 2011). 
Finally, to achieve our ultimate goal of creating a lexicon 
that can flexibly account for a variety of constructions, we 
will examine other constructions as well. While 
determining the set of coercible constructions in a 
language is itself a topic of current research, we propose 
initially to include the widely recognized CAUSE-
RECEIVE and WAY constructions in addition to CM. 
79
References  
Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. 
The Berkeley FrameNet Project. Proceedings of the 17th 
International Conference on Computational Linguistics 
(COLING/ACL-98), pp. 86?90, Montreal. 
Dang, HoaTrang, Karin Kipper, Martha Palmer, and Joseph 
Rosenzweig. 1998. Investigating regular sense extensions 
based on intersective Levin classes. Proceedings of 
COLING-ACL98, pp. 293?299. 
Fillmore, Charles J., Christopher R. Johnson, and Miriam R.L. 
Petruck. 2002. Background to FrameNet. International 
Journal of Lexicography, 16(3):235-250.  
Gerber, Matthew, and Joyce Y. Chai. 2010. Beyond 
NomBank: A study of implicit arguments for nominal 
predicates. Proceedings of the 48th Annual Meeting of the 
Association of Computational Linguistics, pp. 1583?1592, 
Uppsala, Sweden, July. 
Goldberg, A. E. 1995. Constructions: A construction 
grammar approach to argument structure. Chicago: 
University of Chicago Press. 
Goldberg, A. E. 2006. Constructions at work: The nature of 
generalization in language. Oxford: Oxford University 
Press. 
Goldberg, A. E. To appear. Corpus evidence of the viability of 
statistical preemption. Cognitive Linguistics. 
Hwang Jena D., Rodney D. Nielsen and Martha Palmer. 2010. 
Towards a domain-independent semantics: Enhancing 
semantic representation with construction grammar. 
Proceedings of Extracting and Using Constructions in 
Computational Linguistic Workshop, held with NAACL 
HLT 2010, Los Angeles, June. 
Kay, P., and C. J. Fillmore. 1999. Grammatical constructions 
and linguistic generalizations: The What's X Doing Y? 
construction. Language, 75:1?33. 
Kipper, Karin, Anna Korhonen, Neville Ryant, and Martha 
Palmer. 2008. A large-scale classification of English verbs. 
Language Resources and Evaluation Journal, 42:21?40. 
Langacker, R. W. 1987. Foundations of cognitive grammar: 
Theoretical prerequisites. Stanford, CA: Stanford 
University Press. 
Lapata, M., and A. Lascarides. 2003. Detecting novel 
compounds: The role of distributional evidence. 
Proceedings of the 11th Conference of the European 
Chapter of the Association for Computational 
Linguistics(EACL03), pp.235?242. Budapest, Hungary. 
Levin, B. 1993.English Verb Classes and Alternations: A 
Preliminary Investigation. Chicago: Chicago University 
Press.  
 
 
Lieven, E., D. Salomo, and M. Tomasello. 2009. Two-year-
old children?s production of multiword utterances: A 
usage-based analysis. Cognitive Linguistics 20(3):481?507. 
MacWhinney, B. 2000.The CHILDES Project: Tools for 
analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum. 
M?rquez, L., X. Carreras, K. Litkowski, and S. Stevenson. 
2008. Semantic role labeling: An introduction to the special 
issue. Computational Linguistics, 34(2): 145?159. 
Martha Palmer, Jena D. Hwang, Susan Windisch Brown, 
Karin Kipper Schuler and Arrick Lanfranchi. 2009. 
Leveraging lexical resources for the detection of event 
relations. Proceedings of the AAAI 2009 Spring 
Symposium on Learning by Reading, Stanford, CA, March. 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005. 
The Proposition Bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1):71?106. 
Parisien, Christopher, and Suzanne Stevenson. 2011. To 
appear in Proceedings of the 33rd Annual Meeting of the 
Cognitive Science Society, Boston, MA, July. 
Pustejovsky, J., and E. Jezek. 2008. Semantic coercion in 
language: Beyond distributional analysis. Italian Journal of 
Linguistics/RivistaItaliana di Linguistica 20(1): 181?214. 
Shi, Lei, and Rada Mihalcea. 2005. Putting pieces together: 
Combining FrameNet, VerbNet and WordNet for robust 
semantic parsing. Proceedings of the 6th International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Mexico. 
Swier, R., and S. Stevenson. 2004. Unsupervised semantic 
role labeling. Proceedings of the 2004 Conf. on Empirical 
Methods in Natural Language Processing, pp. 95?102, 
Barcelona, Spain. 
Teh, Y. W., M. I. Jordan, M. J.Beal, and D. M.Blei.2006. 
Hierarchical Dirichlet processes. Jrnl of the American 
Statistical Asscn, 101(476): 1566?1581. 
Weischedel, R., E. Hovy, M. Marcus, M. Palmer, .R. Belvin, 
S. Pradan, L. Ramshaw and N. Xue. 2011.OntoNotes: A 
Large Training Corpus for Enhanced Processing. In Part 1: 
Data Acquisition and Linguistic Resources of The 
Handbook of Natural Language Processing and Machine 
Translation: Global Automatic Language Exploitation, 
Eds.: Joseph Olive, Caitlin Christianson, John McCary. 
Springer Verlag, pp. 54-63. 
Zaenen, A., C. Condoravdi, and D. G. Bobrow. 2008. The 
encoding of lexical implications in VerbNet. Proceedings 
of LREC 2008, Morocco, May. 
80
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 21?30,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Semantic Mapping Using Automatic Word Alignment and Semantic Role
Labeling
Shumin Wu
Department of Computer Science
University of Colorado at Boulder
shumin.wu@colorado.edu
Martha Palmer
Department of Linguistics
Univerisity of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
To facilitate the application of semantics in
statistical machine translation, we propose
a broad-coverage predicate-argument struc-
ture mapping technique using automated re-
sources. Our approach utilizes automatic
syntactic and semantic parsers to gener-
ate Chinese-English predicate-argument struc-
tures. The system produced a many-to-many
argument mapping for all PropBank argu-
ment types by computing argument similarity
based on automatic word alignment, achieving
80.5% F-score on numbered argument map-
ping and 64.6% F-score on all arguments. By
measuring predicate-argument structure sim-
ilarity based on the argument mapping, and
formulating the predicate-argument structure
mapping problem as a linear-assignment prob-
lem, the system achieved 84.9% F-score us-
ing automatic SRL, only 3.7% F-score lower
than using gold standard SRL. The map-
ping output covered 49.6% of the annotated
Chinese predicates (which contains predicate-
adjectives that often have no parallel annota-
tions in English) and 80.7% of annotated En-
glish predicates, suggesting its potential as a
valuable resource for improving word align-
ment and reranking MT output.
1 Introduction
As the demand for semantically consistent machine
translation rises (Wu and Fung, 2009a), the need
for a comprehensive semantic mapping tool has be-
come more apparent. With the current architecture
of machine translation decoders, few ways of in-
corporating semantics in MT output include using
word sense disambiguation to select the correct tar-
get translation (Carpuat and Wu, 2007) and reorder-
ing/reranking MT output based on semantic con-
sistencies (Wu and Fung, 2009b) (Carpuat et al,
2010). While a comprehensive semantic mapping
tool can supplement or improve the results of such
techniques, there are many other exciting ideas we
can explore: with automatic SRL, we can improve
coverage (and possibly accuracy) of Chinese seman-
tic class generation (Wu et al, 2010) by running the
system on a large, unannotated parallel corpus. Us-
ing predicate-argument mappings as constraints, it
may be possibly to improve SRL output by perform-
ing joint inference of SRL in source and target lan-
guages simultaneously, much like what Burkett and
Klein (2008) was able to achieve with syntactic pars-
ing.
As the foundation of many machine translation
decoders (DeNeefe and Knight, 2009), word align-
ment has continuously played an important role in
machine translation. There have been several at-
tempts to improve word alignment, most of which
have focused on tree-to-tree alignments of syntac-
tic structures (Zhang et al, 2007; Marec?ek, 2009a).
Our hypothesis is that the predicate-argument struc-
ture alignments can abstract away from language
specific syntactic variation and provide a more ro-
bust, semantically coherent alignment across sen-
tences.
We begin by running GIZA++ (Och and Ney,
2003), one of the most popular alignment tools, to
obtain automatic word alignments between parallel
English/Chinese corpora. To achieve a broader cov-
erage of semantic mappings than just those anno-
21
tated in parallel PropBank-ed corpora, we attempt
to map automatically generated predicate-argument
structures. For each Chinese and English verb pred-
icate pairs within a parallel sentence, we exam-
ine the quality of both the predicate and argument
alignment (using GIZA++ word alignment output)
and devise a many-to-many argument mapping tech-
nique. From that, we pose predicate-argument map-
ping as a linear assignment problem (optimizing the
total similarity of the mapping) and solve it with
the Kuhn-Munkres method (Kuhn, 1955). With
this approach, we were able to incur only a small
predicate-argument F-score degradation over using
manual PropBank annotation. The output also pro-
vides much more fine-grained argument mapping
that can be used for downstream MT applications.
2 Related work
Our basic approach to semantic mapping is similar
to the idea of semantic similarity based on triangu-
lation between parallel corpora outlined in Resnik
(2004) and Madnani et al (2008a; 2008b), but is
implemented here quite differently. It is most sim-
ilar in execution to the work of (Marec?ek, 2009b),
which improves word alignment by aligning tec-
togrammatical trees in a parallel English/Czech cor-
pus. The Czech corpus is first lemmatized because
of the rich morphology, and then the word alignment
is ?symmetrized?. However, this approach does not
explicitly make use of the predicate-argument struc-
ture to confirm the alignments or to suggest new
ones.
Pado? and Lapata (2005; 2006) used word align-
ment and syntax based argument similarity to
project English FrameNet semantic roles to German.
The approach relied on annotated semantic roles on
the source side only, precluding joint inferenece of
the projection using reference or automatic target
side semantic roles.
Fung et al (2007) demonstrated that there is
poor semantic parallelism between Chinese-English
bilingual sentences. Their technique for im-
proving Chinese-English predicate-argument map-
ping (ARGChinese,i 7? ARGEnglish,j) consists of
matching predicates with a bilingual lexicon, com-
puting cosine-similarity (based on lexical transla-
tion) of arguments and tuning on an unannotated
parallel corpus. The system differs from ours in
that it only provided one-to-one mapping of num-
bered arguments and may not be able to detect
predicate mapping with no lexical relations that are
nevertheless semantically related. Later, Wu and
Fung (2009b) used parallel semantic roles to im-
prove MT system outputs. Given the outputs from
Moses (Koehn et al, 2007), a machine translation
decoder, they reordered the outputs based on the best
predicate-argument mapping. The resulting system
showed a 0.5 point BLEU score improvement even
though the BLEU metric often discounts improve-
ment in semantic consistency of MT output.
Choi et al (2009) (and later Wu et al (2010))
showed how to enhance Chinese-English verb align-
ments by exploring predicate-argument structure
alignment using parallel PropBanks. The result-
ing system showed improvement over pure GIZA++
alignment. Those two systems differs from ours
in that they operated on gold standard parses and
semantic roles. The systems also did not pro-
vide explicit argument mapping between the aligned
predicate-argument structures.
3 Resources
To perform automatic semantic mapping, we need
an annotated corpus to evaluate the results. In addi-
tion, we also need a word aligner, a syntactic parser,
and a semantic role labeler (as well as annotated and
unannotated corpora to train each system).
3.1 Corpus
We used the portion of the Penn Chinese TreeBank
with word alignment annotation as the basis for eval-
uating semantic mapping. The word-aligned por-
tion, containing around 2000 parallel sentences, is
exclusive to Xinhua News (and covers around 50%
of the Xinhua corpus in the Chinese TreeBank). We
then merged the word alignment annotation with the
TreeBank and PropBank annotation of Ontonotes
4.0 (Hovy et al, 2006), which includes a wide ar-
ray of data sources like broadcast news, news wire,
magazine, web text, etc. A small percentage of the
2000 sentences were discarded because of tokeniza-
tion differences. We dubbed the resulting 1939 par-
allel sentences as the triple-gold Xinhua corpus.
22
3.2 Word Alignment
We chose GIZA++ (Och and Ney, 2003) as our word
alignment tool primarily because of its popularity,
though there are other alternatives like Lacoste-
Julien et al (2006).
3.3 Phrase Structure Parsing
We chose the Berkeley Parser (Petrov and Klein,
2007) for phrase structure parsing since it has been
tested on both English and Chinese corpora and can
be easily retrained.
3.4 Semantic Role Labeling
For semantic role labeling (SRL), we built our own
system using a fairly standard approach: SRL is
posed as a multi-class classification problem requir-
ing the identification of argument candidates for
each predicate and their argument types. Typi-
cally, argument identification and argument label-
ing are performed in two separate stages because of
time/resource constraints during training/labeling.
For our system, we chose LIBLINEAR (Fan et al,
2008), a library for large linear classification prob-
lems, as the classifier. This alleviated the need to
separate the identification and labeling stages: argu-
ment identification is trained simply by incorporat-
ing the ?NOT-ARG? label into the training data.
Most the of the features used by the classifier are
standard features found in many SRL systems; these
include:
Predicate predicate lemma and its POS tag
Voice indicates the voice of the predicate. For En-
glish, we used the six heuristics detailed by
Igo (2007), which detects both ordinary and
reduced passive constructions. For Chinese,
we simply detected the presence of passive in-
dicator words (those with SB, LB POS tags)
amongst the siblings of the predicate.
Phrase type phrase type of the constituent
Subcategorization phrase structure rule expanding
the predicate parent
Head word the head word and its POS tag of the
constituent
Parent head word whether the head word of the
parent is the same as the head word of the con-
stituent
Position whether the constituent is before or after
the predicate
Path the syntactic tree path from the predicate to
the constituent (as well as various path general-
ization methods)
First word first word and its POS tag of the con-
stituent
Last word last word and its POS tag of the con-
stituent
Syntactic frame the siblings of the constituent
Constituent distance the number of potential con-
stituents with the same phrase type between the
predicate and the constituent
We also created many bigrams (and a few trigrams)
of the above features.
By default, LIBLINEAR uses the one-vs-all ap-
proach for multi-class classification. This does not
always perform well for some easily confusable
class labels. Also, as noted by Xue (2004), cer-
tain features are strong discriminators for argument
identification but not for argument labeling, while
the reverse is true for others. Under such condi-
tions, mixing arguments and non-arguments within
the same class may produce sub-optimal results for a
binary classifier. To address these issues, we built a
pairwise multi-class classifier (using simple major-
ity voting) on top of LIBLINEAR.
The resulting English SRL system, evaluated
using the CoNLL 2005 methodology, achieved a
77.3% F-score on the WSJ corpus, comparable to
the leading system (Surdeanu and Turmo, 2005) us-
ing a single parser output. The Chinese SRL system,
on the other hand, achieved 74.4% F-score on the
triple-gold Xinhua corpus (similar but not directly
comparable to Wu et al (2006) and Xue (2008)
because of differences in TreeBank/PropBank revi-
sions as well as differences in test set).
4 Predicate-arguments mapping
4.1 Argument mapping
To produce a good predicate-argument mapping, we
needed to consider 2 things: whether good argument
mapping can be produced based on argument type
only, and whether each argument only maps to one
argument in the target language.
23
4.1.1 Predicate-dependent argument mapping
Theoretically, PropBank numbered arguments are
supposed to be consistent across predicates: ARG0
typically denotes the agent of the predicate and
ARG1 the theme. While this consistency may hold
true for predicates in the same language, as Fung et
al. (2007) noted, this is not a reliable indicator when
mapping predicate-arguments between Chinese and
English. For example, when comparing the Prop-
Bank frames of the English verb arrive and the syn-
onymous Chinese verb??, we see ARG1 (entity in
motion) for arrive.01 is equivalent to ARG0 (agent)
of ??.01 while ARG4 (end point, destination) is
equivalent to ARG1 (destiny).
4.1.2 Many-to-many argument mapping
Just as there are shortcomings in assuming pred-
icate independent argument mappings, assuming
one-to-one argument mapping may also be overly
restrictive. For example, in the following Chinese
sentence:
? ?? ?? ???? ??? ??
big passage construction invigorated big southwest?s material flow
the predicate??(invigorate) has 2 arguments:
? ARG0: ? ?? ?? (big passage construc-
tion)
? ARG1: ? ????? (big southwest?s ma-
terial flow)
In the parallel English sentence:
Construction of the main passage has activated the
flow of materials in the great southwest
activate has 3 arguments:
? ARG0: construction of the main passage
? ARG1: the flow of materials
? ARGM-LOC: in the great southwest
In these parallel sentences, ARG1 of?? should be
mapped to both ARG1 and ARGM-LOC of activate.
While the English translation of??, invigorate,
is not a direct synonym of activate, they at least have
some distant relationship as indicated by sharing
the inherited hypernym make in the WordNet (Fell-
baum, 1998) database. The same cannot be said for
all predicate-pairs. For example, in the following
parallel sentence fragments:
?? ?? ? ?
on the street people flow like the tide
the Chinese predicate-argument structure for
?(like) is:
? ARG0: ?? (flow of guests)
? ARG1: ? (tide)
? ARGM-LOC:?? (on the street)
while the English predicate-argument structure for
flow is:
? ARG1: people
? ARGM-LOC: on the street
? ARGM-MNR: like the tide
Semantically, the predicate-argument pairs are
equivalent. The argument mapping, however, is
more complex:
? ?.ARG0?? flow.ARG1, flow.V
? ?.V,?.ARG1?? flow.ARGM-MNR
? ?.ARGM-LOC?? flow.ARGM-LOC
Table 1 details the argument mapping for the
triple-gold Xinhua data. The mapping distribution
for ARG0 and ARG1 is relatively deterministic (and
similar to ones found by Fung et al (2007)). Map-
pings involving ARG2-5 and modifier arguments,
on the other hand, are much more varied. Typically,
when there is a many-to-many argument mapping,
it?s constrained to a one-to-two or two-to-one map-
ping. Much more rarely is there a case of a two-to-
two or even more complex mapping.
4.2 Word alignment based argument mapping
To achieve optimal mappings between parallel
predicate-argument structure, we would like to max-
imize the number of words in the mapped argument
set (over the entire set of arguments) while minimiz-
ing the number of unaligned words in the mapped
argument set.
Let ac,i and ac,j denote arguments in Chinese and
English respectively, AI as a set of arguments, Wc,i
as words in argument ac,i, and mape(ai) = We,i
as the word alignment function that takes the source
argument and produces a set of words in the target
24
arg type A0 A1 A2 A3 A4 ADV BNF DIR DIS EXT LOC MNR PRP TMP TPC V
A0 1610 79 25 0 0 28 1 0 0 0 8 5 1 11 1 9
A1 432 2665 128 11 0 83 9 12 0 0 29 12 5 21 3 142
A2 43 310 140 8 3 55 6 9 0 2 20 10 1 4 1 67
A3 2 14 21 7 0 2 4 2 0 0 1 2 1 0 1 4
A4 1 37 9 3 6 0 0 0 0 0 1 0 1 0 0 4
ADV 33 36 9 6 0 307 2 5 6 0 44 121 6 11 2 19
CAU 1 0 0 0 0 1 0 0 0 0 0 0 16 0 0 1
DIR 1 13 3 2 0 1 0 3 0 0 3 0 0 0 0 20
DIS 2 0 0 0 0 69 0 0 40 0 2 1 3 3 0 0
EXT 0 4 0 0 0 26 0 0 0 0 0 0 0 0 0 2
LOC 23 65 13 1 0 3 1 0 0 0 162 0 0 5 0 4
MNR 9 9 5 0 0 260 0 0 0 1 3 34 0 0 0 25
MOD 1 0 0 0 0 159 0 0 0 0 0 0 0 0 0 84
NEG 0 0 0 0 0 24 0 0 0 0 0 0 0 0 0 5
PNC 3 23 11 4 0 1 6 1 0 0 1 2 35 2 0 8
PRD 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 1
TMP 14 21 2 0 0 235 0 3 0 1 8 16 0 647 0 6
V 25 28 22 1 0 211 1 0 1 0 2 12 0 0 0 3278
Table 1: Chinese argument type (column) to English argument type (row) mapping on triple-gold Xinhua corpus
language sentence. We define precision as the frac-
tion of aligned target words in the mapped argument
set:
Pc,I =
|(?i?Imape(ac,i)) ? (?j?JWe,j)|
|?i?Imape(ac,i)|
(1)
and recall as the fraction of source words in the
mapped argument set:
Rc,I =
?
i?I |Wc,i|?
?i |Wc,i|
(2)
We then choose Ac,I that optimizes the F1-score of
Pc and Rc:
Ac,I = argmax
I
2 ? Pc,I ?Rc,I
Pc,I +Rc,I
= Fc,I (3)
Finally, to constrain both source and target argument
set, we optimize:
Ac,I , Ae,J = argmax
I,J
2 ? Fc,I ? Fe,J
Fc,I + Fe,J
= FIJ (4)
To measure similarity between a single pair of
source, target arguments, we define:
Pij =
|mape(ac,i) ?Wj |
|mape(ac,i)|
, Rij =
|mapc(ae,j) ?Wi|
|mapc(ae,j)|
(5)
To generate the set of argument mapping pairs, we
simply choose all pairs of ac,i, ae,j ? Ac,I , Ae,J
where Fij ?  ( > 0).
Directly optimizing equation 4 requires exhaus-
tive search of all argument set combinations between
the source and target, which is NP-complete. While
the typical number of arguments for each predicate
is relatively small, this is nevertheless inefficient.
We performed the following greedy-based approx-
imation with quadratic complexity:
1. Compute the best (based on F-score of equa-
tion 5) pair of source-target argument mappings
for each source argument (target argument may
be reused)
2. Select the remaining argument pair with the
highest F-score
3. Insert the pair in Ac,I , Ae,J if it increases FIJ ,
else discard
4. repeat until all argument pairs are exhausted
5. repeat 1-4 reversing the source and target direc-
tion
6. merge the output of the 2 directions
Much like GIZA++ word alignment where the out-
put of each direction produces only one-to-many
mappings, merging the output of the two directions
produces many-to-many mappings.
25
4.3 One-to-one predicate-argument mapping
To find the best predicate-argument mapping be-
tween Chinese and English parallel sentences, we
assume each predicate in a Chinese or English sen-
tence can only map to one predicate in the target
sentence. As noted by Wu et al (2010), this as-
sumption is mostly valid for the Xinhua news cor-
pus, though occasionally, a predicate from one sen-
tence may align more naturally to two predicates in
the target sentence. This typically occurs with verb
conjunctions. For example the Chinese phrase ??
? ??? (sightseeing and tour) is often translated
to the single English verb ?travel?. As noted by Xue
and Palmer (2009), the Chinese PropBank annotates
predicative adjectives, which tend not to have an
equivalent in the English PropBank. Additionally,
some verbs in one language are nominalized in the
other. This results in a good portion of Chinese or
English predicates in parallel sentences not having
an equivalent in the other language.
With the one-to-one mapping constraint, we op-
timize the mapping by maximizing the sum of the
F1-scores (as defined by equation 4) of the predi-
cates and arguments in the mapping. Let PC and PE
denote the sets of predicates in Chinese and English
respectively, with G(PC , PE) = {g : PC 7? PE} as
the set of possible mappings between the two predi-
cate sets, then the optimal mapping is:
g? = argmax
g?G
?
i,j?g
FCi,Ej (6)
To turn this into a classic linear assignment problem,
we define Cost(PCi , PEj ) = 1 ? FCi,Ej , and (6)
becomes:
g? = argmin
g?G
?
i,j?g
Cost(PC,i, PE,j) (7)
(7) can be solved in polynomial time with the Kuhn-
Munkres algorithm (Kuhn (1955)).
5 Experimental setup
5.1 Reference predicate-argument mapping
To generate reference predicate-argument map-
pings, we ran the mapping system described in sec-
tion 4.2 with a cutoff threshold of FCi,Ej < 0.65
(i.e., alignments with F-score below 0.65 are dis-
carded). We reviewed a small random sample of the
output and found it to have both high precision and
recall, with only occasional discrepancies caused by
possible word alignment errors. If one-to-one argu-
ment mapping is imposed, the reference predicate-
argument mapping will lose 8.2% of the alignments.
For mappings using automatic word alignment, we
chose a cutoff threshold of FCi,Ej < 0.15. This can
easily be tuned for higher precision or recall based
on application needs.
5.2 Parser, SRL, GIZA++
We trained the Berkeley parser and our SRL sys-
tem on Ontonotes 4.0, excluding the triple-gold Xin-
hua sections as well as the non-English or Chinese
sourced portion of the corpus. GIZA++ was trained
on 400K parallel Chinese-English sentences from
various sources with the default parameters. For
the word mapping functions mape(ac), mapc(ae)
in equation 5, instead of taking the word align-
ment intersection of the source-target and target-
source directions as Pado? and Lapata (2006), we
used the two alignment outputs seperately (using the
Chinese-English output when projecting Chinese ar-
gument to English words, and vice versa). On av-
erage (from the 400K corpus), an English sentence
contains 28.5% more tokens than the parallel Chi-
nese sentence (even greater at 36.2% for the Xinhua
portion). Taking either the intersection or union will
significantly affect recall or precision of the align-
ment.
6 Results
6.1 Semantic role labeling
We first provide some results of the SRL system on
the triple-gold Xinhua corpus in table 2. Unlike the
conventional wisdom which expects English SRL
to outperform Chinese SRL, when running on the
Chinese-sourced Xinhua parallel corpus, our SRL
actually performed better on Chinese than English
(74.4% vs 71.8% F-score). The Berkeley parser
output also seemed to be of higher quality on Chi-
nese; the system was able to pick out better con-
stituent candidates in Chinese than English, as ev-
idenced by the higher recall for oracle SRL (92.6%
vs 91.1%). Comparing the quality of the output by
argument type, we found the only argument type
where the Chinese SRL system performed signifi-
26
language type P R F1
Chinese
CoNLL 77.9% 71.1% 74.4%
oracle 100% 92.6% 96.1%
word match 84.8% 74.6% 79.4%
English
CoNLL 75.6% 68.4% 71.8%
oracle 100% 91.1% 95.2%
word match 82.7% 69.4% 75.5%
Table 2: SRL results on triple-gold Xinhua corpus. ?arg
match? is the standard CoNLL 2005 evaluation metric,
?oracle? is the oracle SRL based on automatic parser out-
put, and ?word match? is scoring based on length of ar-
gument overlap with the reference
cantly worse is ARG0 (almost 10% F-score lower).
This is likely caused by dropped pronouns in Chi-
nese sentences (Yang and Xue, 2010), making it
harder for both the syntactic and semantic parsers
to identify the correct subject.
We also report the SRL result scored at word level
instead of at argument level (79.4% F-score for Chi-
nese and 75.5% for English). The CoNLL 2005
shared task scoring (Surdeanu and Turmo, 2005)
discounts arguments that are not a perfect word span
match, even if the system output is semantically
close to the reference argument. While this is im-
portant in some applications of SRL, for other ap-
plications like improving word alignment with SRL,
improving recall on approximate arguments may be
a better trade-off than having high precision on per-
fectly matched arguments. We noticed that while
overall improvement in SRL improves both word
level and argument level performance, for other-
wisely identical systems, we can slightly favor word
level performance (up to 1-3% F-score) by includ-
ing positive training samples that are not a perfect
argument match.
6.2 Predicate-argument mapping
Table 3 details the results of Chinese-English
predicate-argument mapping. Using automatic SRL
and word alignment, the system achieved an 84.9%
F-score, only 3.7% F-score less than using gold stan-
dard SRL annotation. When looking at only ar-
guments, however, the differences are larger: au-
tomatic SRL based output produced an 80.5% F-
score for core arguments. While this compares fa-
vorably to Fung et al (2007)?s 72.5% (albeit with
Evaluation gold P R F1
predicate- yes 88.7% 88.5% 88.6%
argument no 84.6% 85.3% 84.9%
A0-5 label
yes 97.8% 96.2% 97.0%
no 87.0% 74.9% 80.5%
A0-5 span no 67.9% 57.9% 62.5%
all arg label
yes 84.0% 79.3% 81.6%
no 70.3% 59.8% 64.6%
all arg span no 61.6% 52.2% 56.5%
Table 3: Predicate-argument mapping results
different sections of the corpus), it?s 16.5% F-score
lower than gold SRL based output. When including
all arguments, automatic SRL based output achieved
64.6% while the gold SRL based output achieved
81.6%. This indicates that the mapping result for
all arguments is limited by errors in word alignment.
We also report the results of automatic SRL on both
producing the correct argument mappings and word
spans (62.5% for core arguments and 56.5% for all
arguments). This may be relevant for applications
such as joint inference between word alignment and
SRL.
We also experimented with discriminative
(reweighing) word alignment based on part-of-
speech tags of the words to improve the mapping
system but were not able to achieve better results.
This may be due to the top few POS types account-
ing for most of the words in a language, therefore it
did not prove to be a strong discriminator.
6.3 Mapping coverage
Table 4 provides predicate and word coverage de-
tails of the predicate-argument mapping, another
potentially relevant statistic for applications of
predicate-argument mapping. High coverage of
predicates and words in the mappings may provide
more relevant constraints to help reorder MT output
or rerank word alignment. We expect labeling En-
glish nominalized predicate-arguments will help in-
crease both predicate and word coverage in the map-
ping output.
In order to build a comprehensive probability
model of Chinese-English predicate-argument map-
ping, we applied the mapping technique on an unan-
notated 400K parallel sentence corpus. Automatic
27
output type language coverage
triple-gold
predicate Chinese 50.0%
predicate English 81.3%
word Chinese 66.0%
word English 64.2%
automatic
predicate Chinese 49.6%
predicate English 80.7%
word Chinese 57.4%
word English 55.4%
Table 4: Predicate-argument mapping coverage. Predi-
cate coverage denotes the number of mapped predicates
over all predicates in the corpus, word coverage denotes
the number of words in the mapped predicate-arguments
over all words in the corpus
language
PropBank appeared appeared
verb framesets in corpus in mapping
Chinese 16122 8591 7109
English 5473 3689 3121
Table 5: Frameset coverage on the 400K parallel sentence
corpus
SRL found 1.6 million Chinese predicate instances
and 1.3 million English predicate instances. The
mapping system found around 700K predicate-pairs
(with FC,E < 0.3). Table 5 shows the number of
unique verbs in the corpus and contained in the map-
ping results within the Chinese and English Prop-
Bank verb framesets. The corpus also included some
verbs that do not appear in PropBank framesets.
7 Conclusion and future work
We proposed a broad-coverage predicate-argument
mapping system using automatically generated word
alignment and semantic role labeling. We also
provided a competitive Chinese and English SRL
system using a LIBLINEAR classifier and pair-
wise multi-class classification approach. By explor-
ing predicate-argument structure, the mapping sys-
tem is able to generate mappings between seman-
tically similar predicate-argument structures con-
taining non-synonymous predicates, achieving an
84.9% F-score, only 3.7% lower than the F-score
of gold-standard SRL based mappings. Utilizing
word alignment information, the system was able
to provide detailed many-to-many argument map-
pings (occurs in 8.2% of the reference mappings)
for core arguments and modifier arguments, achiev-
ing an 80.5% F-score for core arguments and 64.6%
F-score for all arguments.
While our experiment with discriminative word
alignment based on POS tags did not show improve-
ment, there are other word grouping/weighing met-
rics like n-gram based clustering, verb classification,
term frequency, that may be more appropriate for se-
mantic mapping. With the advent of a predicate-
argument annotation resource for nominalization,
Ontonotes 5, we plan to update our SRL system
to produce nominalized predicate-arguments. This
would potentially increase the predicate-argument
mapping coverage in the corpus as well as increasing
the accuracy of mapping (by reducing the number of
unmappable predicate-arguments), making the map-
ping more useful for downstream applications.
We are also experimenting with a probabilis-
tic approach to predicate-argument mapping to im-
prove the robustness of mapping against word align-
ment errors. Using the output of the current sys-
tem on a large corpus, we can establish mod-
els for p(prede|predc), p(arge|predc, prede, argc)
and refine them through iterations of expectation-
maximization. If this approach shows promise, the
next step would be to explore integrating the map-
ping model directly into GIZA++ for joint inference
of word alignment and predicate-argument mapping.
Other statistical translation specific applications we
would like to explore include extensions of MT out-
put reordering (Wu and Fung, 2009b) and rerank-
ing using predicate-argument mapping, as well as
predicate-argument projection onto the target lan-
guage as an evaluation metric for MT output.
Acknowledgement
We gratefully acknowledge the support of the
National Science Foundation Grants CISE- CRI-
0551615, and a grant from the Defense Advanced
Research Projects Agency (DARPA/IPTO) under
the GALE program, DARPA/CMO Contract No.
HR0011-06-C-0022, subcontract from BBN, Inc.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
28
References
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?08, pages 877?886,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In The 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 61?72.
Marine Carpuat, Yuval Marton, and Nizar Habash. 2010.
Improving arabic-to-english statistical machine trans-
lation by reordering post-verbal subjects for align-
ment. In Proceedings of the ACL 2010 Conference
Short Papers, ACLShort ?10, pages 178?183.
Jinho D. Choi, Martha Palmer, and Nianwen Xue. 2009.
Using parallel propbanks to enhance word-alignments.
In Proceedings of ACL-IJCNLP workshop on Linguis-
tic Annotation (LAW?09), pages 121?124.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP?09), volume 2, pages
727?736.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large linear
classification. Journal of Machine Learning Research,
9:1871?1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai
Wu. 2007. Learning bilingual semantic frames: Shal-
low semantic parsing vs. semantic role projection. In
11th Conference on Theoretical and Methodological
Issues in Machine Translation, pages 75?84.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90 In Proceedings of HLT-NAACL 2006, pages
57?60.
Sean Paul Igo. 2007. Identifying reduced passive voice
constructions in shallow parsing environments. Mas-
ter?s thesis, University of Utah.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the Annual Meeting of the Association for Com-
putational Linguistics (ACL?07), demonstration ses-
sion, pages 177?180.
Harold W. Kuhn. 1955. The hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2:83?97.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the main
conference on Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion of Computational Linguistics, HLT-NAACL ?06,
pages 112?119, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Nitin Madnani, Philip Resnik, Bonnie Dorr, and Richard
Schwartz. 2008a. Applying automatically generated
semantic knowledge: A case study in machine trans-
lation. In NSF Symposium on Semantic Knowledge
Discovery, Organization and Use.
Nitin Madnani, Philip Resnik, Bonnie Dorr, and Richard
Schwartz. 2008b. Are multiple reference translations
necessary? investigating the value of paraphrased ref-
erence translations in parameter optimization. In Pro-
ceedings of the 8th Conference of the Association for
Machine Translation in the Americas (AMTA?08).
David Marec?ek. 2009a. Improving word alignment us-
ing alignment of deep structures. In Proceedings of
the 12th International Conference on Text, Speech and
Dialogue, pages 56?63.
David Marec?ek. 2009b. Using tectogrammatical align-
ment in phrase-based machine translation. In Proceed-
ings of WDS 2009 Contributed Papers, pages 22?27.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sebastian Pado? and Mirella Lapata. 2005. Cross-
linguistic projection of role-semantic information. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 859?866, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sebastian Pado? and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for semantic
projection. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 1161?1168, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In In HLT-NAACL ?07.
Philip Resnik. 2004. Exploiting hidden meanings: Using
bilingual text for monolingual annotation. In Alexan-
der Gelbukh, editor, Lecture Notes in Computer Sci-
ence 2945: Computational Linguistics and Intelligent
Text Processing, pages 283?299. Springer.
29
Mihai Surdeanu and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In Pro-
ceedings of CoNLL-2005 shared task, pages 221?224.
Dekai Wu and Pascale Fung. 2009a. Can semantic
role labeling improve smt? In Proceedings of the
13th Annual Conference of the EAMT, pages 218?225,
Barcelona, Spain.
Dekai Wu and Pascale Fung. 2009b. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL-HLT?09), pages 13?16.
Zhaojun Wu, Yongsheng Yang, and Pascale Fung.
2006. C-assert: Chinese shallow semantic parser.
http://hlt030.cse.ust.hk/research/
c-assert/.
Shumin Wu, Jinho D. Choi, and Martha Palmer. 2010.
Detecting cross-lingual semantic similarity using par-
allel propbanks. In Proceedings of the 9th Confer-
ence of the Association for Machine Translation in the
Americas.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the chinese treebank. Nat. Lang. Eng.,
15(1):143?172.
Nianwen Xue. 2004. Calibrating features for semantic
role labeling. In Proceedings of EMNLP 2004, pages
88?94.
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225?255.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the chinese treebank.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 1382?1390, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In Ma-
chine Translation Summit XI.
30
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1?27,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
CoNLL-2011 Shared Task:
Modeling Unrestricted Coreference in OntoNotes
Sameer Pradhan
BBN Technologies,
Cambridge, MA 02138
pradhan@bbn.com
Lance Ramshaw
BBN Technologies,
Cambridge, MA 02138
lramshaw@bbn.com
Mitchell Marcus
University of Pennsylvania,
Philadelphia, 19104
mitch@linc.cis.upenn.edu
Martha Palmer
University of Colorado,
Boulder, CO 80309
martha.palmer@colorado.edu
Ralph Weischedel
BBN Technologies,
Cambridge, MA 02138
weischedel@bbn.com
Nianwen Xue
Brandeis University,
Waltham, MA 02453
xuen@cs.brandeis.edu
Abstract
The CoNLL-2011 shared task involved pre-
dicting coreference using OntoNotes data. Re-
sources in this field have tended to be lim-
ited to noun phrase coreference, often on a
restricted set of entities, such as ACE enti-
ties. OntoNotes provides a large-scale corpus
of general anaphoric coreference not restricted
to noun phrases or to a specified set of en-
tity types. OntoNotes also provides additional
layers of integrated annotation, capturing ad-
ditional shallow semantic structure. This pa-
per briefly describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task
including the format, pre-processing informa-
tion, and evaluation criteria, and presents and
discusses the results achieved by the partic-
ipating systems. Having a standard test set
and evaluation parameters, all based on a new
resource that provides multiple integrated an-
notation layers (parses, semantic roles, word
senses, named entities and coreference) that
could support joint models, should help to en-
ergize ongoing research in the task of entity
and event coreference.
1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Automatic identification of coreferring entities and
events in text has been an uphill battle for several
decades, partly because it can require world knowl-
edge which is not well-defined and partly owing to
the lack of substantial annotated data. Early work
on corpus-based coreference resolution dates back
to the mid-90s by McCarthy and Lenhert (1995)
where they experimented with using decision trees
and hand-written rules. A systematic study was
then conducted using decision trees by Soon et al
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have been developed
to push the state of the art in coreference resolu-
tion forward (Morton, 2000; Harabagiu et al, 2001;
McCallum and Wellner, 2004; Culotta et al, 2007;
Denis and Baldridge, 2007; Rahman and Ng, 2009;
Haghighi and Klein, 2010). Various different knowl-
edge sources from shallow semantics to encyclo-
pedic knowledge are being exploited (Ponzetto and
Strube, 2005; Ponzetto and Strube, 2006; Versley,
2007; Ng, 2007). Researchers continued finding
novel ways of exploiting ontologies such as Word-
Net. Given that WordNet is a static ontology and
as such has limitation on coverage, more recently,
there have been successful attempts to utilize in-
formation from much larger, collaboratively built
resources such as Wikipedia (Ponzetto and Strube,
2006). In spite of all the progress, current techniques
still rely primarily on surface level features such as
string match, proximity, and edit distance; syntac-
tic features such as apposition; and shallow seman-
tic features such as number, gender, named entities,
semantic class, Hobbs? distance, etc. A better idea
of the progress in the field can be obtained by read-
ing recent survey articles (Ng, 2010) and tutorials
(Ponzetto and Poesio, 2009) dedicated to this sub-
ject.
Corpora to support supervised learning of this
task date back to the Message Understanding Con-
ferences (MUC). These corpora were tagged with
coreferring entities identified by noun phrases in the
text. The de facto standard datasets for current coref-
erence studies are the MUC (Hirschman and Chin-
1
chor, 1997; Chinchor, 2001; Chinchor and Sund-
heim, 2003) and the ACE1 (G. Doddington et al,
2000) corpora. The MUC corpora cover all noun
phrases in text, but represent small training and test
sets. The ACE corpora, on the other hand, have much
more annotation, but are restricted to a small subset
of entities. They are also less consistent, in terms of
inter-annotator agreement (ITA) (Hirschman et al,
1998). This lessens the reliability of statistical ev-
idence in the form of lexical coverage and seman-
tic relatedness that could be derived from the data
and used by a classifier to generate better predic-
tive models. The importance of a well-defined tag-
ging scheme and consistent ITA has been well rec-
ognized and studied in the past (Poesio, 2004; Poe-
sio and Artstein, 2005; Passonneau, 2004). There
is a growing consensus that in order for these to be
most useful for language understanding applications
such as question answering or distillation ? both of
which seek to take information access technology
to the next level ? we need more consistent anno-
tation of larger amounts of broad coverage data for
training better automatic techniques for entity and
event identification. Identification and encoding of
richer knowledge ? possibly linked to knowledge
sources ? and development of learning algorithms
that would effectively incorporate them is a neces-
sary next step towards improving the current state
of the art. The computational learning community,
in general, is also witnessing a move towards eval-
uations based on joint inference, with the two pre-
vious CoNLL tasks (Surdeanu et al, 2008; Hajic? et
al., 2009) devoted to joint learning of syntactic and
semantic dependencies. A principle ingredient for
joint learning is the presence of multiple layers of
semantic information.
One fundamental question still remains, and that
is ? what would it take to improve the state of the art
in coreference resolution that has not been attempted
so far? Many different algorithms have been tried in
the past 15 years, but one thing that is still lacking
is a corpus comprehensively tagged on a large scale
with consistent, multiple layers of semantic infor-
mation. One of the many goals of the OntoNotes
project2 (Hovy et al, 2006; Weischedel et al, 2011)
is to explore whether it can fill this void and help
push the progress further ? not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-
1http://projects.ldc.upenn.edu/ace/data/
2http://www.bbn.com/nlp/ontonotes
ers entities and events not limited to noun phrases
or a limited set of entity types. A small portion of
this corpus from the newswire and broadcast news
genres (?120k) was recently used for a SEMEVAL
task (Recasens et al, 2010). As mentioned earlier,
the coreference layer in OntoNotes constitutes just
one part of a multi-layered, integrated annotation of
shallow semantic structure in text with high inter-
annotator agreement, which also provides a unique
opportunity for performing joint inference over a
substantial body of data.
The remainder of this paper is organized as
follows. Section 2 presents an overview of the
OntoNotes corpus. Section 3 describes the coref-
erence annotation in OntoNotes. Section 4 then de-
scribes the shared task, including the data provided
and the evaluation criteria. Sections 5 and 6 then de-
scribe the participating system results and analyze
the approaches, and Section 7 concludes.
2 The OntoNotes Corpus
The OntoNotes project has created a corpus of large-
scale, accurate, and integrated annotation of multi-
ple levels of the shallow semantic structure in text.
The idea is that this rich, integrated annotation cov-
ering many layers will allow for richer, cross-layer
models enabling significantly better automatic se-
mantic analysis. In addition to coreference, this
data is also tagged with syntactic trees, high cov-
erage verb and some noun propositions, partial verb
and noun word senses, and 18 named entity types.
However, such multi-layer annotations, with com-
plex, cross-layer dependencies, demands a robust,
efficient, scalable mechanism for storing them while
providing efficient, convenient, integrated access to
the the underlying structure. To this effect, it uses a
relational database representation that captures both
the inter- and intra-layer dependencies and also pro-
vides an object-oriented API for efficient, multi-
tiered access to this data (Pradhan et al, 2007a).
This should facilitate the creation of cross-layer fea-
tures in integrated predictive models that will make
use of these annotations.
Although OntoNotes is a multi-lingual resource
with all layers of annotation covering three lan-
guages: English, Chinese and Arabic, for the scope
of this paper, we will just look at the English por-
tion. Over the years of the development of this cor-
pus, there were various priorities that came into play,
and therefore not all the data in the English portion is
annotated with all the different layers of annotation.
There is a core portion, however, which is roughly
2
1.3M words which has been annotated with all the
layers. It comprises ?450k words from newswire,
?150k from magazine articles, ?200k from broad-
cast news, ?200k from broadcast conversations and
?200k web data.
OntoNotes comprises the following layers of an-
notation:
? Syntax ? A syntactic layer representing a re-
vised Penn Treebank (Marcus et al, 1993;
Babko-Malaya et al, 2006).
? Propositions ? The proposition structure of
verbs in the form of a revised PropBank(Palmer
et al, 2005; Babko-Malaya et al, 2006).
? Word Sense ? Coarse grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize cov-
erage. The word sense granularity is tailored
to achieve 90% inter-annotator agreement as
demonstrated by Palmer et al (2007). These
senses are defined in the sense inventory files
and each individual sense has been connected
to multiple WordNet senses. This provides a
direct access to the WordNet semantic struc-
ture for users to make use of. There is also a
mapping from the word senses to the PropBank
frames and to VerbNet (Kipper et al, 2000) and
FrameNet (Fillmore et al, 2003).
? Named Entities ? The corpus was tagged with
a set of 18 proper named entity types that
were well-defined and well-tested for inter-
annotator agreement by Weischedel and Burn-
stein (2005).
? Coreference ? This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a limited
set of entity types (Pradhan et al, 2007b). We
will take a look at this in detail in the next sec-
tion.
3 Coreference in OntoNotes
General anaphoric coreference that spans a rich set
of entities and events ? not restricted to a few types,
as has been characteristic of most coreference data
available until now ? has been tagged with a high
degree of consistency. Attributive coreference is
tagged separately from the more common identity
coreference.
Two different types of coreference are distin-
guished in the OntoNotes data: Identical (IDENT),
and Appositive (APPOS). Appositives are treated
separately because they function as attributions, as
described further below. The IDENT type is used
for anaphoric coreference, meaning links between
pronominal, nominal, and named mentions of spe-
cific referents. It does not include mentions of
generic, underspecified, or abstract entities.
Coreference is annotated for all specific entities
and events. There is no limit on the semantic types
of NP entities that can be considered for coreference,
and in particular, coreference is not limited to ACE
types.
The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by auto-
matically extracting all of the NP mentions from the
Penn Treebank, though the annotators can also add
additional mentions when appropriate. In the fol-
lowing two examples (and later ones), the phrases
notated in bold form the links of an IDENT chain.
(1) She had a good suggestion and it was unani-
mously accepted by all.
(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst?s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are ?slightly above? fiscal 1989 sales of $
155 million.
3.1 Verbs
Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with an-
other verb. The intent is to annotate the VP, but we
mark the single-word head for convenience. This in-
cludes morphologically related nominalizations (3)
and noun phrases that refer to the same event, even
if they are lexically distinct from the verb (4). In the
following two examples, only the chains related to
the growth event are shown.
(3) Sales of passenger cars grew 22%. The strong
growth followed year-to-year increases.
(4) Japan?s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers? Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.
3
3.2 Pronouns
All pronouns and demonstratives are linked to any-
thing that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged as coreferent.)
(5) Senate majority leader Bill Frist likes to tell a
story from his days as a pioneering heart sur-
geon back in Tennessee. A lot of times, Frist re-
calls, *you?d have a critical patient lying there
waiting for a new heart, and *you?d want to
cut, but *you couldn?t start unless *you knew
that the replacement heart would make *it to
the operating room.
3.3 Generic mentions
Generic nominal mentions can be linked with refer-
ring pronouns and other definite mentions, but are
not linked to other generic nominal mentions. This
would allow linking of the bracketed mentions in (6)
and (7), but not (8).
(6) Officials said they are tired of making the same
statements.
(7) Meetings are most productive when they are
held in the morning. Those meetings, however,
generally have the worst attendance.
(8) Allergan Inc. said it received approval to
sell the PhacoFlex intraocular lens, the first
foldable silicone lens available for *cataract
surgery. The lens? foldability enables it to be
inserted in smaller incisions than are now pos-
sible for *cataract surgery.
Bare plurals, as in (6) and (7), are always consid-
ered generic. In example (9) below, there are two
generic instances of parents. These are marked as
distinct IDENT chains (with separate chains distin-
guished by subscripts X, Y and Z), each containing
a generic and the related referring pronouns.
(9) ParentsX should be involved with theirX chil-
dren?s education at home, not in school. TheyX
should see to it that theirX kids don?t play tru-
ant; theyX should make certain that the children
spend enough time doing homework; theyX
should scrutinize the report card. ParentsY are
too likely to blame schools for the educational
limitations of theirY children. If parentsZ are
dissatisfied with a school, theyZ should have
the option of switching to another.
In (10) below, the verb ?halve? cannot be linked
to ?a reduction of 50%?, since ?a reduction? is in-
definite.
(10) Argentina said it will ask creditor banks to
*halve its foreign debt of $64 billion ? the
third-highest in the developing world . Ar-
gentina aspires to reach *a reduction of 50%
in the value of its external debt.
3.4 Pre-modifiers
Proper pre-modifiers can be coreferenced, but
proper nouns that are in a morphologically adjecti-
val form are treated as adjectives, and not corefer-
enced. For example, adjectival forms of GPEs such
as Chinese in ?the Chinese leader?, would not be
linked. Thus we could coreference United States in
?the United States policy? with another referent, but
not American ?the American policy.? GPEs and Na-
tionality acronyms (e.g. U.S.S.R. or U.S.). are also
considered adjectival. Pre-modifier acronyms can be
coreferenced unless they refer to a nationality. Thus
in the examples below, FBI can be coreferenced to
other mentions, but U.S. cannot.
(11) FBI spokesman
(12) *U.S. spokesman
Dates and monetary amounts can be considered
part of a coreference chain even when they occur as
pre-modifiers.
(13) The current account deficit on France?s balance
of payments narrowed to 1.48 billion French
francs ($236.8 million) in August from a re-
vised 2.1 billion francs in July, the Finance
Ministry said. Previously, the July figure was
estimated at a deficit of 613 million francs.
(14) The company?s $150 offer was unexpected.
The firm balked at the price.
3.5 Copular verbs
Attributes signaled by copular structures are not
marked; these are attributes of the referent they mod-
ify, and their relationship to that referent will be
captured through word sense and propositional ar-
gument tagging.
4
(15) JohnX is a linguist. PeopleY are nervous
around JohnX, because heX always corrects
theirY grammar.
Copular (or ?linking?) verbs are those verbs that
function as a copula and are followed by a sub-
ject complement. Some common copular verbs are:
be, appear, feel, look, seem, remain, stay, become,
end up, get. Subject complements following such
verbs are considered attributes, and not linked. Since
Called is copular, neither IDENT nor APPOS corefer-
ence is marked in the following case.
(16) Called Otto?s Original Oat Bran Beer, the brew
costs about $12.75 a case.
3.6 Small clauses
Like copulas, small clause constructions are not
marked. The following example is treated as if the
copula were present (?John considers Fred to be an
idiot?):
(17) John considers *Fred *an idiot.
3.7 Temporal expressions
Temporal expressions such as the following are
linked:
(18) John spent three years in jail. In that time...
Deictic expressions such as now, then, today, to-
morrow, yesterday, etc. can be linked, as well as
other temporal expressions that are relative to the
time of the writing of the article, and which may
therefore require knowledge of the time of the writ-
ing to resolve the coreference. Annotators were al-
lowed to use knowledge from outside the text in re-
solving these cases. In the following example, the
end of this period and that time can be coreferenced,
as can this period and from three years to seven
years.
(19) The limit could range from three years to
seven yearsX, depending on the composition
of the management team and the nature of its
strategic plan. At (the end of (this period)X)Y,
the poison pill would be eliminated automati-
cally, unless a new poison pill were approved
by the then-current shareholders, who would
have an opportunity to evaluate the corpora-
tion?s strategy and management team at that
timeY.
In multi-date temporal expressions, embedded
dates are not separately connected to to other men-
tions of that date. For example in Nov. 2, 1999, Nov.
would not be linked to another instance of November
later in the text.
3.8 Appositives
Because they logically represent attributions, appos-
itives are tagged separately from Identity corefer-
ence. They consist of a head, or referent (a noun
phrase that points to a specific object/concept in the
world), and one or more attributes of that referent.
An appositive construction contains a noun phrase
that modifies an immediately-adjacent noun phrase
(separated only by a comma, colon, dash, or paren-
thesis). It often serves to rename or further define
the first mention. Marking appositive constructions
allows us to capture the attributed property even
though there is no explicit copula.
(20) Johnhead, a linguistattribute
The head of each appositive construction is distin-
guished from the attribute according to the following
heuristic specificity scale, in a decreasing order from
top to bottom:
Type Example
Proper noun John
Pronoun He
Definite NP the man
Indefinite specific NP a man I know
Non-specific NP man
This leads to the following cases:
(21) Johnhead, a linguistattribute
(22) A famous linguistattribute, hehead studied at ...
(23) a principal of the firmattribute, J. Smithhead
In cases where the two members of the appositive
are equivalent in specificity, the left-most member of
the appositive is marked as the head/referent. Defi-
nite NPs include NPs with a definite marker (the) as
well as NPs with a possessive adjective (his). Thus
the first element is the head in all of the following
cases:
(24) The chairman, the man who never gives up
(25) The sheriff, his friend
(26) His friend, the sheriff
In the specificity scale, specific names of diseases
and technologies are classified as proper names,
whether they are capitalized or not.
(27) A dangerous bacteria, bacillium, is found
5
Type Description
Annotator Error An annotator error. This is a catch-all category for cases of errors that do not fit in the other
categories.
Genuine Ambiguity This is just genuinely ambiguous. Often the case with pronouns that have no clear an-
tecedent (especially this & that)
Generics One person thought this was a generic mention, and the other person didn?t
Guidelines The guidelines need to be clear about this example
Callisto Layout Something to do with the usage/design of Callisto
Referents Each annotator thought this was referring to two completely different things
Possessives One person did not mark this possessive
Verb One person did not mark this verb
Pre Modifiers One person did not mark this Pre Modifier
Appositive One person did not mark this appositive
Extent Both people marked the same entity, but one person?s mention was longer
Copula Disagreement arose because this mention is part of a copular structure
a) Either each annotator marked a different half of the copula
b) Or one annotator unnecessarily marked both
Figure 1: Description of various disagreement types
Figure 1: The distribution of disagreements across the various types in Table 2
Sheet1
Page 1
Copulae 2%Appositives 3%Pre Modifiers 3%Verbs 3%Possessives 4%Refer nts 7%Callisto Layout 8%Guidelines 8%Generics 11%Genuine Ambiguity 25%Annotator Error 26%
Copulae
Appositives
Pre Modifiers
Verbs
Possessives
Referents
Callisto Layout
Guidelines
Generics
Genuine Ambiguity
Annotator Error
0% 5% 10% 15% 20% 25% 30%
Figure 2: The distribution of disagreements across the various types in Table 1
When the entity to which an appositive refers is
also mentioned elsewhere, only the single span con-
taining the entire appositive construction is included
in the larger IDENT chain. None of the nested NP
spans are linked. In the example below, the en-
tire span can be linked to later mentions to Richard
Godown. The sub-spans are not included separately
in the IDENT chain.
(28) Richard Godown, president of the Indus-
trial Biotechnology Association
Ages are tagged as attributes (as if they were el-
lipses of, for example, a 42-year-old):
(29) Mr.Smithhead, 42attribute,
3.9 Special Issues
In addition to the ones above, there are some special
cases such as:
? No coreference is marked between an organi-
zation and its members.
Genre ANN1-ANN2 ANN1-ADJ ANN2-ADJ
Newswire 80.9 85.2 88.3
Broadcast News 78.6 83.5 89.4
Broadcast Conversation 86.7 91.6 93.7
Magazine 78.4 83.2 88.8
Web 85.9 92.2 91.2
Table 1: Inter Annotator and Adjudicator agreement for
the Coreference Layer in OntoNotes measured in terms
of the MUC score.
? GPEs are linked to references to their govern-
ments, even when the references are nested
NPs, or the modifier and head of a single NP.
3.10 Annotator Agreement and Analysis
Table 1 shows the inter-annotator and annotator-
adjudicator agreement on all the genres of
OntoNotes. We also analyzed about 15K dis-
agreements in various parts of the data, and grouped
them into one of the categories shown in Figure 1.
Figure 2 shows the distribution of these different
types that were found in that sample. It can be
6
seen that genuine ambiguity and annotator error
are the biggest contributors ? the latter of which is
usually captured during adjudication, thus showing
the increased agreement between the adjudicated
version and the individual annotator version.
4 CoNLL-2011 Coreference Task
This section describes the CoNLL-2011 Corefer-
ence task, including its closed and open track ver-
sions, and characterizes the data used for the task
and how it was prepared.
4.1 Why a Coreference Task?
Despite close to a two-decade history of evaluations
on coreference tasks, variation in the evaluation cri-
teria and in the training data used have made it dif-
ficult for researchers to be clear about the state of
the art or to determine which particular areas require
further attention. There are many different parame-
ters involved in defining a coreference task. Looking
at various numbers reported in literature can greatly
affect the perceived difficulty of the task. It can seem
to be a very hard problem (Soon et al, 2001) or one
that is somewhat easier (Culotta et al, 2007). Given
the space constraints, we refer the reader to Stoy-
anov et al (2009) for a detailed treatment of the
issue.
Limitations in the size and scope of the available
datasets have also constrained research progress.
The MUC and ACE corpora are the two that have
been used most for reporting comparative results,
but they differ in the types of entities and corefer-
ence annotated. The ACE corpus is also one that
evolved over a period of almost five years, with dif-
ferent incarnations of the task definition and dif-
ferent corpus cross-sections on which performance
numbers have been reported, making it hard to un-
tangle and interpret the results.
The availability of the OntoNotes data offered an
opportunity to define a coreference task based on a
larger, more broad-coverage corpus. We have tried
to design the task so that it not only can support the
current evaluation, but also can provide an ongoing
resource for comparing different coreference algo-
rithms and approaches.
4.2 Task Description
The CoNLL-2011 shared task was based on the En-
glish portion of the OntoNotes 4.0 data. The task
was to automatically identify mentions of entities
and events in text and to link the coreferring men-
tions together to form entity/event chains. The target
coreference decisions could be made using automat-
ically predicted information on the other structural
layers including the parses, semantic roles, word
senses, and named entities.
As is customary for CoNLL tasks, there were two
tracks, closed and open. For the closed track, sys-
tems were limited to using the distributed resources,
in order to allow a fair comparison of algorithm per-
formance, while the open track allowed for almost
unrestricted use of external resources in addition to
the provided data.
4.2.1 Closed Track
In the closed track, systems were limited to the pro-
vided data, plus the use of two pre-specified external
resources: i) WordNet and ii) a pre-computed num-
ber and gender table by Bergsma and Lin (2006).
For the training and test data, in addition to the
underlying text, predicted versions of all the supple-
mentary layers of annotation were provided, where
those predictions were derived using off-the-shelf
tools (parsers, semantic role labelers, named entity
taggers, etc.) as described in Section 4.4.2. For the
training data, however, in addition to predicted val-
ues for the other layers, we also provided manual
gold-standard annotations for all the layers. Partici-
pants were allowed to use either the gold-standard or
predicted annotation for training their systems. They
were also free to use the gold-standard data to train
their own models for the various layers of annota-
tion, if they judged that those would either provide
more accurate predictions or alternative predictions
for use as multiple views, or wished to use a lattice
of predictions.
More so than previous CoNLL tasks, corefer-
ence predictions depend on world knowledge, and
many state-of-the-art systems use information from
external resources such as WordNet, which can
add a layer that helps the system to recognize se-
mantic connections between the various lexical-
ized mentions in the text. Therefore, the use of
WordNet was allowed, even for the closed track.
Since word senses in OntoNotes are predominantly3
coarse-grained groupings of WordNet senses, sys-
tems could also map from the predicted or gold-
standard word senses provided to the sets of under-
lying WordNet senses. Another significant piece of
knowledge that is particularly useful for coreference
but that is not available in the layers of OntoNotes is
that of number and gender. There are many different
3There are a few instances of novel senses introduced in
OntoNotes which were not present in WordNet, and so lack a
mapping back to the WordNet senses
7
ways of predicting these values, with differing accu-
racies, so in order to ensure that participants in the
closed track were working from the same data, thus
allowing clearer algorithmic comparisons, we spec-
ified a particular table of number and gender predic-
tions generated by Bergsma and Lin (2006), for use
during both training and testing.
Following the recent CoNLL tradition, partici-
pants were allowed to use both the training and the
development data for training the final model.
4.2.2 Open Track
In addition to resources available in the closed track,
the open track, systems were allowed to use external
resources such as Wikipedia, gazetteers etc. This
track is mainly to get an idea of a performance ceil-
ing on the task at the cost of not getting a compar-
ison across all systems. Another advantage of the
open track is that it might reduce the barriers to par-
ticipation by allowing participants to field existing
research systems that already depend on external re-
sources ? especially if there were hard dependen-
cies on these resources. They can participate in the
task with minimal or no modification to their exist-
ing system.
4.3 Coreference Task Data
Since there are no previously reported numbers on
the full version of OntoNotes, we had to create
a train/development/test partition. The only por-
tion of OntoNotes that has a previously determined,
widely used, standard split is the WSJ portion of the
newswire data. For that subcorpus, we maintained
the same partition. For all the other portions we cre-
ated stratified training, development and test parti-
tions over all the sources in OntoNotes using the pro-
cedure shown in Algorithm 1. The list of training,
development and test document IDs can be found on
the task webpage.4
4.4 Data Preparation
This section gives details of the different annota-
tion layers including the automatic models that were
used to predict them, and describes the formats in
which the data were provided to the participants.
4.4.1 Manual Annotation Gold Layers
We will take a look at the manually annotated, or
gold layers of information that were made available
for the training data.
4http://conll.bbn.com/download/conll-train.id
http://conll.bbn.com/download/conll-dev.id
http://conll.bbn.com/download/conll-test.id
Algorithm 1 Procedure used to create OntoNotes
training, development and test partitions.
Procedure: GENERATE PARTITIONS(ONTONOTES) returns TRAIN,
DEV, TEST
1: TRAIN? ?
2: DEV? ?
3: TEST? ?
4: for all SOURCE ? ONTONOTES do
5: if SOURCE = WALL STREET JOURNAL then
6: TRAIN? TRAIN ? SECTIONS 02 ? 21
7: DEV? DEV ? SECTIONS 00, 01, 22, 24
8: TEST? TEST ? SECTION 23
9: else
10: if Number of files in SOURCE ? 10 then
11: TRAIN? TRAIN ? FILE IDS ending in 1 ? 8
12: DEV? DEV ? FILE IDS ending in 0
13: TEST? TEST ? FILE IDS ending in 9
14: else
15: DEV? DEV ? FILE IDS ending in 0
16: TEST? TEST ? FILE ID ending in the highest number
17: TRAIN? TRAIN ? Remaining FILE IDS for the
SOURCE
18: end if
19: end if
20: end for
21: return TRAIN, DEV, TEST
Coreference The manual coreference annotation
is stored as chains of linked mentions connecting
multiple mentions of the same entity. Coreference is
the only document-level phenomenon in OntoNotes,
and the complexity of annotation increases non-
linearly with the length of a document. Unfortu-
nately, some of the documents ? especially ones in
the broadcast conversation, weblogs, and telephone
conversation genre ? are very long which prohib-
ited us from efficiently annotating them in entirety.
These had to be split into smaller parts. We con-
ducted a few passes to join some adjacent parts, but
since some documents had as many as 17 parts, there
are still multi-part documents in the corpus. Since
the coreference chains are coherent only within each
of these document parts, for this task, each such part
is treated as a separate document. Another thing
to note is that there were some cases of sub-token
annotation in the corpus owing to the fact that to-
kens were not split at hyphens. Cases such as pro-
WalMart had the sub-span WalMart linked with another
instance of the same. The recent Treebank revision
which split tokens at most hyphens, made a majority
of these sub-token annotations go away. There were
still some residual sub-token annotations. Since
subtoken annotations cannot be represented in the
CoNLL format, and they were a very small quantity
? much less than even half a percent ? we decided to
ignore them.
For various reasons, not all the documents in
OntoNotes have been annotated with all the differ-
8
Corpora Words Documents
Total Train Dev Test Total Train Dev Test
MUC-6 25K 12K 13K 60 30 30
MUC-7 40K 19K 21K 67 30 37
ACE (2000-2004) 1M 775K 235K - - -
OntoNotes5 1.3M 1M 136K 142K 2,083(2,999) 1,674(2,374) 202(303) 207(322)
Table 2: Number of documents in the OntoNotes data, and some comparison with the MUC and ACE data sets. The
numbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents.
Each part was considered a separate document for evaluation purposes.
Syntactic category Train Development Test
Count % Count % Count %
NP 60,345 59.71 8,463 59.31 8,629 53.09
PRP 25,472 25.21 3,535 24.78 5,012 30.84
PRP$ 8,889 8.80 1,208 8.47 1,466 9.02
NNP 2,643 2.62 468 3.28 475 2.92
NML 900 0.89 151 1.06 118 0.73
Vx 1,915 1.89 317 2.22 314 1.93
Other 893 0.88 126 0.88 239 1.47
Overall 101,057 100.00 14,268 100.00 16,253 100.00
Table 3: Distribution of mentions in the data by their syn-
tactic category.
Train Development Test
Entities/Chains 26,612 3,752 3,926
Links 74,652 10,539 12,365
Mentions 101,264 14,291 16,291
Table 4: Number of entities, links and mentions in the
OntoNotes 4.0 data.
ent layers of annotation, with full coverage.6 There
is a core portion, however, which is roughly 1.3M
words which has been annotated with all the layers.
This is the portion that we used for the shared task.
The number of documents in the corpus for this
task, for each of the different genres, are shown in
Table 2. Tables 3 and 4 shows the distribution of
mentions by the syntactic categories, and the counts
of entities, links and mentions in the corpus respec-
tively. All of this data has been Treebanked and
PropBanked either as part of the OntoNotes effort
or some preceding effort.
For comparison purposes, Table 2 also lists the
number of documents in the MUC-6, MUC-7, and
ACE (2000-2004) corpora. The MUC-6 data was
taken from the Wall Street Journal, whereas the
MUC-7 data was from the New York Times. The
ACE data spanned many different genres similar to
6Given the nature of word sense annotation, and changes in
project priorities, we could not annotate all the low frequency
verbs and nouns in the corpus. Furthermore, PropBank annota-
tion currently only covers verb predicates.
the ones in OntoNotes.
Parse Trees This represents the syntactic layer
that is a revised version of the Penn Treebank. For
purposes of this task, traces were removed from the
syntactic trees, since the CoNLL-style data format,
being indexed by tokens, does not provide any good
means of conveying that information. Function tags
were also removed, since the parsers that we used
for the predicted syntax layer did not provide them.
One thing that needs to be dealt with in conversa-
tional data is the presence of disfluencies (restarts,
etc.). In the original OntoNotes parses, these are
marked using a special EDITED7 phrase tag ? as was
the case for the Switchboard Treebank. Given the
frequency of disfluencies and the performance with
which one can identify them automatically,8 a prob-
able processing pipeline would filter them out be-
fore parsing. Since we did not have a readily avail-
able tagger for tagging disfluencies, we decided to
remove them using oracle information available in
the Treebank.
Propositions The propositions in OntoNotes con-
stitute PropBank semantic roles. Most of the verb
predicates in the corpus have been annotated with
their arguments. Recent enhancements to the Prop-
Bank to make it synchronize better with the Tree-
bank (Babko-Malaya et al, 2006) have enhanced
the information in the proposition by the addition of
two types of LINKs that represent pragmatic corefer-
ence (LINK-PCR) and selectional preferences (LINK-
SLC). More details can be found in the addendum to
the PropBank guidelines9 in the OntoNotes 4.0 re-
7There is another phrase type ? EMBED in the telephone con-
versation genre which is similar to the EDITED phrase type, and
sometimes identifies insertions, but sometimes contains logical
continuation of phrases, so we decided not to remove that from
the data.
8A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 Precision
and 67 recall.
9doc/propbank/english-propbank.pdf
9
lease. Since the community is not used to this rep-
resentation which relies heavily on the trace struc-
ture in the Treebank which we are excluding, we de-
cided to unfold the LINKs back to their original rep-
resentation as in the Release 1.0 of the Proposition
Bank. This functionality is part of the OntoNotes
DB Tool.10
Word Sense Gold word sense annotation was
supplied using sense numbers as specified in
the OntoNotes list of senses for each lemma.11
The sense inventories that were provided in the
OntoNotes 4.0 release were not all mapped to the lat-
est version 3.0 of WordNet, so we provided a revised
version of the sense inventories, containing mapping
to WordNet 3.0, on the task page for the participants.
Named Entities Named Entities in OntoNotes
data are specified using a catalog of 18 Name types.
Other Layers Discourse plays a vital role in
coreference resolution. In the case of broadcast con-
versation, or telephone conversation data, it partially
manifests in the form of speakers of a given utter-
ance, whereas in weblogs or newsgroups it does so
as the writer, or commenter of a particular article
or thread. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be au-
tomatically deduced, but since it would add addi-
tional complexity to the already complex task, we
decided to provide oracle information of this meta-
data both during training and testing. In other words,
speaker and author identification was not treated
as an annotation layer that needed to be predicted.
This information was provided in the form of an-
other column in the .conll table. There were some
cases of interruptions and interjections that ideally
would associate parts of a sentence to two different
speakers, but since the frequency of this was quite
small, we decided to make an assumption of one
speaker/writer per sentence.
4.4.2 Predicted Annotation Layers
The predicted annotation layers were derived using
automatic models trained using cross-validation on
other portions of OntoNotes data. As mentioned ear-
lier, there are some portions of the OntoNotes corpus
that have not been annotated for coreference but that
have been annotated for other layers. For training
10http://cemantix.org/ontonotes.html
11It should be noted that word sense annotation in OntoNotes
is note complete, so only some of the verbs and nouns have
word sense tags specified.
Senses Lemmas
1 1,506
2 1,046
> 2 1,016
Table 6: Word sense polysemy over verb and noun lem-
mas in OntoNotes
models for each of the layers, where feasible, we
used all the data that we could for that layer from
the training portion of the entire OntoNotes release.
Parse Trees Predicted parse trees were produced
using the Charniak parser (Charniak and Johnson,
2005).12 Some additional tag types used in the
OntoNotes trees were added to the parser?s tagset,
including the NML tag that has recently been added
to capture internal NP structure, and the rules used to
determine head words were appropriately extended.
The parser was then re-trained on the training por-
tion of the release 4.0 data using 10-fold cross-
validation. Table 5 shows the performance of the
re-trained Charniak parser on the CoNLL-2011 test
set. We did not get a chance to re-train the re-ranker,
and since the stock re-ranker crashes when run on n-
best parses containing NMLs, because it has not seen
that tag in training, we could not make use of it.
Word Sense We trained a word sense tagger us-
ing a SVM classifier and contextual word and part
of speech features on all the training portion of the
OntoNotes data. The OntoNotes 4.0 corpus com-
prises a total of 14,662 sense definitions across 4877
verb and noun lemmas13. The distribution of senses
per lemma is as shown in Table 6. Table 7 shows
the performance of this classifier over both the verbs
and nouns in the CoNLL-2011 test set. Again this
performance is not directly comparable to any re-
ported in the literature before, and it seems lower
then performances reported on previous versions
of OntoNotes because this is over all the genres
of OntoNotes, and aggregated over both verbs and
nouns in the CoNLL-2011 test set.
Propositions To predict propositional structure,
ASSERT14 (Pradhan et al, 2005) was used, re-
trained also on all the training portion of the release
12http://bllip.cs.brown.edu/download/reranking-
parserAug06.tar.gz
13The number of lemmas in Table 6 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
7933.
14http://cemantix.org/assert.html
10
All Sentences Sentence len < 40
N POS R P F N R P F
Broadcast Conversation (BC) 2,194 95.93 84.30 84.46 84.38 2124 85.83 85.97 85.90
Broadcast News (BN) 1,344 96.50 84.19 84.28 84.24 1278 85.93 86.04 85.98
Magazine (MZ) 780 95.14 87.11 87.46 87.28 736 87.71 88.04 87.87
Newswire (NW) 2,273 96.95 87.05 87.45 87.25 2082 88.95 89.27 89.11
Telephone Conversation (TC) 1,366 93.52 79.73 80.83 80.28 1359 79.88 80.98 80.43
Weblogs and Newsgroups (WB) 1,658 94.67 83.32 83.20 83.26 1566 85.14 85.07 85.11
Overall 9,615 96.03 85.25 85.43 85.34 9145 86.86 87.02 86.94
Table 5: Parser performance on the CoNLL-2011 test set
Frameset Total Total % Perfect Argument ID + Class
Accuracy Sentences Propositions Propositions P R F
Broadcast Conversation (BC) 0.92 2,037 5,021 52.18 82.55 64.84 72.63
Broadcast News (BN) 0.91 1,252 3,310 53.66 81.64 64.46 72.04
Magazine (MZ) 0.89 780 2,373 47.16 79.98 61.66 69.64
Newswire (NW) 0.93 1,898 4,758 39.72 80.53 62.68 70.49
Weblogs and Newsgroups (WB) 0.92 929 2,174 39.19 81.01 60.65 69.37
Overall 0.91 6,896 17,636 46.82 81.28 63.17 71.09
Table 8: Performance on the propositions and framesets in the CoNLL-2011 test set.
Accuracy
Broadcast Conversation (BC) 0.70
Broadcast News (BN) 0.68
Magazine (MZ) 0.60
Newswire (NW) 0.62
Weblogs and Newsgroups (WB) 0.63
Overall 0.65
Table 7: Word sense performance over both verbs and
nouns in the CoNLL-2011 test set
4.0 data. Given time constraints, we had to per-
form two modifications: i) Instead of a single model
that predicts all arguments including NULL argu-
ments, we had to use the two-stage mode where the
NULL arguments are first filtered out and the remain-
ing NON-NULL arguments are classified into one of
the argument types, and ii) The argument identifi-
cation module used an ensemble of ten classifiers
? each trained on a tenth of the training data and
performed an unweighted voting among them. This
should still give a close to state of the art perfor-
mance given that the argument identification perfor-
mance tends to start to be asymptotic around 10k
training instances. At first glance, the performance
on the newswire genre is much lower than what has
been reported for WSJ Section 23. This could be
attributed to two factors: i) the fact that we had to
compromise on the training method, but more im-
portantly because ii) the newswire in OntoNotes not
only contains WSJ data, but also Xinhua news. One
could try to verify using just the WSJ portion of the
data, but it would be hard as it is not only a sub-
set of the documents that the performance has been
reported on previously, but also the annotation has
been significantly revised; it includes propositions
for be verbs missing from the original PropBank,
and the training data is a subset of the original data
as well. Table 8 shows the detailed performance
numbers.
In addition to automatically predicting the argu-
ments, we also trained a classifier to tag PropBank
frameset IDs in the data using the same word sense
module as mentioned earlier. OntoNotes 4.0 con-
tains a total of 7337 framesets across 5433 verb
lemmas.15 An overwhelming number of them are
monosemous, but the more frequent verbs tend to be
polysemous. Table 9 gives the distribution of num-
ber of framesets per lemma in the PropBank layer of
the OntoNotes 4.0 data.
During automatic processing of the data, we
tagged all the tokens that were tagged with a part
of speech VBx. This means that there would be cases
where the wrong token would be tagged with propo-
sitions. The CoNLL-2005 scorer was used to gener-
ate the scores.
Named Entities BBN?s IdentiFinderTMsystem
was used to predict the named entities. Given the
15The number of lemmas in Table 9 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
4229.
11
Framesets Lemmas
1 2,722
2 321
> 2 181
Table 9: Frameset polysemy across lemmas
Overall BC BN MZ NW TC WB
F F F F F F F
ALL Named Entities 71.8 64.8 72.2 61.5 84.3 39.5 55.2
Cardinal 68.7 51.8 71.1 66.1 82.8 34.0 68.7
Date 76.1 63.7 77.9 66.7 83.7 60.5 56.0
Event 27.6 00.0 34.8 30.8 47.6 - 13.3
Facility 41.9 55.0 16.7 23.1 66.7 00.0 22.9
GPE 87.9 87.5 90.3 73.7 92.9 65.9 88.7
Language 41.2 - 50.0 50.0 00.0 20.0 75.0
Law 63.0 00.0 85.7 00.0 67.9 00.0 50.0
Location 58.4 59.1 59.6 53.3 68.0 00.0 23.5
Money 74.6 16.7 66.7 73.2 79.4 30.8 61.5
NORP 00.0 00.0 00.0 00.0 00.0 00.0 00.0
Ordinal 73.4 73.8 73.4 78.1 78.4 88.9 37.0
Organization 71.0 57.8 67.1 52.9 86.9 21.2 32.1
Percent 71.2 88.9 76.9 69.6 92.1 01.2 71.6
Person 79.6 78.9 87.7 66.7 91.6 65.1 64.8
Product 46.9 00.0 43.8 00.0 81.8 00.0 00.0
Quantity 47.5 25.3 58.3 61.1 71.9 00.0 22.2
Time 58.6 56.9 64.1 42.9 80.0 23.8 51.7
Work of Art 41.9 26.9 37.1 16.0 77.9 00.0 05.6
Table 10: Named Entity performance on the CoNLL-
2011 test set
time constraints, we could not re-train it on the
OntoNotes data and so an existing, pre-trained
model was used, therefore the results are not a
good indicator of the model?s best performance.
The pre-trained model had also used a somewhat
different catalog of name types, which did not
include the OntoNotes NORP type (for nationalities,
organizations, religions, and political parties),
so that category was never predicted. Table 10
shows the overall performance of the tagger on the
CoNLL-2011 test set, as well as the performance
broken down by individual name types. IdentiFinder
performance has been reported to be in the low 90?s
on WSJ test set.
Other Layers As noted above, systems were al-
lowed to make use of gender and number predic-
tions for NPs using the table from Bergsma and Lin
(Bergsma and Lin, 2006).
4.4.3 Data Format
In order to organize the multiple, rich layers of anno-
tation, the OntoNotes project has created a database
representation for the raw annotation layers along
with a Python API to manipulate them (Pradhan et
al., 2007a). In the OntoNotes distribution the data is
organized as one file per layer, per document. The
API requires a certain hierarchical structure with
documents at the leaves inside a hierarchy of lan-
guage, genre, source and section. It comes with var-
ious ways of cleanly querying and manipulating the
data and allows convenient access to the sense in-
ventory and propbank frame files instead of having
to interpret the raw .xml versions. However, main-
taining format consistency with earlier CoNLL tasks
was deemed convenient for sites that already had
tools configured to deal with that format. Therefore,
in order to distribute the data so that one could make
the best of both worlds, we created a new file type
called .conll which logically served as another layer
in addition to the .parse, .prop, .name and .coref
layers. Each .conll file contained a merged repre-
sentation of all the OntoNotes layers in the CoNLL-
style tabular format with one line per token, and with
multiple columns for each token specifying the input
annotation layers relevant to that token, with the fi-
nal column specifying the target coreference layer.
Because OntoNotes is not authorized to distribute
the underlying text, and many of the layers contain
inline annotation, we had to provide a skeletal form
(.skel of the .conll file which was essentially the
.conll file, but with the word column replaced with
a dummy string. We provided an assembly script
that participants could use to create a .conll file tak-
ing as input the .skel file and the top-level directory
of the OntoNotes distribution that they had sepa-
rately downloaded from the LDC16 Once the .conll
file is created, it can be used to create the individual
layers such as .parse, .name, .coref etc. using an-
other set of scripts. Since the propositions and word
sense layers are inherently standoff annotation, they
were provided as is, and did not require that extra
merging step. One thing thing that made this data
creation process a bit tricky was the fact that we had
dissected some of the trees for the conversation data
to remove the EDITED phrases. Table 11 describes
the data provided in each of the column of the .conll
format. Figure 3 shows a sample from a .conll file.
4.5 Evaluation
This section describes the evaluation criteria used.
Unlike for propositions, word sense and named en-
tities, where it is simply a matter of counting the
correct answers, or for parsing, where there are sev-
eral established metrics, evaluating the accuracy of
coreference continues to be contentious. Various al-
16OntoNotes is deeply grateful to the Linguistic Data Con-
sortium for making the source data freely available to the task
participants.
12
Column Type Description
1 Document ID This is a variation on the document filename
2 Part number Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
3 Word number This is the word index in the sentence
4 Word The word itself
5 Part of Speech Part of Speech of the word
6 Parse bit This is the bracketed structure broken before the first open parenthesis in the parse, and the
word/part-of-speech leaf replaced with a *. The full parse can be created by substituting
the asterix with the ([pos] [word]) string (or leaf) and concatenating the items in the
rows of that column.
7 Predicate lemma The predicate lemma is mentioned for the rows for which we have semantic role informa-
tion. All other rows are marked with a -
8 Predicate Frameset ID This is the PropBank frameset ID of the predicate in Column 7.
9 Word sense This is the word sense of the word in Column 3.
10 Speaker/Author This is the speaker or author name where available. Mostly in Broadcast Conversation and
Web Log data.
11 Named Entities These columns identifies the spans representing various named entities.
12:N Predicate Arguments There is one column each of predicate argument structure information for the predicate
mentioned in Column 7.
N Coreference Coreference chain information encoded in a parenthesis structure.
Table 11: Format of the .conll file used on the shared task
#begin document (nw/wsj/07/wsj_0771); part 000
...
...
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S(S* - - - - * * (ARG1* * * -
nw/wsj/07/wsj_0771 0 1 Vandenberg NNP (NP* - - - - (PERSON) (ARG1* * * * (8|(0)
nw/wsj/07/wsj_0771 0 2 and CC * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 3 Rayburn NNP *) - - - - (PERSON) *) * * *(23)|8)
nw/wsj/07/wsj_0771 0 4 are VBP (VP* be 01 1 - * (V*) * * * -
nw/wsj/07/wsj_0771 0 5 heroes NNS (NP(NP*) - - - - * (ARG2* * * * -
nw/wsj/07/wsj_0771 0 6 of IN (PP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 7 mine NN (NP*)))) - - 5 - * *) * * * (15)
nw/wsj/07/wsj_0771 0 8 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 9 ?? ?? *) - - - - * * *) * * -
nw/wsj/07/wsj_0771 0 10 Mr. NNP (NP* - - - - * * (ARG0* (ARG0* * (15
nw/wsj/07/wsj_0771 0 11 Boren NNP *) - - - - (PERSON) * *) *) * 15)
nw/wsj/07/wsj_0771 0 12 says VBZ (VP* say 01 1 - * * (V*) * * -
nw/wsj/07/wsj_0771 0 13 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 14 referring VBG (S(VP* refer 01 2 - * * (ARGM-ADV* (V*) * -
nw/wsj/07/wsj_0771 0 15 as RB (ADVP* - - - - * * * (ARGM-DIS* * -
nw/wsj/07/wsj_0771 0 16 well RB *) - - - - * * * *) * -
nw/wsj/07/wsj_0771 0 17 to IN (PP* - - - - * * * (ARG1* * -
nw/wsj/07/wsj_0771 0 18 Sam NNP (NP(NP* - - - - (PERSON* * * * * (23
nw/wsj/07/wsj_0771 0 19 Rayburn NNP *) - - - - *) * * * * -
nw/wsj/07/wsj_0771 0 20 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 21 the DT (NP(NP* - - - - * * * * (ARG0* -
nw/wsj/07/wsj_0771 0 22 Democratic JJ * - - - - (NORP) * * * * -
nw/wsj/07/wsj_0771 0 23 House NNP * - - - - (ORG) * * * * -
nw/wsj/07/wsj_0771 0 24 speaker NN *) - - - - * * * * *) -
nw/wsj/07/wsj_0771 0 25 who WP (SBAR(WHNP*) - - - - * * * * (R-ARG0*) -
nw/wsj/07/wsj_0771 0 26 cooperated VBD (S(VP* cooperate 01 1 - * * * * (V*) -
nw/wsj/07/wsj_0771 0 27 with IN (PP* - - - - * * * * (ARG1* -
nw/wsj/07/wsj_0771 0 28 President NNP (NP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 29 Eisenhower NNP *))))))))))) - - - - (PERSON) * *) *) *) 23)
nw/wsj/07/wsj_0771 0 30 . . *)) - - - - * * * * * -
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S* - - - - * * * -
nw/wsj/07/wsj_0771 0 1 They PRP (NP*) - - - - * (ARG0*) * (8)
nw/wsj/07/wsj_0771 0 2 allowed VBD (VP* allow 01 1 - * (V*) * -
nw/wsj/07/wsj_0771 0 3 this DT (S(NP* - - - - * (ARG1* (ARG1* (6
nw/wsj/07/wsj_0771 0 4 country NN *) - - 3 - * * *) 6)
nw/wsj/07/wsj_0771 0 5 to TO (VP* - - - - * * * -
nw/wsj/07/wsj_0771 0 6 be VB (VP* be 01 1 - * * (V*) (16)
nw/wsj/07/wsj_0771 0 7 credible JJ (ADJP*))))) - - - - * *) (ARG2*) -
nw/wsj/07/wsj_0771 0 8 . . *)) - - - - * * * -
#end document
Figure 3: Sample portion of the .conll file.
13
ternative metrics have been proposed, as mentioned
below, which weight different features of a proposed
coreference pattern differently. The choice is not
clear in part because the value of a particular set of
coreference predictions is integrally tied to the con-
suming application.
A further issue in defining a coreference metric
concerns the granularity of the mentions, and how
closely the predicted mentions are required to match
those in the gold standard for a coreference predic-
tion to be counted as correct.
Our evaluation criterion was in part driven by the
OntoNotes data structures. OntoNotes coreference
distinguishes between identity coreference and ap-
positive coreference, treating the latter separately
because it is already captured explicitly by other lay-
ers of the OntoNotes annotation. Thus we evaluated
systems only on the identity coreference task, which
links all categories of entities and events together
into equivalent classes.
The situation with mentions for OntoNotes is also
different than it was for MUC or ACE. OntoNotes
data does not explicitly identify the minimum ex-
tents of an entity mention, but it does include hand-
tagged syntactic parses. Thus for the official evalua-
tion, we decided to use the exact spans of mentions
for determining correctness. The NP boundaries
for the test data were pre-extracted from the hand-
tagged Treebank for annotation, and events trig-
gered by verb phrases were tagged using the verbs
themselves. This choice means that scores for the
CoNLL-2011 coreference task are likely to be lower
than for coref evaluations based on MUC, where the
mention spans are specified in the input,17 or those
based on ACE data, where an approximate match is
often allowed based on the specified head of the NP
mention.
4.5.1 Metrics
As noted above, the choice of an evaluation met-
ric for coreference has been a tricky issue and there
does not appear to be any silver bullet approach that
addresses all the concerns. Three metrics have been
proposed for evaluating coreference performance
over an unrestricted set of entity types: i) The link
based MUC metric (Vilain et al, 1995), ii) The men-
tion based B-CUBED metric (Bagga and Baldwin,
1998) and iii) The entity based CEAF (Constrained
Entity Aligned F-measure) metric (Luo, 2005). Very
recently BLANC (BiLateral Assessment of Noun-
Phrase Coreference) measure (Recasens and Hovy,
17as is the case in this evaluation with Gold Mentions
2011) has been proposed as well. Each of the met-
ric tries to address the shortcomings or biases of the
earlier metrics. Given a set of key entities K, and
a set of response entities R, with each entity com-
prising one or more mentions, each metric generates
its variation of a precision and recall measure. The
MUC measure if the oldest and most widely used. It
focuses on the links (or, pairs of mentions) in the
data.18 The number of common links between en-
tities in K and R divided by the number of links
in K represents the recall, whereas, precision is the
number of common links between entities in K and
R divided by the number of links in R. This met-
ric prefers systems that have more mentions per en-
tity ? a system that creates a single entity of all
the mentions will get a 100% recall without signifi-
cant degradation in its precision. And, it ignores re-
call for singleton entities, or entities with only one
mention. The B-CUBED metric tries to addresses
MUCS?s shortcomings, by focusing on the mentions
and computes recall and precision scores for each
mention. If K is the key entity containing mention M,
and R is the response entity containing mention M,
then recall for the mention M is computed as |K?R||K|
and precision for the same is is computed as |K?R||R| .
Overall recall and precision are the average of the
individual mention scores. CEAF aligns every re-
sponse entity with at most one key entity by finding
the best one-to-one mapping between the entities us-
ing an entity similarity metric. This is a maximum
bipartite matching problem and can be solved by
the Kuhn-Munkres algorithm. This is thus a entity
based measure. Depending on the similarity, there
are two variations ? entity based CEAF ? CEAFe and
a mention based CEAF ? CEAFe. Recall is the total
similarity divided by the number of mentions in K,
and precision is the total similarity divided by the
number of mentions in R. Finally, BLANC uses a
variation on the Rand index (Rand, 1971) suitable
for evaluating coreference. There are a few other
measures ? one being the ACE value, but since this
is specific to a restricted set of entities (ACE types),
we did not consider it.
4.5.2 Official Evaluation Metric
In order to determine the best performing system
in the shared task, we needed to associate a single
number with each system. This could have been
one of the metrics above, or some combination of
more than one of them. The choice was not sim-
ple, and while we consulted various researchers in
18The MUC corpora did not tag single mention entities.
14
the field, hoping for a strong consensus, their con-
clusion seemed to be that each metric had its pros
and cons. We settled on the MELA metric by Denis
and Baldridge (2009), which takes a weighted av-
erage of three metrics: MUC, B-CUBED, and CEAF.
The rationale for the combination is that each of the
three metrics represents a different important dimen-
sion, the MUC measure being based on links, the
B-CUBED based on mentions, and the CEAF based
on entities. For a given task, a weighted average
of the three might be optimal, but since we don?t
have an end task in mind, we decided to use the un-
weighted mean of the three metrics as the score on
which the winning system was judged. We decided
to use CEAFe instead of CEAFm.
4.5.3 Scoring Metrics Implementation
We used the same core scorer implementation19 that
was used for the SEMEVAL-2010 task, and which
implemented all the different metrics. There were a
couple of modifications done to this scorer after it
was used for the SEMEVAL-2010 task.
1. Only exact matches were considered cor-
rect. Previously, for SEMEVAL-2010 non-exact
matches were judged partially correct with a
0.5 score if the heads were the same and the
mention extent did not exceed the gold men-
tion.
2. The modifications suggested by Cai and Strube
(2010) were incorporated in the scorer.
Since there are differences in the version used for
CoNLL and the one available on the download site,
and it is possible that the latter would be revised in
the future, we have archived the version of the scorer
on the CoNLL-2011 task webpage.20
5 Systems and Results
About 65 different groups demonstrated interest in
the shared task by registering on the task webpage.
Of these, 23 groups submitted system outputs on the
test set during the evaluation week. 18 groups sub-
mitted only closed track results, 3 groups only open
track results, and 2 groups submitted both closed and
open track results. 2 participants in the closed track,
did not write system papers, so we don?t use their re-
sults in the discussion. Their results will be reported
on the task webpage.
19http://www.lsi.upc.edu/ esapena/downloads/index.php?id=3
20http://conll.bbn.com/download/scorer.v4.tar.gz
The official results for the 18 systems that submit-
ted closed track outputs are shown in Table 12, with
those for the 5 systems that submitted open track
results in Table 13. The official ranking score, the
arithmetic mean of the F-scores of MUC, B-CUBED
and CEAFe, is shown in the rightmost column. For
convenience, systems will be referred to here using
the first portion of the full name, which is unique
within each table.
For completeness, the tables include the raw pre-
cision and recall scores from which the F-scores
were derived. The tables also include two additional
scores (BLANC and CEAFm) that did not factor into
the official ranking score. Useful further analysis
may be possible based on these results beyond the
preliminary results presented here.
As discussed previously in the task description,
we will consider three different test input conditions:
i) Predicted only (Official), ii) Predicted plus gold
mention boundaries, and iii) Predicted plus gold
mentions
5.1 Predicted only (Official)
For the official test, beyond the raw source text,
coreference systems were provided only with the
predictions from automatic engines as to the other
annotation layers (parses, semantic roles, word
senses, and named entities).
In this evaluation it is important to note that the
mention detection score cannot be considered in iso-
lation of the coreference task as has usually been the
case. This is mainly owing to the fact that there are
no singleton entities in the OntoNotes data. Most
systems removed singletons from the response as a
post-processing step, so not only will they not get
credit for the singleton entities that they correctly re-
moved from the data, but they will be penalized for
the ones that they accidentally linked with another
mention. What this number does indicate is the ceil-
ing on recall that a system would have got in absence
of being penalized for making mistakes in corefer-
ence resolution. A close look at the Table 12 indi-
cates a possible outlier in case of the sapena system.
The recall for this system is very high, and precision
way lower than any other system. Further investi-
gations uncovered that the reason for this aberrant
behavior was that fact that this system opted to keep
singletons in the response. By design, the scorer re-
moves singletons that might be still present in the
system, but it does so after the mention detection
accuracy is computed.
The official scores top out in the high 50?s. While
this is lower than the figures cited in previous coref-
15
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
75
.07
66
.81
70
.7
0
61
.76
57
.53
59
.57
68
.40
68
.23
68
.31
56
.37
56
.37
56
.3
7
43
.41
47
.75
45
.4
8
70
.63
76
.21
73
.02
57
.7
9
sap
en
a
92
.39
28
.19
43
.20
56
.32
63
.16
59
.55
62
.75
72
.08
67
.09
53
.51
53
.51
53
.51
44
.75
38
.38
41
.32
69
.50
73
.07
71
.10
55
.99
ch
an
g
68
.08
61
.96
64
.88
57
.15
57
.15
57
.15
67
.14
70
.53
68
.7
9
54
.40
54
.40
54
.40
41
.94
41
.94
41
.94
71
.19
77
.09
73
.7
1
55
.96
nu
gu
es
69
.87
68
.08
68
.96
60
.20
57
.10
58
.61
66
.74
64
.23
65
.46
51
.45
51
.45
51
.45
38
.09
41
.06
39
.52
71
.99
70
.31
71
.11
54
.53
san
tos
67
.80
63
.25
65
.45
59
.21
54
.30
56
.65
68
.79
62
.81
65
.66
49
.54
49
.54
49
.54
35
.86
40
.21
37
.91
73
.37
66
.91
69
.46
53
.41
son
g
57
.81
80
.41
67
.26
53
.73
67
.79
59
.9
5
60
.65
66
.05
63
.23
46
.29
46
.29
46
.29
43
.37
30
.71
35
.96
69
.49
59
.71
61
.47
53
.05
sto
ya
no
v
70
.84
64
.98
67
.78
63
.61
54
.04
58
.43
72
.58
53
.27
61
.44
46
.08
46
.08
46
.08
32
.00
40
.82
35
.88
73
.21
58
.93
60
.88
51
.92
sob
ha
67
.82
62
.09
64
.83
51
.08
49
.88
50
.48
62
.63
65
.43
64
.00
49
.48
49
.48
49
.48
40
.65
41
.82
41
.23
61
.40
68
.35
63
.88
51
.90
ko
bd
an
i
62
.06
60
.04
61
.03
55
.64
51
.50
53
.49
69
.66
62
.43
65
.85
42
.70
42
.70
42
.70
32
.33
35
.40
33
.79
61
.86
63
.51
62
.61
51
.04
zh
ou
61
.08
63
.59
62
.31
45
.65
52
.79
48
.96
57
.14
72
.91
64
.07
47
.53
47
.53
47
.53
43
.19
36
.79
39
.74
61
.10
73
.94
64
.72
50
.92
ch
art
on
65
.90
62
.77
64
.30
55
.09
50
.05
52
.45
66
.26
58
.44
62
.10
46
.82
46
.82
46
.82
34
.33
39
.05
36
.54
69
.94
62
.23
64
.80
50
.36
ya
ng
71
.92
57
.53
63
.93
59
.91
46
.43
52
.31
71
.64
55
.14
62
.32
46
.55
46
.55
46
.55
30
.28
42
.39
35
.33
71
.11
61
.75
64
.63
49
.99
ha
o
64
.50
64
.11
64
.30
57
.89
51
.42
54
.47
67
.83
55
.43
61
.01
45
.07
45
.07
45
.07
30
.08
35
.76
32
.67
72
.61
62
.37
65
.35
49
.38
xin
xin
65
.49
58
.71
61
.92
48
.54
44
.85
46
.62
61
.59
62
.28
61
.93
44
.75
44
.75
44
.75
35
.19
38
.62
36
.83
63
.04
65
.83
64
.27
48
.46
zh
an
g
55
.35
68
.25
61
.13
42
.03
55
.62
47
.88
52
.57
73
.05
61
.14
44
.46
44
.46
44
.46
42
.00
30
.28
35
.19
62
.84
69
.22
65
.21
48
.07
ku
mm
erf
eld
69
.77
56
.97
62
.72
46
.39
39
.56
42
.70
63
.60
57
.30
60
.29
45
.35
45
.35
45
.35
35
.05
42
.26
38
.32
58
.74
61
.58
59
.91
47
.10
zh
ek
ov
a
67
.49
37
.60
48
.29
28
.87
20
.66
24
.08
67
.14
56
.67
61
.46
40
.43
40
.43
40
.43
31
.57
41
.21
35
.75
52
.77
57
.05
53
.77
40
.43
irw
in
17
.06
61
.09
26
.67
12
.45
50
.60
19
.98
35
.07
89
.90
50
.46
31
.68
31
.68
31
.68
45
.84
17
.38
25
.21
51
.48
56
.83
51
.12
31
.88
Ta
ble
12
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
74
.31
67
.87
70
.9
4
62
.83
59
.34
61
.0
3
68
.85
69
.01
68
.9
3
56
.70
56
.70
56
.7
0
43
.29
46
.80
44
.9
8
71
.90
76
.55
73
.96
58
.3
1
cai
67
.15
67
.64
67
.40
56
.73
58
.90
57
.80
64
.60
71
.03
67
.66
53
.37
53
.37
53
.37
42
.71
40
.68
41
.67
69
.77
73
.96
71
.62
55
.71
ury
up
ina
70
.60
66
.31
68
.39
59
.70
55
.70
57
.63
66
.29
64
.12
65
.18
51
.42
51
.42
51
.42
38
.34
42
.17
40
.16
69
.23
68
.54
68
.88
54
.32
kle
nn
er
64
.41
60
.28
62
.28
49
.04
50
.71
49
.86
61
.70
68
.61
64
.97
50
.03
50
.03
50
.03
41
.28
39
.70
40
.48
66
.05
73
.90
69
.05
51
.77
irw
in
24
.60
62
.27
35
.27
18
.56
51
.01
27
.21
38
.97
85
.57
53
.55
33
.86
33
.86
33
.86
43
.33
19
.36
26
.76
51
.62
52
.91
51
.76
35
.84
Ta
ble
13
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
79
.52
71
.25
75
.1
6
65
.87
62
.05
63
.9
0
69
.52
70
.55
70
.0
3
59
.26
59
.26
59
.2
6
46
.29
50
.48
48
.3
0
72
.00
78
.55
74
.7
7
60
.7
4
nu
gu
es
74
.18
70
.74
72
.42
64
.33
60
.05
62
.12
68
.26
65
.17
66
.68
53
.84
53
.84
53
.84
39
.86
44
.23
41
.93
72
.53
71
.04
71
.75
56
.91
ch
an
g
63
.37
73
.18
67
.92
55
.00
65
.50
59
.79
62
.16
76
.65
68
.65
54
.95
54
.95
54
.95
46
.77
37
.17
41
.42
70
.97
79
.30
74
.29
56
.62
san
tos
65
.82
69
.90
67
.80
57
.76
61
.39
59
.52
64
.49
70
.27
67
.26
51
.87
51
.87
51
.87
41
.42
38
.16
39
.72
72
.72
71
.97
72
.34
55
.50
ko
bd
an
i
67
.11
65
.09
66
.08
62
.63
56
.80
59
.57
73
.20
62
.22
67
.27
44
.49
44
.49
44
.49
32
.87
37
.25
34
.92
64
.07
64
.13
64
.10
53
.92
sto
ya
no
v
76
.90
64
.73
70
.29
69
.81
55
.01
61
.54
77
.07
52
.54
62
.48
48
.08
48
.08
48
.08
30
.97
44
.84
36
.64
76
.57
60
.33
62
.96
53
.55
zh
an
g
59
.62
71
.19
64
.89
46
.06
58
.75
51
.64
53
.89
73
.41
62
.16
46
.62
46
.62
46
.62
43
.49
32
.11
36
.95
64
.11
70
.47
66
.54
50
.25
son
g
58
.43
77
.64
66
.68
46
.66
68
.40
55
.48
54
.40
70
.19
61
.29
43
.62
43
.62
43
.62
43
.77
25
.88
32
.53
66
.29
58
.76
60
.22
49
.77
zh
ek
ov
a
69
.19
57
.27
62
.67
33
.48
37
.15
35
.22
55
.47
68
.23
61
.20
41
.31
41
.31
41
.31
38
.29
34
.65
36
.38
53
.45
63
.33
54
.79
44
.27
Ta
ble
14
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
cl
os
ed
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
78
.71
72
.33
75
.39
66
.93
63
.91
65
.39
70
.09
71
.49
70
.78
59
.78
59
.78
59
.78
46
.34
49
.62
47
.92
73
.38
79
.00
75
.83
61
.36
Ta
ble
15
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
op
en
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
16
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
ch
an
g
10
0
10
0
10
0
80
.46
84
.75
82
.55
72
.84
74
.57
73
.70
69
.71
69
.71
69
.71
70
.45
60
.75
65
.24
78
.01
76
.57
77
.26
73
.83
Ta
ble
16
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,c
lo
se
d
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
83
.37
10
0
90
.93
74
.79
89
.68
81
.56
67
.46
86
.88
75
.95
70
.73
70
.73
70
.73
77
.75
51
.05
61
.64
76
.65
85
.85
80
.35
73
.05
Ta
ble
17
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,o
pe
n
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.79
68
.34
72
.3
2
63
.29
58
.96
61
.05
68
.84
68
.72
68
.78
57
.28
57
.28
57
.2
8
44
.19
48
.75
46
.3
6
70
.93
76
.58
73
.36
58
.7
3
sap
en
a
95
.27
29
.07
44
.55
56
.99
63
.91
60
.25
62
.89
72
.31
67
.27
53
.90
53
.90
53
.90
45
.22
38
.70
41
.71
69
.71
73
.32
71
.32
56
.41
ch
an
g
69
.88
63
.61
66
.60
58
.48
58
.48
58
.48
67
.42
70
.91
69
.1
2
55
.21
55
.21
55
.21
42
.66
42
.66
42
.66
71
.42
77
.36
73
.9
6
56
.75
nu
gu
es
72
.96
71
.08
72
.01
62
.68
59
.46
61
.03
67
.24
64
.89
66
.04
52
.82
52
.82
52
.82
39
.25
42
.50
40
.81
72
.57
70
.86
71
.68
55
.96
san
tos
70
.39
65
.67
67
.95
61
.28
56
.20
58
.63
69
.25
63
.16
66
.07
50
.47
50
.47
50
.47
36
.51
41
.15
38
.69
73
.92
67
.32
69
.93
54
.46
son
g
59
.24
82
.39
68
.92
54
.92
69
.29
61
.27
60
.89
66
.27
63
.46
46
.97
46
.97
46
.97
44
.49
31
.15
36
.65
69
.73
59
.87
61
.61
53
.79
sto
ya
no
v
74
.43
68
.28
71
.22
67
.18
57
.08
61
.7
2
74
.06
53
.45
62
.09
47
.40
47
.40
47
.40
32
.78
42
.52
37
.02
74
.10
59
.34
61
.31
53
.61
sob
ha
71
.06
65
.06
67
.93
53
.91
52
.64
53
.27
63
.17
66
.14
64
.62
50
.80
50
.80
50
.80
41
.77
43
.03
42
.39
61
.91
69
.15
64
.49
53
.43
ko
bd
an
i
65
.98
63
.83
64
.89
59
.22
54
.81
56
.93
70
.49
63
.12
66
.60
44
.17
44
.14
44
.15
33
.19
36
.50
34
.77
62
.52
64
.25
63
.32
52
.77
zh
ou
64
.11
66
.74
65
.40
48
.00
55
.51
51
.48
57
.18
73
.71
64
.40
48
.40
48
.40
48
.40
44
.18
37
.35
40
.48
61
.54
74
.86
65
.30
52
.12
ch
art
on
71
.01
67
.64
69
.28
59
.24
53
.82
56
.40
67
.10
59
.02
62
.80
48
.91
48
.91
48
.91
35
.96
41
.39
38
.48
70
.65
62
.71
65
.34
52
.56
ya
ng
73
.73
58
.97
65
.53
61
.23
47
.45
53
.47
71
.88
55
.13
62
.40
47
.05
47
.05
47
.05
30
.54
43
.16
35
.77
71
.39
61
.92
64
.83
50
.55
ha
o
66
.79
66
.38
66
.59
59
.55
52
.89
56
.02
68
.27
55
.46
61
.20
45
.95
45
.95
45
.95
30
.76
36
.81
33
.51
73
.22
62
.73
65
.78
50
.24
xin
xin
69
.05
61
.91
65
.28
50
.99
47
.11
48
.97
61
.59
62
.70
62
.14
45
.64
45
.64
45
.64
35
.86
39
.57
37
.62
63
.42
66
.29
64
.68
49
.58
zh
an
g
57
.41
70
.78
63
.40
43
.48
57
.53
49
.53
52
.44
73
.60
61
.24
44
.97
44
.97
44
.97
42
.71
30
.44
35
.55
63
.12
69
.63
65
.53
48
.77
ku
mm
erf
eld
71
.05
58
.01
63
.87
47
.42
40
.44
43
.65
63
.73
57
.39
60
.39
45
.76
45
.76
45
.76
35
.30
42
.72
38
.66
58
.89
61
.77
60
.07
47
.57
zh
ek
ov
a
72
.65
40
.48
51
.99
31
.73
22
.70
26
.46
66
.92
56
.68
61
.37
41
.04
41
.04
41
.04
31
.93
42
.17
36
.34
53
.09
57
.86
54
.22
41
.39
irw
in
17
.58
62
.96
27
.49
12
.69
51
.59
20
.37
34
.88
89
.98
50
.27
31
.71
31
.71
31
.71
46
.13
17
.33
25
.20
51
.51
56
.93
51
.14
31
.95
Ta
ble
18
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.01
69
.43
72
.5
7
64
.40
60
.83
62
.5
7
69
.34
69
.57
69
.4
5
57
.68
57
.68
57
.6
8
44
.15
47
.85
45
.9
2
72
.23
76
.94
74
.3
2
59
.3
1
cai
69
.32
69
.82
69
.57
58
.39
60
.63
59
.49
64
.88
71
.53
68
.04
54
.36
54
.36
54
.36
43
.74
41
.58
42
.64
70
.13
74
.39
72
.01
56
.72
ury
up
ina
72
.10
67
.72
69
.84
60
.74
56
.68
58
.64
66
.43
64
.25
65
.32
52
.00
52
.00
52
.00
38
.87
42
.85
40
.76
69
.43
68
.73
69
.07
54
.91
kle
nn
er
71
.73
67
.14
69
.36
55
.17
57
.04
56
.09
62
.67
70
.69
66
.44
53
.25
53
.25
53
.25
44
.27
42
.39
43
.31
67
.45
75
.92
70
.68
55
.28
irw
in
25
.24
63
.87
36
.18
18
.90
51
.94
27
.71
38
.79
85
.64
53
.40
33
.89
33
.89
33
.89
43
.59
19
.31
26
.76
51
.66
52
.98
51
.80
35
.96
Ta
ble
19
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
17
erence evaluations, that is as expected, given that the
task here includes predicting the underlying men-
tions and mention boundaries, the insistence on ex-
act match, and given that the relatively easier appos-
itive coreference cases are not included in this mea-
sure. The top-performing system (lee) had a score
of 57.79 which is about 1.8 points higher than that
of the second (sapena) and third (chang) ranking
systems, which scored 55.99 and 55.96 respectively.
Another 1.5 points separates them from the fourth
best score of 54.53 (nugues). Thus the performance
differences between the better-scoring systems were
not large, with only about three points separating the
top four systems.
This becomes even clearer if we merge in the re-
sults of systems that participated only in the open
track but that made relatively limited use of outside
resources.21 Comparing that way, the cai system
scores in the same ball park as the second rank sys-
tems (sapena and chang). The uryupina system sim-
ilarly scores very close to nugues?s 54.53
Given that our choice of the official metric was
somewhat arbitrary, if is also useful to look at
the individual metrics, including the mention-based
CEAFm and BLANC metrics that were not part of
the official metric. The lee system which scored
the best using the official metric does slightly worse
than song on the MUC metric, and also does slightly
worse than chang on the B-CUBED and BLANC met-
rics. However, it does much better than every other
group on the entity-based CEAFe, and this is the pri-
mary reason for its 1.8 point advantage in the offi-
cial score. If the CEAFe measure does indicate the
accuracy of entities in the response, this suggests
that the lee system is doing better on getting coher-
ent entities than any other system. This could be
partly due to the fact that that system is primarily
a precision-based system that would tend to create
purer entities. The CEAFe measure also seems to pe-
nalize other systems more harshly than do the other
measures.
We cannot compare these results to the ones ob-
tained in the SEMEVAL-2010 coreference task using
a small portion of OntoNotes data because it was
only using nominal entities, and had heuristically
added singleton mentions to the OntoNotes data22
21The cai system specifically mentions that, and the only re-
source that the uryupina system used outside of the closed track
setting was the Stanford named entity tagger.
22The documentation that comes with the SEMEVAL data
package from LDC (LDC2011T01) states: ?Only nominal
mentions and identical (IDENT) types were taken from the
OntoNotes coreference annotation, thus excluding coreference
5.2 Predicted plus gold mention boundaries
We also explored performance when the systems
were provided with the gold mention boundaries,
that is, with the exact spans (expressed in terms of
token offsets) for all of the NP constituents in the
human-annotated parse trees for the test data. Sys-
tems could use this additional data to ensure that the
output mention spans in their entity chains would not
clash with those in the answer set. Since this was
a secondary evaluation, it was an optional element,
and not all participants ran their systems on this task
variation. The results for those systems that did par-
ticipate in this optional task are shown in Tables 14
(closed track) and 15 (open track).
Most of the better scoring systems did supply
these results. While all systems did slightly better
here in terms of raw scores, the performance was
not much different from the official task, indicating
that mention boundary errors resulting from prob-
lems in parsing do not contribute significantly to the
final output.23
One side benefit of performing this supplemental
evaluation was that it revealed a subtle bug in the
automatic scoring routine that we were using that
could double-count duplicate correct mentions in a
given entity chain. These can occur, for example, if
the system considers a unit-production NP-PRP com-
bination as two mentions that identify the exact same
token in the text, and reports them as separate men-
tions. Most systems had a filter in their processing
that selected only one of these duplicate mentions,
but the kobdani system considered both as potential
mentions, and its developers tuned their algorithm
using that flawed version of the scorer.
When we fixed the scorer and re-evaluated all of
the systems, the kobdani system was the only one
whose score was affected significantly, dropping by
about 8 points, which lowered that system?s rank
from second to ninth. It is not clear how much of
this was owing to the fact that the system?s param-
relations with verbs and appositives. Since OntoNotes is only
annotated with multi-mention entities, singleton referential ele-
ments were identified heuristically: all NPs and possessive de-
terminers were annotated as singletons excluding those func-
tioning as appositives or as pre-modifiers but for NPs in the
possessive case. In coordinated NPs, single constituents as well
as the entire NPs were considered to be mentions. There is no
reliable heuristic to automatically detect English expletive pro-
nouns, thus they were (although inaccurately) also annotated as
singletons.?
23It would be interesting to measure the overlap between the
entity clusters for these two cases, to see whether there was
any substantial difference in the mention chains, besides the ex-
pected differences in boundaries for individual mentions.
18
eters had been tuned using the scorer with the bug,
which double-credited duplicate mentions. To find
out for sure, one would have to re-tune the system
using the modified scorer.
One difficulty with this supplementary evaluation
using gold mention boundaries is that those bound-
aries alone provide only very partial information.
For the roughly 10% of mentions that the automatic
parser did not correctly identify, while the systems
knew the correct boundaries, they had no hierarchi-
cal parser or semantic role label information, and
they also had to further approximate the already
heuristic head word identification. This incomplete
data complicated the systems? task and also compli-
cates interpretation of the results.
5.3 Predicted plus gold mentions
The final supplementary condition that we explored
was if the systems were supplied with the manually-
annotated spans for exactly those mentions that did
participate in the gold standard coreference chains.
This supplies significantly more information than
the previous case, where exact spans were supplied
for all NPs, since the gold mentions list here will also
include verb headwords that are linked to event NPs,
but will not include singleton mentions, which do
not end up as part of any chain. The latter constraint
makes this test seem somewhat artificial, since it di-
rectly reveals part of what the systems are designed
to determine, but it still has some value in quanti-
fying the impact that mention detection has on the
overall task and what the results are if the mention
detection is perfect.
Since this was a logical extension of the task and
since the data was available to the participants for
the development set, a few of the sites did run ex-
periments of this type. Therefore we decided to pro-
vide the gold mentions data to a few sites who had
reported these scores, so that we could compute the
performance on the test set. The results of these ex-
periments are shown in Tables 16 and 17. The results
show that performance does go up significantly, in-
dicating that it is markedly easier for the systems
to generate better entities given gold mentions. Al-
though, ideally, one would expect a perfect mention
detection score, it is the case that one of the two sys-
tems ? lee ? did not get a 100% Recall. This could
possibly be owing to unlinked singletons that were
removed in post-processing.
The lee system developers also ran a further ex-
periment where both gold mentions for the elements
of the coreference chains and also gold annota-
tions for all the other layers were available to the
system. Surprisingly, the improvement in corefer-
ence performance from having gold annotation of
the other layers was almost negligible. This sug-
gests that either: i) the automatic models are pre-
dicting those layers well enough that switching to
gold doesn?t make much difference; ii) information
from the other layers does not provide much lever-
age for coreference resolution; or iii) current coref-
erence models are not capable of utilizing the infor-
mation from these other layers effectively. Given
the performance numbers on the individual layers
cited earlier, (i) seems unlikely, and we hope that
further research in how best to leverage these lay-
ers will result in models that can benefit from them
more definitively.
5.4 Head word based scoring
In order to check how stringent the official, exact
match scoring is, we also performed a relaxed scor-
ing. Unlike ACE and MUC, the OntoNotes data does
not have manually annotated minimum spans that
a mention must contain to be considered correct.
However, OntoNotes does have manual syntactic
analysis in the form of the Treebank. Therefore, we
decided to approximate the minimum spans by using
the head words of the mentions using the gold stan-
dard syntax tree. If the response mention contained
the head word and did not exceed the true mention
boundary, then it was considered correct ? both from
the point of view of mention detection, and corefer-
ence resolution. The scores using this relaxed strat-
egy for the open and closed track submissions using
predicted data are shown in Tables 18 and 19. It
can be observed that the relaxed, head word based,
scoring does not improve performance very much.
The only exception was the klenner system whose
performance increased from 51.77 to 55.28. Over-
all, the ranking remained quite stable, though it did
change for some adjacent systems which had very
close exact match scores.
5.5 Genre variation
In order to check how the systems did on various
genres, we scored their performance per genre as
well. Tables 20 and 21 summarize genre based per-
formance for the closed and open track participants
respectively. System performance does not seem
to vary as much across the different genres as is
normally the case with language processing tasks,
which could suggest that coreference is relatively
genre insensitive, or it is possible that scores are
two low for the difference to be apparent. Compar-
isons are difficult, however, because the spoken gen-
19
MD MUC BCUB Cm Ce BLANC O MD MUC BCUB Cm Ce BLANC O
F F F F F F F F F F F F F F
lee GENRE zhou GENRE
BC 72.2 60.0 66.2 53.9 43.7 71.7 56.7 BC 64.1 49.5 62.1 45.3 38.8 61.8 50.1
BN 72.0 59.0 68.7 57.6 48.7 68.8 58.8 BN 60.8 45.9 64.4 49.5 41.2 66.8 50.5
MZ 70.1 58.0 72.2 61.6 50.9 75.0 60.4 MZ 58.8 44.4 66.9 50.1 41.8 64.6 51.0
NW 65.4 54.3 69.4 56.5 45.5 70.4 56.4 NW 57.7 44.8 65.7 48.7 40.3 63.1 50.2
TC 75.9 66.8 69.5 59.3 41.3 81.6 59.2 TC 69.2 58.1 60.8 43.1 35.7 62.6 51.5
WB 73.0 63.9 65.7 54.2 42.7 73.4 57.5 WB 67.4 55.4 62.8 47.9 39.2 69.1 52.5
sapena charton
BC 48.7 58.8 64.6 50.8 39.4 70.4 54.3 BC 65.8 53.1 59.1 44.6 35.2 64.4 49.1
BN 47.1 60.0 69.1 57.4 45.0 74.3 58.0 BN 65.5 52.0 64.0 50.0 39.6 65.9 51.9
MZ 35.3 59.2 72.3 60.4 48.2 75.0 59.9 MZ 61.7 46.3 64.6 49.7 39.9 64.1 50.3
NW 35.2 57.9 69.7 55.3 41.9 73.8 56.5 NW 57.6 44.6 64.5 48.2 37.7 67.0 48.9
TC 60.4 64.3 63.3 48.3 35.1 68.8 54.2 TC 73.1 66.8 56.2 42.8 29.9 58.1 51.0
WB 46.3 60.1 62.5 49.1 37.4 67.4 53.3 WB 67.6 57.6 59.3 45.1 33.3 66.6 50.0
chang yang
BC 65.5 56.4 67.1 51.5 39.8 71.6 54.4 BC 65.7 53.8 62.3 46.8 35.0 67.5 50.3
BN 66.6 57.4 69.1 56.0 45.6 70.5 57.4 BN 66.0 53.1 63.8 49.1 40.0 63.1 52.3
MZ 61.6 52.7 71.3 57.6 46.4 72.9 56.8 MZ 58.8 43.9 59.7 42.6 32.8 55.5 45.5
NW 61.0 53.3 69.1 54.1 42.1 71.9 54.8 NW 57.2 44.7 62.9 45.3 35.0 62.7 47.6
TC 72.2 68.5 71.4 59.6 37.7 81.7 59.2 TC 74.2 66.8 66.3 55.3 36.0 76.1 56.4
WB 66.4 59.7 66.7 52.7 39.4 74.7 55.3 WB 67.6 57.6 57.0 42.6 32.1 60.1 48.9
nugues hao
BC 71.4 59.2 62.4 48.2 37.2 68.4 52.9 BC 68.9 58.7 58.9 44.8 31.7 64.9 49.8
BN 70.0 58.5 67.4 54.5 43.1 73.1 56.3 BN 62.0 51.1 63.0 46.2 35.5 64.1 49.9
MZ 65.4 53.6 68.6 54.2 42.2 70.1 54.8 MZ 60.3 46.7 61.5 46.3 34.3 61.9 47.5
NW 61.8 51.9 67.0 51.3 39.2 69.4 52.7 NW 57.2 47.7 63.3 45.5 32.9 66.0 48.0
TC 77.2 69.2 63.9 53.0 37.9 72.2 57.0 TC 67.9 60.4 58.8 44.7 30.3 68.3 49.8
WB 72.9 64.2 63.4 51.1 38.5 74.3 55.4 WB 71.4 61.8 55.7 42.6 30.0 64.4 49.2
santos xinxin
BC 66.6 57.2 64.8 48.5 37.2 68.6 53.0 BC 64.8 47.8 60.2 43.9 35.5 65.1 47.9
BN 66.9 57.3 66.9 52.3 41.0 71.8 55.1 BN 61.5 44.7 63.2 47.0 38.9 65.8 48.9
MZ 62.7 51.0 65.9 48.9 37.8 64.5 51.6 MZ 54.6 35.5 64.5 45.7 37.7 61.0 45.9
NW 58.4 49.5 66.2 48.1 37.4 66.9 51.0 NW 54.3 39.5 64.0 45.0 37.5 61.1 47.0
TC 74.2 66.9 65.9 52.5 35.5 72.5 56.1 TC 74.2 62.0 57.9 45.4 33.4 66.5 51.1
WB 70.4 63.2 63.4 49.5 38.2 70.3 55.0 WB 66.9 52.6 58.5 42.2 35.9 63.4 49.0
song zhang
BC 68.9 61.4 61.0 44.1 34.3 59.5 52.2 BC 65.8 50.6 61.1 45.3 35.5 67.3 49.1
BN 66.2 58.4 64.8 49.0 38.2 65.2 53.8 BN 56.3 43.9 61.0 45.8 35.8 66.8 46.9
MZ 63.7 53.4 65.5 49.9 39.0 63.4 52.6 MZ 57.1 35.1 62.2 44.4 36.1 59.4 44.5
NW 62.4 53.6 64.3 48.0 37.2 62.7 51.7 NW 49.9 37.8 61.8 43.2 35.2 59.8 44.9
TC 76.9 74.4 62.0 43.3 33.2 58.1 56.5 TC 75.4 65.9 60.2 46.0 32.1 67.1 52.7
WB 70.0 63.0 60.1 43.3 31.8 60.8 51.6 WB 69.2 55.4 57.4 42.5 34.6 64.7 49.1
stoyanov kummerfield
BC 69.5 59.1 57.6 43.5 34.0 58.7 50.2 BC 66.4 41.5 55.6 41.7 36.2 57.9 44.4
BN 69.2 59.1 65.4 50.4 40.0 65.5 54.8 BN 68.3 48.2 63.4 51.7 44.7 61.6 52.1
MZ 66.7 55.1 65.5 51.0 39.9 63.7 53.5 MZ 58.0 39.9 65.8 51.0 43.4 64.1 49.7
NW 61.8 52.0 63.3 46.2 36.1 62.0 50.5 NW 55.2 41.3 64.7 46.8 37.0 63.5 47.6
TC 72.6 66.6 57.6 42.3 31.0 57.6 51.7 TC 61.8 34.5 51.5 34.7 30.0 54.1 38.7
WB 71.5 63.9 58.3 44.8 33.1 61.1 51.8 WB 68.2 48.1 56.0 44.4 38.6 59.6 47.6
sobha zhekova
BC 68.3 51.7 61.4 47.8 40.4 62.9 51.2 BC 50.5 23.8 60.6 39.4 35.1 53.4 39.8
BN 66.5 51.9 66.5 53.7 45.5 66.3 54.6 BN 51.2 26.0 62.4 42.5 37.5 54.3 42.0
MZ 68.8 54.9 70.3 58.9 49.3 69.8 58.1 MZ 44.0 22.6 63.4 43.3 37.3 56.0 41.1
NW 55.1 43.1 65.8 48.6 39.0 64.9 49.3 NW 39.7 19.4 62.8 41.0 35.8 53.7 39.3
TC 71.5 55.1 57.5 44.2 36.7 60.5 49.7 TC 59.4 31.6 58.2 37.7 33.6 54.1 41.1
WB 70.5 55.7 59.2 46.6 39.8 62.6 51.6 WB 54.1 27.8 58.7 38.5 34.7 53.0 40.4
kobdani irwin
BC 63.2 56.3 65.8 40.6 32.4 61.9 51.5 BC 23.5 16.1 46.0 29.4 23.6 49.8 28.6
BN 63.5 55.7 68.5 46.9 37.5 64.6 53.9 BN 24.9 20.0 49.7 34.2 27.1 52.9 32.3
MZ 57.5 52.2 69.8 45.7 36.4 61.7 52.8 MZ 23.2 17.9 55.9 36.2 28.5 53.0 34.1
NW 52.2 41.7 64.4 43.2 33.7 62.6 46.6 NW 27.5 21.6 56.4 33.9 27.3 52.6 35.1
TC 67.7 60.2 65.3 36.6 28.5 57.6 51.3 TC 28.0 19.3 38.2 24.5 18.7 49.0 25.4
WB 68.7 62.8 62.4 42.5 32.9 64.0 52.7 WB 33.6 24.8 47.6 29.7 23.0 50.2 31.8
Table 20: Detailed look at the performance per genre for the official, closed track using automatic performance. MD
represents MENTION DETECTION; BCUB represents B-CUBED; Cm represents CEAFm; Ce represents CEAFe and O
represents the OFFICIAL score.
20
res were treated here with perfect speech recognition
accuracy and perfect speaker turn information. Un-
der more realistic application conditions, the spread
in performance between genres might be greater.
MD MUC BCUB Cm Ce BLANC O
F F F F F F F
lee GENRE
BC 72.7 61.7 67.0 54.5 43.6 72.7 57.4
BN 72.0 60.6 69.4 57.9 48.1 70.3 59.3
MZ 69.9 58.4 72.1 61.2 50.1 75.2 60.2
NW 65.3 55.8 70.0 56.7 44.9 71.7 56.9
TC 76.6 68.4 70.4 59.6 40.8 82.1 59.9
WB 73.8 65.5 66.2 54.5 42.1 74.2 57.9
cai
BC 69.7 59.1 66.0 50.5 39.9 69.2 55.0
BN 68.6 57.6 67.8 55.4 45.5 68.2 56.9
MZ 64.0 51.1 69.5 55.9 45.6 71.2 55.4
NW 60.3 49.9 67.8 52.7 41.2 69.1 53.0
TC 75.6 70.5 72.2 59.6 38.0 80.3 60.2
WB 71.7 63.9 65.0 51.8 39.8 72.8 56.2
uryupina
BC 70.2 58.3 62.7 48.7 38.0 68.7 53.0
BN 69.0 57.6 66.8 53.6 43.1 69.2 55.8
MZ 65.7 52.4 68.3 54.3 43.6 68.8 54.8
NW 62.6 52.1 68.3 53.2 41.2 71.3 53.9
TC 75.7 67.1 61.0 50.7 34.6 67.1 54.2
WB 72.0 61.7 60.9 48.8 38.3 67.6 53.6
klenner
BC 63.2 50.3 63.4 48.2 38.9 66.8 50.8
BN 63.1 48.6 65.0 51.0 42.6 66.0 52.1
MZ 59.1 43.7 67.1 52.9 45.3 65.0 52.0
NW 55.3 41.3 65.0 48.0 39.6 64.5 48.7
TC 73.9 64.9 67.9 56.4 39.0 78.0 57.3
WB 66.8 58.1 64.0 50.1 39.6 72.7 53.9
irwin
BC 36.6 27.6 50.9 32.0 25.5 50.2 34.7
BN 30.8 24.6 51.9 36.4 28.6 54.8 35.0
MZ 26.1 20.0 57.3 37.6 29.4 54.3 35.6
NW 32.3 24.7 58.4 34.7 27.9 51.1 37.0
TC 46.4 34.3 44.6 29.4 21.9 51.7 33.6
WB 41.7 32.9 50.5 32.9 25.1 53.2 36.2
Table 21: Detailed look at the performance per genre for
the official, open track using predicted information. MD
represents MENTION DETECTION; BCUB represents B-
CUBED; Cm represents CEAFm; Ce represents CEAFe and
O represents the OFFICIAL score.
6 Approaches
Tables 22 and 23 summarize the approaches of the
participating systems along with some of the impor-
tant dimensions.
Most of the systems broke the problem into two
phases, first identifying the potential mentions in the
text and then linking the mentions to form corefer-
ence chains. Most participants also used rule-based
approaches for mention detection, though two did
use trained models. While trained morels seem able
to better balance precision and recall, and thus to
achieve a higher F-score on the mention task itself,
their recall tends to be quite a bit lower than that
achievable by rule-based systems designed to fa-
vor recall. This impacts coreference scores because
the full coreference system has no way to recover
if the mention detection stage misses a potentially
anaphoric mention.
Only one of the participating systems cai at-
tempted to do joint mention detection and corefer-
ence resolution. While it did not happen to be among
the top-performing systems, the difference in perfor-
mance could be due to the richer features used by
other systems rather than to the use of a joint model.
Most systems represented the markable mentions
internally in terms of the parse tree NP constituent
span, but some systems used shared attribute mod-
els, where the attributes of the merged entity are
determined collectively by heuristically merging the
attribute types and values of the different constituent
mentions.
Various types of trained models were used for pre-
dicting coreference. It is interesting to note that
some of the systems, including the best-performing
one, used a completely rule-based approach even for
this component.
Most participants appear not to have focused
much on eventive coreference, those coreference
chains that build off verbs in the data. This usu-
ally meant that mentions that should have linked to
the eventive verb were instead linked in with some
other entity. Participants may have chosen not to fo-
cus on events because they pose unique challenges
while making up only a small portion of the data.
Roughly 91% of mentions in the data are NPs and
pronouns.
In the systems that used trained models, many
systems used the approach described in Soon et al
(2001) for selecting the positive and negative train-
ing examples, while others used some of the al-
ternative approaches that have been introduced in
the research literature more recently. Many of the
trained systems also were able to improve their per-
formance by using feature selection, though things
varied some depending on the example selection
strategy and the classifier used. Almost half of the
trained systems used the feature selection strategy
from Soon et al (2001) and found it beneficial. It is
not clear whether the other systems did not explore
this path, or whether it just did not prove as useful in
their case.
7 Conclusions
In this paper we described the anaphoric coreference
information and other layers of annotation in the
21
Ta
sk
Sy
nta
x
Le
arn
ing
Fra
me
wo
rk
Ma
rka
ble
Ide
nti
fic
ati
on
Ma
rka
ble
Ve
rb
Fe
atu
re
Se
lec
tio
n
#F
eat
ure
s
Tra
ini
ng
lee
C+
O
P
Ru
le-
ba
sed
Ru
les
to
ex
clu
de
Co
pu
lar
co
nst
ruc
tio
n,
Ap
po
sit
ive
s,P
leo
na
sti
ci
t,e
tc.
Fe
atu
re
de
pe
nd
en
t
wi
th
sha
red
att
rib
ute
s
?
?
?
sap
en
a
C
P
De
cis
ion
Tre
e+
Re
lax
ati
on
La
be
lin
g
NP
(m
ax
im
al
spa
n)
+P
RP
+N
E
+C
ap
ita
liz
ed
no
un
he
uri
sti
c
Fu
llp
hra
se
?
?
Tra
in
+D
ev
ch
an
g
C
P
Le
arn
ing
Ba
sed
Jav
a
NP
,N
E,
PR
P,
PR
P$
Fu
llp
hra
se
?
?
Tra
in
+D
ev
cai
O
P
Co
mp
ute
hy
pe
red
ge
we
igh
tso
n
30
%
of
tra
ini
ng
da
ta
NP
,P
RP
,P
RP
$,
Ba
se
ph
ras
ec
hu
nk
s,
Ple
on
ast
ic
it
filt
er
Fu
llp
hra
se
?
?
?
nu
gu
es
C
D
Lo
gis
tic
Re
gre
ssi
on
(LI
BL
IN
EA
R)
NP
,P
RP
$a
nd
seq
ue
nc
eo
fN
NP
(s)
in
po
st
pro
ces
sin
gu
sin
gA
LI
AS
an
dS
TR
IN
GM
AT
CH
He
ad
wo
rd
?
Fo
rw
ard
+B
ack
wa
rd
sta
rtin
gf
rom
So
on
fea
tur
es
et
24
Tra
in
+D
ev
ury
up
ina
O
P
De
cis
ion
Tre
e.
Di
ffe
ren
t
cla
ssi
fie
rs
for
Pro
no
mi
na
la
nd
no
n-P
ron
om
ina
lm
en
tio
ns
NP
,N
E,
PR
P,
PR
P$
,a
nd
rul
es
to
ex
clu
de
som
es
pe
cifi
cc
ase
s
Fu
llp
hra
se
?
Mu
lti-
Ob
jec
tiv
e
Op
tim
iza
tio
no
nt
hre
e
spl
its
.N
SG
A-
II
46
Tra
in
+D
ev
san
tos
C
P
ET
L
(E
ntr
op
yg
uid
ed
Tra
nsf
orm
ati
on
al
Le
arn
ing
)
co
mm
itte
ea
nd
Ra
nd
om
Fo
res
t
(W
EK
A)
Al
lN
Pa
nd
all
pro
no
un
sa
nd
PE
R,
OR
G,
GP
E
in
NP
Fu
llp
hra
se
?
Inh
ere
nt
to
the
cla
ssi
fie
rs
Tra
in
+D
ev
son
g
C
P
Ma
xE
nt
(O
pe
nN
LP
)
Me
nti
on
de
tec
tio
nc
las
sifi
er
Fu
llp
hra
se
?
Sa
me
fea
tur
es
et,
bu
t
pe
rc
las
sifi
er
40
Tra
in
sto
ya
no
v
C
P
Av
era
ge
dp
erc
ep
tro
n
NE
an
dp
oss
ess
ive
sin
ad
dit
ion
to
AC
E
ba
sed
sys
tem
Fu
llp
hra
se
?
?
76
?
sob
ha
C
P
CR
Ff
or
no
n-p
ron
om
ina
la
nd
sal
ien
ce
fac
tor
for
pro
no
mi
na
l
res
olu
tio
n
Ma
ch
ine
lea
rne
dp
leo
na
sti
ci
t,p
lus
NP
,P
RP
,
PR
P$
an
dN
E
Mi
nim
al
(C
hu
nk
/N
E)
an
dM
ax
im
um
spa
n
?
?
Tra
in
kle
nn
er
O
D
Ru
le-
ba
sed
.S
ali
en
ce
me
asu
re
usi
ng
de
pe
nd
en
cie
sg
en
era
ted
fro
m
tra
ini
ng
da
ta
NP
,N
E,
PR
P,
PR
P$
Sh
are
d
att
rib
ute
d/t
ran
sit
ivi
ty
by
usi
ng
av
irtu
al
pro
tot
yp
e
?
?
?
ko
bd
an
i
C
P
De
cis
ion
Tre
e
NP
(no
me
nti
on
of
PR
P$
)
Sta
rtw
ord
,E
nd
wo
rd
an
dH
ead
of
NP
?
Inf
orm
ati
on
ga
in
rat
io
Tra
in
zh
ou
C
P
SV
M
tre
ek
ern
el
usi
ng
BC
po
rtio
n
of
the
da
ta
Ru
le-
ba
sed
;F
ive
rul
es:
PR
P$
,P
RP
,N
E,
sm
all
est
NP
sub
sum
ing
NE
an
dD
ET
+N
P
Fu
llp
hra
se
?
?
17
Tra
in
+D
ev
ch
art
on
C
P
Mu
lti-
lay
er
pe
rce
ptr
on
Ru
les
ba
sed
on
PO
S,
NE
an
dfi
lte
ro
ut
ple
on
ast
ic
it
usi
ng
rul
e-b
ase
dfi
lte
r
Fu
llp
hra
se
?
?
22
Tra
in
ya
ng
C
P
Ma
xE
nt
(M
AL
LE
T)
NP
,P
RP
,P
RP
$,
pre
-m
od
ifie
rs
an
dv
erb
s
Fu
llp
hra
se
?
?
40
Tra
in
+D
ev
ha
o
C
P
Ma
xE
nt
NP
,P
RP
,P
RP
$,
VB
D
ful
lp
hra
se
?
?
Tra
in
+D
ev
xin
xin
C
P
IL
P/I
nfo
rm
ati
on
ga
in
NP
,P
RP
,P
RP
$
Fu
llp
hra
se
?
Inf
orm
ati
on
ga
in
rat
io
65
?
zh
an
g
C
P
SV
M
IO
B
cla
ssi
fic
ati
on
Fu
llp
hra
se
?
?
?
ku
mm
erfi
eld
C
P
Un
sup
erv
ise
dg
en
era
tiv
em
od
el
NP
,P
RP
,P
RP
$w
ith
ma
xim
al
spa
n
Fu
llp
hra
se
?
?
?
zh
ek
ov
a
C
P
TI
M
BL
me
mo
ry
ba
sed
lea
rne
r
NP
,P
rop
er
no
un
s,P
RP
,P
RP
$,
plu
sv
erb
wi
th
pre
dic
ate
lem
ma
He
ad
wo
rd
?
?
Tra
in
+D
ev
irw
in
C+
O
P
Cl
ass
ific
ati
on
-ba
sed
ran
ke
r
NP
,P
RP
,P
RP
$
Sh
are
da
ttri
bu
tes
?
?
?
Ta
ble
22
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rt
I.I
nt
he
Ta
sk
co
lum
n,
C/
O
rep
res
en
ts
wh
eth
er
the
sys
tem
pa
rtic
ipa
ted
in
the
cl
os
ed
,o
pe
n
or
bo
th
tra
ck
s.
In
the
Sy
nta
xc
olu
mn
,a
Pr
ep
res
en
ts
tha
tth
es
yst
em
su
sed
ap
hra
se
str
uc
tur
eg
ram
ma
rr
ep
res
en
tat
ion
of
syn
tax
,w
he
rea
sa
D
rep
res
en
ts
tha
tth
ey
use
da
de
pe
nd
en
cy
rep
res
en
tat
ion
.
22
Po
sit
ive
Tra
ini
ng
Ex
am
ple
s
Ne
ga
tiv
eT
rai
nin
gE
xa
mp
les
De
co
din
g
Pa
rse
Co
nfi
gu
rat
ion
lee
?
?
Mu
lti-
pa
ss
Sie
ve
s
sap
en
a
Al
lm
en
tio
np
air
sa
nd
lon
ge
ro
fn
est
ed
me
nti
on
sw
ith
co
mm
on
he
ad
ke
pt
Me
nti
on
pa
irs
wi
th
les
sth
an
thr
esh
old
(5)
nu
mb
er
of
dif
fer
en
ta
ttri
bu
te
val
ue
sa
re
co
nsi
de
red
(22
%
ou
to
f9
9%
ori
gin
al
are
dis
car
de
d)
Ite
rat
ive
1-b
est
ch
an
g
Cl
ose
sta
nte
ced
en
t
Al
lp
rec
ed
ing
me
nti
on
sin
au
nio
no
fo
fg
ol
d
an
dp
re
di
ct
ed
me
nti
on
s.
Me
nti
on
sw
he
re
the
firs
tis
pro
no
un
an
do
the
rn
ot
are
no
t
co
nsi
de
red
Be
stl
ink
an
dA
lll
ink
ss
tra
teg
y;
wi
th
an
d
wi
tho
ut
co
nst
rai
nts
?B
est
lin
kw
ith
ou
t
co
nst
rai
nts
wa
ss
ele
cte
df
or
the
offi
cia
lru
n
cai
We
igh
tsa
re
tra
ine
do
np
art
of
the
tra
ini
ng
da
ta
Re
cu
rsi
ve
2-w
ay
Sp
ect
ral
clu
ste
rin
g
(A
ga
rw
al,
20
05
)
nu
gu
es
Cl
ose
stA
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Cl
ose
st-
firs
tc
lus
ter
ing
for
pro
no
un
sa
nd
Be
st-
firs
tc
lus
ter
ing
for
no
n-p
ron
ou
ns
1-b
est
ury
up
ina
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
me
nti
on
pa
irm
od
el
wi
tho
ut
ran
kin
ga
sin
So
on
20
01
san
tos
Ex
ten
de
dv
ers
ion
of
So
on
(20
01
)w
he
re
in
ad
dit
ion
to
the
irs
tra
teg
y,
po
sit
ive
an
dn
ega
tiv
e
ex
am
ple
sf
rom
me
nti
on
sin
the
sen
ten
ce
of
the
clo
ses
tp
rec
ed
ing
an
tec
ed
en
ta
re
co
nsi
de
red
Lim
ite
dn
um
be
ro
fp
rec
ed
ing
me
nti
on
s6
0
for
au
tom
ati
ca
nd
40
giv
en
go
ld
bo
un
da
rie
s;
Ag
gre
ssi
ve
-m
erg
ec
lus
ter
ing
(M
cca
rth
ya
nd
Le
nh
ert
,1
99
5)
son
g
Pre
-cl
ust
er
pa
irm
od
els
sep
ara
te
for
eac
hp
air
NP
-N
P,
NP
-PR
Pa
nd
PR
P-P
RP
Pre
-cl
ust
ers
,w
ith
sin
gle
ton
pro
no
un
pre
-cl
ust
ers
,a
nd
use
clo
ses
t-fi
rst
clu
ste
rin
g.
Di
ffe
ren
tli
nk
mo
de
lsb
ase
do
nt
he
typ
eo
f
lin
kin
gm
en
tio
ns
?N
P-P
RP
,P
RP
-PR
Pa
nd
NP
-N
P
sto
ya
no
v
Sm
art
Pa
irG
en
era
tio
n(
Sm
art
PG
)w
he
re
the
typ
eo
fa
nte
ced
en
tis
de
ter
mi
ne
db
yt
he
typ
eo
f
an
ap
ho
ru
sin
ga
set
of
rul
es
Sin
gle
-lin
kc
lus
ter
ing
by
co
mp
uti
ng
tra
nsi
tiv
ec
los
ure
be
tw
een
pa
irw
ise
po
sit
ive
s.
sob
ha
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Pro
no
mi
na
l:a
llp
rec
ed
ing
NP
sin
the
sen
ten
ce
an
dp
rec
ed
ing
4s
en
ten
ces
kle
nn
er
?
?
Inc
rem
en
tal
en
tity
cre
ati
on
ko
bd
an
i
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
.T
hre
sho
ld
of
10
0w
ord
s
use
df
or
lon
gd
oc
um
en
ts
1-b
est
zh
ou
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
?
ch
art
on
Fro
m
the
en
do
fth
ed
oc
um
en
t,u
nti
la
n
an
tec
ed
en
tis
fou
nd
,o
r1
0m
en
tio
ns
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t
ML
Pw
ith
sco
re
of
0.5
use
df
or
lin
kin
ga
nd
10
me
nti
on
s
ya
ng
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Ma
xim
um
23
sen
ten
ces
to
the
lef
t;
Co
nst
rai
ne
dc
lus
ter
ing
ha
o
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
am
sea
rch
(L
uo
,2
00
4)
Pa
ck
ed
for
est
xin
xin
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
fol
low
ed
by
IL
P
op
tim
iza
tio
n
zh
an
g
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Wi
nd
ow
of
10
0m
ark
ab
les
ku
mm
erfi
eld
?
?
Pre
-a
nd
po
st-
res
olu
tio
nfi
lte
rs
Gi
ve
n+
Be
rke
ley
pa
rse
rp
ars
es;
pa
rse
s
wi
tho
ut
NM
Ls
im
pro
ve
dp
erf
orm
an
ce
sli
gh
tly
;re
-tr
ain
ed
Be
rke
ley
pa
rse
r
zh
ek
ov
a
Ex
am
ple
sin
the
pa
stt
hre
es
en
ten
ces
Fro
m
las
tp
oss
ibl
em
en
tio
ni
nd
oc
um
en
t
irw
in
Cl
ust
er
qu
ery
wi
th
NU
LL
clu
ste
rfo
rd
isc
ou
rse
new
me
nti
on
s
Cl
ust
er-
ran
kin
ga
pp
roa
ch
(ra
hm
an
,2
00
9)
Ta
ble
23
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rtI
I.T
his
foc
use
so
nt
he
wa
yp
osi
tiv
ea
nd
ne
ga
tiv
ee
xa
mp
les
we
re
ge
ne
rat
ed
an
dt
he
de
co
din
gs
tra
teg
yu
sed
.
23
OntoNotes corpus, and presented the results from an
evaluation on learning such unrestricted entities and
events in text. The following represent our conclu-
sions on reviewing the results:
? Perhaps the most surprising finding was that the
best-performing system (lee) was completely
rule-based, rather than trained. This suggests
that their rule-based approach was able to do
a more effective job of combining the multiple
sources of evidence than the trained systems.
The features for coreference prediction are cer-
tainly more complex than for many other lan-
guage processing tasks, which makes it more
challenging to generate effective feature com-
binations. The rule-based approach used by
the best-performing system seemed to benefit
from a heuristic that captured the most con-
fident links before considering less confident
ones, and also made use of the information in
the guidelines in a slightly more refined man-
ner than other systems. They also included ap-
positives and copular constructions in their cal-
culations. Although OntoNotes does not count
those as instances of IDENT coreference, using
that information may have helped their system
discover additional useful links.
? It is interesting to note that the developers of
the lee system also did the experiment of run-
ning their system using gold standard informa-
tion on the individual layers, rather than auto-
matic model predictions. The somewhat sur-
prising result was that using perfect informa-
tion for the other layers did not end up improv-
ing coreference performance much, if at all. It
is not clear whether this means that: i) Auto-
matic predictors for the individual layers are
accurate enough already; ii) Information cap-
tured by those supplementary layers actually
does not provide much leverage for resolving
coreference; or iii) researchers have yet have
found an effective way of capturing and utiliz-
ing the extra information provided by these lay-
ers.
? It does seem that collecting information about
an entity by merging information across the
various attributes of the mentions that comprise
it can be useful, though not all systems that at-
tempted this achieved a benefit.
? System performance did not seem to vary as
much across the different genres as is nor-
mally the case with language processing tasks,
which could suggest that coreference is rela-
tively genre insensitive, or it is possible that
scores are two low for the difference to be ap-
parent. Comparisons are difficult, however, be-
cause the spoken genres were treated here with
perfect speech recognition accuracy and perfect
speaker turn information. Under more realis-
tic application conditions, the spread in perfor-
mance between genres might be greater.
? It is noteworthy that systems did not seem to
attempt the kind of joint inference that could
make use of the full potential of various layers
available in OntoNotes, but this could well have
been owing to the limited time available for the
shared task.
? We had expected to see more attention paid to
event coreference, which is a novel feature in
this data, but again, given the time constraints
and given that events represent only a small
portion of the total, it is not surprising that most
systems chose not to focus on it.
? Scoring coreference seems to remain a signif-
icant challenge. There does not seem to be an
objective way to establish one metric in prefer-
ence to another in the absence of a specific ap-
plication. On the other hand, the system rank-
ings do not seem terribly sensitive to the par-
ticular metric chosen. It is interesting that both
versions of the CEAF metric ? which tries to
capture the goodness of the entities in the out-
put ? seem much lower than the other metric,
though it is not clear whether that means that
our systems are doing a poor job of creating
coherent entities or whether that metric is just
especially harsh.
Finally, it is interesting to note that the problem of
coreference does not seem to be following the same
kind of learning curve that we are used to with other
problems of this sort. While performance has im-
proved somewhat, it is not clear how far we will be
able to go given the strategies at hand, or whether
new techniques will be needed to capture additional
information from the texts or from world knowl-
edge. We hope that this corpus and task will provide
a useful resource for continued experimentation to
help resolve this issue.
Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
24
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022.
We would like to thank all the participants. Without
their hard work, patience and perseverance this eval-
uation would not have been a success. We would
also like to thank the Linguistic Data Consortium
for making the OntoNotes 4.0 corpus freely and
timely available to the participants. Emili Sapena,
who graciously allowed the use of his scorer
implementation, and made available enhancements
and immediately fixed issues that were uncovered
during the evaluation. Finally, we offer our special
thanks to Llu??s Ma`rquez and Joakim Nivre for their
wonderful support and guidance without which this
task would not have been successful.
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in synchronizing the English
treebank and propbank. In Workshop on Frontiers in
Linguistically Annotated Corpora 2006, July.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 33?40, Sydney,
Australia, July.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of the 11th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
?10, pages 28?36.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the Second Meeting of North American Chapter
of the Association of Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL), Ann
Arbor, MI, June.
Nancy Chinchor and Beth Sundheim. 2003. Message
understanding conference (MUC) 6. In LDC2003T13.
Nancy Chinchor. 2001. Message understanding confer-
ence (MUC) 7. In LDC2001T02.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order probabilistic models for
coreference resolution. In HLT/NAACL, pages 81?88.
Pascal Denis and Jason Baldridge. 2007. Joint de-
termination of anaphoricity and coreference resolu-
tion using integer programming. In Proceedings of
HLT/NAACL.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
(42):87?96.
Charles Fillmore, Christopher Johnson, and Miriam R. L.
Petruck. 2003. Background to framenet. Interna-
tional Journal of Lexicography, 16(3).
G. G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassell, and R. Weischedel.
2000. The automatic content extraction (ACE)
program-tasks, data, and evaluation. In Proceedings
of LREC.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1?18, Boulder, Colorado, June.
Sanda M. Harabagiu, Razvan C. Bunescu, and Steven J.
Maiorano. 2001. Text and knowledge mining for
coreference resolution. In NAACL.
L. Hirschman and N. Chinchor. 1997. Coreference task
definition (v3.0, 13 jul 97). In Proceedings of the Sev-
enth Message Understanding Conference.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT/NAACL,
pages 57?60, New York City, USA, June. Association
for Computational Linguistics.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2000. A large-scale classification of
english verbs. Language Resources and Evaluation,
42(1):21 ? 40.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
25
Methods in Natural Language Processing, pages 25?
32, Vancouver, British Columbia, Canada, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330, June.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems (NIPS).
Joseph McCarthy and Wendy Lehnert. 1995. Using de-
cision trees for coreference resolution. In Proceedings
of the Fourteenth International Conference on Artifi-
cial Intelligence, pages 1050?1055.
Thomas S. Morton. 2000. Coreference for nlp applica-
tions. In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics, Octo-
ber.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the IJCAI.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
R. Passonneau. 2004. Computing reliability for corefer-
ence annotation. In Proceedings of LREC.
Massimo Poesio and Ron Artstein. 2005. The reliability
of anaphoric annotation, reconsidered: Taking ambi-
guity into account. In Proceedings of the Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
Massimo Poesio. 2004. The mate/gnome scheme for
anaphoric annotation, revisited. In Proceedings of
SIGDIAL.
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art nlp approaches to coreference resolu-
tion: Theory and practical recipes. In Tutorial Ab-
stracts of ACL-IJCNLP 2009, page 6, Suntec, Singa-
pore, August.
Simone Paolo Ponzetto and Michael Strube. 2005. Se-
mantic role labeling for coreference resolution. In
Companion Volume of the Proceedings of the 11th
Meeting of the European Chapter of the Associa-
tion for Computational Linguistics, pages 143?146,
Trento, Italy, April.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the HLT/NAACL, pages 192?199, New York City,
N.Y., June.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal, 60(1):11?39.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007a. OntoNotes: A Unified Relational Semantic
Representation. International Journal of Semantic
Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007b.
Unrestricted Coreference: Indentifying Entities and
Events in OntoNotes. In in Proceedings of the
IEEE International Conference on Semantic Comput-
ing (ICSC), September 17-19.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977, Singapore, Au-
gust. Association for Computational Linguistics.
W. M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66(336).
Marta Recasens and Eduard Hovy. 2011. Blanc: Im-
plementing the rand index for coreference evaluation.
Natural Language Engineering.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 1?8,
Uppsala, Sweden, July.
W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun phrase.
Computational Linguistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
26
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August.
Yannick Versley. 2007. Antecedent selection techniques
for high-recall coreference resolution. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model theoretic coreference
scoring scheme. In Proceedings of the Sixth Message
Undersatnding Conference (MUC-6), pages 45?52.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog
no.: LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue. 2011. OntoNotes: A
Large Training Corpus for Enhanced Processing. In
Joseph Olive, Caitlin Christianson, and John McCary,
editors, Handbook of Natural Language Processing
and Machine Translation. Springer.
27
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 1?11,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Question Ranking and Selection in Tutorial Dialogues
Lee Beckera and Martha Palmer 2a and Sarel van Vuuren 3a and Wayne Ward 4a,b
aThe Center for Computational Language and Education Research (CLEAR)
University of Colorado Boulder
bBoulder Language Technologies
{lee.becker,martha.palmer,sarel.vanvuuren}@colorado.edu
wward@bltek.com
Abstract
A key challenge for dialogue-based intelligent
tutoring systems lies in selecting follow-up
questions that are not only context relevant
but also encourage self-expression and stimu-
late learning. This paper presents an approach
to ranking candidate questions for a given di-
alogue context and introduces an evaluation
framework for this task. We learn to rank us-
ing judgments collected from expert human
tutors, and we show that adding features de-
rived from a rich, multi-layer dialogue act
representation improves system performance
over baseline lexical and syntactic features to
a level in agreement with the judges. The ex-
perimental results highlight the important fac-
tors in modeling the questioning process. This
work provides a framework for future work
in automatic question generation and it rep-
resents a step toward the larger goal of di-
rectly learning tutorial dialogue policies di-
rectly from human examples.
1 Introduction
Socratic tutoring styles place an emphasis on elicit-
ing information from the learner to help them build
their own connections to the material. The role of a
tutor in a Socratic dialogue is to scaffold the material
and present questions that ultimately lead the student
to an ?A-ha!? moment. Numerous studies have il-
lustrated the effectiveness of Socratic-style tutoring
(VanLehn et al, 2007; Rose et al, 2001; Collins and
Stevens, 1982); consequently recreating the behav-
ior on a computer has long been a goal of research
in Intelligent Tutoring Systems (ITS). Recent suc-
cesses have shown the efficacy of conversational ITS
(Graesser et al, 2005; Litman and Silliman, 2004;
Ward et al, 2011b), however these systems are still
not as effective as human tutors, and much improve-
ment is needed before they can truly claim to be So-
cratic. Furthermore, development and tuning of tu-
torial dialogue behavior requires significant human
effort.
While our overarching goal is to improve ITS
by automatically learning tutorial dialogue strategies
directly from expert tutor behavior, we focus on the
crucial subtask of selecting follow-up questions. Al-
though asking questions is only a subset of the over-
all tutoring process, it is still a complex process that
requires understanding of the dialogue state, the stu-
dent?s ability, and the learning goals.
This work frames question selection as a task of
scoring and ranking candidate questions for a spe-
cific point in the tutorial dialogue. Since dialogue
is a dynamic process with multiple correct possibil-
ities, we do not restrict ourselves only to the moves
and questions found in a corpus of transcripts. In-
stead we posit ?What if we had a fully automatic
question generation system?? and subsequently use
candidate questions hand-authored for each dialogue
context. To explore the mechanisms involved in
ranking follow-up questions against one other, we
pair these questions with judgments of quality from
expert human tutors and extract surface form and
dialogue-based features to train machine learning
classification models to rank the appropriateness of
questions for specific points in a dialogue.
Our results show promise with our best question
1
ranking models exhibiting performance on par with
expert human tutors. Furthermore these experiments
demonstrate the utility and importance of rich dia-
logue move annotation for modeling decision mak-
ing in conversation and tutoring.
2 Background and Related Works
Learning tutorial dialogue policies from corpora is
a growing area of research in natural language pro-
cessing and intelligent tutoring systems. Past studies
have made use of hidden Markov models (Boyer et
al., 2009a) and reinforcement learning (Chi et al,
2010; Chi et al, 2009; Chi et al, 2008) to discover
tutoring strategies. However, these approaches are
typically optimized to maximize learning gains, and
are not necessarily focused on replicating human tu-
tor behavior. Other work has explored specific fac-
tors in questioning such as when to ask ?why? ques-
tions (Rose et al, 2003), provide hints (Tsovaltzi
and Matheson, 2001), or insert discourse markers
(Kim et al, 2000).
There is also an expanding body of work that ap-
plies ranking algorithms toward the task of ques-
tion generation (QG) using approaches such as over-
generation-and-ranking (Heilman and Smith, 2010),
language model ranking (Yao, 2010), and heuristics-
based ranking (Agarwal and Mannem, 2011). While
the focus of these efforts centers on issues of gram-
maticality, fluency, and content selection for auto-
matic creation of standalone questions, we move to
the higher level task of choosing context appropri-
ate questions. Our work merges aspects of these
QG approaches with the sentence planning tradi-
tion from natural language generation (Walker et al,
2001; Rambow et al, 2001). In sentence planning
the goal is to select lexico-structural resources that
encode communicative action. Rather than select-
ing representations, we use them directly as part of
the feature space for learning functions to rank the
questions? actual surface form realization. To our
knowledge there has been no research in ranking the
quality and suitability of questions within a tutorial
dialogue context.
Because questioning tactics depend heavily on the
curriculum and choice of pedagogy, we ground our
investigations within the context of the My Science
Tutor (MyST) intelligent tutoring system (Ward et
al., 2011b), a conversational virtual tutor designed
to improve science learning and understanding for
students in grades 3-5 (ages 8-11). Students using
MyST investigate and discuss science through nat-
ural spoken dialogues and multimedia interactions
with a virtual tutor named Marni. The MyST dia-
logue design and tutoring style is based on a ped-
agogy called Questioning the Author (QtA) (Beck
et al, 1996) which emphasizes open-ended ques-
tions and keying in on student language to promote
self-explanation of concepts, and its curriculum is
based on the Full Option Science System (FOSS) 1
a proven system for inquiry based learning.
3 Data Collection
3.1 MyST Logfiles and Transcripts
For these experiments, we use MyST transcripts col-
lected in a Wizard-of-Oz (WoZ) condition with a hu-
man tutor inserted into the interaction loop. Project
tutors trained in both QtA and in the tutorial sub-
ject matter served as the wizards. During a ses-
sion tutors were responsible for accepting, overrid-
ing, and/or authoring system actions. Tutor wizards
were also responsible for setting the current dialogue
frame to indicate which of the learning goals was
currently in focus. Students talked to MyST via mi-
crophone while MyST communicates using Text-to-
Speech (TTS) in the WoZ setting. A typical MyST
session revolves around a single FOSS lesson and
lasts approximately 15 minutes. To obtain a dia-
logue transcript, tutor moves are taken directly from
the system logfile, while student speech is manu-
ally transcribed from audio. In addition to the di-
alogue text, MyST records additional information
such as timestamps and the current dialogue frame
(i.e. learning goal). In total we make use of tran-
scripts from 122 WoZ dialogues covering 10 units
on magnetism and electricity and 2 in measurement
and standards.
3.2 Dialogue Annotation
Lesson-independent analysis of dialogue requires
a level of abstraction that reduces a dialogue to
its underlying actions and intentions. To address
this need we use the Dialogue Schema Unifying
Speech and Semantics (DISCUSS) (Becker et al,
1http://www.fossweb.com
2
2011), a multidimensional dialogue move taxon-
omy that captures both the pragmatic and seman-
tic interpretation of an utterance. Instead of us-
ing one label, a DISCUSS move is a tuple com-
posed of three dimensions: Dialogue Act, Rhetor-
ical Form, Predicate Type. Together these labels
account for the action, function, and content of an
utterance. This scheme draws from past work in
task-oriented dialogue acts (Bunt, 2009; Core and
Allen, 1997), tutorial act taxonomies (Pilkington,
1999; Tsovaltzi and Karagjosova, 2004; Buckley
and Wolska, 2008; Boyer et al, 2009b) discourse
relations (Mann and Thompson, 1986) and question
taxonomies (Graesser and Person, 1994; Nielsen et
al., 2008).
Dialogue Act (22 tags): The dialogue act dimen-
sion is the top-level dimension in DISCUSS, and its
values govern the possible values for the other di-
mensions. Though the DISCUSS dialogue act layer
seeks to replicate the learnings from other well-
established taxonomies like DIT++ (Bunt, 2009) or
DAMSL (Core and Allen, 1997) wherever possible,
the QtA style of pedagogy driving our tutoring ses-
sions dictated the addition of two tutorial specific
acts: marking and revoicing. A mark act highlights
key words from the student?s speech to draw atten-
tion to a particular term or concept. Like with mark-
ing, revoicing keys in on student language, but in-
stead of highlighting specific words, a revoice act
will summarize or refine the student?s language to
bring clarity to a concept.
Rhetorical Form (22 tags): Although the dia-
logue act is useful for identifying the speaker?s in-
tent, it gives no indication of how the speaker is ad-
vancing the conversation. The rhetorical form re-
fines the dialogue act by providing a link to its sur-
face form realization. Consider the questions ?What
is the battery doing?? and ?Which one is the bat-
tery??. They would both be labeled with Ask dia-
logue acts, but they elicit two very different kinds
of responses. The former, which elicits some form
of description, would be labeled with a Describe
rhetorical form, while the latter is seeking to Iden-
tify an object. Similarly an Assert act from a tutor
could be coupled with a Describe rhetorical form to
introduce new information or with a Recap to recon-
vey a major point.
Predicate Type (19 tags): Beyond knowing the
Reliability Metric DA RF PT
Cohen?s Kappa 0.75 0.72 0.63
Exact Agreement 0.80 0.66 0.56
Partial Agreement 0.89 0.77 0.68
Table 1: Inter-annotator agreement for DISCUSS types
(DA=Dialogue Act, RF=Rhetorical Form, PT=Predicate
Type)
propositional content of an utterance, it is useful to
know how the entities and predicates in a response
relate to one another. A student may mention several
keywords that are semantically similar to the learn-
ing goals, but it is important for a tutor to recognize
whether the student?s language provides a deeper de-
scription of some phenomena or if it is simply a su-
perficial observation. The Predicate Type aims to
categorize the semantic relationships a student may
talk about; whether it is a Procedure, a Function, a
Causal Relation, or some other predicate type.
3.2.1 Annotation
All transcripts used in this experiment have been
annotated with DISCUSS labels at the turn level. A
reliability study using 15% of the transcripts was
conducted to assess inter-rater agreement of DIS-
CUSS tagging. This consisted of 18 doubly anno-
tated transcripts comprised of 828 dialogue utter-
ances.
To assess inter-rater reliability we use Cohen?s
Kappa (?) (Carletta, 1996). Because DISCUSS per-
mits multiple labels per instance, we compute a ?
value for each label and provide a mean for each
DISCUSS dimension. To get an additional sense of
agreement, we use two other metrics: exact agree-
ment and partial agreement. For each of these met-
rics, we treat each annotators? annotations as a per
class bag-of-labels. For exact agreement, each an-
notators? set of labels must match exactly to receive
credit. Partial agreement is defined as the number
of intersecting labels divided by the total number
of unique labels. Together these statistics help to
bound the reliability of the DISCUSS annotation.
Table 1 lists all three metrics broken down by DIS-
CUSS dimension. The ? values show fair agreement
for the dialogue act and rhetorical form dimensions,
whereas the predicate type shows more moderate
agreement. This difference reflects the relative diffi-
3
culty in labeling each dimension, and the agreement
as a whole illustrates the open-endedness of the task.
3.3 Question Authoring
While the long-term plan for this work is to inte-
grate fully automatic question generation into a tu-
toring system, for this study we opted to use manu-
ally authored questions. This allows us to remain
focused on learning to identify context appropri-
ate questions rather than confounding our experi-
ments with issues of question grammaticality and
well-formedness. Even though using multiple au-
thors would provide greater diversity of questions,
to avoid repeated effort and to maintain consistency
in authoring we trained a single question author
in both the FOSS material and MyST QtA tech-
niques. Although he was free to author any ques-
tion he found appropriate, our guidelines primar-
ily emphasized authoring by making permutations
aligned with DISCUSS dimensions while also per-
mitting the author to incorporate changes in word-
ing, learning-goal content, and tutoring tactics. For
example, we taught him to consider how QtA moves
such as Revoicing, Marking, or Recapping could al-
ter otherwise similar questions. To minimize the risk
of rater bias, we explicitly told our author to avoid
using positive feedback expressions such as ?Good
job!? or ?Great!?. Table 2 illustrates how the com-
binations of DISCUSS labels, QtA tactics, and dia-
logue context drives the question generation process.
To simulate the conditions available to both the
human WoZ and computer MyST tutors, the author
was presented with the entire dialogue history pre-
ceding the decision point, the current dialogue frame
(learning goal), and any visuals that may be on-
screen. Question authoring contexts were manually
selected to capture points where students provided
responses to tutor questions. This eliminated the
need to account for other dialogue behavior such as
greetings, closings, or meta-behavior, and allowed
us to focus on follow-up style questions. Because
these question authoring contexts came from actual
tutorial dialogues, we also extracted the original turn
provided by the tutor, and we filtered out turns that
did not contain questions related to the lesson con-
tent. Our corpus has 205 question authoring contexts
comprised of 1025 manually authored questions and
131 questions extracted from the original transcript
yielding 1156 questions in total.
3.4 Ratings Collection
To rate questions, we enlisted the help of four tu-
tors who had previously served as project tutors and
wizards. The raters were presented with much of
the same information used during question author-
ing. The interface included the entire dialogue his-
tory preceding the question decision point and a list
of up to 6 candidate questions (5 manually authored,
1 taken from the original transcript if applicable). To
give a more complete tutoring context, raters also
had access to the lessons? learning goals and the in-
teractive visuals used by MyST.
Previous studies in rating questions (Becker et al,
2009) have found poor inter-rater agreement when
rating questions in isolation. To decrease the task?s
difficulty we instead ask raters to simultaneously
score all candidate questions. Because we did not
want to bias raters, we did not specify specific cri-
teria for question quality. Instead we instructed the
raters to consider the question?s role in assisting stu-
dent understanding of the learning goals and to think
about factors such as tutorial pacing, context appro-
priateness, and content. Scores were collected us-
ing an ordinal 10-point scale ranging from 1 (low-
est/worst) to 10 (highest/best).
Each set of questions was rated by at least three
tutors, and rater assignments were selected to ensure
raters never score questions from sessions they tu-
tored themselves. In total we collected ratings for
1156 question representing a total of 205 question
contexts distributed across 30 transcripts.
3.4.1 Rater Agreement
Because these judgments are subjective, a key
challenge in this work centers on understanding to
what degree the tutors agree with one another. Since
our goal is to rank questions and not to score ques-
tions, we convert each tutors scores for a given con-
text into a rank-ordered list. To compute inter-
rater agreement in ranking, we use Kendall?s-Tau
(? ) rank correlation coefficient. This measure is a
non-parametric statistic that quantifies the similarity
in orderings of data, and it is closely tied to AUC,
the area under the receiver operating characteristics
(ROC) curve. Though Kendall?s-? can vary from -1
to 1, its value is highly task dependent, and it is typ-
4
. . .
T: Tell me more about what is happening with the electricity in a complete circuit.
S: Well the battery sends all the electricity in a circuit to the motor so the motor starts to go.
Candidate Question Frame Element DISCUSS
Q1 Roll over the switch and then in your own
words, tell me again what a complete or
closed circuit is all about.
Same Same Direct/Task/Visual
Ask/Describe/Configuration
Q2 How is this circuit setup? Is it open or closed? Same Same Ask/Select/Configuration
Q3 To summarize, a closed circuit allows the
electricity to flow and the motor to spin. Now
in this circuit, we have a new component. The
switch. What is the switch all about?
Diff Diff Assert/Recap/Proposition
Direct/Task/Visual
Ask/Describe/Function
Q4 You said something about the motor spinning
in a complete circuit. Tell me more about that.
Same Same Revoice/None/None
Ask/Elaborate/CausalRelation
Table 2: Example dialogue context snippet and a collection of candidate questions. The frame, element, and DISCUSS
columns show how the questions vary from one another.
ically lower when the range of possible choices is
narrow as it is in this task. To get a single score we
average ? values across all sets of questions (con-
texts) and all pairs of raters. The mean value for all
pairs of raters and contexts is ? = 0.1478. The inter-
rater statistics are shown in table 3. While inter-rater
agreement is fairly modest, we do see lots of vari-
ation between different pairs of tutors. Addition-
ally, we found that a pair of raters agreed on the top
rated question 33% of the time. This suggests that
despite their common training and experience, the
raters may be using different criteria in rating.
To assess the tutors? internal consistency, we had
each tutor re-rate 60 sets of questions approximately
two months after their first trial, and we computed
self-agreement Kendall?s-? values using the method
above. These statistics are listed in the bottom row
of table 3. In contrast with the inter-rater agreement,
self-agreement is much more consistent giving fur-
ther evidence for a difference in criteria. Together
self and inter-rater agreement help bound expected
system performance in ranking.
4 Automatic Ranking
Because we are more interested in learning to pre-
dict which questions are more suitable for a given
tutoring scenario than we are in assigning specific
scores to questions, we approach the task of ques-
tion selection as a ranking task. To create a gold-
rater A rater B rater C rater D
rater A X 0.2590 0.1418 0.0075
rater B 0.2590 X 0.1217 0.2370
rater C 0.1418 0.1217 X 0.0540
rater D 0.0075 0.2370 0.0540 X
mean 0.1361 0.2059 0.1058 0.0995
self 0.4802 0.4022 0.2327 0.3531
Table 3: Inter-rater rank agreement (Kendall?s-? ). The
bottom row is the self-agreement for contexts they rated
in two separate trials.
standard for training and evaluation we first need to
convert the collective ratings for a set of questions
into a rank-ordered list. While the most straight-
forward way to make this conversion is to average
the ratings for each item, this approach assumes all
raters operate on the same scale. Furthermore, a sin-
gle score does not account for how a question re-
lates to other candidate questions. Instead we create
a single rank-order by tabulating pairwise wins for
all pairs of questions qi, qj , (i 6= j) within a given
dialogue context C. If rating(qi) > rating(qj),
questions qi receives a win. This is summed across
all raters for the context. The question(s) with the
most wins has rank 1. Questions with an equal num-
ber of wins are considered tied and are given the av-
erage ranking of their ordinal positions. For exam-
ple if two questions are tied for second place, they
5
are each assigned a ranking of 2.5.
Using this rank-ordering we then train a pairwise
classifier to learn a preferences function (Cohen et
al., 1998) that determines if one question has a bet-
ter rank than another. For each question qi within a
contextC, we construct a vector of features ?i. For a
pair of questions qi and qj , we then create a new vec-
tor using the difference of features: ?(qi, qj , C) =
?i ? ?j . For training, if rank(qi) < rank(qj), the
classification is positive otherwise it is negative. To
account for the possibility of ties, and to make the
difference measure appear symmetric, we train both
combinations (qi, qj) and (qj , qi). During decoding,
we run the trained classifier on all pairs and tabulate
wins using the approach described above.
For our experiments we train pairwise classi-
fiers using Mallet?s Maximum Entropy (McCallum,
2002) and SVMLight?s Support Vector Machines
models (Joachims, 1999). We also use SVMRank
(Joachims, 1999), which performs the same max-
imum margin separation as SVMLight, but uses
Kendall?s-? as a loss function to optimize for rank
ordering. We run SVMRank with a linear kernel
and model parameters of c = 2.0 and  = 0.0156.
For MaxEnt, we use Mallet?s default model param-
eters. Training and evaluation are carried out us-
ing 10-fold cross validation (3 transcripts per fold,
approximately 7 dialogue contexts per transcript).
Folds are partitioned by FOSS unit, to ensure train-
ing and evaluation are on different lessons. To ex-
plore the impact of DISCUSS representations on this
question ranking task, we train and evaluate models
by incrementally adding additional information ex-
tracted from the DISCUSS annotation.
4.1 Features
When designing features for this task, we wanted to
capture the factors that may play a role in the tutor?s
decision making process during question selection.
When rating, scorers may consider factors such as
the question?s surface form, lesson relevance, con-
textual relevance. The subsections below detail the
motivations and intuitions behind these factors.
4.1.1 Surface Form Features
When presented with a list of questions, a rater
likely bases the decision on his or her initial reaction
to the questions? wording. In some cases, wording
may supercede any other decisions regarding edu-
cational value or dialogue cohesiveness. Question
verbosity is captured by the number of words in the
question feature. Analysis of rater comments also
suggested that preferences are often tied to the ques-
tion?s form and structure. A rough measure of form
comes from the Wh-word features to mark the pres-
ence of the following question words: who, what,
why, where, when, which, and how. Additionally we
use the bag-of-part-of-speech-tags (POS) features to
provide another aspect of the question?s structure.
4.1.2 Lexical Similarity Features
Past work (Ward et al, 2011a) has shown that en-
trainment, the process of automatic alignment be-
tween dialogue partners, is a useful predictor of
learning and is a key factor in facilitating a success-
ful conversation. For question selection, we hypoth-
esize that successful tutors ask questions that dis-
play some degree of semantic entrainment with stu-
dent utterances. In MyST-based tutoring, dialogue
actions are driven by the goal of eliciting student re-
sponses that address the learning goals for the les-
son. Consequently, choosing an appropriate ques-
tion may depend on how closely student responses
align with the learning goals. To model both en-
trainment and lexical similarity we extract features
for unigram and bigram overlap of words, word-
lemmas, and part-of-speech tags between the pairs
below.
? The candidate question and the student?s last
utterance
? The candidate question and the last tutor?s ut-
terance
? The candidate question and the text of the cur-
rent learning goal
? The candidate question and the text of the other
learning goals
Example learning goals for a lesson on circuits are
provided in table 4. The current learning goal is sim-
ply the learning goal in focus at the point of question
asking according to the MyST logfile. Other learn-
ing goals are all other goals for the lesson. Using
the example from the table, if goal 2 is the current
learning goal, then goals 1 and 3 are the other goals.
6
Goal 1: Wires carry electricity and can connect
components
Goal 2: Bulb receives electricity and transforms
electricity into heat
Goal 3: A circuit provides a pathway for energy
to flow
Table 4: Example learning goals
4.1.3 DISCUSS Features
The lexical and surface form features provide
some cues about the content of the question, but
they do not account for the action or intent in tutor-
ing. The DISCUSS annotation allows us to bridge
between the question?s semantics and pragmatically
and focus on what differentiates one question from
another. Basic DISCUSS features include bags of
Dialogue Acts (DA), Rhetorical Forms (RF), and
Predicate types (PT) found in the question?s DIS-
CUSS annotation. We capture the question?s dia-
logue cohesiveness with binary features indicating
whether or not the question?s RF and PT match those
found in the previous student and tutor turns.
4.1.4 Contextualized DISCUSS Features
In tutoring, follow-up questions are licensed by
the questions that precede them. For example a tutor
may be less likely to ask how an object functions un-
til after the object has first been identified by the stu-
dent. Along a different dimension, a tutor?s line of
questioning may change to match a student?s under-
standing of the material. Struggling students may re-
quire additional opportunities to explain themselves,
while advanced students may benefit more from a
more rapid pace of instruction.
We model the conditional relevance of moves
by computing dialogue act transition probabilities
from our corpus of DISCUSS annotated tutorial di-
alogues. Although DISCUSS allows multiple tags
per dialogue turn, we simplify probability calcula-
tions by treating each DISCUSS tuple as a separate
event, and tallying all pairs of turn-turn labels. A
DISCUSS tuple consists of a Dialogue Act (DA),
Rhetorical Form (RF), and Predicate Type (PT),
and we use different subsets of the tuple to com-
pute the transition probabilities listed in equations 1-
3. All probabilities are computed using Laplace-
smoothing. When extracting features, we sum the
log of the probabilities for each DISCUSS label
present in the question.
MyST models dialogue as a sequence of seman-
tic frames which correspond to specific learning
goals. For natural language understanding, MyST
uses Phoenix semantic grammars (Ward, 1994) to
identify which elements within these frames have
been filled. To account for student progress in ques-
tion asking, we compute the conditional probabil-
ity of a DISCUSS label given the percentage of el-
ements filled in the current dialogue frame (equa-
tion 4). This progress percentage is discretized into
bins of 0-25%, 25-50%, 50-75%, and 75-100%.
p(DA,RF, PTquestion|DA,RF, PTstud. turn) (1)
p(DA,RFquestion|DA,RFstudent turn) (2)
p(PTquestion|PTstudent turn) (3)
p(DA,RF, PTques.|% elements filled) (4)
4.2 Evaluation
To evaluate our systems? performance in ranking,
we use two measures commonly used in information
retrieval: the Mean Kendall?s-? measure described
in section 3.4.1 and Mean Reciprocal Rank (MRR).
MRR is the average of the multiplicative inverse of
the rank of the highest ranking question across all
contexts. To account for ties we use the Tau-b vari-
ant of Kendall?s-? , and for MRR we compute re-
ciprocal rank by averaging the system rankings for
all of the questions tied for first. To obtain a gold-
standard ranking for comparison, we combine indi-
vidual raters? ratings using the approached described
in section 4.
5 Results and Discussion
We trained several models to investigate how differ-
ent feature classes influence overall performance in
ranking. The results for these experiments are listed
in Table 5. Because we found comparable perfor-
mance between MaxEnt and SVMLight, we only
report results for MaxEnt and SVMRank models.
In addition to MRR and Kendall?s-? , we list the
number of concordances and discordances in pair-
wise classification to give the reader another sense
of the accuracy associated with rank agreement.
Random Baseline: On average, assigning ran-
dom ranks will yield mean ?=0 and MRR=0.408.
7
Model Features Mean Num. Num. Pairwise MRR
Kendall?s-? Concord. Discord. Accuracy
MaxEnt CONTEXT+DA+PT+MATCH+POS- 0.211 1560 974 0.616 0.516
SVMRank CONTEXT+DA+PT+MATCH+POS- 0.190 1725 1154 0.599 0.555
MaxEnt CONTEXT+DA+RF+PT+MATCH+POS- 0.185 1529 1014 0.601 0.512
MaxEnt DA+RF+PT+MATCH+POS- 0.179 1510 1009 0.599 0.503
MaxEnt DA+RF+PT+MATCH+ 0.163 1506 1044 0.591 0.485
MaxEnt DA+RF+PT+ 0.147 1500 1075 0.583 0.480
MaxEnt DA+RF+ 0.130 1458 1082 0.574 0.476
MaxEnt DA+ 0.120 1417 1076 0.568 0.458
SVMRank Baseline 0.108 1601 1278 0.556 0.473
MaxEnt Baseline 0.105 1410 1115 0.558 0.448
Table 5: System scores by feature set and and machine learning model. Presence or absence of specific features is
denoted with a ?+? or ?-? otherwise the label refers to a set of features. The Baseline features consist of the Surface Form
and Lexical Similarity features described in sections 4.1.1 and 4.1.2. POS are the bag-of-POS surface form features.
DA, RF, and PT refer to the DISCUSS presence features for the Dialogue Act, Rhetorical Form, and Predicate Type
dimensions described in section 4.1.3. MATCH refers specifically to the RF and PT match features. CONTEXT
refers to the Contextualized DISCUSS features described in section 4.1.4. The best scores for each column appear in
boldface.
-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.00
10
20
30
40
50
Freq
uen
cy
?mean=0.211
-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.0Kendall's Tau(?) Range
0
10
20
30
40
50
Freq
uen
cy
?mean=0.105
Figure 1: Distribution of per-context Kendall?s-? values
for the top-scoring system (top), and the baseline system
(bottom).
Baseline System: Our baseline system used all
of the surface form and lexical similarity features
described above. This set of features achieves the
highest rank agreement (? = 0.105) using max-
imum entropy and the highest MRR (0.473) with
SVMRank . This improvement over the random
baseline suggests there is a correlation between a
question?s ranking and its surface form.
DISCUSS System: Table 5 shows system per-
formance steadily improves as additional DISCUSS
features are included in the model. When us-
1 2 3 4 5 6 70.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
p(R
ank
)
1/MRR=1.80
1 2 3 4 5 6 7Mean System Rank
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
p(R
ank
)
1/MRR=2.11
Figure 2: Distribution of per-context system ranks for the
highest rated question for the top-scoring system (top),
and the baseline system (bottom). These ranks are the
inverse of the reciprocal rank used to calculate MRR.
ing DISCUSS features, removing the part-of-speech
features gives an additional bump in performance
suggesting that there is an overlap in information
between DISCUSS representations and POS tags.
Finally, adding contextualized DISCUSS features
pushes our ranking models to their highest level
of agreement with ? = 0.211 using MaxEnt and
MRR=0.555 using SVMRank . Inspection of the
MRR values shows that without taking into account
the possibility of ties the baseline system selects
8
the top-ranked question in 44/205 (21.4%) contexts.
While the system with the best MRR score, correctly
chooses the top-ranked question in 71/205 (34.6%)
contexts ? a rate comparable to how often a pair of
raters agreed on the number-one item (33.4%).
Application of the Wilcoxon signed-rank test
shows the DISCUSS system exhibits statistically
significant improvement over the baseline system in
its distribution of Kendall?s-? values (n = 205, z =
7350, p < 0.001) and distribution of reciprocal
ranks (n = 205, z = 3739, p < 0.001). Figures 1
and 2 give visual confirmation of this improvement,
and highlight the overall reduction in negative ? val-
ues as well as the greater-than-50% increase in like-
lihood of selecting the best question first.
To get another perspective on system perfor-
mance, we evaluated our human raters on the gold-
standard rankings from the subset of questions used
for assessing internal agreement. This yielded a
mean ? between 0.2589 and 0.3619. If we remove
ratings so that the gold-standard does not include the
rater under evaluation, tutor performance drops to
a range of 0.1523 to 0.2432, which is roughly cen-
tered around the agreement exhibited by our best-
performing system.
Looking at the impact of learning algorithms
we see that SVMRank tends to perform better on
MRR while the pairwise maximum entropy mod-
els yield higher ? ?s. One possible explanation for
this discrepancy may stem from the ranking algo-
rithms? different treatment of ties. The pairwise
model permits ties, whereas the scores produced by
SVMRank produce a strict order. Without ties, it is
difficult to exactly match the raters? orderings which
had numerous ties, which can in turn produce an
overall higher number of concordances and discor-
dances than the pairwise classification model.
6 Conclusions and Future Work
We have introduced a framework for learning and
evaluating models for ranking and selecting ques-
tions for a given point in a tutorial dialogue. Fur-
thermore these experiments show that it is feasible
to learn this behavior by coupling predefined ques-
tions with ratings from trained tutors. Supplement-
ing our baseline surface form and lexical similarity
features with additional features extracted from the
dialogue context and DISCUSS dialogue act anno-
tation improves system performance in ranking to a
level on par with expert human tutors. These results
illustrate how question asking depends not only on
the form of the question but also on the underlying
dialogue action, function and content.
In the near future we plan to train models on indi-
vidual tutors to investigate which factors drive in-
dividual preferences in question asking. We also
plan to characterize system performance using auto-
matically labeled DISCUSS annotation. Lastly, we
feel these results provide a natural starting point to
explore automatic generation of questions from the
DISCUSS dialogue move representation.
Acknowledgments
This work was supported by grants from the
NSF (DRL-0733322, DRL-0733323), the IES
(R3053070434) and the DARPA GALE program
(Contract No. HR0011-06-C-0022, a supplement
for VerbNet attached to the subcontract from the
BBN-AGILE Team). Any findings, recommenda-
tions, or conclusions are those of the author and do
not necessarily represent the views of NSF, IES, or
DARPA.
References
Manish Agarwal and Prashanth Mannem. 2011. Auto-
matic gap-fill question generation from text books au-
tomatic gap-fill question generation from text books
automatic gap-fill questions from text books. In Pro-
ceedings of the Sixth Workshop on Innovative Use of
NLP for Building Educational Applications.
I. L. Beck, M. G. McKeown, J. Worthy, C. A. San-
dora, and L. Kucan. 1996. Questioning the au-
thor: A year-long classroom implementation to engage
students with text. The Elementary School Journal,
96(4):387?416.
L. Becker, R. D. Nielsen, and W. Ward. 2009. What a
pilot study says about running a question generation
challenge. In Proceedings of the Second Workshop on
Question Generation, Brighton, England, July.
L. Becker, W. Ward, S. van Vuuren, and M. Palmer. 2011.
Discuss: A dialogue move taxonomy layered over se-
mantic representations. In In Proceedings of the In-
ternational Conference on Computational Semantics
(IWCS) 2011, Oxford, England, January 12-14.
K.E. Boyer, E.Y. Ha, M. Wallis, R. Phillips, M.A. Vouk,
and J.C. Lester. 2009a. Discovering tutorial dialogue
9
strategies with hidden markov models. In Proceed-
ings of the 14th International Conference on Artificial
Intelligence in Education (AIED ?09), pages 141?148,
Brighton, U.K.
K.E. Boyer, W.J. Lahti, R. Phillips, M. D. Wallis, M. A.
Vouk, and J. C. Lester. 2009b. An empirically derived
question taxonomy for task-oriented tutorial dialogue.
In Proceedings of the Second Workshop on Question
Generation, pages 9?16, Brighton, U.K.
M. Buckley and M. Wolska. 2008. A classification of
dialogue actions in tutorial dialogue. In Proceedings
of COLING 2008, pages 73?80. ACL.
H. C. Bunt. 2009. The DIT++ taxonomy for functional
dialogue markup. In Proc. EDAML 2009.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):pp. 249?254.
M. Chi, P. Jordan, K. VanLehn, and M. Hall. 2008. Re-
inforcement learning-based feature selection for devel-
oping pedagogically effective tutorial dialogue tactics.
In Ryan S. Baker, Tiffany Barnes, and Joseph Becker,
editors, Proceedings of the 1st International Confer-
ence on Educational Data Mining, pages pp258?265.
M. Chi, P. W. Jordan, K. VanLehn, and D. J. Litman.
2009. To elicit or to tell: Does it matter? In Artifi-
cial Intelligence in Education, pages 197?204.
M. Chi, K. VanLehn, and D. Litman. 2010. Do micro-
level tutorial decisions matter: Applying reinforce-
ment learning to induce do micro-level tutorial deci-
sions matter. In Vincent Aleven, Judy Kay, and Jack
Mostow, editors, Preceedings of the 10th Internation
Confernce on Intelligent Tutoring Systems (ITS 2010).
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1998. Learning to order things. In Advances
in Neural Information Processing Systems 10 (NIPS
1998).
A. Collins and A. Stevens. 1982. Goals and methods for
inquiry teachers. Advances in Instructional Psychol-
ogy, 2.
M. G. Core and J.F. Allen. 1997. Coding dialogs with the
DAMSL annotation scheme. In AAAI Fall Symposium,
pages 28?35.
A.C. Graesser and N.K. Person. 1994. Question ask-
ing during tutoring. American Educational Research
Journal, 31:104?137.
A.C. Graesser, P. Chipman, B.C Haynes, and A. Olney.
2005. Autotutor: An intelligent tutoring system with
mixed-initiative dialogue. IEEE Transactions in Edu-
cation, 48:612?618.
M. Heilman and N. A. Smith. 2010. Good question! sta-
tistical ranking for question generation. In Proceed-
ings of NAACL/HLT 2010.
T. Joachims. 1999. Making large-scale svm learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
J.H. Kim, M. Glass, R. Freedman, and M.W. Evens.
2000. Learning the use of discourse markers in tuto-
rial dialogue learning the use of discourse markers in
tutorial dialogue. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society.
D. Litman and S. Silliman. 2004. Itspoke: An intel-
ligent tutoring spoken dialogue system. In Compan-
ion Proceedings of the Human Language Technology
Conference: 4th Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
W.C. Mann and S.A Thompson. 1986. Rhetorical struc-
ture theory: Description and construction of text struc-
tures. In In Proceedings of the Third International
Workshop on Text Generation, August.
A. K. McCallum, 2002. MALLET: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu.
R. D. Nielsen, J. Buckingham, G. Knoll, B. Marsh, and
L. Palen. 2008. A taxonomy of questions for ques-
tion generation. In Proceedings of the Workshop on
the Question Generation Shared Task and Evaluation
Challenge, September.
R.M. Pilkington. 1999. Analysing educational dis-
course: The discount scheme. Technical Report 99/2,
Computer Based Learning Unit, University of Leeds.
Owen Rambow, Monica Rogati, and Marilyn A. Walker.
2001. Evaluating a trainable sentence planner for a
spoken dialogue system evaluating a trainable sen-
tence planner for a spoken dialogue system. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL 2001).
C.P. Rose, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. A comparative evalu-
ation of socratic versus didactic tutoring. In Proceed-
ings of Cognitive Sciences Society.
C.P. Rose, D. Bhembe, S. Siler, R. Srivastava, and
K. VanLehn. 2003. The role of why questions in ef-
fective human tutoring. In Proceedings of Artificial
Intelligence in Education (AIED 2003).
D. Tsovaltzi and E. Karagjosova. 2004. A view on dia-
logue move taxonomies for tutorial dialogues. In Pro-
ceedings of SIGDIAL 2004, pages 35?38. ACL.
D. Tsovaltzi and C. Matheson. 2001. Formalising hint-
ing in tutorial dialogues. In In EDILOG: 6th workshop
on the semantics and pragmatics of dialogue, pages
185?192.
K. VanLehn, A.C. Graesser, G.T. Jackson, P. Jordan,
A. Olney, and C.P. Rose. 2007. When are tutorial
dialogues more effective than reading? Cognitive Sci-
ence, 31(1):3?62.
10
Marilyn A. Walker, Owen Rambow, and Monica Rogati.
2001. SPOT: A trainable sentence planner. In Pro-
ceedings of the North American Meeting of the Asso-
ciation for Computational Linguistics (NAACL).
A. Ward, D. Litman, and M. Eskenazi. 2011a. Predict-
ing change in student motivation by measuring cohe-
sion between predicting change in student motivation
by measuring cohesion between tutor and student. In
Proceedings of the Sixth Workshop on Innovative Use
of NLP for Building Educational Applications, pages
136?141.
W. Ward, R. Cole, D. Bolan?os, C. Buchenroth-Martin,
E. Svirsky, S. van Vuuren, T. Weston, J. Zheng, and
L. Becker. 2011b. My science tutor: A conversa-
tional multi-media virtual tutor for elementary school
science. ACM Transactions on Speech and Language
Processing (TSLP), 7(4), August.
W. Ward. 1994. Extracting information from sponta-
neous speech. In Proceedings of the International
Conference on Speech and Language Processing (IC-
SLP).
Xuchen Yao. 2010. Question generation with minimal
recursion semantics. Master?s thesis, Saarland Uni-
versity.
11
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), page 31,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Complex Predicates are Multi-word Expressions
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
295 UCB
Boulder, Colorado 80309-029, USA
Martha.Palmer@colorado.edu
Abstract
Practitioners of English Natural Language Process-
ing often feel fortunate because their tokens are
clearly marked by spaces on either side. However,
the spaces can be quite deceptive, since they ignore
the boundaries of multi-word expressions, such as
noun-noun compounds, verb particle constructions,
light verb constructions and constructions from Con-
struction Grammar, e.g., caused-motion construc-
tions and resultatives. Correctly identifying and han-
dling these types of expressions can be quite chal-
lenging, even from the viewpoint of manual anno-
tation. This talk will review the pervasive nature of
these constructions, touching on Arabic and Hindi as
well as English. Using several illustrative examples
from newswire and medical informatics, current best
practices for annotation and automatic identification
will be described, with an emphasis on contributions
from predicate argument structures.
About the Speaker
Martha Palmer is a Professor of Linguistics and
Computer Science, and a Fellow of the Institute of
Cognitive Science at the University of Colorado.
Her current research is aimed at building domain-
independent and language independent techniques
for semantic interpretation based on linguistically
annotated data, such as Proposition Banks. She has
been the PI on NSF, NIH and DARPA projects for
linguistic annotation (syntax, semantics and prag-
matics) of English, Chinese, Korean, Arabic and
Hindi. She has been a member of the Advisory
Committee for the DARPA TIDES program, Chair
of SIGLEX, Chair of SIGHAN, a past President of
the Association for Computational Linguistics, and
is a Co-Editor of JNLE and of LiLT and is on the
CL Editorial Board. She received her Ph.D. in Arti-
ficial Intelligence from the University of Edinburgh
in 1985.
31
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 126?131,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Semantic Roles for Nominal Predicates: Building a Lexical Resource
Ashwini Vaidya and Martha Palmer and Bhuvana Narasimhan
Dept of Linguistics
Institute of Cognitive Science
University of Colorado, Boulder
Boulder, CO 80309
{vaidyaa, mpalmer, narasimb}@colorado.edu
Abstract
The linguistic annotation of noun-verb com-
plex predicates (also termed as light verb con-
structions) is challenging as these predicates
are highly productive in Hindi. For semantic
role labelling, each argument of the noun-verb
complex predicate must be given a role la-
bel. For complex predicates, frame files need
to be created specifying the role labels for
each noun-verb complex predicate. The cre-
ation of frame files is usually done manually,
but we propose an automatic method to expe-
dite this process. We use two resources for
this method: Hindi PropBank frame files for
simple verbs and the annotated Hindi Tree-
bank. Our method perfectly predicts 65% of
the roles in 3015 unique noun-verb combi-
nations, with an additional 22% partial pre-
dictions, giving us 87% useful predictions to
build our annotation resource.
1 Introduction
Ahmed et al (2012) describe several types of com-
plex predicates that are found in Hindi e.g. morpho-
logical causatives, verb-verb complex predicates and
noun-verb complex predicates. Of the three types,
we will focus on the noun-verb complex predicates
in this paper. Typically, a noun-verb complex pred-
icate chorii ?theft? karnaa ?to do? has two compo-
nents: a noun chorii and a light verb karnaa giving
us the meaning ?steal?. Complex predicates 1 may
be found in English e.g. take a walk and many other
languages such as Japanese, Persian, Arabic and
Chinese (Butt, 1993; Fazly and Stevenson, 2007).
1They are also otherwise known as light verb, support verb
or conjunct verb constructions.
The verbal component in noun-verb complex
predicates (NVC) has reduced predicating power
(although it is inflected for person, number, and gen-
der agreement as well as tense-aspect and mood) and
its nominal complement is considered the true pred-
icate, hence the term ?light verb?. The creation of
a lexical resource for the set of true predicates that
occur in an NVC is important from the point of view
of linguistic annotation. For semantic role labelling
in particular, similar lexical resources have been cre-
ated for complex predicates in English, Arabic and
Chinese (Hwang et al, 2010).
1.1 Background
The goal of this paper is to produce a lexical re-
source for Hindi NVCs. This resource is in the form
of ?frame files?, which are directly utilized for Prop-
Bank annotation. PropBank is an annotated cor-
pus of semantic roles that has been developed for
English, Arabic and Chinese (Palmer et al, 2005;
Palmer et al, 2008; Xue and Palmer, 2003). In
Hindi, the task of PropBank annotation is part of a
larger effort to create a multi-layered treebank for
Hindi as well as Urdu (Palmer et al, 2009).
PropBank annotation assumes that syntactic
parses are already available for a given corpus.
Therefore, Hindi PropBanking is carried out on top
of the syntactically annotated Hindi Dependency
Treebank. As the name suggests, the syntactic rep-
resentation is dependency based, which has several
advantages for the PropBank annotation process (see
Section 3).
The PropBank annotation process for Hindi fol-
lows the same two-step process used for other Prop-
Banks. First, the semantic roles that will occur with
each predicate are defined by a human expert. Then,
126
these definitions or ?frame files? are used to guide
the annotation of predicate-argument structure in a
given corpus.
Semantic roles are annotated in the form of num-
bered arguments. In Table 1 PropBank-style seman-
tic roles are listed for the simple verb de;?to give?:
de.01 ?to give?
Arg0 the giver
Arg1 thing given
Arg2 recipient
Table 1: A frame file
The labels ARG0, ARG1 and ARG2 are always de-
fined on a verb-by-verb basis. The description at
the verb-specific level gives details about each num-
bered argument. In the example above, the num-
bered arguments correspond to the giver, thing given
and recipient. In the Hindi treebank, which consists
of 400,000 words, there are nearly 37,576 predi-
cates, of which 37% have been identified as complex
predicates at the dependency level. This implies that
a sizeable portion of the predicates are NVCs, which
makes the task of manual frame file creation time
consuming.
In order to reduce the effort required for manual
creation of NVC frame files, we propose a novel au-
tomatic method for generating PropBank semantic
roles. The automatically generated semantic roles
will be used to create frame files for each com-
plex predicate in the corpus. Our method accurately
predicts semantic roles for almost two thirds of
the unique nominal-verb combinations, with around
20% partial predictions, giving us a total of 87% use-
ful predictions.
For our implementation, we use linguistic re-
sources in the form of syntactic dependency labels
from the treebank. In addition we also have manu-
ally created, gold standard frame files for Hindi sim-
ple verbs2. In the following sections we provide lin-
guistic background, followed by a detailed descrip-
tion of our method. We conclude with an error anal-
ysis and evaluation section.
2http://verbs.colorado.edu/propbank/framesets-hindi/
2 The Nominal and the Light Verb
Semantic roles for the arguments of the light verb are
determined jointly by the noun as well as the light
verb. Megerdoomian (2001) showed that the light
verb places some restrictions on the semantic role of
its subject in Persian. A similar phenomenon may
be observed for Hindi. Compare example 1 with ex-
ample 2 below:
(1) Raam-ne
Ram-erg
cycle-kii
cycle-gen
chorii
theft
kii
do.prf
?Ram stole a bicycle?
(2) aaj
Today
cycle-kii
cycle-gen
chorii
theft
huii
be.pres
?Today a bicycle was stolen?
PropBank annotation assumes that sentences in
the corpus have already been parsed. The annotation
task involves identification of arguments for a given
NVC and the labelling of these arguments with se-
mantic roles. In example 1 we get an agentive sub-
ject with the light verb kar ?do?. However, when it
is replaced by the unaccusative ho ?become? in Ex-
ample 2, then the resulting clause has a theme argu-
ment as its subject. Note that the nominal chorii in
both examples remains the same. From the point
of view of PropBank annotation, the NVC chorii
kii will have both ARG0 and ARG1, but chorii huii
will only have ARG1 for its single argument cycle.
Hence, the frame file for a given nominal must make
reference to the type of light verb that occurs with it.
The nominal as the true predicate also contributes
its own arguments. In example 3, which shows a full
(non-light) use of the verb de ?give?, there are three
arguments: giver(agent), thing given(theme) and re-
cipient. In contrast the light verb usage zor de ?em-
phasis give; emphasize?, seen in example 4, has a
locative marked argument baat par ?matter on? con-
tributed by the nominal zor ?emphasis?.
(3) Raam-ne
Ram-erg
Mohan ko
Mohan-dat
kitaab
book
dii
give.prf
?Ram gave Mohan a book?
(4) Ram ne
Ram-erg
is
this
baat
matter
par
loc
zor
emphasis
diyaa
give.prf
?Ram emphasized this matter?
127
As both noun and light verb contribute to the se-
mantic roles of their arguments, we require linguis-
tic knowledge about both parts of the NVC. The
semantic roles for the nominal need to specify the
co-occurring light verb and the nominal?s argument
roles must also be captured. Table 2 describes the
desired representation for a nominal frame file.
Frame file for chorii-n(oun)
chorii.01: theft-n light verb: kar?do; to
steal?
Arg0 person who steals
Arg1 thing stolen
chorii.02 : theft-n light verb: ho
?be/become; to get
stolen?
Arg1 thing stolen
Table 2: Frame file for predicate noun chorii ?theft? with
two frequently occurring light verbs ho and kar. If other
light verbs are found to occur, they are added as addi-
tional rolesets as chorii.03, chorii.04 and so on.
This frame file shows the representation of a nom-
inal chorii ?theft? that can occur in combination with
a light verb kar ?do? or ho ?happen?. For each
combination, we derive a different set of PropBank
roles: agent and patient for chorii.01 and theme for
chorii.02. Note that the nominal?s frame actually
contains the roles for the combination of nominal
and light verb, and not the nominal alone.
Nominal frame files such as these have already
been defined for English PropBank.3 However, for
English, many nominals in NVCs are in fact nom-
inalizations of full verbs, which makes it far easier
to derive their frame files (e.g. walk in take a walk
is a full verb). For Hindi, this is not the case, and
a different strategy needs to be employed to derive
these frames automatically.
3 Generating Semantic Roles
The Hindi Treebank has already identified NVC
cases by using a special label pof or ?part-of?. The
Treebank annotators apply this label on the basis of
native speaker intuition. We use the label given by
the Treebank as a means to extract the NVC cases
(the issues related to complex predicate identifica-
tion are beyond the scope of this paper). Once this
3http://verbs.colorado.edu/propbank/framesets-noun/
extraction step is complete, we have a set of nomi-
nals and a corresponding list of light verbs that occur
with them.
In Section 2, we showed that the noun as well
as the light verb in a sentence influence the type of
semantic roles that will occur. Our method builds
on this idea and uses two resources in order to de-
rive linguistic knowledge about the NVC: PropBank
frame files for simple verbs in Hindi and the Hindi
Treebank, annotated with dependency labels. The
next two sections describe the use of these resources
in some detail.
3.1 Karaka to PropBank Mapping
The annotated Hindi Treebank is based on a depen-
dency framework (Begum et al, 2008) and has a
very rich set of dependency labels. These labels
(also known as karaka labels) represent the relations
between a head (e.g. a verb) and its dependents (e.g.
arguments). Using the Treebank we extract all the
dependency karaka label combinations that occur
with a unique instance of an NVC. We filter them
to include argument labels and discard those labels
that are usually used for adjuncts. We then calculate
the most frequently occurring combination of labels
that will occur with that NVC. Finally, we get a tu-
ple consisting of an NVC, a set of karaka argument
labels that occur with it and a count of the number
of times that NVC has occurred in the corpus. The
karaka labels are then mapped onto PropBank la-
bels. We reproduce in Table 3 the numbered argu-
ments to karaka label mapping found in Vaidya et
al., (2011).
PropBank label Treebank label
Arg0 (agent) k1 (karta); k4a (experiencer)
Arg1 (theme,
patient)
k2 (karma)
Arg2 (beneficiary) k4 (beneficiary)
Arg2-ATR(attribute) k1s (attribute)
Arg2-SOU(source) k5 (source)
Arg2-GOL(goal) k2p (goal)
Arg3 (instrument) k3 (instrument)
Table 3: Mapping from Karaka labels to PropBank
3.2 Verb Frames
Our second resource consists of PropBank frames
for full Hindi verbs. Every light verb that occurs in
128
Hindi is also used as a full verb, e.g. de ?give? in
Table 1 may be used both as a ?full? verb as well as
a ?light? verb. As a full verb, it has a frame file in
Hindi PropBank. The set of roles in the full verb
frame is used to generate a ?canonical? verb frame
for each light verb. The argument structure of the
light verb will change when combined with a nom-
inal, which contributes its own arguments. How-
ever, as a default, the canonical argument structure
list captures the fact that most kar ?do? light verbs
are likely to occur with the roles ARG0 and ARG1
respectively or that ho ?become?, an unaccusative
verb, occurs with only ARG1.
3.3 Procedure
Our procedure integrates the two resources de-
scribed above. First, the tuple consisting of karaka
labels for a particular NVC is mapped to PropBank
labels. But many NVC cases occur just once in the
corpus and the karaka label tuple may not be very
reliable. Hence, the likelihood that the mapped tu-
ple accurately depicts the correct semantic frame is
not very high. Secondly, Hindi can drop manda-
tory subjects or objects in a sentence e.g., (vo) ki-
taab paRegaa; ?(He) will read the book?. These are
not inserted by the dependency annotation (Bhatia
et al, 2010) and are not easy to discover automati-
cally (Vaidya et al, 2012). We cannot afford to ig-
nore any of the low frequency cases as each NVC
in the corpus must be annotated with semantic roles.
In order to get reasonable predictions for each NVC,
we use a simple rule. We carry out a mapping from
karaka to PropBank labels only if the NVC occurs at
least 30 times in the corpus. If the NVC occurs fewer
than 30 times, then we use the ?canonical? verb list.
4 Evaluation
The automatic method described in the previous sec-
tion generated 1942 nominal frame files. In or-
der to evaluate the frame files, we opted for man-
ual checking of the automatically generated frames.
The frame files were checked by three linguists and
the checking focused on the validity of the seman-
tic roles. The linguists also indicated whether an-
notation errors or duplicates were present. There
was some risk that the automatically derived frames
could bias the linguists? choice of roles as it is
quicker to accept a given suggestion than propose
an entirely new set of roles for the NVC. As we
had a very large number of automatically gener-
ated frames, all of which would need to be checked
manually anyway, practical concerns determined the
choice of this evaluation.
After this process of checking, the total number
of frame files stood at 1884. These frame files con-
sisted of 3015 rolesets i.e. individual combinations
of a nominal with a light verb (see Table 2). The
original automatically generated rolesets were com-
pared with their hand corrected counterparts (i.e.
manually checked ?gold? rolesets) and evaluated for
accuracy. We used three parameters to compare the
gold rolesets with the automatically generated ones:
a full match, partial match and no match. Table 4
shows the results derived from each resource (Sec-
tion 3) and the total accuracy.
Type of Match Full Partial None Errors
Karaka Mapping 25 31 4 0
Verbal Frames 1929 642 249 143
Totals 1954 673 245 143
% Overall 65 22 8 5
Table 4: Automatic mapping results, total frames=3015
The results show that almost two thirds of the se-
mantic roles are guessed correctly by the automatic
method, with an additional 22% partial predictions,
giving us a total of 87% useful predictions. Only
8% show no match at all between the automatically
generated labels and the gold labels.
When we compare the contribution of the karaka
labels with the verb frames, we find that the verb
frames contribute to the majority of the full matches.
The karaka mapping contributes relatively less as
only 62 NVC types occur more than 30 times in
the corpus. If we reduce our frequency requirement
from of 30 to 5, the accuracy drops by 5%. The bulk
of the cases are thus derived from the simple verb
frames. We think that the detailed information in
the verb frames, such as unaccusativity contributes
towards generating the correct frame files.
It is interesting to observe that nearly 65% accu-
racy can be achieved from the verbal information
alone. The treebank has two light verbs that occur
with high frequency i.e. kar ?do? and ho ?become?.
These combine with a variety of nominals but per-
129
Light verb Full (%) None (%) Total
Uses*
kar?do? 64 8 1038
ho ?be/become? 81 3 549
de ?give? 55 34 157
A ?come? 31 42 36
Table 5: Light verbs ?do? and ?be/become? vs. ?give? and
?come?. *The unique total light verb usages in the corpus
form more consistently than light verbs such as de
?give? or A ?come?. The light verb kar adds inten-
tionality to the NVC, but appears less often with a
set of semantic roles that are quite different from
its original ?full? verb usage. In comparison, the
light verbs such as de ?give? show far more varia-
tion, and as seen from Table 4, will match with au-
tomatically derived frames to a lesser extent. The
set of nominals that occur in combination with kar,
usually seem to require only a doer and a thing
done. Borrowed English verbs such dijain?design?
or Pona?phone? will appear preferentially with kar
in the corpus and as they are foreign words they do
not add arguments of their own.
One of the advantages of creating this lexical re-
source is the availability of gold standard frame files
for around 3000 NVCs in Hindi. As a next step, it
would be useful to use these frames to make some
higher level generalizations about these NVCs. For
example, much work has already been done on au-
tomatic verb classification for simple predicates e.g.
(Merlo and Stevenson, 2001; Schulte im Walde,
2006), and perhaps such classes can be derived for
NVCs. Also, the frame files do not currently address
the problem of polysemous NVCs which could ap-
pear with a different set of semantic roles, which will
be addressed in future work.
Acknowledgments
I am grateful to Archna Bhatia and Richa Srishti for
their help with evaluating the accuracy of the nom-
inal frames. This work is supported by NSF grants
CNS-0751089, CNS-0751171, CNS-0751202, and
CNS-0751213.
References
Tafseer Ahmed, Miriam Butt, Annette Hautli, and Se-
bastian Sulger. 2012. A reference dependency bank
for analyzing complex predicates. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation (LREC?12.
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency Annotation Scheme for Indian Languages.
In Proceedings of The Third International Joint Con-
ference on Natural Language Processing (IJCNLP).
Hyderabad, India.
Archna Bhatia, Rajesh Bhatt, Bhuvana Narasimhan,
Martha Palmer, Owen Rambow, Dipti Misra Sharma,
Michael Tepper, Ashwini Vaidya, and Fei Xia. 2010.
Empty Categories in a Hindi Treebank. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC?10), pages 1863?
1870.
Miriam Butt. 1993. The Light Verb Jungle. In G. Aygen,
C. Bowers, and C. Quinn, editors, Harvard Working
Papers in Linguistics: Papers from the GSAS/Dudley
House workshop on light verbs, volume 9.
Afsaneh Fazly and Suzanne Stevenson. 2007. Au-
tomatic Acquisition of Knowledge about Multiword
Predicates. In Proceedings of PACLIC 19, the 19th
Asia-Pacific Conference on Language, Information
and Computation.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and Martha
Palmer. 2010. PropBank Annotation of Multilingual
Light Verb Constructions. In Proceedings of the Lin-
guistic Annotation Workshop held in conjunction with
ACL-2010.
Karine Megerdoomian. 2001. Event Structure and Com-
plex Predicates in Persian. Canadian Journal of Lin-
guistics, 46:97?125.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
Verb Classification Based on Statistical Distributions
of Argument Structure. Computational Linguistics,
27(3):373?408.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohammed Maamouri, Aous Mansouri, and
Wajdi Zaghouani. 2008. A pilot Arabic PropBank.
In Proceedings of the 6th International Language Re-
sources and Evaluation.
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. Hindi Syntax: Annotating Dependency, Lexical
130
Predicate-Argument Structure, and Phrase Structure.
In Proceedings of ICON-2009: 7th International Con-
ference on Natural Language Processing, Hyderabad.
Sabine Schulte im Walde. 2006. Experiments on the Au-
tomatic Induction of German Semantic Verb Classes.
Computational Linguistics, 32(2):159?194.
Ashwini Vaidya, Jinho D. Choi, Martha Palmer, and Bhu-
vana Narasimhan. 2011. Analysis of the Hindi propo-
sition bank using dependency structure. In Proceed-
ings of the 5th Linguistic Annotation Workshop - LAW
V ?11.
Ashwini Vaidya, Jinho D. Choi, Martha Palmer, and Bhu-
vana Narasimhan. 2012. Empty Argument Insertion
in the Hindi PropBank. In Proceedings of the Eighth
International Conference on Language Resources and
Evaluation - LREC-12, Istanbul.
Nianwen Xue and Martha Palmer. 2003. Annotating the
Propositions in the Penn Chinese Treebank. In Pro-
ceedings of the 2nd SIGHAN workshop on Chinese
language processing, SIGHAN?03, pages 47?54.
131
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 178?186,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Abstract Meaning Representation for Sembanking
Laura Banarescu
SDL
lbanarescu@sdl.com
Claire Bonial
Linguistics Dept.
Univ. Colorado
claire.bonial@colorado.edu
Shu Cai
ISI
USC
shucai@isi.edu
Madalina Georgescu
SDL
mgeorgescu@sdl.com
Kira Griffitt
LDC
kiragrif@ldc.upenn.edu
Ulf Hermjakob
ISI
USC
ulf@isi.edu
Kevin Knight
ISI
USC
knight@isi.edu
Philipp Koehn
School of Informatics
Univ. Edinburgh
pkoehn@inf.ed.ac.uk
Martha Palmer
Linguistics Dept.
Univ. Colorado
martha.palmer@colorado.edu
Nathan Schneider
LTI
CMU
nschneid@cs.cmu.edu
Abstract
We describe Abstract Meaning Represen-
tation (AMR), a semantic representation
language in which we are writing down
the meanings of thousands of English sen-
tences. We hope that a sembank of simple,
whole-sentence semantic structures will
spur new work in statistical natural lan-
guage understanding and generation, like
the Penn Treebank encouraged work on
statistical parsing. This paper gives an
overview of AMR and tools associated
with it.
1 Introduction
Syntactic treebanks have had tremendous impact
on natural language processing. The Penn Tree-
bank is a classic example?a simple, readable file
of natural-language sentences paired with rooted,
labeled syntactic trees. Researchers have ex-
ploited manually-built treebanks to build statisti-
cal parsers that improve in accuracy every year.
This success is due in part to the fact that we have
a single, whole-sentence parsing task, rather than
separate tasks and evaluations for base noun iden-
tification, prepositional phrase attachment, trace
recovery, verb-argument dependencies, etc. Those
smaller tasks are naturally solved as a by-product
of whole-sentence parsing, and in fact, solved bet-
ter than when approached in isolation.
By contrast, semantic annotation today is balka-
nized. We have separate annotations for named en-
tities, co-reference, semantic relations, discourse
connectives, temporal entities, etc. Each annota-
tion has its own associated evaluation, and training
data is split across many resources. We lack a sim-
ple readable sembank of English sentences paired
with their whole-sentence, logical meanings. We
believe a sizable sembank will lead to new work in
statistical natural language understanding (NLU),
resulting in semantic parsers that are as ubiquitous
as syntactic ones, and support natural language
generation (NLG) by providing a logical seman-
tic input.
Of course, when it comes to whole-sentence se-
mantic representations, linguistic and philosophi-
cal work is extensive. We draw on this work to de-
sign an Abstract Meaning Representation (AMR)
appropriate for sembanking. Our basic principles
are:
? AMRs are rooted, labeled graphs that are
easy for people to read, and easy for pro-
grams to traverse.
? AMR aims to abstract away from syntac-
tic idiosyncrasies. We attempt to assign the
same AMR to sentences that have the same
basic meaning. For example, the sentences
?he described her as a genius?, ?his descrip-
tion of her: genius?, and ?she was a ge-
nius, according to his description? are all as-
signed the same AMR.
? AMR makes extensive use of PropBank
framesets (Kingsbury and Palmer, 2002;
Palmer et al, 2005). For example, we rep-
resent a phrase like ?bond investor? using
the frame ?invest-01?, even though no verbs
appear in the phrase.
? AMR is agnostic about how we might want
to derive meanings from strings, or vice-
versa. In translating sentences to AMR, we
do not dictate a particular sequence of rule
applications or provide alignments that re-
flect such rule sequences. This makes sem-
banking very fast, and it allows researchers
to explore their own ideas about how strings
178
are related to meanings.
? AMR is heavily biased towards English. It
is not an Interlingua.
AMR is described in a 50-page annotation guide-
line.1 In this paper, we give a high-level descrip-
tion of AMR, with examples, and we also provide
pointers to software tools for evaluation and sem-
banking.
2 AMR Format
We write down AMRs as rooted, directed, edge-
labeled, leaf-labeled graphs. This is a com-
pletely traditional format, equivalent to the sim-
plest forms of feature structures (Shieber et al,
1986), conjunctions of logical triples, directed
graphs, and PENMAN inputs (Matthiessen and
Bateman, 1991). Figure 1 shows some of these
views for the sentence ?The boy wants to go?. We
use the graph notation for computer processing,
and we adapt the PENMAN notation for human
reading and writing.
3 AMR Content
In neo-Davidsonian fashion (Davidson, 1969), we
introduce variables (or graph nodes) for entities,
events, properties, and states. Leaves are labeled
with concepts, so that ?(b / boy)? refers to an in-
stance (called b) of the concept boy. Relations link
entities, so that ?(d / die-01 :location (p / park))?
means there was a death (d) in the park (p). When
an entity plays multiple roles in a sentence, we
employ re-entrancy in graph notation (nodes with
multiple parents) or variable re-use in PENMAN
notation.
AMR concepts are either English words
(?boy?), PropBank framesets (?want-01?), or spe-
cial keywords. Keywords include special entity
types (?date-entity?, ?world-region?, etc.), quan-
tities (?monetary-quantity?, ?distance-quantity?,
etc.), and logical conjunctions (?and?, etc).
AMR uses approximately 100 relations:
? Frame arguments, following PropBank
conventions. :arg0, :arg1, :arg2, :arg3, :arg4,
:arg5.
? General semantic relations. :accompa-
nier, :age, :beneficiary, :cause, :compared-to,
:concession, :condition, :consist-of, :degree,
:destination, :direction, :domain, :duration,
1AMR guideline: amr.isi.edu/language.html
LOGIC format:
? w, b, g:
instance(w, want-01) ? instance(g, go-01) ?
instance(b, boy) ? arg0(w, b) ?
arg1(w, g) ? arg0(g, b)
AMR format (based on PENMAN):
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
GRAPH format:
Figure 1: Equivalent formats for representating
the meaning of ?The boy wants to go?.
:employed-by, :example, :extent, :frequency,
:instrument, :li, :location, :manner, :medium,
:mod, :mode, :name, :part, :path, :polarity,
:poss, :purpose, :source, :subevent, :subset,
:time, :topic, :value.
? Relations for quantities. :quant, :unit,
:scale.
? Relations for date-entities. :day, :month,
:year, :weekday, :time, :timezone, :quarter,
:dayperiod, :season, :year2, :decade, :cen-
tury, :calendar, :era.
? Relations for lists. :op1, :op2, :op3, :op4,
:op5, :op6, :op7, :op8, :op9, :op10.
AMR also includes the inverses of all these rela-
tions, e.g., :arg0-of, :location-of, and :quant-of. In
addition, every relation has an associated reifica-
tion, which is what we use when we want to mod-
ify the relation itself. For example, the reification
of :location is the concept ?be-located-at-91?.
Our set of concepts and relations is designed to
allow us represent all sentences, taking all words
into account, in a reasonably consistent manner. In
the rest of this section, we give examples of how
AMR represents various kinds of words, phrases,
and sentences. For full documentation, the reader
is referred to the AMR guidelines.
179
Frame arguments. We make heavy use of
PropBank framesets to abstract away from English
syntax. For example, the frameset ?describe-01?
has three pre-defined slots (:arg0 is the describer,
:arg1 is the thing described, and :arg2 is what it is
being described as).
(d / describe-01
:arg0 (m / man)
:arg1 (m2 / mission)
:arg2 (d / disaster))
The man described the mission as a disaster.
The man?s description of the mission:
disaster.
As the man described it, the mission was a
disaster.
Here, we do not annotate words like ?as? or ?it?,
considering them to be syntactic sugar.
General semantic relations. AMR also in-
cludes many non-core relations, such as :benefi-
ciary, :time, and :destination.
(s / hum-02
:arg0 (s2 / soldier)
:beneficiary (g / girl)
:time (w / walk-01
:arg0 g
:destination (t / town)))
The soldier hummed to the girl as she
walked to town.
Co-reference. AMR abstracts away from co-
reference gadgets like pronouns, zero-pronouns,
reflexives, control structures, etc. Instead we re-
use AMR variables, as with ?g? above. AMR
annotates sentences independent of context, so if
a pronoun has no antecedent in the sentence, its
nominative form is used, e.g., ?(h / he)?.
Inverse relations. We obtain rooted structures
by using inverse relations like :arg0-of and :quant-
of.
(s / sing-01
:arg0 (b / boy
:source (c / college)))
The boy from the college sang.
(b / boy
:arg0-of (s / sing-01)
:source (c / college))
the college boy who sang ...
(i / increase-01
:arg1 (n / number
:quant-of (p / panda)))
The number of pandas increased.
The top-level root of an AMR represents the fo-
cus of the sentence or phrase. Once we have se-
lected the root concept for an entire AMR, there
are no more focus considerations?everything else
is driven strictly by semantic relations.
Modals and negation. AMR represents nega-
tion logically with :polarity, and it expresses
modals with concepts.
(g / go-01
:arg0 (b / boy)
:polarity -)
The boy did not go.
(p / possible
:domain (g / go-01
:arg0 (b / boy))
:polarity -))
The boy cannot go.
It?s not possible for the boy to go.
(p / possible
:domain (g / go-01
:arg0 (b / boy)
:polarity -))
It?s possible for the boy not to go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy))
:polarity -)
The boy doesn?t have to go.
The boy isn?t obligated to go.
The boy need not go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy)
:polarity -))
The boy must not go.
It?s obligatory that the boy not go.
(t / think-01
:arg0 (b / boy)
:arg1 (w / win-01
:arg0 (t / team)
:polarity -))
The boy doesn?t think the team will win.
The boy thinks the team won?t win.
Questions. AMR uses the concept ?amr-
unknown?, in place, to indicate wh-questions.
(f / find-01
:arg0 (g / girl)
:arg1 (a / amr-unknown))
What did the girl find?
(f / find-01
:arg0 (g / girl)
:arg1 (b / boy)
:location (a / amr-unknown))
Where did the girl find the boy?
180
(f / find-01
:arg0 (g / girl)
:arg1 (t / toy
:poss (a / amr-unknown)))
Whose toy did the girl find?
Yes-no questions, imperatives, and embedded wh-
clauses are treated separately with the AMR rela-
tion :mode.
Verbs. Nearly every English verb and verb-
particle construction we have encountered has a
corresponding PropBank frameset.
(l / look-05
:arg0 (b / boy)
:arg1 (a / answer))
The boy looked up the answer.
The boy looked the answer up.
AMR abstracts away from light-verb construc-
tions.
(a / adjust-01
:arg0 (g / girl)
:arg1 (m / machine))
The girl adjusted the machine.
The girl made adjustments to the machine.
Nouns.We use PropBank verb framesets to rep-
resent many nouns as well.
(d / destroy-01
:arg0 (b / boy)
:arg1 (r / room))
the destruction of the room by the boy ...
the boy?s destruction of the room ...
The boy destroyed the room.
We never say ?destruction-01? in AMR. Some
nominalizations refer to a whole event, while oth-
ers refer to a role player in an event.
(s / see-01
:arg0 (j / judge)
:arg1 (e / explode-01))
The judge saw the explosion.
(r / read-01
:arg0 (j / judge)
:arg1 (t / thing
:arg1-of (p / propose-01))
The judge read the proposal.
(t / thing
:arg1-of (o / opine-01
:arg0 (g / girl)))
the girl?s opinion
the opinion of the girl
what the girl opined
Many ?-er? nouns invoke PropBank framesets.
This enables us to make use of slots defined for
those framesets.
(p / person
:arg0-of (i / invest-01))
investor
(p / person
:arg0-of (i / invest-01
:arg1 (b / bond)))
bond investor
(p / person
:arg0-of (i / invest-01
:manner (s / small)))
small investor
(w / work-01
:arg0 (b / boy)
:manner (h / hard))
the boy is a hard worker
the boy works hard
However, a treasurer is not someone who trea-
sures, and a president is not (just) someone who
presides.
Adjectives. Various adjectives invoke Prop-
Bank framesets.
(s / spy
:arg0-of (a / attract-01))
the attractive spy
(s / spy
:arg0-of (a / attract-01
:arg1 (w / woman)))
the spy who is attractive to women
?-ed? adjectives frequently invoke verb framesets.
For example, ?acquainted with magic? maps to
?acquaint-01?. However, we are not restricted to
framesets that can be reached through morpholog-
ical simplification.
(f / fear-01
:arg0 (s / soldier)
:arg1 (b / battle-01))
The soldier was afraid of battle.
The soldier feared battle.
The soldier had a fear of battle.
For other adjectives, we have defined new frame-
sets.
(r / responsible-41
:arg1 (b / boy)
:arg2 (w / work))
The boy is responsible for the work.
The boy has responsibility for the work.
While ?the boy responsibles the work? is not good
English, it is perfectly good Chinese. Similarly,
we handle tough-constructions logically.
181
(t / tough
:domain (p / please-01
:arg1 (g / girl)))
Girls are tough to please.
It is tough to please girls.
Pleasing girls is tough.
?please-01? and ?girl? are adjacent in the AMR,
even if they are not adjacent in English. ?-able?
adjectives often invoke the AMR concept ?possi-
ble?, but not always (e.g., a ?taxable fund? is actu-
ally a ?taxed fund?).
(s / sandwich
:arg1-of (e / eat-01
:domain-of (p / possible)))
an edible sandwich
(f / fund
:arg1-of (t / tax-01))
a taxable fund
Pertainym adjectives are normalized to root form.
(b / bomb
:mod (a / atom))
atom bomb
atomic bomb
Prepositions. Most prepositions simply sig-
nal semantic frame elements, and are themselves
dropped from AMR.
(d / default-01
:arg1 (n / nation)
:time (d2 / date-entity
:month 6))
The nation defaulted in June.
Time and location prepositions are kept if they
carry additional information.
(d / default-01
:arg1 (n / nation)
:time (a / after
:op1 (w / war-01))
The nation defaulted after the war.
Occasionally, neither PropBank nor AMR has an
appropriate relation, in which case we hold our
nose and use a :prep-X relation.
(s / sue-01
:arg1 (m / man)
:prep-in (c / case))
The man was sued in the case.
Named entities. Any concept in AMR can be
modified with a :name relation. However, AMR
includes standardized forms for approximately 80
named-entity types, including person, country,
sports-facility, etc.
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown"))
Mollie Brown
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown")
:arg0-of (s / slay-01
:arg1 (o / orc)))
the orc-slaying Mollie Brown
Mollie Brown, who slew orcs
AMR does not normalize multiple ways of re-
ferring to the same concept (e.g., ?US? versus
?United States?). It also avoids analyzing seman-
tic relations inside a named entity?e.g., an orga-
nization named ?Stop Malaria Now? does not in-
voke the ?stop-01? frameset. AMR gives a clean,
uniform treatment to titles, appositives, and other
constructions.
(c / city
:name (n / name
:op1 "Zintan"))
Zintan
the city of Zintan
(p / president
:name (n / name
:op1 "Obama"))
President Obama
Obama, the president ...
(g / group
:name (n / name
:op1 "Elsevier"
:op2 "N.V.")
:mod (c / country
:name (n2 / name
:op1 "Netherlands"))
:arg0-of (p / publish-01))
Elsevier N.V., the Dutch publishing group...
Dutch publishing group Elsevier N.V. ...
Copula. Copulas use the :domain relation.
(w / white
:domain (m / marble))
The marble is white.
(l / lawyer
:domain (w / woman))
The woman is a lawyer.
(a / appropriate
:domain (c / comment)
:polarity -))
The comment is not appropriate.
182
The comment is inappropriate.
Reification. Sometimes we want to use an
AMR relation as a first-class concept?to be able
to modify it, for example. Every AMR relation has
a corresponding reification for this purpose.
(m / marble
:location (j / jar))
the marble in the jar ...
(b / be-located-at-91
:arg1 (m / marble)
:arg2 (j / jar)
:polarity -)
:time (y / yesterday))
The marble was not in the jar yesterday.
If we do not use the reification, we run into trou-
ble.
(m / marble
:location (j / jar
:polarity -)
:time (y / yesterday))
yesterday?s marble in the non-jar ...
Some reifications are standard PropBank frame-
sets (e.g., ?cause-01? for :cause, or ?age-01? for
:age).
This ends the summary of AMR content. For
lack of space, we omit descriptions of compara-
tives, superlatives, conjunction, possession, deter-
miners, date entities, numbers, approximate num-
bers, discourse connectives, and other phenomena
covered in the full AMR guidelines.
4 Limitations of AMR
AMR does not represent inflectional morphology
for tense and number, and it omits articles. This
speeds up the annotation process, and we do not
have a nice semantic target representation for these
phenomena. A lightweight syntactic-style repre-
sentation could be layered in, via an automatic
post-process.
AMR has no universal quantifier. Words like
?all? modify their head concepts. AMR does not
distinguish between real events and hypothetical,
future, or imagined ones. For example, in ?the boy
wants to go?, the instances of ?want-01? and ?go-
01? have the same status, even though the ?go-01?
may or may not happen.
We represent ?history teacher? nicely as ?(p /
person :arg0-of (t / teach-01 :arg1 (h / history)))?.
However, ?history professor? becomes ?(p / pro-
fessor :mod (h / history))?, because ?profess-01?
is not an appropriate frame. It would be reason-
able in such cases to use a NomBank (Meyers et
al., 2004) noun frame with appropriate slots.
5 Creating AMRs
We have developed a power editor for AMR, ac-
cessible by web interface.2 The AMR Editor al-
lows rapid, incremental AMR construction via text
commands and graphical buttons. It includes on-
line documentation of relations, quantities, reifi-
cations, etc., with full examples. Users log in,
and the editor records AMR activity. The ed-
itor also provides significant guidance aimed at
increasing annotator consistency. For example,
users are warned about incorrect relations, discon-
nected AMRs, words that have PropBank frames,
etc. Users can also search existing sembanks for
phrases to see how they were handled in the past.
The editor also allows side-by-side comparison of
AMRs from different users, for training purposes.
In order to assess inter-annotator agreement
(IAA), as well as automatic AMR parsing accu-
racy, we developed the smatch metric (Cai and
Knight, 2013) and associated script.3 Smatch re-
ports the semantic overlap between two AMRs by
viewing each AMR as a conjunction of logical
triples (see Figure 1). Smatch computes precision,
recall, and F-score of one AMR?s triples against
the other?s. To match up variables from two in-
put AMRs, smatch needs to execute a brief search,
looking for the variable mapping that yields the
highest F-score.
Smatch makes no reference to English strings
or word indices, as we do not enforce any par-
ticular string-to-meaning derivation. Instead, we
compare semantic representations directly, in the
same way that the MT metric Bleu (Papineni et
al., 2002) compares target strings without making
reference to the source.
For an initial IAA study, and prior to adjust-
ing the AMR Editor to encourage consistency, 4
expert AMR annotators annotated 100 newswire
sentences and 80 web text sentences. They then
created consensus AMRs through discussion. The
average annotator vs. consensus IAA (smatch) was
0.83 for newswire and 0.79 for web text. When
newly trained annotators doubly annotated 382
web text sentences, their annotator vs. annotator
IAA was 0.71.
2AMR Editor: amr.isi.edu/editor.html
3Smatch: amr.isi.edu/evaluation.html
183
6 Current AMR Bank
We currently have a manually-constructed AMR
bank of several thousand sentences, a subset of
which can be freely downloaded,4 the rest being
distributed via the LDC catalog.
In initially developing AMR, the authors built
consensus AMRs for:
? 225 short sentences for tutorial purposes
? 142 sentences of newswire (*)
? 100 sentences of web data (*)
Trained annotators at LDC then produced AMRs
for:
? 1546 sentences from the novel ?The Little
Prince?
? 1328 sentences of web data
? 1110 sentences of web data (*)
? 926 sentences from Xinhua news (*)
? 214 sentences from CCTV broadcast con-
versation (*)
Collections marked with a star (*) are also in
the OntoNotes corpus (Pradhan et al, 2007;
Weischedel et al, 2011).
Using the AMR Editor, annotators are able to
translate a full sentence into AMR in 7-10 minutes
and postedit an AMR in 1-3 minutes.
7 Related Work
Researchers working on whole-sentence semantic
parsing today typically use small, domain-specific
sembanks like GeoQuery (Wong and Mooney,
2006). The need for larger, broad-coverage sem-
banks has sparked several projects, including the
Groningen Meaning Bank (GMB) (Basile et al,
2012a), UCCA (Abend and Rappoport, 2013),
the Semantic Treebank (ST) (Butler and Yoshi-
moto, 2012), the Prague Dependency Treebank
(Bo?hmova? et al, 2003), and UNL (Uchida et al,
1999; Uchida et al, 1996; Martins, 2012).
Concepts. Most systems use English words
as concepts. AMR uses PropBank frames (e.g.,
?describe-01?), and UNL uses English WordNet
synsets (e.g., ?200752493?).
Relations. GMB uses VerbNet roles (Schuler,
2005), and AMR uses frame-specific PropBank
relations. UNL has a dedicated set of over 30 fre-
quently used relations.
Formalism. GMB meanings are written in
DRT (Kamp et al, 2011), exploiting full first-
4amr.isi.edu/download.html
order logic. GMB and ST both include universal
quantification.
Granularity. GMB and UCCA annotate short
texts, so that the same entity can participate in
events described in different sentences; other sys-
tems annotate individual sentences.
Entities. AMR uses 80 entity types, while
GMB uses 7.
Manual versus automatic. AMR, UNL, and
UCCA annotation is fully manual. GMB and ST
produce meaning representations automatically,
and these can be corrected by experts or crowds
(Venhuizen et al, 2013).
Derivations. AMR and UNL remain agnostic
about the relation between strings and their mean-
ings, considering this a topic of open research.
ST and GMB annotate words and phrases directly,
recording derivations as (for example) Montague-
style compositional semantic rules operating on
CCG parses.
Top-down verus bottom-up. AMR annota-
tors find it fast to construct meanings from the
top down, starting with the main idea of the sen-
tence (though the AMR Editor allows bottom-up
construction). GMB and UCCA annotators work
bottom-up.
Editors, guidelines, genres. These projects
have graphical sembanking tools (e.g., Basile et al
(2012b)), annotation guidelines,5 and sembanks
that cover a wide range of genres, from news to
fiction. UNL and AMR have both annotated many
of the same sentences, providing the potential for
direct comparison.
8 Future Work
Sembanking. Our main goal is to continue
sembanking. We would like to employ a large
sembank to create shared tasks for natural lan-
guage understanding and generation. These
tasks may additionally drive interest in theoreti-
cal frameworks for probabilistically mapping be-
tween graphs and strings (Quernheim and Knight,
2012b; Quernheim and Knight, 2012a; Chiang et
al., 2013).
Applications. Just as syntactic parsing has
found many unanticipated applications, we expect
sembanks and statistical semantic processors to be
used for many purposes. To get started, we are
exploring the use of statistical NLU and NLG in
5UNL guidelines: www.undl.org/unlsys/unl/unl2005
184
a semantics-based machine translation (MT) sys-
tem. In this system, we annotate bilingual Chi-
nese/English data with AMR, then train compo-
nents to map Chinese to AMR, and AMR to En-
glish. A prototype is described by Jones et al
(2012).
Disjunctive AMR. AMR aims to canonicalize
multiple ways of saying the same thing. We plan
to test how well we are doing by building AMRs
on top of large, manually-constructed paraphrase
networks from the HyTER project (Dreyer and
Marcu, 2012). Rather than build individual AMRs
for different paths through a network, we will con-
struct highly-packed disjunctive AMRs. With this
application in mind, we have developed a guide-
line6 for disjunctive AMR. Here is an example:
(o / *OR*
:op1 (t / talk-01)
:op2 (m / meet-03)
:OR (o2 / *OR*
:mod (o3 / official)
:arg1-of (s / sanction-01
:arg0 (s2 / state))))
official talks
state-sanctioned talks
meetings sanctioned by the state
AMR extensions. Finally, we would like
to deepen the AMR language to include more
relations (to replace :mod and :prep-X, for
example), entity normalization (perhaps wik-
ification), quantification, and temporal rela-
tions. Ultimately, we would like to also in-
clude a comprehensive set of more abstract
frames like ?Earthquake-01? (:magnitude, :epi-
center, :casualties), ?CriminalLawsuit-01? (:de-
fendant, :crime, :jurisdiction), and ?Pregnancy-
01? (:father, :mother, :due-date). Projects like
FrameNet (Baker et al, 1998) and CYC (Lenat,
1995) have long pursued such a set.
References
O. Abend and A. Rappoport. 2013. UCCA: A
semantics-based grammatical annotation scheme. In
Proc. IWCS.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berke-
ley FrameNet project. In Proc. COLING.
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012a.
Developing a large semantically annotated corpus.
In Proc. LREC.
6Disjunctive AMR guideline: amr.isi.edu/damr.1.0.pdf
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012b.
A platform for collaborative semantic annotation. In
Proc. EACL demonstrations.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The Prague dependency treebank. In Tree-
banks. Springer.
A. Butler and K. Yoshimoto. 2012. Banking meaning
representations from treebanks. Linguistic Issues in
Language Technology, 7.
S. Cai and K. Knight. 2013. Smatch: An accu-
racy metric for abstract meaning representations. In
Proc. ACL.
D. Chiang, J. Andreas, D. Bauer, K. M. Hermann,
B. Jones, and K. Knight. 2013. Parsing graphs with
hyperedge replacement grammars. In Proc. ACL.
D. Davidson. 1969. The individuation of events.
In N. Rescher, editor, Essays in Honor of Carl G.
Hempel. D. Reidel, Dordrecht.
M. Dreyer and D. Marcu. 2012. Hyter: Meaning-
equivalent semantics for translation evaluation. In
Proc. NAACL.
B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and
K. Knight. 2012. Semantics-based machine trans-
lation with hyperedge replacement grammars. In
Proc. COLING.
H. Kamp, J. Van Genabith, and U. Reyle. 2011. Dis-
course representation theory. In Handbook of philo-
sophical logic, pages 125?394. Springer.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. LREC.
D. B. Lenat. 1995. Cyc: A large-scale investment in
knowledge infrastructure. Communications of the
ACM, 38(11).
R. Martins. 2012. Le Petit Prince in UNL. In Proc.
LREC.
C. M. I. M. Matthiessen and J. A. Bateman. 1991.
Text Generation and Systemic-Functional Linguis-
tics. Pinter, London.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 workshop: Frontiers in corpus anno-
tation.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, Philadelphia, PA.
185
S. Pradhan, E. Hovy, M. Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In-
ternational Journal of Semantic Computing (IJSC),
1(4).
D. Quernheim and K. Knight. 2012a. DAGGER: A
toolkit for automata on directed acyclic graphs. In
Proc. FSMNLP.
D. Quernheim and K. Knight. 2012b. Towards prob-
abilistic acceptors and transducers for feature struc-
tures. In Proc. SSST Workshop.
K. Schuler. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
S. Shieber, F. C. N. Pereira, L. Karttunen, and M. Kay.
1986. Compilation of papers on unification-based
grammar formalisms. Technical Report CSLI-86-
48, Center for the Study of Language and Informa-
tion, Stanford, California.
H. Uchida, M. Zhu, and T. Della Senta. 1996. UNL:
Universal Networking Language?an electronic lan-
guage for communication, understanding and col-
laboration. Technical report, IAS/UNU Tokyo.
H. Uchida, M. Zhu, and T. Della Senta. 1999. A
gift for a millennium. Technical report, IAS/UNU
Tokyo.
N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013.
Gamification for word sense labeling. In Proc.
IWCS.
R. Weischedel, E. Hovy, M. Marcus, M. Palmer,
R. Belvin, S. Pradhan, L. Ramshaw, and N. Xue.
2011. OntoNotes: A large training corpus for en-
hanced processing. In J. Olive, C. Christianson, and
J. McCary, editors, Handbook of Natural Language
Processing and Machine Translation. Springer.
Y. W. Wong and R. J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation.
In Proc. HLT-NAACL.
186
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 94?98,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
An Approach to Take Multi-Word Expressions
Claire Bonial
?
Meredith Green
??
Jenette Preciado
??
Martha Palmer
?
?
Department of Linguistics, University of Colorado at Boulder
??
Institute of Cognitive Science, University of Colorado at Boulder
{Claire.Bonial,Laura.Green,Jenette.Preciado,Martha.Palmer}@colorado.edu
Abstract
This research discusses preliminary efforts to
expand the coverage of the PropBank lexicon
to multi-word and idiomatic expressions, such
as take one for the team. Given overwhelming
numbers of such expressions, an efficient way
for increasing coverage is needed. This re-
search discusses an approach to adding multi-
word expressions to the PropBank lexicon in
an effective yet semantically rich fashion. The
pilot discussed here uses double annotation
of take multi-word expressions, where anno-
tations provide information on the best strat-
egy for adding the multi-word expression to
the lexicon. This work represents an impor-
tant step for enriching the semantic informa-
tion included in the PropBank corpus, which
is a valuable and comprehensive resource for
the field of Natural Language Processing.
1 Introduction
The PropBank (PB) corpus provides informa-
tion associating semantic roles with certain syn-
tactic structures, thereby contributing valuable
training data for Natural Language Process-
ing (NLP) applications (Palmer et al., 2005).
For example, recent research shows that us-
ing semantic role information in machine trans-
lation systems improves performance (Lo, Be-
loucif & Wu, 2013). Despite these successes,
PB could be improved with greater coverage
of multi-word expressions (MWEs). The PB
lexicon (http://verbs.colorado.edu/PB/framesets-
english) is comprised of senses of verb, noun and
adjective relations, with a listing of their seman-
tic roles (thus a sense is referred to as a ?roleset?).
Although the lexicon encompasses nearly 12,000
rolesets, relatively few of these apply to instances
of MWEs. PB has previously treated language
as if it were purely compositional, and has there-
fore lumped the majority of MWEs in with lexi-
cal verb usages. For example, annotations of the
single PB sense of take meaning acquire, come to
have, choose, bring with you from somewhere in-
clude MWEs such as take measures, take comfort
and take advantage, and likely others. Although
PB senses typically, and this sense especially, are
quite coarse-grained, valuable semantic informa-
tion is lost when these distinct MWEs are lumped
together with other lexical senses.
The importance of coverage for MWEs is
underscored by their prevalence. Jackendoff
(1997:156) estimates that the number of MWEs
in a speaker?s lexicon is approximately equal to
the number of single words, and in WordNet
1.7 (Fellbaum, 1998), 41% of the entries were
MWEs (cited in Sag et al., 2002). Furthermore,
Sag (2002) estimates the vocabularies of special-
ized domains will continue to contribute more
MWEs than simplex words. For systems like PB
to continue to provide adequate training data for
NLP systems, coverage must extend to MWEs.
The lack of coverage in this area has already
become problematic for the recently developed
Abstract Meaning Representation (AMR) project
(Banarescu et al., 2013), which relies upon the PB
lexicon, or ?frame files? as the groundwork for its
annotations. As AMR and PB have extended into
more informal domains, such as online discussion
forums and SMS texts, the gaps in coverage of
MWEs have become more and more problematic.
To address this issue, this research discusses a pi-
lot approach to increasing the coverage of the PB
lexicon to a variety of MWEs involving the verb
take, demonstrating a methodology for efficiently
augmenting the lexicon with MWEs.
2 PB Background
PB annotation was developed to provide training
data for supervised machine learning classifiers.
It provides semantic information, including the
94
basic ?who is doing what to whom,? in the form of
predicate-by-predicate semantic role assignments.
The annotation firstly consists of the selection of a
roleset, or a coarse-grained sense of the predicate,
which includes a listing of the roles, expressed
as generic argument numbers, associated with
that sense. Here, for example, is the roleset for
Take.01, mentioned previously:
Take.01: acquire, come to have, choose, bring
Arg0: Taker
Arg1: Thing taken
Arg2: Taken-from, source of thing taken
Arg3: Destination
These argument numbers, along with a variety
of modifier tags, such as temporal and locative,
are assigned to natural language sentences drawn
from a variety of corpora. The roleset and example
sentences serve as a guide to annotators on how to
assign argument numbers to annotation instances.
The goal is to assign these simple, general-purpose
labels consistently across the many possible syn-
tactic realizations of the same event participant or
semantic role.
PB has recently undertaken efforts to expand
the types of predicates that are annotated. Pre-
viously, annotation efforts focused on verbs, but
events generally, and even the same event, can of-
ten be expressed with a variety of different parts of
speech, or with MWEs. For example,
1. He fears bears.
2. His fear of bears...
3. He is afraid of bears.
4. He has a fear of bears.
Thus, it has been necessary to expand PB annota-
tions to provide coverage for noun, adjective and
complex predicates. While this greatly enriches
the semantics that PB is able to capture, it has also
forced the creation of an overwhelming number of
new rolesets, as generally each new predicate type
receives its own set of rolesets. To alleviate this,
PB has opted to begin unifying frame files through
a process of ?aliasing?(Bonial et al., 2014). In
this process, etymologically related concepts are
aliased to each other, and aliased rolesets are uni-
fied, so that there is a single roleset representing,
for example the concept of ?fear,? and this roleset
is used for all syntactic instantiations of that con-
cept.
This methodology is suited to complex pred-
icates, such as light verb constructions (LVCs),
wherein the eventive noun, carrying the bulk of the
event semantics, may have an etymologically re-
lated verb that is identical in its participants or se-
mantic roles (for a description of LVC annotation,
see (Hwang et al., 2010). Thus, have a fear above
is aliased to fear, as take a bath would be aliased
to bathe. In this research, the possibility of ex-
tending aliasing to a variety of MWEs is explored,
such that take it easy, as in ?I?m just going to take
it easy on Saturday,? would be aliased to the exist-
ing lexical verb roleset for relax. In many cases,
the semantics of MWEs are quite complex, adding
shades of meaning that no lexical verb quite cap-
tures. Thus, additional strategies beyond aliasing
are developed; each strategy is discussed in the
following sections.
3 Take Pilot
For the purposes of this pilot, the take MWEs
were gathered from WordNet?s MWE and phrasal
verb entries (Fellbaum, 1998), the Prague Czech-
English Dependency Treebank (Haji?c-2012), and
Afsaneh Fazly?s dissertation work (Fazly, 2007).
Graduate student annotators were trained to use
WordNet, Sketch Engine (Kilgarriff et al., 2004)
and PB to complete double-blind annotation of
these MWEs as a candidate for one of the three fol-
lowing strategies for increasing roleset coverage:
1) Aliasing the MWE to a lexically-similar verb
or noun roleset from PB, 2) proposing the creation
of groups of expressions for which one or several
rolesets will be created, or 3) simply designating
the MWE as an idiomatic expression. First, anno-
tators were to try to choose a verb or noun roleset
from PB that most closely resembled the syntax
and semantics of the MWE. Annotators also made
comments as necessary for difficult cases. The
annotators were considered to have agreed if the
proposed lexical verb or noun alias was the same.
Strategies (2) and (3) were pursued during adjudi-
cation if the annotators were unable to agree upon
an appropriate alias. Each of the possible strate-
gies for increasing coverage is discussed in turn in
the following sections.
3.1 Aliasing
Aliasing involves proposing an existing roleset
from PB as a suitable roleset for future MWE an-
notation. LVCs were the simplest of these to alias
95
since the eventive or stative noun predicate (e.g.:
take a look) may already have an existing role-
set, or there is likely an existing, etymologically
related verb roleset (e.g. verb roleset Look.01).
Some other MWEs were not so straightforward.
For instance, take time off does not include an et-
ymologically related predicate that would easily
encompass the semantics of the MWE, so the an-
notators proposed a roleset that is not as intuitive,
but captures the semantics nonetheless: the role-
set for the noun vacation. This frame allows for
an Agent to take time off, and importantly, what
time is taken off from: take time off from work,
school etc. Selecting an appropriate alias is the
ideal strategy for increasing coverage, because it
does not require the time and effort of manually
creating a new roleset or rolesets.
Both of the instances discussed above are rather
simple cases, where their coverage can be ad-
dressed efficiently through aliasing. However,
many MWE instances were considerably more
difficult to assign to an equivalent roleset. One
such example includes take shape, for which the
annotators decided that shape was an appropriate
roleset. Yet, shape does not quite cover the unique
semantics of take shape, which lacks the possibil-
ity of an Agent. In these cases, the MWEs may
still be aliased, but they should also include an
semantic constraint to convey the semantic differ-
ence, such as ?-Agent? Thus, in some cases, these
types of semantic constraints were used for aliases
that were almost adequate, but lacked some shade
of meaning conveyed by the MWE. In other cases,
the semantic difference between an MWE and ex-
isting lexical verb or noun roleset was too great
to be captured by the addition of such constraints,
thus a new roleset or group of rolesets was created
to address coverage of such MWEs, as described
in the next section.
3.2 Groups of Syntactically/Lexically Similar
Rolesets
In cases in which it was not possible to find a
single adequate alias for an MWE, a group of
rolesets representing different senses of the same
MWE was created. For example, take down can
mean to write something down, to defeat some-
thing, or to deconstruct something. Thus, a group
of take down rolesets were added, with each role-
set reflecting one of these senses.
Similarly, some of the proposed rolesets for
take MWEs were easily subsumed under a more
coarse-grained, new frame in PB. For instance,
take one?s lumps and take it on the chin both
more or less mean to endure or atone for, so com-
bining these in a coarser-grained MWE frame is
both efficient and allows for valuable distinctions
in terms of semantic role labeling. Namely, the
Agent choosing to atone for something, and what
the entity is atoning for. However, such situations
in which it?s possible to create new coarse-grained
MWE rolesets seem to be rare. Some MWEs ini-
tially seem similar enough to combine into a sin-
gle roleset, but further exploration of usages shows
that they are semantically different. Take comfort
and take heart in both involve improving mood,
but take heart in might be more closely-related to
hope in meaning, while take comfort in might sim-
ply mean to cheer up.
3.3 Idiomatic Expression Designation
In cases in which PB annotation would be very dif-
ficult for annotators, due to polysemy or semantics
that cannot be conveyed by aliasing to an exist-
ing roleset, MWEs will be listed for future annota-
tion as Idiomatic Expressions (IE), which get spe-
cial treatment. This designation indicates that the
MWE is so unique that it would require its own
new roleset(s) in PB, and even with these role-
sets, annotators may still have difficulty determin-
ing the appropriate roleset choice or sense of the
MWE. As mentioned previously, creating multi-
ple rolesets for each expression is inefficient, es-
pecially so if the rolesets manually created will be
difficult to distinguish; thus, currently such cases
are simply marked with the generic IE roleset.
The MWE take the count is an illustrative exam-
ple of this type of case. Undergraduate and grad-
uate annotators trained in linguistics tend to have
difficulty with detailed sports references in anno-
tation instances, regardless of how much context
is provided. This MWE applies to several sports
scenarios: one can take the count in boxing or
take the (full) count in baseball, and some usages
were even found for football, where many speak-
ers would use run down the clock. Annotators
unfamiliar with the somewhat esoteric meanings
of these phrases would undoubtedly have trouble
distinguishing the rolesets and arguments of the
rolesets, thus take the count in sports contexts (as
opposed to the LVC take the count, meaning to
count) will simply be designated IE.
96
Currently, IE instances are simply set aside
from the rest of the PB corpus, so as to avoid these
instances adding noise to the data. In the future,
these IE expressions will need to be treated indi-
vidually to determine the best way to capture their
unique semantics.
4 Results & Conclusions
One way of analyzing the validity of this method-
ology is to examine the Inter-Annotator Agree-
ment (IAA) on the proposed alias. After the
training period (in which about 60 MWEs were
investigated as a group), annotators worked on
double-blind annotation of 100 additional MWEs.
Of these, 17 were found to be repeats of earlier
MWEs. Of the remaining 83, annotators agreed
on the exact alias in 32 cases, giving a rather poor,
simple IAA of about 39%. However, the stan-
dards used to calculate IAA were rigid, as only
instances in which the annotators aliased the mul-
tiword expressions to exactly the same lexical verb
or noun roleset were counted as an agreement.
Annotators often disagreed on lexical verbs, but
still chose verbs that were extraordinarily similar.
Take, for example, the MWE take back. One an-
notator chose to alias this MWE to retract while
the other annotator chose reclaim. It is safe to say
that both of these lexical verbs are equally logical
choices for take back and have similar semantic
and syntactic qualities. In other cases, annotators
had discovered different senses in their research
of usages, and therefore the aliases reflect differ-
ent senses of the MWE. Instances like these were
marked as disagreements, resulting in a mislead-
ingly low IAA. After discussion of disagreements,
IAA for these 83 MWEs rose to 78%, leaving 18
MWEs for which the annotators were unable to
agree on a strategy. Annotation proceeded with an
additional 76 MWEs, and for this set annotators
disagreed on only 6 MWEs. This process demon-
strates that although annotators may not agree on
the first alias that comes to mind, they tend to
agree on similar verbs that can capture the seman-
tics of an MWE appropriately. In a final adjudica-
tion pass, adjudicators discussed the cases of dis-
agreement with the annotators and made a final de-
cision on the strategy to be pursued.
In all, 159 unique MWEs were examined in
double-blind annotation. Of these, 21 were dis-
carded either because annotators felt they were
not truly MWEs, and could be treated composi-
tionally, or because they were very slight variants
of other MWEs. The following table shows how
many of the remaining 138 MWEs were agreed
upon for aliasing (and how many of these were
thought to be LVCs), how many cases led to the
addition of new rolesets, how many will be la-
beled IE in future annotation, and how many will
remain classed with the existing Take senses (note
that 4 MWEs were classed as having both a poten-
tial alias for LVC usages, and requiring rolesets
or another strategy for other usages; for example,
take the count discussed above). Overall, this pilot
MWE Example Strategy Count
take tumble Alias-LVC 45
take it easy Alias-nonLVC 55
take down Roleset(s) Created 20
take count IE 4
take home Take.XX 18
Table 1: MWE cases addressed by each strategy.
demonstrated that the approach is promising, con-
sidering that it requires only about 20 new rolesets
to be created, as opposed to over 138 (given that
some MWEs have multiple senses, requiring mul-
tiple rolesets). As annotations move on to addi-
tional MWEs involving other verbs, a similar re-
duction in the roleset workload will be invaluable
to expanding PB.
5 Future Work
The next step in this research is to complete the
roleset unification, which allows the aliasing to
take effect. This process is currently underway.
Once this is complete, an investigation of take
annotations using the unified rolesets will be un-
dertaken, with special focus on whether IAA for
take instances is improved, and whether perfor-
mance of automatic Semantic Role Labeling and
Word Sense Disambiguation applications trained
on this data is improved. If results in these areas
are promising, this research will shift to analyzing
make, get, and have MWEs with this methodology.
Acknowledgments
We gratefully acknowledge the support of the
National Science Foundation Grant NSF-IIS-
1116782, A Bayesian Approach to Dynamic Lex-
ical Resources for Flexible Language Process-
ing, and funding under the BOLT and Machine
97
Reading programs, HR0011-11-C-0145 (BOLT)
FA8750-09-C-0179 (M.R.). Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K.
Griffitt, U. Hermjakob, K. Knight, P. Koehn, M.
Palmer, and N. Schneider 2013. Abstract Mean-
ing Representation for Sembanking. Proceedings of
the Linguistic Annotation Workshop.
Claire Bonial, Julia Bonn, Kathryn Conger, Jena D.
Hwang and Martha Palmer. In preparation. Prop-
Bank: Semantics of New Predicate Types. Pro-
ceedings of the Language Resources and Evaluation
Conference - LREC-2014. Reykjavik, Iceland.
Jan Haji?c, Eva Haji?cov, Jarmila Panevov, Petr Sgall,
Silvie Cinkov, Eva Fu?ckov, Marie Mikulov, Petr
Pajas, Jan Popelka, Ji?r Semeck?y, Jana
?
Sindlerov,
Jan
?
St?epnek, Josef Toman, Zde?nka Ure?sov, Zden?ek
?
Zabokrtsk?y. 2012. Prague Czech-English Depen-
dency Treebank 2.0. Linguistic Data Consortium,
Philadelphia.
Afsaneh Fazly. 2007. Automatic Acquisition of Lexical
Knowledge about Multiword Predicates. PhD The-
sis, Department of Computer Science, University of
Toronto.
Christiane Fellbaum (Ed.) 1998. Wordnet: An Elec-
tronic Lexical Database. MIT press, Cambridge.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue and
Martha Palmer. 2010. PropBank Annotation of
Multilingual Light Verb Constructions Proceedings
of the Linguistic Annotation Workshop held in con-
junction with ACL-2010. Uppsala, Sweden.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The Sketch Engine. Proceedings of
EURALEX. Lorient, France.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. 2013.
Improving machine translation into Chinese by tun-
ing against Chinese MEANT. Proceedings of 10th
International Workshop on Spoken Language Trans-
lation (IWSLT 2013). Heidelberg, Germany.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics
31(1):71?106.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take and Dan Flickinger. 2002. Multiword Expres-
sions: A Pain in the Neck for NLP. In Proceedings
of the Third International Conference on Intelligent
Text processing and Computational Linguistics (CI-
CLING 2002) 1?15. Mexico City, Mexico
98
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 12?20,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
Challenges of Adding Causation to Richer Event Descriptions
Rei Ikuta
*
, William F. Styler IV
+
, Mariah Hamang
*
, Tim O?Gorman
*
, and Martha Palmer
*
Department of Linguistics, University of Colorado at Boulder
*
{rei.ikuta, mariah.hamang, ogormant, martha.palmer}@colorado.edu,
+
will@savethevowels.org
Abstract
The goal of this study is to create guide-
lines for annotating cause-effect relations
as part of the Richer Event Description
schema. We present the challenges faced
using the definition of causation in terms
of counterfactual dependence and propose
new guidelines for cause-effect annotation
using an alternative definition which treats
causation as an intrinsic relation between
events. To support the use of such an in-
trinsic definition, we examine the theoret-
ical problems that the counterfactual def-
inition faces, show how the intrinsic defi-
nition solves those problems, and explain
how the intrinsic definition adheres to psy-
chological reality, at least for our annota-
tion purposes, better than the counterfac-
tual definition. We then evaluate the new
guidelines by presenting results obtained
from pilot annotations of ten documents,
showing that an inter-annotator agreement
(F1-score) of 0.5753 was achieved. The
results provide a benchmark for future
studies concerning cause-effect annotation
in the RED schema.
1 Introduction: The RED schema and
cause-effect relation
Richer Event Description (Styler et al., 2014a)
is an annotation schema which is developed ?as
a synthesis of the THYME-TimeML guidelines
1
,
the Stanford Event coreference guidelines and
the Carnegie Mellon University Event coreference
guidelines.? In other words, it combines Corefer-
ence (Pradhan et al., 2007; Lee et al. , 2012) and
THYME Temporal Relations annotation (Styler
1
The THYME annotation schema also includes corefer-
ence annotation.
et al. (2014b)) to provide a thorough representa-
tion of entities (including events) and their rela-
tions, including temporal relations. An overview
of the annotation process, which shows how coref-
erence and temporal annotations are combined, is
described in the following section.
The RED schema therefore attempts to anno-
tate cause-effect relations, which are annotated in
neither Coreference nor THYME (Styler et al.,
2014b). There is a synergy between annotating
both causal and temporal relations, since causes
necessarily precede their effects.
Other characteristics of the cause-effect annota-
tion in RED are that it allows annotators to make
inferences without relying on explicit connectives
or verbs of causation, that it is not domain specific,
and that it allows the relation to cross one (but not
more than one) sentence boundary.
1.1 The annotation process
The process of RED annotation is divided into
two passes: the first in which entities (including
events) are annotated, and the second in which re-
lations between those entities are annotated.
In the first pass, annotators identify three types
of entities: events (an occurence with a definitive
temporal duration), temporal expressions such as
August 2013, and other entities that have an ex-
istence as opposed to an occurrence (e.g., proper
nouns, objects, and pronouns). Specific properties
of each event are also annotated in this pass (e.g.,
its relation to the document creation time, whether
it is an actual event or a hypothetical event, etc.).
After the annotations in the first pass have been
adjudicated, annotators mark temporal, cause-
effect, and coreference relations between the en-
tities identified in the first pass. Temporal rela-
tions (e.g., before, overlaps, contains) are anno-
tated between two events or between an event and
a TIMEX3, cause-effect relations are annotated
between two events, and coreference relations are
12
annotated between two entities (e.g., President
John F. Kennedy ... he) or two events (an earth-
quake ... the quake). Coreference relations include
part-whole and set-member relations, as well as
identical relations in which two entities share a ref-
erent.
As a result of combining Coreference and
THYME, different coreference and temporal rela-
tions between an event pair can be covered by a
single relation in RED. For example, a part-whole
relation between events annotated in Coreference
(e.g., an incision and a surgury) is a subset of tem-
poral ?contains? relation in RED.
Therefore, the goal of RED is to combine Coref-
erence and THYME annotation, while finding
overlaps between the two and also introducing
cause-effect annotation to achieve a richer repre-
sentation of entities, events, and their relations.
1.2 Overview of the following sections
In the following sections, we present the chal-
lenges faced during our first pilot annotation and
why we decided to change the definition of causa-
tion, from a counterfactual one to an intrinsic one.
To support the use of the intrinsic definition, we
examine the theoretical problems that the coun-
terfactual definition faces, show how the intrinsic
definition solves those problems, and explain how
the intrinsic definition adheres to psychological re-
ality, at least for our annotation purposes, better
than the counterfactual definition. We then pro-
pose new guidelines based on the intrinsic defini-
tion and evaluate them by presenting results ob-
tained from our second pilot annotations of ten
documents, showing that an inter-annotator agree-
ment (F1-score) of 0.5753 was achieved.
2 Challenges of cause-effect annotation
using the counterfactual definition
The pilot annotations were done by three annota-
tors who are native speakers of English and are
experienced in linguistic annotation, on English
proxy reports (i.e., approximations of intelligence
agency reports) written by Garland, et al. (2013).
Our original guidelines were based on the coun-
terfactual definition of causation, as defined be-
low. Early on in the annotation process, the cause-
effect annotation was halted and removed from
the RED schema because there were a number of
cases in which events matched our guidelines for
the cause-effect relation but did not match our in-
tuitions about the relation.
2.1 Counterfactual definition of causation
In the original guidelines for cause-effect rela-
tions, we defined causation as follows:
? ?X caused Y? means if X had not occurred,
Y would not have occurred.
This definition of causation in terms of counterfac-
tual dependence (as philosophers call it) has been
the most popular definition of causation in the field
of philosophy for the past forty years since David
Lewis?s possible world model (Lewis, 1973) and
remain influential in contemporary studies such
as the structural model (Pearl, 2000; Halpern and
Pearl, 2005).
Using this definition, one annotator marked two
causal relations between the two event pairs in the
following sentence
2
:
(1) PYONGYANG INSISTS IT WILL
ALLOW FULL IAEA INSPEC-
TIONS ONLY WHEN A SIGNIFI-
CANT PORTION OF THE PROJECT
AS DEFINED IN THE 1994 ACCORD
IS COMPLETED.
Annotations:
ALLOW causes INSPECTIONS
DEFINED causes PROJECT
These annotations are done perfectly in line with
the guidelines
3
, since there would be no inspec-
tions if there were no allowing, and there would
be no projects if there were no defining (of the
project). Furthermore, one could argue that the
1994 accord causes Pyongyang to insist, since if
there had been no such accord, Pyongyang would
not have been able to insist anything pertaining to
it, although the annotators did refrain from creat-
ing such a causal annotation.
However, the relation between these event pairs
does not match our intuition about what causation
is. For example, the allowing should be consid-
ered as a precondition for the inspections, and not
2
Another annotator who annotated the same text did not
mark any causal relations in this sentence.
3
The annotation guidelines allow future events to be in
causal relations, although the counterfactual definition only
deals with past events, and for quoted speech, narrators are
assumed to be reliable. Thus, future events can participate in
a causal relation if the narrator is certain about the relation.
If the relation is presented to be likely or hypothetical instead
of being actual, annotators can mark such modalities also.
13
the cause. Furthermore, the guideline creates too
many event pairs that are potentially in a cause-
effect relation (such as the accord and the insist-
ing), contributing to confusion among annotators.
A similar issue can be seen in the following sen-
tence, in which the internet should be considered
as a possible precondition of funding, and not the
cause:
(2) THE WORKSHOP WILL STUDY
THE USE OF THE INTERNET TO
PROMOTE TERRORISM AND THE
INTERNET?S ROLE IN FACILITAT-
ING MONEY TRANSACTIONS AND
FUNDING TERRORIST GROUPS.
Annotation:
INTERNET?S causes FUNDING
Therefore, we concluded that the counterfactual
definition of causation is not optimal for our anno-
tation guidelines, and that we need an alternative
definition of causation which does not rely on an-
notators to consider a possible world in which the
cause does not occur.
Such an alternative definition, which we call the
intrinsic definition, has been argued for by Men-
zies (1996; 1999; 2014). Such a definition treats
causation as an intrinsic relation between events,
meaning that it is ?a local relation depending on
the intrinsic properties of the events and what goes
on between them, and nothing else? (Menzies,
2014).
Drawing on Menzies idea, we propose the fol-
lowing definition of causation which is being used
in our new guidelines for cause-effect annotation:
? ?X caused Y? means Y was inevitable given
X.
With this definition, annotators would not have to
consider any possible worlds in which an event did
not occur in order to annotate cause-effect rela-
tions, and only have to focus on whether Y nec-
essarily follows X, according to the context and
their encyclopedic knowledge of the world.
In order to support our use of such a definition,
we also present the challenges that the counterfac-
tual definition faces in terms of theory and psycho-
logical reality in the following sections, and show
how the intrinsic definition solves those problems.
3 Theoretical challenge of the
counterfactual definition
The two situations below illustrate theoretical
challenges which are faced by the counterfactual
definition but not by the intrinsic definition.
3.1 Multiple causes
? There are three events (1, 2 and 3), and three
individuals (A, B, and C). Events 1 and 2
occur at the same time, and event 3 follows
events 1 and 2.
? In event 1, A shoots C in the head.
? In event 2, B shoots C in the heart.
? In event 3, C dies.
? Then, an autopsy reveals that each of the
shots C received (one in the head, shot by A,
and the other in the heart, shot by B) was suf-
ficient by itself to kill C.
In the above situation (a modified version of the
example in Lagnado et al. (2013)), the counter-
factual definition would falsely predict that both
events 1 and 2 are not the causes of event 3, since
even if event 1 did not occur, event 3 would have
occurred because of event 2, and if event 2 did
not occur, event 3 would have occurred because
of event 1.
Acknowledging this problem, Halpern and
Pearl (2005) retain the counterfactual notion and
extend their causal model by stating that counter-
factual dependence should be evaluated relative to
certain contingencies. According to this defini-
tion, the counterfactual dependence of event 1 to
event 3 should be evaluated relative to a contin-
gency in which event 2 does not occur. The ob-
vious problem that this extended model faces is
the difficulty of finding a principled way to de-
cide which contingencies are allowed. Although
Halpern and Pearl (2005) do offer a complex set
of conditions that are aimed at capturing the in-
tuition that one should only invoke contingen-
cies ?that do not interfere with active causal pro-
cesses,? the question of which contingencies are
allowed is non-trivial and is the subject of ongo-
ing debate (Halpern and Hitchcock, 2010; Hiddle-
ston, 2005; Hopkins and Pearl, 2003; Lagnado et
al., 2013).
This situation, however, is easily handled by the
intrinsic definition, since event 3 (the death of C) is
14
inevitable given event 1 (A shooting C in the head)
regardless of other events, and event 3 is inevitable
given event 2 (B shooting C in the heart) regard-
less of other events, according to what we know
about the results of the autopsy. Thus the intrinsic
definition correctly predicts that both events 1 and
2 are equally the causes of event 3.
3.2 Oxygen, lightning, and wildfire
? There are three events (1, 2 and 3). Event 1
is a state encompassing events 2 and 3, and
event 3 follows event 2.
? In event 1, oxygen exists.
? In event 2, a lightning strikes a tree in a forest.
? In event 3, a wildfire starts in the forest.
In this situation described by Halpern and Hitch-
cock (2013), event 1 (the existence of oxygen)
would be predicted as being one of the causes of
event 3 (wildfire), since if oxygen did not exist, a
wildfire would not start. However, they argue that
human intuition would treat only event 2, and not
event 1, as a cause of event 3.
To counter this problem, Halpern and Hitchcock
(2013) again extend the counterfactual model,
stating that potential causes are graded according
to the normality of their witnesses (a witness is
a world in which a potential cause is the actual
cause of an outcome). In this extended model, the
world in which oxygen exists is more normal than
the world in which lightning strikes a particular
tree. Therefore, the lightning, being less normal,
?receives a higher causal grading.? In their causal
model, a static ranking of the witnesses are given
before the processing (i.e., causal inference) starts,
and thus it is possible to compute which witness
receives a higher causal grading.
Unlike the extended counterfactual definition,
the intrinsic definition does not assume a given
ranking of the world, and thus it is especially use-
ful when applied to annotation tasks. For exam-
ple, annotators would identify a causal relation be-
tween the oxygenation and the wildfire in the fol-
lowing sentence:
(3) The oxygenation of the atmosphere
accompanied by a lightning strike trig-
gered the first wildfire in Earth?s history.
But not in the following:
(4) The first wildfire in Earth?s history
was caused by a lightning strike in the
Proterozoic, an era noted for the evolu-
tion of multicellular organisms, glacia-
tions, and the oxygenation of the atmo-
sphere.
Even though the two events (oxygenation and
wildfire) described in the above sentences refer to
the same events in the world, the annotators can
choose whether to note a causal link between them
depending on the inevitability implied by the text.
In sentence (2), it is suggested that the wildfire was
inevitable given the oxygenation and the strike,
thus both of the events would be annotated as the
cause, while sentence (3) does not imply such a
causal relation. This would effectively let the an-
notators avoid marking cause-effect relations be-
tween births and deaths in texts such as obituaries
and medical reports. Such varying interpretations
of texts are not possible with the original counter-
factual definition, or with Halpern and Hitchcocks
extended counterfactual model (2013) which as-
sumes a given ranking of witnesses which is avail-
able to the writer but not to the annotator.
4 Challenge of the counterfactual
definition in terms of psychological
reality
In addition to the theoretical problem that the
counterfactual definition faces, experiments done
by White (2006) have shown that counterfactual
dependence is not used as preferred evidence for
making causal inference when subjects are pas-
sively (i.e., without the ability to intervene) ex-
posed to a scenario in which there are a number
of events affecting one another.
In one of the experiments, subjects are pre-
sented with scenarios concerning two game re-
serves, in each of which live five species, who may
or may not prey on each other. For each reserve,
there are five statements corresponding to five con-
secutive seasons, and each statement describes
whether the population of each of the species has
changed in that season. Based on the statements,
the subjects must decide whether a change in the
population of one species causes changes in that
of the others. The subjects are instructed that if
the population of X changed and that of Y did not
in a given season, they are supposed to conclude
that X does not prey on Y, because if it did, the
populations of both X and Y would have changed.
15
In other words, the subjects are explicitly told to
rely on counterfactual dependence as evidence for
making causal inference. The five statements pro-
vided enough counterfactually dependent relations
for the subjects to reach one correct answer.
However, the results of the experiment show
that only 5 out of 36 subjects made correct judg-
ments on the predator-prey (cause-effect) relations
in both reserves, and the success rates were be-
low optimum and not far above chance. Instead,
the answers by the subjects showed that they were
more likely to rely on the temporal order of events
as the evidence for the causal relations (i.e., ?the
population of X changed in season 1 and that of
Y changed in season 2, thus X must be the preda-
tor of Y?), although they were instructed to rely on
counterfactual dependence within the same season
instead.
White (2006) carried out three additional ex-
periments, one in which he changed the order of
the seasons, another in which subjects were told
that the seasons were in random order and that the
temporal order is irrelevant to the answer, and the
last in which the scenario was changed to a situa-
tion where the levels of five chemicals in a blood
stream affect each other. The subjects? answers
exhibited more reliance on counterfactual depen-
dence in the experiment where they were told that
temporal order is irrelevant, but the other experi-
ments showed similar results with the first experi-
ment.
Thus, White (2006) concludes that there is a
preference for basing causal inference on domain-
specific causal knowledge (i.e., ?the population
change in season 1 must be causally related to
the change in season 2, according to what we
know about ecosystems?) over counterfactual de-
pendence, when such knowledge is available for
use and when subjects are passively exposed
4
to a
complex scenario in which there are a number of
events affecting one another.
These results support our motivation to avoid
using the counterfactual definition, since annota-
tors are passively exposed to text without the abil-
ity to intervene, texts to be annotated are complex
systems in which a number of events may or may
not affect each other, and it is usually the case
4
It has been claimed that subjects perform better in mak-
ing causal inferences on complex structures when they are
actively exposed to (i.e., have the ability to intervene with)
the structures (Lagnado and Sloman, 2004; Sloman and
Lagnado, 2005; Steyvers et al., 2003).
that domain-specific causal knowledge is avail-
able. The use of an intrinsic definition for cause-
effect annotation, on the other hand, is in line with
the results of these experiments, since annotators
would not have to consider any possible worlds
where some event does not occur, and only have to
focus on whether Y necessarily follows X, accord-
ing to the context and their encyclopedic knowl-
edge of the world.
5 The new guidelines
Given the challenges faced by the counterfactual
definition and the advantages of the intrinsic defi-
nition presented above, we developed new guide-
lines for cause-effect annotation which instruct an-
notators as follows:
? In our schema, we annotate ?X CAUSES
Y? if, according to the writer, the particular
EVENT Y was inevitable given the particu-
lar EVENT X.
We then utilized the counterfactual definition as
the definition of precondition relations as follows:
? We annotate ?X PRECONDITIONS Y? if,
according to the writer, had the particu-
lar EVENT X not happened, the particular
EVENT Y would not have happened.
The reason we kept the counterfactual definition
in our guidelines as a definition of a precondition
relation is that the relation defined by counterfac-
tual dependence still gives us information about
the temporal relation between events; if we know
that Y would not have happened if X had not hap-
pened, we also know that X started before Y.
6 The second pilot annotation
Using the new guidelines, ten proxy reports were
each annotated by two annotators. One of them
was among the two annotators who participated in
our first pilot annotation, and the other, who is also
a native speaker of English experienced in linguis-
tic annotation, was trained using the old guidelines
but only started annotating in the RED schema
after the cause-effect annotation was halted, and
thus had not actually annotated cause-effect rela-
tions until the second pilot. The following sections
present the inter-annotator agreement of cause and
precondition annotations done in the ten reports
and the analysis of specific examples where the
annotators disagreed.
16
6.1 Inter-annotator agreement
This section presents the inter-annotator agree-
ment (IAA) obtained from the second pilot annota-
tion, and analyzes the annotations to examine the
sources of disagreement between the annotators.
Perhaps the most important thing to note before
discussing the specific numbers and examples is
that this pilot annotation did not include the ad-
judication stage between the first pass where en-
tities including events and temporal expressions
are identified, and the second pass where the rela-
tions between those entities are marked (see Sec-
tion 1.1 for the specifics of the annotation pro-
cess). Therefore, many of the disagreements in
the causation and precondition annotations involve
disagreements in the first pass.
A total of 114 relations (50 causation and 64
precondition relations) were created by the two an-
notators. Among them, 24 exhibited perfect match
between the annotators, while 18 exhibited par-
tial match (meaning that they agreed on whether
the relation was causation/precondition, but dis-
agreed on other aspects of the relation, such as
the modality and temporal relation
5
) . Among the
114 relations, 72 relations showed disagreements,
but 69 of them involved disagreements in the first
pass. Upon analysis, we judged 41 of those 69
disagreements as being avoidable by introducing
the adjudication stage between the two passes, and
28 as having the potential of surviving adjudica-
tion, meaning that even if the adjudication were
properly done, the same parts of the text may still
cause similar disagreements. Only 3 among the 72
disagreements occurred purely in the second pass,
meaning that the annotators completely agreed on
what the entities involved in the 3 relations should
be, but disagreed on the relation.
Thus, the results give us four types of IAA
(best-case, realistic, worst-case, and extra-strict),
shown in Table 1 as F1-scores.
The best-case IAA assumes that all disagree-
ments involving disagreements in the first pass
5
As well as marking the modality (whether the relation is
stated as being actual, likely or hypothetical) and the temporal
relation (whether the cause ends before the effect starts or
cause overlaps with the effect), annotators have a choice of
marking a relation as ?difficult? when they are not sure of
their annotation. This difficulty marking was not considered
when judging whether the two annotators agreed completely
or not. In other words, even if one annotator marked a relation
as difficult and the other did not, the annotation would be
considered as showing complete agreement as long as other
properties of the annotation matched.
F1-score
Best-case 0.9333
Realistic 0.5753
Worst-case 0.3684
Extra-strict 0.2105
Table 1: Inter-annotator agreement for the second
pilot annotation
will not show up as issues in the second pass, and
only takes into account the 3 disagreements that
occurred purely in the second pass.
The realistic IAA takes into account the 28 dis-
agreements involving disagreements in the first
pass that have the potential of surviving adjudica-
tion.
The worst-case IAA assumes that all disagree-
ments in the first pass survive adjudication.
Finally, the extra-strict IAA allows relations to
be judged as agreeing only when the two anno-
tations completely match, including the modality
and the temporal relations marked together with
causation/precondition.
6.2 Evaluation of the inter-annotator
agreement
This section compares the IAA presented above
with results shown in a previous study by Styler
et al. (2014b) which deals with temporal relation
annotations in the clinical domain. In their study,
Styler et al (2014b) reported results from annota-
tions done on a subset of the THYME colon cancer
corpus, which includes clinical notes and pathol-
ogy reports for 35 patients diagnosed with colon
cancer for a total of 107 documents. Two grad-
uate or undergraduate students in the Department
of Linguistics at the University of Colorado anno-
tated each text. For the annotation guidelines, they
used the THYME-TimeML guidelines which are
also used within the RED guidelines for temporal
relation annotation. Unlike the annotations in this
current study, the temporal relation annotations on
the THYME corpus were done after the identifica-
tion of events and temporal expressions were ad-
judicated (the THYME-TimeML schema does not
identify entities that are not events or temporal ex-
pressions). Therefore, the IAA they presented (Ta-
ble 2) are not affected by the disagreements at the
level of event identification.
The figure for ?participants only? shows the
IAA concerning cases in which the annotators
17
F1-score
Participants only 0.5012
Participants and relation 0.4506
?Contains? relaion 0.5630
Table 2: Inter-annotator agreement presented in
Styler et al. (2014b)
agreed that there is some sort of a temporal re-
lation between the two participants, but did not
necessarily agree on which temporal relation (be-
fore, overlap, contains, etc.) holds between them.
The figure for ?participants and relation? shows
the agreement on both the participants and the type
of the temporal relation. The third figure is the
IAA for the temporal relation ?contains,? which
exhibited the highest IAA among all the temporal
relations.
These figures are significantly higher than the
results reported for the 2012 i2b2 challenge (Sun
et al., 2013), in which the F1-score for ?partici-
pants only? IAA was 0.39.
The realistic IAA of 0.5753 obtained in this cur-
rent study is not far-off from the figures by Styler
et al. (2014b), which shows that causation and pre-
condition annotations using the new guidelines are
indeed feasible.
6.3 Examples of disagreements
Below, we present examples of different types
of disagreements observed in the annotations.
The annotations are represented in the form of
?EVENT relation-relation EVENT.? The first half
of the relation indicates the temporal relation an-
notated between the events, and the latter half
shows whether there was a causation or a precon-
dition relation between the events. For example,
?P before-cause Q? indicates that event P hap-
pened before and caused event Q.
6.3.1 Disagreement in the 1st pass: avoidable
by adjudication
(5) A BUDGET WAS ALLO-
CATED FOR THE BARRIER TO
BE EQUIPPED WITH ELECTRONIC
DETENTION EQUIPMENT.
Annotations by annotators X and Y:
X: ALLOCATED before-preconditions
EQUIPPED
Y: BUDGET before-preconditions
EQUIPPED
In (5), annotator X marked allocated as an event
while not marking budget as an event, and Y an-
notated budget as an event and did not mark allo-
cated as an event. If the adjudication was correctly
done, only marking allocated as an event and not
budget, it is likely that Y would have annotated the
same way as X.
6.3.2 Disagreement in the 1st pass: not
avoidable by adjudication
(6) CRITICS STATE THAT WITH AC-
CESS TO PLUTONIUM AVAILABLE
FROM ROGUE STATES TERROR-
ISTS COULD CONSULT THE DE-
TAILED DOCUMENTS AND BUILD
AN ATOMIC BOMB.
Y: CONSULT before-preconditions
BUILD
X: No relations identified
In (6), the annotators did disagree on whether
the two events consult and build happen after or
overlap with the document creation time (Doc-
Time). X annotated those two events as overlap-
ping the DocTime, while Y annotated them as af-
ter the DocTime. The annotators agreed that those
two events were hypothetical events. Although
such a disagreement about the temporal property
of the events may have caused the disagreements
about whether there should be a precondition rela-
tion, it is likely that X would have missed what Y
had found even if there had been adjudication.
6.3.3 Disagreements in the 2nd pass
(7) THE SMH AND JENNINGS WERE
THEN SUED OVER 3 ARTICLES
PUBLISHED IN THE LEAD-UP TO
THE 000000 OLYMPICS.
X: No relations identified
Y: PUBLISHED before-preconditions
SUED
(8) HEAD OF A TAJIK GOVERN-
MENT AGENCY THAT FIGHTS
DRUG TRAFFICKING AVAZ YUL-
DACHEV STATED THAT HEROIN
USERS ARE ILL AND NEED
TREATMENT.
X: ILL overlap-cause NEED
Y: No relations identified
18
(7) and (8) above show cases in which one an-
notator missed the relation that the other annotator
identified, even though both annotators completely
agreed on the property of the entities involved in
the relation.
7 Conclusion
In this paper, we have presented the challenges
that the counterfactual definition of causation
faces in terms of its application to annotation
guidelines, theory, and psychological reality. We
have shown that the intrinsic definition better suits
our purpose of annotation, and proposed new
guidelines for annotating cause-effect relations us-
ing such a definition. The new guidelines were
evaluated using results obtained from a pilot an-
notation of ten documents. An inter-annotator
agreement (F1-score) of 0.5753 was obtained. We
are currently in the process of training four addi-
tional annotators with the new guidelines, and fu-
ture studies concerning cause-effect annotation in
the RED schema can assess their performances by
using results presented in this paper as a bench-
mark.
Acknowledgments
The project described was supported by DARPA
FA-8750-13-2-0045, subaward 560215 (via LDC)
DEFT: Deep Exploration and Filtering of Text and
NIH: 1 R01 LM010090-01A1, THYME (via Har-
vard). The content is solely the responsibility of
the authors and does not necessarily represent the
official views of DARPA or NIH.
References
Garland, J., Fore, D., Strassel, S., and Grimes, S.
2013. DEFT Phase 1 Narrative Text Source Data
R1 LDC2013E19. Web download file. Philadelphia:
Linguistic Data Consortium
Halpern, J. Y., and Hitchcock, C. 2010. Actual cau-
sation and the art of modeling. In R. Dechter, H.
Geffner,and J. Y. Halpern, eds., Heuristics, proba-
bility and causality: A Tribute to Judea Pearl. (pp.
383?406). London: College Publications.
Halpern, J. Y., and Hitchcock, C. 2013. Compact Rep-
resentations of Extended Causal Models. Cognitive
Science, 37:986?1010.
Halpern, J. Y., and Pearl, J. 2005. Causes and explana-
tions: A structural-model approach. Part I: Causes.
The British Journal for the Philosophy of Science,
56(4):843?887.
Hiddleston, E. 2005. A causal theory of counterfactu-
als. Nous, 39(4):632?657.
Hopkins, M., and Pearl, J. 2003. Clarifying the usage
of structural models for commonsense causal rea-
soning. In P. Doherty, J. McCarthy, M. Williams,
eds., Proceedings of the AAAI Spring Symposium on
Logical Formalization of Commonsense Rea-soning.
(pp. 83?89). Menlo Park, CA: AAAI Press.
Knobe, J., and Fraser, B. 2008. Causal judgment and
moral judgment: Two experiments. In W. Sinnott-
Armstrong, eds., Moral psychology, Volume 2: The
cognitive science of morality. (pp. 441?447). Cam-
bridge, MA: MIT Press.
Lagnado, D. A., Gerstenburg, T., and Zultan, R. 2013.
Causal Responsibility and Counterfactuals. Cogni-
tive Science 37:1036?1073.
Lagnado, D. A., and Sloman, S. 2004. The advan-
tage of timely intervention. Journal of Experimen-
tal Psychology: Learning, Memory and Cognition,
30:856?876.
Lee, H., Recasens, M., Chang, A., Surdeanu, M., and
Jurafsky, D. 2012. Joint entity and event corefer-
ence resolution across documents. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), Jeju Is-
land, 489-500.
Lewis, D. 1973. Causation. The Journal of Philoso-
phy, 70(17):556?567.
Menzies, P. 1996. Probabilistic Causation and the Pre-
emption Problem. Mind, 105:85?117.
Menzies, P. 1999. Intrinsic versus Extrinsic Concep-
tions of Causation. In H. Sankey, ed., Causation and
Laws of Nature, Kluwer Academic Publishers, pp.
313?29.
Menzies, P. 2014. Counterfactual Theories of Causa-
tion. In E. N. Zalta, ed., The Stanford Encyclopedia
of Philosophy. Retrieved from http://plato.
stanford.edu/archives/spr2014/
entries/causation-counterfactual/
Pearl, J. 2000. Causality. Cambridge: Cambridge
University Press.
Pradhan, S. Ramshaw, L., Weischedel, R., MacBride,
J., and Micciulla, L. 2007. Unrestricted
Coreference: Indentifying Entities and Events in
OntoNotes. In Proceedings of the IEEE Interna-
tional Conference on Semantic Computing (ICSC),
September 17-19.
Sloman, S., and Lagnado, D. A. 2005. Do we ?do??
Cognitive Science, 29:5?39.
Steyvers, M., Tenenbaum, J. T., Wagenmakers, E. J.,
and Blum, B. 2003. Inferring causal networks from
observations and interventions. em Cognitive Sci-
ence, 27:453?489.
19
Styler, W., Crooks, K., O?Gorman, T., and Hamang,
M. 2014a. Richer Event Description (RED) Anno-
tation Guidelines. Unpublished manuscript, Univer-
sity of Colorado at Boulder.
Styler, W. F., Bethard, S., Finan, S., Palmer, M., Pra-
dhan, S., de Groen, P. C., Erickson, B., Miller,
T., Lin, C., Savova, G., and Pustejovsky., J.
2014b. Temporal Annotation in the Clinical Do-
main, Transactions of the Association of Compu-
tational Linguistics, 2:143?154.
White, P. A. 2006. How well is causal structure in-
ferred from cooccurrence information? European
Journal of Cognitive Psychology, 18 (3):454?480.
20
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 13?17,
Baltimore, Maryland USA, June 27, 2014. c?2014 Association for Computational Linguistics
SemLink+: FrameNet, VerbNet and Event Ontologies 
 Martha Palmer, Claire Bonial Department of Linguistics University of Colorado  Boulder, CO  mpalmer/Claire.Bonial@colorado.edu 
 Diana McCarthy Department of Theoretical and Applied Linguistics (DTAL)  University of Cambridge  diana@dianamccarthy.co.uk     Abstract This paper reviews the significant contributions FrameNet has made to our understanding of lexical resources, semantic roles and event relations. 1 Introduction  One of the great challenges of Natural Language Processing (NLP) is the multitude of choices that language gives us for expressing the same thing in different ways. This is obviously true when taking other languages into consideration - the same thought can be expressed in English, French, Chinese or Russian, with widely varying results. However, it is also true when considering a single language such as English. Light verb constructions, nominalizations, idioms, slang, paraphrases, and synonyms all give us myriads of alternatives for ?coining a phrase.?  This causes immense difficulty for NLP systems.  No one has made greater contributions to advancing the state of the art of lexical semantics, and its applications to NLP, than Chuck Fillmore.  In this paper we focus on the central role that FrameNet has played in our development of SemLink+ and in our current explorations into event ontologies that can play a practical role in accurate automatic event extraction. 2 Detecting events An elusive goal of current NLP systems is the accurate detection of events ? recognizing the meaningful relations among the topics, people, 
places   and  events   buried  within text. These relations can be very complex, and are not always explicit, requiring subtle semantic interpretation of the data.  For instance, NLP systems must be able to automatically recognize that Stock prices sank and The stock market is falling can be describing the same event. Such an interpretation relies upon a  recognition of the similarity between sinking and falling, as well as noting the connection between stock prices and the stock market, and, finally, acknowledgment that they are playing the same role. A key element in event extraction is the identification of the participants of an event, such as the initiator of an action and any parties affected by it.  Basically who did what to whom, when, where, why and how? Many systems today rely on semantic role labeling to help identify participants, and lexical resources that provide an inventory of possible predicate argument structures for individual lexical items are crucial to the success of semantic role labeling (Palmer,et al., 2010).  3 SemLink+ and  Semantic Roles SemLink (Palmer, 2009) is an ongoing effort to map complementary lexical resources: PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2008), FrameNet (Fillmore et al., 2004), and the recently added OntoNotes (ON) sense groupings (Weischedel, et al., 2011). They all associate semantic information with the propositions in a sentence.  Each was created independently with somewhat differing goals, and they vary in the level and nature of semantic detail represented. FrameNet is the 
13
most fine-grained with the richest semantics, VerbNet     focuses    on     syntactically-based generalizations that carry semantic implications, and the relatively coarse-grained PropBank has been shown to provide the most effective training data for supervised Machine Learning techniques.  Nonetheless, they can be seen as complementary rather than conflicting, and together comprise a whole that is greater than the sum of its parts. SemLink serves as a platform to unify these resources.  The recent addition of ON sense groupings, which can be thought of as a more coarse-grained view of WordNet (Fellbaum, 1998), provides even broader coverage for verbs, and a level of representation that is appropriate for linking between VerbNet class members and FrameNet lexical units, as described below.    SemLink unifies these lexical resources at several different levels.  First by providing type-to-type mappings between the lexical units for each framework.  For PropBank these are the very coarse-grained rolesets, for VerbNet  they are verbs that are members of VerbNet classes, and for FrameNet they are the lexical units associated with each Frame.  The same lemma can have multiple PropBank rolesets and can be in several VerbNet classes and FrameNet frames, but always with different meanings. In general, the mappings from PropBank to VerbNet or FrameNet tend to be 1-many, while the mappings between VerbNet and FrameNet are more likely to be 1-1.  For example, the verb hear has just one coarse-grained sense in PropBank, with the following roleset:  Arg0: hearer Arg1: utterance, sound Arg2: speaker, source of sound  This roleset maps to both the Discover and See classes of VerbNet, and the Hear and Perception_experience frames of FrameNet.      Then, for each lexical unit, SemLink also supplies a mapping between the semantic roles of PropBank and VerbNet, as well as the roles of  VerbNet and FrameNet. PropBank uses very generic labels such as Arg0 and Arg1, which correspond to Dowty?s Prototypical Agent and Patient, respectively (Dowty, 1991).  PropBank has up to six numbered arguments 
for core verb specific roles and for adjuncts it has several generally applicable ArgModifiers that have function tag labels such as: MaNneR, TeMPoral, LOCation, DIRection, GOaL, etc. VerbNet uses more traditional linguistic thematic role labels, with about 30 in total, and assumes adjuncts (ArgM?s) will be supplied by PropBank based semantic role labelers.  FrameNet is even more fine-grained and has frame-specific core and peripheral roles called Frame Elements for each frame, amounting to over 2000 individual Frame Element types.  For example, He talked about politics would receive the following semantic role labels from each framework.1   PropBank (talk.01) HeArg0 talkedRELATION about politicsArg1    VerbNet (Talk-37.5):  HeAGENT talkedRELATION about politicsTOPIC  FrameNet (Statement frame):  HeSPEAKER talkedRELATION about politicsTOPIC      Thanks to Chuck Fillmore?s careful guidance, the rich, meticulously crafted Frames in FrameNet, with their detailed descriptions of all possible arguments and their relations to each other, offer the potential of providing a foundation for inferencing about events and their consequences.  In addition FrameNet has from the beginning been inclusive in its addition of nominal and adjectival forms to the Frames, which greatly increases our coverage of all predicating elements (Bonial, et al., 2014).  There is also a comprehensive FrameNet Constructicon that painstakingly lists many phrasal constructions, such as ?the Xer, the Yer? that cannot be found anywhere else (Fillmore, et al., 2012). Many of these frames, including the constructions, apply equally well to other languages,  as evidenced by the various efforts to develop FrameNets in other languages2 promising a likely benefit to multilingual information 
                                                            1 Arg0 maps to Agent maps to Speaker.  Arg1 maps to Topic maps to Topic. 2 See FrameNet projects in other languages listed at https://framenet.icsi.berkeley.edu/fndrupal/framenets_in_other_languages 
14
processing as well.  Given the close theoretical ties between PropBank, VerbNet and FrameNet, it should be possible to bootstrap from the successful PropBank-based automatic semantic role labelers to equally accurate FrameNet and VerbNet annotators, and to improve overall semantic role labeling performance (Bauer & Rambow, 2011; Dipanjan, et al., 2010; Giuglea & Moschitti, 2006; Merlo & der Plas, 2009; Yi, et al., 2007).  That is one of the primary goals of SemLink.     The first release of SemLink (1.1) contained mappings between these three lexical resources as well as a set of PropBank instances from the Wall Street Journal data with mappings to VerbNet classes and thematic roles (Palmer, 2009).  Our most recent release, SemLink 1.2,3 now includes mappings to FrameNet frames and Frame Elements wherever they are available (FN version 1.5), as well as ON sense groupings (Bonial, et al., 2013). The mapping files between PropBank and VerbNet (version 3.2), and FrameNet have also been checked for consistency and updated to more accurately reflect the current relations between these resources.    This annotated corpus can now be used to train and evaluate VerbNet Class and FrameNet Frame classifiers, to explore clusters of Frame Elements that map to the same VerbNet and PropBank semantic roles, and to evaluate approaches to semantic role labeling that use the type-to-type mappings to bootstrap VerbNet and FrameNet role labels from automatic PropBank semantic role labels. 4 Events, Event Types and Subevents Accurate and informative semantic role labels are an essential component of event extraction, but, although necessary, they are not sufficient. Automatic event detection also requires the ability to distinguish between events which are truly separate, such as Yesterday, John was throwing a ball to Mary and Bill was flying a kite, as opposed to related events such as John was washing the dishes and Mary was drying them.  The second pair could be seen as temporally related subevents of an overall doing the dishes or cleaning up                                                             3 available for download here: http://verbs.colorado.edu/semlink/ 
the kitchen event. It can sometimes be quite challenging to determine the relationship between two events. For instance, earthquakes are quite often associated with the collapse of buildings, as in the following example, The quake destroyed parts of Sausalito.  All tall buildings were demolished.  Many readers might agree that the earthquakes CAUSED the demolishment of the buildings. However, are the building collapses also SUBEVENTs of the earthquakes?  Sometimes they happen a few days later, or immediately, simultaneously with the earthquake. Are they both subevents? In general, for accurate event detection, it would be very useful to know which events must precede, must follow, or cannot be simultaneous with, which other events.   As discussed in the 2013 NAACL Events workshop and this year?s ACL Events workshop, clear, consistent annotation of events and their coreference and causal and temporal relations is a much desired but very challenging goal (Ikuta & Palmer, 2014).  Any assistance that can be provided by lexical resources is welcome. Another very important contribution that FrameNet has made is in the realm of defining these kinds of relations, and others, between frames.  Parent-Child Frame to Frame relations can include Inheritance, Subframe, Perspective On, Using, Causative Of, Inchoative of, and there is also a Precedes temporal ordering relation.   The DEFT working group in Richer Event Descriptions has recently been exploring expanding the ACE and ERE event types, and how they can be mapped onto a broader ontological context.  Exploring the FrameNet relations that the relevant lexical items participate in has been most informative. We first examined the simple LDC ERE classification of Conflict events, which has demonstrations and attacks as siblings (ERE guidelines). We find FrameNet?s classification of attacks as Hostile-Encounters quite useful, and have no argument with it having an Inheritance relation with Intentionally_act, and a Using relation with Taking_sides. Demonstrations, on the other hand, come under the Protest Frame, which has a Using relation with Taking_sides. The FrameNet 
15
organization of demonstrations and attacks, although perfectly justifiable, doesn?t map neatly onto the LDC organization since, although they are close, they are not siblings.  However, by also considering SUMO (Niles & Pease, 2001), the Predicate Matrix (de Lacalle , et al., 2014), WordNet and VerbNet, we were able to develop the upper level partial Event Ontology given in Figure 1, which comfortably incorporates the ERE and FrameNet relations within a broader framework, preserving the key aspects of each.   We are now discussing the ERE Life events, birth, death, injury, marriage, divorce, etc., and FrameNet is again proving to be inspirational.  SemLink+ will encompass our growing Event Ontology, as well as the mappings between the resources and the multiple layers of annotation on the same data.  
 Figure 1 ? SemLink+ Event Ontology, partial  5 Conclusion Since computers do not interact with and experience the world the same way humans do, how could they ever interpret language describing the world the same way humans do?  That NLP has made as much progress as it has is truly phenomenal, and there is much more still that can be done.  Rich, detailed, lexical resources like FrameNet are major stepping stones that will enable continued improvements in the automatic representation of sentences in context. FrameNet, and WordNet, PropBank, VerbNet and SemLink+, provide priceless, invaluable information about myriads of different types of events and the creative ways in which they can be expressed, 
as well as rich details about all of their possible participants.  If we can harness the power of distributional semantics to help us dynamically extend and enrich what has already been manually created, we may find our computers to be much smarter than we ever imagined them to be. Acknowledgments This work has benefited immensely from comments and suggestions during the discussions of the RED working group on Event Ontologies, especially from Teruko Mitamura, Annie Zaenen, Ann Bies, and German Rigau.  We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing, DARPA FA-8750-13-2-0045, subaward 560215 (via LDC) DEFT: Deep Exploration and Filtering of Text, DARPA Machine Reading (via BBN), and NIH: 1 R01 LM010090-01A1, THYME, (via Harvard).  The content is solely the responsibility of the authors and does not necessarily represent the official views of DARPA, NSF or NIH. References Daniel Bauer & Owen Rambow, 2011, Increasing Coverage of Syntactic Subcategorization Patterns in FrameNet Using VerbNet, In the Proceedings of the IEEE Fifth International Conference on Semantic Computing. Claire Bonial, Julia Bonn, Kathryn Conger, Jena Hwang and Martha Palmer, 2014.  PropBank: Semantics of New Predicate Types. The 9th edition of the Language Resources and Evaluation Conference. Reykjavik, Iceland.  Claire Bonial,  Kevin, Stowe, and Martha Palmer, 2013. Renewing and Revising SemLink. The GenLex Workshop on Linked Data in Linguistics, held with GenLex-13.  Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith, 2010. Probabilistic Frame-Semantic Parsing. In Proceedings of the NAACL 2010.  David Dowty, 1991. Thematic Proto-Roles and Argument Selection. Language, 67:547-619 Christiane Fellbaum, 1998. WordNet: An Electronic Lexical Data-base. Language, Speech and Communications. MIT Press 
16
Charles. J. Fillmore; Collin F. Baker, and H. Sato, 2004. FrameNet as a ``Net".  In Proceedings of LREC 2004, 4, pages 1091-1094 Charles J. Fillmore, Russell R. Lee-Goldman, and Russell Rhodes. 2012. ?The FrameNet Constructicon? Boas, H.C. and Sag, I.A. (Eds.) Sign-based Construction Grammar, CSLI Publications. Ana-Maria Guiglea and Alessandro Moschitti. 2006. Semantic role labeling via FrameNet, VerbNet and PropBank. In Proceedings of Coling-ACL 2006, pages 929?936. Rei Ikuta and Martha Palmer (2014) Challenges of Adding Causation to Richer Event Descriptions, In the Proceedings of 2nd Events Workshop, held with ACL 2014, Baltimore, MD. Karin Kipper, Anna Korhonen, Neville Ryant and Martha Palmer. 2008. A Large-Scale Classification of English Verbs. Language Resources and Evaluation Journal, 42(1):21?40 Maddalen Lopez de Lacalle, Egoitz Laparra, German Rigau, 2014, Predicate Matrix: extending SemLink throughWordNet mappings, The 9th edition of the Language Resources and Evaluation Conference. Reykjavik, Iceland.  Ian Niles and Adam Pease, 2001. Towards a Standard Upper Ontology. In Proceedings of the 2nd International Conference on Formal 
Ontology in Information Systems (FOIS-2001), Chris Welty and Barry Smith, eds, Ogunquit, Maine, October 17-19, 2001. Paola Merlo and Lonneke van der Plas. 2009. Abstraction and generalization in semantic role labels: PropBank, VerbNet or both?, In the Proceedings of  ACL 2009. Martha Palmer, 2009. SemLink: Linking PropBank, VerbNet and FrameNet, In the Proceedings of the Generative Lexicon Conference, GenLex-09. Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71?106 Martha Palmer, Daniel Gildea and Nianwen Xue. Semantic Role Labeling. 2010. Synthesis Lectures on Human Language Technology Series, ed. Graeme Hirst, Morgan and Claypoole. Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha Palmer, Robert Belvin, Sameer Pradan, Lance Ramshaw and Nianwen Xue. OntoNotes: A Large Training Corpus for Enhanced Processing, included in Part 1 : Data Acquisition and Linguistic Resources of the Handbook of Natural Language Processing and Machine Translation: Global Automatic Language Exploitation Editors: Joseph Olive, Caitlin Christianson, John McCary, Springer Verglag, pp 54-63, 201 
 
17
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 47?55,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Adapting Predicate Frames for Urdu PropBanking
Riyaz Ahmad Bhat
?
, Naman Jain
?
, Dipti Misra Sharma
?
, Ashwini Vaidya
?
,
Martha Palmer
?
, James Babani
?
and Tafseer Ahmed
?
LTRC, IIIT-H, Hyderabad, India
?
University of Colorado, Boulder, CO 80309 USA
?
DHA Suffa University, Karachi, Pakistan
?
{riyaz.bhat, naman.jain}@research.iiit.ac.in, dipti@iiit.ac.in,
{vaidyaa,mpalmer, james.babani}@colorado.edu, tafseer@dsu.edu.pk
Abstract
Hindi and Urdu are two standardized reg-
isters of what has been called the Hindus-
tani language, which belongs to the Indo-
Aryan language family. Although, both
the varieties share a common grammar,
they differ significantly in their vocabulary
to an extent where both become mutually
incomprehensible (Masica, 1993). Hindi
draws its vocabulary from Sanskrit while
Urdu draws its vocabulary from Persian,
Arabic and even Turkish. In this paper,
we present our efforts to adopt frames of
nominal and verbal predicates that Urdu
shares with either Hindi or Arabic for
Urdu PropBanking. We discuss the fea-
sibility of porting such frames from either
of the sources (Arabic or Hindi) and also
present a simple and reasonably accurate
method to automatically identify the ori-
gin of Urdu words which is a necessary
step in the process of porting such frames.
1 Introduction
Hindi and Urdu, spoken primarily in northern In-
dia and Pakistan, are socially and even officially
considered two different language varieties. How-
ever, such a division between the two is not es-
tablished linguistically. They are two standard-
ized registers of what has been called the Hindus-
tani language, which belongs to the Indo-Aryan
language family. Masica (1993) explains that,
while they are different languages officially, they
are not even different dialects or sub-dialects in
a linguistic sense; rather, they are different liter-
ary styles based on the same linguistically defined
sub-dialect. He further explains that at the collo-
quial level, Hindi and Urdu are nearly identical,
both in terms of core vocabulary and grammar.
However, at formal and literary levels, vocabu-
lary differences begin to loom much larger (Hindi
drawing its higher lexicon from Sanskrit and Urdu
from Persian and Arabic) to the point where the
two styles/languages become mutually unintelligi-
ble. In written form, not only the vocabulary but
the way Urdu and Hindi are written makes one be-
lieve that they are two separate languages. They
are written in separate orthographies, Hindi be-
ing written in Devanagari, and Urdu in a modi-
fied Persio-Arabic script. Given such (apparent)
divergences between the two varieties, two paral-
lel treebanks are being built under The Hindi-Urdu
treebanking Project (Bhatt et al., 2009; Xia et al.,
2009). Both the treebanks follow a multi-layered
and multi-representational framework which fea-
tures Dependency, PropBank and Phrase Structure
annotations. Among the two treebanks the Hindi
treebank is ahead of the Urdu treebank across all
layers. In the case of PropBanking, the Hindi tree-
bank has made considerable progress while Urdu
PropBanking has just started.
The creation of predicate frames is the first step
in PropBanking, which is followed by the actual
annotation of verb instances in corpora. In this
paper, we look at the possibility of porting re-
lated frames from Arabic and Hindi PropBanks for
Urdu PropBanking. Given that Urdu shares its vo-
cabulary with Arabic, Hindi and Persian, we look
at verbal and nominal predicates that Urdu shares
with these languages and try to port and adapt their
frames from the respective PropBanks instead of
creating them afresh. This implies that identifi-
cation of the source of Urdu predicates becomes
a necessary step in this process. Thus, in order
to port the relevant frames, we need to first iden-
tify the source of Urdu predicates and then extract
their frames from the related PropBanks. To state
briefly, we present the following as contributions
of this paper:
? Automatic identification of origin or source
of Urdu vocabulary.
47
? Porting and adapting nominal and verbal
predicate frames from the PropBanks of re-
lated languages.
The rest of the paper is organised as follows: In
the next Section we discuss the Hindi-Urdu tree-
banking project with the focus on PropBanking.
In Section 3, we discuss our efforts to automati-
cally identify the source of Urdu vocabulary and
in Section 4, we discuss the process of adapting
and porting Arabic and Hindi frames for Urdu
PropBanking. Finally we conclude with some
future directions in Section 5.
2 A multi-layered,
multi-representational treebank
Compared to other existing treebanks, Hindi/Urdu
Treebanks (HTB/UTB) are unusual in that they are
multi-layered. They contain three layers of anno-
tation: dependency structure (DS) for annotation
of modified-modifier relations, PropBank-style
annotation (PropBank) for predicate-argument
structure, and an independently motivated phrase-
structure (PS). Each layer has its own framework,
annotation scheme, and detailed annotation guide-
lines. Due to lack of space and relevance to our
work, we only look at PropBanking with reference
to Hindi PropBank, here.
2.1 PropBank Annotation
The first PropBank, the English PropBank (Kings-
bury and Palmer, 2002), originated as a one-
million word subset of the Wall Street Journal
(WSJ) portion of Penn Treebank II (an English
phrase structure treebank). The verbs in the Prop-
Bank are annotated with predicate-argument struc-
tures and provide semantic role labels for each
syntactic argument of a verb. Although these
were deliberately chosen to be generic and theory-
neutral (e.g., ARG0, ARG1), they are intended
to consistently annotate the same semantic role
across syntactic variations. For example, in both
the sentences John broke the window and The win-
dow broke, ?the window? is annotated as ARG1
and as bearing the role of ?Patient?. This reflects
the fact that this argument bears the same seman-
tic role in both the cases, even though it is realized
as the structural subject in one sentence and as the
object in the other. This is the primary difference
between PropBank?s approach to semantic role la-
bels and the Paninian approach to karaka labels,
which it otherwise resembles closely. PropBank?s
ARG0 and ARG1 can be thought of as similar
to Dowty?s prototypical ?Agent? and ?Patient?
(Dowty, 1991). PropBank provides, for each sense
of each annotated verb, its ?roleset?, i.e., the possi-
ble arguments of the predicate, their labels and all
possible syntactic realizations. The primary goal
of PropBank is to supply consistent, simple, gen-
eral purpose labeling of semantic roles for a large
quantity of coherent text that can provide training
data for supervised machine learning algorithms,
in the same way that the Penn Treebank supported
the training of statistical syntactic parsers.
2.1.1 Hindi PropBank
The Hindi PropBank project has differed signif-
icantly from other PropBank projects in that the
semantic role labels are annotated on dependency
trees rather than on phrase structure trees. How-
ever, it is similar in that semantic roles are defined
on a verb-by-verb basis and the description at
the verb-specific level is fine-grained; e.g., a
verb like ?hit? will have ?hitter? and ?hittee?.
These verb-specific roles are then grouped into
broader categories using numbered arguments
(ARG). Each verb can also have a set of modifiers
not specific to the verb (ARGM). In Table 1,
PropBank-style semantic roles are listed for
the simple verb de ?to give?. In the table, the
numbered arguments correspond to the giver,
thing given and recipient. Frame file definitions
are created manually and include role information
as well as a unique roleset ID (e.g. de.01 in Table
1), which is assigned to every sense of a verb. In
addition, for Hindi the frame file also includes the
transitive and causative forms of the verb (if any).
Thus, the frame file for de ?give? will include
dilvaa ?cause to give?.
de.01 to give
Arg0 the giver
Arg1 thing given
Arg2 recipient
Table 1: A Frame File
The annotation process for the PropBank takes
place in two stages: the creation of frame files for
individual verb types, and the annotation of pred-
icate argument structures for each verb instance.
The annotation for each predicate in the corpus
is carried out based on its frame file definitions.
48
The PropBank makes use of two annotation tools
viz. Jubilee (Choi et al., 2010b) and Cornerstone
(Choi et al., 2010a) for PropBank instance annota-
tion and PropBank frame file creation respectively.
For annotation of the Hindi and Urdu PropBank,
the Jubilee annotation tool had to be modified to
display dependency trees and also to provide ad-
ditional labels for the annotation of empty argu-
ments.
3 Identifying the source of Urdu
Vocabulary
Predicting the source of a word is similar to lan-
guage identification where the task is to identify
the language a given document is written in. How-
ever, language identification at word level is more
challenging than a typical document level lan-
guage identification problem. The number of fea-
tures available at document level is much higher
than at word level. The available features for word
level identification are word morphology, syllable
structure and phonemic (letter) inventory of the
language(s).
In the case of Urdu, the problem is even more
complex as the borrowed words don?t necessarily
carry the inflections of their source language and
don?t retain their identity as such (they undergo
phonetic changes as well). For example, khabar
?news? which is an Arabic word declines as per
the morphological paradigm of feminine nom-
inals in Hindi and Urdu as shown in Table (2).
However, despite such challenges, if we look at
the character histogram in Figure (1), we can still
identify the source of a sufficiently large portion
of Urdu vocabulary just by using letter-based
heuristics. For example neither Arabic nor Persian
has aspirated consonants like bH, ph Aspirated
Bilabial Plosives; tSh, dZH Aspirated Alveolar
Fricatives; ?H Aspirated Retroflex Plosive; gH, kh
Aspirated Velar Plosives etc. while Hindi does.
Similarly, the following sounds occur only in
Arabic and Persian: Z Fricative Postalveolar; T,
D Fricative Dental; ? Fricative Pharyngeal; X
Fricative Uvular etc. Using these heuristics we
could identify 2,682 types as Indic, and 3,968
as either Persian or Arabic out of 12,223 unique
types in the Urdu treebank (Bhat and Sharma,
2012).
Singular Plural
Direct khabar khabarain
Oblique khabar khabaron
Table 2: Morphological Paradigm of khabar
This explains the efficiency of n-gram based ap-
proaches to either document level or word level
language identification tasks as reported in the re-
cent literature on the problem (Dunning, 1994;
Elfardy and Diab, 2012; King and Abney, 2013;
Nguyen and Dogruoz, 2014; Lui et al., 2014).
In order to predict the source of an Urdu word,
we frame two classification tasks: (1) binary clas-
sification into Indic and Persio-Arabic and, (2) tri-
class classification into Arabic, Indic and Persian.
Both the problems are modeled using smoothed n-
gram based language models.
3.1 N-gram Language Models
Given a word w to classify into one of k classes
c
1
, c
2
, ... , c
k
, we will choose the class with the
maximum conditional probability:
c
?
= argmax
c
i
p(c
i
|w)
= argmax
c
i
p(w|c
i
) ? p(c
i
)
(1)
The prior distribution p(c) of a class is esti-
mated from the respective training sets shown in
Table (3). Each training set is used to train a
separate letter-based language model to estimate
the probability of word w. The language model
p(w) is implemented as an n-gram model using
the IRSTLM-Toolkit (Federico et al., 2008) with
Kneser-Ney smoothing. The language model is
defined as:
p(w) =
n
?
i=1
p(l
i
|l
i?1
i?k
) (2)
where, l is a letter and k is a parameter indicat-
ing the amount of context used (e.g., k = 4 means
5-gram model).
3.2 Etymological Data
In order to prepare training and testing data
marked with etymological information for our
classification experiments, we used the Online
1
http://www.langsci.ucl.ac.uk/ipa/IPA chart %28C%
292005.pdf
49
bH Z T ? X D sQ tQ dQ Q DQ G f tSh q ?H gH khdZH N ? ph S ? th t?h d?H tS b d g H k dZ m l n p s r t t? V j d?
0
5 ? 10
?2
0.1
0.15
0.2
0.25
0.3
0.35
Alphabets in IPA
1
R
e
l
a
t
i
v
e
F
r
e
q
u
e
n
c
y
Arabic
Hindi
Persian
Urdu
Figure 1: Relative Distribution of Arabic, Hindi, Persian and Urdu Alphabets (Consonants only)
Urdu Dictionary
2
(henceforth OUD). OUD has
been prepared under the supervision of the e-
government Directorate of Pakistan
3
. Apart from
basic definition and meaning, it provides etymo-
logical information for more than 120K Urdu
words. Since the dictionary is freely
4
available
and requires no expertise for extraction of word
etymology which is usually the case with manual
annotation, we could mark the etymological infor-
mation on a reasonably sized word list in a limited
time frame. The statistics are provided in Table
(3). We use Indic as a cover term for all the words
that are either from Sanskrit, Prakrit, Hindi or lo-
cal languages.
Language Data Size Average Token Length
Arabic 6,524 6.8
Indic 3,002 5.5
Persian 4,613 6.5
Table 3: Statistics of Etymological Data
2
http://182.180.102.251:8081/oud/default.aspx
3
www.e-government.gov.pk
4
We are not aware of an offline version of OUD.
3.3 Experiments
We carried out a number of experiments in order
to explore the effect of data size and the order of
n-gram models on the classification performance.
By varying the size of training data, we wanted to
identify the lower bound on the training size with
respect to the classification performance. We var-
ied the training size per training iteration by 1%
for n-grams in the order 1-5 for both the classifi-
cation problems. For each n-gram order 100 ex-
periments were carried out, i.e overall 800 exper-
iments for binary and tri-class classification. The
impact of training size on the classification perfor-
mance is shown in Figures (2) and (3) for binary
and tri-class classification respectively. As ex-
pected, at every iteration the additional data points
introduced into the training data increased the per-
formance of the model. With a mere 3% of the
training data, we could reach a reasonable accu-
racy of 0.85 in terms of F-score for binary classi-
fication and for tri-class classification we reached
the same accuracy with 6% of the data.
Similarly, we tried different order n-gram mod-
els to quantify the effect of character context on
50
the classification performance. As with the in-
crease in data size, increasing the n-gram order
profoundly improved the results. In both the clas-
sification tasks, unigram based models converge
faster than the higher order n-gram based models.
The obvious reason for it is the small, finite set of
characters that a language operates with (? 37 in
Arabic, ? 39 in Persian and ? 48 in Hindi). A
small set of words (unique in our case) is probably
enough to capture at least a single instance of each
character. As no new n-gram is introduced with
subsequent additions of new tokens in the training
data, the accuracy stabilizes. However, the accu-
racy with higher order n-grams kept on increas-
ing with an increase in the data size, though it was
marginal after 5-grams. The abrupt increase after
8,000 training instances is probably due to the ad-
dition of an unknown bigram sequence(s) to the
training data. In particular, the Recall of Persio-
Arabic increased by 2.2%.
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 2: Learning Curves for Binary Classifica-
tion of Urdu Vocabulary
3.4 Results
We performed 10-fold cross validation over all the
instances of the etymological data for both the bi-
nary and tri-class classification tasks. We split the
data into training and testing sets with a ratio of
80:20 using the stratified sampling. Stratified sam-
pling distributes the samples of each class in train-
ing and testing sets with the same percentage as in
the complete set. For all the 10-folds, the order of
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 3: Learning Curves for Tri-class Classifi-
cation of Urdu Vocabulary
n-gram was varied again from 1-5. Tables (4) and
(5) show the consolidated results for these tasks
with a frequency based baseline to evaluate the
classification performance. In both the tasks, we
achieved highest accuracy with language models
trained with 5-gram letter sequence context. The
best results in terms of F-score are 0.96 and 0.93
for binary and tri-class classification respectively.
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.40 0.50 0.40
1-gram 0.89 0.89 0.89
2-gram 0.95 0.95 0.95
3-gram 0.96 0.96 0.96
4-gram 0.96 0.96 0.96
5-gram 0.96 0.96 0.96
Table 4: Results of 10-fold Cross Validation on
Binary Classification
Although, we have achieved quite reasonable
accuracies in both the tasks, a closer look at the
confusion matrices shown in Tables (6) and (7)
show that we can still improve the accuracies by
balancing the size of data across classes. In binary
classification our model is more biased towards
Persio-Arabic as the data is highly imbalanced.
Our binary classifier misclassifies 0.86% of Indic
tokens as Persio-Arabic since the prior probability
of the latter is much higher than that of the former.
While in the case of tri-class classification, using
51
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.15 0.33 0.21
1-gram 0.83 0.83 0.83
2-gram 0.89 0.89 0.89
3-gram 0.91 0.91 0.91
4-gram 0.93 0.93 0.93
5-gram 0.93 0.93 0.93
Table 5: Results of 10-fold Cross Validation on
Tri-Class Classification
higher order n-gram models can resolve the
prominent confusion between Arabic and Persian.
Since both Arabic and Persian share almost the
same phonetic inventory, working with lower
order n-gram models doesn?t seem ideal.
Class Indic Persio-Arabic
Indic 235 60
Persio-Arabic 15 1,057
Table 6: Confusion Matrix of Binary Classifica-
tion
Class Arabic Indic Persian
Arabic 605 5 26
Indic 11 268 18
Persian 22 9 415
Table 7: Confusion Matrix of Tri-class Classifica-
tion
4 Adapting Frames from Arabic and
Hindi PropBanks
As discussed in Section 2.1.1, the creation of pred-
icate frames precedes the actual annotation of verb
instances in a given corpus. In this section, we de-
scribe our approach towards the first stage of Urdu
PropBanking by adapting related predicate frames
from Arabic and Hindi PropBanks (Palmer et al.,
2008; Vaidya et al., 2011). Since a PropBank
is not available for Persian, we could only adapt
those predicate frames which are shared with Ara-
bic and Hindi.
Although, Urdu shares or borrows most of its
literary vocabulary from Arabic and Persian, it re-
tains its simple verb (as opposed to compound or
complex verbs) inventory from Indo-Aryan ances-
try. Verbs from Arabic and Persian are borrowed
less frequently, although there are examples such
as ?khariid? buy, ?farma? say etc.
5
This over-
lap in the verb inventory between Hindi and Urdu
might explain the fact that they share the same
grammar.
The fact that Urdu shares its lexicon with these
languages, prompted us towards exploring the
possibility of using their resources for Urdu Prop-
Banking. We are in the process of adapting frames
for those Urdu predicates that are shared with ei-
ther Arabic or Hindi.
Urdu frame file creation must be carried out for
both simple verbs and complex predicates. Since
Urdu differs very little in simple verb inventory
from Hindi, this simplifies the development pro-
cess as the frames could be ported easily. How-
ever, this is not the case with nominal predicates.
In Urdu, many nominal predicates are borrowed
from Arabic or Persian as shown in Table (8).
Given that a PropBank for Persian is not available,
the task of creating the frames for nominal predi-
cates in Urdu would have been fairly daunting in
the paucity of the Arabic PropBank, as well.
Simple Verbs Nominal Predicates
Language Total Unique Total Unique
Arabic 12 1 6,780 765
Hindi 7,332 441 1,203 258
Persian 69 3 2,276 352
Total 7,413 445 10,259 1,375
Table 8: Urdu Treebank Predicate Statistics
4.1 Simple Verbs
The simple verb inventory of Urdu and Hindi is
almost similar, so the main task was to locate and
extract the relevant frames from Hindi frame files.
Fortunately, with the exception of farmaa ?say?,
all the other simple verbs which Urdu borrows
from Persian or Arabic (cf. Table (8)) were also
borrowed by Hindi. Therefore, the Hindi sim-
ple verb frame files sufficed for porting frames for
Urdu simple verbs.
There were no significant differences found be-
tween the Urdu and Hindi rolesets, which describe
either semantic variants of the same verb or its
causative forms. Further, in order to name the
frame files with their corresponding Urdu lemmas,
we used Konstanz?s Urdu transliteration scheme
5
Borrowed verbs often do not function as simple verbs
rather they are used like nominals in complex predicate con-
structions such as mehsoos in ?mehsoos karnaa? to feel.
52
(Malik et al., 2010) to convert a given lemma into
its romanized form. Since the Hindi frame files
use the WX transliteration scheme
6
, which is not
appropriate for Urdu due to lack of coverage for
Persio-Arabic phonemes or sounds like dQ ?pha-
ryngealized voiced alveolar stop?. The frame files
also contain example sentences for each predicate,
in order to make the PropBank annotation task eas-
ier. While adapting the frame files from Hindi
to Urdu, simply transliterating such examples for
Urdu predicates was not always an option, because
sentences consisting of words with Sanskrit origin
may not be understood by Urdu speakers. Hence,
all the examples in the ported frames have been
replaced with Urdu sentences by an Urdu expert.
In general we find that the Urdu verbs are quite
similar to Hindi verbs, and this simplified our task
of adapting the frames for simple verbs. The
nouns, however, show more variation. Since a
large proportion (up to 50%) of Urdu predicates
are expressed using verb-noun complex predi-
cates, nominal predicates play a crucial role in our
annotation process and must be accounted for.
4.2 Complex Predicates
In the Urdu treebank, there are 17,672 predicates,
of which more than half have been identified as
noun-verb complex predicates (NVC) at the de-
pendency level. Typically, a noun-verb complex
predicate chorii ?theft? karnaa ?to do? has two
components: a noun chorii and a light verb karnaa
giving us the meaning ?steal?. The verbal compo-
nent in NVCs has reduced predicating power (al-
though it is inflected for person, number, and gen-
der agreement as well as tense, aspect and mood)
and its nominal complement is considered the true
predicate. In our annotation of NVCs, we fol-
low a procedure common to all PropBanks, where
we create frame files for the nominal or the ?true?
predicate (Hwang et al., 2010). An example of a
frame file for a noun such as chorii is described in
Table (9).
The creation of a frame file for the set of
true predicates that occur in an NVC is impor-
tant from the point of view of linguistic annota-
tion. Given the large number of NVCs, a semi-
automatic method has been proposed for creating
Hindi nominal frame files, which saves the man-
ual effort required for creating frames for nearly
6
http://en.wikipedia.org/wiki/WX notation
Frame file for chorii-n(oun)
chorii.01: theft-n light verb: kar?do; to steal?
Arg0 person who steals
Arg1 thing stolen
chorii.02 : theft-n light verb: ho ?be/become; to
get stolen?
Arg1 thing stolen
Table 9: Frame file for predicate noun chorii
?theft? with two frequently occurring light verbs
ho and kar. If other light verbs are found to occur,
they are added as additional rolesets as chorii.03,
chorii.04 and so on.
3,015 unique Hindi noun and light verb combina-
tions (Vaidya et al., 2013).
For Urdu, the process of nominal frame file cre-
ation is preceded by the identification of the ety-
mological origin for each nominal. If that nomi-
nal has an Indic or Arabic origin, relevant frames
from Arabic or Hindi PropBanks were adapted for
Urdu. On the other hand, if the Urdu nominal orig-
inates from Persian, then frame creation will be
done either manually or using other available Per-
sian language resources, in the future.
In Table (8), there are around 258 nominal pred-
icates that are common in Hindi and Urdu, so we
directly ported their frames from Hindi PropBank
with minor changes as was done for simple verb
frames. Out of 765 nominal predicates shared with
Arabic, 308 nominal predicate frames have been
ported to Urdu. 98 of these nominal predicate
frames were already present in the Arabic Prop-
Bank and were ported as such. However, for the
remaining 667 unique predicates, frames are be-
ing created manually by Arabic PropBanking ex-
perts and will be ported to Urdu once they become
available.
Porting of Arabic frames to Urdu is not that triv-
ial. We observed that while Urdu borrows vocabu-
lary from Arabic it does not borrow all the senses
for some words. In such cases, the rolesets that are
irrelevant to Urdu have to be discarded manually.
The example sentences for all the frames ported
from Arabic PropBank have to be sourced from
either the web or manually created by an Urdu ex-
pert, as was the case with Hindi simple verbs.
5 Conclusion
In this paper we have exploited the overlap be-
tween the lexicon of Urdu, Arabic and Hindi for
the creation of predicate frames for Urdu Prop-
53
Banking. We presented a simple and accurate clas-
sifier for the identification of source or origin of
Urdu vocabulary which is a necessary step in the
overall process of extraction of predicate frames
from the related PropBanks. In the case of sim-
ple verbs that occur in the Urdu treebank, we have
extracted all the frames from the Hindi PropBank
and adapted them for Urdu PropBanking. Simi-
larly for complex predicates, frames for Urdu tree-
bank nominal predicates are extracted from Hindi
as well as from Arabic PropBanks. Since a Prop-
Bank is not available for Persian, the creation
of frames for shared predicates with Persian is a
prospect for future work. We plan to create these
frames either manually or semi-automatically, us-
ing the available Persian Dependency treebanks
(Rasooli et al., 2011; Rasooli et al., 2013).
Acknowledgments
We would like to thank Himani Chaudhry for her
valuable comments that helped to improve the
quality of this paper.
The work reported in this paper is supported by
the NSF grant (Award Number: CNS 0751202;
CFDA Number: 47.070)
7
.
References
Riyaz Ahmad Bhat and Dipti Misra Sharma. 2012.
A dependency treebank of urdu and its evaluation.
In Proceedings of the Sixth Linguistic Annotation
Workshop, pages 157?165. Association for Compu-
tational Linguistics.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. A multi-representational and multi-layered
treebank for hindi/urdu. In Proceedings of the Third
Linguistic Annotation Workshop, pages 186?189.
Association for Computational Linguistics.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010a. Propbank frameset annotation guidelines us-
ing a dedicated editor, cornerstone. In LREC.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010b. Propbank instance annotation guidelines us-
ing a dedicated editor, jubilee. In LREC.
David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language, 67(3):547?619.
7
Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the author(s)
and do not necessarily reflect the views of the National Sci-
ence Foundation.
Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Heba Elfardy and Mona T Diab. 2012. Token level
identification of linguistic code switching. In COL-
ING (Posters), pages 287?296.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
dling large scale language models. In Interspeech,
pages 1618?1621.
Jena D Hwang, Archna Bhatia, Clare Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and
Martha Palmer. 2010. Propbank annotation of mul-
tilingual light verb constructions. In Proceedings of
the Fourth Linguistic Annotation Workshop, pages
82?90. Association for Computational Linguistics.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110?1119.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In LREC. Citeseer.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation for Computational Linguistics, 2:27?40.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina B?ogel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliterat-
ing urdu for a broad-coverage urdu/hindi lfg gram-
mar. In LREC.
Colin P Masica. 1993. The Indo-Aryan Languages.
Cambridge University Press.
Dong Nguyen and A Seza Dogruoz. 2014. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Martha Palmer, Olga Babko-Malaya, Ann Bies,
Mona T Diab, Mohamed Maamouri, Aous Man-
souri, and Wajdi Zaghouani. 2008. A pilot arabic
propbank. In LREC.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
persian verbs: The first steps towards persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of a
persian syntactic dependency treebank. In Proceed-
ings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
54
Linguistics: Human Language Technologies, pages
306?314.
Ashwini Vaidya, Jinho D Choi, Martha Palmer, and
Bhuvana Narasimhan. 2011. Analysis of the hindi
proposition bank using dependency structure. In
Proceedings of the 5th Linguistic Annotation Work-
shop, pages 21?29. Association for Computational
Linguistics.
Ashwini Vaidya, Martha Palmer, and Bhuvana
Narasimhan. 2013. Semantic roles for nominal
predicates: Building a lexical resource. NAACL
HLT 2013, 13:126.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In The 7th International
Workshop on Treebanks and Linguistic Theories.
Groningen, Netherlands.
55
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 127?136,
Coling 2014, Dublin, Ireland, August 24 2014.
Light verb constructions with ?do? and ?be? in Hindi: A TAG analysis
Ashwini Vaidya
University of Colorado
Boulder, CO
80309, USA
vaidyaa@colorado.edu
Owen Rambow
Columbia University
New York, NY
10115, USA
rambow@ccls.columbia.edu
Martha Palmer
University of Colorado
Boulder, CO
80309, USA
mpalmer@colorado.edu
Abstract
In this paper we present a Lexicalized Feature-based Tree-Adjoining Grammar analysis for a
type of nominal predicate that occurs in combination with the light verbs ?do? and ?be? (Hindi
kar and ho respectively). Light verb constructions are a challenge for computational grammars
because they are a highly productive predicational strategy in Hindi. Such nominals have been
discussed in the literature (Mohanan, 1997; Ahmed and Butt, 2011; Bhatt et al., 2013), but this
work is a first attempt at a Tree-Adjoining Grammar (TAG) representation. We look at three
possibilities for the design of elementary trees in TAG and explore one option in depth using
Hindi data. In this analysis, the nominal is represented with all the arguments of the light verb
construction, while the light verb adjoins into its elementary tree.
1 Introduction
Lexical resource development for computational analyses in Hindi must contend with a large number of
light verb constructions. For instance, in the Hindi Treebank (Palmer et al., 2009), nearly 37% of the
predicates have been annotated as light verb constructions. Hence, the combination of a noun with a light
verb is a productive predicational strategy in Hindi. For example, the noun yaad ?memory? combines
with kar ?do? to form yaad kar ?remember?.
In light verb constructions, the noun is a predicating element along with the light verb. The pres-
ence of two predicating elements representing a single meaning is a challenge for a linguistic theory
that maps between syntax and semantics. Consequently, the argument structure representation for light
verb constructions (LVC) has resulted in two opposing views in syntactic theory. One view supports
a noun-centric analysis of the LVC, where the noun is represented with all the arguments of the LVC
e.g. (Grimshaw and Mester, 1988; Kearns, 1988). The light verb?s only role is to theta-mark the ar-
guments of the LVC, without any semantic contribution. The second view proposes argument sharing
between the noun and the light verb as they both contribute to the argument structure of the LVC (Butt,
1995; Ahmed et al., 2012). We refer to such analyses as verb-centric analyses.
Within the framework of this debate, we propose to use Lexicalized Feature-based Tree Adjoining
Grammar, which is a variant of Tree Adjoining Grammar (TAG). TAG has been used to represent light
verb constructions in French (Abeill?e, 1988) and Korean (Han and Rambow, 2000). The primitive struc-
tures of TAG are its elementary trees, which encapsulate the syntactic and semantic arguments of its
lexical anchor (for a light verb construction, the noun and light verb respectively will be the anchors).
The association of a structural object with a linguistic anchor allows TAG to specify all the linguistic
constraints associated with the anchor over a local domain. This is especially advantageous for compos-
ing the complex argument structure of a LVC. In comparison with other formalisms (e.g. context-free
grammars), this property gives TAG an extended domain of locality.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
127
In this paper, we look at a particular group of nouns that occur with light verbs ?do? and ?be? (kar and
ho) as part of a light verb construction. The same noun alternates with either light verb, resulting in a
change in the argument structure of the verb. For example the noun chorii ?theft? can occur as either chorii
kar ?theft do? or chorii ho ?theft happen?. There are nearly 265 nouns showing this alternation in the Hindi
Treebank (Palmer et al., 2009)
1
. These constitute about 15% of the total light verb constructions in the
Treebank. Note that other light verbs also occur in Hindi e.g. de ?give?, le ?take? etc. but they are not
part of this study.
Section 3 has some examples of these predicating nominals. Before this, Section 2 will introduce the
TAG formalism. Section 4 describes the design of the elementary trees that are the basis of the analysis
and in the final section we summarize our findings and make suggestions for future work.
2 Lexicalized Feature based Tree Adjoining Grammar
Tree-Adjoining Grammar (TAG) is a formal tree-rewriting system that is used to describe the syntax of
natural languages (Joshi and Schabes, 1997). The basic structure of a TAG grammar is an elementary
tree, which is a fragment of a phrase structure tree labelled with both terminal and non-terminal nodes.
The elementary trees are combined by the operations of substitution (where a terminal node is replaced
with a new tree) or adjunction (where an internal node is split to add a new tree).
The elementary trees in TAG can be enriched with feature structures (Vijay-Shanker and Joshi, 1988).
These can capture linguistic descriptions in a more precise manner and also capture adjunction con-
straints. TAG with feature structures is also known as FTAG (Feature-structure based TAG). A TAG can
also be lexicalized i.e., an elementary tree has a lexical item as one of its terminal nodes. Lexicalized
TAG enhanced with feature structures is known as Lexicalized Feature-based Tree-Adjoining Gram-
mar (LF-TAG). This has been used for developing computational grammars for English (XTAG-Group,
2001), French (Abeill?e and Candito, 2000) and Korean (Han et al., 2000). In our analysis, we will also
use LF-TAG, but we will refer to it as LTAG for convenience.
Figure 1 shows the basic steps for composing elementary trees containing feature structures. Each
node has a top and a bottom feature structure. Features can be shared among nodes in an elementary
tree. In the tree for the verb running, the variable
1
is used to show that the verb must share the same
features as the subject NP.
The tree for running is an initial tree with a single terminal for its argument noun phrase (NP). The
tree for is, on the other hand, is a special type of elementary tree called the auxiliary tree. It has a foot
node (marked with an asterisk), which is identical to its root node. The auxiliary tree will adjoin into the
tree for running at the VP node only. The top and bottom feature structures for MODE at the VP node,
have different values (ind and ger), and they cannot unify. This captures an adjunction constraint for
obligatory adjunction and requires adjunction to take place at this node only.
During adjunction, the top of the root of the auxiliary tree (for is) will unify with the top of the
adjunction site. The bottom of the foot of the auxiliary tree will unify with the bottom of the adjunction
site. During substitution, the top node in the tree for Jill unifies with the node at NP.
This results in the second tree in Figure 1, post the operations of substitution and adjunction. In a
final derivation step, top and bottom feature structures at each node will unify, to give the final derived
tree with a single feature structure at each node. The resulting tree is called a derived tree, but another
by-product of the TAG analysis is also the derivation tree. This tree has numbered node labels that record
the history of composition of the elementary trees. For example, the tree for Jill is running can be seen
in Figure 2. The root of this tree is labelled with running, which is an initial tree of the type S.
An important characteristic of lexicalized elementary trees is their correspondence with that lexi-
cal item?s predicate-argument structure. This has sometimes been formalized as the PACP (Predicate-
Argument Co-occurrence Principle) (Frank, 2002). The PACP restricts the structure of the elementary
trees such that they may not be drawn arbitrarily. At the same time, lexicalized TAGs will often have the
1
It is possible that many more nouns occur in this group, but all their alternations are not instantiated in the Treebank
corpus.
128
SVP
[
AGR=
1
MODE=ind
]
[
MODE=ger
]
V
running
NP
[
AGR=
1
]
NP
[]
[
AGR=[PERS=3 num=sg]
]
N
Jill
VP
[
AGR=
2
MODE=
3
]
VP*
[
MODE=ger
]
V
[
MODE=
3
ind
]
[
AGR=
2
[PERS=3 num=sg]
]
is
After substitution
and adjunction:
S
VP
[
AGR=
1
MODE=ind
]
[
AGR=
2
MODE=
3
]
VP
[
MODE=ger
]
[
MODE=ger
]
V
running
V
[
MODE=
3
ind
]
[
AGR=
2
[PERS=3 num=sg]
]
is
NP
[
AGR=
1
]
[
agr=[pers=3 num=sg]
]
Jill
After top-bottom unification:
S
VP
[
AGR=
1
[PERS=3 NUM=sg]
MODE=ind
]
VP
[
MODE=ger
]
V
running
V
[
AGR=
1
MODE=ind
]
is
NP
[
AGR=
1
]
Jill
Figure 1: LTAG showing feature structures and constraints on adjunction (Example adapted from
(Kallmeyer and Osswald, 2013))
running
isJill
1 2
Figure 2: Derivation tree for ?Jill is running?. The dashed node indicates adjunction and the solid node
indicates substitution
129
same lexical item realized as the anchor of varying syntactic realizations. For example a verb such as run
will anchor a different elementary tree for its passive or interrogative variant.
3 Data
In this section, we introduce the nominal predicates that will be the focus of our LTAG analysis. Such
nominals allow an agentive (ergative-marked
2
) subject with the light verb kar ?do?. In contrast, the same
nominal does not have an agentive subject with ho ?be? (Ahmed and Butt, 2011). The alternation with
ho ?be? has an intransitivizing effect. In (1) and (2), a change in the light verb results in the presence or
absence of the agent argument. The nominal chorii is the same, but the LVC in (1) requires only a Theme
argument, whereas (2) needs an Agent and a Theme.
(1) gehene
jewels.M
chorii
theft.F
hue.
be.Perf.MPl
?The jewels got stolen?
(2) Ram-ne
Ram-Erg
gehene
jewels.M.Pl
chorii
theft.F
kiye.
do.Perf.M.Pl
?Ram stole the jewels ?
In English, a similar alternation structure may be found with light verbs in bring to light vs. come to
light (Claridge, 2000). Here, two light verbs bring and come are used to express either a causative or
inchoative reading. In the Hindi examples, the light verb ho ?be? and the light verb kar ?do? are used to
express the inchoative vs. causative reading. In Persian, kardan ?make or do? and ?sodan ?become? are
used in a manner similar (although not identical) to Hindi.
The noun chorii ?theft? belongs to a particular class of nouns where a change in the light verb does
result in a change in the arguments, but the agent argument is always presupposed, irrespective of the
light verb. For instance, the addition of a phrase such as apne-aap ?on its own? is semantically odd with
example 1. This is because the event of ?theft? cannot occur without an agent, although it is unexpressed
with the light verb ho ?be?. Contrast this with 4, where apne-aap is not odd and where the alternation
with kar ?do? is not possible. The non-alternating noun afsos ?regret? occurs with an Experiencer subject,
which can act spontaneously and hence allows the use of apne-aap.
(3) ??aaj
today
apne-aap
own-mine
gehene
jewels.M
chorii
theft.F
hue
be.Pres.MPl
???Today the jewels got stolen by themselves ?
(4) aaj
today
Ram-ko
Ram-Dat
is
this
baat-par
issue-Loc
apne-aap
own-mine
afsos
regret.M
huaa/*kiyaa.
be.Perf.M.Sg/*do.Perf.M.Sg
?Today Ram himself regretted this point/issue ?
In order to model such nominals in TAG we have three options: first, a noun-centric analysis, where the
nominal projects all the arguments of the LVC. In reference to the examples above, this would imply that
the light verb chorii ?theft? would be represented by two trees? i.e., it would appear with two arguments
with kar ?do? and only one with ho ?be?.
The second option is a verb-centric analysis, where the light verb kar ?do? would contribute the agen-
tive argument, and chorii would contribute the object. The nominal?s elementary tree would consist of
only one argument, regardless of whether it combined with kar ?do? or ho ?be?. The third option is to
2
Hindi is a split-ergative language, where ergative case on the subject is found only with transitive verbs in the perfective
aspect. For non-perfective aspect, the subject is nominative.
130
represent the LVC chorii kar ?theft do; steal? as the anchor of a single elementary tree? a single multi-
word expression. While the first two options are worth exploring, we discard the third option for two
reasons: first, the LVC is highly productive in Hindi, which would imply that this would result in too
many elementary trees in the grammar. Second, there is evidence that the LVC forms a phrasal category
in the syntax (Mohanan, 1997; Davison, 2005). This means that individual components of the LVC may
be moved away from each other, emphatic particles or negation may intervene and the noun component
may be independently modified by an adjective. Therefore, the multi-word option would not be the best
approach here. This is in contrast to previous TAG analyses for English LVCs where both nominal and
verb are anchored in the same elementary tree (XTAG-Group, 2001).
Figure 3 shows the derivation trees (cf. Figure 2) for the three different analysis options as described
above for the sentence Ram ne gehene chorii kiye ?Ram stole the jewels?. The LVC in question is chorii
kar ?theft do?. The dashed line indicates adjunction into the elementary tree, whereas the solid line
indicates substitution. In the noun-centric analysis, the light verb adjoins into the nominal?s elementary
tree and contributes no arguments of its own. For the verb-centric analysis, the light verb contributes the
argument Ram, whereas the nominal contributes jewels. Finally, for the multi-word expression tree, theft
and do are both anchors of the elementary tree.
Noun-centric analysis ?
chorii
Ram-ne
gehene
kiye
Verb-centric analysis ? kiye
Ram-ne chorii
gehene
Multi-word analysis ? chorii-kiye
Ram-ne
gehene
Figure 3: Derivation graphs showing three options for the analysis of Ram ne gehene chorii kiye ?Ram
stole the jewels?. The LVC is chorii kiye.
In this paper we explore a noun-centric analysis of Hindi LVCs.
3
In the analysis that follows, we will
describe two elementary trees for a noun like chorii i.e., when it combines with either ho ?be? or kar ?do?.
Making the elementary structures richer and more complex increases ambiguity locally and we then have
more descriptions for the same lexical item. But these structures also capture local dependencies i.e.,
the fact that the lexical item can appear in varying linguistic environments. Second, this is in keeping
with the TAG notion of using complex elementary structures to capture linguistic properties and having
very general operations (substitution and adjunction) to combine these structures. This has been used
effectively in computational applications and is characterised by the slogan complicate locally, simplify
globally (Bangalore and Joshi, 2010).
4 Analysis
In a noun-centric analysis, the light verb does not have arguments of its own. The full array of arguments
for the light verb construction is instead represented in the nominal?s tree. The light verb can only choose
3
Based on the comments of the reviewers we are now considering a revision of the noun-centric analysis in this paper. It
may seem that a verb-centric analysis may be more appropriate for Hindi LVCs. However, due to lack of space, we do not
explore the second option fully in this paper and leave it to future work.
131
the semantic property of the nominal it may combine with (e.g., the light verb ho may combine only with
nominals that have no agentive arguments). Other analyses e.g Ahmed et al. (2012) represent the light
verb kar ?do? with arguments of its own. We discuss this in Section 5.
Our work follows Han and Rambow (2000)?s representation of Sino-Korean LVCs. This work has
also proposed separate trees for the nominal and light verb. The elementary tree of the nominal is an an
initial tree, and as it is considered the true predicate, it also chooses a syntactic structure that will realize
all its arguments. The light verb on the other hand is represented as an auxiliary tree, therefore it is an
adjunct to the nominal?s basic structure. However, as it is a predicate, it is also a special type of auxiliary
tree viz., a predicative auxiliary tree (Abeill?e and Rambow, 2000).
The second feature of this analysis, also based on Han and Rambow (2000)?s work is the idea of the
nominal as an underspecified base form. The nominal?s elementary tree is not specified with respect to
its category, rather, we use the label X, which projects to an XP. We also assume, following Han and
Rambow that each node is specified with the feature CAT which has values like V or N, but the [CAT=N]
feature on the noun is not realized unless the light verb composes with the elementary tree of the nominal.
In addition, although the nominal is not a verb, it has the feature TENSE=- i.e., it is not tensed.
4.1 The light verb
In order to model the light verb kar ?do? in Example 2, we will construct an auxiliary tree with feature
structures, anchored at kar ?do?. Figure 4 shows such an elementary tree. Note that this is a very different
tree from ?full? kar ?do?, which will have all its arguments. The light verb kar is inflected for person,
number, and gender as well as tense and aspect. In this particular example, it is tensed, masculine, plural
and has perfective aspect; therefore it appears as kiye. We assume that morphological analysis has already
taken place in a separate module, such that the correct morphological surface form has been derived for
?do, masculine plural perfective?. In Figure 4, the XP
r
(root) node and its right-branching daughters
are [CAT=V] with linguistic information about gender, number, tense and aspect. The feature AGT=+ at
the top node implies that this auxiliary tree needs to unify with an initial tree that is also [AGT=+]. In
contrast with kar ?do?, the auxiliary tree of the light verb ho ?be? will have [AGT=-].
XP
r
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
VP
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
V
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
kiye
XP
f
[
cat=n tense=- case=nom nagr=-
]
[]
Figure 4: Elementary tree for light verb kar ?do? inflected as kiye?do.masc.pl.perf?
The XP
f
(foot) node has [TENSE=-] and [CAT=N], which will enable it to adjoin into the elementary
tree of a nominal. The CASE value is specified as NOM (nominative) as the light verb will assign nomi-
native case to the noun. The NAGR feature is required when the light verb agrees in number and gender
with the predicative nominal itself (Mohanan, 1997). As this will not occur in the examples we are
working with, the value for NAGR is negative. For other ?standard? cases of agreement, the feature AGR
is used (It is also useful to note that the verbal agreement rule in Hindi differs from English as the verb
agrees with the highest nominative marked argument- and not necessarily the subject (Mohanan, 1995)).
132
S[
cat=v tense=+ perf=+[1] agt=+[2]
]
[
cat=v tense=+ perf=[3] agt=[4]
]
VP
[
cat=v tense=+ perf=[3] agt=[4] agr=[11]
]
[
cat=v tense=+ perf=[5] agt=[6]agr=[10]
]
XP
2
[
cat=v tense=+ perf=[5] agt=[6] agr=[10]
]
[
cat=[19] tense=- nagr=- case=[14]
]
X
[
cat=[19] tense=- nagr=- case=[14]
]
[
cat=[20] tense=- nagr=- case=[15]
]
chorii
NP
2
?
?
?
?
case=nom
cat=n
agr=[11]
?
?
?
NP
1
?
?
?
?
case=erg cat=n
perf=+[1] agt=+[2]
agr=[13]
?
?
?
Figure 5: Tree for nominal chorii ?theft? -agentive, as seen in Ram ne gehene chorii kiye ?Ram stole the
jewels?. The feature clash at XP
2
is marked with a box.
4.2 The nominal
In contrast to the impoverished argument structure of the light verb, the nominal in Figure 5 has the full
array of arguments for chorii ?theft?. The tree is anchored by the lexical item chorii and the non terminals
at NP
1
and NP
2
are marked with a ? for substitution with the actual lexical items.
The position of the arguments roughly follows the configuration described in Bhatt et al. (2013, p. 59)
, where the first position is the ergative-marked argument and is found in a transitive sentence (but only
if the property [PERF=+] is also present.)
The ?second? position is one where the object of the transitive verb is found. In Figure 5, this is
represented as NP
2
and is the nominative marked argument. The elementary tree for the nominal is not
complete, because of the feature clash at XP
2
between [TENSE=+] vs. [TENSE=-]. The feature clash
represents an obligatory adjunction constraint which will require the light verb to adjoin at this node.
The first position in Figure 5 has the features for [PERF=+] and [AGT=+] as a consequence of having
[CASE=ERG]. The agentive argument shares the values for PERF and AGT with the S node. This ensures
that the light verb that adjoins into this tree will match the PERF and AGT values in NP
1
. The argument
in second position NP
2
will share its values for AGR with XP
2
. At XP
2
, the values for PERF, AGT and
AGR should match with the root node of the light verb. Otherwise, adjunction will fail.
The light verb?s tree as shown in Figure 4 will adjoin into the tree of the nominal. Post adjunction and
substitution, we find a composed structure as seen in Figure 7.
The same noun chorii ?theft? may combine with the light verb ho. In that case, non-agentive chorii will
choose an elementary tree such as Figure 6. This elementary tree appears without an agentive argument.
Its single nominative Theme argument has moved to the first position at NP
1
, leaving behind a co-indexed
trace. Figure 6 shows that the site of adjunction into chorii ?theft? (non-agentive) is at XP
1
. Adjunction
cannot take place at XP
2
as the feature clash is higher up at XP
1
. The single nominative argument of
chorii (non-agentive) will move up to NP
1
in order to receive nominative case from the node CAT=V
(Note that the node immediately above NP
2
has an underspecified CAT feature and this requires the
argument to move to a higher position). The tree for non-agentive chorii will always combine with a
light verb that is AGT=-. Its Theme argument will take nominative case irrespective of the tense-aspect
value of the verb.
133
S[
cat=v tense=+ perf=[1] agt=[2] agr=[12]
]
[
cat=v tense=+ perf=[3] agt=[4] agr=[11]
]
XP
1
[
cat=v tense=+ perf=[3] agt=[4] agr=[11]
]
[
cat=[18] tense=- nagr=- case=[14]
]
XP
2
[
cat=[18] tense=- nagr=- case=[14]
]
[
cat=[19] tense=- nagr=- case=[15]
]
X
[
cat=[19] tense=- nagr=- case=[15]
]
[
cat=[20] tense=- nagr=- case=[16]
]
chorii
NP
2
[
cat=n
]
t
i
NP
1
i
?
[
case=nom perf=[1] agt=?[2] agr=[12]
]
Figure 6: Tree for nominal chorii - non agentive as seen in gehene chorii hue ?The jewels were stolen?.
The feature clash this time is higher in the tree at XP
1
and is marked with a box.
S
[
cat=v tense=+ perf=+ agt=+
]
[
cat=v tense=+ perf=+ agt=+
]
VP
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
XP
2
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
VP
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
V
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
kiye
XP
f
[
cat=n tense=-
case=nom nagr=-
]
[
cat=n tense=-
case=nom nagr=-
]
X
[
cat=n tense=-
case=nom nagr=-
]
[
cat=n tense=-
case=nom nagr=-
]
chorii
NP
[
case=nom cat=n
agr=mpl
]
[
case=nom cat=n
agr=mpl
]
gehene
NP
[
case=erg cat=n
perf=+ agt=+ agr=[13]
]
[
case=erg cat=n
perf=+ agt=+ agr=msg
]
Ram ne
Figure 7: Post adjunction of the light verb?s auxiliary tree into the initial tree chorii ?theft? at XP
2
, we
get the complete argument structure. Substitution at the nodes NP
1
and NP
2
gives us Ram ne gehene
chorii kiye ?Ram stole the jewels?
134
5 Discussion
The elementary trees for chorii ?theft??both agentive and non-agentive are able to capture its alternations
with kar ?do? and ho ?be?. This is in contrast to Ahmed et al. (2012)?s approach in an important way.
They do not consider the nominal?s alternation with the light verb ho ?be? as a light verb construction.
Instead, they maintain that it has a resultative reading and provide a different analysis within the Lexical
Functional Grammar (LFG) framework. In fact, the alternation with ho ?be? provides a useful lexical
alternative to an alternative syntactic structure (such as a passive). The alternation of the light verb ho
?be? and kar ?do? is moreover a characteristic of a certain group of nominals only (not all can show this
alternation e.g., intizar ?waiting? cf. Ahmed and Butt (2011)). Therefore, we maintain that chorii ho
?theft happen? is indeed a light verb construction.
Ahmed and Butt (2011)?s analysis looks at the noun and light verb as co-predicators i.e., it is a verb
centric analysis. While this is different from the proposed analysis here, it is not impossible to construct
elementary trees where the light verb?s elementary tree consists of one argument i.e., the subject and the
nominal (with its own argument) adjoins into it. The pros and cons of these two approaches need to be
explored more thoroughly within the TAG framework and we leave this to future work.
While this work has examined one class of nominals that occur as part of light verb constructions,
it does not complete the analysis of light verb constructions in Hindi. The behaviour of other nominal
classes remains to be explored. There are also nominals that occur with light verbs other than kar ?do?
and ho ?be?. Finally, while the work presented here is mainly theoretical, it is in keeping with recent
proposals for extracting a Hindi TAG grammar from a phrase structure treebank (Bhatt et al., 2012;
Mannem et al., 2009). The algorithm in Bhatt et al. (2012) relies on the annotated Hindi Dependency
Treebank and proposes a rule extraction system for elementary trees. Therefore, the description of Hindi
LVCs in TAG would be a useful addition to the implementation of a grammar extraction task.
Acknowledgements
The first author was supported by DAAD (Deutscher Akademischer Austausch Dienst) for a research
stay at the University of Konstanz in 2013-14s. We are also thankful to Prof Miriam Butt for hosting the
first author at the University of Konstanz. We would like to thank Bhuvana Narasimhan, Miriam Butt
and the anonymous reviewers for their useful remarks on this paper. Any errors that remain are our own.
References
Anne Abeill?e and Marie-H?el`ene Candito. 2000. FTAG: A Lexicalized Tree-Adjoining Grammar for French. In
Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing. CSLI Publications.
Anne Abeill?e and Owen Rambow. 2000. Tree Adjoining Grammar: An Overview. In Anne Abeill?e and Owen
Rambow, editors, Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing. CSLI Publica-
tions.
Anne Abeill?e. 1988. Light verb constructions and Extraction out of NP in TAG. In Lynn MacLeod, Gary Larson,
and Diane Brentari, editors, Proceedings of the 24th Annual Meeting of the Chicago Linguistics Society.
Tafseer Ahmed and Miriam Butt. 2011. Discovering Semantic Classes for Urdu N-V Complex Predicates. In
Proceedings of the International Conference on Computational Semantics (IWCS 2011), Oxford.
Tafseer Ahmed, Miriam Butt, Annette Hautli, and Sebastian Sulger. 2012. A reference dependency bank for
analyzing complex predicates. In Proceedings of the Eight International Conference on Language Resources
and Evaluation (LREC?12).
Srinivas Bangalore and Aravind Joshi. 2010. Introduction. In Srinivas Bangalore and Aravind Joshi, editors,
Supertagging: Using Complex Lexical Descriptions in Natural Language Processing, pages 1?31. MIT Press,
Cambridge.
Rajesh Bhatt, Owen Rambow, and Fei Xia. 2012. Creating a Tree Adjoining Grammar from a Multilayer Tree-
bank. In Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms
(TAG+11), pages 162?170.
135
Rajesh Bhatt, Annahita Farudi, and Owen Rambow. 2013. Hindi-Urdu Phrase Structure Annotation Guidelines.
http://verbs.colorado.edu/hindiurdu/guidelines docs/PhraseStructureguidelines.pdf, November.
Miriam Butt. 1995. The Structure of Complex Predicates in Urdu. CSLI Publications, Stanford.
Claudia Claridge. 2000. Multi-word Verbs in Early Modern English: A Corpus-based Study. Editions Rodopi B.
V., Amsterdam-Atlanta edition.
Alice Davison. 2005. Phrasal predicates: How N combines with V in Hindi/Urdu. In Tanmoy Bhattacharya,
editor, Yearbook of South Asian Languages and Linguistics, pages 83?116. Mouton de Gruyter.
Robert Frank. 2002. Phrase Structure Composition and Syntactic Dependencies. MIT Press, Cambridge.
Jane Grimshaw and Armin Mester. 1988. Light verbs and theta-marking. Linguistic Inquiry, 9(2):205?232.
Chung-hye Han and Owen Rambow. 2000. The Sino-Korean light verb construction and lexical argument struc-
ture. In Proceedings of the Fifth International Workshop on Tree-Adjoining Grammars and Related Formalisms,
TAG+5.
Chung-hye Han, Juntae Yoon, Nari Kim, and Martha Palmer. 2000. A Feature based Lexicalized Tree Adjoining
Grammar for Korean. Technical report, Institute for Research in Cognitive Science, University of Pennsylvania,
http://www.cis.upenn.edu/?xtag/koreantag.
Aravind Joshi and Y. Schabes. 1997. Tree-adjoining grammars. In G. Rozenburg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69?124. Springer.
Laura Kallmeyer and Rainer Osswald. 2013. Syntax-driven semantic frame composition in Lexicalized Tree
Adjoining Grammars. Journal of Language Modelling, 1(2):267?330.
Kate Kearns. 1988. Light verbs in English. Manuscript, MIT (revised 2002).
Prashanth Mannem, Aswarth Abhilash, and Akshar Bharati. 2009. LTAG-spinal Treebank and Parser for Hindi.
In Proceedings of ICON-2009: 7th International Conference on Natural Language Processing.
Tara Mohanan. 1995. Wordhood and Lexicality- Noun Incorporation in Hindi. Natural Language and Linguistic
Theory, 13:75?134.
Tara Mohanan. 1997. Multidimensionality of representation- NV complex predicates in Hindi. In Alex Alsina,
Joan Bresnan, and Peter Sells, editors, Complex Predicates. CSLI Publications, Stanford.
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, Owen Rambow, Dipti Misra Sharma, and Fei Xia. 2009.
Hindi Syntax: Annotating Dependency, Lexical Predicate-Argument Structure, and Phrase Structure. In Pro-
ceedings of ICON-2009: 7th International Conference on Natural Language Processing, Hyderabad.
K. Vijay-Shanker and Aravind Joshi. 1988. Feature structure based Tree Adjoining Grammars. In Proceedings of
COLING 1988.
The XTAG-Group. 2001. A Lexicalized Tree Adjoining Grammar for English. Technical report, IRCS, University
of Pennsylvania.
136
