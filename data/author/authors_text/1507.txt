Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177?184,
Prague, June 2007. c?2007 Association for Computational Linguistics
Biology Based Alignments of Paraphrases for Sentence Compression
Joa?o Cordeiro
CLT and Bioinformatics
University of Beira Interior
Covilha?, Portugal
jpaulo@di.ubi.pt
Ga?el Dias
CLT and Bioinformatics
University of Beira Interior
Covilha?, Portugal
ddg@di.ubi.pt
Guillaume Cleuziou
LIFO - University of Orle?ans
Orle?ans, France
guillaume.cleuziou@
univ-orleans.fr
Abstract
1 In this paper, we present a study for ex-
tracting and aligning paraphrases in the con-
text of Sentence Compression. First, we jus-
tify the application of a new measure for the
automatic extraction of paraphrase corpora.
Second, we discuss the work done by (Barzi-
lay & Lee, 2003) who use clustering of para-
phrases to induce rewriting rules. We will
see, through classical visualization method-
ologies (Kruskal & Wish, 1977) and exhaus-
tive experiments, that clustering may not be
the best approach for automatic pattern iden-
tification. Finally, we will provide some re-
sults of different biology based methodolo-
gies for pairwise paraphrase alignment.
1 Introduction
Sentence Compression can be seen as the removal
of redundant words or phrases from an input sen-
tence by creating a new sentence in which the gist
of the original meaning of the sentence remains un-
changed. Sentence Compression takes an impor-
tant place for Natural Language Processing (NLP)
tasks where specific constraints must be satisfied,
such as length in summarization (Barzilay & Lee,
2002; Knight & Marcu, 2002; Shinyama et al, 2002;
Barzilay & Lee, 2003; Le Nguyen & Ho, 2004;
Unno et al, 2006), style in text simplification (Marsi
& Krahmer, 2005) or sentence simplification for
subtitling (Daelemans et al, 2004).
1Project partially funded by Portuguese FCT (Reference:
POSC/PLP/57438/2004)
Generally, Sentence Compression involves per-
forming the following three steps: (1) Extraction
of paraphrases from comparable corpora, (2) Align-
ment of paraphrases and (3) Induction of rewriting
rules. Obviously, each of these steps can be per-
formed in many different ways going from totally
unsupervised to totally supervised.
In this paper, we will focus on the first two steps.
In particular, we will first justify the application of
a new measure for the automatic extraction of para-
phrase corpora. Second, we will discuss the work
done by (Barzilay & Lee, 2003) who use cluster-
ing of paraphrases to induce rewriting rules. We
will see, through classical visualization methodolo-
gies (Kruskal & Wish, 1977) and exhaustive ex-
periments, that clustering may not be the best ap-
proach for automatic pattern identification. Finally,
we will provide some results of different biology
based methodologies for pairwise paraphrase align-
ment.
2 Related Work
Two different approaches have been proposed for
Sentence Compression: purely statistical method-
ologies (Barzilay & Lee, 2003; Le Nguyen & Ho,
2004) and hybrid linguistic/statistic methodologies
(Knight & Marcu, 2002; Shinyama et al, 2002;
Daelemans et al, 2004; Marsi & Krahmer, 2005;
Unno et al, 2006).
As our work is based on the first paradigm, we
will focus on the works proposed by (Barzilay &
Lee, 2003) and (Le Nguyen & Ho, 2004).
(Barzilay & Lee, 2003) present a knowledge-lean
algorithm that uses multiple-sequence alignment to
177
learn generate sentence-level paraphrases essentially
from unannotated corpus data alone. In contrast to
(Barzilay & Lee, 2002), they need neither paral-
lel data nor explicit information about sentence se-
mantics. Rather, they use two comparable corpora.
Their approach has three main steps. First, work-
ing on each of the comparable corpora separately,
they compute lattices compact graph-based repre-
sentations to find commonalities within groups of
structurally similar sentences. Next, they identify
pairs of lattices from the two different corpora that
are paraphrases of each other. Finally, given an input
sentence to be paraphrased, they match it to a lattice
and use a paraphrase from the matched lattices mate
to generate an output sentence.
(Le Nguyen & Ho, 2004) propose a new sentence-
reduction algorithm that do not use syntactic pars-
ing for the input sentence. The algorithm is an ex-
tension of the template-translation algorithm (one of
example-based machine-translation methods) via in-
novative employment of the Hidden Markov model,
which uses the set of template rules learned from ex-
amples.
In particular, (Le Nguyen & Ho, 2004) do not
propose any methodology to automatically extract
paraphrases. Instead, they collect a corpus by per-
forming the decomposition program using news and
their summaries. After correcting them manually,
they obtain more than 1,500 pairs of long and re-
duced sentences. Comparatively, (Barzilay & Lee,
2003) propose to use the N-gram Overlap metric
to capture similarities between sentences and auto-
matically create paraphrase corpora. However, this
choice is arbitrary and mainly leads to the extraction
of quasi-exact or exact matching pairs. For that pur-
pose, we introduce a new metric, the Sumo-Metric.
Unlike (Le Nguyen & Ho, 2004), one interesting
idea proposed by (Barzilay & Lee, 2003) is to clus-
ter similar pairs of paraphrases to apply multiple-
sequence alignment. However, once again, this
choice is not justified and we will see by classi-
cal visualization methodologies (Kruskal & Wish,
1977) and exhaustive experiments by applying dif-
ferent clustering algorithms, that clustering may not
be the best approach for automatic pattern identifi-
cation. As a consequence, we will study global and
local biology based sequence alignments compared
to multi-sequence alignment that may lead to better
results for the induction of rewriting rules.
3 Paraphrase Corpus Construction
Paraphrase corpora are golden resources for learning
monolingual text-to-text rewritten patterns. How-
ever, such corpora are expensive to construct manu-
ally and will always be an imperfect and biased rep-
resentation of the language paraphrase phenomena.
Therefore, reliable automatic methodologies able to
extract paraphrases from text and subsequently cor-
pus construction are crucial, enabling better pattern
identification. In fact, text-to-text generation is a
particularly promising research direction given that
there are naturally occurring examples of compara-
ble texts that convey the same information but are
written in different styles. Web news stories are an
obvious example. Thus, presented with such texts,
one can pair sentences that convey the same infor-
mation, thereby building a training set of rewriting
examples i.e. a paraphrase corpus.
3.1 Paraphrase Identification
A few unsupervised metrics have been applied to
automatic paraphrase identification and extraction
(Barzilay & Lee, 2003; Dolan & Brockett, 2004).
However, these unsupervised methodologies show a
major drawback by extracting quasi-exact2 or even
exact match pairs of sentences as they rely on clas-
sical string similarity measures such as the Edit Dis-
tance in the case of (Dolan & Brockett, 2004) and
word N-gram overlap for (Barzilay & Lee, 2003).
Such pairs are clearly useless.
More recently, (Anonymous, 2007) proposed a
new metric, the Sumo-Metric specially designed
for asymmetrical entailed pairs identification, and
proved better performance over previous established
metrics, even in the specific case when tested with
the Microsoft Paraphrase Research Corpus (Dolan
& Brockett, 2004). For a given sentence pair, hav-
ing each sentence x and y words, and with ? exclu-
sive links between the sentences, the Sumo-Metric is
defined in Equation 1 and 2.
2Almost equal strings, for example: Bush said America is
addicted to oil. and Mr. Bush said America is addicted to oil.
178
S(Sa, Sb) =
8
>><
>>:
S(x, y, ?) if S(x, y, ?) < 1.0
0 if ? = 0
e?k?S(x,y,?) otherwise
(1)
where
S(x, y, ?) = ? log2(
x
? ) + ? log2(
y
? ) (2)
with ?, ? ? [0, 1] and ?+ ? = 1.
(Anonymous, 2007) show that the Sumo-Metric
outperforms all state-of-the-art metrics over all
tested corpora. In particular, it shows systematically
better F-Measure and Accuracy measures over all
other metrics showing an improvement of (1) at least
2.86% in terms of F-Measure and 3.96% in terms
of Accuracy and (2) at most 6.61% in terms of F-
Measure and 6.74% in terms of Accuracy compared
to the second best metric which is also systemati-
cally the word N-gram overlap similarity measure
used by (Barzilay & Lee, 2003).
3.2 Clustering
Literature shows that there are two main reasons to
apply clustering for paraphrase extraction. On one
hand, as (Barzilay & Lee, 2003) evidence, clusters
of paraphrases can lead to better learning of text-to-
text rewriting rules compared to just pairs of para-
phrases. On the other hand, clustering algorithms
may lead to better performance than stand-alone
similarity measures as they may take advantage of
the different structures of sentences in the cluster to
detect a new similar sentence.
However, as (Barzilay & Lee, 2003) do not pro-
pose any evaluation of which clustering algorithm
should be used, we experiment a set of clustering al-
gorithms and present the comparative results. Con-
trarily to what expected, we will see that clustering
is not a worthy effort.
Instead of extracting only sentence pairs from cor-
pora3, one may consider the extraction of paraphrase
sentence clusters. There are many well-known clus-
tering algorithms, which may be applied to a cor-
pus sentence set S = {s1, ..., sn}. Clustering im-
plies the definition of a similarity or (distance) ma-
trix An?n, where each each element aij is the simi-
larity (distance) between sentences si and sj .
3A pair may be seen as a cluster with only two elements.
3.2.1 Experimental Results
We experimented four clustering algorithms on a
corpus of web news stories and then three human
judges manually cross-classified a random sample
of the generated clusters. They were asked to clas-
sify a cluster as a ?wrong cluster? if it contained at
least two sentences without any entailment relation
between them. Results are shown in the next table 1.
Table 1: Precision of clustering algorithms
BASE S-HAC C-HAC QT EM
0.618 0.577 0.569 0.640 0.489
The ?BASE? column is the baseline, where the
Sumo-Metric was applied rather than clustering.
Columns ?S-HAC? and ?C-HAC? express the re-
sults for Single-link and Complete-link Hierarchi-
cal Agglomerative Clustering (Jain et al, 1999).
The ?QT? column shows the Quality Threshold al-
gorithm (Heyer et al, 1999) and the last column
?EM? is the Expectation Maximization clustering al-
gorithm (Hogg et al, 2005).
One main conclusion, from table 1 is that cluster-
ing tends to achieve worst results than simple para-
phrase pair extraction. Only the QT achieves better
results, but if we take the average of the four cluster-
ing algorithms it is equal to 0.568, smaller than the
0.618 baseline. Moreover, these results with the QT
algorithm were applied with a very restrictive value
for cluster attribution as it is shown in table 2 with
an average of almost two sentences per cluster.
Table 2: Figures about clustering algorithms
Algorithm # Sentences/# Clusters
S-HAC 6,23
C-HAC 2,17
QT 2,32
EM 4,16
In fact, table 2 shows that most of the clusters
have less than 6 sentences which leads to question
the results presented by (Barzilay & Lee, 2003) who
only keep the clusters that contain more than 10 sen-
tences. In fact, the first conclusion is that the num-
ber of experimented clusters is very low, and more
important, all clusters with more than 10 sentences
showed to be of very bad quality.
The next subsection will reinforce the sight that
179
clustering is a worthless effort for automatic para-
phrase corpora construction.
3.2.2 Visualization
In this subsection, we propose a visual analy-
sis of the different similarity measures tested pre-
viously: the Edit Distance (Levenshtein, 1966), the
BLEU metric (Papineni et al, 2001), the word N-
gram overlap and the Sumo-Metric. The goal of this
study is mainly to give the reader a visual interpre-
tation about the organization each measure induces
on the data.
To perform this study, we use a Multidimensional
Scaling (MDS) process which is a traditional data
analysis technique. MDS (Kruskal & Wish, 1977)
allows to display the structure of distance-like data
into an Euclidean space.
Since the only available information is a similar-
ity in our case, we transform similarity values into
distance values as in Equation 3.
dij = (sii ? 2sij + sjj)1/2 (3)
This transformation enables to obtain a (pseudo)
distance measure satisfying properties like minimal-
ity, identity and symmetry. On a theoretical point
of view, the measure we obtain is a pseudo-distance
only, since triangular inequality is not necessary sat-
isfied. In practice, the projection space we build with
the MDS from such a pseudo-distance is sufficient to
have an idea about whether data are organized into
classes.
We perform the MDS process on 500 sentences4
randomly selected from the Microsoft Research
Paraphrase Corpus. In particular, the projection over
the three first eigenvectors (or proper vectors) pro-
vides the best visualization where data are clearly
organized into several classes (at least two classes).
The obtained visualizations (Figure 1) show dis-
tinctly that no particular data organization can be
drawn from the used similarity measures. Indeed,
we observe only one central class with some ?satel-
lite? data randomly placed around the class.
The last observation allows us to anticipate on the
results we could obtain with a clustering step. First,
clustering seems not to be a natural way to manage
4The limitation to 500 data is due to computation costs since
MDS requires the diagonalization of the square similarity or
distance matrix.
such data. Then, according to the clustering method
used, several types of clusters can be expected: very
small clusters which contain ?satellite? data (pretty
relevant) or large clusters with part of the main cen-
tral class (pretty irrelevant). These results confirm
the observed figures in the previous subsection and
reinforce the sight that clustering is a worthless ef-
fort for automatic paraphrase corpora construction,
contrarily to what (Barzilay & Lee, 2003) suggest.
4 Biology Based Alignments
Sequence alignments have been extensively ex-
plored in bioinformatics since the beginning of the
Human Genome Project. In general, one wants to
align two sequences of symbols (genes in Biology)
to find structural similarities, differences or transfor-
mations between them.
In NLP, alignment is relevant in sub-domains
like Text Generation (Barzilay & Lee, 2002). In
our work, we employ alignment methods for align-
ing words between two sentences, which are para-
phrases. The words are the base blocks of our se-
quences (sentences).
There are two main classes of pairwise align-
ments: the global and local classes. In the first
one, the algorithms try to fully align both sequences,
admitting gap insertions at a certain cost, while in
the local methods the goal is to find pairwise sub-
alignments. How suitable each algorithm may be
applied to a certain problem is discussed in the next
two subsections.
4.1 Global Alignment
The well established and widely used Needleman-
Wunsch algorithm for pairwise global sequence
alignment, uses dynamic programming to find the
best possible alignment between two sequences. It is
an optimal algorithm. However, it reveals space and
time inefficiency as sequence length increases, since
an m ? n matrix must be maintained and processed
during computations. This is the case with DNA se-
quence alignments, composed by many thousands of
nucleotides. Therefore, a huge optimization effort
were engaged and new algorithms appeared like k-
tuple, not guaranteeing to find optimal alignments
but able to tackle the complexity problem.
In our alignment tasks, we do not have these com-
180
plexity obstacles, because in our corpora the mean
length of a sentence is equal to 20.9 words, which
is considerably smaller than in a DNA sequence.
Therefore an implementation of the Needleman-
Wunsch algorithm has been used to generate optimal
global alignments.
The figure 2 exemplifies a global word alignment
on a paraphrase pair.
4.2 Local Alignment
The Smith-Waterman (SW) algorithm is similar to
the Needleman Wunsch (NW) one, since dynamic
programming is also followed hence denoting the
similar complexity issues, to which our alignment
task is immune. The main difference is that SW
seeks optimal sub-alignments instead of a global
alignment and, as described in the literature, it
is well tailored for pairs with considerable differ-
ences5, in length and type. In table 3 we exemplify
this by showing two character sequences6 where one
may clearly see that SW is preferable:
N Char. Sequences Alignments
1 ABBAXYTRVRVTTRVTR XYTRVFWHWWHGWGFXYTVWGF XYT-V
2 ABCDXYDRQR AB-CDDQZZSTABZCD ABZCD
Table 3: Preferable local alignment cases.
Remark that in the second pair, only the maximal
local sub-alignment is shown. However, there ex-
ists another sub-alignment: (DRQ, D-Q). This means
that local alignment may be tuned to generate not
only the maximum sub-alignment but a set of sub-
alignments that satisfy some criterium, like having
alignment value greater than some minimum thresh-
old. In fact, this is useful in our word alignment
problem and were experimented by adapting the
Smith Waterman algorithm.
4.3 Dynamic Alignment
According to the previous two subsections, where
two alignment strategies were presented, a natu-
ral question rises: which alignment algorithm to
use for our problem of inter-sentence word align-
ment? Initially, we thought to use only the global
5With sufficient similar sequences there is no difference be-
tween NW and SW.
6As in DNA subsequences and is same for word sequences.
alignment Needleman Wunsch algorithm, since a
complete inter-sentence word alignment is obtained.
However, we noticed that this strategy is unappro-
priate for certain pairs, specially when there are syn-
tactical alternations, like in the next example:
During his magnificent speech, :::the ::::::::president
:::::::::remarkably::::::praised::::IBM::::::::research.
:::The::::::::president:::::::praised::::IBM::::::::research, during hisspeech.
If a global alignment is applied for such a pair, then
weird alignments will be generated, like the one that
is shown in the next representation (we use character
sequences for space convenience and try to preserve
the word first letter, from the previous example):
D H M S T P R Q I S _ _ _
_ _ _ _ T P _ Q I S D H S
Here it would be more adequate to apply local align-
ment and extract all relevant sub-alignments. In this
case, two sub-alignments would be generated:
|D H M S| |T P R P I R|
|D H _ S| |T P _ P I R|
Therefore, for inter-paraphrase word alignments,
we propose a dynamic algorithm which chooses the
best alignment to perform: global or local. To com-
pute this pre-scan, we regard the notion of link-
crossing between sequences as illustrated in the fig-
ure 3, where the 4 crossings are signalized with the
small squares.
It is easily verifiable that the maximum number
of crossings, among two sequences with n exclusive
links in between is equal to ? = 12 ? n ? (n ? 1).
We suggest that if a fraction of these crossings holds,
for example 0.4 ? ? or 0.5 ? ?, then a local align-
ment should be used. Remark that the more this frac-
tion tends to 1.0 the more unlikely it is to use global
alignment.
Crossings may be calculated by taking index pairs
?xi, yi? to represent links between sequences, where
xi and yi are respectively the first and second se-
quence indexes, for instance in figure 3 the ?U?
link has pair ?5, 1?. It is easily verifiable that two
links ?xi, yi? and ?xj , yj? have a crossing point if:
(xi ? xj) ? (yi ? yj) < 0.
4.4 Alignment with Similarity Matrix
In bioinformatics, DNA sequence alignment algo-
rithms are usually guided by a scoring function, re-
lated to the field of expertise, that defines what is
181
the mutation probability between nucleotides. These
scoring functions are defined by PAM7 or BLO-
SUM8 matrices and encode evolutionary approx-
imations regarding the rates and probabilities of
amino acid mutations. Different matrices might pro-
duce different alignments.
Subsequently, this motivated the idea of model-
ing word mutation. It seems intuitive to allow such
a word mutation, considering the possible relation-
ships that exit between words: lexical, syntactical
or semantic. For example, it seems evident that be-
tween spirit and spiritual there exists a stronger rela-
tion (higher mutation probability) than between spir-
itual and hamburger.
A natural possibility to choose a word muta-
tion representation function is the Edit-distance
(Levenshtein, 1966) (edist(.,.)) as a negative re-
ward for word alignment. For a given word pair
?wi, wj?, the greater the Edit-distance value, the
more unlikely the word wi will be aligned with
word wj . However, after some early experiments
with this function, it revealed to lead to some prob-
lems by enabling alignments between very differ-
ent words, like ?total, israel?, ?fire,made? or
?troops,members?, despite many good alignments
also achieved. This happens because the Edit-
distance returns relatively small values, unable to
sufficiently penalize different words, like the ones
listed before, to inhibit the alignment. In bioinfor-
matics language, it means that even for such pairs
the mutation probability is still high. Another prob-
lem of the Edit-distance is that it does not distin-
guish between long and small words, for instance
the pairs ?in, by? and ?governor, governed? have
both the Edit-distance equals to 2.
As a consequence, we propose a new func-
tion (Equation 4) for word mutation penaliza-
tion, able to give better answers for the men-
tioned problems. The idea is to divide the Edit-
distance value by the length of the normalized9
maximum common subsequence maxseq(., .) be-
tween both words. For example, the longest
common subsequence for the pair ?w1, w2? =
?reinterpretation, interpreted? is ?interpret?,
7Point Access Mutation.
8Blocks Substitution Matrices.
9The length of the longest common subsequence divided by
the word with maximum length value.
with length equal to 9 and maxseq(w1, w2) =
9
max{16,11} = 0.5625
costAlign(wi, wj) = ? edist(wi, wj)?+maxseq(wi, wj) (4)
where ? is a small value10 that acts like a
?safety hook? against divisions by zero, when
maxseq(wi, wj) = 0.
word 1 word 2 -edist costAlign
rule ruler -1 -1.235
governor governed -2 -2.632
pay paying -3 -5.882
reinterpretation interpreted -7 -12.227
hamburger spiritual -9 -74.312
in by -2 -200.000
Table 4: Word mutation functions comparision.
Remark that with the costAlign(., .) scoring
function the problems with pairs like ?in, by? simply
vanish. The smaller the words, the more constrained
the mutation will be.
5 Experiments and Results
5.1 Corpus of Paraphrases
To test our alignment method, we used two types
of corpora. The first is the ?DUC 2002? corpus
(DUC2002) and the second is automatically ex-
tracted from related web news stories (WNS) auto-
matically extracted. For both original corpora, para-
phrase extraction has been performed by using the
Sumo-Metric and two corpora of paraphrases were
obtained. Afterwards the alignment algorithm was
applied over both corpora.
5.2 Quality of Dynamic Alignment
We tested the proposed alignment methods by giving
a sample of 201 aligned paraphrase sentence pairs
to a human judge and ask to classify each pair as
correct, acorrect11, error 12, and merror13. We also
asked to classify the local alignment choice14 as ad-
equate or inadequate. The results are shown in the
next table:
10We take ? = 0.01.
11Almost correct - minor errors exist
12With some errors.
13With many errors
14Global or local alignment.
182
Global Local
not para correct acorrect error merror adequate
31 108 28 12 8 12/14
15.5% 63.5% 16.5% 7.1% 4.7% 85.7%
Table 5: Precision of alignments.
For global alignments15 we have 11.8% pairs with
relevant errors and 85.7% (12 from 14) of all lo-
cal alignment decisions were classified as adequate.
The not para column shows the number of false
paraphrases identified, revealing a precision value of
84.5% for the Sumo-Metric.
6 Conclusion and Future Work
A set of important steps toward automatic construc-
tion of aligned paraphrase corpora are presented and
inherent relevant issues discussed, like clustering
and alignment. Experiments, by using 4 algorithms
and through visualization techniques, revealed that
clustering is a worthless effort for paraphrase cor-
pora construction, contrary to the literature claims
(Barzilay & Lee, 2003). Therefore simple para-
phrase pair extraction is suggested and by using
a recent and more reliable metric (Sumo-Metric)
(Anonymous, 2007) designed for asymmetrical en-
tailed pairs. We also propose a dynamic choosing of
the alignment algorithm and a word scoring function
for the alignment algorithms.
In the future we intend to clean the automatic
constructed corpus by introducing syntactical con-
straints to filter the wrong alignments. Our next step
will be to employ Machine Learning techniques for
rewriting rule induction, by using this automatically
constructed aligned paraphrase corpus.
References
Barzilay R. and Lee L. 2002. Bootstrapping Lexical
Choice via Multiple-Sequence Alignment. Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, (EMNLP), 164-171.
Barzilay, R., and Lee, L. 2003. Learning to para-
phrase: An unsupervised approach using multiple-
sequence alignment. Proceedings of HLT-NAACL.
15Percentage are calculated by dividing by 170 (201 ? 31)
the number of true paraphrases that exists.
Dolan W.B. and Brockett C. 2004. Unsupervised con-
struction of large paraphrase corpora: Exploiting
massively parallel news sources. Proceedings of 20th
International Conference on Computational Linguis-
tics (COLING 2004).
Anonymous 2007. Learning Paraphrases from WNS
Corpora. Proceedings of 20th International FLAIRS
Conference. AAAI Press. Key West, Florida.
Daelemans W., Hothker A., and Tjong E. 2004. Auto-
matic Sentence Simplification for Subtitling in Dutch
and English. In Proceedings of LREC 2004, Lisbon,
Portugal.
Heyer L.J., Kruglyak S. and Yooseph S. 1999. Exploring
Expression Data: Identification and Analysis of Coex-
pressed Genes. Genome Research, 9:1106-1115.
Hogg R., McKean J., and Craig A. 2005 Introduction
to Mathematical Statistics. Upper Saddle River, NJ:
Pearson Prentice Hall, 359-364.
Jain A., Murty M. and Flynn P. Data clustering: a review.
ACM Computing Surveys, 31:264-323
Knight K. and Marcu D. 2002. Summarization beyond
sentence extraction: A probabilistic approach to sen-
tence compression. Artificial Intelligence, 139(1):91-
107.
Kruskal J. B. and Wish M. 1977. Multidimensional Scal-
ing. Sage Publications. Beverly Hills. CA.
Le Nguyen M., Horiguchi S., A. S., and Ho B. T. 2004.
Example-based sentence reduction using the hidden
markov model. ACM Transactions on Asian Language
Information Processing (TALIP), 3(2):146-158.
Levenshtein V. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions, and Reversals. Soviet
Physice-Doklady, 10:707-710.
Marsi E. and Krahmer E. 2005. Explorations in sentence
fusion. In Proceedings of the 10th European Work-
shop on Natural Language Generation.
Papineni K., Roukos S., Ward T., Zhu W.-J. 2001.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. IBM Research Report RC22176.
Shinyama Y., Sekine S., and Sudo K. 2002. Auto-
matic Paraphrase Acquisition from News Articles. Sao
Diego, USA.
Unno Y., Ninomiya T., Miyao Y. and Tsujii J. 2006.
Trimming CFG Parse Trees for Sentence Compres-
sion Using Machine Learning Approaches. In the Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions.
183
-0.2
-0.15
-0.1
-0.05
 0
 0.05
 0.1
 0.15
 0.2-0.25-0.2-0.15
-0.1-0.05 0
 0.05 0.1 0.15
 0.2-0.3
-0.2-0.1
 0 0.1
 0.2 0.3
Edit Distance
-0.35
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
 0
 0.05
 0.1
 0.15
 0.2-0.2-0.15
-0.1-0.05
 0 0.05
 0.1 0.15
-0.2-0.15
-0.1-0.05
 0 0.05
 0.1 0.15
 0.2
Word N-Gram Family
-0.25
-0.2
-0.15
-0.1
-0.05
 0
 0.05
 0.1
 0.15
 0.2
 0.25-0.2-0.15
-0.1-0.05 0
 0.05 0.1 0.15
 0.2-0.2
-0.15-0.1
-0.05 0
 0.05 0.1
 0.15 0.2
BLEU Metric
-0.04
-0.02
 0
 0.02
 0.04 -0.04
-0.02  0
 0.02 0.04
-0.04-0.02
 0 0.02
 0.04
Sumo-Metric
Figure 1: MDS on 500 sentences with the Edit Distance (top left), the BLEU Metric (top right), the Word
N-Gram Family (bottom left) and the Sumo-Metric (bottom right).
To the horror of their television fans , Miss Ball and Arnaz were divorced in 1960.
__ ___ ______ __ _____ __________ ____ _ ____ Ball and Arnaz ____ divorced in 1960.
Figure 2: Global aligned words in a paraphrase pair.
Figure 3: Crossings between a sequence pair.
184
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 15?22,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
Unsupervised Induction of Sentence Compression Rules
Jo
?
ao Cordeiro
CLT and Bioinformatics
University of Beira Interior
Covilh?a, Portugal
jpaulo@di.ubi.pt
Ga
?
el Dias
CLT and Bioinformatics
University of Beira Interior
Covilh?a, Portugal
ddg@di.ubi.pt
Pavel Brazdil
LIAAD
University of Porto
Porto, Portugal
pbrazdil@liaad.up.pt
Abstract
In this paper, we propose a new unsu-
pervised approach to sentence compres-
sion based on shallow linguistic process-
ing. For that purpose, paraphrase extrac-
tion and alignment is performed over web
news stories extracted automatically from
the web on a daily basis to provide struc-
tured data examples to the learning pro-
cess. Compression rules are then learned
through the application of Inductive Logic
Programming techniques. Qualitative and
quantitative evaluations suggests that this
is a worth following approach, which
might be even improved in the future.
1 Introduction
Sentence compression, simplification or summa-
rization has been an active research subject dur-
ing this decade. A set of approaches involving
machine learning algorithms and statistical mod-
els have been experimented and documented in the
literature and several of these are described next.
1.1 Related Work
In (Knight & Marcu, 2002) two methods were
proposed, one is a probabilistic model - the
noisy channel model - where the probabili-
ties for sentence reduction (P{S
compress
|S)}
1
) are estimated from a training set of 1035
(Sentence, Sentence
compress
) pairs, manually
crafted, while considering lexical and syntacti-
cal features. The other approach learns syntac-
tic tree rewriting rules, defined through four op-
erators: SHIFT, REDUCE DROP and ASSIGN.
Sequences of these operators are learned from the
training set, and each sequence defines a complete
1
In the original paper the P (t|s) notation is used, where t
is the sentence in the target language and s the original sen-
tence in the source language.
transformation from an original sentence to the
compressed version.
In the work of (Le Nguyen & Ho, 2004)
two sentence reduction algorithms were also pro-
posed. The first one is based on template-
translation learning, a method inherited from the
machine translation field, which learns lexical
transformation rules
2
, by observing a set of 1500
(Sentence, Sentence
reduced
) pair, selected from
a news agency and manually tuned to obtain the
training data. Due to complexity difficulties found
for the application of this big lexical ruleset, they
proposed an improvement where a stochastic Hid-
den Markov Model is trained to help in the deci-
sion of which sequence of possible lexical reduc-
tion rules should be applied to a specific case.
An unsupervised approach was included in the
work of (Turner & Charniak, 2005), where train-
ing data are automatically extracted from the Penn
Treebank corpus, to fit a noisy channel model,
similar to the one used by (Knight & Marcu,
2002). Although it seems an interesting approach
to provide new training instances, it still be depen-
dent upon data manually labeled.
More recently, the work of (Clarke & Lapata,
2006) devise a different and quite curious ap-
proach, where the sentence compression task is
defined as an optimization goal, from an Integer
Programming problem. Several constraints are de-
fined, according to language models, linguistic,
and syntactical features. Although this is an unsu-
pervised approach, without using any paralel cor-
pus, it is completely knowledge driven, like a set
of crafted rules and heuristics incorporated into a
system to solve a certain problem.
1.2 Our Proposal
In this paper, we propose a new approach to
this research field, which follows an unsupervised
methodology to learn sentence compression rules
2
Those rules are named there as template-reduction rules.
15
based on shallow linguistic processing. We de-
signed a system composed of four main steps
working in pipeline, where the first three are re-
sponsible for data extraction and preparation and
in the last one the induction process takes place.
The first step gathers web news stories from re-
lated news events collected on a daily basis from
which paraphrases are extracted. In the second
step, word alignment between two sentences of
a paraphrase is processed. In the third step, spe-
cial regions from these aligned paraphrases, called
bubbles, are extracted and conveniently prepro-
cessed to feed the induction process. The whole
sequence is schematized in figure 1.
Figure 1: The Pipeline Architecture.
The induction process generates sentence re-
duction rules which have the following general
structure: L
cond
?X
cond
?R
cond
? suppress(X).
This means that the sentence segment X will
be eliminated if certain conditions hold over left
(L), middle (X) and right (R) segments
3
. In
Figure 2, we present seven different rules which
have been automatically induced from our archi-
tecture. These rules are formed by the conjunc-
tion of several literals, and they define constraints
under which certain sentence subparts may be
deleted, therefore compressing or simplifying the
sentence. The X symbol stands for the segment
3
For the sake of simplicity and compact representation,
we will omit the rule consequent, which is always the same
(?? suppress(X)?), whenever a rule is presented.
Z
(X)
= 1 ? L
c
= NP ?X
1
= JJ ?R
1
= IN (1)
Z
(X)
= 1 ? L
c
= NP ?X
1
= RB ?R
1
= IN (2)
Z
(X)
= 2 ? L
1
= and ?X
1
= the ?R
1
= JJ (3)
Z
(X)
= 2 ? L
1
= the ?X
2
= of ?R
1
= NN (4)
Z
(X)
= 2 ? L
1
= the ?X
c
= NP ?R
1
= NN (5)
Z
(X)
= 3 ? L
c
= PP ?X
1
= the ?R
c
= NP (6)
Z
(X)
= 3 ? L
c
= NP ?X
1
= and ?R
2
= V B (7)
Figure 2: Learned Sentence Compression Rules.
to be dropped, L
(?)
and R
(?)
are conditions over
the left and right contexts respectively. The nu-
meric subscripts indicate the positions
4
where a
segment constraint holds and the c subscript stands
for a syntactic chunk type. The Z
(?)
function com-
putes the length of a given segment, by counting
the number of words it contains. For instance, the
first rule means that a word
5
will be eliminated if
we have a NP (Noun Phrase) chunk in the left
context, and a preposition or subordinating con-
junction, in the right context (R
1
= IN ). The rule
also requires that the elimination word must be an
adjective, as we have X
1
= JJ .
This rule would be applied to the following seg-
ment
6
[NP mutual/jj funds/nns information/nn]
[ADJP available/jj] [PP on/in] [NP
reuters.com/nn]
and would delete the word available giving rise
to the simplified segment:
[NP mutual/jj funds/nns information/nn]
[PP on/in] [NP reuters.com/nn].
Comparatively to all existing works, we propose
in this paper a framework capable to extract com-
pression rules in a real world environment. More-
over, it is fully unsupervised as, at any step of the
process, examples do not need to be labeled.
In the remaining of the paper, we will present
the overall architecture which achieves precision
4
The position starts with 1 and is counted from left to
right, on the word segments, except for the left context, where
it is counted reversely.
5
As we have Z
(X)
= 1, the candidate segment size to
eliminate is equal to one.
6
The segment is marked with part-of-speech tags (POS)
and chunked with a shallow parser. Both transformations
were made with the OpenNLP toolkit.
16
values up to 85.72%, correctness up to 4.03 in 5
and utility up to 85.72%.
2 Data Preparation
Creating relevant training sets, with some thou-
sands examples is a difficult task, as well as is the
migration of such a system to process other lan-
guages. Therefore, we propose an unsupervised
methodology to automatically create a training set
of aligned paraphrases, from electronically avail-
able texts on the web. This step is done through
step one and step two of Figure 1, and the details
are described in the next two subsections.
2.1 Paraphrase Extraction
Our system collects web news stories on a daily
basis, and organized them into clusters, which
are exclusively related to different and unique
events, happening each day: ?a company acqui-
sition?, ?a presidential speech?, ?a bomb attack?,
etc. Usually, such clusters contain near 30 small
or medium news articles, collected from differ-
ent media sources. This environment proves to be
very fruitful for paraphrase extraction, since we
have many sentences conveying similar informa-
tion yet written in a different form.
A few unsupervised metrics have been applied
to automatic paraphrase identification and extrac-
tion (Barzilay & Lee, 2003; Dolan et al, 2004).
However, these unsupervised methodologies show
a major drawback by extracting quasi-exact or
even exact match pairs of sentences as they rely
on classical string similarity measures such as the
Edit Distance in the case of (Dolan et al, 2004)
and Word N-gram Overlap for (Barzilay & Lee,
2003). Such pairs are useless for our purpose,
since we aim to identify asymmetrical paraphrase
pairs to be used for sentence compression rule
induction, as explained in (Cordeiro et al, Oct
2007). There we proposed a new metric, the
Sumo-Metric, specially designed for asymmetrical
entailed pairs identification, and proved better per-
formance over previous established metrics, even
in the specific case when tested with the Microsoft
Paraphrase Research Corpus (Dolan et al, 2004),
which contains mainly symmetrical cases. For a
given sentence pair, having each sentence x and
y words, and with ? exclusive links between the
sentences, the Sumo-Metric is defined in Equation
8 and 9.
S(S
a
, S
b
) =
8
>
>
<
>
>
:
S(x, y, ?) if S(x, y, ?) < 1.0
0 if ? = 0
e
?k?S(x,y,?)
otherwise
(8)
where
S(x, y, ?) = ? log
2
(
x
?
) + ? log
2
(
y
?
) (9)
with ?, ? ? [0, 1] and ?+ ? = 1.
We have shown (Cordeiro et al, Oct 2007) that
Sumo-Metric outperforms all state-of-the-art met-
rics over all tested corpora and allows to identify-
ing similar sentences with high probability to be
paraphrases. In Figure 3, we provide the reader
with an example of an extracted paraphrase.
(1) To the horror of their fans, Miss Ball
and Arnaz were divorced in 1960.
(2) Ball and Arnaz divorced in 1960.
Figure 3: An Assymetrical Paraphrase
2.2 Paraphrase Alignment
From a corpus of asymmetrical paraphrases, we
then use biology-based gene alignment algorithms
to align the words contained in each of the two
sentences within each paraphrase. For that pur-
pose, we implemented two well established algo-
rithms, one identifying local alignments (Smith
& Waterman, 1981) and the other one computing
global alignments (Needleman & Wunsch, 1970).
We also proposed a convenient dynamic strategy
(Cordeiro et al, 2007), which chooses the best
alignment algorithm to be applied to a specific
case at runtime.
The difference between local and global se-
quence alignments is illustrated below, where we
use letters, instead of words, to better fit our paper
space constraints. Suppose that we have the fol-
lowing two sequences: [D,H,M,S,T,P,R,Q,I,S]
and [T,P,Q,I,S,D,H,S] a global alignment
would produce the following pair.
D H M S T P R Q I S _ _ _
_ _ _ _ T P _ Q I S D H S
For the same two sequences, a local alignment
strategy could generate two or more aligned sub-
sequences as follows.
17
|D H M S| |T P R Q I S|
|D H _ S| |T P _ Q I S|
Hence, at this stage of the process, we end with a
corpus of aligned
7
asymmetrical paraphrases. In
Figure 4, we present the alignment of the para-
phrase of Figure 3.
(1) To the horror of their fans ,
(2) __ ___ ______ __ _____ ____ _
(1) Miss Ball and Arnaz were divorced in 1960.
(2) ____ Ball and Arnaz ____ divorced in 1960.
Figure 4: An Aligned Paraphrase
The next section describes how we use this
structured data to extract instances which are go-
ing to feed a learning system.
3 Bubble Extraction
In order to learn rewriting rules, we have focus
our experiences on a special kind of data, se-
lected from the corpus of aligned sentences, and
we named this data as Bubbles
8
. Given two word
aligned sentences, a bubble is a non-empty seg-
ment aligned with an empty segment of the other
sentence of the paraphrase, sharing a ?strong? con-
text. In Figure 5, we show different examples of
bubbles.
the situation here in chicago with the workers
the situation ____ in chicago with the workers
obama talks exclusively with tom brokaw on meet
obama talks ___________ with tom brokaw on meet
Ball and Arnaz were divorced in 1960
Ball and Arnaz ____ divorced in 1960
america is in the exact same seat as sweigert and
america is in ___ _____ same seat as sweigert and
after a while at the regents park gym, the president
after a while at ___ _______ ____ gym, the president
Figure 5: Examples of Bubbles
To extract a bubble, left and right contexts of
equally aligned words must occur, and the proba-
bility of such extraction depends on the contexts
size as well as the size of the region aligned with
the empty space. The main idea is to eliminate
cases where the bubble middle sequence is too
large when compared to the size of left and right
contexts. More precisely, we use the condition in
7
By ?aligned? we mean, from now on, word alignment
between paraphrase sentence pairs.
8
There are other possible regions to explore, but due to
the complexity of this task, we decided to initially work only
with bubbles
Equation 10 to decide whether a bubble should be
extracted or not.
Z
(L)
? Z
(X)
+ Z
(R)
? 0 (10)
whereL andR stand for the left and right contexts,
respectively, and X is the middle region. The Z
(?)
function computes the length of a given segment,
in terms of number of words. For example, in the
first and last examples of Figure 5, we have: 2 ?
1+5 = 6 ? 0 and 4?3+4 = 5 ? 0. In this case,
both bubbles will be extracted. This condition is
defined to prevent from extracting eccentric cases,
as the ones shown in the examples shown in Figure
6, where the conditions respectively fail: 0 ? 8 +
3 = ?5 < 0 and 1? 7 + 2 = ?4 < 0.
To the horror of their fans , Miss Ball and Arnaz
__ ___ ______ __ _____ ____ _ ____ Ball and Arnaz
will vote __ ___ _______ ____ __ _____ __ friday .
____ vote on the amended bill as early as friday .
Figure 6: Examples of Rejected Bubbles
Indeed, we favor examples with high common
contexts and few deleted words to enhance the in-
duction process.
So far, we only consider bubbles where the
middle region is aligned with a void segment
(X
transf
?? ?). However, more general transforma-
tions will be investigated in the future. Indeed, any
transformation X
transf
?? Y , where Y 6= ?, having
Z
(X)
> Z
(Y )
, may be a relevant compression ex-
ample.
Following this methodology, we obtain a huge
set of examples, where relevant sentence transfor-
mations occur. To have an idea about the amount
of data we are working with, from a set of 30 days
web news stories (133.5 MB of raw text), we iden-
tified and extracted 596678 aligned paraphrases,
from which 143761 bubbles were obtained.
In the next section, we show how we explore
Inductive Logic Programming (ILP) techniques to
generalize regularities and find conditions to com-
press sentence segments.
4 The Induction of Compression Rules
Many different algorithms exist to induce knowl-
edge from data. In this paper, we use Inductive
Logic Programming (ILP) (Muggleton, 1991) and
it was a choice based on a set of relevant fea-
tures like: the capacity to generate symbolic and
18
relational knowledge; the possibility to securely
avoid negative instances; the ability to mix differ-
ent types of attribute and to have more control over
the theory search process.
Unlike (Clarke & Lapata, 2006), we aim at
inducing human understandable knowledge, also
known as symbolic knowledge. For that pur-
pose, ILP satisfies perfectly this goal by produc-
ing clauses based on first order logic. Moreover,
most of the learning algorithms require a com-
plete definition and characterization of the feature
set, prior to the learning process, where any at-
tribute must be specified. This is a conceptual bot-
tleneck to many learning problems such as ours,
since we need to combine different types of at-
tributes i.e. lexical, morpho-syntactic and syntac-
tical. With ILP, we only need to define a set of pos-
sible features and the induction process will search
throughout this set.
4.1 The Aleph System
The Aleph system(Srinivasan, 2000) is an empir-
ical ILP system, initially designed to be a pro-
totype for exploring ILP ideas. It has become a
quite mature ILP implementation, used in many
research projects, ranging form Biology to NLP. In
fact, Aleph is the successor of several and ?more
primitive? ILP systems, like: Progol (Muggleton,
1999), FOIL (Quinlan, 1990), and Indlog (Cama-
cho, 1994), among others, and may be appropri-
ately parametrized to emulate any of those older
systems.
One interesting advantage in Aleph is the possi-
bility to learn exclusively from positive instances,
contrarily to what is required by most learning sys-
tems. Moreover, there is theoretical research work
(Muggleton, 1996) demonstrating that the increase
in the learning error tend to be negligible with the
absence of negative examples, as the number of
learning instances increases. This is a relevant
issue, for many learning domains, and specially
ours, where negative examples are not available.
4.2 Learning Instances
In our problem, we define predicates that charac-
terize possible features to be considered during the
induction process. Regarding the structure of our
learning instances (bubbles), we define predicates
which restrict left and right context sequences as
well as the aligned middle sequence. In particu-
lar, we limit the size of our context sequences to
a maximum of three words and, so far, only use
bubbles in which the middle sequence has a max-
imum length of three
9
words. The notion of con-
texts from bubbles is clarified with the next exam-
ple.
L2 L1 X1 X2 X3 R1 R2 R3 R4
L2 L1 __ __ __ R1 R2 R3 R4
For such a case, we consider [L1, L2] as the left
context, [R1, R2, R3] as the right context, and
[X1, X2, X3] as the aligned middle sequence.
Such an example is represented with a Prolog term
with arity 5 (bub/5) in the following manner:
bub(ID, t(3,0), [L1,L2],
[X1,X2,X3]--->[],
[R1,R2,R3]).
The ID is the identifier of the sequence instance,
t/2 defines the ?transformation dimension?, in
this case from 3 words to 0. The third and fifth
arguments are lists with the left and right con-
texts, respectively, and the fourth argument con-
tains the list with the elements deleted from the
middle sequence. It is important to point out that
every L
i
, X
i
andR
i
are structures with 3 elements
such as word/POS/Chunk. For example, the
word president would be represented by the
expanded structure president/nn/np.
4.3 Feature Space
As mentioned previously, with an ILP system, and
in particular with Aleph, the set of attributes is
defined through a set of conditions, expressed in
the form of predicates. These predicates are the
building blocks that will be employed to construct
rules, during the induction process. Hence, our at-
tribute search space is defined using Prolog pred-
icates, which define the complete set of possibil-
ities for rule body construction. In our problem,
we let the induction engine seek generalization
conditions for the bubble main regions (left, mid-
dle, and right). Each condition may be from one
of the four types: dimensional, lexical, POS, and
chunk. Dimensional conditions simply express
the aligned sequence transformation dimensional-
ity. Lexical conditions impose a fixed position to
match a given word. The POS condition is similar
to the lexical one, but more general, as the position
must match a specific part-of-speech tag. Likely,
chunk conditions bind a region to be equal to a
particular chunk type. For example, by looking
9
They represent 83.47% from the total number of ex-
tracted bubbles.
19
at Figure 2, the attentive reader may have noticed
that these three conditions are present in rule 7. In
terms of Aleph declaration mode, these conditions
are defined as follows.
:- modeh(1,rule(+bub)).
:- modeb(1,transfdim(+bub,n(#nat,#nat))).
:- modeb(3,chunk(+bub,#side,#chk)).
:- modeb(
*
,inx(+bub,#side,#k,#tword)).
:- determination(rule/1,transfdim/2).
:- determination(rule/1,chunk/3).
:- determination(rule/1,inx/4).
The inx/4 predicate defines lexical and POS
type conditions, the chunk/3 predicate de-
fines chunking conditions and the transfdim/2
predicate defines the transformation dimension-
ality, which is in the form transfdim(N,0)
with N>0, according to the kind of bubbles we are
working with.
4.4 The Rule Value Function
The Aleph system implements many different
evaluation
10
functions which guide the theory
search process, allowing the basic procedure for
theory construction to be altered. In order to bet-
ter fit to our problem, we define a new evaluation
function calculated as the geometrical mean be-
tween the coverage percentage and the rule size
value, as shown in Equation 11 whereR is the can-
didate rule and Cov(R) is the proportion of posi-
tive instances covered by R and the LV (?) func-
tion defines the rule value in terms of its length,
returning a value in the [0, 1] interval.
V alue(R) =
p
Cov(R)? LV (R) (11)
The V alue(?) function guides the induction
process, by preferring not too general rules having
maximum possible coverage value. As shown in
Figure 7, the V alue(?) function gives preferences
to rules with 3, 4 and 5 literals.
5 Results
The automatic evaluation of a system is always the
best way to do it, due to its objectivity and scal-
ability. However, in many cases it is unfeasible
for several practical reasons, like the unavailability
of data or the difficulty to prepare an appropriate
10
In the Aleph terminology, this function is named as the
?cost? function, despite the fact that it really computes the
value in the sense that the grater the value, the more likely it
is to be chosen.
0
10
20
30
40
50
60
70
80
90
100
10
25
50
90
60
40
20
1 2 3 4 5 6 7
n
o
clauses
value
Figure 7: Rule length value function
dataset. Some supervised learning approach use
manually labeled test sets to evaluated their sys-
tems. However, these are small test sets, for exam-
ple, (Knight & Marcu, 2002) use a set of 1035 sen-
tences to train the system and only 32 sentences
to test it, which is a quite small test set. As a
consequence, it is also important to propose more
through evaluation. In order to assess as clearly
as possible the performance of our methodology
on large datasets, we propose a set of qualitative
and quantitative evaluations based on three differ-
ent measures: Utility, Ngram simplification and
Correctness.
5.1 Evaluation
A relevant issue, not very commonly discussed, is
the Utility of a learned theory. In real life prob-
lems, people may be more interested in the vol-
ume of data processed than the quality of the re-
sults. Maybe, between a system which is 90%
precise and processes only 10% of data, and a sys-
tem with 70% precision, processing 50% of data,
the user would prefer the last one. The Utility
may be a stronger than the Recall measure, used
for the evaluation of supervised learning systems,
because the later measures how many instances
were well identified or processed from the test set
only, and the former takes into account the whole
universe. For example, in a sentence compres-
sion system, it is important to know how many
sentences would be compressed, from the whole
possible set of sentences encountered in electronic
news papers, or in classical literature books, or
both. This is what we mean here by Utility.
The Ngram-Simplification methodology is an
automatic extrinsic test, performed to perceive
how much a given sentence reduction ruleset
would simplify sentences in terms of syntactical
complexity. The answer is not obvious at first
sight, because even smaller sentences can contain
20
more improbable syntactical subsequences than
their uncompressed versions. To evaluate the syn-
tactical complexity of a sentence, we use a 4 ?
gram model and compute a relative
11
sequence
probability as defined in Equation 12 where
~
W =
[t
1
, t
2
, ..., t
m
] is the sequence of part-of-speech
tags for a given sentence with size m.
P{
~
W} =
?
m?n
Y
k=n
P{t
k
| t
k?1
, ..., t
k?n
}
?
1
m
(12)
The third evaluation is qualitative. We measure
the quality of the learned rules when applied to
sentence reduction. The objective is to assess how
correct is the application of the reduction rules.
This evaluation was made through manual annota-
tion for a statistically representative random sam-
ple of compressed sentences. A human judged
the adequacy and Correctness of each compres-
sion rule to a given sentence segment, in a scale
from 1 to 5, where 1 means that it is absolutely in-
correct and inadequate, and 5 that the compression
rule fits perfectly to the situation (sentence) being
analyzed.
To perform our evaluation, a sample of 300 sen-
tences were randomly extracted, where at least one
compression rule had been applied. This eval-
uation set may be subdivided into three subsets,
where 100 instances came from rules with Z
(X)
=
1 (BD1), 100 from rules with Z
(X)
= 2 (BD2),
and the other 100 from rules with Z
(X)
= 3
(BD3). Another random sample, also with 100
cases has been extracted to evaluate our base-line
(BL) which consists in the direct application of
the bubble set to make compressions. This means
that no learning process is performed. Instead, we
store the complete bubble set as if they were rules
by themselves (in the same manner as (Le Nguyen
& Ho, 2004) do).
Table 1 compiles the comparative results
for Correctness, Precision, Utility and Ngram-
simplification for all datasets. In particular,
Ngram-simplification in percentage is the pro-
portion of test cases where P{reduced(
~
W )} ?
P{
~
W}.
Table 1 provides evidence of the improvement
achieved with the induction rules, in comparison
with the base line, on each test parameter: Cor-
rectness, Utility and Ngram-simplification. Con-
11
Because it is raised to the inverse power of m, which is
the number of words in the sentence.
Parameter BL BD1 BD2 BD3
Correctness: 2.93 3.56 4.03 4.01
Precision: 58.60% 71.20% 80.60% 80.20%
Utility: 8.65% 32.67% 85.72% 26.86%
NG-Simpl: 47.39% 89.33% 90.03% 89.23%
Table 1: Results with Four Evaluation Parameters.
sidering the three experiences, BD1, BD2, and
BD3, as a unique evaluation run, we obtained a
mean Correctness quality of 3.867 (i.e. 77.33%
Precision), a mean Utility of 48.45%, and a mean
Ngram-simplification equal to 89.53%, which are
significantly better than the base line.
Moreover, best results overall are obtained for
BD2 with 80.6% Precision, 85.72% Utility and
90.03% Ngram-simplification which means that
we can expect a reduction of two words with high
quality for a great number of sentences. In partic-
ular, Figure 2 shows examples of learned rules.
5.2 Time Complexity
In the earlier
12
days of ILP, the computation time
spent by their systems was a serious difficult ob-
stacle, disabling its implementation for real life
problems. However, nowadays these time ef-
ficiency issues have been overcome, opening a
wide range of application possibilities, for many
problems, from Biology to Natural Language Pro-
cessing. The graph in figure 8, shows that even
with considerable big datasets, our learning sys-
tem (based on Aleph) evidences acceptable feasi-
ble computation time.
0
20
40
60
80
100
120
140
12
27
0
53
0
120
0
10 20 30 40 50 60
10
3
bubbles
seconds
Figure 8: Time spent during the induction process,
for datasets with size expressed in thousands of
bubbles.
To give an idea about the size of an induced
rule set, and taking as an example the learned rules
12
In the 1990-2000 decade.
21
with Z
(X)
= 2, these were learned from a dataset
containing 37271 t(2, 0) bubbles, and in the final
5806 sentence reduction rules were produced.
6 Conclusion and Future Directions
Sentence Compression is an active research topic,
where several relevant contributions have recently
been proposed. However, we believe that many
milestones still need to be reached. In this pa-
per, we propose a new framework in the form of
a pipeline, which processes huge sets of web news
articles and retrieves compression rules in an un-
supervised way. For that purpose, we extract and
align paraphrases, explore and select specific text
characteristics called bubbles and finally induce a
set of logical rules for sentence reduction in a real-
world environment. Although we have only con-
sidered bubbles having Z
(X)
? 3, a sentence may
have a compression length greater than this value,
since several compression rules may be applied to
a single sentence.
Our results evidence good practical applicabil-
ity, both in terms of Utility, Precision and Ngram-
simplification. In particular, we assess results up
to 80.6% Precision, 85.72% Utility and 90.03%
Ngram-simplification for reduction rules of two
word length. Moreover, results were compared to
a base line set of rules produced without learning
and the difference reaches a maximum improve-
ment using Inductive Logic Programming of 22%.
Acknowledgments
This work was supported by the VIPACCESS
project - Ubiquitous Web Access for Visually Im-
paired People. Funding Agency: Fundac??ao para
a Ci?encia e a Tecnologia (Portugal). Reference:
PTDC/PLP/72142/2006.
References
Barzilay R. and Lee L.. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In HLT-NAACL 2003: Main Proceed-
ings, pages 16?23.
Camacho R. 1994. Learning stage transition rules with
Indlog. Gesellschaft f?ur Mathematik und Datenver-
arbeitung MBH., Volume 237 of GMD- Studien, pp.
273-290.
Clarke J., and Lapata M. 2006. Constraint-based Sen-
tence Compression: An Integer Programming Ap-
proach. 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics.
Cordeiro J. and Dias G. and Cleuziou G. 2007. Bi-
ology Based Alignments of Paraphrases for Sen-
tence Compression. In Proceedings of the Workshop
on Textual Entailment and Paraphrasing (ACL-
PASCAL / ACL2007), Prague, Czech Republic.
Cordeiro J. and Dias G. and Brazdir P. October
2007. New Functions for Unsupervised Asymmet-
rical Paraphrase Detection. In Journal of Software.,
Volume:2, Issue:4, Page(s): 12-23. Academy Pub-
lisher. Finland. ISSN: 1796-217X.
Dolan W.B. and Quirck C. and Brockett C. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of 20th International Conference on Com-
putational Linguistics (COLING 2004).
Knight K. and Marcu D. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91-107.
Muggleton S. 1991. Inductive Logic Programming.
New Generation Computing, 8 (4):295-318.
Muggleton S. 1996. Learning from positive data. Pro-
ceedings of the Sixth International Workshop on In-
ductive Logic Programming (ILP-96), LNAI 1314,
Berlin, 1996. Springer-Verlag.
Muggleton S. 1999. Inductive logic programming: Is-
sues, results and the challenge of learning language
in logic. Artificial Intelligence, 114 (1-2), 283?296.
Le Nguyen M., Horiguchi S., A. S., and Ho B. T. 2004.
Example-based sentence reduction using the hidden
markov model. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 3(2):146-
158.
Needleman SB, Wunsch CD. 1970. A general method
applicable to the search for similarities in the amino
acid sequence of two proteins. Journal of Molecular
Biology, 48 (3): 443?53.
Quinlan J. R. 1990. Learning Logical Deinitions from
Relations. Machine Learning., 5 (3), 239-266. 33,
39, 41.
Smith TF, Waterman MS. 1981. Identification of Com-
mon Molecular Subsequences. Journal of Molecu-
lar Biology, 147: 195?197.
Srinivasan A. 2000. The Aleph Manual, Technical
Report. Computing Laboratory, Oxford University,
UK.
Turner J, Charniak E. 2005. Supervised and Unsuper-
vised Learning for Sentence Compression. Proceed-
ings of the 43rd Annual Meeting of the ACL, pages
290-297.
22
