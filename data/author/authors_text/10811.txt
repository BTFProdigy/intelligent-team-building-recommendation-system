An Effective Hybrid Machine Learning Approach for Coreference 
Resolution 
Feiliang Ren 
Natural Language Processing Lab 
College of Information Science and En-
gineering 
Northeastern University, P.R.China 
renfeiliang@ise.neu.edu.cn 
Jingbo Zhu 
Natural Language Processing Lab 
College of Information Science and En-
gineering 
Northeastern University, P.R.China 
zhujingbo@ise.neu.edu.cn 
 
 
Abstract 
We present a hybrid machine learning ap-
proach for coreference resolution. In our 
method, we use CRFs as basic training 
model, use active learning method to gen-
erate combined features so as to make ex-
isted features used more effectively; at last, 
we proposed a novel clustering algorithm 
which used both the linguistics knowledge 
and the statistical knowledge. We built a 
coreference resolution system based on the 
proposed method and evaluate its perform-
ance from three aspects: the contributions 
of active learning; the effects of different 
clustering algorithms; and the resolution 
performance of different kinds of NPs. Ex-
perimental results show that additional per-
formance gain can be obtained by using ac-
tive learning method; clustering algorithm 
has a great effect on coreference resolu-
tion?s performance and our clustering algo-
rithm is very effective; and the key of 
coreference resolution is to improve the 
performance of the normal noun?s resolu-
tion, especially the pronoun?s resolution. 
1 Introduction 
Coreference resolution is the process of determin-
ing whether two noun phrases (NPs) refer to the 
same entity in a document. It is an important task 
in natural language processing and can be classi-
fied into pronoun phrase (denoted as PRO) resolu-
tion, normal noun phrase (denoted as NOM) reso-
lution, and named noun phrase (denoted as NAM) 
resolution. Machine learning approaches recast this 
problem as a classification task based on con-
straints that are learned from an annotated corpus. 
Then a separate clustering mechanism is used to 
construct a partition on the set of NPs.  
Previous machine learning approaches for 
coreference resolution (Soon et al 2001; Ng et al 
2002; Florian et al 2004, etc) usually selected a 
machine learning approach to train a classification 
model, used as many as possible features for the 
training of this classification model, and finally 
used a clustering algorithm to construct a partition 
on the set of NPs based on the statistical data ob-
tained from trained classification model. Their ex-
perimental results showed that different kinds of 
features had different contributions for system?s 
performance, and usually the more features used, 
the better performance obtained. But they rarely 
focused on how to make existed features used 
more effectively; besides, they proposed their own 
clustering algorithm respectively mainly used the 
statistical data obtained from trained classification 
model, they rarely used the linguistics knowledge 
when clustering different kinds of NPs. Also, there 
were fewer experiments conducted to find out the 
effect of a clustering algorithm on final system?s 
performance.  
In this paper, we propose a new hybrid machine 
learning method for coreference resolution. We use 
NP pairs to create training examples; use CRFs as 
a basic classification model, and use active learn-
ing method to generate some combined features so 
as to make existed features used more effectively; 
at last, cluster NPs into entities by a novel cascade 
clustering algorithm.  
The rest of the paper is organized as follows. 
Section 2 presents our coreference resolution sys-
24
Sixth SIGHAN Workshop on Chinese Language Processing
tem in detail. Section 3 is our experiments and dis-
cussions. And at last, we conclude our work in sec-
tion 4. 
2 Coreference Resolution 
There are three basic components for a coreference 
resolution system that uses machine learning ap-
proach: the training set creation, the feature selec-
tion, and the coreference clustering algorithm. We 
will introduce our methods for these components 
respectively as follows. 
2.1 Training Set Creation 
Previous researchers (Soon et al, 2001, Vincent 
Ng et al, 2002, etc) took different creation strate-
gies for positive examples and negative examples. 
Because there were no experimental results 
showed that these kinds of example creation meth-
ods were helpful for system?s performance, we 
create both positive examples and negative exam-
ples in a unified NP pair wise manner.  
Given an input NP chain of an annotated docu-
ment, select a NP in this NP chain from left to right 
one by one, and take every of its right side?s NP, 
we generate a positive example if they refer to the 
same entity or a negative example if they don?t 
refer to the same entity. For example, there is a NP 
chain n1-n2-n3-n4 found in document, we will 
generate following training examples: (n1-n2, 1?  ), 
(n1-n3,  ), (n1-n4,  ), (n2-n3,1? 1? 1?  ), (n2-
n4,  ), and (n3-n4,  ). Where denotes that 
this is a positive example, and denotes that this 
is a negative example.  
1? 1? 1+
1?
2.2 Feature Sets 
In our system, two kinds of features are used. One 
is atomic feature, the other is combined feature. 
We define the features that have only one genera-
tion condition as atomic features, and define the 
union of some atomic features as combined fea-
tures.  
2.2.1 Atomic Features 
All of the atomic features used in our system are 
listed as follows.  
String Match Feature (denoted as Sm): Its possi-
ble values are exact, left, right, included, part, 
alias, and other. If two NPs are exactly string 
matched, return exact; if one NP is the left sub-
string of the other, return left; if one NP is the right 
substring of the other, return right; if all the char-
acters in one NP are appeared in the other but not 
belong to set {left, right}, return included; if some 
(not all) characters in one NP are appeared in the 
other, return part; if one NP is the alias of the other, 
return alias; if two NPs don?t have any common 
characters, return other.  
Lexical Similarity Features (denoted as Ls): 
compute two NP?s similarity and their head words? 
similarity using following formula 1. 
1 2
1 2
1 2
2 (
( , )
( ) ( )
SameChar n n
Sim n n
Len n Len n
, )?= +   (1) 
Here means the common 
characters? number in  and ;  is the 
total characters? number in . 
( , )1 2SameChar n n
1n 2n ( )Len ni
ni
Edit Distance Features (denoted as Ed): compute 
two NP?s edit distance and their head words? edit 
distance (Wagner and Fischer, 1974), and the pos-
sible values are true and false. If the edit distance 
of two NPs (or the head words of these two NPs) 
are less than or equal to 1, return true, else return 
false. 
Distance Features (denoted as Dis): distance be-
tween two NPs in words, NPs, sentences, para-
graphs, and characters. 
Length Ratio Features (denoted as Lr): the length 
ratio of two NPs, and their head words. Their pos-
sible values belong to the range (0 . ,1]
NP?s Semantic Features (denoted as Sem): the 
POSs of two NPs? head words; the types of the two 
NPs (NAM, NOM or PRO); besides, if one of the 
NP is PRO, the semantic features will also include 
this NP?s gender information and plurality infor-
mation. 
Other Features (denoted as Oth): whether two 
NPs are completely made up of capital English 
characters; whether two NPs are completely made 
up of lowercase English characters; whether two 
NPs are completely made up of digits. 
2.2.2 Combined Features Generated by Ac-
tive Learning 
During the process of model training for corefer-
ence resolution, we found that we had very fewer 
available resources compared with previous re-
searchers. In their works, they usually had some 
extra knowledge-based features such as alias table,  
abbreviation table, wordnet and so on; or they  had 
25
Sixth SIGHAN Workshop on Chinese Language Processing
some extra in-house analysis tools such as proper 
name parser, chunk parser, rule-based shallow 
coreference resolution parser, and so on (Hal 
Daume III, etc, 2005; R.Florian, etc, 2004; Vincent 
Ng, etc, 2002; etc). Although we also collected 
some aliases and abbreviations, the amounts are 
very small compared with previous researchers?. 
We hope we can make up for this by making ex-
isted features used more effectively by active 
learning method.  
Formally, active learning studies the closed-loop 
phenomenon of a learner selecting actions or mak-
ing queries that influence what data are added to its 
training set. When actions or queries are selected 
properly, the data requirements for some problems 
decrease drastically (Angluin, 1988; Baum & Lang, 
1991). In our system, we used a pool-based active 
learning framework that is similar as Manabu Sas-
sano (2002) used, this is shown in figure 1.  
 
Figure 1: Our Active Learning Framework 
In this active learning framework, an initial clas-
sifier is trained by CRFs [1] that uses only atomic 
features, and then two human teachers are asked to 
correct some selected wrong classified examples 
independently. During the process of correction, 
without any other available information, system 
only shows the examples that are made up of fea-
tures to the human teachers; then these two human 
teachers have to use the information of some 
atomic features? combinations to decide whether 
two NPs refer to the same entity. We record all 
these atomic features? combinations that used by 
both of these human teachers, and take them as 
combined features. 
For example, if both of these human teachers 
correct a wrong classified example based on the 
knowledge that ?if two NPs are left substring 
                                                 
1 http://www.chasen.org/~taku/software/CRF++/ 
matched, lexical similarity feature is greater than 
0.5, I think they will refer to the same entity?, the 
corresponding combined feature would be de-
scribed as: ?Sm(NPs)-Ls(NPs)?, which denotes the 
human teachers made their decisions based on the 
combination information of ?String Match Fea-
tures? and ?Lexical Similarity Features?. 
 
Figure 2: Selection Algorithm 
1. Select all the wrong classified examples whose 
CRFs? probability  belongs to range [0.4, 0.6] 
2.  Sort these examples in decreasing order. 
3.  Select the top m examples 
In figure 1, ?information? means the valuable 
data that can improve the system?s performance 
after correcting their classification. The selection 
algorithm for ?informative? is the most important 
component in an active learning framework. We 
designed it from the degree of correcting difficulty. 
We know 0.5 is a critical value for an example?s 
classification. For a wrong classified example, the 
closer its probability value to 0.5, the easier for us 
to correct its classification. Following this, our se-
lection algorithm for ?informative? is designed as 
shown in figure 2. 
1. Build an initial classifier 
2. While teacher can correct examples based on
feature combinations 
a) Apply the current classifier to training ex-
amples 
b) Find m most informative training examples
c) Have two teachers correct these examples
based on feature combinations 
d) Add the feature combinations that are used
by both of these two teachers to feature
sets in CRFs and train a new classifier. 
When add new combined features won?t lead to 
a performance improvement, we end active learn-
ing process. Totally we obtained 21 combined fea-
tures from active learning. Some of them are listed 
in table 1.  
Table 1: Some Combined Features 
Sm(NPs)-Sm(HWs)-Ls(NPs)-Ls(HWs) 
Sm(NPs)-Sm(HWs)-Ls(NPs) 
Sm(NPs)-Sm(HWs)-Ls(HWs) 
Sm(NPs)-Sm(HWs)-Lr(NPs)-Lr(HWs) 
Sm(NPs)-Sm(HWs)-Lr(NPs) 
Sm(NPs)-Sm(HWs)-Sem(HW1)-Sem(HW2) 
Sm(NPs)-Sm(HWs)-Sem(NP1)-Sem(NP2) 
Sm(NPs)-Sm(HWs)-Lr(HWs) 
?? 
Here ?Sm(NPs)? means the string match fea-
ture?s value of two NPs, ?Sm(HWs)? means the 
string match feature?s value of two NPs? head 
words. ?HWs? means the head words of two NPs. 
Combined feature ?Sm(NPs)-Sm(HWs)-Ls(NPs)? 
means when correcting a wrong classified example, 
both these human teachers made their decisions 
based on the combination information of Sm(NPs), 
Sm(HWs), and Ls(NPs) . Other combined features 
have the similar explanation. 
26
Sixth SIGHAN Workshop on Chinese Language Processing
And at last, we take all the atomic features and 
the combined features as final features to train the 
final CRFs classifier.  
2.3 Clustering Algorithm 
Formally, let { : be NPs in a docu-
ment. Let us define the set of 
NPs whose types are all NAMs; define 
the set of NPs whose types are 
all NOMs; define the set of 
NPs whose types are all PROs. Let be 
the map from NP index i to entity index
1 }im i n? ? n
1{ ,..., }a a afS N N=
1{ ,..., }o o oS N N= g
k1{ ,..., }p p pS N N=
:g i ja
j . For a 
NP index , let us define 
the set of indices of the 
partially-established entities before clustering , 
and , the set of the partially-
established entities. Let  be the 
(1 )k k n? ?
?{ (1),..., ( 1)}kJ g g k=
km
{ : }k t kE e t J= ?
ije j th?  NP in 
entity. Let i th? ( , )i jprob m m be the probability 
that  and refer to the same entity, and im jm
( , )i jprob m m can be trained from CRFs. 
Given that has been formed before cluster-
ing , can take two possible actions: 
if , then the active NP is said to link 
with the entity
kE
km km
( ) kg k J? km
( )g ke ; otherwise it starts a new en-
tity ( )g ke .  
In this work, P L is used to 
compute the link probability, where t , is 1 
iff links with ; the random variable A is the 
index of the partial entity to which m is linking.  
( 1| , ,k k )E m A t= =
J? k L
km te
k
Our clustering algorithm is shown in figure 3. 
The basic idea of our clustering algorithm is that 
NAMs, NOMs and PROs have different abilities 
starting an entity. For NAMs, they are inherent 
antecedents in entities, so we start entities based on 
them first.  
For NOMs, they have a higher ability of acting 
as antecedents in entities than PROs, but lower 
than NAMs. We cluster them secondly, and add a 
NOM in an existed entity as long as their link 
probability is higher than a threshold. And during 
the process of the link probabilities computations, 
we select a NP in an existed entity carefully, and 
take these two NPs? link probability as the link 
probability between this NOM and current entity. 
The selection strategy is to try to make these link 
probabilities have the greatest distinction.  
And for PROs, they have the lowest ability of 
acting as antecedents in entities, most of the time, 
they won?t be antecedents in entities; so we cluster 
them into an existed entity as long as there is a 
non-zero link probability. 
3 Experiments and Discussions  
Our experiments are conducted on Chinese EDR 
(Entity Detection and Recognize) &EMD (Entity 
Mention Detection) corpora from LDC. These cor-
pora are the training data for ACE (Automatic 
Content Extraction) evaluation 2004 and ACE 
evaluation 2005. These corpora are annotated and 
can be used to train and test the coreference resolu-
tion task directly. 
 
Figure 3: Our Clustering Algorithm 
Input: M = { :1 }im i n? ?  
Output: a partition E of the set M  
Initialize: 0 { {{ : }}}i i i aH e m m S? = ?  
if x c y dm e m e? ? ? ? , c d? , and xm is alias of 
ym , then  ' \{ } { }d c dH H e e e? ? ?  
foreach k om S? that hasn?t been clustered 
    if 0ke is NAM and d? makes ( , ) 0tde NOM? ?
      P= arg max
te
 
| | min
{ ( , ) ( , )}tdk td
d k
prob m e e NOM?
? =
?
esleif 0ke is NAM and , ( , ) 0tdd e NOM?? ==  
     P= arg max
te
 
| | min
{ ( , ) ( , )}tdk td
d k
prob m e e NAM?
? =
?
esleif 0ke is NOM 
P= arg max
te
 0( , )k tprob m e  
 if P ?? , ' \{ } { { }}t t kH H e e m? ? ?  
 else ' { }kH H m? ?  
foreach k pm S? that hasn?t been clustered 
      P= arg max ( , )
t
k
m e
prob m m
?
  
  if 0P > , ' \{ } { { }}t t kH H e e m? ? ?  
  else ' { }kH H m? ?  
return H
27
Sixth SIGHAN Workshop on Chinese Language Processing
In ACE 2004 corpus, there are two types of 
documents: paper news (denoted as newswire) and 
broadcast news (denoted as broadca); for ACE 
2005 corpus, a new type added: web log docu-
ments (denoted as weblogs). Totally there are 438 
documents in ACE 2004 corpus and 636 docu-
ments in ACE 2005 corpus. We randomly divide 
these two corpora into two parts respectively, 75% 
of them for training CRFs model, and 25% of them 
for test. By this way, we get 354 documents for 
training and 84 documents for test in ACE 2004 
corpus; and 513 documents for training and 123 
documents for test in ACE 2005 corpus.  
Some statistics of ACE2005 corpus and 
ACE2004 corpus are shown in table 2. 
Our experiments were classified into three 
groups. Group 1 (denoted as ExperimentA) is de-
signed to evaluate the contributions of active learn-
ing for the system?s performance. We developed 
two systems for ExperimentA, one is a system that 
used only the atomic features for CRFs training 
and we took it as a baseline system, the other is a 
system that used both the atomic features and the 
combined features for CRFs training and we took it 
as our final system. The experimental results are 
shown in table 3 and table 4 for different corpus 
respectively. Bold font is the results of our final 
system, and normal font is the results of baseline 
system. Here we used the clustering algorithm as 
described in figure 3. 
Group 2 (denoted as ExperimentB) is designed 
to investigate the effects of different clustering al-
gorithm for coreference resolution. We imple-
mented another two clustering algorithms: algo-
rithm1 that is proposed by Ng et al (2002) and 
algorithm2 that is proposed by Florian et al (2004). 
We compared the performance of them with our 
clustering algorithm and experimental results are 
shown in table 5. 
Group 3 (denoted as ExperimentC) is designed 
to evaluate the resolution performances of different 
kinds of NPs. We think this is very helpful for us 
to find out the difficulties and bottlenecks of 
coreference resolution; and also is helpful for our 
future work. Experimental results are shown in 
table 6. 
In ExperimentB and ExperimentC, we used 
both atomic features and combined features for 
CRFs classification model training. And in table5, 
table6 and table7, the data before ?/? are experi-
mental results for ACE2005 corpus and the data 
after ?/? are experimental results for ACE2004 
corpus.  
In all of our experiments, we use recall, preci-
sion, and F-measure as evaluation metrics, and de-
noted as R, P, and F for short respectively.  
Table 2: Statistics of ACE2005/2004 Corpora 
 Training Test 
# of all documents 513/354 123/84 
# of broadca 204/204 52/47 
# of newswire 229/150 54/47 
#of weblogs 80/0 17/0 
# of characters 248972/164443 55263/35255
# of NPs 28173/18995 6257/3966
# of entities 12664/8723 2783/1828
# of neg examples 722919/488762 142949/89894
# of  pos examples 72000/44682 15808/8935
Table3: ExperimentA for ACE2005 Corpora 
 R P F 
broadca 79.0/76.2 75.4/72.9 77.2/74.5
newswire 73.2/72.9 68.7/67.8 70.9/70.3
weblogs 72.3/68.5 65.5/63.3 68.8/65.8
total 75.4/73.7 70.9/69.3 73.1/71.4
Table4: ExperimentA for ACE2004 Corpora 
 R P F 
broadca 74.7/71.0 72.4/68.9 73.5/69.9
newswire 77.7/73.1 73.0/68.6 75.2/70.7
Total 76.2/72.0 72.7/68.7 74.4/70.4
Table5: ExperimentB for ACE2005/2004 Corpora 
 R P F 
algorithm1 61.0/63.5 59.5/62.8 60.2/63.2
algorithm2 61.0/62.4 60.7/62.8 60.9/62.6
Ours 75.4/76.2 70.9/72.7 73.1/74.4
Table6: ExperimentC for ACE2005/2004 Corpora 
 R P F 
NAM 80.5/81.4 77.9/79.2 79.2/80.1
NOM 62.6/62.5 54.4/56.8 58.2/59.5
PRO 28.4/29.8 22.7/24.0 25.2/26.6
From table 3 and table 4 we can see that the fi-
nal system?s performance made a notable im-
provement compared with the baseline system in 
both corpora. We know the only difference of 
these two systems is whether used active learning 
method. This indicates that by using active learn-
ing method, we make the existed features used 
more effectively and obtain additional performance 
gain accordingly. One may say that even without 
active learning method, he still can add some com-
bined features during CRFs model training. But 
this can?t guarantee it would make a performance 
28
Sixth SIGHAN Workshop on Chinese Language Processing
improvement at anytime. Active learning method 
provides us a way that makes this combined fea-
tures? selection process goes in a proper manner. 
Generally, a system can obtain an obvious per-
formance improvement after several active learn-
ing iterations. We still noticed that the contribu-
tions of active learning for different kinds of 
documents are different. In ACE04 corpus, both 
kinds of documents? performance obtained almost 
equal improvements; in ACE05 corpus, there is 
almost no performance improvement for newswire 
documents, but broadcast documents? performance 
and web log documents? performance obtained 
greater improvements. We think this is because for 
different kinds of documents, they have different 
kinds of correcting rules (these rules refer to the 
combination methods of atomic features) for the 
wrong classified examples, some of these rules 
may be consistent, but some of them may be con-
flicting. Active learning mechanism will balance 
these conflicts and select a most appropriate global 
optimization for these rules. This can also explain 
why ACE04 corpus obtains more performance im-
provement than ACE05 corpus, because there are 
more kinds of documents in ACE05 corpus, and 
thus it is more likely to lead to rule conflicts during 
active learning process.  
Experimental results in table 5 show that if other 
experimental conditions are the same, there are 
obvious differences among the performances with 
different clustering algorithms. This surprised us 
very much because both algorithm1 and algo-
rithm2 worked very well in their own learning 
frameworks. We know R.Florian et al (2004) first 
proposed algorithm2 using maximum entropy 
model. Is this the reason for the poor performance 
of algorithm2 and algorithm1? To make sure this, 
we conducted other experiments that changed the 
CRFs model to maximum entropy model [2] with-
out changing any other conditions and the experi-
mental results are shown in table 7.  
The experimental results are the same: our clus-
tering algorithm achieved better performance. We 
think this is mainly because the following reason, 
that in our clustering algorithm, we notice the fact 
that different kinds of NPs have different abilities 
of acting as antecedents in an entity, and take dif-
ferent clustering strategy for them respectively, 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html 
this is obvious better than the methods that only 
use statistical data.  
Table7: ExperimentB for ACE2005/2004 Corpora 
with ME Model 
 R P F 
algorithm1 48.9/48.3 44.2/50.3 46.4/49.3
algorithm2 57.4/59.5 52.3/61.4 54.7/60.4
Ours 68.1/69.8 65.7/72.6 66.9/71.2
We also noticed that the experimental results 
with maximum entropy model are poorer than with 
CRFs model. We think this maybe because that the 
combined features are obtained under CRFs model, 
thus they will be more suitable for CRFs model 
than for maximum entropy model, that is to say 
these obtained combined features don?t play the 
same role in maximum entropy model as they do in 
CRFs model. 
Experimental results in table 6 surprised us 
greatly. PRO resolution gets so poor a performance 
that it is only about 1/3 of the NAM resolution?s 
performance. And NOM resolution?s performance 
is also pessimistic, which reaches about 80% of the 
NAM resolution?s performance. After analyses we 
found this is because there is too much confusing 
information for NOM?s resolution and PRO?s reso-
lution and system can hardly distinguish them cor-
rectly with current features description for an ex-
ample. For example, in a Chinese document, a 
NOM ???? (means president) may refer to a 
person A at sometime, but refer to person B at an-
other time, and there is no enough information for 
system to distinguish A and B. It is worse for PRO 
resolution because a PRO can refer to any NAM or 
NOM from a very long distance, there is little in-
formation for the system to distinguish which one 
it really refers to. For example, two PROs that both 
of whom are ??? (means he) , one refers to person 
A, the other refers to person B, even our human can 
hardly distinguish them, not to say the system. 
Fortunately, generally there are more NAMs and 
NOMs in a document, but less PROs. If they have 
similar amounts in a document, you can image 
how poor the performance of the coreference reso-
lution system would be.  
4 Conclusions 
In this paper, we present a hybrid machine learning 
approach for coreference resolution task. It uses 
CRFs as a basic classification model and uses ac-
tive learning method to generate some combined 
29
Sixth SIGHAN Workshop on Chinese Language Processing
features to make existed features used more effec-
tively; and we also proposed an effective clustering 
algorithm that used both the linguistics knowledge 
and the statistical knowledge. Experimental results 
show that additional performance gain can be ob-
tained by using active learning method, clustering 
algorithm has a great effect on coreference resolu-
tion?s performance and our clustering algorithm is 
very effective. Our experimental results also indi-
cate the key of coreference resolution is to improve 
the performance of the NOM resolution, especially 
the PRO resolution; both of them remain chal-
lenges for a coreference resolution system. 
Acknowledgments 
We used the method proposed in this paper for 
Chinese EDR (Entity Detection and Recognition) 
task of ACE07 (Automatic Content Extraction 
2007) and achieved very encouraging result. 
And this work was supported in part by the Na-
tional Natural Science Foundation of China under 
Grant No.60473140; the National 863 High-tech 
Project No.2006AA01Z154; the Program for New 
Century Excellent Talents in University No.NCET-
05-0287; and the National 985 Project No.985-2-
DB-C03. 
Reference 
Andrew Kachites McCallum and Kamal Nigam, 1998. 
Employing EM and pool-based active learning for 
text classification. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pp 
359-367  
Cohn, D., Grahramani, Z., & Jordan, M.1996. Active 
learning with statistical models. Journal of Artificial 
Intelligence Research, 4. pp 129-145 
Cynthia A.Thompson, Mary Leaine Califf, and Ray-
mond J.Mooney. 1999. Active learning for natural 
language parsing and information extraction. In Pro-
ceedings of the Seventeenth International Conference 
on Machine Learning, pp 406-414  
Hal Daume III and Daniel Marcu, 2005, A large-scale 
exploration of effective global features for a joint en-
tity detection and tracking model. Proceedings of 
HLT/EMNLP, 2005 
http://www.nist.gov/speech/tests/ace/ace07/doc, The 
ACE 2007 (ACE07) Evaluation Plan, Evaluation of 
the Detection and Recognition of ACE Entities, Val-
ues, Temporal Expressions, Relations and Events 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data. 
In International Conference on Machine Lear-
ingin(ICML01) 
Lafferty, J., McCallum, A., & Pereira, F. 2001. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proc. ICML 
Manabu Sassano. 2002. An Empirical Study of Active 
Learning with Support Vector Machines for Japanese 
Word Segmentation. Proceeding of the 40th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL), 2002, pp 505-512  
Min Tang, Xiaoqiang Luo, Salim Roukos.2002. Active 
Learning for Statistical Natural Language Parsing. 
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), 2002, 
pp.120-127. 
Pinto, D., McCallum, A., Lee, X., & Croft, W.B. 2003. 
combining classifiers in text categorization. SIGIR? 
03: Proceedings of the Twenty-sixth Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval.  
Radu Florian, Hongyan Jing, Nanda Kambhatla and 
Imed Zitouni, ?Factoring Complex Models: A Case 
Study in Mention Detection?, in Procedings of the 
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the ACL, pages 
473-480, Sydney, July 2006. 
R Florian, H Hassan et al 2004. A statistical model for 
multilingual entity detection and tracking. In Proc. Of 
HLT/NAACL-04, pp1-8 
Sha, F., & Pereira, F. 2003. Shallow parsing with condi-
tional random fields. Proceedings of Human Lan-
guage Technology, NAACL. 
Simon Tong, Daphne Koller. 2001. Support Vector Ma-
chine Active Learning with Applications to Text 
Classification. Journal of Machine Learning Re-
search,(2001) pp45-66. 
V.Ng and C.Cardie. 2002. Improving machine learning 
approaches to coreference resolution. In Proceedings 
of the ACL?02, pp.104-111. 
W.M.Soon, H.T.Ng, et al2001. A Machine Learning 
Approach to Coreference Resolution of Noun 
Phrases. Computational Linguistics, 27(4):521-544 
30
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 143?151,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Chinese-English Organization Name Translation Based on Correla-
tive Expansion 
Feiliang Ren, Muhua Zhu,  Huizhen Wang,   Jingbo Zhu  
Natural Language Processing Lab, Northeastern University, Shenyang, China 
{renfeiliang,zhumuhua}@gmail.com 
{wanghuizhen,zhujingbo}@mail.neu.edu.cn 
Abstract 
This paper presents an approach to trans-
lating Chinese organization names into 
English based on correlative expansion. 
Firstly, some candidate translations are 
generated by using statistical translation 
method. And several correlative named 
entities for the input are retrieved from a 
correlative named entity list. Secondly, 
three kinds of expansion methods are 
used to generate some expanded queries. 
Finally, these queries are submitted to a 
search engine, and the refined translation 
results are mined and re-ranked by using 
the returned web pages. Experimental re-
sults show that this approach outperforms 
the compared system in overall transla-
tion accuracy.  
1 Introduction 
There are three main types of named entity: loca-
tion name, person name, and organization name. 
Organization name translation is a subtask of 
named entity translation. It is crucial for many 
NLP tasks, such as cross-language information 
retrieval, machine translation, question and an-
swering system. For organization name transla-
tion, there are two problems among it which are 
very difficult to handle.  
Problem I: There is no uniform rule that can 
be abided by to select proper translation methods 
for the inside words of an organization name. For 
example, a Chinese word ????, when it is used 
as a modifier for a university, it is translated to 
Northeastern for ?????/Northeastern Uni-
versity?, and is translated to Northeast for ???
????/Northeast Forestry University?, and is 
mapped to Chinese Pinyin Dongbei for ????
???/Dongbei University of Finance and Eco-
nomics?. It is difficult to decide which transla-
tion method should be chosen when we translate 
the inside words of an organization name.  
Problem II: There is no uniform rule that can 
be abided by to select proper translation order 
and proper treatment of particles Here particles 
refer to prepositions and articles) for an input 
organization name. For example, the organiza-
tion name ???????/China Construction 
Bank? and the organization name ??????
?/Agricultural Bank of China?, they are very 
similar both in surface forms and in syntax struc-
tures, but their translation orders are different, 
and their treatments of particles are also different. 
Generally, there are two strategies usually 
used for named entity translation in previous re-
search. One is alignment based approach, and the 
other is generation based approach. Alignment 
based approach (Chen et al 2003; Huang et al 
2003; Hassan and Sorensen, 2005; and so on) 
extracts named entities translation pairs from 
parallel or comparable corpus by some alignment 
technologies, and this approach is not suitable to 
solve the above two problems. Because new or-
ganization names are constantly being created, 
and alignment based method usually fails to 
cover these new organization names that don?t 
occur in the bilingual corpus.  
Traditional generation based approach (Al-
Onaizan and Knight, 2002; Jiang et al.2007; 
Yang et al 2008; and so on) usually consists of 
two parts. Firstly, it will generate some candidate 
translations for the input; then it will re-rank 
these candidate translations to assign the correct 
translations high ranks. Cheng and Zong [2008] 
proposed another generation based approach for 
organization name translation, which directly 
translates organization names according to their 
inherent structures. But their approach still can?t 
solve the above two problems. This is because 
the amount of organization names is so huge and 
many of them have their own special translation 
rules to handle the above two problems. And the 
inherent structures don?t reveal these translation 
rules. Traditional generation based approach is 
suitable for organization name translation. But in 
previous research, the final translation perform-
ance depends on the candidate translation gen-
143
eration process greatly. If this generation process 
failed, it is impossible to obtain correct result 
from the re-ranking process. In response to this, 
Huang et al [2005] proposed a novel method that 
mined key phrase translation form web by using 
topic-relevant hint words. But in their approach, 
they removed the candidate translation genera-
tion process, which will improve extra difficult 
during mining phrase. Besides, in their approach, 
the features considered to obtain topic-relevant 
words are not so comprehensive, which will af-
fect the quality of returned web pages where the 
correct translations are expected to be included. 
There is still much room for the improvement 
process of the topic-relevant words extraction.  
Inspired by the traditional generation based 
named entity translation strategy and the ap-
proach proposed by Huang et al, we propose an 
organization name translation approach that min-
ing the correct translations of input organization 
name from the web. Our aim is to solve the 
above two problems indirectly by retrieving the 
web pages that contain the correct translation of 
the input and mining the correct translation from 
them. Given an input organization name, firstly, 
some candidate translations are generated by us-
ing statistical translation method. And several 
correlative named entities for the input are re-
trieved from a correlative named entity list. Sec-
ondly, expanded queries are generated by using 
three kinds of query expansion methods. Thirdly, 
these queries are submitted to a search engine, 
and the final translation results are mined and re-
ranked by using the returned web pages.  
The rest of this paper is organized as follows, 
section 2 presents the extraction process of cor-
relative named entities, section 3 presents a detail 
description of our translation method for Chinese 
organization name, and section 4 introduces our 
parameter evaluation method, and section 5 is the 
experiments and discussions part, finally conclu-
sions and future work are given in section 6.  
2 Extraction of Correlative Named En-
tities 
The key of our approach is to find some web 
pages that contain the correct translation of the 
input. With the help of correlative named entities 
(here if two named entities are correlative, it 
means that they are usually used to describe the 
same topic), it is easier to find such web pages. 
This is because that in the web, one web page 
usually has one topic. Thus if two named entities 
are correlative, they are very likely to occur in 
pair in some web pages.  
The correlative named entity list is constructed 
in advance. During translation, the correlative 
named entities for the input organization name 
are retrieved from this list directly. To set up this 
correlative named entity list, an about 180GB-
sized collection of web pages are used. Totally 
there are about 100M web pages in this collec-
tion. Named entities are recognized from every 
web page by using a NER tool. This NER tool is 
trained by CRF model 1  with the corpus from 
SIGHAN-20082.  
2.1 Features Used 
During the extraction of correlative named enti-
ties, the following features are considered.  
Co-occurrence in a Document The more of-
ten two named entities co-occur in a document, 
the more likely they are correlative. This feature 
is denoted as 1 2( , )iCoD n n , which means the co-
occurrence of named entities 1n and 2n  in a docu-
ment iD . This feature is also the main feature 
used in Huang et al [2005].   
Co-occurrence in Documents The more often 
two named entities co-occur in different docu-
ments, the more likely they are correlative. This 
feature is denoted as 1 2( , )CoDs n n , which means 
the number of documents that both 1n  and 2n oc-
cur in. 
Distance The closer two named entities is in a 
document, the more likely they are correlative. 
This feature is denoted as 1 2( , )iDistD n n , which 
means the number of words between 1n and 2n  
in a document iD . 
Mutual Information Mutual information is a 
metric to measure the correlation degree of two 
words. The higher two named entities? mutual 
information, the more likely they are correlative. 
And the mutual information of named entities 
1n and 2n  in a document iD is computed as fol-
lowing formula. 
1 2
1 2 1 2
1 2
( , )
( , ) ( , ) log
( ) ( )i
p n n
MID n n p n n
p n p n
= ?  (1) 
Jaccard Similarity Jaccard similarity is also a 
metric to measure the correlative degree of two 
words. The higher two named entities? Jaccard 
                                                 
1 http://www.chasen.org/~taku/software/CRF++/ 
2 http://www.china-language.gov.cn/bakeoff08/ 
144
similarity, the more likely they are correlative. 
And Jaccard similarity is computed as following 
formula. 
1 2
1 2
1 2 1 2
( , )
( , )
( ) ( ) ( , )
CoDs n n
Jaccard n n
D n D n CoDs n n
= + ? (2) 
where ( )iD n  is the number of documents that 
in occurs in, and  ( , )i jCoDs n n  is the number of 
documents that both in  and jn occur in. 
TF-IDF TF-IDF is a weight computation 
method usually used in information retrieval. 
Here for a named entity in , TF-IDF is used to 
measure the importance of its correlative named 
entities. The TF-IDF value of jn in a document 
iD is computed as following formula. 
( ) log
( )i j ij j
N
TF IDF n tf
D n
? = ?               (3) 
where ijtf is the frequency of jn in docu-
ment iD , N is the number of total documents, 
and ( )jD n is the number of documents that 
jn occurs in.  
2.2 Feature Combination 
During the process of feature combination, every 
feature is normalized, and the final correlative 
degree of two named entities is the linear combi-
nation of these normalized features, and it is 
computed as following formula.   
1 2
( , ) ( , )
( , )
( , ) ( , )
k i j
i jk
i j
k i j i j
j k j
CoD n n CoDs n n
C n n
CoD n n CoDs n n
? ?= +
?
?? ?
3 4
1 ( , )( , )
1 ( , )
( , )
k i j
k i jk k
k i j
k i j j kj k
MID n nDistD n n
MID n n
DistD n n
? ?+ +
? ?
????
5 6
( )( , )
( , ) ( )
k j
i j k
i j k j
j k j
TF IDF nJaccard n n
Jaccard n n TF IDF n
? ?
?
+ + ?
?
? ??
(4) 
Finally, for every organization name in , its 
top-K correlative named entities are selected to 
construct the correlative named entity list.  
During translation, the correlative words for 
the input can be retrieved from this correlative 
list directly. If the input is not included in this list, 
the same method as in Huang et al [2005] is 
used to obtain the needed correlative words.  
3 Organization Name Translation 
Based on Correlative Expansion 
3.1 Statistical Translation Module 
The first step of our approach is to generate some 
candidate translations for every input organiza-
tion name. As shown in table 1, these candidate 
translations are used as query stems during query 
expansion. We use Moses3, a state of the art pub-
lic machine translation tool, to generate such 
candidate translations. Here Moses is trained 
with the bilingual corpus that is from the 4th 
China Workshop on Machine Translation4. Total 
there are 868,947 bilingual Chinese-English sen-
tence pairs on news domain in this bilingual cor-
pus. Moses receives an organization name as in-
put, and outputs the N-best results as the candi-
date translations of the input organization name. 
Total there are six features used in Moses: phrase 
translation probability, inverse phrase translation 
probability, lexical translation probability, in-
verse lexical translation probability, language 
model, and sentence length penalty. All the 
needed parameters are trained with MERT 
method (Och, 2003) by using a held-out devel-
opment set.  
3.2 Query Expansions 
Because the amount of available web pages is so 
huge, the query submitted to search engine must 
be well designed. Otherwise, the search engine 
will return large amount of un-related web pages. 
This will enlarge the difficulty of mining phase. 
Here three kinds of expansion methods are pro-
posed to generate some queries by combining the 
clues given by statistical translation method and 
the clues given by correlative named entities of 
the input. And these correlative named entities 
are retrieved from the correlative named entities 
list before the query expansions process. These 
three kinds of expansions are explained as fol-
lows. 
3.2.1 Monolingual Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, and jn is 
one of its correlative named entities. If jn can be 
reliably translated5, we expand is with this reli-
                                                 
3 http://www.statmt.org/moses/  
4 http://www.nlpr.ia.ac.cn/cwmt-2008  
5 A word can be reliably translated means either it has 
a unique dictionary translation or it is a Chinese 
145
able translation ( )jt n  to form a query 
? is + ( )jt n ?. This kind of expansion is called as 
monolingual expansion.  
For two named entities, if they are correlative, 
their translations are likely correlative too. So 
their translations are also likely to occur in pair 
in some web pages. Suppose a query generated 
by this expansion is ? is + ( )jt n ?, if the candidate 
translation is is the correct translation of the in-
put, there must be some returned web pages that 
contain is completely. Otherwise, it is still possi-
ble to obtain some returned web pages that con-
tain the correct translation. This is because that 
the search engine will return both the web pages 
that include the query completely and the web 
pages that include the query partly. And for a 
translation candidate is and the correct transla-
tion 'is , they are very likely to have some com-
mon words, so some of their returned web pages 
may overlap each other. Thus it can be expected 
that when we submit ? is + ( )jt n ? to search en-
gine, it will return some web pages that include 
? 'is + ( )jt n ? or include 'is .  This is very helpful 
for the mining phase. 
3.2.2 Bilingual Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, we ex-
pand is with in  to form a query ? is + in ?. This 
kind of expansion is called as bilingual expan-
sion. 
Bilingual expansion is very useful to verify 
whether a candidate translation is the correct 
translation. To give readers more information or 
they are not sure about the translation of original 
named entity, the Chinese authors usually in-
clude both the original form of a named entity 
and its translation in the mix-language web pages 
[Fei Huang et al 2005]. So the correct translation 
pair is likely to obtain more supports from the 
returned web pages than those incorrect transla-
tion pairs. Thus bilingual expansion is very use-
ful for the re-ranking phase. 
Besides, for an input organization name, if one 
of its incorrect candidate translations is  is very 
                                                                          
person name and can be translated by Pinyin map-
ping.  
similar to the correct translation 'is  in surface 
form, the correct translation is also likely to be 
contained in the returned web pages by using this 
kind of queries. The reason for this is the search 
mechanism of search engine, which has been 
explained above in monolingual expansion. 
3.2.3 Mix-language Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, and jn is 
one of its correlative named entities. We ex-
pand is with jn  to form a query ? is + jn ?. This 
kind of expansion is called as mix-language ex-
pansion.  
Mix-language expansion is a necessary com-
plement to the other two expansions. Besides, 
this mix-language expansion is more prone to 
obtain some mix-language web pages that may 
contain both the original input organization name 
and its correct translation.  
3.3 Mining 
When the expanded queries are submitted to 
search engine, the correct translation of the input 
organization name may be contained in the re-
turned web pages. Because the translation of an 
organization name must be also an organization 
name, we mine the correct translation of the in-
put among the English organization names. Here 
we use the Stanford named entity recognition 
toolkits6  to recognize all the English organiza-
tion names in the returned web pages. Then align 
these recognized organization names to the input 
by considering the following features. 
Mutual Translation Probability The transla-
tion probability measures the semantic equiva-
lence between a source organization name and its 
target candidate translation. And mutual transla-
tion probability measures this semantic equiva-
lence in two directions. For simplicity, here we 
use IBM model-1(Brown et al 1993), which 
computes two organization names? translation 
probability using the following formula. 
11
1
( | ) ( | )
J L
j lJ
lj
p f e p f e
L ==
= ??                  (6) 
where ( | )j lp f e is the lexical translation prob-
ability. Suppose the input organization name 
is in , is is one of the recognized English organi-
                                                 
6  http://nlp.stanford.edu/software/CRF-NER.shtml 
146
zation names, the mutual translation probability 
of in and is  is computed as: 
( , ) ( | ) (1 ) ( | )i i i i i imp n s p n s p s n? ?= + ?      (7) 
Golden Translation Ratio For two organiza-
tion names, their golden translation ratio is de-
fined as the percentage of words in one organiza-
tion name whose reliable transactions can be 
found in another organization name. This feature 
is used to measure the probability of one named 
entity is the translation of the other. It is com-
puted as following formula. 
( , ) ( , )
( , ) (1 )
| | | |
i j j i
i j
i j
G n s G s n
GR n s
n s
? ?= + ?   (8) 
where ( , )i jG n s is the number of golden trans-
lated words from in to js , and ( , )j iG s n  is the 
number of golden translated words from js to in .  
Co-occurrence In Web Pages For an input 
organization name in and a recognized candidate 
translation js , the more often they co-occur in 
different web pages, the more likely they are 
translations of each other. This feature is denoted 
as ( , )i jCoS n s , which means the number of web 
pages that both 1n  and js occur in. 
Input Matching Ratio This feature is defined 
as the percentage of the words in the input that 
can be found in a returned web page. For those 
mix-language web pages, this feature is used to 
measure the probability of the correct translation 
occurring in a returned web page. It is computed 
as the following formula. 
| |
( , )
| |
i k
i k
i
n s
IMR n s
n
?=                             (9) 
where ks is the k th?  returned web page. 
Correlative Named Entities Matching Ratio 
This feature is defined as the percentage of the 
words in a correlative named entity that can be 
found in a returned web page. This feature is also 
used to measure the probability of the correct 
translation occurring in a returned web page. It is 
computed as the following formula. 
| |
_ ( , )
| |
i k
i k
i
c s
CW MR c s
c
?=                   (10) 
The final confidence score of in and jt to be a 
translation pair is measured by following formula. 
As in formula 4, here every factor will be is nor-
malized during computation.   
1 2( , ) ( , ) ( , )i j i j i jC n t mp n t GR n t? ?= +  
4
3
( , )
( , )
( , )
i j
i k
ki j
j
CoSs n n
IMR n s
CoS n n K
??+ + ??
5 _ ( , )i k
i k
CW MR c s
K I
?+ ? ??           (11) 
where K is the number of returned web pages, 
I is the number of correlative named entities for 
the input organization name. 
For every input organization name, we remain 
a fixed number of mined candidate translations 
with the highest confidence scores. And add 
them to the original candidate translation set to 
form a revised candidate translation set.  
3.4 Re-ranking 
The aim of mining is to improve recall. And in 
the re-ranking phase, we hope to improve preci-
sion by assigning the correct translation a higher 
rank. The features considered here for the re-
ranking phase are listed as follows.  
Confidence Score The confidence score of 
in and jt  is not only useful for the mining phase, 
but also is useful for the re-ranking phase. The 
higher this score, the higher rank this candidate 
translation should be assigned.  
Inclusion Ratio For Bilingual Query This 
feature is defined as the percentage of the re-
turned web pages that the bilingual query is 
completely matched. It is computed as the fol-
lowing formula. 
( )
_ ( )
( )
i
i
i
h q
EHR BQ q
H q
=                           (12) 
where ( )ih q is the number of web pages that 
match the query iq completely, and ( )iH q is the 
total number of returned web pages for query iq . 
Candidate Inclusion Ratio for Monolingual 
Query and Mix-language Query This feature is 
defined as the percentage of the returned web 
pages that the candidate translation is completed 
matched. This feature for monolingual query is 
computed as formula 13, and this feature for 
mix-language query is computed as formula 14. 
( )_ ( ) ( )
i
i
i
h sECHR MlQ s H q=                (13) 
( )_ ( ) ( )
i
i
i
h sECHR MixQ s H q=              (14) 
where ( )ih s  is the number of web pages that 
match the candidate translation is completely, and 
147
( )iH q is the total number of returned web pages 
for query iq .  
Finally, the above features are combined with 
following formula.  
2
1( , ) ( , ) _ ( )i j i j i
i
R n t C n t EHR BQ q
N
??= + ?  
3 _ ( )i
i
ECHR MlQ s
M
?+ ?
4 _ ( )i
i
ECHR MixQ s
L
?+ ?              (15) 
where N is the number of candidate transla-
tions, M and L  are the number of monolingual 
queries and mix-language queries respectively. 
At last the revised candidate translation set is 
re-ranked according to this formula, and the top-
K results are outputted as the input?s translation 
results.  
4 Parameters Evaluations 
In above formula (4), formula (11) and formula 
(15), the parameters i? are interpolation feature 
weights, which reflect the importance of different 
features. We use some held-our organization 
name pairs as development set to train these pa-
rameters. For those parameters in formula (4), we 
used those considered features solely one by one, 
and evaluated their importance according to their 
corresponding inclusion ratio of correct transla-
tions when using mix-language expansion and 
the final weights are assigned according to the 
following formula. 
i
i
i
i
InclusionRate
InclusionRate
? = ?                   (16) 
Where iInclusionRate  is the inclusion rate 
when considered feature if  only. The inclusion 
rate is defined as the percentage of correct trans-
lations that are contained in the returned web 
pages as Huang et al[2005] did. 
To obtain the parameters in formula (11), we 
used those considered features solely one by one, 
and computed their corresponding precision on 
development set respectively, and final weights 
are assigned according to following formula. 
i
i
i
i
P
P
? = ?                              (17) 
Where iP  is the precision when considered 
feature if  only. And for the parameters in for-
mula (15), their assignment method is the same 
with the method used for formula (11). 
5 Experiments and Discussions 
We use a Chinese to English organization name 
translation task to evaluate our approach. The 
experiments consist of four parts. Firstly, we 
evaluate the contribution of the correlative 
named entities for obtaining the web pages that 
contain the correct translation of the input. Sec-
ondly, we evaluate the contribution of different 
query expansion methods. Thirdly, we investi-
gate to which extents our approach can solve the 
two problems mentioned in section 1. Finally, we 
evaluate how much our approach can improve 
the overall recall and precision. Note that for 
simplicity, we use 10-best outputs from Moses as 
the original candidate translations for every input. 
And the search engine used here is Live7. 
5.1 Test Set 
The test set consists of 247 Chinese organization 
names recognized from 2,000 web pages that are 
downloaded from Sina8. These test organization 
names are translated by a bilingual speaker given 
the text they appear in. And these translations are 
verified from their official government web 
pages respectively. During translation, we don?t 
use any contextual information. 
5.2 Contribution of Correlative Named En-
tities 
The contribution of correlative named entities is 
evaluated by inclusion rate, and we compare the 
inclusion rate with different amount of correla-
tive named entities and different amount of re-
turned web pages. The experimental results are 
shown in Table 1 (here we use all these three 
kinds of expanding strategies).  
# of correlative named enti-
ties used 
 
1 5 10 
1 0.17 0.39 0.47 
5 0.29 0.63 0.78 
#of web 
pages used
10    0.32 0.76 0.82 
Table 1. Comparisons of inclusion rate  
From these results we can find that our ap-
proach obtains an inclusion rate of 82% when we 
use 10 correlative named entities and 10 returned 
web pages. We notice that there are some Chi-
nese organization names whose correct English 
translations have multiple standards. For exam-
ple,  the organization name ?????is translated 
                                                 
7  http://www.live.com/ 
8  http://news.sina.com.cn/ 
148
into ?Department of Defense? when it refers to a 
department in US, but  is translated into ?Minis-
try of Defence? when it refers to a department in 
UK or in Singapore. This problem affects the 
actual inclusion rate of our approach. Another 
factor that affects the inclusion rate is the search 
engine used. There is a small difference in the 
inclusion rate when different search engines are 
used. For example, the Chinese organization 
name ?????/China CITIC Bank?, because 
the word ???? is an out-of-vocabulary word,  
the best output from Moses is ?of the bank?. 
With such candidate translation, none of our 
three expansion methods works. But when we 
used Google as search engine instead, we mined 
the correct translation. 
From these results we can conclude that by us-
ing correlative named entities, the returned web 
pages are more likely to contain the correct trans-
lations of the input organization names. 
5.3 Contribution of Three Query Expansion 
Methods 
In this section, we evaluate the contribution of 
these three query expansion methods respectively. 
To do this, we use them one by one during trans-
lation, and compare their inclusion rates respec-
tively. Experimental results are shown in Table 2. 
#of web pages 
used 
 
1 5 10
1 0.002 0.0020.004
5 0.017 0.0190.019
Monolingual 
Expansion 
Only 10 0.021 0.0370.051
1 0.112 0.1590.174
5 0.267 0.3270.472
Bilingual 
 Expansion
Only 10 0.285 0.4140.669
1 0.098 0.1380.161
5 0.231 0.3070.386
# of  
correlative 
named enti-
ties used 
Mix-language 
Expansion
Only 10 0.249 0.3980.652
Table 2. Inclusion rate of different kinds of query 
expansion methods 
From Table 2 we can see that bilingual expan-
sion and mix-language expansion play greater 
roles than monolingual expansion in obtaining 
the web pages that contain the correct transla-
tions of the inputs. This is because the condition 
of generating monolingual queries is too strict, 
which requires a reliable translation for the cor-
relative named entity. In most cases, this condi-
tion cannot be satisfied. So for many input or-
ganization names, we cannot generate any mono-
lingual queries for them at all. This is the reason 
why monolingual expansion obtains so poorer an 
inclusion rate compared with the other two ex-
pansions. To evaluate the true contribution of 
monolingual expansion method, we carry out 
another experiment. We select 10 organization 
names randomly from the test set, and translate 
all of their correlative named entities into English 
by a bilingual speaker. Then we evaluate the in-
clusion rate again on this new test set. The ex-
perimental results are shown in Table 3. 
# of correlative named enti-
ties used 
 
1 5 10 
1 0.2 0.3 0.6 
5 0.4 0.7 0.9 
#of web 
pages used
10    0.4 0.8 0.9 
Table 3. Inclusion rate for monolingual expan-
sion on new test set 
From Table 3 we can conclude that, if most of 
the correlative named entities can be reliably 
translated, the queries generated by this mono-
lingual expansion will play greater role in obtain-
ing the web pages that contain the correct trans-
lations of the inputs. 
From those results in Table 2 we can conclude 
that, these three kinds of expansions complement 
each other. Using them together can obtain 
higher inclusion rate than using anyone of them 
only. 
5.4 Efficiency on Solving Problem I and 
Problem II 
In this section, we investigate to which extents 
our approach can solve the two problems men-
tioned in section 1.We compare the wrong trans-
lation numbers caused by these two problems 
(another main kind of translation error is caused 
by the translation of out-of-vocabulary words) 
between Moses and our approach. The experi-
mental results are shown in Table 4.  
 Moses Results Our method
Problem I 44 3 
Problem II 30 0 
Table 4. Comparison of error numbers 
From Table 4 we can see that our approach is 
very effective on solving these two problems. 
Almost all of the errors caused by these two 
problems are corrected by our approach. Only 
three wrong translations are not corrected. This is 
because that there are some Chinese organization 
names whose correct English translations have 
multiple standards, such as the correct translation 
of organization name ?????depends on its 
nationality, which has been explained in section 
5.2. 
149
5.5 Our Approach vs. Other Approaches  
In this section, we compare our approach with 
other two methods: Moses and the approach pro-
posed by Huang et al [2005]. We compare their 
accuracy of Top-K results. For both our approach 
and Huang et al?s approach, we use 10 correla-
tive words for each input organization name and 
use 10 returned web pages for mining the correct 
translation result. The experimental results are 
shown in Table 5. 
 Moses  
Results 
Huang?s 
Results 
Our  
Results 
Top 1 0.09 0.44 0.53 
Top 5 0.18 0.61 0.73 
Top 10 0.31 0.68 0.79 
Table 5. Moses results vs. our results 
Moses is a state-of-the-art translation method, 
but it can hardly handle the organization name 
translation well. In addition to the errors caused 
by the above two problems mentioned in section 
1, the out-of-vocabulary problem is another ob-
stacle for Moses. For example, when translating 
the organization name ?????????
/International Tsunami Information Centre?, be-
cause the word ???? is an out-of-vocabulary 
word, Moses fails to give correct translation. But 
for those approaches that have a web mining 
process during translation, both the out-of-
vocabulary problem and the two problems men-
tioned in section 1 are less serious. This is the 
reason that Moses obtains the lowest perform-
ance compared with the other two approaches. 
Our approach is also superior to Huang?s method, 
as shown in the above table. We think this is be-
cause of the following three reasons. The first 
reason is that in our approach, we use a transla-
tion candidate generation process. Although 
these candidates are usually not so good, they 
can still provide some very useful clue informa-
tion for the web retrieval process. The second 
reason is that the features considered for correla-
tive words extraction in our approach are more 
comprehensive. Most of the time (except for the 
case that the input is not included in the correla-
tive word list) our approach is more prone to ob-
tain better correlative words for the input. The 
third reason is that our approach use more query 
expansion strategies than Huang?s approach. 
These expansion strategies may complement 
each other and improve the probability of obtain-
ing the web pages that contain the correct trans-
lations For example, both Moses and Huang?s 
approach failed to translate the organization 
name ??????????. But in our approach, 
with the candidate translation ?International In-
formation Centre? that is generated by Moses, 
our approach still can obtain the web page that 
contains the correct translation when using bilin-
gual expansion. Thus the correct translation ?In-
ternational Tsunami Information Centre? is 
mined out during the sequent mining process.  
From table 5 we also notice that the final re-
call of our approach is a little lower than the in-
clusion rate as show in table 1. This means that 
our approach doesn?t mine all the correct transla-
tions that are contained in the returned web pages. 
One of the reasons is that some of the input or-
ganization names are not clearly expressed. For 
example, an input organization name ?????
??, although its correct translation ?University 
of California, Berkeley? is contained in the re-
turned web pages, this correct translation cannot 
be mined out by our approach. But if it is ex-
pressed as ??????????????, its 
correct translation can be mined from the re-
turned web pages easily. Besides, the recognition 
errors of NER toolkits will also reduce the final 
recall of our approach.  
6 Conclusions and Future Work 
In this paper, we present a new organization 
name translation approach. It uses some correla-
tive named entities of the input and some query 
expansion strategies to help the search engine to 
retrieve those web pages that contain the correct 
translation of the input. Experimental results 
show that for most of the inputs, their correct 
translations are contained in the returned web 
pages. By mining these correct translations and 
re-ranking them, the two problems mentioned in 
section 1 are solved effectively. And recall and 
precision are also improved correspondingly.  
In the future, we will try to improve the ex-
traction perform of correlative named entities. 
We will also try to apply this approach to the 
person name translation and location name trans-
lation. 
Acknowledgments  
This work was supported by the open fund of 
National Laboratory of Pattern Recognition, In-
stitute of Automation Chinese Academy of Sci-
ence, P.R.C, and was also supported in part by 
National Science Foundation of China 
(60873091), Natural Science Foundation of 
Liaoning Province (20072032) and Shenyang 
Science and Technology Plan (1081235-1-00). 
150
References 
Chen Hsin-Hsi, Changhua Yang, and Ying Lin. 2003. 
Learning formulation and transformation rules for 
multilingual named entities. Proceedings of the 
ACL 2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition. pp1-8. 
Dekang Lin, Shaojun Zhao, Durme Benjamin Van 
Drume, Marius Pasca. Mining Parenthetical Trans-
lations from the Web by Word Alignment,  ACL08. 
pp994-1002. 
Fan Yang, Jun Zhao, Bo Zou, Kang Liu, Feifan Liu. 
2008. Chinese-English Backward Transliteration 
Assisted with Mining Monolingual Web Pages. 
ACL2008. pp541-549. 
Fei Huang, Stephan Vogel and Alex Waibel. 2003. 
Automatic Extraction of Named Entity Translin-
gual Equivalence Based on Multi-feature Cost 
Minimization. Proceedings of the 2003 Annual 
Conference of the Association for Computational 
Linguistics, Workshop on Multilingual and Mixed-
language Named Entity Recognition.   
Fei Huang, Stephan vogel and Alex Waibel. 2004. 
Improving Named Entity Translation Combining 
Phonetic and Semantic Similarities. Proceedings of 
the HLT/NAACL. pp281-288.  
Fei Huang, Ying Zhang, Stephan Vogel. 2005. Min-
ing Key Phrase Translations from Web Corpora. 
HLT-EMNLP2005, pp483-490. 
Feng, Donghui, Yajuan LV, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2004), pp372-379. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. ACL2003. 
pp160-167. 
Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang. Learning 
Transliteration Lexicon from the Web. COL-
ING/ACL2006. pp1129-1136. 
Hany Hassan and Jeffrey Sorensen. 2005. An Inte-
grated Approach for Arabic-English Named Entity 
Translation. Proceedings of ACL Workshop on 
Computational Approaches to Semitic Languages. 
pp87-93. 
Lee, Chun-Jen and Jason S.Chang and Jyh-Shing 
Roger Jang. 2004a. Bilingual named-entity pairs 
extraction from parallel corpora. Proceedings of 
IJCNLP-04 Workshop on Named Entity Recogni-
tion for Natural Language Processing Application. 
pp9-16. 
Lee, Chun-Jen, Jason S.Chang and Thomas C. 
Chuang. 2004b. Alignment of bilingual named en-
tities in parallel corpora using statistical model. 
Lecture Notes in Artificial Intelligence. 3265:144-
153. 
Lee, Chun-Jen, Jason S.Chang, and Jyh-Shing Roger 
Jang. 2005. Extraction of transliteration pairs from 
parallel corpora using a sta Acquisition of English-
Chinese transliterated word pairs from parallel-
aligned text using a statistical transliteration model. 
Information Sciences.  
Long Jiang, Ming Zhou, Lee-Feng Chien, Cheng Niu. 
[2007]. Named Entity Translation with Web Min-
ing and Transliteration. IJCAI-2007. 
Moore, Robert C. 2003. Learning translations of 
named-entity phrases form parallel corpora. ACL-
2003. pp259-266. 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2):263-311.  
Y. Al-Onaizan and K. Knight. 2002. Translating 
named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meeting 
of the Association for Computational Linguistics, 
pp400-408. 
Ying Zhang and Phil Vines Using the Web for Auto-
mated Translation Extraction in Cross-Language 
Information Retrieval. SIGIR2004,pp162-169. 
Yufeng Chen, Chengqing Zong. A Structure-based 
Model for Chinese Organization Name Translation. 
ACM Transactions on Asian Language Information 
Processing, 2008, 7(1), pp1-30. 
151
