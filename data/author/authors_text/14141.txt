Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 161?166,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The LIG machine translation system for WMT 2010
Marion Potet, Laurent Besacier and Herve? Blanchon
LIG Laboratory, GETALP Team
University Joseph Fourier, Grenoble, France.
Marion.Potet@imag.fr
Laurent.Besacier@imag.fr
Herve.Blanchon@imag.fr
Abstract
This paper describes the system submit-
ted by the Laboratory of Informatics of
Grenoble (LIG) for the fifth Workshop
on Statistical Machine Translation. We
participated to the news shared transla-
tion task for the French-English language
pair. We investigated differents techniques
to simply deal with Out-Of-Vocabulary
words in a statistical phrase-based ma-
chine translation system and analyze their
impact on translation quality. The final
submission is a combination between a
standard phrase-based system using the
Moses decoder, with appropriate setups
and pre-processing, and a lemmatized sys-
tem to deal with Out-Of-Vocabulary con-
jugated verbs.
1 Introduction
We participated, for the first time, to the shared
news translation task of the fifth Workshop on Ma-
chine Translation (WMT 2010) for the French-
English language pair. The submission was
performed using a standard phrase-based trans-
lation system with appropriate setups and pre-
processings in order to deal with system?s un-
known words. Indeed, as shown in (Carpuat,
2009), (Habash, 2008) and (Niessen, 2004), han-
dling Ou-of-Vocabulary words with techniques
like lemmatization, phrase table extension or mor-
phological pre-processing is a way to improve
translation quality. After a short presentation of
our baseline system setups we discuss the effect
of Out-Of-Vocabulary words in the system and in-
troduce some ideas we chose to implement. In the
last part, we evaluate their impact on translation
quality using automatic and human evaluations.
2 Baseline System Setup
2.1 Used Resources
We used the provided Europarl and News par-
allel corpora (total 1,638,440 sentences) to train
the translation model and the News monolin-
gual corpora (48,653,884 sentences) to train the
language model. The 2008 News test corpora
(news-test2008; 2,028 sentences) was used to tune
the produced system and last year?s test corpora
(news-test2009; 3,027 sentences) was used for
evaluation purposes. These corpora will be ref-
ered to as Dev and Test later in the paper. As pre-
processing steps, we applied the PERL scripts pro-
vided with the corpora to lowercase and tokenise
the data.
2.2 Language modeling
The target language model is a standard n-gram
language model trained using the SRI language
modeling toolkit (Stocke, 2002) on the news
monolingual corpus. The smoothing technique we
applied is the modified Kneser-Ney discounting
with interpolation.
2.3 Translation modeling
The translation model was trained using the par-
allel corpus described earlier (Europarl+News).
First, the corpus was word aligned and then, the
pairs of source and corresponding target phrases
were extracted from the word-aligned bilingual
training corpus using the scripts provided with
the Moses decoder (Koehn et al, 2007). The re-
sult is a phrase-table containing all the aligned
phrases. This phrase-table, produced by the trans-
lation modeling, is used to extract several transla-
tions models. In our experiment we used thirteen
standard translation models: six distortion models,
a lexicon word-based and a phrase-based transla-
tion model for both direction, and a phrase, word
and distortion penalty.
161
2.4 Tuning and decoding
For the decoding (i.e. translation of the test
set), the system uses a log-linear combination of
the previous target language model and the thir-
teen translation models extracted from the phrase-
table. As the system can be beforehand tuned by
adjusting log-linear combination weights on a de-
velopement corpus, we used the Minimum Error
Rate Training (MERT) method, by (Och, 2003).
3 Ways of Improvements
3.1 Discussion about Out-Of-Vocabulary
words in PBMT systems
Phrase-based statistical machine translation
(PBMT) use phrases as units in the translation
process. A phrase is a sequence of n consecutive
words known by the system. During the training,
these phrases are automaticaly learned and each
source phrase is mapped with its corresponding
target phrase. Throughout test set decoding, a
word not being part of this vocabulary list is
labeled as ?Out-Of-Vocabulary? (OOV) and, as it
doesn?t appear in the translation table, the system
is unable to translate it. During the decoding,
Out-Of-Vocabulary words lead to ?broken?
phrases and degrade translation quality. For these
reasons, we present some techniques to handle
Out-Of-Vocabulary words in a PBMT system and
combine these techniques before evaluating them.
In a preliminary study, we automatically ex-
tracted and manually analyzed OOVs of a 1000
sentences sample extracted from the test cor-
pus (news-test2009). There were altogether 487
OOVs tokens wich include 64.34% proper nouns
and words in foreign languages, 17.62% common
nouns, 15.16% conjugated verbs, 1.84% errors in
source corpus and 1.02% numbers. Note that, as
our system is configured to copy systematically
the OOVs in the produced translated sentence, the
rewriting of proper nouns and words in foreign
language is straightforward in that case. However,
we still have to deal with common nouns and con-
jugated verbs.
Initial sentence:
?Cela ne marchera pas? souligna-t-il par la suite.
Normalised sentence:
?Cela ne marchera pas? il souligna par la suite
Figure 1: Normalisation of the euphonious ?t?
3.2 Term expansion with dictionary
The first idea is to expand the vocabulary size,
more specifically minimizing Out-Of-Vocabulary
common nouns adding a French-English dictio-
nary during the training process. In our experi-
ment, we used a free dictionnary made available
by the Wiktionary1 collaborative project (wich
aims to produce free-content multilingual dictio-
naries). The provided dictionnary, containing
15,200 entries, is added to the bilingual training
corpus before phrase-table extraction.
3.3 Lemmatization of the French source
verbs
To avoid Out-Of-Vocabulary conjugated verbs one
idea is to lemmatize verbs in the source train-
ing and test corpus to train a so-called lemma-
tized system. We used the freely available French
lemmatiser LIA TAGG (Be?chet, 2001). But, ap-
plying lemmatization leads to a loss of informa-
tion (tense, person, number) which may affect
deeply the translation quality. Thus, we decided
to use the lematized system only when OOV verbs
are present in the source sentence to be trans-
lated. Consequently, we differentiate two kinds
of sentences: -sentences containing at least one
OOV conjugated verb, and - sentences which do
not have any conjugated verb (these latter sen-
tences obviously don?t need any lemmatization!).
Thereby, we decided to build a combined trans-
lation system which call the lemmatized system
only when the source sentence contains at least
one Out-Of-Vocabulary conjugated verb (other-
wise, the sentence will be translated by the stan-
dard system). To detect sentences with Out-Of-
Vocabulary conjugated verb we translate each sen-
tence with both systems (lemmatized and stan-
dard), count OOV and use the lemmatized transla-
tion only if it contains less OOV than the standard
translation. For example, a translation containing
k Out-Of-Vocabulary conjugated verbs and n oth-
ers Out-Of-Vocabulary words (in total k+n OOV)
with the standard system, contains, most probably,
only n Out-Of-Vocabulary words with the lemma-
tised system because the conjugated verbs will be
lemmatized, recognized and translated by the sys-
tem.
1http://wiki.webz.cz/dict/
162
3.4 Normalization of a special French form
We observed, in the French source corpra, a spe-
cial French form which generates almost always
Out-Of-Vocabulary words in the English transla-
tion. The special French form, named euphonious
?t?, consists of adding the letter ?t? between a verb
(ended by ?a?, ?e? or ?c?) and a personal pronoun
and, then, inverse them in order to facilitate the
prononciation. The sequence is represented by:
verb-t-pronoun like annonca-t-elle, arrive-t-il, a-
t-on, etc. This form concerns 1.75% of the French
sentences in the test corpus whereas these account
for 0.66% and 0.78% respetively in the training
and the developement corpora. The normalized
proposed form, illustrated below in figure 1, con-
tains the subject pronoun (in first posistion) and
the verb (in the second position). This change has
no influence on the French source sentence and ac-
cordingly on the correctness and fluency of the En-
glish translation.
3.5 Adaptation of the language model
Finally, for each system, we decided to apply dif-
ferent language models and to look at those who
perfom well. In addition to the 5-gram language
model, we trained and tested 3-gram and 4-gram
language models with two different kinds of vo-
cabularies : - the first one (conventional, refered to
as n-gram in table 3) contains an open-vocabulary
extracted from the monolingual English training
data, and - the second one (refered to as n-gram-
vocab in table 3) contains a closed-vocabulary ex-
tracted from the English part of the bilingual train-
ing data. In both cases, language model probabil-
ities are trained from the monolingual LM train-
ing data but, in the second case, the lexicon is re-
stricted to the one of the phrase-table.
4 Experimental results
In the automatic evaluation, the reported evalu-
ation metric is the BLEU score (Papineni et al,
2002) computed by MTEval version 13a. The re-
sults are reported in table 1. Note that in our ex-
periments, according to the resampling method of
(Koehn, 2004), there are significative variations
(improvement or deterioration), with 95% cer-
tainty, only if the difference between two BLEU
scores represent, at least, 0.33 points. To complete
this automatic evaluation, we performed a human
analysis of the systems outputs.
4.1 Standard systems
4.1.1 Term expansion with dictionary
Regarding the results of automatic evaluation (ta-
ble 1, system (2)), adding the dictionary do not
leads to a significant improvement. The OOV
rate and system perplexity are reduced but, ignor-
ing the tuned system which presents lower per-
formance, the BLEU score decreases significatly
on the test set. The BLEU score of the system
augmented with the dictionary is 24.50 whereas
the baseline one is 24.94. So we can conclude
that there is not a meaningfull positive contribu-
tion, probably because the size of the dictionary
is very small regarding the bilingual training cor-
pus. We found out very few Out-Of-Vocabulary
words of the standard system recognized by the
system with the dictionary, see figure 2 for exam-
ple (among them : coupon, cafard, blonde, retar-
dataire, me?dicaments, pamplemousse, etc.). But,
as the dictionnary is very small, most OOV com-
mon words like ho?tesse and clignotant are still un-
known. Regarding the output sentences, we note
that there are very few differences and the quality
is equivalent. The dictionary used is to small to
extend the system?s vocabulary and most of words
still Out-Of-Vocabulary are conjugated verbs and
unrecognized forms.
Baseline system:
A cafard fled before the danger, but if he felt fear?
System with dictionary:
A blues fled before the danger, but if he felt fear?
Figure 2: Example of sentence with an OOV com-
mon noun
4.1.2 Normalisation of special French form
Considering the BLEU score, the normalization of
French euphonious ?t? have, apparently, very few
repercussion on the translation result (table 1, sys-
tem (3)) but the human analysis indicates that, in
our context, the normalisation of euphonious ?t?
brings a clear improvement as seen in example 3.
Consequently, this preprocessing is kept in the fi-
nal system.
4.1.3 Tuning
We can see in table 1 that the usual tuning with
Minimum Error Rate Training algorithm deterio-
rates systematically performance scores on the test
set, for all systems. This can be explained by the
163
System OOVs ppl Dev score Test score
(1) Baseline 2.32% 207 29.72 (19.93) 23.77 (24.94)
(2) + dictionary 2.30% 204 30.01 (23.92) 24.32 (24.50)
(3) + normalization 2.31% 204 30.07 (19.90) 23.99 (24.98)
(4) + normalization + Dev data 2.30% 204 / (/) / (25,05)
Table 1: Standard systems BLEU scores with tuning (without tuning)/ LM 5-gram
Baseline system:
?It will not work? souligna-t-il afterwards.
System with normalisation:
?It will not work? he stressed afterwards.
Figure 3: Example of sentence with a ?verb-t-
pronoun? form
gap between the developement and test corpora (ie
the Dev set may be not representative of the Test
set). So, even if it is recommanded in the standard
process, we do not tune our system (we use the de-
fault weights proposed by the Moses decoder) and
add the developement corpus to train it. In this
case, the training set contains 1,640,468 sentences
(the initial 1,638,440 sentences and the 2,028 sen-
tences of the developement set). This slightly im-
proves the system (from 24.98, the BLEU score
raise to 25,05 after adding the developpement set
to the training).
4.2 Lemmatised systems
Results of lemmatised systems are reported on ta-
ble 2. First, we can notice that, in this particular
case, the tuning (with MERT method) is manda-
tory to adapt the weights of the log linear model.
Our analysis of the tuned weight of the lemma-
tised system shows that, in particular, the word
penalty model has a very low weight (this favours
short sentences) and the lexical word-based trans-
lation models have a very low weight (no use of
the lexical translation probability). We also no-
tice that the lemmatization leads to a real drop-off
of OOV rate (fall from 2.32% for the baseline, to
2.23% for the lemmatized system) and perplexity
(fall from 207 for the baseline, to 178 for the lem-
matized system). We can observe a clear decrease
of the performance with the lemmatized system
(BLEU score of 20.50) compared with a non-
lemmatized one (BLEU score of 24.94). This can
be significatively improved applying euphonious
?t? normalization to the source data (BLEU score
of 22.14). Almost all French OOV conjugated
verbs with the standard system were recognized
by the lemmatized one (trierait, joues, testaient,
immerge?e, e?conomiseraient, baisserait, pre?pares,
etc.) but the small decrease of the translation qual-
ity can be explained, among other things, by sev-
eral tense errors. See illustration in figure 4. So,
we conclude that the systematic normalization of
French verbs, as a pre-process, reduce the Out-Of-
Vocabulary conjugated verbs but decrease slighly
the final translation quality. The use of such a sys-
tem is helpfull especially when the sentence con-
tains conjugated verbs (see example 5).
4.3 Adaptation of the language model
We applied five differents language models (3-
gram and 4-gram language models with selected
vocabulary or not and a 5-gram language model)
to the four standard systems and the two lemma-
tised one. The results, reported in table 3, show
that BLEU score can be significantly different de-
pending on the language model used. For exam-
ple, the fifth system (5) obtained a BLEU score of
21.48 with a 3-gram language model and a BLEU
score of 22.84 with a 4-gram language model. We
can also notice that five out of our six systems out-
perform using a language model with selected vo-
cabulary (n-gram-vocab). One possible explana-
tion is that with LM using selected vocabulary (n-
gram-vocab), there is no loss of probability mass
for english words not present in the translation ta-
ble.
4.4 Final combined system
Considering the previous observations, we believe
that the best choice is to apply the lemmatized
system only if necessary i.e. only if the sentence
contains OOV conjugated verbs, otherwise, a stan-
dard system should be used. We consider system
(4), with 4-gram-vocab language model (selected
vocabulary) without tuning, as the best standard
system and system (6), with 3-gram-vocab lan-
guage model (selected vocabulary) not tuned ei-
ther, as the best lemmatized system. The final
164
System OOVs ppl Dev score Test score
(5) lemmatization 2.23% 178 20.97 (8.57) 20.50 (8.56)
(6) lemmatization + normalization 2.18% 175 27.81 (9.20) 22.14 (10.82)
Table 2: Lemmatised systems BLEU scores with tuning (without tuning)/ LM 5-gram
Baseline system: You will be limited by the absence of exit for headphones.
Lemmatised system: You are limited by the lack of exit for ordinary headphones.
reference: You will be limited by the absence of output on ordinary headphones.
Figure 4: Example of sentences without OOV verbs
system translations are those of the lemmatized
system (6) when we translate sentences with one
or more Out-Of-Vocabulary conjugated verbs and
those of the un-lemmatized system (4) otherwise.
Around 6% of test set sentences were translated
by the lemmatized system. Considering the results
reported in table 4, the combined system?s BLEU
score is comparable to the standard one (25.11
against 25.17).
System Test score sentences
(4) Standard sys. 25.17 94 %
(6) Lemmatised sys. 22.89 6%
(7) Combined 25.11 100 %
Table 4: Combined system?s results and % trans-
lated sentences by each system
5 Human evaluation
We compared two data set. The first set (selected
sent.) contains 301 sentences selected from test
data by the combined system (7) to be translated
by the lemmatized system (6) whereas the second
set (random sent.) contains 301 sentences ran-
domly picked up. The latter is our control data set.
We compared for both groups the translation hy-
pothesis given by the lemmatized system and the
standard one.
We performed a subjective evaluation with the
NIST five points scales to measure fluency and ad-
equacy of each sentences through SECtra w inter-
face (Huynh et al, 2009).We involved a total of 6
volunteers judges (3 for each set). We evaluated
the inter-annotator agreement using a generalized
version of Kappa. The results show a slight to fair
agreement according (Landis, 1977).
The evaluation results, detailled in table 5 and 6,
showed that both fluency and adequacy were im-
proved using our combined system. Indeed, for a
random input (random sent.), the lemmatized sys-
tem lowers the translations quality (fluency and
adequacy are degraded for, respectively, 35.8%
and 37.5% of the sentences), while it improves
the quality for sentences selected by the combined
system (for ?selected sent.?, fluency and adequacy
are improved or stable for 81% of the sentences).
Adequacy selected sent. random sent.
(6) ? (4) 81% 62.4%
(6) < (4) 18.9% 37.5%
Table 5: Subjective evaluation of sentences ade-
quacy ((6) lemmatized system - (4) standard sys-
tem)
Fluency selected sent. random sent.
(6) ? (4) 81% 64.1%
(6)<(4) 18.9% 35.8%
Table 6: Subjective evaluation of sentences flu-
ency ((6) lemmatized system - (4) standard sys-
tem)
6 Conclusion and Discussion
We have described the system used for our sub-
mission to the WMT?10 shared translation task for
the French-English language pair.
We propose dsome very simple techniques to
improve rapidely a statistical machine translation.
Those techniques particularly aim at handling
Out-Of-Vocabulary words in statistical phrase-
based machine translation and lead an improved
fluency in translation results. The submited sys-
tem (see section 4.4) is a combination between a
standard system and a lemmatized system with ap-
propriate setup.
165
Baseline system: At the end of trade, the stock market in the negative bascula.
Lemmatised system: At the end of trade, the stock market exchange stumbled into the negative.
Baseline system: You can choose conseillera.
Lemmatised system: We would advise you, how to choose.
Figure 5: Example of sentences with OOV conjugated verbs
System 3-gram 3-gram-vocab 4-gram 4-gram-vocab 5-gram
(1) 24.60 24.95 24.94 25.11 24.94
(2) 25.14 25.17 24.50 23.49 24.50
(3) 24.88 25.00 24.98 25.15 24.98
(4) 24.92 24.99 25.05 25.17 25.05
(5) 21.48 19.48 22.84 20.18 20.50
(6) 22.60 22.89 22.14 22.24 22.14
Table 3: Systems?s results on test set with differents language models
This system evaluation showed a positive influ-
ence on translation quality, indeed, while the im-
provements on automatic metrics are small, man-
ual inspection suggests a significant improvements
of translation fluency and adequacy.
In future work, we plan to investigate and de-
velop more sophisticated methods to deal with
Out-Of-Vocabulary words, still relying on the an-
alyze of our system output. We believe, for ex-
ample, that an appropriate way to use the dictio-
nary, a sensible pre-processings of French source
texts (in particular normalization of some specific
French forms) and a factorial lemmatization with
the tense information can highly reduce OOV rate
and improve translation quality.
References
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). Prague, Czech Republic.
Papineni K., Roukos S., Ward T., and Zhu W.J. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. ACL-2002: 40th Annual meeting
of the Association for Computational Linguistics,
pp. 311?318. Philadelphia, Pennsylvania, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. International Conference
on Spoken Language Processing, Vol. 2, pp 901?
904. Denver, Colorado, USA.
Frederic Be?chet. 2001. LIA TAGG. http://old.lia.univ
-avignon.fr/chercheurs/bechet/download fred.html.
Franz Josef Och. 2003. Minimum error rate train-
ing for statistical machine translation. Annual Meet-
ing of the Association for Computational Linguistics
(ACL). Sapporo, July.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. conference on
Empirical Methods in Natural Language Processing
(EMNLP), pp 388?395. Barcelona, Spain.
Marine Carpuat. 2009. Toward Using Morphology in
French-English Phrase-based SMT. Workshop on
Machine Translation in European Association for
Computational Linguistics (EACL-WMT), pp 150?
154. Athens, Greece.
Sonja Niessen and Hermann Ney. 2004. Statistical
Machine Translation with Scarce Resources Using
Morpho-syntactic Information. Computational Lin-
guistics, vol. 30, pp 181?204.
Nizar Habash. 2008. Four techniques for Online
Handling of Out-Of-Vocabulary Words in Arabic-
English Statistical Machine Translation. Human
Language Technology Workshop in Association for
Computational Linguistics, (ACL-HTL), pp 57?60.
Columbus, Ohio, USA.
Landis J. R. and Koch G. G.. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics, vol. 33, pp. 159?174.
Herve? Blanchon, Christian Boitet and Cong-Phap
Huynh. 2009. A Web Service Enabling Grad-
able Post-edition of Pre-translations Produced by
Existing Translation Tools: Practical Use to Provide
High-quality Translation of an Online Encyclopedia.
MT Summit XII, Beyond Translation Memories: New
Tools for Translators Workshop, pp 20?27. Ottawa,
Canada.
166
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440?446,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The LIGA (LIG/LIA) Machine Translation System for WMT 2011
Marion Potet1, Raphae?l Rubino2, Benjamin Lecouteux1, Ste?phane Huet2,
Herve? Blanchon1, Laurent Besacier1 and Fabrice Lefe`vre2
1UJF-Grenoble1, UPMF-Grenoble2
LIG UMR 5217
Grenoble, F-38041, France
FirstName.LastName@imag.fr
2Universite? d?Avignon
LIA-CERI
Avignon, F-84911, France
FirstName.LastName@univ-avignon.fr
Abstract
We describe our system for the news com-
mentary translation task of WMT 2011. The
submitted run for the French-English direction
is a combination of two MOSES-based sys-
tems developed at LIG and LIA laboratories.
We report experiments to improve over the
standard phrase-based model using statistical
post-edition, information retrieval methods to
subsample out-of-domain parallel corpora and
ROVER to combine n-best list of hypotheses
output by different systems.
1 Introduction
This year, LIG and LIA have combined their efforts
to produce a joint submission to WMT 2011 for the
French-English translation task. Each group started
by developing its own solution whilst sharing re-
sources (corpora as provided by the organizers but
also aligned data etc) and acquired knowledge (cur-
rent parameters, effect of the size of n-grams, etc.)
with the other. Both LIG and LIA systems are stan-
dard phrase-based translation systems based on the
MOSES toolkit with appropriate carefully-tuned se-
tups. The final LIGA submission is a combination
of the two systems.
We summarize in Section 2 the resources used
and the main characteristics of the systems. Sec-
tions 3 and 4 describe the specificities and report
experiments of resp. the LIG and the LIA system.
Section 5 presents the combination of n-best lists
hypotheses generated by both systems. Finally, we
conclude in Section 6.
2 System overview
2.1 Used data
Globally, our system1 was built using all the French
and English data supplied for the workshop?s shared
translation task, apart from the Gigaword monolin-
gual corpora released by the LDC. Table 1 sums up
the used data and introduces designations that we
follow in the remainder of this paper to refer to cor-
pora. Four corpora were used to build translation
models: news-c, euro, UN and giga, while three
others are employed to train monolingual language
models (LMs). Three bilingual corpora were de-
voted to model tuning: test09 was used for the de-
velopment of the two seed systems (LIG and LIA),
whereas test08 and testcomb08 were used to tune the
weights for system combination. test10 was finally
put aside to compare internally our methods.
2.2 LIG and LIA system characteristics
Both LIG and LIA systems are phrase-based trans-
lation models. All the data were first tokenized with
the tokenizer provided for the workshop. Kneser-
Ney discounted LMs were built from monolingual
corpora using the SRILM toolkit (Stolcke, 2002),
while bilingual corpora were aligned at the word-
level using GIZA++ (Och and Ney, 2003) or its
multi-threaded version MGIZA++ (Gao and Vogel,
2008) for the large corpora UN and giga. Phrase
table and lexicalized reordering models were built
with MOSES (Koehn et al, 2007). Finally, 14 fea-
tures were used in the phrase-based models:
1When not specified otherwise ?our? system refers to the
LIGA system.
440
CORPORA DESIGNATION SIZE (SENTENCES)
English-French Bilingual training
News Commentary v6 news-c 116 k
Europarl v6 euro 1.8 M
United Nation corpus UN 12 M
109 corpus giga 23 M
English Monolingual training
News Commentary v6 mono-news-c 181 k
Shuffled News Crawl corpus (from 2007 to 2011) news-s 25 M
Europarl v6 mono-euro 1.8 M
Development
newstest2008 test08 2,051
newssyscomb2009 testcomb09 502
newstest2009 test09 2,525
Test
newstest2010 test10 2,489
Table 1: Used corpora
? 5 translation model scores,
? 1 distance-based reordering score,
? 6 lexicalized reordering score,
? 1 LM score and
? 1 word penalty score.
The score weights were optimized on the test09 cor-
pus according to the BLEU score with the MERT
method (Och, 2003). The experiments led specifi-
cally with either LIG or LIA system are respectively
described in Sections 3 and 4. Unless otherwise
indicated, all the evaluations were performed using
case-insensitive BLEU and were computed with the
mteval-v13a.pl script provided by NIST. Ta-
ble 2 summarizes the differences between the final
configuration of the systems.
3 The LIG machine translation system
LIG participated for the second time to the WMT
shared news translation task for the French-English
language pair.
3.1 Pre-processing
Training data were first lowercased with the PERL
script provided for the campaign. They were also
processed in order to normalize a special French
form (named euphonious ?t?) as described in (Potet
et al, 2010).
The baseline system was built using a 4-gram LM
trained on the monolingual corpora provided last
year and translation models trained on news-c and
euro (Table 3, System 1). A significant improve-
ment in terms of BLEU is obtained when taking into
account a third corpus, UN, to build translation mod-
els (System 2). The next section describes the LMs
that were trained using the monolingual data pro-
vided this year.
3.2 Language model training
Target LMs are standard 4-gram models trained
on the provided monolingual corpus (mono-news-c,
mono-euro and news-s). We decided to test two dif-
ferent n-gram cut-off settings. The fist set has low
cut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram,
3-gram and 4-gram counts), whereas the second one
(LM2) is more aggressive: 1-5-7-7. Experiment re-
sults (Table 3, Systems 3 and 4) show that resorting
to LM2 leads to an improvement of BLEU with re-
spect to LM1. LM2 was therefore used in the sub-
sequent experiments.
441
FEATURES LIG SYSTEM LIA SYSTEM
Pre-processing Text lowercased Text truecasedNormalization of French euphonious
?t?
Reaccentuation of French words start-
ing with a capital letter
LM Training on mono-news-c, news-s and
mono-euro
Training on mono-news-c and news-s
4-gram models 5-gram models
Translation model
Training on news-c, euro and UN Training on 10 M sentence pairs se-
lected in news-c, euro, UN and giga
Phrase table filtering
Use of -monotone-at-punctuation op-
tion
Table 2: Distinct features between final configurations retained for the LIG and LIA systems
3.3 Translation model training
Translation models were trained from the parallel
corpora news-c, euro and UN. Data were aligned
at the word-level and then used to build standard
phrase-based translation models. We filtered the ob-
tained phrase table using the method described in
(Johnson et al, 2007). Since this technique drasti-
cally reduces the size of the phrase table, while not
degrading (and even slightly improving) the results
on the development and test corpora (System 6), we
decided to employ filtered phrase tables in the final
configuration of the LIG system.
3.4 Tuning
For decoding, the system uses a log-linear com-
bination of translation model scores with the LM
log-probability. We prevent phrase reordering over
punctuation using the MOSES option -monotone-at-
punctuation. As the system can be beforehand tuned
by adjusting the log-linear combination weights on
a development corpus, we used the MERT method
(System 5). Optimizing weights according to BLEU
leads to an improvement with respect to the sys-
tem with MOSES default value weights (System 5
vs System 4).
3.5 Post-processing
We also investigated the interest of a statistical
post-editor (SPE) to improve translation hypotheses.
About 9,000 sentences extracted from the news do-
main test corpora of the 2007?2009 WMT transla-
tion tasks were automatically translated by a sys-
tem very similar to that described in (Potet et al,
2010), then manually post-edited. Manual correc-
tions of translations were performed by means of the
crowd-sourcing platform AMAZON MECHANICAL
TURK2 ($0.15/sent.). These collected data make
a parallel corpus whose source part is MT output
and target part is the human post-edited version of
MT output. This are used to train a phrase-based
SMT (with Moses without the tuning step) that au-
tomatically post-edit the MT output. That aims at
learning how to correct translation hypotheses. Sys-
tem 7 obtained when post-processing MT 1-best out-
put shows a slight improvement. However, SPE was
not used in the final LIG system since we lacked
time to apply SPE on the N-best hypotheses for the
development and test corpora (the N-best being nec-
essary for combination of LIG and LIA systems).
Ths LIGA submission is thus a constrained one.
3.6 Recasing
We trained a phrase-based recaser model on the
news-s corpus using the provided MOSES scripts
and applied it to uppercase translation outputs. A
common and expected loss of around 1.5 case-
sensitive BLEU points was observed on the test cor-
pus (news10) after applying this recaser (System 7)
with respect to the score case-insensitive BLEU pre-
viously measured.
2http://www.mturk.com/mturk/welcome
442
? SYSTEM DESCRIPTION BLEU SCORE
test09 test10
1 Training: euro+news-c 24.89 26.01
2 Training: euro+news-c+UN 25.44 26.43
3 2 + LM1 24.81 27.19
4 2 + LM2 25.37 27.25
5 4 + MERT on test09 26.83 27.53
6 5 + phrase-table filtering 27.09 27.64
7 6 + SPE 27.53 27.74
8 6 + recaser 24.95 26.07
Table 3: Incremental improvement of the LIG system in
terms of case-insensitive BLEU (%), except for line 8
where case-sensitive BLEU (%) are reported
4 The LIA machine translation system
This section describes the particularities of the MT
system which was built at the LIA for its first partic-
ipation to WMT.
4.1 System description
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated French
words starting with a capital letter. We significantly
cleaned up the crawled parallel giga corpus, keeping
19.3 M of the original 22.5 M sentence pairs. For ex-
ample, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with cap-
ital letters were removed. The whole training ma-
terial is truecased, meaning that the words occur-
ing after a strong punctuation mark were lowercased
when they belonged to a dictionary of common all-
lowercased forms; the others were left unchanged.
The training of a 5-gram English LM was re-
strained to the news corpora mono-news-c and news-
s that we consider large enough to ignore other data.
In order to reduce the size of the LM, we first limited
the vocabulary of our model to a 1 M word vocabu-
lary taking the most frequent words in the news cor-
pora. We also resorted to cut-offs to discard infre-
quent n-grams (2-2-3-5 thresholds on 2- to 5-gram
counts) and uses the SRILM option prune, which
allowed us to train the LM on large data with 32 Gb
RAM.
Our translation models are phrase-based models
(PBMs) built with MOSES with the following non-
default settings:
? maximum sentence length of 80 words,
? limit on the number of phrase translations
loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized re-
ordering model scores were optimized on the devel-
opment corpus thanks to the MERT algorithm.
Besides the size of used data, we experimented
with two advanced features made available for
MOSES. Firstly, we filtered phrase tables using the
default setting -l a+e -n 30. This dramatically
reduced phrase tables by dividing their size by a
factor of 5 but did not improve our best configu-
ration from the BLEU score perspective (Table 4,
line 1); the method was therefore not kept in the
LIA system. Secondly, we introduced reordering
constraints in order to consider quoted material as
a block. This method is particularly useful when ci-
tations included in sentences have to be translated.
Two configurations were tested: zone markups in-
clusion around quotes and wall markups inclusion
within zone markups. However, the measured gains
were finally too marginal to include the method in
the final system.
4.2 Parallel corpus subsampling
As the only news parallel corpus provided for the
workshop contains 116 k sentence pairs, we must
resort to parallel out-of-domain corpora in order to
build reliable translation models. Information re-
trieval (IR) methods have been used in the past to
subsample parallel corpora. For example, Hilde-
brand et al (2005) used sentences belonging to the
development and test corpora as queries to select the
k most similar source sentences in an indexed paral-
lel corpus. The retrieved sentence pairs constituted
a training corpus for the translation models.
The RALI submission for WMT10 proposed a
similar approach that builds queries from the mono-
lingual news corpus in order to select sentence pairs
stylistically close to the news domain (Huet et al,
2010). This method has the major interest that it
does not require to build a new training parallel
corpus for each news data set to translate. Fol-
lowing the best configuration tested in (Huet et al,
443
2010), we index the three out-of-domain corpora us-
ing LEMUR3, and build queries from English news-s
sentences where stop words are removed. The 10 top
sentence pairs retrieved per query are selected and
added to the new training corpus if they are not re-
dundant with a sentence pair already collected. The
process is repeated until the training parallel cor-
pus reaches a threshold over the number of retrieved
pairs.
Table 4 reports BLEU scores obtained with the
LIA system using the in-domain corpus news-c and
various amounts of out-of-domain data. MERT was
re-run for each set of training data. The first four
lines display results obtained with the same num-
ber of sentence pairs, which corresponds to the
size of news-c appended to euro. The experiments
show that using euro instead of the first sentences of
UN and giga significantly improves BLEU scores,
which indicates the better adequacy of euro with re-
spect to the test10 corpus. The use of the IR method
to select sentences from euro, UN and giga leads to
a similar BLEU score to the one obtained with euro.
The increase of the collected pairs up to 3 M pairs
generates a significant improvement of 0.9 BLEU
point. A further rise of the amount of collected
pairs does not introduce a major gain since retriev-
ing 10 M sentence pairs only augments BLEU from
29.1 to 29.3. This last configuration which leads to
the best BLEU was used to build the final LIA sys-
tem. Let us note that 2 M, 3 M and 15 M queries
were required to respectively obtain 3 M, 5 M and
10 M sentence pairs because of the removal of re-
dundant sentences in the increased corpus.
For a matter of comparison, a system was also
built taking into account all the training material,
i.e. 37 M sentence pairs4. This last system is out-
performed by our best system built with IR and has
finally close performance to the one obtained with
news-c+euro relatively to the quantity of used data.
5 The system combination
System combination is based on the 500-best out-
puts generated by the LIA and the LIG systems.
3www.lemurproject.org
4For this experiment, the data were split into three parts
to build independent alignment models: news-c+euro, UN and
giga, and they were joined afterwards to build translation mod-
els.
USED PARALLEL CORPORA FILTERING
without with
news-c + euro (1.77 M) 28.1 28.0
news-c + 1.77 M of UN 27.2 -
news-c + 1.77 M of giga 27.1 -
news-c + 1.77 M with IR 28.2 -
news-c + 3 M with IR 29.1 29.0
news-c + 5 M with IR 28.8 -
news-c + 10 M with IR 29.3 29.2
All data 28.9 29.0
Table 4: BLEU (%) on test10 measured with the LIA
system using different training parallel corpora
They both used the MOSES option distinct, en-
suring that the hypotheses produced for a given sen-
tence are different inside an N-best list. Each N-best
list is associated with a set of 14 scores and com-
bined in several steps.
The first step takes as input lowercased 500-best
lists, since preliminary experiments have shown a
better behavior using only lowercased output (with
cased output, combination presents some degrada-
tions). The score combination weights are opti-
mized on the development corpus, in order to max-
imize the BLEU score at the sentence level when
N-best lists are reordered according to the 14 avail-
able scores. To this end, we resorted to the SRILM
nbest-optimize tool to do a simplex-based
Amoeba search (Press et al, 1988) on the error func-
tion with multiple restarts to avoid local minima.
Once the optimized feature weights are com-
puted independently for each system, N-best lists
are turned into confusion networks (Mangu et al,
2000). The 14 features are used to compute poste-
riors relatively to all the hypotheses in the N-best
list. Confusion networks are computed for each sen-
tence and for each system. In Table 5 we present
the ROVER (Fiscus, 1997) results for the LIA and
LIG confusion networks (LIA CNC and LIG CNC).
Then, both confusion networks computed for each
sentence are merged into a single one. A ROVER
is applied on the combined confusion network and
generates a lowercased 1-best.
The final step aims at producing cased hypothe-
ses. The LIA system built from truecased corpora
achieved significantly higher performance than the
444
LIG LIA LIG CNC LIA CNC LIG+LIA
case-insensitive test10 27.6 29.3 28.1 29.4 29.7
BLEU test11 28.5 29.4 28.5 29.3 29.9
case-sensitive test10 26.1 28.4 27.0 28.4 28.7
BLEU test11 26.9 28.4 27.5 28.4 28.8
Table 5: Performance measured before and after combining systems
LIG system trained on lowercased corpora (Table 5,
two last lines). In order to get an improvement when
combining the outputs, we had to adopt the follow-
ing strategy. The 500-best truecased outputs of the
LIA system are first merged in a word graph (and
not a mesh lattice). Then, the lowercased 1-best
previously obtained with ROVER is aligned with the
graph in order to find the closest existing path, which
is equivalent to matching an oracle with the graph.
This method allows for several benefits. The new
hypothesis is based on a ?true? decoding pass gener-
ated by a truecased system and discarded marginal
hypotheses. Moreover, the selected path offers a
better BLEU score than the initial hypothesis with
and without case. This method is better than the one
which consists of applying the LIG recaser (section
3.6) on the combined (un-cased) hypothesis.
The new recased one-best hypothesis is then used
as the final submission for WMT. Our combination
approach improves on test11 the best single sys-
tem by 0.5 case-insensitive BLEU point and by 0.4
case-sensitive BLEU (Table 5). However, it also in-
troduces some mistakes by duplicating in particular
some segments. We plan to apply rules at the seg-
ment level in order to reduce these artifacts.
6 Conclusion
This paper presented two statistical machine trans-
lation systems developed at different sites using
MOSES and the combination of these systems. The
LIGA submission presented this year was ranked
among the best MT system for the French-English
direction. This campaign was the first shot for LIA
and the second for LIG. Beside following the tradi-
tional pipeline for building a phrase-based transla-
tion system, each individual system led to specific
works: LIG worked on using SPE as post-treatment,
LIA focused on extracting useful data from large-
sized corpora. And their combination implied to ad-
dress the interesting issue of matching results from
systems with different casing approaches.
WMT is a great opportunity to chase after perfor-
mance and joining our efforts has allowed to save
considerable amount of time for data preparation
and tuning choices (even when final decisions were
different among systems), yet obtaining very com-
petitive results. This year, our goal was to develop
state-of-the-art systems so as to investigate new ap-
proaches for related topics such as translation with
human-in-the-loop or multilingual interaction sys-
tems (e.g. vocal telephone information-query di-
alogue systems in multiple languages or language
portability of such systems).
References
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates:recognizer output vot-
ing error reduction (ROVER). In Proceedings of the
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?354, Santa Barbara, CA,
USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Proceedings of the
ACL Workshop: Software Engineering, Testing, and
Quality Assurance for Natural Language Processing,
pages 49?57, Columbus, OH, USA.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th confer-
ence of the European Association for Machine Trans-
lation (EAMT), Budapest, Hungary.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry, and
Philippe Langlais. 2010. The RALI machine trans-
lation system for WMT 2010. In Proceedings of the
ACL Joint 5th Workshop on Statistical Machine Trans-
lation and Metrics (WMT), Uppsala, Sweden.
Howard Johnson, Joel Martin, George Foster, and Roland
445
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, jun.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), Companion Vol-
ume, pages 177?180, Prague, Czech Republic, June.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The LIG machine translation for WMT 2010.
In Proceedings of the ACL Joint 5th Workshop on Sta-
tistical Machine Translation and Metrics (WMT), Up-
psala, Sweden.
William H. Press, Brian P. Flannery, Saul A. Teukolsky,
and William T. Vetterling. 1988. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Denver, CO, USA.
446
