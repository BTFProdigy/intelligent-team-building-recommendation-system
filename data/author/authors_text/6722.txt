Proceedings of the ACL 2007 Demo and Poster Sessions, pages 53?56,
Prague, June 2007. c?2007 Association for Computational Linguistics
Deriving an Ambiguous Word?s Part-of-Speech Distribution  
from Unannotated Text 
 
Reinhard Rapp 
Universitat Rovira i Virgili 
Pl. Imperial Tarraco, 1 
E-43005 Tarragona, Spain 
reinhard.rapp@urv.cat 
 
Abstract 
A distributional method for part-of-speech 
induction is presented which, in contrast 
to most previous work, determines the 
part-of-speech distribution of syntacti-
cally ambiguous words without explicitly 
tagging the underlying text corpus. This is 
achieved by assuming that the word pair 
consisting of the left and right neighbor of 
a particular token is characteristic of the 
part of speech at this position, and by 
clustering the neighbor pairs on the basis 
of their middle words as observed in a 
large corpus. The results obtained in this 
way are evaluated by comparing them to 
the part-of-speech distributions as found 
in the manually tagged Brown corpus.   
1 Introduction 
The purpose of this study is to automatically in-
duce a system of word classes that is in agreement 
with human intuition, and then to assign all possi-
ble parts of speech to a given ambiguous or unam-
biguous word. Two of the pioneering studies con-
cerning this as yet not satisfactorily solved prob-
lem are Finch (1993) and Sch?tze (1993) who clas-
sify words according to their context vectors as de-
rived from a corpus. More recent studies try to 
solve the problem of POS induction by combining 
distributional and morphological information (Clark, 
2003; Freitag, 2004), or by clustering words and 
projecting them to POS vectors (Rapp, 2005). 
Whereas all these studies are based on global 
co-occurrence vectors who reflect the overall be-
havior of a word in a corpus, i.e. who in the case of 
syntactically ambiguous words are based on POS-
mixtures, in this paper we raise the question if it is 
really necessary to use an approach based on mix-
tures or if there is some way to avoid the mixing 
beforehand. For this purpose, we suggest to look at 
local contexts instead of global co-occurrence vec-
tors. As can be seen from human performance, in 
almost all cases the local context of a syntactically 
ambiguous word is sufficient to disambiguate its 
part of speech. 
The core assumption underlying our approach, 
which in the context of cognition and child lan-
guage has been proposed by Mintz (2003), is that 
words of a particular part of speech often have the 
same left and right neighbors, i.e. a pair of such 
neighbors can be considered to be characteristic of 
a part of speech. For example, a noun may be sur-
rounded by the pair ?the ... is?, a verb by the pair 
?he ... the?, and an adjective by the pair ?the ... 
thing?. For ease of reference, in the remainder of 
this paper we call these local contexts neighbor 
pairs. The idea is now to cluster the neighbor pairs 
on the basis of the middle words they occur with. 
This way neighbor pairs typical of the same part of 
speech are grouped together. For classification, a 
word is assigned to the cluster where its neighbor 
pairs are found. If its neighbor pairs are spread 
over several clusters, the word can be assumed to 
be ambiguous. This way ambiguity detection fol-
lows naturally from the methodology. 
2 Approach 
Let us illustrate our approach by looking at Table 1. 
The rows in the table are the neighbor pairs that we 
want to consider, and the columns are suitable 
middle words as we find them in a corpus. Most 
words in our example are syntactically unambigu-
ous. Only link can be either a noun or a verb and 
therefore shows the co-occurrence patterns of both. 
Apart from the particular choice of features, what 
distinguishes our approach from most others is that 
we do not cluster the words (columns) which 
would be the more straightforward thing to do. In-
stead we cluster the neighbor pairs (rows). Clus-
tering the columns would be fine for unambiguous 
words, but has the drawback that ambiguous words 
53
tend to be assigned only to the cluster relating to 
their dominant part of speech. This means that no 
ambiguity detection takes place at this stage. 
In contrast, the problem of demixing can be av-
oided by clustering the rows which leads to the 
condensed representation as shown in Table 2. The 
neighbor pairs have been grouped in such a way 
that the resulting clusters correspond to classes that 
can be linguistically interpreted as nouns, adjec-
tives, and verbs. As desired, all unambiguous words 
have been assigned to only a single cluster, and the 
ambiguous word link has been assigned to the two 
appropriate clusters. 
Although it is not obvious from our example, 
there is a drawback of this approach. The disad-
vantage is that by avoiding the ambiguity problem 
for words we introduce it for the neighbor pairs, 
i.e. ambiguities concerning neighbor pairs are not 
resolved. Consider, for example, the neighbor pair 
?then ... comes?, where the middle word can either 
be a personal pronoun like he or a proper noun like 
John. However, we believe that this is a problem 
that for several reasons is of less importance: 
Firstly, we are not explicitly interested in the am-
biguities of neighbor pairs. Secondly, the ambigui-
ties of neighbor pairs seem less frequent and less 
systematic than those of words (an example is the 
omnipresent noun/verb ambiguity in English), and 
therefore the risk of misclusterings is lower. 
Thirdly, this problem can be reduced by consider-
ing longer contexts which tend to be less ambigu-
ous. That is, by choosing an appropriate context 
width a reasonable tradeoff between data sparse-
ness and ambiguity reduction can be chosen.  
 
 car cup discuss link quick seek tall thin 
a ... has         
a ... is         
a ... man         
a ... woman         
the ... has         
the ... is         
the ... man         
the ... woman         
to ... a         
to ... the         
you ... a         
you ... the         
 
Table 1: Matrix of neighbor pairs and their corresponding middle words. 
 
 car cup discuss link quick seek tall thin 
a ... has, a ... is,  
the ... has, the ... is         
a ... man, a ... woman,  
the ... man, the ... woman         
to ... a, to ... the, you ... a,  
you ... the         
 
Table 2: Clusters of neighbor pairs. 
 
3 Implementation 
Our computations are based on the 100 million 
word British National Corpus. As the number of 
word types and neighbor pairs is prohibitively 
high in a corpus of this size, we considered only a 
selected vocabulary, as described in section 4. 
From all neighbor pairs we chose the top 2000 
which had the highest co-occurrence frequency 
with the union of all words in the vocabulary and 
did not contain punctuation marks. 
By searching through the full corpus, we constructed 
a matrix as exemplified in Table 1. However, as a large 
corpus may contain errors and idiosyncrasies, the ma-
trix cells were not filled with binary yes/no decisions, 
but with the frequency of a word type occurring as the 
middle word of the respective neighbor pair. Note that 
we used raw co-occurrence frequencies and did not 
apply any association measure. However, to account 
for the large variation in word frequency and to give an 
equal chance to each word in the subsequent com-
putations, the matrix columns were normalized.  
54
As our method for grouping the rows we used 
K-means clustering with the cosine coefficient as 
our similarity measure. The clustering algorithm 
was started using random initialization. In order to 
be able to easily compare the clustering results 
with expectation, the number of clusters was spe-
cified to correspond to the number of expected 
word classes. 
After the clustering has been completed, to ob-
tain their centroids, in analogy to Table 2 the col-
umn vectors for each cluster are summed up. The 
centroid values for each word can now be inter-
preted as evidence of this word belonging to the 
class described by the respective cluster. For ex-
ample, if we obtained three clusters corresponding 
to nouns, verbs, and adjectives, and if the corre-
sponding centroid values for e.g. the word link 
would be 0.7, 0.3, and 0.0, this could be inter-
preted such that in 70% of its corpus occurrences 
link has the function of a noun, in 30% of the 
cases it appears as a verb, and that it never occurs 
as an adjective. Note that the centroid values for a 
particular word will always add up to 1 since, as 
mentioned above, the column vectors have been 
normalized beforehand. 
As elaborated in Rapp (2007), another useful 
application of the centroid vectors is that they al-
low us to judge the quality of the neighbor pairs 
with respect to their selectivity regarding a parti-
cular word class. If the row vector of a neighbor 
pair is very similar to the centroid of its cluster, 
then it can be assumed that this neighbor pair only 
accepts middle words of the correct class, whereas 
neighbor pairs with lower similarity to the cen-
troid are probably less selective, i.e. they occa-
sionally allow for words from other clusters.  
4 Results 
As our test vocabulary we chose a sample of 50 
words taken from a previous study (Rapp, 2005). 
The list of words is included in Table 3 (columns 
1 and 8). Columns 2 to 4 and 9 to 11 of Table 3 
show the centroid values corresponding to each 
word after the procedure described in the previous 
section has been conducted, that is, the 2000 most 
frequent neighbor pairs of the 50 words were clus-
tered into three groups. For clarity, all values were 
multiplied by 1000 and rounded. 
To facilitate reference, instead of naming each 
cluster by a number or by specifying the corre-
sponding list of neighbor pairs (as done in Table 2), we 
manually selected linguistically motivated names, namely 
noun, verb, and adjective.  
If we look at Table 3, we find that some words, such 
as encourage, imagine, and option, have one value 
close to 1000, with the other two values in the one 
digit range. This is a typical pattern for unambiguous 
words that belong to only one word class. However, 
perhaps unexpectedly, the majority of words has val-
ues in the upper two digit or three digit range in two or 
even three columns. This means that according to our 
system most words seem to be ambiguous in one or 
another way. For example, the word brief, although in 
the majority of cases clearly an adjective in the sense 
of short, can occasionally also occur as a noun (in the 
sense of document) or a verb (in the sense of to instruct 
somebody). In other cases, the occurrences of different 
parts of speech are more balanced. An example is the 
verb to strike versus the noun the strike. 
According to our judgment, the results for all words 
seem roughly plausible. Only the values for rain as a 
noun versus a verb seemed on first glance counterintui-
tive, but can be explained by the fact that for semantic 
reasons the verb rain usually only occurs in third per-
son singular, i.e. in its inflected form rains. 
To provide a more objective measure for the quality 
of the results, columns 5 to 7 and 12 to 14 of Table 3 
show the occurrence frequencies of the 50 words as 
nouns, verbs, and adjectives in the manually POS-
tagged Brown corpus, which is probably almost error 
free (Ku?era, & Francis, 1967). The respective tags in 
the Brown-tagset are NN, VB, and JJ.  
Generally, the POS-distributions of the Brown cor-
pus show a similar pattern as the automatically gener-
ated ones. For example, for drop the ratios of the 
automatically generated numbers 334 / 643 / 24 are 
similar to those of the pattern from the Brown corpus 
which is 24 / 34 / 1. Overall, for 48 of the 50 words the 
outcome with regard to the most likely POS is identi-
cal, with the two exceptions being the ambiguous 
words finance and suit. Although even in these cases 
the correct two parts of speech obtain the emphasis, the 
distribution of the weighting among them is somewhat 
different. 
5 Summary and Future Work 
A statistical approach has been presented which clus-
ters contextual features (neighbor pairs) as observed in 
a large text corpus and derives syntactically oriented 
word classes from the clusters. In addition, for each 
55
word a probability of its occurrence as a member 
of each of the classes is computed.  
Of course, many questions are yet to be ex-
plored, among them the following: Can a singular 
value decomposition (to be in effect only tempo-
rarily for the purpose of clustering) reduce the 
problem of data sparseness? Can biclustering (also 
referred to as co-clustering or two-mode cluster-
ing, i.e. the simultaneous clustering of the rows and 
columns of a matrix) improve results? Does the ap-
proach scale to larger vocabularies? Can it be extended 
to word sense induction by looking at longer distance 
equivalents to middle words and neighbor pairs (which 
could be homographs and pairs of words strongly as-
sociated to them)? All these are strands of research that 
we look forward to explore. 
 
 
 Simulation Brown Corpus  Simulation Brown Corpus 
 Noun Verb Adj. NN VB JJ  Noun Verb Adj. NN VB JJ 
accident 978 8 15 33 0 0 lunch 741 198 60 32 1 0 
belief 972 17 11 64 0 0 maintain 4 993 3 0 60 0 
birth 968 15 18 47 0 0 occur 15 973 13 0 43 0 
breath 946 21 33 51 0 0 option 984 10 7 5 0 0 
brief 132 50 819 8 0 63 pleasure 931 16 54 60 1 0 
broad 59 7 934 0 0 82 protect 4 995 1 0 34 0 
busy 22 22 956 0 1 56 prove 5 989 6 0 53 0 
catch 71 920 9 3 39 0 quick 47 14 938 1 0 58 
critical 51 13 936 0 0 57 rain 881 64 56 66 2 0 
cup 957 23 21 43 1 0 reform 756 221 23 23 3 0 
dangerous 37 29 934 0 0 46 rural 66 13 921 0 0 46 
discuss 3 991 5 0 28 0 screen 842 126 32 42 5 0 
drop 334 643 24 24 34 1 seek 8 955 37 0 69 0 
drug 944 10 46 20 0 0 serve 20 958 22 0 107 0 
empty 48 187 765 0 0 64 slow 43 141 816 0 8 48 
encourage 7 990 3 0 46 0 spring 792 130 78 102 6 0 
establish 2 995 2 0 58 0 strike 544 424 32 25 22 0 
expensive 55 14 931 0 0 44 suit 200 789 11 40 8 0 
familiar 42 17 941 0 0 72 surprise 818 141 41 44 5 3 
finance 483 473 44 9 18 0 tape 868 109 23 31 0 0 
grow 15 973 12 0 61 0 thank 14 983 3 0 35 0 
imagine 4 993 4 0 61 0 thin 32 58 912 0 2 90 
introduction 989 0 11 28 0 0 tiny 27 1 971 0 0 49 
link 667 311 23 12 4 0 wide 9 4 988 0 0 115 
lovely 41 7 952 0 0 44 wild 220 6 774 0 0 51 
 
Table 3: List of 50 words and their values (scaled by 1000) from each of the three cluster centroids. For 
comparison, POS frequencies from the manually tagged Brown corpus are given.  
 
Acknowledgments  
This research was supported by a Marie Curie 
Intra-European Fellowship within the 6th Frame-
work Programme of the European Community.  
References  
Clark, Alexander (2003). Combining distributional 
and morphological information for part of speech 
induction. Proceedings of 10th EACL Conference, 
Budapest, 59?66. 
Finch, Steven (1993). Finding Structure in Language. 
PhD Thesis, University of Edinburgh. 
Freitag, Dayne (2004). Toward unsupervised whole-
corpus tagging. Proc. of 20th COLING, Geneva. 
Ku?era, Henry; Francis, W. Nelson (1967). Compu-
tational Analysis of Present-Day American Eng-
lish. Providence, Rhode Island: Brown University 
Press. 
Mintz, Toben H. (2003). Frequent frames as a cue for 
grammatical categories in child directed speech. 
Cognition, 90, 91?117. 
Rapp, Reinhard (2005). A practical solution to the 
problem of automatic part-of-speech induction 
from text. Proceedings of the 43rd ACL Confer-
ence, Companion Volume, Ann Arbor, MI, 77?80. 
Rapp, Reinhard (2007). Part-of-speech discovery by 
clustering contextual features. In: Reinhold Decker 
and Hans-J. Lenz (eds.): Advances in Data Analy-
sis. Proceedings of the 30th Conference of the Ge-
sellschaft f?r Klassifikation. Heidelberg: Springer,  
627?634.  
Sch?tze, Hinrich (1993). Part-of-speech induction from 
scratch. Proceedings of the 31st ACL Conference, 
Columbus, Ohio, 251?258. 
56
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 133?136,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
The Back-translation Score: Automatic MT Evaluation at the Sentence Level without Reference Translations 
Reinhard Rapp Universitat Rovira i Virgili Avinguda Catalunya, 35 43002 Tarragona, Spain 
reinhard.rapp@urv.cat 
 
Abstract 
Automatic tools for machine translation (MT) evaluation such as BLEU are well established, but have the drawbacks that they do not per-form well at the sentence level and that they presuppose manually translated reference texts. Assuming that the MT system to be evaluated can deal with both directions of a language pair, in this research we suggest to conduct automatic MT evaluation by determining the orthographic similarity between a back-trans-lation and the original source text. This way we eliminate the need for human translated reference texts. By correlating BLEU and back-translation scores with human judg-ments, it could be shown that the back-translation score gives an improved perfor-mance at the sentence level. 
1 Introduction 
The manual evaluation of the results of machine translation systems requires considerable time and effort. For this reason fast and inexpensive automatic methods were developed. They are based on the comparison of a machine translation with a reference translation produced by humans. The comparison is done by determining the num-ber of matching word sequences between both translations. It could be shown that such meth-ods, of which BLEU (Papineni et al, 2002) is the most common, can deliver evaluation results that show a high agreement with human judgments (Papineni et al, 2002; Coughlin, 2003; Koehn & Monz, 2006).  Disadvantages of BLEU and related methods are that a human reference translation is required, and that the results are reliable only at corpus level, i.e. when computed over many sentence pairs (see e.g. Callison-Burch et al, 2006). How-ever, at the sentence level, due to data sparseness the results tend to be unsatisfactory (Agarwal & Lavie, 2008; Callison-Burch et al, 2008). Pap-ineni et al (2002) describe this as follows: 
?BLEU?s strength is that it correlates highly with human judgments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.?  Although in many scenarios the above men-tioned drawbacks may not be a major problem, it is nevertheless desirable to overcome them. This is what we attempt in this paper by introducing the back-translation score. It is based on the as-sumption that the MT system considered can translate a language pair in both directions, which is usually the case. Evaluating the quality of a machine translation now involves translating it back to the source language. The score is then computed by comparing the back-translation to the original source text. Although for this com-parison BLEU could be used, our experiments show that a modified version which we call Or-thoBLEU is better suited for this purpose as it can deal with compounds and inflexional vari-ants in a more appropriate way. Its operation is based on finding matches of character- rather than word-sequences. It resembles algorithms used in translation memory search for locating orthographically similar sentences. The results that we obtain in this work refute to some extend the common belief that back-translation (sometimes also called round-trip translation) is not a suitable means for MT evaluation (Somers, 2005; Koehn, 2005). This belief seems to be largely based on the obvious observation that the back-translation score is highest for a trivial translation system that does nothing and simply leaves all source words in place. On the other hand, according to Somers (2005) ?until now no one as far as we know has published results demonstrating this? (i.e. that back-translation is not useful for MT evaluation).  We would like to add that so far the inappro-priateness of back-translation has only been shown by comparisons with other automatic met-rics (Somers 2005; Koehn, 2005), which are also 
133
flawed. Somers (2005) therefore states: ?To be really sure of our results, we should like to repli-cate the experiments evaluating the translations using a more old-fashioned method involving human ratings of intelligibility.? That is, appar-ently nobody has ever seriously compared back-translation scores to human judgments, so the belief about their inutility seems not sufficiently backed by facts. This is a serious deficit which we try to overcome in this work. 
2 Procedure 
As our test corpus we use the first 100 English and German sentences of the News Corpus which was kindly provided by the organizers of the Third Workshop on Statistical Machine Translation (Callison-Burch et al, 2008). This corpus comprises human translations of articles from various news websites. In the case of the 100 sentences used here, the source language was Hungarian and the translations to English and German were produced from the Hungarian original. As MT evaluation is often based on multilingual corpora, the use of indirect transla-tions appears to be a realistic scenario. The 100 English sentences were translated to German using the online MT-system Babel Fish (http://de.babelfish.yahoo.com/) which is based on Systran technology. Subsequently, the translations were back-translated to English. Table 1 shows a sample sentence and its trans-lations.  English (source) The skyward zoom in food prices is the dominant force behind the speed up in eurozone inflation. German (human translation) 
Hauptgrund f?r den in der Eurozone ge-messenen Anstieg der Inflation seien die rasant steigenden Lebensmittelpreise. German (Babel Fish) 
Die gen Himmel Lebensmittelpreise laut summen innen ist die dominierende Kraft hinter beschleunigen in der Euro-zoneinflation. English (back-translation) 
Towards skies the food prices loud hum inside are dominating Kraft behind accel-erate in the euro zone inflation.  Table 1: Sample sentence, its human translation, and its Babel Fish forward and backward translations.  The Babel Fish translations to German were judged by the author according to the standard criteria of fluency and adequacy. Hereby the scale provided by Koehn & Monz (2006) was used which assigns values between 1 and 5. We then for each sentence computed the mean of its fluency and adequacy values. This somewhat arbitrary measure serves the purposes of desig-nating each sentence a single value, which makes 
the subsequent comparisons with automatic eval-uations easier. Having completed the human judgments, we next computed automatic judgments using the standard BLEU score. For this purpose we used the latest version (v12) of the NIST tool, which can be freely downloaded from the website http://www.nist.gov/speech/tests/mt/. This tool not only computes the BLEU score, but also a slightly modified variant, the so-called NIST score. Whereas the BLEU score assigns equal weights to all word sequences, the NIST score tries to take a sequence?s information con-tent into account by giving less frequent word sequences higher weights. In addition, the so-called brevity penalty, which tries to penalize too short translations, is computed somewhat differ-ently, with the effect that small length differ-ences have less impact on the overall score.  Using the NIST tool, the BLEU and NIST scores for all 100 translated sentences where computed. Hereby, the human translations were taken as reference. In addition, the BLEU and NIST scores were also computed for the back-translations, thereby using the source sentences as reference. By doing so we must emphasize that, as de-scribed in the previous section, the BLEU score was not designed to deliver satisfactory results at the sentence level (Papineni et al, 2002), and this also applies to the closely related NIST score. On the other hand, there are no simple automatic evaluation tools that are suitable at the sentence level. Only the METEOR-System (Agarwal & Lavie, 2008) is a step in this direc-tion. It takes into account inflexional variants and synonyms. However, it is considerably more so-phisticated and is highly dependent on the under-lying large scale linguistic resources. We also think that ? irrespectively of their de-sign goals ? the performance of the established BLEU and NIST scores at the sentence level is of some interest, especially as to our knowledge no other quantitative figures have been published so far. For the current work, as improved evalu-ation at the sentence level is one of the goals, this appears to be the only possibility to at all provide some baseline for a comparison using a well es-tablished automatic system. In an attempt to reduce the concerns that arise from applying BLEU at the sentence level, we introduce OrthoBLEU. Like BLEU OrthoBLEU also compares a machine translation to a refer-ence translation. However, instead of word se-quences sequences of characters are considered, as proposed by Denoual & Lepage (2005). The OrthoBLEU score between two strings is com-
134
puted as the (relative) number of their matching triplets of characters (trigrams). Figure 1 illustra-tes this using the words pineapple and apple pie. As 6 out of 11 trigrams match, the resulting Or-thoBLEU score is 54.5%. The procedure illustrated in Figure 1 is not only applicable to words, but likewise to sen-tences, as punctuation marks, blanks, and special symbols can be treated like any other character. It is obvious that this procedure, which was originally developed for the purpose of fuzzy information retrieval, shows some tolerance with regard to inflexional variants, compounding, and derivations, which should be advantageous in the current setting. The source code of OrthoBLEU was written in C and can be freely downloaded from the following URL: http://www.fask. uni-mainz.de/user/rapp/comtrans/.  Using the OrthoBLEU algorithm, the evalu-ations previously conducted with the NIST tool were repeated. That is, both the Babel Fish trans-lations as well as their back-translations were evaluated, whereby in the first case the human translations and in the second case the source sentences served as references.   
  Figure 1: Computation of the OrthoBLEU score. 
3 Results  
Table 2 gives the average results of the evalua-tions described in the previous section. In col-umns 1 and 2 we find the human evaluation scores for fluency and adequacy, and column 3 combines them to a single score by computing their arithmetic mean. Columns 4 and 5 show the NIST and BLEU scores as computed using the NIST tool. They are based on the Babel Fish translations from English to German, whereby the human translations served as the reference. Column 6 shows the corresponding score based on OrthoBLEU, which delivers values in a range between 0% and 100%. Columns 7 to 9 show 
analogous scores for the back-translations. In this case the English source sentences served as the reference. As can be seen from the table, the val-ues are higher for the back-translations. How-ever, it would be premature to interpret this ob-servation such that the back-translations are bet-ter suited for evaluation purposes. As these are very different tasks with different statistical pro-perties, it would be methodologically incorrect to simply compare the absolute values. Instead we need to compute correlations between automatic and human scores. This we did by correlating all NIST-, BLEU-, and OrthoBLEU scores for all 100 sentences with the corresponding (mean fluency/adequacy) scores from the human evaluation. We computed the Pearson product-moment correlation coeffi-cient for all pairs, with the results being shown in Table 3. Hereby a coefficient of +1 indicates a direct linear relation, a coefficient of -1 indicates an inverse linear relation, and a coefficient of 0 indicates no linear relation. When looking at the ?translation? section of Table 3, as to be expected we obtain very low correlation coefficients for the BLEU and the NIST scores. This confirms their unsuitability for application at the sentence level as expected (see section 1). For the OrthoBLEU score we also get a very low correlation coefficient of 0.075, which means that OrthoBLEU is also unsuitable for evaluation of direct translations at the sen-tence level.  However, when we look at the back-translation section of Table 3, the situation is somewhat different. The correlation coefficient for the NIST score is still slightly negative, indi-cating that trying to take a word sequence?s in-formation content into account is hopeless at the sentence level. However, the correlation coeffi-cient for the BLEU score almost doubles from 0.078 to 0.133, which, however, is still unsatis-factory. But a surprise comes with the Or-thoBLEU score: It more than quadruples from 0.075 to 0.327, which at the sentence level is a rather good value as this result comes close to the correlation coefficient of 0.403 reported by Agarwal & Lavie (2008) as the very best of sev-eral values obtained for the METEOR system. Remember that, as described in section 2, the METEOR system requires a human-generated ref-  
HUMAN EVALUATION AUTOMATIC EVALUATION OF FORWARD-TRANSLATION AUTOMATIC EVALUATION OF BACK-TRANSLATION FLU-
ENCY  ADE-QUACY MEAN NIST BLEU ORTHO-BLEU NIST  BLEU  ORTHO-BLEU  2,49 3,06 2,78 1,31 0,01 39,72% 2,90 0,25 68,94%  Table 2: Average BLEU, NIST and OrthoBLEU scores for the 100 test sentences. 
135
Human evaluation ? NIST -0,169 Human evaluation ? BLEU 0,078 Trans-lation Human evaluation ? OrthoBLEU 0,075 Human evaluation ? NIST -0,102 Human evaluation ? BLEU 0,133 Back-trans-lation Human evaluation ? OrthoBLEU 0,327  Table 3: Correlation coefficients between human and various automatic judgments based on 100 test sen-tences.  erence translation, large linguistic resources and comparatively sophisticated processing, and that all of this is unnecessary for the back-translation score. 
4 Discussion and prospects  
The motivation for this paper resulted from ob-serving a contradiction: On one hand, practi-tioners sometimes recommend that (if one does not understand the target language) a back-translation can give some idea of the translation quality. Our impression has always been that this is obviously true for standard commercial sys-tems. On the other hand, serious scientific publi-cations (Somers, 2005; Koehn, 2005) come to the conclusion that back-translation is com-pletely unsuitable for MT evaluation. The outcome of the current work is in favor of the first point of view, but we should emphasize that we have no doubt about the correctness of the results presented in the publications. The dis-crepancy is likely to result from the following: 
? The previous publications did not compare back-translation scores to human judgments but to BLEU scores only. 
? The introduction of OrthoBLEU improved back-translation scores significantly. 
What remains is the fact that evaluation based on back-translations can be easily fooled, e.g. by a system that does nothing, or that is capable of reversing errors. These obvious deficits have probably motivated reservations against such systems, and we agree that for such reasons they may be unsuitable for use at MT competitions.1 However, there are numerous other applications where such considerations are of less import-
                                                 1 Although there might be a solution to this: It may not always be necessary that forward and backward translations are generated by the same MT system. For example, in an MT competition back-translations could be generated by all competing systems, and the resulting scores could be averaged. 
ance. Also, it might be possible to introduce a penalty for trivial forms of translation, e.g. by counting the number of word sequences (e.g. of length 1 to 4) in a translation that are not found in a corpus of the target language.2   Acknowledgments 
This research was in part supported by a Marie Curie Intra European Fellowship within the 7th European Community Framework Programme. We would also like to thank the anonymous re-viewers for their comments, the providers of the NIST MT evaluation tool, and the organizers of the Third Workshop on Statistical MT for making available the News Corpus. 
References  
Abhaya Agarwal, Alon Lavie. 2008. Meteor, m-bleu and m-ter: Evaluation metrics for high-correlation with human rankings of machine translation out-put. Proc. of the 3rd Workshop on Statistical MT, Columbus, Ohio, 115?118.  Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Josh Schroeder. 2008. Further meta-evaluation of machine translation. Proc. of the 3rd Workshop on Statistical MT, Columbus, 70?106.  Chris Callison-Burch, Miles Osborne, Philipp Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. Proc. of 11th EACL, 249?256.  Deborah Coughlin. 2003. Correlating automated and human assessments of machine translation quality. Proc. of MT Summit IX, New Orleans, 23?27.  Etienne Denoual, Yves Lepage. 2005. BLEU in char-acters: towards automatic MT evaluation in lan-guages without word delimiters. Proc. of 2nd IJCNLP, Companion Volume, 81?86. Philipp Koehn. 2005. Europarl: A parallel corpus for evaluation of machine translation. Proceedings of the 10th MT Summit, Phuket, Thailand, 79?86.  Philipp Koehn, Christof Monz. 2006. Manual and automatic evaluation of machine translation be-tween European languages. Proc. of the Workshop on Statistical MT, New York, 102?121.  Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. Proc. of the 40th Annual Meeting of the ACL, 311?318.  Harold Somers. 2005. Round-trip translation: what is it good for? In Proceedings of the Australasian Language Technology Workshop ALTW 2005. Sydney, Australia. 127?133. 
                                                 2 Looking up single words would not be sufficient as a system establishing any unambiguous 1:1 relationship between the source and the target language vocabu-lary would obtain top scores. 
136
The Computation of Word Associations: 
Comparing Syntagmatic and Paradigmatic Approaches 
Reinhard Rapp 
University of Mainz, FASK 
D-76711 Germersheim, Germany 
rapp@mail.fask.uni-mainz.de 
 
Abstract 
It is shown that basic language processes such 
as the production of free word associations and 
the generation of synonyms can be simulated 
using statistical models that analyze the distri-
bution of words in large text corpora. Accord-
ing to the law of association by contiguity, the 
acquisition of word associations can be ex-
plained by Hebbian learning. The free word as-
sociations as produced by subjects on presenta-
tion of single stimulus words can thus be pre-
dicted by applying first-order statistics to the 
frequencies of word co-occurrences as observed 
in texts. The generation of synonyms can also 
be conducted on co-occurrence data but re-
quires second-order statistics. The reason is that 
synonyms rarely occur together but appear in 
similar lexical neighborhoods. Both approaches 
are systematically compared and are validated 
on empirical data. It turns out that for both 
tasks the performance of the statistical system is 
comparable to the performance of human sub-
jects. 
1 Introduction 
According to Ferdinand de Saussure (1916), there 
are two fundamental types of relations between 
words that he believes correspond to basic opera-
tions of our brain: syntagmatic and paradigmatic 
associations. There is a syntagmatic relation be-
tween two words if they co-occur in spoken or 
written language more frequently than expected 
from chance and if they have different grammatical 
roles in the sentences in which they occur. Typical 
examples are the word pairs coffee ? drink, sun ? 
hot, or teacher ? school. The relation between two 
words is paradigmatic if the two words can sub-
stitute for one another in a sentence without affect-
ing the grammaticality or acceptability of the sen-
tence. Typical examples are synonyms or antonyms 
like quick ? fast, or eat ? drink. Normally, words 
with a paradigmatic relation are the same part of 
speech, whereas words with a syntagmatic relation 
can but need not be the same part of speech. 
In this paper we want to show that the two types 
of relations as defined by de Saussure are reflected 
in the statistical distribution of words in large cor-
pora. We present algorithms that automatically 
retrieve words with either the syntagmatic or the 
paradigmatic type of relationship from corpora and 
perform a quantitative evaluation of our results. 
2 Paradigmatic Associations 
Paradigmatic associations are words with high se-
mantic similarity. According to Ruge (1992), the 
semantic similarity of two words can be computed 
by determining the agreement of their lexical 
neighborhoods. For example, the semantic similarity 
of the words red and blue can be derived from the 
fact that they both frequently co-occur with words 
like color, flower, dress, car, dark, bright, beauti-
ful, and so forth. If for each word in a corpus a co-
occurrence vector is determined whose entries are 
the co-occurrences with all other words in the cor-
pus, then the semantic similarities between words 
can be computed by conducting simple vector 
comparisons. To determine the words most similar 
to a given word, its co-occurrence vector is com-
pared to the co-occurrence vectors of all other 
words using one of the standard similarity measures, 
for example, the cosine coefficient. Those words 
that obtain the best values are considered to be most 
similar. Practical implementations of algorithms 
based on this principle have led to excellent results 
as documented in papers by Ruge (1992), Grefen-
stette (1994), Agarwal (1995), Landauer & Dumais 
(1997), Sch?tze (1997), and Lin (1998). 
2.1 Human Data 
In this section we relate the results of our version of 
such an algorithm to similarity estimates obtained 
by human subjects. Fortunately, we did not need to 
conduct our own experiment to obtain the human?s 
similarity estimates. Instead, such data was kindly 
provided by Thomas K. Landauer, who had taken it 
from the synonym portion of the Test of English as 
a Foreign Language (TOEFL). Originally, the data 
came, along with normative data, from the Educa-
tional Testing Service (Landauer & Dumais 1997). 
The TOEFL is an obligatory test for foreign stu-
dents who would like to study at an American or 
English university. 
The data comprises 80 test items. Each item 
consists of a problem word in testing parlance and 
four alternative words, from which the test taker is 
asked to choose that with the most similar meaning 
to the problem word. For example, given the test 
sentence ?Both boats and trains are used for 
transporting the materials? and the four alternative 
words planes, ships, canoes, and railroads, the 
subject would be expected to choose the word ships, 
which is the one most similar to boats. 
2.2 Corpus 
As mentioned above, our method of simulating this 
kind of behavior is based on regularities in the sta-
tistical distribution of words in a corpus. We chose 
to use the British National Corpus (BNC), a 100-
million-word corpus of written and spoken language 
that was compiled with the intention of providing a 
representative sample of British English. 
Since this corpus is rather large, to save disk 
space and processing time we decided to remove all 
function words from the text. This was done on the 
basis of a list of approximately 200 English function 
words. We also decided to lemmatize the corpus as 
well as the test data. This not only reduces the 
sparse-data problem but also significantly reduces 
the size of the co-occurrence matrix to be computed. 
More details on these two steps of corpus pre-
processing can be found in Rapp (1999).  
2.3 Co-occurrence Counting 
For counting word co-occurrences, as in most other 
studies a fixed window size is chosen and it is de-
termined how often each pair of words occurs 
within a text window of this size. Choosing a win-
dow size usually means a trade-off between two 
parameters: specificity versus the sparse-data prob-
lem. The smaller the window, the stronger the asso-
ciative relation between the words inside the win-
dow, but the more severe the sparse data problem 
(see figure 1 in section 3.2). In our case, with ?1 
word, the window size looks rather small. However, 
this can be justified since we have reduced the ef-
fects of the sparse-data problem by using a large 
corpus and by lemmatizing the corpus. It also 
should be noted that a window size of ?1 applied 
after elimination of the function words is compa-
rable to a window size of ?2 without elimination of 
the function words (assuming that roughly every 
second word is a function word). 
Based on the window size of ?1, we computed a 
co-occurrence matrix of about a million words in 
the lemmatized BNC. Although the resulting matrix 
is extremely large, this was feasible since we used a 
sparse format that does not store zero entries.  
2.4 Computation of Word Similarities 
To determine the words most similar to a given 
word, the co-occurrence vector of this word is com-
pared to all other vectors in the matrix and the 
words are ranked according to the similarity values 
obtained. It is expected that the most similar words 
are ranked first in the sorted list. 
For vector comparison, different similarity 
measures can be considered. Salton & McGill 
(1983) proposed a number of measures, such as the 
cosine coefficient, the Jaccard coefficient, and the 
Dice coefficient. For the computation of related 
terms and synonyms, Ruge (1995) and Landauer & 
Dumais (1997) used the cosine measure, whereas 
Grefenstette (1994, p. 48) used a weighted Jaccard 
measure. We propose here the city-block metric, 
which computes the similarity between two vectors 
X and Y as the sum of the absolute differences of 
corresponding vector positions: 
?
=
?=
n
i
ii YXs
1
 
In a number of experiments we compared it to other 
similarity measures, such as the cosine measure, the 
Jaccard measure (standard and binary), the Euclid-
ean distance, and the scalar product, and found that 
the city-block metric yielded good results (see Rapp, 
1999). 
2.5 Results 
Table 1 shows the top five paradigmatic associa-
tions to six stimulus words. As can be seen from the 
table, nearly all words listed are of the same part of 
speech as the stimulus word. Of course, our defini-
tion of the term paradigmatic association as given 
in the introduction implies this. However, the simu-
lation system never obtained any information on 
part of speech, and so it is nevertheless surprising 
that ? besides computing term similarities ? it im-
plicitly seems to be able to cluster parts of speech. 
This observation is consistent with other studies 
(e.g., Ruge, 1995). 
 
blue cold fruit green tobacco whiskey 
red hot food red cigarette whisky 
green warm flower blue alcohol brandy 
grey dry fish white coal champagne 
yellow drink meat yellow import lemonade 
white cool vegetable grey textile vodka 
 
Table 1: Computed paradigmatic associations. 
 
A qualitative inspection of the word lists generated 
by the system shows that the results are quite 
satisfactory. Paradigmatic associations like blue 
 
 
red, cold 
 
 hot, and tobacco 
 
 cigarette are 
intuitively plausible. However, a quantitative 
evaluation would be preferable, of course, and for 
this reason we did a comparison with the results of 
the human subjects in the TOEFL test. Remember 
that the human subjects had to choose the word 
most similar to a given stimulus word from a list of 
four alternatives.  
In the simulation, we assumed that the system 
had chosen the correct alternative if the correct word 
was ranked highest among the four alternatives. 
This was the case for 55 of the 80 test items, which 
gives us an accuracy of 69%. This accuracy may 
seem low, but it should be taken into account that 
the TOEFL tests the language abilities of prospec-
tive university students and therefore is rather diffi-
cult. Actually, the performance of the average hu-
man test taker was worse than the performance of 
the system. The human subjects were only able to 
solve 51.6 of the test items correctly, which gives an 
accuracy of 64.5%. Please note that in the TOEFL, 
average performance (over several types of tests, 
with the synonym test being just one of them) ad-
mits students to most universities. On the other 
hand, by definition, the test takers did not have a 
native command of English, so the performance of 
native speakers would be expected to be signifi-
cantly better. Another consideration is the fact that 
our simulation program was not designed to make 
use of the context of the test word, so it neglected 
some information that may have been useful for the 
human subjects.  
Nevertheless, the results look encouraging. 
Given that our method is rather simple, let us now 
compare our results to the results obtained with 
more sophisticated methods. One of the methods 
reported in the literature is singular value decompo-
sition (SVD); another is shallow parsing. SVD, as 
described by Sch?tze (1997) and Landauer & Du-
mais (1997), is a method similar to factor analysis 
or multi-dimensional scaling that allows a signifi-
cant reduction of the dimensionality of a matrix with 
minimum information loss. Landauer & Dumais 
(1997) claim that by optimizing the dimensionality 
of the target matrix the performance of their word 
similarity predictions was significantly improved.  
However, on the TOEFL task mentioned above, 
after empirically determining the optimal dimen-
sionality of their matrix, they report an accuracy of 
64.4%. This is somewhat worse than our result of 
69%, which was achieved without SVD and without 
optimizing any parameters. It must be emphasized, 
however, that the validity of this comparison is 
questionable, as many parameters of the two models 
are different, making it unclear which ones are re-
sponsible for the difference. For example, Landauer 
and Dumais used a smaller corpus (4.7 million 
words), a larger window size (151 words on aver-
age), and a different similarity measure (cosine 
measure). We nevertheless tend to interpret the 
results of our comparison as evidence for the view 
that SVD is just another method for smoothing that 
has its greatest benefits for sparse data. However, 
we do not deny the technical value of the method. 
The one-time effort of the dimensionality reduction 
may be well spent in a practical system because all 
subsequent vector comparisons will be speeded up 
considerably with shorter vectors.  
Let us now compare our results to those ob-
tained using shallow parsing, as previously done by 
Grefenstette (1993). The view here is that the win-
dow-based method may work to some extent, but 
that many of the word co-occurrences in a window 
are just incidental and add noise to the significant 
word pairs. A simple method to reduce this problem 
could be to introduce a threshold for the minimum 
number of co-occurrences; a more sophisticated 
method is the use of a (shallow) parser. Ruge 
(1992), who was the first to introduce this method, 
claims that only head-modifier relations, as known 
from dependency grammar, should be considered. 
For example, if we consider the sentence ?Peter 
drives the blue car?, then we should not count the 
co-occurrence of Peter and blue, because blue is 
neither head nor modifier of Peter. Ruge developed 
a shallow parser that is able to determine the head-
modifier relations in unrestricted English text with a 
recall of 85% and a precision of 86% (Ruge, 1995). 
Using this parser she extracted all head-modifier 
relations from the 100 million words of the British 
National Corpus. Thus, the resulting co-occurrence 
matrix only contained the counts of the head-modi-
fier relations. The word similarities were computed 
from this matrix by using the cosine similarity 
measure. Using this method, Ruge achieved an 
accuracy of about 69% in the TOEFL synonym 
task, which is equivalent to our results.  
Again, we need to emphasize that parameters 
other than the basic methodology could have influ-
enced the result, so we need to be cautious with an 
interpretation. However, to us it seems that the view 
that some of the co-occurrences in corpora should 
be considered as noise is wrong, or else if there is 
some noise it obviously cancels out over large cor-
pora. It would be interesting to know how a system 
performed that used all co-occurrences except the 
head-modifier relations. We tend to assume that 
such a system would perform worse, so the parser 
selected the good candidates. However, the experi-
ment has not been done, so we cannot be sure.  
Although the shallow parsing could not improve 
the results in this case, we nevertheless should point 
out its virtues: It improves efficiency since it leads to 
sparser matrices. It also seems to be able to separate 
the relevant from the irrelevant co-occurrences. 
Third, it may be useful for determining the type of 
relationship between words (e.g., synonymy, an-
tonymy, meronymy, hyponymy, etc., see Berland & 
Charniak, 1999). Although this is not within the 
scope of this paper, it is very relevant for related 
tasks, for example, the automatic generation of 
thesauri. 
3 Syntagmatic Associations 
Syntagmatic associations are words that frequently 
occur together. Therefore, an obvious approach to 
extract them from corpora is to look for word pairs 
whose co-occurrence is significantly larger than 
chance. To test for significance, the standard chi-
square test can be used. However, Dunning (1993) 
pointed out that for the purpose of corpus statistics, 
where the sparseness of data is an important issue, it 
is better to use the log-likelihood ratio. It would then 
be assumed that the strongest syntagmatic associa-
tion to a word would be that other word that gets the 
highest log-likelihood score. 
Please note that this method is computationally 
far more efficient than the computation of paradig-
matic associations. For the computation of the syn-
tagmatic associations to a stimulus word only the 
vector of this single word has to be considered, 
whereas for the computation of paradigmatic asso-
ciations the vector of the stimulus word has to be 
compared to the vectors of all other words in the 
vocabulary. The computation of syntagmatic asso-
ciations is said to be of first-order type, whereas the 
computation of paradigmatic associations is of 
second-order type. Algorithms for the computation 
of first-order associations have been used in lexico-
graphy for the extraction of collocations (Smadja, 
1993) and in cognitive psychology for the simula-
tion of associative learning (Wettler & Rapp, 1993). 
3.1 Association Norms 
As we did with the paradigmatic associations, we 
would like to compare the results of our simulation 
to human performance. However, it is difficult to 
say what kind of experiment should be conducted to 
obtain human data. As with the paradigmatic asso-
ciations, we decided not to conduct our own ex-
periment but to use the Edinburgh Associative The-
saurus (EAT), a large collection of association 
norms, as compiled by Kiss et al (1973). Kiss pre-
sented lists of stimulus words to human subjects and 
asked them to write after each word the first word 
that the stimulus word made them think of. Table 2 
gives some examples of the associations the subjects 
came up with. 
As can be seen from the table, not all of the 
associations given by the subjects seem to be of syn-
tagmatic type. For example, the word pairs blue ? 
black or cold ? hot are clearly of paradigmatic type. 
This observation is of importance and will be dis-
cussed later.  
 
blue cold fruit green tobacco whiskey 
sky hot apple grass smoke drink 
black ice juice blue cigarette gin 
green warm orange red pipe bottle 
red water salad yellow poach soda 
white freeze machine field road Scotch 
 
Table 2: Some sample associations from the EAT. 
3.2 Computation 
For the computation of the syntagmatic associations 
we used the same corpus as before, namely the 
British National Corpus. In a preliminary experi-
ment we tested if there is a correlation between the 
occurrence of a stimulus word in the corpus and the 
occurrence of the most frequent associative response 
as given by the subjects. For this purpose, we se-
lected 100 stimulus/response pairs and plotted a bar 
chart from the co-occurrence data (see figure 1). In 
the bar chart, the x-axis corresponds to the distance 
of the response word from the stimulus word (meas-
ured as the number of words separating them), and 
the y-axis corresponds to the occurrence frequency 
of the response word in a particular distance from 
the stimulus word. Please note that for the purpose 
of plotting this bar chart, function words have been 
taken into account. 
 
 
 
Figure 1: Occurrence frequency H of a response word in 
a particular distance A from the corresponding stimulus 
word (averaged over 100 stimulus/response pairs). 
 
As can be seen from the figure, the closer we get to 
the stimulus word, the more likely it is that we find 
an occurrence of its strongest associative response. 
Exceptions are the positions directly neighboring the 
stimulus word. Here it is rather unlikely to find the 
response word. This observation can be explained 
by the fact that content words are most often sepa-
rated by function words, so that the neighboring 
positions are occupied by function words.  
Now that it has been shown that there is some 
relationship between human word associations and 
word co-occurrences, let us briefly introduce our 
algorithm for extracting word associations from 
texts. Based on a window size of ?20 words, we 
first compute the co-occurrence vector for a given 
stimulus word, thereby eliminating all words with a 
corpus frequency of less than 101. We then apply 
the log-likelihood test to this vector. According to 
Lawson & Belica1 the log-likelihood ratio can be 
computed as follows: Given the word W, for each 
co-occurring word S, its window frequency A, its 
residual frequency C in the reference corpus, the 
residual window size B and the residual corpus size 
D are stored in a 2 by 2 contingency table.  
 
 S ?S Total 
W A B A+B 
?W C D C+D 
Total A+C B+D N 
 
Then the log-likelihood statistics are calculated:  
 
))log()()log(
)()log()(
)log()(log
loglogloglog(2
DCDCDB
DBCACA
BABANN
DDCCBBAAG
++?+
+?++?
++?+
+++=
 
 
Finally, the vocabulary is ranked according to de-
scending values of G as computed for each word. 
The word with the highest value is considered to be 
the primary associative response. 
3.3 Results 
In table 3 a few sample association lists as predicted 
by our system are listed. They can be compared to 
the human associative responses given in table 2. 
The valuation of the predictions has to take into 
account that association norms are conglomerates of 
the answers of different subjects that differ consid-
erably from each other. A satisfactory prediction 
would be proven if the difference between the pre-
                                                   
1
 Handout at GLDV Meeting, Frankfurt/Main 1999. 
dicted and the observed responses were about equal 
to the difference between an average subject and the 
rest of the subjects. This is actually the case. For 27 
out of the 100 stimulus words the predicted re-
sponse is equal to the observed primary response. 
This compares to an average of 28 primary re-
sponses given by a subject in the EAT. Other 
evaluation measures lead to similar good results 
(Wettler & Rapp, 1993; Rapp, 1996). 
 
blue cold fruit green tobacco whiskey 
red hot vegetable red advertising drink 
eyes water juice blue smoke Jesse 
sky warm fresh yellow ban bottle 
white weather tree leaves cigarette Irish 
green winter salad colour alcohol pour 
 
Table 3: Results with the co-occurrence-based approach. 
 
We conclude from this that our method seems to be 
well suited to predict the free word associations as 
produced by humans. And as human associations 
are not only of syntagmatic but also of paradigmatic 
type, so does the co-occurrence-based method pre-
dict both types of associations rather well. In the 
ranked lists produced by the system we find a mix-
ture of both types of associations. However, for a 
given association there is no indication whether it is 
of syntagmatic or paradigmatic type. 
We suggest a simple method to distinguish the 
paradigmatic from the syntagmatic associations. 
Remember that the 2nd-order approach described in 
the previous section produced paradigmatic asso-
ciations only. So if we simply remove the words 
produced by the 2nd-order approach from the word 
lists obtained by the 1st-order approach, then this 
should give us solely syntagmatic associations.  
4 Comparison between Syntagmatic 
 and Paradigmatic Associations 
Table 4 compares the top five associations to a few 
stimulus words as produced by the 1st-order and the 
2nd-order approach. In the list, we have printed in 
bold those 1st-order associations that are not among 
the top five in the second-order lists. Further inspec-
tions of these words shows that they are all syntag-
matic associations. So the method proposed seems 
to work in principle. However, we have not yet con-
ducted a systematic quantitative evaluation. Con-
ducting a systematic evaluation is not trivial, since 
the definitions of the terms syntagmatic and para-
digmatic as given in the introduction may not be 
precise enough. Also, for a high recall, the word 
lists considered should be much longer than the top 
five. However, the further down we go in the ranked 
lists, the less typical are the associations. So it is not 
clear where to automatically set a threshold. We did 
not further elaborate on this because for our 
practical work this issue was of lesser importance. 
Although both algorithms are based on word co-
occurrences, our impression is that their strengths 
and weaknesses are rather different. So we see a 
good chance of obtaining an improved generator for 
associations by combining the two methods. 
 
stimulus 1st-order 2nd-order 
blue red red 
 eyes green 
 sky grey 
 white yellow 
 green white 
cold hot hot 
 water warm 
 warm dry 
 weather drink 
 winter cool 
fruit vegetable food 
 juice flower 
 fresh fish 
 tree meat 
 salad vegetable 
green red red 
 blue blue 
 yellow white 
 leaves yellow 
 colour grey 
tobacco advertising cigarette 
 smoke alcohol 
 ban coal 
 cigarette import 
 alcohol textile 
whiskey drink whisky 
 Jesse brandy 
 bottle champagne 
 Irish lemonade 
 pour vodka 
 
Table 4: Comparison between 1st-order and 2nd-order 
associations. 
5 Discussion and Conclusion 
We have described algorithms for the computation 
of 1st-order and 2nd-order associations. The results 
obtained have been compared with the answers of 
human subjects in the free association task and in 
the TOEFL synonym test. It could be shown that 
the performance of our system is comparable to the 
performance of the subjects for both tasks. 
We observed that there seems to be some rela-
tionship between the type of computation performed 
(1st-order versus 2nd-order) and the terms syntag-
matic and paradigmatic as coined by de Saussure. 
Whereas the results of the 2nd-order computation 
are of paradigmatic type exclusively, those of the 
1st-order computation are a mixture of both syn-
tagmatic and paradigmatic associations. Removing 
the 2nd-order associations from the 1st-order as-
sociations leads to solely syntagmatic associations.  
We believe that the observed relation between 
our statistical models and the intuitions of de Saus-
sure are not incidental, and that the striking similar-
ity of the simulation results with the human associa-
tions also has a deeper reason. Our explanation for 
this is that human associative behavior is governed 
by the law of association by contiguity, which is 
well known from psychology (Wettler, Rapp & 
Ferber, 1993). In essence, this means that in the pro-
cess of learning or generating associations the hu-
man mind seems to conduct operations that are 
equivalent to co-occurrence counting, to performing 
significance tests, or to computing vector similarities 
(see also Landauer & Dumais, 1997). However, 
further work is required to find out to what extent 
other language-related tasks can also be explained 
statistically. 
Acknowledgements 
This research was supported by the DFG. I would 
like to thank Manfred Wettler and Gerda Ruge for 
many inspiring discussions. 
References  
Agarwal, R. (1995). Semantic Feature Extraction from 
Technical Texts with Limited Human Intervention. 
Dissertation, Mississippi State University. 
Berland, M., Charniak, E. (1999). Finding Parts in Very 
Large Corpora. In: Proceedings of ACL 1999, Col-
lege Park. 57?64. 
de Saussure, F. (1916/1996). Cours de linguistique g?-
n?rale. Paris: Payot. 
Dunning, T. (1993). Accurate methods for the statistics 
of surprise and coincidence. Computational Linguis-
tics, 19(1), 61-74.  
Grefenstette, G. (1993). Evaluation techniques for 
automatic semantic extraction: comparing syntactic 
and window based approaches. In: Proceedings of the 
Workshop on Acquisition of Lexical Knowledge from 
Text, Columbus, Ohio. 
Grefenstette, G. (1994). Explorations in Automatic The-
saurus Discovery. Dordrecht: Kluwer.  
Kiss, G.R., Armstrong, C., Milroy, R., Piper, J. (1973). 
An associative thesaurus of English and its com-
puter analysis. In: A. Aitken, R. Beiley and N. 
Hamilton-Smith (eds.): The Computer and Literary 
Studies. Edinburgh: University Press. 
Landauer, T. K.; Dumais, S. T. (1997). A solution to 
Plato?s problem: the latent semantic analysis theory of 
acquisition, induction, and representation of know-
ledge. Psychological Review, 104(2), 211?240. 
Lin, D. (1998). Automatic Retrieval and Clustering of 
Similar Words. In: Proceedings of COLING-ACL 
1998, Montreal, Vol. 2, 768?773.  
Rapp, R. (1996). Die Berechnung von Assoziationen. 
Hildesheim: Olms. 
Rapp, R. (1999). Automatic identification of word 
translation from unrelated English and German cor-
pora. In: Proceedings of ACL 1999, College Park. 
519?526. 
Ruge, G. (1992). Experiments on Linguistically Based 
Term Associations. Information Processing & Ma-
nagement 28(3), 317?332. 
Ruge, G. (1995). Wortbedeutung und Termassoziation. 
Hildesheim: Olms. 
Salton, G.; McGill, M. (1983). Introduction to Modern 
Information Retrieval. New York: McGraw-Hill.  
Sch?tze, H. (1997). Ambiguity Resolution in Language 
Learning: Computational and Cognitive Models. 
Stanford: CSLI Publications.  
Smadja, F. (1993). Retrieving collocations from text: 
Xtract. Computational Linguistics 19(1), 143?177. 
Wettler, M.; Rapp, R. (1993). Computation of word 
associations based on the co-occurrences of words in 
large corpora. In: Proceedings of the 1st Workshop on 
Very Large Corpora: Columbus, Ohio, 84?93. 
Wettler, M., Rapp, R., Ferber, R. (1993). Freie Asso-
ziationen und Kontiguit?ten von W?rtern in Texten. 
Zeitschrift f?r Psychologie, 201, 99?108.  
Exploring the Sense Distributions of Homographs
Reinhard Rapp
University of Mainz, FASK
76711 Germersheim, Germany
rrapp@uni-mainz.de
Abstract
This paper quantitatively investigates in
how far local context is useful to disam-
biguate the senses of an ambiguous word.
This is done by comparing the co-occur-
rence frequencies of particular context
words. First, one context word repre-
senting a certain sense is chosen, and then
the co-occurrence frequencies with two
other context words, one of the same and
one of another sense, are compared. As
expected, it turns out that context words
belonging to the same sense have consid-
erably higher co-occurrence frequencies
than words belonging to different senses.
In our study, the sense inventory is taken
from the University of South Florida
homograph norms, and the co-occurrence
counts are based on the British National
Corpus.
1 Introduction
Word sense induction and disambiguation is of
importance for many tasks in speech and lan-
guage processing, such as speech recognition,
machine translation, natural language under-
standing, question answering, and information re-
trieval. As evidenced by several SENSEVAL
sense disambiguation competitions (Kilgarriff &
Palmer, 2000), statistical methods are dominant
in this field. However, none of the published al-
gorithms comes close to human performance in
word sense disambiguation, and it is therefore
unclear in how far the statistical regularities that
are exploited in these algorithms are a solid basis
to eventually solve the problem.
Although this is a difficult question, in this
study we try to give at least a partial answer. Our
starting point is the observation that ambiguous
words can usually be disambiguated by their con-
text, and that certain context words can be seen
as indicators of certain senses. For example, con-
text words such as finger and arm are typical of
the hand meaning of palm, whereas coconut and
oil are typical of its tree meaning. The essence
behind many algorithms for word sense disam-
biguation is to implicitly or explicitly classify all
possible context words into groups relating to
one or another sense. This can be done in a su-
pervised (Yarowsky, 1994), a semi-supervised
(Yarowsky, 1995) or a fully unsupervised way
(Pantel & Lin, 2002).
However, the classification can only work if
the statistical clues are clear enough and if there
are not too many exceptions. In terms of word
co-occurrence statistics, we can say that within
the local contexts of an ambiguous word, context
words typical of the same sense should have high
co-occurrence counts, whereas context words as-
sociated with different senses should have co-
occurrence counts that are considerably lower.
Although the relative success of previous disam-
biguation systems (e.g. Yarowsky, 1995) sug-
gests that this should be the case, the effect has
usually not been quantified as the emphasis was
on a task-based evaluation. Also, in most cases
the amount of context to be used has not been
systematically examined.
2 Methodology
Our starting point is a list of 288 ambiguous
words (homographs) where each comes together
with two associated words that are typical of one
sense and a third associated word that is typical
of another sense. Table 1 shows the first ten en-
tries in the list. It has been derived from the Uni-
versity of South Florida homograph norms (Nel-
son et al, 1980) and is based on a combination of
native speakers? intuition and the expertise of
specialists.
The University of South Florida homograph
norms comprise 320 words which were all se-
lected from Roget?s International Thesaurus
(1962). Each word has at least two distinct mean-
ings that were judged as likely to be understood
by everyone. As described in detail in Nelson et
al. (1980), the compilation of the norms was con-
ducted as follows: 46 subjects wrote down the
first word that came to mind for each of the 320
homographs. In the next step, for each homo-
graph semantic categories were chosen to reflect
155
its meanings. All associative responses given by
the subjects were assigned to one of these catego-
ries. This was first done by four judges individu-
ally, and then, before final categorization, each
response was discussed until a consensus was
achieved.
The data used in our study (first ten items
shown in Table 1) was extracted from these
norms by selecting for each homograph the first
two words relating to its first meaning and the
first word relating to its second meaning.
Thereby we had to abandon those homographs
where all of the subjects? responses had been as-
signed to a single category, so that only one cate-
gory appeared in the homograph norms. This was
the case for 32 words, which is the reason that
our list comprises only 288 instead of 320 items.
Another resource that we use is the British Na-
tional Corpus (BNC), which is a balanced sample
of written and spoken English that comprises
about 100 million words (Burnard & Aston,
1998). This corpus was used without special pre-
processing, i.e. stop words were not removed and
no stemming was conducted. From the corpus we
extracted concordances comprising text windows
of a certain width (e.g. plus and minus 20 words
around the given word) for each of the 288
homographs. For each concordance we computed
two counts: The first is the number of con-
cordance lines where the two words associated
with sense 1 occur together. The second is the
number of concordance lines where the first word
associated with sense 1 and the word associated
with sense 2 co-occur. The expectation is that the
first count should be higher as words associated
to the same sense should co-occur more often
than words associated to different senses.
sense 1 sense 2homo-
graph first asso-
ciation (w1)
second asso-
ciation (w2)
first asso-
ciation (w3)
arm leg hand war
ball game base dance
bar drink beer crow
bark dog loud tree
base ball line bottom
bass fish trout drum
bat ball boy fly
bay Tampa water hound
bear animal woods weight
beam wood ceiling light
Table 1. First ten of 288 homographs and some
associations to their first and second senses.
However, as absolute word frequencies can
vary over several orders of magnitude and as this
effect could influence our co-occurrence counts
in an undesired way, we decided to take this into
account by dividing the co-occurrence counts by
the concordance frequency of the second words
in our pairs. We did not normalize for the fre-
quency of the first word as it is identical for both
pairs and therefore represents a constant factor.
Note that we normalized for the observed fre-
quency within the concordance and not within
the entire corpus.
If we denote the first word associated to
sense 1 with w1, the second word associated with
sense 1 with w2, and the word associated with
sense 2 with w3, the two scores s1 and s2 that we
compute can be described as follows:
In cases where the denominator was zero we as-
signed a score of zero to the whole expression.
For all 288 homographs we compared s1 to s2. If
it turns out that in the vast majority of cases s1 is
higher than s2, then this result would be an indi-
cator that it is promising to use such co-occur-
rence statistics for the assignment of context
words to senses. On the other hand, should this
not be the case, the conclusion would be that this
approach does not have the potential to work and
should be discarded.
As in statistics the results are often not as clear
cut as would be desirable, for comparison we
conducted another experiment to help us with the
interpretation. This time the question was
whether our results were caused by properties of
the homographs or if we had only measured
properties of the context words w1, w2 and w3.
The idea was to conduct the same experiment
again, but this time not based on concordances
but on the entire corpus. However, considering
the entire corpus would make it necessary to use
a different kind of text window for counting the
co-occurrences as there would be no given word
to center the text window around, which could
lead to artefacts and make the comparison prob-
lematic. We therefore decided to use concor-
dances again, but this time not the concordances
of the homographs (first column in Table 1) but
the concordances of all 288 instances of w1 (sec-
ond column in Table 1). This way we had exactly
number of lines where w1 and w2 co-occur
s1 =
occurrence count of w2 within concordance
number of lines where w1 and w3 co-occur
s2 =
occurrence count of w3 within concordance
156
the same window type as in the first experiment,
but this time the entire corpus was taken into ac-
count as all co-occurrences of w2 or w3 with w1
must necessarily appear within the concordance
of w1.
We name the scores resulting from this ex-
periment s3 and s4, where s3 corresponds to s1
and s4 corresponds to s2, with the only difference
being that the concordances of the homographs
are replaced by the concordances of the instances
of w1. Regarding the interpretation of the results,
if the ratio between s3 and s4 should turn out to
be similar to the ratio between s1 and s2, then the
influence of the homographs would be margin-
ally or non existent. If there should be a major
difference, then this would give evidence that, as
desired, a property of the homograph has been
measured.
3 Results and discussion
Following the procedure described in the previ-
ous section, Table 2 gives some quantitative re-
sults. It shows the overall results for the homo-
graph-based concordance and for the w1-based
concordance for different concordance widths. In
each case not only the number of cases is given
where the results correspond to expectations
(s1 > s2 and s3 > s4), but also the number of
cases where the outcome is undecided (s1 = s2
and s3 = s4). Although this adds some redun-
dancy, for convenience also the number of cases
with an unexpected outcome is listed. All three
numbers sum up to 288 which is the total number
of homographs considered.
If we look at the left half of Table 2 which
shows the results for the concordances based on
the homographs, we can see that the number of
correct cases steadily increases with increasing
width of the concordance until a width of ?300 is
reached. At the same time, the number of unde-
cided cases rapidly goes down. At a concordance
width of ?300, the number of correct cases (201)
outnumbers the number of incorrect cases (63) by
a factor of 3.2. Note that the increase of incorrect
cases is probably mostly an artefact of the sparse-
data-problem as the number of undecided cases
decreases faster than the number of correct cases
increases.
On the right half of Table 2 the results for the
concordances based on w1 are given. Here the
number of correct cases starts at a far higher level
for small concordance widths, increases up to a
concordance width of ?10 where it reaches its
maximum, and then decreases slowly. At the
concordance width of ?10 the ratio between cor-
rect and incorrect cases is 2.6.
How can we now interpret these results? What
we can say for sure when we look at the number
of undecided cases is that the problem of data
sparseness is much more severe if we consider
the concordances of the homographs rather than
the concordances of w1. This outcome can be ex-
pected as in the first case we only take a (usually
small) fraction of the full corpus into account,
whereas the second case is equivalent to consid-
ering the full corpus. What we can also say is that
the optimal concordance width depends on data
sparseness. If data is more sparse, we need a
wider concordance width to obtain best results.
concordance of homograph concordance of w1
concordance
width s1 > s2
correct
s1 = s2
undecided
s1 < s2
incorrect
s3 > s4
correct
s3 = s4
undecided
s3 < s4
incorrect
?1 1 287 0 107 135 46
?2 15 273 0 158 69 61
?3 32 249 7 179 40 69
?5 54 222 12 194 21 73
?10 81 181 26 199 13 76
?20 126 127 35 196 7 85
?30 129 105 44 192 5 91
?50 165 69 54 192 2 94
?100 182 44 62 185 1 102
?200 198 29 61 177 1 110
?300 201 24 63 177 1 110
?500 199 19 70 171 1 116
Table 2. Results for homograph-based concordance (left) and for w1-based concordance (right).
157
In case of the full corpus the optimal width is
around ?10 which is similar to average sentence
length. Larger windows seem to reduce saliency
and therefore affect the results adversely. In
comparison, if we look at the concordances of
the homographs, the negative effect on saliency
with increasing concordance width seems to be
more than outweighed by the decrease in sparse-
ness, as the results at a very large width of ?300
are better than the best results for the full corpus.
However, if we used a much larger corpus than
the BNC, it can be expected that best results
would be achieved at a smaller width, and that
these are likely to be better than the ones
achieved using the BNC.
4 Conclusions and future work
Our experiments showed that associations be-
longing to the same sense of a homograph have
far higher co-occurrence counts than associations
belonging to different senses. This is especially
true when we look at the concordances of the
homographs, but ? to a somewhat lesser extend ?
also when we look at the full corpus. The dis-
crepancy between the two approaches can proba-
bly be enlarged by increasing the size of the cor-
pus. However, further investigations are neces-
sary to verify this claim.
With the approach based on the concordances
of the homographs best results were achieved
with concordance widths that are about an order
of magnitude larger than average sentence
length. However, human performance shows that
the context within a sentence usually suffices to
disambiguate a word. A much larger corpus
could possibly solve this problem as it should al-
low to reduce concordance width without loosing
accuracy. However, since human language ac-
quisition seems to be based on the reception of
only in the order of 100 million words (Lan-
dauer & Dumais, 1997, p. 222), and because the
BNC already is of that size, there also must be
another solution to this problem.
Our suggestion is to not look at the co-occur-
rence frequencies of single word pairs, but at the
average co-occurrence frequencies between sev-
eral pairs derived from larger groups of words.
Let us illustrate this by coming back to our ex-
ample in the introduction, where we stated that
context words such as finger and arm are typical
of the hand meaning of palm, whereas coconut
and oil are typical of its tree meaning. The
sparse-data-problem may possibly prevent our
expectation come true, namely that finger and
arm co-occur more often than finger and coco-
nut. But if we add other words that are typical of
the hand meaning, e.g. hold or wrist, then an in-
cidental lack of observed co-occurrences be-
tween a particular pair can be compensated by
co-occurrences between other pairs. Since the
number of possible pairs increases quadratically
with the number of words that are considered,
this should have a significant positive effect on
the sparse-data-problem, which is to be exam-
ined in future work.
Acknowledgments
I would like to thank the three anonymous re-
viewers for their detailed and helpful comments.
References
Burnard, Lou.; Aston, Guy (1998). The BNC
Handbook: Exploring the British National
Corpus with Sara. Edinburgh University
Press.
Kilgarriff, Adam; Palmer, Martha (eds.) (2000).
International Journal of Computers and the
Humanities. Special Issue on SENSEVAL,
34(1-2), 2000.
Landauer, Thomas K.; Dumais, Susan S. (1997).
A solution to Plato?s problem: the latent se-
mantic analysis theory of acquisition, induc-
tion and representation of knowledge. Psy-
chological Review 104(2), 211-240.
Nelson, Douglas L.; McEvoy, Cathy L.; Walling,
John R.; Wheeler, Joseph W. (1980). The
University of South Florida homograph
norms. Behavior Research Methods & Instru-
mentation 12(1), 16-37.
Pantel, Patrick; Lin, Dekang (2002). Discovering
word senses from text. In: Proceedings of
ACM SIGKDD, Edmonton, 613-619.
Roget?s International Thesaurus (3rd ed., 1962).
New York: Crowell.
Yarowsky, David (1994). Decision lists for lexi-
cal ambiguity resolution: application to accent
restoration in Spanish and French. Proceed-
ings of the 32nd Meeting of the ACL, Las Cru-
ces, NM, 88-95.
Yarowsky, David (1995). Unsupervised word
sense disambiguation rivaling supervised me-
thods. Proceedings of the 33rd Meeting of the
ACL, Cambridge, MA, 189-196.
158
A Practical Solution to the Problem of Automatic Word Sense Induction 
Reinhard Rapp 
University of Mainz, FASK 
D-76711 Germersheim, Germany 
   rapp@mail.fask.uni-mainz.de 
 
Abstract 
Recent studies in word sense induction are 
based on clustering global co-occurrence vec-
tors, i.e. vectors that reflect the overall be-
havior of a word in a corpus. If a word is se-
mantically ambiguous, this means that these 
vectors are mixtures of all its senses. Inducing 
a word?s senses therefore involves the difficult 
problem of recovering the sense vectors from 
the mixtures. In this paper we argue that the 
demixing problem can be avoided since the 
contextual behavior of the senses is directly 
observable in the form of the local contexts of 
a word. From human disambiguation perform-
ance we know that the context of a word is 
usually sufficient to determine its sense. Based 
on this observation we describe an algorithm 
that discovers the different senses of an am-
biguous word by clustering its contexts. The 
main difficulty with this approach, namely the 
problem of data sparseness, could be mini-
mized by looking at only the three main di-
mensions of the context matrices. 
1 Introduction 
The topic of this paper is word sense induction, 
that is the automatic discovery of the possible 
senses of a word. A related problem is word sense 
disambiguation: Here the senses are assumed to be 
known and the task is to choose the correct one 
when given an ambiguous word in context. 
Whereas until recently the focus of research had 
been on sense disambiguation, papers like Pantel & 
Lin (2002), Neill (2002), and Rapp (2003) give 
evidence that sense induction now also attracts at-
tention. 
In the approach by Pantel & Lin (2002), all 
words occurring in a parsed corpus are clustered on 
the basis of the distances of their co-occurrence 
vectors. This is called global clustering. Since (by 
looking at differential vectors) their algorithm al-
lows a word to belong to more than one cluster, 
each cluster a word is assigned to can be consid-
ered as one of its senses. A problem that we see 
with this approach is that it allows only as many 
senses as clusters, thereby limiting the granularity 
of the meaning space. This problem is avoided by 
Neill (2002) who uses local instead of global clus-
tering. This means, to find the senses of a given 
word only its close associations are clustered, that 
is for each word new clusters will be found. 
 Despite many differences, to our knowledge al-
most all approaches to sense induction that have 
been published so far have a common limitation: 
They rely on global co-occurrence vectors, i.e. on 
vectors that have been derived from an entire cor-
pus. Since most words are semantically ambigu-
ous, this means that these vectors reflect the sum of 
the contextual behavior of a word?s underlying 
senses, i.e. they are mixtures of all senses occur-
ring in the corpus. 
However, since reconstructing the sense vectors 
from the mixtures is difficult, the question is if we 
really need to base our work on mixtures or if there 
is some way to directly observe the contextual be-
havior of the senses thereby avoiding the mixing 
beforehand. In this paper we suggest to look at lo-
cal instead of global co-occurrence vectors. As can 
be seen from human performance, in almost all 
cases the local context of an ambiguous word is 
sufficient to disambiguate its sense. This means 
that the local context of a word usually carries no 
ambiguities. The aim of this paper is to show how 
this observation whose application tends to se-
verely suffer from the sparse-data problem can be 
successfully exploited for word sense induction. 
2 Approach 
The basic idea is that we do not cluster the 
global co-occurrence vectors of the words (based 
on an entire corpus) but local ones which are de-
rived from the contexts of a single word. That is, 
our computations are based on the concordance of 
a word. Also, we do not consider a term/term but a 
term/context matrix. This means, for each word 
that we want to analyze we get an entire matrix. 
Let us exemplify this using the ambiguous word 
palm with its tree and hand senses. If we assume 
that our corpus has six occurrences of palm, i.e. 
there are six local contexts, then we can derive six 
local co-occurrence vectors for palm. Considering 
only strong associations to palm, these vectors 
could, for example, look as shown in table 1. 
The dots in the matrix indicate if the respective 
word occurs in a context or not. We use binary 
vectors since we assume short contexts where 
words usually occur only once. By looking at the 
matrix it is easy to see that contexts c1, c3, and c6 
seem to relate to the hand sense of palm, whereas 
contexts c2, c4, and c5 relate to its tree sense. Our 
intuitions can be resembled by using a method for 
computing vector similarities, for example the co-
sine coefficient or the (binary) Jaccard-measure. If 
we then apply an appropriate clustering algorithm 
to the context vectors, we should obtain the two 
expected clusters. Each of the two clusters corre-
sponds to one of the senses of palm, and the words 
closest to the geometric centers of the clusters 
should be good descriptors of each sense. 
However, as matrices of the above type can be 
extremely sparse, clustering is a difficult task, and 
common algorithms often deliver sub-optimal re-
sults. Fortunately, the problem of matrix sparse-
ness can be minimized by reducing the dimension-
ality of the matrix. An appropriate algebraic 
method that has the capability to reduce the dimen-
sionality of a rectangular or square matrix in an 
optimal way is singular value decomposition 
(SVD). As shown by Sch?tze (1997) by reducing 
the dimensionality a generalization effect can be 
achieved that often improves the results. The ap-
proach that we suggest in this paper involves re-
ducing the number of columns (contexts) and then 
applying a clustering algorithm to the row vectors 
(words) of the resulting matrix. This works well 
since it is a strength of SVD to reduce the effects 
of sampling errors and to close gaps in the data. 
 
 c1 c2 c3 c4 c5 c6 
arm ?  ?    
beach  ?   ?  
coconut  ?  ? ?  
finger ?  ?    
hand ?  ?   ? 
shoulder ?     ? 
tree  ?  ?   
Table 1: Term/context matrix for the word palm. 
3 Algorithm 
As in previous work (Rapp, 2002), our compu-
tations are based on a partially lemmatized version 
of the British National Corpus (BNC) which has 
the function words removed. Starting from the list 
of 12 ambiguous words provided by Yarowsky 
(1995) which is shown in table 2, we created a 
concordance for each word, with the lines in the 
concordances each relating to a context window of 
?20 words. From the concordances we computed 
12 term/context-matrices (analogous to table 1) 
whose binary entries indicate if a word occurs in a 
particular context or not. Assuming that the 
amount of information that a context word pro-
vides depends on its association strength to the 
ambiguous word, in each matrix we removed all 
words that are not among the top 30 first order as-
sociations to the ambiguous word. These top 30 as-
sociations were computed fully automatically 
based on the log-likelihood ratio. We used the pro-
cedure described in Rapp (2002), with the only 
modification being the multiplication of the log-
likelihood values with a triangular function that 
depends on the logarithm of a word?s frequency. 
This way preference is given to words that are in 
the middle of the frequency range. Figures 1 to 3 
are based on the association lists for the words 
palm and poach. 
Given that our term/context matrices are very 
sparse with each of their individual entries seeming 
somewhat arbitrary, it is necessary to detect the 
regularities in the patterns. For this purpose we ap-
plied the SVD to each of the matrices, thereby re-
ducing their number of columns to the three main 
dimensions. This number of dimensions may seem 
low. However, it turned out that with our relatively 
small matrices (matrix size is the occurrence fre-
quency of a word times the number of associations 
considered) it was sometimes not possible to com-
pute more than three singular values, as there are 
dependencies in the data. Therefore, we decided to 
use three dimensions for all matrices. 
The last step in our procedure involves applying a 
clustering algorithm to the 30 words in each ma-
trix. For our condensed matrices of 3 rows and 30 
columns this is a rather simple task. We decided to 
use the hierarchical clustering algorithm readily 
available in the MATLAB (MATrix LABoratory) 
programming language. After some testing with 
various similarity functions and linkage types, we 
finally opted for the cosine coefficient and single 
linkage which is the combination that apparently 
gave the best results.  
 
axes: grid/tools bass: fish/music 
crane: bird/machine drug: medicine/narcotic 
duty: tax/obligation motion: legal/physical 
palm: tree/hand plant: living/factory 
poach: steal/boil sake: benefit/drink 
space: volume/outer tank: vehicle/container 
Table 2: Ambiguous words and their senses. 
4 Results 
Before we proceed to a quantitative evaluation, 
by looking at a few examples let us first give a 
qualitative impression of some results and consider 
the contribution of SVD to the performance of our 
algorithm. Figure 1 shows a dendrogram for the 
word palm (corpus frequency in the lemmatized 
BNC: 2054) as obtained after applying the algo-
rithm described in the previous section, with the 
only modification that the SVD step was omitted, 
i.e. no dimensionality reduction was performed. 
The horizontal axes in the dendrogram is dissimi-
larity (1 ? cosine), i.e. 0 means identical items and 
1 means no similarity. The vertical axes has no 
special meaning. Only the order of the words is 
chosen in such a way that line crossings are 
avoided when connecting clusters. 
As we can see, the dissimilarities among the top 
30 associations to palm are all in the upper half of 
the scale and not very distinct. The two expected 
clusters for palm, one relating to its hand and the 
other to its tree sense, have essentially been found. 
According to our judgment, all words in the upper 
branch of the hierarchical tree are related to the 
hand sense of palm, and all other words are related 
to its tree sense. However, it is somewhat unsatis-
factory that the word frond seems equally similar 
to both senses, whereas intuitively we would 
clearly put it in the tree section. 
Let us now compare figure 1 to figure 2 which 
has been generated using exactly the same proce-
dure with the only difference that the SVD step 
(reduction to 3 dimensions) has been conducted in 
this case. In figure 2 the similarities are generally 
at a higher level (dissimilarities lower), the relative 
differences are bigger, and the two expected clus-
ters are much more salient. Also, the word frond is 
now well within the tree cluster. Obviously, figure 
2 reflects human intuitions better than figure 1, and 
we can conclude that SVD was able to find the 
right generalizations. Although space constraints 
prevent us from showing similar comparative dia-
grams for other words, we hope that this novel way 
of comparing dendrograms makes it clearer what 
the virtues of SVD are, and that it is more than just 
another method for smoothing. 
Our next example (figure 3) is the dendrogram 
for poach (corpus frequency: 458). It is also based 
on a matrix that had been reduced to 3 dimensions. 
The two main clusters nicely distinguish between 
the two senses of poach, namely boil and steal. 
The upper branch of the hierarchical tree consists 
of words related to cooking, the lower one mainly 
contains words related to the unauthorized killing 
of wildlife in Africa which apparently is an im-
portant topic in the BNC. 
Figure 3 nicely demonstrates what distinguishes 
the clustering of local contexts from the clustering 
of global co-occurrence vectors. To see this, let us 
bring our attention to the various species of ani-
mals that are among the top 30 associations to 
poach. Some of them seem more often affected by 
cooking (pheasant, chicken, salmon), others by 
poaching (elephant, tiger, rhino). According to the 
diagram only the rabbit is equally suitable for both 
activities, although fortunately its affinity to cook-
ing is lower than it is for the chicken, and to poach-
ing it is lower than it is for the rhino. 
That is, by clustering local contexts our algo-
rithm was able to separate the different kinds of 
animals according to their relationship to poach. If 
we instead clustered global vectors, it would most 
likely be impossible to obtain this separation, as 
from a global perspective all animals have most 
properties (context words) in common, so they are 
likely to end up in a single cluster. Note that what 
we exemplified here for animals applies to all link-
age decisions made by the algorithm, i.e. all deci-
sions must be seen from the perspective of the am-
biguous word. 
This implies that often the clustering may be 
counterintuitive from the global perspective that as 
humans we tend to have when looking at isolated 
words. That is, the clusters shown in figures 2 and 
3 can only be understood if the ambiguous words 
they are derived from are known. However, this is 
exactly what we want in sense induction. 
In an attempt to provide a quantitative evaluation 
of our results, for each of the 12 ambiguous words 
shown in table 1 we manually assigned the top 30 
first-order associations to one of the two senses 
provided by Yarowsky (1995). We then looked at 
the first split in our hierarchical trees and assigned 
each of the two clusters to one of the given senses. 
In no case was there any doubt on which way 
round to assign the two clusters to the two given 
senses. Finally, we checked if there were any mis-
classified items in the clusters. 
According to this judgment, on average 25.7 of 
the 30 items were correctly classified, and 4.3 
items were misclassified. This gives an overall ac-
curacy of 85.6%. Reasons for misclassifications 
include the following: Some of the top 30 associa-
tions are more or less neutral towards the senses, 
so even for us it was not always possible to clearly 
assign them to one of the two senses. In other 
cases, outliers led to a poor first split, like if in fig-
ure 1 the first split would be located between frond 
and the rest of the vocabulary. In the case of sake 
the beverage sense is extremely rare in the BNC 
and therefore was not represented among the top 
30 associations. For this reason the clustering algo-
rithm had no chance to find the expected clusters. 
5 Conclusions and prospects 
From the observations described above we con-
clude that avoiding the mixture of senses, i.e. 
clustering local context vectors instead of global 
co-occurrence vectors, is a good way to deal with 
the problem of word sense induction. However, 
there is a  pitfall, as the matrices of local vectors 
are extremely sparse. Fortunately, our simulations 
suggest that computing the main dimensions of a 
matrix through SVD solves the problem of sparse-
ness and greatly improves clustering results. 
Although the results that we presented in this 
paper seem useful even for practical purposes, we 
can not claim that our algorithm is capable of 
finding all the fine grained distinctions that are 
listed in manually created dictionaries such as the 
Longman Dictionary of Contemporary English 
(LDOCE), or in lexical databases such as WordNet. 
For future improvement of the algorithm we see 
two main possibilities: 
1) Considering all context words instead of only 
the top 30 associations would further reduce the 
sparse data problem. However, this requires find-
ing an appropriate association function. This is dif-
ficult, as for example the log-likelihood ratio, al-
though delivering almost perfect rankings, has an 
inappropriate value characteristic: The increase  
in computed strengths is over-proportional for 
stronger associations. This prevents the SVD from 
finding optimal dimensions. 
2) The principle of avoiding mixtures can be ap-
plied more consequently if not only local instead of 
global vectors are used, but if also the parts of 
speech of the context words are considered. By op-
erating on a part-of-speech tagged corpus those 
sense distinctions that have an effect on part of 
speech can be taken into account. 
Acknowledgements 
I would like to thank Manfred Wettler, Robert 
Dale, Hinrich Sch?tze, and Raz Tamir for help and 
discussions, and the DFG for financial support. 
References  
Neill, D. B. (2002). Fully Automatic Word Sense 
Induction by Semantic Clustering. Cambridge 
University, Master?s Thesis, M.Phil. in Com-
puter Speech. 
Pantel, P.; Lin, D. (2002). Discovering word senses 
from text. In: Proceedings of ACM SIGKDD, 
Edmonton, 613?619. 
Rapp, R. (2002). The computation of word asso-
ciations: comparing syntagmatic and paradigma-
tic approaches. Proc. of 19th COLING, Taipei, 
ROC, Vol. 2, 821?827. 
Rapp, R. (2003). Word sense discovery based on 
sense descriptor dissimilarity. In: Ninth Machine 
Translation Summit, New Orleans, 315?322. 
Sch?tze, H. (1997). Ambiguity Resolution in Lan-
guage Learning: Computational and Cognitive 
Models. Stanford: CSLI Publications. 
Yarowsky, D. (1995). Unsupervised word sense 
disambiguation rivaling supervised methods. In: 
Proc. of 33rd ACL, Cambridge, MA, 189?196. 
 
Figure 1: Clustering results for palm without SVD. 
 
Figure 2: Clustering results for palm with SVD. 
 
Figure 3: Clustering results for poach with SVD. 
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 77?80, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Practical Solution to the Problem of  
Automatic Part-of-Speech Induction from Text 
 
Reinhard Rapp 
University of Mainz, FASK 
D-76711 Germersheim, Germany 
rapp@mail.fask.uni-mainz.de 
  
Abstract 
The problem of part-of-speech induction 
from text involves two aspects: Firstly, a 
set of word classes is to be derived auto-
matically. Secondly, each word of a vo-
cabulary is to be assigned to one or sev-
eral of these word classes. In this paper 
we present a method that solves both 
problems with good accuracy. Our ap-
proach adopts a mixture of statistical me-
thods that have been successfully applied 
in word sense induction. Its main advan-
tage over previous attempts is that it re-
duces the syntactic space to only the most 
important dimensions, thereby almost eli-
minating the otherwise omnipresent prob-
lem of data sparseness. 
1 Introduction 
Whereas most previous statistical work concerning 
parts of speech has been on tagging, this paper 
deals with part-of-speech induction. In part-of-
speech induction two phases can be distinguished: 
In the first phase a set of word classes is to be de-
rived automatically on the basis of the distribution 
of the words in a text corpus. These classes should 
be in accordance with human intuitions, i.e. com-
mon distinctions such as nouns, verbs and adjec-
tives are desirable. In the second phase, based on 
its observed usage each word is assigned to one or 
several of the previously defined classes. 
The main reason why part-of-speech induction 
has received far less attention than part-of-speech 
tagging is probably that there seemed no urgent 
need for it as linguists have always considered 
classifying words as one of their core tasks, and as 
a consequence accurate lexicons providing such 
information are readily available for many lan-
guages. Nevertheless, deriving word classes auto-
matically is an interesting intellectual challenge 
with relevance to cognitive science. Also, advan-
tages of the automatic systems are that they should 
be more objective and can provide precise infor-
mation on the likelihood distribution for each of a 
word?s parts of speech, an aspect that is useful for 
statistical machine translation. 
The pioneering work on class based n-gram 
models by Brown et al (1992) was motivated by 
such considerations. In contrast, Sch?tze (1993) by 
applying a neural network approach put the em-
phasis on the cognitive side. More recent work in-
cludes Clark (2003) who combines distributional 
and morphological information, and Freitag (2004) 
who uses a hidden Marcov model in combination 
with co-clustering. 
Most studies use abstract statistical measures 
such as perplexity or the F-measure for evaluation. 
This is good for quantitative comparisons, but 
makes it difficult to check if the results agree with 
human intuitions. In this paper we use a straight-
forward approach for evaluation. It involves check-
ing if the automatically generated word classes 
agree with the word classes known from grammar 
books, and whether the class assignments for each 
word are correct. 
2 Approach 
In principle, word classification can be based on a 
number of different linguistic principles, e.g. on 
phonology, morphology, syntax or semantics. 
However, in this paper we are only interested in 
syntactically motivated word classes. With syntac-
tic classes the aim is that words belonging to the 
same class can substitute for one another in a sen-
tence without affecting its grammaticality. 
As a consequence of the substitutability, when 
looking at a corpus words of the same class typi-
cally have a high agreement concerning their left 
and right neighbors. For example, nouns are fre-
quently preceded by words like a, the, or this, and 
succeeded by words like is, has or in. In statistical 
77
terms, words of the same class have a similar fre-
quency distribution concerning their left and right 
neighbors. To some extend this can also be ob-
served with indirect neighbors, but with them the 
effect is less salient and therefore we do not con-
sider them here. 
The co-occurrence information concerning the 
words in a vocabulary and their neighbors can be 
stored in a matrix as shown in table 1. If we now 
want to discover word classes, we simply compute 
the similarities between all pairs of rows using a 
vector similarity measure such as the cosine coef-
ficient and then cluster the words according to 
these similarities. The expectation is that unambi-
guous nouns like breath and meal form one cluster, 
and that unambiguous verbs like discuss and pro-
tect form another cluster.  
Ambiguous words like link or suit should not 
form a tight cluster but are placed somewhere in 
between the noun and the verb clusters, with the 
exact position depending on the ratios of the occur-
rence frequencies of their readings as either a noun 
or a verb. As this ratio can be arbitrary, according 
to our experience ambiguous words do not se-
verely affect the clustering but only form some 
uniform background noise which more or less can-
cels out in a large vocabulary.1 Note that the cor-
rect assignment of the ambiguous words to clusters 
is not required at this stage, as this is taken care of 
in the next step. 
This step involves computing the differential 
vector of each word from the centroid of its closest 
cluster, and to assign the differential vector to the 
most appropriate other cluster. This process can be 
repeated until the length of the differential vector 
falls below a threshold or, alternatively, the agree-
ment with any of the centroids becomes too low. 
This way an ambiguous word is assigned to several 
parts of speech, starting from the most common 
and proceeding to the least common. Figure 1 il-
lustrates this process. 
                                                           
1
 An alternative to relying on this fortunate but somewhat un-
satisfactory effect would be not to use global co-occurrence 
vectors but local ones, as successfully proposed in word sense 
induction (Rapp, 2004). This means that every occurrence of a 
word obtains a separate row vector in table 1. The problem 
with the resulting extremely sparse matrix is that most vectors 
are either orthogonal to each other or duplicates of some other 
vector, with the consequence that the dimensionality reduction 
that is indispensable for such matrices does not lead to sensi-
ble results. This problem is not as severe in word sense induc-
tion where larger context windows are considered. 
The procedure that we described so far works in 
theory but not well in practice. The problem with it 
is that the matrix is so sparse that sampling errors 
have a strong negative effect on the results of the 
vector comparisons. Fortunately, the problem of 
data sparseness can be minimized by reducing the 
dimensionality of the matrix. An appropriate alge-
braic method that has the capability to reduce the 
dimensionality of a rectangular matrix is Singular 
Value Decomposition (SVD). It has the property 
that when reducing the number of columns the 
similarities between the rows are preserved in the 
best possible way. Whereas in other studies the 
reduction has typically been from several ten thou-
sand to a few hundred, our reduction is from sev-
eral ten thousand to only three. This leads to a very 
strong generalization effect that proves useful for 
our particular task. 
 
 left neighbors right neighbors 
 a we the you a can is well 
breath  11 0 18 0 0 14 19 0 
discuss 0 17 0 10 9 0 0 8 
link  14 6 11 7 10 9 14 3 
meal 15 0 17 0 0 9 12 0 
protect  0 15 1 12 14 0 0 4 
suit 5 0 8 3 0 8 16 2 
 
Table 1. Co-occurrence matrix of adjacent words. 
 
 
 
Figure 1. Constructing the parts of speech for can. 
3 Procedure 
Our computations are based on the unmodified text 
of the 100 million word British National Corpus 
(BNC), i.e. including all function words and with-
out lemmatization. By counting the occurrence 
frequencies for pairs of adjacent words we com-
piled a matrix as exemplified in table 1. As this 
matrix is too large to be processed with our algo-
rithms (SVD and clustering), we decided to restrict 
the number of rows to a vocabulary appropriate for 
evaluation purposes. Since we are not aware of any 
standard vocabulary previously used in related 
work, we manually selected an ad hoc list of 50 
78
words with BNC frequencies between 5000 and 
6000 as shown in table 2. The choice of 50 was 
motivated by the intention to give complete clus-
tering results in graphical form. As we did not 
want to deal with morphology, we used base forms 
only. Also, in order to be able to subjectively judge 
the results, we only selected words where we felt 
reasonably confident about their possible parts of 
speech. Note that the list of words was compiled 
before the start of our experiments and remained 
unchanged thereafter. 
The co-occurrence matrix based on the restricted 
vocabulary and all neighbors occurring in the BNC 
has a size of 50 rows times 28,443 columns. As our 
transformation function we simply use the loga-
rithm after adding one to each value in the matrix.2 
As usual, the one is added for smoothing purposes 
and to avoid problems with zero values. We de-
cided not to use a sophisticated association meas-
ure such as the log-likelihood ratio because it has 
an inappropriate value characteristic that prevents 
the SVD, which is conducted in the next step, from 
finding optimal dimensions.3 
The purpose of the SVD is to reduce the number 
of columns in our matrix to the main dimensions. 
However, it is not clear how many dimensions 
should be computed. Since our aim of identifying 
basic word classes such as nouns or verbs requires 
strong generalizations instead of subtle distinc-
tions, we decided to take only the three main di-
mensions into account, i.e. the resulting matrix has 
a size of 50 rows times 3 columns.4 The last step in 
our procedure involves applying a clustering algo-
rithm to the 50 words corresponding to the rows in 
the matrix. We used hierarchical clustering with 
average linkage, a linkage type that provides con-
siderable tolerance concerning outliers. 
4 Results and Evaluation 
Our results are presented as dendrograms which in 
contrast to 2-dimensional dot-plots have the advan-
tage of being able to correctly show the true dis-
tances between clusters. The two dendrograms in 
figure 2 where both computed by applying the pro-
cedure as described in the previous section, with 
                                                           
2
 For arbitrary vocabularies the row vectors should be divided 
by the corpus frequency of the corresponding word. 
3
 We are currently investigating if replacing the log-likelihood 
values by their ranks can solve this problem. 
4
 Note that larger matrices can require a few more dimensions. 
the only difference that in generating the upper 
dendrogram the SVD-step has been omitted, 
whereas in generating the lower dendrogram it has 
been conducted. Without SVD the expected clus-
ters of verbs, nouns and adjectives are not clearly 
separated, and the adjectives widely and rural are 
placed outside the adjective cluster. With SVD, all 
50 words are in their appropriate clusters and the 
three discovered clusters are much more salient. 
Also, widely and rural are well within the adjective 
cluster. The comparison of the two dendrograms 
indicates that the SVD was capable of making ap-
propriate generalizations. Also, when we look in-
side each cluster we can see that ambiguous words 
like suit, drop or brief are somewhat closer to their 
secondary class than unambiguous words. 
Having obtained the three expected clusters, the 
next investigation concerns the assignment of the 
ambiguous words to additional clusters. As de-
scribed previously, this is done by computing dif-
ferential vectors, and by assigning these to the 
most similar other cluster. Hereby for the cosine 
similarity we set a threshold of 0.8. That is, only if 
the similarity between the differential vector and 
its closest centroid was higher than 0.8 we as-
signed the word to this cluster and continued to 
compute differential vectors. Otherwise we as-
sumed that the differential vector was caused by 
sampling errors and aborted the process of search-
ing for additional class assignments. 
The results from this procedure are shown in ta-
ble 2 where for each of the 50 words all computed 
classes are given in the order as they were obtained 
by the algorithm, i.e. the dominant assignments are 
listed first. Although our algorithm does not name 
the classes, for simplicity we interpret them in the 
obvious way, i.e. as nouns, verbs and adjectives. A 
comparison with WordNet 2.0 choices is given in 
brackets. For example, +N means that WordNet 
lists the additional assignment noun, and -A indi-
cates that the assignment adjective found by the 
algorithm is not listed in WordNet. 
According to this comparison, for all 50 words 
the first reading is correct. For 16 words an addi-
tional second reading was computed which is cor-
rect in 11 cases. 16 of the WordNet assignments 
are missing, among them the verb readings for re-
form, suit, and rain and the noun reading for serve. 
However, as many of the WordNet assignments 
seem rare, it is not clear in how far the omissions 
can be attributed to shortcomings of the algorithm. 
79
  accident N expensive A  reform N (+V) 
  belief N familiar A (+N) rural A  
  birth N (+V) finance N V  screen N (+V) 
  breath N grow V N (-N) seek V (+N) 
  brief A N imagine V  serve V (+N) 
  broad A (+N) introduction N  slow A V  
  busy A V link N V  spring N A V (-A) 
  catch V N lovely A (+N) strike N V 
  critical A lunch N (+V) suit N (+V) 
  cup N (+V) maintain V  surprise N V 
  dangerous A occur V N (-N) tape N V 
  discuss V option N  thank V A (-A) 
  drop V N pleasure N  thin A (+V) 
  drug N (+V) protect V  tiny A 
  empty A V (+N) prove V  widely A N (-N) 
  encourage V quick A (+N) wild A (+N) 
  establish V  rain N (+V)  
 
Table 2. Computed parts of speech for each word. 
5 Summary and Conclusions 
This work was inspired by previous work on word 
sense induction. The results indicate that part of 
speech induction is possible with good success 
based on the analysis of distributional patterns in 
text. The study also gives some insight how SVD 
is capable of significantly improving the results. 
Whereas in a previous paper (Rapp, 2004) we 
found that for word sense induction the local clus-
tering of local vectors is more appropriate than the 
global clustering of global vectors, for part-of-
speech induction our conclusion is that the situa-
tion is exactly the other way round, i.e. the global 
clustering of global vectors is more adequate (see 
footnote 1). This finding is of interest when trying 
to understand the nature of syntax versus semantics 
if expressed in statistical terms. 
Acknowledgements 
I would like to thank Manfred Wettler and Chris-
tian Biemann for comments, Hinrich Sch?tze for 
the SVD-software, and the DFG (German Re-
search Society) for financial support.  
References 
Brown, Peter F.; Della Pietra, Vincent J.; deSouza, Peter 
V.; Lai, Jennifer C.; Mercer, Robert L. (1992). Class-
based n-gram models of natural language. Computa-
tional Linguistics 18(4), 467-479. 
Clark, Alexander (2003). Combining distributional and 
morphological information for part of speech induc-
tion. Proceedings of 10th EACL, Budapest, 59-66. 
Freitag, Dayne (2004). Toward unsupervised whole-
corpus tagging. Proceedings of COLING, Geneva, 
357-363. 
Rapp, Reinhard (2004). A practical solution to the prob-
lem of automatic word sense induction. Proceedings 
of ACL (Companion Volume), Barcelona, 195-198. 
Sch?tze, Hinrich (1993). Part-of-speech induction from 
scratch. Proceedings of ACL, Columbus, 251-258. 
 
 
0.8 
 
 
 
0.4 
 
 
 
0.0 
 
 
1.0 
 
 
 
0.5 
 
 
 
0.0 
 
Figure 2. Syntactic similarities with (lower dendrogram) and without SVD (upper dendrogram). 
80
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2117?2128, Dublin, Ireland, August 23-29 2014.
Using Collections of Human Language Intuitions 
to Measure Corpus Representativeness 
 
Reinhard Rapp 
Aix-Marseille Universit? 
Laboratoire d'Informatique Fondamentale 
163 Avenue de Luminy, 13288 Marseille, France 
reinhardrapp@gmx.de 
 
  
 
Abstract 
In corpus linguistics there have been numerous attempts to compile balanced corpora, result-
ing in text collections such as the Brown Corpus or the British National Corpus. These cor-
pora are meant to reflect the average language use a native speaker typically encounters. But 
is it possible to measure in how far these efforts were successful? Assuming that humans? lan-
guage intuitions are based on our brain?s capability to statistically analyze perceived language 
and to memorize these statistics, we suggest a method for measuring corpus representative-
ness which compares corpus statistics to three types of human language intuitions as collected 
from test persons: Word familiarity, word association, and word relatedness. We compute a 
representativeness score for a corpus by extracting word frequency, word co-occurrence, and 
contextual statistics from it and by comparing these statistics to the human data. The higher 
the similarity, the more representative the corpus should be for the language environments of 
the test persons. Our findings confirm the expectation that corpus size and corpus balancing 
matter. 
1 Introduction 
Balanced corpora, i.e. corpora consisting of a carefully sampled mix of texts, have often been consid-
ered important for providing a standard of average language use. Well known examples of such cor-
pora include the Brown Corpus (Francis & Ku?era, 1989) and the British National Corpus (Burnard & 
Aston, 1998). But to obtain a balance many decisions concerning the corpus design have to be made. 
Biber (1993) mentions, among other things, that it has to be decided for what target population a cor-
pus is meant to be representative, that estimates concerning the quantities of various text types are re-
quired, and that decisions with regard to the number of individual text samples and their sizes have to 
be made. 
However, there is no easy and well established way to verify the success of these measures. Current 
suggestions include, for example, to consider a corpus as representative if it is not dominated by sub-
language (Temnikova et al., 2014), or to more or less give up on the concept of representativeness and 
to concentrate on considering the suitability of a corpus for particular tasks. Saldanha (2009) comes to 
the conclusion that ?The problem with making representativeness the defining characteristic of a cor-
pus is that it is very difficult to evaluate.? 
Our goal here is to make an attempt to measure corpus representativeness in a standardized way, 
thereby avoiding to observe test persons? average language input as this would not be very practical. 
Our starting point is that a representative corpus should reflect as well as possible average language 
use as encountered by native speakers. We also assume that human language acquisition is essentially 
corpus-based (Rapp, 2011). This implies the following: The human brain analyzes particular statistical 
properties of perceived language and memorizes them. During language production these properties 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
2117
are reproduced. It has been shown that in certain test situations it is possible to isolate intuitions re-
lated to some specific statistics. These include the following three which we will utilize for measuring 
corpus representativeness: Word frequency, word co-occurrence, and common context of words. In 
terms of human language intuitions, these three statistical properties relate to word familiarities, word 
associations, and word relatedness. 
What we suggest is to extract data relating to these three types of statistical properties from a corpus 
and to compare it to the respective experimental data as obtained from test persons. The higher the 
average agreement, the more representative the corpus should be for the language environment of the 
test persons.1 
Related work has been conducted by Brisbaert & New (2009), which is mentioned in section 2.1, 
and in our own previous studies (Rapp, 2014a and Rapp, 2014c), of which the current work is an ex-
tension. A nice summary of how to measure corpus representativeness through psycholinguistic meas-
ures is provided in a presentation by Francom & Ussishkin (2011). Gries (2010), though in a slightly 
different context, emphasizes the need of external validation: ?For corpus linguists, that means that our 
measures must be validated against corpus-external evidence because, strictly speaking, as long as we 
corpus linguists do not show that our dispersions and adjusted frequencies correspond to something 
outside of our corpora, we have failed to provide the most elementary aspect of a new measure - its 
validation.?   
The remainder of this paper is structured as follows: We first describe the experimental data used, 
i.e. the familiarity norms, the association norms, and the synonym data (describing word relatedness). 
Next we present the algorithms used to extract the corresponding statistics from the corpora. By com-
paring the human and the corpus-derived data, we introduce three quantitative measures of corpus rep-
resentativeness, which we subsequently combine. The paper concludes with a discussion and an out-
look on future work.  
2 Human language intuitions 
2.1 Word familiarities 
Psychologists have collected word familiarity ratings from test persons. For this purpose, the subjects 
were asked to come up with subjective familiarities for given words. Usually a scale between 1 and 7 
was used, whereby 1 means unfamiliar and 7 means very familiar. The outcome of such experiments 
are the so-called familiarity norms, i.e. large tables listing the subjects' familiarity ratings. In the cur-
rent work we used the familiarity data for 4920 words from an online version2 of the MRC Psycho-
linguistic Database (Coltheart, 1981). 
In previous studies (e.g. Rapp, 2005) it has been shown that there is a strong correlation between the 
human familiarity judgments and the log occurrence frequencies of the words in corpora. For illustra-
tion, Table 1 shows the top five most familiar words in the MRC database together with their frequen-
cies in the Brown corpus and compares them to some of the least familiar words. As can be seen, the 
familiar words have consistently much higher corpus frequencies. To explain this finding, Rapp 
(2005) hypothesized that human familiarity ratings are based on the word frequencies as observed by 
the test persons in the language they perceive in everyday life. 
However, if we assume that the familiarity norms reflect word frequencies in perceived language, 
then it should be possible to use them as a standard for measuring the frequency aspect of corpus rep-
resentativeness. A corpus whose word frequencies are highly correlated to the familiarity norms is 
more likely to be a good surrogate for everyday language, although word frequency of course reflects 
only one of many properties of a corpus. Nevertheless, for a corpus to be representative, it is a neces-
sary (though not sufficient) condition that its word frequencies are similar to those in everyday lan-
guage. 
                                                 
1
 Let us mention that there is some analogy to automatic MT evaluation, namely when computing the BLEU score: There a 
machine translation is compared to a human translation (which is based on human intuitions) by identifying matches be-
tween n-grams of various lengths. Then a combined score is computed from the results obtained for each n-gram length. 
2
 http://websites.psychology.uwa.edu.au/school/MRCDatabase/uwa_mrc.htm 
2118
We should mention that instead of using word familiarity data it is also possible to use reaction 
times as obtained in the word recognition task.3 Brisbaert & New (2009) did so and related the reac-
tion times to the corpus frequencies of the words for the purpose of measuring corpus representative-
ness. In essence, although they tested on other corpora, their findings seem to be similar to what we 
report here based on word familiarities.  
 
FAMILIAR WORDS UNFAMILIAR WORDS 
WORD FAMILIARITY BROWN FREQUENCY WORD FAMILIARITY 
BROWN 
FREQUENCY 
BREAKFAST 6.6 53 LOQUACITY 1.4 1 
AFTERNOON 6.5 106 MIEN 1.4 1 
CLOTHES 6.5 89 YUCCA 1.4 1 
BEDROOM 6.5 52 BURGHER 1.3 1 
DAD  6.5 15 PAEAN 1.3 2 
 
Table 1: Words with high and low familiarity ratings in the MRC Psycholinguistic Database together 
with their frequency counts in the Brown Corpus (words with a corpus frequency of zero are not in-
cluded). 
2.2 Word associations 
The second type of human intuitions to be considered are word associations as obtained from test per-
sons. Such data has been collected from native speakers in large scale experiments, as exemplified in 
the Edinburgh Associative Thesaurus (EAT; Kiss et al., 1973) which is the largest classical collection 
of its kind. The EAT comprises the associative responses as requested from around 100 British stu-
dents for each of 8400 stimulus words and is available online.4 
To collect the data, the subjects were given questionnaires with lists of stimulus words, and were 
asked to write down for each stimulus word the spontaneous association which first came to mind. 
This leads to collections of associations, the so-called association norms, as exemplified in Table 2. 
 
ABOVE CONSTELLAT?ON FEM?N?NE 
below (59) stars (39) masculine (26) 
high (4) star (33) girl (14) 
over (4) sky (5) woman (8) 
sky (4) andromeda (2) female (6) 
all (3) aquarius (2) sex (3) 
up (3) plough (2) beauty (2) 
me (2) aircraft (1) bird (2) 
under (2) cancer (1) girls (2) 
 
Table 2: Top eight associations to three stimulus words as taken from the EAT. The numbers of sub-
jects responding with the respective word are given in brackets.  
2.3 Word relatedness 
The third type of human intuitions which we consider concerns word relatedness. Landauer & Dumais 
(1997) introduced a dataset for testing semantic relatedness, namely the synonym portion of the Test of 
English as a Foreign Language (TOEFL). The TOEFL is an often obligatory test for non-native 
speakers of English who intend to study at a university with English as the teaching language. The 
data used by Landauer & Dumais had been acquired from the Educational Testing Service and com-
prises 80 test items. As summarized in Rapp (2009), each item consists of a problem word embedded 
                                                 
3
 In the so-called word recognition task test persons are presented strings of characters and their task is to decide whether or 
not a string matches an English word. It turns out that the average reaction time is inversely related to the familiarity of a 
word (i.e. the less familiar a word, the longer the reaction time). 
4
 http://www.eat.rl.ac.uk/ 
2119
in a sentence and four alternative words, from which the test taker is asked to choose the one with the 
most similar meaning to the problem word. For example, given the test sentence ?Both boats and 
trains are used for transporting the materials? and the four alternative words planes, ships, canoes, 
and railroads, the subject would be expected to choose the word ships, which is supposed to be the 
one most similar to boats.  
However, Landauer & Dumais (1997) did not use the test sentences. Instead, only the lists of prob-
lem words together with their alternatives were used. A system capable of computing word relatedness 
should be able to determine for each problem word the alternative word which comes closest in mean-
ing. 
Although the TOEFL dataset has been widely used (see e.g. the overview on related work on the 
ACL Wiki5), there are two disadvantages with it: A minor one is that it is not freely available on the 
web. A more severe one is that it is rather small: This means that statistical variation is strong (which 
will be illustrated in section 5.3), and that overfitting can easily happen. That is, a system trained on 
this data may not well perform on other data. 
For this reason we decided to come up with a new dataset which avoids these problems. It is based 
on the index of Fernald's (1896) synonym and antonym dictionary as provided in the Project Guten-
berg version.6 This index lists in alphabetical order English words together with their synonyms. As in 
the dictionary there is no indication as to the quality of a synonym, in order to avoid arbitrary selec-
tions, from this list we removed all words for which several synonyms were listed in the index. In a 
semi-automatic way, we also removed a number of other items, e.g.  those containing multiword units 
or numbers. As a result, we obtained a list of 4050 words together with their synonyms. 
To obtain a dataset analogous to the TOEFL synonym set, we required three alternative words for 
each item. We could have used random words e.g. taken from the vocabulary of the British National 
Corpus (BNC). However, as the BNC is from a much later time period, this might have introduced a 
systematic bias. So we thought we should better use the words from the synonym dictionary itself. 
Note that the synonyms corresponding to the 4050 words represent a much smaller vocabulary as 
many of the synonyms are synonyms for several words. For this reason, we used the headwords them-
selves and applied the following procedure to generate the alternative words from them: 
 
1) We sorted our list of items according to the synonyms in alphabetical order. 
2) As the first column of alternative words, we used the given words but shifted them by 1000 posi-
tions, i.e. positions 1 to 3050 were matched with 1001 to 4050, and positions 3051 to 4050 where 
matched with 1 to 1000. 
3) Analogous for the second column of alternative words, but here we shifted by 2000 positions. 
4) Same for the third column of alternative words, but here we shifted by 3000 positions. 
 
Word Synonym Alternative Words 
abandoned addicted rescind bliss receipts 
abdicate abandon conflict indubitable archaic 
aberration insanity rational meliorate assured 
abetter accessory carnal amicable urbane 
abettor accessory imbruted brotherly policy 
abhorrence abomination kindliness supposition resignation 
abiding permanent remain life stanch 
ability power chimerical frontier diet 
abject pitiful despotic blanch fray 
abjure abandon contest overt disused 
 
Table 3: Ten entries from the synonym dataset derived from Fernald (1896). 
 
                                                 
5
 http://aclweb.org/aclwiki/index.php?title=TOEFL_Synonym_Questions_(State_of_the_art) 
6
 http://www.gutenberg.org/files/28900/28900-h/28900-h.htm 
2120
To give an impression of the dataset, its alphabetically first ten entries are shown in Table 3. Let us 
now quickly discuss some properties of the new dataset:  The pros are that it is about 50 times larger 
than the TOEFL dataset and that it can be freely distributed. The cons are that it is based on somewhat 
outdated language (the dictionary was published in 1896) and that the alternative words were not care-
fully selected but generated in a somewhat arbitrary fashion. Also, it is not known how test persons 
would perform on this dataset, whereas for the TOEFL dataset human performance is known at least 
for some test takers, i.e. non-native speakers of English. A commonality between both datasets is that 
the synonyms were produced by experts, i.e. reflect the experts? language intuitions. 
3 Corpora 
As in previous work (Rapp, 2014a) our corpus representativeness measure is to be applied to a number 
of well known corpora. These are: 
 
1) Brown Corpus (balanced corpus of 1 million words; Francis & Ku?era, 1989)  
2) British National Corpus (BNC; balanced corpus of 100 million words; Burnard & Aston, 1998)  
3) English Wikipedia (300 million words of encyclopaedic texts)7  
4) ukWaC (British English web corpus of 2 billion words)8  
5) English Gigaword Corpus 4th edition (4 billion words of newswire text)9 
 
Both the MRC familiarity norms and the EAT do not distinguish between uppercase and lowercase 
characters. For this reason, we also did not make such a distinction and, in a pre-processing step, con-
verted all corpora as well as the human data to lowercase only. 
For the results presented later we had to measure the size of our corpora and also of partial corpora. 
We do this by counting the number of running words. Hereby, to avoid language specific sophistica-
tions, we count as a word any string which is delimited by either white space (blanks, tabulator, new 
line) or by transitions between alpha and non-alpha characters.10 
4 Procedure 
4.1 Corpus statistics concerning word familiarities (statistics of order zero) 
In the case of word familiarities the statistics extracted from the corpora are the log frequencies of the 
words. The MRC database contains familiarities for 4920 words. As just two of them are multiword 
units, we considered this an inconsistency and removed them, so that 4918 words remained.  
 
Word Word frequency in the BNC 
Word familiarity in 
the MRC database 
a 2247100 632 
abandon 1316 510 
abandonment 500 359 
abasement 20 226 
abatement 137 294 
abbess 57 187 
abdication 124 284 
abdomen 303 426 
abduction 230 413 
aberration 149 208 
 
Table 4: BNC frequencies and MRC familiarities for the (alphabetically) first ten words covered in the 
familiarity norms of the MRC database. 
                                                 
7
 We use the English part of the Wikipedia XML Corpus (Denoyer & Gallinary, 2006). Although this is considerably smaller 
than current versions, it has the advantage that it is an offline copy so that our results can be replicated. 
8
 http://wacky.sslmit.unibo.it/doku.php?id=corpora 
9
 http://catalog.ldc.upenn.edu/LDC2009T13 
10
 Alternatively, it would also be possible to simply count the number of characters for measuring corpus size (though this 
seems less customary).  But word segmentation is required later on anyway (for computing the representativenesss scores). 
2121
The two types of data, namely the word familiarities from the MRC database and the word fre-
quencies as extracted from one of the corpora, were merged as exemplified in Table 4 for the case of 
the BNC. Note that although the test subjects' familiarity judgements were originally on a scale be-
tween 1 (not familiar) and 7 (highly familiar), to avoid decimal numbers when averaging results, all 
ratings were multiplied by 100.  Computing corpus representativeness now simply involves taking the 
logarithm of the frequencies in column 2, and then computing Pearson's correlation coefficient be-
tween the resulting vector and column 3. However, as especially for small corpora many of the word 
frequencies can be zero, and as the logarithm of zero is not defined, we applied the usual heuristic of 
adding one to each frequency count before taking the logarithm. 
4.2 Corpus statistics concerning word associations (1st order statistics) 
As described in Rapp (2014c), we assume that there is a relationship between word associations as 
collected from human subjects and word co-occurrences as observed in a corpus, and our hypothesis is 
that the strength of this relationship can be used as a measure of corpus representativeness. A corpus 
leading to simulated associations akin to the ones collected from humans is likely to be a good surro-
gate for everyday language, although ? similarly to what we said about word frequencies ? word co-
occurrence counts constitute only one of many properties of a corpus.  
For extracting word associations from corpora, in the literature many algorithms were described 
(e.g. Wettler & Rapp, 1989; Church & Hanks, 1990; Wettler et al., 2005). In analogy, we used the fol-
lowing procedure: For all words with a BNC corpus frequency of 50 or higher we computed the co-
occurrence vectors. That is, each vector contains the number of co-occurrences of the stimulus word 
with all other co-occurring words. It counts as a co-occurrence if two words appear together within a 
distance of at most ten words, i.e. a text window of ?10 words around the stimulus word is considered. 
Hereby the exact distance within the window is not taken into account.  
In a further step an association measure was applied to the co-occurrence vectors, namely Ted Dun-
ning's (1993) log-likelihood ratio. The resulting vectors we call association vectors. Given these vec-
tors, the strongest association to a given stimulus word can be determined by simply looking for the 
highest value within the respective association vector. The corresponding word is considered to be the 
associative response predicted by the system. For the same stimulus words used in Table 2, Table 5 
shows some sample associations as computed using the British National Corpus.  
 
ABOVE CONSTELLAT?ON FEM?N?NE 
below (59) stars (39) masculine (26) 
level star (33) women (2) 
average (1) southern gender 
high (4) triangle woman (8) 
feet bright female (6) 
water planet (1) men 
head rather male (1) 
see south  more 
ground find hair 
left map soft 
Table 5: Top ten corpus-derived associations for three stimulus words. The numbers of subjects from 
the EAT responding with the respective word (if larger than zero) are given in brackets. 
 
Concerning evaluation, in principle the idea is to find matches between the human and the corpus-
based associations. One possibility is to simply count the number of cases where the primary associa-
tive response matches the strongest corpus-based association. However, when it comes to very small 
corpus sizes of e.g. just 1000 words (see Section 5), the problem of data sparseness becomes so severe 
that a more tolerant evaluation method leads to more robust results less susceptible to statistical varia-
tion. This is why for measuring accuracy we count the number of cases where the respective primary 
associative response is listed within the top ten corpus-based associations, rather than insisting on a 
2122
match with the strongest association. This simple modification leads to improvements in reliability 
when measuring very low accuracies.  
4.3 Corpus statistics concerning word relatedness (2nd order statistics) 
Our algorithm for computing word relatedness consists of the following three steps: 
 
1) Counting word co-occurrences. 
2) Applying an association measure to the raw co-occurrence counts. 
3) Computing vector similarities. 
 
Steps 1 and 2 are in principle analogous to the previous subsection. Only, as mentioned in Rapp 
(2009), for computing vector similarities it turns out that it is better to consider a smaller window size 
(such as ?1 or ?2 around the given word). Also, we used a simpler association measure, namely 
log(nij+1), whereby nij is the number of co-occurrences between words i  and j, as it slightly outper-
formed the log-likelihood ratio in this particular setting.  
For step 3 (computing vector similarities) we use the standard cosine measure. Table 6 shows some 
results as obtained using the British National Corpus. For a quantitative evaluation we utilized the 
TOEFL synonym data as follows: We compared our system's results to the answers as provided in the 
TOEFL dataset. Remember that in the TOEFL synonym test the subjects had to choose the word most 
similar to a given stimulus word from a list of four alternatives. Accordingly, in the simulation, we 
assumed that the system made the right decision if the correct answer was ranked best among the four 
alternatives. In a further run, we applied exactly the same procedure to the test set derived from Fer-
nald's synonym dictionary. 
 
burden responsibility (0.62), expense (0.61), expenditure (0.59), problem (0.59), cost (0.59)  
arrogant rude (0.62), naive (0.61), stupid (0.61), impatient (0.61), haughty (0.61) 
desperation panic (0.60), despair (0.60), exasperation (0.59), stillness (0.58), impatience (0.58) 
memorandum appendix (0.59), document (0.59), submission (0.57), constitution (0.57), disclosure (0.57) 
trivial unimportant (0.63), ridiculous (0.60), trifling (0.60), straightforward (0.60), bizarre (0.60) 
Table 6: Semantic similarities extracted from the BNC for five English words using only vocabulary 
from the synonym test set based on Fernald (1896). 
5 Results 
5.1 Results based on word familiarities 
These results are given in Figure 1a. There we find in graphical form for each of the five corpora the 
computed Pearson's correlation coefficients between the words' familiarities and their log corpus fre-
quencies. For easier comparison with the other results (which are percentages) we multiply these cor-
relations by 100 and take the product as the familiarity-based representativeness of a corpus. The 
range of values can thus be between 0 and 100, whereby 0 denotes a complete lack of representative-
ness, and 100 denotes perfect representativeness. The representativeness scores are also computed for 
partial corpora, whereby all parts have in common that they start with the beginning of the respective 
corpus.  
We can see in Fig. 1a that, as expected, the representativeness is almost zero if only the first 100 
words of a corpus are taken into account, and gradually increases to at least 67 for the full corpora. 
The horizontal axis has a logarithmic scale, but still the curves flatten with increasing corpus size, es-
pecially above 1 million words. 
5.2 Results based on word associations 
These results are given in Fig. 1b. For each of the five corpora (and their parts) the percentages of 
primary associative responses are given which ranked among the top ten in the corpus-based associa-
tions. These percentages we take as the association-based representativeness of the respective corpus. 
The range of values is between 0 and 100. 
 
2123
  
 
                                         (a) Familiarity                                                                            (b) Association 
 
 
                                        (c) Relatedness                                                                                (d) Average 
 
Fig. 1: Results for the three approaches and their average. 
 
 
Fig. 2: Results for the TOEFL synonym data. 
 
2124
5.3 Results based on word relatedness 
The respective results for the TOEFL synonym data (based on a window size of ?1 words) are given 
in Fig. 2. There we find for each of the five corpora the percentage of TOEFL questions which were 
answered correctly. These percentages we take as the relatedness-based representativeness of the re-
spective corpus. Note that the level for very small corpora is higher here as in the TOEFL data with a 
limited number of candidate words there is a better chance to randomly hit the correct word. As can be 
seen, the curves are somewhat erratic which is an indication that the test set of 80 items is too small. 
For this reason, we did not further use these results but replaced them with those from the synonym 
test set derived from Fernald (1896). The respective results are shown in Figure 1c.11 As can be seen, 
the much higher number of test items leads to smoother curves, but nevertheless the tendencies from 
the TOEFL data are roughly confirmed. 
5.4 Results based on the overall average 
The average of the curves in Fig. 1a to 1c is shown in Fig. 1d. The motivation is that this way all three 
types of statistics are taken into account in a straightforward way. The underlying reasoning is analo-
gous to the BLEU score (Papineni et al., 2002) used in machine translation evaluation: There n-gram 
matches between a machine translation and a reference translation are counted separately for n-grams 
of various lengths, and then the individual scores are combined. 
6 Discussion 
If we compare the curves in Figures 1a, 1b, and 1c it is apparent that the shapes are rather different. 
This can be explained by the order of the respective statistics: The familiarity-based approach uses 
statistics of order zero (word frequencies), the association-based approach first order statistics (word 
co-occurrences), and the relatedness-based approach second order statistics (common context). 
Although for all three methods a flattening of the curves can be expected for large corpora for the 
reason that there is an upper limit of corpus representativeness (100) leading to saturation, apparently 
for the first and second order statistics larger corpora would be required to make this happen.  
Concerning very small partial corpora, for the familiarity based approach the curves quickly rise, 
whereas for the association-based and the relatedness-based approaches the increases in accuracy are 
small at the beginning. This is also to be expected because in a partial corpus of e.g. 1000 words there 
is still a chance to find a particular word, but there is almost no chance to find a particular co-occur-
rence or a common context.  
So these discrepancies between the approaches are not a major surprise. Of more interest is a com-
parison of the results between the different corpora, i.e. their relative performance for each of the 
methods. 
Following Rapp (2014a), concerning the representativeness of our five corpora and their parts, we 
had tried to come up with some hypotheses before we started to compute the results. These were our 
predictions:  
 
1) Representativeness should increase with corpus size.  
2) The Brown corpus and the BNC should be more representative than unbalanced corpora of the 
same size.   
3) The Brown corpus (1 million words) should be more representative than the first million words of 
the British National Corpus as the latter is balanced only over its full size (100 million words), but 
not over its first million words.   
4) For same sizes, we would expect ukWaC to be more representative than Wikipedia as we think 
that corpus heterogeneity is a plus for representativeness. ukWaC is obviously more heterogene-
ous as, for example, it is multi genre multi topic whereas Wikipedia is single genre multi topic.  
5) The Gigaword Corpus should be the least representative for identical sizes. Although, like Wiki-
pedia, it is also single genre multi topic, the distribution of topics is not as wide because in news-
ticker texts there are strong foci e.g. on politics and sports. 
                                                 
11
 As the Synonym-Dataset involves many very rare words, to reduce data sparseness we used a larger window size of ?2 
words to compute these results.  
2125
If we compare these hypotheses to the actual results shown in Figures 1a, 1b, and 1c, the findings are 
as follows: 
Hypothesis 1, namely that the representativeness of all corpora steadily increases with corpus size, 
is clearly confirmed by all three approaches.  
Hypothesis 2, saying that the balanced corpora, namely the Brown corpus and the BNC, should be 
more representative for their sizes than non-balanced corpora, is also confirmed by all approaches. At 
1 million words, these two are the top performers. At 100 million words, the BNC performs best. 
Note, however, that the smaller the corpus sizes, the less predictable the results as the sampling errors 
increase. 
Hypothesis 3 (Brown better than BNC for 1 million words) could sometimes be confirmed but not 
consistently. Instead, for all approaches the results of these two corpora are fairly close. This indicates 
that the BNC also seems to have a fairly good balance over the first million words. Concerning the 
association-based approach, the BNC also has the advantage that its British English should reflect the 
EAT associations (collected in Edinburgh) better than the American English of the Brown corpus. 
Hypothesis 4, namely that ukWaC is better than Wikipedia, is confirmed for the familiarity- and the 
association-based approach, but not for the relatedness-based approach. Our explanation for the dis-
crepancy is that the relatedness-data contains a larger proportion of outdated and rare words, and that 
for rare words the coverage of a corpus becomes more important. In this respect, Wikipedia with its 
wide coverage of topics is likely to have an advantage over the ukWaC corpus.  
Hypothesis 5, saying that the Gigaword corpus should be the least representative, is confirmed for 
almost all corpus sizes.  
Overall, several of our hypotheses were consistently confirmed by all approaches. This finding pro-
vides some evidence that the computed scores are actually related to what might sensibly be consid-
ered as the representativeness of a corpus. 
Concerning the average representativeness score (Fig. 1d), we can conclude that overall it seems to 
make sense to balance a corpus, and that corpus heterogeneity is a plus. 
7 Summary and outlook 
In this work we defined the term corpus representativeness as the ability of a corpus to represent the 
average language use a native speaker encounters in everyday life. As we cannot easily observe test 
persons over years, our suggestion was to utilize human intuitions on word familiarities, on word as-
sociations, and on word relatedness. 
Previous work has provided evidence that human word familiarities are based on word frequencies 
in perceived language (Rapp, 2005), that human word associations are based on the co-occurrences of 
words (Wettler et al., 2005), and that human relatedness judgments are based on common context (cf. 
Harris' (1954) distributional hypothesis). Although all of this may still be controversial, in the current 
work we took these findings for granted but turned round the perspective. We said that a corpus is rep-
resentative for the language environment of a group of persons if the word familiarities, the word as-
sociations, and the predictions of word relatedness derived from it resemble these persons? intuitions. 
For full and partial versions of five well known English corpora we computed the word familiari-
ties, word associations, and word relatedness scores for test sets of several thousand words. We then, 
for each corpus, compared the extracted information to the human data, and computed similarity 
scores which we took as measures of corpus representativeness. We also computed a combined score 
by averaging the results from all three measures. 
A shortcoming of our approach is the following: Our measures are limited in so far as they only 
consider three particular aspects of corpus representativeness, namely word familiarity word associa-
tion, and word relatedness. They do not explicitly consider higher level features e.g. concerning syn-
tax, semantics, pragmatics, or style.12 We nevertheless hope that what we described can serve as a 
starting point for further discussion.  
Concerning future work, a possible strait of research would be to modify the relatedness-based ap-
proach in a way that the WordSimilarity-353 Test Collection13 could be used. This test set provides 
                                                 
12
 In section 5.4, when combining our three approaches, we already mentioned an analogy to the BLEU score. A related 
commonality is that the BLEU score also has these shortcomings. 
13
 http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/ 
2126
direct similarity estimates between words, so a correlation to corpus-derived estimates could be com-
puted in analogy to what we did for the familiarity-based approach. 
We would also like to extend the approach to other corpus statistics which seem relevant for human 
language processing. For example, we might look at associations when given several stimulus words 
(see Rapp, 2014b), or we could try to predict a word from its WordNet synset. The latter would have 
the advantage that WordNets are available for many languages, so the corpus representativeness scores 
could be measured for a number of languages where other human data is scarce. 
Related to this would be the use of the Princeton evocation data14 which provides human similarity 
estimates between WordNet synsets. The aim would be to replicate these similarities using multiword 
associations. 
Acknowledgement 
This research was supported by a Marie Curie Intra European Fellowship within the 7th European 
Community Framework Programme. 
References 
Biber, D. (1993): Representativeness in Corpus Design. Literary and Linguistic Computing, Vol. 8, Nov. 4, 243?
257.  
Brisbaert, M.; New, B. (2009). Moving beyond Ku?era and Francis: A critical evaluation of current word fre-
quency norms and the introduction of a new and improved word frequency measure for American English. 
Behavior Research Methods 41 (4), 977?990. 
Burnard, L.; Aston, G. (1998): The BNC Handbook: Exploring the British National Corpus with Sara. Edin-
burgh: University Press.  
Church, K.W.; Hanks, P. (1990). Word association norms, mutual information, and lexicography. Computational 
Linguistics 16 (1), 22?29.  
Coltheart, M. (1981): The MRC psycholinguistic database. Quarterly Journal of Experimental Psychology, 33A, 
497?505.  
Denoyer, L.; Gallinari, P. (2006): The Wikipedia XML Corpus. SIGIR Forum, 40(1), 64?69.  
Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 
19 (1), 61?74.  
Fernald, J.C. (1896). English Synonyms and Antonyms, 19th edition. New York and London: Funk & Wagnalls 
Company. 
Francis, W.N.; Ku?era, H. (1989): Manual of Information to Accompany a Standard Corpus of Present-Day Ed-
ited American English, for Use with Digital Computers. Providence, R.I.: Brown University, Department of 
Linguistics.  
Francom, J.;  Ussishkin, A. (2011). Converging methodologies: assessing corpus representativeness through psy-
cholinguistic measures. American Association for Corpus Linguistics. Georgia State University, Atlanta, GA. 
http://francojc.files.wordpress.com/2010/01/aacl-2011-converging-methodologies.pdf. 
Gries, S.T. (2010). Dispersions and adjusted frequencies in corpora: further explorations. In S.T. Gries, S. Wulff, 
& M. Davies (eds.): Corpus Linguistic Applications: Current Studies, New Directions. Amsterdam: Rodopi, 
197?212.  
Harris, Z. S. (1954). Distributional structure. Word, 10(23), 146?162.  
Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J. (1973). An associative thesaurus of English and its computer 
analysis. In Aitken, A.J., Bailey, R.W. and Hamilton-Smith, N. (Eds.): The Computer and Literary Studies. 
Edinburgh: University Press, 153?165.   
Landauer, T.K.; Dumais, S.T. (1997). A solution to Plato's problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge. Psychological Review 104 (2), 211?240.  
                                                 
14
 http://wordnet.cs.princeton.edu/downloads.html 
2127
Papineni, K.; Roukos, S.; Ward, T.; Zhu, W.-J. (2002). BLEU: a method for automatic evaluation of machine 
translation. Proceedings of the 40th Annual Meeting of the ACL, Philadelphia, 311?318.  
Rapp, R. (2005): On the relationship between word frequency and word familiarity. In: B. Fisseni; H.-C. 
Schmitz; B. Schr?der; P. Wagner (eds.): Sprachtechnologie, mobile Kommunikation und linguistische Res-
sourcen. Beitr?ge zur GLDV-Tagung 2005 in Bonn. Frankfurt: Peter Lang. 249?263. 
Rapp, R. (2009). The automatic generation of thesauri of related words for English, French, German, and Rus-
sian. International Journal of Speech Technology 11 (3 ), 147?156.  
Rapp, R. (2011). Language acquisition as the detection, memorization, and reproduction of statistical regularities 
in perceived language. Journal of Cognitive Science, Vol. 12, No. 3, 297?322. 
Rapp, R. (2014a). Using word familiarities and word associations to measure corpus representativeness. Pro-
ceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2014), Reyk-
javik, Iceland. 
Rapp, R. (2014b). Corpus-based computation of reverse associations. Proceedings of the Ninth International 
Conference on Language Resources and Evaluation (LREC 2014), Reykjavik, Iceland. 
Rapp, R. (2014c). Using word association norms to measure corpus representativeness. In: A. Gelbukh: Compu-
tational Linguistics and Intelligent Text Processing. 15th International Conference, CICLING 2014, Kath-
mandu, Nepal. Berlin: Springer. 1?13. 
Saldanha, G. (2009): Principles of corpus linguistics and their application to translation studies research. Tradu-
m?tica 7: 1?7.  
Temnikova, I.; Baumgartner Jr., W.A.; Hailu, N.D.; Nikolova, I.; McEnery, T.; Kilgarriff, A.; Angelova, G.; 
Cohen, K.B. (2014). Sublanguage Corpus Analysis Toolkit: a tool for assessing the representativeness and 
sublanguage characteristics of corpora. Proceedings of the Ninth International Conference on Language Re-
sources and Evaluation (LREC 2014), Reykjavik, Iceland.  
Wettler, M., Rapp, R. (1989). A connectionist system to simulate lexical decisions in information retrieval. In: R. 
Pfeifer, Z. Schreter, F. Fogelman, L. Steels (eds.): Connectionism in Perspective. Amsterdam: Elsevier, 463?
469.  
Wettler, M.; Rapp, R.; Sedlmeier, P. (2005). Free word associations correspond to contiguities between words in 
texts. Journal of Quantitative Linguistics 12(2), 111?122.  
 
2128
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 102?109
Manchester, August 2008
The Computation of Associative Responses to Multiword Stimuli 
Reinhard Rapp Universitat Rovira i Virgili Plaza Imperial Tarraco, 1 43005 Tarragona, Spain 
reinhardrapp@gmx.de 
  
Abstract 
It is shown that the behaviour of test per-sons as observed in association experi-ments can be simulated statistically on the basis of the common occurrences of words in large text corpora, thereby ap-plying the law of association by contigu-ity which is well known from psycho-logical learning theory. In particular, the focus of this work is on the prediction of the word associations as obtained from subjects on presentation of multiword stimuli. Results are presented for applica-tions as diverse as crossword puzzle solv-ing and the identification of word transla-tions based on non-parallel texts. 
1 Introduction 
The idea that human memory functions associa-tively goes back to Aristotle who formulated that the sequence of our memories is determined by the concepts of similarity and proximity (Strube, 1984:34). As early as 1879, Francis Galton tried to systematically observe human associative be-haviour by introducing an association experi-ment. In this experiment, given a particular stimulus word, subjects had to respond with the first other word that occurred to them spontane-ously. The resulting tables of associative re-sponses are called association norms. To explain the behavior documented in the as-sociation norms, in the literature a multiplicity of different mechanisms underlying human memory are proposed, thereby, for example, assuming phonological, morphological, syntactical, seman-tical, and contextual relations between words                                                  ? 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http:// creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 
(Wettler, 1980). However, as yet there is no agreement whether these mechanisms should be considered of equal status, or if some may be derived from others.  Already in 1750 the physiologist David Hart-ley suggested that it may be possible to reduce the multiplicity of proposed association laws to only a single one based on temporal contiguity. This was formulated as one of the earliest psy-chological laws by William James (1890: 561): ?Objects once experienced together tend to be-come associated in the imagination, so that when any one of them is thought of, the others are likely to be thought of also, in the same order of sequence or coexistence as before. This state-ment we may name the law of mental association by contiguity.? Assuming that the ?objects? referred to in this law are words, the law of association by contigu-ity implies the following two phases:  1) Learning phase: When perceiving lan-guage, strong associative connections are developed between words that frequently occur in close temporal succession.  2) Retrieval phase: These associations deter-mine the words that come to mind during generation. Only words that are strongly in-terconnected or have strong associations to external stimuli can be uttered or written down.  Pre-supposing the validity of the law of associa-tion, it should be possible to derive free word associations from the distribution of words in texts. Following Church & Hanks (1990), Rapp (2004), and Wettler et al (2005) this actually seems to be successful. The recent simulation algorithms generate results which largely agree with the free word associations as found in the association norms. An example is shown in Ta-ble 1, where the observed and the simulated re-sponses to the stimulus word cold are compared. 
102
OBSERVED 
RESPONSE NUMBER OF SUBJECTS PREDICTED RESPONSE NUMBER OF SUBJECTS hot ice warm water freeze wet feet freezing nose room sneeze sore winter 
 34  10  7  5  3  3  2  2  2  2  2  2  2 
hot winter weather warm water heat ice wet wind temperature shiver freeze rain 
 34  2  0  7  5  1  10  3  0  0  0  3  0  Table 1: Observed and predicted associative responses to the stimulus word cold.  When judging these results it should be kept in mind that among subjects there is some variation of responses. Therefore, the simulation results can be considered satisfactory if the difference between the predicted and the observed answers is on average not larger than the difference be-tween an answer of an average test subject and the answers of the remaining test subjects.  In the current paper we try to build on these results. However, while most previous work con-sidered only associations to individual stimulus words, the question to be dealt with here is whether the associative responses to several stimuli can likewise be predicted from the co-occurrences of words in texts. This is of consid-erable interest as all utterances and texts can be considered as accumulations of stimulus words, which together lead to a systematic activation of other words and concepts in the mind of the lis-tener or reader. How uniform the reactions of test subjects can be upon presentation of several stimulus words can be seen from examples like the word pairs circus ? laugh or King ? girl where subjects tend to think of clown and princess, respectively. Starting from the association norms for individ-ual stimuli, the observed results are not always obvious. For example, in a large database of as-sociation norms, namely the Edinburgh Associa-tive Thesaurus (Kiss et al, 1973), among the re-sponses to King the word princess is completely missing, and the same is true for girl.  This means that the combination of stimulus words can lead to associations which are only weakly linked to the individual words and there-fore cannot easily be deduced from conventional association norms. Accordingly it is not obvious 
whether the method used for the simulation of the associative behavior to single words can be extended in a straightforward way in the case of several stimulus words. The organization of this paper is as follows: We first look at association norms collected for pairs of stimulus words. We then introduce a corpus-based algorithm that simulates the ob-served behavior which is applicable in the case of single or multiple stimuli. We then present some results of the algorithm and apply it to some related problems. 
2 Association norms for word pairs 
For individual English words, several association norms have been published, with the largest be-ing the Edinburgh Associative Thesaurus. How-ever, in the case of several stimulus words hardly any data seems to exist, with Rapp (1996, 1998) being an exception. This is a study that collected the responses of 31 subjects to pairs of German2 nouns. In compiling these association norms, a list of 10 common German nouns had been se-lected, namely M?dchen (girl), Krankheit (ill-ness), Junge (boy), Musik (music), B?rger (citi-zen), Erde (earth), Stra?e (street), K?nig (King), Freude (joy), Sorge (worry). Then all 90 possible pairs of these words were constructed, and the answers of the subjects upon presentation of these pairs were collected. The subjects were asked to come up with the first word spontane-ously coming to mind. In addition, associations to the individual words were also collected. As for the pairs it turned out that word order did not have a noticeable effect on the responses, the responses to pairs differing only in word or-der were merged.  In Table 2 the associative responses as given by the test subjects for two sample pairs of stimulus words are listed. In comparison to re-sponses to individual stimulus words, the re-sponses to pairs of words are generally less uni-form, i.e. there is considerably more variation in the case of word pairs. For example 25 of 31 test subjects come up with the association M?dchen (girl) given the stimulus word Junge (boy). In contrast, the most frequently mentioned associa-tive response upon presentation of the stimulus pair Junge M?dchen (boy girl), which is Kinder (children), is given by only seven test persons.  
                                                 2 As we are not aware of such data for English, the current study was conducted for German, with translations given throughout the paper. 
103
STIMU-
LUS PAIR ASSOCIATIVE RESPONSES Erde (earth)  Sorge (worry) 
Umwelt (environment) 8, Umweltver-schmutzung (environmental pollution) 5, Weltuntergang (end of the world) 2, ai (AI), Ausbeutung (exploitation), Katas-trophen (catastrophe), Klimakatastrophe (climatical catastrophe), Krieg (war), Luft (air), Macht (might), M?ll (gar-bage), Mutter (mother), Ozonloch (ozone hole), Resignation (resignation), ?berbev?lkerung (overpopulation), Umweltzerst?rung (destruction of the environment), unfruchtbar (infertile), Verschmutzung (pollution), Zerst?rung (destruction) K?nig  (King)  M?dchen (girl) 
Prinzessin (princess) 15, K?nigin (queen) 3, Tochter (daughter) 2, Ab-h?ngigkeit (dependency), Dienerin (maid), Hochzeit (wedding), Kinder-spiele (children?s games), Kitsch (kitsch), K?nigspaar (royal couple), M?rchen (fairy tale), Mi?brauch (abuse), Pferd (horse), Vater (father), Vorbild (model)  Table 2: Associations to the stimulus pairs ?Erde Sorge? (earth worry) and ?K?nig M?dchen? (King girl). Figures indicate the number of subjects with the respective response, with the default being one.  For a more exact quantitative analysis of this ob-servation a measure is needed for the homogene-ity of the answers. For this purpose, it was com-puted how many subjects gave the same answer to a particular stimulus pair. On average, this was the case for 4% of the subjects. In comparison, the corresponding value for individual stimulus words is 15%. Thus the impression of a substan-tially larger homogeneity of the associative an-swers for individual stimuli is confirmed. 
3 Simulation program 
The simulation is based on the detection of statis-tical regularities of the common occurrences be-tween the words in a large text corpus. As we did not have a large and at the same time balanced corpus of German at our disposal, we decided to use a corpus of the newspaper Frankfurter All-gemeine Zeitung (FAZ) comprising the years 1993 to 1996 (135 million words). As in the as-sociation experiment the subjects rarely answer with inflected forms or function words, for com-putational reasons we lemmatized this corpus (Lezius, Rapp & Wettler, 1998) and ? based on a list of stop words ? removed closed class words such as articles, pronouns, and particles. 
To determine word co-occurrences, for each word in the corpus it was counted how often its close neighbors occurred within a text window of plus and minus six words. Assuming that ap-proximately every second word is a function word, a window size of plus and minus six words after removal of the function words roughly cor-responds to a window size of plus and minus 12 words without such pre-processing. This is a window size that corresponds with what had been found appropriate for the computation of associations in other studies (e.g. Rapp, 2004). As the co-occurrence counts largely depend on overall word frequency, some association meas-ure needs to be applied to eliminate this unde-sired influence. Many previous studies have shown that the log-likelihood ratio is well suited for this purpose (Dunning, 1993). It successfully eliminates word-frequency effects and empha-sizes significant word pairs by comparing their observed co-occurrence counts with their ex-pected co-occurrence counts. It can be expected that the log-likelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment (Dunning, 1993), although there are other measures which come close in performance (e.g. Rapp, 1998). To compute the associations to pairs of stimu-lus words, it would in principle be possible to consider text positions where both stimulus words occur together, and to count the co-occurrence frequencies with their neighboring words. This would result in a three-dimensional association matrix whose first two dimensions correspond to the two stimulus words and whose third dimension corresponds to their associations. However, the problem of data sparseness would be very severe with such an approach, and it would not scale well if more than two stimulus words were considered. We therefore propose another approach, which to our knowledge is novel in this context: The idea is that a potential associative response to a pair of stimulus words should have a strong and preferably symmetric associative connection to each of the stimulus words, and that a strong as-sociation to only one of them does not suffice. Such a behavior can usually be ensured by a mul-tiplication. However, we do not multiply the association strengths, as the log-likelihood ratio has an inap-propriate (exponential) value characteristic. This value characteristic has the effect that a weak association to one of the stimuli can easily be overcompensated by a very strong association to 
104
the other stimulus, which is not desirable. Instead of multiplying the association strengths, we therefore suggest to multiply their ranks. This improves the results considerably. These considerations lead us to the following procedure: Given an association matrix of vo-cabulary V containing the log-likelihood ratios between all possible pairs of words, to compute the associative response given words a and b, the following steps are conducted:  1) For each word in V (by applying a search-and-compare operation on the association matrix) look up the ranks of words a and b in its list of associations, and compute the prod-uct of these ranks.  2) Sort the words in V according to these prod-ucts, with the sort order such that the lowest value obtains the top rank (i.e. conduct a re-verse sort).  Note that this procedure is somewhat time con-suming as computations are required for each word in a large vocabulary.3 On the plus side, the procedure is applicable to any number of stimu-lus words, and with increasing number of stimuli there is only a moderate increase in computa-tional requirements. (The application presented in section 5.2 successfully processes 30 stimulus words.) A minor issue is the assignment of ranks to words that have identical log-likelihood scores, especially in the frequent case of zero co-occurrence counts. In such cases, the assignment of possibly almost arbitrary ranks could ad-versely affect the results. We therefore suggest assigning corrected ranks, which are to be chosen as the average ranks of all words with identical scores. With large numbers of stimuli, depending on the application it can be helpful to introduce a limit to the maximum rank, thereby reducing the effects of the sparse-data problem. The benefit of this measure is similar to smoothing, but more sophisticated smoothing methods can of course also be considered (as described, e.g. in Church & Gale, 1991). Note that for the current work we only used a rank limit of 10,000, but did not ap-ply any sophisticated smoothing as this usually has little impact if the focus is mainly on the top ranks, as is the case here. 
                                                 3 Considerable time savings are possible by using an index of the non-zero co-occurrences. 
4 Results 
The algorithm as described above was applied to the FAZ corpus. That is, based on a window size of plus and minus six words, an association ma-trix with log-likelihood scores and (in both rows and columns) comprising all words with a corpus frequency of 200 or higher was computed. For each of the 45 word pairs, the top associations as resulting from the product of ranks were com-puted. To give some examples, the following tables show the outcome for a few stimulus pairs. Hereby, the columns in the tables have the fol-lowing meanings:  1) rank 2) corpus frequency of association 3) score (product of stimulus ranks) 4) association  Junge M?dchen (boy girl)  1 247 11.33 f?nfzehnj?hrig (15 year old) 2 2960 9.81 dreizehn (13) 3 398 9.72 gleichaltrig (same age) 4 86559 9.72 alt (old) 5 850 9.66 blond (blond)  B?rger M?dchen (citizen girl)  1 1276 11.51 brav (well behaved) 2 1268 7.26 unschuldig (innocent) 3 223 6.73 ver?ngstigt (scared) 4 979 6.41 anvertrauen (to intrust) 5 362 5.97 bel?stigen (to molest)  Stra?e M?dchen (street girl)  1 2509 7.50 tanzen (to dance) 2 242 7.12 pflastern (to pave) 3 272 6.96 B?rgersteig (sidewalk) 4 529 6.87 Prostitution (prostitution) 5 4367 6.76 begegnen (to encounter)  Sorge M?dchen (worry girl)  1 317 7.03 elterlich (parental) 2 210 6.62 Burschen (fellows) 3 222 6.23 Beschneidung (concision) 4 7508 5.81 Eltern (parents) 5 271 5.77 zw?lfj?hrig (12 year old)  Junge Krankheit (boy illness)  1 8891 7.33 leiden (to suffer) 2 3553 7.14 t?dlich (lethal) 3 16468 7.04 sterben (to die) 4 423 6.83 Heilung (cure) 5 261 6.62 Schizophrenie (schizo-phrenia) 
105
Stra?e Krankheit (street illness)  1 308 6.94 Tuberkulose (tuberculosis) 2 4704 6.74 Unfall (accident) 3 276 6.71 t?ckisch (malicious) 4 232 6.34 heimt?ckisch (malignant) 5 620 6.07 anstecken (to infect)  Stra?e B?rger (street citizen)  1 272 7.21 B?rgersteig (sidewalk) 2 235 7.18 Gibraltar (Gibraltar) 3 207 7.09 flanieren (to stroll) 4 242 7.02 pflastern (to pave) 5 366 6.58 Fu?g?ngerzone (pedestri-an zone)  Sorge Freude  1 6331 1.11 bereiten (to cause) 2 8747 9.21 Anla? (occasion) 3 950 8.54 ?berwiegen (to outweigh) 4 27136 7.54 Grund (reason) 5 248 7.21 ungetr?bt (untroubled)  If we look at all 45 word pairs, we obtain the fol-lowing evaluation: Whereas an associative an-swer given by a subject is on average also given by 4% of the other subjects, only about 0.8% of the subjects give the answer produced in the simulation, i.e. the word ending up on the top rank. However, due to the low number of cases, this value may be subject to some sampling error.  A method less sensitive to sampling errors is to look at the overall simulation ranks of the sub-jects? responses. Hereby it is better to consider the median of the ranks rather than the mean, as the median?s treatment of outliers is more appro-priate. Note that when computing the median, associative responses given by n subjects obtain an n-fold higher weight. To further reduce the effects of outliers, only responses that are given by at least two subjects are taken into account. Under these assumptions, the overall median (computed over all stimulus pairs) has a value of 245. With the total vocabulary of corpus fre-quency 200 and higher comprising about 25000 words, this value is at the 1% level. This com-pares to 12500 at the 50% level, which could be expected in the case of random behaviour. 
5 Applications 
5.1 Crossword puzzle solver 
As crossword puzzles have definitions which usually consist of several words, the proposed algorithm can be applied as a crossword puzzle solver. In order not to reduce this task to a (for 
computers) relatively simple combinatorial prob-lem, we hereby only restrict the ranked list of words as produced by the simulation program to those words that have the correct number of characters, but do not utilize as clues the com-mon characters of horizontal and vertical words.  As an example, Figure 1 shows a crossword puzzle which is attributed to be the world?s first one. It was designed by Arthur Wynne and pub-lished on December 21, 1913 in The New York World. Table 3 shows the definitions of this crossword puzzle together with the supposed so-lutions and the ranks of the respective words as computed by our algorithm based on three differ-ent corpora, namely the British National Corpus (BNC), the years 1990 to 1994 of the newspaper The Guardian, and the English part of the Wikipedia XML Corpus (Denoyer & Gallinari, 2006). These three corpora have a size of roughly 100, 150, and 300 million words, respectively. To allow a better judgment of the simulation re-sults, the number of words of the respective length in the underlying vocabulary is specified in column 5. This vocabulary was chosen to consist of all words that have a corpus frequency of 100 or higher in the BNC but did not occur in our list of about 200 function words. To this vocabulary, all words occurring in the crossword puzzle were added. The purpose of limiting the vocabulary was solely for computational reasons, as our al-gorithm is rather demanding with regard to both execution time and memory requirements. Note that the BNC-based vocabulary was also used for the other somewhat larger corpora as not many words were missing there: In the Guardian corpus of the altogether 34,448 words all but 390 occurred at least one time, and in the larger Wikipedia corpus all but 131. We did not lemma-tize the English corpora as in several cases in-flected forms of words occurred in the definitions or in the solutions of the crossword puzzle.  
 Figure 1: Crossword puzzle by Arthur Wynne. 
106
As described in section 2, for counting the co-occurrences of words a window of plus and mi-nus six words around a given word was consid-ered, and for the computation of the associative strengths the log-likelihood ratio was used. Stop words were also removed from the corpora be-forehand, but no lemmatization was conducted. As many of the words used in the crossword puzzle are rare and several are outdated, solving this problem by a simulation is a non-trivial task. Nevertheless, for the Wikipedia corpus the algo-rithm got 8 of 31 answers ranked among the top five. When inspecting the examples that the algo-rithm got wrong, it appears that these are often the ones where humans would also have difficul-ties. For example, the solution ?side? for ?to agree with? got consistently poor ranks with all   
three corpora. On the other hand, rather surpris-ingly, the solution for ?such and nothing more?, namely ?mere?, received top rankings despite the fact that there are no salient content words in the description. This may be an indication that the algorithm grasps something that is related to cognitive processes. However, a similar example, namely ?what we all should be? (? moral) only obtains a reasonable ranking with the Wikipedia corpus. According to the average rankings (bot-tom line of Table 2), this corpus seems to be bet-ter suited for this task than the other two corpora. 
5.2 Identifying word translations 
The proposed core algorithm also has applica-tions that may come somewhat unexpectedly. What we suggest here is to identify word transla-  
POS. DEFINITION SOLU-TION LENGTH WORDS OF THIS LENGTH RANK BNC RANK GUARDIAN RANK WIKIPEDIA 
2-3 what bargain hunters enjoy sales 5 4254 1014 70 338 4-5 a written acknowledgement receipt 7 5371 2 44 355 6-7 such and nothing more mere 4 2916 16 17 4 10-11 a bird dove 4 2916 17 87 4 14-15 opposed to less more 4 2916 42 34 5 18-19 what this puzzle is hard 4 2916 1486 115 384 22-23 an animal of prey lion 4 2916 84 16 324 26-27 the close of a day evening 7 5371 603 494 185 28-29 to elude evade 5 4254 80 64 38 30-31 the plural of is are 3 1424 238 119 412 8-9 to cultivate farm 4 2916 2316 2783 1070 12-13 a bar of wood or iron rail 4 2916 1658 1419 925 16-17 what artists learn to do draw 4 2916 227 1437 86 20-21 fastened tied 4 2916 15 2335 2078 24-25 found on the seashore sand 4 2916 124 19 757 10-18 the fibre of the gomuti palm doh 3 1424 585 279 711 6-22 what we all should be moral 5 4254 4107 1163 51 4-26 a day dream reverie 6 5371 489 572 2 2-11 a talon sere 4 2916 676 803 492 19-28 a pigeon dove 4 2916 36 8 1 F-7 part of your head face 4 2916 63 20 143 23-30 a river in Russia Neva 4 2916 174 413 3 1-32 to govern rule 4 2916 48 9 13 33-34 an aromatic plant nard 4 2916 616 2753 393 N-8 a fist neif 4 2916 --- --- --- 24-31 to agree with side 4 2916 2836 2393 1387 3-12 part of a ship spar 4 2916 2693 1932 90 20-29 one tane 4 2916 2814 2773 2680 5-27 exchanging trading 7 5371 3444 5216 2347 9-25 sunk in mud mired 5 4254 3 2 1 13-21 a boy lad 3 1424 3 2 2 
AVERAGE RANK 891.6 922.3 520.2  Table 2: Crossword puzzle definitions and the computed ranks of their solutions based on three corpora. (?---? means that a solution does not occur in a corpus (not taken into account when computing average ranks). 
107
tions from monolingual English and German cor-pora, i.e. from corpora that are not translations of each other (Rapp, 1999). This is a rather difficult task.  As our textual basis, for German we use the FAZ corpus as described above, with exactly the same pre-processing. For English we use a simi-larly sized corpus of the newspaper ?The Guard-ian?, with analogous pre-processing.  We apply a two-stage procedure to compute the translation of a source language word: First, by considering the log-likelihood ratios, its strongest source language associations are de-termined and translated to the target language using a small pocket dictionary. Hereby, associa-tions that are missing in the dictionary are dis-carded, and of the remaining associations only the top 30 are selected. The second step exactly corresponds to the computation of associations when given multiple stimulus words as described above. That is, for each word in the target language vocabulary (comprising all words that in the Guardian cor-pus occur with a frequency of 100 or higher) the ranks of the 30 translations are determined, and the product of these ranks is computed. The word obtaining the smallest value for the product is considered to be the translation of the source language word. This algorithm turned out to be a significant improvement over the previous algo-rithm described in Rapp (1999) as it provides a better accuracy and at the same time a considera-bly higher robustness.  Based on this novel algorithm, a large diction-ary for German to English was computed. As for the translation of the source language vectors a base dictionary is required, we adapted for this purpose a small Collins pocket dictionary which comprised in the order of 20 000 entries. In es-sence, the adaptation procedure involves deriving word equations from the dictionary, each consist-ing of the source word and its first translation as mentioned in the dictionary.  To give an impression of the results, the fol-lowing tables show the top ten computed transla-tions for the six words Historie (history), Leib-w?chter (bodyguard), Raumf?hre (space shuttle), spirituell (spiritual), ukrainisch (Ukranian), and umdenken (rethink). Hereby, the columns have the following meanings:   1) Rank of a potential translation 2) Corpus frequency of translation 3) Score assigned to translation  4) Computed translation 
Historie (history)  1 29453 13.73 history 2 4997 12.87 literature 3 4758 8.74 historical 4 2670 0.67 essay 5 6969 0.11 contemporary 6 18909 -1.72 art 7 18382 -2.81 modern 8 15728 -4.31 writing 9 1447 -5.52 photography 10 2442 -5.53 narrative  Leibw?chter (body guard)  1 949 40.02 bodyguard 2 5619 23.34 policeman 3 2535 8.18 gunman 4 26347 3.69 kill 5 9180 2.92 guard 6 401 -0.56 bystander 7 815 -1.24 POLICE 8 8503 -2.33 injured 9 2973 -3.23 stab 10 1876 -3.58 murderer  Raumf?hre (space shuttle)  1 1259 46.20 shuttle 2 666 26.25 Nasa 3 473 25.95 astronaut 4 287 25.76 spacecraft 5 1062 16.92 orbit 6 16086 11.72 space 7 525 9.50 manned 8 125 7.69 cosmonaut 9 254 5.24 mir 10 7080 3.70 plane  spirituell (spritual)  1 2964 56.10 spiritual 2 1380 8.34 Christianity 3 7721 8.08 religious 4 9525 4.10 moral 5 1414 0.63 secular 6 5685 0.06 emotional 7 4678 -1.04 religion 8 6447 -1.49 intellectual 9 8749 -2.25 belief 10 8863 -4.07 cultural  ukrainisch (Ukrainian)  1 1753 50.69 Ukrainian 2 22626 39.88 Russian 3 3205 29.25 Ukraine 4 34572 23.63 Soviet 
108
5 978 21.13 Lithuanian 6 1005 18.88 Kiev 7 10968 15.07 Gorbachev 8 10209 14.51 Yeltsin 9 16616 13.38 republic 10 502 11.71 Latvian  umdenken (rethink)  1 1119 20.76 rethink 2 248 15.46 reassessment 3 84109 13.39 change 4 12497 12.13 reform 5 236 10.00 reappraisal 6 9220 9.97 improvement 7 5212 9.48 implement 8 1139 8.25 overhaul 9 13550 7.89 unless 10 9807 7.88 immediate 
 
6 Summary 
It could be shown that word associations to mul-tiple stimuli as collected from test persons can be predicted with reasonable accuracy using a simu-lation program that analyzes the co-occurrences of words in texts.  This result makes the automatic construction of an associative thesaurus of responses to multiple stimuli feasible. Note that such a thesaurus could not realistically be compiled by collecting the responses of human subjects as there are too many possible combinations of stimuli. Finally, by looking at two sample applications we showed the pracical utility of the method. In principle, there should be many more applica-tions, as all utterances and texts can be con-sidered as collections of stimulus words. A notable one is search word generation in the con-text of internet search engines.  Of course, all existing algorithms for speech and text processing, although often not claiming any cognitive plausibility, necessarily also have some implicit mechanisms that deal with multi-word stimuli. We nevertheless hope that the specific perspective that we presented here may add to a better understanding of the underlying cognitive mechanisms, and that it offers a systematic way of approaching these challenges. 
7 Acknowledgments 
This research was in part supported by a Marie Curie Intra European Fellowship within the 6th European Community Framework Programme. 
References 
Church, Kenneth W.; Gale, William (1991). A com-parison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5(1), 19?54. Church, Kenneth W., Hanks, Patrick (1990). Word association norms, mutual information, and lexico-graphy. Computational Linguistics, 16(1), 22?29. Denoyer, Ludovic; Gallinari, Pattrick (2006). The Wikipedia XML Corpus. SIGIR Forum, 40(1), 64?69. Dunning, Ted (1993). Accurate methods for the sta-tistics of surprise and coincidence. Computational Linguistics, 19(1), 61?74.  Galton, Francis. (1879). Psychometric experiments. Brain, 1, 149?162. James, William (1890). The Principles of Psychology. New York: Dover Publications. Kiss, George R.; Armstrong, Christine; Milroy, Robert; Piper, James (1973). An associative the-saurus of English and its computer analysis. In: A. Aitken, R. Beiley, N. Hamilton-Smith (eds.): The Computer and Literary Studies. Edinburgh Univer-sity Press.  Lezius, Wolfgan; Rapp, Reinhard; Wettler, Mafred (1998). A freely available morphological analyzer, disambiguator, and context sensitive lemmatizer for German. Proceedings of COLING ACL 1998, Montreal, 743?747. Rapp, Reinhard (1996). Die Berechnung von Assozia-tionen: ein korpuslinguistischer Ansatz. Hilde-sheim: Olms.  Rapp, Reinhard (1998). Das Kontiguit?tsprinzip und die Simulation des Assoziierens auf mehrere Sti-mulusw?rter. In: B. Schr?der, W. Lenders, W. Hess, T. Portele: Computer, Linguistik und Phone-tik zwischen Sprache und Sprechen. Frankfurt am Main: Peter Lang, 261?272. Rapp, Reinhard (1999). Automatic identification of word translations from unrelated English and Ger-man corpora. In: Proceedings of the 37th Annual Meeting of the ACL, College Park, MD, 519?526.  Rapp, Reinhard (2004). Word Sense Induction as Sta-tistical Pattern Recognition. In: Ernst Buch-berger (ed.): Tagungsband der 7. Konferenz zur Verarbeitung nat?rlicher Sprache (KONVENS), Universit?t Wien, 161?168. Strube, Gerhard (1984). Assoziation. Berlin: Springer. Wettler, Manfred (1980). Sprache, Ged?chtnis, Ver-stehen. Berlin: de Gruyter. Wettler, Manfred; Rapp, Reinhard; Sedlmeier, Peter (2005). Free word associations correspond to con-tiguities between words in texts. Journal of Quanti-tative Linguistics, 12(2), 111?122. 
109
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 50?59,
Beijing, August 2010
Utilizing Citations of Foreign Words in Corpus-Based Dictionary Generation 
Reinhard Rapp University of Tarragona GRLMC 
reinhardrapp@gmx.de 
Michael Zock Laboratoire d?Informatique Fondamentale CNRS Marseille 
Michael.Zock@lif.univ-mrs.fr 
Abstract 
Previous work concerned with the identi-fication of word translations from text collections has been either based on par-allel or on comparable corpora of the re-spective languages. In the case of compa-rable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single mono-lingual corpus without necessarily requir-ing dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard news-ticker texts, we will show that their co-occurrence-based associations can be successfully used to identify word trans-lations. 
1 Introduction 
The web has popularized information access. As a consequence, the information put on the web evolved, expanding from mainly technical documents in one language (English) to topics concerning nearly any aspect of life in many lan-guages. For this reason it cannot be expected anymore that all web users speak English. Yet users speaking only one of the minority lan-guages will be penalized, finding only a small fraction of web content accessible. Hence they can make only very limited use of what is avail-able. In order to increase information access in-
dependently of the users? mother tongue, auto-matic translation is desirable. Recognizing this need, Google, among others, is providing free machine translation services for any pair of currently 50 languages. 1  However, with 6800 living languages, of which 600 also use a written form, offering comprehensive trans-lation services remains a challenge. The statistical approach to machine trans-lation (SMT), as adopted by Google, relies on parallel corpora, i.e. large collections of existing translations. But it is a daunting task trying to acquire parallel corpora for all possible language pairs. Therefore, it appears that for some lan-guages Google has combined SMT with an inter-lingua approach. This allows optimal exploita-tion of languages for which parallel corpora are easily obtained. These languages are then used as pivots. Note that in phrase-based SMT an inter-lingua approach may operate at the level of the phrase table, which facilitates matters while speeding up the process. At the downside it must be noted that a phrase table derived via a pivot language is generally of lower quality than a phrase table directly compiled from parallel texts (provided the corpus size is similar). Hence, just as for other interlingua approaches, translation quality is severely compromised.  An alternative approach that has been sug-gested is to try to generate the required dictionar-ies from other sources than parallel corpora. Bear in mind that statistical machine translation re-quires a language model and a translation model. To generate the language model only monolin-gual corpora of the target language are required which, for example, can be acquired from the web. If only few such documents exist, one may well conclude that there is probably no real need                                                  1 http://www.google.de/language_tools?hl=de as of April 22, 2010. 
50
for translation involving this particular language. So the main bottleneck are the parallel corpora required to generate a translation model. But the purpose of the translation model is in essence the creation of a bilingual dictionary, be it a diction-ary of individual words or a dictionary of phra-ses. For this reason, if we can find other ways to generate dictionaries for lesser used languages, this will be beneficial not only for the users of these languages but also for the solution of the overall problem of machine translation.  In other words, an important challenge is the generation of dictionaries. Since comparable cor-pora are a far more common resource than paral-lel corpora, attempts to exploit them for diction-ary construction have received considerable at-tention recently.2 One approach is to mine parallel sentences from comparable corpora. Roughly speaking, this can be done by automatically translating a corpus from one language (source language) to another (target language), and then searching in a large corpus of the target language for sentences simi-lar to the translations. The advantage of this pro-cedure is that the sentences retrieved this way are correct sentences as they were produced by hu-mans, whereas the sentences translated by a ma-chine tend to be garbled and of lower quality. However, the big problem with this approach is to ensure that the retrieved sentence pairs are indeed translations of each other. While there is no perfect solution to this problem, several stud-ies have shown that such data can be useful for building or supplementing translation models in SMT (see e. g. Munteanu & Marcu, 2005; Wu & Fung, 2005).  Another approach for exploiting comparable corpora in dictionary generation is based on the observation that word co-occurrence patterns between languages tend to be similar (Fung & McKeown, 1997; Rapp, 1995; Chiao et al, 2004). If, for example, two words X and Y co-occur more often than expected by chance in a corpus of language A, then their translated equi-
                                                 2 There is also the approach of identifying orthograph-ically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closely related languages but appears to have lim-ited scope otherwise. For this reason we will not fur-ther discuss it here. 
valents should also co-occur more frequently than expected in a corpus of language B. A great number of variants of this approach has been proposed, e.g. emphasizing aspects of corpus selection or expanding it to collocations or short phrases (Babych et al, 2007).  What is common to these studies is that they consider the source and the target language as two distinct semantic spaces, without any links at the beginning. Therefore, in order to connect the two, a base dictionary is required, and the pur-pose of the system is to expand this base diction-ary. Building a dictionary from scratch is not possible this way or at least computationally un-feasible (see Rapp, 1995). Whether the assumption of two completely distinct semantic spaces is realistic remains an open issue. Are separate lexical networks really a reasonable model for the processing of different languages by people?  One could say this is a plausible model, as-suming a person lived for some years in one country, and then for some more years in another country, assuming further that this person never looked at a dictionary or another multilingual document and never communicated with a per-son mixing both languages. It is known that this can work. The reason is probably the following: Many words of the basic dictionary assumed above correspond to items of the physical world. These items generally have names in natural languages which can serve as mediators. That the extrapolation to more ab-stract notions is possible has been claimed by Rapp (1999). Still, although persons proceeding this way can easily understand and, after some years, even think in each of the two languages, experience shows that they tend to have some difficulties when making translations, especially literal translations. So, although the above scenario is possible, we do not think that it is a typical one for our modern times. There are certainly good reasons why there are so many language courses, and why there is such an abundance of dictionaries. It is a matter of commonsense that the person try-ing to acquire a new language will look at a mul-tilingual dictionary. He or she will also commu-nicate with other persons who mix languages, for example, relatives, other people from the com-
51
munity of foreigners coming from the same country, teachers in language classes, etc. In many cases there will also be multilingual docu-ments around: leaflets, explanations in a mu-seum, or signs in a public area (e.g. airport). Hence the spoken and written ?corpus? (in-put) on which such a person?s language acquisi-tion process is based is not solely monolingual. While the corpus may be mainly monolingual, it surely will contain some multilingual elements. If we agree on this, our next step could be to acquire transcripts of language teaching classes with bilingual teachers and try to exploit these for dictionary generation. Since obtaining such transcripts in large enough quantities should be much more difficult than obtaining parallel cor-pora, this approach will probably not solve the data acquisition bottleneck which is the practical problem we were about to solve in the first place. The current study is therefore based on news-ticker texts which is a text type very similar to standard newspaper texts. At least for some lan-guages it is available in large quantities. How-ever, this type of text is probably not ideally suited for our purpose. Surprisingly, the reason is that newsticker and newspaper texts tend to be very well edited. This means that the author will typically avoid foreign words, and if ever some remain the respective passages are likely be re-phrased in order to make sure that the text uses familiar vocabulary, easily understandable by the readers. However, this is problematic for our ap-proach which is based on the occurrences of for-eign words in a monolingual text. So this is one of the rare cases where noisy corpora should yield better results than perfectly clean data. On the other hand, as this study suggests a (to our knowledge) novel approach, we consider it important to use a corpus that is generally known and available, and which has not been compiled with this particular purpose in mind. Only this way our results can convincingly give an idea concerning the baseline performance of the sug-gested algorithm. At this stage we consider this more important than optimizing results by com-piling corpora specifically suited for the purpose, even though this will be a logical next step. 
2 Approach and Language Resources 
Starting from the observation that monolingual dictionaries typically include a large number of 
foreign words, we consider the most significant co-occurrences of them as potential translation candidates. This implies that the underlying cor-pus corresponds to the target language, and that it can be utilized for any source language for which it contains a sufficient number of word citations. As this paper is written in English, we chose an English corpus as this should make judging our results convenient for most readers. However, being the world?s most widely spoken language, English tends to be rather self-contained in com-parison to other languages, which may use for-eign words more frequently. In particular, as a side effect of globalization, the use of English terminology is popular in many other languages. Therefore, in order to identify, for example, German?English word translations, it is better to look at occurrences of English words in a Ger-man corpus rather than at occurrences of German words in an English corpus.3 Nevertheless, the corpus we use here is the latest release of the English Gigaword Corpus (Fourth Edition) provided by the Linguistic Data Consortium (Parker et al, 2009). It consists of newswire texts of the time between 1995 and 2008 from the following news agencies:  
? Agence France-Presse, English Service  
? Associated Press Worldstream, English Ser-vice  
? Central News Agency of Taiwan, English Service 
? Los Angeles Times/Washington Post News-wire Service  
? New York Times Newswire Service 
? Xinhua News Agency, English Service  Altogether, the corpus comprises about 3 billion words. Since we are not interested in the transla-tion of function words, and in order to reduce the computational load, we removed all function words that were included in a stop word list for English comprising about 200 items. The stop words had been manually selected from a corpus-derived list of high frequency words. In the resulting corpus associations between words need to be identified, something that is usually done on the basis of co-occurrences. In 
                                                 3 Note that the results of both directions may be com-bined. This is something we leave for future work. 
52
order to count the co-occurrences between pairs of words, a text window comprising the ten words preceding and following a given foreign word is considered. On the resulting co-occur-rence counts a standard association metric like the log-likelihood ratio (Dunning, 1993) is ap-plied.   Note that the above mentioned window size of ?10 words from the given word relates to the preprocessed corpus from which function words have already been removed. Since in English roughly every second word tends to be a function word, the effective window size is about ?20 words. This window size is somewhat larger than what we typically find in other studies. However, the reason for this is quite obvious: As citations of foreign words are rare, we have a severe prob-lem of data sparseness, and by looking at a rela-tively large window we try to somewhat com-pensate for this.4  Despite its simplicity, this procedure of com-puting associations to foreign words already works well for identifying word translations. We simply assume that the strongest association is the best translation. We used this approach for words from three languages: French, German, and Spanish. The results are presented in the next section. In order to measure the quality of our results, for all source words of a language we counted the number of times where the expected English target word obtained the highest associa-tion score. As our gold standard for evaluation we used an existing list of translations as described in Rapp & Zock (2010), i.e. a resource that had not been compiled with the current application in mind. The data consists of 1079 word equations in three languages: English, French, and German. It has been extracted from the respective editions of the Collins GEM dictionaries, whereby when looking up a word only the first entry in the list of possible translations was taken into account. As in the current study we are also interested in Spanish, we manually looked up the main trans-
                                                 4  In preliminary experiments we also experimented with other window sizes. However, as we noticed that changes within a reasonable range of e.g. 5 to 20 words have only little effect, we do not consider them here. 
lations at the leo.dict.org website5 and added an-other column to this resource. Table 1 shows a few sample entries of the resulting list of word equations which were used for evaluating our approach. We should mention that the term word equa-tion is a bit problematic, as most words tend to be ambiguous, and ambiguities tend to vary with language. For this reason, we should, at least in principle, disambiguate all words in our corpus and map them to unambiguous concepts. Next we should use a gold standard using such con-cepts rather than words. Unfortunately, the cur-rent state of the art does not allow doing this with sufficient accuracy. Anyhow, addressing this problem is well beyond the scope of this paper.    
SOURCE LANGUAGES TARGET LANG. 
FRENCH GERMAN SPANISH ENGLISH 
britannique britisch brit?nico British 
P?ques Ostern Pascua Easter 
capable f?hig capaz able 
accent Akzent acento accent 
accident Unfall accidente accident 
accord?on Akkordeon acorde?n accordion 
acide S?ure ?cido acid 
gland Eichel bellota acorn 
action Handlung acci?n action 
avantage Vorteil ventaja advantage  Table 1. Some sample entries from the gold standard of word equations.  So far, for identifying the translations of the 1079 French words, we assumed the following ap-proach: We first computed their associations and then conducted an evaluation by checking for how many words the top association was identi-cal to the English translation found in the gold standard. The same approach was also used for the other languages, namely German and French. Hence, the three source languages were treated completely independently of each other. 
                                                 5 This is a manually edited high quality online diction-ary. Although it can be used for free, in our view for many purposes is as good as or even better than con-ventional printed dictionaries. 
53
However, there are several problems with this approach, in particular:  a) Several correct translations b) Data sparseness c) Homograph trap  Let us discuss these issues point by point.  a)  Several correct translations  Suppose we tried to identify the translation of the German word Stra?e and our gold standard listed street as the correct translation. If, however, our system produced road this would be considered just as much of an error as if it had produced a very remote word such as volcano. Hence, con-sidering only a single word as being correct, which is the consequence of using as gold stan-dard the resource exemplified in Table 1, implies that performance figures are artificially low, giv-ing us only the lower bound of the true perform-ance.  Despite this shortcoming, we will neverthe-less do so for the following reasons: 1) This is a pilot study presenting a new approach. For this reason, clarity has priority over performance. 2) The number of translations listed in a dictionary typically depends on the size of the resource. Hence, there is no absolute difference between correct and incorrect translations. Rather, we need to set a threshold somewhere, and truncat-ing after the first word listed is arguably the clearest and simplest way of doing so. 3) This is the main reason. We want to extend our approach to the multilingual case by (simultaneously) looking at several source languages. Given the fact that each language tends to have its own (i.e. idiosyncratic) ambiguities, we are already satis-fied if words from the various source languages have the same main translation. That all possible translations are identical is very unlikely.   b)  Data sparseness  What will happen if a source word does not oc-cur at all in the corpus, or only once or twice? We mentioned already that an appropriate choice of text genre, corpus size, and window size can somewhat reduce the problem of data sparseness. We also mentioned that by reversing source and target languages we can look at the problem from 
two perspectives, which may yield further im-provement. Nevertheless, these suggestions are limited in scope. Hence, given the nature of our approach, data sparseness will remain the core problem.  Fortunately, there is another possibility which is more promising than the ones mentioned above, provided that we manage to solve the am-biguity problem. The solution consists in consid-ering several source languages concurrently. Suppose that rather than starting from scratch we use existing dictionaries for various languages.6 In this case we can easily generate word equa-tions such as the ones shown in Table 1. We do this by considering as a single item all words appearing in a given row (excluding the target language word), and by computing the associa-tions to this aggregated artificial unit. (This is a simplified proposal. We shall see later how to improve it.) If, for example, we have 10 source languages, then it does not matter that 8 source words do not occur in the corpus, as long as the other two are well represented.   c)  The homograph trap  By this we mean that a word form from the source language also exists in the target lan-guage, but with a different meaning. For exam-ple, let us assume that we wanted to translate the word can (house) from Catalan to English. Sup-pose further that we are lucky and have ten Cata-lan citations with this word in our English cor-pus. But this will not help us because the word can happens to also belong to English, meaning something completely different. Moreover, can is a high frequency word, occurring millions of times in a large corpus. Of course, if we had a perfect word sense disambiguator, we could separate the Catalan and the English occurrences of can, thereby solving the problem.7 Unfortu-nately, existing tools are not powerful enough to do the job. What is worse, such collisions are not 
                                                 6 Which, for example, by using open source tools such as Moses and Giza++ (see www.statmt.org) can be easily generated from parallel corpora, e.g. from the Europarl corpus (Koehn, 2005) or the JRC Acquis corpus (Steinberger et al, 2006). 7 If we assume that foreign words typically occur in clusters, we could also use language identification software. 
54
uncommon between languages using the same script. So what can we do? Our suggestion is ex-actly the same as above for the problem of data sparseness, i.e. to look at several source lan-guages in parallel. But it is clear that collapsing all source words into a single item does not work. If only one of them happens to be also a common word in the target language, it is very likely that its co-occur-rences will override the co-occurrences of the foreign words we are interested in. So there is little chance to come up with a correct result. We propose a relatively simple solution to this problem, which possibly may well be novel in this context. Let us develop the idea.  In preliminary experiments we have tried sev-eral possibilities. Collapsing the source words would be equivalent to adding the respective co-occurrence vectors. This is apparently not ade-quate because, as mentioned above, the vector of a very frequent word would dominate all others. An alternative would be to sum up the associa-tion vectors. By the term association vector we mean the co-occurrence vectors after application of an association measure (in our case the log-likelihood ratio). It turns out that this somewhat reduces the problem without solving it entirely. Another possibility would be vector multiplica-tion. Multiplication is considerably better than addition as a property of multiplication is that moderate but coinciding support for a particular target word from several source words leads to a higher product than strong support by only a few. This is a highly desirable property as it helps us avoiding the homograph trap, and because all values are subject to considerable sampling er-rors. Unfortunately, there is yet another problem. Our association measure of choice, namely the log-likelihood ratio, as typical for ratios, has a skewed value characteristic. Since otherwise our previous experiences with the log-likelihood ra-tio are very good,8 and since it seems reasonably well suited for sparse data (Dunning, 1993), we suggest to multiply log-likelihood ranks rather than log-likelihood scores. This proposal is based on the observation (Dunning, 1993) that rankings of association strengths as produced by the log-                                                 8  To the best of our knowledge no other measure could consistently beat it over a wide range of NLP applications. 
likelihood ratio tend to be highly accurate even at higher ranks. Let us call this procedure the prod-uct-of-rank algorithm This algorithm works as follows: Starting from a vocabulary of target language words (which are the translation candidates), for each of these words an association vector is computed. Next, for each association vector the ranks of all words in the source language word tuple under consideration are determined. Hence, if we have three languages (e.g. English, French and Ger-man) we would get three values. These values are multiplied with each other, and finally all target language words are sorted according to the resulting products. As small ranks stand for strong associations, the word obtaining the smal-lest value is considered to be the translation of the source language tuple. This algorithm turned out to lead to highly plausible rankings and to be robust with regard to sampling errors.9 It is also quite effective in eliminating the homograph problem. 
3 Experimental Results and Evaluation 
Let us first try to see whether the basic assump-tion underlying our approach is sound, namely that we will find a sufficient number of foreign words in our corpus. To check this claim, we have listed in Table 2 for each of the four lan-guages the number of words from the gold stan-dard falling into particular frequency categories. For example, the value of 70 in the field belong-ing to the row 6-10 and the column Spanish means that out of the 1079 Spanish words in our gold standard 70 have a corpus frequency be-tween 6 and 10 in the 4th edition of the English Gigaword Corpus. Apparently, words with zero occurrences or with a very low corpus frequency are problematic because of data sparseness. Yet words with very high frequencies are not less problematic, as they may turn out to have homo-graphs in the target language. As there is no gen-erally accepted definition of what the vocabulary of a given language is, we cannot give precise figures concerning the number of homographs in our gold standard for each language pair. Never-
                                                 9 A further improvement is possible by giving words with identical association strengths not arbitrary rank-ing positions within this group, but an average rank which is to be assigned to all of them. 
55
theless, we believe that Table 2 gives a fair im-pression. By taking a look at the high frequency source language words one can see that the pair French?English has the greatest number of homographs, followed by German?English, and finally Spanish?English.  
Source languages Targ. lang. Corpus fre-quency Ger-man Fre-nch Spa-nish Eng-lish 0 449 329 317 0 1 64 85 43 0 2 26 52 25 0 3 24 39 23 0 4 17 34 27 0 5 7 26 15 0 6-10 32 71 70 0 11-20 50 59 86 0 21-50 63 52 129 0 51-100 50 37 95 1 101-200 52 10 75 3 201-500 50 25 74 6 501-1000 43 18 31 19 1001-10000 100 71 37 245 above 10000 52 171 32 805  Table 2: Corpus frequencies of the words occurring in the gold standard.  As to be expected, the corpus frequencies of the language of the corpus, namely English, are or-ders of magnitude higher than those of the other languages. But the table also gives a good idea concerning the presence of French, German, and Spanish word citations in written English. How-ever, we should not be misled by the overwhelm-ing presence of French words in the high fre-quency ranges, as this mainly reflects the amount of homography. Although pronunciation rules are very different between English and French, spelling tends to be similar, which is why there are lots of homographs. In contrast, Spanish and German usually use different spelling even for words having the same historical roots, which is why homography is far less common.10 
                                                 10 As an example for such spelling conversions, let?s mention that the grapheme c in English is almost con-sistently replaced by k in German, e.g. class ? Klasse and clear ? klar. 
From the figures of Table 2 one may conclude that identifying word translations from a mono-lingual corpus is not easy because of data sparse-ness. Nevertheless it seems possible, at least to some extent. Let us therefore take a look at some results. In our experimental work we first identified word translations for stimulus words from a sin-gle source language, then for stimulus words from two source languages, and finally for stimu-lus words from three source languages.  a)  One source language  We started by conducting separate runs for each of the three source languages (French, German, Spanish) and determined the number of times the algorithm was able to come up with the expected English translation as the top ranked association for the 3 * 1079 source words. Note, however, that hereby we did not consider the full range of possible target words present in the English Gi-gaword corpus as this would include many for-eign words. Instead, we restricted the number of target words to the 1079 English words present in the gold standard. The respective figures are 163 (15.1%) for French, 85 (7.9%) for German, and 97 (9.0%) for Spanish. As can be seen, French clearly per-formed best, which confirms previous studies that the lexical agreement between French and English is surprisingly high. Nevertheless, on average, only 10.7% of the translations were identified correctly, which does not look very good. However, remember that these figures can be considered as a lower bound as we do not take alternative translations into account and as the underlying corpus has not been prepared specifi-cally for this purpose. Note also that the product-of-ranks algorithm has no effect in the case when only a single source language is considered. (If there is only one value, no multiplication takes place.)   b)  Two source languages  Our next step was to combine pairs of source languages. There are three possible pairs, namely French?German, French?Spanish, and German?Spanish. Their respective performance figures are as follows: 217 (21.0%), 225 (20.9%), and 145 (13.4%). Computing the mean of these re-
56
sults yields an average of 18.4%, which is a nice improvement over the initial 10.7% which we had for single source languages. This lends sup-port to our hypothesis that the product-of-ranks algorithm works effectively in this context.  c)  Three source languages  Finally, all three source languages were com-bined, resulting in the correct translation of 248 of the altogether 1079 test items, which corre-sponds to a performance of 23.0%. This further improvement is consistent with our hypothesis that performance should increase when more source languages are considered.  Let us take a closer look at these performance gains. At the beginning we increased the number of source languages by 100% (from 1 to 2), yielding a relative performance increase of 72% (the absolute performance improved from 10.7% to 18.4%). Next we increased the number of source languages by 50% (from 2 to 3) which yielded a relative performance increase of 25% (absolute performance had improved from 18.4% to 23%). This means that the behavior is worse than linear, as in the linear case we should have obtained a further improvement of 72%/2 = 36%. But of course when combining statistics in NLP, hardly ever a linear behavior can be observed, and the above findings seem satisfactory. Never-theless they should be supported by looking at further languages, see Section 4.11 For the case of looking at three source lan-guages in parallel, let us provide data concerning the rank distribution of the expected translations (see the middle column of Table 3). Overall, in 357 of the 1079 cases (33.9%) the expected translation ranks among the top five, and in 392 cases (36.3%) it is among the top ten associa-tions. These results are based on a window size of ?10 words when counting the co-occurrence frequencies. To give an idea that the procedure is robust in this respect, we provide analogous val-                                                 11  Another important question, which we have not dealt with yet, is to what extend the observed gain in performance when increasing the number of source languages is a side effect of a higher likelihood that at least one of the source words happens to be identical to the target word (with the same or a similar mean-ing). In such cases (which might be common when considering related languages), predicting the correct translation is rather easy. 
ues for a window size of ?20 words in the third column of Table 3. As can be seen, apart from the usual statistical fluctuations the difference is hardly noticeable.    Number of items with the respective rank Rank window size ?10 window size ?20 rank could not be computed (all source words un-known) 
11 10 
1 248 247 
2 55 51 
3 32 36 
4 15 19 
5 7 8 
6 16 8 
7 7 6 
8 3 5 
9 3 5 
10 6 4 
above 10 676 680  Table 3: Ranks of the expected translations when all three source languages are combined.     
  EXAMPLE 1    Given word French:  tablier  [7]   Given word German:  Sch?rze  [0]   Given word Spanish:  delantal  [4]    Expected translation into English    according to the gold standard:  apron [3059]    Top 5 translations as computed:      1 apron     [3059]     2 sausage    [9954]     3 sauce   [49139]     4 appetite  [24682]     5 mustard  [13477]   Table 4: Sample results.  
57
  
  EXAMPLE 2    Given word French:  carton  [2671]   Given word German:  Karton     [22]   Given word Spanish:  cart?n       [0]    Expected translation into English    according to gold standard:  cardboard [13714]      Top 5 translations as computed:      1    cardboard  [13714]     2    cigarette  [54583]     3    fold   [43682]     4    milk   [85426]     5    egg   [42948]       Table 5: Sample results.  Having looked at the quantitative results, some sample output may also be of interest. For this purpose, Tables 4 and 5 show sample results for triplets of source language words. Hereby, the numbers in square brackets refer to the corpus frequencies of the respective words in the Eng-lish Gigaword Corpus.  
4 Summary and Future Work 
In this paper we made an attempt to solve the problem of identifying word translations on the basis of a single monolingual corpus where the same corpus is supposed to be used for several language pairs. The basic idea underlying our work is to look at citations of foreign words, to compute their co-occurrence-based associations, and to consider these as translations of the re-spective words. We pointed out some difficulties with this ap-proach, namely the problem of data sparseness and the homograph trap, but were able to suggest and implement at least partial solutions. Using the product-of-ranks algorithm, our main sugges-tion was to look at several source languages in parallel, which at least in theory has the potential to solve the experienced problems. We did not have very high expectations when starting this work and were positively surprised by the resulting performance of up to 25% cor-rectly predicted test items. As pointed out, in or-
der to avoid raising unjustified expectations, we presented somewhat conservative figures which should leave room for improvements.  Obvious extensions of the current work are to increase the number of considered languages and to also use other large monolingual corpora. For example, we could use the web corpora provided by the web-as-a-corpus (WaCky) initiative (Ba-roni et al, 2009). A few such corpora have al-ready been made available recently, and as they are based on a largely automatic acquisition pro-cedure there are probably more to come. This reflects a tendency towards extremely large cor-pora. Processing in the current framework turns out to be unproblematic if sparse matrices are used, as foreign word occurrences are implicitly of low frequency. Although web corpora should be very noisy in comparison to the carefully edited newsticker texts used here, the interesting thing is that ac-cording to the hypothesis formulated in the intro-duction the current approach seems to provide one of the rare occasions where noisy data is bet-ter than perfectly clean data, and we hope that future work will prove this prediction correct. Another possibility for future work is to look at second rather than first order associations, i.e. to consider those words as potential translations of a given foreign word which show similar con-text words. This might be promising in so far as the sparse data problem is less salient in this case. Finally, let us come back to our speculative question from the introduction whether or not people speaking different languages have sepa-rate lexico-semantic networks in their mind. Aparently our experiments did not provide evi-dence for either assumption. But the most straightforward assumption would probably be that our mind does not attach language labels to the words we perceive, and simply treats them all equally. At the lexical level, our mind?s unknown inner workings may be in effect analogous to clustering words according to their observed co-occurrence patterns. The likely result is that in some cases there will be many interconnections between clusters, and in other cases few. De-pending on the language environment experi-enced by a person, we cannot rule out that some of the larger clusters might exactly correspond to languages. But what the current research does 
58
tell us is that there can be a multitude of statisti-cally significant co-occurrences even at non-obvious places. So what we possibly should rule out is that, even across languages, there are sepa-rate clusters without any interconnections. 
Acknowledgments 
Part of this research was supported by a Marie Curie Intra European Fellowship within the 7th European Community Framework Programme. We thank the Linguistic Data Consortium for making available the English Gigaword Corpus, and Lourdes Callau, Maria Dolores Jimenez Lo-pez, and Lilica Voicu for their support in acquir-ing it. 
References 
Babych, Bogdan; Sharoff, Serge; Hartley, Anthony; Mudraya, Olga (2007). Assisting Translators in In-direct Lexical Transfer. Proceedings of the 45th In-ternational Conference of the Association for Com-putational Linguistics ACL 2007, Prague, 136?143.  
Baroni, Marco; Bernardini, Silvia; Ferraresi, Adriano,  Zanchetta, Eros (2009). The WaCky Wide Web: A collection of very large linguistically processed Web-crawled corpora. Journal of Language Re-sources and Evaluation 43 (3): 209?226. 
Chiao, Yun-Chuang; Sta, Jean-David; Zweigenbaum, Pierre (2004). A novel approach to improve word translations extraction from non-parallel, compara-ble corpora. In: Proceedings of the International Joint Conference on Natural Language Processing, Hainan, China. AFNLP. 
Dunning, Ted (1993). Accurate methods for the sta-tistics of surprise and coincidence. Computational Linguistics, 19(1), 61?74. 
Fung, P.; McKeown, K. (1997). Finding terminology translations from non-parallel corpora. Proceedings of the 5th Annual Workshop on Very Large Cor-pora, Hong Kong, 192?202.  
Fung, P.; Yee, L. Y. (1998). An IR approach for trans-lating new words from nonparallel, comparable texts. In: Proceedings of COLING-ACL 1998, Montreal, Vol. 1, 414?420. 
Koehn, Philipp; Knight, Kevin (2002). Learning a translation lexicon from monolingual corpora. In: Unsupervised Lexical Acquisition. Proceeding of the ACL SIGLEX Workshop, 9?16. 
Koehn, Philipp (2005). Europarl: A parallel corpus for statistical machine translation. Proceedings of MT Summit, Phuket, Thailand, 79?86.  
Munteanu, Dragos Stefan; Marcu, Daniel (2005). Im-proving machine translation performance by ex-ploiting non-parallel corpora. Computational Lin-guistics 31(4), 477?504. 
Rapp, Reinhard (1995). Identifying word translations in non-parallel texts. In: Proceedings of the 33rd Meeting of the Association for Computational Lin-guistics. Cambridge, Massachusetts, 320?322.  
Rapp, Reinhard. (1999). Automatic identification of word translations from unrelated English and Ger-man corpora. In: Proceedings of the 37th Annual Meeting of the Association for Computational Lin-guistics 1999, College Park, Maryland. 519?526. 
Rapp, Reinhard; Zock, Michael (2010). Automatic dictionary expansion using non-parallel corpora. In: Andreas Fink, Berthold Lausen, Wilfried Seidel Al-fred Ultsch (Eds.)  Advances in Data Analysis, Data Handling and Business Intelligence. Proceedings of the 32nd Annual Meeting of the GfKl, 2008. Hei-delberg: Springer. 
Steinberger, Ralf; Pouliquen, Bruno; Widiger, Anna; Ignat, Camelia; Erjavec, Toma?; Tufi?, Dan; VARGA, D?niel (2006). The JRC-Acquis: A multi-lingual aligned parallel corpus with 20+ languages. Proceedings of the 5thLREC, Genoa, Italy.  
Wu, Dekai; Fung, Pascale (2005). Inversion transduc-tion grammar constraints for mining parallel sen-tences from quasi-comparable corpora. Proceedings of the Second International Joint Conference on  Natural Language Processing (IJCNLP-2005). Jeju, Korea.  
Parker, Robert, Graff, David; Kong, Junbo; Chen, Ke; Maeda, Kazuaki (2009). English Gigaword. Fourth Edition. Linguistic Data Consortium, Philadelphia.  
59
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 16?25,
Beijing, August 2010
The Noisier the Better: Identifying Multilingual  Word Translations Using a Single Monolingual Corpus 
Reinhard Rapp University of Tarragona GRLMC 
reinhardrapp@gmx.de 
Michael Zock Laboratoire d?Informatique Fondamentale CNRS Marseille 
Michael.Zock@lif.univ-mrs.fr 
 
Abstract 
The automatic generation of dictionaries from raw text has previously been based on parallel or comparable corpora. Here we describe an approach requiring only a single monolingual corpus to generate bilingual dictionaries for several lan-guage pairs. A constraint is that all lan-guage pairs have their target language in common, which needs to be the lan-guage of the underlying corpus. Our ap-proach is based on the observation that monolingual corpora usually contain a considerable number of foreign words. As these are often explained via transla-tions typically occurring close by, we can identify these translations by look-ing at the contexts of a foreign word and by computing its strongest associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five ma-jor European languages. We also com-pare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that re-sults are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 
1 Introduction 
Established methods for the identification of word translations are based on parallel (Brown et al, 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al, 2004). The work using parallel corpora such as Europarl (Koehn, 
2005; Armstrong et al, 1998) or JRC Acquis (Steinberger et al, 2006) typically performs a length-based sentence alignment of the trans-lated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into prac-tice using a number of freely available open source tools such as Moses (Koehn et al, 2007) and Giza++ (Och & Ney, 2003).  However, parallel texts are a scarce resource for many language pairs (Rapp & Mart?n Vide, 2007), which is why methods based on compa-rable corpora have come into focus. One ap-proach is to extract parallel sentences from comparable corpora (Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach relates co-occurrence patterns between languages. Hereby the underlying assumption is that across languages there is a correlation between the co-occurrences of words which are translations of each other. If, for example, in a text of one lan-guage two words A and B co-occur more often than expected by chance, then in a text of an-other language those words which are the trans-lations of A and B should also co-occur more frequently than expected. However, to exploit this observation some bridge needs to be built between the two lan-guages. This can be done via a basic dictionary comprising some essential vocabulary. To put it simply, this kind of dictionary allows a (partial) word-by-word translation from the source to the target language,1 so that the result can be con-sidered as a pair of monolingual corpora. Deal-
                                                 1 Note that this translation can also be conducted at the level of co-occurrence vectors rather than at the text level. 
16
ing only with monolingual corpora means that the established methodology for computing similar words (see e.g. Pantel & Lin, 2002), which is based on Harris? (1954) distributional hypothesis, can be applied. It turns out that the most similar words between the two corpora effectively identify the translations of words. This approach based on comparable corpora considerably relieves the data acquisition bot-tleneck, but has the disadvantage that the results tend to lack accuracy in practice. As an alternative, there is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which has the advan-tage that it does not even require a corpus. A simple word list will suffice. However, this ap-proach works only for closely related languages, and has limited potential otherwise. We propose here to generate dictionaries on the basis of foreign word occurrences in texts. As far as we know, this is a method which has not been tried before. When doing so, a single monolingual corpus can be used for all source languages for which it contains a sufficient number of foreign words. A constraint is that the target language must always be the language of the monolingual corpus,2 which therefore all dictionaries have in common. 
2 Approach and Language Resources 
Starting from the observation that monolingual dictionaries typically include a considerable number of foreign words, the basic idea is to consider the most significant co-occurrences of a foreign word as potential translation candi-dates. This implies that the language of the un-derlying corpus must correspond to the target language, and that this corpus can be utilized for any source language for which word citations are well represented. As the use of foreign language words in texts depends on many parameters, including writer, text type, status of language and cultural back-ground, it is interesting to compare results when varying some of these parameters. However, due to the general scarceness of foreign word                                                  2 Although in principle it would also be possible to determine relations between foreign words from dif-ferent languages within a corpus, this seems not promising as the problem of data sparsity is likely to be prohibitive. 
citations our approach requires very large cor-pora. For this reason, we were only able to vary two parameters, namely language and text type. Some large enough corpora that we had at our disposal were the Gigaword Corpora from the Linguistic Data Consortium (Mendon?a et al, 2009a; Mendon?a et al, 2009b) and the WaCky Corpora described in Sharoff (2006), Baroni et al (2009), and Ferraresi et al (2010). From these, we selected the following for this study:  
? French WaCky Corpus (8.2 GB) 
? German WaCky Corpus (9.9 GB) 
? Italian WaCky Corpus (10.4 GB) 
? French Gigaword 2nd edition (5.0 GB) 
? Spanish Gigaword 2nd edition (6.8 GB)  The memory requirements shown for each cor-pus relate to ANSI coded text only versions. We derived these from the original corpora by re-moving linguistic annotation (for the WaCky corpora) and XML markup, and by converting the coding from UTF8 to ANSI. Both Gigaword corpora consist of news-ticker texts from several press agencies. News-ticker text is a text type closely related to news-paper text. It is usually carefully edited, and the vocabulary is geared towards easy understand-ing for the intended readership. This implies that foreign word citations are kept to a mini-mum. In contrast, the WaCky Corpora have been downloaded from the web and represent a great variety of text types and styles. Hence, not all texts can be expected to have been carefully edited, and mixes between languages are proba-bly more frequent than with newsticker text. As in this work English is the main source language, and as we have dealt with it as a tar-get language already in Rapp & Zock (2010), we do not use the respective English versions of these corpora here. We also do not use the Wikipedia XML Corpora (Denoyer et al, 2006) as these greatly vary in size for different lan-guages which makes comparisons across lan-guages somewhat problematic. In contrast, the sizes of the above corpora are within the same order of magnitude (1 billion words each), which is why we do not control for corpus size here. 
17
Concerning the number of foreign words within these corpora, we might expect that, given the status of English as the world?s pre-miere language, English foreign words should be the most frequent ones in our corpora. As French and Spanish are also prominent lan-guages, foreign words borrowed from them may be less frequent but should still be common, whereas borrowings from German and Italian are expected to be the least likely ones. From this point of view the quality of the results should vary accordingly. But of course there are many other aspects that are important, for ex-ample, relations between countries, cultural background, relatedness between languages, etc. As these are complex influences with intricate interactions, it is impossible to accurately an-ticipate the actual outcome. In other words, ex-perimental work is needed. Let us therefore de-scribe our approach. For identifying word translations within a corpus, we assume that the strongest association to a foreign word is likely to be its translation. This can be justified by typical usage patterns of foreign words often involving, for example, an explanation right after their first occurrence in a text. Associations between words can be com-puted in a straightforward manner by counting word co-occurrences followed by the applica-tion of an association measure on the co-occurrence counts. Co-occurrence counts are based on a text window comprising the 20 words on either side of a given foreign word. On the resulting counts we apply the log-likelihood ratio (Dunning, 1993). As explained by Dunning, this measure has the advantage to be applicable also on low counts, which is an important characteristic in our setting where the problem of data sparseness is particularly se-vere. This is also the reason why we chose a window size somewhat larger than the ones used in most other studies. Despite its simplicity this procedure of com-puting associations to foreign words is well suited for identifying word translations. As mentioned above, we assume that the strongest association to a foreign word is its best transla-tion. We did this for words from five languages (English, French, German, Italian, and Spanish). The results are shown in the next section. In 
order to be able to quantitatively evaluate the quality of our results, we counted for all source words of a language the number of times the expected target word obtained the strongest as-sociation score. Our expectations on what should count as a correct translation had been fixed before run-ning the experiments by creating a gold stan-dard for evaluation. We started from the list of 100 English words (nouns, adjectives and verbs) which had been introduced by Kent & Rosanoff (1910) in a psychological context. We translated these English words into each of the four target languages, namely French, German, Italian, and Spanish. As we are at least to some extent familiar with these languages, and as the Kent/Rosanoff vocabulary is fairly straightforward, we did this manually. In cases where we were aware of ambiguities, we tried to come up with a translation relating to what we assumed to be the most frequent of a word?s possible senses. In case of doubt we consulted a number of written bilingual dictionaries, the dict.leo.org dictionary website, and the transla-tion services provided by Google and Yahoo. For each word, we always produced only a sin-gle translation. In an attempt to provide a com-mon test set, the appendix shows the resulting list of word equations in full length for refer-ence by interested researchers. It should be noted that the concept of word equations is a simplification, as it does not take into account the fact that words tend to be am-biguous, and that ambiguities typically do not match across languages. Despite these short-comings we nevertheless use this concept. Let us give some justification.  Word ambiguities are omnipresent in any language. For example, the English word palm has two meanings (tree and hand) which are usually expressed by different words in other languages. However, for our gold standard we must make a choice. We can not include two or more translations in one word equation as this would contradict the principle that all words in a word equation should share their main sense.  Another problem is that, unless we work with dictionaries derived from parallel corpora, it is difficult to estimate how common a transla-tion is. But if we included less common transla-tions in our list, we would have to give their matches a smaller weight during evaluation. 
18
This, however, is difficult to accomplish accu-rately. This is why, despite their shortcomings, we use word equations in this work. Evaluation of our results involves comparing a predicted translation to the corresponding word in the gold standard. We consider the pre-dicted translation to be correct if there is a match, otherwise we consider it as false. While in principle possible, we do not make any finer distinctions concerning the quality of a match. A problem that we face in our approach is what we call the homograph trap. What we mean by this term is that a foreign word occur-ring in a corpus of a particular language may also be a valid word in this language, yet possi-bly with a different meaning. For example, if the German word rot (meaning red) occurs in an English corpus, its occurrences can not easily be distinguished from occurrences of the English word rot, which is a verb describing the process of decay. Having dealt with this problem in Rapp & Zock (2010) we will not elaborate on it here, rather we will suggest a workaround. The idea is to look only at a very restricted vocabulary, namely the words defined in our gold standard. There we have 100 words in each of the five languages, i.e. 500 words altogether. The ques-tion is how many of these words occur more often than once. Note, however, that apart from English (which was the starting point for the gold standard), repetitions can occur not only across languages but also within a language. For example, the Spanish word sue?o means both sleep and dream, which are distinct entries in the list. The following is a complete list of words showing either of these two types of repetitions, i.e. exact string matches (taking into account capitalization and accents): alto (4), bambino (2), Bible (2), bitter (2), casa (2), commando (2), corto (2), doux (2), duro (2), fruit (2), jus-tice (2), lento (2), lion (2), long (2), luna (2), mano (2), memoria (2), mouton (2), religion (2), sacerdote (2), sue?o (2), table (2), whisky (4). However, as is obvious from this list, these repetitions are due to common vocabulary of the languages, with whisky being a typical example. They are not due to incidental string identity of completely different words. So the latter is not a problem (i.e. causing the identification of wrong 
translations) as long as we do not go beyond the vocabulary defined in our gold standard. For this reason and because dealing with the full vocabulary of our (very large) corpora would be computationally expensive, we de-cided to replace in our corpora all words absent from the gold standard by a common designator for unknown words. Also, in our evaluations, for the target language vocabulary we only use the words occurring in the respective column of the gold standard. So far, we always computed translations to single source words. However, if we assume, for example, that we already have word equa-tions for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm. As sug-gested in Rapp & Zock (2010) this can be done by looking up the ranks of each of the four given words (i.e. the words occurring in a par-ticular word equation) within the association vector of a translation candidate, and by multi-plying these ranks. So for each candidate we obtain a product of ranks. We then assume that the candidate with the smallest product will be the best translation.3  Let us illustrate this by an example: If the given words are the variants of the word nerv-ous in English, French, German, and Spanish, i.e. nervous, nerveux, nerv?s, and nervioso, and if we want to find out their translation into Ital-ian, we would look at the association vectors of each word in our Italian target vocabulary. The association strengths in these vectors need to be inversely sorted, and in each of them we will look up the positions of our four given words. Then for each vector we compute the product of the four ranks, and finally sort the Italian vo-cabulary according to these products. We would then expect that the correct Italian translation, namely nervoso, ends up in the first position, i.e. has the smallest value for its product of ranks. 
                                                 3 Note that, especially in the frequent case of zero-co-occurrences, many words may have the same as-sociation strength, and rankings within such a group of words may be arbitrary within a wide range. To avoid such arbitrariness, it is advisable to assign all words within such a group the same rank, which is chosen to be the average rank within the group. 
19
In the next section, we will show the results for this algorithm in addition to those for single source language words. As a different matter, let us mention that for our above algorithm we do not need an explicit identification of what should count as a foreign word. We only need a list of words to be trans-lated, and a list of target language words con-taining the translation candidates from which to choose. Overlapping vocabulary is permitted. If the overlapping words have the same meaning in both languages, then there is no problem and the identification of the correct translation is rather trivial as co-occurrences of a word with itself tend to be frequent. However, if the over-lapping words have different meanings, then we have what we previously called a homogaph trap. In such (for small vocabularies very rare) cases, it would be helpful to be able to distin-guish the occurrences of the foreign words from those of the homograph. However, this problem essentially boils down to a word sense disam-biguation task (actually a hard case of it as the foreign word occurrences, and with them the respective senses, tend to be rare) which is be-yond the scope of this paper. 
3 Experimental Results and Evaluation 
We applied the following procedure on each of the five corpora: The language of the respective corpus was considered the target language, and the vocabulary of the respective column in the gold standard was taken to be the target lan-guage vocabulary.  
 Source Languages 
 DE EN FR ES IT all 
DE WaCky ? 54 22 18 20 48 
ES Giga 9 42 37 ? 29 56 
FR Giga 15 45 ? 20 14 49 
FR WaCky 27 59 ? 16 21 50 
IT WaCky 17 53 29 27 ? 56 
Average 17.0 50.6 29.3 20.3 21.0 51.8  Table 1: Number of correctly predicted translations for various corpora and source languages. Column all refers to the parallel use of all four source lan-guages using the product-of-ranks algorithm. 
The other languages are referred to as the source languages, and the corresponding columns of the gold standard contain the respective vocabu-laries. Using the algorithm described in the pre-vious section, for each source vocabulary the following procedure was conducted: For every source language word the target vocabulary was sorted according to the respective scores. The word obtaining the first rank was considered to be the predicted translation. This predicted translation was compared to the translation listed in the gold standard. If it matched, the prediction was counted as correct, otherwise as wrong. Table 1 lists the number of correct predic-tions for each corpus and for each source lan-guage. These results lead us to the following three conclusions:   1)  The noisier the better  We have only for one language (French) both a Gigaword and a WaCky corpus. The results based on the WaCky corpus are clearly better for all languages except Spanish. Alternatively, we can also look at the average performance for the five source languages among the three WaCky corpora, which is 30.3, and the analo-gous performance for the two Gigaword cor-pora, which is 26.4. These findings lend some support to our hypothesis that noisy web cor-pora are better suited for our purpose than care-fully edited newsticker corpora, which are probably more successful in avoiding foreign language citations  2)  English words are cited more often  In the bottom row, Table 1 shows for each of the five languages the scores averaged over all corpora. As hypothesized previously, we can take citation frequency as an indicator (among others) of the ?importance? of a language. And citation frequency can be expected to correlate with our scores. With 50.6, the average score for English is far better than for any other lan-guage, thereby underlining its special status among world languages. With an average score of 29.3 French comes next which confirms the hypothesis that it is another world language re-ceiving considerable attention elsewhere. Some-what surprising is the finding that Spanish can not keep up with French and obtains an average 
20
score of 20.3 which is even lower than the 21.0 for Italian. A possible explanation is the fact that we are only dealing with European lan-guages here, and that the cultural influence of the Roman Empire and Italy has been so con-siderable in Europe that it may well account for this. So the status of Spanish in the world may not be well reflected in our selection of corpora. Finally, the average score of 17.0 for German shows that it is the least cited language in our selection of languages. Bear in mind, though, that German is the only clearly Germanic lan-guage here, and that its vocabulary is very dif-ferent from that of the other languages. These are mostly Romanic in type, with English somewhere in between. Therefore, the little overlap in vocabulary might make it hard for French, Italian, and Spanish writers to under-stand and use German foreign words.   3)  Little improvement for several source words  The right column in Table 1 shows the scores if (using the product-of-ranks algorithm) four source languages are taken into account in par-allel. As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal. This contrasts with the findings described in Rapp & Zock (2010) where significant improvements could be achieved by increasing the number of source languages. So this casts some doubt on these. However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement. This is not the case here, where we try to improve on a score of around 50 for English. Remember that this is a somewhat conservative score as we count cor-rect but alternative translations, as errors. As this is already a performance much closer to the optimum, making further performance gains is more difficult. Therefore, perhaps we should take it as a success that the product-of-ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non-English languages was probably mostly detrimental. Having analyzed the quantitative results, to give a better impression of the strengths and weaknesses of our algorithm, for the (according to Table 1) best performing combination of cor-
pus and language pair, namely the French WaCky corpus, English as the source language and French as the target language, Table 2 shows some actual source words and their com-puted translations. 
 
  ESW    CF   ET  RE  CT 
cabbage 9 chou 1 chou blossom 25 fleur 73 commande carpet 39 tapis 1 tapis bitter 59 amer 1 amer hammer 67 marteau 1 marteau bread 82 pain 1 pain citizen 115 citoyen 1 citoyen bath 178 bain 1 bain butterfly 201 papillon 1 papillon eat 208 manger 1 manger butter 220 beurre 59 terre eagle 282 aigle 1 aigle cheese 527 fromage 1 fromage cold 539 froid 1 froid deep 585 profond 1 profond cottage 624 cabanon 1 cabanon earth 702 terre 53 tabac child 735 enfant 1 enfant bed 806 lit 2 table beautiful 923 beau 1 beau care 1267 soin 1 soin hand 1810 main 2 main city 2610 ville 1 ville girl 2673 fille 1 fille green 2861 vert 1 vert blue 2914 bleu 1 bleu hard 3615 dur 1 dur black 9626 noir 1 noir Bible 17791 Bible 1 Bible foot 23548 pied 8 siffler chair 24027 chaise 1 chaise fruit 38544 fruit 1 fruit  Table 2: Results for the language pair English ? French. The meaning of the columns is as follows: ESW = English source word; CF = corpus frequency of English source word; ET = expected translation according to gold standard; RE = computed rank of expected translation; CT = computed translation. 
4 Summary and Future Work 
In this paper we made an attempt to solve the difficult problem of identifying word trans-lations on the basis of a single monolingual cor-
21
pus, whereby the same corpus is used for sev-eral language pairs. The basic idea underlying our work is to look at foreign words, to compute their co-occurrence-based associations, and to consider these as translations of the respective words. Whereas Rapp & Zock (2010) dealt only with an English corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora. We were able to shed some light on criteria influencing per-formance, such as the selection of text type and the direction of a language pair. For example, it is more promising to look at occurrences of English words in a German corpus rather than the other way around. Because of the special status of English it is also advisable to use it as a pivot wherever possible. Perhaps surprisingly, the work may have im-plications regarding cognitive models of second language acquisition. The reason is that it de-scribes how to acquire the vocabulary of a new language from a mixed corpus. This is relevant as traditional foreign language teaching (involv-ing explanations in the native tongue and vo-cabulary learning using bilingual word lists) can be considered as providing such a mixed corpus. Regarding future work, let us outline a plan for the construction of a universal dictionary of all languages which are well enough represented on the web.4 There might be some chance for it, because the algorithm can be extended to work with standard search engines and is also suitable for a bootstrapping approach.  Let us start by assuming that we have a large matrix where the rows correspond to the union of the vocabularies of a considerable number of languages, and the columns correspond to these languages themselves. We presuppose no prior translation knowledge, so that the matrix is completely empty at the beginning (although prior knowledge could be useful for the iterative algorithm to converge). STEP 1: For each word in the vocabulary we perform a search via a search engine such as Google, preferably in an automated fashion via an application programming interface (API). Next, we retrieve as many documents as possi-
                                                 4 Note that this plan could also be adapted to other methodologies (such as Rapp, 1999), and may be more promising with these. 
ble, and separate them according to language.5 Then, for each language for which we have ob-tained the critical mass of documents, we apply our algorithm and compute the respective trans-lations. These are entered into the matrix. As we are interested in word equations, we assume that translations are symmetric. This means that each translation identified can be entered at two positions in the matrix. So at the end of step 1 we have for each word the translations into a number of other languages, but this number may still be small at this stage.  STEP 2: We now look at each row of the ma-trix and feed the words found within the same row into the product-of-ranks algorithm. We do not have to repeat the Google search, as step 1 already provided all documents needed. Be-cause when looking at several source words we have a better chance to find occurrences in our documents, this should give us translations for some more languages in the same row. But we also need to recompute the translations resulting from the previous step as some of them will be erroneous e.g.  for reasons of data sparseness or due to the homograph trap. STEP 3: Repeat step 2 until as many matrix cells as possible are filled with translations. We hope that with each iteration completeness and correctness improve, and that the process con-verges in such a way that the (multilingual) words in each row disambiguate each other, so that ultimately each row corresponds to an un-ambiguous concept. 
Acknowledgments 
Part of this research was supported by a Marie Curie Intra European Fellowship within the 7th European Community Framework Programme. We thank the WaCky group for making avail-able their excellent Web corpora, and the Lin-guistic Data Consortium for providing the Gi-gaword Corpora. We also wish to thank Lourdes Callau, Maria Dolores Jimenez Lopez, and Lilica Voicu for their support in acquiring the LDC corpora. 
                                                 5  If the language identification markup within the retrieved documents turns out to be unreliable (which is unfortunately often the case in practice), standard language identification software can be used. 
22
References 
Armstrong, Susan; Kempen, Masja; McKelvie, David; Petitpierre, Dominique; Rapp, Reinhard; Thompson, Henry (1998). Multilingual Corpora for Cooperation. Proceedings of the 1st Interna-tional Conference on Linguistic Resources and Evaluation (LREC), Granada, Vol. 2, 975?980.  
Baroni, Marco; Bernardini, Silvia; Ferraresi, Adri-ano,  Zanchetta, Eros (2009). The WaCky Wide Web: A collection of very large linguistically pro-cessed Web-crawled corpora. Journal of Language Resources and Evaluation 43 (3): 209-226. 
Brown, Peter; Cocke, John; Della Pietra, Stephen A.; Della Pietra, Vincent J.; Jelinek, Frederick; Laf-ferty, John D.; Mercer, Robert L.; Rossin, Paul S. (1990). A statistical approach to machine transla-tion. Computational Linguistics, 16(2), 79?85. 
Chiao, Yun-Chuang; Sta, Jean-David; Zwei-genbaum, Pierre (2004). A novel approach to im-prove word translations extraction from non-parallel, comparable corpora. In: Proceedings of the International Joint Conference on Natural Language Processing, Hainan, China. AFNLP. 
Denoyer, Ludovic; Gallinari, Pattrick (2006). The Wikipedia XML Corpus. SIGIR Forum, 40(1), 64?69. 
Dunning, T. (1993). Accurate methods for the sta-tistics of surprise and coincidence. Computational Linguistics, 19(1), 61?74.  
Ferraresi, Adriano; Bernardini, Silvia; Picci, Gio-vanni; Baroni, Marco (2010). Web corpora for bi-lingual lexicography: a pilot study of English/ French collocation extraction and translation. In Xiao, Richard (ed.): Using Corpora in Contrastive and Translation Studies. Newcastle: Cambridge Scholars Publishing. 
Fung, Pascale; McKeown, Kathy (1997). Finding terminology translations from non-parallel cor-pora. Proceedings of the 5th Annual Workshop on Very Large Corpora, Hong Kong, 192-202.  
Fung, Pascale; Yee, Lo Yuen (1998). An IR ap-proach for translating new words from nonparallel, comparable texts. In: Proceedings of COLING-ACL 1998, Montreal, Vol. 1, 414-420. 
Harris, Zelig S. (1954). Distributional structure. WORD, 10:146?162.  
Kent, Grace Helen; Rosanoff , A.J. (1910). A study of association in insanity. American Journal of In-sanity 67:317?390. Koehn, Philipp (2005). Europarl: A parallel corpus for statistical machine translation. Proceedings of MT Summit, Phuket, Thailand, 79?86.  
Koehn, Philipp; Hoang, Hieu; Birch, Alexandra; Callison-Burch, Chris; Federico, Marcello; Ber-toldi, Nicola; Cowan, Brooke; Shen, Wade; Moran, Christine; Zens, Richard; Dyer, Chris; Bo-jar, Ond?ej; Constantin, Alexandra; Herbst, Evan (2007). Moses: Open source toolkit for statistical machine translation. In: Proceedings of ACL, Pra-gue, demonstration session, 177?180.  
Koehn, Philipp; Knight, Kevin (2002). Learning a translation lexicon from monolingual corpora. In: Unsupervised Lexical Acquisition. Proceedings of the ACL SIGLEX Workshop, 9?16. 
Mendon?a, Angelo, Graff, David, DiPersio, Denise (2009a). French Gigaword Second Edition. Lingu-istic Data Consortium, Philadelphia. 
Mendon?a, Angelo, Graff, David, DiPersio, Denise (2009b). Spanish Gigaword Second Edition. Lin-guistic Data Consortium, Philadelphia. 
Munteanu, Dragos Stefan; Marcu, Daniel (2005). Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics 31(4), 477?504.  
Och, Franz Josef; Ney, Hermann (2003). A System-atic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1), 19?51. 
Pantel, Patrick; Lin, Dekang (2002). Discovering word senses from text. In: Proceedings of ACM SIGKDD, Edmonton, 613?619 
Rapp, Reinhard (1995). Identifying word translations in non-parallel texts. In: Proceedings of the 33rd Meeting of the Association for Computational Lin-guistics. Cambridge, Massachusetts, 320-322.  
Rapp, Reinhard. (1999). Automatic identification of word translations from unrelated English and German corpora. In: Proceedings of the 37th An-nual Meeting of the Association for Computational Linguistics 1999, College Park, Maryland. 519?526. 
Rapp, Reinhard; Mart?n Vide, Carlos (2007). Statis-tical machine translation without parallel corpora. In: Georg Rehm, Andreas Witt, Lothar Lemnitzer (eds.): Data Structures for Linguistic Resources and Applications. Proceedings of the Biennial GLDV Conference 2007. T?bingen: Gunter Narr Verlag. 231?240.  
Rapp, Reinhard; Zock, Michael (2010). Utilizing Citations of Foreign Words in Corpus-Based Dic-tionary Generation. Proceedings of NLPIX 2010. 
Sharoff, Serge (2006). Creating general-purpose cor-pora using automated search engine queries. In Marco Baroni and Silvia Bernardini (eds.): WaCky! Working papers on the Web as Corpus. Gedit, Bologna, http://wackybook.sslmit.unibo.it/ 
 
23
Steinberger, Ralf; Pouliquen, Bruno; Widiger, Anna; Ignat, Camelia; Erjavec, Toma?; Tufi?, Dan; Varga, D?niel (2006). The JRC-Acquis: A multi-lingual aligned parallel corpus with 20+ languages. Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006). Genoa, Italy.   
Wu, Dekai; Fung, Pascale (2005). Inversion trans-duction grammar constraints for mining parallel sentences from quasi-comparable corpora. Pro-ceedings of the Second International Joint Confer-ence on  Natural Language Processing (IJCNLP-2005). Jeju, Korea.   
Appendix: Gold Standard of 100 Word Equations 
 
 ENGLISH GERMAN FRENCH SPANISH ITALIAN 
1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  
anger baby bath beautiful bed Bible bitter black blossom blue boy bread butter butterfly cabbage care carpet chair cheese child citizen city cold command convenience cottage dark deep doctor dream eagle earth eat foot fruit girl green hammer hand handle hard head health heavy 
Wut Baby Bad sch?n Bett Bibel bitter schwarz Bl?te blau Junge Brot Butter Schmetterling Kohl Pflege Teppich Stuhl K?se Kind B?rger Stadt kalt Kommando Bequemlichkeit H?uschen dunkel tief Arzt Traum Adler Erde essen Fu? Frucht M?dchen gr?n Hammer Hand Griff hart Kopf Gesundheit schwer 
col?re b?b? bain beau lit Bible amer noir fleur bleu gar?on pain beurre papillon chou soin tapis chaise fromage enfant citoyen ville froid commande commodit? cabanon fonc? profond m?decin r?ve aigle terre manger pied fruit fille vert marteau main poign?e dur t?te sant? lourd 
furia beb? ba?o hermoso cama Biblia amargo negro flor azul chico pan mantequilla mariposa col cuidado alfombra silla queso ni?o ciudadano ciudad fr?o comando conveniencia casita oscuro profundo m?dico sue?o ?guila tierra comer pie fruta chica verde martillo mano manejar duro cabeza salud pesado 
rabbia bambino bagno bello letto Bibbia amaro nero fiore blu ragazzo pane burro farfalla cavolo cura tappeto sedia formaggio bambino cittadino citt? freddo comando convenienza casetta buio profondo medico sogno aquila terra mangiare piede frutta ragazza verde martello mano maniglia duro testa salute pesante 
24
45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  
high house hungry joy justice King lamp light lion long loud man memory moon mountain music mutton needle nervous ocean oven priest quick quiet red religion river rough salt scissors sheep short sickness sleep slow smooth soft soldier sour spider square stomach street sweet table thief thirsty tobacco whisky whistle white window wish woman work yellow 
hoch Haus hungrig Freude Gerechtigkeit K?nig Lampe Licht L?we lang laut Mann Ged?chtnis Mond Berg Musik Hammel Nadel nerv?s Ozean Backofen Priester schnell still rot Religion Fluss rau Salz Schere Schaf kurz Krankheit schlafen langsam glatt weich Soldat sauer Spinne Quadrat Magen Stra?e s?? Tisch Dieb durstig Tabak Whisky pfeifen wei? Fenster Wunsch Frau arbeiten gelb 
?lev? maison affam? joie justice roi lampe lumi?re lion long fort homme m?moire lune montagne musique mouton aiguille nerveux oc?an four pr?tre rapide tranquille rouge religion rivi?re rugueux sel ciseaux mouton courte maladie sommeil lent lisse doux soldat acide araign?e carr? estomac rue doux table voleur soif tabac whisky siffler blanc fen?tre d?sir femme travail jaune 
alto casa hambriento alegr?a justicia rey l?mpara luz le?n largo alto hombre memoria luna monta?a m?sica cordero aguja nervioso oc?ano horno sacerdote r?pido tranquilo rojo religi?n r?o ?spero sal tijeras oveja corto enfermedad sue?o lento liso suave soldado agrio ara?a cuadrado est?mago calle dulce mesa ladr?n sediento tabaco whisky silbar blanco ventana deseo mujer trabajo amarillo 
alto casa affamato gioia giustizia re lampada luce leone lungo alto uomo memoria luna montagna musica montone ago nervoso oceano forno sacerdote rapido tranquillo rosso religione fiume ruvido sale forbici pecora corto malattia dormire lento liscio morbido soldato acido ragno quadrato stomaco strada dolce tavolo ladro assetato tabacco whisky fischiare bianco finestra desiderio donna lavoro giallo 
25
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101?112,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Design of a hybrid high quality machine translation system 
Kurt Eberle 
Johanna Gei? 
Mireia Ginest?-Rosell 
Bogdan Babych  
Anthony Hartley 
Reinhard Rapp  
Lingenio GmbH Serge Sharoff 
Karlsruher Stra?e 10 
69 126 Heidelberg, Germany 
Martin Thomas 
Centre for Translation Studies 
University of Leeds 
 Leeds, LS2 9JT, UK 
[k.eberle,j.geiss,m.ginesti-rosell] 
@lingenio.de 
[B.Babych,A.Hartley,R.Rapp, 
S.Sharoff,M.Thomas]@leeds.ac.uk  
  
 
 
Abstract 
This paper gives an overview of the 
ongoing FP7 project HyghTra (2010 ? 
2014). The HyghTra project is conducted 
in a partnership between academia and 
industry involving the University of Leeds 
and Lingenio GmbH (company). It adopts a 
hybrid and bootstrapping approach to the 
enhancement of MT quality by applying 
rule-based analysis and statistical 
evaluation techniques to both parallel and 
comparable corpora in order to extract 
linguistic information and enrich the lexical 
and syntactic resources of the underlying 
(rule-based) MT system that is used for 
analysing the corpora. The project places 
special emphasis on the extension of 
systems to new language pairs and 
corresponding rapid, automated creation of 
high quality resources. The techniques are 
fielded and evaluated within an existing 
commercial MT environment. 
1 Motivation 
Statistical Machine Translation (SMT) has been 
around for about 20 years, and for roughly half of 
this time SMT and the 'traditional' Rule-based 
Machine Translation (RBMT) have been seen as 
competing paradigms. During the last decade 
however, there is a trend and growing interest in 
combining the two methodologies. In our approach 
these two approaches are viewed as 
complementary. 
Advantages of SMT are low cost and robustness, 
but definite disadvantages of (pure) SMT are that it 
needs huge amounts of data, which for many 
language pairs are not available and are unlikely to 
become available in the future. Also, SMT tends to 
disregard important classificatory knowledge (such 
as morphosyntactic, categorical and lexical class 
features), which can be provided and used 
relatively easily within non-statistical 
representations.  
On the other hand, advantages of RBMT are that 
its (grammar and lexical) rules and information are 
understandable by humans and can be exploited for 
a lot of applications outside of translation 
(dictionaries, text understanding, dialogue systems, 
etc.).  
The slot grammar approach used in Lingenio 
systems (cf.  McCord 1989, Eberle 2001) is a 
prime example of such linguistically rich 
representations that can be used for a number of 
different applications. Fig.1 shows this by a 
visualization of (an excerpt of) the entry for the 
ambiguous German verb einstellen in the database 
that underlies (a)  the Lingenio translation 
products, where it links up with corresponding set 
of the transfer rules, and (b) Lingenio?s dictionary 
product TranslateDict, which is primarily intended 
for human translators.   
 
101
 
 
Fig 1 a) data base entry einstellen 
('translation' represents links between SL and T entries) 
 
 
 
 
Fig 1 b) product entry einstellen 
 
The obvious disadvantages of RBMT are high cost, 
weaknesses in dealing with incorrect input and in 
making correct choices with respect to ambiguous 
words, structures, and transfer equivalents. 
SMT output is often surprisingly good with respect 
to short distance collocations, but often misses 
correct choices are missed in cases where 
selectional restrictions take effect on distant words. 
RBMT output is generally good if the parser 
assigns the correct analysis to a sentence and  if the 
target words can be correctly chosen from the set 
of alternatives. However, in the presence of 
ambiguous words and structures, and where 
linguistic information is lacking, the decisions may 
be wrong. 
Given the complementarity of SMT and RBMT 
and their very different strengths and weaknesses, 
we take a view that an optimized MT architecture 
must comprise elements of both paradigms. The 
key issue therefore lies in the identification of such 
elements and how to connect them to each other. 
We propose a specific type of hybrid translation ? 
hybrid high quality translation (HyghTra), where 
core RBMT systems are created and enhanced by a 
range of reliable statistical techniques. 
 
2 Development Methodology 
Many hybrid systems described in the literature 
have attempted to put some analytical abstraction 
on top of an SMT kernel.1 In our view this is not 
the best option because, according to the 
underlying philosophy, SMT is linguistically 
ignorant at the beginning and learns all linguistic 
rules automatically from corpora. However, the 
extracted information is typically represented in 
huge data sets which are not readable by humans in 
a natural way. This means that this type of 
architecture does not easily provide interfaces for 
incorporating linguistic knowledge in a canonical 
and simple way. 
Thus we approach the problem from the other end, 
, integrating information derived from corpora 
using statistical methods into RBMT systems. 
Provided the underlying RBMT systems are 
linguistically sound and sufficiently modular in 
structure, we believe this to have greater potential 
for generating high quality output. 
We currently use and carry out the following work 
plan: 
 
(I) Creation of MT systems  
(with rule-based core MT information and 
statistical extension and training): 
(a) We start out with declarative analysis and 
generation components of the considered 
languages, and with basic bilingual dictionaries 
connecting to one another the entries of relatively 
small vocabularies comprising the most frequent 
words of each language in a given translation pair 
(cf. Fig 1 a). 
(b) Having completed this phase, we extend the 
dictionaries and train the analysis-, transfer- and 
generation-components of the rule-based core 
systems using monolingual and bilingual corpora.  
 
                                                           
1 A prominent early example is Frederking and 
colleagues (Frederking & Nirenburg, 1994). For an 
overview of  hybrid MT till the late nineties see Streiter 
et al (1999). More recent  approaches include Groves & 
Way (2006a, 2006b). Commercial implementations 
include AppTek (http://www.apptek.com) and Language 
Weaver (http://www.languageweaver.com). An ongoing 
MT important project investigating hybrid methods is 
EuroMatrixPlus (http://www.euromatrixplus.net/) 
102
(II) Error detection and improvement cycle:  
(a) We automatically discover the most frequent 
problematic grammatical constructions and 
multiword expressions for commercial RBMT and 
SMT systems using automatic construction-based 
evaluation as proposed in (Babych and Hartley, 
2009) and develop a framework for fixing 
corresponding grammar rules and extending 
grammatical coverage of the systems in a semi-
automatic way. This shortens development time for 
commercial MT and contributes to yielding 
significantly higher translation quality. 
 
(III) Extension to other languages: 
Structural similarity and translation by pivot 
languages is used to obtain extension to further 
languages: 
High-quality translation between closely related 
languages (e.g., Russian and Ukrainian or 
Portuguese and Spanish) can be achieved with 
relatively simple resources (using linguistic 
similarity, but also homomorphism assumptions 
with respect to parallel text, if available), while 
greater efforts are put into ensuring better-quality 
translation between more distant languages (e.g. 
German and Russian). According to our prior 
research (Babych et al, 2007b) the pipeline 
between languages of different similarity results in 
improved translation quality for a larger number of 
language pairs (e.g., MT from Portuguese or 
Ukrainian into German is easier if there are high-
quality analysis and transfer modules for Spanish 
and Russian into German (respectively). Of course, 
(III) draws heavily on the detailed analysis and MT 
systems that the industrial partner in HyghTra 
provides for a number of languages. 
 
In the following sections we give more details of 
the work currently done with regard to (I) and with 
regard to parts of (II): the creation of a new MT 
system following the strategy sketched. We cannot 
go further into detail with (II) and (III) here, which 
will become a priority for future research. 
3 Creation of a new system 
Early pilot studies covering some aspects of the 
strategy described here (using information from 
pivot languages and similarity) showed promising 
results (Rapp, 1999; Rapp & Mart?n Vide, 2007; 
see also Koehn & Knight, 2002). 
We expect that the proposed semi-automatic 
creation of a new MT system as sketched above 
will work best if one of the two languages involved 
is already 'known' by modules to which the system 
has access. Against the background of the pipeline 
approach mentioned above in (III), this means that 
we assume an analysis and translation system that 
continuously grows by 'learning' new languages 
where 'learning' is facilitated by information about 
the languages already 'known' and by exploiting 
similarity assumptions ? and, of course, by being 
fed with information prepared and provided by the 
human 'companion' of the system. 
From this perspective, we assume the following 
steps of extending the system (with work done by 
the 'companion' and work done by the system) 
 
1. Acquire parallel and comparable corpora. 
2. Define a core of the morphology of the new 
language and compile a basic dictionary for the 
most frequent words and translations. 
Morphological representations and features for 
new languages are derived both manually and 
automatically, as proposed in (Babych et al, 
2012 (in preparation)). 
3. Using established alignment technology (e.g. 
Giza++) and parallel corpora, generate a first 
extension of this dictionary. 
4. Expand the dictionary of step 3 using 
comparable corpora as proposed in a study by 
Rapp (1999). This is applicable mainly to single 
word units. 
5. Expand coverage of multiword-units using 
novel technology. 
6. Cross-validate the new dictionary with respect 
to available ones by transitivity. 
7. Integrate the new dictionary into the new MT 
system as developing from reusing components 
and adding new components as in 8. 
8. Complete morphology and spell out declarative 
analysis and generation grammar for the new 
language. 
9. Automatically evaluate the translations of the 
most frequent grammatical constructions and 
multiword expressions in a machine-translated 
corpus, prioritising support for these 
constructions with a type of risk-assessment 
framework proposed in Babych and Hartley 
(2008). 
10. Extend support for high-priority constructions 
semi-automatically by mining correct 
103
translations from parallel corpora. 
11. Train and evaluate the new grammar and 
transfer of the new MT system using the new 
dictionary on the basis of available parallel 
corpora. 
 
The following sections give an overview of the 
different steps. 
Step 1: Acquire parallel and comparable 
corpora 
As our parallel corpus, we use the Europarl. The 
size of the current version is up to 40 million 
words per language, and several of the languages 
we are currently considering are covered. Also, we 
make use of other parallel corpora such as the 
Canadian Hansards (Proceedings of the Canadian 
Parliament) for the English?French language pair. 
For non-EU Languages (mainly Russian), we 
intend to conduct a pilot study to establish the 
feasibility of retrieving parallel corpora from the 
web, a problem for which various approaches have 
been proposed (Resnik, 1999; Munteanu & Marcu, 
2005; Wu & Fung, 2005).  
In addition to the parallel corpora, we will need 
large monolingual corpora in the future (at least 
200 million words) for each of the six languages. 
Here, we intend to use newspaper corpora 
supplemented with text collections downloadable 
from the web.  
The corpora are stored in a database that allows 
for assigning analyses of different depth and nature 
to the sentences and for alignment between the 
sentences and their analyses. The architecture of 
this database and the corresponding analysis and 
evaluation frontend is described in (Eberle et al
2010, 2012). Section Results contains examples of 
such representations. 
Step 2: Compile a basic dictionary for the most 
frequent words 
A prerequisite of the suggested hybrid approach 
with rule-based kernel is to define morphological 
classifications for the new language(s). This is 
done exploiting similarities to the classifications as 
available for the existing languages. Currently, this 
has been carried out for Dutch (on the basis of 
German) and for Spanish (on the basis of 
French/other Romance languages). The most 
frequent words (the basic vocabulary of a 
language) are typically also the most ambiguous 
ones. Since the Lingenio systems are lexically 
driven transfer systems (cf. Eberle 2001), we 
define (a) structural conditions,  which inform the 
choice of the possible target words (single words 
or multiword expressions) and (b)restructuring 
conditions, as necessary (cf. Fig 1 a:  attributes 
'transfer conditions' and 'structural change'). In 
order to ensure quality this must be done by human 
lexicographers and therefore costly for a large 
dictionary. However, we manually create only very 
small basic dictionaries and extend these (semi-
automatically) step 3 and those which follow. 
Some important morphosyntactic features of the 
language are derived from a monolingual corpus 
annotated with publicly available part-of-speech 
taggers and lemmatisers. However, these tools 
often do not explicitly represent linguistic features 
needed for the generation stage in RBMT. In 
(Babych et al, 2012) we propose a systematic 
approach to recovering such missing generation-
oriented representations from grammar models and 
statistical combinatorial properties of annotated 
features. 
Step 3: Generating dictionary extensions from 
parallel corpora 
Based on parallel corpora, dictionaries can be 
derived using established techniques of automatic 
sentence alignment and word alignment. For 
sentence alignment, the length-based Gale & 
Church aligner (1993) can be used, or ? 
alternatively ? Dan Melamed?s GSA-algorithm 
(Geometric Sentence Alignment; Melamed, 1999).  
For segmentation of text we use corresponding 
Lingenio-tools (unpublished).2 
For word alignment Giza++ (Och & Ney, 2003) is 
the standard tool. Given a word alignment, the 
extraction of a (SMT) dictionary is relatively 
straightforward. With the exception of sentence 
segmentation, these algorithms are largely 
language independent and can be used for all of the 
languages that we consider. We did this for a 
number of language pairs on the basis of the 
                                                           
2  If these cannot be applied because of  lack of 
information about a language, we intend to use the 
algorithm by Kiss & Strunk (2006). An open-source 
implementation of parts of the Kiss & Strunk algorithm 
is available from Patrick Tschorn at 
http://www.denkselbst.de/sentrick/index.html. 
104
Europarl-texts considered (as stored in our 
database). In order to optimize the results we use 
the dictionaries of step 1 as set of cognates (cf. 
Simard at al 1992, Gough & Way 2004), as well as 
other words easily obtainable from the internet that 
can be used for this purpose (like company names 
and other named entities with cross-language 
identity and terminology translations). Using the 
morphology component of the new language and 
the categorial information from the transfer 
relation, we compute the basic forms of the 
inflected words found. Later, we intend to further 
improve the accuracy of word alignment by 
exploiting chunk type syntactic information of the 
narrow context of the words (cf. Eberle & Rapp 
2008). An early stage variant of this is already used 
in Lingenio products. The corresponding function 
AutoLearn<word> extracts new word relations on 
the basis of existing dictionaries and (partial) 
syntactic analyses. (Fig 2 gives an example). 
 
 
 
 
 
 
 
 
 
 
Fig 2 AutoLearn<word>: new entries using 
transfer links and syntactic analysis 
 
Given the relatively small size of the available 
parallel corpora, we expect that the automatically 
generated dictionaries will comprise about 20,000 
entries each (This corresponds to first results on 
the basis of German?English). This is far too 
small for a serious general purpose MT system. 
Note that, in comparison, the English?German 
dictionary used in the current Lingenio MT 
product comprises more than 480,000 keywords 
and phrases. 
Step 4: Expanding dictionaries using 
comparable corpora (word equations) 
In order to expand the dictionaries using a set of 
monolingual comparable corpora, the basic 
approach pioneered by Fung & McKeown (1997) 
and Rapp (1995, 1999) is to be further developed 
and refined in the second phase of the project as to 
obtain a practical tool that can be used in an 
industrial context. 
The basic assumption underlying the approach 
is that across languages there is a correlation 
between the co-occurrences of words that are 
translations of each other. If ? for example ? in a 
text of one language two words A and B co-occur 
more often than expected by chance, then in a text 
of another language those words that are 
translations of A and B should also co-occur more 
frequently than expected. It is further assumed that 
a small dictionary (as generated in step 2) is 
available at the beginning, and that the aim is to 
expand this basic lexicon. Using a corpus of the 
target language, first a co-occurrence matrix is 
computed whose rows are all word types occurring 
in the corpus and whose columns are all target 
words appearing in the basic lexicon. Next a word 
of the source language is considered whose 
translation is to be determined. Using the source-
language corpus, a co-occurrence vector for this 
word is computed. Then all known words in this 
vector are translated to the target language. As the 
basic lexicon is small, only some of the 
translations are known. All unknown words are 
discarded from the vector and the vector positions 
are sorted in order to match the vectors of the 
target-language matrix. Using standard measures 
for vector similarity, the resulting vector is 
compared to all vectors in the co-occurrence 
matrix of the target language. The vector with the 
highest similarity is considered to be the 
translation of our source-language word. 
From a previous pilot study (Rapp, 1999) it can 
be expected that this methodology achieves an 
accuracy in the order of 70%, which means that 
only a relatively modest amount of manual post-
editing is required.  
The automatically generated results are 
improved and the amount of post-editing is 
reduced by exploiting sense (disambiguation) 
information as available from the analysis 
component for the 'known' language of the new 
language pair.. Also we try to exploit categorial 
and underspecified syntactic information of the 
contexts of the words similar to what has been 
suggested for improving word alignment in the 
previous step (see also Fig.2). Also, as the frequent 
words are already covered by the basic lexicon 
(whose production from parallel corpora on the 
basis of a manually compiled kernel does not show 
 
105
an ambiguity problem of similar significance), and 
as experience shows that most low frequency 
words in a full-size lexicon tend to be 
unambiguous, the ambiguity problem is reduced 
further for the words investigated and extracted by 
this comparison method. 
Step 5: Expanding dictionaries using 
comparable corpora (multiword units) 
In order to account for technical terms, idioms, 
collocations, and typical short phrases, an 
important feature of an MT lexicon is a high 
coverage of multiword units. Very recent work 
conducted at the University of Leeds (Sharoff et 
al., 2006) shows that dictionary entries for such 
multiword units can be derived from comparable 
corpora if a dictionary of single words is available. 
It could even be shown that this methodology can 
be superior to deriving multiword-units from 
parallel corpora (Babych et al, 2007). This is a 
major breakthrough as comparable corpora are far 
easier to acquire than parallel corpora. It even 
opens up the possibility of building domain-
specific dictionaries by using texts from different 
domains. 
The outline of the algorithm is as follows: 
? Extract collocations from a corpus of the 
source language (Smadja, 1993) 
? To translate a collocation, look up all its 
words using any dictionary 
? Generate all possible permutations 
(sequences) of the word translations 
? Count the occurrence frequencies of these 
sequences in a corpus of the target 
language and test for significance 
? Consider the most significant sequence to 
be the translation of the source language 
collocation 
Of course, in later steps of the project, we will 
experiment on filtering these sequences by 
exploiting structural knowledge similarly to what 
was described in the two previous steps. This can 
be obtained on the basis of the declarative analysis 
component of the new language which is 
developed in parallel. 
Step 6: Cross-validate dictionaries 
The combination of the corpus-based methods for 
automatic dictionary generation as described in 
steps 3 to 5 will lead to high coverage dictionaries 
as the availability of very large monolingual 
corpora is no major problem for our languages. 
However, as all steps are error prone, it can be 
expected that a considerable number of dictionary 
entries (e.g. 50%) are not correct. To facilitate (but 
not eliminate) the manual verification of the 
dictionary, we will  perform an automatic cross-
check which utilizes the dictionaries? property of 
transitivity. What we mean by this is that if we 
have two dictionaries, one translating from 
language A to language B, the other from language 
B to language C, then we can also translate from 
language A to C by use of the intermediate 
language (or interlingua) B. That is, the property of 
transitivity, although having some limitations due 
to ambiguity problems, can be exploited to 
automatically generate a raw dictionary for A to C. 
Lingenio  has some experience with this method 
having exploited it for extending and improving its 
English ? French dictionaries using French ? 
German and German ? English. 
As the corpus-based approach (steps 3 to 5) 
allows us to also generate this type of dictionary  
via comparable corpora, we have two different 
ways to generate a dictionary for a particular 
language pair. This means that we can validate one 
with the other. Furthermore, with increasing 
number of language pairs created, there are more 
and more languages that can serve as interlingua or 
'pivot': This, step by step, gives an increasing 
potential for mutual cross-validation.  
Specific attention will be paid to automating as 
far as possible the creation of selectional 
restrictions to be assigned to the transfer relations 
of the new dictionaries in all steps of dictionary 
creation (2?6). We will try to do this on the basis 
of the analysis components as available for the 
languages considered: These are: a completely 
worked out analysis component for the 'old' 
language, a declarative (chunk parsing) component 
for the new one (compare the two following steps 
for this).  
Step 7: Integrate dictionaries in existing 
machine translation systems 
Lingenio has a relatively rich infrastructure for 
automatic importation of various kinds of lexical 
information into the database used by the analyses 
and translation systems. If necessary the 
information on hand (for instance from 
conventional dictionaries of publishing houses) is 
106
completed and normalized during or before 
importation. This may be executed completely 
automatically ? by using the existing analyses 
components and resources respectively as 
databases ? or interactively ? by asking the 
lexicographer for additional information, if needed. 
For example, there may be a list of multiword 
expressions to be imported into the database. In 
order to have available correct syntactic and 
semantic information for these expressions, they 
are analysed by the parser of the corresponding 
language. From the analysis found, the information 
necessary to describe the new lemma in the lexicon 
with respect to semantic type and syntactic 
structure is obtained. The same information is used 
to automatically create correct restructuring 
constraints for translation relations which use the 
new lemma as target. If the parser does not find a 
sound syntactic description, for example because 
some basic information or the expression is 
missing in the lexical database, the lexicographer is 
asked for the missing information or is handed 
over the expression to code it manually.  
Using these tools importation of new lexical 
information, as provided in the previous steps, is 
considerably accelerated.  
Step 8: Compile rule bases for new language 
pairs 
Although experience clearly shows that 
construction and maintenance of the dictionaries is 
by far the most expensive task in (rule-based) 
Machine Translation, the grammars (analysis and 
generation) must of course be developed and 
maintained also. Lingenio has longstanding 
experience with the development of grammars, 
dictionaries and all other components of RBMT.  
The used grammar formalism (slot grammar, 
cf. McCord 1991) is unification based and its 
structuring focuses on dependency, where phrases 
are analysed into heads and grammatical roles ? so 
called (complement and adjunct) slots.  
The grammar formalism and basic rule types 
are designed in a very general way in order to 
allow good portability from one language to 
another such that spelling out the declarative part 
of a grammar does not take very much time (2-4 
person months approx. for relatively similar 
languages like Romance languages according to 
our experience). The portation of linguistic rules to 
new languages is also facilitated by the modular 
design with clearly defined interfaces that make it 
relatively straightforward to integrate information 
from corpora. 
Given a parallel corpus as acquired in step 1, 
the following procedure defines grammar develop-
ment:  
 
1. Define a declarative grammar for the new 
language and train this grammar on the parallel 
-corpus according to the following steps: 
2. Use a chunk parser for the grammar on the 
basis of an efficient part-of-speech tagger for 
the new language.  
3. Combine the chunk analyses of the sentence, 
according to suggestions for packed syntactic 
structures (cf. Schiehlen 2001 and others) and 
underspecified representation structures 
respectively (cf. Eberle, 2004, and others), 
such that the result represents a disjunction of 
the possible analyses of the sentence. 
4. Filter the alternatives of the representation by 
using mapping constraints between source and 
target sentence as can be computed from the 
lexical transfer relations and the structural 
analysis of the sentence. For instance, if we 
know, as in the example of the last section, that 
in the source sentence there is a relative clause 
with lexical elements A, B, . . . modifying a 
head H and that there are translations TH, TA, 
TB, . . . of H, A, B,. . . , in the target sentence 
which, among other possibilities, can be 
supposed to stand in a similar structural 
relation there, then we prefer this relation to 
the competing structural possibilities. (Fig. 3 in 
section results shows the corresponding 
selection for a German-Spanish example in the 
project database). 
5. For each of the remaining structural 
possibilities of the thus revised underspecified 
representation, take its lexical material and 
underspecified structuring as a context for its 
successful firing. For instance, if the 
possibility is left that O is the direct object of 
VP, where VP is an underspecified verbal 
phrase and O an underspecified nominal 
phrase (i.e. where details of the substructuring 
are not spelled out), take the sentence as a 
reference for direct object complementation 
and O and VP as contexts which accept this 
complementation. 
107
6. Develop more abstract conditions from the 
conditions learned according to (5) and 
integrate the different cases. 
7. Tune the results using standard methods of 
corpus-based linguistics. Among other things 
this means: Distinguish between training and 
test corpora, adjust weights according to the 
results of test runs, etc. 
 
The basic idea of the proposed learning procedure 
is similar to that used with respect to learning 
lexical transfer relations: Do not define the 
statistical model for the ?ignorant? state, where the 
surface items of the bilingual corpora are 
considered. Instead, define it for appropriate 
maximally abstract analyses of the sentences 
(which, of course, must be available 
automatically), because, then, much smaller sets of 
data will do. Here, the important question is: What 
is the most abstract level of representation that can 
be reached automatically and which shows reliable 
results? We think that it is the level of 
underspecified syntactic description as used in the 
procedure above. 
The result of training the grammar is a set of 
rules which assign weights and contexts to each 
filler rule of the declarative grammar and thus 
allow to estimate how likely it is that a particular 
rule is applied in a particular context in comparison 
with other rules (Fig. 4 and 5 in section results 
give an overview of the relevance of  grammar 
rules and their triggering conditions w.r.t. 
German).  
We mentioned that the task of translating texts 
into each other does not presuppose that each 
ambiguity in a source sentence is resolved. On the 
contrary, translation should be ambiguity 
preserving (cf. Kay, Gawron & Norvig 1994, 
compare the example above). It is obvious that 
underspecified syntactic representations as 
suggested here are also especially suited for 
preserving ambiguities appropriately.  
Step 9: Automatically evaluate translations of 
the most frequent grammatical constructions 
and multiword expressions in a machine-
translated corpus 
In a later work package of the project, we will run 
a large parallel corpus through available 
(competitive) MT engines, which will be enhanced 
by automatic dictionaries developed during the 
previous stages. On the source-language side of the 
corpus we will automatically generate lists of 
frequent multiword expressions (MWEs) and 
grammatical constructions using the methodology 
proposed in (Sharoff et al, 2006). For each of the 
identified MWEs and constructions we will 
generate a parallel concordance using open-source 
CSAR architecture developed by the Leeds team 
(Sharoff, 2006). The concordance will be 
generated by running queries to the sentence-
aligned parallel corpora and will return lists of 
corresponding sentences from gold-standard 
human translations and corresponding sentences 
generated by MT. Each of these concordances will 
be automatically evaluated using standard MT 
evaluation metrics, such as BLEU. Under these 
settings parallel concordances will be used as 
standard MT evaluation corpora in an automated 
MT evaluation scenario. 
Normally BLEU gives reliable results for MT 
corpora over 7000 words. However, in (Babych 
and Hartley, 2009; Babych and Hartley, 2008) we 
demonstrated that if the corpus is constructed in 
this controlled way, where evaluated fragments of 
sentences are selected as local contexts for specific 
multiword expressions or grammatical 
constructions, then BLEU scores have another 
?island of stability? for much smaller corpora, 
which now may consist of only five or more 
aligned concordance lines. This concordance-based 
evaluation scenario gives correct predictions of 
translation quality for the local context of each of 
the evaluated expressions. 
The scores for the evaluated MWEs and 
constructions will be put in a risk-assessment 
framework, where we will balance the frequency 
of constructions and their translation quality. The 
top priority receive the most frequent expressions 
that are the most problematic ones for a particular 
MT engine, i.e., with queries with lowest BLEU 
scores for their concordances. This framework will 
allow MT developers to work down the priority list 
and correct or extend coverage for those 
constructions which will have the biggest impact 
on MT quality. 
Step 10: Extend support for high-priority 
constructions semi-automatically by mining 
correct translations from parallel corpora 
At this stage we will automate the procedure of 
correcting errors and extending coverage for 
108
problematic MWEs and grammatical 
constructions, identified in Step 9. For this we will 
exploit alignment between source-language 
sentences and gold-standard human translations. In 
the target human translations we will identify 
linguistically-motivated multiword expressions, 
e.g., using part-of-speech patterns or tf-idf 
distribution templates (Babych et al, 2007) and 
run standard alignment tools (e.g., GIZA++) for 
finding the most probable candidate MWEs that 
correspond to the problematic source-language 
expressions. Source and target MWEs paired in 
this way will form the basis for automatically-
generated grammar rules. The rules will normally 
generalise several pairs of MWEs, and may be 
underspecified for certain lexical or morphological 
features. Later such rules will be manually checked 
and corrected by language specialists in MT 
development teams that work on specific 
translation directions. 
This procedure will allow to speed up the grammar 
development procedure for large-scale MT projects 
and will focus on grammatical constructions with 
the highest impact on MT quality, establishing 
them as a top priority for MT developers. In 
HyghTra and with respect to the languages 
considered there, this procedure will be integrated 
into the grammar development and optimization of 
step 8, in particular it will be related to step 4 of 
the procedure sketched there. With regard to 
integration, we aim at an interleaved architecture in 
the long run.  
Step 11: Bootstrap the system 
In Step 11, the new grammar and the transfer of 
the new MT system and the new dictionary may be 
mutually trained further using the steps before and 
applying the system to additional corpora. 
 
4 Results 
Declarative slot grammars for Dutch and Spanish 
have been developed using the patterns of German 
and French ? where declarative  means that there 
has been used no relevant semantic or other 
information in order to spell out weighting or 
filters for rule application -- the only constraint 
being morphosyntactic accessibility. The necessary 
morphological information has been adapted 
similarly from the corresponding model languages. 
The basic dictionaries have been compiled 
manually (Dutch) or extracted from a conventional 
electronic dictionary (translateDict Spanish).  
For a subset of the Spanish corpus (reference 
sentences of the grammar, parts of the open source 
Leeds corpus (Sharoff, 2006), and Europarl), 
syntactic analyses have been computed and stored 
in the database. As the number of analyses grows 
extremely with the length of sentences, only 
relatively short sentences (up to 15 words)  have 
been considered. These analyses are currently 
compared to the analyses of the German 
translations of the corresponding sentences (one 
translation per sentence), which are taken as a kind 
of 'gold' standard as the German analysis 
component (as part of the translation products) has 
proven to be sufficiently reliable. On the basis of 
the comparison a preference on the competitive 
analyses of the Spanish sentence is entailed and 
used for defining a statistical evaluation 
component for the Spanish grammar. Fig.3 shows 
the corresponding representations in the database 
for the sentence Aumenta la demana de energ?a 
el?ctrica por la ola de calor3  and its translation die 
Nachfrage nach Strom steigt wegen der 
Hitzewelle/the demand for electricity increases 
because of the heat-wave. 
 
 
 
 
 
 
 
 
 
 
Fig.3 Selection of analyses via correspondences 
(prefer first Spanish analysis because of subj-congruity) 
 
The analyses are associated with the corresponding 
creation protocols, which are structured lists whose 
items describe, via the identifiers, which rule has 
been applied when and to what structures in the 
process of creating the analysis. From the selection 
of a best analysis for a sentence, we can entail the 
circumstances under which the application of 
particular rules are preferred. This has been carried 
                                                           
3 Sentence taken from the online newspaper El D?a de 
Concepci?n del Uruguay 
 
 
109
out - not yet for the 'new' language Spanish, but for 
the 'known' language German, in order to obtain a 
measure about how correctly the existing grammar 
evaluation component can be replaced by the 
results of the corresponding statistical study.  
 
Fig.4  Frequency of applications of rules 
 
 
 cluster 
applications 
similarity feas  mod feas head 
383, 384,.. 0,86 sent, ... emosentaffv,.. 
557,558,566,.. 0,68 denselb,.. gebv, ... 
 
Fig.5  Preliminary constraints related to grammar 
rule clusters 
 
Fig.4 shows the distribution of rule usages within 
the training set of analyses (of approx.30.000 
sentences). 390 different rules were used with a 
total of 133708 rule applications. The subject rule 
(383) and the noun determiner rule (46) the most 
used rules (35% of all applications). Fig 5. 
illustrates the preliminary results of a clustering 
algorithm where different rule applications are 
grouped into clusters and the key features of the 
head and modifier phrases for each cluster are 
extracted. 
Currently, we try to determine further and tare 
the linguistic features and the weighting which 
models best the evaluation for German. (The gold 
standard that is used in this test is the set of 
analyses mentioned above). The investigations are 
not yet completed, but preliminary results on the 
basis of the morphosyntactic and semantic 
properties of the neighboring elements are 
promising. After consolidation, the findings will be 
transferred to Spanish on the basis of the selection 
procedure illustrated in Fig. 3. The next step of 
grammar training in the immediate future will 
consist of  changing the focus to underspecified 
analyses as described in step 8 
5 Conclusions 
The project tries to make state-of-the-art statistical 
methods available for dictionary development and 
grammar development for a rule-based dominated 
industrial setting and to exploit such methods 
there.  
With regard to SMT dictionary creation, it goes 
beyond the current state of the art as it also aims at 
developing and applying algorithms for the semi-
automatic generation of bilingual dictionaries from 
unrelated monolingual (i.e., comparable) corpora 
of the source and the target language, instead of 
using relatively literally translated (i.e., parallel) 
texts only. Comparable corpora are far easier to 
obtain than parallel corpora. Therefore the 
approach offers a solution to the serious data 
acquisition bottleneck in SMT. This approach is 
also more cognitively plausible than previous 
suggestions on this topic, since human bilinguality 
is normally not based on memorizing parallel texts. 
Our suggestion models human capacity to translate 
texts using linguistic knowledge acquired from 
monolingual data, so it also exemplifies many 
more features of a truly self-learning MT system 
(shared also by a human translator).  
In addition, the proposal suggests a new 
method for spelling out grammars and parsers for 
languages by splitting grammars into declarative 
kernels and trainable decision algorithms and by 
exploiting cross-linguistic knowledge for 
optimizing the results of the corresponding parsers.   
For developing different components and 
dictionaries for the system a bootstrapping 
architecture is suggested that uses the acquired 
lexical information for training the grammar of the 
new language, which in turn uses the 
(underspecified) parser results for optimizing the 
lexical information in the corresponding translation 
dictionaries. We expect that the suggested methods 
significantly improve translation quality and 
reduce the costs of creating new language pairs for 
Machine Translation. The preliminary results 
obtained so far in the project appear promising. 
6 Acknowledgments 
This research is supported by a Marie Curie IAPP 
project taking place within the 7th European 
Community Framework Programme (Grant 
agreement no.: 251534) 
110
7 References 
Armstrong, S.; Kempen, M.; McKelvie, D.; Petitpierre, D.; 
Rapp, R.; Thompson, H. (1998). Multilingual Corpora 
for Cooperation. Proceedings of the 1st International 
Conference on Linguistic Resources and Evaluation 
(LREC), Granada, Vol. 2, 975?980. 
Babych, B., Hartley, A., Sharoff S.; Mudraya, O. (2007). 
Assisting Translators in Indirect Lexical Transfer. 
Proceedings of the 45th Annual Meeting of the ACL.  
Babych, B., Anthony Hartley, & Serge Sharoff (2007b) 
Translating from under-resourced languages: 
comparing direct transfer against pivot translation. 
Proceedings of MT Summit XI, 10-14 September 
2007, Copenhagen, Denmark, 29-35 
Babych, B. & Hartley, A. (2008). Automated MT Evaluation 
for Error Analysis: Automatic Discovery of Potential 
Translation Errors for Multiword Expressions. ELRA 
Workshop on Evaluation ?Looking into the Future of 
Evaluation: When automatic metrics meet task-based  
and performance-based approaches?. Marrakech, 
Morocco 27 May 2008. Proceedings of LREC?08. 
Babych, B. and Hartley, A. (2009). Automated error analysis 
for multiword expressions: using BLEU-type scores 
for automatic discovery of potential translation errors. 
Linguistica Antverpiensia, New Series (8/2009): 
Journal of translation and interpreting studies. Special 
Issue on Evaluation of Translation Technology. 
Babych, B., Babych, S. and Eberle, K. (2012). Deriving 
generation-oriented MT resources from corpora: case 
study and evaluation of de/het classification for Dutch 
Noun (in preparation) 
Baroni, M.; Bernardini, S. (2004). BootCaT: Bootstrapping 
corpora and terms from the web. Proceedings of 
LREC 2004.  
Callison-Burch, C., Miles Osborne, & Philipp Koehn: Re-
evaluating the role of BLEU in machine translation 
research. EACL-2006: 11th Conference of the 
European Chapter of the Association for 
Computational Linguistics, Trento, Italy, April 3-7, 
2006; pp.249-256  
Charniak, E.; Knight, K.; Yamada, K. (2003). Syntax-based 
language models for statistical machine translation". 
Proceedings of MT Summit IX. 
Eberle, Kurt (2001). FUDR-based MT, head switching and the 
lexicon. Proceedings of the the eighth Machine 
Translation Summit, Santiage de Compostela.  
Eberle, Kurt (2004). Flat underspecified representation and its 
meaning for a fragment of German. 
Habilitationsschrift, Universit?t Stuttgart. 
Eberle, K.; Rapp, R. (2008). Rapid Construction of 
Explicative Dictionaries Using Hybrid Machine 
Translation. In: Storrer, A.;  Geyken, A.; Siebert, A.; 
W?rzner, K._M (eds.) Text Resources and Lexical 
Knowledge: Selected Papers from the 9th Conference 
on Natural Language Processing KONVENS 2008. 
Berlin: Mouton de Gruyter..  
Eckart,K., Eberle, K.; Heid, U. (2010) An infrastructure for 
more reliable corpus analysis. Proceedings of the 
Workshop on Web Services and Processing Pipelines 
in HLT of LREC-2010 , Valetta. 
Eberle, K.; Eckart,K., Heid, U.,Haselbach, B. (2012) A 
tool/database interface for multi-level analyses. 
Proceedings of LREC-2012 , Istanbul. 
Frederking, R.; Nirenburg, S.; Farwell, D.;  Helmreich, S.; 
Hovy, E.; Knight, K.; Beale, S.; Domashnev, C.; 
Attardo, D.; Grannes, D.; Brown, R. (1994). Integrated 
Translation from Multiple Sources within the Pangloss 
MARK II Machine Translation System. Proceedings 
of Machine Translation of the Americas, 73?80. 
Frederking, Robert and Sergei Nirenburg (1994). Three heads 
are better than one. In: Proceedings of ANLP-94, 
Stuttgart, Germany.  
Fung, P.; McKeown, K. (1997). Finding terminology 
translations from non-parallel corpora. Proceedings of 
the 5th Annual Workshop on Very Large Corpora, 
Hong Kong: August 1997, 192-202.  
Gale, W.A.; Church, K.W. (1993). A progrm for aligning 
sentences in bilingual corpora. Computational 
Linguistics, 19(1), 75?102. 
Gonz?lez, J.; Antonio L. Lagarda, Jos? R. Navarro, Laura 
Eliodoro, Adri? Gim?nez, Francisco Casacuberta, Joan 
M. de Val and Ferran Fabregat (2004). SisHiTra: A 
Spanish-to-Catalan hybrid machine translation system. 
Berlin: Springer LNCS. 
Gough, N., Way, A. (2004). Example-Based Controlled 
Translation. Proceedings of the Ninth Workshop of the 
European Association for Machine Translation, 
Valetta, Malta.  
Groves, D. & Way, A. (2006b). Hybridity in MT: Experiments 
on the Europarl Corpus. In Proceedings of the 11th 
Conference of the European Association for Machine 
Translation, Oslo, Norway, 115?124. 
Groves, D.; Way, A. (2006a). Hybrid data-driven models of 
machine translation. Machine Translation, 19(3?4). 
Special Issue on Example-Based Machine Translation. 
301?323. 
Habash, N.; Dorr, B. (2002). Handling translation 
divergences: Combining statistical and symbolic 
techniques in generation-heavy machine translation. 
Proceedings of AMTA-2002, Tiburon, California, 
USA. 
Kiss, T.; Strunk, J. (2006): Unsupervised multilingual 
sentence boundary detection. Computational 
Linguistics 32(4), 485?525. 
Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical 
Machine Translation. Proceedings of MT Summit X, 
Phuket, Thailand 
Koehn, P.; Knight, K. (2002). Learning a translation lexicon 
from monolingual corpora. In: Proceedings of ACL-02 
Workshop on Unsupervised Lexical Acquisition, 
Philadelphia PA. 
Language Industry Monitor (1992). Statistical methods 
gaining ground. In: Language Industry Monitor, 
September/October 1992 issue. 
111
McCord, M. (1989). A new version of the machine translation 
system LMT.  Journal of Literary and Linguistic 
Computing, 4, 218?299. 
McCord, M. (1991). The slot grammar system.  In: Wedekind, 
J., Rohrer, C.(eds): Unification in Grammar, MIT-
Press. 
Melamed, I. Dan (1999). Bitext maps and aligment via pattern 
recognition. Computational Linguistics, 25(1), 107?
130. 
Munteanu, D.S.; Marcu, D. (2005). Improving machine 
translation performance by exploiting non-parallel 
corpora. Computational Linguistics, 31(4), 477?504. 
Och, F.J.; Ney, H. (2002). Discriminative trainig and 
maximum entropy models for statistical machine 
translation. Proceedings of the  Annual Meeting of the 
Association for Computational Linguistics, 
Philadelphia, PA, 295?302.  
Och, F.J.; Ney, H. (2003). A systematic comparison of various 
statistical alignment models. Computational 
Linguistics, 29(1), 19?51. 
Papineni, K.; Roukos, S.; Ward, T.; Zhu, W. (2002). BLEU: A 
method for automatic evaluation of machine 
translation. In: Proceedings of the 40th Annual 
Meeting of the ACL, Philadelphia, PA, 311?318. 
Rapp, R. (1995). Identifying word translations in non-parallel 
texts. In: Proceedings of the 33rd Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, 1995, 320?322 
Rapp, R. (1999). Automatic identification of word translations 
from unrelated English and German corpora. In: 
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics 1999, College 
Park, Maryland. 519?526. 
Rapp, R. (2004). A freely available automatically generated 
thesaurus of related words. In: Proceedings of the 
Fourth International Conference on Language 
Resources and Evaluation (LREC), Lisbon, Vol. II, 
395?398. 
Rapp, R.; Martin Vide, C. (2007). Statistical machine 
translation without parallel corpora. In: Georg Rehm, 
Andreas Witt, Lothar Lemnitzer (eds.): Data 
Structures for Linguistic Resources and Applications. 
Proceedings of the Biennial GLDV Conference 2007. 
T?bingen: Gunter Narr. 231?240 
Resnik, R. (1999). Mining the web for bilingual text. 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics. 
Sato, S.; Nagao, M. (1990). Toward memory-based 
translation. Proceedings of COLING 1990, 247?252. 
Schiehlen, M. (2001) Syntactic Underspecification. In: Special 
Research Area 340 ? Final report, University of 
Stuttgart.  
Sharoff, S. (2006) Open-source corpora: using the net to fish 
for linguistic data. In International Journal of Corpus 
Linguistics 11(4), 435?462.  
Sharoff, S.; Babych, B.; Hartley, A. (2006). Using comparable 
corpora to solve problems difficult for human 
translators. In: Proceedings of COLING/ACL 2006, 
739?746.  
Sharoff, S. (2006). A uniform interface to large-scale 
linguistic resources. In Proceedings of the Fifth 
Language Resources and Evaluation Conference, 
LREC-2006, Genoa. 
Simard, M., Foster, G., Isabelle, P. (1992). Using Cognates to 
Align Sentences in Bilingual Corpora. Proceeedings of 
the International Conference on Theoretical and 
Methodological Issues, Montr?al. 
Smadja, F. (1993). Retrieving collocations from text: Xtract. 
Computational Linguistics, 19(1), 143?177. 
Streiter, O., Carl, M., Haller, J. (eds)(1999). Hybrid 
Approaches to Machine Translation. IAI working 
papers 36. 
Streiter, O.; Carl, M.; Iomdin, L.L.: 2000, A Virtual 
Translation Machine for Hybrid Machine Translation'. 
In: Proceedings of the Dialogue'2000 International 
Seminar in Computational Linguistics and 
Applications. Tarusa, Russia.  
Streiter, O.; Iomdin, L.L. (2000). Learning Lessons from 
Bilingual Corpora: Benefits for Machine Translation. 
International Journal of Corpus Linguistics, 5(2), 199?
230. 
Thurmair, G. (2005). Hybrid architectures for machine 
translation systems. Language Resources and 
Evaluation, 39 (1), 91?108. 
Thurmair, G. (2006). Using corpus information to improve 
MT quality. Proceedings of the LR4Trans-III 
Workshop, LREC, Genova. 
Thurmair, G. (2007) Automatic evaluation in MT system 
production. MT Summit XI Workshop: Automatic 
procedures in MT evaluation, 11 September 2007, 
Copenhagen, Denmark, 
Veronis, Jean (2006). Technologies du Langue. Actualit?s ? 
Comentaires ? R?flexions. Translation. Systran or 
Reverso? 
http://aixtal.blogspot.com/2006/01/translation-systran-
or-reverso.html  
Wu, D., Fung, P. (2005). Inversion transduction grammar 
constraints for mining parallel sentences from quasi-
comparable corpora. Second International Joint 
Conference on Natural Language Processing 
(IJCNLP-2005). Jeju, Korea. 
 
112
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 1?6,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Workshop on Hybrid Approaches to Translation:
Overview and Developments
Marta R. Costa-jussa`, Rafael E. Banchs
Institute for Infocomm Research1
Patrik Lambert
Barcelona Media3
Kurt Eberle
Lingenio GmbH4
Reinhard Rapp
Aix-Marseille Universite?, LIF2
Bogdan Babych
University of Leeds5
1{vismrc,rembanchs}@i2r.a-star.edu.sg, 2reinhardrapp@gmx.de,
3patrik.lambert@barcelonamedia.org, 4k.eberle@lingenio.de,
5b.babych@leeds.ac.uk
Abstract
A current increasing trend in machine
translation is to combine data-driven and
rule-based techniques. Such combinations
typically involve the hybridization of dif-
ferent paradigms such as, for instance,
the introduction of linguistic knowledge
into statistical paradigms, the incorpora-
tion of data-driven components into rule-
based paradigms, or the pre- and post-
processing of either sort of translation sys-
tem outputs. Aiming at bringing together
researchers and practitioners from the dif-
ferent multidisciplinary areas working in
these directions, as well as at creating a
brainstorming and discussion venue for
Hybrid Translation approaches, the Hy-
Tra initiative was born. This paper gives
an overview of the Second Workshop on
Hybrid Approaches to Translation (HyTra
2013) concerning its motivation, contents
and outcomes.
1 Introduction
Machine translation (MT) has continuously been
evolving from different perspectives. Early sys-
tems were basically dictionary-based. These ap-
proaches were further developed to more complex
systems based on analysis, transfer and genera-
tion. The objective was to climb up (and down)
in the well-known Vauquois pyramid (see Figure
1) to facilitate the transfer phase or to even mini-
mize the transfer by using an interlingua system.
But then, corpus-based approaches irrupted, gen-
erating a turning point in the field by putting aside
the analysis, generation and transfer phases.
Although there had been such a tendency right
from the beginning (Wilks, 1994), in the last
Figure 1: Vauquois pyramid (image from
Wikipedia).
years, the corpus-based approaches have reached
a point where many researchers assume that rely-
ing exclusively on data might have serious limi-
tations. Therefore, research has focused either on
syntactical/hierarchical-based methods or on try-
ing to augment the popular phrase-based systems
by incorporating linguistic knowledge. In addi-
tion, and given the fact that research on rule-based
has never stopped, there have been several propos-
als of hybrid architectures combining both rule-
based and data-driven approaches.
In summary, there is currently a clear trend to-
wards hybridization, with researchers adding mor-
phological, syntactic and semantic knowledge to
statistical systems, as well as combining data-
driven methods with existing rule-based systems.
In this paper we provide a general overview
of current approaches to hybrid MT within the
context of the Second Workshop on Hybrid Ap-
proaches to Translation (HyTra 2013). In our
overview, we classify hybrid MT approaches ac-
cording to the linguistic levels that they address.
We then briefly summarize the contributions pre-
sented and collected in this volume.
1
The paper is organized as follows. First, we mo-
tivate and summarize the main aspects of the Hy-
Tra initiative. Then, we present a general overview
of the accepted papers and discuss them within
the context of other state-of-the-art research in the
area. Finally, we present our conclusions and dis-
cuss our proposed view of future directions for
Hybrid MT research.
2 Overview of the HyTra Initiative
The HyTra initiative started in response to the in-
creasing interest in hybrid approaches to machine
translation, which is reflected on the substantial
amount of work conducted on this topic. An-
other important motivation was the observation
that, up to now, no single paradigm has been able
to successfully solve to a satisfactory extent all of
the many challenges that the problem of machine
translation poses.
The first HyTra workshop took part in conjunc-
tion with the EACL 2012 conference (Costa-jussa`
et al, 2012). The Second HyTra Workshop, which
was co-organized by the authors of this paper, has
been co-located with the ACL 2013 conference
(Costa-jussa` et al, 2013). The workshop has been
supported by an extensive programme committee
comprising members from over 30 organizations
and representing more than 20 countries. As the
outcome of a comprehensive peer reviewing pro-
cess, and based on the recommendations of the
programme committee, 15 papers were finally se-
lected for either oral or poster presentation at the
workshop.
The workshop also had the privilege to be hon-
ored by two exceptional keynote speeches:
? Controlled Ascent: Imbuing Statistical MT
with Linguistic Knowledge by Will Lewis and
Chris Quirk (2013), Microsoft research. The
intersection of rule-based and statistical ap-
proaches in MT is explored, with a particular
focus on past and current work done at Mi-
crosoft Research. One of their motivations
for a hybrid approach is the observation that
the times are over when huge improvements
in translation quality were possible by sim-
ply adding more data to statistical systems.
The reason is that most of the readily avail-
able parallel data has already been found.
? How much hybridity do we have? by Her-
mann Ney, RWTH Aachen. It is pointed
out that after about 25 years the statistical
approach to MT has been widely accepted
as an alternative to the classical approach
with manually designed rules. But in prac-
tice most statistical MT systems make use
of manually designed rules at least for pre-
processing in order to improve MT quality.
This is exemplified by looking at the RWTH
MT systems.
3 Hybrid Approaches Organized by
Linguistic Levels
?Hybridization? of MT can be understood as com-
bination of several MT systems (possibly of very
different architecture) where the single systems
translate in parallel and compete for the best re-
sult (which is chosen by the integrating meta sys-
tem). The workshop and the papers do not fo-
cus on this ?coarse-grained? hybridization (Eisele
et al, 2008), but on a more ?fine grained? one
where the systems mix information from differ-
ent levels of linguistic representations (see Fig-
ure 2). In the past and mostly in the framework
of rule-based machine translation (RBMT) it has
been experimented with information from nearly
every level including phonetics and phonology
for speech recognition and synthesis in speech-
to-speech systems (Wahlster, 2000) and includ-
ing pragmatics for dialog translation (Batliner et
al., 2000a; Batliner et al, 2000b) and text coher-
ence phenomena (Le Nagard and Koehn, 2010).
With respect to work with emphasis on statisti-
cal machine translation (SMT) and derivations of
it mainly those information levels have been used
that address text in the sense of sets of sentences.
As most of the workshop papers relate to this
perspective - i.e. on hybridization which is de-
fined using SMT as backbone, in this introduc-
tion we can do with distinguishing between ap-
proaches focused on morphology, syntax, and se-
mantics. There are of course approaches which
deal with more than one of these levels in an in-
tegrated manner, which are commonly refered to
as multilevel approaches. As the case of treat-
ing syntax and morphology concurrently is espe-
cially common, we also consider morpho-syntax
as a separate multilevel approach.
3.1 Morphological approaches
The main approaches of statistical MT that ex-
ploit morphology can be classified into segmen-
tation, generation, and enriching approaches. The
2
Figure 2: Major linguistic levels (image from
Wikipedia).
first one attempts to minimize the vocabulary of
highly inflected languages in order to symmetrize
the (lexical granularity of the) source and the tar-
get language. The second one assumes that, due
to data sparseness, not all morphological forms
can be learned from parallel corpora and, there-
fore, proposes techniques to learn new morpho-
logical forms. The last one tries to enrich poorly
inflected languages to compensate for their lack of
morphology. In HyTra 2013, approaches treating
morphology were addressed by the following con-
tributions:
? Toral (2013) explores the selection of data to
train domain-specific language models (LM)
from non-domain specific corpora by means
of simplified morphology forms (such as
lemmas). The benefit of this technique is
tested using automatic metrics in the English-
to-Spanish task. Results show an improve-
ment of up to 8.17% of perplexity reduction
over the baseline system.
? Rios Gonzalez and Goehring (2013) propose
machine learning techniques to decide on the
correct form of a verb depending on the con-
text. Basically they use tree-banks to train the
classifiers. Results show that they are able
to disambiguate up to 89% of the Quechua
verbs.
3.2 Syntactic approaches
Syntax had been addressed originally in SMT in
the form of so called phrase-based SMT with-
out any reference to linguistic structures; during
the last decade (or more) the approach evolved
to or, respectively, was complemented by - work
on syntax-based models in the linguistic sense of
the word. Most such approaches can be classi-
fied into three different types of architecture that
are defined by the type of syntactic analysis used
for the source language and the type of generation
aimed at for the target language: tree-to-tree, tree-
to-string and string-to-tree. Additionally, there
are also the so called hierarchical systems, which
combine the phrase-based and syntax-based ap-
proaches by using phrases as translation-units and
automatically generated context free grammars as
rules. Approaches dealing with the syntactic ap-
proach in HyTra 2013 include the following pa-
pers:
? Green and Zabokrtsky? (2013) study three dif-
ferent ways to ensemble parsing techniques
and provide results in MT. They compute cor-
relations between parsing quality and transla-
tion quality, showing that NIST is more cor-
related than BLEU.
? Han et al (2013) provide a framework for
pre-reordering to make Chinese word order
more similar to Japanese. To this purpose,
they use unlabelled dependency structures of
sentences and POS tags to identify verbal
blocks and move them from after-the-object
positions (SVO) to before-the-object posi-
tions (SOV).
? Nath Patel et al (2013) also propose a pre-
reordering technique, which uses a limited
set of rules based on parse-tree modification
rules and manual revision. The set of rules is
specifically listed in detail.
? Saers et al (2013) report an unsupervised
learning model that induces phrasal ITGs by
breaking rules into smaller ones using mini-
mum description length. The resulting trans-
lation model provides a basis for generaliza-
tion to more abstract transduction grammars
with informative non-terminals.
3.3 Morphosyntactical approaches
In linguistic theories, morphology and syntax are
often considered and represented simultaneously
(not only in unification-based approaches) and the
same is true for MT systems.
3
? Laki et al (2013) combine pre-reordering
rules with morphological and factored mod-
els for English-to-Turkish.
? Li et al (2013) propose pre-reordering rules
to be used for alignment-based reordering,
and corresponding POS-based restructuring
of the input. Basically, they focus on tak-
ing advantage of the fact that Korean has
compound words, which - for the purpose of
alignment - are split and reordered similarly
to Chinese.
? Turki Khemakhem et al (2013) present
work about an English-Arabic SMT sys-
tem that uses morphological decomposition
and morpho-syntactic annotation of the target
language and incorporates the correspond-
ing information in a statistical feature model.
Essentially, the statistical feature language
model replaces words by feature arrays.
3.4 Semantic approaches
The introduction of semantics in statistical MT has
been approached to solve word sense disambigua-
tion challenges covering the area of lexical seman-
tics and, more recently, there have been different
techniques using semantic roles covering shallow
semantics, as well as the use of distributional se-
mantics for improving translation unit selection.
Approaches treating the incorporation of seman-
tics into MT in HyTra 2013 include the following
research work:
? Rudnick et al (2013) present a combina-
tion of Maximum Entropy Markov Models
and HMM to perform lexical selection in
the sense of cross-lingual word sense disam-
biguation (i.e. by choice from the set of trans-
lation alternatives). The system is meant to
be integrated into a RBMT system.
? Boujelbane (2013) proposes to build a bilin-
gual lexicon for the Tunisian dialect us-
ing modern standard Arabic (MSA). The
methodology is based on leveraging the large
available annotated MSA resources by ex-
ploiting MSA-dialect similarities and ad-
dressing the known differences. The author
studies morphological, syntactic and lexical
differences by exploiting Penn Arabic Tree-
bank, and uses the differences to develop
rules and to build dialectal concepts.
? Bouillon et al (2013) presents two method-
ologies to correct homophone confusions.
The first one is based on hand-coded rules
and the second one is based on weighted
graphs derived from a pronunciation re-
source.
3.5 Other multilevel approaches
In a number of linguistic theories information
from the morphological, syntactic and semantic
level is considered conjointly and merged in cor-
responding representations (a RBMT example is
LFG (Lexical Functional Grammars) analysis and
the corresponding XLE translation architecture).
In HyTra 2013 there are three approaches dealing
with multilevel information:
? Pal et al (2013) propose a combination of
aligners: GIZA++, Berkeley and rule-based
for English-Bengali.
? Hsieh et al (2013) use comparable corpora
extracted from Wikipedia to extract parallel
fragments for the purpose of extending an
English-Bengali training corpus.
? Tambouratzis et al (2013) describe a hybrid
MT architecture that uses very few bilingual
corpus and a large monolingual one. The
linguistic information is extracted using
pattern recognition techniques.
Table 1 summarizes the papers that have been
presented in the Second HyTra Workshop. The
papers are arranged into the table according to the
linguistic level they address.
4 Conclusions and further work
The success of the Second HyTra Workshop con-
firms that research in hybrid approaches to MT
systems is a very active and promising area. The
MT community seems to agree that pure data-
driven or rule-based paradigms have strong lim-
itations and that hybrid systems are a promising
direction to overcome most of these limitations.
Considerable progress has been made in this area
recently, as demonstrated by consistent improve-
ments for different language pairs and translation
tasks.
The research community is working hard, with
strong collaborations and with more resources at
hand than ever before. However, it is not clear
4
Morphological (Toral, 2013) Hybrid Selection of LM Training Data Using Linguistic Information and Perplexity
(Gonzales and Goehring, 2013) Machine Learning disambiguation of Quechua verb morphology
Syntax (Green and Zabokrtsky?, 2013) Improvements to SBMT using Ensemble Dependency Parser
(Han et al, 2013) Using unlabeled dependency parsing for pre-reordering for Chinese-to-Japanese SMT
(Patel et al, 2013) Reordering rules for English-Hindi SMT
(Saers et al, 2013) Unsupervised transduction grammar induction via MDL
Morpho-syntactic (Laki et al, 2013) English to Hungarian morpheme-based SMT system with reordering rules
(Li et al, 2013) Experiments with POS-based restructuring and alignment based reordering for SMT
(Khemakhem et al, 2013) Integrating morpho-syntactic feature for English Arabic SMT
Semantic (Rudnick and Gasser, 2013) Lexical Selection for Hybrid MT with Sequence Labeling
(Boujelbane et al, 2013) Building bilingual lexicon to create dialect Tunisian corpora and adapt LM
(Bouillon et al, 2013) Two approaches to correcting homophone confusions in a hybrid SMT based system
Multilevels (Pal et al, 2013) A hybrid Word alignment model for PBSMT
(Hsieh et al, 2013) Uses of monolingual in-domain corpora for cross-domain adaptation with hybrid MT approaches
(Tambouratzis et al, 2013) Overview of a language-independent hybrid MT methodology
Table 1: HyTra 2013 paper overview.
whether technological breakthroughs as in the past
are still possible are still possible, or if MT will be
turning into a research field with only incremen-
tal advances. The question is: have we reached
the point at which only refinements to existing ap-
proaches are needed? Or, on the contrary, do we
need a new turning point?
Our guess is that, similar to the inflection point
giving rise to the statistical MT approach during
the last decade of the twentieth century, once again
there might occur a new discovery which will rev-
olutionize further the research on MT. We cannot
know whether hybrid approaches will be involved;
but, in any case, this seems to be a good and smart
direction as it is open to the full spectrum of ideas
and, thus, it should help to push the field forward.
Acknowledgments
This workshop has been supported by the Sev-
enth Framework Program of the European Com-
mission through the Marie Curie actions HyghTra,
IMTraP, AutoWordNet and CrossLingMind and
the Spanish ?Ministerio de Econom??a y Competi-
tividad? and the European Regional Development
Fund through SpeechTech4all. We would like to
thank the funding institution and all people who
contributed towards making the workshop a suc-
cess. For a more comprehensive list of acknowl-
edgments refer to the preface of this volume.
References
Anton Batliner, J. Buckow, Heinrich Niemann, Elmar
No?th, and Volker Warnke, 2000a. The Prosody
Module, pages 106?121. New York, Berlin.
Anton Batliner, Richard Huber, Heinrich Niemann, El-
mar No?th, Jo?rg Spilker, and K. Fischer, 2000b. The
Recognition of Emotion, pages 122?130. New York,
Berlin.
Pierrette Bouillon, Johanna Gerlach, Ulrich Germann,
Barry Haddow, and Manny Rayner. 2013. Two ap-
proaches to correcting homophone confusions in a
hybrid machine translation system. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Rahma Boujelbane, Mariem Ellouze khemekhem, Si-
war BenAyed, and Lamia HadrichBelguith. 2013.
Building bilingual lexicon to create dialect tunisian
corpora and adapt language model. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Marta R. Costa-jussa`, Patrik Lambert, Rafael E.
Banchs, Reinhard Rapp, and Bogdan Babych, edi-
tors. 2012. Proceedings of the Joint Workshop on
Exploiting Synergies between Information Retrieval
and Machine Translation (ESIRMT) and Hybrid Ap-
proaches to Machine Translation (HyTra). As-
sociation for Computational Linguistics, Avignon,
France, April.
Marta R. Costa-jussa`, Patrik Lambert, Rafael E.
Banchs, Reinhard Rapp, Bogdan Babych, and Kurl
Eberle, editors. 2013. Proceedings of the Sec-
ond Workshop on Hybrid Approaches to Translation
(HyTra). Association for Computational Linguis-
tics, Sofia, Bulgaria, August.
Andreas Eisele, Christian Federmann, Hans Uszkoreit,
Herve? Saint-Amand, Martin Kay, Michael Jelling-
haus, Sabine Hunsicker, Teresa Herrmann, and
Yu Chen. 2008. Hybrid machine translation archi-
tectures within and beyond the euromatrix project.
In John Hutchins and Walther v.Hahn, editors, 12th
annual conference of the European Association for
Machine Translation (EAMT), pages 27?34, Ham-
burg, Germany.
Annette Rios Gonzales and Anne Goehring. 2013.
Machine learning disambiguation of quechua verb
morphology. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Nathan Green and Zdenek Zabokrtsky?. 2013. Im-
provements to syntax-based machine translation us-
ing ensemble dependency parsers. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
5
Dan Han, Pascual Martinez-Gomez, Yusuke Miyao,
Katsuhito Sudoh, and Masaaki NAGATA. 2013.
Using unlabeled dependency parsing for pre-
reordering for chinese-to-japanese statistical ma-
chine translation. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation, Sofia.
An-Chang Hsieh, Hen-Hsen Huang, and Hsin-Hsi
Chen. 2013. Uses of monolingual in-domain cor-
pora for cross-domain adaptation with hybrid mt ap-
proaches. In ACL Workshop on Hybrid Machine Ap-
proaches to Translation, Sofia.
Ines Turki Khemakhem, Salma Jamoussi, and Abdel-
majid Ben Hamadou. 2013. Integrating morpho-
syntactic feature in english-arabic statistical ma-
chine translation. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation, Sofia.
La?szlo? Laki, Attila Novak, and Borba?la Siklo?si. 2013.
English to hungarian morpheme-based statistical
machine translation system with reordering rules. In
ACL Workshop on Hybrid Machine Approaches to
Translation, Sofia.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden, July. Association for
Computational Linguistics.
Will Lewis and Chris Quirk. 2013. Controlled ascent:
Imbuing statistical mt with linguistic knowledge. In
ACL Workshop on Hybrid Machine Approaches to
Translation, Sofia.
Shuo Li, Derek F. Wong, and Lidia S. Chao.
2013. Experiments with pos-based restructuring and
alignment-based reordering for statistical machine
translation. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Santanu Pal, Sudip Naskar, and Sivaji Bandyopadhyay.
2013. A hybrid word alignment model for phrase-
based statistical machine translation. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale, and
Sasikumar M. 2013. Reordering rules for english-
hindi smt. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Alex Rudnick and Michael Gasser. 2013. Lexical se-
lection for hybrid mt with sequence labeling. In ACL
Workshop on Hybrid Machine Approaches to Trans-
lation, Sofia.
Markus Saers, Karteek Addanki, and Dekai Wu. 2013.
Unsupervised transduction grammar induction via
minimum description length. In ACL Workshop on
Hybrid Machine Approaches to Translation, Sofia.
George Tambouratzis, Sokratis Sofianopoulos, and
Marina Vassiliou. 2013. Language-independent hy-
brid mt with presemt. In ACL Workshop on Hybrid
Machine Approaches to Translation, Sofia.
Antonio Toral. 2013. Hybrid selection of language
model training data using linguistic information and
perplexity. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of Speech-to-Speech Translation. Springer,
Berlin, Heidelberg, New York.
Yorick Wilks. 1994. Stone soup and the french
room: The empiricist-rationalist debate about ma-
chine translation. Current Issues in Computational
Linguistics: in honor of Don Walker, pages 585?
594. Pisa, Italy: Giardini / Dordrecht, The Nether-
lands: Kluwer Academic.
6
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 43?48,
Gothenburg, Sweden, April 26 2014. c?2014 Association for Computational Linguistics
How Well Can a Corpus-Derived Co-Occurrence Network  Simulate Human Associative Behavior? 
 Gemma Bel Enguix Reinhard Rapp Michael Zock  Aix-Marseille Universit?, Laboratoire d'Informatique Fondamentale UMR 7279, Case 901, 163 Avenue de Luminy, F-13288 Marseille  gemma.belenguix@gmail.com reinhardrapp@gmx.de zock@free.fr   Abstract 
Free word associations are the words people spontaneously come up with in re-sponse to a stimulus word. Such informa-tion has been collected from test persons and stored in databases.  A well known example is the Edinburgh Associative Thesaurus (EAT). We will show in this paper that this kind of knowledge can be acquired automatically from corpora, en-abling the computer to produce similar associative responses as people do. While in the past test sets typically consisted of approximately 100 words, we will use here a large part of the EAT which, in to-tal, comprises 8400 words. Apart from extending the test set, we consider differ-ent properties of words: saliency, fre-quency and part-of-speech. For each fea-ture categorize our test set, and we com-pare the simulation results to those based on the EAT. It turns out that there are surprising similarities which supports our claim that a corpus-derived co-occur-rence network can simulate human asso-ciative behavior, i.e. an important part of language acquisition and verbal behavior. 1 Introduction Word associations in general and free word asso-ciation in particular (Galton, 1879) have been used by psychologists of various schools1 to un-derstand the human mind (memory, cognition, language) and the hidden mechanisms driving peoples? thoughts, utterances, and actions. In the case of free word associations, a person typically hears or reads a word, and is asked to produce the first other word coming to mind. Kent & Ro-sanoff (1910) have used this method for compar-                                                1  For example, cognitive psychology (Collins and Loftus, 1975,), psycholinguistics (Clark, 1970) and psychoanalysis (Freud, 1901; Jung & Riklin, 1906). 
isons, introducing to this end 100 emotionally neutral test words. Having conducted the first large scale study of word associations (1000 test persons) they reached the conclusion that there was a great uniformity concerning people's asso-ciations, that is, speakers of a language share sta-ble, comparable associative networks (Istifci, 2010).  In this paper, we are mainly interested in the automatic acquisition of associations by com-puter. More precisely, we want to check whether a corpus-based method allows us to build auto-matically an associative network akin to the one in peoples? mind, that is, a network able to mim-ic human behavior. This means, given a stimulus word the system is supposed to produce the same responses as people do. We know since the old Greeks that thoughts and their expressions (words) are linked via associations. Yet, what we still do not know is the nature of these links. Al-so, links vary in terms of strength. Associationist learning theory (Schwartz & Reisberg, 1991) ex-plains how these strengths (or weights) are ac-quired. The strength between two perceived events increases by a constant fraction of a max-imally possible increment at each co-occurrence, and decreases in the opposite case.  Wettler et al. (2005) have shown that this mechanism can be replicated by looking at word co-occurrence frequencies in large text collec-tions. But there had been earlier corpus-linguistic work: For example, Wettler & Rapp (1989) com-pared several association measures in order to find search terms to be used for queries in infor-mation retrieval. Church & Hanks (1990) sug-gested to use mutual information, an information theoretic measure, for computing association strength. Prior to this, a lot of work had been done without reliance of corpora. For example, Collins & Loftus (1975) used associative seman-tic networks to show the distance between words. Others (Rosenzweig, 1961:358; Ekpo-Ufot, 1978) tried to show the universal status of a large subset of associations. While all these findings are important, we will not consider them further 
43
here. Rather we will focus on the claim that a -corpus-derived co-occurrence network is able to mimic human associative behavior. Such a network consists of nodes, which in our case correspond to words (or lemmas), and of weights connecting the nodes. The strengths of these weights are computed on the basis of word co-occurrence data, and by optionally ap-plying an association measure. But there are many association measures. Given their number and diversity some researchers (Evert & Krenn, 2001) felt that there was a need to define some criteria and methods in order to allow for quanti-tative comparisons via task-based evaluations. Pursuing a similar goal, Pecina & Schlesinger (2006) compared 82 different association measures for collocation extraction, while Hoang et al. (2009) classified them. Michelbacher et al. (2011) investigated the potential of asymmetric association measures, i.e. "associations whose associational strength is significantly greater in one direction (e.g., from Pyrrhic to victory) than in the other (e.g., from victory to Pyrrhic)". Washtell & Markert (2009) tried to determine whether word associations should be computed via window-based co-occurrence counts or rather via a windowless approach measuring the dis-tances between words. Our work is related to previous studies com-paring human word associations with those de-rived from corpus statistics (e.g. Wettler et al., 2005; Tamir, 2005, Seidensticker, 2006). The main differences are that we categorize our stim-ulus words and present results for each class, and that we have a stronger focus on the graph aspect of our network. 2 Resources and processing In order to simulate human associative behavior via corpora, we need them to encode knowledge that people typically have, that is, encyclopedic or universally shared knowledge (e.g. Paris capi-tal of France) and episodic knowledge (i.e. knowledge momentarily true: Nadal winner of the French Open). To meet these goals we de-cided to use the British National Corpus (BNC, Burnard & Aston, 1998) as it is well balanced and relatively large (about 100 million words of contemporary British English). To lemmatize the corpus we used the NLTK (Bird et al., 2009) which for this purpose utilizes information from WordNet. Hence, inflected forms (e.g. wheels or bigger) were replaced by their base forms (e.g. wheel or big). This reduces 
noise and data sparsity while improving speed and accuracy during evaluation. Since this latter is based on exact string matching, our system would consider wheels, produced in response to car, as a mistake as the primary associative re-sponse of the test persons is wheel, the singular form. Lemmatization solves this problem. Since we were interested here only in content words (nouns, verbs, and adjectives) we removed all other words from the BNC. To evaluate the performance of our system we compared its results with the associations col-lected by Kiss et al. (1973), the Edinburgh Asso-ciative Thesaurus. The association norms of the EAT were produced by presenting each stimulus word to 100 subjects, and by collecting their re-sponses. The subjects were 17 to 22 year old British students. Table 1 shows the associations produced by at least five participants in response to the stimulus words bath and cold together with the number of participants producing them.  bath cold observed response number of subjects observed response number of subjects water tub clean hot 
20 8 5 5 
hot ice warm water 
34 10 7 5 Table 1: Extracts from the EAT for the stimulus words bath and cold. The EAT lists the associations to 8400 stimu-lus words. Since we were only interested in nouns, verbs, and adjectives, we eliminated all other words and also multiword units (e.g. a lot). After having lemmatized the data with the NLTK we obtained a list of 5910 test items which is considerably more than the usual 100 used in many previous studies (e.g. Wettler et al., 2005). 3 A graph-based approach for comput-ing word associations Unlike previous work (Wettler et al. 2005; Church & Hanks, 1990) which is described in the terminology of the well known vector space model, in the construction of the current system we had a graph-based approach in mind so we describe the system in such terms. We built up a graph on the basis of the nouns, verbs, and adjec-tives occurring in the corpus, these tokens being the nodes of the graph.2 The links (also called                                                 2  As preliminary experiments have shown, including func-tion words in the graph can create noise in the retrieval of 
44
weights, connections, or edges) between these nodes are zero at the beginning, and are incre-mented by one whenever the two connected words co-occur in the corpus as direct neigh-bors.3 Put differently, the weight of each link represents the number of times two words (nodes) co-occur in the corpus. The associations to a given stimulus word are calculated by searching the nodes which are di-rect neighbors of this stimulus word, and by ranking them according to the weights of the connections. Given a graph G=V,E with V={i,j,?,n} as its set of vertices and E as its set of edges linking pairs of nodes over V, we ex-press by N(i) the neighborhood of a node i ?V, where N(i) is defined as every j?V | ei,j ?E. 4 Results Given the way this network is built, one could expect the system to retrieve only syntagmati-cally related words, i.e. words often occurring in close proximity (e.g. blue ? sky). Yet, to our surprise, the system also retrieves many paradig-matic associations, that is, words which can sub-stitute each other (e.g. blue ? red). Table 2 shows some results. While not all computed primary responses are identical to the ones produced by humans (in the EAT), the re-sponses seem perfectly plausible. This raises the question whether the answers are within the bandwidth of variation of human associative be-havior. We measured the quality of our results by counting (for all 5910 items) the number of times the subjects participating in the creation of the EAT had given the same answer as our system. This number is 6.2 on average. In comparison, the number of other subjects giving the same an-swer as an average test person is 5.8. If the two numbers were identical, our system would be perfectly within the range of variation of the hu-man associative responses, i.e. our system's an-swers could hardly be distinguished from the ones given by a human. This is actually the case. The answers of our system are, on average, even slightly closer to the ones given by the test per-sons than the answers of a randomly selected test person.  
                                                                       associations. Hence we preferred to keep only these three categories. 3  Note that this refers to the pre-processed corpus where all stopwords have been removed. 
Stimulus Word Human Prima-ry Response Computed Pri-mary Response afraid fear person anger hate frustration baby boy mother bath water shower beautiful ugly woman bed sleep hospital bible book God bitter sweet taste black white white blossom flower white  Table 2: Comparison between human and computed associ-ations for the 10 alphabetically first words of the Kent/Ro-sanoff (1910) list.  In the following subsections we split our set of 5910 test items into three categories to check how well each one of them matches our intuition that a corpus-derived co-occurrence network can indeed simulate human associative behavior. 4.1 Word saliency Our goal is twofold: find out to what extend the saliency of a stimulus word has an effect on the homogeneity of human responses, and whether these findings can also be replicated in our com-puter simulation. To this end we divided our 5910 EAT stimu-lus words into six categories, i.e. saliency classes (SC). Saliency is defined here as the proportion of subjects producing the Primary Associative Response (PAR), this latter being the response produced by the largest number of subjects.    SC 1:  less than 10% producing the PAR (10.7%)  SC 2:  10 to 20% producing the PAR (36.0%) SC 3:  20 to 30% producing the PAR (24.3%) SC 4:  30 to 40% producing the PAR (13.3%) SC 5:  40 to 50% producing the PAR (8.0%) SC 6:  more than 50% producing the PAR (7.6%)  The percentages at the end of each line denote the proportion of words belonging to the respec-tive saliency class. All classes are reasonably well covered. Here are some representative words for each class:   SC 1:  leader, professor, yellow  SC 2:  horse, mountain, semaphore  SC 3:  chief, jungle, kiss  SC 4:  driver, monarchy, tornado  SC 5:  aid, cell, gasoline SC 6:  black, aunt, woman  
45
As can be seen from these examples, our intui-tions do not easily allow us to make predictions concerning the saliency classifications of words.  Figure 1 (blue curve) shows how well our sys-tem performs for each class. For the words in each class we counted the average number of times a human subject had come up with the same associative response as the system. It ap-pears that the system's performance is best for very salient words, performing less well in the opposite case. Note that this correlates perfectly well with the observed human associative behav-ior: Our system tends to produce the same an-swers as people for stimulus words yielding ho-mogeneous human responses. Likewise, the sys-tem?s answers tend to differ in cases where peo-ples? answers are heterogeneous. The red curve in Figure 1 shows for each sali-ency class the number of persons giving the same associative answer as an average test person. As can be seen this line is almost identical to the one representing the system's performance, which means that the system's behavior is very similar to human behavior with respect to saliency.   
   Fig. 1: Quality of our system's (blue curve) and an average test person's (red curve) performance (measured as the num-ber of matching responses found in the EAT) with respect to saliency. 4.2 Word frequency Encouraged by the findings for saliency, we con-ducted a similar experiment for word frequency. In this case the EAT stimulus words were split into frequency classes according to their corpus  frequencies in the BNC.  Since a logarithmic scale seems to be appro-priate for word frequencies (Rapp, 2005; van Heuven et al., in press), we used the following six frequency classes (FC):   FC1: 1 occurrence BNC (0.5%)  FC2: from 1 to 10 occurrences BNC (9.2%)  FC3: form 10 to 100 occurrences BNC (30.2%) FC4: from 100 to 1000 occurrences BNC (42.6%) 
FC5: from 1000 to 10000 occurrences BNC (17.3%) FC6: from 10000 to 100000 occurrences BNC (0.1%)  As can be seen from the percentages at the end of each line, extremes, i.e. very high and very low frequencies are covered only marginally.  In the first group we find words like cornuco-pia, jewelry4 and quaff, each appearing only once in the corpus, while the frequency class 6 con-tains only high frequency words such as the (auxiliary) verbs be, do, have, and make.  The results obtained for the frequency classes are shown in Figure 2.  As can be seen, the gen-eral tendency is that the results improve with de-creasing frequency. Our explanation for this is that frequent words tend to be more polysemous, and that increased ambiguity tends to yield more heterogeneous responses. For example, the am-biguous stimulus word palm is likely to evoke not only responses related to its tree sense, but also to its hand sense.  
  Fig. 2: Quality of our system's (blue curve) and an average test person's (red curve) performance with respect to fre-quency.  Whereas for mid frequency words the results for the test persons and in the simulation show a high agreement, this is not the case for high fre-quency and for low frequency words. For high frequency words (FC 6) a plausible explanation might be the sampling error due to the low sam-ple size of only 0.1% of the stimulus words in the EAT test set. However, for low frequency words the sample sizes are larger and the dis-crepancy is clearly systematic. Our explanation is that in this case we might have a systematic sampling error concerning the observed frequen-cies. The simulation has an advantage because the frequency classes were set up according to                                                 4  Note that this is the American spelling which is rare in the BNC. The British spelling is jewellery. 
46
the BNC frequencies rather than according to the subjective frequencies (= word familiarities) of the test persons. For example, the words of FC 1 are guaranteed to occur in the BNC, while it is not certain at all that the test persons ever en-countered them. This leads to a systematic bias in favor of the simulation results. 4.3 Part of speech In a last experiment we considered the results for the three parts of speech used in our system, namely nouns, verbs, and adjectives. We as-signed to each word in the EAT test set its part of speech. Syntactically ambiguous words (which can belong to several parts of speech) were as-signed to their most frequently occurring part of speech. Of the 5910 EAT items, 89.2% were classified as nouns, 2.4% as verbs, and 8.4% as adjectives.  
  Fig. 3: Quality of our system's (blue curve) and an average test person's (red curve) performance with respect to parts of the speech.  For the three categories we obtained the re-sults shown in Figure 3. The results are best for nouns and worst for verbs. Our explanation for this is once again average word ambiguity which is higher for verbs than it is for nouns. As with the saliency classes, we have again a high corre-lation between the results produced by humans and the ones produced by machine. 5 Discussion and conclusion We have presented a novel graph-based algo-rithm for the computation of word associations. The goal was to check whether and to what ex-tent an automatically built association network based on a large text corpus would yield similar results to the ones produced by humans. The re-sults were evaluated with a test set comprising all nouns, verbs, and adjectives of the EAT stimulus 
words. This test set is considerably larger than the ones used in most previous computational as-sociation studies. Contrary to what could be expected our sys-tem predicts not only syntagmatic but also para-digmatic relations. For instance, the pairs black ? white, bread ? butter and boy ? girl are cor-rectly computed. This shows that texts contain not only word pairs encoding syntagmatic rela-tions but also pairs encoding paradigmatic rela-tions. The results also show that statistical co-occurrence-based methods are suitable for tasks that traditionally were supposed to require more sophisticated symbolic approaches. In sum, our approach allows not only to cor-rectly predict thousands of associations, it also matches human performance in other respects: For the first time it was shown that the predic-tions for salient words are much better than for non-salient ones. Similarly, concerning word frequency and part of speech the simulated re-sults also closely mimic the behavior as found in the human data.  Altogether, our results provide evidence that human associative behavior as observed in the classical association experiments can be modeled by exploiting the co-occurrences of words in large text corpora. There seems to be a circulari-ty: (a) the word co-occurrences found in text and speech5 appear to be externalized forms of the associations stored in the human brain, and (b) the associations stored in the brain appear to be internalized forms of the co-occurrences as found in text and speech. This contradiction disappears as soon as we realize that time has elapsed be-tween these two events. Hence, one network may be fed by the other, and this may go on. Note that our corpus-based approach has fur-ther virtues: (a) it allows to generate associations from corpora covering particular time spans; (b) it can produce associations based on corpora covering specific topics; (c) it accounts for the fact that languages, hence associations, change over time. Think of the ideas associated with Dominique Strauss-Kahn, one of the top candi-dates before the last presidential campaign in France. While the associations prior to May 18, 2011 were probably IMF, politics or election, the ones after the Sofitel event were probably quite different, shifting towards a much more delicate topic.                                                   5  Note that the BNC also contains transcribed speech. 
47
Acknowledgments This research was supported by the Marie Curie Intra European Fellowships DynNetLAc and Au-toWordNet within the 7th European Community Framework Programme. References Bird, S.; Klein, E. and Loper, E. (2009). Natural Lan-guage Processing with Python. O'Reilley Media. Burnard, L. and Aston, G. (1998). The BNC Hand-book: Exploring the British National Corpus. Ed-inburgh: Edinburgh University Press.  Church, K.W. and Hanks, P. (1990). Word association norms, mutual information, and lexicography. Computational Linguistics 16 (1), 22?29.  Clark, H. H. (1970). Word associations and linguistic theory. In J. Lyons (Ed.), New horizons in linguis-tics (pp. 271-286). Baltimore: Penguin.  Collins, A. M. and Loftus, E. F. (1975). A spreading-activation theory of semantic processing. Psycho-logical Review 8. Vol. 82, No. 6, 407-428. Ekpo-Ufot, A. (1978). Word associations: a compara-tive study among college students in Nigeria and the United States. Journal of Cross-Cultural Psy-chology, Vol. 9(4), 455-468.  Evert, S. and Krenn, B. (2001). Methods for qualita-tive evaluation of lexical association measures. In Proceedings of the 39th Annual Meeting of the As-sociation of Computational Linguistics, Toulouse, France, 188-915.  Freud, S. (1901/1975). The psychopathology of eve-ryday life. Harmondsworth: Penguin. http://psych-classics.yorku.ca/Freud/Psycho/chap5.htm Galton, F. (1879). Psychometric experiments. Brain (2), 149-162.  Van Heuven, W.J.B., Mandera, P., Keuleers, E., & Brysbaert, M. (in press). Subtlex-UK: A new and improved word frequency database for British English. Quarterly Journal of Experimental Psychology. Hoang, H.H, Kim, S. N. and Kan, M.Y. (2009). A re-examination of lexical association measures. Pro-ceedings of the Workshop on Multiword Expres-sions, ACL-IJCNLP 2009, Suntec, Singapore, 31-39.  Istifci, I. (2010). Playing with words: a study on word association responses. The Journal of International Social Research, 3(10), 360?368 Jung, C. and F. Riklin. 1906. Experimentelle Untersu-chungen ?ber Assoziationen Gesunder. In Jung, C. G., editor, Diagnostische Assoziationsstudien, 7?145. Barth, Leipzig. 
Kent, G.H. and Rosanoff, A.J. (1910). A study of as-sociation in insanity. American Journal of Insanity, 67, 37?96, 317?390.  Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J. (1973). An associative thesaurus of English and its computer analysis. In: A. Aitken, R. Beiley, N. Hamilton-Smith (eds.): The Computer and Literary Studies. Edinburgh University Press.  Michelbacher, L., Evert, S. and Sch?tze, H. (2011). Asymmetry in corpus-derived and human associa-tions. Corpus Linguistics and Linguistic Theory, Vo. 7, No. 2, 245?276.  Pecina, P., and Schlesinger, P. (2006). Combining as-sociation measures for collocation extraction. Pro-ceedings of the 21th International Conference on Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics (COLING/ACL 2006), Sydney, Australia, 651-658. Rapp, R. (2005). On the relationship between word frequency and word familiarity. In: B. Fisseni; H.-C. Schmitz; B. Schr?der; P. Wagner (Hg.): Sprach-technologie, mobile Kommunikation und linguisti-sche Ressourcen. Beitr?ge zur GLDV-Tagung 2005 in Bonn. Frankfurt: Peter Lang. 249?263. Rosenzweig, M. R. (1961). Comparisons among word-assocation responses in English, French, German, and Italian. The American Journal of Psy-chology, Vol. 74, No. 3, 347-360. Schwartz, B. and Reisberg, D. (1991). Learning and Memory. New York: Norton.  Seidensticker, P. (2006). Simulation von Wortassozia-tionen mit Hilfe von mathematischen Lernmodellen in der Psychologie. Dissertation an der Universit?t Paderborn.  Tamir, R. (2005). A Random Walk through Human Associations. Proceedings of ICDM 2005: 442-449.  Washtell, J.; Markert, K. (2009). A comparison of windowless and window-based computational as-sociation measures as predictors of syntagmatic human associations. Proceedings of the 2009 Con-ference on Empirical Methods in Natural Lan-guage Processing (EMNLP '09), Volume 2, 628-637 Wettler, M. and Rapp, R. (1989). A connectionist sys-tem to simulate lexical decisions in information re-trieval. In: R. Pfeifer, Z. Schreter, F. Fogelman, L. Steels (eds.): Connectionism in Perspective. Am-sterdam: Elsevier, 463?469.  Wettler, M., Rapp, R. and Sedlmeier, P. (2005). Free word associations correspond to contiguities be-tween words in texts. Journal of Quantitative Lin-guistics 12(2), 111?122.  
48
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87?95,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Extracting Multiword Translations  
from Aligned Comparable Documents 
 
Reinhard Rapp Serge Sharoff 
Aix-Marseille Universit?, Laboratoire 
d'Informatique Fondamentale 
F-13288 Marseille, France 
University of Leeds 
Centre for Translation Studies 
Leeds, LS2 9JT, UK 
reinhardrapp@gmx.de S.Sharoff@leeds.ac.uk 
 
Abstract 
Most previous attempts to identify trans-
lations of multiword expressions using 
comparable corpora relied on dictionaries 
of single words. The translation of a mul-
tiword was then constructed from the 
translations of its components. In con-
trast, in this work we try to determine the 
translation of a multiword unit by analyz-
ing its contextual behaviour in aligned 
comparable documents, thereby not pre-
supposing any given dictionary. Whereas 
with this method translation results for 
single words are rather good, the results 
for multiword units are considerably 
worse. This is an indication that the type 
of multiword expressions considered here 
is too infrequent to provide a sufficient 
amount of contextual information. Thus 
indirectly it is confirmed that it should 
make sense to look at the contextual be-
haviour of the components of a multi-
word expression individually, and to 
combine the results. 
1 Introduction 
The task of identifying word translations from 
comparable text has received considerable atten-
tion. Some early papers include Fung (1995) and 
Rapp (1995). Fung (1995) utilized a context het-
erogeneity measure, thereby assuming that words 
with productive context in one language translate 
to words with productive context in another lan-
guage, and words with rigid context translate into 
words with rigid context. In contrast, the under-
lying assumption in Rapp (1995) was that words 
which are translations of each other show similar 
co-occurrence patterns across languages. This 
assumption is effectively an extension of Harris' 
(1954) distributional hypotheses to the multilin-
gual case. 
This work was further elaborated in some by 
now classical papers, such as Fung & Yee (1998) 
and Rapp (1999). Based on these papers, the 
standard approach is to start from a dictionary of 
seed words, and to assume that the words occur-
ring in the context of a source language word 
have similar meanings as the words occurring in 
the context of its target language translation. 
There have been suggestions to eliminate the 
need for the seed dictionary. However, most at-
tempts, such as Rapp (1995), Diab & Finch 
(2000) and Haghighi et al. (2008) did not work to 
an extent that the results would be useful for 
practical purposes. Only recently a more pro-
mising approach has been investigated: Schafer 
& Yarowsky (2002), Hassan & Mihalcea (2009), 
Prochasson & Fung (2011) and Rapp et al. 
(2012) look at aligned comparable documents 
and deal with them in analogy to the treatment of 
aligned parallel sentences, i.e. effectively doing a 
word alignment in a very noisy environment. 
This approach has been rather successful and it 
was possible to improve on previous results. This 
is therefore the approach which we will pursue in 
the current paper.  
However, in contrast to the above mentioned 
papers the focus of our work is on multiword 
expressions, and we will compare the perform-
ance of our algorithm when applied to multiword 
expressions and when applied to single words.  
There has been some previous work on identi-
fying the translations of multiword units using 
comparable corpora, such as Robitaille et al. 
(2006), Babych et al. (2007), Daille & Morin 
(2012); Delpech et al. (2012). However, none of 
this work utilizes aligned comparable documents, 
and the underlying assumption is that the transla-
tion of a multiword unit can be determined by 
looking at its components individually, and by 
merging the results. 
In contrast, we try to explore whether the 
translation of a multiword unit can be determined 
solely by looking at its contextual behavior, i.e. 
whether it is possible to also apply the standard 
approach as successfully used for single words. 
The underlying fundamental question is whether 
the meaning of a multiword unit is determined by 
87
the contextual behavior of the full unit, or by the 
contextual behavior of its components (or by a 
mix of both). But multiword expressions are of 
complex nature, as expressed e.g. by Moon 
(1998): "there is no unified phenomenon to de-
scribe but rather a complex of features that inter-
act in various, often untidy, ways and represent a 
broad continuum between non-compositional (or 
idiomatic) and compositional groups of words." 
The current paper is an attempt to systematically 
approach one aspect of this complexity. 
2 Approach 
Our approach is based on the usual assumption 
that there is a correlation between the patterns of 
word-co-occurrence across languages. However, 
instead of presupposing a bilingual dictionary it 
only requires pre-aligned comparable documents, 
i.e. small or medium sized documents aligned 
across languages which are known to deal with 
similar topics. This could be, for example, news-
paper articles, scientific papers, contributions to 
discussion groups, or encyclopaedic articles. As 
Wikipedia is a large resource and readily avail-
able for many languages, we decided to base our 
study on this encyclopaedia. The Wikipedias 
have the so-called interlanguage links which are 
manually inserted by the authors and connect 
articles referring to the same headword in differ-
ent languages. 
Given that each Wikipedia community con-
tributes in its own language, only occasionally an 
article connected in this way will be an exact 
translation of a foreign language article, and in 
most cases the contents will be rather different. 
On the positive side, the link structure of the in-
terlanguage links tends to be quite dense. For 
example, of the 1,114,696 German Wikipedia 
articles, 603,437 have a link to the corresponding 
English Wikipedia article. 
2.1 Pre-processing and MWE extraction 
We used the same versions of Wikipedia as in 
Rapp et al. (2012) and used the same processing. 
After download, each Wikipedia was minimally 
processed to extract the plain text contents of the 
articles. In this process all templates, e.g. 
'infoboxes', as well as tables were removed, and 
we kept only the webpages with more than 500 
characters of running text (including white 
space). Linguistic processing steps included to-
kenisation, tagging and lemmatisation using the 
default UTF-8 versions of the respective Tree-
Tagger resources (Schmid, 1994). 
From the pre-processed English and German 
Wikipedia, we extracted the multiword expres-
sions using two simple principles, a negative 
POS filter and a containment filter. The negative 
POS filter operates in a rule-based fashion on the 
complete list of n-grams by removing the un-
likely candidates according to a set of con-
straints, such as the presence of determiners or 
prepositions at the edges of expressions, see a 
similar method used by (Justeson & Katz, 1995). 
With some further extensions this was also used 
to produce the multiword lists for the dictionary 
of translation equivalents (Babych et al., 2007). 
We did not use positive shallow filters. These 
would need to capture the relatively complex 
structure of the noun, verb and prepositional 
phrases, while avoiding noise. This can often 
lead to a lack of recall when more complex con-
structions cannot be captured. In contrast, nega-
tive shallow filters simply avoid obvious noise, 
while passing other multiword expressions 
(MWEs) through, which are very often legiti-
mate syntactic constructions in a language in 
question. For example, the following English 
filters1 rejected personal pronouns (PP) and con-
junctions (CC) at the edges of expressions (using 
the Penn Treebank tagset as implemented by 
Treetagger): 
 
^[^ ]+~~PP |~~PP$ 
^[^ ]+~~CC |~~CC$ 
 
Similarly, any MWE candidates including proper 
nouns (NP) and numerals (CD) were discarded: 
 
~~NP 
~~CD 
 
In the end, this helps in improving the recall rate 
while using a relatively small number of pat-
terns: 18 patterns were used for English, 11 for 
German. 
The containment filter further rejects MWEs 
by removing those that regularly occur as a part 
of a longer acceptable MWE. For example, 
graphical user is an acceptable expression pass-
ing through the POS filter, but it is rejected by 
the containment filter since the overwhelming 
majority of its uses are in the containing MWE 
graphical user interface (1507 vs 1304 uses in 
Wikipedia, since MWEs are still possible, e.g., 
graphical user environment).  
                                                 
1
 We use here the standard notation for regular ex-
pressions as implemented in Perl (Friedl, 2002). For 
example, '^' means 'beginning of line' and '$' means 
'end of line'.  
88
English keyterms for 'Airbus 320 family' 
Score f Keyterm 
34.88 4 final_JJ assembly_NN 
31.22 3 firm_NN order_NN 
30.73 3 series_NN aircraft_NN 
29.07 4 flight_NN control_NN 
27.38 3 wing_NN area_NN 
23.26 3 final_JJ approach_NN 
22.19 2 lose_VV life_NN 
20.63 6 passenger_NN and_CC crew_NN 
17.54 2 first_JJ derivative_NN 
17.34 2 fly-by-wire_NN flight_NN control_NN 
16.63 3 flight_NN deck_NN 
16.41 2 crew_NN die_VV 
15.08 2 pilot_NN error_NN 
14.98 2 passenger_NN capacity_NN 
14.38 2 turbofan_NN engine_NN 
14.03 2 development_NN cost_NN 
12.30 2 maiden_JJ flight_NN 
11.54 2 direct_JJ competition_NN 
10.75 2 overall_JJ length_NN 
10.39 2 overrun_VV the_DT runway_NN 
9.54 2 flight_NN control_NN system_NN 
9.31 2 fuel_NN consumption_NN 
8.63 2 roll_VV out_RP 
7.86 3 crew_NN member_NN 
7.54 2 crew_NN on_IN board_NN 
7.33 2 bad_JJ weather_NN 
6.63 2 landing_NN gear_NN 
 
German keyterms for 'Airbus-A320-Familie' 
Score f Keyterm 
155.25 20 Triebwerk 
62.88 4 Fly-by-Wire-System 
59.76 8 Erstflug 
57.67 8 Absturz 
43.79 4 Endmontage 
43.70 4 Hauptfahrwerk 
41.77 4 Tragfl?gel 
36.52 8 Unfall 
35.90 6 Ungl?ck 
33.25 3 Abfluggewicht 
33.10 5 Auslieferung 
30.01 3 Treibstoffverbrauch 
29.00 2 Triebwerkstyp 
28.51 2 Zwillingsreifen 
18.20 2 Absturz_NN verursachen_VV 
16.28 3 Passagier_NN Platz_NN 
16.23 2 Triebwerk_NN antreiben_VV 
13.41 2 Steuerung_NN d_AR Flugzeug_NN 
12.52 2 Absturz_NN f?hren_VV 
11.68 2 Rumpf_NN befinden_VV 
8.59 2 Insasse_NN ums_AP Leben_NN 
8.55 2 Zeitpunkt_NN d_AR Ungl?ck_NN 
Table 1. English and German keyterms for 'Airbus 320 fam-
ily' (lists truncated). Score = log-likelihood score; f = occur-
rence frequency of keyterm; NN = noun; VV = verb; AR = 
article; AP = article+preposition; JJ = adjective; CC = con-
junction; RP = preposition. 
 
2.2 Keyterm extraction 
As the aligned English and German Wikipedia 
documents are typically not translations of each 
other, we cannot apply the usual procedures and 
tools as available for parallel texts (e.g. the Gale 
& Church sentence aligner and the Giza++ word 
alignment tool). Instead we conduct a two step 
procedure:  
1. We first extract salient terms (single word or 
multiword) from each of the documents. 
2. We then align these terms across languages 
using an approach inspired by a connectionist 
(Rumelhart & McClelland, 1987) Winner-
Takes-It-All Network. The respective algo-
rithm is called WINTIAN and is described in 
Rapp et al. (2012) and in Rapp (1996).  
For term extraction, the occurrence frequency of 
a term in a particular document is compared to 
its average occurrence frequency in all Wikipe-
dia documents, whereby a high discrepancy indi-
cates a strong keyness. Following Rayson & 
Garside (2000), we use the log-likelihood score 
to measure keyness, since it has been shown to 
be robust to small numbers of instances. This 
robustness is important as many Wikipedia arti-
cles are rather short.  
This procedure leads to multiword keyterms as 
exemplified in Table 1 for the Wikipedia entry 
Airbus A320 family.  Because of compounding in 
German, many single-word German expressions 
are translated into multiword expressions in Eng-
lish. So we chose to include single-word expres-
sions into the German candidate list for align-
ment with English multiwords.  
One of the problems in obtaining multiword 
keyterms from the Wikipedia articles is relative 
data sparseness. Usually, the frequency of an 
individual multiword expression within a Wiki-
pedia article is between 2 and 4. Therefore we 
had to use a less conservative threshold of 6.63 
(1% significance level) rather than the more 
standard 15.13 (0.01% significance level) for the 
log-likelihood score (see Rayson & Garside, 
2000, and http://ucrel. lancs.ac.uk/llwizard.html). 
2.3 Term alignment 
The WINTIAN algorithm is used for establishing 
term alignments across languages. As a more 
detailed technical description is given in Rapp et 
al. (2012) and in Rapp (1996), we only briefly 
describe this algorithm here, thereby focusing on 
the neural network analogy. The algorithm can 
be considered as an artificial neural network 
where the nodes are all English and German 
89
terms occurring in the keyterm lists. Each Eng-
lish term has connections to all German terms. 
The connections are all initialized with values of 
one when the algorithm is started, but will serve 
as a measure of the translation probabilities after 
the completion of the algorithm. One after the 
other, the network is fed with the pairs of corre-
sponding keyterm lists. Each German term acti-
vates the corresponding German node with an 
activity of one. This activity is then propagated 
to all English terms occurring in the correspond-
ing list of keyterms. The distribution of the activ-
ity is not equal, but in proportion to the connect-
ing weights. This unequal distribution has no 
effect at the beginning when all weights are one, 
but later on leads to rapid activity increases for 
pairs of terms which often occur in correspond-
ing keyterm lists. The assumption is that these 
are translations of each other. Using Hebbian 
learning (Rumelhart & McClelland, 1987) the 
activity changes are stored in the connections. 
We use a heuristic to avoid the effect that fre-
quent keyterms dominate the network: When 
more than 50 of the connections to a particular 
English node have weights higher than one, the 
weakest 20 of them are reset to one. This way 
only translations which are frequently confirmed 
can build up high weights. 
It turned out that the algorithm shows a robust 
behaviour in practice, which is important as the 
corresponding keyterm lists tend to be very noisy 
and, especially for multiword expressions, in 
many cases may contain hardly any terms that 
are actually translations of each other. Reasons 
are that corresponding Wikipedia articles are of-
ten written from different perspectives, that the 
variation in length can be considerable across 
languages, and that multiword expressions tend 
to show more variability with regard to their 
translations than single words. 
3 Results and evaluation 
3.1 Results for single words 
In this subsection we report on our previous re-
sults for single words (Rapp et al., 2012) as these 
serve as a baseline for our new results concern-
ing multiword units. 
The WINTIAN algorithm requires as input 
vocabularies of the source and the target lan-
guage. For both English and German, we con-
structed these as follows: Based on the keyword 
lists for the respective Wikipedia, we counted the 
number of occurrences of each keyword, and 
then applied a threshold of five, i.e. all keywords 
with a lower frequency were eliminated. The rea-
soning behind this is that rare keywords are of 
not much use due to data sparseness. This re-
sulted in a vocabulary size of 133,806 for Eng-
lish, and of 144,251 for German. 
Using the WINTIAN algorithm, the English 
translations for all 144,251 words occurring in 
the German vocabulary were computed. Table 2 
shows the results for the German word Stra?e 
(which means street). 
For a quantitative evaluation we used the 
ML1000 test set comprising 1000 English-
German translations (see Rapp et al., 2012). We 
verified in how many cases our algorithm had 
assigned the expected translation (as provided by 
the gold standard) the top rank among all 
133,806 translation candidates. (Candidates are 
all words occurring in the English vocabulary.) 
This was the case for 381 of the 1000 items, 
which gives us an accuracy of 38.1%. Let us 
mention that this result refers to exact matches 
with the word equations in the gold standard. As 
in reality due to word ambiguity other transla-
tions might also be acceptable (e.g. for Stra?e 
not only street but also road would be accept-
able), these figures are conservative and can be 
seen as a lower bound of the actual performance.  
 
GIVEN GERMAN 
WORD Stra?e 
EXPECTED 
TRANSLATION street 
 
LL-SCORE TRANSLATION 
1 215.3 road 
2 148.2 street 
3 66.0 traffic 
4 46.0 Road 
5 42.6 route 
6 34.6 building 
 
Table 2. Computed translations for Stra?e. 
 
3.2 Results for multiword expressions 
In analogy to the procedure for single words, for 
the WINTIAN algorithm we also needed to de-
fine English and German vocabularies of multi-
word terms. For English, we selected all multi-
word terms which occurred at least three times in 
the lists of English key terms, and for German 
those which occurred at least four times in the 
lists of German key terms. This resulted in simi-
lar sized vocabularies of 114,796 terms for Eng-
lish, and 131,170 for German. Note that the 
threshold for German had to be selected higher 
not because German has more inflectional vari-
ants (which does not matter as we are working 
90
with lemmatized data), but because - other than 
the English - the German vocabulary also in-
cludes unigrams. The reason for this is that Ger-
man is highly compositional, so that English 
multiword units are often translated by German 
unigrams. 
Using the WINTIAN algorithm, the English 
translations for all 131,170 words occurring in 
the German multiword vocabulary were com-
puted, and in another run the German translations 
for all 114,796 English words. Table 3 shows 
some sample results.  
For a quantitative evaluation, we did not have 
a gold standard at hand. As multiword expres-
sions show a high degree of variability with re-
gard to their translations, so that it is hard to 
come up with all possibilities, we first decided 
not to construct a gold standard, but instead did a 
manual evaluation. For this purpose, we ran-
domly selected 100 of the German multiword 
expressions with an occurrence frequency above 
nine, and verified their computed translations 
(i.e. the top ranked item for each) manually. We 
distinguished three categories: 1) Acceptable 
translation; 2) Associatively related to an accept-
able translation; 3) Unrelated to an acceptable 
translation.  
 
 
 English ? German 
 husband_NN and_CC wife_NN 
Rank Aktivity Translation 
1 2.98 Eheleute 
2 1.09 Voraussetzung 
3 1.08 Kirchenrecht 
4 0.76 Trennung 
5 0.35 Mann 
6 0.24 Kirche 
7 0.08 Mischehe 
8 0.08 Diakon 
 
 
 German ? English 
 Eheleute 
Rank Aktivity Translation 
1 3.01 husband_NN_and_CC_wife_NN 
2 1.26 married_JJ_couple_NN 
3 1.02 civil_JJ_law_NN 
4 1.02 equitable_JJ_distribution_NN 
5 1.02 community_NN_property_NN 
6 0.52 law_NN_jurisdiction_NN 
7 0.05 racing_NN_history_NN 
8 0.05 great_JJ_female_JJ 
 
Table 3. Sample results for translation directions EN ? DE 
and DE ? EN. 
 
We also did the same computation for the reverse 
language direction, i.e. for English to German. 
The results are listed in Table 4. These results 
indicate that our procedure, although currently 
state of the art for single words, does not work 
well for multiword units. We investigated the 
data and located the following problems: 
? The problem of data sparseness is, on average, 
considerably more severe for multiword ex-
pressions than it is for single words. 
 
? Although the English and the German vocabu-
lary each contain more than 100,000 items,  
their overlap is still limited. The reason is that 
the number of possible multiword units is very 
high, far higher than the number of words in a 
language. 
 
? We considered only multiword units up to 
length three, but in some cases this may not 
suffice for an acceptable translation. 
 
? In the aligned keyterm lists, only rarely correct 
translations of the source language terms oc-
cur. Apparently the reason is the high variabil-
ity of multiword translations. 
Hereby he last point seems to have a particularly 
severe negative effect on translation quality. 
However, all of these findings are of fundamen-
tal nature and contribute to the insight that at 
least for our set of multiword expressions com-
positionality seems to be more important than 
contextuality. 
 
German ? English 
Judgment Num-ber 
Example taken from actual 
data 
Acceptable 5 Jugendherberge ?  
youth_NN hostel_NN 
Association 38 Maischeg?rung ?  
oak_NN barrel_NN 
Unacceptable 57 Stachelbeere ?  
horror_NN film_NN 
 
English ? German 
Judgment Num-ber 
Example taken from actual 
data 
Acceptable 6 amino_NN acid_NN ? 
Aminos?ure 
Association 52 iron_NN mine_NN ? Ei-
senerz 
Unacceptable 42 kill_VV more_JJ ? Welt-
meistertitel_NN im_AP 
Schwergewicht_NN 
Table 4. Quantitative results involving MWEs. 
 
91
3.3 Large scale evaluation 
As a manual evaluation like the one described 
above is time consuming and subjective, we 
thought about how we could efficiently come up 
with a gold standard for multiword expressions 
with the aim of conducting a large scale auto-
matic evaluation. We had the idea to determine 
the correspondences between our English and 
German MWEs via translation information as 
extracted from a word-aligned parallel corpus. 
Such data we had readily at hand from a pre-
vious project called COMTRANS. During this 
project we had constructed a large bilingual dic-
tionary of bigrams, i.e. of pairs of adjacent words 
in the source language. For constructing the dic-
tionary, we word-aligned the English and Ger-
man parts of the Europarl corpus. For this pur-
pose, using Moses default settings, we combined 
two symmetric runs of Giza++, which considera-
bly improves alignment quality. Then we deter-
mined and extracted for each English bigram the 
German word or word sequence which had been 
used for its translation. Discontinuities of one or 
several word positions were allowed and were 
indicated by the wildcard ?*?. As the above me-
thod for word alignment produces many unjusti-
fied empty assignments (i.e. assignments where a 
source language word pair is erroneously as-
sumed to have no equivalent in the target lan-
guage sentence), so that the majority of these is 
incorrect, all empty assignments were removed 
from the dictionary. 
In the dictionary, for each source language 
word pair its absolute frequency and the absolute 
and relative frequencies of its translation(s) are 
given. To filter out spurious assignments, thresh-
olds of 2 for the absolute and 10% for the rela-
tive frequency of a translation were used. The 
resulting dictionary is available online.2  Table 5 
shows a small extract of the altogether 371,590 
dictionary entries. Alternatively, we could have 
started from a Moses phrase table, but it was eas-
ier for us to use our own data. 
Although the quality of our bigram dictionary 
seems reasonably good, it contains a lot of items 
which are not really interesting multiword ex-
pressions (e.g. arbitrary word sequences such as 
credible if or the discontinuous word sequences 
on the target language side). For this reason we 
filtered the dictionary using the lists of Wikipe-
                                                 
2
 http://www.ftsk.uni-mainz.de/user/rapp/comtrans/ 
There click on "Dictionaries of word pairs" and then 
download "English - German". 
dia-derived multiword expressions as described 
in section 2.1. These contained 418,627 items for 
English and 1,212,341 candidate items for Ger-
man (the latter included unigram compounds). 
That is, in the dictionary those items were re-
moved where either the English side did not 
match any of the English MWEs, or where the 
German side did not match any of the German 
candidates.  
This intersection resulted in a reduction of our 
bigram dictionary from 371,590 items to 137,701 
items. Table 6 shows the results after filtering the 
items listed in Table 5. Note that occasionally 
reasonable MWEs are eliminated if they happen 
not to occur in Wikipedia, or if the algorithm for 
extracting the MWEs does not identify them. 
The reduced dictionary we considered as an 
appropriate gold standard for the automatic eval-
uation of our system. 
 
ENGLISH BIGRAM GERMAN TRANSLATION 
credible if  dann glaubw?rdig * wenn  
credible if  glaubhaft * wenn  
credible if  glaubw?rdig * wenn  
credible in  in * Glaubw?rdigkeit  
credible in  in * glaubw?rdig  
credible investigation  glaubw?rdige Untersuchung  
credible labelling  glaubw?rdige Kennzeichnung  
credible manner  glaubw?rdig  
credible military  glaubw?rdige milit?rische  
credible military  glaubw?rdigen milit?rischen  
credible only  nur dann glaubw?rdig  
credible partner  glaubw?rdiger Partner  
credible policy  Politik * glaubw?rdig  
credible policy  glaubw?rdige Politik  
credible reports  glaubw?rdige Berichte  
credible response  glaubw?rdige Antwort  
credible solution  glaubw?rdige L?sung  
credible system  glaubw?rdiges System  
credible threat  glaubhafte Androhung  
credible to  f?r * glaubw?rdig  
credible to  glaubw?rdig 
Table 5. Extract from the COMTRANS bigram dictionary. 
 
ENGLISH BIGRAM GERMAN TRANSLATION 
credible investigation glaubw?rdige Untersuchung 
credible only nur dann glaubw?rdig 
credible policy glaubw?rdige Politik 
credible response glaubw?rdige Antwort 
credible solution glaubw?rdige L?sung 
credible system glaubw?rdiges System 
credible threat glaubhafte Androhung 
credible to glaubw?rdig 
Table 6. Extract from the bigram dictionary after filtering. 
 
92
As in section 3.2, the next step was to apply 
the keyword extraction algorithm to the English 
and the German Wikipedia documents. Hereby 
only terms occurring in the gold standard dic-
tionary were taken into account. But it turned out 
that, when using the same log-likelihood thresh-
old as in section 3.2, only few keyterms were 
assigned: on average less than one per document. 
This had already been a problem in 3.2, but it 
was now considerably more severe as this time 
the MWE lists had been filtered, and as the filter-
ing had been on the basis of another type of cor-
pus (Europarl rather than Wikipedia). 
This is why, after some preliminary experi-
ments with various thresholds, we finally de-
cided to disable the log-likelihood threshold. In-
stead, on the English side, all keyterms from the 
gold standard were used if they occurred at least 
once in the respective Wikipedia document. On 
the German side, as here we had many unigram 
compounds which tend to be more stable and 
therefore more repetitive than MWEs, we used 
the keyterms if the occurred at least twice. This 
way for most documents we obtained at least a 
few keyterms. 
When running the WINTIAN algorithm on the 
parallel keyword lists, in some cases reasonable 
results were obtained. For example, for the direc-
tion English to German, the system translates 
information society with Informationsgesell-
schaft, and education policy with Bildungs-
politik. As WINTIAN is symmetric and can 
likewise produce a dictionary in the opposite di-
rection, we also generated the results for German 
to English. Here, among the good examples, are 
Telekommunikationsmarkt, which is translated as 
telecommunications market, and Werbekam-
pagne, which is translated as  advertising cam-
paign. However, these are selected examples 
showing that the algorithm works in principle. 
Of more interest is the quantitative evaluation 
which is based on thousands of test words and 
uses the gold standard dictionary. For English to 
German we obtained an accuracy of 0.77% if 
only the top ranked word is taken into account, 
i.e. if this word matches the expected translation. 
This improves to 1.6% if it suffices that the ex-
pected translation is ranked among the top ten 
words. The respective figures for German to 
English are 1.41% and 2.04%. 
The finding that German to English performs 
better can be explained by the fact that other than 
English German is a highly inflectional lan-
guage. That is, when generating translations it is 
more likely for German that an inflectional vari-
ant not matching the gold standard translation is 
ranked first, thus adversely affecting perform-
ance. 
A question more difficult to answer is why the 
results based on the gold standard are considera-
bly worse than the ones reported in section 3.2 
which were based on human judgment. We see 
the following reasons: 
 
? The evaluation in section 3.2 used only a 
small sample so might be not very reliable. 
Also, other than here, it considered only 
source language words with frequencies 
above nine. 
? Unlike the candidate expressions, the gold 
standard data is not lemmatized on the target 
language side. 
? The hard string matching used for the gold-
standard-based evaluation does not allow for 
inflectional variants. 
? The gold-standard-based evaluation used 
terms resulting from the intersection of term 
lists based on Wikipedia and Europarl. It is 
clear that this led to a reduction of average 
term frequency (if measured on the basis of 
Wikipedia), thus increasing the problem of 
data sparseness. 
? As for the same reason the log-likelihood 
threshold had to be abandoned, on average 
less salient terms had to be used. This is 
likely to additionally reduce accuracy. 
? For many terms the gold standard lists sev-
eral possible translations. In the current im-
plementation of the evaluation algorithm 
only one of them is counted as correct. 3 
However, in the human evaluation any rea-
sonable translation was accepted. 
? Some reasonable MWE candidates extracted 
from Wikipedia are not present in the gold 
standard, for example credible evidence, 
credible source, and credible witness are not 
frequent enough in Europarl to be selected 
for alignment. 
 
We should perhaps mention that it would be pos-
sible to come up with better looking accuracies 
by presenting results for selected subsets of the 
source language terms. For example, one could 
concentrate on terms with particularly good cov-
                                                 
3
 This can be justified because an optimal algorithm 
should provide all possible translations of a term. If 
only some translations are provided, only partial 
credit should be given. But this is likely to average 
out over large numbers, so the simple version seems 
acceptable. 
93
erage. Another possibility would be to consider 
MWEs consisting of nouns only. This we actu-
ally did by limiting source and target language 
vocabulary (of MWEs) to compound nouns. The 
results were as follows: 
 
    English to German (top 1):  1.81% 
    English to German (top 10):  3.75% 
    German to English (top 1):  2.03% 
    German to English (top 10):  3.16% 
 
As can be seen, these results look somewhat bet-
ter. But this is only for the reason that translating 
compound nouns appears to be a comparatively 
easier task on average.  
4 Conclusions and future work 
We have presented a method for identifying term 
translations using aligned comparable docu-
ments. Although it is based on a knowledge poor 
approach and does not presuppose a seed lexi-
con, it delivers competitive results for single 
words.  
A disadvantage of our method is that it pre-
supposes that the alignments of the comparable 
documents are known. On the other hand, there 
are methods for finding such alignments auto-
matically not only in special cases such as 
Wikipedia and newspaper texts, but also in the 
case of unstructured texts (although these meth-
ods may require a seed lexicon). 
Concerning the question from the introduc-
tion, namely whether the translation (and conse-
quently also the meaning) of a multiword unit is 
determined compositionally or contextually, our 
answer is as follows: For the type of multiword 
units we were investigating, namely automati-
cally extracted collocations, our results indicate 
that looking at their contextual behavior usually 
does not suffice. The reasons seem to be that 
their contextual behavior shows a high degree of 
variability, that their translations tend to be less 
salient than those of single words, and that the 
problem of data sparseness is considerably more 
severe. 
It must be seen, however, that there are many 
types of multiword expressions, such as idioms, 
metaphorical expressions, named entities, fixed 
phrases, noun compounds, compound verbs, 
compound adjectives, and so on, so that our re-
sults are not automatically applicable to all of 
them. Therefore, in future work we intend to 
compare the behavior of different types of mul-
tiword expressions (e.g. multiword named enti-
ties and short phrases such as those used in 
phrase-based machine translations) and to quan-
tify in how far their behavior is compositional or 
contextual. 
Acknowledgment 
This research was supported by a Marie Curie 
Intra European Fellowship within the 7th Euro-
pean Community Framework Programme. 
References 
Babych, B., Sharoff, S., Hartley, A., and Mudraya, O. 
(2007). Assisting Translators in Indirect Lexical 
Transfer. Proceedings of the 45th Annual Meeting 
of the Association for Computational Linguistics 
ACL 2007, Prague, Czech Republic.  
Daille, B.; Morin, E. (2012). Revising the composi-
tional method for terminology acquisition from 
comparable corpora. Proceedings of Coling 2012, 
Mumbai.  
Delpech, E.; Daille, B.; Morin, E., Lemaire, C. 
(2012). Extraction of domain-specific bilingual 
lexicon from comparable corpora: compositional 
translation and ranking. Proceedings of Coling 
2012, Mumbai.  
Diab, M., Finch, S. (2000): A statistical wordlevel 
translation model for comparable corpora. In: Pro-
ceedings of the Conference on Content-Based Mul-
timedia Information Access (RIAO). 
Friedl, J. (2002). Mastering Regular Expressions. 
O'Reilly.  
Fung, P. (1995). Compiling bilingual lexicon entries 
from a non-parallel English-Chinese corpus. In: 
Proceedings of the  Third Annual Workshop on Ve-
ry Large Corpora, Boston, Massachusetts. 173-
183.  
Fung, P.; Yee, L. Y. (1998). An IR approach for 
translating new words from nonparallel, compara-
ble texts. Proceedings of  COLING/ACL 1998, 
Montreal, Canada. 414-420.  
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., Klein, 
D. (2008): Learning bilingual lexicons from mono-
lingual corpora. In: Proceedings of ACL-HLT 
2008, Columbus, Ohio. 771-779.  
Harris, Z.S. (1954). Distributional structure. Word, 
10(23), 146?162. 
Hassan, S., Mihalcea, R. (2009): Cross-lingual seman-
tic relatedness using encyclopedic knowledge. In: 
Proceedings of EMNLP.  
Justeson, J.S.; Katz, S.M. (1995). Techninal terminol-
ogy: some linguistic properties and an algorithm for 
identification in text. Natural Language  Engineer-
ing, 1(1): 9?27. 
Moon, R.E. 1998. Fixed Expressions and Idioms in 
English: A Corpus-based Approach. Oxford: Clar-
endon Press.  
94
Prochasson, E., Fung, P. (2011). Rare word transla-
tion extraction from aligned comparable docu-
ments. In: Proceedings of ACL-HLT. Portland .  
Rapp, R. (1995). Identifying word translations in non-
parallel texts. In: Proceedings of the 33rd Annual 
Meeting of the ACL. Cambridge, MA, 320-322.  
Rapp, R. (1996). Die Berechnung von Assoziationen. 
Hildesheim: Olms. 
Rapp, R. (1999). Automatic identification of word 
translations from unrelated English and German 
corpora. Proceedings of the 37th Annual Meeting of 
the Association for Computational Linguistics, Col-
lege Park, Maryland. 519?526. 
Rapp, R., Sharoff,  S., Babych, B. (2012). Identifying 
word translations from comparable documents 
without a seed lexicon. In: Proceedings of the 8th 
Language Resources and Evaluation Conference, 
LREC 2012, Istanbul.  
 
Rayson, P.; Garside, R. (2000). Comparing corpora 
using frequency profiling. Proceedings of the 
Workshop on Comparing Corpora (WCC '00 ), Vol-
ume 9, 1?6. 
Robitaille, X., Sasaki, Y., Tonoike, M., Sato, S., Utsu-
ro, T. (2006). Compiling French-Japanese termi-
nologies from the web. In: Proceedings of the 11th 
Conference of EACL, Trento, Italy, 225-232.  
Rumelhart, D.E.; McClelland, J.L. (1987). Parallel 
Distributed Processing. Explorations in the Micro-
structure of Cognition. Volume 1: Foundations. 
MIT Press. 
Schafer, C., Yarowsky, D (2002).: Inducing transla-
tion lexicons via diverse similarity measures and 
bridge languages. In: Proceedings of CoNLL. 
Schmid, H. (1994). Probabilistic part-of-speech tag-
ging using decision trees. International Conference 
on New Methods in Language Processing, 44?49. 
 
95
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 1?14,
Dublin, Ireland, August 23, 2014.
The CogALex-IV Shared Task on the Lexical Access Problem 
 
 
Reinhard Rapp 
Aix-Marseille Universit? 
13288 Marseille 
France 
reinhardrapp@gmx.de 
 
Michael Zock 
Aix-Marseille Universit? 
13288 Marseille 
France 
michael.zock@lif.univ-mrs.fr 
 
 
Abstract 
The shared task of the 4th Workshop on Cognitive Aspects of the Lexicon (CogALex-
IV) was devoted to a subtask of the lexical access problem, namely multi-stimulus as-
sociation. In this task, participants were supposed to determine automatically an ex-
pected response based on a number of received stimulus words. We describe here the 
task definition, the theoretical background, the training and test data sets, and the 
evaluation procedure used for ranking the participating systems. We also summarize 
the approaches used and present the results of the evaluation. In conclusion, the out-
come of the competition are a number of systems which provide very good solutions to 
the problem. 
1 Introduction 
In the framework of CogALex-IV (co-located with COLING 2014 in Dublin) we invited colleagues to 
participate in a shared task devoted to the lexical access problem in language production. Our aim was 
to make a quantitative comparison between different systems based on a shared set of data and using 
the same evaluation metric. 
The lexical access problem is very relevant for this workshop series as the quality of a dictionary 
depends not only on its coverage, but also on the accessibility of the information. Put differently, a 
crucial point of dictionary development is word access by the language producer, an often neglected 
aspect. Access strategies vary with the task (text understanding versus text production) and the knowl-
edge available at the very moment of consultation (words, concepts, speech sounds). Unlike readers 
who look for meanings, writers start from them, searching for the corresponding words. While paper 
dictionaries are static, permitting only limited strategies for accessing information, their electronic 
counterparts promise dynamic, proactive search via multiple criteria (meaning, sound, related words) 
and via diverse access routes. Navigation takes place in a huge conceptual lexical space, and the re-
sults are displayable in a multitude of forms (e.g. as trees, as lists, as graphs, or sorted alphabetically, 
by topic, by frequency).  
Given a great number of possibilities of approaching the lexical access problem, we felt that for a 
competition it was necessary to narrow down the choices in order to be able to come up with a clear 
task definition. Therefore the CogALex shared task focused on a crucial subtask, namely multi-stimu-
lus association. What we mean by this is the following. Suppose we were looking for a word matching 
the following description: tasty nut with hard shell originally from Australia, but could not retrieve the 
corresponding and intended form macadamia. This is the well known tip-of-the-tongue problem where 
an author knows the word but fails to access its form, even though he is able to retrieve certain fea-
tures of it (meaning, sound, syllables, ...). People being in the tip-of-the-tongue state always remember 
something concerning the elusive word (Brown & Mc Neill, 1966). This being so, it would be nice to 
have a system accepting this kind of information as input, and which then proposes a number of can-
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 
1
didates which ideally should contain the target word. Given the above example, we might enter tasty, 
nut, hard, shell, and Australia, and the system would be supposed to come up with one or several as-
sociated words such as macadamia, walnut, cashew, or coconut. 
This paper is meant to provide an overview on the shared task and on its results. It is organized as 
follows: Section 2 gives some background concerning the theory of word finding. Section 3 describes 
the task definition and Section 4 the training and the test data sets and the evaluation procedure. Sec-
tion 5 lists the participating systems, tries to characterize the different approaches, and presents the 
results. For all systems but one, further details are given in the separate papers (in these proceedings) 
as provided by the members of the participating groups. Section 6 summarizes the conclusions. 
2 The problem of word finding 
One could imagine many kinds of shared tasks within the framework of the CogALex workshop. Yet, 
we have focused here on a very specific problem, namely word finding. To this end we have defined a 
task demanding participants to come up with a system able to compute reversed word associations. 
While in the standard association experiment people are asked to provide the associations coming to 
their mind given some stimulus (prime), we have reversed this situation. Given a set of associations, 
the system was supposed to predict its trigger. More concretely speaking, participants were given 2000 
sets of words, each set containing five words. The task was to determine automatically the sixth ele-
ment, i.e. the prime (or stimulus), evoking the five words. One could object that this task does not 
really address the word access problem or its solution, but this is not quite so as we will try to show.  
In particular, it seems quite reasonable to claim that an association network with bi-directional links 
(see Rapp, 2014) is a suitable resource to support word ?finding?. Since words are connected via bi-
directional links either of the connected items can be the source or the target during the search (or dur-
ing navigation). 
Although systems designed for the shared task can have many applications (see Section 6), a proto-
typical one is the tip-of-the-tongue problem, which is a special case (yet a quite frequent one) of word 
access. So let us briefly describe this problem and the steps needed to overcome it.  
One of the most vexing problems in speaking or writing is that one knows a given word, yet fails to 
access it when needed. Suppose, you were looking for a word expressing the following ideas: superior 
dark coffee made of beans from Arabia, but could not retrieve the intended word mocha. What will 
you do in a case like this? You know the meaning, you know how or when to use the corresponding 
word, and in principle you even seem to know its spoken or written form, since you have used it some 
time ago (for more details, see Zock et al., 2010). Yet for some unknown reason you simply cannot 
access it at the very moment of writing or speaking. The just described situation is called anomia or 
dysnomia, which in less technical terms means that a person has a word finding problem. This case is 
often assimilated with the tip-of-the-tongue phenomenon, which technically speaking is not quite cor-
rect, but this shall not concern us here.1 
To resolve the problem, one can think of many strategies. For example, one can ask somebody, by 
providing him some hints (cues) hoping that the person can guess the elusive word. Such hints could 
take various forms like a description (definition or circumlocution), an association or the role played 
by the target word, say, instrument used for eating Chinese food when searching for chopsticks. Of 
course, one can also search in an external resource (dictionary). Unfortunately, most dictionaries are 
primarily designed for the language recipient and not particularly well suited to assist the language 
producer. And even if there are quite a number of promising proposals,2 a lot more could be done 
these days with the help of corpora, computers, and language technology. 
                                                 
1
 The tip-of-the-tongue phenomenon (http://en.wikipedia.org/wiki/Tip_of_the_tongue) is a weak form of an ano-
mic aphasia (http://en.wikipedia.org/wiki/Anomic_aphasia). Yet, unlike the latter, it is only momentary. It is 
characterized by the fact that the person (speaker/writer) has only partial access to the word s/he is looking for. 
The typically lacking parts are phonological (syllables, phonemes). Since all information except this last one 
seems to be available, and since this is the one preceding articulation, we say: the word is stuck on the tip of the 
tongue. 
2
 Think of Roget?s Thesaurus (Roget, 1852), WordNet (Fellbaum, 1998; Miller et al., 1990), Longman?s Lang-
uage Activator (Summers, 1993), the Oxford Reverse Dictionary (Edmonds, 1999) or OneLook which combines 
a dictionary, WordNet, and an encyclopedia, Wikipedia (http://onelook.com/reverse-dictionary.shtml). 
2
This being said, to build a dictionary for the language producer, certain provisions must be made, 
and it is easy to understand why. When searching a word form (target), the dictionary user will cer-
tainly not search in the entire resource. He will rather navigate in a substantially smaller subset (Zock, 
2014; Zock & Cristea, 2014). The question is, how to build this reduced space and how to support then 
navigation. We will deal here mainly with this first step of search space reduction as it is crucial and 
this is where associations come into play (Deese, 1965; Cramer, 1968). 
The experiments concerning the tip-of-the-tongue problem have systematically shown (Aitchison, 
2003; Brown, 1991; Brown & McNeill, 1996) that users being in this state always know ?something? 
concerning the target word: fragments of the meaning, origin, number of syllables, etc. This being so, 
any of this could be used to guide the search. 
Suppose we focused only on the semantic aspects. In such a case it is reasonable to assume that the 
target form can be found on the basis of its defining elements (bag of the words contained in the defi-
nition). While not being perfect, this works quite well (Dutoit & Nugues, 2002; El-Kahlout & Oflazer, 
2004; Mandala et al., 1999; Michiels, 1982). Actually, even Google - although not designed for this - 
is able to recover in many cases the elusive word. Just try the following example, spring, typically 
found in Iceland or in the Yellowstone National Park, discharging hot water and steam, and chances 
are that you will find the target word geyser. Although not perfect, this is nevertheless quite useful. 
However, this represents only one kind of cognitive state (knowledge of the definition), and this is cer-
tainly neither the only one nor the most frequent one. Indeed, there are many situations where it is hard 
to come up with a precise definition, and in this case other types of information are used to initiate 
search, for example, co-occurrences, associations, etc. Hence, if our target is mocha it may be accessi-
ble not only via its definitional terms (coffee, beverage, ...) but also via any of its associates: black, 
hot, drink, Java, etc. This is the point where associations come to the centre stage. 
Some of the related recently published work has been cited in Rapp (2014), and some other is men-
tioned by the authors participating in the shared task. Therefore, let us focus here primarily on some of 
the earlier and nowadays often overlooked related work. 
Associative networks have been very popular in Artificial Intelligence at the end of the nineteen-
seventies (Findler, 1979). They were proposed to be used for many tasks such as word sense disam-
biguation, finding brand names, reading between the lines, subliminal communication, brainstorming, 
and supporting word finding. That is, the tip-of-the-tongue problem is but one of the many possible 
applications. 
The study of associative networks was motivated by the goal to understand the organization of the 
human memory and the mental lexicon. This led to the building of lexical graphs like WordNet (Fell-
baum, 1998), the study of the tip-of-the-tongue problem (Brown & Mc Neill, 1966), error analysis 
(Fromkin, 1980, 1973) and priming experiments. Priming is said to take place if exposure to one 
stimulus increases significantly the response to another. Meyer and Schvaneveldt (1971) showed in 
their seminal experiments that people were faster in deciding that a string of letters is a word when it 
was followed by an associatively or semantically related word. For example, nurse is recognized more 
quickly following doctor than following bread. These findings supported also the idea of activation 
spreading as a method of access or search (Collins & Loftus, 1975). 
Associative networks can be considered as a special type of semantic network which were intro-
duced by Richens (1956) and by Ceccato (1956) for quite a different purpose. They were meant to 
serve as an interlingua for machine translation. These knowledge representation structures were then 
further developed in the sixties by Simmons (1963) and Quillian (1963, 1966, 1967, 1968, 1969). 
They finally became famous due to the work done by Quillian and two psychologistst (Collins & Quil-
lian, 1969 & 1970 and Collins & Loftus, 1975). Note that semantic networks can represent language at 
various levels of granularity: word, sentence (Sowa, 1984) or discourse (Mann & Thomson, 1988). 
Also, and very relevant for us here is the fact that at the word level, they can represent its semantics, 
i.e. meaning (Nogier & Zock, 1992), or its place withing the global structure of the mental lexicon 
(Miller, 1995; Aitchison, 2003; Bonin, 2004). In this latter case words are connected by associations 
rather than by deep-case roles, and the resulting graphs show word neighborhood (Schvaneveldt, 
1989). The fact that the mental lexicon exhibits ?small world? characteristics (http://en.wikipedia.org/ 
wiki/Small-world_network) has been shown by Vitevitch (2008) and by Sporns and colleagues (2004). 
For the construction of associative networks knowledge about associations is required. Such knowl-
edge can be obtained in two different ways. One is to ask people what a given term (say cat) evokes in 
3
their mind (say dog, mouse, etc.). Another option is to look at word co-occurrences in corpora, and to 
derive the associations from them (which, strictly speaking, pre-supposes that the human brain is also 
doing this). For the purpose of having a gold standard for the shared task, by using the EAT, we have 
opted for the first possibility. In contrast, most systems constructed by the shared task participants rely 
on the second.  
3 Task definition 
The participants received lists of five given words (primes) such as circus, funny, nose, fool, and Coco 
and were supposed to compute the word most closely associated to all of them. In this case, the word 
clown would be the expected response. Table 1 shows some more examples. 
 
Given Words Target Word 
gin, drink, scotch, bottle, soda whisky 
wheel, driver, bus, drive, lorry car 
neck, animal, zoo, long, tall giraffe 
holiday, work, sun, summer, abroad vacation 
home, garden, door, boat, chimney house 
blue, cloud, stars, night, high sky 
 
Table 1. Lists of given words together with their targets. 
 
We provided a training set of 2000 sets of five input words (multiword stimuli), together with the ex-
pected target words (associative responses). The way how the datasets were produced will be de-
scribed in the next section. The participants had about five weeks to train their systems on this data. 
After the training phase, we released a test set containing another 2000 sets of five input words, but 
without providing the expected target words.  
The participants were given five days to run their systems on the test data,3 with the goal of predict-
ing the target words. For each system, we compared the results to the expected target words and com-
puted an accuracy based on the number of exact string matches (but without taking capitalization into 
account). The participants were invited to submit a paper describing their approach and their results.  
For the participating systems, we distinguished two categories:  
1) Unrestricted systems. They could use any kind of data to compute their results.  
2) Restricted systems based on ukWaC: These systems were only allowed to draw on the freely 
available ukWaC corpus (Ferraresi et al., 2008)4 in order to extract information on word asso-
ciations. The ukWaC corpus comprises about 2 billion words of web texts and provides also 
lemma and part-of-speech information. 
Participants could compete in either category or in both. They were encouraged to further improve on 
their results outside of the competition after the deadline, and to describe these advances in their pa-
pers (in these proceedings). 
4 Training and test data sets and evaluation procedure 
The training and the test data sets were both derived from the Edinburgh Associative Thesaurus (EAT; 
Kiss et al., 1973). The EAT lists for each of 8400 stimulus words up to 100 associative responses as 
obtained from test persons who were asked to produce the word coming spontaneously to their mind. 
As the EAT uses uppercase characters only, and as this might not suit everybody's needs, we de-
cided to modify its capitalization. For this purpose, for each word occurring in the EAT, we looked up 
which form of capitalization showed the highest occurrence frequency in the British National Corpus 
(Burnard & Aston, 1998). By this form we replaced the respective word. E.g. DOOR was replaced by 
                                                 
3
 The exact dates were: training data release:  March 27, 2014; test data release: May 5, 2014; final results due:  
May 9, 2014. 
4
 http://wacky.sslmit.unibo.it/doku.php?id=corpora. 
4
door, and GOD was replaced by God. This way we hoped to come close to what might have been pro-
duced during compilation of the EAT if case distinctions had been taken into account.5 Since this 
method is not perfect, e.g. words often occurring in sentence initial position might be falsely capital-
ized, we did some manual checking, but cannot claim to have achieved perfection. 
Next, for each stimulus word, only the top five associations (i.e. the associations produced by the 
largest number of test person) were retained, and all other associations were discarded. The decision to 
keep only a small number of associations was motivated by the results of Rapp (2013) which indicate 
that associations produced by very few test persons tend to be of arbitrary nature. We also wanted to 
avoid unnecessary complications, which is why we decided on a fixed number, although the exact 
choice of five is of course somewhat arbitrary. 
From the remaining dataset we removed all items which contained non-alphabetical characters. We 
also removed items which contained words that did not occur in the BNC. The reason for this is that 
quite a few of them are misspellings. By these measures, the number of items was reduced from ini-
tially 8400 to 7416.  
From these we randomly selected 4000 items. 2000 of these were used as our training data set. The 
remaining 2000 were used as our test data set, but of course for the test set we removed the stimulus 
words. Tables 2 and 3 show the alphabetically first 20 items in each data set.6 
The participating teams were asked to submit a list of 2000 words reflecting their predictions con-
cerning the 2000 items of the test data set. For evaluation, we simply compared these 2000 words to 
the expected results (as taken from the EAT) by counting the number of exact matches, with the only 
flexibility that word capitalization was not taken into account. 
There are a number of reasons why it was very difficult for the teams to get the target words exactly 
right: 
1) In many cases, the given words might almost quite as strongly point to other target words. For 
example, when given the words gin, drink, scotch, bottle, and soda, instead of the target word 
whisky the alternative spelling whiskey should also be fine, and possibly some other beverages 
might also be acceptable. 
2) The target vocabulary was not restricted in any way, so in principle hundred thousands of 
words had to be considered. 
3) Although most of the target words were base forms, the training and the test sets also contain a 
good number of cases where the target words were inflected forms. Of course it is almost im-
possible to get these inflected forms exactly right. 
Because of these difficulties we expected low performance figures (e.g. below 10%) in the competi-
tion7 and were positively surprised by some of the actual results (see Section 5). 
Concerning point 1 (other acceptable solutions) our data source did not provide any, so it was not 
practical for us to try to come up with alternative solutions in the chosen reverse association frame-
work. 
Concerning point 2 (restriction of target vocabulary), of course all teams had to make assumptions 
about the underlying vocabulary, as it is already difficult to fix boundaries for the English vocabulary, 
and occasionally even foreign words or names might occur as associations. In this respect all results 
have to be taken with caution, as some teams might have been more lucky than others in making good 
guesses concerning the target vocabulary.8 
 
                                                 
5
 Note that the participants of the shared task were nevertheless free to discard all case distinctions if their ap-
proach would not require them. During evaluation, case distinctions were not taken into account. 
6
 From http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/pst.html the full data sets 
can be downloaded 
7
 Note that the results of up to 54% reported in Rapp (2014) were obtained using different data sets and severely 
restricted vocabularies, so these cannot be used for comparison. 
8
 For such reasons we had requested to include such information in the papers. We concede that a competition 
with a pre-defined target vocabulary might have been more fair by reducing the influence of chance. But we 
were also very interested in the approaches on how to limit this vocabulary, so this was an important part of the 
shared task. 
5
 Target Word Given Words 
a  B the alphabet an man 
abound  plenty many lots around leap 
about  around turn round now time 
above  below high over sky all 
abrasive  rough sandpaper rub cutting hard 
absence  away fonder illness leave presence 
absent  away minded gone present ill 
absurdity  stupid ridiculous mad stupidity clown 
accents  dialects language foreign speech French 
accordion  music piano play player instrument 
accountant  money chartered clerk office turf 
accrue  gather gain money acquire collect 
achieve  nothing attain gain success win 
acids  alkalis alkali bases burn science 
acknowledged  letter receipt accepted received replied 
acquaintance  friend know person friends casual 
acquired  got obtained gained taste bought 
acrid  smell bitter acid smoke dry 
actions  words deeds movement movements reactions 
actual  real fact happening truth exact 
 
Table 2: Extract from the training set. 
 
 
Given Words 
able incapable brown clever good 
able knowledge skill clever can 
about near nearly almost roughly 
above earth clouds God skies 
above meditation crosses passes rises 
abuse wrong bad destroy use 
accusative calling case Latin nominative 
ache courage blood stomach intestine 
ache nail dentist pick paste 
aches hurt agony stomach period 
action arc knee reaction jerk 
actor theatre door coach Act 
actress stage play man theatre 
addict pot store hash medicine 
Africa Bible priest abroad doctor 
again fresh afresh old morning 
against angry bad fight hostile 
age time epoch period years 
aid assistant kind mother good 
aid eyes aids see eye 
 
Table 3: Extract from the test set. The respective (undisclosed) target words are shown in Table 4. 
 
 
6
Concerning point 3 (matches of inflected forms) the ETS team had correctly pointed out that perform-
ance figures would significantly improve if matches with alternative inflected forms of the same word 
would also be counted as correct. For this purpose, the team kindly provided expanded versions of the 
target words for the training and for the test data set which were obtained using an in-house morpho-
logical tool. Table 4 shows the respective data for the alphabetically first 20 target words of the test 
data set. As we assumed that only the absolute but not the relative performance of the systems (rank-
ing in competition) would be affected by this measure, we decided not to include this in the standard 
procedure, but nevertheless forwarded the data to all teams and encouraged them to conduct such an 
evaluation by themselves outside of the competition (and some actually did so). Let us nevertheless 
point out our main concerns:  
1) Many target words are ambiguous, and in some cases the range of inflected forms depends on 
the way how the ambiguity is resolved. Assume, for example, that the target word form is can 
which might be an auxiliary verb or a noun. In this case, the inflected form cans in the ex-
panded list would only be correct if the target word can referred to the noun, but not if it re-
ferred to the auxiliary verb (see also Lezius et al., 1998). Of course one could try to disam-
biguate the target words based on the given words. But this is a non trivial task likely to be er-
ror prone and possibly controversial. 
2) In principle, such considerations might also apply to the given words, i.e. they could also be 
expanded. But in this case the disambiguation task is even more difficult as it is not clear what 
should be considered as context (i.e. as clues for disambiguation). 
Although point 2 could be left to the participants, our aim was to avoid any such complications, in or-
der to keep the focus on the core part of the shared task. So, as far as we as organizers were concerned, 
we decided not to consider inflectional variation. 
Let us now comment on the overall character of the shared task. It should be noted that this task is 
actually the reverse association task as described in Rapp (2013, 2014). That is, the shared task par-
ticipants were supposed to consider the associations from the EAT as their given words, and their task 
was to determine the original stimulus words.  
 
Word Morphological expansions 
capable  
ability abilities 
approximately 
 
heavens heaven 
transcends transcending, transcend, transcended 
misuse misusing, misused, misuses 
vocative vocatives 
guts gut, gutted, gutting 
tooth tooths 
pains pain, paining, pained 
reflex reflexes 
stage staging, staged, stages 
actor actors 
drug drugging, drugs, drugged 
missionary missionaries 
anew  
antagonistic  
era eras 
helper helpers 
visual visuals 
 
Table 4: Morphological expansions of the first 20 words in the test data set. 
 
7
However, we had not disclosed the nature of the data until after the competition mainly for the follow-
ing reasons: 
 
1) To avoid reverse engineering approaches based on the EAT or similar association norms. 
2) To avoid leading participants in a particular direction. For us it seemed most important to obtain 
approaches as diverse as possible. And as this was the first shared task devoted to multi-
stimulus associations, we thought that this would be a unique opportunity to obtain contribu-
tions as unbiased as possible. 
 
On the other hand we had concerns about the fairness of not disclosing the nature of the data. Firstly, 
some of the participants might discover its origin and thus possibly have an advantage. Secondly, it is 
not clear in how far the reverse association task is prototypical enough for the lexical access problem 
as to assume that in terms of relative system performance the two tasks are comparable. In any case, 
concerning the lexical access problem we saw no chance of acquiring large scale data sets within the 
given time frame, so it was clear that this was not feasible.  
When, after the competition, we disclosed the nature of the data, we invited the participants to com-
ment on these issues in their papers, and it was very interesting for us to learn about the different 
views.  
5 Participating systems and results 
Altogether 15 teams expressed their interest to participate in the shared task. Of these, ten teams actu-
ally submitted results, of which one (BRNO) participated in both tracks (ukWaC and unrestricted), and 
another (SAAR) provided two solutions for the unrestricted track. The teams who submitted results 
are listed in Table 5, where each team is assigned a short Team ID which is derived from the institu-
tion names. In Table 6 for each team we make an attempt to give short characterizations of the ap-
proaches and the resources used. 
Most approaches are variants of analyzing word co-occurrence statistics as derived from large text 
corpora. Several teams, among them the best performing ones, use for this purpose the open source 
tool Word2Vec which provides two neural network-based model architectures for computing continu-
ous vector representations of words from very large data sets (Mikolov, 2013a; Mikolov, 2013b). In 
contrast, the RACAI team uses WordNet relation chains, a method which makes absolutely sense, but 
seems to severely suffer from data sparseness issues (i.e. there are much fewer WordNet relations be-
tween words than there are non-random word co-occurrences within large corpora). This finding is 
confirmed by the BRNO and UBC teams who tried out both approaches (corpus-based and WordNet-
based) and came to the conclusion that the corpus-based approach performed considerably better. 
Let us emphasize that we consider this type of findings a valuable output of the shared task and 
therefore are very grateful to the teams who pursued the WordNet-based approach that they shared 
these results although they were all well aware that, despite excellent scientific work, the respective 
performance figures were not very competitive. 
Table 7 shows the results of the competition, ranked according to the accuracy of the results, and 
indicating the respective track (ukWAC or unrestricted). As some teams (AMU, QUT, SOEN, ranks 7 
to 9) could not quite make it for the deadline, they were granted an extension of three days. On the top 
four positions are submissions who all used the above mentioned Word2Vec tool, indicating that this 
software is well suited for this task. Note that the winning system (IIITH) opted for the CBOW (con-
tinuous bag-of-words) architecture, whereas the other three opted for the skip-gram architecture. This 
might be an explanation for the differences in the results. However, this must be further analyzed as 
there are also other differences, including the assumptions constraining the target vocabulary, which, 
as described in Section 4, is an important issue. For example, the IIITH team used a frequency thresh-
old of 25 while making word vectors using Word2Vec. In addition, when calculating PMI (pointwise 
mutual information) associations, a frequency threshold (for bigrams) of 3 was used (see sections 4.1 
and 4.2 of their paper).  
It should be mentioned that, like some others (see e.g. the papers by the ETS and by the RACAI 
teams), the IIITH team was able to improve on their results after the shared task deadline. Whereas for 
their submission they had used a re-ranking procedure based on point-wise mutual information (PMI), 
later on they used weighted PMI as their association measure. This improved their results from 
8
30.45% to 34.9%. Likewise, the ETS team could improve their results from 14.95% to 18.90%. And 
the RACAI team (who used a WordNet-based approach) was able to almost double their results from 
1.50% to 2.95%. 
 
Team ID Affiliation Team members / Authors of papers 
AMU Aix-Marseille University, France Gemma Bel-Enguix 
BRNO Brno University of Technology, Czech Republic Lubomir Otrusina, Pavel Smrz 
ETS Educational Testing Service, Princeton, USA Michael Flor, Beata Beigman Klebanov 
IIIT International Institute of Information Technology (IIIT), Hyderabad, India Urmi Gosh, Sambhav Jain, Soma Paul 
LEIPZIG University of Leipzig, Germany 
Rico Feist, Daniel Gerighausen, Manuel 
Konrad, Georg Richter, Thomas Eckart, 
Dirk Goldhahn, Uwe Quasthoff 
QUT Queensland University of Technology, Brisbane, Australia Laurianne Sitbon, Lance De Vine 
RACAI Romanian Academy Research Institute for Artificial Intelligence, Bukarest, Romania 
Catalin Mititelu, Verginica Barbu Mit-
itelu 
SAAR Saarland University, Germany Asad Sayeed (no paper) 
SOEN Universities of Stuttgart, Osnabr?ck, and Erlangen-N?rnberg, Germany Gabrielle Lapesa, Stefan Evert 
UBC University of the Basque Country, Spain Josu Goikoetxea, Eneko Agirre, Aitor Soroa 
 
Table 5: Participating teams. 
 
Team ID Approach Resources used 
AMU Co-occurrence-based lexical graph British National Corpus 
BRNO Word2Vec from Python package GenSim (skip-gram architecture) ukWaC, ClueWeb12, WordNet 
ETS 
Aggregating co-occurrence-based 
association strengths to individual 
cue words 
English Gigaword 2003, ETS in-
house corpus 
IIITH Word2Vec using CBOW architec-ture and re-ranking ukWaC 
LEIPZIG Sum of co-occurrence-based sig-
nificance values Leipzig corpora collection 
QUT 
Own implementation similar to the 
Word2Vec package (skip-gram 
architecture) 
ukWaC 
RACAI Shortest WordNet relations chain 
and maximum entropy modeling 
Princeton WordNet, Google n-
gram corpus 
SAAR Co-occurrence-based ukWaC and others 
SOEN 
Ranking according to average (co-
occurrence-based) association 
strength or according to distri-
butional similarity 
ukWaC 
UBC 
Word2Vec (skip-gram architec-
ture), random walks, personalized 
PageRank 
Google news corpus, Wikipedia, 
WordNet 
 
Table 6: Overview on approaches and resources. 
9
To give a rough idea on how much the results can be improved when inflectional variants are toler-
ated during evaluation (see Section 4), let us mention that the IIITH team did so. This way their results 
improved from 34.90% (as obtained after the deadline) to 39.55. Likewise, in the case of the ETS team 
the results improved from 14.95% to 20.25%. (For details see the respective contributions in these 
proceedings.) 
Concerning the two tracks of the competition, namely ukWaC and unrestricted, it appears that the 
ukWaC corpus contains already enough information to solve the task. Evidence for this is provided by 
the BRNO team which submitted results in both tracks and where the improvements were minimal 
(19.85% vs. 19.65%). Another indication is that, unexpectedly, the winning IIITH team was in the 
ukWaC track. 
For details on all other approaches (except SAAR) see the papers provided by the participating 
teams in these proceedings. Ideas that occurred when discussing the shared task with other colleagues 
were that Adam Kilgarriff's SketchEngine might be a useful tool for solving the lexical access problem 
(thanks to Eva Schaeffer-Lacroix for pointing this out), and that it may be useful to take syntax into 
account (thanks to Eric Wehrli and Luka Nerima). The latter would be in analogy to the generation of 
distributional thesauri where working with parsed rather than raw corpora has been shown to lead to 
very good quality (see e.g. Pantel & Lin, 2002). This way, rather than taking all word co-occurrences 
into account, the focus can be laid on selected relations between words, such as e.g. head-modifier or 
subject-object relations.  
 
Rank Team ID Accuracy (%) Track 
1 IIITH 30.45 ukWAC 
2 BRNO 19.85 unrestricted 
3 BRNO 19.65 ukWaC 
4 UBC 16.35 unrestricted 
5 ETS 14.95 unrestricted 
6 LEIPZIG 14.05 unrestricted 
7 SOEN 13.10 ukWaC 
8 AMU 9.10 unrestricted 
9 QUT 4.25 ukWaC 
10 SAAR 3.50 unrestricted 
11 SAAR 2.60 unrestricted 
12 RACAI 1.50 unrestricted 
 
Table 7: Results of the shared task. 
6 Discussion and conclusions 
For the shared task of finding associations to multiple stimuli, by the participants accuracies of up to 
30% (35% after the deadline) were reported. Given the very conservative evaluation procedure (see 
Section 4) which relies on exact matches and does not give any credit to alternative solutions, this is a 
very good result which considerably exceeded our expectations. Although we do not have comparative 
figures on human performance, our guess is that humans would not be able to do much better on this. 
So, in some sense, it seems that we have rather perfect results. 
But what does this mean? Is there any psycholinguistic relevance? And is the task which we ad-
dressed here of any relevance for practical work in computational linguistics? 
Let us first discuss the question of psycholinguistic relevance. In Rapp (2011) we have argued that 
human language intuitions are based on the detection, memorization, and reproduction of statistical 
regularities in perceived language. But we have only discussed this for single words. Now we can do 
so for multiword stimuli. And it seems that the same mechanisms that apply to single word stimuli are 
also valid in the case of multiwords. Apparently, from a relatively limited corpus such as ukWaC, in-
tuitively plausible associations to an almost unlimited number of multiword stimuli can be derived. 
This is in analogy to human language acquisition: Due to limitations of the input channel a person can 
only perceive a few hundred million words during lifetime. But this limited information seems to suf-
fice to have intuitions on almost anything that is language related. 
10
This is a contradiction only on first glance: Apparently, language is a highly compressed form of in-
formation where all co-occurrences of words or word-sequences count (and were literally counted by 
most algorithms!). Therefore its information content is far higher than it may appear, and this provides 
a solution to the often discussed argument concerning the poverty of the stimulus (Landauer & Du-
mais, 1997). With regard to language, it seems there simply is no poverty of the stimulus, but instead 
the human language is a highly condensed form of extremely rich information. As the capacities of the 
input and the output channels are very limited, evolution was probably forced to optimize on this. 
As the systems participating in the shared task can simulate human intuitions concerning zillions of 
possible multiword stimuli, it is likely that their algorithms grasp some of the essence that governs the 
respective inference processes taking place in human memory. In particular, they provide evidence 
that human association processing is also co-occurrence based, and that this not only applies to asso-
ciations to single stimulus words as shown by Wettler et al. (2005), but also to associations concerning 
multiple stimuli. 
Concerning the practical relevance of the work, our feeling is that such systems will be useful addi-
tions to many language-related tasks requiring human-like intuitions for the reason that human lan-
guage intuitions seem to be based on associative learning. Let us come up with some examples of pos-
sible applications: 
1) Augment associative resources such as the EAT. 
2) Tip-of-the-tongue problem: Recall elusive words.  
3) Lexical access: Rather than relying on alphabetical order, encyclopedias and dictionaries can be 
accessed associatively (e.g. president of Poland ? Bronislaw Komorowski). 
4) Generating thesauri of related words: Related words in the sense of Pantel & Lin (2002) are 
second order associations. The words related to a given word can be determined by computing 
its associations, and by then computing the multi-stimulus associations to these. 
5) Question answering: Questions can be considered as multiword stimuli, answers as their asso-
ciations (e.g. height of Eiffel Tower ? 324 m). 
6) Paraphrasing: The meaning of a phrase can be characterized by the associations resulting from 
its content words. Paraphrases are likely to lead to similar associations. 
7) Search word generation in information retrieval: Keywords used in search queries can be aug-
mented with relevant other keywords. 
8) Advertising: The effect of an advertisement can be described by the associations evoked by the 
words that are used in it. 
9) Word sense induction and disambiguation: Word contexts can be replaced by their multi-stimu-
lus associations. This way the effects of word choice will be reduced when clustering contexts. 
10) Machine translation: Translations can be seen as associations across languages (seed dictionary 
is required, see below). 
Of course, most of the above has already been dealt with using other approaches. But, when looking at 
the respective (statistical) algorithms more closely, it seems often the case that researchers have intui-
tively chosen statistics which show some analogy to multi-stimulus associations. So what we suggest 
here is not entirely new. We nevertheless hope that the current framework can be useful. Firstly, it 
draws a connection to psycholinguistic evidence. And secondly, as done in the shared task, it allows to 
optimize the core algorithm independently of particular applications.  
To be a bit more explicit, let us try to sketch a possible agenda of some future work which we 
would be happy to see: Let us start from the hypothesis that the meaning of a short sentence or phrase 
can be characterized by the vector resulting from taking its content words as multiword stimuli, and by 
computing their associations. For example, given the sentence John laughed in the circus, we would 
take John, laugh, and circus as our stimulus words, and the resulting association vector could be ex-
pected to have high values at its positions corresponding to clown, nose, and fun. For conciseness, let 
11
us call this type of vector meaning vector.9 Now let us look at the sentences Someone walks across the 
border and A person passes customs. The two sentences do not share a single word. But the associa-
tions derived from them should be nevertheless similar, because associations such as toll, officer, or 
country can be expected to come up in both cases. That is, their meaning vectors should be similar, 
and this similarity can be quantified e.g. by computing the cosine similarity between them. We thus 
have a method which allows us to measure the similarity between sentences in a way that to some ex-
tend takes their meanings into account.  
Finally, we can try to cross language barriers and make the step to association-based machine trans-
lation (ABMT). To translate a source language phrase, we compute its meaning vector. Presupposing 
that we have a basic dictionary, in analogy to Rapp (1999) we can translate this meaning vector into 
the target language.10 Further assuming that we already know the meaning vectors of a very large 
number of target language phrases, we next select the target language meaning vector which is most 
similar to the source language meaning vector. The respective target language phrase can be consid-
ered to be the translation of the source language phrase. Optionally, to improve translation quality, the 
target language phrase can be modified by adding, removing, substituting, or reordering words with 
the aim of improving the similarity between the meaning vectors of the source and target language 
phrases. 
Acknowledgments 
This work was supported by a Marie Curie Intra European Fellowship within the 7th European Com-
munity Framework Programme. We would like to thank George R. Kiss and colleagues for creating 
the Edinburgh Associative Thesaurus, and Michael Wilson for making it publicly available. Many 
thanks also to Adriano Ferraresi and colleagues for providing the ukWaC corpus, and to the partici-
pants of the shared task for their contributions and comments, as well as for the pleasant cooperation. 
References 
Aitchison, J. (2003). Words in the Mind: an Introduction to the Mental Lexicon. Oxford, Blackwell.  
Bonin, P. (2004). Mental Lexicon: Some Words to Talk about Words. Nova Science Publishers.  
Brown, A. (1991). A review of the tip of the tongue experience. Psychological Bulletin, 10, 204?223.  
Brown, R. & Mc Neill, D. (1966). The tip of the tongue phenomenon. Journal of Verbal Learning and Verbal 
Behavior, 5: 325?337.  
Burnard, L.; Aston, G. (1998): The BNC Handbook: Exploring the British National Corpus with Sara. Edin-
burgh: University Press. 
Ceccato, S. (1956). La grammatiea insegnata alle machine. Civilt? delle Machine, Nos. 1 & 2. 
Collins, A.M. & Quillian, M.R. (1969). Retrieval time from semantic memory. Journal of verbal learning and 
verbal behavior 8 (2): 240?247.  
Collins, A.M. & Quillian, M.R. (1970). Does category size affect categorization time? Journal of verbal learning 
and verbal behavior 9 (4): 432?438. 
Collins, A.M. & Loftus, E.F. (1975). A spreading-activation theory of semantic processing. Psychological Re-
view 8.  
Cramer, P. (1968). Word Association. Academic Press, New York. 
Deese, J. (1965). The structure of associations in language and thought. Johns Hopkins Press. Baltimore 
                                                 
9
 As this is a bag-of-words approach which does not take syntax into account, of course we do not claim that 
such a vector can grasp all of a sentence's meaning. 
10
 Note that gaps in dictionary coverage can be typically tolerated in such a setting as associations tend to be 
common words. That is, in principle the method allows to correctly translate words which are not in the diction-
ary. This is a property giving it some plausibility as a model for the cognitive processes underlying human trans-
lation. 
12
Dutoit, D. and P. Nugues (2002): A lexical network and an algorithm to find words from definitions. In Frank 
van Harmelen (ed.): ECAI2002, Proceedings of the 15th European Conference on Artificial Intelligence, Ly-
on, 450?454. 
Edmonds, D. (ed.), (1999). The Oxford Reverse Dictionary, Oxford University Press, Oxford, 1999. 
El-Kahlout, I. D. and K. Oflazer. (2004). Use of Wordnet for Retrieving Words from Their Meanings. Procee-
dings of the 2nd Global WordNet Conference, Brno, 118?123. 
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database and some of its Applications. MIT Press. 
Ferraresi, A.; Zanchetta, E.; Baroni M.; Bernardini, S. (2008). Introducing and evaluating ukWaC, a very large 
web-derived corpus of English. In: S. Evert, A. Kilgarriff and S. Sharoff (eds.): Proceedings of the 4th Web as 
Corpus Workshop (WAC-4) ? Can we beat Google?, Marrakech. 
Findler, N. (editor). (1979). Associative Networks: The Representation and Use of Knowledge by Computers. 
Academic Press, Inc., Orlando, FL, USA. 
Fromkin V. (ed.). (1980). Errors in linguistic performance: Slips of the tongue, ear, pen and hand. New York: 
Academic Press. 
Fromkin, V. (ed.) (1973): Speech errors as linguistic evidence. The Hague: Mouton Publishers 
Kiss, G., Armstrong, C., Milroy, R. & Piper, J. (1973). An associative thesaurus of English and its computer 
analysis. In: A. Aitken, R. Beiley and N. Hamilton-Smith (eds.): The Computer and Literary Studies. Edin-
burgh: University Press. 
Landauer, T.K.; Dumais, S.T. (1997). A solution to Plato's problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge. Psychological Review 104 (2), 211?240. 
Lezius, W.; Rapp, R.; Wettler, M. (1998). A freely available morphology system, part-of-speech tagger, and con-
text-sensitive lemmatizer for German. In: Proceedings of COLING-ACL 1998, Montreal, Vol. 2, 743?748. 
Mandala, R., Tokunaga, T. & Tanaka, H. (1999). Complementing WordNet with Roget?s and Corpus-based The-
sauri for Information Retrieval. Proceedings of EACL. 
Mann, W. C. Thompson, S. A. (1988). Rhetorical structure theory: Toward a functional theory of text organiza-
tion. Text 8(3), 243?281.  
Meyer, D.E. & Schvaneveldt, R.W. (1971). Facilitation in recognizing pairs of words: Evidence of a dependence 
between retrieval operations. Journal of Experimental Psychology 90: 227?234. 
Michiels, A. (1982). Exploiting a Large Dictionary Database. PhD Thesis, University of Li?ge, mimeographed.  
Mikolov, T.; Chen, K.; Corrado, G.; Dean, J. (2013a). Efficient estimation of word representations in vector 
space. CoRR, abs/1301.3781. 
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.S.; Dean, J. (2013b). Distributed representations of words and 
phrases and their compositionality. Advances in Neural Information Processing Systems, 3111?3119. 
Miller, G. A. (1995). WordNet : A lexical database for english. Communications of the ACM, 38 (11), 39?41. 
Miller, G.A. (ed.) (1990): WordNet: An On-Line Lexical Database. International Journal of Lexicography, 3(4), 
235?244.  
Nogier, J.F. & Zock, M. (1992) Lexical choice by pattern matching. Knowledge Based Systems, Vol. 5, No 3, 
Butterworth. 
Pantel, P.; Lin, D. (2002): Discovering Word Senses from Text. Proceedings of ACM Conference on Knowledge 
Discovery and Data Mining (KDD-02). Edmonton, Canada , 613?619. 
Quillian, M. R. (1967). Word concepts: A theory and simulation of some basic semantic capabilities. Behavioral 
Science, 12(5), 410?430.  
Quillian, M. R. (1968). Semantic memory. Semantic Information Processing, 227?270.  
Quillian, M. R. (1969). The teachable language comprehender: a simulation program and theory of language. 
Communications of the ACM, 12(8), 459-476.  
Quillian, R. (1963). A notation for representing conceptual information: An application to semantics and me-
chanical English paraphrasing. SP-1395, System Development Corporation, Santa Monica.  
13
Quillian, R. (1966). Semantic Memory. Unpublished doctoral dissertation, Carnegie Institute of Technology. 
Rapp, R. (1999). Automatic identification of word translations from unrelated English and German corpora. In: 
Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics 1999, College Park, 
Maryland. 519?526. 
Rapp, R. (2011). Language acquisition as the detection, memorization, and reproduction of statistical regularities 
in perceived language. Journal of Cognitive Science, Vol. 12, No. 3, 297?322. 
Rapp, R. (2013). From stimulus to associations and back. Proceedings of the 10th Workshop on Natural Langu-
age Processing and Cognitive Science, Marseille, France. 
Rapp, R. (2014). Corpus-based computation of reverse associations. Proceedings of the Ninth International Con-
ference on Language Resources and Evaluation (LREC 2014), Reykjavik, Island. 
Richens, R. H. (1956) Preprogramming for mechanical translation, Mechanical Translation 3 (1), 20?25. 
Roget, P. (1852). Thesaurus of English Words and Phrases. Longman, London. 
Schvaneveldt, R. (ed.) (1989). Pathfinder Associative Networks: studies in knowledge organization. Ablex. Nor-
wood, New Jersey, US. 
Simmons, R. (1963). Synthetic language behavior. Data Processing Management 5 (12): 11?18.  
Sowa, John F. (1984). Conceptual Structures: Information Processing in Mind and Machine, Addison-Wesley, 
Reading, MA. 
Sporns, O., Chialvo, D. R., Kaiser, M., & Hilgetag, C. C. (2004). Organization, development and function of 
complex brain networks. Trends in Cognitive Sciences, 8, 418?425. 
Summers, D. (1993). Language Activator: the world?s first production dictionary. Longman, London. 
Vitevitch, M. (2008). What can graph theory tell us about word learning and lexical retrieval? Journal of Speech, 
Language, and Hearing Research , 51:408?422. 
Wettler, M.; Rapp, R.; Sedlmeier, P. (2005). Free word associations correspond to contiguities between words in 
texts. Journal of Quantitative Linguistics 12(2), 111?122. 
Zock, M. (2014). How to overcome the tip-of-the-tongue problem with the help of a computer. Proceedings of 
CogALex-IV, COLING, Dublin, Ireland 
Zock, M.; Cristea, D. (2014). You shall find the target via its companion words: specification of tools and re-
sources to overcome the tip-of-the-tongue problem. Proceedings of the 11th International Workshop on Natu-
ral Language Processing and Cognitive Science (NLPCS), Venice. 
Zock, M.; Ferret, O.; Schwab, D. (2010). Deliberate word access : an intuition, a roadmap and some preliminary 
empirical results. International Journal of Speech Technology, 13(4), 107?117. 
 
14
