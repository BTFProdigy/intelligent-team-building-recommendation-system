Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 138?142,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Karlsruhe Institute for Technology Translation System for the
ACL-WMT 2010
Jan Niehues, Teresa Herrmann, Mohammed Mediani and Alex Waibel
Karlsruhe Instiute of Technolgy
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes our phrase-based Sta-
tistical Machine Translation (SMT) sys-
tem for the WMT10 Translation Task. We
submitted translations for the German to
English and English to German transla-
tion tasks. Compared to state-of-the-art
phrase-based systems we preformed addi-
tional preprocessing and used a discrim-
inative word alignment approach. The
word reordering was modeled using POS
information and we extended the transla-
tion model with additional features.
1 Introduction
In this paper we describe the systems that we
built for our participation in the Shared Trans-
lation Task of the ACL 2010 Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR. Our translations are generated using
a state-of-the-art phrase-based translation system
and applying different extensions and modifica-
tions including Discriminative Word Alignment,
a POS-based reordering model and bilingual lan-
guage models using POS and stem information.
Depending on the source and target languages,
the proposed models differ in their benefit for the
translation task and also expose different correl-
ative effects. The Sections 2 to 4 introduce the
characteristics of the baseline system and the sup-
plementary models. In Section 5 we present the
performance of the system variants applying the
different models and chose the systems used for
creating the submissions for the English-German
and German-English translation task. Section 6
draws conclusions and suggests directions for fu-
ture work.
2 Baseline System
The baseline systems for the translation directions
German-English and English-German are both de-
veloped using Discriminative Word Alignment
(Niehues and Vogel, 2008) and the Moses Toolkit
(Koehn et al, 2007) for extracting phrase pairs
and generating the phrase table from the discrimi-
native word alignments. The difficult reordering
between German and English was modeled us-
ing POS-based reordering rules. These rules were
learned using a word-aligned parallel corpus. The
POS tags for the reordering models are generated
using the TreeTagger (Schmid, 1994) for all lan-
guages.
Translation is performed by the STTK Decoder
(Vogel, 2003) and all systems are optimized to-
wards BLEU using Minimum Error Rate Training
as proposed in Venugopal et al (2005).
2.1 Training, Development and Test Data
We used the data provided for the WMT for train-
ing, optimizing and testing our systems: Our
training corpus consists of Europarl and News
Commentary data, for optimization we use new-
stest2008 as development set and newstest2009 as
test set.
The baseline language models are trained on
the target language part of the Europarl and News
Commentary corpora. Additional, bigger lan-
guage models were trained on monolingual cor-
pora. For both systems the News corpus was used
while an English language model was also trained
on the even bigger Gigaword corpus.
2.2 Preprocessing
The training data was preprocessed before used for
training. In this step different normalizations were
done like mapping different types of quotes. In
the end the first word of every sentence was smart-
cased.
138
For the German text, additional preprocessing
steps were applied. First, the older German data
uses the old German orthography whereas the
newer parts of the corpus use the new German
orthography. We tried to normalize the text by
converting the whole text to the new German or-
thography. In a first step, we search for words that
are only correct according to the old writing rules.
Therefore, we selected all words in the corpus, that
are correct according to the hunspell lexicon1 us-
ing the old rules, but not correct according to the
hunspell lexicon using the new rules. In a second
step we tried to find the correct spelling according
to the new rules. We first applied rules describing
how words changed from one spelling system to
the other, for example replacing ??? by ?ss?. If the
new word is a correct word according to the hun-
spell lexicon using the new spelling rules, we map
the words.
When translating from German to English, we
apply compound splitting as described in Koehn
and Knight (2003) to the German corpus.
As a last preprocessing step we remove sen-
tences that are too long and empty lines to obtain
the final training corpus.
3 Word Reordering Model
Reordering was applied on the source side prior
to decoding through the generation of lattices en-
coding possible reorderings of each source sen-
tence that better match the word sequence in the
target language. These possible reorderings were
learned based on the POS of the source language
words in the training corpus and the information
about alignments between source and target lan-
guage words in the corpus. For short-range re-
orderings, continuous reordering rules were ap-
plied to the test sentences (Rottmann and Vogel,
2007). To model the long-range reorderings be-
tween German and English, different types of non-
continuous reordering rules were applied depend-
ing on the translation direction. (Niehues and
Kolss, 2009). When translating from English to
German, most of the changes in word order con-
sist in a shift to the right while typical word shifts
in German to English translations take place in the
reverse direction.
1http://hunspell.sourceforge.net/
4 Translation Model
The translation model was trained on the parallel
corpus and the word alignment was generated by
a discriminative word alignment model, which is
described below. The phrase table was trained us-
ing the Moses training scripts, but for the German
to English system we used a different phrase ex-
traction method described in detail in Section 4.2.
In addition, we applied phrase table smoothing as
described in Foster et al (2006). Furthermore, we
extended the translation model by additional fea-
tures for unaligned words and introduced bilingual
language models.
4.1 Word Alignment
In most phrase-based SMT systems the heuristic
grow-diag-final-and is used to combine the align-
ments generated by GIZA++ from both direc-
tions. Then these alignments are used to extract
the phrase pairs.
We used a discriminative word alignment model
(DWA) to generate the alignments as described in
Niehues and Vogel (2008) instead. This model is
trained on a small amount of hand-aligned data
and uses the lexical probability as well as the fer-
tilities generated by the PGIZA++2 Toolkit and
POS information. We used all local features, the
GIZA and indicator fertility features as well as
first order features for 6 directions. The model was
trained in three steps, first using maximum likeli-
hood optimization and afterwards it was optimized
towards the alignment error rate. For more details
see Niehues and Vogel (2008).
4.2 Lattice Phrase Extraction
In translations from German to English, we often
have the case that the English verb is aligned to
both parts of the German verb. Since this phrase
pair is not continuous on the German side, it can-
not be extracted. The phrase could be extracted, if
we also reorder the training corpus.
For the test sentences the POS-based reordering
allows us to change the word order in the source
sentence so that the sentence can be translated
more easily. If we apply this also to the train-
ing sentences, we would be able to extract the
phrase pairs for originally discontinuous phrases
and could apply them during translation of the re-
ordered test sentences.
2http://www.cs.cmu.edu/?qing/
139
Therefore, we build lattices that encode the dif-
ferent reorderings for every training sentence, as
described in Niehues et al (2009). Then we can
not only extract phrase pairs from the monotone
source path, but also from the reordered paths. So
it would be possible to extract the example men-
tioned before, if both parts of the verb were put
together by a reordering rule. To limit the num-
ber of extracted phrase pairs, we extract a source
phrase only once per sentence even if it may be
found on different paths. Furthermore, we do not
use the weights in the lattice.
If we used the same rules as for reordering the
test sets, the lattice would be so big that the num-
ber of extracted phrase pairs would be still too
high. As mentioned before, the word reordering
is mainly a problem at the phrase extraction stage
if one word is aligned to two words which are
far away from each other in the sentence. There-
fore, the short-range reordering rules do not help
much in this case. So, only the long-range reorder-
ing rules were used to generate the lattices for the
training corpus.
4.3 Unaligned Word Feature
Guzman et al (2009) analyzed the role of the word
alignment in the phrase extraction process. To bet-
ter model the relation between word alignment and
the phrase extraction process, they introduced two
new features into the log-linear model. One fea-
ture counts the number of unaligned words on the
source side and the other one does the same for the
target side. Using these additional features they
showed improvements on the Chinese to English
translation task. In order to investigate the impact
on closer related languages like English and Ger-
man, we incorporated those two features into our
systems.
4.4 Bilingual Word language model
Motivated by the improvements in translation
quality that could be achieved by using the n-gram
based approach to statistical machine translation,
for example by Allauzen et al (2009), we tried
to integrate a bilingual language model into our
phrase-based translation system.
To be able to integrate the approach easily into a
standard phrase-based SMT system, a token in the
bilingual language model is defined to consist of
a target word and all source words it is aligned to.
The tokens are ordered according to the target lan-
guage word order. Then the additional tokens can
be introduced into the decoder as an additional tar-
get factor. Consequently, no additional implemen-
tation work is needed to integrate this feature.
If we have the German sentence Ich bin nach
Hause gegangen with the English translation I
went home, the resulting bilingual text would look
like this: I Ich went bin gegangen home Hause.
As shown in the example, one problem with this
approach is that unaligned source words are ig-
nored in the model. One solution could be to have
a second bilingual text ordered according to the
source side. But since the target sentence and not
the source sentence is generated from left to right
during decoding, the integration of a source side
language model is more complex. Therefore, as
a first approach we only used a language model
based on the target word order.
4.5 Bilingual POS language model
The main advantage of POS-based information
is that there are less data sparsity problems and
therefore a longer context can be considered. Con-
sequently, if we want to use this information in the
translation model of a phrase-based SMT system,
the POS-based phrase pairs should be longer than
the word-based ones. But this is not possible in
many decoders or it leads to additional computa-
tion overhead.
If we instead use a bilingual POS-based lan-
guage model, the context length of the language
model is independent from the other models. Con-
sequently, a longer context can be considered for
the POS-based language model than for the word-
based bilingual language model or the phrase
pairs.
Instead of using POS-based information, this
approach can also be applied with other additional
linguistic word-level information like word stems.
5 Results
We submitted translations for English-German
and German-English for the Shared Translation
Task. In the following we present the experiments
we conducted for both translation directions ap-
plying the aforementioned models and extensions
to the baseline systems. The performance of each
individual system configuration was measured ap-
plying the BLEU metric. All BLEU scores are cal-
culated on the lower-cased translation hypotheses.
The individual systems that were used to create the
submission are indicated in bold.
140
5.1 English-German
The baseline system for English-German applies
short-range reordering rules and discriminative
word alignment. The language model is trained
on the News corpus. By expanding the coverage
of the rules to enable long-range reordering, the
score on the test set could be slightly improved.
We then combined the target language part of the
Europarl and News Commentary corpora with the
News corpus to build a bigger language model
which resulted in an increase of 0.11 BLEU points
on the development set and an increase of 0.25
points on the test set. Applying the bilingual lan-
guage model as described above led to 0.04 points
improvement on the test set.
Table 1: Translation results for English-German
(BLEU Score)
System Dev Test
Baseline 15.30 15.40
+ Long-range Reordering 15.25 15.44
+ EPNC LM 15.36 15.69
+ bilingual Word LM 15.37 15.73
+ bilingual POS LM 15.42 15.67
+ unaligned Word Feature 15.65 15.66
+ bilingual Stem LM 15.57 15.74
This system was used to create the submis-
sion to the Shared Translation Task of the WMT
2010. After submission we performed additional
experiments which only led to inconclusive re-
sults. Adding the bilingual POS language model
and introducing the unaligned word feature to the
phrase table only improved on the development
set, while the scores on the test set decreased. A
third bilingual language model based on stem in-
formation again only showed noteworthy effects
on the development set.
5.2 German-English
For the German to English translation system,
the baseline system already uses short-range re-
ordering rules and the discriminative word align-
ment. This system applies only the language
model trained on the News corpus. By adding the
possibility to model long-range reorderings with
POS-based rules, we could improve the system by
0.6 BLEU points. Adding the big language model
using also the English Gigaword corpus we could
improve by 0.3 BLEU points. We got an addi-
tional improvement by 0.1 BLEU points by adding
lattice phrase extraction.
Both the word-based and POS-based bilingual
language model could improve the translation
quality measured in BLEU. Together they im-
proved the system performance by 0.2 BLEU
points.
The best results could be achieved by using also
the unaligned word feature for source and target
words leading to the best performance on the test
set (22.09).
Table 2: Translation results for German-English
(BLEU Score)
System Dev Test
Baseline 20.94 20.83
+ Long-range Reordering 21.52 21.43
+ Gigaword LM 21.90 21.71
+ Lattice Phrase Extraction 21.94 21.81
+ bilingual Word LM 21.94 21.87
+ bilingual POS LM 22.02 22.05
+ unaligned Word Feature 22.09 22.09
6 Conclusions
For our participation in the WMT 2010 we built
translation systems for German to English and En-
glish to German. We addressed to the difficult
word reordering when translating from or to Ger-
man by using POS-based reordering rules during
decoding and by using lattice-based phrase extrac-
tion during training. By applying those methods
we achieved substantially better results for both
translation directions.
Furthermore, we tried to improve the translation
quality by introducing additional features to the
translation model. On the one hand we included
bilingual language models based on different word
factors into the log-linear model. This led to very
slight improvements which differed also with re-
spect to language and data set. We will investigate
in the future whether further improvements are
achievable with this approach. On the other hand
we included the unaligned word feature which has
been applied successfully for other language pairs.
The improvements we could gain with this method
are not as big as the ones reported for other lan-
guages, but still the performance of our systems
could be improved using this feature.
141
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Franc?ois Yvon. 2009. LIMSI?s statistical trans-
lation system for WMT?09. In Fourth Workshop
on Statistical Machine Translation (WMT 2009),
Athens, Greece.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Conference on Empirical
Methods in Natural Language Processing (EMNLP
2006), Sydney, Australia.
Francisco Guzman, Qin Gao, and Stephan Vogel.
2009. Reassessment of the Role of Phrase Extrac-
tion in PBSMT. In MT Summit XII, Ottawa, Ontario,
Canada.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In ACL 2007, Demonstration Session,
Prague, Czech Republic, June 23.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
142
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 379?385,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2011
Teresa Herrmann, Mohammed Mediani, Jan Niehues and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based SMT
systems developed for our participation
in the WMT11 Shared Translation Task.
Translations for English?German and
English?French were generated using a
phrase-based translation system which is
extended by additional models such as
bilingual and fine-grained POS language
models, POS-based reordering, lattice phrase
extraction and discriminative word alignment.
Furthermore, we present a special filtering
method for the English-French Giga corpus
and the phrase scoring step in the training is
parallelized.
1 Introduction
In this paper we describe our systems for the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French. We use a
phrase-based decoder that can use lattices as input
and developed several models that extend the stan-
dard log-linear model combination of phrase-based
MT. These include advanced reordering models and
corresponding adaptations to the phrase extraction
process as well as extension to the translation and
language model in form of discriminative word
alignment and a bilingual language model to ex-
tend source word context. For English-German, lan-
guage models based on fine-grained part-of-speech
tags were used to address the difficult target lan-
guage generation due to the rich morphology of Ger-
man.
We also present a filtering method directly ad-
dressing the problems of web-crawled corpora,
which enabled us to make use of the French-English
Giga corpus. Another novelty in our systems this
year is the parallel phrase scoring method that re-
duces the time needed for training which is espe-
cially convenient for such big corpora as the Giga
corpus.
2 System Description
The baseline systems for all languages use a trans-
lation model that is trained on EPPS and the News
Commentary corpus and the phrase table is based
on a GIZA++ word alignment. The language model
was trained on the monolingual parts of the same
corpora by the SRILM Toolkit (Stolcke, 2002). It
is a 4-gram SRI language model using Kneser-Ney
smoothing.
The problem of word reordering is addressed us-
ing the POS-based reordering model as described
in Section 2.4. The part-of-speech tags for the re-
ordering model are obtained using the TreeTagger
(Schmid, 1994).
An in-house phrase-based decoder (Vogel, 2003)
is used to perform translation and optimization with
regard to the BLEU score is done using Minimum
Error Rate Training as described in Venugopal et al
(2005). During decoding only the top 20 translation
options for every source phrase were considered.
2.1 Data
We trained all systems using the parallel EPPS and
News Commentary corpora. In addition, the UN
corpus and the Giga corpus were used for training
379
the French-English systems.
Optimization was done for most languages using
the news-test2008 data set and news-test2010 was
used as test set. The only exception is German-
English, where news-test2009 was used for opti-
mization due to system combination arrangements.
The language models for the baseline systems were
trained on the monolingual versions of the training
corpora. Later on, we used the News Shuffle and the
Gigaword corpus to train bigger language models.
For training a discriminative word alignment model,
a small amount of hand-aligned data was used.
2.2 Preprocessing
The training data is preprocessed prior to training
the system. This includes normalizing special sym-
bols, smart-casing the first words of each sentence
and removing long sentences and sentences with
length mismatch.
For the German parts of the training corpus
we use the hunspell1 lexicon to map words writ-
ten according to old German spelling to new Ger-
man spelling, to obtain a corpus with homogenous
spelling.
Compound splitting as described in Koehn and
Knight (2003) is applied to the German part of the
corpus for the German-to-English system to reduce
the out-of-vocabulary problem for German com-
pound words.
2.3 Special filtering of the Giga parallel Corpus
The Giga corpus incorporates non-neglegible
amounts of noise even after our usual preprocess-
ing. This noise may be due to different causes.
For instance: non-standard HTML characters,
meaningless parts composed of only hypertext
codes, sentences which are only partial translation
of the source, or eventually not a correct translation
at all.
Such noisy pairs potentially degrade the transla-
tion model quality, therefore it seemed more conve-
nient to eliminate them.
Given the size of the corpus, this task could not be
performed manually. Consequently, we used an au-
tomatic classifier inspired by the work of Munteanu
and Marcu (2005) on comparable corpora. This clas-
1http://hunspell.sourceforge.net/
sifier should be able to filter out the pairs which
likely are not beneficial for the translation model.
In order to reliably decide about the classifier to
use, we evaluated several techniques. The training
and test sets for this evaluation were built respec-
tively from nc-dev2007 and nc-devtest2007. In each
set, about 30% randomly selected source sentences
switch positions with the immediate following so
that they form negative examples. We also used lex-
ical dictionaries in both directions based on EPPS
and UN corpora.
We relied on seven features in our classifiers:
IBM1 score in both directions, number of unaligned
source words, the difference in number of words be-
tween source and target, the maximum source word
fertility, number of unaligned target words, and the
maximum target word fertility. It is noteworthy
that all the features requiring alignment information
(such as the unaligned source words) were computed
on the basis of the Viterbi path of the IBM1 align-
ment. The following classifiers were used:
Regression Choose either class based on a
weighted linear combination of the features
and a fixed threshold of 0.5.
Logistic regression The probability of the class is
expressed as a sigmoid of a linear combination
of the different features. Then the class with
the highest probability is picked.
Maximum entropy classifier We used the same set
of features to train a maximum entropy classi-
fier using the Megam package2.
Support vector machines classifier An SVM clas-
sifier was trained using the SVM-light pack-
age3.
Results of these experiments are summarized in
Table 1.
The regression weights were estimated so that to
minimize the squared error. This gave us a pretty
poor F-measure score of 90.42%. Given that the lo-
gistic regression is more suited for binary classifica-
tion in our case than the normal regression, it led to
significant increase in the performance. The training
2http://www.cs.utah.edu/?hal/megam/
3http://svmlight.joachims.org/
380
Approach Precision Recall F-measure
Regression 93.81 87.27 90.42
LogReg 93.43 94.84 94.13
MaxEnt 93.69 94.54 94.11
SVM 98.20 96.87 97.53
Table 1: Results of the filtering experiments
was held by maximizing the likelihood to the data
with L2 regularization (with ? = 0.1). This gave an
F-measure score of 94.78%.
The maximum entropy classifier performed better
than the logistic regression in terms of precision but
however it had worse F-measure.
Significant improvements could be noticed us-
ing the SVM classifier in both precision and recall:
98.20% precision, 96.87% recall, and thus 97.53%
F-measure.
As a result, we used the SVM classifier to filter
the Giga parallel corpus. The corpus contained orig-
inally around 22.52 million pairs. After preprocess-
ing and filtering it was reduced to 16.7 million pairs.
Thus throwing around 6 million pairs.
2.4 Word Reordering
In contrast to modeling the reordering by a distance-
based reordering model and/or a lexicalized distor-
tion model, we use a different approach that relies
on part-of-speech (POS) sequences. By abstracting
from surface words to parts-of-speech, we expect to
model the reordering more accurately.
2.4.1 POS-based Reordering Model
To model reordering we first learn probabilistic
rules from the POS tags of the words in the train-
ing corpus and the alignment information. Contin-
uous reordering rules are extracted as described in
Rottmann and Vogel (2007) to model short-range re-
orderings. When translating between German and
English, we apply a modified reordering model with
non-continuous rules to cover also long-range re-
orderings (Niehues and Kolss, 2009). The reorder-
ing rules are applied to the source text and the orig-
inal order of words and the reordered sentence vari-
ants generated by the rules are encoded in a word
lattice which is used as input to the decoder.
2.4.2 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract the phrase pairs for orig-
inally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths.
To limit the number of extracted phrase pairs, we
extract a source phrase only once per sentence even
if it is found in different paths.
2.5 Translation and Language Models
In addition to the models used in the baseline sys-
tem described above we conducted experiments in-
cluding additional models that enhance translation
quality by introducing alternative or additional in-
formation into the translation or language modelling
process.
2.5.1 Discriminative Word Alignment
In most of our systems we use the PGIZA++
Toolkit4 to generate alignments between words in
the training corpora. The word alignments are gen-
erated in both directions and the grow-diag-final-and
heuristic is used to combine them. The phrase ex-
traction is then done based on this word alignment.
In the English-German system we applied the
Discriminative Word Alignment approach as de-
scribed in Niehues and Vogel (2008) instead. This
alignment model is trained on a small corpus of
hand-aligned data and uses the lexical probability
as well as the fertilities generated by the PGIZA++
Toolkit and POS information.
2.5.2 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder according to the best com-
bination of phrases that maximize the translation
and language model scores. This segmentation into
phrases leads to the loss of context information at
the phrase boundaries. Although more target side
context is available to the language model, source
4http://www.cs.cmu.edu/?qing/
381
side context would also be valuable for the decoder
when searching for the best translation hypothesis.
To make also source language context available we
use a bilingual language model, an additional lan-
guage model in the phrase-based system in which
each token consist of a target word and all source
words it is aligned to. The bilingual tokens enter
the translation process as an additional target factor
and the bilingual language model is applied to the
additional factor like a normal language model. For
more details see (Niehues et al, 2011).
2.5.3 Parallel phrase scoring
The process of phrase scoring is held in two runs.
The objective of the first run is to compute the nec-
essary counts and to estimate the scores, all based
on the source phrases; while the second run is sim-
ilarly held based on the target phrases. Thus, the
extracted phrases have to be sorted twice: once by
source phrase and once by target phrase. These two
sorting operations are almost always done on an ex-
ternal storage device and hence consume most of the
time spent in this step.
The phrase scoring step was reimplemented in or-
der to exploit the available computation resources
more efficiently and therefore reduce the process-
ing time. It uses optimized sorting algorithms for
large data volumes which cannot fit into memory
(Vitter, 2008). In its core, our implementation re-
lies on STXXL: an extension of the STL library for
external memory (Kettner, 2005) and on OpenMP
for shared memory parallelization (Chapman et al,
2007).
Table 2 shows a comparison between Moses and
our phrase scoring tools. The comparison was held
using sixteen-core 64-bit machines with 128 Gb
RAM, where the files are accessed through NFS on
a RAID disk. The experiments show that the gain
grows linearly with the size of input with an average
of 40% of speed up.
2.5.4 POS Language Models
In addition to surface word language models, we
did experiments with language models based on
part-of-speech for English-German. We expect that
having additional information in form of probabil-
ities of part-of-speech sequences should help espe-
cially in case of the rich morphology of German and
#pairs(G) Moses ?103(s) KIT ?103(s)
0.203 25.99 17.58
1.444 184.19 103.41
1.693 230.97 132.79
Table 2: Comparison of Moses and KIT phrase extraction
systems
therefore the more difficult target language genera-
tion.
The part-of-speeches were generated using the
TreeTagger and the RFTagger (Schmid and Laws,
2008), which produces more fine-grained tags that
include also person, gender and case information.
While the TreeTagger assigns 54 different POS tags
to the 357K German words in the corpus, the RF-
Tagger produces 756 different fine-grained tags on
the same corpus.
We tried n-gram lengths of 4 and 7. While no im-
provement in translation quality could be achieved
using the POS language models based on the normal
POS tags, the 4-gram POS language model based
on fine-grained tags could improve the translation
system by 0.2 BLEU points as shown in Table 3.
Surprisingly, increasing the n-gram length to 7 de-
creased the translation quality again.
To investigate the impact of context length, we
performed an analysis on the outputs of two different
systems, one without a POS language model and one
with the 4-gram fine-grained POS language model.
For each of the translations we calculated the aver-
age length of the n-grams in the translation when
applying one of the two language models using 4-
grams of surface words or parts-of-speech. The re-
sults are also shown in Table 3.
The average n-gram length of surface words on
the translation generated by the system without POS
language model and the one using the 4-gram POS
language model stays practically the same. When
measuring the n-gram length using the 4-gram POS
language model, the context increases to 3.4. This
increase of context is not surprising, since with
the more general POS tags longer contexts can be
matched. Comparing the POS context length for
the two translations, we can see that the context in-
creases from 3.18 to 3.40 due to longer matching
POS sequences. This means that the system using
382
the POS language model actually generates trans-
lations with more probable POS sequences so that
longer matches are possible. Also the perplexity
drops by half since the POS language model helps
constructing sentences that have a better structure.
System BLEU avg. ngram length PPL
Word POS POS
no POS LM 16.64 2.77 3.18 66.78
POS LM 16.88 2.81 3.40 33.36
Table 3: Analysis of context length
3 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to the
workshop. The following sections describe the ex-
periments for the individual language pairs and show
the translation results. The results are reported as
case-sensitive BLEU scores (Papineni et al, 2002)
on one reference translation.
3.1 German-English
The German-to-English baseline system applies
short-range reordering rules and uses a language
model trained on the EPPS and News Commen-
tary. By exchanging the baseline language model
by one trained on the News Shuffle corpus we im-
prove the translation quality considerably, by more
than 3 BLEU points. When we expand the cov-
erage of the reordering rules to enable long-range
reordering we can improve even further by 0.4 and
adding a second language model trained on the En-
glish Gigaword corpus we gain another 0.3 BLEU
points. To ensure that the phrase table also includes
reordered phrases, we use lattice phrase extraction
and can achieve a small improvement. Finally, a
bilingual language model is added to extend the con-
text of source language words available for transla-
tion, reaching the best score of 23.35 BLEU points.
This system was used for generating the translation
submitted to the German-English Translation Task.
3.2 English-German
The English-to-German baseline system also in-
cludes short-range reordering and uses translation
System Dev Test
Baseline 18.49 19.10
+ NewsShuffle LM 20.63 22.24
+ LongRange Reordering 21.00 22.68
+ Additional Giga LM 21.80 22.92
+ Lattice Phrase Extraction 21.87 22.96
+ Bilingual LM 22.05 23.35
Table 4: Translation results for German-English
and language model based on EPPS and News Com-
mentary. Exchanging the language model by the
News Shuffle language model again yields a big im-
provement by 2.3 BLEU points. Adding long-range
reordering improves a lot on the development set
while the score on the test set remains practically
the same. Replacing the GIZA++ alignments by
alignments generated using the Discriminative Word
Alignment Model again only leads to a small im-
provement. By using the bilingual language model
to increase context we can gain 0.1 BLEU points
and by adding the part-of-speech language model
with rich parts-of-speech including case, number
and gender information for German we achieve the
best score of 16.88. This system was used to gener-
ate the translation used for submission.
System Dev Test
Baseline 13.55 14.19
+ NewsShuffle LM 15.10 16.46
+ LongRange Reordering 15.79 16.46
+ DWA 15.81 16.52
+ Bilingual LM 15.85 16.64
+ POS LM 15.88 16.88
Table 5: Translation results for English-German
3.3 English-French
Table 6 summarizes how our system for English-
French evolved. The baseline system for this direc-
tion was trained on the EPPS and News Commen-
tary corpora, while the language model was trained
on the French part of the EPPS, News Commen-
tary and UN parallel corpora. Some improvement
could be already seen by introducing the short-range
reorderings trained on the baseline parallel corpus.
383
Apparently, the UN data brought only slight im-
provement to the overall performance. On the other
hand, adding bigger language models trained on the
monolingual French version of EPPS, News Com-
mentary and the News Shuffle together with the
French Gigaword corpus introduces an improvement
of 3.7 on test. Using a system trained only on the
Giga corpus data with the same last configuration
shows a significant gain. It showed an improvement
of around 1.0. We were able to obtain some further
improvements by merging the translation models of
the last two systems. i.e. the one system based on
EPPS, UN, and News Commentary and the other on
the Giga corpus. This merging increased our score
by 0.2. Finally, our submitted system for this direc-
tion was obtained by using a single language model
trained on the union of all the French corpora in-
stead of using multiple models. This resulted in an
improvement of 0.1 leading to our best score: 28.28.
System Dev Test
Baseline 20.62 22.36
+ Reordering 21.29 23.11
+ UN 21.27 23.24
+ Big LMs 23.77 26.90
Giga data 24.53 27.94
Merge 24.74 28.14
+ Merged LMs 25.07 28.28
Table 6: Translation results for English-French
3.4 French-English
The development of our system for the French-
English direction is summarized in Table 7. Our sys-
tem for this direction evolved quite similarly to the
opposite direction. The largest improvement accom-
panied the integration of the bigger language mod-
els (trained on the English version of EPPS, News
Commentary, News Shuffle and the Gigaword cor-
pus): 3.3 BLEU points, whereas smaller improve-
ments could be gained by applying the short reorder-
ing rules and almost no change by including the UN
data. Further gains were obtained by training the
system on the Giga corpus added to the previous
parallel data. This increased our performance by
0.6. The submitted system was obtained by aug-
menting the last system with a bilingual language
model adding around 0.2 to the previous score and
thus giving 28.34 as final score.
System Dev Test
Baseline 20.76 23.78
+ Reordering 21.42 24.28
+ UN 21.55 24.21
+ Big LMs 24.16 27.55
+ Giga data 24.86 28.17
+ BiLM 25.01 28.34
Table 7: Translation results for French-English
4 Conclusions
We have presented the systems for our participation
in the WMT 2011 Evaluation for English?German
and English?French. For English?French, a spe-
cial filtering method for web-crawled data was de-
veloped. In addition, a parallel phrase scoring tech-
nique was implemented that could speed up the MT
training process tremendously. Using these two fea-
tures, we were able to integrate the huge amounts of
data available in the Giga corpus into our systems
translating between English and French.
We applied POS-based reordering to improve our
translations in all directions, using short-range re-
ordering for English?French and long-range re-
ordering for English?German. For German-
English, reordering also the training corpus lead to
further improvements of the translation quality.
A Discriminative Word Alignment Model led to
an increase in BLEU for English-German. For this
direction we also tried fine-grained POS language
models of different n-gram lengths. The best trans-
lations could be obtained by using 4-grams.
For nearly all experiments, a bilingual language
model was applied that expands the context of
source words that can be considered during decod-
ing. The improvements range from 0.1 to 0.4 in
BLEU score.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
384
References
Barbara Chapman, Gabriele Jost, and Ruud van der Pas.
2007. Using OpenMP: Portable Shared Memory Par-
allel Programming (Scientific and Engineering Com-
putation). The MIT Press.
Roman Dementiev Lutz Kettner. 2005. Stxxl: Stan-
dard template library for xxl data sets. In Proceedings
of ESA 2005. Volume 3669 of LNCS, pages 640?651.
Springer.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In EACL, Budapest,
Hungary.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation of
Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. In COL-
ING 2008, Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Beyond
(WPT-05), Ann Arbor, MI.
Jeffrey Scott Vitter. 2008. Algorithms and Data Struc-
tures for External Memory. now Publishers Inc.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
385
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349?355,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2012
Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa Herrmann, Eunah Cho and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based SMT
systems developed for our participation
in the WMT12 Shared Translation Task.
Translations for English?German and
English?French were generated using a
phrase-based translation system which is
extended by additional models such as
bilingual, fine-grained part-of-speech (POS)
and automatic cluster language models and
discriminative word lexica. In addition, we
explicitly handle out-of-vocabulary (OOV)
words in German, if we have translations for
other morphological forms of the same stem.
Furthermore, we extended the POS-based
reordering approach to also use information
from syntactic trees.
1 Introduction
In this paper, we describe our systems for the
NAACL 2012 Seventh Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French. We use a
phrase-based decoder that can use lattices as input
and developed several models that extend the stan-
dard log-linear model combination of phrase-based
MT. In addition to the POS-based reordering model
used in past years, for German-English we extended
it to also use rules learned using syntax trees.
The translation model was extended by the bilin-
gual language model and a discriminative word lex-
icon using a maximum entropy classifier. For the
French-English and English-French translation sys-
tems, we also used phrase table adaptation to avoid
overestimation of the probabilities of the huge, but
noisy Giga corpus. In the German-English system,
we tried to learn translations for OOV words by ex-
ploring different morphological forms of the OOVs
with the same lemma.
Furthermore, we combined different language
models in the log-linear model. We used word-
based language models trained on different parts of
the training corpus as well as POS-based language
models using fine-grained POS information and lan-
guage models trained on automatic word clusters.
The paper is organized as follows: The next sec-
tion gives a detailed description of our systems in-
cluding all the models. The translation results for
all directions are presented afterwards and we close
with a conclusion.
2 System Description
For the French?English systems the phrase table
is based on a GIZA++ word alignment, while the
systems for German?English use a discriminative
word alignment as described in Niehues and Vogel
(2008). The language models are 4-gram SRI lan-
guage models using Kneser-Ney smoothing trained
by the SRILM Toolkit (Stolcke, 2002).
The problem of word reordering is addressed with
POS-based and tree-based reordering models as de-
scribed in Section 2.3. The POS tags used in the
reordering model are obtained using the TreeTagger
(Schmid, 1994). The syntactic parse trees are gen-
erated using the Stanford Parser (Rafferty and Man-
ning, 2008).
An in-house phrase-based decoder (Vogel, 2003)
is used to perform translation. Optimization with
349
regard to the BLEU score is done using Minimum
Error Rate Training as described in Venugopal et al
(2005). During decoding only the top 10 translation
options for every source phrase are considered.
2.1 Data
Our translation models were trained on the EPPS
and News Commentary (NC) corpora. Furthermore,
the additional available data for French and English
(i.e. UN and Giga corpora) were exploited in the
corresponding systems.
The systems were tuned with the news-test2011
data, while news-test2011 was used for testing in all
our systems. We trained language models for each
language on the monolingual part of the training cor-
pora as well as the News Shuffle and the Gigaword
(version 4) corpora. The discriminative word align-
ment model was trained on 500 hand-aligned sen-
tences selected from the EPPS corpus.
2.2 Preprocessing
The training data is preprocessed prior to training
the system. This includes normalizing special sym-
bols, smart-casing the first word of each sentence
and removing long sentences and sentences with
length mismatch.
For the German parts of the training corpus, in
order to obtain a homogenous spelling, we use the
hunspell1 lexicon to map words written according to
old German spelling rules to new German spelling
rules.
In order to reduce the OOV problem of German
compound words, Compound splitting as described
in Koehn and Knight (2003) is applied to the Ger-
man part of the corpus for the German-to-English
system.
The Giga corpus received a special preprocessing
by removing noisy pairs using an SVM classifier as
described in Mediani et al (2011). The SVM clas-
sifier training and test sets consist of randomly se-
lected sentence pairs from the corpora of EPPS, NC,
tuning, and test sets. Giving at the end around 16
million sentence pairs.
2.3 Word Reordering
In contrast to modeling the reordering by a distance-
based reordering model and/or a lexicalized distor-
1http://hunspell.sourceforge.net/
tion model, we use a different approach that relies on
POS sequences. By abstracting from surface words
to POS, we expect to model the reordering more ac-
curately. For German-to-English, we additionally
apply reordering rules learned from syntactic parse
trees.
2.3.1 POS-based Reordering Model
In order to build the POS-based reordering model,
we first learn probabilistic rules from the POS tags
of the training corpus and the alignment. Contin-
uous reordering rules are extracted as described in
Rottmann and Vogel (2007) to model short-range re-
orderings. When translating between German and
English, we apply a modified reordering model with
non-continuous rules to cover also long-range re-
orderings (Niehues and Kolss, 2009).
2.3.2 Tree-based Reordering Model
Word order is quite different between German and
English. And during translation especially verbs or
verb particles need to be shifted over a long dis-
tance in a sentence. Using discontinuous POS rules
already improves the translation tremendously. In
addition, we apply a tree-based reordering model
for the German-English translation. Syntactic parse
trees provide information about the words in a sen-
tence that form constituents and should therefore be
treated as inseparable units by the reordering model.
For the tree-based reordering model, syntactic parse
trees are generated for the whole training corpus.
Then the word alignment between the source and
target language part of the corpus is used to learn
rules on how to reorder the constituents in a Ger-
man source sentence to make it matches the English
target sentence word order better. In order to apply
the rules to the source text, POS tags and a parse
tree are generated for each sentence. Then the POS-
based and tree-based reordering rules are applied.
The original order of words as well as the reordered
sentence variants generated by the rules are encoded
in a word lattice. The lattice is then used as input to
the decoder.
For the test sentences, the reordering based on
POS and trees allows us to change the word order
in the source sentence so that the sentence can be
translated more easily. In addition, we build reorder-
ing lattices for all training sentences and then extract
350
phrase pairs from the monotone source path as well
as from the reordered paths.
2.4 Translation Models
In addition to the models used in the baseline system
described above, we conducted experiments includ-
ing additional models that enhance translation qual-
ity by introducing alternative or additional informa-
tion into the translation modeling process.
2.4.1 Phrase table adaptation
Since the Giga corpus is huge, but noisy, it is
advantageous to also use the translation probabil-
ities of the phrase pair extracted only from the
more reliable EPPS and News commentary cor-
pus. Therefore, we build two phrase tables for the
French?English system. One trained on all data
and the other only trained on the EPPS and News
commentary corpus. The two models are then com-
bined using a log-linear combination to achieve the
adaptation towards the cleaner corpora as described
in (Niehues et al, 2010). The newly created trans-
lation model uses the four scores from the general
model as well as the two smoothed relative frequen-
cies of both directions from the smaller, but cleaner
model. If a phrase pair does not occur in the in-
domain part, a default score is used instead of a rela-
tive frequency. In our case, we used the lowest prob-
ability.
2.4.2 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder according to the best com-
bination of phrases that maximize the translation
and language model scores. This segmentation into
phrases leads to the loss of context information at
the phrase boundaries. Although more target side
context is available to the language model, source
side context would also be valuable for the decoder
when searching for the best translation hypothesis.
To make also source language context available we
use a bilingual language model, in which each token
consists of a target word and all source words it is
aligned to. The bilingual tokens enter the translation
process as an additional target factor and the bilin-
gual language model is applied to the additional fac-
tor like a normal language model. For more details
see Niehues et al (2011).
2.4.3 Discriminative Word Lexica
Mauser et al (2009) have shown that the use
of discriminative word lexica (DWL) can improve
the translation quality. For every target word, they
trained a maximum entropy model to determine
whether this target word should be in the translated
sentence or not using one feature per one source
word.
When applying DWL in our experiments, we
would like to have the same conditions for the train-
ing and test case. For this we would need to change
the score of the feature only if a new word is added
to the hypothesis. If a word is added the second time,
we do not want to change the feature value. In order
to keep track of this, additional bookkeeping would
be required. Also the other models in our translation
system will prevent us from using a word too often.
Therefore, we ignore this problem and can calcu-
late the score for every phrase pair before starting
with the translation. This leads to the following def-
inition of the model:
p(e|f) =
J?
j=1
p(ej |f) (1)
In this definition, p(ej |f) is calculated using a max-
imum likelihood classifier.
Each classifier is trained independently on the
parallel training data. All sentences pairs where the
target word e occurs in the target sentence are used
as positive examples. We could now use all other
sentences as negative examples. But in many of
these sentences, we would anyway not generate the
target word, since there is no phrase pair that trans-
lates any of the source words into the target word.
Therefore, we build a target vocabulary for every
training sentence. This vocabulary consists of all
target side words of phrase pairs matching a source
phrase in the source part of the training sentence.
Then we use all sentence pairs where e is in the tar-
get vocabulary but not in the target sentences as neg-
ative examples. This has shown to have a postive
influence on the translation quality (Mediani et al,
2011) and also reduces training time.
2.4.4 Quasi-Morphological Operations for
OOV words
Since German is a highly inflected language, there
will be always some word forms of a given Ger-
351
Figure 1: Quasi-morphological operations
man lemma that did not occur in the training data.
In order to be able to also translate unseen word
forms, we try to learn quasi-morphological opera-
tions that change the lexical entry of a known word
form to the unknown word form. These have shown
to be beneficial in Niehues and Waibel (2011) using
Wikipedia2 titles. The idea is illustrated in Figure 1.
If we look at the data, our system is able to trans-
late a German word Kamin (engl. chimney), but not
the dative plural form Kaminen. To address this
problem, we try to automatically learn rules how
words can be modified. If we look at the example,
we would like the system to learn the following rule.
If an ?en? is appended to a German word, as it is
done when creating the dative plural form of Kami-
nen, we need to add an ?s? to the end of the English
word in order to perform the same morphological
word transformation. We use only rules where the
ending of the word has at most 3 letters.
Depending on the POS, number, gender or case of
the involved words, the same operation on the source
side does not necessarily correspond to the same op-
eration on the target side.
To account for this ambiguity, we rank the differ-
ent target operation using the following four features
and use the best ranked one. Firstly, we should not
generate target words that do not exist. Here, we
have an advantage that we can use monolingual data
to determine whether the word exists. In addition,
a target operation that often coincides with a given
source operation should be better than one that is
rarely used together with the source operation. We
therefore look at pairs of entries in the lexicon and
count in how many of them the source operation can
be applied to the source side and the target operation
can be applied to the target side. We then use only
operations that occur at least ten times. Furthermore,
2http://www.wikipedia.org/
we use the ending of the source and target word to
determine which pair of operations should be used.
Integration We only use the proposed method for
OOVs and do not try to improve translations of
words that the baseline system already covers. We
look for phrase pairs, for which a source operation
ops exists that changes one of the source words f1
into the OOV word f2. Since we need to apply a
target operation to one word on the target side of the
phrase pair, we only consider phrase pairs where f1
is aligned to one of the target words of the phrase
containing e1. If a target operation exists given f1
and ops, we select the one with the highest rank.
Then we generate a new phrase pair by applying
ops to f1 and opt to e1 keeping the original scores
from the phrase pairs, since the original and syn-
thesized phrase pair are not directly competing any-
way. We do not add several phrase pairs generated
by different operations, since we would then need to
add the features used for ranking the operations into
the MERT. This is problematic, since the operations
were only used for very few words and therefore a
good estimation of the weights is not possible.
2.5 Language Models
The 4-gram language models generated by the
SRILM toolkit are used as the main language mod-
els for all of our systems. For English-French and
French-English systems, we use a good quality cor-
pus as in-domain data to train in-domain language
models. Additionally, we apply the POS and clus-
ter language models in different systems. All lan-
guage models are integrated into the translation sys-
tem by a log-linear combination and received opti-
mal weights during tuning by the MERT.
2.5.1 POS Language Models
The POS language model is trained on the POS
sequences of the target language. In this evalua-
tion, the POS language model is applied for the
English-German system. We expect that having ad-
ditional information in form of probabilities of POS
sequences should help especially in case of the rich
morphology of German. The POS tags are gener-
ated with the RFTagger (Schmid and Laws, 2008)
for German, which produces fine-grained tags that
include person, gender and case information. We
352
use a 9-gram language model on the News Shuf-
fle corpus and the German side of all parallel cor-
pora. More details and discussions about the POS
language model can be found in Herrmann et al
(2011).
2.5.2 Cluster Language Models
The cluster language model follows a similar idea
as the POS language model. Since there is a data
sparsity problem when we substitute words with the
word classes, it is possible to make use of larger
context information. In the POS language model,
POS tags are the word classes. Here, we generated
word classes in a different way. First, we cluster
the words in the corpus using the MKCLS algorithm
(Och, 1999) given a number of classes. Second, we
replace the words in the corpus by their cluster IDs.
Finally, we train an n-gram language model on this
corpus consisting of cluster IDs. Generally, all clus-
ter language models used in our systems are 5-gram.
3 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to the
workshop. The following sections describe the ex-
periments for the individual language pairs and show
the translation results. The results are reported as
case-sensitive BLEU scores (Papineni et al, 2002)
on one reference translation.
3.1 German-English
The experiments for the German-English translation
system are summarized in Table 1. The Baseline
system uses POS-based reordering, discriminative
word alignment and a language model trained on the
News Shuffle corpus. By adding lattice phrase ex-
traction small improvements of the translation qual-
ity could be gained.
Further improvements could be gained by adding
a language model trained on the Gigaword corpus
and adding a bilingual and cluster-based language
model. We used 50 word classes and trained a 5-
gram language model. Afterwards, the translation
quality was improved by also using a discriminative
word lexicon. Finally, the best system was achieved
by using Tree-based reordering and using special
treatment for the OOVs. This system generates a
BLEU score of 22.31 on the test data. For the last
two systems, we did not perform new optimization
runs.
System Dev Test
Baseline 23.64 21.32
+ Lattice Phrase Extraction 23.76 21.36
+ Gigaward Language Model 24.01 21.73
+ Bilingual LM 24.19 21.91
+ Cluster LM 24.16 22.09
+ DWL 24.19 22.19
+ Tree-based Reordering - 22.26
+ OOV - 22.31
Table 1: Translation results for German-English
3.2 English-German
The English-German baseline system uses also
POS-based reordering, discriminative word align-
ment and a language model based on EPPS, NC and
News Shuffle. A small gain could be achieved by the
POS-based language model and the bilingual lan-
guage model. Further gain was achieved by using
also a cluster-based language model. For this lan-
guage model, we use 100 word classes and trained
a 5-gram language model. Finally, the best system
uses the discriminative word lexicon.
System Dev Test
Baseline 17.06 15.57
+ POSLM 17.27 15.63
+ Bilingual LM 17.40 15.78
+ Cluster LM 17.77 16.06
+ DWL 17.75 16.28
Table 2: Translation results for English-German
3.3 English-French
Table 3 summarizes how our English-French sys-
tem evolved. The baseline system here was trained
on the EPPS, NC, and UN corpora, while the lan-
guage model was trained on all the French part of
the parallel corpora (including the Giga corpus). It
also uses short-range reordering trained on EPPS
and NC. This system had a BLEU score of around
26.7. The Giga parallel data turned out to be quite
353
beneficial for this task. It improves the scores by
more than 1 BLEU point. More importantly, addi-
tional language models boosted the system quality:
around 1.8 points. In fact, three language models
were log-linearly combined: In addition to the afore-
mentioned, two additional language models were
trained on the monolingual sets (one for News and
one for Gigaword). We could get an improvement
of around 0.2 by retraining the reordering rules on
EPPS and NC only, but using Giza alignment from
the whole data. Adapting the translation model by
using EPPS and NC as in-domain data improves the
BLEU score by only 0.1. This small improvement
might be due to the fact that the news domain is
very broad and that the Giga corpus has already been
carefully cleaned and filtered. Furthermore, using a
bilingual language model enhances the BLEU score
by almost 0.3. Finally, incorporating a cluster lan-
guage model adds an additional 0.1 to the score.
This leads to a system with 30.58.
System Dev Test
Baseline 24.96 26.67
+ GigParData 26.12 28.16
+ Big LMs 29.22 29.92
+ All Reo 29.14 30.10
+ PT Adaptation 29.15 30.22
+ Bilingual LM 29.17 30.49
+ Cluster LM 29.08 30.58
Table 3: Translation results for English-French
3.4 French-English
The development of our system for the French-
English direction is summarized in Table 4. The
baseline system for this direction was trained on the
EPPS, NC, UN and Giga parallel corpora, while the
language model was trained on the French part of the
parallel training corpora. The baseline system in-
cludes the POS-based reordering model with short-
range rules. The largest improvement of 1.7 BLEU
score was achieved by the integration of the bigger
language models which are trained on the English
version of News Shuffle and the Gigaword corpus
(v4). We did not add the language models from the
monolingual English version of EPPS and NC data,
since the experiments have shown that they did not
provide improvement in our system. The second
largest improvement came from the domain adap-
tation that includes an in-domain language model
and adaptations to the phrase extraction. The BLEU
score has improved about 1 BLEU in total. The in-
domain data we used here are parallel EPPS and NC
corpus. Further gains were obtained by augmenting
the system with a bilingual language model adding
around 0.2 BLEU to the previous score. The sub-
mitted system was obtained by adding the cluster
5-gram language model trained on the News Shuf-
fle corpus with 100 clusters and thus giving 30.25 as
the final score.
System Dev Test
Baseline 25.81 27.15
+ Indomain LM 26.17 27.91
+ PT Adaptation 26.33 28.11
+ Big LMs 28.90 29.82
+ Bilingual LM 29.14 30.09
+ Cluster LM 29.31 30.25
Table 4: Translation results for French-English
4 Conclusions
We have presented the systems for our participation
in the WMT 2012 Evaluation for English?German
and English?French. In all systems we could im-
prove by using a class-based language model. Fur-
thermore, the translation quality could be improved
by using a discriminative word lexicon. Therefore,
we trained a maximum entropy classifier for ev-
ery target word. For English?French, adapting the
phrase table helps to avoid using wrong parts of the
noisy Giga corpus. For the German-to-English sys-
tem, we could improve the translation quality addi-
tionally by using a tree-based reordering model and
by special handling of OOV words. For the inverse
direction we could improve the translation quality
by using a 9-gram language model trained on the
fine-grained POS tags.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
354
References
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The karlsruhe institute of
technology translation systems for the wmt 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 379?385, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In EACL, Budapest,
Hungary.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The kit english-
french translation systems for iwslt 2011. In Proceed-
ings of the eight International Workshop on Spoken
Language Translation (IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using wikipedia to
translate domain-specific terms in smt. In Proceedings
of the eight International Workshop on Spoken Lan-
guage Translation (IWSLT).
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Marcello Federico, Ian Lane, Michael Paul, and
Franc?ois Yvon, editors, Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 93?98.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Associa-
tion for Computational Linguistics, EACL ?99, pages
71?76, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three german treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation of
Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. In COL-
ING 2008, Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Beyond
(WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
355
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104?108,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2013
Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan Niehues,
Teresa Herrmann, Isabel Slawik and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based
SMT systems developed for our partici-
pation in the WMT13 Shared Translation
Task. Translations for English?German
and English?French were generated us-
ing a phrase-based translation system
which is extended by additional models
such as bilingual, fine-grained part-of-
speech (POS) and automatic cluster lan-
guage models and discriminative word
lexica (DWL). In addition, we combined
reordering models on different sentence
abstraction levels.
1 Introduction
In this paper, we describe our systems for the
ACL 2013 Eighth Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French using a
phrase-based decoder with lattice input.
The paper is organized as follows: the next sec-
tion gives a detailed description of our systems
including all the models. The translation results
for all directions are presented afterwards and we
close with a conclusion.
2 System Description
The phrase table is based on a GIZA++ word
alignment for the French?English systems. For
the German?English systems we use a Discrim-
inative Word Alignment (DWA) as described in
Niehues and Vogel (2008). For every source
phrase only the top 10 translation options are con-
sidered during decoding. The SRILM Toolkit
(Stolcke, 2002) is used for training SRI language
models using Kneser-Ney smoothing.
For the word reordering between languages, we
used POS-based reordering models as described in
Section 4. In addition to it, tree-based reordering
model and lexicalized reordering were added for
German?English systems.
An in-house phrase-based decoder (Vogel,
2003) is used to perform translation. The trans-
lation was optimized using Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) towards better BLEU (Papineni et al,
2002) scores.
2.1 Data
The Europarl corpus (EPPS) and News Commen-
tary (NC) corpus were used for training our trans-
lation models. We trained language models for
each language on the monolingual part of the
training corpora as well as the News Shuffle and
the Gigaword corpora. The additional data such as
web-crawled corpus, UN and Giga corpora were
used after filtering. The filtering work for this data
is discussed in Section 3.
For the German?English systems we use the
news-test2010 set for tuning, while the news-
test2011 set is used for the French?English sys-
tems. For testing, news-test2012 set was used for
all systems.
2.2 Preprocessing
The training data is preprocessed prior to train-
ing the system. This includes normalizing special
symbols, smart-casing the first word of each sen-
tence and removing long sentences and sentence
pairs with length mismatch.
Compound splitting is applied to the German
part of the corpus of the German?English system
as described in Koehn and Knight (2003).
3 Filtering of Noisy Pairs
The filtering was applied on the corpora which
are found to be noisy. Namely, the Giga English-
French parallel corpus and the all the new web-
crawled data . The operation was performed using
104
an SVM classifier as in our past systems (Medi-
ani et al, 2011). For each pair, the required lexica
were extracted from Giza alignment of the corre-
sponding EPPS and NC corpora. Furthermore, for
the web-crawled data, higher precision classifiers
were trained by providing a larger number of neg-
ative examples to the classifier.
After filtering, we could still find English sen-
tences in the other part of the corpus. Therefore,
we performed a language identification (LID)-
based filtering afterwards (performed only on the
French-English corpora, in this participation).
4 Word Reordering
Word reordering was modeled based on POS se-
quences. For the German?English system, re-
ordering rules learned from syntactic parse trees
were used in addition.
4.1 POS-based Reordering Model
In order to train the POS-based reordering model,
probabilistic rules were learned based on the POS
tags from the TreeTagger (Schmid and Laws,
2008) of the training corpus and the alignment. As
described in Rottmann and Vogel (2007), continu-
ous reordering rules are extracted. This modeling
of short-range reorderings was extended so that it
can cover also long-range reorderings with non-
continuous rules (Niehues and Kolss, 2009), for
German?English systems.
4.2 Tree-based Reordering Model
In addition to the POS-based reordering, we
apply a tree-based reordering model for the
German?English translation to better address the
differences in word order between German and
English. We use the Stanford Parser (Rafferty and
Manning, 2008) to generate syntactic parse trees
for the source side of the training corpus. Then
we use the word alignment between source and
target language to learn rules on how to reorder
the constituents in a German source sentence to
make it match the English target sentence word or-
der better (Herrmann et al, 2013). The POS-based
and tree-based reordering rules are applied to each
input sentence. The resulting reordered sentence
variants as well as the original sentence order are
encoded in a word lattice. The lattice is then used
as input to the decoder.
4.3 Lexicalized Reordering
The lexicalized reordering model stores the re-
ordering probabilities for each phrase pair. Pos-
sible reordering orientations at the incoming and
outgoing phrase boundaries are monotone, swap
or discontinuous. With the POS- and tree-based
reordering word lattices encode different reorder-
ing variants. In order to apply the lexicalized re-
ordering model, we store the original position of
each word in the lattice. At each phrase boundary
at the end, the reordering orientation with respect
to the original position of the words is checked.
The probability for the respective orientation is in-
cluded as an additional score.
5 Translation Models
In addition to the models used in the baseline sys-
tem described above, we conducted experiments
including additional models that enhance trans-
lation quality by introducing alternative or addi-
tional information into the translation modeling
process.
5.1 Bilingual Language Model
During the decoding the source sentence is seg-
mented so that the best combination of phrases
which maximizes the scores is available. How-
ever, this causes some loss of context information
at the phrase boundaries. In order to make bilin-
gual context available, we use a bilingual language
model (Niehues et al, 2011). In the bilingual lan-
guage model, each token consists of a target word
and all source words it is aligned to.
5.2 Discriminative Word Lexicon
Mauser et al (2009) introduced the Discriminative
Word Lexicon (DWL) into phrase-based machine
translation. In this approach, a maximum entropy
model is used to determine the probability of using
a target word in the translation.
In this evaluation, we used two extensions to
this work as shown in (Niehues and Waibel, 2013).
First, we added additional features to model the
order of the source words better. Instead of rep-
resenting the source sentence as a bag-of-words,
we used a bag-of-n-grams. We used n-grams up to
the order of three and applied count filtering to the
features for higher order n-grams.
Furthermore, we created the training examples
differently in order to focus on addressing errors
of the other models of the phrase-based translation
105
system. We first translated the whole corpus with a
baseline system. Then we only used the words that
occur in the N-Best List and not in the reference as
negative examples instead of using all words that
do not occur in the reference.
5.3 Quasi-Morphological Operations
Because of the inflected characteristic of the
German language, we try to learn quasi-
morphological operations that change the lexi-
cal entry of a known word form to the out-of-
vocabulary (OOV) word form as described in
Niehues and Waibel (2012).
5.4 Phrase Table Adaptation
For the French?English systems, we built two
phrase tables; one trained with all data and the
other trained only with the EPPS and NC cor-
pora. This is due to the fact that Giga corpus is big
but noisy and EPPS and NC corpus are more reli-
able. The two models are combined log-linearly to
achieve the adaptation towards the cleaner corpora
as described in Niehues et al (2010).
6 Language Models
The 4-gram language models generated by the
SRILM toolkit are used as the main language
models for all of our systems. For the
English?French systems, we use a good quality
corpus as in-domain data to train in-domain lan-
guage models. Additionally, we apply the POS
and cluster language models in different systems.
For the German?English system, we build sepa-
rate language models using each corpus and com-
bine them linearly before the decoding by mini-
mizing the perplexity. Language models are inte-
grated into the translation system by a log-linear
combination and receive optimal weights during
tuning by the MERT.
6.1 POS Language Models
For the English?German system, we use the POS
language model, which is trained on the POS se-
quence of the target language. The POS tags are
generated using the RFTagger (Schmid and Laws,
2008) for German. The RFTagger generates fine-
grained tags which include person, gender, and
case information. The language model is trained
with up to 9-gram information, using the German
side of the parallel EPPS and NC corpus, as well
as the News Shuffle corpus.
6.2 Cluster Language Models
In order to use larger context information, we use
a cluster language model for all our systems. The
cluster language model is based on the idea shown
in Och (1999). Using the MKCLS algorithm, we
cluster the words in the corpus, given a number
of classes. Then words in the corpus are replaced
with their cluster IDs. Using these cluster IDs,
we train n-gram language models as well as a
phrase table with this additional factor of cluster
ID. Our submitted systems have diversed range of
the number of clusters as well as n-gram.
7 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to
the workshop. The results are reported as case-
sensitive BLEU scores on one reference transla-
tion.
7.1 German?English
The experiments for the German to English trans-
lation system are summarized in Table 1. The
baseline system uses POS-based reordering, DWA
with lattice phrase extraction and language models
trained on the News Shuffle corpus and Giga cor-
pus separately. Then we added a 5-gram cluster
LM trained with 1,000 word classes. By adding a
language model using the filtered crawled data we
gained 0.3 BLEU on the test set. For this we com-
bined all language models linearly. The filtered
crawled data was also used to generate a phrase
table, which brought another improvement of 0.85
BLEU. Applying tree-based reordering improved
the BLEU score, and the performance had more
gain by adding the extended DWL, namely us-
ing both bag-of-ngrams and n-best lists. While
lexicalized reordering gave us a slight gain, we
added morphological operation and gained more
improvements.
7.2 English?German
The English to German baseline system uses POS-
based reordering and language models using par-
allel data (EPPS and NC) as shown in Table 2.
Gradual gains were achieved by changing align-
ment from GIZA++ to DWA, adding a bilingual
language model as well as a language model based
on the POS tokens. A 9-gram cluster-based lan-
guage model with 100 word classes gave us a
106
System Dev Test
Baseline 24.15 22.79
+ Cluster LM 24.18 22.84
+ Crawled Data LM (Comb.) 24.53 23.14
+ Crawled Data PT 25.38 23.99
+ Tree Rules 25.80 24.16
+ Extended DWL 25.59 24.54
+ Lexicalized Reordering 26.04 24.55
+ Morphological Operation - 24.62
Table 1: Translation results for German?English
small gain. Improving the reordering using lexi-
alized reordering gave us gain on the optimization
set. Using DWL let us have more improvements
on our test set. By using the filtered crawled data,
we gained a big improvement of 0.46 BLEU on
the test set. Then we extended the DWL with bag
of n-grams and n-best lists to achieve additional
improvements. Finally, the best system includes
lattices generated using tree rules.
System Dev Test
Baseline 17.00 16.24
+ DWA 17.27 16.53
+ Bilingual LM 17.27 16.59
+ POS LM 17.46 16.66
+ Cluster LM 17.49 16.68
+ Lexicalized Reordering 17.57 16.68
+ DWL 17.58 16.77
+ Crawled Data 18.43 17.23
+ Extended DWL 18.66 17.57
+ Tree Rules 18.63 17.70
Table 2: Translation results for English?German
7.3 French?English
Table 3 reports some remarkable improvements
as we combined several techniques on the
French?English direction. The baseline system
was trained on parallel corpora such as EPPS, NC
and Giga, while the language model was trained
on the English part of those corpora plus News
Shuffle. The newly presented web-crawled data
helps to achieve almost 0.6 BLEU points more
on test set. Adding bilingual language model and
cluster language model does not show a significant
impact. Further gains were achieved by the adap-
tation of in-domain data into general-theme phrase
table, bringing 0.15 BLEU better on the test set.
When we added the DWL feature, it notably im-
proves the system by 0.25 BLEU points, resulting
in our best system.
System Dev Test
Baseline 30.33 29.35
+ Crawled Data 30.59 29.93
+ Bilingual and Cluster LMs 30.67 30.01
+ In-Domain PT Adaptation 31.17 30.16
+ DWL 31.07 30.40
Table 3: Translation results for French?English
7.4 English?French
In the baseline system, EPPS, NC, Giga and News
Shuffle corpora are used for language modeling.
The big phrase tables tailored EPPC, NC and Giga
data. The system also uses short-range reordering
trained on EPPS and NC. Adding parallel and fil-
tered crawl data improves the system. It was fur-
ther enhanced by the integration of a 4-gram bilin-
gual language model. Moreover, the best config-
uration of 9-gram language model trained on 500
clusters of French texts gains 0.25 BLEU points
improvement. We also conducted phrase-table
adaptation from the general one into the domain
covered by EPPS and NC data and it helps as well.
The initial try-out with lexicalized reordering fea-
ture showed an improvement of 0.23 points on the
development set, but a surprising reduction on the
test set, thus we decided to take the system after
adaptation as our best English?French system.
System Dev Test
Baseline 30.50 27.77
+ Crawled Data 31.05 27.87
+ Bilingual LM 31.23 28.50
+ Cluster LM 31.58 28.75
+ In-Domain PT Adaptation 31.88 29.12
+ Lexicalized Reordering 32.11 28.98
Table 4: Translation results for English?French
8 Conclusions
We have presented the systems for our par-
ticipation in the WMT 2013 Evaluation for
English?German and English?French. All sys-
tems use a class-based language model as well
as a bilingual language model. Using a DWL
with source context improved the translation qual-
ity of English?German systems. Also for these
systems, we could improve even more with a
tree-based reordering model. Special handling
107
of OOV words improved German?English sys-
tem, while for the inverse direction the language
model with fine-grained POS tags was helpful. For
English?French, phrase table adaptation helps to
avoid using wrong parts of the noisy Giga corpus.
9 Acknowledgements
This work was partly achieved as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. The research lead-
ing to these results has received funding from
the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
n? 287658.
References
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The kit english-
french translation systems for iwslt 2011. In Pro-
ceedings of the eight International Workshop on
Spoken Language Translation (IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues and Alex Waibel. 2012. Detailed Analysis
of different Strategies for Phrase Table Adaptation
in SMT. In Proceedings of the American Machine
Translation Association (AMTA), San Diego, Cali-
fornia, October.
Jan Niehues and Alex Waibel. 2013. An MT Error-
driven Discriminative Word Lexicon using Sentence
Structure Features. In Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013), Sofia, Bul-
garia.
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Marcello Federico, Ian Lane, Michael Paul, and
Franc?ois Yvon, editors, Proceedings of the seventh
International Workshop on Spoken Language Trans-
lation (IWSLT), pages 93?98.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
the ninth conference on European chapter of the As-
sociation for Computational Linguistics, EACL ?99,
pages 71?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German, Columbus, Ohio.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, Great Britain.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
108
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185?192,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joint WMT 2013 Submission of the QUAERO Project
?Stephan Peitz, ?Saab Mansour, ?Matthias Huck, ?Markus Freitag, ?Hermann Ney,
?Eunah Cho, ?Teresa Herrmann, ?Mohammed Mediani, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Quoc Khanh Do,
?Bianka Buschbeck, ?Tonio Wandmacher
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint submis-
sion of the QUAERO project for the
German?English translation task of the
ACL 2013 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013).
The submission was a system combina-
tion of the output of four different transla-
tion systems provided by RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy (KIT), LIMSI-CNRS and SYSTRAN
Software, Inc. The translations were
joined using the RWTH?s system com-
bination approach. Experimental results
show improvements of up to 1.2 points in
BLEU and 1.2 points in TER compared to
the best single translation.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in ma-
chine translation is mainly assigned to the four
groups participating in this joint submission. The
aim of this submission was to show the quality of
a joint translation by combining the knowledge of
the four project partners. Each group develop and
maintain their own different machine translation
system. These single systems differ not only in
their general approach, but also in the preprocess-
ing of training and test data. To take advantage
of these differences of each translation system, we
combined all hypotheses of the different systems,
using the RWTH system combination approach.
This paper is structured as follows. First, the
different engines of all four groups are introduced.
In Section 3, the RWTH Aachen system combina-
tion approach is presented. Experiments with dif-
ferent system selections for system combination
are described in Section 4. This paper is concluded
in Section 5.
2 Translation Systems
For WMT 2013, each QUAERO partner trained
their systems on the parallel Europarl (EPPS),
News Commentary (NC) corpora and the web-
crawled corpus. All single systems were tuned on
the newstest2009 and newstest2010 development
set. The newstest2011 development set was used
to tune the system combination parameters. Fi-
nally, on newstest2012 the results of the different
system combination settings are compared. In this
Section, all four different translation engines are
presented.
2.1 RWTH Aachen Single System
For the WMT 2013 evaluation, RWTH utilized a
phrase-based decoder based on (Wuebker et al,
2012) which is part of RWTH?s open-source SMT
toolkit Jane 2.1 1. GIZA++ (Och and Ney, 2003)
was employed to train a word alignment, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
After phrase pair extraction from the word-
aligned parallel corpus, the translation probabil-
ities are estimated by relative frequencies. The
standard feature set alo includes an n-gram lan-
guage model, phrase-level IBM-1 and word-,
phrase- and distortion-penalties, which are com-
bined in log-linear fashion. Furthermore, we used
an additional reordering model as described in
(Galley and Manning, 2008). By this model six
1http://www-i6.informatik.rwth-aachen.
de/jane/
185
additional feature are added to the log-linear com-
bination. The model weights are optimized with
standard Mert (Och, 2003a) on 200-best lists. The
optimization criterion is BLEU.
2.1.1 Preprocessing
In order to reduce the source vocabulary size trans-
lation, the German text was preprocessed by split-
ting German compound words with the frequency-
based method described in (Koehn and Knight,
2003). To further reduce translation complexity
for the phrase-based approach, we performed the
long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.2 Translation Model
We applied filtering and weighting for domain-
adaptation similarly to (Mansour et al, 2011) and
(Mansour and Ney, 2012). For filtering the bilin-
gual data, a combination of LM and IBM Model
1 scores was used. In addition, we performed
weighted phrase extraction by using a combined
LM and IBM Model 1 weight.
2.1.3 Language Model
During decoding a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl,
the 109 French-English, UN and LDC Gigaword
Fourth Edition corpora.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
The training data was preprocessed prior to the
training. Symbols such as quotes, dashes and
apostrophes are normalized. Then the first words
of each sentence are smart-cased. For the Ger-
man part of the training corpus, the hunspell2 lex-
icon was used, in order to learn a mapping from
old German spelling to new German writing rules.
Compound-splitting was also performed as de-
scribed in Koehn and Knight (2003). We also re-
moved very long sentences, empty lines, and sen-
tences which show big mismatch on the length.
2.2.2 Filtering
The web-crawled corpus was filtered using an
SVM classifier as described in (Mediani et al,
2011). The lexica used in this filtering task were
obtained from Giza alignments trained on the
2http://hunspell.sourceforge.net/
cleaner corpora, EPPS and NC. Assuming that this
corpus is very noisy, we biased our classifier more
towards precision than recall. This was realized
by giving higher number of false examples (80%
of the training data).
This filtering technique ruled out more than
38% of the corpus (the unfiltered corpus contains
around 2.4M pairs, 0.9M of which were rejected
in the filtering task).
2.2.3 System Overview
The in-house phrase-based decoder (Vogel, 2003)
is used to perform decoding. Optimization with
regard to the BLEU score is done using Minimum
Error Rate Training (MERT) as described in Venu-
gopal et al (2005).
2.2.4 Reordering Model
We applied part-of-speech (POS) based reordering
using probabilistic continuous (Rottmann and Vo-
gel, 2007) and discontinuous (Niehues and Kolss,
2009) rules. This was learned using POS tags gen-
erated by the TreeTagger (Schmid, 1994) for short
and long range reorderings respectively.
In addition to this POS-based reordering, we
also used tree-based reordering rules. Syntactic
parse trees of the whole training corpus and the
word alignment between source and target lan-
guage are used to learn rules on how to reorder the
constituents in a German source sentence to make
it match the English target sentence word order
better (Herrmann et al, 2013). The training corpus
was parsed by the Stanford parser (Rafferty and
Manning, 2008). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are en-
coded in a word lattice which is used as input to
the decoder.
Moreover, our reordering model was extended
so that it could include the features of lexicalized
reordering model. The reordering probabilities for
each phrase pair are stored as well as the origi-
nal position of each word in the lattice. During
the decoding, the reordering origin of the words
is checked along with its probability added as an
additional score.
2.2.5 Translation Models
The translation model uses the parallel data of
EPPS, NC, and the filtered web-crawled data. As
word alignment, we used the Discriminative Word
Alignment (DWA) as shown in (Niehues and Vo-
186
gel, 2008). The phrase pairs were extracted using
different source word order suggested by the POS-
based reordering models presented previously as
described in (Niehues et al, 2009).
In order to extend the context of source lan-
guage words, we applied a bilingual language
model (Niehues et al, 2011). A Discriminative
Word Lexicon (DWL) introduced in (Mauser et
al., 2009) was extended so that it could take the
source context also into the account. For this,
we used a bag-of-ngrams instead of representing
the source sentence as a bag-of-words. Filtering
based on counts was then applied to the features
for higher order n-grams. In addition to this, the
training examples were created differently so that
we only used the words that occur in the n-best list
but not in the reference as negative example.
2.2.6 Language Models
We build separate language models and combined
them prior to decoding. As word-token based
language models, one language model is built on
EPPS, NC, and giga corpus, while another one is
built using crawled data. We combined the LMs
linearly by minimizing the perplexity on the de-
velopment data. As a bilingual language model we
used the EPPS, NC, and the web-crawled data and
combined them. Furthermore, we use a 5-gram
cluster-based language model with 1,000 word
clusters, which was trained on the EPPS and NC
corpus. The word clusters were created using the
MKCLS algorithm.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine transla-
tion system based on bilingual n-gram3. In this
approach, the translation model relies on a spe-
cific decomposition of the joint probability of a
sentence pair using the n-gram assumption: a sen-
tence pair is decomposed into a sequence of bilin-
gual units called tuples, defining a joint segmen-
tation of the source and target. In the approach of
(Marin?o et al, 2006), this segmentation is a by-
product of source reordering which ultimately de-
rives from initial word and phrase alignments.
2.3.2 An overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using
3http://ncode.limsi.fr/
a n-gram model of (source,target) pairs (Casacu-
berta and Vidal, 2004). Training this model re-
quires to reorder source sentences so as to match
the target word order. This is performed by
a stochastic finite-state reordering model, which
uses part-of-speech information4 to generalize re-
ordering patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized re-
ordering models (Tillmann, 2004) aiming at pre-
dicting the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. The four lexicon mod-
els are similar to the ones use in a standard phrase
based system: two scores correspond to the rel-
ative frequencies of the tuples and two lexical
weights estimated from the automatically gener-
ated word alignments. The weights associated to
feature functions are optimally combined using a
discriminative training framework (Och, 2003b).
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the
tuple extraction process. The resulting reordering
hypotheses are passed to the decoder in the form
of word lattices (Crego and Mario, 2006).
2.3.3 Continuous space translation models
One critical issue with standard n-gram translation
models is that the elementary units are bilingual
pairs, which means that the underlying vocabu-
lary can be quite large, even for small translation
tasks. Unfortunately, the parallel data available to
train these models are typically order of magni-
tudes smaller than the corresponding monolingual
corpora used to train target language models. It is
very likely then, that such models should face se-
vere estimation problems. In such setting, using
neural network language model techniques seem
all the more appropriate. For this study, we fol-
low the recommendations of Le et al (2012), who
propose to factor the joint probability of a sen-
tence pair by decomposing tuples in two (source
and target) parts, and further each part in words.
This yields a word factored translation model that
4Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
187
can be estimated in a continuous space using the
SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the com-
putational cost of computing n-gram probabilities.
The solution used here was to resort to a two pass
approach: the first pass uses a conventional back-
off n-gram model to produce a k-best list; in the
second pass, the k-best list is reordered using the
probabilities of m-gram SOUL translation models.
In the following experiments, we used a fixed con-
text size for SOUL of m= 10, and used k = 300.
2.3.4 Corpora and data pre-processing
All the parallel data allowed in the constrained
task are pooled together to create a single par-
allel corpus. This corpus is word-aligned using
MGIZA++5 with default settings. For the English
monolingual training data, we used the same setup
as last year6 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we also took advantage of our in-
house text processing tools for the tokenization
and detokenization steps (Dchelotte et al, 2008)
and our system is built in ?true-case?. As Ger-
man is morphologically more complex than En-
glish, the default policy which consists in treat-
ing each word form independently is plagued with
data sparsity, which is detrimental both at training
and decoding time. Thus, the German side was
normalized using a specific pre-processing scheme
(described in (Allauzen et al, 2010; Durgar El-
Kahlout and Yvon, 2010)), which notably aims at
reducing the lexical redundancy by (i) normalizing
the orthography, (ii) neutralizing most inflections
and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
In the past few years, SYSTRAN has been focus-
ing on the introduction of statistical approaches
to its rule-based backbone, leading to Hybrid Ma-
chine Translation.
The technique of Statistical Post-Editing
(Dugast et al, 2007) is used to automatically edit
the output of the rule-based system. A Statistical
Post-Editing (SPE) module is generated from a
bilingual corpus. It is basically a translation mod-
ule by itself, however it is trained on rule-based
5http://geek.kyloo.net/software
6The fifth edition of the English Gigaword
(LDC2011T07) was not used.
translations and reference data. It applies correc-
tions and adaptations learned from a phrase-based
5-gram language model. Using this two-step
process will implicitly keep long distance re-
lations and other constraints determined by the
rule-based system while significantly improving
phrasal fluency. It has the advantage that quality
improvements can be achieved with very little
but targeted bilingual data, thus significantly
reducing training time and increasing translation
performance.
The basic setup of the SPE component is identi-
cal to the one described in (Dugast et al, 2007).
A statistical translation model is trained on the
rule-based translation of the source and the target
side of the parallel corpus. Language models are
trained on each target half of the parallel corpora
and also on additional in-domain corpora. More-
over, the following measures - limiting unwanted
statistical effects - were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is sig-
nificantly reduced. In addition, entity trans-
lation is handled more reliably by the rule-
based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the
reference translation) is used to produce an
additional parallel corpus (whose target is
identical to the source). This was added to the
parallel text in order to improve word align-
ment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side
are also discarded.
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained on 2M
phrases from the news/europarl and Common-
Crawl corpora, provided as training data for WMT
2013. Weights for these separate models were
tuned by the Mert algorithm provided in the Moses
toolkit (Koehn et al, 2007), using the provided
news development set.
188
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
3 RWTH Aachen System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses gener-
ated with different translation engines. First, a
word to word alignment for the given single sys-
tem hypotheses is produced. In a second step a
confusion network is constructed. Then, the hy-
pothesis with the highest probability is extracted
from this confusion network. For the alignment
procedure, each of the given single systems gen-
erates one confusion network with its own as pri-
mary system. To this primary system all other hy-
potheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1. The final network for one source sen-
tence is the union of all confusion networks gen-
erated from the different primary systems. That
allows the system combination to select the word
order from different system outputs.
Before performing system combination, each
translation output was normalized by tokenization
and lowercasing. The output of the combination
was then truecased based on the original truecased
output.
The model weights of the system combination
are optimized with standard Mert (Och, 2003a)
on 100-best lists. We add one voting feature for
each single system to the log-linear framework of
the system combination. The voting feature fires
for each word the single system agrees on. More-
over, a word penalty, a language model trained on
the input hypotheses, a binary feature which pe-
nalizes word deletions in the confusion network
and a primary feature which marks the system
which provides the word order are combined in
this log-linear model. The optimization criterion
is 4BLEU-TER.
4 Experimental Results
In this year?s experiments, we tried to improve the
result of the system combination further by com-
bining single systems tuned on different develop-
Table 1: Comparison of single systems tuned on
newstest2009 and newstest2010. The results are
reported on newstest2012.
single systems tuned on newstest2012
newstest BLEU TER
KIT 2009 24.6 58.4
2010 24.6 58.6
LIMSI 2009 22.5 61.5
2010 22.6 59.8
SYSTRAN 2009 20.9 63.3
2010 21.2 62.2
RWTH 2009 23.7 60.8
2010 24.4 58.8
ment sets. The idea is to achieve a more stable
performance in terms of translation quality, if the
single systems are not optimized on the same data
set. In Table 1, the results of each provided single
system tuned on newstest2009 and newstest2010
are shown. For RWTH, LIMSI and SYSTRAN,
it seems that the performance of the single system
depends on the chosen tuning set. However, the
translation quality of the single systems provided
by KIT is stable.
As initial approach and for the final submis-
sion, we grouped single systems with dissimilar
approaches. Thus, KIT (phrase-based SMT) and
SYSTRAN (rule-based MT) tuned their system on
newstest2010, while RWTH (phrase-based SMT)
and LIMSI (n-gram) optimized on newstest2009.
To compare the impact of this approach, all pos-
sible combinations were checked (Table 2). How-
ever, it seems that the translation quality can not be
improved by this approach. For the test set (new-
stest2012), BLEU is steady around 25.6 points.
Even if the single system with lowest BLEU are
combined (KIT 2010, LIMSI 2009, SYSTRAN
2010, RWTH 2009), the translation quality in
terms of BLEU is comparable with the combina-
tion of the best single systems (KIT 2009, LIMSI
2010, SYSTRAN 2010, RWTH 2010). However,
we could gain 1.0 point in TER.
Due to the fact, that for the final submission the
initial grouping was available only, we kept this
189
Table 2: Comparison of different system combination settings. For each possible combination of systems
tuned on different tuning sets, a system combination was set up, re-tuned on newstest2011 and evaluated
on newstest2012. The setting used for further experiments is set in boldface.
single systems system combinations
KIT LIMSI SYSTRAN RWTH newstest2011 newstest2012
tuned on newstest BLEU TER BLEU TER
2009 2009 2009 2009 24.6 58.0 25.6 56.8
2010 2010 2010 2010 24.2 58.1 25.6 57.7
2010 2009 2009 2009 24.5 57.9 25.7 57.4
2009 2010 2009 2009 24.4 58.3 25.7 57.0
2009 2009 2010 2009 24.5 57.9 25.6 57.0
2009 2009 2009 2010 24.5 58.0 25.6 56.8
2009 2010 2010 2010 24.1 57.5 25.4 56.4
2010 2009 2010 2010 24.3 57.6 25.6 56.9
2010 2010 2009 2010 24.2 58.0 25.6 57.3
2010 2010 2010 2009 24.3 57.9 25.5 57.6
2010 2010 2009 2009 24.4 58.1 25.6 57.5
2009 2009 2010 2010 24.4 57.8 25.5 56.6
2009 2010 2010 2009 24.4 58.2 25.5 57.0
2009 2010 2009 2010 24.2 57.8 25.5 56.8
2010 2009 2009 2010 24.4 57.9 25.6 57.4
2010 2009 2010 2009 24.4 57.7 25.6 57.4
Table 3: Results of the final submission (bold-
face) compared with best single system on new-
stest2012.
newstest2011 newstest2012
BLEU TER BLEU TER
best single 23.2 60.9 24.6 58.4
system comb. 24.4 57.7 25.6 57.4
+ IBM-1 24.6 58.1 25.6 57.6
+ bigLM 24.6 57.9 25.8 57.2
combination. To improve this baseline further, two
additional models were added. We applied lexi-
cal smoothing (IBM-1) and an additional language
model (bigLM) trained on the English side of the
parallel data and the News shuffle corpus. The re-
sults are presented in Table 3.
The baseline was slightly improved by 0.2
points in BLEU and TER. Note, this system com-
bination was the final submission.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, the partners of the QUAERO
project (Karlsruhe Institute of Technology, RWTH
Aachen University, LIMSI-CNRS and SYSTRAN
Software, Inc.) provided a joint submission. By
joining the output of four different translation sys-
tems with RWTH?s system combination, we re-
ported an improvement of up to 1.2 points in
BLEU and TER.
Combining systems optimized on different tun-
ing sets does not seem to improve the translation
quality. However, by adding additional model, the
baseline was slightly improved.
All in all, we conclude that the variability in
terms of BLEU does not influence the final result.
It seems that using different approaches of MT in
a system combination is more important (Freitag
et al, 2012).
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
190
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Josep M. Crego and Jose? B. Mario. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on systran?s rule-based trans-
lation system. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 220?223, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Daniel Dchelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Markus Freitag, Stephan Peitz, Matthias Huck, Her-
mann Ney, Teresa Herrmann, Jan Niehues, Alex
Waibel, Alexandre Allauzen, Gilles Adda, Bianka
Buschbeck, Josep Maria Crego, and Jean Senellart.
2012. Joint wmt 2012 submission of the quaero
project. In NAACL 2012 Seventh Workshop on Sta-
tistical Machine Translation, pages 322?329, Mon-
treal, Canada, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 847?855, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177?180, Prague, Czech Republic, June.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An Automatic Metric for MT Evaluation
with High Levels of Correlation with Human Judg-
ments. pages 228?231, Prague, Czech Republic,
June.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Sab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
191
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation Systems for IWSLT
2011. In Proceedings of the Eighth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003a. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Franz Josef Och. 2003b. Minimum error rate training
in statistical machine translation. In ACL ?03: Proc.
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Evelyne Tzoukermann and SusanEditors Arm-
strong, editors, Proceedings of the ACL SIGDAT-
Workshop, pages 47?50. Kluwer Academic Publish-
ers.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Int. Conf. on Spo-
ken Language Processing, volume 2, pages 901?
904, Denver, Colorado, USA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
192
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130?135,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2014
Teresa Herrmann, Mohammed Mediani, Eunah Cho, Thanh-Le Ha,
Jan Niehues, Isabel Slawik, Yuqi Zhang and Alex Waibel
Institute for Anthropomatics and Robotics
KIT - Karlsruhe Institute of Technology
firstname.lastname@kit.edu
Abstract
In this paper, we present the KIT
systems participating in the Shared
Translation Task translating between
English?German and English?French.
All translations are generated using
phrase-based translation systems, using
different kinds of word-based, part-of-
speech-based and cluster-based language
models trained on the provided data.
Additional models include bilingual lan-
guage models, reordering models based
on part-of-speech tags and syntactic parse
trees, as well as a lexicalized reordering
model. In order to make use of noisy
web-crawled data, we apply filtering
and data selection methods for language
modeling. A discriminative word lexicon
using source context information proved
beneficial for all translation directions.
1 Introduction
We describe the KIT systems for the Shared Trans-
lation Task of the ACL 2014 Ninth Workshop on
Statistical Machine Translation. We participated
in the English?German and English?French
translation directions, using a phrase-based de-
coder with lattice input.
The paper is organized as follows: the next sec-
tion describes the data used for each translation
direction. Section 3 gives a detailed description of
our systems including all the models. The trans-
lation results for all directions are presented after-
wards and we close with a conclusion.
2 Data
We utilize the provided EPPS, NC and Common
Crawl parallel corpora for English?German and
German?English, plus Giga for English?French
and French?English. The monolingual part
of those parallel corpora, the News Shuffle
corpus for all four directions and additionally
the Gigaword corpus for English?French and
German?English are used as monolingual train-
ing data for the different language models. For
optimizing the system parameters, newstest2012
and newstest2013 are used as development and
test data respectively.
3 System Description
Before training we perform a common preprocess-
ing of the raw data, which includes removing long
sentences and sentences with a length mismatch
exceeding a certain threshold. Afterwards, we nor-
malize special symbols, dates, and numbers. Then
we perform smart-casing of the first letter of every
sentence. Compound splitting (Koehn and Knight,
2003) is performed on the source side of the cor-
pus for German?English translation. In order to
improve the quality of the web-crawled Common
Crawl corpus, we filter out noisy sentence pairs us-
ing an SVM classifier for all four translation tasks
as described in Mediani et al. (2011).
Unless stated otherwise, we use 4-gram lan-
guage models (LM) with modified Kneser-Ney
smoothing, trained with the SRILM toolkit (Stol-
cke, 2002). All translations are generated by
an in-house phrase-based translation system (Vo-
gel, 2003), and we use Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) for optimization. The word align-
ment of the parallel corpora is generated using
the GIZA++ Toolkit (Och and Ney, 2003) for
both directions. Afterwards, the alignments are
combined using the grow-diag-final-and heuris-
tic. For English?German, we use discrimi-
native word alignment trained on hand-aligned
data as described in Niehues and Vogel (2008).
The phrase table (PT) is built using the Moses
toolkit (Koehn et al., 2007). The phrase scoring
for the small data sets (German?English) is also
130
done by the Moses toolkit, whereas the bigger sets
(French?English) are scored by our in-house par-
allel phrase scorer (Mediani et al., 2012a). The
phrase pair probabilities are computed using mod-
ified Kneser-Ney smoothing as described in Foster
et al. (2006).
Since German is a highly inflected language,
we try to alleviate the out-of-vocabulary prob-
lem through quasi-morphological operations that
change the lexical entry of a known word form to
an unknown word form as described in Niehues
and Waibel (2011).
3.1 Word Reordering Models
We apply automatically learned reordering rules
based on part-of-speech (POS) sequences and syn-
tactic parse tree constituents to perform source
sentence reordering according to the target lan-
guage word order. The rules are learned
from a parallel corpus with POS tags (Schmid,
1994) for the source side and a word align-
ment to learn reordering rules that cover short
range (Rottmann and Vogel, 2007) and long
range reorderings (Niehues and Kolss, 2009).
In addition, we apply a tree-based reordering
model (Herrmann et al., 2013) to better address
the differences in word order between German and
English. Here, a word alignment and syntactic
parse trees (Rafferty and Manning, 2008; Klein
and Manning, 2003) for the source side of the
training corpus are required to learn rules on how
to reorder the constituents in the source sentence.
The POS-based and tree-based reordering rules
are applied to each input sentence before transla-
tion. The resulting reordered sentence variants as
well as the original sentence are encoded in a re-
ordering lattice. The lattice, which also includes
the original position of each word, is used as input
to the decoder.
In order to acquire phrase pairs matching the
reordered sentence variants, we perform lattice
phrase extraction (LPE) on the training corpus
where phrase are extracted from the reordered
word lattices instead of the original sentences.
In addition, we use a lexicalized reordering
model (Koehn et al., 2005) which stores reorder-
ing probabilities for each phrase pair. During
decoding the lexicalized reordering model deter-
mines the reordering orientation of each phrase
pair at the phrase boundaries. The probability for
the respective orientation with respect to the orig-
inal position of the words is included as an addi-
tional score in the log-linear model of the transla-
tion system.
3.2 Adaptation
In the French?English and English?French sys-
tems, we perform adaptation for translation mod-
els as well as for language models. The EPPS and
NC corpora are used as in-domain data for the di-
rection English?French, while NC corpus is the
in-domain data for French?English.
Two phrase tables are built: one is the out-
of-domain phrase table, which is trained on all
corpora; the other is the in-domain phrase table,
which is trained on in-domain data. We adapt the
translation model by using the scores from the two
phrase tables with the backoff approach described
in Niehues and Waibel (2012). This results in a
phrase table with six scores, the four scores from
the general phrase table as well as the two condi-
tional probabilities from the in-domain phrase ta-
ble. In addition, we take the union of the candidate
phrase pairs collected from both phrase tables A
detailed description of the union method can be
found in Mediani et al. (2012b).
The language model is adapted by log-linearly
combining the general language model and an in-
domain language model. We train a separate lan-
guage model using only the in-domain data. Then
it is used as an additional language model during
decoding. Optimal weights are set during tuning
by MERT.
3.3 Special Language Models
In addition to word-based language models, we
use different types of non-word language models
for each of the systems. With the help of a bilin-
gual language model (Niehues et al., 2011) we
are able to increase the bilingual context between
source and target words beyond phrase bound-
aries. This language model is trained on bilin-
gual tokens created from a target word and all its
aligned source words. The tokens are ordered ac-
cording to the target language word order.
Furthermore, we use language models based
on fine-grained part-of-speech tags (Schmid and
Laws, 2008) as well as word classes to allevi-
ate the sparsity problem for surface words. The
word classes are automatically learned by clus-
tering the words of the corpus using the MKCLS
algorithm (Och, 1999). These n-gram language
models are trained on the target language corpus,
131
where the words have been replaced either by their
corresponding POS tag or cluster ID. During de-
coding, these language models are used as addi-
tional models in the log-linear combination.
The data selection language model is trained
on data automatically selected using cross-entropy
differences between development sets from pre-
vious WMT workshops and the noisy crawled
data (Moore and Lewis, 2010). We selected the
top 10M sentences to train this language model.
3.4 Discriminative Word Lexicon
A discriminative word lexicon (DWL) models the
probability of a target word appearing in the trans-
lation given the words of the source sentence.
DWLs were first introduced by Mauser et al.
(2009). For every target word, they train a maxi-
mum entropy model to determine whether this tar-
get word should be in the translated sentence or
not using one feature per source word.
We use two simplifications of this model that
have shown beneficial to translation quality and
training time in the past (Mediani et al., 2011).
Firstly, we calculate the score for every phrase pair
before translating. Secondly, we restrict the nega-
tive training examples to words that occur within
matching phrase pairs.
In this evaluation, we extended the DWL
with n-gram source context features proposed
by Niehues and Waibel (2013). Instead of rep-
resenting the source sentence as a bag-of-words,
we model it as a bag-of-n-grams. This allows us
to include information about source word order in
the model. We used one feature per n-gram up to
the order of three and applied count filtering for
bigrams and trigrams.
4 Results
This section presents the participating systems
used for the submissions in the four translation
directions of the evaluation. We describe the in-
dividual components that form part of each of
the systems and report the translation qualities
achieved during system development. The scores
are reported in case-sensitive BLEU (Papineni et
al., 2002).
4.1 English-French
The development of our English?French system
is shown in Table 1.
It is noteworthy that, for this direction, we chose
to tune on a subset of 1,000 pairs from news-
test2012, due to the long time the whole set takes
to be decoded. In a preliminary set of experiments
(not reported here), we found no significant differ-
ences between tuning on the small or the big devel-
opment sets. The translation model of the baseline
system is trained on the whole parallel data after
filtering (EPPS, NC, Common Crawl, Giga). The
same data was also used for language modeling.
We also use POS-based reordering.
The biggest improvement was due to using two
additional language models. One consists of a log-
linear interpolation of individual language models
trained on the target side of the parallel data, the
News shuffle, Gigaword and NC corpora. In ad-
dition, an in-domain language model trained only
on NC data is used. This improves the score by
more than 1.4 points. Adaptation of the translation
model towards a smaller model trained on EPPS
and NC brings an additional 0.3 points.
Another 0.3 BLEU points could be gained by
using other special language models: a bilingual
language model together with a 4-gram cluster
language model (trained on all monolingual data
using the MKCLS tool and 500 clusters). Incor-
porating a lexicalized reordering model into the
system had a very noticeable effect on test namely
more than half a BLEU point.
Finally, using a discriminative word lexicon
with source context has a very small positive ef-
fect on the test score, however more than 0.3 on
dev. This final configuration was the basis of our
submitted official translation.
System Dev Test
Baseline 15.63 27.61
+ Big LMs 16.56 29.02
+ PT Adaptation 16.77 29.32
+ Bilingual + Cluster LM 16.87 29.64
+ Lexicalized Reordering 16.92 30.17
+ Source DWL 17.28 30.19
Table 1: Experiments for English?French
4.2 French-English
Several experiments were conducted for the
French?English translation system. They are
summarized in Table 2.
The baseline system is essentially a phrase-
based translation system with some preprocess-
132
ing steps on the source side and utilizing the
short-range POS-based reordering on all parallel
data and fine-grained monolingual corpora such as
EPPS and NC.
Adapting the translation model using a small in-
domain phrase table trained on NC data only helps
us gain more than 0.4 BLEU points.
Using non-word language models including a
bilingual language model and a 4-gram 50-cluster
language model trained on the whole parallel data
attains 0.24 BLEU points on the test set.
Lexicalized reordering improves our system on
the development set by 0.3 BLEU points but has
less effect on the test set with a minor improve-
ment of around 0.1 BLEU points.
We achieve our best system, which is used for
the evaluation, by adding a DWL with source con-
text yielding 31.54 BLEU points on the test set.
System Dev Test
Baseline 30.16 30.70
+ LM Adaptation 30.58 30.94
+ PT Adaptation 30.69 31.14
+ Bilingual + Cluster LM 30.85 31.38
+ Lexicalized Reordering 31.14 31.46
+ Source DWL 31.19 31.54
Table 2: Experiments for French?English
4.3 English-German
Table 3 presents how the English-German transla-
tion system is improved step by step.
In the baseline system, we used parallel data
which consists of the EPPS and NC corpora. The
phrase table is built using discriminative word
alignment. For word reordering, we use word lat-
tices with long range reordering rules. Five lan-
guage models are used in the baseline system; two
word-based language models, a bilingual language
model, and two 9-gram POS-based language mod-
els. The two word-based language models use 4-
gram context and are trained on the parallel data
and the filtered Common Crawl data separately,
while the bilingual language model is built only
on the Common Crawl corpus. The two POS-
based language models are also based on the paral-
lel data and the filtered crawled data, respectively.
When using a 9-gram cluster language model,
we get a slight improvement. The cluster is trained
with 1,000 classes using EPPS, NC, and Common
Crawl data.
We use the filtered crawled data in addition to
the parallel data in order to build the phrase table;
this gave us 1 BLEU point of improvement.
The system is improved by 0.1 BLEU points
when we use lattice phrase extraction along with
lexicalized reordering rules.
Tree-based reordering rules improved the sys-
tem performance further by another 0.1 BLEU
points.
By reducing the context of the two POS-based
language models from 9-grams to 5-grams and
shortening the context of the language model
trained on word classes to 4-grams, the score on
the development set hardly changes but we can see
a slightly improvement for the test case.
Finally, we use the DWL with source context
and build a big bilingual language model using
both the crawled and parallel data. By doing so,
we improved the translation performance by an-
other 0.3 BLEU points. This system was used for
the translation of the official test set.
System Dev Test
Baseline 16.64 18.60
+ Cluster LM 16.76 18.66
+ Common Crawl Data 17.27 19.66
+ LPE + Lexicalized Reordering 17.45 19.75
+ Tree Rules 17.53 19.85
+ Shorter n-grams 17.55 19.92
+ Source DWL + Big BiLM 17.82 20.21
Table 3: Experiments for English?German
4.4 German-English
Table 4 shows the development steps of the
German-English translation system.
For the baseline system, the training data of the
translation model consists of EPPS, NC and the
filtered parallel crawled data. The phrase table
is built using GIZA++ word alignment and lattice
phrase extraction. All language models are trained
with SRILM and scored in the decoding process
with KenLM (Heafield, 2011). We use word lat-
tices generated by short and long range reordering
rules as input to the decoder. In addition, a bilin-
gual language model and a target language model
trained on word clusters with 1,000 classes are in-
cluded in the system.
Enhancing the word reordering with tree-based
reordering rules and a lexicalized reordering
133
model improved the system performance by 0.6
BLEU points.
Adding a language model trained on selected
data from the monolingual corpora gave another
small improvement.
The DWL with source context increased the
score on the test set by another 0.5 BLEU points
and applying morphological operations to un-
known words reduced the out-of-vocabulary rate,
even though no improvement in BLEU can be ob-
served. This system was used to generate the
translation submitted to the evaluation.
System Dev Test
Baseline 24.40 26.34
+ Tree Rules 24.71 26.86
+ Lexicalized Reordering 24.89 26.93
+ LM Data Selection 24.96 27.03
+ Source DWL 25.32 27.53
+ Morphological Operations - 27.53
Table 4: Experiments for German?English
5 Conclusion
In this paper, we have described the systems
developed for our participation in the Shared
Translation Task of the WMT 2014 evaluation
for English?German and English?French. All
translations were generated using a phrase-based
translation system which was extended by addi-
tional models such as bilingual and fine-grained
part-of-speech language models. Discriminative
word lexica with source context proved beneficial
in all four language directions.
For English-French translation using a smaller
development set performed reasonably well and
reduced development time. The most noticeable
gain comes from log-linear interpolation of multi-
ple language models.
Due to the large amounts and diversity of
the data available for French-English, adapta-
tion methods and non-word language models con-
tribute the major improvements to the system.
For English-German translation, the crawled
data and a DWL using source context to guide
word choice brought most of the improvements.
Enhanced word reordering models, namely
tree-based reordering rules and a lexicalized re-
ordering model as well as the source-side fea-
tures for the discriminative word lexicon helped
improve the system performance for German-
English translation.
In average we achieved an improvement of over
1.5 BLEU over the respective baselines for all our
systems.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658.
References
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proceedings of the 2006 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), Sydney, Australia.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, Scotland, United Kingdom.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL 2003), Sapporo, Japan.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of the Eleventh Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2003), Budapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the Second International Workshop
on Spoken Language Translation (IWSLT 2005),
Pittsburgh, PA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL
2007), Demonstration Session, Prague, Czech Re-
public.
134
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models.
In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Suntec, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight International
Workshop on Spoken Language Translation (IWSLT
2011), San Francisco, CA, USA.
Mohammed Mediani, Jan Niehues, and Alex Waibel.
2012a. Parallel Phrase Scoring for Extra-large Cor-
pora. In The Prague Bulletin of Mathematical Lin-
guistics, number 98.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunach Cho, Teresa Herrmann, Rainer
K?argel, and Alexander Waibel. 2012b. The KIT
Translation Systems for IWSLT 2012. In Proceed-
ings of the Ninth International Workshop on Spoken
Language Translation (IWSLT 2012), Hong Kong,
HK.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In Proceedings
of the ACL 2010 Conference Short Papers, Uppsala,
Sweden.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation (WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation (WMT 2008), Columbus, OH,
USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In
Proceedings of the Eight International Workshop on
Spoken Language Translation (IWSLT 2008), San
Francisco, CA, USA.
J. Niehues and A. Waibel. 2012. Detailed Analysis of
Different Strategies for Phrase Table Adaptation in
SMT. In Proceedings of the Tenth Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA 2012), San Diego, CA, USA.
J. Niehues and A. Waibel. 2013. An MT Error-Driven
Discriminative Word Lexicon using Sentence Struc-
ture Features. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, Scotland, United King-
dom.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for Deter-
mining Bilingual Word Classes. In Proceedings of
the Ninth Conference of the European Chapter of the
Association for Computational Linguistics (EACL
1999), Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German, Columbus, OH,
USA.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of the
11th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI
2007), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In In-
ternational Conference on Computational Linguis-
tics (COLING 2008), Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
135
