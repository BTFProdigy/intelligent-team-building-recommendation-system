Corpus-Based Analysis of Japanese Relative
Clause Constructions
Takeshi Abekawa1 and Manabu Okumura2
1 Interdisciplinary Graduate School of Science and Engineering,
Tokyo Institute of Technology, Japan
abekawa@lr.pi.titech.ac.jp
2 Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
oku@pi.titech.ac.jp
Abstract. Japanese relative clause constructions (RCC?s) are defined as
being the NP?s of structure ?S NP?, noting the lack of a relative pronoun
or any other explicit form of noun-clause demarcation. Japanese relative
clause modification should be classified into at least two major semantic
types: case-slot gapping and head restrictive. However, these types for
relative clause modification cannot apparently be distinguished. In this
paper we propose a method of identifying a RCC?s type with a machine
learning technique. The features used in our approach are not only rep-
resenting RCC?s characteristics, but also automatically obtained from
large corpora. The results we obtained from evaluation revealed that our
method outperformed the traditional case frame-based method, and the
features that we presented were effective in identifying RCC?s types.
1 Introduction
Japanese relative clause constructions (RCC?s) are defined as being the NP?s of
structure ?S NP?, noting the lack of a relative pronoun or any other explicit form
of noun-clause demarcation[1]. Japanese relative clause constructions should be
classified into at least two major semantic types: case-slot gapping and head
restrictive. However, these types for relative clause constructions cannot appar-
ently be distinguished.
Given the types of Japanese relative clause constructions and a corpus of
Japanese relative clause construction instances, we present a machine learning
based approach to classifying RCC?s. We present a set of lexical and semantic
features that characterize RCC?s, and integrate them as a classifier to determine
RCC types. We use decision tree learning as the machine learning algorithm.
Distinguishing case-slot gapping and head restrictive relative clauses, or re-
solving the semantic relationship between the relative clause and its head noun
has several application domains, such as machine translation from Japanese[5].
It also has a place in text understanding tasks, such as splitting a long sentence
into multiple shorter sentences, and removing less important clauses to shorten
a sentence[6].
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 46?57, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Corpus-Based Analysis of Japanese Relative Clause Constructions 47
Previously, relative clauses had been analyzed with rule-based methods that
utilized case frames[5,2]. Using hand-crafted rules and knowledge creates several
problems: the high cost of constructing them, and lower scalability and coverage.
Recently, due to the availability of very large corpora, corpus-based and ma-
chine learning-based approaches have been actively investigated[7]. Cooccurrence
information between nouns and verbs can be calculated from the syntactically
parsed corpus, and this information can be used preferentially instead of hand-
crafted case frames to determine whether a noun can be the filler of a case-slot
of a verb[7,11].
However, merely using the cooccurrence information between nouns and
verbs instead of case frames cannot provide a good solution to the analysis of
Japanese relative clauses. Clauses with high occurrence probability of the main
verb and the head noun can sometimes be head restrictive. Moreover, just be-
cause the head noun can be the filler of a case-slot of the verb does not always
mean that the clause as case-slot gapping. We have to rely on several differ-
ent clues in order to realize accurate classification. Therefore, in this paper we
present eight features are effective in classifying case-slot gapping and head re-
strictive relative clauses. Most of the features can be automatically acquired by
statistically analyzing a corpus as explained in section 4.
In section 2 we first describe the nature of Japanese RCC?s, and in section
3 we outline previous work on the analysis of Japanese relative clauses. In sec-
tion 4 we explain the features that we present in this paper, and in section 5
we explain the machine learning-based classifier, which uses the features in sec-
tion 4. In section 6 we describe the evaluation of the system and discuss the
experimental results.
2 Japanese Relative Clause Constructions
Japanese relative clause constructions have the structure ?S NP?, and constitute
a noun phrase as a whole. We will term the modifying S the ?relative clause?, the
modified NP the ?head NP?, and the overall NP a ?relative clause construction?
or RCC[2]. Example RCCs are:
RCC should be classified into at least two major semantic types: case-slot gap-
ping and head restrictive. With case-slot gapping RCC?s (also called ?inner?
(a)???? ?? ?
saury grill man
?the mani who ?i grills a saury?
(b)??? ????? ??
everyone know information
?the informationi which everyone knows ?i?
(c)???? ?? ??
saury grill smell
?the smell of saury grilled?
48 T. Abekawa and M. Okumura
relative clauses[14]), the head NP can be considered to have been gapped from a
case slot subcategorized by the main verb of the relative clause. Head restrictive
RCC?s (also called ?outer? relative clause[14]) occur when the relative clause mod-
ifies the head NP. In (a), the head NP ?? (man) can be the subject of the main verb
of the relative clause, and in (b), the head NP ?? (information) can be object of
the main verb. These RCC type are ?inner? relative clauses. In (c) the head NP ??
(smell) cannot fill the gap in the relative clause, and RCC type is ?outer?.
The inherent difficulty in determining the type of RCC derives from the fact
that these two types of RCC are syntactically identical. Even if the relative clause
has case-slot gapping, the type of that clause is not always ?inner?, because in
Japanese the main verb of the relative clause has often zero pronoun. We thus
have to disambiguate the individual RCC instances.
3 Related Work
Previous work on analyzing Japanese relative clauses has used case frames as
useful information. They have first tried to find the case frame for the main verb
of the relative clause and embedded the nouns in the clause into its case-slots.
The head noun is then tried to be embedded into the remaining case-slot in the
case frame. To determine whether a relative clause instance is ?outer? clause,
they have beforehand constructed a dictionary of the nouns that can be modi-
fied by ?outer? clause, such as ??(purpose), or ??(opinion). In one approach[5],
the instance is determined to be ?outer? clause, if the head noun is included
in the dictionary, regardless of the main verb of the relative clause. In another
approach[12], the instance is determined to be ?outer?, if the head noun cannot
be embedded into a case-slot and the head noun is included in the dictionary.
Recently, cooccurrence information between verbs and nouns has been used
in analysis. Kawahara and Kurohashi[7] automatically extracted case frames
from very large corpora, and used the case frames to analyze Japanese relative
clauses. However, they judged the instances as ?outer? clauses, only if case-slot
filling did not succeed.
Murata[11] presented a statistical method of classifying whether the relative
clause is an ?inner? or an ?outer? clause. However this method cannot correctly
classify ?outer? relative clause which had high cooccurrence probability of the
main verbs and the head nouns.
4 Feature Set to Classify RCC Type
In this section, we present eight features that can be considered to be effective
in classifying ?inner? and ?outer? relative clauses.
1. Degree of possibility where the head noun can be modified by the
?outer? relative clause (degree of ?outerness?).
In Japanese, there are two ways of modification between verbs and nouns: nouns
modify verbs by filling a case-slot (noun ? verb), and verbs modify nouns in
Corpus-Based Analysis of Japanese Relative Clause Constructions 49
Table 1. Comparison of the number of cooccurring verbs
relative clauses case-slots
noun freq. verb No. freq. verb No.
(intent) 8,732 941 14,216 677
(fact) 5,454 1,448 7,301 754
(preparation) 2,268 428 2,720 74
(people) 6,681 1,367 10,026 1,998
(city) 1,172 449 3,688 857
(television) 2,740 707 30,627 2,228
relative clauses (verb ? noun). Some pairs of a verb and a noun can cooccur
only in RCC, and cannot cooccur by filling a case-slot of the verb. For example,
Therefore, we can measure the likelihood that the noun will be modified by
?outer? relative clauses, by calculating the difference in the frequency distribution
of verbs cooccurring in relative clauses against the frequency distribution of verbs
cooccurring in the case-slot relation (If the difference is larger, the probability
that the noun can be modified by the ?outer? relative clause becomes larger).
We calculate the likelihood as J(Pk(v|n), Pm(v|n)), the Jensen-Shannon dis-
tance between the cooccurrence probability where nouns fill the case-slots of
verbs(Pk(v|n)) and the cooccurrence probability where verbs cooccur with nouns
in relative clauses(Pm(v|n)). Given two probability distributions p,q, the Jensen-
Shannon distance is defined by the following formula[9]:
J(p, q) =
1
2
[
D(p||p + q
2
) + D(q||p + q
2
)
]
. (1)
D(p||q) is the Kullback-Leibler distance and defined by the following formula[3]:
D(p||q) =
?
i
pi log
pi
qi
. (2)
noun ???? (preparation) and verb ???? (run) can cooccur with each other as
the main verb of a relative clause and its head noun, as in ?????? (prepara-
tion for running), though the noun cannot fill any case-slots of the verb, as in *?
?????? (*preparation runs). For nouns, some verbs only cooccur in relative
clauses, and a number of such verbs tend to be modified by ?outer? clauses.
Table 1 shows the occurrence frequency of sample nouns and the number of
their cooccurring verbs in the relative clauses or in the case-slot relations. For
nouns that do not tend to be modified by ?outer? clauses, such as ????(people),
???? (city), and ?????(television), the ratio between the frequency and the
number of verbs is almost the same between the relative clause and case-slot
cases. On the contrary, for nouns that tend to be modified by ?outer? clauses,
such as ????(intent), ???? (fact), and ????(preparation), the number of
verbs is much bigger in relation to clause cases, although the frequency is smaller.
The reason may be, as previously explained, that some verbs cooccur with the
nouns that tend to be modified by the ?outer? clause only in relative clause
constructions.
50 T. Abekawa and M. Okumura
Table 2. ?outerness? of example nouns
We use the Jensen-Shannon distance rather than the Kullback-Leibler distance,
because the former is symmetric and has stability in various sizes of probability
distribution experimentally. Pk(v|n) and Pm(v|n) are calculated as follows:
Pk(v|n) =
fk(n, v)
fk(n)
, (3)
Pm(v|n) =
fm(n, v)
fm(n)
, (4)
where fk(n, v) is the cooccurrence frequency where noun n fills a case-slots of
verb v, and fk(n) is the frequency of the noun that occurs in the case-slot
of verbs. Similarly, fm(n, v) and fm(n) are the frequencies for relative clause
constructions. Table 2 shows the ?outerness? of sample nouns. The values of the
nouns that are often modified by ?outer? clauses are higher than those of the
nouns which tend to be modified by ?inner? clauses.
2. Cooccurrence information between head noun and main verb of
relative clause.
For a relative clause instance to be an ?inner? clause, the head noun has to fill
a case-slot of the main verb of the relative clause. Consider the following two
examples:
Whether a noun can fill a case-slot of a verb has been traditionally determined
using case frames. However, we use the cooccurrence information between the
head noun and the main verb. In this paper, the cooccurrence between nouns and
verbs is measured by mutual information. Taking into account the information
on case particles, mutual information is calculated with the following formula:
I(n, k; v) = log
p(n, k, v)
p(n, k)p(v)
, (5)
noun ?? ?? ?? ?? ?? ???
(intent) (fact) (preparation) (people) (city) (television)
J(Pk, Pm) 0.546 0.360 0.616 0.160 0.155 0.159
(a) ???? ?
resonate sound
?the soundi that ?i resonates?
(b)???? ?
destruct sound
?the destruction sound?
In (a), ??? (sound) can be the subject (??? case) of the main verb ??????
(resonate). On the contrary, in (b) ??? cannot fill any case-slots of the main
verb ?????? (destruct) and can be considered to be modified by the ?outer?
relative clause. Therefore, if the head noun can fill a case-slot of the main verb,
the relation can be more plausibly assessed as ?inner?.
Corpus-Based Analysis of Japanese Relative Clause Constructions 51
3. Which case-slots are already filled for main verb by nouns in relative
clause.
As previously explained, if the head noun can fill the case-slot of the main verb of
the relative clause, the RCC instance can be judged as an ?inner? clause. However,
if the case-slot that the head noun can fill is already filled by the noun in the
relative clause, and hence unavailable for case-slot gapping, the rule cannot be
applied. Consider, for example, the following two cases:
Taking the situation into account, if a noun in the relative clause fills a case-
slot of the main verb, the mutual information for the case-slot is set to a very
small value Mmin.
4. Whether the head noun is modified by modifiers other than the
relative clause (other modifier).
Previous work on analyzing Japanese relative clauses has taking into account
only the head noun, and has not taking into account modifiers other than the
relative clause. Consider the following two examples:
where p(n, k) is the probability that noun n will cooccur with case particle k
and p(n, k, v) is the cooccurrence probability for noun n, case particle k and verb
v, and p(v) is the occurrence probability for verb v. The following seven case
particles were taken into account: (???,???,???,???,???,???, and ????).
This is because only these case-slots can, in fact, be gapped to the head noun to
construct the relative clause.
(a)????? ?
hear story
?the storyi that (someone) heard ?i?
(b)??? ????? ?
Japanese comic story hear story
?the story that (someone) heard a Japanese comic story?
In (a), since ??? (story) can fill the object (??? case) case-slot of the main verb
???? (hear), the relation can be judged as ?inner?. However, in (b), since the
object (??? case) case-slot of the main verb ???? is already filled by the noun
???? (Japanese comic story), and ??? cannot fill any case-slot, the instance
is judged as ?outer?.
(a)?? ?? ??
him talk purpose
?the purpose that (someone) talk (something) to him?
?the purposei that (someone) talk ?i to him?
(b)?? ?? ??? ??
him talk trip purpose
?the purpose of the trip i that (I) talk ?i to him?
(a) has two interpretations. The first interpretation is that ???? (purpose) do
not fill the remaining case-slots of the main verb ???? (talk) and can be con-
52 T. Abekawa and M. Okumura
If the head noun is modified by modifiers other than the relative clause, such
as adjectives, compound nouns, and ?AB?(B of A), the relative clause type tends
to be ?inner?. The function of ?outer? relative clause describes the content of the
head noun. If the head noun is modified by a modifier, the relative clause need
not describe it. Therefore, the type of relative clause tends to be ?inner?.
To implement the idea, we define a feature ?other modifier?. If the head
noun is modified by any modifiers other than the relative clause, its value is 1,
otherwise, 03.
5. Whether head noun tends to be modified
As for the nouns which tend to be modified by ?outer? relative clauses, the relative
clauses describe the content of the head nouns. It is difficult to understand their
meaning without any modification. Therefore we calculate the percentage to
what degree nouns are modified by any modifier in large corpora. Table 3 shows
the percentage for example nouns.
Table 3. Percentage of modification
3 In the experiment, we use syntactic annotated corpus. Therefore, other modifier
elements are already identified.
sidered to be modified by the ?outer? relative clause. The second interpretation
is that ???? can be the direct object(??? case) of the main verb ???? and
can be considered to be modified by the ?inner? relative clause. On the contrary,
(b) has only the interpretation of ?inner?.
?? ?? ??? ? Average of
(intention) (field) (television) (he) all nouns
0.983 0.973 0.287 0.155 0.460
The percentages of nouns ????(intention) and ????(field), which tend to
be modified by ?outer? relative clause, are close to 1, that is to say, such nouns
must have any modification. We consider, the higher this percentage, the higher
the possibility that the noun is modified by ?outer? relative clause.
6. Percentage where ????? is inserted between relative clause and
head nouns
????? is a function expression that is sometimes inserted between relative
clauses and head nouns. Table 4 shows the percentage where ????? cooccurs
with example nouns.
?? ? ?? ?? Average of
(opinion) (rumor) (place) (people) all nouns
0.335 0.246 0.007 0.008 0.007
Table 4. The percentage of ????? cooccurring with noun
Corpus-Based Analysis of Japanese Relative Clause Constructions 53
7. Whether head noun tends to be modified by past-tensed relative
clauses(tense information)
Table 5. Tense of main verb and distribution of inner/outer
To implement this idea, we first calculated deviations in the distribution of
tense for the relative clauses. The percentage of past-tense main verbs in all
relative clauses, Rpast, and the average for all the nouns were calculated. Table
6 shows the results for sample nouns.
Table 6. Percentage of past-tense main verbs
4 In Japanese, there are just two tense surface markers: present and past. Therefore,
future tense is indicated by the present tense on the surface.
The percentages of nouns ????(opinion) and ???(rumor), which tend to
be modified by ?outer? relative clause, are higher than the average. We consider,
the higher this percentage, the higher possibility that the noun is modified by
?outer? relative clause.
Some nouns tend to be modified by past-tense relative clauses, and others
tend to be modified by those in the present tense. Consider, for example, the
following two nouns: ????(plan) and ???? (memory). Both nouns are con-
sidered to imply the concept of time (future or past) 4 .
?? ??
(plan) (memory)
tense inner outer inner outer
present 6 89 12 0
past 5 0 5 83
For each of the two nouns ????(plan) and ????(memory), we examined
100 relative clause instances that had the noun as the head noun (Table 5).If
the head noun implies the concept of time, the tense of the main verb of the
relative clause tends to coincide with this concept. Furthermore, note that the
tense of the main verb of ?outer? relative clauses is the same as the time concept
of the head noun. From this, if the noun tends to be modified by a specific-tense
relative clause, the relative clause tends to be ?outer?, and if the tense of the
main verb contradicts the time concept of the head noun (tense of frequently
modified relative clauses), the relative clause should be determined as ?inner?.
?? ?? ?? ?? Average of
(plan) (memory) (place) (people) all nouns
0.032 0.958 0.333 0.422 0.322
For a head noun which does not imply the concept of time (???? (place) and
????(people)), the percentage is near average. On the contrary, ????(plan)
and ????(memory) which imply the concept of time have an extreme value.
54 T. Abekawa and M. Okumura
Taking into account the actual tense of the relative clause instances, we
calculated the following score:
Vpast
{
Rpast ? AV Gpast in case of present tense
AV Gpast ? Rpast in case of past tense
(6)
For a head noun not implying the concept of time, in either tense of the main
verb, the score is rather low, and a decision on inner/outer might not be af-
fected by the score. For a head noun implying the concept of time, the ab-
solute value of the score is rather large, and if the tense of the main verb is
the same as the time concept, the score becomes negative; otherwise the score
becomes positive.
8. Whether main verb has a sense of ?exclusion?
The last feature is for identifying exceptional ?outer? relative clause. Consider
the following two examples:
5 Machine Learning Based Classifier for RCC Type
We integrated the eight features in described the last section and used the ma-
chine learning approach to determine the RCC type. We used C5.0[13] as the
machine learning algorithm.
C5.0 is a decision-tree based classification system that has been used in nat-
ural language processing, such as text classification, chunking, text summariza-
tion, and ellipsis resolution[10]. C5.0 takes a set of training instances with a
feature vector and correct type as input, and induces a classifier which charac-
terizes the given feature space.
Since we use only eight features, we think even the state of the art machine
learning method like SVM would yield almost the same accuracy as decision-tree.
Furthermore decision-tree are more easily interpreted by human than SVMs.
(a)??? ?? ?????
Japan except Asian countries
?Asian countries except Japan?
(b)???? ??? ??
injured people except passenger
?the passenger except injured people?
These examples are ?outer? relative clauses, and this RCC type is identified by
the main verb which has sense of exclusion. There are a few verbs which indicate
the RCC type by itself. Therefore, we defined a feature ?excluding verb?. If the
main verb contains a character ??? (which has sense of exclusion), the feature
is set to 1, otherwise, 0.
Corpus-Based Analysis of Japanese Relative Clause Constructions 55
6 Evaluation
6.1 Experiment
Cooccurrence and other statistical information used in this work were calculated
from the corpus of a collection of twenty-two years of newspaper articles. The
corpus was parsed with KNP[8], which is a rule-based Japanese syntactic parser.
The cooccurrence information we obtained was as follows: the number of fk(n, v)
was about 60.8 million, and the number of fm(n, v) was about 12.4 million.
The data used in the evaluation was a set of RCC instances randomly ex-
tracted from the EDR corpus[4] which had syntactically analyzed. Then, a label,
whether the relative clause is ?inner? or ?outer?, was manually annotated. The
statistics on the data are shown in Table 7. Evaluation with C5.0 was carried
out by way of 5-fold cross validation.
Table 7. Statistics on evaluation data
Total Inner Outer
749 580 169
Table 8. Experimental results
Inner Outer
accuracy precision recall precision recall
Baseline 0.774 0.774 1.000 - -
Cooccurrence information only 0.787 0.836 0.906 0.520 0.366
Case frame 0.830 0.868 0.921 0.657 0.521
Our approach 0.902 0.931 0.942 0.794 0.762
Fig. 1. Generated decision tree
...excluding verb = 1: outer(exceptinal type) (22/2)
:.excluding verb = 0:
:..outerness <= 0.212: inner (444/6)
outerness > 0.212:
:..other modifier = 1: inner (84/17)
other modifier = 0:
:..cooccurrence("?" case) > -9.10: inner (28/4)
cooccurrence("?" case) <= -9.10:
:..percentage of "???" > 0.027: outer (105/14)
percentage of "???" <= 0.027:
:..percentage of modified <= 0.735: inner (25/2)
percentage of modified > 0.735:
:..cooccurrence("?" case) <= -13.1:outer (31/5)
cooccurrence("?" case) > -13.1:inner (10/2)
56 T. Abekawa and M. Okumura
The baseline we used determines all instances as ?inner? relative clauses. We
also compared our approach with the traditional method with case frames, and
a method that uses only cooccurrence information (features 2 and 3 in section 4.
An evaluation measure is an accuracy, which is defined as the number of correctly
identified RCCs divided by the number of all RCCs. And for inner/outer relative
clauses, precision and recall are calculated.
Precision =
#number of correctly identified relative clauses
#number of inner/outer attempted by system
Recall =
#number of correctly identified relative clauses
#number of inner/outer relative clauses
The results are shown in Table 8. The generated decision tree from all instances
is shown in Figure 1. The last values on each line, for example ?22/2? and ?444/6?,
indicated ?number of applied examples / number of misclassification?.
6.2 Discussion
Accuracy of our approach is higher than that of the traditional approach. Our
approach works well especially for identifying ?outer? relative clause. Further-
more, using only cooccurrence information could not yield better performance
for ?outer? relative clause. Therefore, we conclude that the features in our ap-
proach can effectively identify the ?outer? relative clause.
Figure 1 shows that the most contributive feature except ?excluding verb?
is the degree of ?outerness?. This feature can classify many instances with high
accuracy (98.6%=438/444). If the degree of ?outerness? is smaller than certain
threshold, RCC type is ?inner? with high probability.
The second contributing feature is the ?other modifier?. If modifiers other
than the relative clause exist, RCC type is ?inner?. However, the accuracy of this
feature is not so good compared with other features.
We unfortunately could not find the ?tense information? in our decision tree.
We consider the reason to be that nouns which imply the concept of time are
very few, and there might be no instances which contain them.
7 Conclusions
In this paper, we presented eight lexical and semantic features that characterized
RCC, and we integrated them using machine learning approach to determine the
RCC type.
Evaluation proved that our approach outperformed the traditional case
frame-based method, and the features that we presented were effective in classi-
fying types into ?inner? and ?outer? relative clauses.
After identification of ?inner? clauses, case identification will be necessary for
semantic analysis. This will be considered in future work.
Corpus-Based Analysis of Japanese Relative Clause Constructions 57
References
1. Baldwin, T., Tokunaga, T. and Tanaka, H.: The parameter-based analysis of
Japanese relative clause constructions. In IPSJ SIGNote on Natural Language 134-
8 (1999) 55-62
2. Baldwin, T.: Making Sense of Japanese Relative Clause Constructions. In Proceed-
ings of the Second Workshop on Text Meaning and Interpretation (2004) 49-56.
3. Dagan, I., Lee, L. and Pereira, F.: Similarity-based models of word cooccurrence
probabilities. Machine Learning 34 (1999) 65-81
4. EDR.: EDR electronic dictionary technical guide. Technical Report TR045,
Japanese Electronic Dictionary Research Institute Ltd (1995)
5. Ikehara, S., Shirai, S., Yokoo, A. and Nakaiwa, H.: Toward an MT system with-
out pre-editing effect of new methods in ALT-J/E . In Proceedings of the Third
Machine Translation Summit (1991)
6. Ishizako, T., Kataoka, A., Masuyama, S., Yamamoto, K. and Nakagawa, S.: Re-
duction of overlapping expressions using dependency relations. Natural Language
Processing 7(4) (2000) 119-142. (in Japanese)
7. Kawahara, D. and Kurohashi, S.: Fertilization of case frame dictionary for robust
Japanese case analysis. In Proceedings of the 19th International Conference on
Computational Linguistics (2002) 425-431
8. Kurohashi, S. and Nagao, M.: Kn parser: Japanese dependency/case structure ana-
lyzer. In Proceeding of the International Workshop on Sharable Natural Language
Resources (1994) 48-55
9. Lin, J.: Divergence measures based on the shannon entropy. IEEE TRANSAC-
TIONS ON INFORMATION THEORY. 37(1) (1991) 145-151
10. Manning, C. and Schutze, H.: Foundations of Statistical Natural Language Pro-
cessing. MIT Press (1999)
11. Murata, M.: Extraction of negative examples based on positive examples automatic
detection of mis-spelled Japanese expressions and relative clauses that do not have
case relations with their heads . In IPSJ SIGNote on Natural Language 144-15
(2001) 105-112. (in Japanese)
12. Narita, H.: Parsing Japanese clauses modifying nominals. In IPSJ SIGNote on
Natural Language 99-11 (1994) 79-86. (in Japanese)
13. Quinlan, J.: C4.5: Programs for Machine Learning. Morgan Kaufmann (1993)
14. Teramura, H.: Rentai-shuushoku no shintakusu to imi. No.1-4. Nihongo Nihon-
bunka 4-7 (1975-1978) (in Japanese)
What Prompts Translators to Modify Draft Translations?
An Analysis of Basic Modification Patterns for Use in
the Automatic Notification of Awkwardly Translated Text
Takeshi Abekawa and Kyo Kageura
Library and Information Science Course
Graduate School of Education,
University of Tokyo
{abekawa,kyo}@p.u-tokyo.ac.jp
Abstract
In human translation, translators first make
draft translations and then modify them.
This paper analyses these modifications, in
order to identify the features that trigger
modification. Our goal is to construct a sys-
tem that notifies (English-to-Japanese) vol-
unteer translators of awkward translations.
After manually classifying the basic modifi-
cation patterns, we analysed the factors that
trigger a change in verb voice from passive
to active using SVM. An experimental re-
sult shows good prospects for the automatic
identification of candidates for modification.
1 Introduction
We are currently developing an English-to-Japanese
translation aid system aimed at volunteer transla-
tors mainly working online (Abekawa and Kageura,
2007), As part of this project, we are developing a
module that notifies (inexperienced) translators of
awkwardly translated expressions that may need re-
finement or editing.
In most cases, translators first make draft trans-
lations, and then examine and edit them later, often
repeatedly. Thus there are normally at least two ver-
sions of a given translation, i.e. a draft and the final
translation. In commercial translation environments,
it is sometimes the case that texts are first translated
by inexperienced translators and then edited by ex-
perienced translators. However, this does not ap-
ply to voluntary translation. In addition, volunteer
translators tend to be less experienced than commer-
cial translators, and devote less time to editing. It
would therefore be of great help to these translators
if the CAT system automatically pointed out awk-
ward translations for possible modification. In order
to realise such a system, it is necessary to first clarify
(i) the basic types of modification made by transla-
tors to draft translations, and (ii) what triggers these
modifications.
In section 2 we introduce the data used in this
study. In section 3, we clarify the nature of modifica-
tion in the translation process. In section 4, we iden-
tify the actual modification patterns in the data. In
section 5, focusing on ?the change from the passive
to the active voice? pattern, we analyse and clarify
the triggers that may lead to modification. Section 6
is devoted to an experiment in which machine learn-
ing methods are used to detect modification candi-
dates. The importance of the various triggers is ex-
amined, and the performance of the system is evalu-
ated.
2 The data
The data used in the present study is the Japanese
translation of an English book about the problem of
peak oil (Leggett, 2005). The book is aimed at a
popular audience and is relevant to the sort of texts
we have in mind, because the majority of texts vol-
unteer translators translate deal with current affairs,
social issues, politics, culture and sports, and/or eco-
nomic issues for a popular audience1. The data con-
sists of the English original (henceforth ?English?),
the draft Japanese translation (?Draft?) and the fi-
nal Japanese translation (?Final?). The ?Draft? was
made by two translators (one with two years? experi-
ence and the other with five years? experience), and
1Software localisation is another area of translation in which
volunteers are heavily involved. We do not include it in our
target because it has different characteristics.
241
?????? ?? ???? ? ? ?? ? ?? ? ??? ?? ? ??? ? ? ? ??? ? ???? ? ?? ? ??? ?? ? ?? ?? ? ?
????????? ? ?? ? ? ?? ?? ? ?? ? ?? ? ?? ? ? ? ??? ? ?? ?? ???? ? ??? ?? ?? ?? ?? ? ?
Figure 1: An example of word alignment using GIZA++
the ?Final? was made by a translator with 12 years?
experience. Table 1 gives the quantities of the data.
?English? ?Draft? ?Final?
Number of sentences 4,587 4,629 4,648
Number of words 92,300 127,838 132,989
(Average per sentence) 20.1 27.6 28.6
Table 1: Basic quantities of the data
3 Nature of the modification process
State Cause
1. Mistranslation English is complex
2. Text is confusing English is complex / Trans-
lation is too literal
3. Text is unnatural Translation is too literal /
Japanese is underexamined
4. Against modi-
fiers? taste
Different Japanese ?model?
is assumed
5. Against editorial
policy
Lack of surface editing
Table 2: States in the draft and their causes
As little research has been carried out into the pro-
cess by which translators modify draft translations,
we manually analysed a part of the data in which
modifications were made, in consultation with a
translator. In the modification process, the translator
first recognises (though often not consciously) one
of a number of states in a draft translation and the
underlying cause of the state. S/he then modifies the
draft translation if necessary. Table 2 shows the ba-
sic classification of states and possible causes. Al-
though the states are conceptually clear, it is not nec-
essarily the case that translators can judge the state
of a given translation consistently, because judging
a sentence as being ?natural? or ?confusing? is not
a binary process but a graded one, and the distinc-
tion between different states is often not immedi-
ately clear. Many concrete modification patterns
found in the data are covered in translation textbooks
(Anzai, 1995; Nakamura, 2003). However, although
it is obvious in some cases that a section of trans-
lated text needs to be modified, in other cases it is
less clear, and judgments will vary according to the
translator. The task that automatic notification ad-
dresses, therefore, is essentially an ambiguous one,
even though the actual system output may be binary.
We also identified the distinction between two
types of modification: (i) ?generative? modification,
in which the modified translation is generated on the
spot, with reference to the English original; and (ii)
?considered? modification, in which alternate ex-
pressions (phrases, collocations, etc.) are retrieved
from the depository of useful, elegant, or conven-
tional expressions in the translator?s mind. These
two types of modification can be activated in the face
of one token of modification at once.
4 Modification patterns
The most natural way to classify modification pat-
terns is by means of basic linguistic labels such as
?change of voice? or ?change from nominal modifi-
cation to adverbial modification? (cf. Anzai, 1995).
These modification patterns consist of one or more
primitive operations. For instance, a ?change of
voice? may consist of such primitive operations as
?changing the case-marker of the subject,? ?swap-
ping the position of subject and object,? etc.
As preparation, we extracted modification pat-
terns from the data2. In order to do so, we first
aligned the ?Draft? and the ?Final? at the sentence
level using DP matching, and then at the morpheme
level using GIZA++ (Och and Ney, 2003). Figure
1 illustrates an example of word/morpheme level
2This task is similar to the acquisition of paraphrase knowl-
edge (Barzilay and McKeown, 2001; Shinyama et al, 2002;
Quirk et al 2004; Barzilay and Lee, 2003; Dolan et al, 2004).
However, our aim here is to clarify basic modification patterns
and not automatic identification.
242
English: If it was perceived to be true by the majority of Thinkers, ...
?Draft?: ??? ?????? ??? ?????? ???????
JINRUI-NO TASUU-NIYOTTE SORE-GA SINJITU-DE-ARU-TO NINSIKI-SA-RERE-BA
(thinkersgenitive) (majorityablative) (itsubject) (to be true) (be perceived)
?Final?: ??? ??? ??? ??? ??????
JINRUI-NO TASUU-GA SORE-WO SINJITU-TO NINSIKI-SURE-BA
(thinkersgenitive) (majoritysubject) (itobject) (to be true) (perceive)
Primitive replace(?NIYOTTE?, replace(?GA?, delete(?DE?) delete(?RARERU?)
operations: ?GA?) ?WO?) delete(?ARU?)
Table 3: An example of a primitive modification operation
alignment. Changes in word order occur frequently,
as is shown in Figure 1, and the ?Final? and the
?Draft? are not completely parallel at the word or
morpheme level. As a result, GIZA++ sometimes
misaligns the units.
From the aligned ?Draft? and ?Final? data, we
identified the primitive operations. We limited these
operations to syntactic operations and semantic op-
erations such as the changing of content words, be-
cause the latter is hard to generalise with a small
amount of data. Primitive operations were extracted
by calculating the difference between correspond-
ing bunsetsu, which basically consist of a content
word and postpositions/suffixes, in the ?Draft? and
in the ?Final?. An example is given in Table 3. Ta-
ble 4 shows the five most frequent changes in verb
inflections and case markers, which are two domi-
nant classes of primitive operation. In addition, we
observed deletions and insertions of Sahen verbs.
Modification patterns were identified by observ-
ing the degree of co-occurrence among these prim-
itive operations. We used Cabocha3 to identify the
syntactic dependencies and used the log-likelihood
ratio (LLR) to calculate the degree of co-occurrence
of primitive operations that occupy syntactically de-
pendent positions. Table 5 shows the top five pair-
wise co-occurrence patterns.
inflection del. ins. case marker del. ins.
DA 379 291 NI 476 384
TE 269 358 GA 387 502
TA 247 306 NO 366 204
RARERU 224 122 WO 293 421
IRU 197 267 DE 203 193
Table 4: Frequent primitive operations
3http://chasen.org/?taku/software/cabocha/
Three main modification patterns were identified:
(i) a change from the passive to the active voice (226
cases); (ii) a change from a Sahen verb to a Sa-
hen noun (208 cases); and (iii) a change from nom-
inal modification to clausal structure. These pat-
terns have been discussed in studies of paraphrases
(Inui and Fujita, 2004) and in translation textbooks
(Anzai, 1995; Nakamura, 2003). We focus on ?the
change from the passive to the active voice?. It is
one of the most important and interesting modifica-
tion patterns because (i) it is mostly concerned with
the main clausal structure in which other modifica-
tions are embedded; and (ii) the use of active and
passive voices differs greatly between English and
Japanese and thus there will be much to reveal.
5 Triggers that lead to modification
Given a draft translation, an experienced translator
will be able to recognise any problematic states in it
(see Table 2), identify the causes of these states and
deal with them. As computers (and inexperienced
translators) cannot do the same (cf. Sun et al, 2007),
it is necessary to break these causes down into com-
putationally tractable triggers. Keeping in mind the
nature of the modification process discussed in sec-
tion 3, we analysed the actual data, this time with
the help of a translator and a linguist.
At the topmost level, two types of triggers were
identified: (i) ?pushing? triggers that are identified
as negative characteristics of the draft translation ex-
pressions themselves; and (ii) ?pulling? triggers that
come from outside (from the depository of expres-
sions in the translator?s mind) and work as concrete
?model translations?. The distinction is not entirely
clear, because a model is needed in order to iden-
tify negative characteristics, and some sort of neg-
ative impression is needed for the ?model transla-
tion? to be called up. The distinction is nevertheless
243
LLR f(a,b) f(a) f(b) operation a operation b plain expression
146.2 28 35 224 replace(NIYOTTE,GA) delete(RARERU) A NIYOTTE B SARERU?A GA B SURU
105.2 34 90 224 replace(GA,WO) delete(RARERU) A GA B SARERU?A WO B SURU
91.7 34 115 208 replace(NO,GA) delete(SAHEN) A NO B?A GA B SURU
90.9 26 61 208 replace(NO,WO) delete(SAHEN) A NO B?A WO B SURU
36.3 15 68 168 replace(NI,WO) intransitive?transitive A NI B SURU?A WO C SURU
Table 5: Five of the most frequent co-occurrence patterns between two primitive operations
important, both theoretically and practically. Theo-
retically, it corresponds to the types of modification
observed in section 3. From the practical point of
view, the first type is related to the general structural
modelling (in its broad sense) of language, while the
second is closely related to the status of individual
lexicalised expressions. Correspondingly, an NLP
system that addresses the first type needs to assume
a language model, while a system that addresses the
second type needs to call on the relevant external
data on the spot. We address the first type of trig-
ger, because we can hypothesise that the modifica-
tion by change of voice is mainly related to the struc-
tural nature of expressions. It should also be noted
that, from the machine learning point of view, there
are positive and negative features which respectively
promote and restrict the modification.
We classified the features that may represent po-
tential triggers into five groups:
(A) Features related to the readability of the En-
glish, because the complexity of English sentences
(cf. Fry, 1968; Gunning, 1959) can affect the qual-
ity of draft translations. Thus the number of words
in a sentence, length of words, number of verbs in
a sentence, number of commas, etc. can be used as
tractable features for automatic treatment.
(B) Features reflecting the correspondence be-
tween the English and the draft Japanese trans-
lation. Translations that are very literal, either lex-
ically or structurally, are often also awkward. On
the other hand, a high degree of word order corre-
spondence can be a positive sign (cf. Anzai, 1995),
because it indicates that the information flow in En-
glish is maintained and the Japanese translation is
well examined.
(C) Features related to the Japanese target verbs.
The characteristics of the target verbs should affect
the environments in which they occur.
(D) Features related to the ?naturalness? of the
Japanese. Repetitions or redundancies of elements
or sound patterns may lead to unnatural Japanese
sentences.
(E) Features related to the complexity of the
Japanese. If a draft translation is too complex, it
may be confusing or hard to read. Structural com-
plexity, the length of a sentence, the number of com-
mas, etc. can be used as triggers that reflect the com-
plexity of the Japanese translation.
Table 6 shows the computationally tractable fea-
tures we defined within this framework. Features
with ?#? in their name are numeric features and the
others are binary features (taking either 0 or 1).
6 Detecting modification candidates
Using these features, we carried out an experiment
of automatic identification of modification candi-
dates. As a machine learning method, we used
SVM (Vapnik, 1995). The aim of the experiment
was twofold: (i) to observe the feasibility of auto-
matic notification of modification candidates, and
(ii) to examine the factors that trigger modifications
in more detail.
6.1 Experimental setup
In the application of SVM, we reduced the number
of binary features by using those that have higher
correlations with positive and negative examples, us-
ing mutual information (MI). Table 7 shows features
that have high correlations with positive and nega-
tive features (eight for each).
SVM settings: The liner kernel was used. For a
numeric feature X , the value x is normalized by z-
score, norm(x) = x?avg(X)?
var(X)
, where avg(x) is the
empirical mean of X and var(X) is the variance of
X.
Data: The numbers of positive and negative cases
in the data are 226 and 894, respectively (1120 in
total). In order to balance the positive and negative
examples, we used an equal number of examples for
training.
244
(A)
EN#word: the number of words in the English sentence
EN#pause: the number of delimiters in the English sen-
tence
EN#verb: the number of verbs in the English sentence
EN#VVN: the number of VNN verbs in the English sen-
tence
EN#word len: the average number of characters in a word
(B)
EPOS: POS of the English word corresponding
to the target Japanese verb
EPOS before: POS of a word before the English word
corresponding to the target Japanese verb
EPOS after: POS of a word after the English word cor-
responding to the target Japanese verb
EPOS before:POS : a bigram of EPOS before and EPOS
EPOS:POS after: a bigram of EPOS and EPOS after
EJ#translation: translation probability between the
source and target language sentences
(C)
Fsuffix: a suffix following the target verb
Fparticle: a particle following the target verb
Fpause park: a pause mark following the target verb
Dmodifying case: case marker of the element that modifies the
target verb
Dmodifying agent: case marker of the element that modifies the
target verb, if its case element has an AGENT
attribute
Dfunctional: functional noun which is modified by the tar-
get verb
Dmodified case: case marker of the element that is modified by
the target verb
Sfirst agent: first case element in the sentence has an
AGENT attribute
Sbefore passive: Is there a passive verb before the target
verb in the sentence?
Safter passive: Is there a passive verb after the target verb
in the sentence?
(D)
Nmodifying voice: the voice of the verb that modifies the
target verb
Nmodifying voice: the voice of the verb that is modified
by the target verb
Ngrandparent voice: the voice of the grandparent verb of
the target verb
Ngrandchild voice: the voice of the grandchild verb of the
target verb
Ncase adjacency; bigram consists of a particle of the tar-
get verb and a particle of the adja-
cency bunsetsu chunk
(E)
J#morpheme: the number of morphemes in the target
Japanese sentence
J#pause: the number of pause marks in the target
Japanese sentence
J#verb: the number of verbs in the target Japanese
sentence
J#passive: the number of verbs with passive voice in the
target Japanese sentence
J#depth: depth of the modifier which modifies the tar-
get verb
Table 6: Features
Methods of evaluation: We used (i) 10-fold cross
validation to check the power of classifiers for un-
known data and (ii) a partially closed test in which
the 226 positive and negative examples were used
for training and 1120 data were evaluated, in order
to observe the realistic prospects for actual use.
6.2 Result of experiment and feature analysis
Table 8 shows the results. Though they are reason-
able, the overall accuracy, especially for the partially
closed test, shows that the method is in need of im-
provement.
In order to evaluate the effectiveness of the fea-
ture sets, we carried out experiments only using and
without using each feature set. Table 9 shows that
how efficient is each feature set defined in Table 6.
The left-hand column in Table 9 shows the result
with all feature sets except focal feature set, and the
right-hand column shows the result when only the
focal feature set was used.
The experiment showed that the feature set that
contributed most was C (features related to the
Japanese target verbs). We also carried out an exper-
iment to check which features are effective among
this set, in the same manner as the experiments for
checking the effectiveness of the feature sets. The
result showed that the feature Dmodifying case is the
feature that contributed the most by far. In Japanese,
case markers are strongly correlated with the voice
of verbs, and the coverage of this feature for tokens
related to voice is high because it is common for a
verb to be modified by the case element with the case
marker.
It became clear that the numeric features A and
E contribute little to the overall accuracy. Table 10
shows the correlation coefficient between the nu-
meric features and correct answers. The table shows
that there is no noticeable relation between the nu-
245
accuracy (+)precision (+)recall (-)precision (-)recall
Cross validation 0.646 (291/452) 0.656 (138/214) 0.614 (138/226) 0.643 (153/238) 0.677 (153/226)
Partially closed 0.521 (583/1120) 0.277 (193/697) 0.854 (193/226) 0.922 (390/423) 0.436 (390/894)
Table 8: The accuracy of classification
without this feature set using only this feature set
feature set accuracy (+)precision (+)recall accuracy (+)precision (+)recall
(A) 0.638 0.638 (144/226) 0.639 (144/226) 0.521 0.541 (62/115) 0.277 (62/226)
(B) 0.634 0.649 (132/203) 0.584 (132/226) 0.563 0.549 (159/290) 0.705 (159/226)
(C) 0.579 0.576 (136/237) 0.604 (136/226) 0.610 0.620 (128/207) 0.570 (128/226)
(D) 0.645 0.654 (138/212) 0.615 (138/226) 0.523 0.679 (19/29) 0.087 (19/226)
(E) 0.629 0.666 (117/175) 0.518 (117/226) 0.492 0.491 (101/205) 0.447 (101/226)
Table 9: The evaluation result for each feature set
feature MI f(+) f(-)
Dmodifying agent=NIYOTTE 0.843 15 17
EPOS:POS after=VVN:NN 0.656 14 22
EPOS before=IN 0.536 10 19
EPOS before=JJ 0.530 12 23
Dmodified case=GA 0.428 13 29
Ngrandparent voice=passive 0.408 17 39
Ngrandchild voice=passive 0.368 14 34
EPOS=VVZ 0.368 14 34
Fsuffix=NARU 0.225 0 23
Ncase adjacency=GA:TO 0.225 0 12
Fsuffix=SHIMAU 0.225 0 16
EPOS=RB 0.225 0 10
EPOS:POS after=VVG:DT 0.225 0 10
EPOS:POS after=VVN:TO 0.179 2 42
EPOS:POS after=VVN:SENT 0.159 3 44
Dmodifying agent=NI 0.154 4 54
Table 7: Features which have high correlation with
positive and negative examples
meric features and the correct results. We introduced
most numeric features based on the study of read-
ability. In readability studies, however, these fea-
tures are defined in terms of the overall document,
and not in terms of individual sentences or of verb
phrases. It would be preferable to develop numer-
ical features that can properly reflect the nature of
individual sentences or smaller constructions.
Table 9 shows that the result when only using the
feature set D has a very low recall, but the highest
feature set (A)
EN#word 0.038
EN#pause -0.069
EN#verb -0.003
EN#VVN -0.061
EN#word len 0.033
feature set (E)
J#morpheme 0.083
J#pause 0.011
J#verb 0.056
J#passive 0.035
J#depth 0.098
Table 10: The correlation coefficient between each
feature and correct answer
precision of all the feature sets. This mean that there
are not many occasions on which the feature set D
can be applied, but when it is applied, the result is re-
liable. The feature set D thus is efficient as a trigger
once it is applied, and the different treatment of the
tokens that contain this feature set may contribute to
the performance improvement.
6.3 Diagnosis
The critical cases from the point of view of improv-
ing the performance are the false positives and false
negatives. We thus manually analysed the false pos-
itives and false negatives obtained in the partially
closed experiment (in the actual application envi-
ronment, as much training data as available should
be used; we thus used the results of the partially
246
closed experiment here). For the false positive, we
extracted 100 sample sentences from 504 sentences.
For the false negative we used all 33 sentences. We
asked two translators to judge whether (i) it would be
better to modify the draft translations or (ii) it would
not be necessary to modify the draft translations.
6.3.1 False positives
From the 100 sample sentences, we excluded 23
cases, 18 of which were judged as in need of mod-
ification by one of the translators and 5 of which
were judged as in need of modification by both of
the translators. We manually analysed the remaining
77 cases. Rather than the problems with the features
that we used, we identified the potential factors that
would contribute to the restriction of modification.
Three types of restricting factor were recognised:
1. The nature of individual verbs allows or re-
quires the passive voice. Within the data, three
subtypes were identified, i.e. (i) the use of the
passive is natural irrespective of context, as in ?
???? (consumed)? (48 cases); (ii) the use of
the passive is natural within certain fixed syn-
tactic patterns, as in ?X ????? Y (Y called
X)? (10 cases); and (iii) the passive is used as
part of a common collocation, as in ??????
?? (attacked by anxiety)? (2 cases);
2. The use of the active voice is blocked by selec-
tional restrictions, as in ???????? (a sedi-
ment made by ...)? (1 case); and
3. The structure of the sentence requires the pas-
sive, as in ??????????????????
????????????????????? (The
biggest companies were all companies making
cars, in which most of the oil was consumed)?
(16 cases).
Together they cover 73 cases (in 4 out of 77 cases
we could not identify the factor, and in 4 of the 73
cases two of the above factors were identified). It is
anticipated that the first type (60 cases; about 85%)
could be dealt with by introducing ?pulling? trig-
gers, i.e. using large corpora to identify the char-
acteristics of the use of voice for individual verbs,
in order to enable the system to judge the desirabil-
ity of given expressions vis-a`-vis the conventional
alternatives. To deal with the second type requires
a detailed semantic description of nouns, which is
difficult to achieve, though in some cases it could
be approximated by collocational tendencies. In
regards to the third type of false positive, we ex-
pected that the type of features used in the experi-
ment would have been sufficient to eliminate them,
but this was not the case. In fact, many of the fea-
tures require discourse level information, such as the
choice of subject within the flow of discourse, in or-
der to function properly, which we did not take into
account. Although high-performance discourse pro-
cessing is still in an embryonic stage, in the setting
of the present study the correspondence between key
information in English and that in Japanese could be
used to deal with this type of false positive.
6.3.2 False negatives
Here, it is necessary to find factors that would pro-
mote modification. Among the 33 false negatives, 4
were judged as not in need of modification by both
the translators. We thus examined the remaining 29
cases. In 13 cases, the verb was replaced by another
verb. Including these cases, we identified four basic
factors that are related to triggering modification:
1. The nature of the individual verbs strongly re-
quires the active voice, either independently or
within the particular context, as in ??????
???? (was asked by)? (9 cases);
2. The structure of the sentence is rendered rather
awkward by the use of passives, as in ?????
??????????????????? (a report
published in ...... by analysts)? (4 cases);
3. A given lexical collocation is unnatural or awk-
ward, as in ???????????????????
??????? (that all investments be screened
is collectively insisted)? (2 cases); and
4. A lexicalised collocation in the draft was sub-
tly awkward and there is a better collocation or
expression that fits the situation (14 cases).
Together they cover 26 cases. We could not iden-
tify features in 3 cases. As in false positives, the first,
second and fourth types (22 cases or about 85% are
fully covered by these three types) could be dealt
with by introducing ?pulling? triggers, using large
external corpora.
For the overall data, we would expect that around
85% of 388 (77% of 504 cases) false positives (330
247
cases) could be dealt with by introducing ?pulling?
triggers. If these false positives could be removed
completely, the precision would become well over
0.5 (193/(697-330)) and the ratio of notified cases
would become about one third ((697-330)/1120) of
the total relevant cases. Though it is unreasonable to
assume this ideal case, this indicates that the fea-
tures we defined and introduced in this study ?
though limited to those related to ?pushing? triggers
? were effective, and that what we have achieved
by using these features is very promising in terms
of realising a system that notifies users of awkward
translations.
7 Conclusions
In this paper, we examined the factors that trig-
ger modifications when translators are revising
draft translations, and identified computationally
tractable features relevant to the modification. We
carried out an experiment for automatic detection
of modification candidates. The result was highly
promising, though it revealed several issues that
need to be addressed further.
Following the results reported in this paper, we
are currently working on.
(i) extending the experiment by introducing out-
side data to carry out open experiments (we
have obtained draft and final translations of
three more books);
(ii) introducing the degree of necessity for modifi-
cations by asking translators to judge the data;
and
(iii) further examining the features used in the ex-
periment for the improvement of performance.
In addition, we are experimenting with a method for
making use of large-scale external corpora in order
to deal with ?pulling?-type triggers, with additional
features taken from large external corpora.
Acknowledgement
This research is partly supported by grant-in-aid (A)
17200018 ?Construction of online multilingual ref-
erence tools for aiding translators? by the Japan
Society for the Promotion of Sciences (JSPS), and
also by grant-in-aid from The HAKUHO FOUNDA-
TION, Tokyo.
References
Abekawa, T. and Kageura, K. 2007. A translation aid
system with a stratified lookup interface. In Proc. of
ACL 2007 Demos and Poster Sessions, p. 5?8.
Anzai, T. 1995. Eibun Hon?yaku Jutu (in Japanese).
Tokyo: Chikuma.
Barzilay, R. and McKeown, K. R. 2001. Extracting para-
phrases from a parallel corpus. In Proc. of ACL 2001,
p. 50-57.
Barzilay, R. and Lee, L. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In Proc. of HLT-NAACL 2003, p. 16-23.
Dolan, B. et. al. 2004. Unsupervised construction of
large paraphrase corpora: Exploiting massively paral-
lel news sources alignment. In Proc. of COLING 2004,
p. 350-356.
Fry, E. 1968. A readability formula that saves time.
Journal of Reading, 11, p. 513-516, 575-578.
Gunning, R. 1959. The Technique of Clear Writing.
New York: McGraw-Hill.
Haruno, M. and Yamazaki, T. 1996. High-performance
bilingual text alignment using statistical and dictionary
information. In Proc. of ACL 1996, p. 131-138.
Inui, K. and Fujita, A. 2004. A survey on paraphrase
generation and recognition. Journal of Natural Lan-
guage Processing, 11(5), p. 131-138.
Leggett, J. 2005. Half Gone. London: Portobello. [Ma-
suoka, K. et. al. trans. 2006. Peak Oil Panic. Tokyo:
Sakuhinsha.]
Nakamura, Y. 2003. Eiwa Hon?yaku no Genri Gihou (in
Japanese). Tokyo: Nichigai Associates.
Och, F. J. and Ney, H. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1), p. 19-51.
Quirk, C., Brocktt, C. and Dolan, W. B. 2004 Monolin-
gual machine translation for paraphrase generation. In
Proc. of EMNLP 2004, p. 142-149.
Schmid, H. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proc. of NeMLAP, p. 44-49.
Shinyama, Y. et. al. 2002. Automatic paraphrase acqui-
sition from news articles. In Proc. of HLT 2002, p.
40-46.
Sun, et. al. 2007. Detecting erroneous sentences using
automatically mined sequential patterns. In Proc. of
ACL 2007, p. 81-88.
Vapnik, V. N. 1995. The Nature of Statistical Learning
Theory. New York: Springer.
248
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 833?840,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Japanese Dependency Parsing Using Co-occurrence Information and a
Combination of Case Elements
Takeshi Abekawa
Graduate School of Education
University of Tokyo
abekawa@p.u-tokyo.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
In this paper, we present a method that
improves Japanese dependency parsing by
using large-scale statistical information. It
takes into account two kinds of informa-
tion not considered in previous statistical
(machine learning based) parsing meth-
ods: information about dependency rela-
tions among the case elements of a verb,
and information about co-occurrence re-
lations between a verb and its case ele-
ment. This information can be collected
from the results of automatic dependency
parsing of large-scale corpora. The results
of an experiment in which our method was
used to rerank the results obtained using an
existing machine learning based parsing
method showed that our method can im-
prove the accuracy of the results obtained
using the existing method.
1 Introduction
Dependency parsing is a basic technology for pro-
cessing Japanese and has been the subject of much
research. The Japanese dependency structure is
usually represented by the relationship between
phrasal units called bunsetsu, each of which con-
sists of one or more content words that may be
followed by any number of function words. The
dependency between two bunsetsus is direct from
a dependent to its head.
Manually written rules have usually been used
to determine which bunsetsu another bunsetsu
tends to modify, but this method poses problems in
terms of the coverage and consistency of the rules.
The recent availability of larger-scale corpora an-
notated with dependency information has thus re-
sulted in more work on statistical dependency
analysis technologies that use machine learning al-
gorithms (Kudo and Matsumoto, 2002; Sassano,
2004; Uchimoto et al, 1999; Uchimoto et al,
2000).
Work on statistical Japanese dependency analy-
sis has usually assumed that all the dependency re-
lations in a sentence are independent of each other,
and has considered the bunsetsus in a sentence in-
dependently when judging whether or not a pair
of bunsetsus is in a dependency relation. In judg-
ing which bunsetsu a bunsetsu modifies, this type
of work has used as features the information of
two bunsetsus, such as the head words of the two
bunsetsus, and the morphemes at the ends of the
bunsetsus (Uchimoto et al, 1999). It is necessary,
however, to also consider features for the contex-
tual information of the two bunsetsus. One such
feature is the constraint that two case elements
with the same case do not modify a verb.
Statistical Japanese dependency analysis takes
into account syntactic information but tends not to
take into account lexical information, such as co-
occurrence between a case element and a verb.
The recent availability of more corpora has en-
abled much information about dependency rela-
tions to be obtained by using a Japanese depen-
dency analyzer such as KNP (Kurohashi and Na-
gao, 1994) or CaboCha (Kudo and Matsumoto,
2002). Although this information is less accu-
rate than manually annotated information, these
automatic analyzers provide a large amount of
co-occurrence information as well as information
about combinations of multiple cases that tend to
modify a verb.
In this paper, we present a method for improv-
ing the accuracy of Japanese dependency analy-
sis by representing the lexical information of co-
occurrence and dependency relations of multiple
cases as statistical models. We also show the re-
sults of experiments demonstrating the effective-
ness of our method.
833
Keisatsu-de umibe-dehitori-de arui-teiru syonen-wo hogo-shita
(The police/subj) (on the beach)(alone) (was walking) (boy/obj) (had custody)
(The police had custody of the boy who was walking alone on the beach.)
Figure 1: Example of a Japanese sentence, bunsetsu and dependencies
2 Parsing Japanese
The Japanese language is basically an SOV lan-
guage, but word order is relatively free. In English
the syntactic function of each word is represented
by word order, while in Japanese it is represented
by postpositions. For example, one or more post-
positions following a noun play a role similar to
the declension of nouns in German, which indi-
cates grammatical case.
The syntax of a Japanese sentence is analyzed
by using segments, called bunsetsu, that usually
contain one or more content words like a noun,
verb, or adjective, and zero or more function
words like a particle (case marker) or verb/noun
suffix. By defining a bunsetsu in this manner, we
can analyze a sentence in a way similar to that used
when analyzing the grammatical roles of words in
inflected languages like German.
Japanese dependencies have the following char-
acteristics:
? Each bunsetsu except the rightmost one has
only one head.
? Each head bunsetsu is always placed to the
right of (i.e. after) its modifier.
? Dependencies do not cross one another.
Statistical Japanese dependency analyzers
(Kudo and Matsumoto, 2005; Kudo and Mat-
sumoto, 2002; Sassano, 2004; Uchimoto et al,
1999; Uchimoto et al, 2000) automatically learn
the likelihood of dependencies from a tagged
corpus and calculate the best dependencies for an
input sentence. These likelihoods are learned by
considering the features of bunsetsus such as their
character strings, parts of speech, and inflection
types, as well as information between bunsetsus
such as punctuation and the distance between
bunsetsus. The weight of given features is learned
from a training corpus by calculating the weights
from the frequencies of the features in the training
data.
3 Japanese dependency analysis taking
account of co-occurrence information
and a combination of multiple cases
One constraint in Japanese is that multiple nouns
of the same case do not modify a verb. Previ-
ous work on Japanese dependency analysis has as-
sumed that all the dependency relations are inde-
pendent of one another. It is therefore necessary
to also consider such a constraint as a feature for
contextual information. Uchimoto et al, for ex-
ample, used as such a feature whether a particu-
lar type of bunsetsu is between two bunsetsus in a
dependency relation (Uchimoto et al, 1999), and
Sassano used information about what is just be-
fore and after the modifying bunsetsu and modi-
fyee bunsetsu (Sassano, 2004).
In the artificial example shown in Figure 1, it
is natural to consider that ?keisatsu-de? will mod-
ify ?hogo-shita?. Statistical Japanese dependency
analyzers (Uchimoto et al, 2000; Kudo and Mat-
sumoto, 2002), however, will output the result
where ?keisatsu-de? modifies ?arui-teiru?. This is
because in sentences without internal punctuation
a noun tends to modify the nearest verb, and these
analyzers do not take into account a combination
of multiple cases.
Another kind of information useful in depen-
dency analysis is the co-occurrence of a noun and
a verb, which indicates to what degree the noun
tends to modify the verb. In the above example,
the possible modifyees of ?keisatsu-de? are ?arui-
teiru? and ?hogo-shita?. Taking into account in-
formation about the co-occurrence of ?keisatsu-
de? and ?arui-teiru? and of ?keisatsu-de? and
?hogo-shita? makes it obvious that ?keisatsu-de?
is more likely to modify ?hogo-shita?.
834
In summary, we think that statistical Japanese
dependency analysis needs to take into account
at least two more kinds of information: the de-
pendency relation between multiple cases where
multiple nouns of the same case do not modify a
verb, and the co-occurrence of nouns and verbs.
One way to use such information in statistical de-
pendency analysis is to directly use it as features.
However, Kehler et al pointed out that this does
not make the analysis more accurate (Kehler et al,
2004). This paper therefore presents a model that
uses the co-occurrence information separately and
reranks the analysis candidates generated by the
existing machine learning model.
4 Our proposed model
We first introduce the notation for the explanation
of the dependency structure T :
m(T ) : the number of verbs in T
vi(T ) : the i-th verb in T
ci(T ) : the number of case elements that mod-
ify the i-th verb in T
esi(T ) : the set of case elements that modify the
i-th verb in T
rsi(T ) : the set of particles in the set of case el-
ements that modify the i-th verb in T
nsi(T ) : the set of nouns in the set of case ele-
ments that modify the i-th verb in T
ri,j(T ) : the j-th particle that modifies the i-th
verb in T
ni,j(T ) : the j-th noun that modifies the i-th verb
in T
We defined case element as a pair of a noun
and following particles. For the dependency
structure we assume the conditional probability
P (esi(T )|vi(T )) that the set of case elements
esi(T ) depends on the vi(T ), and assume the set
of case elements esi(T ) is composed of the set of
noun nsi(T ) and particles rsi(T ).
P (esi(T )|vi(T ))
def= P (rsi(T ), nsi(T )|vi(T )) (1)
= P (rsi(T )|vi(T )) ?
P (nsi(T )|rsi(T ), vi(T )) (2)
' P (rsi(T )|vi(T )) ?
ci(T )
?
j=1
P (ni,j(T)|rsi(T),vi(T)) (3)
' P (rsi(T )|vi(T )) ?
ci(T )
?
j=1
P (ni,j(T)|ri,j(T),vi(T)) (4)
In the transformation from Equation (2) to Equa-
tion (3), we assume that the set of noun nsi(T ) is
independent of the verb vi(T ). And in the trans-
formation from Equation (3) to Equation (4), we
assume that the noun ni,j(T ) is dependent on only
its following particle ri,j(T ).
Now we assume the dependency structure T of
the whole sentence is composed of only the depen-
dency relation between case elements and verbs,
and propose the sentence probability defined by
Equation (5).
P (T ) =
m(T )
?
i=1
P (rsi(T )|vi(T )) ?
ci(T )
?
j=1
P (ni,j(T )|ri,j(T ), vi(T )) (5)
We call P (rsi(T )|vi(T )) the co-occurrence prob-
ability of the particle set and the verb, and we
call P (ni,j(T )|ri,j(T ), vi(T )) the co-occurrence
probability of the case element set and the verb.
In the actual dependency analysis, we try to se-
lect the dependency structure T? that maximizes
the Equation (5) from the possible parses T for the
inputted sentence:
T? = argmax
T
m(T )
?
i=1
P (rsi(T )|vi(T )) ?
ci(T )
?
j=1
P (ni,j(T )|ri,j(T ), vi(T )). (6)
The proposed model is inspired by the semantic
role labeling method (Gildea and Jurafsky, 2002),
which uses the frame element group in place of the
particle set.
It differs from the previous parsing models in
that we take into account the dependency relations
among particles in the set of case elements that
modify a verb. This information can constrain the
combination of particles (cases) among bunsetsus
that modify a verb. Assuming the independence
among particles, we can rewrite Equation (5) as
P (T ) =
m(T )
?
i=1
ci(T )
?
j=1
P (ni,j(T ), ri,j(T )|vi(T )). (7)
4.1 Syntactic property of a verb
In Japanese, the ?ha? case that indicates a topic
tends to modify the main verb in a sentence and
tends not to modify a verb in a relative clause. The
835
verb: ?aru-ku? verb: ?hogo-suru?
case elements particle set case elements particle set
a keisatsu-de umibe-de hitori-de { de,de,de } syonen-wo {wo}
b umibe-de hitori-de {de,de} keisatsu-de syonen-wo {de,wo}
c hitori-de {de} keisatsu-de umibe-de syonen-wo {de,de,wo}
d {none} keisatsu-de umibe-de hitori-de syonen-wo { de,de,de,wo }
Table 1: Analytical process of the example sentence
co-occurrence probability of the particle set there-
fore tends to be different for verbs with different
syntactic properties.
Like (Shirai, 1998), to take into account the re-
liance of the co-occurrence probability of the par-
ticle set on the syntactic property of a verb, instead
of using P (rsi(T )|vi(T )) in Equation (5), we use
P (rsi(T )|syni(T ), vi(T )), where syni(T ) is the
syntactic property of the i-th verb in T and takes
one of the following three values:
?verb? when v modifies another verb
?noun? when v modifies a noun
?main? when v modifies nothing (when it is at the
end of the sentence, and is the main verb)
4.2 Illustration of model application
Here, we illustrate the process of applying our pro-
posed model to the example sentence in Figure 1,
for which there are four possible combinations of
dependency relations. The bunsetsu combinations
and corresponding sets of particles are listed in Ta-
ble 1. In the analytical process, we calculate for
all the combinations the co-occurrence probability
of the case element set (bunsetsu set) and the co-
occurrence probability of the particle set, and we
select the T? that maximizes the probability.
Some of the co-occurrence probabilities of the
particle sets for the verbs ?aru-ku? and ?hogo-
suru? in the sentence are listed in Table 2. How to
estimate these probabilities is described in section
5.3. Basically, the larger the number of particles,
the lower the probability is. As you can see in the
comparison between {de, wo} and {de, de}, the
probability becomes lower when multiple same
cases are included. Therefore, the probability can
reflect the constraint that multiple case elements
of the same particle tend not to modify a verb.
5 Experiments
We evaluated the effectiveness of our model ex-
perimentally. Since our model treats only the de-
rsi P (rsi|noun, v1) P (rsi|main, v2)
v1 = ?aru-ku? v2 = ?hogo-suru?
{none} 0.29 0.35
{wo} 0.30 0.24
{ga} 0.056 0.072
{ni} 0.040 0.041
{de} 0.032 0.033
{ha} 0.035 0.041
{de, wo} 0.022 0.018
{de, de} 0.00038 0.00038
{de, de, wo} 0.00022 0.00018
{de, de, de} 0.0000019 0.0000018
{de, de, de, wo} 0.00000085 0.00000070
Table 2: Example of the co-occurrence probabili-
ties of particle sets
pendency relations between a noun and a verb, we
cannot determine all the dependency relations in
a sentence. We therefore use one of the currently
available dependency analyzers to generate an or-
dered list of n-best possible parses for the sentence
and then use our proposed model to rerank them
and select the best parse.
5.1 Dependency analyzer for outputting
n-best parses
We generated the n-best parses by using the ?pos-
terior context model? (Uchimoto et al, 2000). The
features we used were those in (Uchimoto et al,
1999) and their combinations. We also added our
original features and their combinations, with ref-
erence to (Sassano, 2004; Kudo and Matsumoto,
2002), but we removed the features that had a fre-
quency of less than 30 in our training data. The
total number of features is thus 105,608.
5.2 Reranking method
Because our model considers only the dependency
relations between a noun and a verb, and thus
cannot determine all the dependency relations in
a sentence, we restricted the possible parses for
836
reranking as illustrated in Figure 2. The possi-
ble parses for reranking were the first-ranked parse
and those of the next-best parses in which the
verb to modify was different from that in the first-
ranked one. For example, parses 1 and 3 in Figure
2 are the only candidates for reranking. In our ex-
periments, n is set to 50.
The score we used for reranking the parses was
the product of the probability of the posterior con-
text model and the probability of our proposed
model:
score = Pcontext(T )? ? P (T ), (8)
where Pcontext(T ) is the probability of the poste-
rior context model. The ? here is a parameter with
which we can adjust the balance of the two proba-
bilities, and is fixed to the best value by consider-
ing development data (different from the training
data)1.
Reranking
Candidate 1
Candidate 2
Candidate 3
Candidate 4
: Case element : Verb
Candidate
Candidate
Figure 2: Selection of possible parses for reranking
Many methods for reranking the parsing of En-
glish sentences have been proposed (Charniak and
Johnson, 2005; Collins and Koo, 2005; Hender-
son and Titov, 2005), all of which are discrimina-
tive methods which learn the difference between
the best parse and next-best parses. While our
reranking model using generation probability is
quite simple, we can easily verify our hypothesis
that the two proposed probabilities have an effect
on improving the parsing accuracy. We can also
verify that the parsing accuracy improves by using
imprecise information obtained from an automati-
cally parsed corpus.
Klein and Manning proposed a generative
model in which syntactic (PCFG) and semantic
(lexical dependency) structures are scored with
separate models (Klein and Manning, 2002), but
1In our experiments, ? is set to 2.0 using development
data.
they do not take into account the combination of
dependencies. Shirai et al also proposed a statis-
tical model of Japanese language which integrates
lexical association statistics with syntactic prefer-
ence (Shirai et al, 1998). Our proposed model dif-
fers from their method in that it explicitly uses the
combination of multiple cases.
5.3 Estimation of co-occurrence probability
We estimated the co-occurrence probability of the
particle set and the co-occurrence probability of
the case element set used in our model by analyz-
ing a large-scale corpus. We collected a 30-year
newspaper corpus2, applied the morphological an-
alyzer JUMAN (Kurohashi and Nagao, 1998b),
and then applied the dependency analyzer with
a posterior context model3. To ensure that we
collected reliable co-occurrence information, we
removed the information for the bunsetsus with
punctuation4.
Like (Torisawa, 2001), we estimated the co-
occurrence probability P (?n, r, v?) of the case
element set (noun n, particle r, and verb v)
by using probabilistic latent semantic indexing
(PLSI) (Hofmann, 1999)5. If ?n, r, v? is the
co-occurrence of n and ?r, v?, we can calculate
P (?n, r, v?) by using the following equation:
P (?n, r, v?) =
?
z?Z
P (n|z)P (?r, v?|z)P (z), (9)
where z indicates a latent semantic class of co-
occurrence (hidden class). Probabilistic parame-
ters P (n|z), P (?r, v?|z), and P (z) in Equation (9)
can be estimated by using the EM algorithm. In
our experiments, the dimension of the hidden class
z was set to 300. As a result, the collected ?n, r, v?
total 102,581,924 pairs. The number of n and v is
57,315 and 15,098, respectively.
The particles for which the co-occurrence prob-
ability was estimated were the set of case particles,
the ?ha? case particle, and a class of ?fukujoshi?
213 years? worth of articles from the Mainichi Shimbun,
14 years? worth from the Yomiuri Shimbun, and 3 years?
worth from the Asahi Shimbun.
3We used the following package for calculation of
Maximum Entropy:
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
4The result of dependency analysis with a posterior con-
text model for the Kyodai Corpus showed that the accuracy
for the bunsetsu without punctuation is 90.6%, while the ac-
curacy is only 76.4% for those with punctuation.
5We used the following package for calculation of PLSI:
http://chasen.org/?taku/software/plsi/.
837
Bunsetsu accuracy Sentence accuracy
Whole data Context model 90.95% (73,390/80,695) 54.40% (5,052/9,287)
Our model 91.21% (73,603/80,695) 55.17% (5,124/9,287)
Only for reranked sentences Context model 90.72% (68,971/76,026) 48,33% (3,813/7,889)
Our model 91.00% (69,184/76,026) 49.25% (3,885/7,889)
Only for case elements Context model 91.80% (28,849/31,427) ?
Our model 92.47% (29,062/31,427) ?
Table 3: Accuracy before/after reranking
particles. Therefore, the total number of particles
was 10.
We also estimated the co-occurrence probability
of the particle set P (rs|syn, v) by using PLSI. We
regarded the triple ?rs, syn, v? (the co-occurrence
of particle set rs, verb v, and the syntactic prop-
erty syn) as the co-occurrence of rs and ?syn, v?.
The dimension of the hidden class was 100. The
total number of ?rs, syn, v? pairs was 1,016,508,
v was 18,423, and rs was 1,490. The particle set
should be treated not as a non-ordered set but as
an occurrence ordered set. However, we think cor-
rect probability estimation using an occurrence or-
dered set is difficult, because it gives rise to an ex-
plosion in the number of combination,
5.4 Experimental environment
The evaluation data we used was Kyodai Cor-
pus 3.0, a corpus manually annotated with depen-
dency relations (Kurohashi and Nagao, 1998a).
The statistics of the data are as follows:
? Training data: 24,263 sentences, 234,474
bunsetsus
? Development data: 4,833 sentences, 47,580
bunsetsus
? Test data: 9,287 sentences, 89,982 bunsetsus
The test data contained 31,427 case elements, and
28,801 verbs.
The evaluation measures we used were bunsetsu
accuracy (the percentage of bunsetsu for which the
correct modifyee was identified) and sentence ac-
curacy (the percentage of sentences for which the
correct dependency structure was identified).
5.5 Experimental results
5.5.1 Evaluation of our model
Our first experiment evaluated the effectiveness
of reranking with our proposed model. Bunsetsu
Our reranking model
correct incorrect
Context model correct 73,119 271
incorrect 484 6,821
Table 4: 2 ? 2 contingency table of the number of
correct bunsetsu (posterior context model ? our
model)
and sentence accuracies before and after rerank-
ing, for the entire set of test data as well as for
only those sentences whose parse was actually
reranked, are listed in Table 3.
The results showed that the accuracy could be
improved by using our proposed model to rerank
the results obtained with the posterior context
model. McNemar testing showed that the null hy-
pothesis that there is no difference between the ac-
curacy of the results obtained with the posterior
context model and those obtained with our model
could be rejected with a p value < 0.01. The
difference in accuracy is therefore significant.
5.5.2 Comparing variant models
We next experimentally compare the following
variations of the proposed model:
(a) one in which the case element set is assumed
to be independent [Equation (7)]
(b) one using the co-occurrence probability of
the particle set, P (rs|syn, v), in our model
(c) one using only the co-occurrence probability
of the case element, P (n|r, v), in our model
(d) one not taking into account the syntactic
property of a verb (i,e. a model in which
the co-occurrence probability is defined as
P (r|v), without the syntactic property syn)
(e) one in which the co-occurrence probability of
the case element, P (n|r, v), is simply added
838
Bunsetsu Sentence
accuracy accuracy
Context model 90.95% 54.40%
Our model 91.21% 55.17%
model (a) 91.12% 54.90%
model (b) 91.10% 54.69%
model (c) 91.11% 54.91%
model (d) 91.15% 54.82%
model (e) 90.96% 54.33%
model (f) 89.50% 48.33%
Kudo et al2005 91.37% 56.00%
Table 5: Comparison of various models
to a feature set used in the posterior context
model
(f) one using only our proposed probabilities
without the probability of the posterior con-
text model
The accuracies obtained with each of these
models are listed in Table 5, from which we can
conclude that it is effective to take into account the
dependency between case elements because model
(a) is less accurate than our model.
Since the accuracy of model (d) is comparable
to that of our model, we can conclude that the con-
sideration of the syntactic property of a verb does
not necessarily improve dependency analysis.
The accuracy of model (e), which uses the co-
occurrence probability of the case element set as
features in the posterior context model, is compa-
rable to that of the posterior context model. This
result is similar to the one obtained by (Kehler et
al., 2004), where the task was anaphora resolution.
Although we think the co-occurrence probability
is useful information for dependency analysis, this
result shows that simply adding it as a feature does
not improve the accuracy.
5.5.3 Changing the amount of training data
Changing the size of the training data set, we
investigated whether the degree of accuracy im-
provement due to reranking depends on the accu-
racy of the existing dependency analyzer.
Figure 3 shows that the accuracy improvement
is constant even if the accuracy of the dependency
analyzer is varied.
5.6 Discussion
The score used in reranking is the product of the
probability of the posterior context model and the
 0.894
 0.896
 0.898
 0.9
 0.902
 0.904
 0.906
 0.908
 0.91
 0.912
 0.914
 4000  6000  8000  10000  12000  14000  16000  18000  20000  22000  24000  26000No. of training sentences
Buns
etsu 
acc
ura
cy
posterior context modelproposed model
Figure 3: Bunsetsu accuracy when the size of the
training data is changed
probability of our proposed model. The results in
Table 5 show that the parsing accuracy of model
(f), which uses only the probabilities obtained with
our proposed model, is quite low. We think the
reason for this is that our two co-occurrence prob-
abilities cannot take account of syntactic proper-
ties, such as punctuation and the distance between
two bunsetsus, which improve dependency analy-
sis.
Furthermore, when the sentence has multiple
verbs and case elements, the constraint of our pro-
posed model tends to distribute case elements to
each verb equally. To investigate such bias, we
calculated the variance of the number of case ele-
ments per verb.
Table 6 shows that the variance for our proposed
model (Equation [5]) is the lowest, and this model
distributes case elements to each verb equally. The
variance of the posterior context model is higher
than that of the test data, probably because the
syntactic constraint in this model affects parsing
too much. Therefore the variance of the reranking
model (Equation [8]), which is the combination
of our proposed model and the posterior context
model, is close to that of the test data.
The best parse which uses this data set is (Kudo
and Matsumoto, 2005), and their parsing accuracy
is 91.37%. The features and the parsing method
used by their model are almost equal to the poste-
rior context model, but they use a different method
of probability estimation. If their model could
generate n-best parsing and attach some kind of
score to each parse tree, we would combine their
model in place of the posterior context model.
At the stage of incorporating the proposed ap-
proach to a parser, the consistency with other pos-
839
context model test data Equation [8] Equation [5]
variance (?2) 0.724 0.702 0.696 0.666
*The average number of elements per verb is 1.078.
Table 6: The variance of the number of elements per verb
sible methods that deal with other relations should
be taken into account. This will be one of our fu-
ture tasks.
6 Conclusion
We presented a method of improving Japanese de-
pendency parsing by using large-scale statistical
information. Our method takes into account two
types of information, not considered in previous
statistical (machine learning based) parsing meth-
ods. One is information about the dependency re-
lations among the case elements of a verb, and the
other is information about co-occurrence relations
between a verb and its case element. Experimen-
tal results showed that our method can improve the
accuracy of the existing method.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 173?180.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 181?188.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd Annual Inter-
national SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 50?57.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004. The (non)utility of
predicate-argument frequencies for pronoun inter-
pretation. In Proceedings of the HLT/NAACL 2004,
pages 289?296.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), pages 3?
10.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
CoNLL 2002: Proceedings of the 6th Conference on
Natural Language Learning 2002 (COLING 2002
Post-Conference Workshops), pages 63?69.
Taku Kudo and Yuji Matsumoto. 2005. Japanese de-
pendency parsing using relative preference of depen-
dency. Transactions of Information Processing So-
ciety of Japan, 46(4):1082?1092. (in Japanese).
Sadao Kurohashi andMakoto Nagao. 1994. Kn parser:
Japanese dependency/case structure analyzer. In
Proceedings of the Workshop on Sharable Natural
Language Resources, pages 48?55.
Sadao Kurohashi and Makoto Nagao. 1998a. Building
a Japanese parsed corpus while improving the pars-
ing system. In Proceedings of the 1st International
Conference on Language Resources and Evaluation,
pages 719?724.
Sadao Kurohashi andMakoto Nagao. 1998b. Japanese
Morphological Analysis System JUMAN version
3.5. Department of Informatics, Kyoto University.
(in Japanese).
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proceedings of the COLING
2004, pages 8?14.
Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and
Hozumi Tanaka. 1998. An empirical evaluation on
statistical parsing of Japanese sentences using lexi-
cal association statistics. In Proceedings of the 3rd
Conference on EMNLP, pages 80?87.
Kiyoaki Shirai. 1998. The integrated natural language
processing using statistical information. Technical
Report TR98?0004, Department of Computer Sci-
ence, Tokyo Institute of Technology. (in Japanese).
Kentaro Torisawa. 2001. An unsupervised method for
canonicalization of Japanese postpositions. In Pro-
ceedings of the 6th Natural Language Processing
Pacific Rim Symposium (NLPRS), pages 211?218.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese dependency structure analy-
sis based on maximum entropy models. Transac-
tions of Information Processing Society of Japan,
40(9):3397?3407. (in Japanese).
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine,
and Hitoshi Isahara. 2000. Dependency model
using posterior context. In Proceedings of the
Sixth International Workshop on Parsing Technol-
ogy (IWPT2000), pages 321?322.
840
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 5?8,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Translation Aid System with a Stratified Lookup Interface
Takeshi Abekawa and Kyo Kageura
Library and Information Science Course
Graduate School of Education,
University of Tokyo, Japan
{abekawa,kyo}@p.u-tokyo.ac.jp
Abstract
We are currently developing a translation
aid system specially designed for English-
to-Japanese volunteer translators working
mainly online. In this paper we introduce
the stratified reference lookup interface that
has been incorporated into the source text
area of the system, which distinguishes three
user awareness levels depending on the type
and nature of the reference unit. The dif-
ferent awareness levels are assigned to ref-
erence units from a variety of reference
sources, according to the criteria of ?com-
position?, ?difficulty?, ?speciality? and ?re-
source type?.
1 Introduction
A number of translation aid systems have been de-
veloped so far (Bowker, 2002; Gow, 2003). Some
systems such as TRADOS have proved useful for
some translators and translation companies1. How-
ever, volunteer (and in some case freelance) trans-
lators do not tend to use these systems (Fulford and
Zafra, 2004; Fulford, 2001; Kageura et al, 2006),
for a variety of reasons: most of them are too expen-
sive for volunteer translators2; the available func-
tions do not match the translators? needs and work
style; volunteer translators are under no pressure
from clients to use the system, etc. This does not
mean, however, that volunteer translators are satis-
fied with their working environment.
Against this backdrop, we are developing a trans-
lation aid system specially designed for English-to-
Japanese volunteer translators working mainly on-
line. This paper introduces the stratified reference
1http://www.trados.com/
2Omega-T, http://www.omegat.org/
lookup/notification interface that has been incorpo-
rated into the source text area of the system, which
distinguishes three user awareness levels depending
on the type and nature of the reference unit. We
show how awareness scores are given to the refer-
ence units and how these scores are reflected in the
way the reference units are displayed.
2 Background
2.1 Characteristics of target translators
Volunteer translators involved in translating English
online documents into Japanese have a variety of
backgrounds. Some are professional translators,
some are interested in the topic, some translate as a
part of their NGO activities, etc3. They nevertheless
share a few basic characteristics: (i) they are native
speakers of Japanese (the target language: TL); (ii)
most of them do not have a native-level command in
English (the source language: SL); (iii) they do not
use a translation aid system or MT; (iv) they want to
reduce the burden involved in the process of transla-
tion; (v) they spend a huge amount of time looking
up reference sources; (vi) the smallest basic unit of
translation is the paragraph and ?at a glance? read-
ability of the SL text is very important. A translation
aid system for these translators should provide en-
hanced and easy-to-use reference lookup functions
with quality reference sources. An important point
expressed by some translators is that they do not
want a system that makes decisions on their behalf;
they want the system to help them make decisions
by making it easier for them to access references.
Decision-making by translations in fact constitutes
an essential part of the translation process (Munday,
2001; Venuti, 2004).
3We carried out a questionnaire survey of 15 volunteer trans-
lators and interviewed 5 translators.
5
Some of these characteristics contrast with those
of professional translators, for instance, in Canada
or in the EU. They have native command in both
the source and target languages; they went through
university-level training in translation; many of them
have a speciality domain; they work on the principle
that ?time is money? 4. For this type of translator,
facilitating target text input can be important, as is
shown in the TransType system (Foster et al, 2002;
Macklovitch, 2006).
2.2 Reference units and lookup patterns
The major types of reference unit can be sum-
marised as follows (Kageura et al, 2006).
Ordinary words: Translators are mostly satisfied
with the information provided in existing dictionar-
ies. Looking up these references is not a huge bur-
den, though reducing it would be preferable.
Idioms and phrases: Translators are mostly sat-
isfied with the information provided in dictionaries.
However, the lookup process is onerous and many
translators worry about failing to recognise idioms
in SL texts (as they can often be interpreted liter-
ally), which may lead to mistranslations.
Technical terms: Translators are not satisfied
with the available reference resources 5; they tend
to search the Internet directly. Translators tend to be
concerned with failing to recognise technical terms.
Proper names: Translators are not satisfied with
the available reference resources. They worry more
about misidentifying the referent. For the identifica-
tion of the referent, they rely on the Internet.
3 The translation aid system: QRedit
3.1 System overview
The system we are developing, QRedit, has been de-
signed with the following policies: making it less
onerous for translators to do what they are currently
doing; providing information efficiently to facilitate
decision-making by translators; providing functions
in a manner that matches translators? behaviour.
QRedit operates on the client server model. It is
implemented by Java and run on Tomcat. Users ac-
4Personal communication with Professor Elliott
Macklovitch at the University of Montreal, Canada.
5With the advent of Wikipedia, this problem is gradually
becoming less important.
cess the system through Web browsers. The inte-
grated editor interface is divided into two main ar-
eas: the SL text area and the TL editing area. These
scroll synchronically. To enable translators to main-
tain their work rhythm, the keyboard cursor is al-
ways bound to the TL editing area (Abekawa and
Kageura, 2007).
3.2 Reference lookup functions
Reference lookup functions are activated when an
SL text is loaded. Relevant information (translation
candidates and related information) is displayed in
response to the user?s mouse action. In addition to
simple dictionary lookup, the system also provides
flexible multi-word unit lookup mechanisms. For
instance, it can automatically look up the dictionary
entry ?with one?s tongue in one?s cheek? for the ex-
pression ?He said that with his big fat tongue in his
big fat cheek? or ?head screwed on right? for ?head
screwed on wrong? (Kanehira et al, 2006).
The reference information can be displayed in two
ways: a simplified display in a small popup window
that shows only the translation candidates, and a full
display in a large window that shows the full refer-
ence information. The former is for quick reference
and the latter for in-depth examination.
Currently, Sanseido?s Grand Concise English-
Japanese Dictionary, Eijiro6, List of technical terms
in 23 domains, and Wikipedia are provided as refer-
ence sources.
4 Stratified reference lookup interface
In relation to reference lookup functions, the follow-
ing points are of utmost importance:
1. In the process of translation, translators often
check multiple reference resources and exam-
ine several meanings in SL and expressions in
TL. We define the provision of ?good informa-
tion? for the translator by the system as infor-
mation that the translator can use to make his
or her own decisions.
2. The system should show the range of avail-
able information in a manner that corresponds
to the translator?s reference lookup needs and
behaviour.
6http://www.eijiro.jp/
6
The reference lookup functions can be divided
into two kinds: (i) those that notify the user of the
existence of the reference unit, and (ii) those that
provide reference information. Even if a linguistic
unit is registered in reference sources, if the transla-
tor is unaware of its existence, (s)he will not look
up the reference, which may result in mistransla-
tion. It is therefore preferable for the system to no-
tify the user of the possible reference units. On the
other hand, the richer the reference sources become,
the greater the number of candidates for notification,
which would reduce the readability of SL texts dra-
matically. It was necessary to resolve this conflict
by striking an appropriate balance between the no-
tification function and user needs in both reference
lookup and the readability of the SL text.
4.1 Awareness levels
To resolve this conflict, we introduced three transla-
tor ?awareness levels?:
? Awareness level -2: Linguistic units that the
translator may not notice, which will lead to
mistranslation. The system always actively no-
tifies translators of the existence of this type of
unit, by underlining it. Idioms and complex
technical terms are natural candidates for this
awareness level.
? Awareness level -1: Linguistic units that trans-
lators may be vaguely aware of or may suspect
exist and would like to check. To enable the
user to check their existence easily, the rele-
vant units are displayed in bold when the user
moves the cursor over the relevant unit or its
constituent parts with the mouse. Compounds,
easy idioms and fixed expressions are candi-
dates for this level.
? Awareness level 0: Linguistic units that the
user can always identify. Single words and easy
compounds are candidates for this level.
In all these cases, the system displays reference in-
formation when the user clicks on the relevant unit
with the mouse.
4.2 Assignment of awareness levels
The awareness levels defined above are assigned to
the reference units on the basis of the following four
characteristics:
C(unit): The compositional nature of the unit.
Single words can always be identified in texts, so
the score 0 is assigned to them. The score -1 is as-
signed to compound units. The score -2 is assigned
to idioms and compound units with gaps.
D(unit): The difficulty of the linguistic unit for a
standard volunteer translator. For units in the list of
elementary expressions7, the score 1 is given. The
score 0 is assigned to words, phrases and idioms
listed in general dictionaries. The score -1 is as-
signed to units registered only in technical term lists.
S(unit): The degree of domain dependency of the
unit. The score -1 is assigned to units that belong to
the domain which is specified by the user. The score
0 is assigned to all the other units. The domain infor-
mation is extracted from the domain tags in ordinary
dictionaries and technical term lists. For Wikipedia
entries the category information is used.
R(unit): The type of reference source to which the
unit belongs. We distinguish between dictionaries
and encyclopaedia, corresponding to the user?s in-
formation search behaviour. The score -1 is assigned
to units which are registered in the encyclopaedia
(currently Wikipedia8 ), because the fact that fac-
tual information is registered in existing reference
sources implies that there is additional information
relating to these units which the translator might
benefit from knowing. The score 0 is assigned to
units in dictionaries and technical term lists.
The overall score A(unit) for the awareness level
of a linguistic unit is calculated by:
A(unit) = C(unit)+D(unit)+S(unit)+R(unit).
Table 1 shows the summary of awareness levels
and the scores of each characteristic. For instance, in
an the SL sentence ?The airplane took right off.?, the
C(take off) = ?2, D(take off) = 1, S(take off) =
0 and R(take off) = 0; hence A(take off) = ?1.
A score lower than -2 is normalised to -2, and a
score higher than 0 is normalised to 0, because we
assume three awareness levels are convenient for re-
alising the corresponding notification interface and
7This list consists of 1,654 idioms and phrases taken from
multiple sources for junior high school and high school level
English reference sources published in Japan.
8As the English Wikipedia has entries for a majority of or-
dinary words, we only assign the score -1 to proper names.
7
A(unit) : awareness level <= -2 -1 >= 0
Mode of alert always emphasis by mouse-over none
Score -2 -1 0 1
C(unit) : composition compound unit with gap compound unit single word
D(unit) : difficulty technical term general term elementary term
S(unit) : speciality specified domain general domain
R(unit) : resource type encyclopaedia dictionary
Table 1: Awareness levels and the scores of each characteristic
are optimal from the point of view of the user?s
search behaviour. We are currently examining user
customisation functions.
5 Conclusion
In this paper, we introduced a stratified reference
lookup interface within a translation aid environ-
ment specially designed for English-to-Japanese on-
line volunteer translators. We described the incorpo-
ration into the system of different ?awareness levels?
for linguistic units registered in multiple reference
sources in order to optimise the reference lookup in-
terface. The incorporation of these levels stemmed
from the basic understanding we arrived at after con-
sulting with actual translators that functions should
fit translators? actual behaviour. Although the effec-
tiveness of this interface is yet to be fully examined
in real-world situations, the basic concept should be
useful as the idea of awareness level comes from
feedback by monitors who used the first version of
the system.
Although in this paper we focused on the use
of established reference resources, we are currently
developing (i) a mechanism for recycling relevant
existing documents, (ii) dynamic lookup of proper
name transliteration on the Internet, and (iii) dy-
namic detection of translation candidates for com-
plex technical terms. How to fully integrate these
functions into the system is our next challenge.
References
Takeshi Abekawa and Kyo Kageura. 2007. Qredit:
An integrated editor system to support online volun-
teer translators. In Proceedings of Digital Humanities
2007 Poster/Demos.
Lynne Bowker. 2002. Computer-aided Translation Tech-
nology: A Practical Introduction. Ottawa: University
of Ottawa Press.
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 148?
155.
Heather Fulford and Joaqu??n Granell Zafra. 2004. The
uptake of online tools and web-based language re-
sources by freelance translators. In Proceedings of
the Second International Workshop on Language Re-
sources for Translation Work, Research and Training,
pages 37?44.
Heather Fulford. 2001. Translation tools: An ex-
ploratory study of their adoption by UK freelance
translators. Machine Translation, 16(3):219?232.
Francie Gow. 2003. Metrics for Evaluating Translation
Memory Software. PhD thesis, Ottawa: University of
Ottawa.
Kyo Kageura, Satoshi Sato, Koichi Takeuchi, Takehito
Utsuro, Keita Tsuji, and Teruo Koyama. 2006. Im-
proving the usability of language reference tools for
translators. In Proceedings of the 10th of Annual
Meeting of Japanese Natural Language Processing,
pages 707?710.
Kou Kanehira, Kazuki Hirao, Koichi Takeuchi, and Kyo
Kageura. 2006. Development of a flexible idiom
lookup system with variation rules. In Proceedings
of the 10th Annual Meeting of Japanese Natural Lan-
guage Processing, pages 711?714.
Elliott Macklovitch. 2006. Transtype2: the last word.
In Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC2006),
pages 167?172.
Jeremy Munday. 2001. Introducing Translation Studies:
Theories and Applications. London: Routledge.
Lawrence Venuti. 2004. The Translation Studies Reader.
London: Routledge, second edition.
8
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 65?68,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Fast decoding and Easy Implementation:
Transliteration as Sequential Labeling
Eiji ARAMAKI
The University of Tokyo
eiji.aramaki@gmail.com
Takeshi ABEKAWWA
National Institute of Informatics
abekawa@nii.ac.jp
Abstract
Although most of previous translitera-
tion methods are based on a generative
model, this paper presents a discrimi-
native transliteration model using condi-
tional random fields. We regard charac-
ter(s) as a kind of label, which enables
us to consider a transliteration process as
a sequential labeling process. This ap-
proach has two advantages: (1) fast decod-
ing and (2) easy implementation. Experi-
mental results yielded competitive perfor-
mance, demonstrating the feasibility of the
proposed approach.
1 Introduction
To date, most transliteration methods have relied
on a generative model which resembles a statisti-
cal machine translation (SMT) model. Although
the generative approach has appealing feasibility,
it usually suffers from parameter settings, length
biases and decoding time.
We assume a transliteration process as a kind
of sequential labeling that is widely employed for
various tasks, such as Named Entity Recognition
(NER), part-of-speech (POS) labeling, and so on.
Figure 1 shows a lattice of both the transliteration
and POS labeling. As shown in that figure, both
tasks share a similar work frame: (1) an input se-
quence is decomposed into several segments; then
(2) each segments produces a label. Although the
label represents a POS in POS labeling, it repre-
sents a character (or a character sequence) in the
transliteration task.
The proposed approach entails three risks.
1. Numerous Label Variation: Although POS
requires only 10?20 labels at most, a translit-
eration process requires numerous labels. In
fact, Japanese katakana requires more than
260 labels in the following experiment (we
Figure 1: (i) Part-of-Speech Lattice and (ii)
Transliteration Lattice.
consider combinations of characters as a la-
bel). Such a huge label set might require ex-
tremely heavy calculation.
2. No Gold Standard Data: We build the gold
standard label from character alignment us-
ing GIZA++ 1. Of course, such gold standard
data contain alignment errors, which might
decrease labeling performance.
3. No Language Model: The proposed ap-
proach cannot incorporate the target language
model.
In spite of the disadvantages listed above, the
proposed method offers two strong advantages.
1. Fast Decoding: Decoding (more pre-
cisely labeling) is extremely fast (0.12?0.58
s/input). Such rapid decoding is useful for
various applications, for example, a query ex-
pansion for a search engine and so on 2.
1http://www.fjoch.com/GIZA++.html
2A fast transliteration demonstration is available at the
web site; http://akebia.hcc.h.u-tokyo.ac.jp/NEWS/
65
Figure 2: Conversion from Training set to Gold
Standard Labels
2. Easy Implementation: Because sequential
labeling is a traditional research topic, vari-
ous algorithms and tools are available. Using
them, we can easily realize various transliter-
ation systems in any language pairs.
The experimental results empirically demon-
strate that the proposed method is competitive
in several language directions (e.g. English?
Chinese).
2 Method
We developed a two-stage labeling system. First,
an input term is decomposed into several segments
(STEP1). Next, each segmentation produces sym-
bol(s) (STEP2).
2.1 STEP1: Chunking
For a given noun phrase, consisting n characters,
the system gave a label (L1...Ln) that represents
segmentations.
The segmentation is expressed as two types of
labels (label B and I), where B signifies a begin-
ning of the segmentation, and I signifies the end
of segmentation. This representation is similar to
the IOB representation, which is used in Named
Entity Recognition (NER) or chunking.
For label prediction, we used Conditional Ran-
dom Fields (CRFs), which is a state-of-the-art la-
beling algorithm. We regard a source character it-
self as a CRF feature. The window size is three
(the current character and previous/next charac-
ter).
2.2 STEP2: Symbol production
Next, the system estimates labels (T1...Tm) for
each segmentation, where m is the number of seg-
Table 1: Corpora and Sizes
Notation Language Train Test
EN-CH English?Chinese 31,961 2,896
EN-JA English?Japanese 27,993 1,489
EN-KO English?Korean 4,840 989
EN-HI English?Hindi 10,014 1,000
EN-TA English?Tamil 8,037 1,000
EN-KA English?Kannada 8,065 1,000
EN-RU English?Russian 5,977 1,000
* EN-CH is provided by (Li et al, 2004); EN-
TA, EN-KA, EN-HI and EN-RU are from (Kumaran
and Kellner, 2007); EN-JA and EN-KO are from
http://www.cjk.org/.
mentations (the number of B labels in STEP1).
The label of this step directly represents a target
language character(s). The method of building a
gold standard label is described in the next sub-
section.
Like STEP1, we use CRFs, and regard source
characters as a feature (window size=3).
2.3 Conversion from Alignment to Labels
First, character alignment is estimated using
GIZA++ as shown at the top of Fig. 2. The align-
ment direction is a target- language-to-English, as-
suming that n English characters correspond to a
target language character.
The STEP1 label is generated for each English
character. If the alignment is 1:1, we give the char-
acter aB label. If the alignment is n : 1, we assign
the first character a B label, and give the others I .
Note that we regard null alignment as a continu-
ance of the last segmentation (I).
The STEP2 label is generated for each English
segmentation (B or BI?). If a segmentation cor-
responds to two or more characters in the target
side, we regard the entire sequence as a label (see
T5 in Fig. 2).
3 Experiments
3.1 Corpus, Evaluation, and Setting
To evaluate the performance of our system,
we used a training-set and test-set provided by
NEWS3(Table 1).
We used the following six metrics (Table 2) us-
ing 10 output candidates. A white paper4 presents
the detailed definitions. For learning, we used
CRF++5 with standard parameters (f=20, c=.5).
3http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/
4https://translit.i2r.a-star.edu.sg/news2009/whitepaper/
5http://crfpp.sourceforge.net/
66
Table 3: Results in Test-set
ACC MeanF MRR MAPref MAP10 MAPsys
EN?CH 0.580 0.826 0.653 0.580 0.199 0.199
EN?RU 0.531 0.912 0.635 0.531 0.219 0.219
EN?JA 0.457 0.828 0.576 0.445 0.194 0.194
EN?TA 0.365 0.884 0.504 0.360 0.172 0.172
EN?HI 0.363 0.864 0.503 0.360 0.170 0.170
EN?KA 0.324 0.856 0.438 0.315 0.148 0.148
EN?KO 0.170 0.512 0.218 0.170 0.069 0.069
Table 2: Evaluation Metrics
ACC Word Accuracy in Top 1.
MeanF
The meanF measures the fuzzy accu-
racy that is defined by the edit dis-
tance and Longest Common Subse-
quence (LCS).
MRR
Mean Reciprocal Rank. 1/MRR tells
approximately the average rank of the
correct transliteration.
MAPref
Measures the precision in the n?best
candidates tightly for each reference.
MAP10 Measures the precision in the 10-bestcandidates.
MAPsys
Measures the precision in the top Ki-
best candidates produced by the system.
3.2 Results and Discussion
Table 3 presents the performance. As shown in the
table, a significant difference was found between
languages (from low (0.17) to high (0.58)).
The high accuracy results(EN-CH or EN-RU)
are competitive with other systems (the middle
rank among the NEWS participating systems).
However, several language results (such as EN-
KO) were found to have poor performance.
We investigated the difference between high-
performance languages and the others. Table 4
shows the training/test times and the number of
labels. As shown in the table, wide divergence is
apparent in the number of labels. For example,
although EN?KO requires numerous labels (536
labels), EN?RU needs only 131 labels. This diver-
gence roughly corresponds to both training-time
and accuracy as follows: (1) EN?KO requires long
training time (11 minutes) which gave poor per-
formance (0.17 ACC), and (2) EN?RU requires
short training (only 26.3 seconds) which gave high
performance (0.53 ACC). This suggests that if the
number of labels is small, we successfully convert
transliteration into a sequential labeling task.
The test time seemed to have no relation to
Table 4: Average Test time, Training Time, and
the number of labels (label variation).
Language Test Train # of labels
EN?KO 0.436s 11m09.5s 536
EN?CH 0.201s 6m18.9s 283
EN?JA 0.247s 4m44.3s 269
EN?KA 0.190s 2m26.6s 231
EN?HI 0.302s 1m55.6s 268
EN?TA 0.124s 1m32.9s 207
EN?RU 0.580s 0m26.3s 131
* Test time is the average labeling time for an input. Training
time is the average training time for 1000 labels.
both training time and performance. To investi-
gate what gave effects on test time is a subject for
our future work.
4 Related Works
Most previous transliteration studies have re-
lied on a generative model resembling the IBM
model(Brown et al, 1993). This approach is ap-
plicable to various languages: for Japanese (Goto
et al, 2004; Knight and Graehl, 1998), Korean(Oh
and Choi, 2002; Oh and Choi, 2005; Oh and
Isahara, 2007), Arabic(Stalls and Knight, 1998;
Sherif and Kondrak, 2007), Chinese(Li et al,
2007), and Persian(Karimi et al, 2007). As de-
scribed previously, the proposed discriminative
approach differs from them.
Another perspective is that of how to repre-
sent transliteration phenomena. Methods can be
classified into three main types: (1) grapheme-
based (Li et al, 2004), (2) phoneme-based (Knight
and Graehl, 1998), and (3) combinations of these
methods (hybrid-model(Bilac and Tanaka, 2004),
and a correspondence-based model(Oh and Choi,
2002; Oh and Choi, 2005) re-ranking model (Oh
and Isahara, 2007)). Our proposed method em-
ploys a grapheme-based approach. Employing
phonemes is a challenge reserved for future stud-
ies.
Aramaki et al (2008) proposed a discrimina-
67
tive transliteration approach using Support Vector
Machines (SVMs). However, their goal, which is
to judge whether two terms come from the same
English words or not, differs from this paper goal.
5 Conclusions
This paper presents a discriminative translitera-
tion model using a sequential labeling technique.
Experimental results yielded competitive perfor-
mance, demonstrating the feasibility of the pro-
posed approach. In the future, how to incorporate
more rich information, such as language model
and phoneme, is remaining problem. We believe
this task conversion, from generation to sequential
labeling, can be useful for several practical appli-
cations.
ACKNOWLEDGMENT
Part of this research is supported by Japanese
Grant-in-Aid for Scientific Research (A) Num-
ber:20680006.
References
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and
Kazuhiko Ohe. 2008. Orthographic disambiguation
incorporating transliterated probability. In Proceed-
ings of International Joint Conference on Natural
Language Processing (IJCNLP2008), pages 48?55.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In Pro-
ceedings of The 20th International Conference on
Computational Linguistics (COLING2004), pages
597?603.
Peter F. Brown, Stephen A. Della Pietra, Vi cent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2).
Isao Goto, Naoto Kato, Terumasa Ehara, and Hideki
Tanaka. 2004. Back transliteration from Japanese
to English using target English context. In Proceed-
ings of The 20th International Conference on Com-
putational Linguistics (COLING2004), pages 827?
833.
Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.
2007. Collapsed consonant and vowel models: New
approaches for English-Persian transliteration and
back-transliteration. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL2007), pages 648?655.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In SIGIR
?07: Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 721?722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proceedings of the Meeting of the Association for
Computational Linguistics (ACL2004), pages 159?
166.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, andMinghui
Dong. 2007. Semantic transliteration of per-
sonal names. In Proceedings of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL2007), pages 120?127.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proceedings of The 19th In-
ternational Conference on Computational Linguis-
tics (COLING2002), pages 758?764.
Jong-HoonOh and Key-Sun Choi. 2005. An ensemble
of grapheme and phoneme for machine translitera-
tion. In Proceedings of Second International Joint
Conference on Natural Language Processing (IJC-
NLP2005), pages 450?461.
Jong-Hoon Oh and Hitoshi Isahara. 2007. Machine
transliteration using multiple transliteration engines
and hypothesis re-ranking. In Proceedings of MT
Summit XI, pages 353?360.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics (ACL2007), pages 944?951.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in arabic text.
In Proceedings of The International Conference
on Computational Linguistics and the 36th Annual
Meeting of the Association of Computational Lin-
guistics (COLING-ACL1998) Workshop on Compu-
tational Approaches to Semitic Languages.
68
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 256?265, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Framework of Automatic Text Summarization
Using Reinforcement Learning
Seonggi Ryang
Graduate School of Information
Science and Technology
University of Tokyo
sryang@is.s.u-tokyo.ac.jp
Takeshi Abekawa
National Institute of Informatics
abekawa@nii.ac.jp
Abstract
We present a new approach to the problem
of automatic text summarization called Au-
tomatic Summarization using Reinforcement
Learning (ASRL) in this paper, which models
the process of constructing a summary within
the framework of reinforcement learning and
attempts to optimize the given score function
with the given feature representation of a sum-
mary. We demonstrate that the method of re-
inforcement learning can be adapted to auto-
matic summarization problems naturally and
simply, and other summarizing techniques,
such as sentence compression, can be easily
adapted as actions of the framework.
The experimental results indicated ASRL was
superior to the best performing method in
DUC2004 and comparable to the state of the
art ILP-style method, in terms of ROUGE
scores. The results also revealed ASRL can
search for sub-optimal solutions efficiently
under conditions for effectively selecting fea-
tures and the score function.
1 Introduction
Automatic text summarization aims to automatically
produce a short and well-organized summary of sin-
gle or multiple documents (Mani, 2001). Automatic
summarization, especially multi-document summa-
rization, has been an increasingly important task in
recent years, because of the exponential explosion
of available information. The brief summary that
the summarization system produces allows readers
to quickly and easily understand the content of orig-
inal documents without having to read each individ-
ual document, and it should be helpful for dealing
with enormous amounts of information.
The extractive approach to automatic summariza-
tion is a popular and well-known approach in this
field, which creates a summary by directly selecting
some textual units (e.g., words and sentences) from
the original documents, because it is difficult to gen-
uinely evaluate and guarantee the linguistic quality
of the produced summary.
One of the most well-known extractive ap-
proaches is maximal marginal relevance (MMR),
which scores each textual unit and extracts the unit
that has the highest score in terms of the MMR cri-
teria (Goldstein et al2000). Greedy MMR-style
algorithms are widely used; however, they cannot
take into account the whole quality of the sum-
mary due to their greediness, although a summary
should convey all the information in given docu-
ments. Global inference algorithms for the extrac-
tive approach have been researched widely in recent
years (Filatova and Hatzivassiloglou, 2004; McDon-
ald, 2007; Takamura and Okumura, 2009) to con-
sider whether the summary is ?good? as a whole.
These algorithms formulate the problem as integer
linear programming (ILP) to optimize the score:
however, as ILP is non-deterministic polynomial-
time hard (NP-hard), the time complexity is very
large. Consequently, we need some more efficient
algorithm for calculations.
We present a new approach to the problem of au-
tomatic text summarization called Automatic Sum-
marization using Reinforcement Learning (ASRL),
which models the process of construction of a sum-
mary within the framework of reinforcement learn-
256
ing and attempts to optimize the given score function
with the given feature representation of a summary.
We demonstrate that the method of reinforcement
learning can be adapted to problems with automatic
summarization naturally and simply, and other sum-
marizing techniques, such as sentence compression,
can be easily adapted as actions of the framework,
which should be helpful to enhance the quality of
the summary that is produced. This is the first paper
utilizing reinforcement learning for problems with
automatic summarization of text.
We evaluated ASRL with the DUC2004 summa-
rization task 2, and the experimental results revealed
ASRL is superior to the best method of performance
in DUC2004 and comparable with the state of the
art ILP-style method, based on maximum coverage
with the knapsack constraint problem, in terms of
ROUGE scores with experimental settings. We also
evaluated ASRL in terms of optimality and execu-
tion time. The experimental results indicated ASRL
can search the state space efficiently for some sub-
optimal solutions under the condition of effectively
selecting features and the score function, and pro-
duce a summary whose score denotes the expecta-
tion of the score of the same features? states. The
evaluation of the quality of a produced summary
only depends on the given score function, and there-
fore it is easy to adapt the new method of evaluation
without having to modify the structure of the frame-
work.
2 Formulation of Extractive Approach
We first focus on the extractive approach, which is
directly used to produce a summary by extracting
some textual units, by avoiding the difficulty of hav-
ing to consider the genuine linguistic quality of a
summary.
The given document (or documents) in extractive
summarization approaches is reduced to the set of
textual units: D = {x1, x2, ? ? ? , xn}, where n is
the size of the set, and xi denotes individual textual
units. Note that any textual unit is permitted, such
as character, word, sentence, phrase, and concep-
tual unit. If we determine a sentence is a textual unit
to be extracted, the formulated problem is a problem
of extracting sentences from the source document,
which is one of the most popular settings for sum-
marization tasks.
Next, we define the score function, score(S), for
any subset of the document: S ? D. Subset S is one
of the summaries of the given document. The aim of
this summarization problem is to find the summary
that maximizes this function when the score function
is given. The score function is typically defined by
taking into consideration the tradeoff between rele-
vance and redundancy.
Then, we define length function L(S), which in-
dicates the length of summary S. The length is
also arbitrary, which can be based on the character,
word, and sentence. We assume the limitation of
summary length K is given in summarization tasks.
Finally, we define the extractive approach of the
automatic summarization problem as:
S? = argmax
S?D
score(S) (1)
s.t. L(S) ? K.
3 Motivation
We can regard the extractive approach as a search
problem. It is extremely difficult to solve this search
problem because the final result of evaluation given
by the given score function is not available until it
finishes, and we therefore need to try all combina-
tions of textual units. Consequently, the score func-
tion, which denotes some criterion for the quality
of a summary, tends to be determined so that the
function can be decomposed to components and it
is solved with global inference algorithms, such as
ILP. However, both decomposing the score func-
tion properly and utilizing the evaluation of half-way
process of searches are generally difficult. For ex-
ample, let us assume that we design the score func-
tion by using some complex semantic considerations
to take into account the readability of a summary,
and the score is efficiently calculated if the whole
summary is given. Then, formulating the problem
as a global inference problem and solving it with
methods of integer linear programming might gen-
erally be difficult, because of the complex compo-
sition of the score function, despite the ease with
which the whole summary is evaluated. The read-
ability score might be based on extremely complex
calculations of dependency relations, or a great deal
of external knowledge the summarizer cannot know
257
merely from the source documents. In fact, it is ideal
that we can only directly utilize the score function,
in the sense that we do not have to consider the de-
composed form of the given score function.
We need to consider the problem with automatic
summarization to be the same as that with reinforce-
ment learning to handle these problems. Reinforce-
ment learning is one of the solutions to three prob-
lems.
? The learning of the agent only depends on the
reward provided by the environment.
? Furthermore, the reward is delayed, in the sense
that the agent cannot immediately know the ac-
tual evaluation of the executed action.
? The agent only estimates the value of the
state with the information on rewards, without
knowledge of the actual form of the score func-
tion, to maximize future rewards.
We suggest the formulation of the problem as we
have just described will enable us to freely design
the score function without limitations and expand
the capabilities of automatic summarization.
4 Models of Extractive Approach for
Reinforcement Learning
4.1 Reinforcement Learning
Reinforcement learning is a powerful method of
solving planning problems, especially problems for-
mulated as Markov decision processes (MDPs) (Sut-
ton and Barto, 1998). The agent of reinforcement
learning repeats three steps until terminated at each
episode in the learning process.
1. The agent observes current state s from the en-
vironment, contained in state space S .
2. Next, it determines and executes next action a
according to current policy ?. Action a is con-
tained in the action space limited by the current
state: A(s), which is a subset of whole action
space A =
?
s?S A(s). Policy ? is the strat-
egy for selecting action, represented as a con-
ditional distribution of actions: p(a|s).
3. It then observes next state s? and receives re-
ward r from the environment.
The aim of reinforcement learning is to find optimal
policy ?? only with information on sample trajecto-
ries and to reward the experienced agent.
We describe how to adapt the extractive approach
to the problem of reinforcement learning in the sec-
tions that follow.
4.2 State
A state denotes a summary. We represent state s
as a tuple of summary S (a set of textual units) and
additional state variables: s = (S,A, f). We assume
s has the history of actionsA that the agent executed
to achieve this state. Additionally, s has the binary
state variable, f ? {0, 1}, which denotes whether s
is a terminal state or not. Initial state s0 is (?, ?, 0).
We assume the d-dimensional feature representa-
tion of state s: ?(s) ? Rd, which only depends on
the feature of summary ??(S) ? Rd?1. Given ??(S),
we define the features as:
?(s) =
{
(??(S), 0)T (L(S) ? K)
(0, 1)T (K < L(S)) . (2)
This definition denotes that summaries that violate
the length limitation are shrunk to a single feature,
(0, 1)T, which means it is not a summary.
Note the features of the state only depend on the
features of the summary, not on the executed actions
to achieve the state. Unlike naive search methods,
this property has the potential for different states to
be represented as the same vector, which has the
same features. The agent, however, should search as
many possible states as it can. Therefore, the gen-
eralization function of the feature representation is
of utmost importance. The accurate selection of fea-
tures contributes to reducing the search space and
provides efficient learning as will be discussed later.
4.3 Action
An action denotes a transition operation that pro-
duces a new state from a current state. We assumed
all actions were deterministic in this study. We de-
fine inserti(1 ? i ? n) actions, each of which
inserts textual unit xi to the current state unless the
state is terminated, as described in the following di-
258
agram:
st at st+1
?
?
St
At
0
?
?
inserti?????
?
?
St ? {xi}
At ? {inserti}
0
?
? . (3)
In addition to insertion actions, we define finish
that terminates the current episode in reinforcement
learning:
st at st+1
?
?
St
At
0
?
?
finish?????
?
?
St
At ? {finish}
1
?
? (4)
Note that ft = 1 means state st is a terminal state.
Then, the whole action set, A, is defined by
inserti and finish:
A = {insert1, insert2, ? ? ? , insertn,finish}. (5)
We can calculate the available actions limited by
state st:
A(st) =
{
A\At (L(St) ? K)
{finish} (K < L(St))
. (6)
This definition means that the agent may execute one
of the actions that have not yet been executed in this
episode, and it has no choice but to finish if the sum-
mary of the current state already violates length lim-
itations.
4.4 Reward
The agent receives a reward from the environment
as some kind of criterion of how good the action the
agent executed was. If the current state is st, the
agent executes at, and the state makes a transition
into st+1; then, the agent receives the reward, rt+1:
rt+1=
?
?
?
score(St) (at = finish, L(St) ? K)
?Rpenalty (at = finish,K < L(St))
0 (otherwise)
, (7)
where Rpenalty > 0.
The agent can receive the score awarded by the
given score function if and only if the executed ac-
tion is finish and the summary length is appropri-
ate. If the summary length is inappropriate but the
executed action is finish, the environment awards
a penalty to the agent. The most important point of
this definition is that the agent receives nothing un-
der the condition where the next state is not termi-
nated. In this sense, the reward is delayed. Due to
this definition, maximizing the expectation of future
rewards is equivalent to maximizing the given score
function, and we do not need to consider the decom-
posed form of the score function, i.e., we only need
to consider the final score of the whole summary.
4.5 Value Function Approximation
Our aim is to find the optimal policy. This is
achieved by obtaining the optimal state value func-
tion, V ?(s), because if we obtain this, the greedy
policy is optimal, which determines the action so as
to maximize the state value after the transition oc-
curred. Therefore, our aim is equivalent to finding
V ?(s). Let us try to estimate the state value func-
tion with parameter ? ? Rd:
V (s) = ?T?(s). (8)
We can also represent and estimate the action value
function, Q(s, a), by using V (s):
Q(s, a) = r + ?V (s?), (9)
where the execution of a causes the state transition
from s to s? and the agent receives reward r, and
?(0 ? ? ? 1) is the discount rate. Note that all
actions are deterministic in this study.
By using these value functions, we define the
policy as the conditional distribution, p(a|s; ?, ?),
which is parameterized by ? and a temperature pa-
rameter ? :
p(a|s; ?, ?) = e
Q(s,a)/?
?
a? eQ(s,a
?)/? . (10)
Temperature ? decreases as learning progresses,
which causes the policy to be greedier. This softmax
selection strategy is called Boltzmann selection.
4.6 Learning Algorithm
The goal of learning is to estimate ?. We use the
TD (?) algorithm with function approximation (Sut-
ton and Barto, 1998). Algorithm 1 represents the
whole system of our method, called Automatic Sum-
marization using Reinforcement Learning (ASRL)
259
Algorithm 1 ASRL
Input: document D = {x1, x2, ? ? ? , xn},
score function score(S)
1: initialize ? = 0
2: for k = 1 to N do
3: s? (?, ?, 0)
// initial state
4: e = 0
5: while s is not terminated do
6: a ? p(a|s; ?, ?k)
// selects action with current policy
7: (s?, r)? execute(s, a)
// observes next state and receive reward
8: ? ? r + ??T?(s?)? ?T?(s)
// calculates TD-error
9: e? ??e + ?(s)
// updates the eligibility trace
10: ? ? ? + ?k?e
// learning with current learning rate
11: s? s?
12: end while
13: end for
14: s? (?, ?, 0)
15: while s is not terminated do
16: a? maxa Q(s, a)
// selects action greedily
with the learned policy
17: (s?, r)? execute(s, a)
18: s? s?
19: end while
20: return the summary of s
in this paper. N is the number of learning episodes,
and e(? Rd) and ?(0 ? ? ? 1) correspond to the el-
igibility trace and the trace decay parameter. The el-
igibility trace, e, conveys all information on the fea-
tures of states that the agent previously experienced,
with previously decaying influences of features due
to decay parameter ? and discount rate ? (Line 9).
Line 1 initializes parameter ? to start up its learn-
ing. The following procedures from Lines 2 to 13
learn ? with the TD (?) algorithm, by using infor-
mation on actual interactions with the environment.
Learning rate ?k and temperature parameter ?k de-
cay as the learning episode progresses. The best
summary with the obtained policy is calculated in
steps from Lines 14 to 19. If the agent can estimate
? properly, greedy output is the optimal solution.
5 Models of Combined Approach for
Reinforcement Learning
We formulated the extractive approach as a problem
with reinforcement learning in the previous section.
In fact, we can also formulate a more general model
of summarization, since evaluation only depends on
the final state and it is not actually very important to
regard the given documents as a set of textual units
contained in the original documents.
We explain how to take into account other meth-
ods within the ASRL framework by modifying the
models in this section, with an example of sentence
compression. We assume that we have a method of
sentence compression, comp(x), and that a textual
unit to be extracted is a sentence. What we have to
do is to only simply modify the definitions of the
state and action. Note that this is just one example
of the combined method. Even other summarization
systems can be similarly adapted to ASRL.
5.1 State
We do not want to execute sentence compression
twice, so we have to modify the state variables to
convey the information: s = (S,A, c, f), where
c ? {0, 1}, and S,A, and f are the same definitions
as previously described.
5.2 Action
We add deterministic action comp toA, which pro-
duces the new summary constructed by compressing
the last inserted sentence of the current summary:
st at st+1
?
?
?
?
St
At
0
0
?
?
?
?
comp?????
?
?
?
?
St\{xc} ? {comp(xc)}
At ? {comp}
1
0
?
?
?
?
,(11)
where xc is the last sentence that is inserted into St.
Next, we modify inserti and finish:
st at st+1
?
?
?
?
St
At
ct
0
?
?
?
?
inserti?????
?
?
?
?
St ? {xi}
At ? {inserti}
0
0
?
?
?
?
,(12)
260
st at st+1
?
?
?
?
St
At
ct
0
?
?
?
?
finish?????
?
?
?
?
St
At ? {finish}
ct
1
?
?
?
?
. (13)
Note comp ? A(st) may be executed if and only if
ct = 0. inserti resets c to 0.
6 Experiments
We conducted three experiments in this study. First,
we evaluated our method with ROUGE metrics
(Lin, 2004), in terms of ROUGE-1, ROUGE-2, and
ROUGE-L. Second, we conducted an experiment on
measuring the optimization capabilities of ASRL,
with the scores we obtained and the execution time.
Third, we evaluated ASRL taking into consideration
sentence compression by using a very naive method,
in terms of ROUGE-1, ROUGE-2, and ROUGE-3.
6.1 Experimental Settings
We used sentences as textual units for the extrac-
tive approach in this research. Each sentence and
document were represented as a bag-of-words vec-
tor with tf*idf values, with stopwords removed. All
tokens were stemmed by using Porter?s stemmer
(Porter, 1980).
We experimented with our proposed method on
the dataset of DUC2004 task2. This is a multi-
document summarization task that contains 50 docu-
ment clusters, each of which has 10 documents. We
set up the length limitation to 665 bytes, used in the
evaluation of DUC2004.
We set up the parameters of ASRL where the
number of episodes N = 300, the training rate
?k = 0.001 ? 101/(100 + k1.1), and the tempera-
ture ?k = 1.0 ? 0.987k?1 where k was the number of
episodes that decayed as learning progressed. Both
discount rate ? and trace decay parameter ? were
fixed to 1 for episodic tasks. The penalty, Rpenalty,
was fixed to 1.
We used the following score function in this
study:
score(S) =
?
xi?S
?sRel(xi)
?
?
xi,xj?S,i<j
(1? ?s)Red(xi, xj), (14)
where
Rel(xi) = Sim(xi, D) + Pos(xi)?1 (15)
Red(xi, xj) = Sim(xi, xj). (16)
?s is the parameter for the trade-off between
relevance and redundancy, Sim(xi, D) and
Sim(xi, xj) correspond to the cosine similarities
between sentence xi and the sentence set of the
given original documents D, and between sentence
xi and sentence xj . Pos(xi) is the position of
the occurrence of xi when we index sentences in
each document from top to bottom with one origin.
This score function was determined by reference
to McDonald (2007). We set ?s = 0.9 in this
experiment.
We designed ??(S), i.e., the vector representation
of a summary, to adapt it to the summarization prob-
lem as follows.
? Coverage of important words: The elements
are the top 100 words in terms of the tf*idf of
the given document with binary representation.
? Coverage ratio: This is calculated by counting
up the number of top 100 elements included in
the summary.
? Redundancy ratio: This is calculated by
counting up the number of elements that exces-
sively cover the top 100 elements.
? Length ratio: This is the ratio between the
length of the summary and length limitationK.
? Position: This feature takes into consideration
the position of sentence occurrences. It is cal-
culated with
?
x?S Pos(x)?1.
Consequently, ??(S) is a 104-dimensional vector.
We executed ASRL 10 times with the settings pre-
viously described and used all the results for evalu-
ation.
We used the dataset of DUC2003, which is a simi-
lar task that contains 30 document clusters and each
cluster had 10 documents, to determine ?k and ?s.
We determined the parameters so that they would
converge properly and become close to the opti-
mal solutions calculated by ILP, under the condi-
tions that the described feature representation and
the score function were given.
261
ROUGE-1 ROUGE-2 ROUGE-L
ASRL 0.39013 0.09479 0.33769
MCKP 0.39033 0.09613 0.34225
PEER65 0.38279 0.09217 0.33099
ILP 0.34712 0.07528 0.31241
GREEDY 0.30618 0.06400 0.27507
Table 1: Results of ROUGE evaluation compared with
other peers in DUC2004. Scores for ILP and GREEDY
have statistically significant differences from scores of
ASRL.
6.2 Evaluation
We compared ASRL with four other conventional
methods.
? GREEDY: This method is a simple greedy al-
gorithm, which repeats the selection of the sen-
tence with the highest score of the remaining
sentences by using an MMR-like method of
scoring as follows:
x = argmax
x?D\S
[?sRel(x)
?(1? ?s)max
xi?S
Red(x, xi)], (17)
where S is the current summary.
? ILP: This indicates the method proposed by
McDonald (2007) for maximizing the score
function (14) with integer linear programming.
? PEER65: This is the best performing system in
task 2 of the DUC2004 competition in terms of
ROUGE-1 proposed by Conroy et al2004).
? MCKP: This method was proposed by Taka-
mura and Okamura (2009). MCKP defines an
automatic summarization problem as a maxi-
mum coverage problem with a knapsack con-
straint, which uses conceptual units (Filatova
and Hatzivassiloglou, 2004), and composes the
meaning of sentences, as textual units and at-
tempts to cover as many units as possible under
the knapsack constraint.
7 Results
7.1 Evaluation with ROUGE
We evaluated our method of ASRL with ROUGE,
in terms of ROUGE-1, ROUGE-2, and ROUGE-L.
ROUGE-1 ROUGE-2 ROUGE-L
ASRL.0 0.39274 0.09537 0.34010
ASRL.1 0.39243 0.09683 0.33855
ASRL.2 0.39241 0.09597 0.34070
ASRL.3 0.39190 0.09580 0.33898
ASRL.4 0.39054 0.09579 0.33663
ASRL.5 0.38911 0.09395 0.33551
ASRL.6 0.38866 0.09392 0.33701
ASRL.7 0.38854 0.09338 0.33661
ASRL.8 0.38821 0.09363 0.33833
ASRL.9 0.38532 0.09281 0.33321
Table 2: Results of ROGUE evaluation for each ASRL
peer of 10 results in DUC2004. ASRL did not converge
with stable solution with these experimental settings be-
cause of property of randomness.
The experimental results are summarized in Tables
1 and 2. Table 1 lists the results for the comparison
and Table 2 lists all the results for ASRL peers.
The results imply ASRL is superior to PEER65,
ILP, and GREEDY, and comparable to MCKP with
these experimental settings in terms of ROUGEmet-
rics. Note that ASRL is a kind of approximate
method, because actions are selected probabilisti-
cally and the method of reinforcement learning oc-
casionally converges with some sub-optimal solu-
tion. This can be expected from Table 2, which in-
dicates the results vary although each ASRL solu-
tion converged with some solution. However, in this
experiment, ASRL achieved higher ROUGE scores
than ILP, which achieved optimal solutions. This
seems to have been caused by the properties of the
features, which we will discuss later. It seems this
feature representation is useful for efficiently search-
ing the feature space. The method of mapping a state
to features is, however, approximate in the sense that
some states will shrink to the same feature vector,
and ASRL therefore has no tendency to converge
with some stable solution.
7.2 Evaluation of Optimization Capabilities
Since we proposed our method as an approach to ap-
proximate optimization, there was the possibility of
convergence with some sub-optimal solution as pre-
viously discussed. We also evaluated our approach
from the point of view of the obtained scores and the
execution time to confirm whether our method had
262
0 50 100 150 200 250 300
Episode
?0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
A
v
e
r
a
g
e
 
o
f
 
S
c
o
r
e
s
ASRL
GREEDY
ILP
Figure 1: Average score for each episode in ASRL in
DUC2004. Horizontal lines indicate scores of summaries
obtained with ILP and GREEDY.
optimization capabilities.
The experimental results are plotted in Figures
1 and 2. Figure 1 plots the average for the re-
wards (i.e., scores) that the agent obtained for each
episode. The horizontal line for ILP is the average
for the optimal scores of (14). The score in ASRL
increases as the number of episodes increases, and
overtakes the score of GREEDY at some episode.
The agent attempts to come close to the optimal
score line of ILP but seems to fail, and finally con-
verges to some local optimal solution. We should
increase the number of episodes, adjust parameters
? and ? , and select more appropriate features for
the state to improve the optimization capabilities of
ASRL.
Figure 2 plots the execution time for each peer.
The horizontal axis is the number of textual units,
i.e., the number of sentences in this experiment. The
vertical axis is the execution time taken by the task.
The plots of ASRL and ILP fit a linear function for
the former and an exponential function for the lat-
ter. The experimental results indicate that while the
execution time for ILP tends to increase exponen-
tially, that for ASRL increases linearly. The time
complexity of ASRL is linear with respect to the
number of actions because the agent has to select
the next action from the available actions for each
episode, whose time complexity is naively O(|A|).
As inserti actions are dominant in the extractive
100 200 300 400 500 600 700
The number of textual units
0
1000
2000
3000
4000
5000
6000
E
x
e
c
u
t
i
o
n
 
t
i
m
e
 
(
s
e
c
)
ASRL
ASRL(fit)
ILP
ILP(fit)
Figure 2: Execution time on number of textual units for
each problem in DUC2004. Plot of ASRL is fitted to lin-
ear function and that of ILP is fitted to exponential func-
tion.
approach, the execution time increases linearly with
respect to the number of textual units. However, ILP
has to take into account the combinations of textual
units, whose number increases exponentially.
In conclusion, both the experimental results in-
dicate that ASRL efficiently calculated a summary
that was sub-optimal, but that was of relatively high-
quality in terms of ROUGE metrics, with the exper-
imental settings we used.
7.3 Evaluation of Effects of Sentence
Compression
We also evaluated the combined approach with sen-
tence compression. We evaluated the method de-
scribed in Section 5 called ASRLC in this experi-
ment for the sake of convenience. We used a very
naive method of sentence compression for this ex-
periment, which compressed a sentence to only im-
portant words, i.e., selecting word order by using
the tf*idf score to compress the length to about
half. This method of compression did not take into
consideration either readability or linguistic quality.
Note we wanted to confirm what effect the other
methods would have, and we expected this to im-
prove the ROUGE-1 score. We used the ROUGE-3
score in this evaluation instead of ROUGE-L, to con-
firm whether naive sentence compression occurred.
The experimental results are summarized in Ta-
263
ROUGE-1 ROUGE-2 ROUGE-3
ASRL 0.39013 0.09479 0.03435
ASRLC 0.39141 0.09259 0.03239
Table 3: Evaluation of combined methods.
ble 3, which indicates ROUGE-1 increases but
ROUGE-2 and ROUGE-3 decrease as expected. The
variations, however, are small. This phenomenon
was reported by Lin (2003) in that the effectiveness
of sentence compression by local optimization at the
sentence level was insufficient. Therefore, we would
have to consider the range of applications with the
combined method.
8 Discussion
8.1 Local Optimality of ASRL
We will discuss why ASRL seems to converge with
some ?good? local optimum with the described ex-
perimental settings in this section.
Since our model of the state value function was
simply linear and our parameter estimation was im-
plemented by TD (?), which is a simple method
in RL, it seems simply employing more efficient
or state-of-the-art reinforcement learning methods
may improve the performance of ASRL, such as
GTD and GTD2 (Sutton et al2009b; Sutton et al
2009a). These methods basically only contribute to
faster convergence, and the score that they will con-
verge to might not differ significantly. As a result, it
would not matter much which method was used for
optimization.
The main point of this problem is modeling the
feature representation of states, and this causes
sub-optimality. The vector representation of states
shrinks the different states to a single representation,
i.e., the agent regards states whose features are simi-
lar to be similar states. Due to this property, the pol-
icy of reinforcement learning is learned to maximize
the expected score of each feature vector, which
includes many states. Such sub-optimality aver-
agely balanced by the feature representation raises
the possibility of achieving states that have a high-
quality summary with a low score, since we do not
have a genuine score function.
Thus, the most important thing in our method
is to intentionally design the features of states and
the score function, so that the agent can generalize
states, while taking into consideration truly-essential
features for the required summarization. It would be
useful if the forms of features and the score function
could be arbitrarily designed by the user because
there is the capability of obtaining a high-quality
summaries.
8.2 Potential of Combined Method
Other useful methods, even other summarization
systems, can easily be adapted to ASRL as was de-
scribed in Section 5. The experimental results re-
vealed that sentence compression has some effect.
In fact, all operations that produce a new summary
from an old summary can be used, i.e., even other
summarizing methods can be employed for an ac-
tion. We assumed a general combined method may
have a great deal of potential to enhance the quality
of summaries.
8.3 Can We Obtain ?a Global Policy??
We formulated each summarization task as a rein-
forcement learning task in this paper, i.e., where
each learned policy differs. As this may be a little
unnatural, we wanted to obtain a single learned pol-
icy, i.e., a global policy.
However, we assessed that we cannot achieve a
global policy with these feature and score function
settings because the best vector, which is the fea-
ture representation of the summary that achieves an
optimal score under the current settings, seems to
vary for each cluster, even if the domain of the clus-
ters is the same (e.g., a news domain). Having said
that, we simultaneously surmised that we could ob-
tain a global policy if we could obtain a highly gen-
eral, crucial, and efficient feature representation of a
summary. We also think a global policy is essential
in terms of reinforcement learning and we intend to
attempt to achieve this in future work.
9 Conclusion
We presented a new approach to the problem of
automatic text summarization called ASRL in this
paper, which models the process of constructing
a summary with the framework of reinforcement
learning and attempts to optimize the given score
function with the given feature representation.
264
The experimental results demonstrated ASRL
tends to converge sub-optimally, and excessively de-
pends on the formulation of features and the score
function. Although it is difficult, we believe this for-
mulation would enable us to improve the quality of
summaries by designing them freely.
We intend to employ the ROUGE score as the
score function in future work, and obtain the param-
eters of the state value function. Using these results,
we will attempt to obtain a single learned policy by
employing the ROUGE score or human evaluations
as rewards. We also intend to consider efficient fea-
tures and a score to achieve stable convergence. In
addition, we plan to use other methods of function
approximation, such as RBF networks.
References
J.M. Conroy, J.D. Schlesinger, J. Goldstein, and D.P. O ?
leary. 2004. Left-brain/right-brain multi-document
summarization. In Proceedings of the Document Un-
derstanding Conference (DUC 2004).
E. Filatova and V. Hatzivassiloglou. 2004. A formal
model for information selection in multi-sentence text
extraction. In Proceedings of the 20th international
conference on Computational Linguistics, page 397.
Association for Computational Linguistics.
J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz.
2000. Multi-document summarization by sentence
extraction. In Proceedings of the 2000 NAACL-
ANLPWorkshop on Automatic summarization-Volume
4, pages 40?48. Association for Computational Lin-
guistics.
C.Y. Lin. 2003. Improving summarization performance
by sentence compression: a pilot study. In Proceed-
ings of the sixth international workshop on Informa-
tion retrieval with Asian languages-Volume 11, pages
1?8. Association for Computational Linguistics.
C.Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In Proceedings of the workshop
on text summarization branches out (WAS 2004), vol-
ume 16.
I. Mani. 2001. Automatic summarization, volume 3.
John Benjamins Pub Co.
R. McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. Advances
in Information Retrieval, pages 557?564.
MF Porter. 1980. An algorithm for suffix stripping.
Program: electronic library and information systems,
14(3):130?137.
R.S. Sutton and A.G. Barto. 1998. Reinforcement learn-
ing: An introduction, volume 1. Cambridge Univ
Press.
R.S. Sutton, H.R. Maei, D. Precup, S. Bhatnagar,
D. Silver, C. Szepesva?ri, and E. Wiewiora. 2009a.
Fast gradient-descent methods for temporal-difference
learning with linear function approximation. In Pro-
ceedings of the 26th Annual International Conference
on Machine Learning, pages 993?1000. ACM.
R.S. Sutton, C. Szepesva?ri, and H.R. Maei. 2009b. A
convergent o (n) algorithm for off-policy temporal-
difference learning with linear function approxima-
tion.
H. Takamura and M. Okumura. 2009. Text summariza-
tion model based on maximum coverage problem and
its variant. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 781?789. Association for
Computational Linguistics.
265
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 63?66,
Beijing, August 2010
Helping Volunteer Translators, Fostering Language Resources
Masao Utiyama
MASTAR Project
NICT
mutiyama@nict.go.jp
Takeshi Abekawa
National Institute
of Informatics
abekawa@nii.ac.jp
Eiichiro Sumita
MASTAR Project
NICT
eiichiro.sumita@nict.go.jp
Kyo Kageura
Tokyo University
kyo@p.u-tokyo.ac.jp
Abstract
This paper introduces a website called
Minna no Hon?yaku (MNH, ?Translation
for All?), which hosts online volunteer
translators. Its core features are (1) a
set of translation aid tools, (2) high qual-
ity, comprehensive language resources,
and (3) the legal sharing of translations.
As of May 2010, there are about 1200
users and 4 groups registered to MNH.
The groups using it include such major
Figure 1: Screenshot of ?Minna no Hon?yaku?NGOs as Amnesty International Japan
site (http://trans- )and Democracy Now! Japan. aid.jp
1 Introduction
This paper introduces a website called Minna Second, MNH provides comprehensive lan-
no Hon?yaku (MNH, ?Translation for All?, Fig- guage resources, which are easily looked up in
ure 1), which hosts online volunteer translators QRedit. MNH, in cooperation with Sanseido,
(Utiyama et al, 2009).1 Its core features are (1) a provides ?Grand Concise English Japanese Dic-
set of translation aid tools, (2) high quality, com- tionary? (Sanseido, 2001) and plans to provide
prehensive language resources, and (3) the legal ?Grand Concise Japanese English Dictionary?
sharing of translations. (Sanseido, 2002) in fiscal year 2010. These dic-
First, the translation aid tools in MNH con- tionaries have about 360,000 and 320,000 en-
sist of the translation aid editor, QRedit, a bilin- tries, respectively, and are widely accepted as
gual concordancer, and a bilingual term extrac- standard and comprehensive dictionaries among
tion tool. These tools help volunteer translators translators. MNH also provides seamless access
to translate their documents easily as described to the web. For example, MNH provides a dictio-
in Section 3. These tools also produce language nary that was made from the English Wikipedia.
resources that are useful for natural language This enable translators to reference Wikipedia
processing as the byproduct of their use as de- articles during the translation process as if they
scribed in Section 4. are looking up dictionaries.
1Currently, MNH hosts volunteer translators who trans- Third, MNH uses Creative Commons Li-
late Japanese (English) documents into English (Japanese). censes (CCLs) to help translators share their
The English and Japanese interfaces are available at http: translations. CCLs are essential for sharing and//trans-aid.jp/en and http://trans-aid.
jp/ja, respectively. opening translations.
63
Figure 2: Screenshot of QRedit
2 Related work
There are many translation support tools, such
as Google Translator Toolkit, WikiBABEL (Ku-
maran et al, 2009), BEYtrans (Bey et al, 2008),
Caitra (Koehn, 2009) and Idiom WorldServer
system,2 an online multilingual document man-
agement system with translation memory func-
tions.
The functions that MNH provides are closer
to those provided by Idiom WorldServer, but
MNH provides a high-quality bilingual dictio-
naries and functions for seamless Wikipedia and
web searches within the integrated translation
aid editor QRedit. It also enables translators to
share their translations, which are also used as
language resources.
3 Helping Volunteer translators
This section describes a set of translation aid
tools installed in MNH.
3.1 QRedit
QRedit is a translation aid system which is de-
signed for volunteer translators working mainly
online (Abekawa and Kageura, 2007). When a
URL of a source language (SL) text is given to
QRedit, it loads the corresponding text into the
left panel, as shown in Figure 2. Then, QRedit
automatically looks up all words in the SL text.
When a user clicks an SL word, its translation
candidates are displayed in a pop-up window.
2http://www.idiominc.com/en/
Figure 3: Screenshot of bilingual concordancer
3.2 Bilingual concordancer
The translations published on MNH are used
to make a parallel corpus by using a sentence
alignment method (Utiyama and Isahara, 2003).
MNH also has parallel texts from the Amnesty
International Japan, Democracy Now! Japan,
and open source software manuals (Ishisaka et
al., 2009). These parallel texts are searched by
using a simple bilingual concordancer as shown
in Figure 3.
3.3 Bilingual term extraction tool
MNH has a bilingual term extraction tool that
is composed of a translation estimation tool
(Tonoike et al, 2006) and a term extraction tool
(Nakagawa and Mori, 2003).
First, we apply the translation estimation tool
to extract Japanese term candidates and their En-
glish translation candidates. Next, we apply the
term extraction tool to extract English term can-
didates. If these English term candidates are
found in the English translation candidates, then,
we accept these term candidates as the transla-
tions of those Japanese term candidates.
4 Fostering language resources
Being a ?one stop? translation aid tool for on-
line translators, MNH incorporates mechanisms
which enable users to naturally foster impor-
tant translation resources, i.e. terminological re-
sources and translation logs.
64
4.1 Terminological resources
As with most translation-aid systems, MNH pro-
vides functions that enable users to register their
own terminologies. Users can assign the status
of availability to the registered terms. They can
keep the registered terms for private use, make
them available for a specified group of people,
or make them publicly available. Several NGO
groups are using MNH for their translation activ-
ities. For instance, Amnesty International, which
uses MNH, maintains a list of term translations
in the field of human rights by which translators
should abide. Thus groups such as Amnesty up-
load a pre-compiled list of terms and make them
available among volunteers. It is our assumption
and aim that these groups make their termino-
logical resources not only available among the
group but also publicly available, which will cre-
ate win-win situation: NGOs and other groups
which make their lists of terms available will
have more chance of recruiting volunteer trans-
lators, while MNH has more chance of attracting
further users.
At the time of writing this paper (May 2010),
56,319 terms are registered, of which 45,843 are
made publicly available. More than 80 per cent
of the registered terms are made public. Cur-
rently, MNH does not identify duplicated terms
registered by different users, but when the num-
ber of registered terms become larger, this and
other aspects of quality control of registered
terms will become an important issue.
4.2 Translation corpus
Another important language resources accumu-
lated on MNH is the translation corpus. As
mentioned in the introduction, being a hosting
site, MNH naturally accumulates source and tar-
get documents with a clear copyright status. Of
particular importance in MNH, however, is that
it can accumulate a corpus that contains draft
and final translations made by human together
with their source texts (henceforth SDF corpus
for succinctness). This type of corpus is im-
portant and useful, because it can be used for
the training of inexperienced translators (for in-
stance, the MeLLANGE corpus, which contains
different versions of translation, is well known
for its usefulness in translator training (MeL-
LANGE, 2009)) and also because it provides
a useful information for improving the perfor-
mance of machine translation and translation-aid
systems. While the importance of such corpora
has been widely recognized, the construction of
such a corpus is not easy because the data are
not readily available due to the reluctance on the
side of translators of releasing the draft transla-
tion data.
The basic mechanisms of accumulating SDF
corpus is simple. Translators using MNH save
their translations to keep the data when they fin-
ish the translation. MNH keeps the log of up
to 10 versions of translation for each document.
MNH introduced two saving modes, i.e. snap-
shot mode and normal mode. The translation
version saved in the normal mode is overwrit-
ten when the next version is saved. Translation
versions saved in snapshot mode are retained, up
to 10 versions. Translators can thus consciously
keep the versions of their translations.
MNH can collect not only draft and final trans-
lations made by a single translator, but also those
made by different translators. MNH has a func-
tion that enables users to give permission for
other translators registered with MNH to edit
their original translations, thus facilitating the
collaborative translations. Such permission can
be open-ended, or restricted to a particular group
of users.
This function is of particular importance
for NGOs, NPOs, university classes and other
groups involved in group-based translation. In
these groups, it is a common process in transla-
tion that a draft translation is first made by inex-
perienced translators, which is then revised and
finalized by experienced translators. If an inex-
perienced translator gives permission of editing
his/her draft translations to experienced transla-
tors, the logs of revisions, including the draft and
final versions, will be kept on MNH database.
This is particularly important and useful for
the self-training of inexperienced translators and
thus potentially extremely effective for NGOs
and other groups that rely heavily on volunteer
65
Figure 4: Comparative view of different transla-
tion versions
translators. Many NGOs face chronically the
problem of a paucity of good volunteer transla-
tors. The retention rate of volunteer translators is
low, which increase the burden of a small num-
ber of experienced translators, leaving them no
time to give advice to inexperienced translators,
which further reduce the retention rate of volun-
teers. To overcome this vicious cycle, mecha-
nisms to enable inexperienced volunteer trans-
lators to train themselves in the cycle of actual
translation activities is urgently needed and ex-
pected to be highly effective. MNH provides a
comparative view function of any pairwise trans-
lation versions of the same document, as shown
in Figure 4. Translators can check which parts
are modified very easily through the compara-
tive view screen, which can effectively works as
a transfer of translation knowledge from experi-
enced translators to inexperienced translators.
At the time of writing this paper, MNH con-
tains 1850 documents that have more than one
translation versions, of which 764 are published.
The number of documents translated by a group
(more than one translator) is 110, of which 48 are
published. Although the number of translations
made by more than one translators is relatively
small, they are steadily increasing both in num-
ber and in ratio.
5 Conclusion
We have developed a website called Minna no
Hon?yaku (MNH, ?Translation for All?), which
hosts online volunteer translators. We plan to ex-
tend MNH to other language pairs in our future
work.
References
Abekawa, Takeshi and Kyo Kageura. 2007. QRedit:
An integrated editor system to support online vol-
unteer translators. In Digital humanities, pages 3?
5.
Bey, Y., K. Kageura, and C. Boitet. 2008. BEY-
Trans: A Wiki-based environment for helping on-
line volunteer translators. Yuste, E. ed. Topics in
Language Resources for Translation and Localisa-
tion. Amsterdam: John Benjamins. p. 139?154.
Ishisaka, Tatsuya, Masao Utiyama, Eiichiro Sumita,
and Kazuhide Yamamoto. 2009. Development of
a Japanese-English software manual parallel cor-
pus. In MT summit.
Koehn, Philipp. 2009. A web-based interactive com-
puter aided translation tool. In ACL-IJCNLP Soft-
ware Demonstrations.
Kumaran, A, K Saravanan, Naren Datha, B Ashok,
and Vikram Dendi. 2009. Wikibabel: A wiki-style
platform for creation of parallel data. In ACL-
IJCNLP Software Demonstrations.
MeLLANGE. 2009. Mellange. ttp://corpus.
leeds.ac.uk/mellange/ltc.tml.
Nakagawa, Hiroshi and Tatsunori Mori. 2003. Au-
tomaic term recognition based on statistics of com-
pound nouns and their components. Terminology,
9(2):201?209.
Sanseido. 2001. Grand Concise English Japanese
Dictionary. Tokyo, Sanseido.
Sanseido. 2002. Grand Concise Japanese English
Dictionary. Tokyo, Sanseido.
Tonoike, Masatsugu, Mitsuhiro Kida, Toshihiro Tak-
agi, Yasuhiro Sasaki, Takehito Utsuro, and Satoshi
Sato. 2006. A comparative study on composi-
tional translation estimation usign a domain/topic-
specific corpus collected from the web. In Proc. of
the 2nd International Workshop on Web as Corpus,
pages 11?18.
Utiyama, Masao and Hitoshi Isahara. 2003. Reli-
able measures for aligning Japanese-English news
articles and sentences. In ACL, pages 72?79.
Utiyama, Masao, Takeshi Abekawa, Eiichiro Sumita,
and Kyo Kageura. 2009. Hosting volunteer trans-
lators. In MT summit.
66
