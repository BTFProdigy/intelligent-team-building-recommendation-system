Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 40?46,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic?Generation?of?Tamil?Lyrics?for?Melodies
Ananth?Ramakrishnan?A Sankar?Kuppan Sobha?Lalitha?Devi
AU?KBC?Research?Centre
MIT?Campus,?Anna?University
Chennai,?India
AU?KBC?Research?Centre
MIT?Campus,?Anna?University
Chennai,?India
AU?KBC?Research?Centre?
MIT?Campus,?Anna?University
Chennai,?India
ananthrk@au?kbc.org sankar@au?kbc.org sobha@au?kbc.org
Abstract
This ?paper ?presents ?our ?on?going ?work ? to?
automatically ? generate ? lyrics ? for ? a ? given?
melody, ? for ? phonetic ? languages ? such ? as?
Tamil.?We?approach?the?task?of?identifying?
the?required?syllable?pattern?for?the?lyric?as?
a?sequence?labeling?problem?and?hence?use?
the?popular?CRF++?toolkit ? for? learning. ?A?
corpus?comprising?of?10?melodies?was?used?
to?train?the?system?to?understand?the?syllable?
patterns.?The?trained?model?is?then?used?to?
guess?the?syllabic?pattern?for?a?new?melody?
to?produce?an?optimal?sequence?of?syllables.?
This?sequence?is?presented?to?the?Sentence?
Generation?module?which?uses?the?Dijkstra's?
shortest ?path?algorithm?to?come?up?with?a?
meaningful ? phrase ? matching ? the ? syllabic?
pattern.
1 Introduction
In?an?attempt?to?define?poetry?(Manurung,?2004),?
provides?three?properties?for?a?natural?language?arti?
fact?to?be?considered?a?poetic?work,?viz.,?Meaning?
fulness?(M),?Grammaticality?(G)?and?Poeticness?(P).?
A?complete?poetry?generation?system?must?generate?
texts?that?adhere?to?all?the?three?properties.?In?this?
work,?our?attempt?would?be?to?generate?meaningful?
lyrics?that?match?the?melody?and?the?poetic?aspects?
of?the?lyric?will?be?tackled?in?future?works.?
According?to?on?line?resources?such?as ?How?to?
write ? lyrics? (Demeter, ? 2001), ? the ? generated ? lyric?
must?have?Rhythm,?Rhyme?and?Repetition.?
One?of?the?recent?attempts?for?automatically?gen?
erating ? lyrics ? for ? a ? given ? melody ? is ? the ? Tra?la?
Lyrics?system?(Oliveira?et?al.,?2007).?This?system?
uses?the ?ABC? notation?(Gonzato,?2003)?for?repre?
senting?melody?and?the?corresponding?suite?of?tools?
for?analyzing?the?melodies.?The?key?aspect?of?the?
system ? is ? its ? attempt ? to ? detect ? the ? strong ? beats?
present?in?the?given?melody?and?associating?words?
with?stressed?syllables? in? the?corresponding?posi?
tions.?It?also?evaluates?three?lyric?generation?strate?
gies ? (Oliveira ? et ? al., ? 2007) ? ? ? random?
words+rhymes, ? sentence ? templates+rhymes ? and?
grammar+rhymes.??Of?these?strategies,?the?sentence?
templates+rhymes?approach?attempts?for?syntactical?
coherence?and?the?grammar+rhymes?approach?uses?
a?grammar?to?derive?Portuguese?sentence?templates.?
From?the?demo?runs?presented,?we?see?that?the?sys?
tem?can?generate?grammatical?sentences?(when?us?
ing?an?appropriate?strategy).?However,?there?is?no?
attempt?to?bring?Meaningfulness?in?the?lyrics.
2 Lyric?Generation?for?Tamil
Tamil,?our?target?language?for?generating?lyrics,?is?a?
phonetic? language. ?There? is ? a ?one?to?one? relation?
between?the?grapheme?and?phoneme.?We?make?use?
of?this?property?in?coming?up?with?a?generic?repre?
sentation?for?all?words?in?the?language.?This?repre?
sentation,?based?on?the?phonemic?syllables,?consists?
40
of? the? following? three? labels: ?Kuril? (short ?vowel,?
represented?by?K), ?Nedil? (long?vowel,?represented?
by?N)?and?Mei?(consonants,?represented?by?M).??For?
example,?the?word?thA?ma?rai?(lotus)?will?be?repre?
sented ? as ? N?K?N? (long ? vowel ? followed ?by ? short?
vowel?followed?by?another?long?vowel).?This?repre?
sentation?scheme,?herein?after?referred?as?KNM?rep?
resentation,?is?used?throughout?our?system???train?
ing,?melody?analysis?and?as?input?to?the?sentence?
generation?module.
3 Approach
Our ? approach ? to ? generating ? lyrics ? for ? the ? given?
melody?is?a?two?step?process?(Figure?1).?The?first?
step?is?to?analyze?the?input?melody?and?output?a?se?
ries ? of ? syllable ? patterns ? in ?KNM? representation?
scheme ? along ? with ? tentative ? word ? and ? sentence?
boundary.?The?subsequent?step?involves?filling?the?
syllable ?pattern ?with ?words ? from? the ? corpus ? that?
match?the?given?syllable?pattern?and?any?rhyme?re?
quirements.?We?approach?the?first?aspect?as?a?Se?
quence ? Labeling ? problem ? and ? use ? the ? popular?
CRF++ ? toolkit ? (Kudo, ? 2005) ? to ? label ? the ? input?
melody?in ?ABC? notation ?(Gonzato,?2003)? with?ap?
propriate?syllable?categories?(Kuril,?Nedil?and?Mei).?
This?system?is?trained?with?sample?film?songs?and?
their?corresponding?lyrics?(in ?KNM? scheme)?as?in?
put.?The?trained?model?is?then?used?to?label?the?giv?
en?input?melody.?The?syllable?pattern,?thus?generat?
ed?for?the?input?melody,?is?provided?to?a?Sentence?
Generation?Module?that?finds?suitable?lyrics?satisfy?
ing ? the ? following ? constraints: ? a.) ? Words ? should?
match?the?syllable?pattern?b.)The?sequence?of?words?
should?have?a?meaning.?We?achieve?this?by?using?
the?popular?Dijkstra's?Shortest?Path?Algorithm?(Cor?
men?et?al.,?1990)?against?a?pre?built?corpus?of?Uni?
gram?and?Bigram?of?Words.
4 Melody?Analysis
The?goal?of?the?Melody?Analysis?is?to?analyze?the?
input?melody?and?suggest?a?possible?KNM?represen?
tation?scheme?that?will?match?the?melody.?Since?our?
representation?of?melody?is?based?on?the?ABC?Nota?
tion?(Gonzato,?2003),?which?is?textual,?we?approach?
this?problem?as?labeling?the?ABC?notation?using?the?
KNM?representation?scheme.
Figure?1.?System?Approach
4.1 Characteristics?of?Melody
Every?melody?follows?a?Meter,?which?provides?the?
basic?design?principles?in?music.?Some?of?the?most?
frequently?used?meters?we?encountered?in?the?film?
songs?that?we?used?are?2/4,?3/4,?4/4,?6/8,?9/8?and?
12/8???that?indicate?the?number?of?Notes?played?in?
41
the?given?interval.?Each?Note?is?represented?by?the?
character?set?A,?B,?C,?D,?E,?F?and?G???which?are?
called?as?main?notes?and?A#,?C#,?D#,?F#?and?G#???
which?are?called?Sharp?Notes.?Thus,?for?any?given?
Meter?in?the?melody,?we?can?find?the?sequence?of?
Notes?with?the?corresponding?duration?for?which?the?
Note?is?played?in?that?meter.
For?the?purpose?of?generating?lyrics,?we?need?to?
fit?one?syllable?for?each?of?the?notes?in?the?melody.
4.2 Conditional?Random?Fields
Conditional ?Random?Fields(CRF) ? (Lafferty?et ?al.,?
2001)?is?a?Machine?Learning?technique?that?has?per?
formed?well?for?sequence?labeling?problems?such?as?
POS?tagging,?Chunking?and?Named?Entity?Recogni?
tion. ? It ? overcomes ? the ? difficulties ? faced ? in ? other?
techniques?like?Hidden?Markov?Models(HMM)?and?
Maximum?Entropy?Markov?Model(MEMM).
(Lafferty ?et ? al., ?2001) ?define?Conditional ?Ran?
dom?Fields?as?follows:??Let?G?=?(V,?E)?be?a?graph?
such?that?Y?=?(Yv)?v???V,?so?that?Y?is?indexed?by?the?
vertices?of ?G.?Then?(X,Y)?is?a?conditional?random?
field?in?case,?when?conditioned?on ?X, ?the?random?
variables?Yv?obey?the?Markov?property?with?respect?
to ? the ? graph: ? p(Yv|X,Yw,w?v) ? = ?p(Yv|X,Yw,w~v),?
where?w~v?means?that?w?and?v?are?neighbors?in?G?.
Here?X?denotes?a?sentence?and?Y?denotes?the?label?se?
quence.?The?label?sequence?y?which?maximizes?the?like?
lihood?probability?p?(y|x)?will?be?considered?as?the?cor?
rect ? sequence, ?while ? testing ? for ?new?sentence ?x ?with?
CRF?model??.?The?likelihood?probability?p?(y|x)?is?ex?
pressed?as?follows.
where??k?and??k?are?parameters?from?CRF?model???
and?fk?and?gk?are?the?binary?feature?functions?that?we?
need?to?give?for? training? the ?CRF? model.?This? is?
where?we?integrate?the?specific?features?of?the?prob?
lem?into?the?machine?learning?models?like?CRF.?
4.3 Feature?Templates
There?are?three?models?that?need?to?be?learnt,?viz,?
labeling?notes?with?KNM?scheme,?identifying?word?
boundaries ? and ? identifying ? line ? boundaries. ? We?
present?below?the?features?used?to?learn?each?of?the?
above.
4.3.1 Learning?KNM?labels
In?addition?to?the?labels?K,?N?and?M,?there?are?also?
other?non?syllable?features?that?need?to?be?identified?
in?the?melody.?Thus,?the?complete?list?of?labels?in?
clude,?K,?N,?KM,?NM,?TIE,?OPEN,?CLOSE,?PRE?
and?BAR.?
K???short?vowel
N???long?vowel
KM???short?vowel?followed?by?consonant
NM???long?vowel?followed?by?consonants
TIE???presence?of?a?Tie?in?the?meter
OPEN???opening?of?a?tie
CLOSE???closing?of?a?tie
PRE???Note?that?follows?a?tie
BAR???End?of?meter.
The?following?are?the?list?of?features?considered:
? Current?Note
? Previous?Note?+?Current?Note?+?Next?Note
? Previous?to?previous?Note?+?Previous?Note?
+?Current?Note?+?Next?Note?+?Next?to?next?
Note
? Current?Note/Duration
? Previous ? Note/Duration ? + ? Current?
Note/Duration?+?Next?Note/Duration
? Previous?to?previous?Note/Duration?+?Pre?
vious?Note/Duration?+?Current?Note/Dura?
tion?+?Next?Note/Duration?+?Next?to?next?
Note/Duration.
42
4.3.2 Word?Boundary
Another?important?aspect?in?analyzing?the?melody?is?
to?spot?potential?word?boundaries.?While?in?many?
cases,?the?presence?of?bars?could?indicate?potential?
word?boundaries,?there?are?also?cases?where?a?given?
word?can?span?a?bar?(especially?due?to?the?presence?
of?Ties).??Hence,?we?need?to?explicitly?train?our?sys?
tem?to?identify?potential?word?boundaries.?The?fea?
tures?used?to?identify?the?boundaries?of?words?are?
mostly?the?same?as?for?learning?the?KNM?labels,?but?
with?the?addition?of?considering?two?more?previous?
notes?along?with?their?durations.?
4.3.3 Sentence?Boundary
As?with?Word?Boundary? ,we?cannot?assume?sen?
tence?boundaries?based?on?the?musical?notation?and?
hence?we?also?train?our?system?to?identify?potential?
sentence?boundaries.?Sentence?boundary?identifica?
tion?happens?after?the?word?boundaries?are?identi?
fied?and?hence?this?additional?feature?is?used?along?
with ? the ? above?mentioned ? features ? for ? sentence?
boundary?training.?
5 Sentence?Generation
The?goal?of?the?Sentence?Generation?module?is?to?
generate?a?meaningful?phrase?that?matches?the?input?
pattern?given?in ?KNM? scheme.?For?example,?given?
an? input ?pattern?such?as ?'KMKM? NKM? ?NKN', ? it?
should?generate?a?phrase?consisting?of?three?words?
each?of?them?matching?their?respective?pattern.
5.1 Corpus?selection?and?pre?processing
Since ? we ? are ? interested ? in ? generating ? lyrics ? for?
melody,? the?corpus?we?chose?consisted?mainly?of?
poems?and?short ? stories. ?The?only?pre?processing?
involved ? was ? to ? remove ? any ? special ? characters?
(such?as??(?),?$?%?&,?etc.)?from?the?text.?From?this?
corpus, ? we ? index ? all ? Unigram ? and ? Bigram ? of?
Words.?Each?word?is?marked?with?its?KNM?syllable?
pattern?and?their?frequency?of?occurrence?in?the?cor?
pus.?The?Bigram?list?contains?only?the?frequency?of?
occurrence.?
5.2 Graph?Construction
Given?an?input?pattern?(say ?'KMKM?NKM?NKN'),?
we?construct?a?directed?graph?with?the?list?of?words?
satisfying?each?pattern,?as?represented?by?Figure?2.
Figure?2.?Graph?Construction
The?edge?from?word?Wij?(of,?say?pattern?KMKM)?to?
Wrs ?(of,?say?pattern?NKM)?is?weighted?based?on?the?
frequency?values?collected?from?the?corpus?and?is?
calculated?as?follows:
????????????????????????#?(Wij?followed?by?Wrs)?
P(Wrs?/?Wij)?=???___________________????(Eqn.?1)
????????????????????????????????#?(Wij)
Since? the?Shortest?Path?Algorithm?picks? the?path?
with?the?least?cost,?we?need?to?weight?the?edges?in?
such?a?way?that?a?higher?probability?sequence?gets?
the?least?cost?(C).?Thus,?we?measure?Cost(Wrs/Wij)?
as:
Cost(Wrs/Wij)?=?1???P(Wrs/Wij) (from?Eqn.?1)????(Eqn.?2)
By?default,?the?cost?from?the?START?node?to?the?
first?list?of?words?and?the?cost?from?the?last?list?of?
words?to?END?node?is?fixed?as?1.
5.3 Preferential?selection?of?paths
One?of?the?shortcomings?of?using?the?Shortest?Path?
Algorithm?is?that,?for?the?given?input?pattern?and?the?
given?Corpus,? the?algorithm?will ?always?generate?
the?same?phrase?(with?the?least?cost).?In?addition?to?
43
this?problem,?when?the?melody?demands,?we?need?
to?generate?rhyming?words.?Lastly,?we?need?to?han?
dle?the?case?where?the?corpus?may?not?have?a?phrase?
that?matches?the?complete?pattern.?We?tackle?all?the?
above?issues?by?biasing?the?Shortest?Path?Algorithm?
by?changing?the?cost?of?the?edges.?
5.3.1 Bias?initial?word
In?order?to?generate?different?phrases?for?the?same?
pattern?(say?KMKM??NKM??NKN),?we?pick?a?random?
word?that?matches?the?initial?pattern?(KMKM)?and?
fix?the?cost?of?the?edge?from?START?to?the?random?
word?to?0.?As?the?default?cost?from?START?to?all?
leading?words?is?1,?this?biases?the?algorithm?to?find?
a?pattern?that?starts?with?the?random?word.?Howev?
er,?if?there?exists?a?phrase,?whose??overall?cost??is?
still ? less ? than ? the ? one ? starting ? with ? the ? random?
phrase,?the?algorithm?will?output?the?same?phrase.?
In?order?to?avoid?this,?we?provide?multiple?random?
words?and?pick?the?one?that?truly?generates?a?unique?
phrase.
5.3.2 Rhyming?Words
When?there?is?a?need?to?generate?phrases?that?rhyme?
with?any?previously?generated?phrases,?especially?in?
line?endings,?we?use?the?same?biasing?technique?to?
prefer?certain?words?over?others.?The?motivation?to?
concentrate?on?line?endings?is?based?on?our?assump?
tion?that?the?notes?in?melody?would?be?similar?for?
the ? rhyming ? words ? and ? thus ? our ? representation?
scheme?involving?Mei(M)?(consonants)?would?han?
dle ? the ? stressed? syllables. ?The ?path? finding?algo?
rithm,?can?take?as?input?a?word?and?a?position,?with?
which?the?new?phrase?should?rhyme?in?the?given?po?
sition.?In?this?case,?we?generate?all?the?words?that?
rhyme?with?the?given?word?by?using?the?Maximum?
substring ? matching ? technique. ? That ? is, ? the ? word?
with?the?maximum?substring?common?to?the?input?
word,?in?word?endings,?is?considered?as?a?rhyming?
word. ? For ? example, ? given ? an ? input ? word ? 'kOyil'?
(temple),?the?rhyming?words?would?be?'vAyil'?(gate)?
and? 'veyil' ? (sun). ?As?can?be?seen,?both? the?words?
have?the?suffix? 'yil' ?common?with?the?input?word.?
Thus,?as?earlier,?the?cost?of?the?edges?in?the?paths?
leading?to?such?rhyming?words?will?be?set?to?0,?thus?
biasing?the?algorithm?to?pick?these?paths.?One?an?
other?way?would?be?have?only?those?nodes?corre?
sponding? to ? the ? rhyming?words ? (discarding?other?
non?rhyming?words).?However,? in?the?case?where?
no?rhyming?words?are?present?in?the?corpus,?this?ap?
proach?can?lead?to?a?graph?with?an?incomplete?path.?
Hence?we?use? the ?approach?of ?biasing? the ?graph?
paths?that?can?pick?the?rhyming?words,? if?present?
and?provide?a?non?rhyming?word,?if?none?was?avail?
able.
5.3.3 Edit?Distance?Matching
There?can?also?be?cases?when?there?is?no?phrase?that?
exactly?matches? the?given?input?pattern?sequence,?
though?the?corpus?might?contain?individual?words?
matching?each?pattern?in?the?sequence.?In?this?case,?
we?relax?the?matches?using?the ?Edit?Distance?met?
ric.?Thus,?for?the?given?pattern ?NKN, ?we?also?list?
words?that?match ?NKK, ?KKN,?etc.?Since?the?input?
patterns ? are ? deemed? to ? fit ? the ? given ? melody, ? an?
Edit?Distance?Matching?can?turn?up?words?that?need?
not?match?the?given?melody?and?hence?should?be?
used?only?when?there?are?no?phrases?matching?the?
input?pattern.?Another?approach,?though?practically?
not?possible,?is?to?have?a??big?enough?corpus??that?
contains?at?least?one?phrase?matching?each?pattern.
6 Experiments
We ? conducted ? the ? experiments ? as ? two ? separate?
steps,?one?for?the ?CRF? engine?and?another?for?the?
Sentence?Generation?module.
For?the?CRF?engine,?we?collected?and?used?Tamil?
film ? songs' ? tune ? and ? lyrics, ? as ? they ? were ? easily?
available?from?the?web.?The?tunes?were?converted?
to?the?ABC?notation?and?their?lyrics?were?converted?
to?the?KNM?representation?scheme.?The?notes?from?
the?tune?and?the?syllables?in?the?corresponding?lyric?
44
(in ? their ? respective ? representation ? schemes) ? were?
manually ? mapped ? with ? each ? other. ? An ? example?
training? file ? for ? the ?CRF? engine? for ? learning? the?
KNM?representation?scheme?is?presented?below:
Note Duration Label
B ? K
C ? N
B ? K
A ? KM
G ? K
? 0 tie
[ 0 open
A ? pre
G ? K
] 0 close
B 4 K
Table?1.?KNM?scheme?learning???training?file
Similarly,?for?the?word?boundary?identification,?the?
same?input?is?used?but?with?the?labels?corresponding?
to?word?boundaries?such?as?W?B?(word?beginning),?
W?I?(word?intermediate),?etc.?(Table?2):
Note Duration Label
B ? W?B
C ? W?I
B ? W?I
A ? W?I
G ? W?B
? 0 Tie
[ 0 open
A ? pre
G ? W?I
] 0 close
B 4 W?I
Table?2.?Word?boundary?learning???training?file
For?sentence?boundary?identification,?the?output?
from?the?word?boundary?identification?is?used?and?
hence?it?is?run?after?the?word?boundary?identifica?
tion?is?complete.?Thus,?the?input?to?the?CRF?engine?
in?this?case?would?be?like?the?one?in?(Table?3),?with?
labels?corresponding?to?sentence?boundary?such?as?
S?B?(sentence?beginning)?and?S?I?(sentence?interme?
diate):
Note Duration Word?
Boundary
Sentence?
Boundary
B ? W?B S?B
c ? W?I S?I
B ? W?I S?I
A ? W?I S?I
G ? W?B S?I
? 0 Tie S?I
[ 0 Open S?I
A ? Pre S?I
G ? W?I S?I
] 0 close S?I
B 4 W?I S?B
Table?3.?Sentence?boundary?learning???training?file
For ? the ?Sentence?Generation ?module, ?we ?used?
short?stories,?poems?and?Tamil?lyrics?across?various?
themes?such?as?love,?appreciation?of?nature,?patrio?
tism,?etc.?From?this,?all?the?special?characters?were?
removed?and?the?list?of?Unigram?and?Bigram?Words?
were?collected?along?with?their?frequencies.
Based?on?the?limited?experiments?performed?on?
the?trained?CRF?model,?we?observe?that?the?feature?
set?presented?for?Syllable?identification?seem?to?per?
form? reasonably ? and ? identifies ? the ? syllables ?with?
70%?accuracy?for?manually?tagged?melodies.?How?
ever,?we?could?not?objectively?evaluate? the?Word?
and?Sentence?Boundary?identification?process?as?the?
resulting?boundaries?can?also?be?considered?as?valid?
boundaries. ? In ? general, ? the ? word ? and ? sentence?
boundaries?are?the?choice?of?the?lyricist?and?hence?
the?results?can?be?considered?as?another?valid?way?
to?generate?lyric.?Also,?we?feel?that?the?number?of?
training?samples?(10?melodies)?supplied?for?training?
45
the ?CRF?engine? is ?very? less ? for ? it ? to ? reasonably?
learn ? the ? nuances ? that ? are ? present ? in ? real?word?
lyrics.
Some?of?the?syllable?patterns?identified?from?the?
tune?and?the?corresponding?sentences?generated?are?
given?below:
Pattern:?'KK?KK?KKK
?????NKKM?KMKK'
Output:??????????
??????????????
Translation:?In?small?age
???????????I?recollected
As?the?syllable?patterns?get?longer,?we?had?to?re?
sort?to?using?Edit?Distance?in?order?to?find?matching?
sentences.?One?such?output?is?presented?below:
Pattern:??KMKMKM?KMKM?NKN
?????????????NKMKM?NMKKM?NKN?
Output:????????????????
?????????????????????
Translation:?We?can?see?here?in?Tamil
???????????????????Proclaiming?aloud
7 Limitations?and?Future?Work?
From?the?initial?set?of?experiments,?we?see?that?it?is?
possible?to?generate?a?syllable?pattern?that?closely?
matches?the?input?tune.?Currently,?we?do?not?consid?
er? ?the?identification?of?strong?beats?in?the?melody?
and?are?expecting?the?presence?of?Mei?(M)?to?take?
care?of?stressed?syllables.?We?also?expect?the?same?
strategy?to?work?for?other?South?Indian?languages?as?
well. ?The?current ?Lyric ?Generation?algorithm? ? is?
simplistic,?in?that?it?can?generate?short?meaningful?
phrases, ? but ? generating ? longer ? phrases ? require?
adding?constraints ? (such? as ?closest ?matching?pat?
terns)?that?defeats?the?purpose?of?matching?with?the?
tune. ?Also, ? the ? current ?method?generates ?phrases?
that?are?independent?of?the?previous?phrases.?This?
leads ? to ? lyrics ? that ? are ? meaningful ? in ? parts, ? but?
meaningless?on?the?whole.
Future?work?can?involve?introducing??semantic ?
similarity??across?phrases?in?a?lyric,?thereby?gener?
ating?lyrics?that?provide?a?coherent?meaning.?Also,?
experiments ? can ? be ? conducted ?with ?different ? do?
main?corpus?to?generate?lyrics?for?a?given?situation?
(such?as?Love,?Death,?Travel,?etc.)? ?Other?sentence?
generation?strategies,?such?as?an?Evolutionary?Algo?
rithm? (as?suggested?in?(Manurung,?2004))?can?also?
be?attempted.?Once?a?coherent?meaningful?lyric?is?
generated,?further?improvements?can?be?towards?in?
corporating?poetic?aspects?in?the?lyric.
References?
Demeter. ? 2001. ?How ? to ? write ? Lyrics ?
http://everything2.com/index.pl?node=How ? to ? write?
lyrics.?
Guido?Gonzato.?2003. ?The?ABCPlus?Project? http://abc?
plus.sourceforge.net.
Hanna?M.?Wallach.?2004. ?Conditional?Random?Fields:?
An ? Introduction. ? Technical ? Report ? MS?CIS?04?21.?
Department ?of ?Computer ?and? Information ?Science,?
University?of?Pennsylvania.
Hisar ? Maruli ? Manurung. ? 2004. ?An ? evolutionary? ap?
proach?to?poetry?generation.?Ph.D.?Thesis,?University?
of?Edinburg.
Hugo?R.?Goncalo?Oliveira,?F.?Amilcar?Cardoso,?and?F.?
Camara?Perreira.?2007. ?Tra?La?Lyrics:?An?approach?
to?generate?text?based?on?rhythm.?Proceedings?of?the?
Fourth ? International ? Joint ? Workshop ? on ? Computa?
tional?Creativity,?IJWCC'07,?London:47?54.
Hugo?R.?Goncalo?Oliveira,?F.?Amilcar?Cardoso,?and?F.?
Camara ?Perreira. ?2007. ?Exploring?difference?strate?
gies?for?the?automatic?generation?of?song?lyrics?with?
Tra?La?Lyrics.?Proceedings?of?the?Portuguese?Confer?
ence ? on ? Artificial ? Intelligence ? (EPIA ? 2007),?
Guimar?es,?Portugal:57?68
John ? Lafferty, ? Andrew ? McCallum, ? and ? Fernando?
Pereira. ? 2001. ?Conditional? Random? Fields: ? Proba?
bilistic?Models?for?Segmenting?and?Labeling?Sequence ?
data. ? ?Proceedings ? of ? the ? Eighteenth ? International?
Conference ? on ? Machine ? Learning ? (ICML ? 2001),?
Williams?College,?Willamstown,?MA,?USA:282?289
Taku ?Kudo. ?2005. ?CRF++:?Yet?Another ?CRF? toolkit.?
http://crfpp.sourceforge.net.
Thomas?H.?Cormen.,?Charles?E.?Leiserson.,?Ronald?L.?
Rivest. ? 1990. ?Introduction? to ? Algorithms. ? Prentice?
Hall:?527?531.
46
Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 31?39,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An alternate approach towards meaningful lyric generation in Tamil 
Ananth Ramakrishnan A Sobha Lalitha Devi 
AU-KBC Research Centre 
MIT Campus of Anna University 
Chennai, India 
AU-KBC Research Centre 
MIT Campus of Anna University
Chennai, India 
ananthrk@au-kbc.org sobha@au-kbc.org 
Abstract 
This paper presents our on-going work to 
improve the lyric generation component of 
the Automatic Lyric Generation system for 
the Tamil Language. An earlier version of 
the system used an n-gram based model to 
generate lyrics that match the given melody. 
This paper identifies some of the deficien-
cies in the melody analysis and text genera-
tion components of the earlier system and 
explains the new approach used to tackle 
those drawbacks. The two central approach-
es discussed in this paper are: (1) An im-
proved mapping scheme for matching melo-
dy with words and (2) Knowledge-based 
Text Generation algorithm based on an ex-
isting Ontology and Tamil Morphology Ge-
nerator. 
1 Introduction 
In an attempt to define poetry (Manurung, 2004), 
provides three properties for a natural language 
artifact to be considered a poetic work, viz., Mea-
ningfulness (M), Grammaticality (G) and Poetic-
ness (P). A complete poetry generation system 
must generate texts that adhere to all the three 
properties. (Ananth et. al., 2009) explains an ap-
proach for automatically generating Tamil lyrics, 
given a melody, which attempts to generate mea-
ningful lyrics that match the melody.  
The existing approach (Ananth et. al., 2009) to au-
tomatically generate Tamil lyrics that match the 
given tune in ABC format (Gonzato, 2003) in-
volves two steps. The first step is to analyze the 
input melody and output a series of possible sylla-
ble patterns in KNM representation scheme - a 
scheme for representing all words in the language, 
where, K stands for Kuril ((C)V, where V is a 
short vowel), N stands for Nedil ((C)V, where V is 
a long vowel) and M stands for Mei or Ottru (con-
sonants) - that match the given melody, along with 
tentative word and sentence boundary. This melo-
dy analysis system was trained with sample film 
songs and their corresponding lyrics collected from 
the web. The tunes were converted to ABC Nota-
tion (Gonzato, 2003) and their lyrics were 
represented in KNM scheme. The trained model 
was then used to label the given input melody. 
The subsequent step uses a Sentence Generator 
module to generate lines that match the given syl-
lable pattern with words satisfying the following 
constraints: a) Words should match the syllable 
pattern and b) The sequence of words should have 
a meaning. This was achieved by using n-Gram 
models learnt from a Tamil text corpus.  
Though the system manages to generate sentences 
that match the syllable pattern, it has the following 
limitations: 
1) When no words are found matching a given 
syllable pattern, alternate patterns that are 
close to the given pattern, as suggested by the 
Edit Distance Algorithm, are considered. This 
algorithm treats the syllable patterns as strings 
for finding close patterns and hence, can pro-
vide choices that do not agree with the input 
melody. 
2) The Sentence Generation is based on the n-
Gram model learnt from a text corpus. This 
can result in sentences that do not have a cohe-
rent meaning. Also, since only bi-grams are 
considered, it can generate sentences that are 
ungrammatical due to Person-Number-Gender 
(PNG) agreement issues. 
This paper is an attempt to propose alternate ap-
proaches in order to overcome the above limita-
tions. 
31
2 Limitations of existing approach 
2.1 Finding close matches to syllable patterns 
In the existing system, when no words are found 
matching the given syllable pattern (either due to a 
small corpus or rarity of the pattern), the closest 
patterns are considered as alternatives. The closest 
match to a given syllable pattern is generated based 
on the Edit Distance algorithm. For example, if the 
input sequence is given as "NKN" (long vowel - 
short vowel - long vowel) and if no words are 
found matching NKN, closest matches for NKN 
are generated. Thus, if an edit distance of 1 is con-
sidered, the alternate pattern choices are "KKN", 
"NKM", "NNN", "NMN", etc. However, not all of 
these syllable patterns can fit the original music 
notes. 
As an example, consider the Tamil word ?thA-ma-
rai? (lotus) that fits the pattern NKN. Suppose no 
words that match the pattern NKN was present in 
the corpus and other close patterns were opted for, 
we get: 
Pat. Word Meaning Match 
KKN tha-va-Lai Frog No match 
NKM thA-ba-m Longing No match 
NNN kO-sA-lai Cow Hut Close Match 
NMN pA-p-pA Child No match 
Table 1. Alternative patterns for ?NKN? 
None of the above words can be used in the place 
of ?thA-ma-rai?, a good fit for a NKN pattern, as 
they don?t phonetically match (except for a close-
but-not-exact ?kO-sA-lai?) and hence cannot be 
used as part of the lyric without affecting the in-
tended melody.  
2.2 Ungrammatical or meaningless genera-
tion 
The Sentence Generation algorithm was based on 
the n-Gram model built from a text corpus. Given 
that n-Gram based generation schemes have in-
built bias towards shorter strings, it can end-up 
generating meaningless and ungrammatical sen-
tences. As observed in (Ananth et.al., 2009), we 
can get sentences such as: 
(* avan-He-3sm  nadandhu-walk sendrAlY-3sf) 
(He reached by walking) 
Here, the subject avan (He), which is a 3rd person, 
singular, masculine noun, does not agree with the 
verb sendrAlY , which is 3rd person, singular, femi-
nine. Thus, the noun and the verb do not agree on 
the gender. The correct sentence should be: 
(avan-3sm nadandhu sendrAn-3sm) 
This is happening because the bi-gram score for 
could be greater than 
. 
Similar disagreements can happen for other aspects 
such as person or number. Though performing a 
joint probability across words would help in reduc-
ing such errors, it would slow down the generation 
process. 
In addition to the above ungrammatical generation 
problem, the system can also generate meaningless 
sentences. Though, some of them can be consi-
dered as a poetic license, most of them were just 
non-sensical. For example, consider the following 
sentence generated by the n-Gram sentence genera-
tion system: 
(adhu-that idhu-this en-my) 
(that this my) 
The above sentence does not convey any coherent 
meaning. 
2.3 Ability to control theme/choice of words 
Given the nature of the Sentence generation algo-
rithm, it is not possible for the program to hand-
pick specific words and phrases. That is, the whole 
generation process is guided by the probability 
values and hence it is not possible to bias the algo-
rithm to produce utterances belonging to a particu-
lar theme.  
In the subsequent sections, we explain the alterna-
tive approaches to tackle the above limitations. 
32
3 Closest Syllable Patterns 
The existing approach uses the KNM Notation for 
representing all words in the language. This pho-
netic representation is at the most basic level, i.e., 
alphabets, and hence can be used to represent all 
words in the language. The KNM notation is gen-
erated by the melody analyzer and is used through-
out the system for generating sentences. Though 
this representation scheme is at the most basic lev-
el, it does not help in cases where we are looking 
for alternate or close matches. Thus, we need to 
come up with a representation scheme at a higher 
level of abstraction that will help us in providing 
valid choices without compromising the require-
ments of the melody. To this end, we hereby pro-
pose to use elements from classic poetry metric 
rules in Tamil Grammar (Bala et. al., 2003) as de-
fined in the oldest Tamil Grammar work, Tholkap-
piyam (Tholkappiyar, 5th Century B.C.). 
3.1 Meter in classical Tamil Poetry  
Meter is the basic rhythmic structure of a verse and 
the basic term that refers to Tamil meter is pA. 
Each line in the poem is called an adi, which, in 
turn, is made up of a certain number of metrical 
feet known as the ceer (words/tokens). Each ceer
is composed of a certain metrical units called asai
(syllables) which are made up of letters (vowels 
and consonants) that have certain intrinsic 
length/duration, known as mAthirai. The above 
entities are known as the core structural compo-
nents of a Tamil poem (Rajam, 1992)  
The basic metrical unit asai is mostly based on 
vowel length. There are two basic types of asai: 
nEr asai (straightness) and niRai asai (in a row; 
array). The nEr asai has the pattern (C)V(C)(C)
and niRai asai, (C)VCV(C)(C). These longest-
matching basic asai patterns are expanded to 
represent non-monosyllabic words, but for our 
needs, we use these two basic asai patterns for the 
new representation scheme. 
3.2 asai-based Representation Scheme  
In the new representation scheme, the constituents 
of the KNM representation scheme are converted 
to nEr or niRai asai before being sent to the Sen-
tence Generator module. The Sentence Generator 
module, in turn, makes use of this new representa-
tion scheme for picking words as well as for find-
ing alternatives. In this new representation scheme, 
a nEr asai is represented as Ne and a niRai asai is 
represented as Ni. 
The following table illustrates the mapping re-
quired for converting between the two representa-
tion schemes: 
KNM Representation asai representation 
K Ne 
KM(0?.2) Ne 
N Ne 
NM(0?2) Ne 
KK Ni 
KKM(0?2) Ni 
KN Ni 
KNM(0?2) Ni 
Table 2. KNM to asai representation 
For example, an output line such as, for example, 
?KK  KK  KKK? in the old representation scheme 
will be converted as ?Ni  Ni  NiNe? in the new re-
presentation based on asai. This means that the 
line should contain three ceer(words/tokens) and 
the first word should be a nirai asai, second word 
should be a nirai asai and the third word contains 
two syllables with a nirai asai followed by nEr 
asai. 
This new representation scheme helps in coming 
up with alternatives without affecting the metrical 
needs of the melody as the alternatives have the 
same mAthirai (length/duration). Thus, if we are 
given a pattern such as ?NiNe?, we have several 
valid choices such as ?KKK? (originally given), 
?KKMK?, ?KKMKM?, ?KKN?, ?KKMN? and 
?KKMNM?. We can use words that match any of 
the above patterns without compromising the dura-
tion imposed by the original music note. This way 
of choosing alternatives is much better than using 
the Edit Distance algorithm as it is based on the 
original meter requirements as against matching 
string patterns. 
To use the previous example of ?thA-ma-rai? (lo-
tus) (NKN) in this new representation scheme, we 
get, ?NeNi? and all the following words will 
match: 
33
Word KNM scheme 
nE-ra-lai (straight wave) NKN 
Sa-nj-nja-la-m (doubt) KMKKM 
Ma-ng-ka-la-m (auspicious) KMKKM 
a-m-bi-kai (goddess) KMKN 
vE-ng-ka-ta-m (Venkatam ? a 
name) 
NMKKM 
Table 3. NKN alternatives using asai representation 
The above (valid) choices such as KMKKM, 
NMKKM, etc. are not possible with just using the 
Edit Distance algorithm. Thus, the architecture of 
the system now consists of a new component for 
this conversion (Figure 1) 
Figure 1. System Approach with new ASAI converter 
4 Knowledge-based Sentence Generation 
The goal of the Sentence Generation module is to 
generate sentences matching the input pattern giv-
en in the new asai representation scheme. The ex-
isting system generated sentences based on the n-
Gram language model created from a text corpus 
of poems and film songs. However, as explained 
earlier, this can result in ungrammatical or mea-
ningless sentences being generated. In order to 
overcome this limitation, the Sentence Generation 
module is completely overhauled using a know-
ledge-based approach. A Tamil Morphology gene-
rator component, built in-house, is used to generate 
grammatically correct sentences from this know-
ledge base. 
4.1 Knowledge Base 
The knowledge base consists of: (a) set of verbs 
along with their selectional restriction rules (b) 
hand-coded sub-categorization Ontology with 
nouns and (c) list of adjectives and adverbs learned 
from a text corpus.  
4.1.1 Verbs and Selectional Restrictions 
Selectional restriction is defined as the right of the 
verb to select its arguments. Verb is the nucleus of 
a sentence and has the nature of choosing its argu-
ments. Any particular verb can take its arguments 
only according to its selectional restriction con-
straints. When these constraints are violated, the 
meaning of the sentence is affected. This violation 
of selectional restriction rules may lead to semanti-
cally wrong sentences or figurative usages. Cor-
rectness of a sentence not only depends on the syn-
tactic correctness, but also with the semantic inter-
pretation of the sentence. 
4.1.2 Syntactic Classification 
Verbs can be broadly classified into three divi-
sions, viz., monadic, dyadic and triadic verbs.   
Monadic verbs can have only one argument - the 
subject. Dyadic verbs can have two arguments -
subject and object. Triadic verbs can take three 
arguments - subject, direct and indirect objects. 
But there is no strict rule that the triadic verbs 
should have all three arguments or the dyadic verbs 
should have the two arguments filled. There can be 
overlaps between these groups of verbs. Triadic 
verb can drop the indirect object and have a Prepo-
sitional Phrase (PP) attached with the sentence. 
Dyadic verb can drop the object and still give a 
valid sentence. The verbs are grouped according to 
the sub-categorization information of the subject 
and object nouns. The sub-categorization features 
are explained in the following section. At present, 
we are using only Monadic and Dyadic verbs for 
our sentence generation purposes. 
4.1.3 Sub-Categorization 
Sub-categorization features explain the nature of 
the noun. The subject and object nouns are ana-
34
lyzed using these features. These features may in-
clude the type of noun, its characteristics, state etc. 
Sub-categorization information includes the fea-
tures such as [?animate], [?concrete], [?edible] 
etc.  
Some of the features and the meanings are listed 
below: 
[+animate] All animals, human beings 
[+human] All human beings 
[+female] Animals/human beings of 
feminine gender 
[+solid] Things that are in solid state 
[+vehicle] All vehicles 
[+concrete] Things that physically exist 
[-concrete] Things that do not physically 
exist 
[+edible] Things that can be eaten 
[-edible] Things that cannot be eaten 
[+movable] Things that are movable 
[-movable] Things that are not movable 
Table 4. Sub-categorization Features 
4.1.4 Ontology of Nouns 
The sub-categorization features are used in the 
formulation of general Ontology of Nouns. It is 
made with respect to the usage of language. The 
Ontology that is developed has the following sa-
lient features: 
? It is a language-based Ontology originally 
developed for English and has been cus-
tomized for Tamil 
? Nodes in the Ontology are the actual sub-
categorization features of Nouns 
? It is made according to the use of nouns in 
the Tamil language 
? Each node will have a list of nouns as en-
tries for that node 
The complete Ontology can be found in (Arulmoz-
hi, et. al., 2006) 
4.1.5 Contents of Knowledge Base 
At present, the knowledge-base consists of 116 
unique verbs, 373 selectional restriction rules and 
771 Nouns in the Ontology. 
The verbs list includes both cognitive as well as 
non-cognitive verbs. Examples of verbs include 
pAr (to see), kelY (to listen), vA (to come), thEtu
(to search), piti (to catch), po (to go), kal (to learn), 
etc. 
The selectional restriction rules are stored as fol-
lows: 
Verb=>subject_category;subject_case=>object_c
ategory;object_case. 
When a verb does not take any object, the keyword 
[no_obj] is used to denote the same. In addition to 
the subject and object categories, the rule also con-
tains the appropriate case markers to be used for 
the subject and object nouns. This additional in-
formation is stored for use by the Morph Genera-
tion component. 
Some examples of selectional restriction rules are 
given below: 
pAr=>[+living,+animate,+vertebrate,+mammal,
+human];NOM=>[no_obj]  
pAr=>[+living,+animate,+vertebrate,+mammal,
+human];NOM=> 
[+living,+animate,+vertebrate,+mammal,+human
];ACC 
pi-
ti=>[+living,+animate,+vertebrate,+mammal,+h
uman];NOM=>[living,+concrete,+movable,+artif
act,+solid,+instrument,-
vehicle,+implements];NOM 
pi-
ti=>[+living,+animate,+vertebrate,+mammal,+h
uman];NOM=>[no_obj]  
Here, ACC, NOM, DAT, etc. denote the case mark-
ers to be used for the subject and object nouns. 
The 771 Nouns are stored across several files ac-
cording to their position in the Ontology. An On-
tology map is used to determine the list of nouns 
present in a particular node position. 
35
4.1.6 Adjectives and Adverbs 
In addition to the verbs and nouns mentioned 
above, the knowledge-base also contains a list of 
adjective-noun and adverb-verb bi-grams learnt 
from a text corpus. This information is used to 
augment the Sentence Generator with words from 
these POS categories.  
4.2 Tamil Morphological Generator 
Tamil is a densely agglutinative language and dis-
plays a unique structural formation of words by the 
addition of suffixes representing various senses or 
grammatical categories, to the roots or stems. The 
senses such as person, number, gender and case are 
linked to a noun root in an orderly fashion. The 
verbal categories such as transitive, causative, 
tense and person, number and gender are added to 
a verbal root or stem. Thus, with the given know-
ledge-base and a Tamil Morphological generator 
component one can generate grammatically correct 
sentences. 
We use the Tamil Morphological Generator com-
ponent (Menaka et. al., 2010) to generate inflec-
tions of subject/object nouns with appropriate 
number & case and the verbs with person, number 
and gender suffixes.  
4.3 Sentence Generation 
Given a line in asai representation scheme, the 
sentence generation module is responsible for ge-
nerating a grammatically correct and meaningful 
sentence matching the given asai scheme. It 
achieves the same by using the knowledge-base 
along with the Tamil Morphology Generator com-
ponent (Figure 2). In addition to the asai represen-
tation, the module also accepts the tense in which 
the sentence must be written. The rest of the para-
meters such as person, gender and case are auto-
matically deduced by the module.  
Figure 2. Sentence Generator module 
The algorithm for generating a matching sentence 
is as follows: 
1. Pick a selectional restriction rule, R in random
2. For each noun, SUB_N in subject_category of 
rule, R: 
2.1 Guess the gender for SUB_N based on sub-
ject_category 
2.2 For each noun, OBJ_N in object_category: 
2.2.1 Use Morphology Generator component 
to get morphed nouns & verbs based on tense, per-
son, gender and case. 
2.2.2 Generate sentences of the form [SUB_N] 
[OBJ_N] [VERB]  
2.2.3 Add adjectives or adverbs, if needed 
2.2.4 Repeat words, if needed 
2.2.4 Add to list of sentences generated  
  
3. Check the list of sentences against the asai pat-
tern. If matches, return sentence. Otherwise, go to 
step 1. 
Table 5. Sentence Generation Algorithm 
36
Details about steps such as matching against asai
pattern, gender identification, word repetition and 
adding adjectives/adverbs are explained below. 
4.3.1 Matching against asai pattern 
The list of sentences generated from the module 
are compared against the given asai pattern. The 
matching could either be an exact match or a re-
ordered match. That is, since Tamil is a relatively 
free word-order language, the generated sentence 
can also be re-ordered, if required, to match the 
given asai pattern. However, when adjectives or 
adverbs are added to the sentence, they need to 
maintain their position in front of the noun or verb 
respectively and hence they are not re-ordered. For 
now, we do not weight the sentences and hence 
return the first matching sentence. 
4.3.2 Gender Identification 
As noted in the algorithm, the gender needs to be 
automatically guessed. In Tamil, the gender of the 
subject is denoted by the appropriate suffix in the 
verb. If a personal pro-noun such as nAnY (I) or nI
(you) is used as subject, then any of masculine or 
feminine gender can be used without affecting the 
grammatical correctness of the verb. In this case, 
the program uses the default value of masculine 
gender. If the subject is not a personal pronoun, the 
gender for the verb is guessed based on the sub-
ject_category of the subject noun. If the sub-
ject_category explicitly mentions [+human, 
+living, +female,?], then feminine gender is re-
turned. If the subject_category explicitly mentions 
[+human, +living, -female,?], then masculine 
gender is returned. Otherwise, if [+human, 
+living,?] is present, but there is no explicit men-
tion of +female or ?female, it defaults to honorific 
suffix. In all other cases, neuter gender is returned.  
4.3.3 Adding adjectives and adverbs 
The Sentence Generator module using the selec-
tional restriction rules can only create sentences of 
the form ?[subject] [object] [verb]?. However, 
typical lyrics will not always contain just three 
word sentences and thus, the ability to put more 
words in a sentence generated by our system is 
required. In such cases, a look-up list of adjectives 
and adverbs is used for filling the additional words 
required by the syllable pattern. This look-up list is 
generated from a POS-tagged text corpus from 
which the list of adjective-noun, adverb-verb bi-
grams are added to the look-up list. Whenever a 
sentence needs more than three words, this look-up 
list is consulted to generate sentences that add the 
relevant adjectives to subject or object nouns and 
relevant adverbs before the verb. Each possible 
combination of such sentences is generated and 
added to the list of sentences.   
4.3.4 Word repetition 
An additional approach to handle lines with more 
than three words is to repeat certain words already 
present in the ?[subject] [object] [verb]? output. If 
an adjective or adverb is already added to the sen-
tence, then preference for repetition is given to the 
adjective/adverb subject to the constraints of the 
input asai scheme. Otherwise, the verb is chosen 
for repetition. Finally, the subject and object nouns 
are considered.  
5 Experiments 
The goal of the experiment was to validate whether 
the sentences generated using the Knowledge-
based approach are more grammatical and mea-
ningful than the n-Gram approach. In order to test 
this hypothesis, a set of 10 syllable patterns was 
given to the old n-Gram system and 30 sentences 
were generated from them. The new knowledge-
based approach was also given the syllable patterns 
and the resulting 32 sentences were collected. In 
order to avoid any bias, these 62 sentences were 
interleaved in a single document and this document 
was given to five human evaluators for scoring 
each sentence. The scoring methodology is as fol-
lows: 
Score Meaning 
1 Incorrect 
2 Grammatically perfect, but no mean-
ing at all 
3 Grammatically correct but only par-
tially meaningful 
4 Both Grammar and Meaning are only 
partially correct 
5 Perfect 
Table 6. Scoring methodology 
Based on the scores given by the human evalua-
tors, the sentences generated using the n-Gram ap-
37
proach scored an average of 2.06, whereas the sen-
tences generated using the knowledge-based ap-
proach scored an average of 4.13. This clearly de-
monstrates that the new approach results in consis-
tently more grammatical and meaningful sen-
tences.  
A break-down of statistics based on the scores giv-
en by each evaluator is given below (Table 7): 
E-1 E-2 E-3 E-4 E-5 
Avg. Score 
(KB)*
4.5 4.38 4.06 4.09 3.63 
Avg. Score (n-
G) *
2.37 1 3.3 2.13 1.5 
# Sentences 
scoring 5 (KB) 
25 25 23 20 14 
# Sentences 
scoring 5 (n-G) 
6 0 14 1 0 
# Sentences 
scoring 1 (KB) 
2 0 7 4 7 
# Sentences 
scoring 1 (n-G) 
16 30 11 19 25 
Table 7. Detailed Statistics 
*KB = Knowledge-based approach and n-G = n-
Gram based approach. 
A subset of syllable patterns given to the system 
and the sentences generated by the system are giv-
en below: 
Input NM KKMKMKMK 
KMNM 
Intermediate Form Ne NiNeNeNe NeNe 
Sentences 
(nAm-we arangathukku-stadium  vanthOm-came) 
(We came to the stadium) 
(nee-You siraichAlaikku-prison vanthAi-came) 
(You came to the prison) 
Input NN KKNN NMNM 
Intermediate Form NeNe NiNeNe NeNe 
Sentences 
(* rAjA-King  nadanathai-dance  kEttAr-listen) 
(The King listened to the dance) 
(neengal-You  piditheergal-caught  kaiyai-hand) 
(You caught the hand) 
Here, the sentence ?rAjA-King  nadanathai-dance  
kEttAr-listened? (The King listened to the dance) is  
generated due to the fact that the noun dance is 
taken from the Ontology node ?content? that also 
contains nouns for music, drama, etc. for which the 
verb listen matches perfectly. Thus, this semanti-
cally meaningless sentence is generated due to the 
present sub-categorization levels of the nouns On-
tology. In addition to this, Ontology based genera-
tion can also create semantically meaningless sen-
tences when a verb has more than one sense and 
the appropriate sense is not taken into considera-
tion. 
The next sentence ?neengal-You  piditheergal-
caught  kaiyai-hand? (You caught the hand) is an 
example of a sentence in which the verb and object 
noun were re-ordered to match the input pattern. 
6 Limitations and Future Work  
From the initial set of experiments, we see that the 
knowledge-based approach results in generating 
grammatically correct and mostly meaningful sen-
tences. Also, unlike the Edit Distance algorithm, 
the new asai representation scheme consistently 
provides valid choices and alternatives for syllable 
patterns, thus resulting in better coverage. 
We are also currently working on introducing co-
hesion across multiple lines of the verse by (a) 
grouping related verbs, (b) using semantically re-
lated verbs (such as Synonym, Antonym, Hy-
ponym, etc.) from previous sentences and (c) pick-
ing rules that can result in using the same subject 
or object. 
The main drawback of the current knowledge-
based approach is the lack of poetic sentences and 
hence the poetic aspect of the verse needs im-
provement. Although we attempt to introduce 
structural poeticness by rhyme and repetition, the 
content aspect of the poem remains a bottleneck 
given our approach of using selectional restriction 
rules that does not lend well for figurative sen-
tences. 
38
References  
Ananth Ramakrishnan, Sankar Kuppan, and Sobha Lali-
tha Devi. 2009. Automatic Generation of Tamil Lyr-
ics for Melodies. Proceedings of the Workshop on 
Computational Approaches to Linguistic Creativity, 
CALC'09, Boulder, Colorado:40-46. 
Arulmozhi P, Sobha. L. 2006. Semantic Tagging for 
Language Processing. 34th All India conference for 
Dravidian Linguistics (June 22-24, 2006), Trivan-
drum, India. 
Bala Sundara Raman L, Ishwar S, and Sanjeeth Kumar 
Ravindranath. 2003. Context Free Grammar for Nat-
ural Language Constructs ? An implementation for 
Venpa Class of Tamil Poetry. 6th International Tamil 
Internet Conference and Exhibition, Tamil Internet 
2003 (August 22-24, 2003), Chennai, India. 
Guido Gonzato. 2003. The ABCPlus Project
http://abcplus.sourceforge.net. 
Hisar Maruli Manurung. 2004. An evolutionary ap-
proach to poetry generation. Ph.D. Thesis, Universi-
ty of Edinburg. 
Menaka S, Vijay Sundar Ram, and Sobha Lalitha Devi. 
2010. Morphological Generator for Tamil. Proceed-
ings of the Knowledge Sharing event on Morpholog-
ical Analysers and Generators (March 22-23, 2010), 
LDC-IL, Mysore, India:82-96. 
Rajam V.S. 1992. A Reference Grammar of Classical 
Tamil Poetry (150 B.C.-pre-5th/6th century A.D.). 
Memoirs of the American Philosophical Society, 
Philadelphia: 113-240. 
Tholkaappiyar. 5th Century B.C. Tholkaapiyam - 
http://www.tamil.net/projectmadurai/pub/pm0100/tol
kap.pdf. 
39
