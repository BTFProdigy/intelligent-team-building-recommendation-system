Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 44?47,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Experiences with English-Hindi, English-Tamil and English-Kannada
Transliteration Tasks at NEWS 2009
Manoj Kumar Chinnakotla and Om P. Damani
Department of Computer Science and Engineering,
IIT Bombay,
Mumbai, India
{manoj,damani}@cse.iitb.ac.in
Abstract
We use a Phrase-Based Statistical Ma-
chine Translation approach to Translitera-
tion where the words are replaced by char-
acters and sentences by words. We employ
the standard SMT tools like GIZA++ for
learning alignments and Moses for learn-
ing the phrase tables and decoding. Be-
sides tuning the standard SMT parame-
ters, we focus on tuning the Character Se-
quence Model (CSM) related parameters
like order of the CSM, weight assigned to
CSM during decoding and corpus used for
CSM estimation. Our results show that
paying sufficient attention to CSM pays
off in terms of increased transliteration ac-
curacies.
1 Introduction
Transliteration of Named-Entities (NEs) is an im-
portant problem that affects the accuracy of many
NLP applications like Cross Lingual Search and
Machine Translation. Transliteration is defined
as the process of automatically mapping a given
grapheme sequence in the source language to a
grapheme sequence in the target language such
that it preserves the pronunciation of the origi-
nal source word. A Grapheme refers to the unit
of written language which expresses a phoneme
in the language. Multiple alphabets could be
used to express a grapheme. For example, sh
is considered a single grapheme expressing the
phoneme /SH/. For phonetic orthography like De-
vanagari, each grapheme corresponds to a unique
phoneme. However, for English, a grapheme like
c may map to multiple phonemes /S/,/K/. An ex-
ample of transliteration is mapping the Devana-
gari grapheme sequence E?s h{rF to its phoneti-
cally equivalent grapheme sequence Prince Harry
in English.
This paper discusses our transliteration ap-
proach taken for the NEWS 2009 Machine
Transliteration Shared Task [Li et al2009b, Li et
al.2009a]. We model the transliteration problem
as a Phrased-Based Machine Translation prob-
lem. Later, using the development set, we tune
the various parameters of the system like order of
the Character Sequence Model (CSM), typically
called language model, weight assigned to CSM
during decoding and corpus used to estimate the
CSM. Our results show that paying sufficient at-
tention to the CSM pays off in terms of improved
accuracies.
2 Phrase-Based SMT Approach to
Transliteration
In the Phrase-Based SMT Approach to Transliter-
ation [Sherif and Kondrak2007, Huang2005], the
words are replaced by characters and sentences are
replaced by words. The corresponding noisy chan-
nel model formulation where a given english word
e is to be transliterated into a foreign word h, is
given as:
h? = argmax
h
Pr(h|e)
= argmax
h
Pr(e|h) ? Pr(h) (1)
In Equation 1, Pr(e|h) is known as the translation
model which gives the probability that the char-
acter sequence h could be transliterated to e and
Pr(h) is known as the character sequence model
typically called language model which gives the
probability that the character sequence h forms a
valid word in the target language.
44
Ta
sk
Ru
n
Op
tim
al 
Pa
ram
ete
r S
et
Ac
cu
rac
y 
in 
top
-1 
 
Me
an
 F-
 
sc
ore
  
MR
R  
MA
Pr
ef
MA
P1
0  
MA
Ps
ys
En
gli
sh
-H
ind
i
Sta
nd
ard
 
LM
 O
rde
r: 5
, 
LM
 W
eig
ht:
 
0.6
0.4
7
0.8
6
0.5
8
0.4
7
0.1
8
0.2
0
En
gli
sh
-H
ind
i
No
n- 
sta
nd
ard
LM
 O
rde
r: 5
, 
LM
 W
eig
ht:
 
0.6
0.5
2
0.8
7
0.6
2
0.5
2
0.1
9
0.2
1
En
gli
sh
-Ta
mi
l
Sta
nd
ard
 
LM
 O
rde
r: 5
, 
LM
 W
eig
ht:
 
0.3
0.4
5
0.8
8
0.5
6
0.4
5
0.1
8
0.1
8
En
gli
sh
- 
Ka
nn
ad
a
Sta
nd
ard
 
LM
 O
rde
r: 5
, 
LM
 W
eig
ht:
 
0.3
0.4
4
0.8
7
0.5
5
0.4
4
0.1
7
0.1
8
Figure 1: NEWS 2009 Development Set Results
Ta
sk
Ru
n  
Ac
cu
rac
y i
n 
top
-1 
 
Me
an
 F-
 
sc
ore
  
MR
R  
MA
Pr
ef
MA
P1
0  
MA
Ps
ys
En
gli
sh
-H
ind
i
Sta
nd
ard
 
0.4
2
0.8
6
0.5
4
0.4
2
0.1
8
0.2
0
En
gli
sh
-H
ind
i
No
n-s
tan
da
rd
0.4
9
0.8
7
0.5
9
0.4
8
0.2
0
0.2
3
En
gli
sh
-Ta
mi
l
Sta
nd
ard
 
0.4
1
0.8
9
0.5
4
0.4
0
0.1
8
0.1
8
En
gli
sh
-K
an
na
da
Sta
nd
ard
 
0.3
6
0.8
6
0.4
8
0.3
5
0.1
6
0.1
6
Figure 2: NEWS 2009 Test Set Results
Given the parallel training data pairs, we pre-
processed the source (English) and target (Hindi,
Tamil and Kannada) strings into character se-
quences. We then ran the GIZA++ [Och and
Ney2003] aligner with default options to obtain
the character-level alignments. For alignment, ex-
cept for Hindi, we used single character-level units
without any segmentation. In case of Hindi, we
did a simple segmentation where we added the
halant character (U094D) to the previous Hindi
character. Moses Toolkit [Hoang et al2007] was
then used to learn the phrase-tables for English-
Hindi, English-Tamil and English-Kannada. We
also learnt the character sequence models on the
target language training words using the SRILM
toolkit [Stolcke2002]. Given a new English word,
we split the word into sequence of characters and
run the Moses decoder with the phrase-table of tar-
get language obtained above to get the transliter-
ated word. We ran Moses with the DISTINCT op-
tion to obtain the top k distinct transliterated op-
tions.
2.1 Moses Parameter Tuning
The Moses decoder computes the cost of each
translation as a product of probability costs of four
models: a) translation model b) language model
c) distortion model and d) word penalty as shown
in Equation 2. The distortion model controls the
Ta
sk
Ru
n  
Bas
eli
ne
 
Mo
de
l (LM
 
Or
de
r N=3)
Bes
t R
un
% Imp
rove
me
nt
En
gli
sh
-H
ind
i
Sta
nd
ard
 
0.4
0.4
2
5.0
0
En
gli
sh
-H
ind
i
No
n-s
tan
da
rd
0.3
7
0.4
9
32
.43
En
gli
sh
-Ta
mi
l
Sta
nd
ard
 
0.3
9
0.4
5
15
.38
En
gli
sh
- 
Ka
nn
ad
a
Sta
nd
ard
 
0.3
6
0.3
6
0.0
0
Figure 3: Improvements Obtained over Baseline
on Test Set due to Language Model Tuning
cost of re-ordering phrases (transliteration units)
in a given sentence (word) and the word penalty
model controls the length of the final translation.
The parameters ?T , ?CSM , ?D and ?W control
the relative importance given to each of the above
models.
Pr(h|e) = PrT (e|h)
?T ? PrCSM (h)
?CSM ?
PrD(h, e)
?D ? ?length(h)??W (2)
Since no re-ordering of phrases is required during
translation task, we assign a zero weight to ?D.
Similarly, we varied the word penalty factor ?W
between {?1, 0,+1} and found that it achieves
maximum accuracy at 0. All the above tuning was
done with a trigram CSM and default weight (0.5)
in Moses for ?T .
45
2.2 Improving CSM Performance
In addition to the above mentioned parameters,
we varied the order of the CSM and the mono-
lingual corpus used to estimate the CSM. For each
task, we started with a trigram CSM as mentioned
above and tuned both the order of the CSM and
?CSM on the development set. The optimal set
of parameters and the development set results are
shown in Figure 1. In addition, we use a mono-
lingual Hindi corpus of around 0.4 million doc-
uments called Guruji corpus. We extracted the
2.6 million unique words from the above corpus
and trained a CSM on that. This CSM which was
learnt on the monolingual Hindi corpus was used
for the non-standard Hindi run. We repeat the
above procedure of tuning the order of CSM and
?CSM and find the optimal set of parameters for
the non-standard run on the development set.
3 Results and Discussion
The details of the NEWS 2009 dataset for Hindi,
Kannada and Tamil are given in [Li et al2009a,
Kumaran and Kellner2007]. The final results of
our system on the test set are shown in Figure 2.
Figure 3 shows the improvements obtained on test
set by tuning the CSM parameters. The trigram
CSM model used along with the optimal Moses
parameter set tuned on development set was taken
as baseline for the above experiments. The results
show that a major improvement (32.43%) was ob-
tained in the non-standard run where the monolin-
gual Hindi corpus was used to learn the CSM. Be-
cause of the use of monolingual Hindi corpus in
the non-standard run, the transliteration accuracy
improved by 22.5% when compared to the stan-
dard run. The improvements (15.38%) obtained in
Tamil are also significant. However, the improve-
ment in Hindi standard run was not significant. In
Kannada, there was no improvement due to tuning
of LM parameters. This needs further investiga-
tion.
The above results clearly highlight the impor-
tance of improving CSM accuracy since it helps
in improving the transliteration accuracy. More-
over, improving the CSM accuracy only requires
monolingual language resources which are easy
to obtain when compared to parallel transliteration
training data.
4 Conclusion
We presented the transliteration system which we
used for our participation in the NEWS 2009 Ma-
chine Transliteration Shared Task on Translitera-
tion. We took a Phrase-Based SMT approach to
transliteration where words are replaced by char-
acters and sentences by words. In addition to the
standard SMT parameters, we tuned the CSM re-
lated parameters like order of the CSM, weight as-
signed to CSM and corpus used to estimate the
CSM. Our results show that improving the ac-
curacy of CSM pays off in terms of improved
transliteration accuracies.
Acknowledgements
We would like to thank the Indian search-engine
company Guruji (http://www.guruji.com)
for providing us the Hindi web content which was
used to train the language model for our non-
standard Hindi runs.
References
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and
Ondej Bojar. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In In Proceed-
ings of ACL, Demonstration Session, pages 177?
180.
Fei Huang. 2005. Cluster-specific Named Entity
Transliteration. In HLT ?05: Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 435?442, Morristown, NJ, USA. Association
for Computational Linguistics.
A. Kumaran and Tobias Kellner. 2007. A Generic
Framework for Machine Transliteration. In SIGIR
?07: Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 721?722, New
York, NY, USA. ACM.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report on NEWS 2009 Ma-
chine Transliteration Shared Task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009).
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of NEWS 2009
Machine Transliteration Shared Task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009).
46
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
Based Transliteration. In In Proceedings of ACL
2007. The Association for Computer Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In In Proceedings of Intl.
Conf. on Spoken Language Processing.
47
Hindi and Marathi to English Cross Language Information 
 
Manoj Kumar Chinnakotla, Sagar Ranadive, Om P. Damani and Pushpak 
Bhattacharyya 
 
Abstract 
 
In this paper, we present our Hindi ->English and Marathi ->English CLIR 
systems developed as part of our participation in the CLEF 2007 Ad-Hoc 
Bilingual task. We take a query translation based approach using bi-lingual 
dictionaries. Query words not found in the dictionary are transliterated using 
a simple lookup table based transliteration approach. The resultant 
transliteration is then compared with the index items of the corpus to return 
the `k' closest English index words of the given Hindi/Marathi word. The 
resulting multiple translation/transliteration choices for each query word are 
disambiguated using an iterative page-rank style algorithm, proposed in the 
literature, which makes use of term-term co-occurrence statistics to produce 
the final translated query. Using the above approach, for Hindi, we achieve a 
Mean Average Precision (MAP) of 0.2366 in title which is 61.36% of 
monolingual performance and a MAP of 0.2952 in title and description 
which is 67.06% of monolingual performance. For Marathi, we achieve a 
MAP of 0.2163 in title which is 56.09% of monolingual performance. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1346?1356,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Multilingual Pseudo-Relevance Feedback: Performance Study of
Assisting Languages
Manoj K. Chinnakotla Karthik Raman Pushpak Bhattacharyya
Department of Computer Science and Engineering
Indian Institute of Technology, Bombay,
Mumbai, India
{manoj,karthikr,pb}@cse.iitb.ac.in
Abstract
In a previous work of ours Chinnakotla
et al (2010) we introduced a novel
framework for Pseudo-Relevance Feed-
back (PRF) called MultiPRF. Given a
query in one language called Source, we
used English as the Assisting Language to
improve the performance of PRF for the
source language. MulitiPRF showed re-
markable improvement over plain Model
Based Feedback (MBF) uniformly for 4
languages, viz., French, German, Hungar-
ian and Finnish with English as the as-
sisting language. This fact inspired us
to study the effect of any source-assistant
pair on MultiPRF performance from out
of a set of languages with widely differ-
ent characteristics, viz., Dutch, English,
Finnish, French, German and Spanish.
Carrying this further, we looked into the
effect of using two assisting languages to-
gether on PRF.
The present paper is a report of these in-
vestigations, their results and conclusions
drawn therefrom. While performance im-
provement on MultiPRF is observed what-
ever the assisting language and whatever
the source, observations are mixed when
two assisting languages are used simul-
taneously. Interestingly, the performance
improvement is more pronounced when
the source and assisting languages are
closely related, e.g., French and Spanish.
1 Introduction
The central problem of Information Retrieval (IR)
is to satisfy the user?s information need, which is
typically expressed through a short (typically 2-3
words) and often ambiguous query. The problem
of matching the user?s query to the documents is
rendered difficult by natural language phenomena
like morphological variations, polysemy and syn-
onymy. Relevance Feedback (RF) tries to over-
come these problems by eliciting user feedback
on the relevance of documents obtained from the
initial ranking and then uses it to automatically
refine the query. Since user input is hard to ob-
tain, Pseudo-Relevance Feedback (PRF) (Buckley
et al, 1994; Xu and Croft, 2000; Mitra et al, 1998)
is used as an alternative, wherein RF is performed
by assuming the top k documents from the initial
retrieval as being relevant to the query. Based on
the above assumption, the terms in the feedback
document set are analyzed to choose the most dis-
tinguishing set of terms that characterize the feed-
back documents and as a result the relevance of
a document. Query refinement is done by adding
the terms obtained through PRF, along with their
weights, to the actual query.
Although PRF has been shown to improve re-
trieval, it suffers from the following drawbacks:
(a) the type of term associations obtained for query
expansion is restricted to co-occurrence based re-
lationships in the feedback documents, and thus
other types of term associations such as lexical and
semantic relations (morphological variants, syn-
onyms) are not explicitly captured, and (b) due to
the inherent assumption in PRF, i.e., relevance of
top k documents, performance is sensitive to that
of the initial retrieval algorithm and as a result is
not robust.
Multilingual Pseudo-Relevance Feedback
(MultiPRF) (Chinnakotla et al, 2010) is a novel
framework for PRF to overcome both the above
limitations of PRF. It does so by taking the help of
a different language called the assisting language.
In MultiPRF, given a query in source language
L1, the query is automatically translated into
the assisting language L2 and PRF performed
in the assisting language. The resultant terms
are translated back into L1 using a probabilistic
bi-lingual dictionary. The translated feedback
1346
model, is then combined with the original feed-
back model of L1 to obtain the final model which
is used to re-rank the corpus. MulitiPRF showed
remarkable improvement on standard CLEF
collections over plain Model Based Feedback
(MBF) uniformly for 4 languages, viz., French,
German, Hungarian and Finnish with English as
the assisting language. This fact inspired us to
study the effect of any source-assistant pair on
PRF performance from out of a set of languages
with widely different characteristics, viz., Dutch,
English, Finnish, French, German and Spanish.
Carrying this further, we looked into the effect of
using two assisting languages together on PRF.
The present paper is a report of these in-
vestigations, their results and conclusions drawn
therefrom. While performance improvement on
PRF is observed whatever the assisting language
and whatever the source, observations are mixed
when two assisting languages are used simulta-
neously. Interestingly, the performance improve-
ment is more pronounced when the source and as-
sisting languages are closely related, e.g., French
and Spanish.
The paper is organized as follows: Section 2,
discusses the related work. Section 3, explains the
Language Modeling (LM) based PRF approach.
Section 4, describes the MultiPRF approach. Sec-
tion 5 discusses the experimental set up. Section 6
presents the results, and studies the effect of vary-
ing the assisting language and incorporates mul-
tiple assisting languages. Finally, Section 7 con-
cludes the paper by summarizing and outlining fu-
ture work.
2 Related Work
PRF has been successfully applied in various IR
frameworks like vector space models, probabilis-
tic IR and language modeling (Buckley et al,
1994; Jones et al, 2000; Lavrenko and Croft,
2001; Zhai and Lafferty, 2001). Several ap-
proaches have been proposed to improve the per-
formance and robustness of PRF. Some of the rep-
resentative techniques are (i) Refining the feed-
back document set (Mitra et al, 1998; Sakai et
al., 2005), (ii) Refining the terms obtained through
PRF by selecting good expansion terms (Cao et
al., 2008) and (iii) Using selective query expan-
sion (Amati et al, 2004; Cronen-Townsend et al,
2004) and (iv) Varying the importance of docu-
ments in the feedback set (Tao and Zhai, 2006).
Another direction of work, often reported in the
TREC Robust Track, is to use a large external col-
lection like Wikipedia or the Web as a source of
expansion terms (Xu et al, 2009; Voorhees, 2006).
The intuition behind the above approach is that
if the query does not have many relevant docu-
ments in the collection then any improvements in
the modeling of PRF is bound to perform poorly
due to query drift.
Several approaches have been proposed for
including different types of lexically and se-
mantically related terms during query expansion.
Voorhees (1994) use Wordnet for query expan-
sion and report negative results. Recently, random
walk models (Lafferty and Zhai, 2001; Collins-
Thompson and Callan, 2005) have been used to
learn a rich set of term level associations by com-
bining evidence from various kinds of information
sources like WordNet, Web etc. Metzler and Croft
(2007) propose a feature based approach called la-
tent concept expansion to model term dependen-
cies.
All the above mentioned approaches use the re-
sources available within the language to improve
the performance of PRF. However, we make use of
a second language to improve the performance of
PRF. Our proposed approach is especially attrac-
tive in the case of resource-constrained languages
where the original retrieval is bad due to poor cov-
erage of the collection and/or inherent complexity
of query processing (for example term conflation)
in those languages.
Jourlin et al (1999) use parallel blind relevance
feedback, i.e. they use blind relevance feedback on
a larger, more reliable parallel corpus, to improve
retrieval performance on imperfect transcriptions
of speech. Another related idea is by Xu et al
(2002), where a statistical thesaurus is learned us-
ing the probabilistic bilingual dictionaries of Ara-
bic to English and English to Arabic. Meij et
al. (2009) tries to expand a query in a differ-
ent language using language models for domain-
specific retrieval, but in a very different setting.
Since our method uses a corpus in the assisting
language from a similar time period, it can be
likened to the work by Talvensaari et al (2007)
who used comparable corpora for Cross-Lingual
Information Retrieval (CLIR). Other work pertain-
ing to document alignment in comparable corpora,
such as Braschler and Scha?uble (1998), Lavrenko
et al (2002), also share certain common themes
with our approach. Recent work by Gao et al
1347
(2008) uses English to improve the performance
over a subset of Chinese queries whose transla-
tions in English are unambiguous. They use inter-
document similarities across languages to improve
the ranking performance. However, cross lan-
guage document similarity measurement is in it-
self known to be an hard problem and the scale of
their experimentation is quite small.
3 PRF in the LM Framework
The Language Modeling (LM) Framework allows
PRF to be modelled in a principled manner. In the
LM approach, documents and queries are modeled
using multinomial distribution over words called
document language model P (w|D) and query lan-
guage model P (w|?Q) respectively. For a given
query, the document language models are ranked
based on their proximity to the query language
model, measured using KL-Divergence.
KL(?Q||D) =
X
w
P (w|?Q) ? log
P (w|?Q)
P (w|D)
Since the query length is short, it is difficult to es-
timate ?Q accurately using the query alone. In
PRF, the top k documents obtained through the ini-
tial ranking algorithm are assumed to be relevant
and used as feedback for improving the estima-
tion of ?Q. The feedback documents contain both
relevant and noisy terms from which the feedback
language model is inferred based on a Generative
Mixture Model (Zhai and Lafferty, 2001).
Let DF = {d1, d2, . . . , dk} be the top k docu-
ments retrieved using the initial ranking algorithm.
Zhai and Lafferty (Zhai and Lafferty, 2001) model
the feedback document setDF as a mixture of two
distributions: (a) the feedback language model and
(b) the collection model P (w|C). The feedback
language model is inferred using the EM Algo-
rithm (Dempster et al, 1977), which iteratively
accumulates probability mass on the most distin-
guishing terms, i.e. terms which are more fre-
quent in the feedback document set than in the
entire collection. To maintain query focus the fi-
nal converged feedback model, ?F is interpolated
with the initial query model ?Q to obtain the final
query model ?Final.
?Final = (1? ?) ??Q + ? ??F
?Final is used to re-rank the corpus using the
KL-Divergence ranking function to obtain the fi-
nal ranked list of documents. Henceforth, we refer
Initial Retrieval Algorithm(LM Based Query Likelihood)
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k? Results Top ?k? Results
PRF(Model Based Feedback)
PRF(Model Based Feedback)
L1 Index L2  Index
Final Ranked List Of Documents in L1
FeedbackModel Interpolation Relevance ModelTranslation 
KL-Divergence Ranking Function
Feedback Model ?L2Feedback Model ?L1
Query in L1 Translated Query to L2
ProbabilisticDictionaryL2? L1
TranslatedFeedback Model
Query Model ?Q
Figure 1: Schematic of the Multilingual PRF Approach
Symbol Description
?Q Query Language Model
?FL1 Feedback Language Model obtained from PRF in L1
?FL2 Feedback Language Model obtained from PRF in L2
?TransL1 Feedback Model Translated from L2 to L1
t(f |e) Probabilistic Bi-Lingual Dictionary from L2 to L1
?, ? Interpolation coefficients coefficients used in MultiPRF
Table 2: Glossary of Symbols used in explaining MultiPRF
to the above technique as Model Based Feedback
(MBF).
4 Multilingual PRF (MultiPRF)
The schematic of the MultiPRF approach is shown
in Figure 1. Given a query Q in the source lan-
guage L1, we automatically translate the query
into the assisting language L2. We then rank the
documents in the L2 collection using the query
likelihood ranking function (John Lafferty and
Chengxiang Zhai, 2003). Using the top k doc-
uments, we estimate the feedback model using
MBF as described in the previous section. Simi-
larly, we also estimate a feedback model using the
original query and the top k documents retrieved
from the initial ranking in L1. Let the resultant
feedback models be ?FL2 and ?
F
L1 respectively.
The feedback model estimated in the assisting lan-
guage ?FL2 is translated back into language L1
using a probabilistic bi-lingual dictionary t(f |e)
from L2 ? L1 as follows:
P (f |?TransL1 ) =
X
? e in L2
t(f |e) ? P (e|?FL2 ) (1)
The probabilistic bi-lingual dictionary t(f |e) is
1348
Language CLEF Collection Identifier Description
No. of 
Documents
No. of Unique 
Terms
CLEF Topics 
(No. of Topics)
English
EN-00+01+02 LA Times 94 113005 174669 -
EN-03+05+06 LA Times 94, Glasgow Herald 95 169477 234083 -
EN-02+03 LA Times 94, Glasgow Herald 95 169477 234083 91-200 (67)
French
FR-00 Le Monde 94 44013 127065 1-40 (29) 
FR-01+02 Le Monde 94, French SDA 94  87191 159809 41-140 (88) 
FR-02+03 Le Monde 94, French SDA 94-95 129806 182214 91-200 (67)
FR-03+05 Le Monde 94, French SDA 94-95 129806 182214 141-200,251-300 (99) 
FR-06 Le Monde 94-95, French SDA 94-95 177452 231429 301-350 (48) 
German
DE-00 Frankfurter Rundschau 94, Der Spiegel 94-95 153694 791093 1-40 (33) 
DE-01+02 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94 225371 782304 41-140 (85) 
DE-02+03 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94-95 294809 867072 91-200 (67)
DE-03 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94-95 294809 867072 141-200 (51) 
Finnish FI-02+03+04 Aamulehti 94-95 55344 531160 91-250 (119) FI-02+03 Aamulehti 94-95 55344 531160 91-200 (67)
Dutch NL-02+03 NRC Handelsblad 94-95, Algemeen Dagblad 94-95 190604 575582 91-200 (67)
Spanish ES-02+03 EFE 94, EFE 95 454045 340250 91-200 (67)
Table 1: Details of the CLEF Datasets used for Evaluating the MultiPRF approach. The number shown in brackets of the final
column CLEF Topics indicate the actual number of topics used during evaluation.
Source Term Top Aligned Terms in Target
French English
ame?ricain american, us, united, state, america
nation nation, un, united, state, country
et?ude study, research, assess, investigate, survey
German English
flugzeug aircraft, plane, aeroplane, air, flight
spiele play, game, stake, role, player
verha?ltnis relationship, relate, balance, proportion
Table 3: Top Translation Alternatives for some sample words
in Probabilistic Bi-Lingual Dictionary
learned from a parallel sentence-aligned corpora
in L1?L2 based on word level alignments. Tiede-
mann (Tiedemann, 2001) has shown that the trans-
lation alternatives found using word alignments
could be used to infer various morphological and
semantic relations between terms. In Table 3,
we show the top translation alternatives for some
sample words. For example, the French word
ame?ricain (american) brings different variants of
the translation like american, america, us, united,
state, america which are lexically and semanti-
cally related. Hence, the probabilistic bi-lingual
dictionary acts as a rich source of morphologically
and semantically related feedback terms. Thus,
during this step, of translating the feedback model
as given in Equation 1, the translation model adds
related terms in L1 which have their source as the
term from feedback model ?FL2 . The final Multi-
PRF model is obtained by interpolating the above
translated feedback model with the original query
model and the feedback model of language L1 as
given below:
?MultiL1 = (1? ? ? ?) ??Q + ? ??
F
L1
+ ? ??TransL1 (2)
Since we want to retain the query focus during
back translation the feedback model in L2 is inter-
polated with the translated query before transla-
tion of the L2 feedback model. The parameters ?
and ? control the relative importance of the orig-
inal query model, feedback model of L1 and the
translated feedback model obtained from L1 and
are tuned based on the choice of L1 and L2.
5 Experimental Setup
We evaluate the performance of our system us-
ing the standard CLEF evaluation data in six lan-
guages, widely varying in their familial relation-
ships - Dutch, German, English, French, Span-
ish and Finnish using more than 600 topics. The
details of the collections and their corresponding
topics used for MultiPRF are given in Table 1.
Note that, in each experiment, we choose assist-
ing collections such that the topics in the source
language are covered in the assisting collection so
as to get meaningful feedback terms. In all the top-
ics, we only use the title field. We ignore the top-
ics which have no relevant documents as the true
performance on those topics cannot be evaluated.
We demonstrate the performance of MultiPRF
approach with French, German and Finnish as
source languages and Dutch, English and Span-
ish as the assisting language. We later vary the
assisting language, for each source language and
study the effects. We use the Terrier IR platform
(Ounis et al, 2005) for indexing the documents.
We perform standard tokenization, stop word re-
moval and stemming. We use the Porter Stemmer
for English and the stemmers available through the
Snowball package for other languages. Other than
these, we do not perform any language-specific
processing on the languages. In case of French,
1349
Collection Assist. Lang P@5 P@10 M AP GMAPM BF MultiPRF % Impr. MBF MultiPRF % Impr. MBF MultiPRF % Impr. MBF MultiPRF % Impr.
FR - 00 EN 0.4690 0.5241 11.76
?
0.4000 0.4000 0.00 0.4220 0.4393 4.10 0.2961 0.3413 15.27ES 0.5034 7.35 ? 0.4103 2.59 0.4418 4.69 0.3382 14.22NL 0.5034 7.35 0.4103 2.59 0.4451 5.47 0.3445 16.34
FR - 01+02 EN 0.4636 0.4818 3.92 0.4068 0.4386 7.82
?
0.4342 0.4535 4.43
?
0.2395 0.2721 13.61ES 0.4977 7.35 ? 0.4363 7.26 ? 0.4416 1.70 0.2349 -1.92NL 0.4818 3.92 0.4409 8.38 ? 0.4375 0.76 0.2534 5.80
FR - 03+05 EN 0.4545 0.4768 4.89
?
0.4040 0.4202 4
?
0.3529 0.3694 4.67
?
0.1324 0.1411 6.57ES 0.4727 4.00 0.4080 1.00 0.3582 1.50 0.1325 0.07NL 0.4525 -0.44 0.4010 -0.75 0.3513 0.45 0.1319 -0.38
FR - 06 EN 0.4917 0.5083 3.39 0.4625 0.4729 2.25 0.3837 0.4104 6.97 0.2174 0.2810 29.25ES 0.5083 3.39 0.4687 1.35 0.3918 2.12 0.2617 20.38NL 0.5083 3.39 0.4646 0.45 0.3864 0.71 0.2266 4.23
DE- 00 EN 0.2303 0.3212 39.47
?
0.2394 0.2939 22.78
?
0.2158 0.2273 5.31 0.0023 0.0191 730.43ES 0.3212 39.47 ? 0.2818 17.71 ? 0.2376 10.09 0.0123 434.78NL 0.3151 36.82 ? 0.2818 17.71 ? 0.2331 8.00 0.0122 430.43
DE- 01+02 EN 0.5341 0.6000 12.34
?
0.4864 0.5318 9.35
?
0.4229 0.4576 8.2
?
0.1765 0.2721 9.19ES 0.5682 6.39 ? 0.5091 4.67 ? 0.4459 5.43 0.2309 30.82NL 0.5773 8.09 ? 0.5114 5.15 ? 0.4498 6.35 ? 0.2355 33.43
DE- 03 EN 0.5098 0.5412 6.15 0.4784 0.4980 4.10 0.4274 0.4355 1.91 0.1243 0.1771 42.48ES 0.5647 10.77 ? 0.4980 4.10 0.4568 6.89 ? 0.1645 32.34NL 0.5529 8.45 ? 0.4941 3.27 0.4347 1.72 0.1490 19.87
FI- 02+03+04 EN 0.3782 0.4034 6.67
?
0.3059 0.3319 8.52
?
0.3966 0.4246 7.06
?
0.1344 0.2272 69.05ES 0.3879 2.58 0.3267 6.81 0.3881 -2.15 0.1755 30.58NL 0.3948 4.40 0.3301 7.92 0.4077 2.79 0.1839 36.83
Table 4: Results comparing the performance of MultiPRF over baseline MBF on CLEF collections with English (EN), Spanish
(ES) and Dutch (NL) as assisting languages. Results marked as ? indicate that the improvement was found to be statistically
significant over the baseline at 90% confidence level (? = 0.01) when tested using a paired two-tailed t-test.
since some function words like l?, d? etc., occur as
prefixes to a word, we strip them off during index-
ing and query processing, since it significantly im-
proves the baseline performance. We use standard
evaluation measures like MAP, P@5 and P@10
for evaluation. Additionally, for assessing robust-
ness, we use the Geometric Mean Average Preci-
sion (GMAP) metric (Robertson, 2006) which is
also used in the TREC Robust Track (Voorhees,
2006). The probabilistic bi-lingual dictionary used
in MultiPRF was learnt automatically by running
GIZA++: a word alignment tool (Och and Ney,
2003) on a parallel sentence aligned corpora. For
all the above language pairs we used the Europarl
Corpus (Philipp, 2005). We use Google Trans-
late as the query translation system as it has been
shown to perform well for the task (Wu et al,
2008). We use the MBF approach explained in
Section 3 as a baseline for comparison. We use
two-stage Dirichlet smoothing with the optimal
parameters tuned based on the collection (Zhai and
Lafferty, 2004). We tune the parameters of MBF,
specifically ? and ?, and choose the values which
give the optimal performance on a given collec-
tion. We uniformly choose the top ten documents
for feedback. Table 4 gives the overall results.
6 Results and Discussion
In Table 4, we see the performance of the Multi-
PRF approach for three assisting languages, and
how it compares with the baseline MBF meth-
ods. We find MultiPRF to consistently outperform
the baseline value on all metrics, namely MAP
(where significant improvements range from 4.4%
to 7.1%); P@5 (significant improvements range
from 4.9% to 39.5% and P@10 (where MultiPRF
has significant gains varying from 4% to 22.8%).
Additionally we also find MultiPRF to be more ro-
bust than the baseline, as indicated by the GMAP
score, where improvements vary from 4.2% to
730%. Furthermore we notice these trends hold
across different assisting languages, with Span-
ish and Dutch outperforming English as the as-
sisting language on some of the French and Ger-
man collections. On performing a more detailed
study of the results we identify the main reason
for improvements in our approach is the ability to
obtain good feedback terms in the assisting lan-
guage coupled with the introduction of lexically
and semantically related terms during the back-
translation step.
In Table 5, we see some examples, which illus-
trates the feedback terms brought by the MultiPRF
method. As can be seen by these example, the
gains achieved by MultiPRF are primarily due to
one of three reasons: (a) Good Feedback in As-
sisting Language: If the feedback model in the
assisting language contains good terms, then the
back-translation process will introduce the corre-
sponding feedback terms in the source language,
thus leading to improved performance. As an
example of this phenomena, consider the French
Query ?Maladie de Creutzfeldt-Jakob?. In this
case the original feedback model also performs
1350
TOPIC NO
ASSIST 
LANG.
SOURCE LANGUAGE 
QUERY
TRANSLATED 
QUERY
QUERY 
MEANING
M BF 
M AP
M P RF 
M AP
M BF - Top Representative Terms 
(With Meaning) Excl. Query 
Terms
MultiPRF - Top Representative 
Terms (With Meaning) Excl. Query 
Terms
GERMAN '01: 
TOPIC 61 EN
?lkatastrophe in 
Sibirien Oil Spill in Siberia
Siberian Oil 
Catastrophe 0.618 0.812
exxon , million,  ol (oil), tonn, 
russisch (russian), olp (oil), 
moskau (moscow), us
olverschmutz (oil pollution), ol, 
russisch, erdol (petroleum), russland
(russia), olunfall(oil spill), olp
GERMAN '02: 
TOPIC 105 ES Bronchialasthma El asma bronquial
Bronchial 
Asthma 0.062 0.636
chronisch (chronic), pet, athlet 
(athlete), ekrank (ill), gesund 
(healthy),  tuberkulos 
(tuberculosis), patient, reis (rice), 
person
asthma, allergi, krankheit (disease), 
allerg (allergenic), chronisch, 
hauterkrank (illness of skin), arzt 
(doctor), erkrank (ill)
FRENCH '02: 
TOPIC 107 NL Ing?nierie g?n?tique
Genetische 
Manipulatie
Genetic 
Engineering 0.145 0.357
d?velopp (developed), ?volu 
(evolved), product, produit 
(product), mol?culair (molecular)
genetic, gen, engineering, d?velopp, 
product
FRENCH '06: 
TOPIC 256 EN
Maladie de 
Creutzfeldt -Jakob Creutzfeldt -Jakob
Creutzfeldt -
Jakob Disease 0.507 0.688
malad (illness), produit (product), 
animal (animal), hormon 
(hormone)
malad, humain (human), bovin 
(bovine), enc?phalopath (suffering 
from encephalitis), scientif, recherch 
(research)
GERMAN '03: 
TOPIC 157 EN
Siegerinnen von 
Wimbledon
Champions of 
Wimbledon
Wimbledon 
Lady Winners 0.074 0.146
telefonbuch (phone book), sieg 
(victory), titelseit (front page), 
telekom (telecommunication), 
graf
gross (large), verfecht (champion), 
sampra (sampras), 6, champion, 
steffi, verteidigt (defendending), 
martina, jovotna , navratilova
GERMAN '01: 
TOPIC 91 ES AI in Lateinamerika
La gripe aviar en 
Am?rica Latina
AI in Latin 
America 0.456 0.098
international, amnesty, 
strassenkind (street child),  
kolumbi (Columbian), land, brasili
(Brazil), menschenrecht (human 
rights), polizei (police)
karib (Caribbean), land, brasili, 
schuld (blame), amerika, kalt (cold), 
welt (world), forschung (research)
GERMAN '03: 
TOPIC 196 EN
Fusion japanischer 
Banken
Fusion of Japanese 
banks
Merger of 
Japanese Banks 0.572 0.264
daiwa, tokyo, filial (branch), 
zusammenschluss (merger)
kernfusion (nuclear fusion), 
zentralbank (central bank), daiwa, 
weltbank (world bank), 
investitionsbank (investment bank)
FRENCH '03: 
TOPIC 152 NL Les droits de l'enfant
De rechten van het 
kind Child Rights 0.479 0.284
convent (convention), franc, 
international, onun (united 
nations), r?serv (reserve)
per (father), convent, franc, jurid
(legal), homm (man), cour (court), 
biolog
Table 5: Qualitative comparison of feedback terms given by MultiPRF and MBF on representative queries where positive and
negative results were observed in French and German collections.
quite strongly with a MAP score of 0.507. Al-
though there is no significant topic drift in this
case, there are not many relevant terms apart from
the query terms. However the same query per-
forms very well in English with all the documents
in the feedback set of the English corpus being rel-
evant, thus resulting in informative feedback terms
such as {bovin, scientif, recherch}. (b) Finding
Synonyms/Morphological Variations: Another sit-
uation in which MultiPRF leads to large improve-
ments is when it finds semantically/lexically re-
lated terms to the query terms which the origi-
nal feedback model was unable to. For example,
consider the French query ?Inge?nierie g?n?tique?.
While the feedback model was unable to find
any of the synonyms of the query terms, due to
their lack of co-occurence with the query terms,
the MultiPRF model was able to get these terms,
which are introduced primarily during the back-
translation process. Thus terms like {genetic, gen,
engineering}, which are synonyms of the query
words, are found thus resulting in improved per-
formance. (c) Combination of Above Factors:
Sometimes a combination of the above two factors
causes improvements in the performance as in the
German query ?O?lkatastrophein Sibirien?. For
this query, MultiPRF finds good feedback terms
such as {russisch, russland} while also obtaining
semantically related terms such as {olverschmutz,
erdol, olunfall}.
Although all of the previously described exam-
ples had good quality translations of the query
in the assisting language, as mentioned in (Chin-
nakotla et al, 2010), the MultiPRF approach is
robust to suboptimal translation quality as well.
To see how MultiPRF leads to improvements even
with errors in query translation consider the Ger-
man Query ?Siegerinnen von Wimbledon?. When
this is translated to English, the term ?Lady? is
dropped, this causes only ?Wimbledon Champi-
ons? to remain. As can be observed, this causes
terms like sampras to come up in the MultiPRF
model. However, while the MultiPRF model has
some terms pertaining to Men?s Winners of Wim-
bledon as well, the original feedback model suf-
fers from severe topic drift, with irrelevant terms
such as {telefonbuch, telekom} also amongst the
top terms. Thus we notice that despite the er-
ror in query translation MultiPRF still manages to
correct the drift of the original feedback model,
while also introducing relevant terms such as
{verfecht, steffi, martina, novotna, navratilova}
as well. Thus as shown in (Chinnakotla et al,
2010), having a better query translation system
can only lead to better performance. We also
perform a detailed error analysis and found three
main reasons for MultiPRF failing: (i) Inaccura-
cies in query translation (including the presence of
out-of-vocabulary terms). This is seen in the Ger-
man Query AI in Lateinamerika, which wrongly
translates to Avian Flu in Latin America in Span-
ish thus affecting performance. (ii) Poor retrieval
in Assisting Language. Consider the French query
Les droits de l?enfant, for which due to topic drift
in English, MultiPRF performance reduces. (iii)
In a few rare cases inaccuracy in the back transla-
1351
(a) Source:French (FR-01+02) Assist:Spanish (b) Source:German (DE-01+02) Assist:Dutch
(c) Source:Finnish (FI-02+03+04) Assist:English
Figure 2: Results showing the sensitivity of MultiPRF performance to parameters ? and ? for French, German and Finnish.
tion affects performance as well.
6.1 Parameter Sensitivity Analysis
The MultiPRF parameters ? and ? in Equation
2 control the relative importance assigned to the
original feedback model in source language L1,
the translated feedback model obtained from as-
sisting language L2 and the original query terms.
We varied the ? and ? parameters for French, Ger-
man and Finnish collections with English, Dutch
and Spanish as assisting languages and studied its
effect on MAP of MultiPRF. The results are shown
in Figure 2. The results show that, in all the three
collections, the optimal value of the parameters
almost remains the same and lies in the range of
0.4-0.48. Due to the above reason, we arbitrarily
choose the parameters in the above range and do
not use any technique to learn these parameters.
6.2 Effect of Assisting Language Choice
In this section, we discuss the effect of varying
the assisting language. Besides, we also study
the inter and intra familial behaviour of source-
assisting language pairs. In order to ensure that
the results are comparable across languages, we
indexed the collections from the years 2002, 2003
and use common topics from the topic range 91-
200 that have relevant documents across all the six
languages. The number of such common topics
were 67. For each source language, we use the
other languages as assisting collections and study
the performance of MultiPRF. Since query trans-
lation quality varies across language pairs, we an-
alyze the behaviour of MultiPRF in the following
two scenarios: (a) Using ideal query translation
(b) Using Google Translate for query translation.
In ideal query translation setup, in order to elim-
inate its effect, we skip the query translation step
and use the corresponding original topics for each
target language instead. The results for both the
above scenarios are given in Tables 6 and 7.
From the results, we firstly observe that besides
English, other languages such as French, Spanish,
German and Dutch act as good assisting languages
and help in improving performance over mono-
lingual MBF. We also observe that the best as-
sisting language varies with the source language.
However, the crucial factors of the assisting lan-
guage which influence the performance of Multi-
PRF are: (a) Monolingual PRF Performance: The
main motivation for using a different language was
to get good feedback terms, especially in case of
queries which fail in the source language. Hence,
an assisting language in which the monolingual
feedback performance itself is poor, is unlikely
to give any performance gains. This observation
is evident in case of Finnish, which has the low-
est Monolingual MBF performance. The results
show that Finnish is the least helpful of assist-
ing languages, with performance similar to those
of the baselines. We also observe that the three
best performing assistant languages, i.e. English,
French and Spanish, have the highest monolingual
performances as well, thus further validating the
claim. One possible reason for this is the relative
1352
Source 
Lang.
Assisting Language Source 
Lang.MBF English German Dutch Spanish French Finnish 
English 
MAP 
-
0.4464 ( -0.7%) 0.4471 (-0.5%) 0.4566 (+1.6%) 0.4563 (+1.5%) 0.4545 (+1.1%) 0.4495
P@5 0.4925 ( -0.6%) 0.5045 (+1.8%) 0.5164 (+4.2%) 0.5075 (+2.4%) 0.5194 (+4.8%) 0.4955
P@10 0.4343 (+0.4%) 0.4373 (+1.0%) 0.4537 (+4.8%) 0.4343 (+0.4%) 0.4373 (+1.0%) 0.4328
German 
MAP 0.4229 (+4.9%)
-
0.4346 (+7.8%) 0.4314 (+7.0%) 0.411 (+1.9%) 0.3863 ( -4.2%) 0.4033
P@5 0.5851 (+14%) 0.5851 (+14%) 0.5791 (+12.8%) 0.594 (+15.7%) 0.5522 (+7.6%) 0.5134
P@10 0.5284 (+11.3%) 0.5209 (+9.8%) 0.5179 (+9.1%) 0.5149 (+8.5%) 0.5075 (+6.9%) 0.4746
Dutch 
MAP 0.4317 (+4%) 0.4453 (+7.2%)
-
0.4275 (+2.9%) 0.4241 (+2.1%) 0.3971 ( -4.4%) 0.4153
P@5 0.5642 (+11.8%) 0.5731 (+13.6%) 0.5343 (+5.9%) 0.5582 (+10.6%) 0.5045 (0%) 0.5045
P @10 0.5075 (+9%) 0.4925 (+5.8%) 0.4896 (+5.1%) 0.5015 (+7.7%) 0.4806 (+3.2%) 0.4657
Spanish MAP 0.4667 ( -2.9%) 0.4749 ( -1.2%) 0.4744 (-1.3%)
-
0.4609 ( -4.1%)
0.4311 ( -
10.3%) 0.4805
P@5 0.62 ( -2.9%) 0.6418 (+0.5%) 0.6299 (-1.4%) 0.6269 ( -1.6%) 0.6149 ( -3.7%) 0.6388
P@10 0.5625 ( -1.8%) 0.5806 (+1.3%) 0.5851 (+2.1%) 0.5627 ( -1.8%) 0.5478 ( -4.4%) 0.5731
French 
MAP 0.4658 (+6.9%) 0.4526 (+3.9%) 0.4374 (+0.4%) 0.4634 (+6.4%)
-
0.4451 (+2.2%) 0.4356
P@5 0.4925 (+3.1%) 0.4806  (+0.6%) 0.4567 (-4.4%) 0.4925 (+3.1%) 0.4836 (+1.3%) 0.4776
P@10 0.4358 (+3.9%) 0.4239 (+1%) 0.4224 (+0.7%) 0.4388 (+4.6%) 0.4209 (+0.4%) 0.4194
Finnish
MAP 0.3411 ( -4.7%) 0.3796 (+6.1%) 0.3722 (+4%) 0.369 (+3.1%) 0.3553 ( -0.7%)
-
0.3578
P@5 0.394 (+3.1%) 0.403 (+5.5%) 0.406 (+6.3%) 0.4119 (+7.8%) 0.397 (+3.9%) 0.3821
P@10 0.3463 (+11.5%) 0.3582 (+15.4%) 0.3478 (+12%) 0.3448 (+11%) 0.3433 (+10.6%) 0.3105
Table 6: Results showing the performance of MultiPRF with different source and assisting languages using Google Translate
for query translation step. The intra-familial affinity could be observed from the elements close to the diagonal.
ease of processing in these languages. (b) Familial
Similarity Between Languages: We observe that
the performance of MultiPRF is good if the as-
sisting language is from the same language fam-
ily. Birch et al (2008) show that the language
family is a strong predictor of machine transla-
tion performance. Hence, the query translation
and back translation quality improves if the source
and assisting languages belong to the same family.
For example, in the Germanic family, the source-
assisting language pairs German-English, Dutch-
English, Dutch-German and German-Dutch show
good performance. Similarly, in Romance family,
the performance of French-Spanish confirms this
behaviour. In some cases, we observe that Multi-
PRF scores decent improvements even when the
assisting language does not belong to the same
language family as witnessed in French-English
and English-French. This is primarily due to their
strong monolingual MBF performance.
6.3 Effect of Language Family on Back
Translation Performance
As already mentioned, the performance of Multi-
PRF is good if the source and assisting languages
belong to the same family. In this section, we ver-
ify the above intuition by studying the impact of
language family on back translation performance.
The experiment designed is as follows: Given a
query in source language L1, the ideal translation
in assisting language L2 is used to compute the
query model in L2 using only the query terms.
Then, without performing PRF the query model
Source 
Lang.
Assisting Language
M BF MPRFFR ES DE NL EN FI
French - 0.3686 0.3113 0.3366 0.4338 0.3011 0.4342 0.4535
Spanish 0.3647 - 0.3440 0.3476 0.3954 0.3036 0.5000 0.4892
German 0.2729 0.2736 - 0.2951 0.2107 0.2266 0.4229 0.4576
Dutch 0.2663 0.2836 0.2902 - 0.2757 0.2372 0.3968 0.3989
Table 8: Effect of Language Family on Back Translation
Performance measured through MultiPRF MAP. 100 Topics
from years 2001 and 2002 were used for all languages.
is directly back translated from L2 into L1 and
finally documents are re-ranked using this trans-
lated feedback model. Since the automatic query
translation and PRF steps have been eliminated,
the only factor which influences the MultiPRF per-
formance is the back-translation step. This means
that the source-assisting language pairs for which
the back-translation is good will score a higher
performance. The results of the above experiment
is shown in Table 8. For each source language,
the best performing assisting languages have been
highlighted.
The results show that the performance of
closely related languages like French-Spanish and
German-Dutch is more when compared to other
source-assistant language pairs. This shows that
in case of closely related languages, the back-
translation step succeeds in adding good terms
which are relevant like morphological variants,
synonyms and other semantically related terms.
Hence, familial closeness of the assisting language
helps in boosting the MultiPRF performance. An
exception to this trend is English as assisting lan-
1353
Source 
Lang.
Assisting Language Source 
Lang.MBF English German Dutch Spanish French Finnish 
English 
MAP 
-
0.4513 (+0.4%) 0.4475 ( -0.4%) 0.4695 (+4.5%) 0.4665 (+3.8%) 0.4416 ( -1.7%) 0.4495
P @5 0.5104 (+3.0%) 0.5104 (+3.0%) 0.5343 (+7.8%) 0.5403 (+9.0%) 0.4806 ( -3.0%) 0.4955
P@10 0.4373 (+1.0%) 0.4358 (+0.7%) 0.4597 (+6.2%) 0.4582 (+5.9%) 0.4164 ( -3.8%) 0.4328
German 
MAP 0.4427 (+9.8%)
-
0.4306 (+6.8%) 0.4404 (+9.2%) 0.4104 (+1.8%) 0.3993 ( -1.0%) 0.4033
P@5 0.606 (+18%) 0.5672 (+10.5%) 0.594 (+15.7%) 0.5761 (+12.2%) 0.5552 (+8.1%) 0.5134
P @10 0.5373 (+13.2%) 0.503 (+6.0%) 0.5299 (+11.7%) 0.494 (+4.1%) 0.5 (+5.4%) 0.4746
Dutch 
MAP 0.4361 (+5.0%) 0.4344 (+4.6%)
-
0.4227 (+1.8%) 0.4304 (+3.6%) 0.4134 ( -0.5%) 0.4153
P@5 0.5761 (+14.2%) 0.5552 (+10%) 0.5403 (+7.1%) 0.5463 (+8.3%) 0.5433 (+7.7%) 0.5045
P @10 0.5254 (+12.8%) 0.497 (+6.7%) 0.4776 (+2.6%) 0.5134 (+10.2%) 0.4925 (+5.8%) 0.4657
Spanish 
MAP 0.4665 ( -2.9%) 0.4773 ( -0.7%) 0.4733 ( -1.5%)
-
0.4839 (+0.7%) 0.4412 ( -8.2%) 0.4805
P@5 0.6507 (+1.8%) 0.6448 (+0.9%) 0.6507 (+1.8%) 0.6478 (+1.4%) 0.597 ( -6.5%) 0.6388
P@10 0.5791 (+1.0%) 0.5791 (+1.0%) 0.5761 (+0.5%) 0.5866 (+2.4%) 0.5567 ( -2.9%) 0.5731
French 
MAP 0.4591 (+5.4%) 0.4514 (+3.6%) 0.4409 (+1.2%) 0.4712 (+8.2%)
-
0.4354 (0%) 0.4356
P@5 0.4925 (+3.1%) 0.4776 (0%) 0.4776 (0%) 0.4995 (+4.6%) 0.4955 (+3.8%) 0.4776
P @10 0.4463 (+6.4%) 0.4313 (+2.8%) 0.4373 (+4.3%) 0.4448 (+6.1%) 0.4209 (+0.3%) 0.4194
Finnish
MAP 0.3733 (+4.3%) 0.3559 ( -0.5%) 0.3676 (+2.7%) 0.3594 (+0.4%) 0.371 (+3.7%)
-
0.3578
P@5 0.4149 (+8.6%) 0.385 (+0.7%) 0.388 (+1.6%) 0.388 (+1.6%) 0.3911 (+2.4%) 0.3821
P@10 0.3567 (+14.9%) 0.31 ( -0.2%) 0.3253 (+4.8%) 0.32 (+3.1%) 0.3239 (+4.3%) 0.3105
Table 7: Results showing the performance of MultiPRF without using automatic query translation i.e. by using corresponding
original queries in assisting collection. The results show the potential of MultiPRF by establishing a performance upper bound.
guage which shows good performance across both
families.
6.4 Multiple Assisting Languages
So far, we have only considered a single assist-
ing language. However, a natural extension to
the method which comes to mind, is using mul-
tiple assisting languages. In other words, com-
bining the evidence from all the feedback mod-
els of more than one assisting language, to get a
feedback model which is better than that obtained
using a single assisting language. To check how
this simple extension works, we performed exper-
iments using a pair of assisting languages. In these
experiments for a given source language (from
amongst the 6 previously mentioned languages)
we tried using all pairs of assisting languages (for
each source language, we have 10 pairs possible).
To obtain the final model, we simply interpolate all
the feedback models with the initial query model,
in a similar manner as done in MultiPRF. The re-
sults for these experiments are given in Table 9.
As we see, out of the 60 possible combinations
of source language and assisting language pairs,
we obtain improvements of greater than 3% in 16
cases. Here the improvements are with respect to
the best model amongst the two MultiPRF mod-
els corresponding to each of the two assisting lan-
guages, with the same source language. Thus we
observe that a simple linear interpolation of mod-
els is not the best way of combining evidence from
multiple assisting languages. We also observe than
when German or Spanish are used as one of the
two assisting languages, they are most likely to
Source 
Language
Assisting Language Pairs with 
Improvement > 3%
English FR-DE (4.5%),  FR -ES (4.8%), DE-NL (+3.1%)
French EN-DE (4.1%), DE -ES (3.4%), NL-FI (4.8 %)
German None
Spanish None
Dutch
EN-DE (3.9%), DE -FR (4.1%), FR -ES (3.8%), DE-ES 
(3.9%)
Finnish
EN-ES (3.2%), FR -DE (4.6%), FR -ES (6.4%),  
DE-ES (11.2%), DE -NL (4.4%), ES -NL (5.9%)
Total - 16
EN ? 3 Pairs; FR ? 6 Pairs; DE ? 10 Pairs; 
ES - 8 Pairs; NL ? 4 Pairs; FI ? 1 Pair
Table 9: Summary of MultiPRF Results with Two Assisting
Languages. The improvements described above are with re-
spect to maximum MultiPRF MAP obtained using either L1
or L2 alone as assisting language.
lead to improvements. A more detailed study of
this observation needs to be done to explain this.
7 Conclusion and Future Work
We studied the effect of different source-assistant
pairs and multiple assisting languages on the per-
formance of MultiPRF. Experiments across a wide
range of language pairs with varied degree of fa-
milial relationships show that MultiPRF improves
performance in most cases with the performance
improvement being more pronounced when the
source and assisting languages are closely related.
We also notice that the results are mixed when two
assisting languages are used simultaneously. As
part of future work, we plan to vary the model
interpolation parameters dynamically to improve
the performance in case of multiple assisting lan-
guages.
Acknowledgements
The first author was supported by a fellowship
award from Infosys Technologies Ltd., India. We
would like to thank Mr. Vishal Vachhani for his
help in running the experiments.
1354
References
Giambattista Amati, Claudio Carpineto, and Giovanni Ro-
mano. 2004. Query Difficulty, Robustness, and Selec-
tive Application of Query Expansion. In ECIR ?04, pages
127?137.
Alexandra Birch, Miles Osborne and Philipp Koehn. 2008.
Predicting Success in Machine Translation. In EMNLP
?08, pages 745-754, ACL.
Martin Braschler and Carol Peters. 2004. Cross-Language
Evaluation Forum: Objectives, Results, Achievements.
Inf. Retr., 7(1-2):7?31.
Martin Braschler and Peter Scha?uble. 1998. Multilingual In-
formation Retrieval based on Document Alignment Tech-
niques. In ECDL ?98, pages 183?197, Springer-Verlag.
Chris Buckley, Gerald Salton, James Allan, and Amit Sing-
hal. 1994. Automatic Query Expansion using SMART :
TREC 3. In TREC-3, pages 69?80.
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen
Robertson. 2008. Selecting Good Expansion Terms for
Pseudo-Relevance Feedback. In SIGIR ?08, pages 243?
250. ACM.
Manoj K. Chinnakotla, Karthik Raman, and Pushpak Bhat-
tacharyya. 2010. Multilingual PRF: English Lends a
Helping Hand. In SIGIR ?10, ACM.
Kevyn Collins-Thompson and Jamie Callan. 2005. Query
Expansion Using Random Walk Models. In CIKM ?05,
pages 704?711. ACM.
Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.
2004. A Framework for Selective Query Expansion. In
CIKM ?04, pages 236?237. ACM.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two Lan-
guages Are More Informative Than One. In ACL ?91,
pages 130?137. ACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum Like-
lihood from Incomplete Data via the EM Algorithm. Jour-
nal of the Royal Statistical Society, 39:1?38.
T. Susan Dumais, A. Todd Letsche, L. Michael Littman, and
K. Thomas Landauer. 1997. Automatic Cross-Language
Retrieval Using Latent Semantic Indexing. In AAAI ?97,
pages 18?24.
Wei Gao, John Blitzer, and Ming Zhou. 2008. Using English
Information in Non-English Web Search. In iNEWS ?08,
pages 17?24. ACM.
David Hawking, Paul Thistlewaite, and Donna Harman.
1999. Scaling Up the TREC Collection. Inf. Retr., 1(1-
2):115?137.
Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard
Zens, Rwth Aachen, Alexandra Constantin, Marcello Fed-
erico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade
Shen, Christine Moran, and Ondej Bojar. 2007. Moses:
Open Source Toolkit for Statistical Machine Translation.
In ACL ?07, pages 177?180.
P. Jourlin, S. E. Johnson, K. Spa?rck Jones and P. C. Wood-
land. 1999. Improving Retrieval on Imperfect Speech
Transcriptions (Poster Abstract). In SIGIR ?99, pages
283?284. ACM.
John Lafferty and Chengxiang Zhai. 2003. Probabilistic Rel-
evance Models Based on Document and Query Genera-
tion. Language Modeling for Information Retrieval, pages
1?10. Kluwer International Series on IR.
K. Sparck Jones, S. Walker, and S. E. Robertson. 2000. A
Probabilistic Model of Information Retrieval: Develop-
ment and Comparative Experiments. Inf. Process. Man-
age., 36(6):779?808.
John Lafferty and Chengxiang Zhai. 2001. Document Lan-
guage Models, Query Models, and Risk Minimization for
Information Retrieval. In SIGIR ?01, pages 111?119.
ACM.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based
Language Models. In SIGIR ?01, pages 120?127. ACM.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-Lingual Relevance Models. In SIGIR ?02,
pages 175?182, ACM.
Edgar Meij, Dolf Trieschnigg, Maarten Rijke de, and Wessel
Kraaij. 2009. Conceptual Language Models for Domain-
specific Retrieval. Information Processing & Manage-
ment, 2009.
Donald Metzler and W. Bruce Croft. 2007. Latent Concept
Expansion Using Markov Random Fields. In SIGIR ?07,
pages 311?318. ACM.
Mandar Mitra, Amit Singhal, and Chris Buckley. 1998. Im-
proving Automatic Query Expansion. In SIGIR ?98, pages
206?214. ACM.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
I. Ounis, G. Amati, Plachouras V., B. He, C. Macdonald, and
Johnson. 2005. Terrier Information Retrieval Platform.
In ECIR ?05, volume 3408 of Lecture Notes in Computer
Science, pages 517?519. Springer.
Koehn Philipp. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In MT Summit ?05.
Stephen Robertson. 2006. On GMAP: and Other Transfor-
mations. In CIKM ?06, pages 78?83. ACM.
Tetsuya Sakai, Toshihiko Manabe, and Makoto Koyama.
2005. Flexible Pseudo-Relevance Feedback Via Selective
Sampling. ACM TALIP, 4(2):111?135.
Tao Tao and ChengXiang Zhai. 2006. Regularized Esti-
mation of Mixture Models for Robust Pseudo-Relevance
Feedback. In SIGIR ?06, pages 162?169. ACM.
Tuomas Talvensaari, Jorma Laurikkala, Kalervo Ja?rvelin,
Martti Juhola, and Heikki Keskustalo. 2007. Creating and
Exploiting a Comparable Corpus in Cross-language Infor-
mation Retrieval. ACM Trans. Inf. Syst., 25(1):4, 2007.
Jrg Tiedemann. 2001. The Use of Parallel Corpora in Mono-
lingual Lexicography - How word alignment can identify
morphological and semantic relations. In COMPLEX ?01,
pages 143?151.
Ellen M. Voorhees. 1994. Query Expansion Using Lexical-
Semantic Relations. In SIGIR ?94, pages 61?69. Springer-
Verlag.
1355
Ellen Voorhees. 2006. Overview of the TREC 2005 Robust
Retrieval Track. In TREC 2005, Gaithersburg, MD. NIST.
Dan Wu, Daqing He, Heng Ji, and Ralph Grishman. 2008.
A Study of Using an Out-of-Box Commercial MT System
for Query Translation in CLIR. In iNEWS ?08, pages 71?
76. ACM.
Jinxi Xu and W. Bruce Croft. 2000. Improving the Effective-
ness of Information Retrieval with Local Context Analy-
sis. ACM Trans. Inf. Syst., 18(1):79?112.
Jinxi Xu, Alexander Fraser, and Ralph Weischedel. 2002.
Empirical Studies in Strategies for Arabic Retrieval. In
SIGIR ?02, pages 269?274. ACM.
Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009.
Query Dependent Pseudo-Relevance Feedback Based on
Wikipedia. In SIGIR ?09, pages 59?66. ACM.
Chengxiang Zhai and John Lafferty. 2001. Model-based
Feedback in the Language Modeling approach to Infor-
mation Retrieval. In CIKM ?01, pages 403?410. ACM.
Chengxiang Zhai and John Lafferty. 2004. A Study of
Smoothing Methods for Language Models applied to In-
formation Retrieval. ACM Transactions on Information
Systems, 22(2):179?214.
1356
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 70?78,
Beijing, August 2010
More Languages, More MAP?: A Study of Multiple Assisting Languages
in Multilingual PRF
Vishal Vachhani Manoj K. Chinnakotla Mitesh M. Khapra Pushpak Bhattacharyya
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay
{vishalv,manoj,miteshk,pb}@cse.iitb.ac.in
Abstract
Multilingual Pseudo-Relevance Feedback
(MultiPRF) is a framework to improve
the PRF of a source language by taking
the help of another language called as-
sisting language. In this paper, we ex-
tend the MultiPRF framework to include
multiple assisting languages. We consider
three different configurations to incorpo-
rate multiple assisting languages - a) Par-
allel - all assisting languages combined
simultaneously b) Serial - assisting lan-
guages combined in sequence one after
another and c) Selective - dynamically se-
lecting the best feedback model for each
query. We study their effect on MultiPRF
performance. Results using multiple as-
sisting languages are mixed and it helps in
boosting MultiPRF accuracy only in some
cases. We also observe that MultiPRF be-
comes more robust with increase in num-
ber of assisting languages.
1 Introduction
Pseudo-Relevance Feedback (PRF) (Buckley et
al., 1994; Xu and Croft, 2000; Mitra et al, 1998)
is known to be an effective technique to im-
prove the effectiveness of Information Retrieval
(IR) systems. In PRF, the top ?k? documents
from the ranked list retrieved using the initial key-
word query are assumed to be relevant. Later,
these documents are used to refine the user query
and the final ranked list is obtained using the
above refined query. Although PRF has been
shown to improve retrieval, it suffers from the
following drawbacks: (a) Lexical and Semantic
Non-Inclusion: the type of term associations ob-
tained for query expansion is restricted to only
co-occurrence based relationships in the feedback
documents and (b) Lack of Robustness: due to
the inherent assumption in PRF, i.e., relevance
of top k documents, performance is sensitive to
that of the initial retrieval algorithm and as a re-
sult is not robust. Typically, larger coverage en-
sures higher proportion of relevant documents in
the top k retrieval (Hawking et al, 1999). How-
ever, some resource-constrained languages do not
have adequate information coverage in their own
language. For example, languages like Hungarian
and Finnish have meager online content in their
own languages.
Multilingual Pseudo-Relevance Feedback
(MultiPRF) (Chinnakotla et al, 2010a) is a
novel framework for PRF to overcome the above
limitations of PRF. It does so by taking the help of
a different language called the assisting language.
Thus, the performance of a resource-constrained
language could be improved by harnessing the
good coverage of another language. MulitiPRF
showed significant improvements on standard
CLEF collections (Braschler and Peters, 2004)
over state-of-art PRF system. On the web, each
language has its own exclusive topical coverage
besides sharing a large number of common topics
with other languages. For example, information
about Saudi Arabia government policies and
regulations is more likely to be found in Arabic
language web and also information about a local
event in Spain is more likely to be covered in
Spanish web than in English. Hence, using
multiple languages in conjunction is more likely
to ensure satisfaction of the user information need
and hence will be more robust.
In this paper, we extend the MultiPRF frame-
work to multiple assisting languages. We study
70
the various possible ways of combining the mod-
els learned from multiple assisting languages. We
propose three different configurations for includ-
ing multiple assisting languages in MultiPRF - a)
Parallel b) Serial and c) Selective. In Parallel com-
bination, all the assisting languages are combined
simultaneously using interpolation. In Serial con-
figuration, the assisting languages are applied in
sequence one after another and finally, in Selec-
tive configuration, the best feedback model is dy-
namically chosen for each query. We experiment
with each of the above configurations and present
both quantitative and qualitative analysis of the re-
sults. Results using multiple assisting languages
are mixed and it helps in boosting MultiPRF ac-
curacy only in some cases. We also observe that
MultiPRF becomes more robust with increase in
number of assisting languages. Besides, we also
study the relation between number of assisting
languages, coverage and the MultiPRF accuracy.
The paper is organized as follows: Section 2,
explains the Language Modeling (LM) based PRF
approach. Section 3, describes the MultiPRF ap-
proach. Section 4 explains the various configu-
rations to extend MultiPRF for multiple assisting
languages. Section 6 presents the results and dis-
cussions. Finally, Section 7 concludes the paper.
2 PRF in the LM Framework
The Language Modeling (LM) Framework allows
PRF to be modeled in a principled manner. In the
LM approach, documents and queries are mod-
eled using multinomial distribution over words
called document language model P (w|D) and
query language model P (w|?Q) respectively. For
a given query, the document language models are
ranked based on their proximity to the query lan-
guage model, measured using KL-Divergence.
KL(?Q||D) =
?
w
P (w|?Q) ? log
P (w|?Q)
P (w|D)
Since the query length is short, it is difficult to es-
timate ?Q accurately using the query alone. In
PRF, the top k documents obtained through the
initial ranking algorithm are assumed to be rele-
vant and used as feedback for improving the es-
timation of ?Q. The feedback documents con-
tain both relevant and noisy terms from which
Symbol Description
?Q Query Language Model
?FL1 Feedback Language Model obtained from PRF in L1
?FL2 Feedback Language Model obtained from PRF in L2
?TransL1 Feedback Model Translated from L2 to L1
t(f |e) Probabilistic Bi-Lingual Dictionary from L2 to L1
?, ? Interpolation coefficients coefficients used in MultiPRF
Table 1: Glossary of Symbols used in explaining MultiPRF
the feedback language model is inferred based on
a Generative Mixture Model (Zhai and Lafferty,
2001).
Let DF = {d1, d2, . . . , dk} be the top k doc-
uments retrieved using the initial ranking algo-
rithm. Zhai and Lafferty (Zhai and Lafferty, 2001)
model the feedback document setDF as a mixture
of two distributions: (a) the feedback language
model and (b) the collection model P (w|C). The
feedback language model is inferred using the EM
Algorithm (Dempster et al, 1977), which itera-
tively accumulates probability mass on the most
distinguishing terms, i.e. terms which are more
frequent in the feedback document set than in the
entire collection. To maintain query focus the fi-
nal converged feedback model, ?F is interpolated
with the initial query model ?Q to obtain the final
query model ?Final.
?Final = (1? ?) ??Q + ? ??F
?Final is used to re-rank the corpus using the
KL-Divergence ranking function to obtain the fi-
nal ranked list of documents. Henceforth, we refer
to the above technique as Model Based Feedback
(MBF).
3 Multilingual Pseudo-Relevance
Feedback (MultiPRF)
Chinnakotla et al (Chinnakotla et al, 2010a;
Chinnakotla et al, 2010b) propose the MultiPRF
approach which overcomes the fundamental limi-
tations of PRF with the help of an assisting collec-
tion in a different language. Given a query Q in
the source language L1, it is automatically trans-
lated into the assisting language L2. The docu-
ments in the L2 collection are ranked using the
query likelihood ranking function (John Lafferty
and Chengxiang Zhai, 2003). Using the top k doc-
uments, they estimate the feedback model using
MBF as described in the previous section. Simi-
larly, they also estimate a feedback model using
71
the original query and the top k documents re-
trieved from the initial ranking in L1. Let the re-
sultant feedback models be ?FL2 and ?FL1 respec-tively. The feedback model estimated in the as-
sisting language ?FL2 is translated back into lan-guage L1 using a probabilistic bi-lingual dictio-
nary t(f |e) from L2 ? L1 as follows:
P (f |?TransL1 ) =
?
? e in L2
t(f |e) ? P (e|?FL2) (1)
The probabilistic bi-lingual dictionary t(f |e) is
learned from a parallel sentence-aligned corpora
in L1 ? L2 based on word level alignments. The
probabilistic bi-lingual dictionary acts as a rich
source of morphologically and semantically re-
lated feedback terms. Thus, the translation model
adds related terms in L1 which have their source
as the term from feedback model ?FL2 . The finalMultiPRF model is obtained by interpolating the
above translated feedback model with the original
query model and the feedback model of language
L1 as given below:
?MultiL1 = (1? ? ? ?) ??Q + ? ??FL1 + ? ??TransL1(2)
In order to retain the query focus during back
translation, the feedback model in L2 is interpo-
lated with the translated query before translation
of the L2 feedback model. The parameters ? and
? control the relative importance of the original
query model, feedback model of L1 and the trans-
lated feedback model obtained from L1 and are
tuned based on the choice of L1 and L2.
4 Extending MultiPRF to Multiple
Assisting Languages
In this section, we extend the MultiPRF model
described earlier to multiple assisting languages.
Since each language produces a different feed-
back model, there could be different ways of com-
bining these models as suggested below.
Parallel: One way is to include the new assist-
ing language model using one more interpo-
lation coefficient which gives the effect of us-
ing multiple assisting languages in parallel.
Serial: Alternately, we can have a serial combi-
nation wherein language L2 is first assisted
Initial Retrieval(LM Based Query Likelihood)
Top ?k?Results
PRF(Model Based Feedback)
L  Index
Final Ranked List Of Documents in L
Feedback ModelInterpolation
Relevance ModelTranslation 
KL-Divergence Ranking Function
Feedback Model  ?L1Feedback Model ?L
Query in L Translated Query to L1
ProbabilisticDictionaryL1? LQuery Model ?Q
Translated Query to LnInitial Retrieval(LM Based Query Likelihood)
Top ?k?Results
PRF(Model Based Feedback)
L1 Index
Relevance ModelTranslation 
Feedback Model  ?Ln
Initial Retrieval
Top ?k?Results
PRF(Model Based Feedback)
LnIndex
ProbabilisticDictionaryLn? L
Figure 1: Schematic of the Multilingual PRF Approach Us-
ing Parallel Assistance
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k?Results
PRF(Model Based Feedback)
FeedbackModel Interpolation
KL-Divergence Ranking Function
L Index
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k? Results
PRF(Model Based Feedback)
L2  Index
Relevance ModelTranslation 
L1 Index
Feedback Model    ?L1
Query in L1
Initial Retrieval Algorithm(LM Based Query Likelihood)
Top ?k? Results
PRF(Model Based Feedback)
FeedbackModel Interpolation
Feedback Model    ?L2
Query in L2
Top ?k? ResultsPRF(Model Based Feedback) KL Divergence Ranking
ProbabilisticDictionaryL2 ? L1
Relevance ModelTranslation 
ProbabilisticDictionaryL1? L
Feedback Model    ?LQuery Model ?Q
Figure 2: Schematic of the Multilingual PRF Approach Us-
ing Serial Assistance
by language L3 and then this MultiPRF sys-
tem is used to assist the source language L1.
Selective: Finally, we can have selective assis-
tance wherein we dynamically select which
assisting language to use based on the input
query.
Below we describe each of these systems in detail.
4.1 Parallel Combination
The MultiPRF model as explained in section 3 in-
terpolates the query model of L1 with the MBF
of L1 and the translated feedback model of the
assisting language L2. The most natural exten-
sion to this approach is to translate the query into
multiple languages instead of a single language
and collect the feedback terms from the initial re-
72
Language CLEF Collection Identifier Description
No. of 
Documents
No. of Unique 
Terms CLEF Topics (No. of Topics)
English EN-02+03 LA Times 94, Glasgow Herald 95 169477 234083 91-200 (67)
French FR-02+03 Le Monde 94, French SDA 94-95 129806 182214 91-200 (67)
German DE-02+03 Frankfurter Rundschau 94, Der Spiegel 94-95, German SDA 94-95 294809 867072 91-200 (67)
Finnish FI-02+03 Aamulehti 94-95 55344 531160 91-200 (67)
Dutch NL-02+03 NRC Handelsblad 94-95, Algemeen Dagblad 94-95 190604 575582 91-200 (67)
Spanish ES-02+03 EFE 94, EFE 95 454045 340250 91-200 (67)
Table 2: Details of the CLEF Datasets used for Evaluating the MultiPRF approach. The number shown in brackets of the final
column CLEF Topics indicate the actual number of topics used during evaluation.
trieval of each of these languages. The translated
feedback models resulting from each of these re-
trievals can then be interpolated to get the final
parallel MultiPRF model. Specifically, if L1 is the
source language and L2, L3, . . . Ln are assisting
languages then final parallel MultiPRF model can
be obtained by generalizing Equation 2 as shown
below:
?MultiAssistL1 = (1? ? ?
X
i
?i) ??Q + ? ??F +
X
i
?i ??TransLi
(3)
The schematic representation of parallel combina-
tion is shown in Figure 1.
4.2 Serial Combination
Let L1 be the source language and let L2 and L3
be two assisting languages. A serial combination
can then be achieved by cascading two MultiPRF
systems as described below:
1. Construct a MultiPRF system with L2 as
the source language and L3 as the assist-
ing language. We call this system as L2L3-
MultiPRF system.
2. Next, construct a MultiPRF system with L1
as the source language and L2L3-MultiPRF
as the assisting system.
As compared to a single assistance system where
only L2 is used as the assisting language for
L1, here the performance of language L2 is first
boosted using L3 as the assisting language. This
boosted system is then used for assisting L1. Also
note that unlike parallel assistance here we do
not introduce an extra interpolation co-efficient in
the original MultiPRF model given in Equation 2.
The schematic representation of serial combina-
tion is shown in Figure 2.
4.3 Selective Assistance
We motivate selective assistance by posing the
following question: ?Given a source language
L1 and two assisting languages L2 and L3, is
it possible that L2 is ideal for assisting some
queries whereas L3 is ideal for assisting some
other queries?? For example, suppose L2 has a
rich collection of TOURISM documents whereas
L3 has a rich collection of HEALTH documents.
Now, given a query pertaining to TOURISM do-
main one might expect L2 to serve as a better as-
sisting language whereas given a query pertaining
to the HEALTH domain one might expect L3 to
serve as a better assisting language. This intuition
can be captured by suitably changing the interpo-
lation model as shown below:
?BestL = SelectBestModel(?
F
L ,?
Trans
L1 ,?
Trans
L2 ,?
Trans
L12 )
?MultiL1 = (1? ?) ??Q + ? ??
Best
L (4)
where, SelectBestModel() gives the best
model for a particular query using the algorithm
mentioned below which is based on minimizing
the query drift as described in (?):
1. Obtain the four feedback models, viz.,
?FL ,?TransL1 ,?
Trans
L2 ,?
Trans
L12
2. Build a language model (say, LM ) using
queryQ and top-100 documents of initial re-
trieval in language L.
3. Find the KL-Divergence between LM and
the four models obtained during step 1.
4. Select the model which has minimum KL-
Divergence score from LM . Call this model
?BestL .
5. Get the final model by interpolating the
query model, ?Q, with ?BestL .
73
5 Experimental Setup
We evaluate the performance of our system us-
ing the standard CLEF evaluation data in six lan-
guages, widely varying in their familial relation-
ships - Dutch, German, English, French, Spanish
and Finnish. The details of the collections and
their corresponding topics used for MultiPRF are
given in Table 2. Note that, in each experiment,
we choose assisting collections such that the top-
ics in the source language are covered in the as-
sisting collection so as to get meaningful feedback
terms. In all the topics, we only use the title field.
We ignore the topics which have no relevant docu-
ments as the true performance on those topics can-
not be evaluated.
We use the Terrier IR platform (Ounis et al,
2005) for indexing the documents. We perform
standard tokenization, stop word removal and
stemming. We use the Porter Stemmer for English
and the stemmers available through the Snowball
package for other languages. Other than these,
we do not perform any language-specific process-
ing on the languages. In case of French, since
some function words like l?, d? etc., occur as pre-
fixes to a word, we strip them off during index-
ing and query processing, since it significantly im-
proves the baseline performance. We use standard
evaluation measures like MAP, P@5 and P@10
for evaluation. Additionally, for assessing robust-
ness, we use the Geometric Mean Average Preci-
sion (GMAP) metric (Robertson, 2006) which is
also used in the TREC Robust Track (Voorhees,
2006). The probabilistic bi-lingual dictionary
used in MultiPRF was learnt automatically by run-
ning GIZA++: a word alignment tool (Och and
Ney, 2003) on a parallel sentence aligned corpora.
For all the above language pairs we used the Eu-
roparl Corpus (Philipp, 2005). We use Google
Translate as the query translation system as it has
been shown to perform well for the task (Wu et
al., 2008). We use two-stage Dirichlet smooth-
ing with the optimal parameters tuned based on
the collection (Zhai and Lafferty, 2004). We tune
the parameters of MBF, specifically ? and ?, and
choose the values which give the optimal perfor-
mance on a given collection. We observe that the
optimal parameters ? and ? are uniform across
collections and vary in the range 0.4-0.48. We
Source
Langs
Assist.
Langs
MBF MultiPRF
(L1)
MultiPRF
(L2)
MultiPRF
(L1,L2)
EN
DE-NL
MAP 0.4495 0.4464 0.4471 0.4885(4.8)?
P@5 0.4955 0.4925 0.5045 0.5164(2.4)
P@10 0.4328 0.4343 0.4373 0.4463(2.1)
DE-FI
MAP 0.4495 0.4464 0.4545 0.4713(3.7)?
P@5 0.4955 0.4925 0.5194 0.5224(1.2)
P@10 0.4328 0.4343 0.4373 0.4507(3.1)
NL-ES
MAP 0.4495 0.4471 0.4566 0.4757(4.2)?
P@5 0.4955 0.5045 0.5164 0.5224(0.6)
P@10 0.4328 0.4373 0.4537 0.4448(2.4)
ES-FR
MAP 0.4495 0.4566 0.4563 0.48(5.1)?
P@5 0.4955 0.5164 0.5075 0.5224(1.2)
P@10 0.4328 0.4537 0.4343 0.4388(-3.3)
ES-FI
MAP 0.4495 0.4566 0.4545 0.48(5.1)?
P@5 0.4955 0.5164 0.5194 0.5254(1.7)
P@10 0.4328 0.4537 0.4373 0.4403(-3.0)
FR-FI
MAP 0.4495 0.4563 0.4545 0.4774(4.6)
P@5 0.4955 0.5075 0.5194 0.5284(4.1)?
P@10 0.4328 0.4343 0.4373 0.4373(0.7)
FI
EN-FR
MAP 0.3578 0.3411 0.3553 0.3688(3.8)
P@5 0.3821 0.394 0.397 0.4149(4.5)?
P@10 0.3105 0.3463 0.3433 0.3433(0.1)
NL-DE
MAP 0.3578 0.3722 0.3796 0.3929(3.5)
P@5 0.3821 0.406 0.403 0.4149(3.0)
P@10 0.3105 0.3478 0.3582 0.3597(0.4)
ES-DE
MAP 0.3578 0.369 0.3796 0.4058(6.9)?
P@5 0.3821 0.4119 0.403 0.4239(5.2)
P@10 0.3105 0.3448 0.3582 0.3612(0.8)
FR-DE
MAP 0.3578 0.3553 0.3796 0.3988(5.1)?
P@5 0.3821 0.397 0.403 0.406(0.7)
P@10 0.3105 0.3433 0.3582 0.3507(-2.1)
NL-ES
MAP 0.3578 0.3722 0.369 0.3875(4.1)?
P@5 0.3821 0.406 0.4119 0.4060.0)
P@10 0.3105 0.3478 0.3448 0.3537(1.7)
NL-FR
MAP 0.3578 0.3722 0.3553 0.3875(4.1)?
P@5 0.3821 0.406 0.397 0.409(0.7)
P@10 0.3105 0.3478 0.3433 0.3463(-0.4)
ES-FR
MAP 0.3578 0.369 0.3553 0.3823(3.6)
P@5 0.3821 0.4119 0.397 0.4119(0.0)
P@10 0.3105 0.3448 0.3433 0.3418(-0.9)
FR EN-ES
MAP 0.4356 0.4658 0.4634 0.4803(3.1)
P@5 0.4776 0.4925 0.4925 0.4985(1.2)
P@10 0.4194 0.4358 0.4388 0.4493(3.1)?
Table 3: Comparison of MultiPRF Multiple Assisting Lan-
guages using parallel assistance framework with MultiPRF
with single assisting language. Only language pairs where
positive improvements were obtained are reported here. Re-
sults marked as ? indicate that the improvement was sta-
tistically significant over baseline (Maximum of MultiPRF
with single assisting language) at 90% confidence level (? =
0.01) when tested using a paired two-tailed t-test.
uniformly choose the top ten documents for feed-
back.
6 Results and Discussion
Tables ?? and ?? present the results for Multi-
PRF with two assisting languages using paral-
lel assistance and selective assistance framework.
Out of the total 60 possible combinations, in Ta-
ble ??, we only report the combinations where
we have obtained positive improvements greater
than 3%. We observe most improvements in En-
glish, Finnish and French. We did not observe any
improvements using the serial assistance frame-
work over MultiPRF with single assisting lan-
74
Source
Langs
Assist.
Langs
Parallel Model Selective Model
EN DE-NL
MAP 0.4651 0.4848
P@5 0.5254 0.5224
P@10 0.4493 0.4522
NL-FI
MAP 0.4387 0.4502
P@5 0.5015 0.5164
P@10 0.4284 0.4358
DE
EN-FR
MAP 0.4097 0.4302
P@5 0.594 0.5851
P@10 0.5149 0.5179
FR-ES
MAP 0.4215 0.4333
P@5 0.591 0.591
P@10 0.5239 0.5209
FR-NL
MAP 0.4139 0.4236
P@5 0.5701 0.5701
P@10 0.5075 0.5134
FR-FI
MAP 0.3925 0.4055
P@5 0.5101 0.5642
P@10 0.4851 0.5
NL-FI
MAP 0.3974 0.4192
P@5 0.5731 0.5612
P@10 0.497 0.503
ES EN-FI
MAP 0.4436 0.4501
P@5 0.6179 0.6269
P@10 0.5567 0.5657
DE-FI
MAP 0.4542 0.465
P@5 0.6269 0.6179
P@10 0.5627 0.5582
NL-FI
MAP 0.4531 0.4611
P@5 0.6269 0.6299
P@10 0.5627 0.5627
Table 4: Results showing the positive improvements of Mul-
tiPRF with selective assistance framework over MultiPRF
with parallel assistance framework.
guage. Hence, we do not report their results as
the results were almost equivalent to single as-
sisting language. As shown in Table ??, selec-
tive assistance does give decent improvements in
some language pairs. An interesting point to note
in selective assistance is that it helps languages
like Spanish whose monolingual performance and
document coverage are both high.
6.1 Qualitative Comparison of Feedback
Terms using Multiple Languages
In this section, we qualitatively compare the re-
sults of MultiPRF with two assisting languages
with that of MultiPRF with single assisting lan-
guage, based on the top feedback terms obtained
by each model. Specifically, in Table 5 we com-
pare the terms obtained by MultiPRF using (i)
Only L1 as assisting language, (ii) Only L2 as as-
sisting language and (iii) Both L1 and L2 as as-
sisting languages in a parallel combination. For
example, the first row in the above table shows
the terms obtained by each model for the En-
glish query ?Golden Globes 1994?. Here, L1 is
French and L2 is Spanish. Terms like ?Gold?
and ?Prize? appearing in the translated feedback
model of L1 cause a drift in the topic towards
?Gold Prize? resulting in a lower MAP score
(0.33). Similarly, the terms like ?forrest? and
?spielberg? appearing in the translated feedback
model of L2 cause a drift in topic towards For-
rest Gump and Spielberg Oscars resulting in a
MAP score (0.5). However, when the models
from two languages are combined, terms which
cause a topic drift get ranked lower and as a result
the focus of the query is wrenched back. A sim-
ilar observation was made for the English query
?Damages in Ozone Layer? using French (L1)
and Spanish (L2) as assisting languages. Here,
terms from the translated feedback model of L1
cause a drift in topic towards ?militri bacteria?
whereas the terms from the translated feedback
model of L2 cause a drift in topic towards ?iraq
war?. However, in the combined model these
terms get lower rank there by bringing back the
focus of the query. For the Finnish query ?Lasten
oikeudet? (Children?s Rights), in German (L1),
the topic drift is introduced by terms like ?las,
gram, yhteis?. In case of Dutch (L2), the query
drift is caused by ?mandy, richard, slovakia? (L2)
and in the case of combined model, these terms
get less weightage and the relevant terms like
?laps, oikeuks, vanhemp? which are common in
both models, receive higher weightage causing an
improvement in query performance.
Next, we look at a few negative examples where
the parallel combination actually performs poorer
than the individual models. This happens when
some drift-terms (i.e., terms which can cause
topic drift) get mutually reinforced by both the
models. For example, for the German query
?Konkurs der Baring-Bank? (Bankruptcy of Bar-
ing Bank) the term ?share market? which was ac-
tually ranked lower in the individual models gets
boosted in the combined model resulting in a drift
in topic. Similarly, for the German query ?Ehren-
Oscar fu?r italienische Regisseure? (Honorary Os-
car for Italian directors) the term ?head office?
which was actually ranked lower in the individual
models gets ranked higher in the combined model
due to mutual reinforcement resulting in a topic
drift.
75
TOPIC NO.
QUERIES
(Meaning in 
Eng.)
TRANSLATED ENGLISH 
QUERIES 
(Assisting Lang.)
L1 
M AP
L2
M AP
L1 - L2
M AP
Representative Terms with L1 as
Single Assisting Language (With 
M eaning)
Representative Terms with L2 as
Single Assisting Language (With 
Meaning)
Representative Terms with L1& L2 as 
Assisting Langs. (With Meaning)
English ?03 
TOPIC 165 Globes 1994
Golden Globes 1994 (FR)
Globos de Oro 1994 (ES) 0.33 0.5 1
Gold, prize, oscar, nomin, best award, 
hollywood , actor, director ,actress, world, 
won ,list, winner, televi , foreign ,year, press 
world, nomin, film, award, delici, planet, 
earth, actress, list, drama, director, actor, 
spielberg, music, movie, forrest, hank 
oscar, nomin, best, award, hollywood actor, 
director, cinema, televi , music, actress, 
drama, role, hank, foreign, gold
Finnish '03
TOPIC 152
Lasten oikeudet
(Children?s
Rights)
Rechte des Kindes (DE)
Kinderrechten (NL) 0.2 0.25 0.37
laps (child), oikeuks (rights), oikeud (rights),
kind, oikeus (right), is? (father), oikeut
(justify ), vanhemp (parent), vanhem
(parents), las, gram, yhteis , unicef, sunt,
? iti(mother), yleissopimnks (conventions)
oikeuks (rights), laps (child), oikeud (right),
mandy , richard, slovakia , t?h?nast (to date),
tuomar (judge), tyto , kid, , nuor (young
people), nuort (young ), sano(said) , 
perustam(establishing)
laps (child), oikeuks (rights), oikeud (rights),
oikeus (right), is? (father, parent), vanhemp
(parent), vanhem (parents), oikeut (justify),
las, mandy , nuort (young ), richard, nuor
(young people), slovakia , t?h?nast (to date),
English ?03
TOPIC 148
Damages in 
Ozone Layer
Dommages ? la couche 
d'ozone (FR)
Destrucci?n de la capa de 
ozono (ES)
0.08 0.07 0.2 damag, militri, uv , layer, condition, chemic, bacteria, ban, radiat, ultraviolet
damag, weather, atmospher, earth, problem, 
report, research, harm, iraq , war, scandal, 
illigel, latin, hair
damag, uv , layer,weather , atmospher, earth, 
problem, report, research , utraviolet , chemic
German '03
TOPIC 180
Konkurs der
Baring -Bank
(Bankruptcy of 
Baring Bank )
Bankruptcy of Barings (EN)
Baringsin
Konkurssi (FI) 0.55 0.51 0.33
zentralbank(central bank),bankrott(bank 
cruptcy), investitionsbank, sigapur, london , 
britisch, index, tokio, england, 
werbung(advertising), japan
fall, konkurs, bankrott(Bankruptcy), 
warnsignal(warning), ignoriert, 
zusammenbruch (collepse), london, singapur, 
britisch(british), dollar, tokio, druck(pressur), 
handel(trade) 
aktienmarkt(share market), investitionsbank , 
bankrott, zentralbank (central bank), federal, 
singapur, london, britisch, index, tokio, dollar, 
druck, england, dokument(document)
German '03
TOPIC 198
Ehren-Oscar f?r
italienische
Regisseure
(Honorary Oscar 
for Italian 
directors)
Honorary Oscar for Italian 
Directors (EN)
Kunnia -Oscar italialaisille
elokuvaohjaajille (FI)
0.5 0.35 0.2
Direktor(director), film, regierungschef(prime) 
, best antonionis, antonionins, lieb, 
geschicht(history) , paris, preis, berlin, 
monitor, kamera
Generaldirektion(General director), film, 
ehrenmitglied, regisseur, direktor, verleih , 
itali, oscar, award, antonionins
generaldirektion(head office), 
ehrenmitglied(honorable member), 
regierungschef(prime), regisseur(director 
),oscar, genossenschaftsbank (corporate 
bank)
Table 5: Qualitative Comparison of MultiPRF Results using two assisting languages with single assisting language.
6.2 Effect of Coverage on MultiPRF
Accuracy
A study of the results obtained for MultiPRF using
single assisting language and multiple assisting
languages with different source languages showed
that certain languages are more suited to be ben-
efited by assisting languages. In particular, lan-
guages having smaller collections are more likely
to be benefited if assisted by a language having a
larger collection size. For example, Finnish which
has the smallest collection (55344 documents)
showed maximum improvement when supported
by assisting language(s). Based on this observa-
tion, we plotted a graph of the collection size of a
source language v/s the average improvement ob-
tained by using two assisting languages to see if
their exists a correlation between these two fac-
tors. As shown in Figure 3, there indeed exists a
high correlation between these two entities. At
one extreme, we have a language like Spanish
which has the largest collection (454045 docu-
ments) and is not benefited much by assisting lan-
guages. On the other extreme, we have Finnish
which has the smallest collection size and is ben-
efited most by assisting languages.
454.045 (Spanish)
294.809 (German)
190.604 (Dutch) 169.477 (English)
129.806 (French)
55.344 (Finnish)
0
50
100
150
200
250
300
350
400
450
500
0 1 2 3 4 5 6 7
Coverage(No.of Docs in Thousands)
Avg. Improvement in MAP of MultiPRF using two Assisting Languages (%)
Figure 3: Effect of Coverage on Average MultiPRF MAP
using Two Assisting Languages.
6.3 Effect of Number of Assisting Languages
on MultiPRF Accuracy
Another interesting question which needs to be
addressed is ?Whether it helps to use more than
two assisting languages?? and if so ?Is there an
optimum number of assisting languages beyond
which there will be no improvement??. To an-
swer these questions, we performed experiments
using 1-4 assisting languages with each source
language. As seen in Figure 4, in general as the
number of assisting languages increases the per-
formance saturates (typically after 3 languages).
Thus, for 5 out of the 6 source languages, the per-
formance saturates after 3 languages which is in
line with what we would intuitively expect. How-
ever, in the case of German, on an average, the
76
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0. 49
0 2 4 6
MAP
No. of. Assisting Langs.
English
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0 2 4 6
MAP
No. of Assisting Langs.
French
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0 2 4 6
MAP
No. of Assisting Langs.
Finnish
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0 2 4 6
MAP
No. of Assisting Langs.
German
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0 2 4 6
MAP
No. of Assisting Langs.
Dutch
Avg. MAP
MBF
0. 35
0. 37
0. 39
0. 41
0. 43
0. 45
0. 47
0. 49
0 2 4 6
MAP
No. of Assisting Langs.
Spanish
Avg. MAP
MBF
Figure 4: Effect of Number of Assisting Languages on Avg. MultiPRF Performance with Multiple Assistance.
0
0. 05
0. 1
0. 15
0. 2
0. 25
0. 3
0. 35
0. 4
English French German Spanish Dutch Finnish
Avg
. G
MA
P
Source Language
MBF
1
2
3
4
Figure 5: Effect of Number of Assisting Languages on Ro-
bustness measured through GMAP.
performance drops as the number of assisting lan-
guages is increased. This drop is counter intuitive
and needs further investigation.
6.4 Effect of Number of Assisting Languages
on Robustness
One of the primary motivations for including mul-
tiple assisting languages in MultiPRF was to in-
crease the robustness of retrieval through better
coverage. We varied the number of assisting lan-
guages for each source and studied the average
GMAP. The results are shown in Figure 5. We
observe that in almost all the source languages,
the GMAP value increases with number of assist-
ing languages and then reaches a saturation after
reaching three languages.
7 Conclusion
In this paper, we extended the MultiPRF frame-
work to multiple assisting languages. We pre-
sented three different configurations for including
multiple assisting languages - a) Parallel b) Serial
and c) Selective. We observe that the results are
mixed with parallel and selective assistance show-
ing improvements in some cases. We also observe
that the robustness of MultiPRF increases with
number of assisting languages. We analyzed the
influence of coverage of MultiPRF accuracy and
observed that it is inversely correlated. Finally,
increasing the number of assisting languages in-
creases the MultiPRF accuracy to some extent and
then it saturates beyond that limit. Many of the
above results (negative results of serial, selective
configurations etc.) require deeper investigation
which we plan to take up in future.
References
Braschler, Martin and Carol Peters. 2004. Cross-
language evaluation forum: Objectives, results,
achievements. Inf. Retr., 7(1-2):7?31.
Buckley, Chris, Gerald Salton, James Allan, and Amit
Singhal. 1994. Automatic query expansion using
smart : Trec 3. In Proceedings of The Third Text
REtrieval Conference (TREC-3, pages 69?80.
Chinnakotla, Manoj K., Karthik Raman, and Push-
pak Bhattacharyya. 2010a. Multilingual pseudo-
77
relevance feedback: English lends a helping hand.
In ACM SIGIR 2010, Geneva, Switzerland, July.
ACM.
Chinnakotla, Manoj K., Karthik Raman, and Push-
pak Bhattacharyya. 2010b. Multilingual pseudo-
relevance feedback: Performance study of assisting
languages. In ACL 2010, Uppsala, Sweeden, July.
ACL.
Dempster, A., N. Laird, and D. Rubin. 1977. Maxi-
mum Likelihood from Incomplete Data via the EM
Algorithm. Journal of the Royal Statistical Society,
39:1?38.
Hawking, David, Paul Thistlewaite, and Donna Har-
man. 1999. Scaling up the trec collection. Inf. Retr.,
1(1-2):115?137.
John Lafferty and Chengxiang Zhai. 2003. Proba-
bilistic Relevance Models Based on Document and
Query Generation. In Language Modeling for Infor-
mation Retrieval, volume 13, pages 1?10. Kluwer
International Series on IR.
Mitra, Mandar, Amit Singhal, and Chris Buckley.
1998. Improving automatic query expansion. In
SIGIR ?98: Proceedings of the 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 206?214,
New York, NY, USA. ACM.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Ounis, I., G. Amati, Plachouras V., B. He, C. Macdon-
ald, and Johnson. 2005. Terrier Information Re-
trieval Platform. In Proceedings of the 27th Euro-
pean Conference on IR Research (ECIR 2005), vol-
ume 3408 of Lecture Notes in Computer Science,
pages 517?519. Springer.
Philipp, Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Robertson, Stephen. 2006. On gmap: and other trans-
formations. In CIKM ?06: Proceedings of the 15th
ACM international conference on Information and
knowledge management, pages 78?83, New York,
NY, USA. ACM.
Voorhees, Ellen. 2006. Overview of the trec 2005
robust retrieval track. In E. M. Voorhees and L.
P. Buckland, editors, The Fourteenth Text REtrieval
Conference, TREC 2005, Gaithersburg, MD. NIST.
Wu, Dan, Daqing He, Heng Ji, and Ralph Grishman.
2008. A study of using an out-of-box commercial
mt system for query translation in clir. In iNEWS
?08: Proceeding of the 2nd ACM workshop on Im-
proving non english web searching, pages 71?76,
New York, NY, USA. ACM.
Xu, Jinxi and W. Bruce Croft. 2000. Improving the ef-
fectiveness of information retrieval with local con-
text analysis. ACM Trans. Inf. Syst., 18(1):79?112.
Zhai, Chengxiang and John Lafferty. 2001. Model-
based Feedback in the Language Modeling ap-
proach to Information Retrieval. In CIKM ?01: Pro-
ceedings of the tenth international conference on In-
formation and knowledge management, pages 403?
410, New York, NY, USA. ACM Press.
Zhai, Chengxiang and John Lafferty. 2004. A Study of
Smoothing Methods for Language Models applied
to Information Retrieval. ACM Transactions on In-
formation Systems, 22(2):179?214.
78
