Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 159?164,
Prague, June 2007. c?2007 Association for Computational Linguistics
Machine Learning Based Semantic Inference: Experiments and Ob-
servations at RTE-3 
Baoli Li1, Joseph Irwin1, Ernest V. Garcia2, and Ashwin Ram1 
 
1 College of Computing 
Georgia Institute of Technology 
Atlanta, GA 30332, USA 
baoli@gatech.edu 
gtg519g@mail.gatech.edu 
ashwin@cc.gatech.edu 
 
2 Department of Radiology 
School of Medicine, Emory University 
Atlanta, GA 30322, USA 
Ernest.Garcia@emoryhealthcare.org 
 
Abstract 
Textual Entailment Recognition is a se-
mantic inference task that is required in 
many natural language processing (NLP) 
applications. In this paper, we present our 
system for the third PASCAL recognizing 
textual entailment (RTE-3) challenge. The 
system is built on a machine learning 
framework with the following features de-
rived by state-of-the-art NLP techniques: 
lexical semantic similarity (LSS), named 
entities (NE), dependent content word pairs 
(DEP), average distance (DIST), negation 
(NG), task (TK), and text length (LEN). On 
the RTE-3 test dataset, our system achieves 
the accuracy of 0.64 and 0.6488 for the two 
official submissions, respectively. Experi-
mental results show that LSS and NE are 
the most effective features. Further analy-
ses indicate that a baseline dummy system 
can achieve accuracy 0.545 on the RTE-3 
test dataset, which makes RTE-3 relatively 
easier than RTE-2 and RTE-1. In addition, 
we demonstrate with examples that the cur-
rent Average Precision measure and its 
evaluation process need to be changed. 
1 Introduction 
Textual entailment is a relation between two text 
snippets in which the meaning of one snippet, 
called the hypothesis (H), can be inferred from the 
other snippet, called the text (T). Textual 
entailment recognition is the task of deciding 
whether a given T entails a given H. An example 
pair (pair id 5) from the RTE-3 development 
dataset is as follows: 
 
T: A bus collision with a truck in Uganda has resulted 
in at least 30 fatalities and has left a further 21 injured. 
H: 30 die in a bus collision in Uganda. 
 
Given such a pair, a recognizing textual entail-
ment (RTE) system should output its judgement 
about whether or not an entailment relation holds 
between them. For the above example pair, H is 
entailed by T. 
The PASCAL Recognizing Textual Entailment 
Challenge is an annual challenge on this task 
which has been held since 2005 (Dagan et al, 
2006; Bar-Haim et al 2006). As textual entailment 
recognition is thought to be a common underlying 
semantic inference task for many natural language 
processing applications, such as Information Ex-
traction (IE), Information Retrieval (IR), Question 
Answering (QA), and Document Summarization 
(SUM), the PASCAL RTE Challenge has been 
gaining more and more attention in the NLP com-
munity. In the past challenges, various approaches 
to recognizing textual entailment have been pro-
posed, from syntactic analysis to logical inference 
(Bar-Haim et al 2006). 
As a new participant, we have two goals by at-
tending the RTE-3 Challenge: first, we would like 
to explore how state-of-the-art language techniques 
help to deal with this semantic inference problem; 
second, we try to obtain a more thorough knowl-
edge of this research and its state-of-the-art. 
Inspired by the success of machine learning 
techniques in RTE-2, we employ the same strategy 
in our RTE-3 system. Several lexical, syntactical, 
and semantical language analysis techniques are 
159
explored to derive effective features for determin-
ing textual entailment relation. Then, a general 
machine learning algorithm is applied on the trans-
formed data for training and prediction. Our two 
official submissions achieve accuracy 0.64 and 
0.6488, respectively.  
In the rest of this paper we describe the detail of 
our system and analyze the results. Section 2 gives 
the overview of our system, while Section 3 dis-
cusses the various features in-depth. We present 
our experiments and discussions in Section 4, and 
conclude in Section 5. 
2 System Description 
Figure 1 gives the architecture of our RTE-3 sys-
tem, which finishes the process of both training 
and prediction in two stages. At the first stage, a T-
H pair goes through language processing and fea-
ture extraction modules, and is finally converted to 
a set of feature-values. At the second stage, a ma-
chine learning algorithm is applied to obtain an 
inference/prediction model when training or output 
its decision when predicting. 
In the language processing module, we try to 
analyze T-H pairs with the state-of-the-art NLP 
techniques, including lexical, syntactical, and se-
mantical analyses. We first split text into sentences, 
and tag the Part of Speech (POS) of each word. 
The text with POS information is then fed into 
three separate modules: a named entities recog-
nizer, a word sense disambiguation (WSD) module, 
and a dependency parser. These language analyz-
ers output their own intermediate representations 
for the feature extraction module. 
We produce seven features for each T-H pair: 
lexical semantic similarity (LSS), named entities 
(NE), dependent content word pairs (DEP), aver-
age distance (DIST), negation (NG), task (TK), 
and text length (LEN). The last two features are 
extracted from each pair itself, while others are 
based on the results of language analyzers. 
The resources that we used in our RTE-3 system 
include: 
OAK: a general English analysis tool (Sekine 
2002). It is used for sentence splitting, POS tag-
ging, and named entities recognition. 
WordNet::SenseRelate::Allwords package: a 
word sense disambiguation (WSD) module for as-
signing each content word a sense from WordNet 
(Pedersen et al, 2005). It is used in WSD module. 
 
Figure 1. System Architecture. 
 
WordNet::Similarity package: a Perl module 
that implements a variety of semantic similarity 
and relatedness measures based on WordNet (Pe-
dersen et al, 2005). This package is used for deriv-
ing LSS and DIST features in feature extraction 
module. 
C&C parser: a powerful CCG parser (Clark 
and Curran 2004). We use C&C parser to obtain 
dependent content word pairs in dependency pars-
ing module. 
WEKA: the widely used data mining software 
(Witten&Frank 2005). We have experimented with 
several machine learning algorithms implemented 
in WEKA at the second stage. 
3 Features 
In this section, we explain the seven features that 
we employ in our RTE-3 system. 
3.1 Lexical Semantic Similarity (LSS) 
Let H={HW 1, HW 2, ?, HW m} be the set of words in 
a hypothesis, and T={TW 1, TW 2, ?, TW n} the set of 
words in a text, then the lexical semantic similarity 
feature LSS for a T-H pair is calculated as the fol-
lowing equation: 
?
?
=
i
i
i
i
ii
ji
j
HWIDF
HWIDF
HWHWSSim
TWHWSSim
MAX
THLSS )(
))(*)),(
),(((
),( . (1) 
where IDF(w) return the Inverse Document Fre-
quency (IDF) value of word w, and SSim is any 
function for calculating the semantic relatedness 
between two words. We use WordNet::Similarity 
160
package to calculate the semantic similarity of two 
content words in WordNet (Fellbaum 1998). This 
package provides many different semantic related-
ness measures. In our system, we use the Lesk re-
latedness measure for function SSim, as it can be 
used to make comparisons between concepts of 
different parts of speech (POS) (Baner-
jee&Pedersen, 2002). Because the value of SSim 
may be larger than 1, we normalize the original 
value from the WordNet::Similarity package to 
guarantee it fall between 0 and 1. 
For the words out of WordNet, e.g. new proper 
nouns, we use the following strategy: if two words 
match exactly, the similarity between them is 1; 
otherwise, the similarity is 0. 
It needs to be pointed out that Equation (1) is a 
variant of the text semantic similarity proposed in 
(Mihalcea et al 2006). However, in Equation (1), 
we take into account out of vocabulary words and 
normalization for some word-to-word similarity 
metrics that may be larger than 1. 
In addition, we use an IDF dictionary from 
MEAD (Radev et al 2001; http://www.summari-
zation.com/mead/) for retrieving the IDF value for 
each word. For the words out of the IDF diction-
ary, we assign a default value 3.0. 
3.2 Named Entities (NE) 
Named Entities are important semantic information 
carriers, which convey more specific information 
than individual component words. Intuitively, we 
can assume that all named entities in a hypothesis 
would appear in a textual snippet which entails the 
hypothesis. Otherwise, it is very likely that the en-
tailment relation in a T-H pair doesn?t hold. Based 
on this assumption, we derive a NE feature for 
each T-H pair as follows: 
??
??
?
>
?
=
=
.0|)(_|,|)(_|
|)(_)(_|
,0|)(_|,                       1                 
),( HSNEif
HSNE
TSNEHSNE
HSNEif
THNE
 
Function NE_S derives the set of named entities 
from a textual snippet. When we search in T the 
counterpart of a named entity in H, we use a looser 
matching strategy: if a named entity neA in H is 
consumed by a named entity neB in T, neA and 
neB are thought to be matched. We use the English 
analysis tool OAK (Sekine 2002) to recognize 
named entities in textual snippets. 
3.3 Dependent Content Word Pairs (DEP) 
With the NE feature, we can capture some local 
dependency relations between words, but we may 
miss many dependency relations expressed in a 
long distance. These missed long distance depend-
ency relations may be helpful for determining 
whether entailment holds between H and T. So, we 
design a DEP feature as follows: 
??
??
?
>
?
=
=
.0|)(_|,|)(_|
|)(_)(_|
,0|)(_|,                       1                    
),( HSDEPif
HSDEP
TSDEPHSDEP
HSDEPif
THDEP
 
Function DEP_S derives the set of dependent 
content word pairs from a textual snippet. We re-
quire that the two content words of each pair 
should be dependent directly or linked with at most 
one function word. We use C&C parser (Clark and 
Curran 2004) to parse the dependency structure of 
a textual snippet and then derive the dependent 
content word pairs. We don?t consider the type of 
dependency relation between two linked words. 
3.4 Average Distance (DIST) 
The DIST feature measures the distance between 
unmapped tokens in the text. Adams (2006) uses a 
simple count of the number of unmapped tokens in 
the text that occur between two mapped tokens, 
scaled to the length of the hypothesis. Our system 
uses a different approach, i.e. measuring the aver-
age length of the gaps between mapped tokens. 
The number of tokens in the text between each 
consecutive pair of mapped tokens is summed up, 
and this sum is divided by the number of gaps 
(equivalent to the number of tokens ? 1). In this 
formula, consecutive mapped tokens in the text 
count as gaps of 0, so a prevalence of consecutive 
mapped tokens lowers the value for this feature. 
The purpose of this approach is to reduce the effect 
of long appositives, which may not be mapped to 
the hypothesis but should not rule out entailment. 
3.5 Negation (NG) 
The Negation feature is very simple. We simply 
count the occurrences of negative words from a list 
in both the hypothesis (nh) and the text (nt). The list 
includes some common negating affixes. Then the 
value is: 
??
??
?
=
otherwise 0,
parity  samethe have n and n if 1,T)NEG(H, th  
161
3.6 Task (TK) 
The Task feature is simply the task domain from 
which the text-hypothesis pair was drawn. The 
values are Question Answering (QA), Information 
Retrieval (IR), Information Extraction (IE), and 
Multi-Document Summarization (SUM).  
3.7 Text Length (LEN) 
The Text Length feature is drawn directly from the 
length attribute of each T-H pair. Based on the 
length of T, its value is either ?short? or ?long?. 
4 Experiments and Discussions 
We run several experiments using various datasets 
to train and test models, as well as different com-
binations of features. We also experiment with 
several different machine learning algorithms, in-
cluding support vector machine, decision tree, k-
nearest neighbor, na?ve bayes, and so on. Decision 
tree algorithm achieves the best results in all ex-
periments during development. Therefore, we 
choose to use decision tree algorithm (J48 in 
WEKA) at the machine learning stage. 
4.1 RTE-3 Datasets 
RTE-3 organizers provide two datasets, i.e. a de-
velopment set and a test set, each consisting of 800 
T-H pairs. In both sets pairs are annotated accord-
ing to the task the example was drawn from and its 
length. The length annotation is introduced in this 
year?s competition, and has a value of either 
?long? or ?short.? In addition, the development set 
is annotated as to whether each pair is in an en-
tailment relation or not. 
In order to aid our analysis, we compile some 
statistics on the datasets of RTE-3. Statistics on the 
development dataset are given in Table 1, while 
those on the test dataset appear in Table 2. 
From these two tables, we found the distribution 
of different kinds of pairs is not balanced in both 
the RTE-3 development dataset and the RTE-3 test 
dataset. 412 entailed pairs appear in the develop-
ment dataset, where 410 pairs in the test dataset are 
marked as ?YES?. Thus, the first baseline system 
that outputs all ?YES? achieves accuracy 0.5125. 
If we consider task information (IE, IR, QA, and 
SUM) and assume the two datasets have the same 
?YES? and ?NO? distribution for each task, we 
will derive the second baseline system, which can 
get accuracy 0.5450. Similarly, if we further con-
sider length information (short and long) and as-
sume the two datasets have the same ?YES? and 
?NO? distribution for each task with length infor-
mation, we will derive the third baseline system, 
which can also get accuracy 0.5450. 
Table 1. Statistical Information of the RTE-3 De-
velopment Dataset. 
Table 2. Statistical Information of the RTE-3 Test 
Dataset. 
As different kinds of pairs are evenly distributed 
in RTE-1 and RTE-2 datasets, the baseline system 
for RTE-1 and RTE-2 that assumes all ?YES? or 
all ?NO? can only achieve accuracy 0.5. The rela-
tively higher baseline performance for RTE-3 data-
sets (0.545 vs. 0.5) makes us expect that the aver-
age accuracy may be higher than those in previous 
RTE Challenges. 
Another observation is that the numbers of long 
pairs in both datasets are very limited. Only 
NO 11 1.38% IE 
YES 17 2.13% 
NO 22 2.75% IR 
YES 21 2.63% 
NO 20 2.50% QA 
YES 27 3.38% 
NO 4 0.50% 
Long 
(135) 
SUM 
YES 13 1.63% 
NO 80 10.00% IE 
YES 92 11.50% 
NO 89 11.13% IR 
YES 68 8.50% 
NO 73 9.13% QA 
YES 80 10.00% 
NO 89 11.13% 
Short 
(665) 
SUM 
YES 94 11.75% 
NO 11 1.38% IE 
YES 8 1.00% 
NO 31 3.88% IR 
YES 23 2.88% 
NO 13 1.63% QA 
YES 22 2.75% 
NO 4 0.50% 
Long 
(117) 
SUM 
YES 5 0.63% 
NO 84 10.50% IE 
YES 97 12.13% 
NO 82 10.25% IR 
YES 64 8.00% 
NO 81 10.13% QA 
YES 84 10.50% 
NO 84 10.50% 
Short 
(683) 
SUM 
YES 107 13.38% 
162
16.88% and 14.63% pairs are long in the develop-
ment dataset and the test dataset respectively. 
4.2 Evaluation Measures 
Systems are evaluated by simple accuracy as in 
Equation (2); that is, the number of pairs (C) clas-
sified correctly over the total number of pairs (N). 
This score can be further broken down according 
to task.  
N
CAccuracy = .                                      (2) 
There is another scoring available for ranked re-
sults, Average Precision, which aims to evaluate 
the ability of systems to rank all the T-H pairs in 
the test set according to their entailment confi-
dence (in decreasing order from the most certain 
entailment to the least certain). It is calculated as in 
Equation (3).  
?=
=
N
i i
iNepiE
R
AvgP
1
)(*)(1
.                        (3) 
Where R is the total number of positive pairs in 
the test set, E(i) is 1 if the i-th pair is positive and 0 
otherwise, and Nep(i) returns the number of posi-
tive pairs in the top i pairs. 
Table 3. Our Official RTE-3 Run Results. 
4.3 Official RTE-3 Results 
The official results for our system are shown in 
Table 3. For our first run, the model was trained on 
all the datasets from the two previous challenges as 
well as the RTE-3 development set, using only the 
LSS, NE, and TK features. This feature combina-
tion achieves the best performance on the RTE-3 
development dataset in our experiments. For the 
second run, the model was trained only on the 
RTE-3 development dataset, but adding other two 
features LEN and DIST. We hope these two fea-
tures may be helpful for differentiating pairs with 
different length. 
RUN2 with five features achieves better results 
than RUN1. It performs better on IE, QA and SUM 
tasks than RUN1, but poorer on IR task. Both runs 
obtain the best performance on QA task, and per-
form very poor on IE task. For the IE task itself, a 
baseline system can get accuracy 0.525. RUN1 
cannot beat this baseline system on IE task, while 
RUN2 only has a trivial advantage over it. In fur-
ther analysis on the detailed results, we found that 
our system tends to label all IE pairs as entailed 
ones, because most of the IE pairs exhibit higher 
lexical overlapping between T and H. In our opin-
ion, word order and long syntactic structures may 
be helpful for dealing with IE pairs. We will ex-
plore this idea and other methods to improve RTE 
systems on IE pairs in our future research. 
Table 4. Accuracy by task and selected feature set 
on the RTE-3 Test dataset (Trained on the RTE-3 
development dataset). 
4.4 Discussions 
4.4.1 Feature Analysis 
Table 4 lays out the results of using various feature 
combinations to train the classifier. All of the 
models were trained on the RTE 3 development 
dataset only. 
It is obvious that the LSS and NE features have 
the most utility. The DIST and LEN features seem 
useless for this dataset, as these features them-
selves can not beat the baseline system with accu-
racy 0.545. Systems with individual features per-
form similarly on SUM pairs except NG, and on IE 
pairs except NG and DEP features. However, on 
IR and QA pairs, they behave quite differently. For 
example, system with NE feature achieves accu-
racy 0.78 on QA pairs, while system with DEP 
feature obtains 0.575. NE and LSS features have 
similar effects, but NE is more useful for QA pairs. 
Accuracy by Task 
RUN Overall Accuracy IE IR QA SUM 
1 0.6400 0.5100 0.6600 0.7950 0.5950 
2 0.6488 0.5300 0.6350 0.8050 0.6250 
Accuracy by Task 
Feature Set 
IE IR QA SUM 
Acc. 
LSS 0.530 0.660 0.720 0.595 0.6263 
NE 0.520 0.620 0.780 0.580 0.6250 
         DEP 0.495 0.625 0.575 0.570 0.5663 
          TK 0.525 0.565 0.530 0.560 0.5450 
                    DIST 0.525 0.435 0.530 0.560 0.5125 
          NG 0.555 0.505 0.590 0.535 0.5463 
                    LEN 0.525 0.435 0.530 0.560 0.5125 
LSS+NE 0.525 0.645 0.805 0.585 0.6400 
LSS+NE+DEP 0.520 0.650 0.810 0.580 0.6400 
LSS+NE+TK 0.530 0.625 0.805 0.595 0.6388 
LSS+NE+TK+LEN 0.530 0.630 0.805 0.625 0.6475 
LSS+NE+TK+DEP 0.530 0.625 0.805 0.620 0.6450 
LSS+NE+TK+DEP+NG 0.460 0.625 0.785 0.655 0.6313 
LSS+NE+TK+LEN+DEP 0.525 0.615 0.790  0.600 0.6325 
LSS+NE+TK+LEN+DIST 
(run2) 0.530 0.635 0.805 0.625 0.6488 
All Features 0.500 0.590 0.790 0.630 0.6275 
163
It is interesting to note that some features im-
prove the score in some combinations, but in oth-
ers they decrease it. For instance, although DEP 
scores above the baseline at 0.5663, when added to 
the combination of LSS, NE, TK, and LEN it low-
ers the overall accuracy by 1.5%. 
4.4.2 About Average Precision Measure 
As we mentioned in section 4.2, Average Precision 
(AvgP) is expected to evaluate the ranking ability 
of a system according to confidence values. How-
ever, we found that the current evaluation process 
and the measure itself have some problems and 
need to be modified for RTE evaluation. 
On one hand, the current evaluation process 
doesn?t consider tied cases where many pairs may 
have the same confidence value. It is reasonable to 
assume that the order of tied pairs will be random. 
Accordingly, the derived Average Precision will 
vary. 
Let?s look at a simple example: suppose we 
have two pairs c and d, and c is the only one posi-
tive entailment pair. Here, R=1, N=2 for Equation 
(3). Two systems X and Y output ranked results as 
{c, d} and {d,c} respectively. According to Equa-
tion (3), the AvgP value of system X is 1, where 
that of system Y is 0.5. If these two systems assign 
same confidence value for both pairs, we can not 
conclude that system X is better than system Y. 
To avoid this problem, we suggest requiring that 
each system for ranked submission output its con-
fidence for each pair. Then, when calculating Av-
erage Precision measure, we first re-rank the list 
with these confidence values and true answers for 
each pair. For tied pairs, we rank pairs with true 
answer ?NO? before those with positive entailment 
relation. By this way, we can produce a stable and 
more reasonable Average Precision value. For ex-
ample, in the above example, the modified average 
precisions for both systems will be 0.5. 
On the other hand, from the Equation (3), we 
know that the upper bound of Average Precision is 
1. At the same time, we can also derive a lower 
bound for this measure as in Equation (4). It corre-
sponds to the worst system which places all the 
negative pairs before all the positive pairs. The 
lower bound of Average Precision for RTE-3 test 
dataset is 0.3172. 
?
?
?
=
?
=
1
0
1
_
R
j jN
jR
R
AvgPLB .                       (4) 
As the values of N and R change, the lower 
bound of Average Precision will vary. Therefore, 
the original Average Precision measure as in Equa-
tion (3) is not an ideal one for comparison across 
datasets. 
To solve this problem, we propose a normalized 
Average Precision measure as in Equation (5). 
AvgPLB
AvgPLBAvgPAvgPNorm
_1
_
_
?
?
= .            (5) 
5 Conclusion and Future Work 
In this paper, we report our RTE-3 system. The 
system was built on a machine learning framework 
with features produced by state-of-the-art NLP 
techniques. Lexical semantic similarity and Named 
entities are the two most effective features. Data 
analysis shows a higher baseline performance for 
RTE-3 than RTE-1 and RTE-2, and the current 
Average Precision measure needs to be changed. 
As T-H pairs from IE task are the most difficult 
ones, we will focus on these pairs in our future re-
search. 
References 
Rod Adams. 2006. Textual Entailment Through Extended Lexical 
Overlap. In Proceedings of RTE-2 Workshop. 
Satanjeev Banerjee and Ted Pedersen. 2002. An Adapted Lesk Algo-
rithm for Word Sense Disambiguation Using WordNet. In Pro-
ceedings of CICLING-02. 
Roy Bar-Haim et al 2006. The Second PASCAL Recognising Textual 
Entailment Challenge. In Proceedings of RTE-2 Workshop. 
Stephen Clark and James R. Curran. 2004. Parsing the WSJ using 
CCG and Log-Linear Models. In Proceedings of ACL-04. 
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PAS-
CAL Recognising Textual Entailment Challenge. In Qui?onero-
Candela et al (editors.), MLCW 2005, LNAI Volume 3944. 
Christiane Fellbaum. 1998. WordNet: an Electronic Lexical Database. 
MIT Press. 
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Cor-
pus-based and Knowledge-based Measures of Text Semantic Simi-
larity. In Proceedings of AAAI-06. 
Ted Pedersen et al 2005. Maximizing Semantic Relatedness to Per-
form Word Sense Disambiguation. Research Report UMSI 
2005/25, Supercomputing Institute, University of Minnesota. 
Dragomir Radev, Sasha Blair-Goldensohn, and ZhuZhang. 2001. 
Experiments in single and multidocument summarization using 
MEAD. In Proceedings of DUC 2001. 
Satoshi Sekine. 2002. Manual of Oak System (version 0.1). Computer 
Science Department, New York University, 
http://nlp.cs.nyu.edu/oak. 
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine 
learning tools and techniques. Morgan Kaufmann, San Francisco. 
 
164
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 86?92,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Narrative Schema as World Knowledge for Coreference Resolution
Joseph Irwin
Nara Institute of
Science and Technology
Nara Prefecture, Japan
joseph-i@is.naist.jp
Mamoru Komachi
Nara Institute of
Science and Technology
Nara Prefecture, Japan
komachi@is.naist.jp
Yuji Matsumoto
Nara Institute of
Science and Technology
Nara Prefecture, Japan
matsu@is.naist.jp
Abstract
In this paper we describe the system with
which we participated in the CoNLL-2011
Shared Task on modelling coreference. Our
system is based on a cluster-ranking model
proposed by Rahman and Ng (2009), with
novel semantic features based on recent re-
search on narrative event schema (Chambers
and Jurafsky, 2009). We demonstrate some
improvements over the baseline when using
schema information, although the effect var-
ied between the metrics used. We also explore
the impact of various features on our system?s
performance.
1 Introduction
Coreference resolution is a problem for automated
document understanding. We say two segments of
a natural-language document corefer when they re-
fer to the same real-world entity. The segments of
a document which refer to an entity are called men-
tions. In coreference resolution tasks, mentions are
usually restricted to noun phrases.
The goal of the CoNLL-2011 Shared Task (Prad-
han et al, 2011) is to model unrestricted coreference
using the OntoNotes corpus. The OntoNotes cor-
pus is annotated with several layers of syntactic and
semantic information, making it a rich resource for
investigating coreference resolution (Pradhan et al,
2007).
We participated in both the ?open? and ?closed?
tracks. The ?closed? track requires systems to only
use the provided data, while the ?open? track al-
lows use of external data. We created a baseline
system based on the cluster-ranking model proposed
by Rahman and Ng (2009). We then experimented
with adding novel semantic features derived from
co-referring predicate-argument chains. These nar-
rative schema were developed by Chambers and Ju-
rafsky (2009). They are described in more detail in
a later section.
2 Related Work
Supervised machine-learning approaches to corefer-
ence resolution have been researched for almost two
decades. Recently, the state of the art seems to be
moving away from the early mention-pair classifica-
tion model toward entity-based models. Ng (2010)
provides an excellent overview of the history and re-
cent developments within the field.
Both entity-mention and mention-pair models are
formulated as binary classification problems; how-
ever, ranking may be a more natural approach to
coreference resolution (Ng, 2010; Rahman and Ng,
2009). Rahman and Ng (2009) in particular pro-
pose the cluster-ranking model which we used in our
baseline. In another approach, Daume? and Marcu
(2005) apply their Learning as Search Optimization
framework to coreference resolution, and show good
results.
Feature selection is important for good perfor-
mance in coreference resolution. Ng (2010) dis-
cusses commonly used features, and analyses of
the contribution of various features can be found in
(Daume? and Marcu, 2005; Rahman and Ng, 2011;
Ponzetto and Strube, 2006b). Surprisingly, Rahman
and Ng (2011) demonstrated that a system using al-
most exclusively lexical features could outperform
86
systems which used more traditional sets of features.
Although string features have a large effect on
performance, it is recognized that the use of seman-
tic information is important for further improvement
(Ng, 2010; Ponzetto and Strube, 2006a; Ponzetto
and Strube, 2006b; Haghighi and Klein, 2010). The
use of predicate-argument structure has been ex-
plored by Ponzetto and Strube (2006b; 2006a).
3 Narrative Schema for Coreference
Narrative schema are extracted from large-scale cor-
pora using coreference information to identify pred-
icates whose arguments often corefer. Similarity
measures are used to build up schema consisting
of one or more event chains ? chains of typically-
coreferring predicate arguments (Chambers and Ju-
rafsky, 2009). Each chain corresponds to a role in
the schema.
A role defines a class of participants in the
schema. Conceptually, if a schema is present in a
document, than each role in the schema corresponds
to an entity in the document. An example schema is
shown with some typical participants in Figure 1. In
this paper the temporal order of events in the schema
is not considered.
prohibit
require
allow
bar
violate
subj. obj.
law, bill, rule,
amendment
company, mi-
crosoft, govern-
ment, banks
Figure 1: An example narrative schema with two roles.
Narrative schema are similar to the script con-
cept put forth by Schank and Abelson (1977). Like
scripts, narrative schema can capture complex struc-
tured information about events described in natural
language documents (Schank and Abelson, 1977;
Abelson, 1981; Chambers and Jurafsky, 2009).
We hypothesize that narrative schema can be a
good source of information for making coreference
decisions. One reason they could be useful is that
they can directly capture the fact that arguments of
certain predicates are relatively more likely to refer
to the same entity. In fact, they can capture global
information about verbs ranging over the entire doc-
ument, which we expect may lead to greater accu-
racy when combined with the incremental clustering
algorithm we employ.
Additionally, the information that two predicates
often share arguments yields semantic information
about the argument words themselves. For exam-
ple, if the subjects of the verbs eat and drink often
corefer, we may be able to infer that words which
occur in the subject position of these verbs share
some property (e.g., animacy). This last conjec-
ture is somewhat validated by Ponzetto and Strube
(2006b), who reported that including predicate-
argument pairs as features improved the perfor-
mance of a coreference resolver.
4 System Description
4.1 Overview
We built a coreference resolution system based on
the cluster-ranking algorithm proposed by Rahman
and Ng (2009). During document processing main-
tains a list of clusters of coreferring mentions which
are created iteratively. Our system uses a determin-
istic mention-detection algorithm that extracts can-
didate NPs from a document. We process the men-
tions in order of appearance in the document. For
each mention a ranking query is created, with fea-
tures generated from the clusters created so far. In
each query we include a null-cluster instance, to al-
low joint learning of discourse-new detection, fol-
lowing (Rahman and Ng, 2009).
For training, each mention is assigned to its cor-
rect cluster according to the coreference annota-
tion. The resulting queries are used to train a
classification-based ranker.
In testing, the ranking model thus learned is used
to rank the clusters in each query as it is created;
the active mention is assigned to the cluster with the
highest rank.
A data-flow diagram for our system is shown in
Figure 2.
87
Document
Mention
Extraction
Feature
Extraction
Entities
Narrative
Schema
Database
Cluster
Ranking
Figure 2: System execution flow
4.2 Cluster-ranking Model
Our baseline system uses a cluster-ranking model
proposed by Rahman and Ng (2009; 2011). In this
model, clusters are iteratively constructed after con-
sidering each active mention in a document in order.
During training, features are created between the ac-
tive mention and each cluster created so far. A rank
is assigned such that the cluster which is coreferent
to the active mention has the highest value, and each
non-coreferent cluster is assigned the same, lower
rank (The exact values are irrelevant to learning a
ranking; for the experiments in this paper we used
the values 2 and 1). In this way it is possible to
learn to preferentially rank correct clustering deci-
sions higher.
For classification, instances are constructed ex-
actly the same way as for training, except that for
each active mention, a query must be constructed
and ranked by the classifier in order to proceed with
the clustering. After the query for each active men-
tion has been ranked, the mention is assigned to the
cluster with the highest ranking, and the algorithm
proceeds to the next mention.
4.3 Notation
In the following sections, mk is the active mention
currently being considered, mj is a candidate an-
tecedent mention, and cj is the cluster to which it
belongs. Most of the features used in our system ac-
tually apply to a pair of mentions (i.e., mk and mj)
or to a single mention (either mk or mj). To cre-
ate a training or test instance using mk and cj , the
features which apply to mj are converted to cluster-
level features by a procedure described in 4.6.
4.4 Joint Anaphoric Mention Detection
We follow Rahman and Ng (2009) in jointly learn-
ing to detect anaphoric mentions along with resolv-
ing coreference relations. For each active mention
mk, an instance for a ?null? cluster is also created,
with rank 2 if the mention is not coreferent with
any preceding mention, or rank 1 if it has an an-
tecedent. This allows the ranker the option of mak-
ing mk discourse-new. To create this instance, only
the features which involve just mk are used.
4.5 Features
The features used in our system are shown in Table
1. For the NE features we directly use the types from
the OntoNotes annotation. 1
4.6 Making Cluster-Level Features
Each feature which applies to mj must be converted
to a cluster-level feature. We follow the proce-
dure described in (Rahman and Ng, 2009). This
procedure uses binary features whose values corre-
spond to being logically true or false. Multi-valued
features are first converted into equivalent sets of
binary-valued features. For each binary-valued fea-
ture, four corresponding cluster-level features are
created, whose values are determined by four logical
1The set of types is: PERSON, NORP, FACILITY, ORGA-
NIZATION, GPE, LOCATION, PRODUCT, EVENT, WORK,
LAW, LANGUAGE, DATE, TIME, PERCENT, MONEY,
QUANTITY, ORDINAL, CARDINAL
88
Features involving mj only
SUBJECT Y if mj is the grammatical subject of a verb; N otherwise
*NE_TYPE1 the NE label for mj if there is one else NONE
Features involving mk only
DEFINITE Y if the first word of mk is the; N otherwise
DEMONSTRATIVE Y if the first word of mk is one of this, that, these, or those; N otherwise
DEF_DEM_NA Y if neither DEFINITE nor DEMONSTRATIVE is Y; N otherwise
PRONOUN2 Y if mk is a personal pronoun; N otherwise
PROTYPE2 nominative case of mk if mk is a pronoun or NA if it is not (e.g., HE if mk is him)
NE_TYPE2 the NE label for mk if there is one
Features involving both mj and mk
DISTANCE how many sentences separate mj and mk; the values are A) same sentence, B) previous sentence,
and C) two sentences ago or more
HEAD_MATCH Y if the head words are the same; N otherwise
PRONOUN_MATCH if either of mj and mk is not a pronoun, NA; if the nominative case of mj and mk is the same, C; I
otherwise
*NE_TYPE? the concatenation of the NE labels of mj and mk (if either or both are not labelled NEs, the feature
is created using NONE as the corresponding label)
SCHEMA_PAIR_MATCH Y if mj and mk appear in the same role in a schema, and N if they do not
Features involving cj and mk
SCHEMA_CLUSTER_MATCH a cluster-level feature between mk and cj (details in Section 4.7)
Table 1: Features implemented in our coreference resolver. Binary-valued features have values of YES or NO. Multi-
valued features are converted into equivalent sets of binary-valued features before being used to create the cluster-level
features used by the ranker.
predicates: NONE, MOST-FALSE, MOST-TRUE,
and ALL.
To be precise, a feature F may be thought of as a
function taking mj as a parameter, e.g., F (mj). To
simplify notation, features which apply to the pair
mj ,mk take mk as an implicit parameter. The log-
ical predicates then compare the two counts n =
|{mj | F (mj) = true}| and C = |cj |. The re-
sulting features are shown in Table 2.
NONE F TRUE iff n = 0
MOST-FALSE F TRUE iff n < C2
MOST-TRUE F TRUE iff C2 ? n < C
ALL F TRUE iff n = C
Table 2: Cluster-level features created from binary-
valued feature F
The two features marked with * are treated
differently. For each value of NE_TYPE1 and
NE_TYPE?, a new cluster-level feature is cre-
ated whose value is the number of times that fea-
ture/value appeared in the cluster (i.e., if there were
two PERSON NEs in a cluster then the feature
NE_TYPE1_PERSON would have the value 2).
4.7 SCHEMA_CLUSTER_MATCH
The SCHEMA_CLUSTER_MATCH feature is ac-
tually three features, which are calculated over an
entire candidate antecedent cluster cj . First a list is
created of all of the schema roles which the men-
tions in cj participate in, and sorted in decreasing
order according to how many mentions in cj par-
ticipate in each. Then, the value of the feature
SCHEMA_CLUSTER_MATCHn is Y if mention
mk also participates in the nth schema role in the
list, for n = 1, 2, 3. If it does not, or if the corre-
sponding nth schema role has fewer than two partic-
ipants in cj , the value of this feature is N.
4.8 Implementation Details
Our system was implemented in Python, in order to
make use of the NLTK library2. For the ranker we
used SVMrank, an efficient implementation for train-
ing ranking SVMs (Joachims, 2006) 3.
2http://www.nltk.org/
3http://svmlight.joachims.org/
89
R P F1
MUC 12.45% 50.60% 19.98
CLOSED B3 35.07% 89.90% 50.46
CEAF 45.84% 17.38% 25.21
Overall score: 31.88
MUC 18.56% 51.01% 27.21
OPEN B3 38.97% 85.57% 53.55
CEAF 43.33% 19.36% 26.76
Overall score: 35.84
Table 3: Official system results
5 Experiments and Results
5.1 CoNLL System Submission
We submitted two results to the CoNLL-2011
Shared Task. In the ?closed? track we submitted the
results of our baseline system without the schema
features, trained on all documents in both the train-
ing and development portions of the OntoNotes cor-
pus.
We also submitted a result in the ?open? track:
a version of our system with the schema features
added. Due to issues with the implementation of this
second version, however, we were only able to sub-
mit results from a model trained on just the WSJ por-
tion of the training dataset. For the schema features,
we used a database of narrative schema released by
Chambers and Jurafsky (2010) ? specifically the list
of schemas of size 12. 4
The official system scores for our system are
listed in Table 3. We can attribute some of the low
performance of our system to features which are too
noisy, and to having not enough features compared
to the large size of the dataset. It is likely that these
two factors adversely impact the ability of the SVM
to learn effectively. In fact, the features which we in-
troduced partially to provide more features to learn
with, the NE features, had the worst impact on per-
formance according to later analysis. Because of a
problem with our implementation, we were unable
to get an accurate idea of our system?s performance
until after the submission deadline.
4Available at http://cs.stanford.edu/people/nc/schemas/
R P F1
MUC 12.77% 57.66% 20.91
Baseline B3 35.1% 91.05% 50.67
CEAF 47.80% 17.29% 25.40
MUC 12.78% 54.84% 20.73
+SCHEMA B3 35.75% 90.39% 51.24
CEAF 46.62% 17.43% 25.38
Table 4: Schema features evaluated on the development
set. Training used the entire training dataset.
5.2 Using Narrative Schema as World
Knowledge for Coreference Resolution
We conducted an evaluation of the baseline without
schema features against a model with both schema
features added. The results are shown in Table 4.
The results were mixed, with B3 going up and
MUC and CEAF falling slightly. Cross-validation
using just the development set showed a more posi-
tive picture, however, with both MUC and B3 scores
increasing more than 1 point (p = 0.06 and p <
0.01, respectively), and CEAF increasing about 0.5
points as well (although this was not significant at
p > 0.1). 5
One problem with the schema features that we
had anticipated was that they may have a problem
with sparseness. We had originally intended to ex-
tract schema using the coreference annotation in
OntoNotes, predicting that this would help alleviate
the problem; however, due to time constraints we
were unable to complete this effort.
5.3 Feature Analysis
We conducted a feature ablation analysis on our
baseline system to better understand the contribu-
tion of each feature to overall performance. The
results are shown in Table 5. We removed fea-
tures in blocks of related features; -HEAD removes
HEAD MATCH; -DIST removes the DISTANCE
feature; -SUBJ is the baseline system without SUB-
JECT; -PRO is the baseline system without PRO-
NOUN2, PROTYPE2, and PRONOUN MATCH;
-DEF DEM removes DEFINITE, DEMONSTRA-
TIVE, and DEF DEM NA; and -NE removes the
named entity features.
5All significance tests were performed with a two-tailed t-
test.
90
MUC 12.77% 57.66% 20.91
Baseline B3 35.1% 91.05% 50.67
CEAF 47.80% 17.29% 25.40
R P F1 ?F1
MUC 0.00% 33.33% 0.01 -20.90
-HEAD B3 26.27% 99.98% 41.61 -9.06
CEAF 52.88% 13.89% 22.00 -3.40
MUC 0.39% 60.86% 0.79 -20.12
-DIST B3 26.59% 99.72% 41.99 -8.68
CEAF 52.76% 13.99% 22.11 -3.29
MUC 12.47% 47.69% 19.78 -1.13
-SUBJ B3 36.54% 87.80% 51.61 0.94
CEAF 43.75% 17.22% 24.72 -0.68
MUC 18.36% 55.98% 27.65 6.74
-PRO B3 37.45% 85.78% 52.14 1.47
CEAF 47.86% 19.19% 27.40 2.00
MUC 18.90% 51.72% 27.68 6.77
-DEF_DEM B3 41.65% 86.11% 56.14 5.47
CEAF 46.39% 21.61% 29.48 4.08
MUC 22.76% 49.5% 31.18 10.27
-NE B3 46.78% 84.92% 60.33 9.66
CEAF 45.65% 25.19% 32.46 7.06
Table 5: Effect of each feature on performance.
The fact that for three of the features, removing
the feature actually improved performance is trou-
bling. Possibly these features were too noisy; we
need to improve the baseline features for future ex-
periments.
6 Conclusions
Semantic information is necessary for many tasks in
natural language processing. Most often this infor-
mation is used in the form of relationships between
words ? for example, how semantically similar two
words are, or which nouns are the objects of a verb.
However, it is likely that humans make use of much
higher-level information than the similarity between
two concepts when processing language (Abelson,
1981). We attempted to take advantage of recent de-
velopments in automatically aquiring just this sort
of information, and demonstrated the possibility of
making use of it in NLP tasks such as coreference.
However, we need to improve both the implementa-
tion and data for this approach to be practical.
For future work, we intend to investigate avenues
for improving the aquisition and use of the narra-
tive schema information, and also compare narra-
tive schema with other types of semantic informa-
tion in coreference resolution. Because coreference
information is central to the extraction of narrative
schema, the joint learning of coreference resolution
and narrative schema is another area we would like
to explore.
References
Robert P. Abelson. 1981. Psychological status of the
script concept. American Psychologist, 36(7):715?
729.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised Learning of Narrative Schemas and their Partic-
ipants. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 602?610, Suntec, Singapore.
Nathanael Chambers and Dan Jurafsky. 2010. A
database of narrative schemas. In Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC 2010), Malta.
Hal Daume? and Daniel Marcu. 2005. A large-scale ex-
ploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing -
HLT ?05, pages 97?104, Morristown, NJ, USA.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining KDD 06, pages 217?226.
Vincent Ng. 2010. Supervised noun phrase coreference
research: the first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1396?1411.
Simone Paolo Ponzetto and Michael Strube. 2006a.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 192?
199.
Simone Paolo Ponzetto and Michael Strube. 2006b. Se-
mantic role labeling for coreference resolution. In
Proceedings of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
91
Linguistics - EACL ?06, pages 143?146, Morristown,
NJ, USA.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In International Conference on Seman-
tic Computing (ICSC 2007), pages 446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011), Portland, Oregon.
Altaf Rahman and Vincent Ng. 2009. Supervised Mod-
els for Coreference Resolution. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 968?977, Singapore.
Altaf Rahman and Vincent Ng. 2011. Narrowing
the Modeling Gap: A Cluster-Ranking Approach to
Coreference Resolution. Journal of Artificial Intelli-
gence Research, 40:469?521.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding: An inquiry into hu-
man knowledge structures. Lawrence Erlbaum, Ox-
ford, England.
92
