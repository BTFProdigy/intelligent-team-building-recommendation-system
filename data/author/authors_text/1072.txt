Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 38?44,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Inferring the semantics of temporal prepositions in Italian
Tommaso Caselli          Valeria Quochi
ILC-CNR
Via Moruzzi, 1 56123, Pisa, Italy 
Dip. Linguistica ?T.Bolelli?, Universit? degli Studi di Pisa
Via S.ta Maria, 36, 56100, Pisa, Italy 
tommaso.caselli,valeria.quochi@ilc.cnr.it
Abstract
In this work we report on the results of a 
preliminary corpus study of Italian on the 
semantics of temporal prepositions, which 
is part of a wider project on the automatic 
recognition of temporal relations. The cor-
pus data collected supports our hypothesis 
that each temporal preposition can be asso-
ciated with one prototypical temporal rela-
tion, and that deviations from the prototype 
can be explained as determined by the oc-
currence of different semantic patterns. The 
motivation behind this approach is to im-
prove methods for temporal annotation of 
texts for content based access to informa-
tion. The corpus study described in this pa-
per led to the development of a preliminary 
set of heuristics for automatic annotation of
temporal relations in text/discourse. 
1 Introduction
In this work we report on the preliminary results 
of a corpus study, of contemporary Italian, on tem-
poral relations that hold between a temporal ad-
junct and an event as a way to determine the se-
mantics of temporal prepositions. We claim, fol-
lowing Schilder and Habel (2001), that the seman-
tics of temporal prepositions is rel (e, t), where rel
is used to indicate the temporal relation associated 
with a certain preposition, t represents the meaning 
of the Temporal Expression (timex), and e the 
meaning of the event description involved. 
Prepositions introducing a temporal adjunct are 
explicit signals of temporal relations. The ability to 
determine temporal relations between timexes in-
troduced by prepositions and events is fundamental 
for several NLP tasks like Open-Domain Question-
Answering systems (Hartrumpf et al 2006, and 
Pustejovsky et al 2002) and for Textual Entail-
ment and Reasoning.
The corpus data collected seems to support our 
hypothesis that each temporal preposition can be 
associated with one prototypical temporal relation, 
and that deviations from the prototype can be ex-
plained as determined the occurrences of different 
semantic pattern.
The work described in this paper is part of a lar-
ger project we are conducting on temporal dis-
course processing in Italian, as proposed in Mani 
and Pustejovsky (2004).
2 Background
This section presents a brief overview of the Ti-
meML specification language (Pustejovsky et al 
2005), which has been used as the starting point for 
this work, and some theoretical issues on Italian 
prepositions. 
2.1 TimeML
The TimeML specification language (Pustejovsky 
et al 2005) offers a guideline for annotation of 
timexes, events and their relations. Like other an-
notation schemes1, TimeML keeps separated tem-
poral expressions and events, tagged, respectively, 
with TIMEX3 and EVENT. In addition, two other 
tags are used: SIGNAL and LINK.
    The EVENT tag is used to annotate events, de-
fined as something which occur or happen, and 
                                                
1 Filatova and Hovy (2001), Schilder and Habel (2001), 
Setzer (2001). 
38
states, defined as situations in which something 
holds true.
    Temporal expressions, or timexes, like day times 
(noon, the evening, 1p.m?), dates of different 
granularity (yesterday, February 2 2007, last week, 
last spring, last centuries?), durations (five hours, 
in recent years?) and sets (twice a day?), are 
annotated with the TIMEX3 tag. This tag is based 
on specifications given by Ferro et al (2001) and 
Setzer (2001). Each timex is assigned to one of the 
following types: DATE, for calendar times, TIME, 
for times of the day, even if indefinites (e.g. ?the 
evening?), DURATION, for timexes expressing a 
duration, and SET, for sets of times. Each timex is 
further assigned a value, according to the ISO 8601 
specifications (for instance, 3 anni ?3 years? is 
normalized as ?P3Y?, i.e. a ?period of 3 years?).
   Function words which explicitly signal a relation 
between two elements (timex and event, timex and 
timex, or event and event) are tagged with SIG-
NAL. 
     Finally, the LINK tag is used to specify the re-
lation between two entities. It may indicate a tem-
poral relation (TLINK), a subordinating relation 
(SLINK) or an aspectual relation (ALINK). The 
TLINK tag, which is pivotal for the present work, 
comprises 15 relations, only 13 of which are purely 
temporal. The 13 relations can be seen as derived 
from Allen?s (1984) temporal logic, and 6 of them 
are binary relations - one being the inverse of the 
other. These relations (simultaneous, in-
cludes, is_included, during, 
inv_during, begin, end, begun_by, 
ended_by, before, after) make explicit the 
temporal relation holding between two elements. 
2.2 Temporal PPs in Italian
Italian prepositions can be divided into two main 
groups: monosyllabic like a, da, in, per, tra, -and 
polysyllabic ones like fino a ?up to?, dopo ?after?,, 
prima ?before??This difference at a surface level 
reflects a difference also at a semantic level: 
monosyllabic prepositions are either semantically 
empty elements (i.e. when they are particles pre-
selected by the VP), or they bear a very abstract 
relational meaning, which gets specialized on the 
basis of the co-text; polysyllabic prepositions, on 
the other hand, have a more specific meaning of 
their own. For instance, the preposition dopo ?af-
ter? always means ?subsequently, afterwards?, dis-
regarding its co-text; which makes the identifica-
tion of the relation between the elements involved 
an easier task. In addition to this, most prepositions, 
both polysyllabic and monosyllabic, belong to dif-
ferent semantic fields, e.g. spatial, temporal, man-
ner or other. 
    For the purpose of this work, any preposition 
followed by a timex, as defined in TimeML (Sec-
tion 2.1), is considered a temporal preposition. 
Consequently, we will speak of Temporal PP for 
any sequence of the form ?preposition + timex?. 
In Italian, as in many other languages, the form 
that Temporal PPs, or temporal adjuncts, may take 
is influenced by the aspect and actionality of the 
VP. In traditional grammars, for instance, it is 
claimed that they can be introduced by in if the 
lexical aspect denotes a telic event (e.g. (1)) and by 
per if the lexical aspect denotes a process or a par-
ticular subclass of telic events, i.e. achievements 
(e.g. (2)). Moreover, these kinds of Temporal PPs  
necessarily refer to the conclusion of the process 
denoted by the events and thus are incompatible 
with the progressive aspect:
1) a. Maria ha pulito la stanza in mezz?ora.
           [Maria cleaned the room in half an hour]
       b. La pizza arriva in cinque minuti.
           [The pizza will arrive in five minutes]
2) a. Marco ha lavorato per due ore.
           [Marco has worked for two hours]
b. Marco mi prest? il libro per due giorni.
    [Marco lend me his book for two days]
    The influence of the aspect and actionality of the 
VP has an impact also in the identification of their 
meaning. In particular, in example 1) a. the prepo-
sition signals that the event of cleaning the room 
lasted for half an hour, while in the example 1) b. 
the event of arriving takes place after five minutes 
from the utterance time. In example 1), thus, the 
same Temporal PP, i.e. IN + timex,  has two dif-
ferent meanings, signalled by the relations in-
cludes and after. The different temporal rela-
tions are determined by two different semantic pat-
terns: [DURATIVE_Verb] + in + [TIMEX type: 
DURATION] for 1) a, and [TELIC_Verb] + in + 
[TIMEX type: DURATION], for 1) b.
39
3 The corpus study
In order to verify our hypothesis that the most fre-
quent temporal relations represents the prototypical 
meaning of a temporal preposition2, a corpus study 
has been conducted. It is important to note that we 
do not refer to frequency tout court, but is fre-
quency with respect to a certain semantic pattern.     
Since we want to develop a system for automatic 
annotation of temporal relations, a 5 million word
syntactically shallow parsed corpus of contempo-
rary Italian, drawn from the PAROLE corpus, has 
been used3. 
    All occurrences of a prepositional chunk with 
their left contexts has then been automatically ex-
tracted and imported into a database structure us-
ing a dedicated chunkanalyser tool 4 . This auto-
matically generated DB was then augmented with 
ontological information from the SIMPLE Ontol-
ogy, by associating the head noun of each preposi-
tional chunk to its ontological type, and has been 
queried in order to extract all instances of Tempo-
ral PPs, by restricting the nouns headed by preposi-
tions to the type ?TIME?, which is defined in SIM-
PLE as ?all nouns referring to temporal expres-
sions? (SIMPLE Deliverable 2.1: 245). 
    To identify the meaning of temporal preposi-
tions, therefore, we considered sequences of the 
form:
   Fin Vb Chunk + Prep Chunk: semtype= TIME
where Fin Vb Chunk is a shallow syntactic con-
stituent headed by a finite verb and corresponds to 
the ?anchoring? event, and Prep Chunk is the 
prepositional phrase that represents an instance of 
a timex. To get a more complete picture of the dis-
tribution of Temporal PPs in text, we extracted 
sequences from zero up to a maximum of two in-
tervening chunks, obtaining a set of about 14,000 
such sequences.
    A first observation is about the distribution of 
the Temporal PPs. As illustrated in Table 1 (below) 
Temporal PPs tend to occur immediately after the 
event they are linked to.
                                                
2 We assume and extend Haspelmath?s (forth.) proposal on the 
explanatory and predictive power of frequency of use. 
3 The corpus was parsed with the CHUNK-IT shallow parser 
(Lenci et al 2003).
4 By courtesy of Ing. E. Chiavaccini.
Sequence Distance # Occurrences
Fin_Vb  + PP (Time) 0 5859
Fin_Vb + PP (Time) 1 4592
Fin_Vb + PP (Time) 2 3677
Table 1. Occurrences of Temporal PPs with respect 
to the distance from the event.
    The data in Table 1 show that Temporal PPs 
have a behavior similar to modifiers, like adjec-
tives anchoring on the time axis of the event they 
refer to. 
3.1 Annotating Temporal Relations
To identify the semantics of temporal prepositions, 
a subcorpus of 1057 sequences of Fin Vb Chunk + 
Prep Chunks (Time) was manually annotated by 
one investigator with temporal relations in a bot-
tom-up approach. 
     The tags used for the temporal relation annota-
tion were taken from the TimeML TLINK values 
(see Section 2.1). This will restrict the set of possi-
ble relations to a finite set. To ease the task, we 
excluded the inverse relations for includes, 
during, begin, and end. In order to understand 
the role of the co-text, we also marked the types of 
timexes according to the TimeML TIMEX3 tag 
(ibid.). In this annotation experiment we did not 
consider information from the VP because it will 
be relevant to explain the deviations from the pro-
totype.  
. To facilitate the assignment of the right temporal 
relation, we have used paraphrase tests. All the 
paraphrases used have the same scheme, based on 
the formula rel (e, t), illustrated in the 3):
3) The event/state of X is R timex.
where X stands for the event identified by the Fin 
Vb Chunk, R is the set of temporal relations and 
timex is the temporal expression of the Temporal 
PP. This means that the sequence in 4):
4) [[Vfin[Sono stato sposato]  [[ PP[per quatto 
anni]]
?I have been married for four years?
can be paraphrased as 5):
5) The state of ?being married? happened 
during four years.
40
The only temporal relation that is not para-
phrased in this way is simultaneous, which cor-
responds to 6):
6) The event/state X HAPPENS(-ED) AT 
timex.   
4  Results
Among the 1057 sequences in our sub-corpus, we 
found that only 37.46% (for a total of 449 ex-
cerpts) where real of instances of Temporal PPs, 
the others being either false positives or complex 
timexes, i.e. timexes realized by a sequence of a 
NP followed by a PP introduced by ?di? (of), as in 
the following example:
7) [NP[la notte]] [PP[di Natale]
       ?the Christmas night?
     In Table 2 (below) we report the temporal 
prepositions identified in the corpus:     
Temporal Preposition # occurrences
In ?in? 91
A ?at/on? 64
Da ?from/since/for? 37
Dopo ?after? 1
Attraverso ?through? 1
Di ?of? 43
Durante ?during? 5
Entro ?by? 9
Fino a ?up to? 6
Fino da ?since? 3
Oltre ?beyond? 1
Per ?for? 50
Tra ?in? 3
Verso ?towards? 1
Table 2. Instances of temporal prepositions in the 
corpus.
     The relative low number of real Temporal PPs 
can negatively influence the analysis and the iden-
tification of the semantics of the temporal preposi-
tions. In order to verify whether the data collected 
could represent a solid and consistent baseline for 
further analysis, we analysed all instances of false 
positive timexes. With the exception of a few 
cases, which could have been easily recognized by 
means of a Timex Grammar, we found out that 
482/608 instances are represented by nouns which 
have some sort of temporal value but whose as-
signment to the semantic type ?Time? in the On-
tology do not correspond to the given definition 
(Section 3), e.g: colazione ?breakfast?, scuola
?school?, presidenza ?presidency?, and many others. 
    Therefore, we performed a new extraction of 
sequences excluding all instances of false positives. 
The new results are very different since more than 
56.03% of all prepositional chunks are Temporal 
PPs. This provides support to the fact that the se-
quences extracted from the sub-corpus, though 
small in number, can be considered as a consistent 
starting point for identifying the semantics of tem-
poral prepositions. In particular, the prepositions 
presented in Table 2 correspond to the most fre-
quent prepositions which give rise to temporal re-
lations between timexes and events. Though small, 
the 449 sequences prove to be reliable: we have 
identified a total of 320 temporal relations, as illus-
trated in Table 3:
Temporal Relation # occurrences
Includes 87
During 72
Before 11
After 11
Imm_before 1
Imm_after 2
Simultaneous 5
Beginning 52
Ending 10
No Temporal Link 60
No Assigned 9
Table 3. Kinds of Temporal Relation Identified.
5 Inferring Preposition Semantics    
The analysis we propose for each single preposi-
tion provides information on its semantics. Such 
information is obtained on the basis of the fre-
quency5 with which a given temporal relation is 
associated or coded by that preposition. We claim, 
as already stated, that temporal relations coded by 
prepositions are signals of a certain semantic pat-
tern. Different temporal relations coded by the 
same preposition signal different semantic pattern. 
According to the frequency with which a temporal 
relation, or a semantic pattern, occurs, it is consid-
ered either as the prototypical (i.e. most frequent) 
meaning or as a deviation from the norm, whose 
                                                
5 Note that what counts is relative frequencies, and not 
absolute frequencies.
41
explanation relies in the analysis of the semantic 
pattern in which it occurs. It is for this reason that a 
major role in this analysis is played by the types of 
timexes which follow the preposition. Keeping 
track of their types, according to the TimeML clas-
sification (Section 2.1), is very useful mainly for 
cases where the same temporal preposition codes
different temporal relations depending on the type 
of the timex by which it is followed. In other 
words, it is a way to assess the semantic pattern 
which has been used to code that meaning. In the 
following sections we will focus on the semantics 
of the most frequent temporal prepositions, that is 
in ?in?, a ?at, on?, per ?for?6, da ?for, since, from?. 
Cases of low frequency temporal relations are not 
analyzed here because they would require both 
more data and a separate investigation.
5.1 Prepositions per and da
These two prepositions, although they encode dif-
ferent temporal relations, are presented in a unique 
subsection due to their extremely similar coherent 
distribution across temporal relations. In particular, 
the 80% (40/50) of per identifies a DURING tem-
poral relation, and 83.78% (31/37) of da identifies 
a BEGIN temporal relation. 
    From these data, we can represent the semantics 
of per as follows:
8) ?(e,??(t,?URIN G?e,?))
and that of da as:
9) ?(e,??(t,?EGIN ?e,t))
5.2 The Preposition in
The preposition in is by far the most used temporal 
preposition. In our corpus there are 91 occurrences 
of this preposition, distributed as follows:
INCLUDES (57/91: 62.63%)
DURING (19/91: 20.87%)
AFTER  (6/91: 6.59%)
BEGIN (3/91: 3.29%)
SIMULTANEOUS (2/91: 2.19%)
No LINK (2/91: 2.19%)
END (1/91: 1.09%)
                                                
6Note that the Italian preposition ?per? corresponds only 
to a subset of uses of the English preposition ?for? as in 
the example: 
a) Suon? per un?ora [She played for an hour.]
    Following our idea that the most frequent rela-
tion represents the prototypical meaning of the 
preposition; we claim that Temporal PPs intro-
duced by in tend to code a relation of inclusion, 
semantically represented as:
10) ?(e,??(t,?NCLU DES(?,? )).
     Since this preposition is not exclusively used 
with this meaning, the data forces us to provide an 
explanation for the other relations identified, in 
particular for DURING, AFTER and BEGIN. 
     Considering the DURING relation, we analyzed 
the types of timexes governed by the preposition 
but found that type distinctions did not help. Nev-
ertheless, we observed a clearcut regularity analys-
ing the normalized values of the timexes involved: 
we found that, whenever the timexes are definite 
quantified intervals of time (e.g. 2 days, 3 years, 
half an hour) or temporally anchored instants, in 
encodes the temporal relation of DURING, thus 
deviating from the default interpretation repre-
sented in 10). 
    The relation AFTER shares with DURING the 
restriction on the normalized values of the timexes. 
However, for the AFTER relation there is a strong 
contribution from the VP, as claimed in traditional 
grammars. In such cases, it is the actionality of the 
VP that forces the interpretation of in to express 
the AFTER relation. In fact, this relation appears to 
occur only with achievement verbs, which inher-
ently focus on the telos ? or ending point (see ex-
ample 1) b Section 1). 
   Finally, the BEGIN relation can be found only 
with aspectual verbs, e.g. iniziare ?begin? or 
riprendere ?resume?. In these cases the preposition 
does not really work as a temporal preposition, but 
more as a particle selected by the verb. 
5.3 The Preposition a
The preposition a presents a non-trivial distribu-
tion, which makes it difficult to identify a proto-
typical value:
INCLUDES (20/64: 31.25%)
No LINK (19/64: 29.68%)
BEGINS (7/64: 10.93%)
ENDS (4/64: 6.25%)
SIMULTANEOUS (2/64: 3.12%)
42
     However, with NoLINK relations the preposi-
tion a does not have a temporal value, rather it is 
used to express either quantities of time (and it 
usually corresponds to ?how many times an event 
occurs or happens?) or it can be considered as a 
particle selected by the VP. Therefore, if we ex-
clude the NoLINK relations, we can consider that  
a Temporal PP introduced by a typically expresses 
a relation of inclusion. Further support to this ob-
servation can be observed in the possibility of sub-
stituting a with in, at least in the temporal domain. 
The semantics of the preposition is the following:
11) ?(e,??(t,?NCLU DES(e,?) ).
    As for the BEGINS and ENDS relations, the 
behaviour is the same as for the preposition in, i.e. 
they are activated by aspectual verbs. 
6 Conclusion and Future Work
In this preliminary study we showed that preposi-
tions heading a Temporal PP can be associated 
with one default temporal relation and that devia-
tions from the norm are due to co-textual influ-
ences. The prototypical semantics of temporal 
prepositions can be represented as in 8)-11). 
    We also showed that the normalized values of 
timexes play a major role in the identification of 
temporal preposition semantics, more than the bare 
identification of their types. Instances of deviations 
from the prototypical meaning which could not be 
explained by differences in the timexes forced us 
to analyse the VPs, thus providing useful informa-
tion for the definition of the heuristics.
    An important result of this work is the definition 
of a preliminary set of heuristics for automatic an-
notation of temporal relations in text/discourse. 
Our study also suggests a possible refinement of 
the SIMPLE Ontology aimed at its usability for 
temporal relation identification; and it can be seen 
as a starting point for the development of a Timex 
Grammar.
In the next future we intend to implement this 
set of heuristics with a machine learning algorithm 
to evaluate their reliability. All wrongly annotated 
relations could be used for the identification of the 
relevant information to determine the contribution 
of the VP. 
Some issues are still open and need further re-
search, in particular it will be necessary to investi-
gate the role of some ?complex? Temporal PPs 
(e.g. in questo momento ?in this moment?, which 
can be paraphrased as ?now?), and how to extract 
the meaning of Temporal PPs as suggested in 
Schilder (2004).
References
Allen F. James. 1984. Towards a General Theory of 
Action and Time. Artificial Intelligence, (23):123-54
Ferro Lisa, Mani Inderjeet, Sundheim Beth and Wilson 
George. 2001. TIDES Temporal Annotation Guide-
lines: Version 1.0.2. MITRE Technical Report, MTR 
01W0000041 
Filatova, Elena and Hovy, Eduard. 2001. Assigning 
Time-Stamps To Event ?Clauses. Proceedings of the 
ACL Workshop on Temporal and Spatial Information, 
Toulouse, France, 6-1 July, pages 88-95
Haspelmath, Martin. 2007 (forthcoming). Frequency vs. 
iconicity in explaining grammatical asymmetries 
(ms).
Lassen Tine. 2006. An Ontology-Based View on Prepo-
sitional Senses. Proceedings of the Third ACL-
SIGSEM Workshop on Prepositions pages 45-50.
Lenci Alessandro, Montemagni Simonetta and Vito 
Pirrelli. 2003. CHUNK-IT. An Italian Shallow Parser 
for Robust Syntactic Annotation, in Linguistica Com-
putazionale (16-17).
Mani Inderjeet and James Pustejovsky. 2004. Temporal 
Discourse Models for Narrative Structure. ACL 
Workspoh on Discourse Annotation
Hartrumpf Sven, Helbig Hermann and Rainer Osswald. 
2006. Semantic Interpretation of Prepositions for 
NLP Applications. Proceedings of the Third ACL-
SIGSEM Workshop on Prepositions, pages 29-36.
Pustejovky James, Belanger Louis, Casta?o Jos?,  Gai-
zauskas Robert, Hanks Paul, Ingria Bob, Katz Gra-
ham, Radev Dragomir, Rumshisky Anna, Sanfilippo 
Antonio, Sauri Roser, Setzer Andrea, Sundheim Beth 
and Marc Verhagen, 2002. NRRC Summer Workshop 
on Temporal and Event Recognition for QA Systems.
Pustejovsky James, Ingria Robert, Saur? Roser, Casta?o 
Jos?, Littman Jessica, Gaizauskas Robert, Setzer An-
drea, Katz Graham and Inderjeet Mani. 2005. The 
Specification Language TimeML. The Language of 
Time: A Reader, Mani Inderjeet, Pustejovsky james 
and Robert Gaizauskas (eds), OUP. 
43
Ruimy N., et al 1998. The European LE-PAROLE Pro-
ject: The Italian Syntactic Lexicon. Proceedings of 
the LREC1998, Granada, Spain.
Saint-Dizier Patrick.2006. Syntax and Semantics of 
Prepositions, (ed.), Springer, Dordrecht, The Nether-
lands.
Schilder Frank and Habel Christopher. 2001. Semantic 
Tagging Of News Messages. Processing of the ACL 
Workshop on Temporal and Spatial Information, 
Toulouse, France, 6-1 July, pages 65-72
Schilder Frank. 2004 Extracting meaning fron Temporal 
Nouns and Temporal Prepositions. ACM Transac-
tions on Asian Language Information Processing, (3) 
1:33-50
Setzer Andrea. 2001. Temporal Information in News-
wire Article: an Annotation Scheme and Corpus
Study, Ph.D. Thesis, University of Sheffield. 
SIMPLE Work Package D2.1, available at < 
http://www.ub.es/gilcub/SIMPLE/simple.html>.
Van Eynde Frank. 2006. On the prepositions which in-
troduce an adjunct of duration. Proceedings of the 
Third ACL-SIGSEM Workshop on Prepositions pages 
73-80.
44
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 414?418,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Automatic Domain Assignment for Word Sense Alignment
Tommaso Caselli
TrentoRISE / Via Sommarive, 18
38123 Povo, Italy
t.caselli@gmail.com
Carlo Strapparava
FBK / Via Sommarive, 18
38123 Povo, Italy
strappa@fbk.eu
Abstract
This paper reports on the development of a hy-
brid and simple method based on a machine
learning classifier (Naive Bayes), Word Sense
Disambiguation and rules, for the automatic
assignment of WordNet Domains to nominal
entries of a lexicographic dictionary, the Senso
Comune De Mauro Lexicon. The system ob-
tained an F1 score of 0.58, with a Precision
of 0.70. We further used the automatically as-
signed domains to filter out word sense align-
ments between MultiWordNet and Senso Co-
mune. This has led to an improvement in the
quality of the sense alignments showing the
validity of the approach for domain assign-
ment and the importance of domain informa-
tion for achieving good sense alignments.
1 Introduction and Problem Statement
Lexical knowledge, i.e. how words are used and ex-
press meaning, plays a key role in Natural Language
Processing. Lexical knowledge is available in many
different forms, ranging from unstructured terminolo-
gies (i.e. word list), to full fledged computational lexica
and ontologies (e.g. WordNet (Fellbaum, 1998)). The
process of creation of lexical resources is costly both
in terms of money and time. To overcome these lim-
its, semi-automatic approaches have been developed
(e.g. MultiWordNet (Pianta et al., 2002)) with differ-
ent levels of success. Furthermore, important informa-
tion is scattered in different resources and difficult to
use. Semantic interoperability between resources could
represent a viable solution to allow reusability and de-
velop more robust and powerful resources. Word sense
alignment (WSA) qualifies as the preliminary require-
ment for achieving this goal (Matuschek and Gurevych,
2013).
WSA aims at creating lists of pairs of senses from
two, or more, (lexical-semantic) resources which de-
note the same meaning. Different approaches to WSA
have been proposed and they all share some common
elements, namely: i.) the extensive use of sense de-
scriptions of the words (e.g. WordNet glosses); and ii.)
the extension of the basic sense descriptions with addi-
tional information such as hypernyms, synonyms and
domain or category labels.
The purpose of this work is two folded: first, we exper-
iment on the automatic assignment of domain labels to
sense descriptions, and then, evaluate the impact of this
information for improving an existing sense aligned
dataset for nouns. Previous works has demonstrated
that domain labels are a good feature for obtaining high
quality alignments of entries (Navigli, 2006; Toral et
al., 2009; Navigli and Ponzetto, 2012). The Word-
Net (WN) Domains (Magnini and Cavaglia, 2000; Ben-
tivogli et al., 2004) have been selected as reference do-
main labels. We will use as candidate lexico-semantic
resources to be aligned two Italian lexica, namely, Mul-
tiWordNet (MWN) and the Senso Comune De Mauro
Lexicon (SCDM) (Vetere et al., 2011).
The two resources differ in terms of modelization: the
former, MWN, is an Italian version of WN obtained
through the ?expand model? (Vossen, 1996) and per-
fectly aligned to Princeton WN 1.6, while the latter,
SCDM, is a machine readable dictionary obtained from
a paper-based reference lexicographic dictionary, De
Mauro GRADIT. Major issues for WSA of the lexica
concern the following aspects:
? SCMD has no structure of word senses (i.e. no
taxonomy, no synonymy relations, no distinction
between core senses and subsenses for polyse-
mous entries) unlike MWN;
? SCDM has no domain or category labels associ-
ated to senses (with the exception of specific ter-
minological entries) unlike MWN;
? the Italian section of MWN has only 2,481 glosses
in Italian over 28,517 synsets for nouns (i.e.
8.7%).
The remainder of this paper is organized as follows:
Section 2 will report on the methodology and exper-
iments implemented for the automatic assignment of
the WN Domains to the SCDM entries. Section 3 will
describe the dataset used for the evaluation of the WSA
experiments and the use of the WN Domains for filter-
ing the sense alignments. Finally, Section 4 illustrates
conclusion and future work.
2 Methodology and Experiments
The WN Domains consist of a set of 166 hierarchically
organized labels which have been associated to each
414
Classifiers P R F1 10-Fold F1
NaiveBayes
lemma
0.77 0.58 0.66 0.66
MaxEnt
lemma
0.70 0.49 0.58 0.63
NaiveBayes
wsd
0.77 0.58 0.66 0.69
MaxEnt
wsd
0.74 0.54 0.62 0.67
Table 2: Results for the Naive Bayes and Maximum Entropy binary classifiers.
synset
1
and express a subject field label (e.g. SPORT,
MEDICINE). A special label, FACTOTUM, has been
used for those synsets which can appear in almost all
subject fields.
The identification of a domain label to the nominal en-
tries in the SCDM Lexicon is based the ?One Domain
per Discourse? (ODD) hypothesis applied to the sense
descriptions. We have used a reduced set of domains
labels (45 normalized domains) following (Magnini et
al., 2001).
To assign the WN domain label to the SCDM entries,
we have developed a hybrid method: first a binary clas-
sifier is applied to the SCDM sense descriptions to dis-
criminate between two domain values, FACTOTUM
and OTHER, where the OTHER value includes all re-
maining 44 normalized domains. After this, all entries
classified with the OTHER value are analyzed by a rule
based system and associated with a specific domain la-
bel (i.e. SPORT, MEDICINE, FOOD . . . ).
2.1 Classifier and feature selection
We have developed a training set by manually align-
ing noun senses between the two lexica. The sense
alignment allows us to associate all the information of a
synset to a corresponding entry in the SCDM lexicon,
including the WN Domain label. Concerning the test
set, we have used an existing dataset of aligned noun
pairs as in (Caselli et al., 2014). We report in Table 1
the figures for the training and test sets. Multiple align-
ments with the same domain label have been excluded
from the training set.
Characteristics Training Set Test Set
# lemmas 131 46
# of aligned pairs 369 166
# of SCDM senses 747 216
# of MWN synsets 675 229
# SCDM with
WN Domain label 350 118
Table 1: Training and test sets for the classifier.
In order for the classifier to predict the binary do-
main labels (FACTOTUM and OTHER), each sense
description of the SCDM Lexicon has been repre-
sented by means of a two-dimensional feature vector
(e.g. for training data: BINARY DOMAIN LABEL
1
The full set of labels and hierarchy is available at
http://wndomains.fbk.eu/hierarchy.html
GENERIC:val SPECIFIC:val). Feature values have
been obtained through two strategies:
? lemma label: we extract all normalized domain
labels associated to each sense of each lemma in
the sense description from MWN. The value of
the feature GENERIC corresponds to the sum of
the FACTOTUM labels. The value of the fea-
ture SPECIFIC corresponds to the sum of all other
specific domain labels (e.g. MEDICINE, SPORT
etc.) after they have been collapsed into a single
value (i.e. NOT-FACTOTUM).
? word sense label: for each sense description, we
have first performed Word Sense Disambiguation
by means of an adapted version to Italian of the
UKB package
2
(Agirre et al., 2010; Agirre et al.,
2014)
3
. Only the highest ranked synset, and as-
sociated WN Domain(s), was retained as good.
Similarly to the lemma label strategy, the sum of
the domain label FACTOTUM is assigned to the
feature GENERIC, while the sum of all other do-
main labels collapsed into the single value NOT-
FACTOTUM is assigned to the feature SPECIFIC.
We experimented with two classifiers: Naive Bayes
and Maximum Entropy as implemented in the MAL-
LET package (McCallum, 2002). We illustrate the re-
sults in Table 2. The classifiers have been evaluated
with respect to standard Precision (P), Recall (R) and
F1 against the test set. Ten-fold cross validation has
been performed on the training set as well. Classifiers
trained with the first strategy will be associated with the
label lemma, while those trained with the second strat-
egy with the label wsd.
Both classifiers obtains good results with respect to
the test data in terms of Precision and Recall. The
Naive Bayes classifier outperforms the Maximum En-
tropy one in both training approaches, suggesting better
generalization capabilities even in presence of a small
training set and basic features. The role of WSD has
a positive impact, namely for the Maximum Entropy
classifier (Precision +4 points, Recall +5 points with
respect to the lemma label). Although such a positive
effect of the WSD does not emerge for the Naive Bayes
classifier with respect to the test set, we can still ob-
serve an improvement over the ten-fold cross valida-
tion (F1= 0.69 vs. F1=0.66). We finally selected the
2
Available at http://ixa2.si.ehu.es/ukb/
3
We used the WN Multilingual Central Repository as
knowledge base and the MWN entries as dictionary
415
predictions of Naive Bayes
wsd
classifier as input to the
rule-based system as it provides the highest scores.
2.2 Rules for WN Domain assignment
The rule based classifier for final WN Domain assign-
ment works as follows:
? lemmatized and word sense disambiguated lem-
mas in the sense descriptions are associated with
the corresponding WN Domains from MWN;
? frequency counts on the WN Domain labels is ap-
plied; the most frequent WN Domain is assigned
as the correct WN Domain of the nominal entry;
? in case two or more WN Domains have same fre-
quency, the following assignment strategy is ap-
plied: if the frequency scores of the WN Do-
mains is equal to 1, the value FACTOTUM is se-
lected; on the contrary, if the frequency score is
higher than 1, all WN Domain labels are retained
as good.
We report the results on final domain assignment
in Table 3. The final system, NaiveBayes+Rules, has
been compared to two baselines. Both baselines ap-
ply frequency counts over the WN Domains labels
of the lemmas of the sense descriptions for the en-
tire set of the 45 normalized domain values, including
the FACTOTUM label, as explained in Section2. The
Baseline
lemma
assigns the domain by taking into ac-
count every WN Domain associated to each lemma. On
the other hand, the Baseline
wsd
selects only the WN
Domain of sense disambiguated lemmas. WSD for the
second baseline has been performed by applying the
same method described in Section 2.1. The results of
both baselines have high values for Precision (0.58 for
Baseline
lemma
, 0.70 for Baseline
wsd
). We consider
this as a further support to the validity of the ODD hy-
pothesis which seems to hold even for text descriptions
like dictionary glosses which normally use generic lex-
ical items to illustrate word senses. It is also interesting
to notice that WSD on its own has a positive impact in
Baseline
wsd
system for the assignment of specific do-
main labels (F1=0.53).
The hybrid system performs better than both base-
lines in terms of F1 scores (F1=0.58 vs. F1=0.45 for
Baseline
lemma
vs. F1=0.53 for Baseline
wsd
). How-
ever, both the hybrid system and the Baseline
wsd
ob-
tain the same Precision. To better evaluate the per-
formance of our hybrid approach, we computed the
paired t-test. The results of the hybrid system are sta-
tistically significant with respect to the Baseline
lemma
(p < 0.05) and for Recall only when compared to the
Baseline
wsd
.
To further analyze the difference between the hybrid
system and the Baseline
wsd
, we performed an error
analysis on their outputs. We have identified that the
hybrid system is more accurate in the prediction of the
System P R F1
NaiveBayes
wsd
+Rules 0.70? 0.50?? 0.58?
Baseline
lemma
0.58 0.36 0.45
Baseline
wsd
0.70 0.43 0.53
Table 3: Results of WN Domain Assignment over the
SDCM entries. Statistical significance of the Naive-
Bayes+Rules system has been marked with a ? for the
Baseline
lemma
and with a ? for the Baseline
wsd
FACTOTUM class with respect to the baseline. In par-
ticular, the accuracy of the hybrid system on this class
is 79% while that of the baseline is only 65%. In addi-
tion to this, the hybrid system provides better results in
terms of Recall (R=0.50 vs. R=0.43). Although compa-
rable, the hybrid system provides more accurate results
with respect to the baseline.
3 Domain Filtering for WSA
This section reports on the experiments for improving
existing WSA for nouns between SDCM and MWN. In
this work we have used the same dataset and alignment
methods as in (Caselli et al., 2014), shortly described
here:
? Lexical Match: for each word w and for each
sense s in the given resources R ? {MWN,
SCDM}, we constructed a sense descriptions
d
R
(s) as a bag of words in Italian. The alignment
is based on counting the number of overlapping
tokens between the two strings, normalized by the
length of the strings;
? Cosine Similarity: we used the Personalized Page
Rank (PPR) algorithm (Agirre et al., 2010) with
WN 3.0 as knowledge base extended with the
?Princeton Annotated Gloss Corpus?. Once the
PPR vector pairs are obtained, the alignment is
obtained on the basis of the cosine score for each
pair
4
.
The dataset consists of 166 pairs of aligned senses
from MWN and SCDM for 46 nominal lemmas
(see also column ?Test set? in Table 1). Overall,
SCDM covers 53.71The main difference with respect
to (Caselli et al., 2014) is that the proposed alignments
have been additionally filtered on the basis of the output
of the WN domain system (NaiveBayes
wsd
+Rules). In
particular, for each aligned pair which was considered
as good in (Caselli et al., 2014), we have applied a fur-
ther filtering based on the WN domain system results
as follows: if two senses are aligned but do not have
the same domain, they are excluded from the WSA re-
sults, otherwise they are retained. Table 4 illustrates
4
The vectors for the SCDM entries were obtained by, first,
applying Google Translate API to get the English translations
and, then, PPR over WN 3.0.
416
System P R F1
LexicalMatch 0.76 (0.69) 0.27 (0.44) 0.40 (0.55)
Cosine noThreshold 0.27 (0.12) 0.47 (0.94) 0.35 (0.21)
Cosine > 0.1 0.77 (0.52) 0.21 (0.32) 0.33 (0.40)
Cosine > 0.2 0.87 (0.77) 0.14 (0.21) 0.24 (0.33)
LexicalMatch+Cosine > 0.1 0.73 (na) 0.40 (na) 0.51 (na)
LexicalMatch+Cosine > 0.2 0.77 (0.67) 0.37 (0.61) 0.50 (0.64)
Table 4: Results for WSA of nouns with domain filtering.
the results of the WSA approaches with domain fil-
ters. We report in brackets the results from (Caselli et
al., 2014). The filtering based on WN Domains has a
big impact on Precision and contributes to increase the
quality of the aligned senses. Although, in general, we
have a downgrading of the performance with respect to
Recall, the increase in Precision will reduce the man-
ual post-processing effort to fully aligned the two re-
sources
5
. Furthermore, it is interesting to notice that,
when merging together the results of the pre-filtered
alignments from the two alignment approaches (Lex-
icalMatch+Cosine > 0.1 and LexicalMatch+Cosine >
0.2), we still have a very high Precision (> 0.70) and an
increase in Recall (> 0.40) with respect to the results of
each approach. Finally, we want to point out that what
was reported as the best alignment results in (Caselli
et al., 2014), namely LexicalMatch+Cosine > 0.2, can
be obtained, at least for Precision, with a lower filtering
cut-off threshold on the Cosine Similarity approach (i.e
cut-off threshold at or higher than 0.1)
4 Conclusions and Future Work
This work describes a hybrid approach based on a
Naive Bayes classifier, Word Sense Disambiguation
and rules for assigning WN Domains to nominal sense
descriptions of a lexicographic dictionary, the Senso
Comune De Mauro Lexicon. The assignment of do-
main labels has been used to improve WSA results on
nouns between the Senso Comune Lexicon and Mul-
tiWordNet. The results support some observations,
namely: i.) domain filtering plays an important role
in WSA, namely as a strategy to exclude wrong align-
ments (false positives) and improve the quality of the
aligned pairs; ii.) the method we have proposed is a vi-
able approach for automatically enriching existing lex-
ical resources in a reliable way; and iii.) the ODD hy-
pothesis also apply to sense descriptions.
An advantage of our approach is its simplicity. We have
used features based on frequency counts and obtained
good results, with a Precision of 0.70 for automatic WN
Domain assignment. Nevertheless, an important role
is played by Word Sense Disambiguation. The use of
domain labels obtained from sense disambiguated lem-
mas improves both the results of the classifier and those
5
The F1 of 0.64 in (Caselli et al., 2014) is obtained with a
Precision of 0.67, suggesting that some alignments are false
positives
of the rules. The absence of statistical significance with
respect to the Baseline
wsd
is not to be considered as a
negative result. As the error analysis has showed, the
classifier mostly contributes to the identification of the
FACTOTUM value, which tends to be overestimated
even with sense disambiguated lemmas, and to Recall.
We are planning to extend this work to include do-
main clusters to improve the domain assignment re-
sults, namely in terms of Recall.
Acknowledgments
One of the author wants to thank Vrije Univerisiteit
Amsterdam for sponsoring the attendance to the
EMNLP conference.
References
Eneko Agirre, Montse Cuadros, German Rigau, and
Aitor Soroa. 2010. Exploring knowledge bases
for similarity. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC?10), Valletta, Malta,
may. European Language Resources Association
(ELRA).
Eneko Agirre, Oier L?opez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57?84.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini,
and Emanuele Pianta. 2004. Revising the wordnet
domains hierarchy: semantics, coverage and balanc-
ing. In Proceedings of the Workshop on Multilin-
gual Linguistic Resources, pages 101?108. Associa-
tion for Computational Linguistics.
Tommaso Caselli, Carlo Strapparava, Laure Vieu, and
Guido Vetere. 2014. Aligning an italianwordnet
with a lexicographic dictionary: Coping with limited
data. In Proceedings of the Seventh Global WordNet
Conference, pages 290?298.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Commu-
nication). MIT Press.
417
Bernardo Magnini and Gabriela Cavaglia. 2000. Inte-
grating subject field codes into wordnet. In Proceed-
ings of the conference on International Language
Resources and Evaluation (LREC 2000).
Bernardo Magnini, Carlo Strapparava, Giovanni Pez-
zulo, and Alfio Gliozzo. 2001. Using domain in-
formation for word sense disambiguation. In The
Proceedings of the Second International Workshop
on Evaluating Word Sense Disambiguation Systems,
pages 111?114. Association for Computational Lin-
guistics.
Michael Matuschek and Iryna Gurevych. 2013.
Dijkstra-wsa: A graph-based approach to word sense
alignment. Transactions of the Association for Com-
putational Linguistics (TACL), 2:to appear.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Rada Mihalcea. 2007. Using Wikipedia for automatic
word sense disambiguation. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, Rochester, New York.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 44
th
Annual Meet-
ing of the Association for Computational Linguis-
tics joint with the 21
st
International Conference on
Computational Linguistics (COLING-ACL), Sydney,
Australia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
peoples web meets linguistic knowledge: Automatic
sense alignment of Wikipedia and WordNet. In
Proceedings of the 9th International Conference on
Computational Semantics, pages 205?214, Singa-
pore, January.
Emanuele Pianta, Luisa Bentivogli, and Cristian Gi-
rardi. 2002. MultiWordNet: developing an aligned
multilingual database. In First International Confer-
ence on Global WordNet, Mysore, India.
German Rigau and Agirre Eneko. 1995. Disambiguat-
ing bilingual nominal entries against WordNet. In
Proceedings of workshop The Computational Lexi-
con, 7th European Summer School in Logic, Lan-
guage and Information, Barcelona, Spain.
Adriana Roventini, Nilda Ruimy, Rita Marinelli,
Marisa Ulivieri, and Michele Mammini. 2007.
Mapping concrete entities from PAROLE-SIMPLE-
CLIPS to ItalWordNet: Methodology and results. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, Prague, Czech Republic, June.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Pro-
ceedings of the Third international conference on
Advances in Web Intelligence, AWIC?05, Berlin,
Heidelberg. Springer-Verlag.
Antonio Toral, Oscar Ferr?andez, Eneko Aguirre, and
Rafael Munoz. 2009. A study on linking and disam-
biguating wikipedia categories to wordnet using text
similarity. Proceedings of the International Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP 2009).
Guido Vetere, Alessandro Oltramari, Isabella Chiari,
Elisabetta Jezek, Laure Vieu, and Fabio Massimo
Zanzotto. 2011. Senso Comune, an open knowl-
edge base for italian. JTraitement Automatique des
Langues, 53(3):217?243.
Piek Vossen. 1996. Right or wrong: Combining lexi-
cal resources in the eurowordnet project. In Euralex,
volume 96, pages 715?728. Citeseer.
418
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 57?62,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 13: TempEval-2
Marc Verhagen
?
, Roser Saur??
?
, Tommaso Caselli
?
and James Pustejovsky
?
? Computer Science Department, Brandeis University, Massachusetts, USA
?Barcelona Media, Barcelona, Spain ? ILC-CNR, Pisa, Italy
marc@cs.brandeis.edu roser.sauri@barcelonamedia.org
tommaso.caselli@ilc.cnr.it jamesp@cs.brandeis.edu
Abstract
Tempeval-2 comprises evaluation tasks for
time expressions, events and temporal re-
lations, the latter of which was split up in
four sub tasks, motivated by the notion that
smaller subtasks would make both data
preparation and temporal relation extrac-
tion easier. Manually annotated data were
provided for six languages: Chinese, En-
glish, French, Italian, Korean and Spanish.
1 Introduction
The ultimate aim of temporal processing is the au-
tomatic identification of all temporal referring ex-
pressions, events and temporal relations within a
text. However, addressing this aim is beyond the
scope of an evaluation challenge and a more mod-
est approach is appropriate.
The 2007 SemEval task, TempEval-1 (Verhagen
et al, 2007; Verhagen et al, 2009), was an initial
evaluation exercise based on three limited tempo-
ral ordering and anchoring tasks that were consid-
ered realistic both from the perspective of assem-
bling resources for development and testing and
from the perspective of developing systems capa-
ble of addressing the tasks.
1
TempEval-2 is based on TempEval-1, but is
more elaborate in two respects: (i) it is a multilin-
gual task, and (ii) it consists of six subtasks rather
than three.
In the rest of this paper, we first introduce the
data that we are dealing with. Which gets us in
a position to present the list of task introduced by
TempEval-2, including some motivation as to why
we feel that it is a good idea to split up temporal
relation classification into sub tasks. We proceed
by shortly describing the data resources and their
creation, followed by the performance of the sys-
tems that participated in the tasks.
1
The Semeval-2007 task was actually known simply as
TempEval, but here we use Tempeval-1 to avoid confusion.
2 TempEval Annotation
The TempEval annotation language is a simplified
version of TimeML.
2
using three TimeML tags:
TIMEX3, EVENT and TLINK.
TIMEX3 tags the time expressions in the text and
is identical to the TIMEX3 tag in TimeML. Times
can be expressed syntactically by adverbial or
prepositional phrases, as shown in the following
example.
(1) a. on Thursday
b. November 15, 2004
c. Thursday evening
d. in the late 80?s
e. later this afternoon
The two main attributes of the TIMEX3 tag are
TYPE and VAL, both shown in the example (2).
(2) November 22, 2004
type="DATE" val="2004-11-22"
For TempEval-2, we distinguish four temporal
types: TIME (at 2:45 p.m.), DATE (January 27,
1920, yesterday), DURATION (two weeks) and SET
(every Monday morning). The VAL attribute as-
sumes values according to an extension of the ISO
8601 standard, as enhanced by TIMEX2.
Each document has one special TIMEX3 tag,
the Document Creation Time (DCT), which is in-
terpreted as an interval that spans a whole day.
The EVENT tag is used to annotate those ele-
ments in a text that describe what is conventionally
referred to as an eventuality. Syntactically, events
are typically expressed as inflected verbs, although
event nominals, such as ?crash? in killed by the
crash, should also be annotated as EVENTS. The
most salient event attributes encode tense, aspect,
modality and polarity information. Examples of
some of these features are shown below:
2
See http://www.timeml.org for language speci-
fications and annotation guidelines
57
(3) should have bought
tense="PAST" aspect="PERFECTIVE"
modality="SHOULD" polarity="POS"
(4) did not teach
tense="PAST" aspect="NONE"
modality="NONE" polarity="NEG"
The relation types for the TimeML TLINK tag
form a fine-grained set based on James Allen?s
interval logic (Allen, 1983). For TempEval, the
set of labels was simplified to aid data preparation
and to reduce the complexity of the task. We use
only six relation types including the three core re-
lations BEFORE, AFTER, and OVERLAP, the two
less specific relations BEFORE-OR-OVERLAP and
OVERLAP-OR-AFTER for ambiguous cases, and fi-
nally the relation VAGUE for those cases where no
particular relation can be established.
Temporal relations come in two broad flavours:
anchorings of events to time expressions and or-
derings of events. Events can be anchored to an
adjacent time expression as in examples 5 and 6 or
to the document creation time as in 7.
(5) Mary taught
e1
on Tuesday morning
t1
OVERLAP(e1,t1)
(6) They cancelled the evening
t2
class
e2
OVERLAP(e2,t2)
(7) Most troops will leave
e1
Iraq by August of
2010. AFTER(e1,dct)
The country defaulted
e2
on debts for that en-
tire year. BEFORE(e2,dct)
In addition, events can be ordered relative to
other events, as in the examples below.
(8) The President spoke
e1
to the nation on
Tuesday on the financial crisis. He had
conferred
e2
with his cabinet regarding pol-
icy the day before. AFTER(e1,e2)
(9) The students heard
e1
a fire alarm
e2
.
OVERLAP(e1,e2)
(10) He said
e1
they had postponed
e2
the meeting.
AFTER(e1,e2)
3 TempEval-2 Tasks
We can now define the six TempEval tasks:
A. Determine the extent of the time expressions
in a text as defined by the TimeML TIMEX3
tag. In addition, determine value of the fea-
tures TYPE and VAL.
B. Determine the extent of the events in a text
as defined by the TimeML EVENT tag. In
addition, determine the value of the features
CLASS, TENSE, ASPECT, POLARITY, and
MODALITY.
C. Determine the temporal relation between an
event and a time expression in the same
sentence. This task is further restricted by
requiring that either the event syntactically
dominates the time expression or the event
and time expression occur in the same noun
phrase.
D. Determine the temporal relation between an
event and the document creation time.
E. Determine the temporal relation between two
main events in consecutive sentences.
F. Determine the temporal relation between two
events where one event syntactically domi-
nates the other event.
Of these tasks, C, D and E were also defined for
TempEval-1. However, the syntactic locality re-
striction in task C was not present in TempEval-1.
Task participants could choose to either do all
tasks, focus on the time expression task, focus on
the event task, or focus on the four temporal rela-
tion tasks. In addition, participants could choose
one or more of the six languages for which we pro-
vided data: Chinese, English, French, Italian, Ko-
rean, and Spanish.
We feel that well-defined tasks allow us to struc-
ture the workflow, allowing us to create task-
specific guidelines and using task-specific anno-
tation tools to speed up annotation. More im-
portantly, each task can be evaluated in a fairly
straightforward way, contrary to for example the
problems that pop up when evaluating two com-
plex temporal graphs for the same document. In
addition, tasks can be ranked, allowing systems to
feed the results of one (more precise) task as a fea-
ture into another task.
Splitting the task into substask reduces the error
rate in the manual annotation, and that merging
the different sub-task into a unique layer as a post-
processing operation (see figure 1) provides better
58
Figure 1: Merging Relations
and more reliable results (annotated data) than do-
ing a complex task all at once.
4 Data Preparation
The data for the five languages were prepared in-
dependently of each other and do not comprise a
parallel corpus. However, annotation specifica-
tions and guidelines for the five languages were
developed in conjunction with one other, in many
cases based on version 1.2.1 of the TimeML an-
notation guidelines for English
3
. Not all corpora
contained data for all six tasks. Table 1 gives the
size of the training set and the relation tasks that
were included.
language tokens C D E F X
Chinese 23,000 X X X X
English 63,000 X X X X
Italian 27,000 X X X
French 19,000 X
Korean 14,000
Spanish 68,000 X X
Table 1: Corpus size and relation tasks
All corpora include event and timex annota-
tion. The French corpus contained a subcorpus
with temporal relations but these relations were
not split into the four tasks C through F.
Annotation proceeded in two phases: a dual
annotation phase where two annotators annotate
each document and an adjudication phase where
a judge resolves disagreements between the an-
notators. Most languages used BAT, the Brandeis
Annotation Tool (Verhagen, 2010), a generic web-
based annotation tool that is centered around the
notion of annotation tasks. With the task decom-
position allowed by BAT, it is possible to structure
the complex task of temporal annotation by split-
ting it up in as many sub tasks as seems useful. As
3
See http://www.timeml.org.
such, BAT was well-suited for TempEval-2 anno-
tation.
We now give a few more details on the English
and Spanish data, skipping the other languages for
reasons that will become obvious at the beginning
of section 6.
The English data sets were based on TimeBank
(Pustejovsky et al, 2003; Boguraev et al, 2007),
a hand-built gold standard of annotated texts us-
ing the TimeML markup scheme.
4
However, all
event annotation was reviewed to make sure that
the annotation complied with the latest guidelines
and all temporal relations were added according to
the Tempeval-2 relation tasks, using the specified
relation types.
The data released for the TempEval-2 Spanish
edition is a fragment of the Spanish TimeBank,
currently under development. Its documents are
originally from the Spanish part of the AnCora
corpus (Taul?e et al, 2008). Data preparation fol-
lowed the annotation guidelines created to deal
with the specificities of event and timex expres-
sions in Spanish (Saur?? et al, 2009a; Saur?? et al,
2009b).
5 Evaluation Metrics
For the extents of events and time expres-
sions (tasks A and B), precision, recall and the
f1-measure are used as evaluation metrics, using
the following formulas:
precision = tp/(tp + fp)
recall = tp/(tp + fn)
f -measure = 2 ? (P ? R)/(P + R)
Where tp is the number of tokens that are part
of an extent in both key and response, fp is the
number of tokens that are part of an extent in the
response but not in the key, and fn is the number
of tokens that are part of an extent in the key but
not in the response.
For attributes of events and time expressions
(the second part of tasks A and B) and for relation
types (tasks C through F) we use an even simpler
metric: the number of correct answers divided by
the number of answers.
4
See www.timeml.org for details on TimeML, Time-
Bank is distributed free of charge by the Linguistic
Data Consortium (www.ldc.upenn.edu), catalog num-
ber LDC2006T08.
59
6 System Results
Eight teams participated in TempEval-2, submit-
ting a grand total of eighteen systems. Some of
these systems only participated in one or two tasks
while others participated in all tasks. The distribu-
tion over the six languages was very uneven: six-
teen systems for English, two for Spanish and one
for English and Spanish.
The results for task A, recognition and normal-
ization of time expressions, are given in tables 2
and 3.
team p r f type val
UC3M 0.90 0.87 0.88 0.91 0.83
TIPSem 0.95 0.87 0.91 0.91 0.78
TIPSem-B 0.97 0.81 0.88 0.99 0.75
Table 2: Task A results for Spanish
team p r f type val
Edinburgh 0.85 0.82 0.84 0.84 0.63
HeidelTime1 0.90 0.82 0.86 0.96 0.85
HeidelTime2 0.82 0.91 0.86 0.92 0.77
JU CSE 0.55 0.17 0.26 0.00 0.00
KUL 0.78 0.82 0.80 0.91 0.55
KUL Run 2 0.73 0.88 0.80 0.91 0.55
KUL Run 3 0.85 0.84 0.84 0.91 0.55
KUL Run 4 0.76 0.83 0.80 0.91 0.51
KUL Run 5 0.75 0.85 0.80 0.91 0.51
TERSEO 0.76 0.66 0.71 0.98 0.65
TIPSem 0.92 0.80 0.85 0.92 0.65
TIPSem-B 0.88 0.60 0.71 0.88 0.59
TRIOS 0.85 0.85 0.85 0.94 0.76
TRIPS 0.85 0.85 0.85 0.94 0.76
USFD2 0.84 0.79 0.82 0.90 0.17
Table 3: Task A results for English
The results for Spanish are more uniform and
generally higher than the results for English.
For Spanish, the f-measure for TIMEX3 extents
ranges from 0.88 through 0.91 with an average of
0.89; for English the f-measure ranges from 0.26
through 0.86, for an average of 0.78. However,
due to the small sample size it is hard to make
any generalizations. In both languages, type de-
tection clearly was a simpler task than determining
the value.
The results for task B, event recognition, are given
in tables 4 and 5. Both tables contain results for
both Spanish and English, the first part of each ta-
ble contains the results for Spanish and the next
part the results for English.
team p r f
TIPSem 0.90 0.86 0.88
TIPSem-B 0.92 0.85 0.88
team p r f
Edinburgh 0.75 0.85 0.80
JU CSE 0.48 0.56 0.52
TIPSem 0.81 0.86 0.83
TIPSem-B 0.83 0.81 0.82
TRIOS 0.80 0.74 0.77
TRIPS 0.55 0.88 0.68
Table 4: Event extent results
The column headers in table 5 are abbrevia-
tions for polarity (pol), mood (moo), modality
(mod), tense (tns), aspect (asp) and class (cl). Note
that the English team chose to include modality
whereas the Spanish team used mood.
team pol moo tns asp cl
TIPSem 0.92 0.80 0.96 0.89 0.66
TIPSem-B 0.92 0.79 0.96 0.89 0.66
team pol mod tns asp cl
Edinburgh 0.99 0.99 0.92 0.98 0.76
JU CSE 0.98 0.98 0.30 0.95 0.53
TIPSem 0.98 0.97 0.86 0.97 0.79
TIPSem-B 0.98 0.98 0.85 0.97 0.79
TRIOS 0.99 0.95 0.91 0.98 0.77
TRIPS 0.99 0.96 0.67 0.97 0.67
Table 5: Event attribute results
As with the time expressions results, the sample
size for Spanish is small, but note again the higher
f-measure for event extents in Spanish.
Table 6 shows the results for all relation tasks, with
the Spanish systems in the first two rows and the
English systems in the last six rows. Recall that for
Spanish the training and test sets only contained
data for tasks C and D.
Interestingly, the version of the TIPSem sys-
tems that were applied to the Spanish data did
much better on task C compared to its English
cousins, but much worse on task D, which is rather
puzzling.
Such a difference in performance of the systems
could be due to differences in annotation accurate-
ness, or it could be due to some particularities of
how the two languages express certain temporal
60
team C D E F
TIPSem 0.81 0.59 - -
TIPSem-B 0.81 0.59 - -
JU CSE 0.63 0.80 0.56 0.56
NCSU-indi 0.63 0.68 0.48 0.66
NCSU-joint 0.62 0.21 0.51 0.25
TIPSem 0.55 0.82 0.55 0.59
TIPSem-B 0.54 0.81 0.55 0.60
TRIOS 0.65 0.79 0.56 0.60
TRIPS 0.63 0.76 0.58 0.59
USFD2 0.63 - 0.45 -
Table 6: Results for relation tasks
aspects, or perhaps the one corpus is more ho-
mogeneous than the other. Again, there are not
enough data points, but the issue deserves further
attention.
For each task, the test data provided the event
pairs or event-timex pairs with the relation type
set to NONE and participating systems would re-
place that value with one of the six allowed rela-
tion types. However, participating systems were
allowed to not replace NONE and not be penalized
for it. Those cases would not be counted when
compiling the scores in table 6. Table 7 lists those
systems that did not classify all relation and the
percentage of relations for each task that those sys-
tems did not classify.
team C D E F
TRIOS 25% 19% 36% 31%
TRIPS 20% 10% 17% 10%
Table 7: Percentage not classified
A comparison with the Tempeval-1 results from
Semeval-2007 may be of interest. Six systems
participated in the TempEval-1 tasks, compared
to seven or eight systems for TempEval-2. Table
8 lists the average scores and the standard devi-
ations for all the tasks (on the English data) that
Tempeval-1 and Tempeval-2 have in common.
C D E
tempeval-1 average 0.59 0.76 0.51
stddev 0.03 0.03 0.05
tempeval-2 average 0.61 0.70 0.53
stddev 0.04 0.22 0.05
Table 8: Comparing Tempevals
The results are very similar except for task D,
but if we take a away the one outlier (the NCSU-
joint score of 0.21) then the average becomes 0.78
with a standard deviation of 0.05. However, we
had expected that for TempEval-2 the systems
would score better on task C since we added the
restriction that the event and time expression had
to be syntactically adjacent. It is not clear why the
results on task C have not improved.
7 Conclusion
In this paper, we described the TempEval-2 task
within the SemEval 2010 competition. This task
involves identifying the temporal relations be-
tween events and temporal expressions in text. Us-
ing a subset of TimeML temporal relations, we
show how temporal relations and anchorings can
be annotated and identified in six different lan-
guages. The markup language adopted presents
a descriptive framework with which to examine
the temporal aspects of natural language informa-
tion, demonstrating in particular, how tense and
temporal information is encoded in specific sen-
tences, and how temporal relations are encoded
between events and temporal expressions. This
work paves the way towards establishing a broad
and open standard metadata markup language for
natural language texts, examining events, tempo-
ral expressions, and their orderings.
One thing that would need to be addressed in
a follow-up task is what the optimal number of
tasks is. Tempeval-2 had six tasks, spread out over
six languages. This brought about some logisti-
cal challenges that delayed data delivery and may
have given rise to a situation where there was sim-
ply not enough time for many systems to properly
prepare. And clearly, the shared task was not suc-
cessful in attracting systems to four of the six lan-
guages.
8 Acknowledgements
Many people were involved in TempEval-2. We
want to express our gratitude to the following key
contributors: Nianwen Xue, Estela Saquete, Lo-
tus Goldberg, Seohyun Im, Andr?e Bittar, Nicoletta
Calzolari, Jessica Moszkowicz and Hyopil Shin.
Additional thanks to Joan Banach, Judith
Domingo, Pau Gim?enez, Jimena del Solar, Teresa
Su?nol, Allyson Ettinger, Sharon Spivak, Nahed
Abul-Hassan, Ari Abelman, John Polson, Alexan-
dra Nunez, Virginia Partridge, , Amber Stubbs,
Alex Plotnick, Yuping Zhou, Philippe Muller and
61
Irina Prodanof.
The work on the Spanish corpus was supported
by a EU Marie Curie International Reintegration
Grant (PIRG04-GA-2008-239414). Work on the
English corpus was supported under the NSF-CRI
grant 0551615, ?Towards a Comprehensive Lin-
guistic Annotation of Language? and the NSF-
INT-0753069 project ?Sustainable Interoperabil-
ity for Language Technology (SILT)?, funded by
the National Science Foundation.
Finally, thanks to all the participants, for stick-
ing with a task that was not always as flawless and
timely as it could have been in a perfect world.
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and
Marc Verhagen. 2007. Timebank evolution as a
community resource for timeml parsing. Language
Resource and Evaluation, 41(1):91?115.
James Pustejovsky, David Day, Lisa Ferro, Robert
Gaizauskas, Patrick Hanks, Marcia Lazo, Roser
Saur??, Andrew See, Andrea Setzer, and Beth Sund-
heim. 2003. The TimeBank Corpus. Corpus Lin-
guistics, March.
Roser Saur??, Olga Batiukova, and James Pustejovsky.
2009a. Annotating events in spanish. timeml an-
notation guidelines. Technical Report Version
TempEval-2010., Barcelona Media - Innovation
Center.
Roser Saur??, Estela Saquete, and James Pustejovsky.
2009b. Annotating time expressions in spanish.
timeml annotation guidelines. Technical Report
Version TempEval-2010, Barcelona Media - Inno-
vation Center.
Mariona Taul?e, Toni Mart??, and Marta Recasens. 2008.
Ancora: Multilevel annotated corpora for catalan
and spanish. In Proceedings of the LREC 2008,
Marrakesh, Morocco.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval tempo-
ral relation identification. In Proc. of the Fourth
Int. Workshop on Semantic Evaluations (SemEval-
2007), pages 75?80, Prague, Czech Republic, June.
Association for Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The tempeval challenge: iden-
tifying temporal relations in text. Language Re-
sources and Evaluation.
Marc Verhagen. 2010. The Brandeis Annotation Tool.
In Language Resources and Evaluation Conference,
LREC 2010, Malta.
62
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 284?288,
Dublin, Ireland, August 23-24, 2014.
FBK-TR: Applying SVM with Multiple Linguistic Features for
Cross-Level Semantic Similarity
Ngoc Phuoc An Vo
Fondazione Bruno Kessler
University of Trento
Trento, Italy
ngoc@fbk.eu
Tommaso Caselli
TrentoRISE
Trento, Italy
t.caselli@trentorise.eu
Octavian Popescu
Fondazione Bruno Kessler
Trento, Italy
popescu@fbk.eu
Abstract
Recently, the task of measuring seman-
tic similarity between given texts has
drawn much attention from the Natural
Language Processing community. Espe-
cially, the task becomes more interesting
when it comes to measuring the seman-
tic similarity between different-sized texts,
e.g paragraph-sentence, sentence-phrase,
phrase-word, etc. In this paper, we, the
FBK-TR team, describe our system par-
ticipating in Task 3 "Cross-Level Seman-
tic Similarity", at SemEval 2014. We also
report the results obtained by our system,
compared to the baseline and other partic-
ipating systems in this task.
1 Introduction
Measuring semantic text similarity has become a
hot trend in NLP as it can be applied to other
tasks, e.g. Information Retrieval, Paraphrasing,
Machine Translation Evaluation, Text Summariza-
tion, Question and Answering, and others. Several
approaches proposed to measure the semantic sim-
ilarity between given texts. The first approach is
based on vector space models (VSMs) (Meadow,
1992). A VSM transforms given texts into "bag-
of-words" and presents them as vectors. Then, it
deploys different distance metrics to compute the
closeness between vectors, which will return as
the distance or similarity between given texts. The
next well-known approach is using text-alignment.
By assuming that two given texts are semantically
similar, they could be aligned on word or phrase
levels. The alignment quality can serve as a simi-
larity measure. "It typically pairs words from the
two texts by maximizing the summation of the
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
word similarity of the resulting pairs" (Mihalcea
et al., 2006). In contrast, the third approach uses
machine learning techniques to learn models built
from different lexical, semantic and syntactic fea-
tures and then give predictions on degree of simi-
larity between given texts (?ari
?
c et al., 2012).
At SemEval 2014, the Task 3 "Cross-Level Se-
mantic Similarity" (Jurgens et al., 2014) is to eval-
uate the semantic similarity across different sizes
of texts, in particular, a larger-sized text is com-
pared to a smaller-sized one. The task consists
of four types of semantic similarity comparison:
paragraph to sentence, sentence to phrase, phrase
to word, and word to sense. The degree of similar-
ity ranges from 0 (different meanings) to 4 (simi-
lar meanings). For evaluation, systems were eval-
uated, first, within comparison type and second,
across all comparison types. Two methods are
used to evaluate between system outputs and gold
standard (human annotation), which are Pearson
correlation and Spearman?s rank correlation (rho).
The FBK-TR team participated in this task with
three different runs. In this paper, we present a
clear and comprehensive description of our sys-
tem which obtained competitive results. Our main
approach is using machine learning technique to
learn models from different lexical and semantic
features from train corpora to make prediction on
the test corpora. We used support vector machine
(SVM) regression model to solve the task.
The remainder of the paper is organized as fol-
lows. Section 2 presents the system overview.
Sections 3, 4 and 5 describe the Semantic Word
Similarity, String Similarity and other features, re-
spectively. Section 6 discusses about SVM ap-
proach. Section 7 presents the experiment settings
for each subtask. Finally, Sections 8 and 9 present
the evaluation and conclusion.
284
Figure 1: System Overview.
2 System Overview
Our system was built on different linguistic fea-
tures as shown in Figure 1. By constructing a
pipeline system, each linguistic feature can be
used independently or together with others to mea-
sure the semantic similarity of given texts as well
as to evaluate the significance of each feature to
the accuracy of system?s predictions. On top of
this, the system is expandable and scalable for
adopting more useful features aiming for improv-
ing the accuracy.
3 Semantic Word Similarity Measures
At the lexical level, we built a simple, yet effec-
tive Semantic Word Similarity model consisting of
three components: WordNet similarity, Wikipedia
relatedness and Latent Semantic Analysis (LSA).
These components played important and compli-
mentary roles to each other.
3.1 Data Processing
We used the TreeTagger tool (Schmid, 1994) to
extract Part-of-Speech (POS) from each given
text, then tokenize and lemmatize it. On the basis
of the POS tags, we only picked lemmas of con-
tent words (Nouns and Verbs) from the given texts
and then paired them up regarding to similar POS
tags.
3.2 WordNet Similarity and Levenshtein
Distance
WordNet (Fellbaum, 1999) is a lexical database
for the English language in which words are
grouped into sets of synonyms (namely synsets,
each expressing a distinct concept) to provide
short, general definitions, and record the vari-
ous semantic relations between synsets. We used
Perdersen?s package WordNet:Similarity (Peder-
sen et al., 2004) to obtain similarity scores for
the lexical items covered in WordNet. Similarity
scores have been computed by means of the Lin
measure (Lin, 1998). The Lin measure is built on
Resnik?s measure of similarity (Resnik, 1995):
Sim
lin
=
2 ? IC(LCS)
IC(concept
1
) + IC(concept
2
)
(1)
where IC(LCS) is the information content (IC) of
the least common subsumer (LCS) of two con-
cepts.
To overcome the limit in coverage of WordNet,
we applied the Levenshtein distance (Levenshtein,
1966). The distance between two words is defined
by the minimum number of operations (insertions,
deletions and substitutions) needed to transform
one word into the other.
3.3 Wikipedia Relatedness
Wikipedia Miner (Milne and Witten, 2013) is a
Java-based package developed for extracting se-
mantic information from Wikipedia. Through our
experiments, we observed that Wikipedia related-
ness plays an important role for providing extra
information to measure the semantic similarity be-
tween words. We used the package Wikipedia
Miner from University of Waikato (New Zealand)
to extract additional relatedness scores between
words.
3.4 Latent Semantic Analysis (LSA)
We also took advantage from corpus-based ap-
proaches to measure the semantic similarity be-
tween words by using Latent Semantic Analysis
(LSA) technique (Landauer et al., 1998). LSA as-
sumes that similar and/or related words in terms
of meaning will occur in similar text contexts. In
general, a LSA matrix is built from a large cor-
pus. Rows in the matrix represent unique words
and columns represent paragraphs or documents.
The content of the matrix corresponds to the word
count per paragraph/document. Matrix size is then
reduced by means of Single Value Decomposition
(SVD) technique. Once the matrix has been ob-
tained, similarity and/or relatedness between the
words is computed by means of cosine values
(scaled between 0 and 1) for each word vector
in the matrix. Values close to 1 are assumed to
285
be very similar/related, otherwise dissimilar. We
trained our LSA model on the British National
Corpus (BNC)
1
and Wikipedia
2
corpora.
4 String Similarity Measures
The Longest Common Substring (LCS) is the
longest string in common between two or more
strings. Two given texts are considered similar if
they are overlapping/covering each other (e.g sen-
tence 1 covers a part of sentence 2, or otherwise).
We implemented a simple algorithm to extract the
LCS between two given texts. Then we divided the
LCS length by the product of normalized lengths
of two given texts and used it as a feature.
4.1 Analysis Before and After LCS
After extracting the LCS between two given texts,
we also considered the similarity for the parts be-
fore and after the LCS. The similarity between the
text portions before and after the LSC has been ob-
tained by means of the Lin measure and the Lev-
enshtein distance.
5 Other Features
To take into account other levels of analysis for se-
mantic similarity between texts, we extended our
features by means of topic modeling and Named
Entities.
5.1 Topic Modeling (Latent Dirichlet
Allocation - LDA)
Topic modeling is a generative model of docu-
ments which allows to discover topics embedded
in a document collection and their balance in each
document. If two given texts are expressing the
same topic, they should be considered highly sim-
ilar. We applied topic modeling, particularly, La-
tent Dirichlet allocation (LDA) (Blei et al., 2003)
to predict the topics expressed by given texts.
The MALLET topic model package (McCal-
lum, 2002) is a Java-based tool used for inferring
hidden "topics" in new document collections us-
ing trained models. We used Mallet topic model-
ing tool to build different models using BNC and
Wikipedia corpora.
We noticed that, in LDA, the number of top-
ics plays an important role to fine grained predic-
tions. Hence, we built different models for differ-
ent numbers of topics, from minimum 20 topics to
1
http://www.natcorp.ox.ac.uk
2
http://en.wikipedia.org/wiki/Wikipedia:Database_download
maximum 500 topics (20, 50, 100, 150, 200, 250,
300, 350, 400, 450 and 500). From the proportion
vectors (distribution of documents over topics) of
given texts, we applied three different measures to
compute the distance between each pair of texts,
which are Cosine similarity, Kullback-Leibler and
Jensen-Shannon divergences (Gella et al., 2013).
5.2 Named-Entity Recognition (NER)
NER aims at identifying and classifying entities
in a text with respect to a predefined set of cate-
gories such as person names, organizations, loca-
tions, time expressions, quantities, monetary val-
ues, percentages, etc. By exploring the training
set, we observed that there are lot of texts in this
task containing named entities. We deployed the
Stanford Named Entity Recognizer tool (Finkel et
al., 2005) to extract the similar and overlapping
named entities between two given texts. Then we
divided the number of similar/overlapping named
entities by the sum length of two given texts.
6 Support Vector Machines (SVMs)
Support vector machine (SVM) (Cortes and Vap-
nik, 1995) is a type of supervised learning ap-
proaches. We used the LibSVM package (Chang
and Lin, 2011) to learn models from the different
linguistic features described above. However, in
SVM the problem of finding optimal kernel pa-
rameters is critical and important for the learning
process. Hence, we used practical advice (Hsu et
al., 2003) for data scaling and a grid-search pro-
cess for finding the optimal parameters (C and
gamma) for building models. We trained the SVM
models in a regression framework.
7 Experiment Settings
For subtasks paragraph-to-sentence and sentence-
to-phrase, since the length between two units is
completely different, we decided, first to apply
topic model to identify if two given texts are ex-
pressing a same topic. Furthermore, named enti-
ties play an important role in these subtasks. How-
ever, as there are many named entities which are
not English words and cannot be identified by the
NER tool, we developed a program to detect and
identify common words occurring in both given
texts. Then we continued to extract other lexical
and semantic features to measure the similarity be-
tween the two texts.
286
Team Para2Sent Para2Sent
(Pearson) (Spearman)
UNAL-NLP, run2 (ranked 1st) 0.837 0.820
ECNU, run1(ranked 1st) 0.834 0.821
FBK-TR, run2 0.77 0.775
FBK-TR, run3 0.759 0.770
FBK-TR, run1 0.751 0.759
Baseline (LCS) 0.527 0.613
Table 1: Results for paragraph-to-sentence.
Team Sent2Phr Sent2Phr
(Pearson) (Spearman)
Meerkat_Mafia, 0.777 0.760
SuperSaiyan (ranked 1st)
FBK-TR, run3 0.702 0.695
FBK-TR, run1 0.685 0.681
FBK-TR, run2 0.648 0.642
Baseline (LCS) 0.562 0.626
Table 2: Results for sentence-to-phrase.
For the subtask word-to-sense, we used the Se-
mantic Word Similarity model which consists of
three components: WordNet similarity, Wikipedia
relatedness and LSA similarity (described in sec-
tion 3). For phrase-to-word, we extracted all
glosses of the given word, then computed the simi-
larity between the given phrase and each extracted
gloss. Finally, we selected the highest similarity
score for result.
8 Evaluations
As a result, we report our performance in the four
subtasks as follows.
8.1 Subtasks: Paragraph-to-Sentence and
Sentence-to-Phrase
The evaluation results using Pearson and Spear-
man correlations show the difference between our
system and best system in these two subtasks in
the Tables 1 and 2.
Team Para2Sent Sent2Phr Phr2Word Word2Sens Sum
SimCompass 0.811 0.742 0.415 0.356 2.324
(ranked 1st)
FBK-TR 0.759 0.702 0.305 0.155 1.95
Baseline 0.527 0.562 0.165 0.109 1.363
Table 3: Overall result using Pearson.
Team Para2Sent Sent2Phr Phr2Word Word2Sens Sum
SimCompass 0.801 0.728 0.424 0.344 2.297
(ranked 1st)
FBK-TR 0.770 0.695 0.298 0.150 1.913
Baseline 0.613 0.626 0.162 0.130 1.528
Table 4: Overall result using Spearman.
8.2 Subtasks: Phrase-to-Word and
Word-to-Sense
Even though we did not submit the results as
they looked very low, we report the scores for
the phrase-to-word and word-to-sense subtasks. In
the phrase-to-word subtask, we obtained a Pearson
score of 0.305 and Spearman value of 0.298. As
for the word-to-sense subtask, we scored 0.155 for
Pearson and 0.150 for Spearman.
Overall, with the submitted results for two sub-
tasks described in Section 8.1, our system?s runs
ranked 20th, 21st and 22nd among 38 participat-
ing systems. However, by taking into account the
un-submitted results for the two other subtasks,
our best run obtained 1.95 (Pearson correlation)
and 1.913 (Spearman correlation), which can be
ranked in the top 10 among 38 systems (figures
are reported in Table 3 and 4).
9 Conclusions and Future Work
In this paper, we describe our system participating
in the Task 3, at SemEval 2014. We present a com-
pact system using machine learning approach (par-
ticularly, SVMs) to learn models from a set of lex-
ical and semantic features to predict the degree of
similarity between different-sized texts. Although
we only submitted the results for two out of four
subtasks, we obtained competitive results among
the other participants. For future work, we are
planning to increase the number of topics in LDA,
as more fine-grained topics should allow predict-
ing better similarity scores. Finally, we will inves-
tigate more on the use of syntactic features.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology (TIST), 2(3):27.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
287
Vector Networks. Machine learning, 20(3):273?
297.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370.
Spandana Gella, Bahar Salehi, Marco Lui, Karl
Grieser, Paul Cook, and Timothy Baldwin. 2013.
Unimelb_nlp-core: Integrating predictions from
multiple domains and feature sets for estimating se-
mantic textual similarity. Atlanta, Georgia, USA,
page 207.
Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, et al.
2003. A Practical Guide to Support Vector Classifi-
cation.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014) August 23-24, 2014, Dublin,
Ireland.
Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse processes, 25(2-3):259?284.
Vladimir I Levenshtein. 1966. Binary Codes Capable
of Correcting Deletions, Insertions and Reversals.
In Soviet physics doklady, volume 10, page 707.
Dekang Lin. 1998. An Information-Theoretic Defini-
tion of Similarity. In ICML, volume 98, pages 296?
304.
Andrew Kachites McCallum. 2002. Mallet: A Ma-
chine Learning for Language Toolkit.
Charles T Meadow. 1992. Text Information Retrieval
Systems. Academic Press, Inc., Orlando, FL, USA.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. In AAAI, vol-
ume 6, pages 775?780.
David Milne and Ian H Witten. 2013. An Open-
Source Toolkit for Mining Wikipedia. Artificial In-
telligence, 194:222?239.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - Measuring the Re-
latedness of Concepts. In Demonstration Papers at
HLT-NAACL 2004, pages 38?41.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. arXiv
preprint cmp-lg/9511007.
Frane ?ari?c, Goran Glava?, Mladen Karan, Jan ?najder,
and Bojana Dalbelo Ba?i?c. 2012. Takelab: Sys-
tems for Measuring Semantic Text Similarity. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 441?448.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
international conference on new methods in lan-
guage processing, volume 12, pages 44?49. Manch-
ester, UK.
288
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 289?293,
Dublin, Ireland, August 23-24, 2014.
FBK-TR: SVM for Semantic Relatedness and Corpus Patterns for RTE
Ngoc Phuoc An Vo
Fondazione Bruno Kessler
University of Trento
Trento, Italy
ngoc@fbk.eu
Octavian Popescu
Fondazione Bruno Kessler
Trento, Italy
popescu@fbk.eu
Tommaso Caselli
TrentoRISE
Trento, Italy
t.caselli@trentorise.eu
Abstract
This paper reports the description and
scores of our system, FBK-TR, which
participated at the SemEval 2014 task
#1 "Evaluation of Compositional Distribu-
tional Semantic Models on Full Sentences
through Semantic Relatedness and Entail-
ment". The system consists of two parts:
one for computing semantic relatedness,
based on SVM, and the other for identi-
fying the entailment values on the basis
of both semantic relatedness scores and
entailment patterns based on verb-specific
semantic frames. The system ranked 11
th
on both tasks with competitive results.
1 Introduction
In the Natural Language Processing community,
meaning related tasks have gained an increasing
popularity. These tasks focus, in general, on a
couple of short pieces of text, like pair of sen-
tences, and the systems are required to infer a cer-
tain meaning relationship that exists between these
texts. Two of the most popular meaning related
tasks are the identification of Semantic Text Sim-
ilarity (STS) and Recognizing Textual Entailment
(RTE). The STS tasks require to identify the de-
gree of similarity (or relatedness) that exists be-
tween two text fragments (sentences, paragraphs,
. . . ), where similarity is a broad concept and its
value is normally obtained by averaging the opin-
ion of several annotators. The RTE task requires
the identification of a directional relation between
a pair of text fragments, namely a text (T) and a
hypothesis (H). The relation (T? H) holds when-
ever the truth of H follows from T.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
At SemEval 2014, the Task #1 "Evaluation
of Compositional Distributional Semantic Models
on Full Sentences through Semantic Relatedness
and Entailment" (Marelli et al., 2014a) primarily
aimed at evaluating Compositional Distributional
Semantic Models (CDSMs) of meaning over two
subtasks, namely semantic relatedness and tex-
tual entailment (ENTAILMENT, CONTRADIC-
TION and NEUTRAL), over pairs of sentences
(Marelli et al., 2014b). Concerning the relatedness
subtask, the system outputs are evaluated against
gold standard ratings in two ways, using Pearson
correlation and Spearman?s rank correlation (rho).
The Pearson correlation is used for evaluating and
ranking the participating systems. Similarly, for
the textual entailment subtask, system outputs are
evaluated against a gold standard rating with re-
spect to accuracy.
Our team, FBK-TR, participated in both sub-
tasks with five different runs. In this paper, we
present a comprehensive description of our system
which obtained competitive results in both tasks
and which is not based on CDSMs. Our approach
for the relatedness task is based on machine learn-
ing techniques to learn models from different lexi-
cal and semantic features from the train corpus and
then to make prediction on the test corpus. Par-
ticularly, we used support vector machine (SVM)
(Chang and Lin, 2011), regression model to solve
this subtask. On the other hand, the textual en-
tailment task uses a methodology mainly based on
corpus patterns automatically extracted from an-
notated text corpora.
The remainder of the paper is organized as
follows: Section 2 presents the SVM system
for semantic relatedness. Section 3 describes
the methodology used for extracting patterns and
computing the textual entailment values. Finally,
Section 4 discusses about the evaluations and Sec-
tion 5 presents conclusions and future work.
289
Figure 1: Schema of the system for computing entailment.
2 System Overview for Semantic
Relatedness Subtask
Concerning the Semantic Relatedness subtask our
SVM system is built on different linguistic fea-
tures, ranging from relatedness at the lexical level
(WordNet based measures, Wikipedia relatedness
and Latent Semantic Analysis), to sentence level,
including topic modeling based on Latent Dirich-
let allocation (LDA) and string similarity (Longest
Common Substring).
2.1 Lexical Features
At the lexical level, we built a simple, yet effective
Semantic Word Relatedness model, which con-
sists of 3 components: WordNet similarity (based
on the Lin measure as implemented in Pedersen
package WordNet:Similarity (Pedersen et
al., 2004), Wikipedia relatedness (as provided by
the Wikipedia Miner package (Milne and Witten,
2013)), and Latent Semantic Analysis (Landauer
et al., 1998), with a model trained on the British
National Corpus (BNC)
1
and Wikipedia. At this
level of analysis, we concentrated only on the
best matched (lemma) pairs of content words, i.e.
Noun-Noun, Verb-Verb, extracted from each sen-
tence pair. The content words have been automati-
cally extracted by means of part-of-speech tagging
(TreeTagger (Schmid, 1994)) and lemmatization.
For words which are not present in WordNet,
the relatedness score has been obtained by means
of the Levenshtein distance (Levenshtein, 1966).
1
http://www.natcorp.ox.ac.uk
2.2 Topic Modeling
We have applied topic modeling based on Latent
Dirichlet allocation (LDA) (Blei et al., 2003) as
implemented in the MALLET package (McCal-
lum, 2002). The topic model was developed us-
ing the BNC and Wikipedia (with the numbers
of topics varying from 20 to 500 topics). From
the proportion vectors (distribution of documents
over topics) of the given texts, we apply 3 differ-
ent measures (Cosine similarity, Kullback-Leibler
and Jensen-Shannon divergences) to compute the
distances between each pair of sentences.
2.3 String Similarity: Longest Common
Substring
As for the string level, two given sentences are
considered similar/related if they are overlap-
ping/covering each other (e.g sentence 1 covers
a part of sentence 2, or otherwise). Hence, we
considered the text overlapping between two
given texts as a feature for our system. The
extraction of the features at the string level was
computed in two steps: first, we obtained Longest
Common Substring between two given sentences.
After this, we also considered measuring the
similarity for the parts before and after the LCS
between two given texts, by means of the Lin
measure and the Levenshtein distance.
3 System Overview for RTE Subtask
The system for the identification of the entailment
values is illustrated in Figure 1. Entailment values
290
are computed starting from a baseline (only EN-
TAILMENT and NEUTRAL values) which relies
on the output (i.e. scores) of the semantic related-
ness system. After this step, two groups of entail-
ment patterns are applied whether the surface form
of a sentence pair is affirmative (i.e. absence of
negation words) or negative. Each type of pattern
provides in output an associated entailment value
which corresponds to the final value assigned by
the system.
The entailment patterns are based on verb-
specific semantic frames that include both syn-
tactic and semantic information. Hence, we have
explicit access to the information that individual
words have and to the process of combining them
in bigger units, namely phrases, which carry out
meanings. The patterns have two properties: i.)
the senses of the words inside the pattern are sta-
ble, they do not change whatever context is added
to the left, right or inside the phrase matching the
pattern, and ii.) the replacement of a word with an-
other word belonging to a certain class changes the
senses of the words. Patterns with these properties
are called Sense Discriminative Patterns (SDPs).
It has been noted (Popescu et al., 2011) that we can
associate to a phrase that is matched by an SDP a
set of phrases for which an entailment relationship
is decidable showing that there is a direct relation-
ship between SDPs and entailment .
SDP patterns have been obtained from large
parsed corpora. To maximize the accuracy of the
corpus we have chosen sentences containing at
maximum two finite verbs from BNC and Anno-
tated English Gigaword. We parsed this corpus
with the Stanford parser, discarding the sentences
from the Annotated English Gigaword which have
a different parsing. Each words is replaced with
their possible SUMO attributes (Niles and Pease,
2003). Only the following Stanford dependen-
cies are retained as valid [n, nsub]sbj, [d,i,p]obj,
prep, [x,c]comp. We considered only the most fre-
quent occurrences of such patterns for each verb.
To cluster into a single SDP pattern, all patterns
that are sense auto-determinative, we used the
OntoNotes (Hovy et al., 2006) and CPA (Hanks,
2008) lexica. Inside each cluster, we searched
for the most general hypernyms for each syntac-
tic slot such that there are no common patterns
between clusters (Popescu, 2013). However, the
patterns thus obtained are not sufficient enough
for the task. Some expressions may be the para-
phrasis a word in the context of an SDP. To ex-
tract this information, we considered all the pairs
in training that are in an ENTAILMENT relation-
ship, with a high relatedness score (4 to 5), and we
extracted the parts that are different for each gram-
matical slot. In this way, we compiled a list of
quasi synonym phrases that can be replaced inside
an SDP without affecting the replacement. This
is the only component that depends on the train-
ing corpus. Figure 2 describes the algorithm for
computing entailment on the basis of the SDPs.
The following subsections illustrate the identifi-
cation of entailment relation for affirmative sen-
tences and negated sentences.
Figure 2: Algorithm for computing entailment.
3.1 Entailment on Affirmative Sentences
Affirmative sentences use three types of entail-
ment patterns. The switch baseline and hyponym
patterns works in this way: If two sentences are
matched by the same SDP, and the difference be-
tween them is that the second one contains a hy-
pernym on the same syntactic position, then the
first one is entailed by the second (i.e. ENTAIL-
MENT). If the two SDPs are such that the dif-
ference between them is that the second contains
a word which is not synonym, hypernym or hy-
ponym on the same syntactic position, then there is
no entailment between the two phrases (i.e. NEU-
TRAL). The entailment direction is from the sen-
tence that contains the hyponym toward the other
291
sentence. The antonym patterns check if the two
SDPs are the same, with the only difference be-
ing in the verb of the second sentence being an
antonym of the verb in the first sentence (i.e.
CONTRADICTION).
3.2 Entailment on Negative Sentences
As for negated sentences, we distinguish between
existential negative phrases (i.e. there is no or
there are no) and factual negative ones (presence
of a negative polarity word). An assumption re-
lated to each SDP is that it entails the existence
of any of the component of the pattern which can
be expressed by means of dedicated phrases. A
SDP of the kind "[Human] beat [Animal]", en-
tails both phrases, namely there is a [Human] and
there is a [Animal]. We call this set of associ-
ated existential phrases, Existential Assumptions
(EAs). This type of existential entailment obtained
through the usage of SDP has a direct consequence
for handling the ENTAILMENT, CONTRADIC-
TION and NEUTRAL types of entailment when
one of the phrases is negated. If the first phrase
belongs to the EA of the second one, then the
first phrase is entailed by the second phrase; if the
first phrase is an existential negation of a phrase
belonging to the EA set of the second phrase,
meaning that it contains the string there is/are no,
then the first one is a contradiction of the second
phrase; if neither the first phrase, nor its negation
belong to the EA set of the second phrase, then the
two sentences are neutral with respect to the en-
tailment. The general rule described in 3.1 applies
to these types of phrases as well: replacing a word
on the same syntactic slot inside a phrase that is
matched by a SDP leads to a CONTRADICTION
type of entailment, if the replacement is a hyper-
nym of the original word. Similarly, the approach
can be applied to factual negative phrases. The
scope of negation is considered to be the extension
of the SDP and thus the negative set of EAs.
4 Evaluation and Ranking
Table 1 illustrates the results for Pearson and
Spearman correlations for the relatedness subtask
on the test set. Table 2 reports the Accuracy values
for the entailment subtask on the test set.
Concerning the relatedness results our systems
ranked 11
th
out of 17 participating systems. Best
score of our system is reported in Table 1. One
of the main reason for the relatively low results
Team Pearson Spearman
ECNU_run1 (ranked 1
st
) 0.82795 0.76892
FBK-TR_run3 0.70892 0.64430
Table 1: Results for semantic relatedness subtask.
Team Accuracy
Illinois-LH_run1 (ranked 1
st
) 84.575
FBK-TR_run3 75.401
?FBK-TR_baseline 64.080
?FBK-TR_new 85.082
Table 2: Results for entailment subtask.
of the systems for this subtask concerns the fact
that it is designed for a general-level of texts (i.e.
compositionality is not taken into account).
As for the entailment subtask, our system
ranked 11
th
out of 18 participating systems. The
submitted results of the system are illustrated in
Table 2 and are compared against the best system,
our baseline system (?FBK-TR_baseline) as de-
scribed in Figure 1, and a new version of the par-
ticipating system after fixing some bugs in the sub-
mitted version due to the processing of the parser?s
output (?FBK-TR_new). The new version of the
system scores in the top provides a new state of the
art result, with an improvement of 10 points with
respect to our submitted system.
5 Conclusion and Future Work
This paper reports the description of our system,
FBK-TR, which implements a general SVM se-
mantic relatedness system based on distributional
features (LSA, LDA), knowledge-based related
features (WordNet and Wikipedia) and string over-
lap (LCS). On top of that, we added structural in-
formation at both semantic and syntactic level by
using SDP patterns. The system reached compet-
itive results in both subtasks. By correcting some
bugs in the entailment scripts, we obtained an im-
provement over our submitted systems as well as
for the best ranking system. We plan to improve
and extend the relatedness system by means of
compositional methods. Finally, the entailment
system can be improved by taking into account
additional linguistic evidences, such as the alter-
nation between indefinite and definite determiners,
noun modifiers and semantically empty heads.
292
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology (TIST), 2(3):27.
Patrick Hanks. 2008. Mapping meaning onto use: a
Pattern Dictionary of English Verbs. In Proceedings
of the AACL 2008.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In Proceedings of
the human language technology conference of the
NAACL, Companion Volume: Short Papers, pages
57?60.
Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse processes, 25(2-3):259?284.
Vladimir I Levenshtein. 1966. Binary Codes Capa-
ble of Correcting Deletions, Insertions and Rever-
sals. In Soviet Physics Doklady, volume 10, page
707.
M Marelli, L Bentivogli, M Baroni, R Bernardi,
S Menini, and R Zamparelli. 2014a. Semeval-2014
Task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation, August 23-24, 2014, Dublin,
Ireland.
M Marelli, S Menini, M Baroni, L Bentivogli,
R Bernardi, and R Zamparelli. 2014b. A SICK
cure for the evaluation of compositional distribu-
tional semantic models. In Proceedings of LREC
2014, Reykjavik (Iceland): ELRA.
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit.
David Milne and Ian H Witten. 2013. An Open-
Source Toolkit for Mining Wikipedia. Artificial In-
telligence, 194:222?239.
Ian Niles and Adam Pease. 2003. Mapping Word-
Net to the SUMO Ontology. In Proceedings of the
IEEE International Knowledge Engineering Confer-
ence, pages 23?26.
Ted Pedersen, Patwardhan Siddharth, and Michelizzi
Jason. 2004. Wordnet::Similarity: Measuring the
Relatedness of Concepts. In Proceedings of the
HLT-NAACL 2004.
Octavian Popescu, Elena Cabrio, and Bernardo
Magnini. 2011. Textual Entailment Using Chain
Clarifying Relationships. In Proceedings of the IJ-
CAI Workshop Learning by Reasoning and its Appli-
cations in Intelligent Question-Answering.
Octavian Popescu. 2013. Learning Corpus Patterns
Using Finite State Automata. In Proceedings of the
ICSC 2013.
Helmut Schmid. 1994. Probabilistic Part-of-Sspeech
Tagging Using Decision Trees. In Proceedings of
international conference on new methods in lan-
guage processing, volume 12, pages 44?49. Manch-
ester, UK.
293
Proceedings of the Fifth Law Workshop (LAW V), pages 143?151,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Annotating Events, Temporal Expressions and Relations in Italian: 
the It-TimeML Experience for the Ita-TimeBank 
 
 
Tommaso Caselli Valentina Bartalesi Lenzi Rachele Sprugnoli 
ILC ?A.Zampolli? - CNR  
Via G. Moruzzi, 1  
56124 Pisa 
CELCT 
Via della Cascata 56/c  
38123 Povo (TN) 
CELCT 
Via della Cascata 56/c  
38123 Povo (TN) 
caselli@ilc.cnt.it bartalesi@celct.it sprugnoli@celct.it 
Emanuele Pianta Irina Prodanof 
CELCT 
Via della Cascata 56/c  
38123 Povo (TN) 
ILC ?A.Zampolli? - CNR  
Via G. Moruzzi, 1  
56124 Pisa 
pianta@fbk.eu prodanof@ilc.cnr.it 
 
Abstract 
This paper presents the annotation 
guidelines and specifications which have 
been developed for the creation of the 
Italian TimeBank, a language resource 
composed of two corpora manually 
annotated with temporal and event 
information. In particular, the adaptation 
of the TimeML scheme to Italian is 
described, and a special attention is 
given to the methodology used for the 
realization of the annotation 
specifications, which are strategic in 
order to create good quality annotated 
resources and to justify the annotated 
items. The reliability of the It-TimeML 
guidelines and specifications is 
evaluated on the basis of the results of 
the inter-coder agreement performed 
during the annotation of the two corpora. 
? Introduction 
In recent years a renewed interest in temporal 
processing has spread in the NLP community, 
thanks to the success of the TimeML annotation 
scheme (Pustejovsky et al, 2003a) and to the 
availability of annotated resources, such as the 
English and French TimeBanks (Pustejovsky et 
al., 2003b; Bittar, 2010) and the TempEval 
corpora (Verhagen et al, 2010). 
The ISO TC 37 / SC 4 initiative 
(?Terminology and other language and content 
resources?) and the TempEval-2 contest have 
contributed to the development of TimeML-
compliant annotation schemes in languages 
other than English, namely Spanish, Korean, 
Chinese, French and Italian. Once the 
corresponding corpora will be completed and 
made available, the NLP community will benefit 
from having access to different language 
resources with a common layer of annotation 
which could boost studies in multilingual 
temporal processing and improve the 
performance of complex multilingual NLP 
systems, such as Question-Answering and 
Textual Entailment. 
This paper focuses on the annotation 
guidelines and specifications which have been 
developed for the creation of the Italian 
TimeBank (hereafter, Ita-TimeBank). The 
distinction between annotation guidelines and 
annotation specifications is of utmost 
importance in order to distinguish between the 
abstract, formal definition of an annotation 
scheme and the actual realization of the 
annotated language resource. In addition to this, 
documenting the annotation specification 
facilitates the reduplication of annotations and 
justify the annotated items. 
The paper is organized as follows: Section 2 
will describe in detail specific issues related to 
the temporal annotation of Italian for the two 
main tags of the TimeML annotation scheme, 
143
namely <EVENT> and <TIMEX3>. Section 3 
will present the realization of the annotation 
specifications and will document them. Section 
4 focuses on the evaluation of the annotation 
scheme on the Ita-TimeBank, formed by two 
corpora independently realized by applying the 
annotation specifications. Finally, in Section 5 
conclusions and extensions to the current 
annotation effort will be reported. 
Notice that, for clarity's sake, in this paper the 
examples will focus only on the tag (or attribute 
or link) under discussion. 
? It-TimeML: Extensions and 
Language Specific Issues 
Applying an annotation scheme to a language 
other than the one for which it was initially 
developed, requires a careful study of the 
language specific issues related to the linguistic 
phenomena taken into account (Im et al, 2009; 
Bittar, 2008). 
TimeML focuses on Events (i.e. actions, 
states, and processes - <EVENT> tag), 
Temporal Expressions (i.e. durations, calendar 
dates, times of day and sets of time - 
<TIMEX3> tag), Signals (e.g. temporal 
prepositions and subordinators - <SIGNAL> 
tag) and various kind of dependencies between 
Events and/or Temporal Expressions (i.e. 
temporal, aspectual and subordination relations - 
<TLINK>, <ALINK> and <SLINK> tags 
respectively). 
An ISO language-independent specification 
of TimeML is under development but it is still 
in the enquiry stage1. For this reason, in the 
following subsections we will mostly compare 
the Italian annotation guidelines with the latest 
version of the English annotation guidelines 
(TimeML Working group, 2010), focusing on 
the two main tags, i.e <EVENT> and 
<TIMEX3>, in Italian. 
2.1 The <EVENT> tag 
The <EVENT> tag is used to mark-up instances 
of eventualities (Bach, 1986). This category 
comprises all types of actions (punctual or 
durative) and states as well. With respect to 
                                                          
1
http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalog
ue_detail.htm?csnumber=37331 
previous annotations schemes (Katz and Arosio, 
2001, Filatova and Hovy, 2001, Setzer and 
Gaizauskas, 2001 among other), TimeML 
allows for annotating as Events not only verbs 
but also nouns, adjectives and prepositional 
phrases. 
In the adaptation to Italian, two annotation 
principles adopted for English, that is an 
orientation towards surface linguistic 
phenomena and the notion of minimal chunk for 
the tag extent, have been preserved without 
major modifications. The main differences with 
respect to the English version rely i.) in the 
attribute list; and ii.) in the attributes values. 
In Italian 12 core attributes apply with respect 
to the 10 attributes in English. The newly 
introduced attributes are MOOD and VFORM 
which capture key distinctions of the Tense-
Mood-Aspect (TMA) system of the Italian 
language. These two attributes are common to 
other languages, such as Spanish, Catalan, 
French and Korean. 
The MOOD attribute captures the contrastive 
grammatical expression of different modalities 
of presentation of an Event when realized by a 
verb. Annotating this attribute is important since 
grammatical modality has an impact on the 
identification of temporal and subordinating 
relations, and on the assessment of 
veridicity/factivity values. Mood in Italian is 
expressed as part of the verb morphology and 
not by means of modal auxiliary verbs as in 
English (e.g. through the auxiliary ?would?),. 
Thus, the solution to deal with this phenomenon 
adopted for English TimeML (where the main 
verb is annotated with the attribute 
MODALITY=?would?, see below) is not 
applicable in Italian unless relevant information 
is lost. The values of the MOOD attribute, as 
listed below, have been adapted to Italian and 
extended with respect to those proposed in the 
ISO-TimeML specification: 
 
? NONE: it is used as the default value and 
corresponds to the Indicative mood: 
(1.) Le forze dell?ordine hanno <EVENT 
? mood="NONE"> schierato </EVENT> 
3.000 agenti. [The police has deployed 
3,000 agents.] 
 
144
? CONDITIONAL: it signals the conditional 
mood which is used to speak of an Event 
whose realization is dependent on a certain 
condition, or to signal the future-in-the-
past: 
(2.) <EVENT ... mood="COND"> 
Mangerei </EVENT> del pesce. [I would 
eat fish.] 
  
? SUBJUNCTIVE: it has several uses in 
independent clauses and is required for 
certain types of dependent clauses. 
(3.) Voglio che tu te ne <EVENT ? 
mood="SUBJUNCTIVE">vada</EVENT> 
[I want you to go.] 
  
? IMPERATIVE: it is used to express direct 
commands or requests, to signal a 
prohibition, permission or any other kind of 
exhortation. 
 
The attribute VFORM is responsible for 
distinguishing between non-finite and finite 
forms of verbal Events. Its values are: 
 
? NONE: it is the default value and signals 
finite verb forms: 
(4.) Le forze dell?ordine hanno <EVENT 
? vForm="NONE">schierato</EVENT> 
3.000 agenti. [The police has deployed 
3,000 agents.] 
 
? INFINITIVE: for infinitive verb forms: 
(5.) Non ? possibile <EVENT ? 
vForm=''INFINITIVE''>viaggiare</EVEN
T>. [It?s not possible to travel.] 
 
? GERUND: for gerundive verb forms: 
(6.) Ha evitato l'incidente <EVENT ? 
vForm=''GERUND''> andando </EVENT> 
piano. [Driving slowly, he avoided the 
incident.] 
 
? PARTICIPLE: for participle verb forms: 
(7.) <EVENT ? vForm=?PARTICIPLE?> 
Vista </EVENT> Maria, se ne and?. 
[Having seen Maria, he left.] 
 
As for attribute values, the most important 
changes introduced for Italian in comparison 
with the English TimeML, are related to the 
ASPECT and MODALITY attributes. 
The ASPECT attribute captures standard 
distinctions in the grammatical category of 
aspect or Event viewpoint (Smith, 1991). In 
English TimeML it has the following values: i.) 
PROGRESSIVE; ii.) PERFECTIVE; iii.) 
PERFECTIVE_PROGRESSIVE, or iv.) NONE. 
The main differences with respect to the English 
guidelines concern the following points:  
i.) the absence of the value 
PERFECTIVE_PROGRESSIVE and  
ii.) the presence of the value 
IMPERFECTIVE, which is part of the ISO 
TimeML current definition.  
These differences are due to language specific 
phenomena related to the expression of the 
grammatical aspect in Italian and English and to 
the application of the TimeML surface oriented 
annotation philosophy. In particular, the 
assignment of the aspectual values is strictly 
determined by the verb surface forms. For 
instance, in English the verb form ?is teaching? 
requires the PROGRESSIVE value. On the 
other hand, the Italian counterpart of ?is 
teaching? can be realized in two ways: either by 
means of the simple present (insegna [s/he 
teaches]) or by means of a specific verbal 
periphrasis (sta insegnando [s/he is teaching]). 
In order to distinguish between these two verb 
forms, and to account also for other typical 
Romance languages tense forms, such as the 
Italian Imperfetto, the use of the additional 
IMPERFECTIVE value is necessary. Thus, 
insegna [s/he teaches], as well as the Imperfetto 
insegnava [s/he was teaching] are annotated as 
IMPERFECTIVE, whereas sta insegnando [s/he 
is teaching] is annotated as PROGRESSIVE. On 
the other hand, the absence of the 
PERFECTIVE_PROGRESSIVE value, used for 
English tense forms of the kind ?he has been 
teaching?, is due to the lack of Italian verb 
surface forms which may require its use. 
In English, modal verbs are not annotated as 
Events and the MODALITY attribute is 
associated to the main verb (the value of the 
attribute is the token corresponding to the modal 
verb). Unlike English modals, Italian modal 
verbs, such as potere [can/could; may/might], 
volere [want; will/would] and dovere 
[must/have to; ought to; shall/should], are to be 
145
considered similar to other lexical verbs in that 
it is possible to assign them values for tense and 
aspect. Consequently, each instance of Italian 
modal verbs will be annotated with the tag 
<EVENT>. The value of the MODALITY 
attribute is the lemma of the verb (e.g. dovere). 
A further language specific aspect concerns 
the annotation of verbal periphrases, that is 
special constructions with at least two verbs 
(and sometimes other words) that behave as a 
group like a single verb would. In Italian, it is 
possible to identify different instances of verbal 
periphrases, namely: 
 
? aspectual periphrases (example 8 below), 
which encode progressive or habitual 
aspect; 
? modal periphrases (example 9), which 
encode modality not realized by proper 
modal verbs;  
? phasal periphrases (example 10), which 
encode information on a particular phase in 
the description of an Event. 
 
Following Bertinetto (1991), in the last two 
cases, i.e. modal periphrases and phasal 
periphrases, both verbal elements involved 
should be annotated, while in the case of the 
aspectual periphrasis only the main verb (verb 
head) has to be marked; e.g.: 
(8.) Maria stava <EVENT ? 
ASPECT=?PROGRESSIVE?> mangiando. 
[Maria was eating] 
(9.) Il compito di matematica <EVENT ... 
MODALITY=?ANDARE?> va </EVENT> 
<EVENT ... > svolto </EVENT> per domani. 
[Maths exercises must be done for tomorrow]  
(10.) I contestatori hanno <EVENT ... 
CLASS=?ASPECTUAL?> iniziato </EVENT> 
a <EVENT> lanciare </EVENT> pietre. 
[Demonstrators started to throw stones.] 
Similarly to what proposed for English, in 
presence of multi-tokens realization of Events, 
two main annotation strategies have been 
followed: 
 
? in case the multi-token Event expression 
corresponds to an instance of a collocation 
or of an idiomatic expression, then only the 
head (verbal, nominal or other) of the 
expression is marked up;  
? in case the multi-token Event is realized by 
light verb expressions, then two separate 
<EVENT> tags are to be created both for 
the verb and the nominal/prepositional 
complement.  
2.2 The <TIMEX3> tag  
The TIMEX3 tag relies on and is as much 
compliant as possible with the TIDES TIMEX2 
annotation. The Italian adaptation of this 
annotation scheme is presented in Magnini et al 
(2006). The only difference concerns the 
annotation of articulated prepositions which are 
annotated as signals, while in the TIMEX2 
specifications they are considered as part of the 
textual realization of Temporal Expressions: 
(11a.) <TIMEX2 ?> nel 2011 </TIMEX2> 
[in 2011] 
(11b.) <SIGNAL ?> nel </SIGNAL> 
<TIMEX3?>2011</TIMEX3> [in 2011] 
On the other hand, with respect to the 
TIMEX3 annotation of other languages such as 
English, we decided to follow the TIMEX2 
specification by annotating many adjectives as 
Temporal Expressions (e.g. recente [recent], ex 
[former]) and including modifiers like che 
rimane in l?anno che rimane [the remaining 
year] into the extent of the TIMEX3 tag since it 
is essential for the normalization of temporal 
expressions. 
3 From Annotation Guidelines to 
Specifications ?
As already stated, the annotation guidelines 
represent an abstract, formal level of description 
which, in this case, is mainly based on a detailed 
study of the relevant linguistic levels. Once the 
guidelines are applied to real language data, 
further issues arise and need to be tackled. This 
section focuses on a method for developing 
annotation specifications. Annotation 
specifications are to be seen as the actual 
realization of the annotation guidelines. The 
identification and distinction of annotation 
guidelines from annotation specification is of 
major importance as it is to be conceived as a 
new level of Best Practice for the creation of 
146
semantically annotated Language Resources 
(Calzolari and Caselli, 2009). 
The process of realization of the annotation 
specifications is strategic both to realize good 
quality annotated resources and to justify why 
certain textual items have to be annotated. As 
for the It-TimeML experience we will illustrate 
this process by making reference and reporting 
examples for two tags, namely for the 
<EVENT> and the <TLINK> tags. 
As a general procedure for the development 
of the annotation specifications, we have taken 
inspiration from the DAMSL Manual (Core and 
Allen, 1997). Different decision trees have been 
created for each task. For instance, for the 
annotation of the <EVENT> tag, four different 
decision trees have been designed for each POS 
(i.e. nouns, verbs, adjectives and prepositional 
phrases) which could be involved in the 
realization of an Event. In particular, the most 
complex decision tree is that developed for noun 
annotation. The identification of the eventive 
reading of nouns has been formalized into a 
discrimination process of different properties: 
firstly superficial properties are taken into 
consideration, i.e. whether a morphologically 
related verb exists or not, and whether the noun 
co-occurs with special verb predicates (for 
instance aspectual verbs such as iniziare [to 
start] or light verbs such as fare [to do]); then, 
deeper semantic properties are analyzed, which 
involve other levels such as word sense 
disambiguation and noun classification (e.g. 
whether the noun is a functional or an 
incremental one). 
Other decision trees have been improved to 
avoid inconsistencies in Event classification. 
For instance, the identification of Reporting 
Events showed to be problematic because of the 
vague definition adopted in the guidelines. A 
Reporting Event is a giving information speech 
act in which a communicator conveys a message 
to an addressee. To help annotators in deciding 
whether an event is a Reporting one, the 
annotation specifications suggest to rely on 
FrameNet as a starting point (Baker, et al 
1998). More specifically, an Italian lexical unit 
has been classified as Reporting if it is the 
translation equivalent of one of the lexical units 
assigned to the Communication frame, which 
has Message as a core element. Among the 
frames using and inherited from the 
Communication frame, only the ones having the 
Message as a core element and conveying a 
giving information speech act have been 
selected and the lexical units belonging to them 
have been classified as Reporting Events: e.g. 
urlare [to scream] from the 
Communication_noise frame, sottolineare [to 
stress] from the Convey_importance frame, 
dichiarare [to declare] from the Statement 
frame. 
Similarly, for the identification of TLINKs, a 
set of decision trees has been developed to 
identify the conditions under which a temporal 
relation is to be annotated and a method to 
decide the value of the reltype attribute. For 
instance, the annotation of temporal relations 
between nominal Events and Temporal 
Expressions in the same sentence is allowed 
only when the Temporal Expression is realized 
either by an adjective or a prepositional phrase 
of the form ''di (of) + TEMPORAL 
EXPRESSION'' e.g.: 
(12.) La <EVENT eid=''e1'' ... > riunione 
</EVENT> <SIGNAL sid=''s1'' ... > di 
</SIGNAL> <TIMEX3 tid=''t1'' ... > ieri 
</TIMEX3> [yesterday meeting] 
<TLINK lid=''l1'' eventInstanceID=''e01'' 
relatedToTime=''t01'' signalID="s1" 
relType=''IS_INCLUDED''/> 
In addition, decision trees based on the idea 
that signals provide useful information to 
TLINK classification have been used to assign 
the reltype value to TLINKs holding between a 
duration and an Event. For example, the pattern 
?EVENT + tra (in) + DURATION? identifies 
the value AFTER, while the pattern ?EVENT + 
per (for) + DURATION? is associated with the 
value MEASURE. 
(13.) Il pacco <EVENT eid=''e1'' ... >arriver? 
</EVENT> <SIGNAL sid=''s1'' ... > tra 
</SIGNAL> <TIMEX3 tid=''t1'' ... > due giorni 
</TIMEX3> [the package will arrive in two 
days] 
<TLINK lid=''l1'' eventInstanceID=''e1'' 
relatedToTime=''t1'' signalID="s1" 
relType=''AFTER?/> 
(14.) Sono stati <EVENT eid=''e1'' ... > 
sposati </EVENT> <SIGNAL sid=''s1'' ... > per 
</SIGNAL> <TIMEX3 tid=''t1'' ... > dieci anni 
147
</TIMEX3> [they have been married for ten 
years] 
<TLINK lid=''l1'' eventInstanceID=''e1'' 
relatedToTime=''t1'' signalID="s1" 
relType=''MEASURE?/> 
The advantages of this formalization are 
many. The impact of the annotators' subjectivity 
is limited, thus reducing the risk of 
disagreement. Moreover, trees can then be 
easily used either as features for the 
development of a automatic learner or as 
instructions in a rule-based automatic annotation 
system. 
? Evaluating Annotations 
Two corpora have been developed in parallel 
following the It-TimeML annotation scheme, 
namely the CELCT corpus and the ILC corpus. 
Once these two corpora will be completed and 
released, they will form the Italian TimeBank 
providing the NLP community with the largest 
resource annotated with temporal and event 
information (more than 150K tokens). 
In this section, the two corpora are briefly 
described and the results of the inter-coder 
agreement (Artstein and Poesio, 2008) achieved 
during their annotation are compared in order to 
evaluate the quality of the guidelines and of the 
resources. 
The CELCT corpus has been created within 
the LiveMemories project2 and it consists of 
news stories taken from the Italian Content 
Annotation Bank (I-CAB, Magnini et al, 
2006). More than 180,000 tokens have been 
annotated with Temporal Expressions and 
more than 90,000 tokens have been annotated 
also with Events, Signals and Links. The 
Brandeis Annotation Tool3 (BAT) has been 
used for the pilot annotation and for the 
automatic computation of the inter-coder 
agreement on the extent and the attributes of 
Temporal Expressions, Events and Signals. 
After the pilot annotation, the first prototype of 
the CELCT Annotation Tool (CAT) has been 
used to perform the annotation and to compute 
the inter-coder agreement on Links. For what 
concern the annotation effort, the work on 
                                                          
2
 http://www.livememories.org 
3
 http://www.timeml.org/site/bat/ 
Temporal Expressions, Events and Signals 
involved 2 annotators while 3 annotators have 
been engaged in the annotation of Links. The 
annotation started in January 2010 and required 
a total of 1.3 person/years. Table 1 shows the 
total number of annotated markables together 
with the results of the inter-coder agreement on 
tag extent performed by two annotators on a 
subset of the corpus of about four thousand 
tokens. For the annotation of Event and Signal 
extents, statistics include average precision and 
recall and Cohen? kappa, while the Dice 
Coefficient has been computed for the extent of 
Links and Temporal Expressions. 
 
Markable # Agreement 
TIMEX3 4,852 Dice=0.94 
EVENT 17,554 K=0.93 P&R=0.94 
SIGNAL 2,045 K=0.88 P&R=0.88  
TLINK 3,373 Dice=0.86 
SLINK 3,985 Dice=0.93 
ALINK 238 Dice=0.90 
Table 1: Annotated markables and results of 
the inter-coder agreement on tag extent4 
 
Table 2 provides the value of Fleiss? kappa 
computed for the annotation of Temporal 
Expression, Event and Link attributes. 
 
Tag and attribute Agreement-Kappa 
TIMEX3.type  1.00 
TIMEX3.value 0.92 
TIMEX3.mod 0.89 
EVENT.aspect  0.96  
EVENT.class  0.87  
EVENT.modality  1.00  
EVENT.mood  0.90  
EVENT.polarity  1.00  
EVENT.pos  1.00  
EVENT.tense  0.94  
EVENT.vform  0.98  
TLINK.relType 0.88 
SLINK.relType 0.93 
ALINK.relType 1.00 
Table 2: Inter-coder agreement on 
attributes 
                                                          
4 Please note that the number of annotated Temporal 
Expressions is calculated on a total of 180,000 tokens, 
while the number of Events, Signals and Links is 
calculated on more than 90,000 tokens. 
148
The ILC corpus is composed of 171 
newspaper stories collected from the Italian 
Syntactic-Semantic Treebank, the PAROLE 
corpus and the web for a total of 68,000 
tokens (40,398 tokens are freely available, the 
remaining are available with restrictions). The 
news reports were selected to be comparable 
in content and size to the English TimeBank 
and they are mainly about international and 
national affairs, political and financial subject. 
The annotation of Temporal Expressions, 
Event extents and Signals has been completed 
while the annotation of Event attributes and 
LINKs is a work in progress. A subset of the 
corpus has been used as data set in the 
TempEval-2 evaluation campaign organized 
within SemEval-2 in 2010. So far the 
annotation has been performed thanks to eight 
voluntary students under the supervision of 
two judges using BAT. The annotation started 
in March 2009 and is requiring a total of 3 
person/years. Table 3 reports the total number 
of Temporal Expressions, Events, Signals and 
TLINKs together with the results of the inter-
coder agreement on tag extent performed on 
about 30,000 tokens. To measure the 
agreement on tag extents, average precision 
and recall and Cohen? kappa have been 
calculated. The annotation of Temporal Links 
has been divided into three subtasks: the first 
subtask is the relation between two Temporal 
Expressions, the second is the relation 
between an Event and a Temporal Expression, 
the third regards the relation between two 
Events. 
 
Markable # Agreement 
TIMEX3 2,314 K=0.95 P&R= 0.95 
EVENT 10,633 K=0.87 P&R= 0.86 
SIGNAL 1,704 K=0.83 P&R= 0.84 
 
T
L
I
N
K 
TIMEX3?
TIMEX3 
353 K=0.95 
EVENT?
TIMEX3 
512 K=0.87 
EVENT?
EVENT 
1,014 in progress 
Table 3: Annotated markables and results of 
the inter-coder agreement on tag extent 
 
The values of Fleiss? kappa computed for 
the assignment of attribute values are 
illustrated in Table 4. 
 
Tag and attribute Agreement ? Kappa 
TIMEX3.type  0.96 
TIMEX3.value 0.96 
TIMEX3.mod 0.97 
EVENT.aspect  0.93  
EVENT.class  0.82  
EVENT.modality  0.92  
EVENT.mood  0.89  
EVENT.polarity  0.75  
EVENT.pos  0.95  
EVENT.tense  0.97  
EVENT.vform  0.94  
TLINK.relType in progress 
Table 4: Annotated TLINKs and results of the 
inter-coder agreement 
 
Given the data reported in the above tables, 
it is possible to claim that the results of the 
inter-coder agreement are good and 
comparable beyond the different annotation 
method used to develop the two corpora. So 
far, the ILC corpus has been annotated 
without time constraints by several annotators 
with varying backgrounds in linguistics using 
BAT. With this web-based tool, each file has 
been assigned to many annotators and an 
adjudication phase on discrepancies has been 
performed by an expert judge. As required by 
BAT, the annotation has been divided into 
many annotation layers so each annotator 
focused only on a specific set of It-TimeML 
tags. On the other hand, few expert annotators 
have been involved in the development of the 
CELCT corpus interacting and negotiating 
common solutions to controversial 
annotations. With respect to BAT, the CELCT 
Annotation Tool is stand-alone and it does not 
require neither the parallel annotation of the 
same text, nor the decomposition of 
annotation tasks allowing to have flexibility in 
the annotation process and a unitary view of 
all annotation layers. These features are 
helpful when working with strict project 
deadlines. 
A comparison with the inter-coder agreement 
achieved during the annotation of the English 
TimeBank 1.2 (Pustejovsky et al, 2006a), 
shows that the scores obtained for the CELCT 
149
and the ILC corpora are substantially higher in 
the following results: (i) average precision and 
recall on the identification of tag extent (e.g. 
0.83 vs. 0.95 of ILC Corpus and 0.94 of CELCT 
Corpus for TIMEX3; 0.78 vs. 0.87 of ILC 
Corpus and 0.93 of CECLT Corpus); (ii) kappa 
score on Event classification (0.67 vs. 0.82 of 
ILC Corpus and 0.87 of the CELCT Corpus); 
(iii) kappa score on TLINK classification (0.77 
vs. 0.86 of CELCT Corpus). 
The similarity of the agreement results among 
the three resources and the improvement of the 
scores obtained on the CELCT and the ILC 
corpora with respect to the English TimeBank 
1.2, can be taken as an indication of the quality 
and coverage of the It-TimeML annotation 
guidelines and specifications. Annotators 
showed to perform consistently demonstrating 
the reliability of the annotation scheme. 
? Conclusions and Future Works 
This paper reports on the creation of a new 
semantic resource for Italian which has been 
developed independently but with a joint effort 
between two different research institutions. The 
Ita-TimeBank will represent a large corpus 
annotated with information for temporal 
processing which can boost the multilingual 
research in this field and represent a case study 
for the creation of semantic annotated resources. 
One of the most interesting point of this work 
is represented by the methodology followed for 
the development of the corpora: in addition to 
the guidelines, annotation specifications have 
been created in order to report in detail the 
actual choices done during the annotation. This 
element should be pushed forward in the 
community as a new best practice for the 
creation of good quality semantically annotated 
resources. 
The results obtained show the reliability of 
the adaptation of the annotation guidelines to 
Italian and of the methodology used for the 
creation of the resources. 
Future works will concentrate in different 
directions, mainly due to the research interests 
of the two groups which have taken part to this 
effort but they will be coordinated. 
An interesting aspect which could be 
investigated is the annotation of the anaphoric 
relations between Events. This effort could be 
done in a more reliable way since the primary 
linguistic items have been already annotated. 
Moreover, this should boost research in the 
development of annotation schemes which could 
be easily integrated with each other without 
losing descriptive and representational 
information for other language phenomena. 
Another topic to deepen regards the definition 
of the appropriate argument structure in It-
TimeML in order to annotate relations between 
entities (e.g. persons and organizations) and 
Events in which they are involved (Pustejovsky 
et al, 2006b). 
As regards the distribution of the Ita-
TimeBank, the resource will soon be available 
in an in-line format. In order to integrate the 
temporal annotation with other linguistic 
annotations, a standoff version of the Ita-
TimeBank needs to be developed. When this is 
made available, we plan to merge the manual 
annotation of temporal and event information 
with other types of linguistic stand-off 
annotations (i.e. tokenization, lemma, PoS, 
multi-words, various kinds of named entities) 
which are already available for the I-CAB 
corpus.  
In order to encourage research on systems 
capable of temporal inference and event-based 
reasoning, the Ita-TimeBank could be used as 
gold standard within specific evaluation 
campaigns as the next TempEval initiative. 
Finally, the use of crowdsourcing will be 
explored to reduce annotation effort in terms of 
financial cost and time. The most difficult 
challenge to face will be the splitting of a 
complicated annotation scheme as It-TimeML 
into simple tasks which can be effectively 
performed by not expert contributors. 
Acknowledgments 
The development of the CELCT corpus has 
been supported by the LiveMemories project 
(Active Digital Memories of Collective Life), 
funded by the Autonomous Province of Trento 
under the Major Projects 2006 research 
program. We would like to thank Alessandro 
Marchetti, Giovanni Moretti and Marc 
Verhagen who collaborated with us in 
processing and annotating the CELCT corpus. 
150
References 
Andr? Bittar. 2008. Annotation des informations 
temporelles dans des textes en fran?ais,. In 
Proceedings of RECITAL 2008, Avignon, France. 
Andr? Bittar. 2010. Building a TimeBank for French: 
A Reference Corpus Annotated According to the 
ISO-TimeML Standard. PhD Thesis. 
Andrea Setzer and Robert Gaizauskas.2001. A Pilot 
Study On Annotating Temporal Relations In Text. 
In: Proceedings of the ACL 2001 Workshop on 
Temporal and Spatial Information Processing. 
Bernardo Magnini, Emanuele Pianta, Christian 
Girardi, Matteo Negri, Lorenza Romano, Manuela 
Speranza, Valentina Bartalesi Lenzi and Rachele 
Sprugnoli. 2006. I-CAB: the Italian Content 
Annotation Bank. In Proceedings of LREC 2006, 
Genova, Italy. 
Bernardo Magnini, Matteo Negri, Emanuele Pianta, 
Manuela Speranza, Valentina Bartalesi Lenzi, and 
Rachele Sprugnoli. 2006. Italian Content 
Annotation Bank (I-CAB): Temporal Expressions 
(V.2.0). Technical Report, FBK-irst. 
Carlota S. Smith. 1991. The Parameter of Aspect. 
Kluwer, Dordrecht. 
Collin F., Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In: 
Proceedings of the COLING-ACL, pages 86-90. 
Montreal, Canada. 
Elena Filatova and Eduard Hovy. 2001. Assigning 
Time-Stamps To Event-Clauses. In: Proceedings 
of the ACL 2001 Workshop on Temporal and 
Spatial Information Processing. 
Emmon Bach. 1986. The algebra of events. 
Linguistics and Philosophy, 9, 5?16. 
Graham Katz and Fabrizio Arosio. 2001. The 
Annotation Of Temporal Information In Natural 
Language Sentences. In: Proceedings of the ACL 
2001 Workshop on Temporal and Spatial 
Information Processing. 
ISO: Language Resource Management ? Semantic 
Annotation Framework (SemAF) - Part 1: Time 
and Events. Secretariat KATS, August 2007. ISO 
Report ISO/TC37/SC4 N269 version 19 (ISO/WD 
24617-1). 
James Pustejovsky, Jessica Littman and Roser Saur?. 
2006b. Argument Structure in TimeML. In: 
Graham Katz, James Pustejovsky and Frank 
Schilder (eds.) Dagstuhl Seminar Proceedings. 
Internationales Begegnungs- und 
Forschungszentrum (IB-FI), Schloss Dagstuhl, 
Germany. 
James Pustejovsky, Jessica Littman, Roser Saur?, and 
Marc Verhagen. 2006a. TimeBank 1.2 
Documentation. 
http://timeml.org/site/timebank/documentation-
1.2.html 
James Pustejovsky, Jos? Casta?o, Robert Ingria, 
Roser Saur?, Robert Gaizauskas, Andrea Setzer 
and Graham Katz. 2003a. TimeML: Robust 
Specification of Event and Temporal Expressions 
in Text. In: Proceedings of IWCS-5, Fifth 
International Workshop on Computational 
Semantics. 
James Pustejovsky, Patrick Hanks, Roser, Saur?, 
Andrew See, Robert Gaizauskas, Andrea Setzer, 
Dragomir Radev, Beth Sundheim, David Day,Lisa 
Ferro, and Marcia Lazo. 2003b. The TIMEBANK 
corpus. In: Proceedings of Corpus Linguistics 
2003, pages 647-656. 
Marc Verhagen, Roser Saur?, Tommaso Caselli and 
James Pustejovsky. 2010. SemEval-2010 Task 13: 
TempEval-2. In: Proceedings of the 5th 
International Workshop on Semantic Evaluation. 
Mark G. Core and James F. Allen. 1997. Coding 
Dialogs with the DAMSL Annotation Scheme. In: 
Working Notes of AAAI Fall Symposium on 
Communicative Action in Humans and Machines. 
Nicoletta Calzolari, and Tommaso Caselli 2009. 
Short Report on the FLaReNet / SILT Workshop 
and Panel on Semantic Annotation, TR-ILC-CNR. 
Pier Marco Bertinetto. 1991. Il verbo. In: R. L. and 
G. Salvi (eds.) Grande Grammatica Italiana di 
Consultazione, volume II, pages 13-161. Il 
Mulino. 
Ron Artstein and Massimo Poesio. Inter-coder 
agreement for computational linguistics. 
Computational Linguistics, pages 555?596, 2008. 
Seohyun Im, Hyunjo You, Hayun Jang, Seungho 
Nam, and Hyopil Shin. 2009. KTimeML: 
Specification of Temporal and Event Expressions 
in Korean Text. In: Proceedings of the 7th 
workshop on Asian Language Resources in 
conjunction with ACL-IJCNLP 2009, Suntec City, 
Singapore. 
TimeML Working Group. 2010. TimeML 
Annotation Guidelines version 1.3.Manuscript, 
Brandeis University. 
151
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 153?160,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
EMOCause: An Easy-adaptable Approach to Emotion Cause Contexts 
 Irene Russo  Tommaso Caselli Francesco Rubino ILC ?A.Zampolli? ? CNR  Via G. Moruzzi, 1- 56124 Pisa {irene.russo}{tommaso.caselli}{francesco.rubino}@ilc.cnr.it Ester Boldrini  Patricio Mart?nez-Barco DSLI ? University of Alicante Ap. de Correos, 99 ? 03080 Alicante {eboldrini}{patricio}@dlsi.ua.es    Abstract 
In this paper we present a method to automatically identify linguistic contexts which contain possible causes of emotions or emotional states from Italian newspaper articles (La Repubblica Corpus). Our methodology is based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge on emotional states and emotion eliciting situations. Our approach has been evaluated with respect to manually annotated data. The results obtained so far are satisfying and support the validity of the methodology proposed. 
1 Introduction As it has been demonstrated in Balahur et al (2010), mining the web to discriminate between objective and subjective content and extract the relevant opinions about a specific target is today a crucial as well as a challenging task due to the growing amount of available information.  Opinions are just a part of the subjective content, which is expressed in texts. Emotions and emotional states are a further set of subjective data. Natural Language is commonly used to express emotions, attribute them and, most importantly, to indicate their cause(s).  Due to the importance of linking the emotion to its cause, a recent subtask of Sentiment Analysis 
(SA) consists in the detection of the emotion cause event (ECE, Lee et al, 2010; Chen et al, 2010) and focuses on the identification of the phrase (if present, as in 1 in bold) mentioning the event that is related to the emotional state (in italics):   (1) Non poteva mancare un accenno alla strage di Bologna, che costringe l' animo a infinita vergogna. [There was a mention of Bologna massacre,   that forces us to feel ashamed.]  This kind of information is extremely interesting, since it can provide pragmatic knowledge about content words and their emotional/subjective polarity and consequently it can be employed for building up useful applications with practical purposes.  The paper focuses on the development of a method for the identification of Italian sentences which contain an emotion cause phrase. Our approach is based on the interplay between linguistic patterns which allow the retrieval of emotion ? emotion cause phrase couples and on the exploitation of an associated incremental repository of commonsense knowledge about events which elicit emotions or emotional states. The methodology is only partially language dependent and this approach can be easily extended to other languages such as Spanish. The repository is one of the main results of this work. It allows the discovery of pragmatic knowledge associated with various content 
153
words and can assign them a polarity value which can be further exploited in more complex SA and Opinion Mining tasks. The present paper is structured as follows. Section 2 shortly describes related work and state of the art on this task. Section 3 focuses on the description of the methodology. Section 4 describes the annotation scheme and the corpus used for the creation of the test set. Section 5 reports on the experiments and their results. Conclusions and future works are described in Section 6. 1 Related Works Emotional states are often triggered by the perception of external events (pre-events) (Wierzbicka, 1999). In addition to this, emotional states can also be the cause of events (post-events; Chun-Ren, 2010). This suggests to consider emotional states as a pivot and structure the relations between emotional states and related events as a tri-tuple of two pairs:  (2) <<pre-events, emotional state> <emotional state, post-event>>  This study focuses on the relationship between the first pair of the tri-tuple, namely pre-events (or ECE), and emotional states.  Previous works on this task have been carried out for Chinese (Lee et al, 2009, Chen et al, 2009, Lee et al, 2010). ECE can be explicitly expressed as arguments, events, propositions, nominalizations and nominals. Lee et al(2010) restrict the definition of ECE as the immediate cause of the emotional state which does not necessarily correspond to the actual emotional state trigger or what leads to the emotional state.  Their work considers all possible linguistic realization of EKs (nouns, verbs, adjectives, prepositional phrases) and ECEs.  On the basis of an annotated corpus, correlations between emotional states and ECEs have been studied in terms of different linguistic cues (e.g. position of the cause events, presence of epistemic markers...) thus identifying seven groups of cues. After that, they have been implemented in a rule-based system, which is able to identify: i.) the EK; ii.) the ECE and its position (same sentence as the EK, previous sentence with 
respect to the EK, following sentence with respect to the EK) and (iii.) the experiencer of the emotional state(s).The system evaluation has been performed on the annotated corpus in two phases: firstly, identifying those sentences containing a co-occurrence of EK and ECE; secondly, for those contexts where an EK and ECE co-occurs, identifying the correct ECE. Standard Precision, Recall and F-measure have been used. The baseline is computed by assuming that the first verb on the left of the EK is the ECE. The system outperforms the baseline f-score by 0.19. Although the results are not very high, the system accuracy for the detection of ECEs is reported to be three times more accurate than the baseline. 2 Emotional states between linguistic patterns and commonsense knowledge The work of Lee et al (2010) represents the starting point for the development of our method. We depart from their approach in the following points: i.) use of data mining techniques (clustering plus a classifier) to automatically induce the rules for sentential contexts in which an event cause phrase is expressed; and ii.) exploitation of a commonsense repository of EK - eliciting ECE noun couples for the identification of the correct ECE noun. The remaining of this section will describe in details the creation of the repository and the methodology we have adopted. 2.1 A source for commonsense knowledge of EKs and ECEs in Italian Recently crowdsourcing techniques that exploit the functionalities of the Web 2.0 have been used in AI and NLP for reducing the efforts, costs and time for the creation of Language Resources. We have exploited the data from an on-line initiative launched in December 2010 by the Italian newspaper ?Il Corriere della Sera? which asked its readers to describe the year 2010 with 10 words. 2,378 people participated in the data collection for a total of 22,469 words. We exploited these data to identify preliminary couples of emotional states and cause events, and thus create a repository of affective commonsense knowledge, by extracting all 
154
bigrams realized by nouns for a total of 18,240 couples noun1-noun2. After this operation, an adapted Italian version of WN-Affect (Strapparava ? Valitutti, 2004) obtained by means of mapping procedures through MultiWordNet (MWN) has been applied to each item of the bigrams. By means of a simple query, we have extracted all bigrams where at least one item has an associated sense corresponding to the ?emotion? category in WN-Affect. We have applied WN-Affect again to these results and extracted only those bigrams where the unclassified item corresponded to the WN-Affect label of ?emotion eliciting situation?. Finally, two lists of keywords have been obtained: one denoting EKs (133 lemmas) and the other denoting possible ECEs associated with a specific EK. The possible ECEs have been extended by exploiting MWN synsets and lexical relations of similar-to, pertains-to, attribute and is-value-of. We have filtered the set of ECE keywords by selecting only those nouns whose top nodes uniquely belongs to the following ontological classes, namely: event, state, phenomenon, and act. After this operation we have 161 nominal lemmas of possible ECEs.  2.2 Exploiting the repository for pattern induction The preliminary version of the repository of EK - ECE couples has been exploited in order to identify relevant syntagmatic patterns for the detection of nominal ECEs. The pattern induction phase has been performed on a parsed version of a large corpus of Italian, the La Repubblica Corpus (Baroni et al, 2004).  We have implemented a pattern extractor that takes as input the couples of the seed words from the commonsense repository and extracted all combinations of EKs and its/their associated ECEs occurring in the same sentence, with a distance ranging from 1 to 8 possible intervening parts-of-speech. We have thus obtained 1,339 possible patterns. This set has been cleaned both on the basis of pattern frequencies and with manual exploration. In total 47 patterns were selected and were settled among the features for the clustering and classifier ensemble which will be exploited for the identification of the 
sentential contexts which may contain an emotion cause phrase (see Section 5 for details). 3 Developing a gold standard and related annotation scheme With the purpose of evaluating the validity and reliability of our approach, a reference annotated corpus (gold standard) has been created.  The data collection has been performed in a semi-automatic way. In particular, we have extracted from an Italian lexicon, SIMPLE/CLIPS (Ruimy et al, 2003), all nouns marked with semantic type ?Sentiment? to avoid biases for the evaluation and measure the coverage of the commonsense repository. The keywords have been used to query the La Repubblica Corpus and thus creating the corpus collection. We have restricted the length of the documents to be annotated to a maximum of three sentences, namely the sentence containing the emotion keyword, the one preceding it and the sentence immediately following. As a justification for this choice, we have assumed that causes are a local focus discourse phenomenon and should not be found at a long distance with respect to their effects (i.e. the emotion keyword). Finally, the corpus is composed by 6,000 text snippets for a total of 738,558 tokens.  The corresponding annotation scheme, It-EmoCause, is based on recommendations and previous experience in event annotation (ISO-TimeML), emotion event annotation (Lee et al, 2009, Chen et al, 2010), emotion and affective computing annotation (EARL1, the HUMAINE Emotion Annotation and Representation Language, EmotiBlog, Boldrini et al 2010). The scheme applies at two levels: phrase level and token level and it allows nested tags. Figure 1 reports the BNF description of the scheme.  Text consuming markables are  <emotionWord>, <causePhrase> and <causeEmotion> tags, which are responsible, respectively, for marking the emotion keyword, the phrase expressing the cause emotion event and the token expressing the cause emotion. The values of the attribute emotionClass is derived from Ekman                                                            1 http://emotion-research.net/earl 
155
(1972)'s classification and extended with the value UNDERSPECIFIED. This value is used as a cover term for all other types of emotion reducing disagreement and allowing further classifications on the basis of more detailed and different lists of emotions that each user can specify. Finally, the non-text consuming <EmLink> link puts in relation the cause emotion event or phrase with the emotion keyword.  entry ::= <emotionWord> <causePhrase>+ <ELink>*  <emotionWord> ::= ewid lemma emotionClass appraisalDimension, emotionHolder polarity comment ewid ::= ew<digit> lemma ::= CDATA emotionClass ::= HAPPINESS | ANGER | FEAR | SURPRISE| SADNESS| DISGUST |               UNDERSPECIFIED appraisalDimension ::= CDATA emotionHolder ::= CDATA polarity ::= POSITIVE | NEGATIVE comment ::= CDATA  <causePhrase> ::= epid <causeEmotion>+ epid ::= ep<digit> <causeEmotion> ::= eid lemma eid ::= e<digit> lemma ::= CDATA  <EmLink> ::= elid linkType emotionInstanceID causeEventInstanceID causePhraseID comment elid ::= el<digit> linkType ::= POSITIVE | NEGATIVE  relatedToEmotion ::= IDREF {relatedToEmotion ::= ewid} causeEventID ::= IDREF {causeEventID ::= eid} causePhraseID  ::= IDREF {causePhraseID ::= epid} comment ::= CDATA Figure 1 ? BNF description of the EmoContext Scheme  The annotation has been performed by two expert linguists and validated by a judge. The tool used for the annotation is the Brandeis Annotation Tool (BAT)2. The corpus is currently under annotation and we concentrated mainly on the development of a test set. Not all markables and attributes have been annotated in this phase.  
                                                           2 http://www.batcaves.org/bat/tool/ 
The inter-annotator agreement (IAA)3 on the detection of the cause event and the cause phrase are not satisfactory. To have reliable data, we have adopted a correction strategy by asking the annotators to assign a common value to disagreements. This has increased the IAA on cause emotion to K=0.45, and P&R= 0.46. A revision procedure of the annotation guidelines is necessary and annotation specifications must be developed so that the disagreement can be further reduced. Table 1 reports the figures about the annotated data so far.  It-EmoContext Corpus # of tokens 32,525 # of emotion keyword 356 # of cause emotion 84 # of causePhrase emotion  104 # emotion ? cause emotion couples 95 # of emotion ? cause phrase couples 121 Agreement on emotion keyword detection K = 0.91 P&R = 0.91 Agreement on cause emotion detection K = 0.34 P&R = 0.33 Agreement on causePhrase detection K = 0.21 P&R = 0.26 Table 1 - It-EmoContext Corpus Figures 4 Emotion cause detection: experiments and results In order to find out a set of rules for the detection of emotion cause phrase contexts, we experimented a combination of Machine Learning techniques, namely clustering and rule induction classifier algorithms. In particular, we want to exploit the output of a clustering algorithm as input to a rule learner classifier both available in the Weka platform (Witten and Frank, 2005). The clustering algorithm is the Expectation-Maximization algorithm (EM; Hofmann and Puzicha, 1998). The EM is an unsupervised algorithm, commonly used for model-based                                                            3 Cohen's Kappa, Precision and Recall have been used for computing the IAA. 
156
clustering and also applied in SA tasks (Takamura et al 2006). In this work, we equipped the EM clustering model with syntagmatic, lexical and contextual features. The clustering algorithm has been trained on 2,000 corpus instances of the potential EK - ECE couples of the repository from the La Repubblica corpus along with a three sentence context (i.e the sentence immediately preceding and that immediately following the sentence containing the EK). Four groups of features have been identified: the first set of features corresponds to a re-adaptation of the rules implemented in Lee et al (2010); the second set of features implements the 47 syntagmatic patterns that specifically codify the relation between the EK and the ECE (see Section 3.2); the last two set of features are composed, respectively, by a list of intra-sentential bigrams, trigrams and fourgrams for a total of 364 different part-of-speech sequences with the EK as the first element and by a list of 6 relevant collocational patterns which express cause-effect relationship between the  ECE and the EK, manually identified on the basis of the authors' intuitions. In Table 2 some examples of each group of features are reported4.   Group of feature Instance Re-adaptation of Lee et al, 2010's rules Presence of an ECE after the EK in the same sentence Syntagmatic patterns manually identified S E S | S E RI S | S V RI A S ... Bigrams, trigrams and fourgrams POS sequences S EA | S EA AP | S EA AP S  Relevant collocational patterns S A per RD/RI S ... Table 2 ? Features for the EM cluster.  We expected two data clusters, one which includes cause emotion sentential contexts where the EK and the emotion cause co-occurs in the same sentence and another where either 
                                                           4 The tags S, EA, RI and similar reported for the last three groups of features are abbreviations for the POS used by the parser. The complete list can be found at http://medialab.di.unipi.it/wiki/Tanl_POS_Tagset 
the emotion cause it is not present or it occurs in a different sentence (i.e. the one before the EK or in the one following it). In order to evaluate the goodness of the cluster configuration created by the Weka version of the EM algorithm, we have run different clustering experiments. The results of each clustering analysis have been passed to the Weka PART rule-induction classifier. The best results were those which confirmed our working hypothesis, i.e. two clusters. The first cluster contains 869 items while the second 1,131 items.  The PART classifier provided a total of 49 detection rules for the detection of EK ? ECE contexts. The classifier identifies the occurrence of a cause phrase in the same sentence but is not able to identify the noun which corresponds to the ECE. The evaluation of the classifier has been performed on the 121 couples of EK ? cause phrase of the test set. As we are aiming at spotting nominal causes of EKs, we have computed the baseline by considering as the correct phrase containing the ECE the first noun phrase occurring at the right of the emotion keyword and in the same sentence since this kind of ECEs tends to occur mostly at this position. In this way the baseline has an accuracy of  0.14 (only 33 NPs were correct over a total of 227 NPs at the right of the EKs). By applying the rules of the PART classifier, we have obtained an overall accuracy of 0.71, outperforming the baseline. As for the identification of the EK - cause phrase couples occurring in the same sentence, we computed standard Precision, Recall and F-measure. The results are reported in Table 3. The system tends to have a high precision (0.70) and a low recall (0.58).    Total Correct P R F EK ? cause phrase couple 121 85 0.70 0.58 0.63 Table 3 ? Evaluation of the classifier in detecting EF ? cause phrase couples.  After this, we tried to identify the correct nominal ECE in the cause phrase. Provided the reduced dimensions of the annotated corpus, no training set was available to train a further 
157
classifier. Thus, to perform this task we decided to exploit the commonsense repository. However, the first version of the repository is too small to obtain any relevant results. We enlarged it by applying two set of features (the syntagmatic patterns manually identified and the collocational patterns used for the clustering analysis). 4.1 Incrementing the repository and discovering EK ? ECE couples Our hypothesis is that the identification of the ECE(s) in context could be performed by looking for a plausible set of nouns which are associated with a specific EK and assumed to be its cause. This type of information is exactly the one contained in the repository described in Section 3.1. In order to work with a larger data set of ECE entries per emotion keyword, we have applied the syntagmatic patterns manually identified and the collocational patterns on two corpora: i.) La Repubblica and ii.) ItWaC5 (Baroni et al, 2009). For each EK - ECE couple identified we have kept track of the co-occurrence frequencies and computed the Mutual Information (MI). Frequency and MI are extremely relevant because they provide a reliability threshold for each couple of EK and ECE. In Table 4 we report some co-occurrences of the EK ?ansia? [anxiety] and ECEs.  ECE Frequency (La Repubblica Corpus) Mutual  Information crisi [crisis] 119 5,514 angoscia [anguish] 80 8.762 guerra [war] 185 6.609 pianificazione [planning] 1 4.117 ricostruzione [reconstruction] 19 5.630 Table 4- ECEs co-occurrences with EK ?ansia?[anxiety].  Each ECE has been associated to a probability measure of eventivity derived from MWN top                                                            5 http://wacky.sslmit.unibo.it 
ontological classes, obtained from the ratio between 1 and the sum of all top ontological classes associated to the ECE lemma. The top nodes ?event?, ?state?, ?phenomenon?, and ?act? have been considered as a unique top class by applying the TimeML definition of event6. This measure is useful in case more than one ECEs is occurring in the context in analysis as a disambiguation strategy. In fact, if more than one ECEs is present, that with the higher frequency, MI and eventivity score should be preferred.  Furthermore, to make the repository more effective and also to associate an emotional polarity to the ECEs (i.e. whether they have positive, negative or neutral values) we have further extended the set of information by exploiting WN-Affect 1.1. In particular we have associated each EK to its emotional category (e.g. despondency, resentment, joy) and its emotional superclass (e.g. positive-emotion, negative-emotion, ambiguous-emotion).  This extended version of the repository has been applied to identify the correct ECE noun for the 95 couples of EK ? ECE in the test set. We have splitted the whole set of EK ? ECE couples into two subgroups: i.) EK ? ECE couples occurring in the same sentence (82/95); and ii.) EK ? ECE couples occurring in different sentences (13/95). By applying the repository to the first group, we were able to correctly identify 50% (41/82) of the ECE nouns for each specific EK when occurring in the same sentence. Moreover, we applied the repository also to the EK ? ECE couples of the second group: a rough 30.76% (4/13) of the ECE occurring in sentences other that the one containing the EK can be correctly retrieved without increasing the number of false positives. This is possible thanks to the probability score computed by means of MWN top ontological classes, even if the number of annotated examples is too small to justify strong conclusions. 
                                                           6 To clarify, the ECE ?guerra? [war] has four senses in MWN. Three of them belong to the top ontological class of ?event? and one to ?state?. This possible ECE has 1 top ontological node, and its eventivity mesure is 1. 
158
5 Conclusions and future works  In this paper we describe a methodology based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge of EK ? ECE couples, which can be integrated into more complex systems for SA and Opinion Mining. The experimental results show that clustering techniques (EM clustering model) and a rule learner classifier (the PART classifier) can be efficiently combined to select and induce relevant linguistic patterns for the discovery of EK ? ECE couples in the same sentence. The information thus collected has been organized into the repository of commonsense knowledge about emotions and their possible causes. The repository has been extended by using corpora of varying dimensions (la Repubblica and ItWaC) and effectively used to identify ECEs of specific emotion keywords.  One interesting aspect of this approach is represented by the reduced manual effort both for the identification of linguistic patterns for the extraction of reliable information and for the maintenance and extension of specific language resources which can be applied also to domains other than SA. In addition to this, the method can be extended and applied to identify ECE realized by other POS, such as verbs and adjectives. As future works, we aim to extend the repository by extracting data from the Web and connecting it to SentiWordNet and WN-Affect. In particular, the connection to the existing language resources could be used to spot possible misclassifications and polarity values. Acknowledgments The authors want to thank the RSC Media Group. This work has been partially founded by the projects TEXTMESS 2.0 (TIN2009-13391-C04-01), Prometeo (PROMETEO/2009/199), the Generalitat valenciana (ACOMP/2011/001) and the EU FP7 project METANET (grant agreement n? 249119) References  Baccianella S., A. Esuli and F. Sebastiani. (2010) . SentiWordNet 3.0: An Enhanced Lexical Resource 
for Sentiment Analysis and Opinion Mining. In:  Proceedings of the 7th conference on International Language Resources and Evaluation (LREC 2010), Malta, May 2010 Balahur A., R. Steinberger, M.A. Kabadjov, V. Zavarella, E. van der Goot, M. Halkia, B. Pouliquen, J. Belyaeva. (2010). Sentiment Analysis in the News. In: Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC 2010), Malta, May 2010.  Baroni, M., Bernardini, S., Comastri, F., Piccioni, L., Volpi, A., Aston, G.,Mazzoleni, M. (2004). Introducing the ?la Repubblica? corpus: A large, annotated, TEI(XML)-compliant corpus of newspaper italian. In: Proceedings of the 4th International conference on Language Resources and Evaluation (LREC-04), Lisbon, May 2004.  Boldrini E, A. Balahur, P. Mart?nez-Barco and A. Montoyo. (2010). EmotiBlog: a finer-grained and more precise learning of subjectivity expression models. In: Proceedings of the Fourth Linguistic Annotation Workshop (LAW IV '10). Association for Computational Linguistics. Chen Y., S.Y.M. Lee, S. Li, and C. Huang. (2010) Emotion Cause Detection with Linguistic Constructions. In: Proceeding of the 23rd International Conference on Computational Linguistics (COLING 2010). Ekman, P. (1972). Universals And Cultural Differences In Facial Expressions Of Emotions.In: J. Cole (ed.), Nebraska Symposium on Motivation, 1971. Lincoln, Neb.: University of Nebraska Press, 1972. pp. 207- 283.3. Huang, C. (2010). Emotions as Events (and Cause as Pre-Events). Communication at the Chinese Temporal/discourse annotation workshop, Los Angeles, June 2010,. Lee S.Y.M., Y. Chen, C. Huang. (2010). A Text-driven Rule-based System for Emotion Cause Detection. In: Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text. Pianta, E., Bentivogli, L., Girardi, C. (2002). Multiwordnet: Developing and aligned multilingual database. In: Proceedings of the First International Conference on Global WordNet, Mysore, India, January 2002. Pustejovsky, J., Castao, J., Saur`?, R., Ingria, R., Gaizauskas, R., Setzer, A., Katz, G. (2003). TimeML: Robust specification of event and temporal expressions in text. In: Proceedings of 
159
the 5th International Workshop on Computational Semantics (IWCS-5). Ruimy, N., Monachini, M., Gola, E., Calzolari, N., Fiorentino, M.D., Ulivieri, M., Rossi, S. (2003). A computational semantic lexicon of italian: SIMPLE. In: Linguistica Computazionale XVIII-XIX, Pisa, pp. 821?64 Schroeder M., H. Pirker and M. Lamolle. (2006). First Suggestion for an Emotion Annotation and Representation Language. In: Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), Genoa, May 2006.  
Strapparava C. and A. Valitutti. (2004) WordNet-Affect: an affective extension ofWordNet". In: Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon, May 2004. Takamura H., I. Takashi, M. Okumura. (2006). Latent Variables Models for Semantic Orientation of Phrases. In: Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2006).  Wierzbicka, A. (1999) Emotion Across Languages and Cultures Diversity and Universals. Cambidge.CUP. 
160
