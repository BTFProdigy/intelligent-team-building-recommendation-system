Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 577?587, Dublin, Ireland, August 23-29 2014.
Cross-lingual Discourse Relation Analysis:
A corpus study and a semi-supervised classification system
Junyi Jessy Li
1
, Marine Carpuat
2
and Ani Nenkova
1
1
University of Pennsylvania, Philadelphia, PA 19104, USA
{ljunyi, nenkova}@seas.upenn.edu
2
National Research Council Canada, Ottawa, ON K1A 0R6, Canada
marine.carpuat@nrc.gc.ca
Abstract
We present a cross-lingual discourse relation analysis based on a parallel corpus with discourse
information available only for one language. First, we conduct a corpus study to explore dif-
ferences in discourse organization between Chinese and English, including differences in infor-
mation packaging, implicit/explicit discourse expression divergence, and discourse connective
ambiguities. Second, we introduce a novel approach to learning to recognize discourse relations,
using the parallel corpus instead of discourse annotation in the language of interest. Our result-
ing semi-supervised system reaches state-of-art performance on the task of discourse relation
detection, and outperforms a supervised system on discourse relation classification.
1 Introduction
The analysis of the way spans of text semantically connect with each other to create a coherent text has
a rich theoretical and empirical tradition (Mann and Thompson, 1988; Marcu, 1997; Di Eugenio et al.,
1997; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however,
labelled datasets were rare and rather small.
The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of
maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse
parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the
use of discourse connectives in English news text and have developed methods for the identification of
discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and
Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al.,
2014). Some have applied the insights and classifiers to standard natural language processing tasks such
as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal
dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012).
A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource
in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse
relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic
(Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012).
On the other hand, for the vast majority of languages, such well-annotated resource for discourse re-
lations is not available. In our work we carry the valuable annotations in the PDTB over to another
language?Chinese?using parallel corpora. Projecting information available in one language onto an-
other has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov,
2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and
Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior
work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives
(Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly
infers discourse relations using resources only available in another language.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
577
The goal of our work is not only to measure the accuracy with which discourse relations can be iden-
tified in another language without annotations beyond the PDTB, but also to catalog the differences in
discourse relation realization across different languages, Chinese and English in our case. We show
that the two languages vastly differ in how information is packaged into a sentence, which also leads
to differences in the implicit/explicit expression of discourse relations and the ambiguities in discourse
connectives. These differences challenge the currently accepted distinctions between syntax and dis-
course between the two languages for applications such as machine translation. Then we present our
semi-supervised learning algorithm to recognize explicit discourse relations in Chinese, relying solely
on discourse information available in English. For multiway classification, our system outperforms a
supervised system trained on the existing pilot dataset of discourse relations in Chinese (Zhou and Xue,
2012). In the task of binary classification for identifying specific discourse relations, the performance of
our system is within 4% accuracy of that of the supervised system for all but one relations.
2 Data
As our parallel corpus, we use the newswire portion of the GALE Chinese-English Word Alignment and
Tagging Training corpus (parts 1 and 2). The corpus contains 2,175 newswire articles, corresponding to
6,255 translation segments with 248,999 Chinese characters. These articles were translated into English
by human translators. Gold standard word alignments are available for this corpus. A minimal match
alignment approach (Li et al., 2010) was adopted for creating the gold standard, namely, alignments are
between an English word and only the necessary Chinese characters. We repurpose this resource created
for machine translation research for our cross-lingual discourse analysis. The availability of manual
alignments between Chinese discourse connectives and their English translation makes it possible to
conduct a reliable analysis by focusing on actual cross-lingual divergences, without noise introduced by
potential errors from automatic aligners.
1
We use a highly accurate supervised classifier for English explicit discourse relations (Pitler and
Nenkova, 2009)
2
to automatically annotate the English portion of the GALE parallel corpus. The classi-
fier was trained on the PDTB to identify discourse relations explicitly signaled by a set of 100 discourse
connectives such as however, because, while or for example. For each instance of the 100 words or ex-
pressions, the classifier predicts if the expression is used as a discourse connective or if the instance is
a non-discourse connective sense of the phrase or word. For each instance predicted to be a discourse
connective, the classifier identifies the discourse relation signaled by the connective: TEMPORAL, COM-
PARISON, CONTINGENCY or EXPANSION. In our work we predict the same five categories for Chinese
expressions which can serve as discourse connectives.
For evaluation and the study of discourse connective ambiguities, we use a development set from
the Chinese Discourse Treebank (CDTB) (Zhou and Xue, 2012) consisting of 170 documents
3
. In the
CDTB, an annotation style similar to the PDTB is applied on the texts from the Chinese Treebank corpus
(Xue et al., 2005). For a discourse connective, one of eight discourse relation senses is annotated. All
of these classes are subsumed by the four top-level relations in the PDTB. We map them to the PDTB
relation senses according to their definitions:
Alternative? Expansion; Causation? Contingency; Conditional? Contingency; Conjunction? Expansion; Contrast?
Comparison; Expansion? Expansion; Purpose? Contingency; Temporal? Temporal.
3 Information packaging characteristics
The notion of sentence in Chinese is very different from that in English. Punctuation marks were in-
troduced in the early 20
th
century; sentences resemble more a collection of related information than
structurally well-defined syntactic units as in English. In fact, commas are often ambiguous, signaling
1
While cross-lingual projection could be directly applied to automatic word alignments, discourse relation analysis raises
some specific challenges because the main target of analysis (discourse connectives) are function words, which do not have
as much of an impact on the final analysis in applications focusing on content words. As a result, we exclusively use manual
alignment links in this study, and will address issues raised by automatic alignments in future work.
2
The classifier is available at http://www.cis.upenn.edu/?epitler/discourse.html
3
This is an on-going annotation project. We are grateful to the authors for providing us with their valuable development set.
578
% data avg-length std-length
1-many 18.83 61.42 28.85
1-1 81.17 35.73 25.34
Table 1: Percentage, length and standard deviation of sentences for which one Chinese source sentence
is translated into one (1-1) or multiple (1-many) English sentences. Length is calculated based on the
number of Chinese characters.
either clausal subordination, coordination or end-of-sentence (as construed from an English-centric point
of view). Automatic systems have been developed to disambiguate the function of commas (Jin et al.,
2004; Xue and Yang, 2011). This is a rather interesting phenomenon for discourse processing, as the
English equivalents of Chinese sentences are in fact multi-sentential discourses in English.
The GALE corpus allows us to examine how often this mismatch of discourse organization occurs.
Here we look for Chinese source sentences that were translated into multiple English sentences by the
human translators. Consider the following example in which the corresponding clauses on both sides are
numbered and marked in square brackets:
source [????????????????????????]
1
?[???????????????????]
2
?[??
?????????????????]
3
?
ref [In recent years, new phrases such as ?disaster relief diplomacy? and ?disaster relief aid? have appeared constantly]
1
. [In
relation to the issue of disaster relief, all countries have been silently competing with one another and comparing offerings]
2
.
[Some countries are trying to establish various kinds of international alliance in the name of disaster relief]
3
.
In this example, the Chinese sentence packed the following related content into a single sentence:
the occurrence of the new phrases about disaster relief, the competition among the countries related
to disaster relief, and alliances in the name of disaster relief. The phrases expressing this information
are separated by the commas in the source Chinese sentence because they are about a single concept
?disaster relief?. However, this information needs to be partitioned into three different sentences, each
with different subjects, when translated to English.
In the GALE corpus, we identified 1,178 (out of total 6,255) source sentences with reference transla-
tions containing more than one sentence. In other words, sentence/discourse mismatch between Chinese
and English occurs for 18.83% of the data. Table 1 shows the portion of data involved in such mismatch,
with percentage, mean and standard deviation of source sentence length. Not surprisingly, Chinese sen-
tences that require multiple sentences in their English translation are much longer. These long sentences
are fairly common, which suggests that the difference in information packaging is highly prevalent and
could potentially affect key applications such as machine translation, where systems are trained on a
sentence to sentence basis.
We will return to the discussion of this mismatch later, when we discuss how English and Chinese also
appear to differ in the way discourse relations are signaled. Briefly, the issue is that relations that are
explicit in one language may become implicit in the other, easily inferred by the reader but not marked
by a discourse connective. Also, there is an increase in the sense ambiguity of discourse connectives
related to EXPANSION relations in Chinese.
4 Implicit and explicit relations
In this section, we present two other differences between the two languages related to discourse orga-
nization. One is the need for a discourse relation expressed implicitly in one language to be expressed
explicitly in another. The other is the difference of the ambiguity of discourse connectives across the two
languages. Before the discussion of these interesting asymmetries, we first present the method for direct
projection of discourse relations using the GALE gold standard alignments, which we use to gather a set
of explicit discourse connectives in Chinese.
4.1 Direct projection
Thus far we have available a parallel Chinese/English corpus, discourse connectives automatically tagged
with their senses on the English side and manual alignments of atomic units between English and Chi-
579
Comparison Contingency Expansion Temporal
CH/EN mismatch 63 109 360 195
all 551 469 1198 885
% data 11.43 23.24 30.05 22.03
Table 2: Numbers and percentages of Chinese/English implicit/explicit mismatches.
nese. So for each discourse connective in an English sentence, it is straightforward to identify the corre-
sponding expressions in the Chinese sentence following the gold standard alignments. Then the aligned
Chinese expression can be assigned a discourse tag?non-discourse use or one of the four main discourse
relation types?which is the same as in the English translation. We call the resulting annotation on the
Chinese sentences discourse projection.
Further we discard potential expressions of Chinese connectives if they occurred with the same part of
speech only once in the entire corpus. The result is a list of a total of 118 Chinese discourse connectives
harvested using direct projection.
4.2 Implicit or Explicit?
A discourse relation can be expressed either with an explicit connective (e.g. however, since), or implic-
itly without a connective, in which case the relation would have to be inferred by the reader. Languages
may differ in how they express discourse relations.
We investigate such implicit/explicit mismatch using direct projection. Specifically, we study the cases
in which an English discourse connective is not aligned to any part of its corresponding Chinese sentence.
In this case, the human translator explicitly expressed a discourse relation that was implicitly conveyed
in the corresponding Chinese sentence.
The following four examples illustrate a Chinese/English implicit/explicit mismatch for each of the
TEMPORAL, COMPARISON, CONTINGENCY and EXPANSION relation, respectively. On the Chinese side
we also mark the position of the inserted English connective.
source [????4?27?]
1
?[??????????????????????????]
2
?when
TEMPORAL
[???
?????????????????]
3
?
ref [On april 27 local time]
1
, [Afghan president Karzai and other important officials were forced to flee the scene]
3
[when
TEMPORAL
a military parade in Kabul, Afghanistan commemorating victory in the fight against the soviet invasion was
attacked]
2
.
source [??????????????]
1
?[??????????]
2
?while
COMPARISON
[??????]
3
?
ref [However, of the ten commonly-used languages today]
1
, [Arabic only ranks fourth]
2
, [while
COMPARISON
English ranks first]
3
.
source [??????????????????????]
1
??[??????????????????
??]
2
?since
CONTINGENCY
[????????????]
3
??
ref [Tung Ta-Wei, head representative for China Airlines in Shanghai, told reporters]
1
, ?[presently, the cross-strait charter
flights are still not ?direct flights? in the true sense of the term]
2
, [since
CONTINGENCY
they still have to pass through the hong kong
flight information region]
3
. ?
source [????]
1
?[?????????????????????]
2
?and
EXPANSION
[????????????
?]
3
?
ref [Liu Binjie said]
1
, [a key area of development for the Chinese publishing industry will be participating in international
competition]
2
, [and
EXPANSION
in the future the two sides can strengthen their cooperation in this area]
3
.
The first example is particularly interesting from a discourse point of view as it combines information
ordering considerations along with the implicit/explicit expression of discourse relations: not only is the
connective when missing in Chinese but the two arguments of the connective appeared in reverse order
in the English translation of the sentence, with the comma omitted.
In Table 2, we show the numbers and percentages of Chinese/English implicit/explicit mismatches for
each relation. We also list the ten connectives that are most frequently associated with the mismatch (i.e.,
were added to the reference translation), in the format of connective (# mismatches) below:
and (341), when (120), while (45), if (37), so that (29), but (23), after (22), so (22), as (21), then (18)
This analysis reveals that the EXPANSION relation is more likely to be implicitly expressed in Chinese,
although in other relations this phenomenon is also present.
580
Connective Senses Connective Senses
? COMPARISON (7) EXPANSION (2) ? CONTINGENCY (1) EXPANSION (1)
? COMPARISON (1) EXPANSION (2) ?...?? TEMPORAL (3) EXPANSION (2)
? CONTINGENCY (1) EXPANSION (3) ?? TEMPORAL (1) EXPANSION (1)
Table 3: Ambiguous Chinese connectives, according to manual annotations in the development CDTB.
A similar mismatch also happens when an English discourse connective is aligned to a punctuation
mark in Chinese, illustrated in the following example, where the comma underlined in the source sen-
tence was translated to and, thus to an explicit EXPANSION in English:
source [????????????]
1
?[??????]
2
?[??????]
3
?[?????????]
4
?
ref [Most of the contingent?s squadrons garrisoned along the border]
1
[are stationed in remote areas]
2
[where the natural
conditions are rough]
3
[and
EXPANSION
the construction of informatization relatively lags behind]
4
.
The insertion of the explicit discourse connective and makes the use of punctuation between ?rough
conditions? and ?informatization? unnecessary in English. Through our direct projection we found 136
such implicit to explicit transformations with commas and 5 with semicolons. All of them are of the
relation EXPANSION, further highlighting the differences in information packaging between the two lan-
guages.
4.3 Ambiguity of connectives
Although most of the English discourse connectives identified in the PDTB are not ambiguous, some of
the most frequently used ones are (Pitler et al., 2008; Miltsakaki et al., 2008). For example, while can
signal both TEMPORAL and COMPARISON relations; since, as can signal both TEMPORAL and CONTIN-
GENCY relations. Discourse connectives in different languages have different ambiguities; prior work
has shown that it is easier to disambiguate the sense of an ambiguous connective when parallel cor-
pora are available (Meyer et al., 2011). The two languages analyzed in Meyer et al. (2011), English
and French, are closely related European languages; here we investigate such differences in ambiguities
between English and Chinese connectives.
Specifically, using the connectives collected from direct projection, we inspect the relations annotated
for these connectives in the Chinese Discourse Treebank development set, and extract connectives such
that the majority sense they signal constitutes less than 90% of their total occurrences. Unlike in English
where the vast majority of ambiguities are between TEMPORAL and some other sense, we find that all
such connectives in Chinese are ambiguous between some relation and EXPANSION. An example of
ambiguity between TEMPORAL and EXPANSION is shown below:
source???????????????????
TEMPORAL
????????????
ref Only in this way can Dujkovic sit back and do nothing and look on others disinterestedly when
TEMPORAL
getting his full salary
per contract.
source??????????
EXPANSION
??????????????????
ref While
EXPANSION
reducing driving time, they are also mixing gasoline with cooking oil recycled from restaurants.
In the first case, there is a synchrony relation between Dujkovic?s ?sitting back and doing nothing?,
and ?getting his full salary?. In the second case, ?reducing driving time? and ?mixing gasoline with
cooking oil? are a list of methods for saving gasoline.
In Table 3 we list these ambiguous Chinese connectives, their senses and the frequency with which
they were annotated. The ambiguities we see here are very different from those in English where the
TEMPORAL?CONTINGENCY and COMPARISON?CONTINGENCY ambiguities are most prominent.
5 Predicting discourse relation sense in Chinese
Our analysis so far has revealed considerable differences in the expression of discourse relations in
Chinese and English. We now show that projected annotations can be used to disambiguate Chinese
discourse connectives despite these differences.
581
5.1 Learning with unlabeled data
The main idea of learning by projection across parallel corpora is to use a classifier to annotate the En-
glish portion of the data, then project the discourse relation sense labels onto the corresponding Chinese
sentences. Then a classifier can be trained using features gathered on the Chinese portion of the data.
However, labels gathered from direct projections are not suitable for learning systems without extra
processing. If an English connective is aligned to one of the Chinese connectives, we can transfer its label
from English to the Chinese connective. However, it is highly likely that a Chinese connective appears in
the source sentence but the reference translation used an alternative expression or paraphrase rather than
the 100 identified connectives in the PDTB. It is difficult to distinguish through direct projection if an
explicit discourse connective in Chinese was expressed implicitly in English or if the Chinese expression
was used in a non-discourse sense.
The possibilities described above imply that in our work, we cannot assume that through direct pro-
jection we have a fully labeled dataset for discourse connective senses in Chinese. Instead we have a
mixture of data with labeled positive examples (when an explicit English connective was aligned to the
phrase) and unlabeled examples (where there was no explicit discourse connective in English, so the
Chinese expression is either used in a non-discourse sense or is expressed implicitly or using alternative
expressions in English, and thus the label is unknown).
Luckily, learning from positive and unlabeled examples, especially for binary classification, is a fairly
well studied problem in machine learning (Lee and Liu, 2003; Liu et al., 2003; Elkan and Noto, 2008).
We adopt such methods as part of our semi-supervised learning system.
In this work, we propose the following components for relation classification:
(Noisy) data labeling Classify each instance of a possible connective on the English side of the corpus
into either non-discourse use, or one of TEMPORAL, CONTINGENCY, COMPARISON or EXPANSION. If
the English connective signals one of the four relations, transfer the labels to the connectives expressed
in the corresponding Chinese sentences through alignments, as described in Section 4.1.
Train sense classifier This classifier is trained only on the Chinese expressions labeled as one of the
four main classes of discourse relation. We can train either a binary classifier to predict if a connec-
tive expresses a particular relation, or a 4-way classifier which assigns the most probable sense to each
connective. The potentially problematic labels for the non-discourse class are not used in this stage.
Train discourse use classifier This classifier has to use the potentially problematic data, where we
cannot distinguish negative examples from untagged positive examples. The problem is solved as a
cascade of classifiers, an approach developed in Elkan and Noto (2008). The idea is to train a noisy
classifier that produces a soft score for the data?a probability of being in the class rather than a strict
class assignment.
Let y be the true discourse use class to be predicted: y = 1 for examples of discourse use, and y = 0
for examples of non-discourse use. Let l indicate whether the example is labeled as discourse use (l = 1),
or unlabeled (l = 0, unknown or non-discourse use). First, we use a logistic regression classifier LR to
estimate P (l = 1|y = 1). Let?s call this estimate e. Using LR, e can be estimated as
?
x?P
LR(x)/|P |,
where P is the set of the original positively labeled examples, LR(x) is the probability of expression x
to be labeled positively. We then use the estimator e to calculate the estimated value of P (y = 1|l = 0),
the probability of an expression being discourse use from the original unlabeled examples:
w =
LR(x)
e
/
1? LR(x)
1? e
In the second stage, each of the unlabeled examples are duplicated, once as a positive example with
weight w and once as a negative example with weight 1? w. Our second stage classifier?linear-kernel
SVM with weights for each example?is trained on the combined set of positive examples (discourse use)
and the duplicated version of the unlabeled examples (unknown and non-discourse use class). When w
is close to 0.5, the example is practically noise (with labels 0.5 and -0.5) and does not affect the learning
of parameters much. Weights closer to 1 practically reassign the originally non-discourse use example to
582
the discourse use class (labels 1 and 0); a weight close to 0 leaves the example as one of the non-discourse
use instances (with labels 0 and -1).
Test phase In testing, first the second-stage SVM model for discourse vs. non-discourse use is applied.
For only the expression predicted to be discourse connectives (discourse use), we run the sense classifier
to do binary or multiway relation classification. Binary classification labels whether a connective sig-
nals a particular relation; multiway classification labels one of the five possible classes: non-discourse
use, TEMPORAL, COMPARISON, CONTINGENCY and EXPANSION. This series of classifiers results in a
system that can assign the same labels as the classifiers trained for English.
To complete our presentation of the approach, we now turn to describe the features used to represent
instances of potential discourse connectives.
5.2 Features
The following set of features for each expression we need to classify are extracted solely from the Chinese
part of the corpus
4
. The syntactic parse trees were obtained automatically (Levy and Manning, 2003).
Connective The connective expressions themselves. The vast majority of connectives (at least in En-
glish) are unambiguous, so using the identity of the connective is a hard-to-beat baseline for sense pre-
diction (Pitler et al., 2008).
Categories The syntactic category of the expression itself, as well as that of its parents, and its left and
right siblings (if any). These features are adapted from Pitler and Nenkova (2009).
Depth Depth of the expressions?s syntactic category in the parse tree for the sentence.
POS bigram Bigram of part-of-speech tags of the entire sentence.
Production pairs Parent-child node category pairs, gathered from subtrees of two ancestors starting
from the parent of the expression?s self-category. For example, a subtree IP?NP VP would yield the
features (IP NP) and (IP VP). Production rules have shown to be effective for implicit discourse relation
classification (Lin et al., 2009; Park and Cardie, 2012). This is a less sparse adaptation of such features.
Punctuation This class corresponds to two features. The first feature takes one of the three possible
values: if the expression starts a sentence, if there is a punctuation to the immediate left of the expression,
or none of above. The second feature has two values corresponding to whether there is a punctuation to
the expression?s immediate right.
Sequence pairs Left-to-right sequence pairs of node categories, gathered from subtrees of two ancestors
starting from the parent of the expression?s self-category. For example, a subtree IP?NP VP PU would
yield the features (NP VP) and (VP PU).
Size of ancestor nodes The number of children a node has, calculated with three ancestors starting from
the parent of the expression?s self-category.
# characters The number of Chinese characters in the connective expression.
5.3 Classification results
In this section, we demonstrate the effectiveness of learning discourse relations through parallel data pro-
jection and semi-supervised learning. We use the GALE corpus for training and the Chinese Discourse
Treebank development set (CDTB-dev) for testing. There are 5,136 training instances and 490 testing
instances. In addition, we compare performance with 10-fold cross validation results over CDTB-dev.
We obtain predictions for each fold and evaluate on the combined data from all folds, instead of av-
eraging performance for each fold. In this way the results from 10-fold validation and those from the
semi-supervised classifier trained on projected data are directly comparable. The LIBLINEAR pack-
age (Fan et al., 2008) was used for binary classification (including the discourse use classifier
5
), and
SVM-Multiclass (Tsochantaridis et al., 2004) with linear kernel was used for multiway classification.
4
As a reminder, the list of possible connectives was derived from direct projection after pruning items that occurred only
once with a particular part-of-speech. There is a total of 118 such expressions for Chinese.
5
http://www.csie.ntu.edu.tw/?cjlin/libsvmtools/#weights for data instances
583
Baseline Cascade Supervised
A F P/R A F P/R A F P/R
connective (C) 67.62 51.23 75.45/38.79 70.29 65.88 66.35/65.42 77.96 75.00 75.00/75.00
C+tree depth 69.39 55.36 77.50/43.06 69.80 66.36 65.18/67.59 78.57 75.86 75.34/76.39
C+categories 66.94 52.63 71.43/41.67 70.82 66.97 66.82/67.13 83.67 82.61 77.87/87.96
C+size of ancestor 67.96 52.57 75.65/40.28 71.22 67.59 67.12/68.06 76.12 72.73 73.24/72.22
C+POS bigram 70.00 58.12 75.56/47.22 74.29 70.42 71.43/69.44 81.84 80.35 76.79/84.26
C+punctuation 67.96 52.85 75.21/40.74 73.67 70.48 69.68/71.30 82.65 80.81 78.85/82.87
above, combined 70.61 60.00 75.00/50.00 75.10 71.63 71.96/71.30 82.04 80.00 78.57/81.48
Table 4: Accuracy, F-measure and precision/recall for classifying discourse/non-discourse use of con-
nective expressions, for top features and for the combined feature set.
5-way Baseline Projection 5-way Supervised
connective (C) 0.6332 0.6434 0.6114
C+tree depth 0.5959 0.6367 0.6384
C+punctuation 0.6224 0.6776 0.6425
C+size of ancestor 0.5939 0.6469 0.6073
C+categories 0.5837 0.6633 0.6359
C+POS bigram 0.6469 0.6980 0.6714
above, combined 0.6245 0.7020 0.6355
Table 5: Multiway discourse relation classification accuracies, for top and the combined features.
Discourse vs. non-discourse To demonstrate the cascade learning component in our system, we first
show results from the intermediate stage of the discourse vs. non-discourse prediction task. We compare
three systems: our cascade approach for handling noisy labels for non-discourse use, a baseline trained
only on the original noisy non-discourse labels (this corresponds to the hard-label performance of the
first stage classifier in our approach) and a supervised system trained on CDTB-dev (where predictions
are obtained in 10-fold cross validation fashion).
In Table 4 we show the accuracy, precision/recall and F measure for each system, using connective
expressions themselves and the five features that gave the best performance on the test set.
Cascade learning achieved a strong boost over the baseline with significant improvements on recall,
although it does not perform as well as the fully supervised system. The features most useful for this task
are POS bigrams and punctuations; syntactic category features are very useful for the supervised system,
but not as useful for the cascade system.
Multiway classification Now we show how our system performs for the complete task of multiway
classification of discourse relations for Chinese, recognizing each expression either as non-discourse use
or one of the four discourse relation senses. We compare our semi-supervised multiway classification
system against: (i) a baseline system that performs 5-way classification with the noisy labels from direct
projection in the GALE data (again corresponding to the hard-label performance of the first stage clas-
sifier in our approach); (ii) a supervised system for 5-way classification trained on CDTB-dev (where
predictions are obtained in 10-fold cross-validation fashion).
Table 5 records the accuracies for the connective expression and the five features performed best for
this task. The top features for multiway relation classification, in addition to connectives, are part-of-
speech bigrams, punctuations, and syntactic categories.
Notably, without any annotated data on the Chinese side, the projected semi-supervised system out-
performs the 5-way supervised system for all but one of the features, and is significantly better when the
top features are combined (70.2% vs. 63.55%). This finding justifies the idea and feasibility of using
parallel corpora for discourse relation classification.
Binary classification Finally, we present results and the most informative features for binary classifi-
cation of each relation sense individually. The semi-supervised projection system is compared against
a fully supervised binary classification system over 10-fold CDTB-dev, with accuracies and F scores
584
Projection Supervised Feature set
A F A F
COMPARISON 94.49 59.70 96.33 57.14 Connective, categories, size of ancestor, # characters, POS bigram
CONTINGENCY 92.65 41.94 96.33 70.97 Connective, production pairs
EXPANSION 85.10 69.20 87.96 77.20 Connective, categories, production pairs, sequence pairs, POS bigram
TEMPORAL 88.37 48.65 94.08 60.47 Connective, categories, production pairs, sequence pairs
Table 6: Accuracy and F measure for binary classification for each relation, including features that
significantly improves performance beyond the identity of the connective itself.
shown in Table 6. The feature sets included are the ones that significantly improve the F measure of a
relation compared to that when using the connective expressions alone.
For accuracies, the semi-supervised system is only slightly (1.8-3.7%) below that of the supervised
system for three of the four relations. On the other hand, F measures of the semi-supervised system are
not as good as the supervised system except for the COMPARISON relation. The feature categories indi-
cate that for Chinese discourse connectives, different feature sets are appropriate for different relations.
6 Conclusion
We investigated the tasks of discourse analysis and recognition without manual annotation. Instead, we
used parallel corpora to project automatic annotations available on one side (English) to the other (Chi-
nese). First, we conducted a corpus study which demonstrates the differences in information packaging
and discourse organization between English and Chinese. We highlighted the existence of long sentences
in Chinese that correspond to multiple sentences in English, mismatches between discourse expressions
that are implicit vs. explicit in the two languages, and differences in the ambiguity of discourse connec-
tives. Second, we presented a semi-supervised system that learns to predict discourse relations from the
noisy annotations derived from parallel corpora. On the multiway discourse relation classification task,
our system outperforms a fully supervised system trained using clean gold-standard annotation in the
targeted language.
References
Amal Al-Saif and Katja Markert. 2010. The Leeds Arabic Discourse Treebank: Annotating discourse connectives
for Arabic. In Proceedings of the International Conference on Language Resources and Evaluation (LREC).
David Allbritton and Johanna Moore. 1999. Discourse cues in narrative text: Using production to predict compre-
hension. In AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 600?609.
Barbara Di Eugenio, Johanna D Moore, and Massimo Paolucci. 1997. Learning features that predict cue usage.
In Proceedings of the Annual Meeting of the Association for Computational Linguistics and Eighth Conference
of the European Chapter of the Association for Computational Linguistics (EACL), pages 80?87.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011. Minimally supervised event causality identification. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 294?
303.
Charles Elkan and Keith Noto. 2008. Learning classifiers from only positive and unlabeled data. In Proceedings
of the ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 213?220.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. The Journal of Machine Learning Research, 9:1871?1874.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projec-
tion constraints. In Proceedings of the Joint Conference of the Annual Meeting of the ACL and the International
Joint Conference on Natural Language Processing of the AFNLP (ACL), pages 369?377.
585
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via
syntactic projection across parallel texts. Natural Language Engineering, 11(3):311?325, September.
Meixun Jin, Mi-Young Kim, Dongil Kim, and Jong-Hyeok Lee. 2004. Segmentation of Chinese long sentences
using commas. In Proceedings of the SIGHAN Workshop on Chinese Language Processing, pages 1?8.
Richard Johansson and Pierre Nugues. 2006. A FrameNet-based semantic role labeler for Swedish. In Proceed-
ings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436?443.
Wee Sun Lee and Bing Liu. 2003. Learning with positive and unlabeled examples using weighted logistic regres-
sion. In Proceedings of the International Conference on Machine Learning (ICML), volume 3, pages 448?455.
Roger Levy and Christopher D. Manning. 2003. Is it harder to parse Chinese, or the Chinese Treebank? In
Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 439?446.
Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie Strassel, and Kazuaki Maeda. 2010. Enriching word alignment
with linguistic tags. In Proceedings of the International Conference on Language Resources and Evaluation
(LREC).
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Dis-
course Treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse
relations. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (ACL-HLT), pages 997?1006.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151?184, 4.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and Philip S Yu. 2003. Building text classifiers using positive
and unlabeled examples. In Proceedings of the IEEE International Conference on Data Mining (ICDM), pages
179?186.
William C. Mann and Sandra Thompson. 1988. Rhetorical Structure Theory: toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
Daniel Marcu. 1997. The rhetorical parsing of natural language texts. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association
for Computational Linguistics (EACL), pages 96?103. Association for Computational Linguistics.
Thomas Meyer and Andrei Popescu-Belis. 2012. Using sense-labeled discourse connectives for statistical machine
translation. In Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and
Machine Translation and Hybrid Approaches to Machine Translation (ESIRMT-HyTra), pages 129?138.
Thomas Meyer, Andrei Popescu-Belis, Sandrine Zufferey, and Bruno Cartoni. 2011. Multilingual annotation and
disambiguation of discourse connectives for machine translation. In Proceedings of the Annual Meeting of the
Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 194?203.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Aravind Joshi. 2008. Sense annotation in the Penn Discourse
Treebank. In Proceedings of the International Conference on Computational Linguistics and Intelligent Text
Processing (CICLing), pages 275?286.
Umangi Oza, Rashmi Prasad, Sudheer Kolachina, Dipti Misra Sharma, and Aravind Joshi. 2009. The Hindi
Discourse Relation Bank. In Proceedings of the Third Linguistic Annotation Workshop, pages 158?161. Asso-
ciation for Computational Linguistics.
Sebastian Pado and Mirella Lapata. 2005. Cross-linguistic projection of role-semantic information. In Proceed-
ings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT-EMNLP), pages 859?866.
Joonsuk Park and Claire Cardie. 2012. Improving implicit discourse relation recognition through feature set
optimization. In Proceedings of the Annual Meeting of the Special Interest Group on Discourse and Dialogue
(SIGDIAL), pages 108?112.
Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
186?195.
586
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP Conference: Short Papers, pages 13?16.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, and Aravind Joshi. 2008. Easily
identifiable discourse relations. In Proceedings of the Conference on Computational Linguistics (COLING):
Posters, page 87?90.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations
in text. In Proceedings of the Joint Conference of the Annual Meeting of the ACL and the International Joint
Conference on Natural Language Processing of the AFNLP (ACL), pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse TreeBank 2.0. In Proceedings of the International Conference on Language Re-
sources and Evaluation (LREC).
Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2010. Realization of discourse relations by other means:
Alternative lexicalizations. In Proceedings of the Conference on Computational Linguistics (COLING): Posters,
pages 1023?1031.
Frank Schilder. 2002. Robust discourse parsing via discourse markers, topicality and position. Natural Language
Engineering, 8(3):235?255.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine
learning for interdependent and structured output spaces. In Proceedings of the International Conference on
Machine Learning (ICML), page 104.
Lonneke van der Plas, Paola Merlo, and James Henderson. 2011. Scaling up automatic cross-lingual semantic role
annotation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (ACL-HLT), pages 299?304.
Ben Wellner and James Pustejovsky. 2007. Automatically identifying the arguments of discourse connectives. In
Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages 92?101.
Nianwen Xue and Yaqin Yang. 2011. Chinese sentence segmentation as comma classification. In Proceedings of
the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-
HLT): Short Papers, pages 631?635.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. 2005. The penn chinese treebank: Phrase structure
annotation of a large corpus. Natural Language Engineering, 11(2):207?238, June.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proceedings of the First International Conference on Human Language
Technology Research (HLT), pages 1?8.
Deniz Zeyrek and Bonnie Webber. 2008. A discourse resource for Turkish: Annotating discourse connectives in
the METU corpus. In Proceedings of the 6th Workshop on Asian Language Resources, pages 65?72.
Yuping Zhou and Nianwen Xue. 2012. PDTB-style discourse annotation of Chinese text. In Proceedings of the
Annual Meeting of the Association for Computational Linguistics (ACL), pages 69?77.
587
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 283?288,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Assessing the Discourse Factors that Influence the Quality of Machine
Translation
Junyi Jessy Li
University of Pennsylvania
ljunyi@seas.upenn.edu
Marine Carpuat
National Research Council Canada
marine.carpuat@nrc.gc.ca
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
We present a study of aspects of discourse
structure ? specifically discourse devices
used to organize information in a sen-
tence ? that significantly impact the qual-
ity of machine translation. Our analysis
is based on manual evaluations of trans-
lations of news from Chinese and Ara-
bic to English. We find that there is a
particularly strong mismatch in the no-
tion of what constitutes a sentence in Chi-
nese and English, which occurs often and
is associated with significant degradation
in translation quality. Also related to
lower translation quality is the need to em-
ploy multiple explicit discourse connec-
tives (because, but, etc.), as well as the
presence of ambiguous discourse connec-
tives in the English translation. Further-
more, the mismatches between discourse
expressions across languages significantly
impact translation quality.
1 Introduction
In this study we examine how the use of dis-
course devices to organize information in a sen-
tence ? and the mismatch in their usage across
languages ? influence machine translation (MT)
quality. The goal is to identify discourse process-
ing tasks with high potential for improving trans-
lation systems.
Historically MT researchers have focused their
attention on the mismatch of linear realization of
syntactic arguments (Galley et al, 2004; Collins
et al, 2005), lexico-morphological mismatch
(Minkov et al, 2007; Habash and Sadat, 2006)
and word polysemy (Carpuat and Wu, 2007; Chan
et al, 2007). Discourse structure has largely been
considered irrelevant to MT, mostly due to the as-
sumption that discourse analysis is needed to inter-
pret multi-sentential text while statistical MT sys-
tems are trained to translate a single sentence in
one language into a single sentence in another.
However, discourse devices are at play in the or-
ganization of information into complex sentences.
The mere definition of sentence may differ across
languages. Chinese for example is anecdotally
known to allow for very long sentences which at
times require the use of multiple English sentences
to express the same content and preserve gram-
maticality. Similarly discourse connectives like
because, but, since and while often relate informa-
tion expressed in simple sentential clauses. There
are a number of possible complications in trans-
lating these connectives: they may be ambiguous
between possible senses, e.g., English while is am-
biguous between COMPARISON and TEMPORAL;
explicit discourse connectives may be translated
into implicit discourse relations or translated in
morphology rather than lexical items (Meyer and
Webber, 2013; Meyer and Pol?akov?a, 2013).
In our work, we quantify the relationship be-
tween information packaging, discourse devices,
and translation quality.
2 Data and experiment settings
We examine the quality of translations to English
from Chinese and Arabic using Human-targeted
Translation Edit Rates (HTER) (Snover et al,
2006), which roughly captures the minimal num-
ber of edits necessary to transform the system
output into an acceptable English translation of
the source sentence. By comparing MT output
with post-edited references, HTER provides more
reliable estimates of translation quality than us-
ing translated references, especially at the seg-
ment level. The data for the analysis is drawn
from an extended set of newswire reports in the
2008/2010 NIST Metrics for Machine Translation
283
GALE Evaluation set
1
. For Chinese, there are
305 sentences (segments) translated to English by
three different translation systems. For Arabic,
there are 363 Arabic sentences (segments) trans-
lated by two systems.
The presence of discourse devices is analyzed
only on the English side: the reference, the system
hypothesis and its edited translation. Discourse
connectives and their senses are identified using
existing tools developed for English. Beyond its
practical limitations, analyzing the reference inter-
estingly reflects the choices made by the human
translator: whether to choose to use a discourse
connective, or to insert one to make an implicit re-
lation on the source side explicit on the target side.
We first conduct analysis of variance (ANOVA)
with HTER as dependent variable and the dis-
course factors as independent variables, and sys-
tems as subjects. We examine within-subject sig-
nificance in each ANOVA model. For discourse
factors that are significant at the 95% confidence
level or higher according to the ANOVA analy-
sis, we provide detailed breakdown of the system
HTER for each value of the discourse factor.
In this paper we do not compare the perfor-
mance of individual systems, but instead seek to
understand if a discourse phenomena is problem-
atic across systems.
2
3 Sentence length and HTER
The presence of complex discourse structure is
likely to be associated with longer sentences. It
stands to reason that long sentences will be harder
to process automatically and this reasoning has
motivated the first approaches to text simplifica-
tion (Chandrasekar et al, 1996). So before turning
to the analysis of discourse phenomena, we exam-
ine the correlation between translation quality and
sentence length. A strong correlation between the
two would call for revival of interest in text sim-
plification where syntactically complex sentences
are transformed into several shorter sentences as a
preprocessing step.
We find however that no strong relationship ex-
ists between the two, as shown by the correlation
coefficients between HTER values and the number
of words in each segment in Table 1.
1
Data used in this work includes more documents and the
human edits not present in the official release.
2
For the readers with keen interest in system comparison,
we note that according to ANOVA none of the differences in
system performance on this data is statistically significant.
Lan. Sys1 Sys2 Sys3
ZH 0.097 (0.099) 0.117 (0.152) 0.144 (0.173)
AR 0.071(0.148) -0.089 (-0.029) -
Table 1: Pearson (Spearman) correlation coeffi-
cient between segment length and HTER values.
Next we examine if sentence?discourse diver-
gence between languages and the presence of (am-
biguous) discourse connectives would be more in-
dicative of the expected translation quality.
4 When a sentence becomes discourse
Some languages allow more information to be
packed into a single sentence than is possible in
another language, making single-sentence transla-
tions cumbersome and often ungrammatical. Chi-
nese is known for sentences of this kind; for exam-
ple, the usage of punctuation is very different in
Chinese in the sense that a comma can sometimes
function as a full stop in English, motivating a se-
ries of disambiguation tasks (Jin et al, 2004; Xue
and Yang, 2011; Xu and Li, 2013). Special han-
dling of long Chinese sentences were also shown
to improve machine translation (Jin and Liu, 2010;
Yin et al, 2007).
To investigate the prevalence of sentences in the
source language (Chinese and Arabic in our case)
that do not confirm to the notion of sentence in the
target language (English for the purposes of this
study), we separate the translation segments in the
source language into two classes: a source sen-
tence is considered 1-1 if the reference translation
consists of exactly one sentence, and 1-many if the
reference contains more than one sentence.
For Chinese, 26.2% of the source segments are
1-many. These sentences tend to be much longer
than average (36.6% of all words in all reference
translations are part of such segments). For Ara-
bic, the numbers are 15.2% and 26.3%, respec-
tively. Below is an example of a 1-many Chinese
segment, along with the human reference and its
translation by one of the systems:
[source] ??????Erinys????????RISC?
????????????????????????
?????
[ref] Russian police claim that Erinys has an important com-
petitor RISC. The last people Litvinenko saw while he was
alive, Lugovoi and his friends, were all engaged in these in-
dustries.
[sys] Russian police have claimed that a major competitor,
Litvinenko his last meeting with friends are engaged in these
industries.
We conducted ANOVA on HTER, separately
for each language, with type of segment (1-1 or
284
AOV Arabic Chinese
Pr(> F ) 0.209 0.0045*
1-1 1-many
System HTER HTER
ZH-Sys1 16.22 19.03*
ZH-Sys2 19.54 21.02
ZH-Sys3 20.64 23.86*
Table 2: ANOVA for both languages; average
HTER for the three Chinese to English systems,
stratified on type of segment (1-1 and 1-many). An
(*) denotes significance at p < 0.05.
1-many) as the independent variable and systems
treated as subjects. The test revealed that there is
a significant difference in translation quality be-
tween 1-1 and 1-many segments for Chinese but
not for Arabic. For the Chinese to English systems
we further ran a Wilcoxon rank sum test to iden-
tify the statistical significance in performance for
individual systems. For two of the three systems
the difference is significant, as shown in Table 2.
We have now established that 1-many segments
in Chinese to English translation are highly preva-
lent and their translations are of consistently lower
quality compared to 1-1 segments. This finding
suggests a cross language discourse analysis task
of identifying Chinese sentences that cannot be
translated into single English sentences. This task
may be related to existing efforts in comma dis-
ambiguation in Chinese (Jin et al, 2004; Xue and
Yang, 2011; Xu and Li, 2013) but the relation-
ship between the two problems needs to be clar-
ified in follow up work. Once 1-many segments
are identified, source-side text simplification tech-
niques may be developed (Siddharthan, 2006) to
improve translation quality.
5 Explicit discourse relations
Explicit discourse relations such as COMPARISON,
CONTINGENCY or TEMPORAL are signaled by
an explicit connective, i.e., however or because.
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) provides annotations for the arguments
and relation senses of one hundred pre-selected
discourse connectives over the news portion of
the Penn Treebank corpus (Marcus et al, 1993).
Based on the PDTB, accurate systems for explicit
discourse relation identification have been devel-
oped (Pitler and Nenkova, 2009; Lin et al, 2014).
The accuracy of these systems is 94% or higher,
close to human performance on the task. Here we
AOV Arabic Chinese
Pr(> F ) 0.39 0.0058*
No Conn > 1 Conn
all % data (ZH) 53.77 15.08
1-many % data (ZH) 13.77 5.25
HTER mean HTER mean
all ZH-Sys1 16.11 19.84
+
ZH-Sys2 19.96 22.39
ZH-Sys3 20.70 25.00*
1-many ZH-Sys1 16.94 22.75
+
ZH-Sys2 20.47 23.25
ZH-Sys3 22.30 29.68*
Table 3: Number of connectives: ANOVA for both
languages; proportion of data in each factor level
and average HTER for the three Chinese-English
systems, of the entire dataset and of 1-many trans-
lations. An (*) or (+) sign denotes significance at
95% and 90% confidence levels, respectively.
study the influence of explicit discourse relations
on machine translation quality and their interac-
tion with 1-1 and 1-many segments.
5.1 Number of connectives
We identify discourse connectives and their senses
(TEMPORAL, COMPARISON, CONTINGENCY or
EXPANSION) in each reference segment using the
system in Pitler and Nenkova (2009)
3
. We com-
pare the translation quality obtained on segments
with reference translation containing no discourse
connective, exactly one discourse connective and
more than one discourse connective.
The ANOVA indicates that the number of con-
nectives is not a significant factor for Arabic trans-
lation, but significantly impacts Chinese transla-
tion quality. A closer inspection using Wilcoxon
rank sum tests reveals that the difference in trans-
lation quality is statistically significant only be-
tween the groups of segments with no connective
vs. those with more than one connective. Addi-
tionally, we ran Wilcoxon rank sum test over 1-
1 and 1-many segments individually and find that
the presence of discourse connectives is associated
with worse quality only in the latter case. Effects
above are illustrated in Table 3.
5.2 Ambiguity of connectives
A number of discourse connectives are ambiguous
with respect to the discourse relation they convey.
For example, while can signal either COMPARI-
3
http://www.cis.upenn.edu/?epitler/discourse.html; We
used the Stanford Parser (Klein and Manning, 2003).
285
AOV Arabic Chinese
Pr(> F ) 0.57 0.00014*
has-amb-conn no-amb-conn
System HTER mean HTER mean
ZH-Sys1 21.57 16.34*
ZH-Sys2 21.44 19.72
ZH-Sys3 27.47 20.69*
Table 4: ANOVA for both languages; average
HTER for the three Chinese systems for segments
with (11.80% of all data) and without an ambigu-
ous connective in the reference translation. An (*)
denotes significance at p < 0.05.
SON or TEMPORAL relations and since can signal
either CONTINGENCY or TEMPORAL. In transla-
tion this becomes a problem when the ambiguity
is present in one language but not in the other.
In such cases the sense in source ought to be dis-
ambiguated before translation. Here we compare
the translation quality of segments which contain
ambiguous discourse connectives in the reference
translation to those that do not. This analysis gives
lower bound on the translation quality degradation
associated with discourse phenomena as it does
not capture problems arising from connective am-
biguity on the source side.
We base our classification of discourse connec-
tives into ambiguous or not according to the dis-
tribution of their senses in the PDTB. We call a
connective ambiguous if its most frequent sense
among COMPARISON, CONTINGENCY, EXPAN-
SION, TEMPORAL accounts for less than 80% of
occurrence of that connective in the PDTB. Nine-
teen connectives meet this criterion of ambiguity.
4
In the ANOVA tests for each language, we com-
pared the quality of segments which contained an
ambiguous connective in the reference with those
that do not, with systems treated as subjects. For
Arabic the presence of ambiguous connective did
not yield a statistically significant difference. The
difference however was highly significant for Chi-
nese, as shown in Table 4.
The finding that discourse connective ambigu-
ity is associated with change in translation quality
for Chinese but not for Arabic is rather interesting.
It appears that the language pair in translation im-
pacts the expected gains from discourse analysis
on translation.
4
The ambiguous connectives are: as, as if, as long as, as
though, finally, if and when, in the end, in turn, lest, mean-
while, much as, neither...nor, now that, rather, since, ulti-
mately, when, when and if, while
AOV Event Arabic Chinese
Pr(> F ) Contingency 0.61 0.028*
Comp.:Temp. 0.047* 0.0041*
Chinese HTER HTER
Contingency ? Contingency
Sys1 20.15 16.72
Sys2 21.69 19.80
Sys3 25.87 21.16
+
Comp.?Temp. ?(Comp.?Temp.)
Sys1 23.58 16.64*
Sys2 26.16 19.63*
Sys3 27.20 21.21
+
Table 5: ANOVA for both languages; average
HTER for Chinese sentences containing a CON-
TINGENCY relation (6.89% of all data) or both
COMPARISON and TEMPORAL (4.59% of all data).
An (*) or (+) sign denotes significance at 95% and
90% confidence levels, respectively.
5.3 Relation senses
Here we study whether discourse relations of spe-
cific senses pose more difficulties on translations
than others and whether there are interactions be-
tween senses. In the ANOVA analysis we used a
binary factor for each of the four possible senses.
For example, we compare the translation quality
of segments that contain COMPARISON relations
in the reference translation with those that do not.
The relation sense makes a significant differ-
ence in translation quality for Chinese but not for
Arabic. For Chinese specifically sentences that ex-
press CONTINGENCY relations have worse qual-
ity translations than sentences that do not express
CONTINGENCY. One explanation for this ten-
dency may be that CONTINGENCY in Chinese con-
tains more ambiguity with other relations such as
TEMPORAL, as tense is expressed lexically in Chi-
nese (no morphological tense marking on verbs).
Finally, the interaction between COMPARISON and
TEMPORAL is significant for both languages.
Table 5 shows the effect of relation sense on
HTER values for Chinese.
6 Human edits of discourse connectives
A relation expressed implicitly without a connec-
tive in one language may need to be explicit in
another. Moreover, the expressions themselves
are used differently; for example, the paired con-
nective ???...??? (despite...but) in Chinese
should not be translated into two redundant con-
nectives in English. It is also possible that the
source language contains an explicit discourse
286
connective which is not translated in the target lan-
guage, as has been quantitatively studied recently
by Meyer and Webber (2013). An example from
our dataset is shown below:
[source] ????????????????????
??????????????
[ref] Still some others can receive further professional game
training in universities and later(Temporal) be employed as
technical consultants by large game manufacturers, etc.
[sys] Some people may go to the university games profes-
sional education, which is appointed as the big game manu-
facturers such as technical advisers.
[edited] Some people may go to university to receive profes-
sional game education, and later(Temporal) be appointed by
the big game manufacturers as technical advisers.
The system fails to translate the discourse con-
nective ???? (later), leading to a probable mis-
interpretation between receiving education and be-
ing appointed as technical advisors.
Due to the lack of reliable tools and resources,
we approximate mismatches between discourse
expressions in the source and MT output using
discourse-related edits. We identify explicit dis-
course connectives and their senses in the system
translation and the human edited version of that
translation. Then we consider the following mu-
tually exclusive possibilities: (i) there are no dis-
course connectives in either the system output or
the edit; (ii) the system output and its edited ver-
sion contain exactly the same discourse connec-
tives with the same senses; (iii) there is a discourse
connective present in the system output but not in
the edit or vice versa. In the ANOVA we use a
factor with three levels corresponding to the three
cases described above. The factor is significant for
both Chinese and Arabic. In both languages, the
mismatch case (iii) involves significantly higher
HTER than either case (i) or (ii). The human edit
rate in the mismatch class is on average four points
greater than that in the other classes.
Obviously, the mismatch in implicit/explicit ex-
pression of discourse relation is related to the
first problem we studied, i.e., if the source seg-
ment is translated into one or multiple sentences
in English, since discourse relations between adja-
cent sentences are more often implicit (than intra-
sentence ones). For this reason we performed a
Wilcoxon rank sum test for the translation qual-
ity of segments with discourse mismatch condi-
tioned on whether the segment was 1-1 or 1-many.
For both languages a significant difference was
found for 1-1 sentences but not 1-many. Table 6
shows the proportion of data in each of the con-
ditioned classes and the average HTER for sen-
% data Mismatch Mismatch ?Mismatch
(1-1) (1-1)
Arabic 21.27 15.47 69.34
Chinese 29.51 17.05 56.82
AOV Arabic Chinese
Pr(> F ) 4.0? 10
?6
* 4.1? 10
?11
*
HTER HTER
?Mismatch Mismatch
AR-Sys1 11.23 15.92*
AR-Sys2 11.64 15.74*
ZH-Sys1 15.57 20.72*
ZH-Sys2 19.02 22.34*
ZH-Sys3 11.64 15.74*
?Mismatch|1-1 Mismatch|1-1
AR-Sys1 10.86 16.24*
AR-Sys2 11.58 16.65*
ZH-Sys1 15.47 19.13*
ZH-Sys2 18.68 22.52*
ZH-Sys3 19.57 26.07*
Table 6: Data portions, ANOVA for both lan-
guages and average HTER for segments where
there is a discourse mismatch between system and
edited translations. An (*) denotes significance at
p < 0.05.
tences from the mismatch case (iii) where a dis-
course connective was edited and the others (no
such edits). Translation quality degrades signifi-
cantly for all systems for the mismatch case, over
all data as well as 1-1 segments.
7 Conclusion
We showed that translation from Chinese to En-
glish is made more difficult by various discourse
events such as the use of discourse connectives,
the ambiguity of the connectives and the type of
relations they signal. None of these discourse fac-
tors has a significant impact on translation qual-
ity from Arabic to English. Translation quality
from both languages is adversely affected by trans-
lations of discourse relations expressed implicitly
in one language but explicitly in the other or by
paired connectives. Our experiments indicate that
discourse usage may affect machine translation
between some language pairs but not others, and
for particular relations such as CONTINGENCY.
Finally, we established the need to identify sen-
tences in the source language that would be trans-
lated into multiple sentences in English. Espe-
cially in translating from Chinese to English, there
is a large number of such sentences which are cur-
rently translated much worse than other sentences.
287
References
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 33?40.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text
simplification. In Proceedings of the 16th Con-
ference on Computational Linguistics (COLING),
pages 1041?1044.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 531?540.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL), pages 273?280.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL): Short Papers, pages 49?52.
Yaohong Jin and Zhiying Liu. 2010. Improving
Chinese-English patent machine translation using
sentence segmentation. In International Conference
on Natural Language Processing and Knowledge
Engineering (NLP-KE), pages 1?6.
Meixun Jin, Mi-Young Kim, Dongil Kim, and Jong-
Hyeok Lee. 2004. Segmentation of Chinese
long sentences using commas. In Proceedings of
the Third SIGHAN Workshop on Chinese Language
Processing (SIGHAN), pages 1?8.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems, volume 15.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151?184, 4.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics - Special issue on using large
corpora, 19(2):313?330.
Thomas Meyer and Lucie Pol?akov?a. 2013. Machine
translation with many manually labeled discourse
connectives. In Proceedings of the Workshop on
Discourse in Machine Translation (DiscoMT), pages
43?50.
Thomas Meyer and Bonnie Webber. 2013. Implicita-
tion of discourse connectives in (machine) transla-
tion. In Proceedings of the Workshop on Discourse
in Machine Translation (DiscoMT), pages 19?26.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 128?135.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence: Short Papers, pages 13?16.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC).
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language and Com-
putation, 4(1):77?109.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Shengqin Xu and Peifeng Li. 2013. Recognizing Chi-
nese elementary discourse unit on comma. In Inter-
national Conference on Asian Language Processing
(IALP), pages 3?6.
Nianwen Xue and Yaqin Yang. 2011. Chinese sen-
tence segmentation as comma classification. In Pro-
ceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies (HLT): Short Papers, pages
631?635.
Dapeng Yin, F. Ren, Peilin Jiang, and S. Kuroiwa.
2007. Chinese complex long sentences processing
method for Chinese-Japanese machine translation.
In International Conference on Natural Language
Processing and Knowledge Engineering (NLP-KE),
pages 170?175.
288
Proceedings of the SIGDIAL 2014 Conference, pages 142?150,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Addressing Class Imbalance for Improved Recognition of Implicit
Discourse Relations
Junyi Jessy Li
University of Pennsylvania
ljunyi@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
In this paper we address the problem of
skewed class distribution in implicit dis-
course relation recognition. We examine
the performance of classifiers for both bi-
nary classification predicting if a particu-
lar relation holds or not and for multi-class
prediction. We review prior work to point
out that the problem has been addressed
differently for the binary and multi-class
problems. We demonstrate that adopting
a unified approach can significantly im-
prove the performance of multi-class pre-
diction. We also propose an approach that
makes better use of the full annotations
in the training set when downsampling is
used. We report significant absolute im-
provements in performance in multi-class
prediction, as well as significant improve-
ment of binary classifiers for detecting the
presence of implicit Temporal, Compari-
son and Contingency relations.
1 Introduction
Discourse relations holding between adjacent sen-
tences in text play an essential role in establishing
local coherence and contribute to the semantic in-
terpretation of the text. For example, the causal re-
lationship is helpful for textual entailment or ques-
tion answering while restatement and exemplifica-
tion are important for automatic summarization.
Predicting the type of implicit relations, which
are not signaled by any of the common explicit
discourse connectives such as because, however,
has proven to be a most challenging task in dis-
course analysis. The Penn Discourse Treebank
(PDTB) (Prasad et al., 2008) provided valuable
annotations of implicit relations. Most research to
date has focused on developing and refining lex-
ical and linguistically rich features for the task
(Pitler et al., 2009; Lin et al., 2009; Park and
Cardie, 2012). Mostly ignored remains the prob-
lem of addressing the highly skewed distribution
of implicit discourse relations. Only about 35% of
pairs of adjacent sentences in the PDTB are con-
nected by three of the four top level discourse re-
lation: 5% participate in Temporal relation, 10%
in Comparison (contrast) and 20% in Contingency
(causal) relations. The remaining pairs are con-
nected by the catch-all Expansion relation (40%)
or by some other linguistic devices (24%). Finer
grained relations of interest to particular applica-
tions account for increasingly smaller percentage
of the PDTB data.
Class imbalance is particularly problematic for
training a binary classifier to distinguish one rela-
tion from the rest. As we will show later, it also
impacts the performance of multi-class prediction
in which each pair of sentences is labeled with one
of the five possible relations.
All prior work has resorted to downsampling
the training data for binary classifiers to distin-
guish a particular relation and use the full train-
ing set for multi-class prediction. In this pa-
per we compare several methods for address-
ing the skewed class distribution during training:
downsampling, upsampling and computing fea-
ture weights and performing feature selection on
the unaltered full training data. A major motiva-
tion for our work is to establish if any of the alter-
natives to downsampling would prove beneficial,
because in downsampling most of the expensively
annotated data is not used in the model. In addi-
tion, we seek to align the treatment of data imbal-
ance for the binary and multi-class tasks. We show
that downsampling in general leads to the best pre-
diction accuracy but that the alternative models
provide complementary information and signifi-
cant improvement can be obtained by combining
both types of models. We also report significant
improvement of multi-class prediction accuracy,
142
achieved by using the alternative binary classifiers
to perform the task.
2 The Penn Discourse Treebank
In the PDTB, discourse relations are viewed as a
predicate with two arguments. The predicate is
the relation, the arguments correspond to the min-
imum spans of text whose interpretations are the
abstract objects between which the relation holds.
Consider the following example of a contrast rela-
tion. The italic and bold fonts mark the arguments
of the relation.
Commonwealth Edison said the ruling could force it to slash
its 1989 earnings by $1.55 a share. [Implicit = BY COM-
PARISON] For 1988, Commonwealth Edison reported
earnings of $737.5 million, or $3.01 a share.
For explicit relations, the predicate is marked by
a discourse connective that occurs in the text, i.e.
because, however, for example.
Implicit relations are marked between adjacent
sentences in the same paragraph. They are inferred
by the reader but are not lexically marked. Alter-
native lexicalizations (AltLex) are the ones where
there is a phrase in the sentence implying the rela-
tion but the phrase itself was not one of the explicit
discourse connectives. There are 16,224 and 624
examples of implicit and AltLex relations, respec-
tively, in the PDTB.
The sense of discourse relations in the PDTB
is organized in a three-tiered hierarchy. The four
top level relations are: Temporal (the two argu-
ments are related temporally), Comparison (con-
trast), Contingency (causal) and Expansion (one
argument is the expansion of the other and contin-
ues the context) (Miltsakaki et al., 2008). These
are the classes we focus on in our work.
Finally, 5,210 pairs of adjacent sentences were
marked as related by an entity relation (EntRel),
by virtue of the repetition of the same entity or
topic. EntRels were marked only if no other rela-
tion could be identified and they are not considered
a discourse relation, rather an alternative discourse
phenomena related to entity coherence (Grosz et
al., 1995). There are 254 pairs of sentences where
no discourse relation was identified (NoRel).
Pitler et al. (2008) has shown that performance
as high as 93% in accuracy can be easily achieved
for the explicit relations, because the connective it-
self is a highly informative feature. Efforts in iden-
tifying the argument spans have also yielded high
accuracies (Lin et al., 2014; Elwell and Baldridge,
2008; Ghosh et al., 2011).
However, in the absence of a connective, recog-
nizing non-explicit relations, which includes im-
plicit relations, alternative lexicalizations, entity
relation and no relation present, has proven to be a
real challenge. Prior work on supervised implicit
discourse recognition studied a wide range of fea-
tures including lexical, syntactic, verb classes, se-
mantic groups via General Inquirer and polarity
(Pitler et al., 2009; Lin et al., 2009). Park and
Cardie (2012) studied the combination of features
and achieved better performance with a different
combination for each individual relation. Meth-
ods for improving the sparsity of lexical represen-
tations have been proposed (Hernault et al., 2010;
Biran and McKeown, 2013), as well as web-driven
approaches which reduce the problem to explicit
relation recognition (Hong et al., 2012).
Remarkably, no prior work has discussed the
highly skewed class distribution of discourse re-
lation types. The tacitly adopted solution has been
to downsample the negative examples for one-vs-
all binary classification aimed at discovering if a
particular relation holds and keeping the full train-
ing set for multi-class prediction.
To highlight the problem, in Table 1 we show
the distribution of implicit relation classes in the
entire PDTB. In our work, we aim to develop clas-
sifiers to identify the four top-level relations listed
in the table
1
.
# of samples Percentage
Temporal 1038 4.3%
Comparison 2550 11.3%
Contingency 4532 20%
Expansion 9082 40%
Table 1: Distribution of implicit relations in the
PDTB.
3 Experimental settings
In our experiments, we used all non-explicit in-
stances in the PDTB sections 2-19 for training and
those in sections 20-24 for testing. Like most stud-
ies, we kept sections 0-1 as development set. In
order to ensure we have a large enough test set to
properly perform tests for statistical significance
over F scores and balanced accuracies, we did not
follow previous work (Lin et al., 2014; Park and
Cardie, 2012) that used only section 23 or sec-
tions 23-24 for testing. Also, the traditional rule
of thumb is to split the available data into training
1
The rest of the data are EntRel/NoRel.
143
and testing sets with 80%/20% ratio. Our choice
ensures that this is the case for all of the relations.
The only features that we use in our experiments
are production rules. We exclude features that oc-
cur fewer than five times in the training set. Pro-
duction rules are the state-of-the-art representation
for discourse relation recognition. This represen-
tation leads to only slightly lower results than a
system including a much larger variety of features
in the first end-to-end PDTB style discourse parser
(Lin et al., 2014) .
The production rule representation is based on
the constituency parse of the arguments and in-
cludes both syntactic and lexical information. A
production rule is the parent with an left-to-right
ordered list of all of its children in the parse tree
(for example, S?NP VP). All non-terminal nodes
are included as a parent, from the sentence head
to the part-of-speech of a terminal. Thus words
that occur in each sentence augmented with their
part of speech are part of the representation (for
example, NN?company), along with more gen-
eral structures of the sentence corresponding to
production rules with only non-terminals on the
right-hand side.
There are three features corresponding to a pro-
duction rule, tracking if the rule occurs in the parse
of first argument of the relation, in the second, or
in both.
Adopting this representation allows us to fo-
cus on the issue of class imbalance and how
the choices of tackling this problem affect even-
tual prediction performance. Our findings are
representation-independent and will most likely
extend to other representations.
We train and evaluate a binary classifier with
linear kernel using SVMLight
2
(Joachims, 1999)
for each of the four top level classes of relations:
Temporal, Comparison, Contingency and Expan-
sion. We used SVM-Multiclass
3
for standard mul-
tiway classification. We also develop and evaluate
two approaches for multiway classification for the
four classes plus the additional class of entity rela-
tion and no relation.
Due to the uneven distribution of classes, we use
precision, recall and f-measure to measure binary
prediction performance. For multiway classifica-
2
http://svmlight.joachims.org/
3
http://svmlight.joachims.org/svm multiclass.html
tion, we use the balanced accuracy (BAC):
BAC =
1
k
k
?
i=1
c
i
n
i
, (1)
where k is the number of relations to predict, c
i
is
the number of instances of relation i that are cor-
rectly predicted, n
i
is the total number of instances
of relation i.
Balanced accuracy (or averaged accuracy) has
a more intuitive interpretation than F-measure. It
is not dominated by the majority class as much as
standard accuracy is. For example for two classes,
in a dataset where one class makes up 90% of the
data, predicting the majority class has accuracy of
90% but balanced accuracy of 45%.
In testing, we keep the original distribution in-
tact and make predictions for all pairs of adjacent
sentences in the same paragraph that do not have
an explicit discourse relation
4
. In order to per-
form tests for statistical significance over F scores,
precision, recall and balanced accuracies, we ran-
domly partitioned the testing data into 10 groups.
We kept the data distribution in each group as
close as possible to the overall testing set. To com-
pare the performance of two different systems, a
paired t-test is performed over these 10 groups.
4 Why downsampling?
Binary classification As mentioned in the pre-
vious sections, in all prior work of supervised im-
plicit relation classification, the technique to cope
with highly skewed distribution for binary classi-
fication is to downsample the negative training in-
stances so that the sizes of positive and negative
classes are equal. The reason for doing so is that
the classifier can achieve high accuracy just by ig-
noring the small class, learning nothing and aways
predicting the larger class. We illustrate this ef-
fect in Table 2. Without downsampling, the only
reasonable F measure is achieved for Expansion
where the smaller class accounts for 40% of the
data. Note that with downsampling, the recogni-
tion of Expansion is also improved considerably.
Multiway classification In prior work multiway
classifiers are trained on all available training data.
As we just saw, however, this approach leads
4
Note the contrast with prior work where in some cases
EntRels are part of Expansion, or in some cases the perfor-
mance of methods is evaluated only on pairs of sentences
where a discourse relation holds, excluding EntRels, NoRels
or AltLexs.
144
All data Downsample
Temp. 0 (nan/0.0) 15.52 (8.8/65.4)
Comp. 2.17 (71.4/1.1) 27.65 (17.3/69.2)
Cont. 0.96 (100.0/0.5) 47.14 (34.5/74.5)
Exp. 44.27 (54.9/37.1) 55.42 (49.3/63.3)
Table 2: F measure (precision/recall) of binary
classification: including all of the data vs down-
sampling.
to poor results in identifying the core Temporal,
Comparison and Contingency discourse relations.
We propose an alternative approach to multi-class
prediction, based on binary one-against-all classi-
fiers for each of the four discourse relations, in-
cluding Expansion, trained using downsampling.
The intuition is that an instance of adjacent sen-
tences S
i
is assigned to a discourse relation R
j
if the binary classifier for R
j
recognizes S
i
as a
positive instance with confidence higher than that
of the classifiers for other relations. If none of
the binary classifiers recognizes the instance as a
positive example, the instance is assigned to class
EntRel/NoRel. This approach modifies the way
multi-class classifiers are normally constructed by
including downsampling and having special treat-
ment of the EntRel/NoRel class.
Specifically, we first use the four binary classi-
fiers C
j
for each relation j to get the confidence p
j
of instance i belonging to class j. We approximate
the confidence by the distance to the hyperplane
separating the two classes, which SVMLight pro-
vides. If at least one p
j
is greater than zero, assign
instance i the class k where the classifier confi-
dence is the highest. If none of the p
j
?s is greater
than zero, assign i to be the EntRel/NoRel class.
We show balanced accuracies of these two mul-
tiway classification methods in Table 3.
Multiway SVM One-Against-All
5-way 32.58 37.15
Table 3: Balanced accuracies for SVM-Multiclass
and one-against-all 5-way classification.
The one-against-all approach leads to 5% abso-
lute improvement in performance. A t-test anal-
ysis confirms that the difference is significant at
p < 0.05. Note that the improvement comes en-
tirely from acknowledging that skewed class dis-
tribution poses a problem for the task and by ad-
dressing the problem in the same way for binary
and multi-class prediction.
5 Using more data
Although downsampling gives much better per-
formance than simply including all of the origi-
nal data, it still appears to be an undesirable so-
lution because in essence it throws away much of
the annotated data. This means that for the small-
est relations, as much as 90% of the data will
not be used. Feature selection and feature val-
ues are computed only based on this much smaller
dataset and do not properly reflect the information
about discourse relations encoded in the PDTB. In
this section we first discuss some of the widely
used methods for handling skewed data distribu-
tion, that is, weighted cost and upsampling. First,
we show that with highly skewed distributions, the
two methods result in almost identical classifiers.
Then we introduce a method for feature selection
and shaping which computes feature weights on
the full dataset and thus captures much of the in-
formation lost in downsampling.
5.1 Weighted cost and upsampling
A number of methods have been developed for
the skewed distribution problem (Morik et al.,
1999; Veropoulos et al., 1999; Akbani et al., 2004;
Batista et al., 2004; Chawla et al., 2002). Here we
highlight weighted cost and random upsampling,
which are known to work well and widely used.
The idea behind weighted cost (Morik et al.,
1999; Veropoulos et al., 1999) is to use weights
to adjust the penalties for false positives and false
negatives in the objective function. As in Morik
et al. (1999), we specify the cost factor to be the
ratio of the size of the negative class vs. that of the
positive class.
In the case of upsampling, instead of ran-
domly downsampling negative instances, positive
instances are randomly upsampled. In our exper-
iments we randomly replicate positive instances
with replacement until the numbers of positive and
negative instances are equal to each other.
The binary and multiway classification results
for these two methods are shown in Table 4 and
Table 5. For binary classification, we can see sig-
nificantly higher F score for the smallest Temporal
class. Weighted cost is also able to achieve signif-
icantly better F-score for Expansion. For Compar-
ison and Contingency, the F-scores are similar to
that of plain downsampling. The balanced accura-
cies of multi-class classification with either meth-
ods are lower, or significantly lower in the case of
145
weighted cost, than using downsampling in one-
against-all manner.
Upsample WeightCost
Temp. 20.35* (16.8/25.9) 20.61* (16.9/26.3)
Comp. 28.11 (20.6/44.5) 28.38 (19.9/49.6)
Cont. 46.46 (37.4/61.3) 46.36 (34.6/70.1)
Exp. 54.93 (50.3/60.5) 57.43* (43.9/83.1)
Table 4: F-measure (precision/recall) of binary
classification: upsampling vs. weighted cost.
For Temporal and Comparison relations listed
in Table 4, we noticed an interesting similarity
between the F and precision values of upsam-
pling and weighted cost. To quantify this simi-
larity, we calculated the Q-statistic (Kuncheva and
Whitaker, 2003) between the two classifiers. The
Q-statistic is a measurement of classifier agree-
ment raging between -1 and 1, defined as:
Q
w,u
=
N
11
N
00
?N
01
N
10
N
11
N
00
+N
01
N
10
(2)
Where w denotes the system using weighted cost,
u denotes the upsampling system. N
11
means both
systems make a correct prediction, N
00
means
both systems are incorrect, N
10
means w is incor-
rect but u is correct, and N
01
means w is correct
but u is incorrect.
We have the following Q statistics: Tempo-
ral: 0.999, Comparison: 0.9938, Contingency:
0.9746, Expansion: 0.7762. These are good in-
dicators that for highly skewed relations, the two
methods give classifiers that behave almost identi-
cally on the test data. In the discussions that fol-
low, we discuss only weighted cost to avoid redun-
dancy.
5.2 Feature selection and shaping
While weighted cost or upsampling can give bet-
ter performance over downsampling for some rela-
tions, their disadvantages towards multi-class clas-
sification and the obvious favor towards the major-
ity class give rise to the following question: is it
possible to inform the classifier of the information
encoded in the annotation of all of the data while
still using downsampling to handle the skewed
class distribution? Our proposal is feature value
augmentation. Here we introduce a relational ma-
trix in which we calculate augmented feature val-
ues via feature shaping. We first compute the val-
ues of features on the entire training set, then use
the downsampled set for training with these val-
ues. In this way we pass on to the classifiers infor-
mation about the relative importance of features
gleaned from the entire training data.
5.2.1 Feature shaping
The idea of feature shaping was introduced in the
context of improving the performance of linear
SVMs (Forman et al., 2009). In linear SVMs
the prediction is based on a linear combination of
weight?feature values. The sign of weight indi-
cates the preference for a class (positive or nega-
tive), the value of the feature should correspond to
how strongly it indicates that class. Thus, features
that are strongly discriminative should have high
values so that they can contribute more to the final
class decision. Here we augment feature values
for a relation according to the following criteria:
1. Features are considered ?good? if they strongly
indicate the presence of the relation; 2. Features
are considered ?good? if they strongly indicate the
absence of the relation; 3. features are considered
?bad? if their presence give no information about
either the presence or the absence of the relation.
To capture this information, we first construct a
relation matrix M with each entry M
ij
defined as
the conditional probability of relationR
j
given the
feature F
i
computed as the maximum likelihood
estimate from the full training set:
M
ij
= P (R
j
|F
i
)
Each column of the relation matrix captures the
predictive power of each feature to a certain re-
lation. A feature with value M
ij
higher than the
column mean indicates that it is predictive for the
presence of relation j, while a feature with M
ij
lower than the mean is predictive for its absence;
the strength of such indication depends on how far
away M
ij
is from the mean: the further away it is,
the more valuable this feature should be for rela-
tion j. With this idea we give the following aug-
mented value for each feature:
M
?
ij
=
{
M
ij
, if M
ij
? ?
j
.
?
j
+ (?
j
?M
ij
), if M
ij
< ?
j
.
(3)
where ?
j
is the mean of the jth column corre-
sponding to the jth relation.
Given a feature F
i
, very small and very high
probabilities of a certain relation j, i.e., P (R
j
|F
i
),
are both useful information. However, in linear
SVMs, lower values of a feature would mean that
it contributes less to the decision of the class. By
146
feature shaping, we allow features that strongly in-
dicate the absence of a class to influence the deci-
sion and rely on the classifier to identify the nega-
tive association and reflect it by assigning a nega-
tive weight to these features.
When constructing the relation matrix, we used
the top four relation classes along with an En-
tRel/NoRel class. We computed the matrix before
downsampling to preserve the natural data distri-
bution and features that strongly indicate the ab-
sence of a class, then downsample the negative
data just like the previous downsampling setting.
5.2.2 Feature selection
The relation matrix also provides information for
feature selection using a binomial test for signifi-
cance, B(n,N, p), which gives the probability of
observing a feature n times in N instances of a
relation if the probability of any feature occurring
with the relation is p. For each relation, we use the
binomial test to pick the features that occur signif-
icantly more or less often than expected with the
relation. In the binomial test, p is set to be equal to
the probability of that relation in the PDTB train-
ing set. We select only the features which result in
a low p-value for the binomial test for at least some
relation. We used 9-fold cross validation on the
training data to pick the best p-values for each re-
lation individually; all best p-values were between
0.1 and 0.2.
Result listing Table 5 and Table 6 show the mul-
tiway and binary classification performance using
feature shaping and feature selection. We also
show the precision and recall for binary classifiers.
Multiway SVM One-Against-All
AllData 32.58 NA
Downsample NA 37.15
Upsample NA 36.63
Weighted Cost NA 34.23
Selection 32.52 38.42*
Shaping NA 38.81**
Shape+Sel NA 39.13**
Table 5: Balanced accuracy for multiway
SVM and one-against-all for 5-way classification.
One asterisk (*) means significantly better than
weighted cost and upsampling, and two means sig-
nificantly better than downsampling, at p < 0.05.
For multi-way classification, performing feature
shaping leads to significant improvements over
downsampling, upsampling and weighted cost.
The binomial method for feature selection that
relies on the full training data distribution has a
similar effect. Combined feature shaping and se-
lection leads to 2% absolute improvement in dis-
course relation recognition. For binary classifica-
tion, though, the improvement is significant only
for Temporal.
6 Classifier analysis and combination
6.1 Discussion of precision and recall
A careful examination of Tables 5 and 6 leads
to some intriguing observations. For the most
skewed relations, if we consider not only the F
measure, but also the precision and recall, there
is an interesting difference between the systems.
While downsampling has the lowest precision, it
gives the highest recall. The case for weighted cost
is another story. For highly skewed relations such
as Temporal and Comparison, it gives the highest
precision and the lowest recall; but as the data set
balances out in downsampling, the classifier shifts
towards high recall and low precision.
We can also rank the three feature augmentation
techniques in terms of how much they reflect dis-
tributional information in the training data. Fea-
ture selection reflects the training data least among
the three, because it uses information from all of
the data to select the features, but the feature val-
ues are still either 1 or 0. Feature shaping engages
more data because the value of a feature encodes
its relative ?effectiveness? for a relation. We can
see that feature selection gives slightly higher pre-
cision than just downsampling; feature shaping,
on the other hand, gives precision and recall val-
ues between these two. This is most obvious in
smaller relations, i.e. Temporal and Comparison.
To see if this trend is statistically significant, we
did a paired t-test over the precision and recall for
each system and each relation. For the Temporal
relation, all systems that use more data have sig-
nificantly higher (p < 0.05) precision than that
for downsampling. For Comparison, the changes
in precision are either significant or tend towards
significance for three methods: feature shaping
(p < 0.1), feature shaping+election (p < 0.1)
and weighted cost (p < 0.05). For Contingency,
feature shaping gives an improvement in precision
that tends toward significance (p < 0.1). The
drops in recall using feature shaping or weighted
cost for the above three relations are significant
(p < 0.05). For the Expansion relation, being the
largest class with 40% positive data, changes in
147
Downsample WeightCost Selection Shaping Shape+Sel
Temp. 15.52 (8.8/65.4) 20.61* (16.9/26.3) 18.47* (10.7/65.9) 20.37* (12.6/53.2) 21.30* (13.7/47.8)
Comp. 27.65 (17.3/69.2) 28.38 (19.9/49.6) 26.98 (17.4/60.1) 27.79 (18.3/58.2) 26.92 (18.7/48.2)
Cont. 47.14 (34.5/74.5) 46.36 (34.6/70.1) 47.45 (34.7/75.2) 47.62 (35.4/72.9) 46.93 (35.2/70.5)
Exp. 55.42 (49.3/63.3) 57.43* (43.9/83.1) 55.52 (49.3/63.5) 55.13 (49.3/62.5) 54.90 (49.2/62.1)
Table 6: F score (precision/recall) of classifiers with feature augmentation. Asterisk(*) means F score or
BAC is significantly greater than plain downsampling at p < 0.05.
precision and recall with downsampling systems
are not significant; yet weighted cost shifted to-
wards predicting more of the positive instances,
i.e., giving a significantly higher recall by trading
with a significantly lower precision (p < 0.05).
6.2 Discussion of classifier similarity
To better understand the differences of classi-
fier behaviors under the weighted cost and each
downsampling technique (plain downsampling,
feature selection, feature shaping, feature shap-
ing+selection), in Table 7 we show the percentage
of test instances that the weighted cost system and
each downsample system agree or do not agree. In
particular, we study the following situations:
1. The downsample system predicts correctly
but the weighted cost system does not (?D+C-?);
2. The weighted cost system predicts correctly
but the downsample system does not (?D-C+?);
3. Both systems are correct (?D+C+?).
At a glance of the Q statistic, it seems that the
systems are not behaving very differently. How-
ever, as only the sum of disagreements is reflected
in the Q statistic, we look more closely at where
the systems do not agree in each situation. If we
focus on the rarer Temporal and Comparison re-
lations, first note that in the plain downsampling
vs. weighted cost, the percentage of test instances
in the ?D+C-? column is much smaller than that
in the ?D-C+? column. This aligns with the above
observation that plain downsampling gives much
lower precision for these relations than weighted
cost. Now, as more data is engaged from first
using feature selection, then using feature shap-
ing, then using both, the percentage of instances
where both systems predict correctly increase. At
the same time, there is a drop in the percentage of
test instances in the ?D-C+? column. This trend is
also a reflection of the observation that as more
data is engaged, the precision got higher as the
recall drops lower. As the data gets more evenly
distributed, this phenomenon fades away. The ta-
ble also reveals a subtle difference between fea-
ture shaping and feature selection. Compared to
D+C- D-C+ D+C+ Q
(%) (%) (%) Stat
Temporal
Downsamp 2.56 28.27 61.47 0.73
Selection 2.91 22.04 67.71 0.77
Shaping 2.61 13.36 76.39 0.89
Sel+Shape 2.83 10.42 79.32 0.90
Comparison
Downsamp 5.74 18.24 53.76 0.84
Selection 7.72 16.14 55.85 0.80
Shaping 6.14 11.95 60.04 0.89
Sel+Shape 9.69 10.99 61.01 0.83
Contingency
Downsamp 6.88 7.89 58.74 0.93
Selection 8.01 8.92 57.70 0.91
Shaping 7.07 6.73 59.90 0.94
Sel+Shape 8.68 8.13 58.49 0.91
Expansion
Downsamp 16.39 8.23 44.66 0.82
Selection 17.87 9.71 43.18 0.76
Shaping 16.64 8.45 44.44 0.81
Sel+Shape 18.36 10.30 42.59 0.73
Table 7: Q statistics and agreements (in percent-
ages) of each downsampling system vs. weighted
cost. ?D? denotes the respective downsample sys-
tem in the left most column; ?C? denotes the
weighted cost system. A ?+? means that a system
makes a correct prediction; a ?-? means a system
makes an incorrect prediction.
downsampling, feature selection introduces an in-
crease in the column ?D+C-? (i.e. the weighted
cost system makes a mistake but the downsample
system is correct). Feature shaping, on the other
hand, do not necessarily increase this new kind of
difference between classifiers.
6.3 Classifier combination
Our classifier comparisons revealed that for highly
skewed distributions, there are consistent differ-
ences in the performance of classifiers obtained by
using the training data in different ways. It stands
to reason that a combination of these classifiers
with different strengths will result in an overall im-
proved classifier. This idea is explored here.
Suppose on a sample i, the downsampling clas-
sifier predicts the target class with confidence p
id
,
and the weighted cost classifier predicts the target
148
class with confidence p
ic
. Here again we approx-
imate the confidence of the class by the distance
from the hyperplane dividing the two classes. We
weight the two predictions and get a new predic-
tion confidence by:
p
?
i
=
?
d
p
id
+ ?
u
p
ic
?
d
+ ?
c
. (4)
where the ?s are parameters we want to encode
how much we trust each classifier. To get these
values, we train the classifiers and get the accura-
cies from each of them on the development set.
Since we are using linear SVMs in our experi-
ments, we mark the sample as positive if p
i
> 0,
and negative otherwise.
The results for the combination are shown in Ta-
ble 8. We include the original performances of the
classifiers by themselves for reference.
F measure For Temporal, the combined classi-
fier performs better than the original classifiers.
We see significant (p < 0.05) improvements over
the corresponding downsampling system and the
weighted cost system. If feature shaping is in-
volved in the combination, it is also having bet-
ter performance that tend toward significance (p <
0.1) over the weighted cost classifier. For Compar-
ison, the benefits of a combined system is also ob-
vious for feature shaping and/or selection. Feature
shaping combined with weighted cost gives sig-
nificantly (p < 0.05) better performance than ei-
ther of them individually, and feature selection and
shaping+selection combined with weighted cost is
better than themselves alone. For Contingency,
though weighted cost do not give better results, the
improvement tends toward significance (p < 0.1)
when combined with plain downsampling. For Ex-
pansion where weighted cost gives the lowest pre-
cision, combination with other classifiers do not
give significant improvements over F scores.
Precision and recall We can also compare the
precision and recall for each system before and af-
ter combination. In all but one cases for Temporal
and Comparison, we observe significantly higher
precision and much lower recall after the combi-
nation. The case for Expansion is just the opposite
as expected.
7 Conclusion
In this paper, we studied the effect of the use of an-
notated data for binary and multiway classification
Original Combined
Classifier Classifier
Temporal
WeightCost 20.61 (16.9/26.3)
Downsamp 15.52 (8.8/65.4) 21.78* (14.9/40.5)
Selection 18.47 (10.7/65.9) 22.99* (15.8/42.0)
Shaping 20.37 (12.6/53.2) 23.88* (17.5/37.6)
Sel+Shape 21.30 (13.7/47.8) 23.72* (17.7/36.1)
Comparison
WeightCost 28.38 (19.9/49.6)
Downsamp 27.65 (17.3/69.2) 28.72 (19.3/56.4)
Selection 26.98 (17.4/60.1) 29.25? (20.1/54.0)
Shaping 27.79 (18.3/58.2) 29.89*
+
(20.5/54.9)
Sel+Shape 26.92 (18.7/48.2) 29.83* (21.3/50.0)
Contingency
WeightCost 46.36 (34.6/70.1)
Downsamp 47.14 (34.5/74.5) 48.38
+
(35.9/74.4)
Selection 47.45 (34.7/75.2) 47.76
+
(35.5/72.9)
Shaping 47.62 (35.4/72.9) 48.16
+
(36.0/72.9)
Sel+Shape 46.93 (35.2/70.5) 47.37 (35.6/70.7)
Expansion
WeightCost 57.43 (43.9/83.1)
Downsamp 55.42 (49.3/63.3) 56.61* (46.4/72.7)
Selection 55.52 (49.3/63.5) 57.10* (46.5/73.0)
Shaping 55.13 (49.3/62.5) 56.74* (46.4/73.0)
Sel+Shape 54.90 (49.2/62.1) 57.06* (46.4/74.0)
Table 8: Classifier combination results for binary
classification. An asterisk(*) means significantly
better than the corresponding downsampling sys-
tem at, and a plus(+) means significantly better
than weighted cost, at p < 0.05. Improvements
that tend toward significance (p < 0.1) are not
shown here but are discussed in the text.
in supervised implicit discourse relation recogni-
tion. The starting point of our work was to estab-
lish the effectiveness of downsampling negative
examples, which was practiced but not experimen-
tally investigated in prior work. We also evalu-
ated alternative solutions to the skewed data prob-
lem, as downsampling throws away most of the
data. We examined the effect of upsampling and
weighted cost. In addition, we introduced the rela-
tion matrix to give more emphasis on informative
features through augmenting the feature value via
feature shaping. We found that as we summarize
more detailed information about the data in the full
training set, performance for multiway classifica-
tion gets better. We also observed through preci-
sion and recall that there are fundamental differ-
ences between downsampling and weighted cost,
and this difference can be beneficially exploited
by combining the two classifiers. We showed that
our way of doing such combination gives signifi-
cantly higher performance results for binary clas-
sification in the case of rarer relations.
149
References
Rehan Akbani, Stephen Kwek, and Nathalie Japkow-
icz. 2004. Applying support vector machines to
imbalanced datasets. In Machine Learning: ECML
2004, pages 39?50.
Gustavo E. A. P. A. Batista, Ronaldo C. Prati, and
Maria Carolina Monard. 2004. A study of the
behavior of several methods for balancing machine
learning training data. ACM SIGKDD Explorations
Newsletter - Special issue on learning from imbal-
anced datasets, 6(1):20?29, June.
Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL): Short Papers, pages 69?73.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. SMOTE:
Synthetic minority over-sampling technique. Jour-
nal of Artificial Intelligence Research, 16(1):321?
357, June.
R. Elwell and J. Baldridge. 2008. Discourse connec-
tive argument identification with connective specific
rankers. In IEEE International Conference on Se-
mantic Computing (IEEE-ICSC), pages 198 ?205.
George Forman, Martin Scholz, and Shyamsundar Ra-
jaram. 2009. Feature shaping for linear SVM classi-
fiers. In Proceedings of the 15th ACM International
Conference on Knowledge Discovery and Data Min-
ing (KDD), pages 299?308.
Sucheta Ghosh, Richard Johansson, Giuseppe Ric-
cardi, and Sara Tonelli. 2011. Shallow discourse
parsing with conditional random fields. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing (IJCNLP), pages
1071?1079.
Barbara J. Grosz, Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21:203?225.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010. A semi-supervised approach to im-
prove classification of infrequent discourse relations
using feature vector extension. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 399?409.
Yu Hong, Xiaopei Zhou, Tingting Che, Jianmin Yao,
Qiaoming Zhu, and Guodong Zhou. 2012. Cross-
argument inference for implicit discourse relation
recognition. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 295?304.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169?184.
Ludmila I. Kuncheva and Christopher J. Whitaker.
2003. Measures of diversity in classifier ensembles
and their relationship with the ensemble accuracy.
Machine Learning, 51(2):181?207, May.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151?184, 4.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-
avind Joshi. 2008. Sense annotation in the Penn
Discourse Treebank. In Proceedings of the 9th
International Conference on Computational Lin-
guistics and Intelligent Text Processing (CICLing),
pages 275?286.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning
with a knowledge-based approach - a case study in
intensive care monitoring. In Proceedings of the Six-
teenth International Conference on Machine Learn-
ing (ICML), pages 268?277.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL), pages 108?112.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Eas-
ily identifiable discourse relations. In Proceed-
ings of the International Conference on Computa-
tional Linguistics (COLING): Companion volume:
Posters, pages 87?90.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP (ACL-IJCNLP),
pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC).
Konstantinos Veropoulos, Colin Campbell, and Nello
Cristianini. 1999. Controlling the sensitivity of sup-
port vector machines. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence
(IJCAI), volume 1999, pages 55?60.
150
Proceedings of the SIGDIAL 2014 Conference, pages 199?207,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Reducing Sparsity Improves the Recognition of Implicit Discourse
Relations
Junyi Jessy Li
University of Pennsylvania
ljunyi@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
The earliest work on automatic detec-
tion of implicit discourse relations relied
on lexical features. More recently, re-
searchers have demonstrated that syntactic
features are superior to lexical features for
the task. In this paper we re-examine the
two classes of state of the art representa-
tions: syntactic production rules and word
pair features. In particular, we focus on the
need to reduce sparsity in instance repre-
sentation, demonstrating that different rep-
resentation choices even for the same class
of features may exacerbate sparsity issues
and reduce performance. We present re-
sults that clearly reveal that lexicalization
of the syntactic features is necessary for
good performance. We introduce a novel,
less sparse, syntactic representation which
leads to improvement in discourse rela-
tion recognition. Finally, we demonstrate
that classifiers trained on different repre-
sentations, especially lexical ones, behave
rather differently and thus could likely be
combined in future systems.
1 Introduction
Implicit discourse relations hold between adjacent
sentences in the same paragraph, and are not sig-
naled by any of the common explicit discourse
connectives such as because, however, meanwhile,
etc. Consider the two examples below, drawn from
the Penn Discourse Treebank (PDTB) (Prasad et
al., 2008), of a causal and a contrast relation, re-
spectively. The italic and bold fonts mark the ar-
guments of the relation, i.e the portions of the text
connected by the discourse relation.
Ex1: Mrs Yeargin is lying. [Implicit = BECAUSE] They
found students in an advanced class a year earlier who
said she gave them similar help.
Ex2: Back downtown, the execs squeezed in a few meetings at
the hotel before boarding the buses again. [Implicit = BUT]
This time, it was for dinner and dancing - a block away.
The task is undisputedly hard, partly because it
is hard to come up with intuitive feature represen-
tations for the problem. Lexical and syntactic fea-
tures form the basis of the most successful stud-
ies on supervised prediction of implicit discourse
relations in the PDTB. Lexical features were the
focus of the earliest work in discourse recogni-
tion, when cross product of words (word pairs)
in the two spans connected via a discourse re-
lation was studied. Later, grammatical produc-
tions were found to be more effective. Features
of other classes such as verbs, inquirer tags, posi-
tions were also studied, but they only marginally
improve upon syntactic features.
In this study, we compare the most commonly
used lexical and syntactic features. We show that
representations that minimize sparsity issues are
superior to their sparse counterparts, i.e. the bet-
ter representations are those for which informative
features occur in larger portions of the data. Not
surprisingly, lexical features are more sparse (oc-
curring in fewer instances in the dataset) than syn-
tactic features; the superiority of syntactic repre-
sentations may thus be partially explained by this
property.
More surprising findings come from a closer ex-
amination of instance representation approaches
in prior work. We first discuss how choices in
prior work have in fact exacerbated the sparsity
problem of lexical features. Then, we introduce
a new syntactically informed feature class, which
is less sparse than prior lexical and syntactic fea-
tures, and improves significantly the classification
of implicit discourse relations.
Given these findings, we address the question
if any lexical information at all should be pre-
served in discourse parsers. We find that purely
syntactic representations show lower recognition
199
for most relations, indicating that lexical features,
albeit sparse, are necessary for the task. Lexical
features also account for a high percentage of the
most predictive features.
We further quantify the agreement of predic-
tions produced from classifiers using different in-
stance representations. We find that our novel syn-
tactic representation is better for implicit discourse
relation prediction than prior syntactic feature be-
cause it has higher overall accuracy and makes
correct predictions for instances for which the al-
ternative representations are also correct. Differ-
ent representation of lexical features however ap-
pear complementary to each other, with markedly
higher fraction of instances recognized correctly
by only one of the models.
Our work advances the state of the art in implicit
discourse recognition by clarifying the extent to
which sparsity issues influence predictions, by in-
troducing a strong syntactic representation and by
documenting the need for further more complex
integration of lexical information.
2 The Penn Discourse Treebank
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) contains annotations for five types of
discourse relations over the Penn Treebank corpus
(Marcus et al., 1993). Explicit relations are those
signaled by a discourse connective that occurs in
the text, such as ?because?, ?however?, ?for ex-
ample?. Implicit relations are annotated between
adjacent sentences in the same paragraph. There
are no discourse connectives between the two sen-
tences, and the annotators were asked to insert a
connective while marking their senses. Some pairs
of sentences do not contain one of the explicit dis-
course connectives, but the insertion of a connec-
tive provides redundant information into the text.
For example, they may contain phrases such as
?the consequence of the act?. These are marked
Alternative Lexicalizations (AltLex). Entity rela-
tions (EntRel) are adjacent sentences that are only
related via the same entity or topic. Finally, sen-
tences where no discourse relations were identi-
fied were marked NoRel. In this work, we consider
AltLex to be part of the Implicit relations, and En-
tRel to be part of NoRel.
All connectives, either explicit or implicitly in-
serted, are associated with two arguments of the
minimal span of text conveying the semantic con-
tent between which the relation holds. This is il-
lustrated in the following example where the two
arguments are marked in bold and italic:
Ex: They stopped delivering junk mail. [Implicit = SO] Now
thousands of mailers go straight into the trash.
Relation senses in the PDTB are drawn from
a 3-level hierarchy. The top level relations are
Comparison (arg1 and arg2 holds a contrast rela-
tion), Contingency (arg1 and arg2 are causally re-
lated), Expansion (arg2 further describes arg1) and
Temporal (arg1 and arg2 are temporally related).
Some of the largest second-tier relations are under
Expansion, which include Conjunction (arg2 pro-
vides new information to arg1), Instantiation (arg2
exemplifies arg1) and Restatement (arg2 semanti-
cally repeats arg1).
In our experiments we use the four top level re-
lations as well as the above three subclasses of Ex-
pansion. All of these subclasses occur with fre-
quencies similar to those of the Contingency and
Comparison classes, with thousands of examples
in the PDTB.
1
We show the distribution of the
classes below:
Temporal 1038 Comparison 2550
Contingency 4532 Instantiation 1483
Restatement 3271 Conjunction 3646
EntRel/NoRel 5464
3 Experimental settings
In our experiments we use only lexical and syntac-
tic features. This choice is motivated by the fact
that lexical features have been used most widely
for the task and that recent work has demon-
strated that syntactic features are the single best
type of representation. Adding additional features
only minimally improves performance (Lin et al.,
2009). By zeroing in only on these classes of fea-
tures we are able to discuss more clearly the im-
pact that different instance representation have on
sparsity and classifier performance.
We use gold-standard parses from the original
Penn Treebank for syntax features.
To ensure that our conclusions are based on
analysis of the most common relations, we train
binary SVM classifiers
2
for the seven relations de-
scribed above. We adopt the standard practice in
1
All other sub-classes of implicit relations are too small
for general practical applications. For example the Alterna-
tive class and Concession class have only 185 and 228 oc-
currences, respectively, in the 16,224 implicit relation anno-
tations of the PDTB.
2
We use SVMLight (Joachims, 1999) with linear kernel.
200
prior work and downsampled the negative class so
the number of positive and negative samples are
equal in the training set.
3
Our training set consists of PDTB sections 2-
19. The testing set consists of sections 20-24. Like
most studies, we do not include sections 0-1 in the
training set. We expanded the test set (sections 23
or 23-24) used in previous work (Lin et al., 2014;
Park and Cardie, 2012) to ensure the number of
examples of the smaller relations, particularly of
Temporal or Instantiation, are suitable for carrying
out reliable tests for statistical significance.
Some of the discourse relations are much larger
than others, so we report our results in term of F-
measure for each relation and average unweighted
accuracy. Significance tests over F scores were
carried out using a paired t-test. To do this, the
test set is randomly partitioned into ten groups. In
each group, the relation distribution was kept as
close as possible to the overall test set.
4 Sparsity and pure lexical
representations
By far the most common features used for rep-
resenting implicit discourse relations are lexical
(Sporleder and Lascarides, 2008; Pitler et al.,
2009; Lin et al., 2009; Hernault et al., 2010;
Park and Cardie, 2012). Early studies have sug-
gested that lexical features, word pairs (cross-
product of the words in the first and second ar-
gument) in particular, will be powerful predictors
of discourse relations (Marcu and Echihabi, 2002;
Blair-Goldensohn et al., 2007). The intuition be-
hind word pairs was that semantic relations be-
tween the lexical items, such as drought?famine,
child?adult, may in turn signal causal or contrast
discourse relations. Later it has been shown that
word pair features do not appear to capture such
semantic relationship between words (Pitler et al.,
2009) and that syntactic features lead to higher ac-
curacies (Lin et al., 2009; Zhou et al., 2010; Park
and Cardie, 2012). Recently, Biran and McKeown
(2013) aggregated word pair features with explicit
connectives and reported improvements over the
original word pairs as features.
In this section, we show that the representation
of lexical features play a direct role in feature spar-
sity and ultimately affects prediction performance.
The first two studies that specifically addressed
3
We also did not include features that occurred less than
5 times in the training set.
# Features Avg. F Avg. Accuracy
word-pairs 92128 29.46 57.22
binary-lexical 12116 31.79 60.42
Table 1: F-scores and average accuracies of paired
and binary representations of words.
the problem of predicting implicit discourse re-
lations in the PDTB made use of very different
instance representations. Pitler et al. (2009) rep-
resent instances of discourse relations in a vec-
tor space defined by word pairs, i.e. the cross-
product of the words that appear in the two argu-
ments of the relation. There, features are of the
form (w
1
, w
2
) where w
1
? arg1 and w
2
? arg2.
If there are N words in the entire vocabulary, the
size of each instance would be N ?N .
In contrast, Lin et al. (2009) represent instances
by tracking the occurrences of grammatical pro-
ductions in the syntactic parse of argument spans.
There are three indicator features associated with
each production: whether the production appears
in arg1, in arg2, and in both arguments. For a
grammar with N production rules, the size of the
vector representing an instance will be 3N . For
convenience we call this ?binary representation?,
in contrast to the word-pair features in which the
cross product of words constitute the representa-
tion. Note that the cross-product approach has
been extended to a wide variety of features (Pitler
et al., 2009; Park and Cardie, 2012). In the ex-
periments that follow we will demonstrate that bi-
nary representations lead to less sparse features
and higher prediction accuracy.
Lin et al. (2009) found that their syntactic fea-
tures are more powerful than the word pair fea-
tures. Here we show that the advantage comes not
only from the inclusion of syntactic information
but also from the less sparse instance representa-
tion they used for syntactic features. In Table 1
we show the number of features for each repre-
sentation and the average F score and accuracy for
word pairs and words with binary representation
(binary-lexical). The results for each relation are
shown in Table 8 and discussed in Section 7.
Using binary representation for lexical informa-
tion outperforms word pairs. Thus, the difference
in how lexical information is represented accounts
for a considerable portion of the improvement re-
ported in Lin et al. (2009). Most notably, for the
Instantiation class, we see a 7.7% increase in F-
score. On average, the less sparse representation
201
translates into 2.34% absolute improvement in F-
score and 3.2% absolute improvement in accuracy.
From this point on we adopt the binary represen-
tation for the features discussed.
5 Sparsity and syntactic features
Grammatical production rules were first used for
discourse relation representation in Lin et al.
(2009). They were identified as the most suitable
representation, that lead to highest performance in
a couple of independent studies (Lin et al., 2009;
Park and Cardie, 2012). The comparison repre-
sentations covered a number of semantic classes
related to sentiment, polarity and verb information
and dependency representations of syntax.
Production rules correspond to tree chunks in
the constituency parse of a sentence, i.e. a node
in the syntactic parse tree with all of its children,
which in turn correspond to grammar rules ap-
plied in the derivation of the tree, such as S?NP
VP. This syntactic representation subsumes lexi-
cal representations because of the production rules
with part-of-speech on the left-hand side and a lex-
ical item on the right-hand side.
We propose that the sparsity of production rules
can be reduced even further by introducing a new
representation of the parse tree. Specifically, in-
stead of having full production rules where a sin-
gle feature records the parent and all its children,
all (parent,child) pairs in the constituency parse
tree are used. For example, the rule S?NP VP
will now become two features, S?NP and S?VP.
Note that the leaves of the tree, i.e. the part-of-
speech?word features are not changed. For ease
of reference we call this new representation ?pro-
duction sticks?. In this section we show that F
scores and accuracies for implicit discourse rela-
tion prediction based on production sticks is sig-
nificantly higher than using full production rules.
First, Table 2 illustrates the contrast in sparsity
among the lexical, production rule and stick repre-
sentations. The table gives the rate of occurrence
of each feature class, which is defined as the av-
erage fraction of features with non-zero values in
the representation of instances in the entire train-
ing set. Specifically, let N be the total number of
features, m
i
be the number of features triggered in
instance i, then the rate of occurrence is
m
i
N
.
The table clearly shows that the number of fea-
tures in the three representations is comparable,
but they vary notably in their rate of occurrence.
# Features Rate of Occurrence
sticks 14,165 0.00623
prodrules 16,173 0.00374
binary-lexical 12,116 0.00276
word-pairs 92,128 0.00113
Table 2: Number of features and rate of occur-
rence for binary lexical representation, production
rules and sticks.
Avg. F Avg. Accuracy
sticks 34.73 64.89
prodrules 33.69 63.55
binary-lexical 31.79 60.42
word-pairs 29.46 57.22
Table 3: F-scores and average accuracies of pro-
duction rules and production sticks.
Sticks have almost twice the rate of occurrence of
that of full production rules. Both syntactic rep-
resentations have much larger rate of occurrence
than lexical features, and the rate of occurrence of
word pairs is more than twice smaller than that of
the binary lexical representation.
Next, in Table 3, we give binary classifica-
tion prediction results based on both full rules
and sticks. The first two rows of Table 3 com-
pare full production rules (prodrules) with produc-
tion sticks (sticks) using the binary representation.
They both outperform the binary lexical represen-
tation. Again our results confirm that the better
performance of production rule features is partly
because they are less sparse than lexical represen-
tations, with an average of 1.04% F-score increase.
Individually the F scores of 6 of the 7 relations are
improved as shown in Table 8.
6 How important are lexical features?
Production rules or sticks include lexical items
with their part-of-speech tags. These are the sub-
set of features that contribute most to sparsity is-
sues. In this section we test if these lexical fea-
tures contribute to the performance or if they can
be removed without noticeable degradation due to
its intrinsic sparsity. It turns out that it is not ad-
visable to remove the lexical features entirely, as
performance decreases substantially if we do so.
6.1 Classification without lexical items
We start our exploration of the influence of lexical
items on the accuracy of prediction by inspecting
the performances of the classifiers with production
rules and sticks, but without the lexical items and
their parts of speech. Table 4 lists the average F
202
Avg. F Avg. Accuracy
prodrules 33.69 63.55
sticks 34.73 64.89
prodrules-nolex 32.30 62.03
sticks-nolex 33.86 63.99
Table 4: F-scores and average accuracies of pro-
duction rules and sticks, with (rows 1-2) and with-
out (rows 3-4) lexical items.
# Features Rate of Occurrence
prodrules 16,173 0.00374
sticks 14,165 0.00623
prodrules-nolex 3470 0.00902
sticks-nolex 922 0.0619
Table 5: Number of features and rate of occur-
rence for production rules and sticks, with (rows
1-2) and without (rows 3-4) lexical items.
scores and accuracies. Table 8 provides detailed
results for individual relations. Here prodrules-
nolex and sticks-nolex denote full production rules
without lexical items, and production sticks with-
out lexical items, respectively. In all but two re-
lations, lexical items contribute to better classifier
performance.
When lexical items are not included in the rep-
resentation, the number of features is reduced to
fewer than 30% of that in the original full produc-
tion rules. At the same time however, including
the lexical items in the representation improves
performance even more than introducing the less
sparse production stick representation. Production
sticks with lexical information also perform bet-
ter than the same representation without the POS-
word sticks.
The number of features and their rates of occur-
rences are listed in Table 5. It again confirms that
the less sparse stick representation leads to better
classifier performance. Not surprisingly, purely
syntactic features (without the lexical items) are
much less sparse than syntax features with lexical
items present. However the classifier performance
is worse without the lexical features. This contrast
highlights the importance of a reasonable tradeoff
between attempts to reduce sparsity and the need
to preserve lexical features.
6.2 Feature selection
So far our discussion was based on the behavior
of models trained on a complete set of relatively
frequent syntactic and lexical features (occurring
more than five times in the training data). Feature
selection is a way to reasonably prune out the set
Relation %-nonlex %-allfeats
Temporal 25.56 10.95
Comparison 25.40 15.51
Contingency 20.12 25.05
Conjunction 21.15 19.20
Instantiation 25.08 16.16
Restatement 22.16 17.35
Expansion 18.36 18.66
Table 6: Non-lexical features selected using fea-
ture selection. %-nonlex records the percentage of
non-lexical features among all features selected;
%-allfeats records the percentage of selected non-
lexical features among all non-lexical features.
and reduce sparsity issues in the model. In fact
feature selection has been used in the majority of
prior work (Pitler et al., 2009; Lin et al., 2009;
Park and Cardie, 2012).
Here we perform feature selection and exam-
ine the proportion of syntactic and lexical features
among the most informative features. We use the
?
2
test of independence, computed on the follow-
ing contingency table for each feature F
i
and for
each relation R
j
:
F
i
?R
j
|F
i
? ?R
j
?F
i
?R
j
|?F
i
? ?R
j
Each cell in the above table records the num-
ber of training instances in which F
i
and R
j
are
present or absent. We set our level of confidence
to p < 0.1.
Table 6 lists the proportions of non-lexical items
among the most informative features selected (col-
umn 2). It also lists the percentage of selected non-
lexical items among all the 922 purely syntactic
features from production rule and production stick
representations (column 3). For all relations, at
most about a quarter of the most informative fea-
tures are non-lexical and they only take up 10%-
25% of all possible non-lexical features. The pre-
diction results using only these features are either
higher than or comparable to that without feature
selection (sticks-?
2
in Table 8). These numbers
suggest that lexical terms play a significant role as
part of the syntactic representations.
In Table 8 we record the F scores and accura-
cies for each relation under each feature represen-
tation. The representations are sorted according to
descending F scores for each relation. Notice that
?
2
feature selection on sticks is the best represen-
tation for the three smallest relations: Compari-
son, Instantiation and Temporal.
203
This finding led us to look into the selected lex-
ical features for these three classes. We found that
these most prominent features in fact capture some
semantic information. We list the top ten most pre-
dictive lexical features for these three relations be-
low, with examples. Somewhat disturbingly, many
of them are style or domain specific to the Wall
Street Journal that PDTB was built on.
Comparison a1a2 NN share a1a2 NNS cents a1a2 CC or
a1a2 CD million a1a2 QP $ a1a2 NP $ a2 RB n?t
a1a2 NN % a2 JJ year a2 IN of
For Comparison (contrast), the top lexical fea-
tures are words that occur in both argument 1 and
argument 2. Contrast within the financial domain,
such as ?share?, ?cents? and numbers between ar-
guments are captured by these features. Consider
the following example:
Ex. Analyst estimate the value of the BellSouth proposal at
about $115 to $125 a share. [Implicit=AND] They value
McCaw?s bid at $112 to $118 a share .
Here the contrast clearly happens with the value
estimation for two different parties.
Instantiation a2 SINV ? a2 SINV , a2 SINV ? a2 SINV .
a1 DT some a2 S a2 VBZ says a1 NP , a2 NP , a1 DT a
For Instantiation (arg2 gives an example of
arg1), besides words such as ?some? or ?a? that
sometimes mark a set of events, many attribution
features are selected. it turns out many Instanti-
ation instances in the PDTB involve argument 2
being an inverted declarative sentence that signals
a quote as illustrate by the following example:
Ex. Unease is widespread among exchange members. [Im-
plicit=FOR EXAMPLE] ? I can?t think of any reason to
join Lloyd?s now, ? says Keith Whitten, a British business-
man and a Lloyd?s member since 1979.
Temporal a1 VBD plunged a2 VBZ is a2 RB later
a1 VBD was a2 VBD responded a1a2 PRP he
a1 WRB when a1 PRP he a1 VBZ is a2 VBP are
For Temporal, verbs like plunge and responded
are selected. Words such as plunged are quite do-
main specific to stock markets, but words such as
later and responded are likely more general indi-
cators of the relation.
The presence of pronouns was also a predictive
feature. Consider the following example:
Ex. A Yale law school graduate , he began his career in cor-
porate law and then put in years at Metromedia Inc. and the
William Morris talent agency. [Implicit=THEN] In 1976, he
joined CBS Sports to head business affairs and, five years
later, became its president.
Overall, it is fairly easy to see that certain se-
mantic information was captured by these fea-
tures, such as similar structures in a pair of sen-
tences holding a contrast relation, the use of verbs
in a Temporal relation. However, it is rather unset-
tling to also see that some of these characteristics
are largely style or domain specific. For exam-
ple, for an Instantiation in an educational scenario
where the tutor provides an example for a concept,
it is highly unlikely that attribution features will be
helpful. Therefore, part of the question of finding
a general class of features that carry over to other
styles or domains of text still remain unanswered.
7 Per-relation evaluation
Table 8 lists the F-scores and accuracies of each
representation mentioned in this work for predict-
ing individual relation classes. For each relation,
the representations are ordered by decreasing F-
score. We tested the results for statistical signifi-
cance of the change in F-score. We compare all
the representations with the best and the worse
representations for the relation. A ?Y? marks a
significance level of p ? 0.05 for the comparison
with the best or worst representation, a ?T? marks
a significance level of p ? 0.1, which means a
tendency towards significance.
For all relations, production sticks, either with
or without feature selection, is the top represen-
tation. Sticks without lexical items also under-
perform those including the lexical items for 6 of
the 7 relations. Notably, production rules without
lexical items are among the three worst represen-
tations, outperforming only the pure lexical fea-
tures in some cases. This is a strong indication
that being both a sparse syntactic representation
and lacking lexical information, these features are
not favored in this task. Pure lexical features give
the worst or second to worst F scores, significantly
worse than the alternatives in most of the cases.
In Table 7 we list the binary classification re-
sults from prior work: feature selected word pairs
(Pitler et al., 2009), aggregated word pairs (Biran
and McKeown, 2013), production rules only (Park
and Cardie, 2012), and the best combination pos-
sible from a variety of features (Park and Cardie,
2012), all of which include production rules. We
aim to compare the relative gains in performance
with different representations. Note that the abso-
lute results from prior work are not exactly com-
parable to ours for two reasons ? the training
204
Sys. Pitler et al. Biran-McKeown
Feat. wordpair-implicit aggregated wp
Comp. 20.96 (42.55) 24.38 (61.72)
Cont. 43.79 (61.92) 44.03 (66.78)
Expa. 63.84 (60.28) 66.48 (60.93)
Temp. 16.21 (61.98) 19.54 (68.09)
Sys. Park-Cardie Park-Cardie
Feat. prodrules best combination
Comp. 30.04 (75.84) 31.32 (74.66)
Cont. 47.80 (71.90) 49.82 (72.09)
Expa. 77.64 (69.60) 79.22 (69.14)
Temp. 20.96 (63.36) 26.57 (79.32)
Table 7: F-score (accuracy) of prior systems. Note
that the absolute numbers are not exactly compa-
rable with ours because of the important reasons
explained in this section.
and testing sets are different; how Expansion, En-
tRel/NoRel and AltLex relations are treated differ-
ently in each work. The only meaningful indicator
here is the absolute size of improvement. The table
shows that our introduction of production sticks
led to improvements comparable to those reported
in prior work.
The aggregated word pair is a less sparse ver-
sion of the word pair features, where each pair
is converted into weights associated with an ex-
plicit connective. Just as the less sparse binary
lexical representation presented previously, the ag-
gregated word pairs also gave better performance.
None of the three lexical features, however, sur-
passes raw production rules, which again echoes
our finding that binary lexical features are not bet-
ter than the full production rules. Finally, we
note that a combination of features gives better F-
scores.
8 Discussion: are the features
complementary?
So far we have discussed how different represen-
tations for lexical and syntactic features can af-
fect the classifier performances. We focused on
the dilemma of how to reduce sparsity while still
preserving the useful lexical features. An impor-
tant question remains as whether these representa-
tions are complementary, that is, how different is
the classifier behaving under different feature sets
and if it makes sense to combine the features.
We compare the classifier output on the test data
with two methods in Table 9: the Q-statistic and
the percentage of data which the two classifiers
disagree (Kuncheva and Whitaker, 2003).
sig- sig-
Representation F (A) best worst
Comparison
sticks-?
2
27.78 (62.83) N/A Y
prodrules 27.65 (59.5) - Y
sticks 27.50 (60.73) - Y
sticks-nolex 27.01 (59.63) - Y
prodrules-nolex 26.40 (58.47) T Y
binary-lexical 24.73 (58.32) Y -
word-pairs 22.68 (45.03) Y N/A
Conjunction
sticks 27.55 (63.82) N/A T
sticks-?
2
27.53 (64.06) - T
prodrules 27.02 (63.91) - -
sticks-nolex 26.56 (61.03) T -
binary-lexical 25.90 (61.77) Y -
prodrules-nolex 25.20 (62.83) T N/A
word-pairs 25.18 (74.51) T -
Contingency
sticks 48.90 (67.49) N/A Y
sticks-?
2
48.55 (67.76) - Y
sticks-nolex 48.08 (67.69) - Y
prodrules 47.14 (65.61) T Y
prodrules-nolex 45.79 (63.99) Y Y
binary-lexical 44.17 (62.68) Y Y
word-pairs 40.57 (50.53) Y N/A
Expansion
sticks 56.48 (61.75) N/A Y
sticks-?
2
56.30 (62.26) - Y
sticks-nolex 55.43 (60.56) - Y
prodrules 55.42 (61.05) - Y
binary-lexical 54.20 (59.26) Y -
word-pairs 53.65 (56.64) Y -
prodrules-nolex 53.53 (58.79) Y N/A
Instantiation
sticks-?
2
30.34 (74.54) N/A Y
sticks 29.93 (73.80) - Y
prodrules 29.59 (72.20) - Y
sticks-nolex 28.22 (72.66) Y Y
prodrules-nolex 27.83 (70.72) Y Y
binary-lexical 27.29 (70.05) Y Y
word-pairs 20.22 (51.00) Y N/A
Restatement
sticks 35.74 (61.45) N/A Y
sticks-?
2
34.93 (61.42) - Y
sticks-nolex 34.62 (61.08) T Y
prodrules 33.52 (58.54) T Y
prodrules-nolex 32.05 (56.84) Y -
binary-lexical 31.27 (57.41) Y T
word-pairs 29.81 (47.42) Y N/A
Temporal
sticks-?
2
17.97 (66.67) N/A Y
sticks-nolex 17.08 (65.27) T Y
sticks 17.04 (65.22) T Y
prodrules 15.51 (64.04) Y -
prodrules-nolex 15.29 (62.56) Y -
binary-lexical 14.97 (61.92) Y -
word-pairs 14.10 (75.38) Y N/A
Table 8: F-score (accuracy) of each relation for
each feature representation. The representations
in each relation are sorted in descending order.
The column ?sig-best? marks the significance test
result against the best representation, the col-
umn ?sig-worst? marks the significance test re-
sult against the worst representation. ?Y? denotes
p ? 0.05, ?T? denotes p ? 0.1.
205
Q-statistic is a measure of agreement between
two systems s
1
and s
2
formulated as follows:
Q
s
1
,s
2
=
N
11
N
00
?N
01
N
10
N
11
N
00
+N
01
N
10
Where N denotes the number of instances, a sub-
script 1 on the left means s
1
is correct, and a sub-
script 1 on the right means s
2
is correct.
There are several rather surprising findings.
Most notably, word pairs and binary lexical repre-
sentations give very different classification results
in each relation. Their predictions disagree on at
least 25% of the data. This finding drastically con-
trast the fact that they are both lexical features and
that they both make use of the argument annota-
tions in the PDTB. A comparison of the percent-
ages and their differences in F scores or accuracies
easily shows that it is not the case that binary lex-
ical models correctly predict instances word pairs
made mistakes on, but that they are disagreeing in
both ways. Thus, given the previous discussion
that lexical items are useful, it is possible the most
suitable representation would combine both views
of lexical distribution.
Even more surprisingly, the difference in classi-
fier behavior is not as big when we compare lex-
ical and syntactic representations. The disagree-
ment of production sticks with and without lexi-
cal features are the smallest, even though, as we
have shown previously, the majority of production
sticks are lexical features with part-of-speech tags.
If we compare binary lexical features with produc-
tion sticks, the disagreement becomes bigger, but
still not as big as word pairs vs. binary lexical.
Besides the differences in classification, the big-
ger picture of improving implicit discourse rela-
tion classification is finding a set of feature repre-
sentations that are able to complement each other
to improve the classification. A direct conclusion
here is that one should not limit the focus on fea-
tures in different categories (for example, lexical
or syntax), but also features in the same category
represented differently (for example, word pairs or
binary lexical).
9 Conclusion
In this work we study implicit discourse relation
classification from the perspective of the interplay
between lexical and syntactic feature representa-
tion. We are particularly interested in the trade-
off between reducing sparsity and preserving lex-
ical features. We first emphasize the important
Rel. Q-stat Disagreement
word-pairs vs. binary-lexical
Comparison 0.65 33.55
Conjunction 0.71 28.47
Contingency 0.81 26.35
Expansion 0.69 29.38
Instantiation 0.75 31.33
Restatement 0.76 28.42
Temporal 0.25 25.34
binary-lexical vs. sticks
Comparison 0.78 25.49
Conjunction 0.78 24.67
Contingency 0.86 20.68
Expansion 0.80 24.28
Instantiation 0.83 20.75
Restatement 0.76 26.72
Temporal 0.86 20.61
sticks vs. prodrules
Comparison 0.88 19.77
Conjunction 0.89 18.43
Contingency 0.94 14.00
Expansion 0.88 19.18
Instantiation 0.90 16.34
Restatement 0.89 18.88
Temporal 0.90 17.94
sticks vs. sticks-nolex
Comparison 0.94 14.61
Conjunction 0.92 16.63
Contingency 0.97 10.16
Expansion 0.91 17.35
Instantiation 0.97 9.51
Restatement 0.97 11.26
Temporal 0.98 8.42
Table 9: Q statistic and disagreement of different
classes of representations
role of sparsity for traditional word-pair represen-
tations and how a less sparse representation could
improve performance. Then we proposed a less
sparse feature representation for production rules,
the best feature category so far, that further im-
proves classification. We study the role of lexical
features and show the contrast between the spar-
sity problem they brought along and their domi-
nant presence in the highly ranked features. Also,
lexical features included in syntactic features that
are most informative to the classifiers are found to
be style or domain specific in certain relations. Fi-
nally, we compare the representations in terms of
classifier disagreement and showed that within the
same feature category different feature representa-
tion can also be complementary with each other.
References
Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL): Short Papers, pages 69?73.
206
Sasha Blair-Goldensohn, Kathleen McKeown, and
Owen Rambow. 2007. Building and refining
rhetorical-semantic relation models. In Human Lan-
guage Technologies: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 428?435.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010. A semi-supervised approach to im-
prove classification of infrequent discourse relations
using feature vector extension. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 399?409.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169?184.
Ludmila I. Kuncheva and Christopher J. Whitaker.
2003. Measures of diversity in classifier ensembles
and their relationship with the ensemble accuracy.
Machine Learning, 51(2):181?207, May.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151?184, 4.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 368?375.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics - Special issue on using large
corpora, 19(2):313?330.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL), pages 108?112.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP (ACL-IJCNLP),
pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC).
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: An assessment. Natural Language En-
gineering, 14(3):369?416, July.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recogni-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING),
pages 1507?1514.
207
