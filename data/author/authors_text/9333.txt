 1 
Discriminative Slot Detection Using Kernel Methods 
Shubin Zhao, Adam Meyers, Ralph Grishman 
Department of Computer Science 
New York University 
715 Broadway, New York, NY 10003 
shubinz, meyers, grishman@cs.nyu.edu 
 
Abstract 
Most traditional information extraction 
approaches are generative models that assume 
events exist in text in certain patterns and these 
patterns can be regenerated in various ways. 
These assumptions limited the syntactic clues 
being considered for finding an event and 
confined these approaches to a particular 
syntactic level. This paper presents a 
discriminative framework based on kernel SVMs 
that takes into account different levels of 
syntactic information and automatically 
identifies the appropriate clues. Kernels are used 
to represent certain levels of syntactic structure 
and can be combined in principled ways as input 
for an SVM. We will show that by combining a 
low level sequence kernel with a high level 
kernel on a GLARF dependency graph, the new 
approach outperformed a good rule-based 
system on slot filler detection for MUC-6. 
1 Introduction 
The goal of Information Extraction (IE) is to 
extract structured facts of interest from text and 
present them in databases or templates. Much of 
the IE research was promoted by the US 
Government-sponsored MUCs (Message 
Understanding Conferences). The techniques used 
by Information Extraction depend greatly on the 
sublanguage used in a domain, such as financial 
news or medical records. The training data for an 
IE system is often sparse since the target domain 
changes quickly. Traditional IE approaches try to 
generate patterns for events by various means 
using training data. For example, the FASTUS 
(Appelt et al, 1996) and Proteus (Grishman, 1996) 
systems, which performed well for MUC-6, used 
hand-written rules for event patterns. The symbolic 
learning systems, like AutoSlog (Riloff, 1993) and 
CRYSTAL (Fisher et al, 1996), generated patterns 
automatically from specific examples (text 
segments) using generalization and predefined 
pattern templates. There are also statistical 
approaches (Miller et al, 1998) (Collins et al, 
1998) trying to encode event patterns in statistical 
CFG grammars. All of these approaches assume 
events occur in text in certain patterns. However 
this assumption may not be completely correct and 
it limits the syntactic information considered by 
these approaches for finding events, such as 
information on global features from levels other 
than deep processing. This paper will show that a 
simple bag-of-words model can give us reliable 
information about event occurrence. When training 
data is limited, these other approaches may also be 
less effective in their ability to generate reliable 
patterns.    
  The idea for overcoming these problems is to 
avoid making any prior assumption about the 
syntactic structure an event may assume; instead, 
we should consider all syntactic features in the 
target text and use a discriminative classifier to 
decide that automatically. Discriminative 
classifiers make no attempt to resolve the structure 
of the target classes. They only care about the 
decision boundary to separate the classes. In our 
case, we only need criteria to predict event 
elements from text using the syntactic features 
provided. This seems a more suitable solution for 
IE where training data is often sparse. 
 This paper presents an approach that uses kernel 
functions to represent different levels of syntactic 
structure (information). With the properties of 
kernel functions, individual kernels can be 
combined freely into comprehensive kernels that 
cross syntactic levels. The classifier we chose to 
use is SVM (Support Vector Machine), mostly due 
to its ability to work in high dimensional feature 
spaces. The experimental results of this approach 
show that it can outperform a hand-crafted rule 
system for the MUC-6 management succession 
domain. 
2 Background 
2.1 Information Extraction 
The major task of IE is to find the elements of an 
event from text and combine them to form 
templates or populate databases.  Most of these 
elements are named entities (NEs) involved in the 
event. To determine which entities in text are 
involved, we need to find reliable clues around 
each entity. The extraction procedure starts with 
 2 
text preprocessing, ranging from tokenization and 
part-of-speech tagging to NE identification and 
parsing. Traditional approaches would use various 
methods of analyzing the results of deep 
preprocessing to find patterns. Here we propose to 
use support vector machines to identify clues 
automatically from the outputs of different levels 
of preprocessing. 
2.2 Support Vector Machine 
For a two-class classifier, with separable training 
data, when given a set of n labeled vector examples 
    }1,1{),,(),...,,(),,( 2211 ?+?inn yyXyXyX ,  
a support vector machine (Vapnik, 1998) produces 
the separating hyperplane with largest margin 
among all the hyperplanes that successfully 
classify the examples. Suppose that all the 
examples satisfy the following constraint:  
             1),( ?+><? bXWy ii  
It is easy to see that the margin between the two  
bounding hyperplanes 1, ?=+>< bXW i is 
2/||W||. So maximizing the margin is equivalent to 
minimizing ||W||2 subject to the separation 
constraint above. In machine learning theory, this 
margin relates to the upper bound of the VC-
dimension of a support vector machine. Increasing 
the margin reduces the VC-dimension of the 
learning system, thus increasing the generalization 
capability of the system.  So a support vector 
machine produces a classifier with optimal 
generalization capability. This property enables 
SVMs to work in high dimensional vector spaces. 
2.3 Kernel SVM 
The vectors in SVM are usually feature vectors 
extracted by a certain procedure from the original 
objects, such as images or sentences. Since the 
only operator used in SVM is the dot product 
between two vectors, we can replace this operator 
by a function ),( ji SS?  on the object domain. In 
our case, Si and Sj are sentences. Mathematically 
this is still valid as long as ),( ji SS?  satisfies 
Mercer?s condition 1 . Function ),( ji SS?  is often 
referred to as a kernel function or just a kernel.  
Kernel functions provide a way to compute the 
similarity between two objects without 
transforming them into features.  
   
The kernel set has the following properties: 
                                                     
1
 The matrix must be positive semi-definite 
1. If ),(1 yxK  and ),(2 yxK are kernels on YX ? , 
0, >?? , then ),(),( 21 yxKyxK ?? +  is a kernel 
on YX ? . 
2. If ),(1 yxK  and ),(2 yxK are kernels on YX ? , 
then ),(),( 21 yxKyxK ?  is a kernel on YX ? . 
3. If ),(1 yxK  is a kernel on YX ? and 
),(2 vuK  is a kernel on VU ? , then 
),(),()),(),,(( 21 vuKyxKvyuxK += is a kernel 
on )()( VYUX ??? . 
When we have kernels representing information 
from different sources, these properties enable us 
to incorporate them into one kernel. The general 
kernels such as RBF or polynomial kernels (M?ller 
et al, 2001), which extend features nonlinearly 
into higher dimensional space, can also be applied 
to either the combination kernel or to each 
component kernel individually. 
2.4 Related Work 
  There have been a number of SVM applications 
in NLP using particular levels of syntactic 
information. (Lodhi et al, 2002) compared a word-
based string kernel and n-gram kernels at the 
sequence level for a text categorization task. The 
experimental results showed that the n-gram 
kernels performed quite well for the task. Although 
string kernels can capture common word 
subsequences with gaps, its geometric penalty 
factor may not be suitable for weighting the long 
distance features. (Collins et al, 2001) suggested 
kernels on parse trees and other structures for 
general NLP tasks. These kernels count small 
subcomponents multiple times so that in practice 
one has to be careful to avoid overfitting. This can 
be achieved by limiting the matching depth or 
using a penalty factor to downweight large 
components.  
(Zelenko et al, 2003) devised a kernel on 
shallow parse trees to detect relations between 
named entities, such as the person-affiliation 
relation between a person name and an 
organization name. The so-called relation kernel 
matches from the roots of two trees and continues 
recursively to the leaf nodes if the types of two 
nodes match.  
All the kernels used in these works were applied 
to a particular syntactic level. This paper presents 
an approach for information extraction that uses 
kernels to combine information from different 
levels and automatically identify which 
information contributes to the task. This 
framework can also be applied to other NLP tasks. 
 
 3 
3 A Discriminative Framework 
  The discriminative framework proposed here is 
called ARES (Automated Recognition of Event 
Slots). It makes no assumption about the text 
structure of events. Instead, kernels are used to 
represent syntactic information from various 
syntactic sources. The structure of ARES is shown 
in Fig 1. The preprocessing modules include a 
part-of-speech tagger, name tagger, sentence parser 
and GLARF parser, but are not limited to these. 
Other general tools can also be included, which are 
not shown in the diagram. The triangles in the 
diagram are kernels that encode the corresponding 
syntactic processing result. In the training phase, 
the target slot fillers are labeled in the text so that 
SVM slot detectors can be trained through the 
kernels to find fillers for the key slots of events. In 
the testing phase, the SVM classifier will predict 
the slot fillers from unlabeled text and a merging 
procedure will merge slots into events if necessary. 
The main kernel we propose to use is on GLARF 
(Meyers et al, 2001) dependency graphs. 
 
 
Fig 1. Structure of the discriminative model 
 
  The idea is that an IE model should not commit 
itself to any syntactic level. The low level 
information, such as word collocations, may also 
give us important clues. Our experimentation will 
show that for the MUC-6 management succession 
domain, even bag-of-words or n-grams can give us 
helpful information about event occurrence. 
3.1 Syntactic Kernels 
  To make use of syntactic information from 
different levels, we can develop kernel functions or 
syntactic kernels to represent a certain level of 
syntactic structure. The possible syntactic kernels 
include 
? Sequence kernels: representing sequence 
level information, such as bag-of-words, n-
grams, string kernel, etc. 
? Phrase kernel: representing information at 
an intermediate level, such as kernels 
based on multiword expressions, chunks or 
shallow parse trees. 
? Parsing kernel: representing detailed 
syntactic structure of a sentence, such as 
kernels based on parse trees or dependency 
graphs. 
 
  These kernels can be used alone or combined 
with each other using the properties of kernels. 
They can also be combined with high-order kernels 
like polynomial or RBF kernels, either individually 
or on the resulting kernel. 
As the depth of analysis of the preprocessing 
increases, the accuracy of the result decreases. 
Combining the results of deeper processing with 
those of shallower processing (such as n-grams) 
can also give us a back-off ability to recover from 
errors in deep processing. 
In practice each kernel can be tested for the task 
as the sole input to an SVM to determine if this 
level of information is helpful or not. After 
figuring out all the useful kernels, we can try to 
combine them to make a comprehensive kernel as 
final input to the classifier. The way to combine 
them and the parameters in combination can be 
determined using validation data. 
4 Introduction to GLARF 
GLARF (Grammatical and Logical Argument 
Regularization Framework) [Meyers et al, 2001] is 
a  hand-coded system that produces comprehensive 
word dependency graphs from Penn TreeBank-II 
(PTB-II) parse trees to facilitate applications like 
information extraction. GLARF is designed to 
enhance PTB-II parsing to produce more detailed 
information not provided by parsing, such as 
information about object, indirect object and 
appositive relations. GLARF can capture more 
regularization in text by transforming non-
canonical (passive, filler-gap) constructions into 
their canonical forms (simple declarative clauses). 
This is very helpful for information extraction 
where training data is often sparse. It also 
represents all syntactic phenomena in uniform 
typed PRED-ARG structures, which is convenient 
for computational purposes. For a sentence, 
GLARF outputs depencency triples derived 
automatically from the GLARF typed feature 
structures [Meyers et al, 2001]. A directed 
dependency graph of the sentence can also be 
constructed from the depencency triples. The 
following is the output of GLARF for the sentence 
?Tom Donilon, who also could get a senior job 
??. 
<SBJ,   get,  Tom Donilon> 
<OBJ,  get,   job> 
<ADV,  get,  also> 
<AUX,  get,  could> 
<T-POS,  job, a> 
  
 
 
 
 
Texts 
Input 
 
 
Output 
Templates 
POS      
Tagger 
Sent      
Parser 
Glarf      
Parser 
Name      
Tagger 
SGML     
Parser Event      
Merger 
Slot
 D
etecto
r
 
Documents 
 4 
<A-POS,  job,  senior> 
  . . . 
GLARF can produce logical relations in addition 
to surface relations, which is helpful for IE tasks. It 
can also generate output containing the base form 
of words so that different tenses of verbs can be 
regularized. Because of all these features, our main 
kernels are based on the GLARF dependency 
triples or dependency graphs.  
5 Event and Slot Kernels 
Here we will introduce the kernels used by ARES 
for event occurrence detection (EOD) and slot 
filler detection (SFD).  
5.1 EOD Kernels 
  In Information Extraction, one interesting issue 
is event occurrence detection, which is determining 
whether a sentence contains an event occurrence or 
not. If this information is given, it would be much 
easier to find the relevant entities for an event from 
the current sentence or surrounding sentences. 
Traditional approaches do matching (for slot 
filling) on all sentences, even though most of them 
do not contain any event at all. Event occurrence 
detection is similar to sentence level information 
retrieval, so simple models like bag-of-words or n-
grams could work well. We tried two kernels to do 
this, one is a sequence level n-gram kernel and the 
other is a GLARF-based kernel that matches 
syntactic details between sentences. In the 
following formulae, we will use an identity 
function ),( yxI that gives 1 when yx ?  and 0 
otherwise, where x and y are strings or vectors of 
strings.  
 
1. N-gram kernel ),( 21 SSN?  that counts common 
n-grams between two sentences. Given two 
sentence: >=<
1
,..., 211 NwwwS , and >=< 2,..., 211 NwwwS ,  
a bigram kernel ),( 21 SSbi?  is 
??
?
=
++
?
=
><><
1
1
11
1
1
21
),,,(
N
j
jjii
N
i
wwwwI .   
Kernels can be inclusive, in other words, the 
trigram kernel includes bigrams and unigrams. For 
the unigram kernel a stop list is used that removes 
words other than nouns, verbs, adjectives and 
adverbs. 
2. Glarf kernel ),( 21 GGg? : this kernel is based 
on the GLARF dependency result. Given the triple 
outputs of two sentences produced by  
GLARF: },,{1 ><= iii aprG , 11 Ni ??  and 
},,{2 ><= jjj aprG , 21 Nj ?? , where ri, pi, ai 
correspond to the role label, predicate word and 
argument word respectively in GLARF output, it 
matches the two triples, their predicates and 
arguments respectively. So ),( 21 GGg?  equals 
)),(),(),,,,,((
21
11
??
==
++><><
N
j
jijijjjiii
N
i
aaIppIapraprI ??  
In our experiments, ? and ?  were set to 1. 
5.2 SFD Kernels 
 Slot filler detection (SFD) is the task of 
determining which named entities fill a slot in 
some event template.  Two kernels were proposed 
for SFD: the first one matches local contexts of 
two target NEs, while the second one combines the 
first one with an n-gram EOD kernel.  
  1. ),(1 jiSFD GG? : This kernel was also defined 
on a GLARF dependency graph (DG), a directed 
graph constructed from its typed PRED-ARG 
outputs. The arcs labeled with roles go from 
predicate words to argument words. This kernel 
matches local context surrounding a name in a 
GLARF dependency graph. In preprocessing, all 
the names of the same type are translated into one 
symbol (a special word). The matching starts from 
two anchor nodes (NE nodes of the same type) in 
the two DG?s and recursively goes from these 
nodes to their successors and predecessors, until 
the words associated with nodes do not match. In 
our experiment, the matching depth was set to 2. 
Each node n contains a predicate word w
 
and  
relation pairs },{ >< ii ar , pi ??1  representing 
its p arguments and the roles associated with them.  
A matching function ),( 21 nnC  is defined as 
??
==
+><><
21
11
)),(),,,((
p
j
jijjii
p
i
rrIararI . 
Then ),(1 jiSFD GG? : can be written as 
??
?
?
?
?
?
?
++
ji
jj
ii
ji
jj
ii
nn
Eedn
Eedn
ji
nn
ESuccn
ESuccn
jiji nnCnnCEEC
)(Pr
)(Pr
)(
)(
),(),(),(  
where Ei and Ej are the anchor nodes in the two 
DG?s; ji nn ? is true if the predicate words 
associated with them match. Functions Succ(n) and 
Pred(n) give the successor and predecessor node 
set of a node n. The reason for setting a depth limit 
is that it covers most of the local syntax of a node 
(before matching stops); another reason is that the 
cycles currently present in GLARF dependency 
graph prohibit unbounded recursive matching. 
  2. ),(2 jiSFD SS? : This kernel combines linearly 
the n-gram event kernel and the slot kernel above, 
 5 
in the hope that the general event occurrence 
information provided by EOD kernel can help the 
slot kernel to ignore NEs in sentences that do not 
contain any event occurrence.  
),(),(),( 12 jiSFDjiNjiSFD GGSSSS ????? += , 
where ?? , were set to be 1 in our experiments. 
The Glarf event kernel was not used, simply 
because it uses information from the same source 
as ),(1 jiSFD GG? . The n-gram kernel was chosen 
to be the trigram kernel, which gives us the best 
EOD performance among n-gram kernels. 
We also tried the dependency graph kernel 
proposed by (Collins et al, 2001), but it did not 
give us better result. 
6 Experiments 
6.1 Corpus 
  The experiments of ARES were done on the 
MUC-6 corporate management succession domain 
using the official training data and, for the final 
experiment, the official test data as well. The 
training data was split into a training set (80%) and 
validation set (20%). In ARES, the text was 
preprocessed by the Proteus NE tagger and 
Charniak sentence parser. Then the GLARF 
processor produced dependency graphs based on 
the parse trees and NE results. All the names were 
transformed into symbols representing their types, 
such as #PERSON# for all person names. The 
reason is that we think the name itself does not 
provide a significant clue; the only thing that 
matters is what type of name occurs at certain 
position. 
  Two tasks have been tried: one is EOD (event 
occurrence detection) on sentences; the other is 
SFD (slot filler detection) on named entities, 
including person names and job titles. EOD is to 
determine whether a sentence contains an event or 
not. This would give us general information about 
sentence-level event occurrences. SFD is to find 
name fillers for event slots. The slots we 
experimented with were the person name and job 
title slots in MUC-6. We used the SVM package 
SVMlight in our experiments, embedding our own 
kernels as custom kernels. 
6.2 EOD Experiments 
  In this experiment, ARES was trained on the 
official MUC-6 training data to do event 
occurrence detection. The data contains 1940 
sentences, of which 158 are labeled as positive 
instances (contain an event). Five-fold cross 
validation was used so that the training and test set 
contain 80% and 20% of the data respectively. 
Three kernels defined in the previous section were 
tried. Table 1 shows the performance of each 
kernel. Three n-gram kernels were tested: unigram, 
bigram and trigram. Subsequences longer than 
trigrams were also tried, but did not yield better 
results. 
  The results show that the trigram kernel 
performed the best among n-gram kernels. GLARF 
kernel did better than n-gram kernels, which is 
reasonable because it incorporates detailed syntax 
of a sentence. But generally speaking, the n-gram 
kernels alone performed fairly well for this task, 
which indicates that low level text processing can 
also provide useful information. The mix kernel 
that combines the trigram kernel with GLARF 
kernel gave the best performance, which might 
indicate that the low level information provides 
additional clues or helps to overcome errors in 
deep processing. 
 
Kernel Precision Recall F-score 
Unigram 66.0% 66.5% 66.3% 
Bigram 73.9% 60.3% 66.4% 
Trigram 77.5% 61.5% 68.6% 
GLARF 77.5% 63.9% 70.1% 
Mix 81.5% 66.4% 73.2% 
 
Table 1. EOD performance of ARES using 
different kernels. The Mix kernel is a linear 
combination of the trigram kernel and the Glarf 
kernel. 
6.3 SFD Experiments 
The slot filler detection (SFD) task is to find the 
named entities in text that can fill the 
corresponding slots of an event.2 We treat job title 
as a named entity throughout this paper, although it 
is not included in the traditional MUC named 
entity set. The slots we used for evaluation were 
PERSON_IN (the person who took a position),  
PERSON_OUT (the person who left a position) 
and POST (the position involved). We generated 
the two person slots from the official MUC-6 
templates and the corresponding filler strings in 
text were labeled. Three SVM predictors were 
trained to find name fillers of each slot. Two 
experiments have been tried on MUC-6 training 
data using five-fold cross validation. 
  The first experiment of ARES used slot kernel 
),(1 jiSFD GG?  alone, relying solely on local 
                                                     
2
 We used this task for evaluation, rather than the 
official MUC template-filling task, in order to assess the 
system?s ability to identify slot fillers separately from its 
ability to combine them into templates. 
 6 
context around a NE. From the performance table 
(Table 2), we can see that local context can give a 
fairly good clue for finding PERSON_IN and 
POST, but not for PERSON_OUT. The main 
reason is that local context might be not enough to 
determine a PERSON_OUT filler. It often requires 
inference or other semantic information. For 
example, the sentence ?Aaron Spelling, the 
company's vice president, was named president.?, 
indicates that ?Aaron Spelling? left the position of 
vice president, therefore it should be a 
PERSON_OUT. But the sentence ?Aaron Spelling, 
the company's vice president, said ??, which is 
very similar to first one in syntax, has no such 
indication at all. In complicated cases, a person can 
even hold two positions at the same time. 
 
Accuracy Precision Recall F-score 
PER_IN 63.6% 62.5% 63.1% 
PER_OUT 54.8% 54.2% 54.5% 
POST 64.4% 55.2% 59.4% 
 
Table 2. SFD performance of ARES using kernel 
),(1 jiSFD GG? . 
 
  In this experiment, the SVM predictor 
considered all the names identified by the NE 
tagger; however, most of the sentences do not 
contain an event occurrence at all, so NEs in these 
sentences should be ignored no matter what their 
local context is. To achieve this we need general 
information about event occurrence, and this is just 
what the EOD kernel can provide. In our second 
experiment, we tested the kernel ),(2 jiSFD SS? , 
which is a linear combination of the trigram EOD 
kernel and the SFD kernel ),(1 jiSFD GG? . Table 3 
shows the performance of the combination kernel, 
from which we can see that there is clear 
performance improvement for all three slots. We 
also tried to use the mix kernel which gave us the 
best EOD performance, but it did not yield a better 
result. The reason we think is that the GLARF 
EOD kernel and SFD kernel are from the same 
syntactic source, so the information was repeated. 
 
Accuracy Precision Recall F-score 
PER_IN 86.6% 60.5% 71.2% 
PER_OUT 69.2% 58.2% 63.2% 
POST 68.5% 68.9% 68.7% 
 
Table 3. SFD performance of ARES using kernel 
),(2 jiSFD SS? . It combines the Glarf SFD kernel 
with trigram EOD kernel. For PER_OUT,  
unigram EOD kernel was used. 
 
Since five-fold cross validation was used, ARES 
was trained on 80% of the MUC-6 training data in 
these two experiments.  
6.4 Comparison with MUC-6 System 
This experiment was done on the official MUC-
6 training and test data, which contain 50K words 
and 40K words respectively. ARES used the 
official corpora as training and test sets, except that 
in the training data, all the slot fillers were 
manually labeled. We compared the performance 
of ARES with the NYU Proteus system, a rule-
based system that performed well for MUC-6. To 
score the performance for these three slots, we 
generated the slot-filler pairs as keys for a 
document from the official MUC-6 templates and 
removed duplicate pairs. The scorer matches the 
filler string in the response file of ARES to the 
keys.  The response result for Proteus was 
extracted in the same way from its template output. 
Table 4. shows the result of ARES using the 
combination kernel in the previous experiment. 
 
Accuracy Precision Recall F-score 
PER_IN 77.3% 62.2% 68.9% 
PER_OUT 58.9% 69.7% 63.9% 
POST 77.1% 71.5% 73.6% 
 
Table 4. Slot performance ARES using kernel 
),(2 jiSFD SS?  on MUC-6 test data.  
 
Table 5 shows the test result of the Proteus 
system. Comparing the numbers we can see that 
for slot PERSON_IN and POST, ARES 
outperformed the Proteus system by a few points. 
The result is promising considering that this model 
is fully automatic and does not involve any post-
processing. As for the PERSON_OUT slot, the 
performance of ARES was not as good. As we 
have discussed before, relying purely on syntax 
might not help us much;  we may need an 
inference model to resolve this problem. 
 
Accuracy Precision Recall F-score 
PER_IN 85.7% 51.2% 64.1% 
PER_OUT 78.4% 58.6% 67.1% 
POST 83.3% 59.7% 69.5% 
 
Table 5. Slot performance of the rule-based 
Proteus system for MUC-6. 
7 Related Work 
(Chieu et al, 2003) reported a feature-based 
SVM system (ALICE) to extract MUC-4 events of 
 7 
terrorist attacks. The Alice-ME system 
demonstrated competitive performance with rule-
based systems. The features used by Alice are 
mainly from parsing. Comparing with ALICE, our 
system uses kernels on dependency graphs to 
replace explicit features, an approach which is 
fully automatic and requires no enumeration of 
features. The model we proposed can combine 
information from different syntactic levels in 
principled ways. In our experiments, we used both 
word sequence  information and parsing level 
syntax information. The training data for ALICE 
contains 1700 documents, while for our system it 
is just 100 documents. When data is sparse, it is 
more difficult for an automatic system to 
outperform a rule-based system that incorporates 
general knowledges. 
8 Discussion and Further Works 
    This paper describes a discriminative approach 
that can use syntactic clues automatically for slot 
filler detection. It outperformed a hand-crafted 
system on sparse data by considering different 
levels of syntactic clues. The result also shows that 
low level syntactic information can also come into 
play in finding events, thus it should not be ignored 
in the IE framework. 
    For slot filler detection, several classifiers were 
trained to find names for each slot and there is no 
correlation among these classifiers. However, 
entity slots in events are often strongly correlated, 
for example the PER_IN and POST slots for 
management succession events. Since these 
classifiers take the same input and produce 
different results, correlation models can be used to 
integrate these classifiers so that the identification 
of slot fillers might benefit each other.  
    It would also be interesting to experiment with 
the tasks that are more difficult for pattern 
matching, such as determining the on-the-job 
status property in MUC-6. Since events often span 
multiple sentences, another direction is to explore 
cross-sentence models, which is difficult for 
traditional approaches. For our approach it is 
possible to extend the kernel from one sentence to 
multiple sentences, taking into account the 
correlation between NE?s in adjacent sentences. 
9 Acknowledgements 
This research was supported in part by the 
Defense Advanced Research Projects Agency as 
part of the TIDES program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare 
Systems Center, San Diego, and by the National 
Science Foundation under Grant ITS-0325657.  
This paper does not necessarily reflect the position 
of the U.S. Government. 
References  
D. Appelt, J. Hobbs, J. Bear, D. Israel, M. Kameyama,  
A. Kehler, D. Martin, K. Meyers, and M. Tyson 
1996. SRI International FASTUS system: MUC-6 test 
results and analysis. In Proceedings of the Sixth 
Message Understanding Conference.  
H. L. Chieu, H. T. Ng, & Y. K. Lee. 2003. Closing the 
Gap: Learning-Based Information Extraction 
Rivaling Knowledge-Engineering Methods. In 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics.  
M. Collins and S. Miller. 1998. Semantic Tagging using 
a Probabilistic Context Free Grammar, In 
Proceedings of the Sixth Workshop on Very Large 
Corpora. 
M. Collins and N. Duffy. 2001. Convolution Kernels for 
Natural Language, Advances in Neural Information 
Processing Systems 14, MIT Press. 
D. Fisher, S. Soderland, J. McCarthy, F. Feng and W. 
Lehnert. 1996. Description of The UMass System As 
Used For MUC-6. In Proceedings of the Sixth 
Message Understanding Conference. 
R. Grishman. 1996. The NYU System for MUC-6 or 
Where's the Syntax?. In Proceedings of the Sixth 
Message Understanding Conference.  
H. Lodhi, C. Sander, J. Shawe-Taylor, N. Christianini 
and C. Watkins. 2002. Text Classification using 
String Kernels. Journal of Machine Learning 
Research. 
A. Meyers, R. Grishman, M. Kosaka and S. Zhao. 2001. 
Covering Treebanks with GLARF. In Proceedings of 
of the ACL Workshop on Sharing Tools and 
Resources. 
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. 
Schwartz, R. Stone, and R. Weischedel. 1998. BBN: 
Description of The SIFT System As Used For MUC-
7, In Proceedings of the Seventh Message 
Understanding Conference. 
K.-R. M?ller, S. Mika, G. Ratsch, K. Tsuda, B. 
Scholkopf. 2001. An introduction to kernel-based 
learning algorithms, IEEE Trans. Neural Networks, 
12, 2, pages 181-201. 
E. Riloff. 1993. Automatically constructing a dictionary 
for information extraction tasks. In Proceedings of 
the 11th National Conference on Artificial 
Intelligence, 811-816. 
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience Publication. 
D. Zelenko, C. Aone and A. Richardella. 2003. Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 
 
Proceedings of the 43rd Annual Meeting of the ACL, pages 419?426,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
Extracting Relations with Integrated Information Using Kernel Methods 
 
 
                                        Shubin Zhao               Ralph Grishman 
Department of Computer Science 
New York University 
715 Broadway, 7th Floor, New York, NY 10003 
              shubinz@cs.nyu.edu     grishman@cs.nyu.edu 
 
 
 
 
Abstract 
Entity relation detection is a form of in-
formation extraction that finds predefined 
relations between pairs of entities in text. 
This paper describes a relation detection 
approach that combines clues from differ-
ent levels of syntactic processing using 
kernel methods. Information from three 
different levels of processing is consid-
ered: tokenization, sentence parsing and 
deep dependency analysis. Each source of 
information is represented by kernel func-
tions. Then composite kernels are devel-
oped to integrate and extend individual 
kernels so that processing errors occurring 
at one level can be overcome by informa-
tion from other levels. We present an 
evaluation of these methods on the 2004 
ACE relation detection task, using Sup-
port Vector Machines, and show that each 
level of syntactic processing contributes 
useful information for this task. When 
evaluated on the official test data, our ap-
proach produced very competitive ACE 
value scores. We also compare the SVM 
with KNN on different kernels.  
1 Introduction 
Information extraction subsumes a broad range of 
tasks, including the extraction of entities, relations 
and events from various text sources, such as 
newswire documents and broadcast transcripts. 
One such task, relation detection, finds instances 
of predefined relations between pairs of entities, 
such as a Located-In relation between the entities 
Centre College and Danville, KY in the phrase 
Centre College in Danville, KY. The ?entities? are 
the individuals of selected semantic types (such as 
people, organizations, countries, ?) which are re-
ferred to in the text. 
    Prior approaches to this task (Miller et al, 2000; 
Zelenko et al, 2003) have relied on partial or full 
syntactic analysis. Syntactic analysis can find rela-
tions not readily identified based on sequences of 
tokens alone. Even ?deeper? representations, such 
as logical syntactic relations or predicate-argument 
structure, can in principle capture additional gener-
alizations and thus lead to the identification of ad-
ditional instances of relations. However, a general 
problem in Natural Language Processing is that as 
the processing gets deeper, it becomes less accu-
rate. For instance, the current accuracy of tokeniza-
tion, chunking and sentence parsing for English is 
about 99%, 92%, and 90% respectively. Algo-
rithms based solely on deeper representations in-
evitably suffer from the errors in computing these 
representations. On the other hand, low level proc-
essing such as tokenization will be more accurate, 
and may also contain useful information missed by 
deep processing of text. Systems based on a single 
level of representation are forced to choose be-
tween shallower representations, which will have 
fewer errors, and deeper representations, which 
may be more general. 
    Based on these observations, Zhao et al (2004) 
proposed a discriminative model to combine in-
formation from different syntactic sources using a 
kernel SVM (Support Vector Machine). We 
showed that adding sentence level word trigrams 
as global information to local dependency context 
boosted the performance of finding slot fillers for 
419
 management succession events. This paper de-
scribes an extension of this approach to the identi-
fication of entity relations, in which syntactic 
information from sentence tokenization, parsing 
and deep dependency analysis is combined using 
kernel methods. At each level, kernel functions (or 
kernels) are developed to represent the syntactic 
information. Five kernels have been developed for 
this task, including two at the surface level, one at 
the parsing level and two at the deep dependency 
level. Our experiments show that each level of 
processing may contribute useful clues for this 
task, including surface information like word bi-
grams. Adding kernels one by one continuously 
improves performance. The experiments were car-
ried out on the ACE RDR (Relation Detection and 
Recognition) task with annotated entities. Using 
SVM as a classifier along with the full composite 
kernel produced the best performance on this task. 
This paper will also show a comparison of SVM 
and KNN (k-Nearest-Neighbors) under different 
kernel setups. 
2 Kernel Methods  
Many machine learning algorithms involve only 
the dot product of vectors in a feature space, in 
which each vector represents an object in the ob-
ject domain. Kernel methods (Muller et al, 2001) 
can be seen as a generalization of feature-based 
algorithms, in which the dot product is replaced by 
a kernel function (or kernel) ?(X,Y) between two 
vectors, or even between two objects. Mathemati-
cally, as long as ?(X,Y) is symmetric and the ker-
nel matrix formed by ? is positive semi-definite, it 
forms a valid dot product in an implicit Hilbert 
space. In this implicit space, a kernel can be bro-
ken down into features, although the dimension of 
the feature space could be infinite. 
   Normal feature-based learning can be imple-
mented in kernel functions, but we can do more 
than that with kernels. First, there are many well-
known kernels, such as polynomial and radial basis 
kernels, which extend normal features into a high 
order space with very little computational cost. 
This could make a linearly non-separable problem 
separable in the high order feature space. Second, 
kernel functions have many nice combination 
properties: for example, the sum or product of ex-
isting kernels is a valid kernel. This forms the basis 
for the approach described in this paper. With 
these combination properties, we can combine in-
dividual kernels representing information from 
different sources in a principled way.  
   Many classifiers can be used with kernels. The 
most popular ones are SVM, KNN, and voted per-
ceptrons. Support Vector Machines (Vapnik, 1998; 
Cristianini and Shawe-Taylor, 2000) are linear 
classifiers that produce a separating hyperplane 
with largest margin. This property gives it good 
generalization ability in high-dimensional spaces, 
making it a good classifier for our approach where 
using all the levels of linguistic clues could result 
in a huge number of features. Given all the levels 
of features incorporated in kernels and training 
data with target examples labeled, an SVM can 
pick up the features that best separate the targets 
from other examples, no matter which level these 
features are from. In cases where an error occurs in 
one processing result (especially deep processing) 
and the features related to it become noisy, SVM 
may pick up clues from other sources which are 
not so noisy. This forms the basic idea of our ap-
proach. Therefore under this scheme we can over-
come errors introduced by one processing level; 
more particularly, we expect accurate low level 
information to help with less accurate deep level 
information. 
3 Related Work  
Collins et al (1997) and Miller et al (2000) used 
statistical parsing models to extract relational facts 
from text, which avoided pipeline processing of 
data. However, their results are essentially based 
on the output of sentence parsing, which is a deep 
processing of text. So their approaches are vulner-
able to errors in parsing. Collins et al (1997) ad-
dressed a simplified task within a confined context 
in a target sentence.  
Zelenko et al (2003) described a recursive ker-
nel based on shallow parse trees to detect person-
affiliation and organization-location relations, in 
which a relation example is the least common sub-
tree containing two entity nodes. The kernel 
matches nodes starting from the roots of two sub-
trees and going recursively to the leaves. For each 
pair of nodes, a subsequence kernel on their child 
nodes is invoked, which matches either contiguous 
or non-contiguous subsequences of node. Com-
pared with full parsing, shallow parsing is more 
reliable. But this model is based solely on the out-
420
 put of shallow parsing so it is still vulnerable to 
irrecoverable parsing errors. In their experiments, 
incorrectly parsed sentences were eliminated.  
Culotta and Sorensen (2004) described a slightly 
generalized version of this kernel based on de-
pendency trees. Since their kernel is a recursive 
match from the root of a dependency tree down to 
the leaves where the entity nodes reside, a success-
ful match of two relation examples requires their 
entity nodes to be at the same depth of the tree. 
This is a strong constraint on the matching of syn-
tax so it is not surprising that the model has good 
precision but very low recall. In their solution a 
bag-of-words kernel was used to compensate for 
this problem. In our approach, more flexible ker-
nels are used to capture regularization in syntax, 
and more levels of syntactic information are con-
sidered. 
Kambhatla (2004) described a Maximum En-
tropy model using features from various syntactic 
sources, but the number of features they used is 
limited and the selection of features has to be a 
manual process.1 In our model, we use kernels to 
incorporate more syntactic information and let a 
Support Vector Machine decide which clue is cru-
cial. Some of the kernels are extended to generate 
high order features. We think a discriminative clas-
sifier trained with all the available syntactic fea-
tures should do better on the sparse data. 
4 Kernel Relation Detection 
4.1 ACE Relation Detection Task 
ACE (Automatic Content Extraction)2 is a research 
and development program in information extrac-
tion sponsored by the U.S. Government. The 2004 
evaluation defined seven major types of relations 
between seven types of entities. The entity types 
are PER (Person), ORG (Organization), FAC (Fa-
cility), GPE (Geo-Political Entity: countries, cities, 
etc.), LOC (Location), WEA (Weapon) and VEH 
(Vehicle). Each mention of an entity has a mention 
type: NAM (proper name), NOM (nominal) or 
                                                          
1 Kambhatla also evaluated his system on the ACE relation 
detection task, but the results are reported for the 2003 task, 
which used different relations and different training and test 
data, and did not use hand-annotated entities, so they cannot 
be readily compared to our results. 
2Task description: http://www.itl.nist.gov/iad/894.01/tests/ace/ 
  ACE guidelines: http://www.ldc.upenn.edu/Projects/ACE/ 
PRO (pronoun); for example George W. Bush, the 
president and he respectively. The seven relation 
types are EMP-ORG (Employ-
ment/Membership/Subsidiary), PHYS (Physical), 
PER-SOC (Personal/Social), GPE-AFF (GPE-
Affiliation), Other-AFF (Person/ORG Affiliation), 
ART (Agent-Artifact) and DISC (Discourse). 
There are also 27 relation subtypes defined by 
ACE, but this paper only focuses on detection of 
relation types. Table 1 lists examples of each rela-
tion type. 
 
Type Example 
EMP-ORG the CEO of Microsoft 
PHYS a military base in Germany 
GPE-AFF U.S.  businessman 
PER-SOC a spokesman for the senator 
DISC many of these people 
ART the makers of the Kursk 
Other-AFF Cuban-American  people 
 
Table 1. ACE relation types and examples. The 
heads of the two entity arguments in a relation are 
marked. Types are listed in decreasing order of 
frequency of occurrence in the ACE corpus. 
 
  Figure 1 shows a sample newswire sentence, in 
which three relations are marked. In this sentence, 
we expect to find a PHYS relation between Hez-
bollah forces and areas, a PHYS relation between 
Syrian troops and areas and an EMP-ORG relation 
between Syrian troops and Syrian. In our ap-
proach, input text is preprocessed by the Charniak 
sentence parser (including tokenization and POS 
tagging) and the GLARF (Meyers et al, 2001) de-
pendency analyzer produced by NYU. Based on 
treebank parsing, GLARF produces labeled deep 
dependencies between words (syntactic relations 
such as logical subject and logical object). It han-
dles linguistic phenomena like passives, relatives, 
reduced relatives, conjunctions, etc.  
 
Figure 1. Example sentence from newswire text  
4.2 Definitions 
In our model, kernels incorporate information from 
PHYS PHYS EMP-ORG
That's because Israel was expected to retaliate against 
Hezbollah forces in areas controlled by Syrian troops. 
421
 tokenization, parsing and deep dependency analy-
sis. A relation candidate R is defined as 
 R = (arg1, arg2, seq, link, path), 
where arg1 and arg2 are the two entity arguments 
which may be related; seq=(t1, t2, ?, tn) is a token 
vector that covers the arguments and intervening 
words; link=(t1, t2, ?, tm) is also a token vector, 
generated from seq and the parse tree; path is a 
dependency path connecting arg1 and arg2 in the 
dependency graph produced by GLARF. path can 
be empty if no such dependency path exists. The 
difference between link and seq is that link only 
retains the ?important? words in seq in terms of 
syntax. For example, all noun phrases occurring in 
seq are replaced by their heads. Words and con-
stituent types in a stop list, such as time expres-
sions, are also removed. 
  A token T is defined as a string triple, 
T = (word, pos, base), 
where word, pos and base are strings representing 
the word, part-of-speech and morphological base 
form of T. Entity is a token augmented with other 
attributes, 
             E = (tk, type, subtype, mtype), 
where tk is the token associated with E; type, sub-
type and mtype are strings representing the entity 
type, subtype and mention type of E. The subtype 
contains more specific information about an entity. 
For example, for a GPE entity, the subtype tells 
whether it is a country name, city name and so on. 
Mention type includes NAM, NOM and PRO. 
  It is worth pointing out that we always treat an 
entity as a single token: for a nominal, it refers to 
its head, such as boys in the two boys; for a proper 
name, all the words are connected into one token, 
such as Bashar_Assad. So in a relation example R 
whose seq is (t1, t2, ?, tn), it is always true that 
arg1=t1 and arg2=tn. For names, the base form of 
an entity is its ACE type (person, organization, 
etc.). To introduce dependencies, we define a de-
pendency token to be a token augmented with a 
vector of dependency arcs, 
           DT=(word, pos, base, dseq),     
where dseq = (arc1, ... , arcn ). A dependency arc is 
            ARC = (w, dw, label, e),  
where w is the current token; dw is a token con-
nected by a dependency to w; and label and e are 
the role label and direction of this dependency arc 
respectively. From now on we upgrade the type of 
tk in arg1 and arg2 to be dependency tokens. Fi-
nally, path is a vector of dependency arcs, 
     path = (arc1 , ... , arcl ),  
where l is the length of the path and arci (1?i?l) 
satisfies arc1.w=arg1.tk, arci+1.w=arci.dw and 
arcl.dw=arg2.tk. So path is a chain of dependencies 
connecting the two arguments in R. The arcs in it 
do not have to be in the same direction. 
 
 
 
Figure 2. Illustration of a relation example R. The 
link sequence is generated from seq by removing 
some unimportant words based on syntax. The de-
pendency links are generated by GLARF. 
 
  Figure 2 shows a relation example generated from 
the text ?? in areas controlled by Syrian troops?. 
In this relation example R, arg1 is ((?areas?, 
?NNS?, ?area?, dseq), ?LOC?, ?Region?, 
?NOM?), and arg1.dseq is ((OBJ, areas, in, 1), 
(OBJ, areas, controlled, 1)). arg2 is ((?troops?, 
?NNS?, ?troop?, dseq), ?ORG?, ?Government?, 
?NOM?) and arg2.dseq = ((A-POS, troops, Syrian, 
0), (SBJ, troops, controlled, 1)). path is ((OBJ, ar-
eas, controlled, 1), (SBJ, controlled, troops, 0)). 
The value 0 in a dependency arc indicates forward 
direction from w to dw, and 1 indicates backward 
direction. The seq and link sequences of R are 
shown in Figure 2. 
  Some relations occur only between very restricted 
types of entities, but this is not true for every type 
of relation. For example, PER-SOC is a relation 
mainly between two person entities, while PHYS 
can happen between any type of entity and a GPE 
or LOC entity. 
4.3 Syntactic Kernels 
In this section we will describe the kernels de-
signed for different syntactic sources and explain 
the intuition behind them. 
  We define two kernels to match relation examples 
at surface level. Using the notation just defined, we 
can write the two surface kernels as follows: 
1) Argument kernel 
troopsareas controlled by 
A-POS OBJ 
arg1 arg2 SBJ 
OBJ
path 
in
seq 
link 
areas controlled by Syrian troops
COMP 
422
  
 
where KE is a kernel that matches two entities, 
 
 
 
 
 
 
KT is a kernel that matches two tokens. I(x, y) is a 
binary string match operator that gives 1 if x=y 
and 0 otherwise. Kernel ?1 matches attributes of 
two entity arguments respectively, such as type, 
subtype and lexical head of an entity. This is based 
on the observation that there are type constraints 
on the two arguments. For instance PER-SOC is a 
relation mostly between two person entities. So the 
attributes of the entities are crucial clues. Lexical 
information is also important to distinguish relation 
types. For instance, in the phrase U.S. president 
there is an EMP-ORG relation between president 
and U.S., while in a U.S. businessman there is a 
GPE-AFF relation between businessman and U.S. 
2)  Bigram kernel 
 
 
where  
 
 
 
Operator <t1, t2> concatenates all the string ele-
ments in tokens t1 and t2 to produce a new token. 
So ?2 is a kernel that simply matches unigrams and 
bigrams between the seq sequences of two relation 
examples. The information this kernel provides is 
faithful to the text. 
3) Link sequence kernel 
 
 
 
 
where min_len is the length of the shorter link se-
quence in R1 and R2. ?3 is a kernel that matches 
token by token between the link sequences of two 
relation examples. Since relations often occur in a 
short context, we expect many of them have simi-
lar link sequences. 
4) Dependency path kernel 
 
 
where  
 
 
 
 
             ).',.()).',.( earcearcIdwarcdwarcK jijiT ?  
  Intuitively the dependency path connecting two 
arguments could provide a high level of syntactic 
regularization. However, a complete match of two 
dependency paths is rare. So this kernel matches 
the component arcs in two dependency paths in a 
pairwise fashion. Two arcs can match only when 
they are in the same direction. In cases where two 
paths do not match exactly, this kernel can still tell 
us how similar they are. In our experiments we 
placed an upper bound on the length of depend-
ency paths for which we computed a non-zero ker-
nel. 
5) Local dependency 
 
 
where 
 
 
 
 
         ).',.()).',.( earcearcIdwarcdwarcK jijiT ?  
  This kernel matches the local dependency context 
around the relation arguments. This can be helpful 
especially when the dependency path between ar-
guments does not exist. We also hope the depend-
encies on each argument may provide some useful 
clues about the entity or connection of the entity to 
the context outside of the relation example.  
4.4 Composite Kernels 
Having defined all the kernels representing shallow 
and deep processing results, we can define com-
posite kernels to combine and extend the individ-
ual kernels.  
1) Polynomial extension  
 
 
  This kernel combines the argument kernel ?1 and 
link kernel ?3 and applies a second-degree poly-
nomial kernel to extend them. The combination of 
?1 and ?3 covers the most important clues for this 
task: information about the two arguments and the 
word link between them. The polynomial exten-
sion is equivalent to adding pairs of features as 
),arg.,arg.(),( 21
2,1
211 ii
i
E RRKRR ?
=
=?
++= ).,.().,.(),( 212121 typeEtypeEItkEtkEKEEK TE
).,.().,.( 2121 mtypeEmtypeEIsubtypeEsubtypeEI +
+= ).,.(),( 2121 wordTwordTITTKT
).,.().,.( 2121 baseTbaseTIposTposTI +
),.,.(),( 21212 seqRseqRKRR seq=?
? ?
<? <?
+=
lenseqi lenseqj
jiTseq tktkKseqseqK
.0 .'0
)',(('),(
))',',,( 11 ><>< ++ jjiiT tktktktkK
).,.(),( 21213 linkRlinkRKRR link=?
,)..,..( 21
min_0
ii
leni
T ktlinkRktlinkRK?
<?
=
),.,.(),( 21214 pathRpathRKRR path=?
)',( pathpathK path
? ?
<? <?
+=
lenpathi lenpathj
ji labelarclabelarcI
.0 .'0
).',.(((
,).arg.,.arg.(),(
2,1
21215 ?
=
=
i
iiD dseqRdseqRKRR?
)',( dseqdseqK D
? ?
<? <?
+=
lendseqi lendseqj
ji labelarclabelarcI
.0 .'0
).',.((
4/)()(),( 23131211 ???? +++=? RR
423
 new features. Intuitively this introduces new fea-
tures like: the subtype of the first argument is a 
country name and the word of the second argument 
is president, which could be a good clue for an 
EMP-ORG relation. The polynomial kernel is 
down weighted by a normalization factor because 
we do not want the high order features to over-
whelm the original ones. In our experiment, using 
polynomial kernels with degree higher than 2 does 
not produce better results. 
2) Full kernel 
 
 
This is the final kernel we used for this task, which 
is a combination of all the previous kernels. In our 
experiments, we set al the scalar factors to 1. Dif-
ferent values were tried, but keeping the original 
weight for each kernel yielded the best results for 
this task. 
  All the individual kernels we designed are ex-
plicit. Each kernel can be seen as a matching of 
features and these features are enumerable on the 
given data. So it is clear that they are all valid ker-
nels. Since the kernel function set is closed under 
linear combination and polynomial extension, the 
composite kernels are also valid. The reason we 
propose to use a feature-based kernel is that we can 
have a clear idea of what syntactic clues it repre-
sents and what kind of information it misses. This 
is important when developing or refining kernels, 
so that we can make them generate complementary 
information from different syntactic processing 
results. 
5 Experiments  
Experiments were carried out on the ACE RDR 
(Relation Detection and Recognition) task using 
hand-annotated entities, provided as part of the 
ACE evaluation. The ACE corpora contain docu-
ments from two sources: newswire (nwire) docu-
ments and broadcast news transcripts (bnews). In 
this section we will compare performance of dif-
ferent kernel setups trained with SVM, as well as 
different classifiers, KNN and SVM, with the same 
kernel setup. The SVM package we used is 
SVMlight. The training parameters were chosen us-
ing cross-validation. One-against-all classification 
was applied to each pair of entities in a sentence. 
When SVM predictions conflict on a relation ex-
ample, the one with larger margin will be selected 
as the final answer. 
5.1 Corpus 
The ACE RDR training data contains 348 docu-
ments, 125K words and 4400 relations. It consists 
of both nwire and bnews documents. Evaluation of 
kernels was done on the training data using 5-fold 
cross-validation. We also evaluated the full kernel 
setup with SVM on the official test data, which is 
about half the size of the training data. All the data 
is preprocessed by the Charniak parser and 
GLARF dependency analyzer. Then relation ex-
amples are generated based these results. 
5.2 Results 
  Table 2 shows the performance of the SVM on 
different kernel setups. The kernel setups in this 
experiment are incremental. From this table we can 
see that adding kernels continuously improves the 
performance, which indicates they provide 
additional clues to the previous setup. The argu-
ment kernel treats the two arguments as 
independent entities. The link sequence kernel 
introduces the syntactic connection between 
arguments, so adding it to the argument kernel 
boosted the performance. Setup F shows the 
performance of adding only dependency kernels to 
the argument kernel. The performance is not as 
good as setup B, indicating that dependency 
information alone is not as crucial as the link 
sequence.  
 
 Kernel           Performance   prec       recall    F-score 
A Argument (?1) 52.96%    58.47%   55.58% 
B A + link (?1+?3) 58.77%    71.25%   64.41%* 
C B-poly (?1) 66.98%    70.33%   68.61%* 
D C + dep (?1+?4+?5) 69.10%    71.41%   70.23%* 
E D + bigram (?2) 69.23%    70.50%   70.35% 
F A + dep (?1+?4+?5) 57.86%    68.50%   62.73% 
 
Table 2. SVM performance on incremental kernel 
setups. Each setup adds one level of kernels to the 
previous one except setup F. Evaluated on the 
ACE training data with 5-fold cross-validation. F-
scores marked by * are significantly better than the 
previous setup (at 95% confidence level). 
 
2541212 ),( ?????? +++?=? RR
424
   Another observation is that adding the bigram 
kernel in the presence of all other level of kernels 
improved both precision and recall, indicating that 
it helped in both correcting errors in other 
processing results and providing supplementary 
information missed by other levels of analysis. In 
another experiment evaluated on the nwire data 
only (about half of the training data), adding the 
bigram kernel improved F-score 0.5% and this 
improvement is statistically significant.  
   
Type KNN (?1+?3) KNN (?2) SVM (?2) 
EMP-ORG 75.43% 72.66% 77.76% 
PHYS 62.19 % 61.97% 66.37% 
GPE-AFF 58.67% 56.22% 62.13% 
PER-SOC 65.11% 65.61% 73.46% 
DISC 68.20% 62.91% 66.24% 
ART 69.59% 68.65% 67.68% 
Other-AFF 51.05% 55.20% 46.55% 
Total 67.44% 65.69% 70.35% 
 
Table 3. Performance of SVM and KNN (k=3) on 
different kernel setups. Types are ordered in de-
creasing order of frequency of occurrence in the 
ACE corpus. In SVM training, the same 
parameters were used for all 7 types.  
 
  Table 3 shows the performance of SVM and 
KNN (k Nearest Neighbors) on different kernel 
setups. For KNN, k was set to 3. In the first setup 
of KNN, the two kernels which seem to contain 
most of the important information are used. It 
performs quite well when compared with the SVM 
result. The other two tests are based on the full 
kernel setup. For the two KNN experiments, 
adding more kernels (features) does not help. The 
reason might be that all kernels (features) were 
weighted equally in the composite kernel ?2 and 
this may not be optimal for KNN. Another reason 
is that the polynomial extension of kernels does not 
have any benefit in KNN because it is a monotonic 
transformation of similarity values. So the results 
of KNN on kernel (?1+?3) and ?1 would be ex-
actly the same. We also tried different k for KNN 
and k=3 seems to be the best choice in either case.  
  For the four major types of relations SVM does 
better than KNN, probably due to SVM?s 
generalization ability in the presence of large 
numbers of features. For the last three types with 
many fewer examples, performance of SVM is not 
as good as KNN. The reason we think is that 
training of SVM on these types is not sufficient. 
We tried different training parameters for the types 
with fewer examples, but no dramatic 
improvement obtained. 
  We also evaluated our approach on the official 
ACE RDR test data and obtained very competitive 
scores.3 The primary scoring metric4 for the ACE 
evaluation is a 'value' score, which is computed by 
deducting from 100 a penalty for each missing and 
spurious relation; the penalty depends on the types 
of the arguments to the relation. The value scores 
produced by the ACE scorer for nwire and bnews 
test data are 71.7 and 68.0 repectively. The value 
score on all data is 70.1.5 The scorer also reports an 
F-score based on full or partial match of relations 
to the keys. The unweighted F-score for this test 
produced by the ACE scorer on all data is 76.0%. 
For this evaluation we used nearest neighbor to 
determine argument ordering and relation 
subtypes. 
  The classification scheme in our experiments is 
one-against-all. It turned out there is not so much 
confusion between relation types. The confusion 
matrix of predictions is fairly clean. We also tried 
pairwise classification, and it did not help much. 
6 Discussion 
In this paper, we have shown that using kernels to 
combine information from different syntactic 
sources performed well on the entity relation 
detection task. Our experiments show that each 
level of syntactic processing contains useful 
information for the task. Combining them may 
provide complementary information to overcome 
errors arising from linguistic analysis. Especially, 
low level information obtained with high reliability 
helped with the other deep processing results. This 
design feature of our approach should be best 
employed when the preprocessing errors at each 
level are independent, namely when there is no 
dependency between the preprocessing modules. 
The model was tested on text with annotated 
entities, but its design is generic. It can work with 
                                                          
3 As ACE participants, we are bound by the participation 
agreement not to disclose other sites? scores, so no direct 
comparison can be provided. 
4 http://www.nist.gov/speech/tests/ace/ace04/software.htm 
5 No comparable inter-annotator agreement scores are avail-
able for this task, with pre-defined entities.  However, the 
agreement scores across multiple sites for similar relation 
tagging tasks done in early 2005, using the value metric, 
ranged from about 0.70 to 0.80. 
425
 noisy entity detection input from an automatic 
tagger. With all the existing information from other 
processing levels, this model can be also expected 
to recover from errors in entity tagging. 
7 Further Work 
Kernel functions have many nice properties. There 
are also many well known kernels, such as radial 
basis kernels, which have proven successful in 
other areas. In the work described here, only linear 
combinations and polynomial extensions of kernels 
have been evaluated. We can explore other kernel 
properties to integrate the existing syntactic 
kernels. In another direction, training data is often 
sparse for IE tasks. String matching is not 
sufficient to capture semantic similarity of words. 
One solution is to use general purpose corpora to 
create clusters of similar words; another option is 
to use available resources like WordNet. These 
word similarities can be readily incorporated into 
the kernel framework.  To deal with sparse data, 
we can also use deeper text analysis to capture 
more regularities from the data. Such analysis may 
be based on newly-annotated corpora like 
PropBank (Kingsbury and Palmer, 2002) at the 
University of Pennsylvania and NomBank (Meyers 
et al, 2004) at New York University. Analyzers 
based on these resources can generate regularized 
semantic representations for lexically or 
syntactically related sentence structures. Although 
deeper analysis may even be less accurate, our 
framework is designed to handle this and still 
obtain some improvement in performance. 
8 Acknowledgement 
This research was supported in part by the Defense 
Advanced Research Projects Agency under Grant 
N66001-04-1-8920 from SPAWAR San Diego, 
and by the National Science Foundation under 
Grant ITS-0325657. This paper does not necessar-
ily reflect the position of the U.S. Government. We 
wish to thank Adam Meyers of the NYU NLP 
group for his help in producing deep dependency 
analyses. 
References  
M. Collins and S. Miller. 1997. Semantic tagging using 
a probabilistic context free grammar. In Proceedings 
of the 6th Workshop on Very Large Corpora. 
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to support vector machines. Cambridge Univer-
sity Press. 
A. Culotta and J. Sorensen. 2004. Dependency Tree 
Kernels for Relation Extraction. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics. 
D. Gildea and M. Palmer. 2002. The Necessity of Pars-
ing for Predicate Argument Recognition. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics. 
N. Kambhatla. 2004. Combining Lexical, Syntactic, and 
Semantic Features with Maximum Entropy Models 
for Extracting Relations. In Proceedings of the 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics. 
P. Kingsbury and M. Palmer. 2002. From treebank to 
propbank. In Proceedings of the 3rd International 
Conference on Language Resources and Evaluation 
(LREC-2002). 
C. D. Manning and H. Schutze 2002. Foundations of 
Statistical Natural Language Processing. The MIT 
Press, page 454-455. 
A. Meyers, R. Grishman, M. Kosaka and S. Zhao. 2001. 
Covering Treebanks with GLARF. In Proceedings of 
the 39th Annual Meeting of the Association for 
Computational Linguistics. 
A. Meyers, R. Reeves, Catherine Macleod, Rachel 
Szekeley, Veronkia Zielinska, Brian Young, and R. 
Grishman. 2004. The Cross-Breeding of Dictionar-
ies. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC-
2004).  
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 
2000. A novel use of statistical parsing to extract in-
formation from text. In 6th Applied Natural Lan-
guage Processing Conference. 
K.-R. M?ller, S. Mika, G. Ratsch, K. Tsuda and B. 
Scholkopf. 2001. An introduction to kernel-based 
learning algorithms, IEEE Trans. Neural Networks, 
12, 2, pages 181-201. 
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience Publication. 
D. Zelenko, C. Aone and A. Richardella. 2003. Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 
Shubin Zhao, Adam Meyers, Ralph Grishman. 2004. 
Discriminative Slot Detection Using Kernel Methods. 
In the Proceedings of the 20th International Confer-
ence on Computational Linguistics. 
426
Covering Treebanks with GLARF
Adam Meyers
 
and Ralph Grishman   and Michiko Kosaka  and Shubin Zhao  
 
New York University, 719 Broadway, 7th Floor, NY, NY 10003 USA
 Monmouth University, West Long Branch, N.J. 07764, USA
meyers/grishman/shubinz@cs.nyu.edu, kosaka@monmouth.edu
Abstract
This paper introduces GLARF, a frame-
work for predicate argument structure.
We report on converting the Penn Tree-
bank II into GLARF by automatic
methods that achieved about 90% pre-
cision/recall on test sentences from the
Penn Treebank. Plans for a corpus
of hand-corrected output, extensions of
GLARF to Japanese and applications
for MT are also discussed.
1 Introduction
Applications using annotated corpora are often,
by design, limited by the information found in
those corpora. Since most English treebanks pro-
vide limited predicate-argument (PRED-ARG)
information, parsers based on these treebanks do
not produce more detailed predicate argument
structures (PRED-ARG structures). The Penn
Treebank II (Marcus et al, 1994) marks sub-
jects (SBJ), logical objects of passives (LGS),
some reduced relative clauses (RRC), as well as
other grammatical information, but does not mark
each constituent with a grammatical role. In our
view, a full PRED-ARG description of a sen-
tence would do just that: assign each constituent
a grammatical role that relates that constituent to
one or more other constituents in the sentence.
For example, the role HEAD relates a constituent
to its parent and the role OBJ relates a constituent
to the HEAD of its parent. We believe that the
absence of this detail limits the range of appli-
cations for treebank-based parsers. In particu-
lar, they limit the extent to which it is possible
to generalize, e.g., marking IND-OBJ and OBJ
roles allows one to generalize a single pattern to
cover two related examples (?John gave Mary a
book? = ?John gave a book to Mary?). Distin-
guishing complement PPs (COMP) from adjunct
PPs (ADV) is useful because the former is likely
to have an idiosyncratic interpretation, e.g., the
object of ?at? in ?John is angry at Mary? is not
a locative and should be distinguished from the
locative case by many applications.
In an attempt to fill this gap, we have begun
a project to add this information using both au-
tomatic procedures and hand-annotation. We are
implementing automatic procedures for mapping
the Penn Treebank II (PTB) into a PRED-ARG
representation and then we are correcting the out-
put of these procedures manually. In particular,
we are hoping to encode information that will en-
able a greater level of regularization across lin-
guistic structures than is possible with PTB.
This paper introduces GLARF, the Grammati-
cal and Logical Argument Representation Frame-
work. We designed GLARF with four objec-
tives in mind: (1) capturing regularizations ?
noncanonical constructions (e.g., passives, filler-
gap constructions, etc.) are represented in terms
of their canonical counterparts (simple declara-
tive clauses); (2) representing all phenomena us-
ing one simple data structure: the typed feature
structure (3) consistently labeling all arguments
and adjuncts for phrases with clear heads; and (4)
producing clear and consistent PRED-ARGs for
phrases that do not have heads, e.g., conjoined
structures, named entities, etc. ? rather than try-
ing to squeeze these phrases into an X-bar mold,
we customized our representations to reflect their
head-less properties. We believe that a framework
for PRED-ARG needs to satisfy these objectives
to adequately cover a corpus like PTB.
We believe that GLARF, because of its uni-
form treatment of PRED-ARG relations, will be
valuable for many applications, including ques-
tion answering, information extraction, and ma-
chine translation. In particular, for MT, we ex-
pect it will benefit procedures which learn trans-
lation rules from syntactically analyzed parallel
corpora, such as (Matsumoto et al, 1993; Mey-
ers et al, 1996). Much closer alignments will
be possible using GLARF, because of its multi-
ple levels of representation, than would be pos-
sible with surface structure alone (An example is
provided at the end of Section 2). For this reason,
we are currently investigating the extension of our
mapping procedure to treebanks of Japanese (the
Kyoto Corpus) and Spanish (the UAM Treebank
(Moreno et al, 2000)). Ultimately, we intend to
create a parallel trilingual treebank using a com-
bination of automatic methods and human correc-
tion. Such a treebank would be valuable resource
for corpus-trained MT systems.
The primary goal of this paper is to discuss the
considerations for adding PRED-ARG informa-
tion to PTB, and to report on the performance of
our mapping procedure. We intend to wait until
these procedures are mature before beginning an-
notation on a larger scale. We also describe our
initial research on covering the Kyoto Corpus of
Japanese with GLARF.
2 Previous Treebanks
There are several corpora annotated with PRED-
ARG information, but each encode some dis-
tinctions that are different. The Susanne Cor-
pus (Sampson, 1995) consists of about 1/6 of the
Brown Corpus annotated with detailed syntactic
information. Unlike GLARF, the Susanne frame-
work does not guarantee that each constituent be
assigned a grammatical role. Some grammatical
roles (e.g., subject, object) are marked explicitly,
others are implied by phrasetags (Fr corresponds
to the GLARF node label SBAR under a REL-
ATIVE arc label) and other constituents are not
assigned roles (e.g., constituents of NPs). Apart
from this concern, it is reasonable to ask why
we did not adapt this scheme for our use. Su-
sanne?s granularity surpasses PTB-based GLARF
in many areas with about 350 wordtags (part of
speech) and 100 phrasetags (phrase node labels).
However, GLARF would express many of the de-
tails in other ways, using fewer node and part of
speech (POS) labels and more attributes and role
labels. In the feature structure tradition, GLARF
can represent varying levels of detail by adding
or subtracting attributes or defining subsumption
hierarchies. Thus both Susanne?s NP1p word-
tag and Penn?s NNP wordtag would correspond
to GLARF?s NNP POS tag. A GLARF-style
Susanne analysis of ?Ontario, Canada? is (NP
(PROVINCE (NNP Ontario)) (PUNCTUATION
(, ,)) (COUNTRY (NNP Canada)) (PATTERN
NAME) (SEM-FEATURE LOC)). A GLARF-
style PTB analysis uses the roles NAME1 and
NAME2 instead of PROVINCE and COUNTRY,
where name roles (NAME1, NAME2) are more
general than PROVINCE and COUNTRY in a
subsumption hierarchy. In contrast, attempts to
convert PTB into Susanne would fail because de-
tail would be unavailable. Similarly, attempts to
convert Susanne into the PTB framework would
lose information. In summary, GLARF?s ability
to represent varying levels of detail allows dif-
ferent types of treebank formats to be converted
into GLARF, even if they cannot be converted into
each other. Perhaps, GLARF can become a lingua
franca among annotated treebanks.
The Negra Corpus (Brants et al, 1997) pro-
vides PRED-ARG information for German, simi-
lar in granularity to GLARF. The most significant
difference is that GLARF regularizes some phe-
nomena which a Negra version of English would
probably not, e.g., control phenomena. Another
novel feature of GLARF is the ability to represent
paraphrases (in the Harrisian sense) that are not
entirely syntactic, e.g., nominalizations as sen-
tences. Other schemes seem to only regularize
strictly syntactic phenomena.
3 The Structure of GLARF
In GLARF, each sentence is represented by a
typed feature structure. As is standard, we
model feature structures as single-rooted directed
acyclic graphs (DAGs). Each nonterminal is la-
beled with a phrase category, and each leaf is la-
beled with either: (a) a (PTB) POS label and a
word (eat, fish, etc.) or (b) an attribute value (e.g.,
singular, passive, etc.). Types are based on non-
terminal node labels, POSs and other attributes
(Carpenter, 1992). Each arc bears a feature label
which represents either a grammatical role (SBJ,
OBJ, etc.) or some attribute of a word or phrase
(morphological features, tense, semantic features,
etc.).1 For example, the subject of a sentence is
the head of a SBJ arc, an attribute like SINGU-
LAR is the head of a GRAM-NUMBER arc, etc.
A constituent involved in multiple surface or log-
ical relations may be at the head of multiple arcs.
For example, the surface subject (S-SBJ) of a pas-
sive verb is also the logical object (L-OBJ). These
two roles are represented as two arcs which share
the same head. This sort of structure sharing anal-
ysis originates with Relational Grammar and re-
lated frameworks (Perlmutter, 1984; Johnson and
Postal, 1980) and is common in Feature Structure
frameworks (LFG, HPSG, etc.). Following (John-
son et al, 1993)2, arcs are typed. There are five
different types of role labels:
 Attribute roles: Gram-Number (grammati-
cal number), Mood, Tense, Sem-Feature (se-
mantic features like temporal/locative), etc.
 Surface-only relations (prefixed with S-),
e.g., the surface subject (S-SBJ) of a passive.
 Logical-only Roles (prefixed with L-), e.g.,
the logical object (L-OBJ) of a passive.
 Intermediate roles (prefixed with I-) repre-
senting neither surface, nor logical positions.
In ?John seemed to be kidnapped by aliens?,
?John? is the surface subject of ?seem?, the
logical object of ?kidnapped?, and the in-
termediate subject of ?to be?. Intermedi-
ate arcs capture are helpful for modeling the
way sentences conform to constraints. The
intermediate subject arc obeys lexical con-
straints and connect the surface subjects of
?seem? (COMLEX Syntax class TO-INF-
RS (Macleod et al, 1998a)) to the subject
of the infinitive. However, the subject of the
infinitive in this case is not a logical sub-
ject due to the passive. In some cases, in-
termediate arcs are subject to number agree-
ment, e.g., in ?Which aliens did you say
were seen??, the I-SBJ of ?were seen? agrees
with ?were?.
 Combined surface/logical roles (unprefixed
arcs, which we refer to as SL- arcs). For ex-
1A few grammatical roles are nonfunctional, e.g., a con-
stituent can have multiple ADV constituents. We number
these roles (ADV1, ADV2,  ) to preserve functionality.
2That paper uses two arc types: category and relational.
ample, ?John? in ?John ate cheese? would be
the target of a SBJ subject arc.
Logical relations, encoded with SL- and L-
arcs, are defined more broadly in GLARF than
in most frameworks. Any regularization from a
non-canonical linguistic structure to a canonical
one results in logical relations. Following (Harris,
1968) and others, our model of canonical linguis-
tic structure is the tensed active indicative sen-
tence with no missing arguments. The following
argument types will be at the head of logical (L-)
arcs based on counterparts in canonical sentences
which are at the head of SL- arcs: logical argu-
ments of passives, understood subjects of infini-
tives, understood fillers of gaps, and interpreted
arguments of nominalizations (In ?Rome?s de-
struction of Carthage?, ?Rome? is the logical sub-
ject and ?Carthage? is the logical object). While
canonical sentence structure provides one level
of regularization, canonical verb argument struc-
tures provide another. In the case of argument al-
ternations (Levin, 1993), the same role marks an
alternating argument regardless of where it occurs
in a sentence. Thus ?the man? is the indirect ob-
ject (IND-OBJ) and ?a dollar? is the direct object
(OBJ) in both ?She gave the man a dollar? and
?She gave a dollar to the man? (the dative alter-
nation). Similarly, ?the people? is the logical ob-
ject (L-OBJ) of both ?The people evacuated from
the town? and ?The troops evacuated the people
from the town?, when we assume the appropriate
regularization. Encoding this information allows
applications to generalize. For example, a single
Information Extraction pattern that recognizes the
IND-OBJ/OBJ distinction would be able to han-
dle these two examples. Without this distinction,
2 patterns would be needed.
Due to the diverse types of logical roles, we
sub-type roles according to the type of regu-
larization that they reflect. Depending on the
application, one can apply different filters to a
detailed GLARF representation, only looking at
certain types of arcs. For example, one might
choose all logical (L- and SL-) roles for an
application that is trying to acquire selection
restrictions, or all surface (S- and SL-) roles
if one was interested in obtaining a surface
parse. For other applications, one might want to
choose between subtypes of logical arcs. Given
(S (NP-SBJ (PRP they))
(VP (VP (VBD spent)
(NP-2 ($ $)
(CD 325,000)
(-NONE- *U*))
(PP-TMP-3 (IN in)
(NP (CD 1989))))
(CC and)
(VP (NP=2 ($ $)
(CD 340,000)
(-NONE- *U*))
(PP-TMP=3 (IN in)
(NP (CD 1990))))))
Figure 1: Penn representation of gapping
a trilingual treebank, suppose that a Spanish
treebank sentence corresponds to a Japanese
nominalization phrase and an English nominal-
ization phrase, e.g.,
Disney ha comprado Apple Computers
Disney?s acquisition of Apple Computers
Furthermore, suppose that the English treebank
analyzes the nominalization phrase both as an
NP (Disney = possessive, Apple Computers =
object of preposition) and as a paraphrase of a
sentence (Disney = subject, Apple Computers
= object). For an MT system that aligns the
Spanish and English graph representation, it
may be useful to view the nominalization phrase
in terms of the clausal arguments. However,
in a Japanese/English system, we may only
want to look at the structure of the English
nominalization phrase as an NP.
4 GLARF and the Penn Treebank
This section focuses on some characteristics of
English GLARF and how we map PTB into
GLARF, as exemplified by mapping the PTB rep-
resentation in Figure 1 to the GLARF representa-
tion in Figure 2. In the process, we will discuss
how some of the more interesting linguistic phe-
nomena are represented in GLARF.
4.1 Mapping into GLARF
Our procedure for mapping PTB into GLARF
uses a sequence of transformations. The first
transformation applies to PTB, and the out-
put of each 	

 is the input of
