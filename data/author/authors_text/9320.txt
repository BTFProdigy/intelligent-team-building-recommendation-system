Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 480?483,
Prague, June 2007. c?2007 Association for Computational Linguistics
WIT: Web People Search Disambiguation using Random Walks
Jose? Iria, Lei Xia, Ziqi Zhang
The University of Sheffield
211 Portobello Street
Sheffield S1 4DP, United Kingdom
{j.iria, l.xia, z.zhang}@sheffield.ac.uk
Abstract
In this paper, we describe our work on a ran-
dom walks-based approach to disambiguat-
ing people in web search results, and the im-
plementation of a system that supports such
approach, which we used to participate at
Semeval?07 Web People Search task.
1 Introduction
Finding information about people on the Web us-
ing a search engine is far from being a quick and
easy process. There is very often a many-to-many
mapping of person names to the actual persons, that
is, several persons may share the same name, and
several names may refer to the same person. In
fact, person names are highly ambiguous: (Guha and
Garg, 2004) reports that only 90.000 thousand dif-
ferent names are shared by 100 million people ac-
cording to the U.S. Census Bureau. This creates the
need to disambiguate the several referents typically
found in the web pages returned by a query for a
given person name.
The Semeval?07 Web People Search challenge
(Artiles et al, 2007) formally evaluated systems on
this task. In this paper, we describe our work on
a random walks-based approach to disambiguating
people in web search results, heavily influenced by
(Minkov et al, 2006). This particular model was
chosen due to its elegance in seamlessly combining
lexico-syntactic features local to a given webpage
with topological features derived from its place in
the network formed by the hyperlinked web pages
returned by the query, to arrive at one single mea-
sure of similarity between any two pages.
2 Proposed Method
In a nutshell, our approach 1) uses a graph to model
the web pages returned by the search engine query,
2) discards irrelevant web pages using a few sim-
ple hand-crafted heuristics, 3) computes a similarity
matrix for web pages using random walks over the
graph, and 4) finally clusters the web pages given the
similarity matrix. The next subsections detail these
steps.
2.1 Web People Search Graph
We build a directed weighted typed graph from the
corpus. The graph is a 5-tuple G = (V,E, t, l, w),
where V is the set of nodes,E : V ?V is the ordered
set of edges, t : V ? T is the node type function
(T = {t1, . . . , t|T |} is a set of types), l : E ? L is
the edge label function (L = {l1, . . . , l|L|} is a set of
labels), and w : L ? R is the label weight function.
We structure our problem domain with the types and
labels presented in Figure 1.
In order to transform the text into a graph that
conforms to the model shown, we take the output of
standard NLP tools and input it as nodes and edges
into the graph, indexing nodes by string value to en-
sure that identical contents for any given node type
are merged into a single node in the graph. To pro-
cess the corpus, we run a standard NLP pipeline
seperately over the metadata, title and body of the
HTML pages, but not before having transformed
its contents as much into plain text as possible, by
removing HTML tags, javascript code, etc. The
pipeline used is composed of tokenization, removal
of stop words and infrequent words, and stemming
with Porter?s algorithm. The resulting graph at this
480
Figure 1: The data representation model adopted
stage consists of the nodes of type Token, Webpage,
Metadata, Title and Body, properly interconnected.
We then run a named entity recognizer to associate
NE tags to the respective documents, via the con-
stituent words of the NE. The information about the
original URL of page is given by the corpus, while
Host is trivially obtained from it. We finalise the
graph by inserting an edge of type linked by between
any web page linked by another in the corpus, and
an edge of type related to between any web page re-
lated to another in the corpus, as given by Google?s
related: operator.
For the named entity recognition task, we have
compared GATE and OpenNLP toolkits. Although
both toolkits show comparable results, OpenNLP
demonstrated faster performance. Moreover, some
documents in the corpus consisted of very exten-
sive lists of names (e.g. phonebook records) which
slowed the NER to a halt in practice. To compen-
sate for this, we applied a chunking window at the
beginning and end of each body content and around
each occurrence of the person name being consid-
ered (and its variants determined heuristically). The
window size used was 3000 characters in length,
and an overlap between windows results in a merged
window.
2.2 Discarding using heuristics
To discard irrelevant documents within the corpus,
we manually devised two heuristics rules for classi-
fication by observing the training data at hand. The
heuristics are 1) whether the page has content at all,
2) whether the page contains at least one appearance
of mentioned person name with its variants. This
simple classification showed high precision and low
recall on the training data. We also tried a SVM-
based classifier trained on a typical bag-of-words
feature vector space obtained from the training data,
but found the such classifier not to be sufficiently re-
liable.
2.3 Random Walks Model
We aim to determine the similarity between any two
nodes of type Webpage in the graph. In our work,
similarity between two nodes in the graph is ob-
tained by employing a random walks model. A ran-
dom walk, sometimes called a ?drunkard?s walk,? is
a formalization of the intuitive idea of taking suc-
cessive steps in a graph, each in a random direction
(Lova?sz, 2004). Intuitively, the ?harder? it is for a
drunkard to arrive at a given webpage starting from
another, the less similar the two pages are.
Our model defines weights for each edge type,
which, informally, determine the relevance of each
feature type to establish a similarity between any
two pages. Let Ltd = {l(x, y) : (x, y) ? E ?
T (x) = td} be the set of possible labels for edges
leaving nodes of type td. We require that the weights
form a probability distribution over Ltd , i.e.
?
l?Ltd
w(l) = 1 (1)
We build an adjacency matrix of locally appropriate
similarity between nodes as
Wij =
{ ?
lk?L
w(lk)
|(i,?)?E:l(i,?)=lk|
, (i, j) ? E
0, otherwise
(2)
where Wij is the ith-line and jth-column entry of
W , indexed by V . Equation 2 distributes uniformly
the weight of edges of the same type leaving a given
node. We could choose to distribute them otherwise,
e.g. we could distribute the weights according to
some string similarity function or language model
(Erkan, 2006), depending on the label.
We associate the state of a Markov chain to ev-
ery node of the graph, that is, to each node i we
associate the one-step probability P (0)(j|i) of a ran-
dom walker traversing to an adjacent node j. These
481
probabilities are expressed by the row stochastic ma-
trix D?1W , where D is the diagonal degree ma-
trix given by Dii =
?
k Wik. The ?reinforced?
similarity between two nodes in the graph is given
by the t-step transition probability P (t)(j|i), which
can be simply computed by a matrix power, i.e.,
P (t)(j|i) = [(D?1W )t]ij .
Note that t should not be very large in our case.
The probability distribution of an infinite random
walk over the nodes, called the stationary distribu-
tion of the graph, is uninteresting to us for cluster-
ing purposes since it gives an information related to
the global structure of the graph. It is often used as
a measure to rank the structural importance of the
nodes in a graph (Page et al, 1998). For clustering,
we are more interested in the local similarities inside
a cluster of nodes that separate them from the rest of
the graph. Also, in practice, using t > 2 leads to
high computational cost requirements, as the matrix
becomes more dense as t grows.
Equation 2 introduces the need to learn the func-
tion w. In other words, we need to tune the model to
use the most relevant features for this particular task.
Tuning is performed on the training set by compar-
ing the standard purity and inverse purity measures
of the clusters against the gold standard, and using
a simulated annealing optimization method as de-
scribed in (Nie et al, 2005).
2.4 Commute Time Distance
The algorithm takes as input a symmetric similarity
matrix S, which we derive from the random walk
model of the previous section as follows. We com-
pute the Euclidean Commute Time (ECT) distance
(Saerens et al, 2004) of any two nodes of type Web-
page in the graph. The ECT distance is (also) based
on a random walk model, and presents the inter-
esting property of decreasing when the number of
paths connecting two nodes increases or when the
length of any path decreases, which makes it well-
suited for clustering tasks. Another nice property
of ECT is that it is non-parametric, so no tuning
is required here. ECT has connections with princi-
pal component analysis and spectral theory (Saerens
et al, 2004).
In particular, we are interested in the average
commute time quantity, n(i, j), which is defined as
the average number of steps a random walker, start-
ing in state i, will take before entering a given state j
for the first time, and go back to i. That is, n(i, j) =
m(j|i) + m(i|j), where the quantity m(j|i), called
the average first-passage time, is defined as the av-
erage number of steps a random walker, starting in
state i, will take to enter state j for the first time. We
compute the average first-passage time iteratively by
means of the following recurrence:
{
m(i|j) = 1 +
?|V |
k=1,k 6=i P
(t)(k|j)m(i|k), j 6= i
m(i|i) = 0
(3)
where P (t)(?|?) is the t-step transition probability of
the random walk model over G presented in the pre-
vious section.
Informally, we may regard the random walk
model presented in the previous section as a ?re-
fined? document similarity measure, replacing, e.g.,
the typical TF-IDF measure with a measure that
works in a similar way but over all features rep-
resented in the graph, whereas we can regard the
ECTmeasure presented in this section as a ?booster?
to a basic clustering techniques (cf. next section),
achieved by means of coupling clustering with a ran-
dom walk-based distance which has been shown to
be competitive with state-of-the-art algorithms such
as spectral clustering (Luh Yen et al, 2007).
2.5 Clustering
Clustering aims at partitioning n given data points
into k clusters, such that points within a cluster are
more similar to each other than ones taken from dif-
ferent clusters. An important feature of the clus-
tering algorithm that we require for the problem at
hand is its ability to determine the number k of nat-
ural clusters, since any number of referents may be
present in the web search results. However, most
clustering algorithms require this number to be an
input, which means that they may break up or com-
bine natural clusters, or even create clusters when no
natural ones exist in the data.
We use a form of group-average agglomerative
clustering as described in (Fleischman and Hovy,
2004), shown in Table 1, which works fast for this
problem. A difficult problem (with any clustering
approach) has to do with the number of initial clus-
ters or, alternatively, with setting a threshold for
when to stop clustering. This threshold could po-
482
Input: symmetric similarity matrix S, threshold ?
Output: a set of clusters C
1. (i, j)? find min score in S
2. if Sij > ? then exit
3. place i and j in the same cluster in C (merging
existing clusters of i and j if needed)
4. (average pairs of edges connecting to nodes i,j
from any node k)
4a. Sik ? (Sik + Sjk)/2, k 6= i, j
4b. Ski ? (Ski + Skj)/2, k 6= i, j
5. remove j-th column and j-th line from S (effec-
tively merging nodes i,j into a single node)
6. goto 1
7. return clusters C
Table 1: The simple group-average agglomerative
clustering algorithm used
tentially also be optimized using the training data;
however, we have opted for unsupervised heuristics
to do that, e.g. the well-known Calinski&Harabasz
stopping rule (Calinski&Harabasz, 1974).
3 Results Obtained
The results obtained by the system are presented in
the following table. The evaluation measures used
were f-measure, purity and inverse purity - for a de-
tailed description refer to the task description (Ar-
tiles et al, 2007).
aver f05 aver f02 aver pur aver inv pur
0,49 0,66 0,36 0,93
The results are below average for this Semeval
task, and should not be regarded as representative
of the approach adopted, since the authors have had
limited time available to ensure a pristine implemen-
tation of the whole approach.
References
Artiles, J., Gonzalo, J., & Sekine, S. (2007). The
SemEval-2007 WePS Evaluation: Establishing a
benchmark for the Web People Search Task. In Pro-
ceedings of Semeval 2007, Association for Computa-
tional Linguistics.
Calinski and Harabasz (1974). A Dendrite Method for
Cluster Analysis Communications in Statistics, 3(1),
1974, 1-27.
Erkan, G. (2006). Language model-based document
clustering using random walks. Proceedings of the
main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics (pp. 479?486).
Association for Computational Linguistics.
Fleischman, M. B., & Hovy, E. (2004). Multi-document
person name resolution. Proceedings of the ACL 2004.
Association for Computational Linguistics.
Guha, R. V., & Garg, A. (2003). Disambiguating People
in Search. TAP: Building the Semantic Web.. ACM
Press.
Luh Yen, Francois Fouss, C. D., Francq, P., & Saerens,
M. (2007). Graph nodes clustering based on the
commute-time kernel. To appear in the proceedings of
the 11th Pacific-Asia Conference on Knowledge Dis-
covery and Data Mining (PAKDD 2007). Lecture
Notes in Computer Science (LNCS).
Minkov, E., Cohen, W. W., & Ng, A. Y. (2006). Con-
textual search and name disambiguation in email us-
ing graphs. SIGIR ?06: Proceedings of the 29th
annual international ACM SIGIR conference on Re-
search and development in information retrieval (pp.
27?34). ACM Press.
Nie, Z., Zhang, Y., Wen, J. R., & Ma, W. Y. (2005).
Object-level ranking: Bringing order to web objects.
Proceedings of WWW?05.
Page, L., Brin, S., Motwani, R., & Winograd, T. (1998).
The pagerank citation ranking: Bringing order to
the web (Technical Report). Stanford Digital Library
Technologies Project.
Saerens, M., Fouss, F., Yen, L., & Dupont, P. (2004). The
principal components analysis of a graph, and its re-
lationships to spectral clustering. Proceedings of the
15th European Conference on Machine Learning.
La?szlo? Lova?sz (1993). RandomWalks on Graphs: A Sur-
vey. Combinatorics, Paul Erdos is Eighty (Volume 2),
Keszthely (Hungary), 1993, p 1-46..
483
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
A Novel Approach to Automatic Gazetteer Generation using  
Wikipedia 
 
Ziqi Zhang 
University of Sheffield, UK 
z.zhang@dcs.shef.ac.uk 
Jos? Iria 
University of Sheffield, UK 
j.iria@dcs.shef.ac.uk 
  
Abstract 
Gazetteers or entity dictionaries are important 
knowledge resources for solving a wide range of 
NLP problems, such as entity extraction. We in-
troduce a novel method to automatically generate 
gazetteers from seed lists using an external 
knowledge resource, the Wikipedia. Unlike pre-
vious methods, our method exploits the rich con-
tent and various structural elements of Wikipe-
dia, and does not rely on language- or domain-
specific knowledge. Furthermore, applying the 
extended gazetteers to an entity extraction task in 
a scientific domain, we empirically observed a 
significant improvement in system accuracy 
when compared with those using seed gazetteers. 
1 Introduction 
Entity extraction is the task of identifying and 
classifying atomic text elements into predefined 
categories such as person names, place names, 
and organization names. Entity extraction often 
serves as a fundamental step for complex Natural 
Language Processing (NLP) applications such as 
information retrieval, question answering, and 
machine translation. It has been recognized that 
in this task, gazetteers, or entity dictionaries, play 
a crucial role (Roberts et al 2008). In addition, 
they serve as important resources for other stu-
dies, such as assessing level of ambiguities of a 
language, and disambiguation (Maynard et al 
2004).  
Because building and maintaining high quality 
gazetteers by hand is very time consuming (Ka-
zama and Torisawa, 2008), many solutions have 
proposed generating gazetteers automatically 
from existing resources. In particular, the success 
that solutions which exploit Wikipedia1 have 
been enjoying in many other NLP applications 
has encouraged a number of research works on 
automatic gazetteer generation to use Wikipedia, 
                                                          
1 http://en.wikipedia.org 
such as works by Toral and Mu?oz (2006), and 
Kazama and Torisawa (2007). 
Unfortunately, current systems still present 
several limitations. First, none have exploited the 
full content and structure of Wikipedia articles, 
but instead, only make use of the article?s first 
sentence. However, the full content and structure 
of Wikipedia carry rich information that has been 
proven useful in many other NLP problems, such 
as document classification (Gabrilovich and 
Markovitch, 2006), entity disambiguation (Bu-
nescu and Pa?ca, 2006), and semantic relatedness 
(Strube and Ponzetto, 2006). Second, no other 
works have evaluated their methods in the con-
text of entity extraction tasks. Evaluating these 
generated gazetteers in real NLP applications is 
important, because the quality of these gazetteers 
has a major impact on the performance of NLP 
applications that make use of them. Third, the 
majority of approaches focus on newswire do-
main and the four classic entity types location 
(LOC), person (PER), organization (ORG) and 
miscellaneous (MISC), which have been studied 
extensively. However, it has been argued that 
entity extraction is often much harder in scientif-
ic domains due to complexity of domain lan-
guages, density of information and specificity of 
classes (Murphy et al 2006; Byrne, 2007; Noba-
ta et al 2000).  
In this paper we propose a novel approach to 
automatically generating gazetteers using exter-
nal knowledge resources. Our method is lan-
guage- and domain- independent, and scalable. 
We show that the content and various structural 
elements of Wikipedia can be successfully ex-
ploited to generate high quality gazetteers. To 
assess gazetteer quality, we evaluate it in the 
context of entity extraction in the scientific do-
main of Archaeology, and demonstrate that the 
generated gazetteers improve the performance of 
an SVM-based entity tagger across all entity 
types on an archaeological corpus. 
The rest of the paper is structured as follows. 
In the next section, we review related work. In 
section 3 we explain our methodology for auto-
1
matic gazetteer generation. Section 4 introduces 
the problem domain and describes the experi-
ments conducted. Section 5 presents and dis-
cusses the results. Finally we conclude with an 
outline of future work. 
2 Related Work 
Currently, existing methods to automatic gazet-
teer generation can be categorized into two 
mainstreams; pattern driven approach and know-
ledge resource approach.  
The pattern driven approach uses domain- 
and language specific patterns to extract candi-
date entities from unlabeled corpora. The idea is 
to include features derived from unlabeled data 
to improve a supervised learning model. For ex-
ample, Riloff and Jones (1999) introduced a 
bootstrapping algorithm which starts from seed 
lists and, iteratively learns and refines domain 
specific extraction patterns for a semantic cate-
gory that are then used for building dictionaries 
from unlabeled data. Talukdar et al(2006), also 
starting with seed entity lists, apply pattern in-
duction to an unlabeled corpus and then use the 
induced patterns to extract candidate entities 
from the corpus to build extended gazetteers. 
They showed that using the token membership 
feature with the extended gazetteer improved the 
performance of a Conditional Random Field 
(CRF) entity tagger; Kozareva (2006) designed 
language specific extraction patterns and valida-
tion rules to build Spanish location (LOC), per-
son (PER) and organization (ORG) gazetteers 
from unlabeled data, and used these to improve a 
supervised entity tagger.  
However, the pattern driven approach has 
been criticized for weak domain adaptability and 
inadequate extensibility due to the specificity of 
derived patterns.  (Toral and Mu?oz, 2006; Ka-
zama and Torisawa, 2008).  Also, often it is dif-
ficult and time-consuming to develop domain- 
and language-specific patterns. 
The knowledge resource approach, attempts 
to solve these problems by relying on the abun-
dant information and domain-independent struc-
tures in existing large-scale knowledge re-
sources. Magnini et al(2002) used WordNet as a 
gazetteer together with rules to extract entities 
such as LOC, PER and ORG. They used two re-
lations in WordNet; Word_Class, referring to 
concepts bringing external evidence; and 
Word_Instance, referring to particular instances 
of those concepts. Concepts belonging to 
Word_Class are used to identify trigger words 
for candidate entities in corpus, while concepts 
of Word_Instance are used directly as lookup 
dictionaries. They achieved good results on a 
newswire corpus. The main limitation of Word-
Net is lack of domain specific vocabulary, which 
is critical to domain specific applications 
(Sch?tze and Pedersen, 1997). Roberts et al
(2008) used terminology extracted from UMLS 
as gazetteers and tested it in an entity extraction 
task over a medical corpus. Contrary to Word-
Net, UMLS is an example of a domain specific 
knowledge resource, thus its application is also 
limited. 
 
Recently, the exponential growth in informa-
tion content in Wikipedia has made this Web 
resource increasingly popular for solving a wide 
range of NLP problems and across different do-
mains.  
Concerning automatic gazetteer generation, 
Toral and Mu?oz (2006) tried to build gazetteers 
for LOC, PER, and ORG by extracting all noun 
phrases from the first sentences of Wikipedia 
articles. Next they map the noun phrases to 
WorldNet synsets, and follow the hyperonymy 
hierarchy until they reach a synset belonging to 
the entity class of interest. However, they did not 
evaluate the generated gazetteers in the context 
of entity extraction. Due to lack of domain spe-
cific knowledge in WordNet, their method is li-
mited if applied to domain specific gazetteer 
generation. In contrast, our method overcomes 
this limitation since it doesn?t rely on any re-
sources other than Wikipedia. Another funda-
mental difference is that our method exploits 
more complex structures of Wikipedia. 
 Kazama and Torisawa (2007) argued that 
while traditional gazetteers map word sequences 
to predefined entity categories such as ?London 
? {LOCATION}?, a gazetteer is useful as long 
as it returns consistent labels even if these are not 
predefined categories. Following this hypothesis, 
they mapped Wikipedia article titles to their 
hypernyms by extracting the first noun phrase 
after be in the first sentence of the article, and 
used these as gazetteers in an entity extraction 
task. In their experiment, they mapped over 
39,000 search candidates to approximately 1,200 
hypernyms; and using these hypernyms as cate-
gory labels in an entity extraction task showed an 
improvement in system performance. Later, Ka-
zama and Torisawa (2008) did the same in 
another experiment on a Japanese corpus and 
achieved consistent results. Although novel, their 
method in fact bypasses the real problem of ge-
2
nerating gazetteers of specific entity types. Our 
method is essentially different in this aspect. In 
addition, they only use the first sentence of Wi-
kipedia articles.  
3 Automatic Gazetteer Generation ? the 
Methodology 
In this section, we describe our methodology for 
automatic gazetteer generation using the know-
ledge resource approach. 
3.1 Wikipedia as the knowledge resource 
To demonstrate the validity of our approach, we 
have selected the English Wikipedia as the ex-
ternal knowledge resource. Wikipedia is a free 
multilingual and collaborative online encyclope-
dia that is growing rapidly and offers good quali-
ty of information (Giles, 2005). Articles in Wiki-
pedia are identified by unique names, and refer 
to specific entities. Wikipedia articles have many 
useful structures for knowledge extraction; for 
example, articles are inter-connected by hyper-
links carrying relations (Gabrilovich and Marko-
vitch, 2006); articles about similar topics are ca-
tegorized under the same labels, or grouped in 
lists; categories are organized as taxonomies, and 
each category is associated with one or more 
parent categories (Bunescu and Pa?ca, 2006). 
These relations are useful for identifying related 
articles and thus entities, which is important for 
automatic gazetteer generation. Compared to 
other knowledge resources such as WordNet and 
UMLS, Wikipedia covers significantly larger 
amounts of information across different domains, 
therefore, it is more suitable for building domain-
specific gazetteers. For example, as of February 
2009, there are only 147,287 unique words in 
WordNet2, whereas the English Wikipedia is 
significantly larger with over 2.5 million articles. 
A study by Holloway (2007) identified that by 
2005 there were already 78,977 unique catego-
ries divided into 1,069 disconnected category 
clusters, which can be considered as the same 
number of different domains. 
3.2 The methodology 
We propose an automatic gazetteer generation 
method using Wikipedia article contents, hyper-
links, and category structures, which can gener-
ate entity gazetteers of any type. Our method 
                                                          
2 According to 
http://wordnet.princeton.edu/man/wnstats.7WN , February 
2009 
takes input seed entities of any type, and extends 
them to more complete lists of the same type. It 
is based on three hypotheses; 
1. Wikipedia contains articles about domain 
specific seed entities. 
2. Using articles about the seed entities, we 
can extract fine-grained type labels for 
them, which can be considered as a list 
of hypernyms of the seed entities, and 
predefined entity type hyponyms of the 
seeds. 
3. Following the links on Wikipedia ar-
ticles, we can reach a large collection of 
articles that are related to the source ar-
ticles. If a related article?s type label (as 
extracted above) matches any of those 
extracted for seed entities, we consider it 
a similar entity of the predefined type. 
Naturally, we divide our methods into three 
steps; firstly we match a seed entity to a Wikipe-
dia article (the matching phase); next we label 
seed entities using the articles extracted for them 
and build a pool of fine-grained type labels for 
the seed entities (the labeling phase); finally we 
extract similar entities by following links in ar-
ticles of seed entities (the expansion phase). The 
pseudo-algorithm is illustrated in Figure 1.  
3.2.1 Matching seed entities to Wikipedia 
article 
For a given seed entity, we firstly use the exact 
phrase to retrieve Wikipedia articles. If not 
found, we use the leftmost longest match, as 
done by Kazama and Torisawa (2007). In Wiki-
pedia, searches for ambiguous phrases are redi-
rected to a Disambiguation Page, from which 
users have to manually select a sense. We filter 
out any matches that are directed to disambigua-
tion pages. This filtering strategy is also applied 
to step 3 in extracting candidate entities. 
3.2.2 Labeling seed entities 
After retrieving Wikipedia articles for all seed 
entities, we extract fine-grained type labels from 
these articles. We identified two types of infor-
mation from Wikipedia that can extract potential-
ly reliable labels.  
 
3
 
Figure 1. The proposed pseudo-algorithm for gazet-
teer generation from the content and various structural 
elements of Wikipedia 
 
As Kazama and Torisawa (2007) observed, in the 
first sentence of an article, the head noun of the 
noun phrase just after be is most likely the 
hypernym of the entity of interest, and thus a 
good category label. There are two pitfalls to this 
approach. First, the head noun may be too gener-
ic to represent a domain-specific label. For ex-
ample, following their approach the label ex-
tracted for the archaeological term ?Classical 
Stage?3 from the sentence ?The Classic Stage is 
an archaeological term describing a particular 
developmental level.? is ?term?, which is the 
head noun of ?archaeological term?. Clearly in 
such case the phrase is more domain-specific. 
For this reason we use the exact noun phrase as 
category label in our work. Second, their method 
ignores a correlative conjunction which in most 
cases indicates equivalently useful labels. For 
example, the two noun phrases in italic in the 
sentence ?Sheffield is a city and metropolitan 
borough in South Yorkshire, England? are equal-
ly useful labels for the article ?Sheffield?. There-
fore, we also extract the noun phrase connected 
by a correlative conjunction as the label. We ap-
ply this method to articles retrieved in 3.2.1. For 
                                                          
3Any Wikipedia examples for illustration in this paper make 
use of the English Wikipedia, February 2009, unless other-
wise stated. 
simplicity, we refer to this approach to labeling 
seed entities as FirstSentenceLabeling, and the 
labels created as Ls. Note that our method is es-
sentially different from Kazama and Torisawa as 
we do not add these extracted nouns to gazet-
teers; instead, we only use them for guiding the 
extraction of candidate entities, as described in 
section 3.2.3. 
As mentioned in section 3.1, similar articles 
in Wikipedia are manually grouped under the 
same categories by their authors, and categories 
are further organized as a taxonomy. As a result, 
we extract category labels of articles as fine-
grained type labels and consider them to be 
hypernyms of the entity?s article. We refer to this 
method as CategoryLabeling, and apply it to the 
seed entities to create a list of category labels, 
which we denote by Lc. 
Three situations arise in which the Category-
Labeling introduces noisy labels. First, some 
articles are categorized under a category with the 
same title as the article itself. For example, the 
article about ?Bronze Age? is categorized under 
category ?Bronze Age?. In this case, we explore 
the next higher level of the category tree, i.e., we 
extract categories of the category ?Bronze Age?, 
including ?2nd Millennium?, ?3rd millennium 
BC?, ?Bronze?, ?Periods and stages in Archaeo-
logy?, and ?Prehistory?. Second, some categories 
are meaningless and for management purposes, 
such as ?Articles to be Merged since 2008?, 
?Wikipedia Templates?. For these, we manually 
create a small list of ?stop? categories to be dis-
carded. Third, according to Strube and Ponzetto 
(2008), the category hierarchy is sometimes noi-
sy. To reduce noisy labels, we only keep labels 
that are extracted for at least 2 seed entities.  
 
Once a pool of fine-grained type labels have 
been created, in the next step we consider them 
as fine-grained and immediate hypernyms of the 
seed entities, and use them as control vocabulary 
to guide the extraction of candidate entities. 
3.2.3 Extracting candidate entities 
To extract candidate entities, we first identify 
from Wikipedia the entities that are related to the 
seed entities. Then we select from them those 
candidates that share one or more common 
hypernyms with the seed entities. The intuition is 
that in the taxonomy, nodes that share common 
immediate parents are mostly related, and, there-
fore, good candidates for extended gazetteers.  
Input: seed entities SE of type T 
Output: new entities NE of type T 
STEP 1 (section 3.2.1)  
1.1. Initialize Set P as articles for SE; 
1.2. For each entity e: SE 
1.3.     Retrieve Wikipedia article p for e; 
1.4.     Add p to P; 
STEP 2 (section 3.2.2) 
2.1. Initialize Set L 
2.2. For each p: P 
2.3.      Extract fine grained type labels l; 
2.4.      Add l to L; 
STEP 3 (section 3.2.3) 
3.1. Initialize Set HL; 
3.2. For each p: P 
3.3.     Add hyperlinks from p to HL; 
3.4. If necessary, recursively crawl extracted    
hyperlinks and repeat 3.2 and 3.3 
3.5. For each link hl: HL  
3.6.  Extract fine grained type labels l?; 
3.7. If L contains l? 
3.8.     Add title of hl to NE; 
3.9.     Add titles of redirect links of hl to 
NE; 
4
We extract related entities by following the 
hyperlinks from the articles retrieved for the seed 
entities, as by section 3.2.1. This is because in 
Wikipedia, articles often contain mentions of 
entities that also have a corresponding article, 
and these mentions are represented as outgoing 
hyperlinks. They link the main article of an enti-
ty (source entity) to other sets of entities (related 
entities). Therefore, by following these links we 
can reach a large set of related entities to the seed 
list. To reduce noise, we also filter out links to 
disambiguation pages as in section 3.2.1. Next, 
for each candidate in the related set, we use the 
two labeling approaches introduced in section 
3.2.2 to extract its type labels.  If any of these are 
included by the control vocabulary built with the 
same labeling approach, we accept them into the 
extended gazetteers. That is, if the control voca-
bulary is built by FirstSentenceLabeling we on-
ly use FirstSentenceLabeling to label the candi-
date. The same applies to CategoryLabeling. 
One can easily extend this stage by recursively 
crawling the hyperlinks contained in the re-
trieved pages. In addition, some Wikipedia ar-
ticles have one or more redirecting links, which 
groups several surface forms of a single entity. 
For example a search for ?army base? is redi-
rected to article ?military base?. These surface 
forms can be considered as synonyms, and we 
thus also select them for extend gazetteers.  
 
After applying the above processes to all seed 
entity articles, we obtain the output extended 
gazetteers of domain-specific types. To eliminate 
potentially ambiguous entities, for each extended 
gazetteer, we exclude entities that are found in 
domain-independent gazetteers. For example, we 
use a generic person name gazetteer to exclude 
ambiguous person names from the extended ga-
zetteers for LOC.  
4 Experiments 
In this section we describe our experiments. Our 
goal is to build extended gazetteers using the 
methods proposed in section 3, and test them in 
an entity extraction task to improve a baseline 
system. First we introduce the setting, an entity 
extraction task in the archaeological domain; 
next we describe data preparation including 
training data annotation and gazetteer generation; 
then, we introduce our baseline; and finally 
present the results. 
4.1 The Problem Domain 
The problem of entity extraction has been stu-
died extensively across different domains, par-
ticularly in newswire articles (Talukdar et al
2006), bio-medical science (Roberts et al 2008). 
In this experiment, we present the problem with-
in the domain of archaeology, which is a discip-
line that has a long history of active fieldwork 
and a significant amount of legacy data dating 
back to the nineteenth century and earlier. Jeffrey 
et al(2009) reports that despite the existing fast-
growing large corpora, little has been done to 
develop high quality meta-data for efficient 
access to information in these datasets, which has 
become a pressing issue in archaeology. To our 
best knowledge, three works have piloted the 
research on using information extraction tech-
niques for automatic meta-data generation in this 
field. Greengrass et al(2008) applied entity and 
relation extraction to historical court records to 
extract names, locations and trial names and their 
relations; Amrani et al(2008) used a series of 
text-mining technologies to extract archaeologi-
cal knowledge from specialized texts, one of 
these tasks concerns entity extraction. Byrne 
(2007) applied entity and relation extraction to a 
corpus of archaeology site records. Her work 
concentrated on nested entity recognition of 11 
entity types. 
Our work deals with archaeological entity ex-
traction from un-structured legacy data, which 
mostly consist of full-length archaeological re-
ports varying from 5 to over a hundred pages. 
According to Jeffrey et al(2009), three types of 
entities are most useful to an archaeologist; 
? Subject (SUB) ? topics that reports refer 
to, such as findings of artifacts and mo-
numents. It is the most ambiguous type 
because it covers various specialized 
domains such as warfare, architecture, 
agriculture, machinery, and education. 
For example ?Roman pottery?, ?spear-
head?, and ?courtyard?. 
? Temporal terms (TEM) ? archaeological 
dates of interest, which are written in a 
number of ways, such as years ?1066 - 
1211?, ?circa 800AD?; centuries ?C11?, 
?the 1st century?; concepts ?Bronze 
Age?, ?Medieval?; and acronyms such as 
?BA? (Bronze Age), ?MED? (Medieval). 
? Location (LOC) ? place names of inter-
est, such as place names and site ad-
dresses related to a finding or excava-
tion. In our study, these refer to UK-
specific places. 
 
5
Source Domain Tag Density 
astro-ph Astronomy 5.4% 
MUC7 Newswire 11.8% 
GENIA Biomedical 33.8% 
AHDS-
selected 
Archaeology 9.2% 
Table 1. Comparison of tag density in four test corpo-
ra for entity extraction tasks. The ?AHDS-selected? 
corpus used in this work has a tag density comparable 
to that of MUC7 
4.2 Corpus and resources 
We developed and tested our system on 30 full 
length UK archaeological reports archived by the 
Arts and Humanities Data Service (AHDS)4. 
These articles vary from 5 to 120 pages, with a 
total of 225,475 words. The corpus is tagged by 
three archaeologists, and is used for building and 
testing the entity extraction system. Compared to 
other test data reported in Murphy et al(2006), 
our task can be considered hard, due to the hete-
rogeneity of information of the entity types and 
lower tag density in the corpus (the percentage of 
words tagged as entities), see Table 1. Also, ac-
cording to Vlachos (2007), full length articles are 
harder than abstracts, which are found common 
in biomedical domain. This corpus is then split 
into five equal parts for a five-fold cross valida-
tion experiment.  
For seed gazetteers, we used the MIDAS Pe-
riod list5 as the gazetteer for TEM, the Thesaurus 
of Monuments Types (TMT2008) from English 
Heritage6 and the Thesaurus of Archaeology Ob-
jects from the STAR project7 as gazetteers for 
SUB, and the UK Government list of administra-
tive areas as the gazetteer for LOC. In the fol-
lowing sections, we will refer to these gazetteers 
as GAZ_original. 
4.3 Automatic gazetteer generation 
We used the seed gazetteers together with the 
methods presented in section 3 to build new ga-
zetteers for each entity type, and merge them 
with the seeds as extended gazetteers to be tested 
in our experiments. Since we introduced two me-
thods for labeling seed entities (section 3.2.2), 
which are also used separately for selecting ex-
tracted candidate entities (section 3.2.3), we de-
sign four experiments to test the methods sepa-
                                                          
4 http://ahds.ac.uk/ 
5 http://www.midas-heritage.info and http://www.fish-
forum.info 
6 http://thesaurus.english-heritage.org.uk 
7 http://hypermedia.research.glam.ac.uk/kos/STAR/ 
rately as well as in combination; specifically for 
each entity type, GAZ_EXTfirstsent denotes the ex-
tended gazetteer built using FirstSentenceLabe-
ling for labeling seed entities and selecting can-
didate entities; GAZ_EXTcategory refers to the ex-
tended gazetteer built with CategoryLabeling; 
GAZ_EXTunion merges entities in two extended 
gazetteers into a single gazetteer; while 
GAZ_EXTintersect is the intersection of 
GAZ_EXTfirstsent and GAZ_EXTcategory i.e., taking 
only entities that appear in both. Table 2 lists 
statistics of the gazetteers and Table 3 displays 
example type labels extracted by the two me-
thods. 
To implement the entity extraction system, we 
used Runes8 data representation framework, a 
collection of information extraction modules 
from T-rex9, and the machine learning frame-
work Aleph10. The core of the tagger system is a 
SVM classifier. We used the Java Wikipedia Li-
brary11 (JWPL v0.452b) and the Wikipedia dump 
of Feb 2007 published with it. 
4.4 Feature selection and baseline system 
We trained our baseline system by tuning feature 
sets used and the size of the token window to 
consider for feature generation; and we select the 
best performing setting as the baseline. Later we 
add official gazetteers in section 4.1 and ex-
tended gazetteers as in section 4.3 to the base-
lines and use gazetteer membership as an addi-
tional feature to empirically verify the improve-
ment in system accuracy. 
 
The baseline setting thus used a window size of 5 
and the following feature set: 
? Morphological root of a token 
? Exact token string 
? Orthographic type (e.g., lowercase, up-
percase) 
? Token kind (e.g., number, word) 
4.5 Result 
Table 4 displays the results obtained under each 
setting, using the standard metrics of Recall (R), 
Precision (P) and F-measure (F1). The bottom 
row illustrates Inter Annotator Agreement (IAA) 
                                                          
8 http://runes.sourceforge.net/ 
9 http://t-rex.sourceforge.net/ 
10 http://aleph-ml.sourceforge.net/ 
11 http://www.ukp.tu-darmstadt.de/software/jwpl/ 
6
 
 LOC SUB TEM 
GAZ_original 11,786 (8,228 found) 5,725 (4,320 found) 61 (43 found) 
GAZ_EXTfirstsent 19,385 (7,599)  11,182 (5,457) 163 (102) 
GAZ_EXTcategory 18,861 (7,075) 13,480 (7,745) 305 (245) 
GAZ_EXTunion 23,741 (11,955) 16,697 (10,972) 333 (272) 
GAZ_EXTintersect 14,022 (2,236) 7,455 (1,730) 133 (72) 
Table 2. Number of unique entities in each gazetteer, including official and extended versions. 
GAZ_EXT includes GAZ_original. For GAZ_original, numbers in brackets are the number of entities 
found in Wikipedia. For others, they are the number of extracted entities that are new to the correspond-
ing GAZ_original 
 
LOC SUB TEM 
FirstSentence-
Labeling (597) 
CategoryLabeling 
(779) 
FirstSentence-
Labeling (1342) 
CategoryLabe-
ling (761)  
FirstSentence-
Labeling (11) 
CategoryLabe-
ling 
(10) 
village, 
small village, 
place, 
town, 
civil parish 
villages in north 
Yorkshire, 
north Yorkshire geo-
graphy stubs, 
villages in Norfolk, 
villages in Somerset, 
English market towns 
facility, 
building,  
ship, 
tool, 
device, 
establishment 
ship types, 
monument 
types, 
gardening, 
fortification, 
architecture 
stubs 
period, 
archaeological 
period, 
era, 
century, 
millennium 
Periods and 
stages in arc-
haeology, 
Bronze age, 
middle ages, 
historical eras, 
centuries 
Table 3. Top 5 most frequently extracted (counted by number of seed entities sharing that label) fine-
grained type labels for each entity type. Numbers in brackets are the number of unique labels extracted 
 
 LOC SUB TEM 
 P R F1 P R F1 P R F1 
Baseline (B) 69.4 67.4 68.4 69.6 62.3 65.7 82.3 81.4 81.8 
B+ GAZ_original 69.0 72.1 70.5 69.7 65.4 67.5 82.3 82.7 82.5 
B+ GAZ_EXTfirstsent 69.9 76.7 73.1 70.0 68.3 69.1 82.6 84.6 83.6 
B+ EXTcategory 69.1 75.1 72.0 68.8 67.0 67.9 82.0 83.7 82.8 
B+ EXTunion 68.9 75.0 71.8 69.8 66.5 68.1 82.4 83.4 82.9 
B+ EXTintersect 69.3 76.2 72.6 69.7 67.6 68.6 82.6 84.3 83.4 
IAA - - 75.3 - - 63.6 - - 79.9 
Table 4. Experimental results showing accuracy of systems in the entity extraction task for each type of entities, 
varying the feature set used. Baseline performances are marked in italic. Better performances than baselines 
achieved by our systems are highlighted in bold.
between the annotators on a shared sample cor-
pus of the same kind as that for building the sys-
tem, calculated using the metric by Hripcsak and 
Rothschild (2005). The metric is equivalent to 
scoring one annotator against the other using the 
F1 metric, and in practice system performance 
can be slightly higher than IAA (Roberts et al 
2008). The IAA figures for all types of entities 
are low, indicating that the entity extraction task 
for the archaeological domain is difficult, which 
is consistent with Byrne (2007)?s finding. 
5 Discussion 
As shown in Table 2, our methods have generat-
ed domain specific gazetteers that almost 
doubled the original seed gazetteers in every oc-
casion, even for the smallest seed gazetteer of 
TEM. This proves our hypotheses formulated in 
section 3.1, that by utilizing the hyperonymy re-
lation and exploring information in an external 
resource, one can extend a gazetteer by entities 
of similar types without utilizing language- and 
domain-specific knowledge. Also by taking the 
intersection of entities generated by the two labe-
ling methods (bottom row of table 2), we see that 
the overlap is relatively small (from 30%-40% of 
the list generated by either method), indicating 
that the extended gazetteers produced by the two 
methods are quite different, and may be used to 
complement each other. Combining figures in 
Table 3, we see that both methods extract fine-
grained type-labels that on average extract 4 - 14 
candidate entities. 
The quality of the gazetteers can be checked 
using the figures in Table 4. First, all extended 
gazetteers improved over the baselines for the 
three entity types, with the highest increase in F1 
of 4.7%, 3.4% and 1.8% for LOC, SUB, and 
7
TEM respectively. In addition, they all outper-
form the original gazetteers, indicating that the 
quality of extended gazetteers is good for the 
entity extraction task.  
By comparing the effects of each extended 
gazetteer, we notice that using the gazetteers 
built with type-labels extracted from the first 
sentence of Wikipedia article always outper-
forms using those built via the Wikipedia catego-
ries, indicating that the first method (FirstSen-
tenceLabeling) results in better quality gazet-
teers. This is due to two reasons. First, the cate-
gory tree in Wikipedia is not a strict taxonomy, 
and does not always contain is-a relationships 
(Strube and Ponzetto, 2006). Although we have 
eliminated categories that are extracted for only 
one seed entity, the results indicate the extended 
gazetteers are still noisier than those built by 
FirstSentenceLabeling. To illustrate, the articles 
for SUB seed entities ?quiver? and ?arrowhead? 
are both categorized under ?Archery?, which 
permits noisy candidates such as ?Bowhunting?, 
?Camel archer? and ?archer?. Applying a stricter 
filtering threshold may resolve this problem. 
Second, compared to Wikipedia categories, the 
labels extracted from the first sentences are 
sometimes very fine-grained and restrictive. For 
example, the labels extracted for ?Buckingham-
shire? from the first sentence are ?ceremonial 
Home County? and ?Non-metropolitan County?, 
both of which are UK-specific LOC concepts. 
These rather restrictive labels help control the 
gazetteer expansion within the domain of inter-
est. The better performance with FirstSentence-
Labeling indicates that such restrictions have 
played a positive role in reducing noise in the 
labels generated, and then improving the quality 
of candidate entities.  
We also tested effects of combining the two 
approaches, and noticed that taking the intersec-
tion of gazetteers generated by the two ap-
proaches outperform the union, but figures are 
still lower than the single best method. This is 
understandable because by permitting members 
of noisier gazetteers the system performance de-
grades. 
6 Conclusion 
We have presented a novel language- and do-
main- independent approach for automatically 
generating domain-specific gazetteers for entity 
recognition tasks using Wikipedia. Unlike pre-
vious approaches, our approach makes use of 
richer content and structural elements of Wikipe-
dia. By applying this approach to a corpus of the 
Archaeology domain, we empirically observed a 
significant improvement in system accuracy 
when compared with the baseline systems, and 
the baselines plus original gazetteers.  
The extensibility and domain adaptability of 
our methods still need further investigation. In 
particular, our methods can be extended to intro-
duce several statistical filtering thresholds to 
control the label generation and candidate entity 
extraction in an attempt to reduce noise; also the 
effect of recursively crawling Wikipedia articles 
in the candidate extraction stage is worth study-
ing. Additionally, it would be interesting to study 
other structures of Wikipedia, such as list struc-
tures and info boxes, in gazetteer generation. In 
future we will investigate into these possibilities, 
and also test our approach in different domains. 
Acknowledgement 
This work is funded by the Archaeotools12 project that 
is carried out by Archaeology Data Service, Universi-
ty of York, UK and the Organisation, Information and 
Knowledge Group (OAK) of University of Sheffield, 
UK. 
 
References  
Ahmed Amrani, Vichken Abajian, Yves Kodratoff, 
and Oriane Matte-Tailliez. 2008. A Chain of Text-
mining to Extract Information in Archaeology. In 
Proceedings of Information and Communication 
Technologies: From Theory to Applications, ICT-
TA 2008, 1-5. 
Razva Bunescu and Marius Pa?ca. Using Encycloped-
ic Knowledge for Named Entity Disambiguation. 
In Proceedings of EACL2006 
Kate Byrne. Nested Named Entity Recognition in 
Historical Archive Text. In Proceedings of Interna-
tional Conference on Semantic Computing, 2007. 
Evgeniy Gabrilovich and Shaul Markovitch. Over-
coming the Brittleness Bottleneck using Wikipe-
dia: Enhancing Text Categorization with Encyclo-
pedic Knowledge. In Proceedings of the Twenty-
First National Conference on Artificial Intelli-
gence, 1301-1306, Boston, 2006. 
Jim Giles. Internet Encyclopedias Go Head to Head. 
In Nature 438. 2005. 900-901.  
Mark Greengras, Sam Chapman, Jamie McLaughlin, 
Ravish Bhagdev and Fabio Ciravegna. Finding 
Needles in Haystacks: Data-mining in Distributed 
Historical Datasets. In The Virtual Representation 
of the Past. London, Ashgate. 2008 
                                                          
12 http://ads.ahds.ac.uk/project/archaeotools/ 
8
George Hripcsak and Adam S. Rothschild. Agree-
ment, the F-measure and Reliability in Information 
Retrieval: In Journal of the American Medical In-
formatics Association, 296-298. 2005 
Todd Holloway, Miran Bozicevic and Katy B?rner. 
Analyzing and Visualizing the Semantic Coverage 
of Wikipedia and its Authors. In Complexity, Vo-
lumn 12, issue 3, 30-40. 2007 
Stuart Jeffrey, Julian Richards, Fabio Ciravegna, Ste-
wart Waller, Sam Chapman and Ziqi Zhang. 2009. 
The Archaeotools project: Faceted Classification 
and Natural Language Processing in an Archaeo-
logical Context. To appear in special Theme Issues 
of the Philosophical Transactions of the Royal So-
ciety A,"Crossing Boundaries: Computational 
Science, E-Science and Global E-Infrastructures". 
Jun?ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing Gazetteers for Named Entity Recognition by 
Large-scale Clustering of Dependency Relations. 
In Proceedings of ACL-2008: HLT, 407-415.  
Jun?ichi Kazama and Kentaro Torisawa. Exploting 
Wikipedia as External Knowledge for Named Enti-
ty Recognition. In Proceedings of EMNLP-2007 
and Computational Natural Language Learning 
2007. 698-707. 
Zornista Kozareva. 2006. Bootstrapping Named Enti-
ty Recognition with Automatically Generated Ga-
zetteer Lists. In EACL-2006-SRW.  
Bernardo Magnini, Matto Negri, Roberto Prevete and 
Hristo Tanev. AWordNet-Based Approach to 
Named Entity Recognition. In Proceedings of 
COLING-2002 on SEMANET: building and using 
semantic networks. 1-7 
Diana Maynard, Kalina Bontcheva and Hamish Cun-
ningham. Automatic Language-Independent Induc-
tion of Gazetteer Lists. In Proceedings of 
LREC2004. 
Tara Murphy, Tara Mcintosh and James R Curran. 
Named Entity Recognition for Astronomy Litera-
ture. In Proceedings of the Australasian Language 
Technology Workshop, 2006. 
Chikashi Nobata, Nigel Collier and Jun?ichi Tsujii. 
Comparison between Tagged Corpora for the 
Named Entity Task. In Proceedings of the Work-
shop on Comparing Corpora at ACL2000. 
Ellen Riloff and Rosie Jones. 1999. Learning Dictio-
naries for Information Extraction by Multi-level 
Bootstrapping. In Proceedings of the Sixteenth Na-
tional Conference on Artificial Intelligence, 474-
479.  
Angus Roberts, Robert Gaizauskas, Mark Hepple and 
Yikun Guo. Combining Terminology Resources 
and Statistical Methods for Entity Recognition: an 
Evaluation. In Proceedings of LREC2008. 
Hinrich Sch?tze and Jan O. Pedersen. A co-
occurrence-based thesaurus and two applications to 
Information Retrieval. In Information Processing 
and Management: an International Journal, 1997. 
33(3): 307-318 
Michael Strube and Simone Paolo Ponzetto. WikiRe-
late! Computing Semantic Relatedness Using Wi-
kipedia. In Proceedings of the 21st National Confe-
rence on Artificial Intelligence, 2006. 1419 - 1424 
Partha Pratim Talukdar, Thorsten Brants, Mark Li-
berman and Fernando Pereira. 2006. A Context 
Pattern Induction Method for Named Entity Ex-
traction. In Proceedings of CoNLL-2006, 141-148. 
Antonio Toral and Rafael Mu?oz. 2006. A Proposal 
to Automatically Build and Maintain Gazetteers for 
Named Entity Recognition by using Wikipedia. In 
Proceedings of Workshop on New Text, 11th Con-
ference of the European Chapter of the Association 
for Computational Linguistics 2006. 
Andreas Vlachos. Evaluating and Combining Bio-
medical Named Entity Recognition Systems. In 
Workshop: Biological translational and clinical 
language processing. 2007 
9
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 991?1002,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Harnessing different knowledge sources to measure semantic relatedness 
under a uniform model 
 
Ziqi Zhang Anna Lisa Gentile Fabio Ciravegna 
Department of Computer Science, University of Sheffield 
211 Portobello, Regent Court 
Sheffield, S1 4DP 
z.zhang@dcs.shef.ac.
uk 
a.l.gentile@dcs.shef
.ac.uk 
f.ciravegna@dcs.shef
.ac.uk 
 
Abstract 
Measuring semantic relatedness between 
words or concepts is a crucial process to 
many Natural Language Processing tasks. 
Exiting methods exploit semantic evidence 
from a single knowledge source, and are 
predominantly evaluated only in the 
general domain. This paper introduces a 
method of harnessing different knowledge 
sources under a uniform model for 
measuring semantic relatedness between 
words or concepts. Using Wikipedia and 
WordNet as examples, and evaluated in 
both the general and biomedical domains, it 
successfully combines strengths from both 
knowledge sources and outperforms state-
of-the-art on many datasets. 
1    Introduction 
Semantic relatedness (SR) measures how much 
two (strings of) words or concepts are related by 
encompassing all kinds of relations between them 
(Strube and Ponzetto, 2006). It is more general 
than semantic similarity. SR is often an important 
pre-processing step to many complex Natural 
Language Processing (NLP) tasks, such as Word 
Sense Disambiguation (Leacock and Chodorow, 
1998; Han and Zhao, 2010), and information 
retrieval (Finkelstein et al, 2002). In the 
biomedical domain, SR is an important technique 
for discovering gene functions and interactions 
(Wu et al, 2005; Ye et al, 2005).  
There is an abundant literature on measuring 
SR between words or concepts. Typically, these 
methods extract semantic evidence of words and 
concepts from a background knowledge source, 
with which their relatedness is assessed. The 
knowledge sources can be unstructured documents 
or (semi-)structured resources such as Wikipedia, 
WordNet, and domain specific ontologies (e.g., the 
Gene Ontology1).  
In this paper, we identify two issues that have 
not been addressed in the previous works. First, 
existing works typically employ a single 
knowledge source of semantic evidence. Research 
(Strube and Ponzetto, 2006; Zesch and Gurevych, 
2010; Zhang et al, 2010) has shown that the 
accuracy of an SR method differs depending on the 
choice of the knowledge sources, and there is no 
conclusion which knowledge source is superior to 
others. Zhang et al (2010) argue that this indicates 
different knowledge sources may complement each 
other. Second, the majority of SR methods have 
been evaluated in general domains only, except a 
few earlier WordNet-based methods that have been 
adapted to biomedical ontologies and evaluated in 
that domain (Lord et al, 2003; Pedersen et al, 
2006; Pozo et al, 2008). Given the significant 
attention that SR has received in specific domains 
(Pesquita et al, 2007), evaluation of SR methods 
in specific domains is increasingly important.  
This paper addresses these issues by proposing 
a generic and uniform model for computing SR 
between words or concepts using multiple 
knowledge sources, and evaluating the proposed 
method in both general and specific domains. The 
method combines and integrates semantic evidence 
of words or concepts extracted from any 
knowledge source in a generic graph 
representation, with which the SR between 
concepts or words is computed. Using two of the 
most popular general-domain knowledge sources, 
                                                         
1 http://www.geneontology.org/, last retrieved in Mar. 2011 
991
Wikipedia and WordNet as examples, the method 
is evaluated on 7 benchmarking datasets, including 
three datasets from the biomedical domain and 
four from the general domain. It has achieved 
excellent results: compared to the baselines that 
use each single knowledge sources, combining 
both knowledge sources has improved the accuracy 
on all datasets by 2~11%; compared to state-of-
the-art on the general domain datasets, the method 
achieves the best results on three datasets; and on 
the other three biomedical datasets, it obtains the 
best result in one case; and second and third best 
results on the other two among eight participating 
methods, where all other competitors exploit some 
domain-specific knowledge sources.  
The remainder of this paper is organized as 
follows. Section 2 discusses related work; Section 
3 presents the proposed method; Section 4 
describes the experiments and evaluation; Section 
5 discusses results and findings; Section 6 
concludes this paper. 
2    Related work 
2.1    SR methods 
Methods for computing SR can be classified into 
path based, Information Content (IC) based, 
statistical and hybrid methods. Path based 
methods (Hirst and St-Onge, 1998; Leacock and 
Chodorow, 1998; Pekar and Staab, 2002; Rada et 
al., 1989; Wu and Palmer, 1994) measure SR 
between words or concepts as a function of their 
distance in a semantic network, usually calculated 
based on the path connecting the words or concepts 
by certain semantic (typically is-a) links. IC based 
methods (Jiang and Conrath, 1997; Lin, 1998; 
Pirro et al, 2009; Resnik, 1995; Seco et al, 2004) 
assess relatedness between words or concepts by 
the amount of information they share, usually 
determined by a higher level concept that 
subsumes both concepts in a taxonomic structure. 
Statistical methods measure relatedness between 
words or concepts based on their distribution of 
contextual evidence. This can be formalized as co-
occurrence statistics collected from unstructured 
documents (Chen et al, 2006; Cilibrasi and 
Vitanyi, 2007; Matsuo et al, 2006), or 
distributional concept or word vectors with 
features extracted from either unstructured 
documents (Harrington, 2010; Wojtinnek and 
Pulman, 2011) or (semi-)structured knowledge 
resources (Agirre et al, 2009; Gabrilovich and 
Markovitch, 2007; Gouws et al, 2010; Zesch and 
Gurevych, 2007; Zhang et al, 2010). Hybrid 
methods combine different purebred methods in 
certain ways. For example Riensche et al (2007) 
employ both an IC based method (Resnik, 1995) 
and a statistical method (cosine vector similarity) 
in their study. Pozo et al (2008) derive a taxonomy 
of terms from unstructured documents by applying 
hierarchical clustering based on corpus statistics, 
then apply path based method on this taxonomy to 
compute SR. Han and Zhao (2010) use one IC 
based method and two statistical methods to 
compute SR, then derive an aggregated score.  
2.2    SR knowledge sources and domains 
Computing SR requires background knowledge 
about concepts or words, which can be extracted 
from unstructured corpora, semi-structured and 
structured knowledge resources. Unstructured 
corpora are easier to create and cheaper to 
maintain, however, semantic relations between 
words or concepts are implicit. Methods (Chen et 
al., 2006; Cilibrasi and Vitanyi 2007; Matsuo et al, 
2006) that exploit unstructured corpora typically 
depend on distributional statistics, and thus may 
ignore important semantic evidences present in 
(semi-)structured knowledge sources (Pan and 
Farrell, 2007). Recent studies (Harrington, 2010; 
Pozo et al, 2008; Wojtinnek and Pulman, 2011) 
propose to pre-process a corpus to learn a semantic 
network, with which SR is computed. This creates 
high pre-processing cost; also, the choice of corpus 
and its size often have a direct correlation with the 
accuracy of SR methods (Batet et al, 2010). 
(Semi-)Structured knowledge sources on the 
other hand, organize semantic knowledge about 
concepts and words explicitly and interlink them 
with semantic relations. They have been popular 
choices in the studies of SR, and they include 
lexical resources such as WordNet, Wiktionary, 
and (semi-)structured encyclopedic resources such 
as Wikipedia. WordNet has been used in earlier 
studies (Hirst and St-Onge, 1998; Jiang and 
Conrath, 1997; Lin, 1998; Leacock and Chodorow 
1998; Resnik, 1995; Seco et al, 2004; Wu and 
Palmer, 1994) and is still a preferred knowledge 
source in recent works (Agirre et al, 2009). 
However, its effectiveness may be hindered by its 
lack of coverage of specialized lexicons and 
domain specific concepts (Strube and Ponzetto, 
992
2006; Zhang et al, 2010). Wikipedia and 
Wiktionary are collaboratively maintained know-
ledge sources and therefore may overcome this 
limitation. Wikipedia in particular, is found to have 
reasonable coverage of many domains (Holloway 
et al, 2007; Halavais, 2008). It has become 
increasingly popular in SR studies recently. 
However, research (Zesch and Gurevych, 2010) 
have shown that methods based on Wikipedia have 
no clear advantage over WordNet-based methods 
on some general domain datasets in terms of 
accuracy, while Zhang et al (2010) argue that 
different knowledge sources may complement each 
other, and SR methods may benefit from 
harnessing different knowledge sources.  
Several studies (Lord et al, 2003; Pedersen et 
al., 2006; Petrakis et al, 2006; Pozo et al, 2008) 
have adapted state-of-the-art to domain specific 
knowledge sources (e.g., the Gene Ontology, the 
MeSH2) and evaluated them therein. Despite these 
efforts, a large proportion of state-of-the-art is still 
only evaluated in the general domain.  
2.3    SR methods similar to this work 
Few works have attempted at combining different 
knowledge sources in SR studies, especially (semi-
)structured knowledge sources. The closest studies 
are Han and Zhao (2010) and Tsang and Stevenson 
(2010). Han and Zhao firstly compute SR between 
words using three state-of-the-art SR methods 
separately. Next, one score is chosen subject to an 
arbitrary preference order, and used to create a 
connected graph of weighted edges between 
words. A recursive function is then applied to the 
graph to compute final SR scores between words. 
Essentially, each SR method is applied in isolation 
and features from different sources are used 
separately with each distinctive method. Although 
this retains advantages of each method, the 
limitations of them are also combined.  
Tsang and Stevenson (2010) combine WordNet 
and unstructured documents by weighing each 
word found in WordNet using its frequency 
observed in a large corpus. The frequencies 
however, are sensitive to the choice of corpus, thus 
different corpora may result in different accuracies. 
Furthermore, their method is only applicable to 
computing SR between pairs of sets of words or 
concepts.  
                                                         
2 http://www.nlm.nih.gov/mesh/ last retrieved in March 2011 
3    Methodology  
We define a set of requirements for SR methods 
that harness different knowledge sources: 
? It should improve over the same method 
based on a single knowledge source 
? It should be generic and applicable to any 
knowledge source 
? It should be robust in dealing with 
knowledge source specific features but 
also tolerate the quality and coverage 
issues of individual knowledge source 
Our method of harnessing different knowledge 
sources contains four steps. Firstly (Section 3.1), 
each word or word segment is searched in each 
knowledge source to identify their contexts that is 
specific to that knowledge source. We define a 
context as the representation of meaning or a 
concept for a word. In the following, we say that 
each context is associated with a distinct concept. 
Secondly (Section 3.2), for each concept of an 
input word, features are extracted from its context 
and a graph representation of each concept and 
their features is created. Thirdly (Section 3.3), 
cross-source contexts are mapped where they refer 
to the same concept, thus their features from 
different sources can be combined to derive an 
enriched representation. This creates a final, 
uniform graph representation where input words 
are connected by shared features of their 
underlying candidate concepts. Then (Section 3.4) 
the graph is submitted to a generic algorithm to 
compute SR between words. 
In the following, we discuss details with respect 
to different types of knowledge sources, while 
focusing on Wikipedia and WordNet in our 
experiments for two reasons. First, they are used 
by the majority of SR methods and are therefore 
most representative knowledge sources. Second, 
they have strongly distinctive and complementary 
characteristics, which make ideal testbeds for the 
requirements. On one hand, WordNet is a lexical 
resource containing rich and strict semantic 
relations between words, but lacks coverage of 
specialized vocabularies. On the other hand, 
Wikipedia is a semi-structured resource with good 
coverage of domains and named entities, but the 
semantic knowledge is organized in a looser way. 
993
3.1    Context retrieval 
Given a pair of words or word segments, we firstly 
identify contexts representing the underlying 
meanings or concepts from each knowledge 
source. For lexical resources, this could be 
distinctive word senses. In WordNet (WN), a 
context corresponds to a single synset, which 
corresponds to a concept. We search each word in 
WordNet and extract all possible synsets. Let w be 
a word or word segment (e.g., ?cat?), and   
   
    
      
      
    be the set of k concepts of w 
extracted from WordNet.  
Using Wikipedia (WK) as an example semi-
structured resource, the context can be an article 
that describes a unique concept. Thus we search 
for underlying articles that describe different 
concepts. Firstly, we search w in Wikipedia, where 
three situations may be anticipated. If a single non-
disambiguation page describing a concept is 
returned, the concept is selected and the retrieval is 
complete. In the second case, a disambiguation 
page linking to all possible concept pages may be 
returned. This page lists all underlying concepts 
and entities referenced by w as links and a short 
description with each link. In this case, we always 
keep the first concept page, which is found often to 
be the most common sense of the word; 
additionally, we select other concept pages whose 
short descriptions contain the word w. We do not 
select all linked pages because many of these in 
fact link to a concept relevant to w, but not 
necessarily a candidate sense of w. Thirdly, if no 
pages are returned for w, we search for the most 
relevant page using w as keyword(s) in an inverted 
index of all Wikipedia pages (e.g., via search 
engines). We denote concepts retrieved from 
Wikipedia as   
       
      
      
   .  
For unstructured sources such as documents, a 
simple approach could be defining a word context 
as a text passage around each occurrence of w, and 
grouping similar contexts of w as representation of 
its underlying meanings, or concepts. Alternatively, 
more complex approaches such as Pozo et al 
(2008) and Harrington (2010) may be applied to 
extract a lexical network of words, whereby similar 
methods to WordNet can be applied. 
3.2 Feature extraction and representation 
Next, for each concept identified from a 
knowledge source, features are extracted from their 
corresponding contexts. In our case, for each 
    
  , we follow the work by Zhang et al 
(2010) to extract four types of features from their 
corresponding Wikipedia pages. Figure 1 shows an 
example representation of a concept and its 
Wikipedia features: 
? Words from page titles and redirection 
links (can be considered as synonyms) 
? Words from categories, used as higher 
level hypernyms in some studies (Zesch et 
al., 2010; Strube and Ponzetto, 2006) 
? Words from outgoing links 
? Top n most frequent words from a page 
 
Figure 1. Representation of the concept ?cat, the 
mammal? using different types of features 
extracted from Wikipedia. The shaded circle 
represents the concept; ovals represent feature 
values; edges connecting feature values to the 
concept and <labels> represent feature types 
 
For each     
  , we extract ten features from 
WordNet: hypernyms, hyponyms, meronyms, 
holonyms, synonyms, antonyms, attributes, ?see 
also? words, ?related? words, and gloss. These are 
also represented in the same way as in Figure 1.  
With unstructured sources, contextual words 
can be used as features. Alternatively, if a lexical 
network is extracted, features may be extracted in a 
similar way to those of WordNet. 
 
Additionally, with WordNet and Wikipedia, we 
also propose several intra-resource feature merging 
strategies to study the effect of feature 
diversification. This is because, while some 
approaches (such as Agirre et al, 2009; 
Harrington, 2010; Yeh et al, 2009) do not 
distinguish different feature types in graph 
construction, or adopt a bag-of-words feature 
representation (such as Zesch and Gurevych, 
2010), others (such as Yazdani and Popescu-Belis, 
2010; Zhang et al, 2010) have used differentiated 
994
feature types and weights in their model. We 
therefore carry out studies to investigate this issue. 
Specifically, for the original four Wikipedia 
features, we create a bag-of-words feature that 
simply merges all feature types (i.e., all edges in 
Figure 1 will have the same label). For the original 
ten WordNet features, we propose two merged 
representations corresponding to that of Wikipedia, 
so as to support the studies of feature enrichment 
in the following section. We introduce a bag-of-
words feature that collapses all different feature 
types, and a four-feature representation as follow: 
? wn-synant merges WordNet synonyms and 
antonyms.  
? wn-hypoer merges WordNet hypernyms 
and hyponyms, collectively representing 
features by ?is-a? semantic relation 
? wn-assc merges WordNet meronyms, 
holonyms, related and ?see also?, which 
are features corresponding to associative 
relations  
? wn-dist merges WordNet gloss and 
attributes that generally describe a concept.  
3.3 Concept mapping and feature enrichment 
Our method essentially harnesses different 
knowledge sources by combining features 
extracted from different sources in a uniform 
model. This requires two sub-processes: cross-
source concept mapping and cross-source 
feature enrichment.  
In cross-source concept mapping, concepts 
extracted from different knowledge sources are 
mapped according to similar meanings such that 
cross-source features can be combined. To do so, 
we select the concepts from one knowledge source 
as the reference concept set; then concepts from 
other knowledge sources are mapped to reference 
concepts of similar meanings. There can be 
different criteria of choosing reference knowledge 
source concepts. Empirically, we found it 
necessary to choose the knowledge source with 
broader coverage and richer features. This will be 
discussed later in Section 5. Following this 
strategy, in our example,   
   is chosen as 
reference concepts, and for each   
     
  we 
select a   
     
   such that   
   and   
   refer to 
the same meaning. To do so, we apply a simple 
maximum set overlap metric to their feature 
values. Let F(c) be a function that returns all 
feature values of c as bag-of-words, then for each 
  
     
  , it is mapped to a   
   such that 
     
          
     is maximized among all 
  
     
  . The resulting concept candidates are 
denoted as   
    
, where   
    
=    
     
    is a 
mapped set of concepts potentially referring to the 
same meaning. If   
     then   
    
 
  
   
        
  . 
Next, cross-source feature enrichment creates 
a uniform feature representation for each mapped 
sets of concepts. The process can be considered as 
enriching the features from one knowledge source 
with others. The most straightforward approach is 
to simply collect features extracted from each 
knowledge source on to a single graph, retaining 
the diversity in feature types. For example, Figure 
2 shows a graph representation based on the 
collection of the four Wikipedia features and the 
four derived WordNet features. We refer to this 
approach as ?feature combination?.  
 
Figure 2. Representation of ?cat, the mammal? 
after concept mapping and feature combination 
 
On the other hand, cross-source features may be 
merged according to their semantics.  For example, 
WordNet and Wikipedia contain features based on 
synonyms of concepts; while Wikipedia and 
unstructured documents contain word distribution-
al features. Thus we define ?feature integration? 
as merging feature types from different knowledge 
sources into single types of features based on their 
similarity in semantics.  With WordNet and Wiki-
pedia, we integrate features as below (Figure 3): 
? merged-synant merges Wikipedia page 
titles and redirection links with wn-synant 
? merged-hypoer merges merges Wikipedia 
categories with wn-hypoer 
995
? merged-assc merges Wikipedia links with 
wn-assc. We consider Wikipedia links bear 
other associative relations and are 
therefore merged with features extracted 
by other WordNet relations 
? merged-dist merges Wikipedia frequent n 
words with wn-dist.  
 
Figure 3. Representation of ?cat, the mammal? 
after concept mapping and feature integration 
 
Note that the difference between cross-source 
feature combination and integration is that the 
former introduces more types of features, whereas 
the latter retains same number of feature types but 
increases feature values for each type. Both have 
the effect of establishing additional path (via 
features) between concepts, but in different ways. 
 
With intra-resource feature diversification, cross-
source feature combination and feature 
integration, we create a total of nine intra- and 
cross-source feature representations to be tested 
with the uniform random walk model: 
? four types of Wikipedia features (wk-4F) 
? one type of Wikipedia features (wk-1F) 
? ten types of WordNet features (wn-10F) 
? four types of WordNet features (wn-4F) 
? one type of WordNet features (wn-1F) 
? wk-4F combines wn-4F: wk-4F+wn4F,C 
? wk-4F integrates wn-4F: wk-4F+wn4F,I 
? wk-1F combines wn-1F: wk-1F+wn1F,C 
? wk-1F integrates wn-1F: wk-1F+wn1F,I 
3.4 Computing SR using the graph 
The algorithm for computing SR using the graph is 
based on the idea of random walk. It formalizes the 
idea that taking successive steps along the paths in 
a graph, the ?easier? it is to arrive at a target node 
starting from a source node, the more related the 
two nodes are. Following the previous steps, the 
feature representations of all candidate concepts 
relevant to the input word pairs are joined, which 
creates a single undirected, weighted, bi-partite 
graph. Let G = (V, E) be the graph, where V is the 
set of nodes (concepts and feature values); E is the 
set of edges (feature types) that connect concepts 
and features. As shown in Figure 4, different 
concepts are connected if they share same values 
of same types of features, namely, there exists a 
path that connects one concept to another.  
 
Figure 4. Paths are established between different 
concepts if they share values of same feature types 
<bold underlined> 
Using Figure 4 it is easier to comprehend the 
difference between feature combination and 
integration. Since concept nodes can only be 
connected by same types of edges (feature types), 
feature combination increases the chances of 
connectivity by adding in more types of edges, 
while integration merges similar types of edges 
across knowledge sources and increases the 
number of feature nodes connected by each type.  
From the graph, we start by building an 
adjacency matrix W of initial probability 
distribution: 
??
??
?
??
??
? ?????? ? ?
otherwise
EjililEi
lw
W Ll k
k
ij
k
,0
),(,|),(:),(|
)( [1] 
Where Wij is the i
th-line and jth-column entry of W, 
indexed by V; l(i, j) is a function that returns the 
type of edge (i.e., type of feature) connecting 
nodes i and j; L is the set of all possible types; w(l) 
returns the weight for that type. Essentially, L is 
the collection of all feature types, and w(l) assigns 
996
a weight to a particular feature type. Next, we 
compute the transition probability matrix P(t)(j|i) = 
[(D?1W)t]ij (Dii = ?kWik), which returns the 
probability of reaching other nodes from a starting 
node on the graph after t steps. In this method, we 
follow the work by Rowe and Ciravegna (2010) to 
set t=2 in order to preserve locally connected 
nodes. Next, we extract the probability vectors 
corresponding to concept nodes from P, and 
compute pair-wise relatedness using the cosine 
function. Effectively, this formalizes the notion 
that two concepts related to a third concept is also 
semantically related, which is similar to the 
hypothesis proposed by Patwardhan and Pedersen 
(2006) in their method based on second-order 
context vectors. The final SR between the input 
word pair is the maximum pair-wise concept SR. 
4    Experiment and evaluation 
We evaluate the method based on correlation 
against human judgment (gold standard) on seven 
benchmarking datasets covering both general and 
technical domains. These include four general 
domain datasets: the Rubenstein and Goodenough 
(1965) dataset containing 65 pairs of nouns 
(RG65); the Miller and Charles (1991) dataset that 
is a subset of the RG-65 dataset and contains 30 
pairs (MC30); the Finkelstein et al (2002) dataset 
with 353 pairs of words, including nouns, verbs, 
adjectives, as well as named entities. This contains 
two subsets, a set of 153 pairs (Fin153) and a set of 
200 (Fin200) pairs each annotated by a different 
groups of annotators. Zesch and Gurevych (2010) 
show largely varying Inter-Annotator-Agreement 
(IAA) between the two sets (Table 1), and argue 
that they should be treated as separate datasets. 
Three biomedical datasets are selected to evaluate 
domain-specific performance of the proposed 
method. These include a set of 36 MeSH term pairs 
in Petrakis et al (2006) (MeSH36), 30 pairs of 
medical terms annotated by a group of physicians 
as in Pedersen et al (2006) (Ped30-p) and the same 
set annotated by a different group of medical 
coders (Ped30-c). Table 1 shows statistics of the 
seven datasets.  
The correlation is computed using the 
Spearman rank order coefficient for two reasons. 
First, it is a better metric than other alternatives 
(Zesch and Gurevych, 2010). Second, it is 
consistent with the majority of studies such that 
results can be compared.  
 
Dataset Size Domain IAA 
MC30 30 General 0.9 
RG65 65 General 0.8 
Fin153 153 General 0.73 
Fin200 200 General 0.55 
Ped30-p 30 Biomedical 0.68 
Ped30-c 30 Biomedical 0.78 
MeSH36 36 Biomedical - 
Table 1: Information of benchmarking datasets 
 
We distribute feature weights w(l) across 
different feature types L evenly in each feature 
representation. Although Zhang et al (2010) show 
that discriminated feature weights leads to 
improved accuracy; this is not the focus of this 
study. Since we aim to investigate the effects of 
harnessing different knowledge sources, we 
obtained baseline performances by applying the 
method to those feature representations based on 
single knowledge sources (i.e., wk-4F, wk-1F, wn-
10F, wn-4F, wn-1F). Tables 2 and 3 show the best 
results obtained with baselines and corresponding 
knowledge sources and feature representation.  
 
Dataset Corr. Feature Coverage (% pairs) 
MC30 0.77 wn-1F 77% 
RG65 0.71 wn-1F 65% 
Fin153 0.45 wn-4F 82% 
Fin200 0.35 wn-4F 76% 
Ped30-p 0.66 wn-4F 33% 
Ped30-c 0.8 wn-4F 33% 
MeSH36 0.49 wn-1F 50% 
Table 2: Correlation obtained using WordNet.  
Many word pairs are not covered due to sparse 
feature space and lack of coverage. Only covered 
pairs are accounted. 
 
Dataset Corr. Feature 
MC30 0.74 wk-1F 
RG65 0.67 wk-1F 
Fin153 0.7 wk-1F 
Fin200 0.51 wk-4F 
Ped30-p 0.53 wk-4F 
Ped30-c 0.58 wk-4F 
MeSH36 0.73 wk-4F 
Table 3: Correlation obtained using only 
Wikipedia. All word pairs are 100% covered. 
 
997
Tables 4 ? 6 show results obtained with 
enriched feature representation. 
 
 Combination (C) Integration (I) 
Dataset wn-4F + 
wk-4F 
wn-1F + 
wk-1F 
wn-4F + 
wk-4F 
wn-1F 
+ wk-1F 
MC30 0.77 0.8 0.8 0.79 
RG65 0.74 0.73 0.73 0.729 
Fin153 0.73 0.75 0.74 0.73 
Fin200 0.52 0.54 0.53 0.54 
Ped30-p 0.63 0.52 0.64 0.47 
Ped30-c 0.64 0.52 0.67 0.49 
MeSH36 0.7 0.694 0.75 0.7 
Table 4: Correlation obtained using both 
knowledge sources. Word pairs are 100% covered. 
 
 KS and # of feature types 
 WN WK WK+WN,C WK+WN, I  
MC30 1 1 1 4 
RG65 1 1 4 4 
Fin153 4 1 1 4 
Fin200 4 4 1 1 
Ped30-p 4 4 4 4 
Ped30-c 4 4 4 4 
MeSH36 1 4 4 4 
Table 5: Number of feature types with which best 
results are obtained on each dataset. KS: 
Knowledge Source 
 
 Single KS Multiple KS Impr. 
Dataset Best corr. Best corr. Strategy  
MC30 0.74 0.8 C/I 0.06 
RG65 0.67 0.74 C 0.07 
Fin153 0.7 0.75 C 0.05 
Fin200 0.51 0.54 C/I 0.03 
Ped30-p 0.53 0.64 I 0.11 
Ped30-c 0.58 0.67 I 0.09 
MeSH36 0.73 0.75 I 0.02 
Table 6: Improvement achieved by harnessing 
multiple KSs. Best correlation with single KS is 
based on Wikipedia, which provides 100% 
coverage of word pairs. 
 
 
Tables 7 and 8 compare our method against state-
of-the-art. For Table 8, figures for other state-of-
the-art systems can be found in corresponding 
publications; while we only list the best 
performing systems for comparison. 
 
 
 
 
 
 MC30 RG65 Fin153 Fin200 KS 
best of 
WN+WK  
0.8 0.74 0.75 0.54 Both 
Rad89* 0.75 0.79 0.33 0.24 WN 
LC98* 0.75 0.79 0.33 0.24 WN 
WP94* 0.77 0.78 0.38 0.24 WN 
HS98* 0.76 0.79 0.33 0.32 WN 
Res95* 0.72 0.74 0.35 0.26 WN 
JC97* 0.68 0.58 0.28 0.10 WN 
Lin98* 0.67 0.60 0.27 0.17 WN 
Zes07* 0.77 0.82 0.6 0.51 WK 
GM07* 0.67 0.75 0.69 0.51 WK 
Zha10 0.71 0.76 0.71 0.46 WK 
Table 73: Comparison against state-of-the-art in the 
general domain. (* figures from Zesch and 
Gurevych, 2010) 
 
 Ped30-p Ped30-c MeSH36 KS 
best of 
WN+WK 
0.64 0.67 0.75 WN+
WK 
Pet06 best - - 0.74 MeSH 
Ped06 best 0.84 0.75 - GO, D 
Ped06 second 0.62 0.68 - GO, D 
Table 84: Comparison against state-of-the-art in the 
biomedical domain. GO ? Gene Ontology; D ? 
document sets.  
 
Given the fact that some datasets (i.e., MC30, 
Ped30-p, Ped30-c, MeSH36) have a relatively low 
sample size, we cannot always be sure that 
correlation values are accurate or occurred by 
chance. Therefore, we measure the statistical 
significance of correlation by computing the p-
value for the correlation values reported for our 
system in Tables 7 and 8. For all cases, a p-value 
of less than 0.001 is obtained, which indicates that 
correlation values are statistically significant. 
                                                         
3 Rada (1989) (Rad89); Leacock and Chodorow (1998) 
(LC98); Wu and Palmer (1994) (WP04); Hirst and St-Onge 
(1998) (HS98); Resnik (1995) (Res95); Jiang and Conrath 
(1997) (JC97); Lin (1998) (Lin98); Zesch and Gurevych 
(2007) (ZG07); Gabrilovich and Markovitch (2007) (GM07); 
Zhang et al (2010) (Zha10) 
4 Petrakis et al (2006) (Pet06); Pedersen et al (2006) (Ped06). 
Original participating systems can be found in these works. 
998
5    Discussion  
Single v.s. multiple knowledge sources As shown 
in Table 6, considering the best performances 
across all feature enrichment strategies and feature 
sets, the proposed method successfully harnessed 
different knowledge sources and improved over the 
baselines using single knowledge sources by 0.02 
~ 0.11. The biggest improvement (0.11) is on a 
domain-specific dataset, on which the method 
based on single knowledge source performed 
poorly in terms of coverage and accuracy. The best 
enrichment strategy that has consistently improved 
the baselines is wk-4F+wn-4F, Integration (Table 
4 v.s. Table 3).  With features enriched from 
multiple knowledge sources, the method also 
consistently improved over their corresponding 
single-source features on all datasets, except 
MeSH36, on which wk-4F+wn-4F, Combination 
(Table 4) slightly reduced the accuracy obtained 
with wk-4F (Table 3) only.  
The large proportion of uncovered word pairs 
using WordNet is due to its lack of coverage of 
specialized lexicons, and sparser semantic content. 
For example, of all 115 distinctive terms in the 
Ped30 and MeSH36 datasets, 30% are not included 
in WordNet. And of all 447 distinctive words in all 
general domain datasets, only 69% have multiple 
synonyms. Features such as attributes and ?see 
also? are present for less than 20 words. This is the 
reason that some approaches using WordNet (e.g., 
Agirre et al, 2009) require a graph of all WordNet 
lexicons to be built, thus intermediate words may 
?bridge? input words even if they do not connect 
directly by their features. Nevertheless, the 
improvement in accuracy and 100% coverage after 
harnessing both knowledge sources suggests that 
they complement each other well. On one hand, 
Wikipedia brings its strength in domain and 
content coverage; on the other hand, WordNet 
brings useful semantic evidences for words that are 
covered. 
Concept mapping and feature enrichment 
methods While the set overlap based method for 
cross-source concept mapping using the reference 
knowledge source concepts is simple and proved 
successful, the accuracy of mapping and its 
correlation with the accuracy of the SR method 
was not studied. This will be explored in the future. 
Also, alternative mapping methods will be 
investigated. For example, Toral and Mu?oz (2006) 
describe a different method of mapping Wikipedia 
articles to WordNet synsets; one could also adopt a 
simple disambiguation process to select the best 
candidate concept from each knowledge source 
suited for the input word pairs, whereby cross-
source concept mapping becomes straightforward. 
In terms of feature enrichment strategies, there is 
no strong indication (Table 6) of which (feature 
combination v.s. integration) is more effective, 
although the system consistently outperforms the 
baselines (Table 4 v.s. Table 3) with the wk-
4F+wn-4F, Integration strategy. 
Feature diversification v.s. unification Table 
5 suggests that in most cases, differentiating 
feature types leads to better results than merging 
them uniformly, despite the knowledge sources 
used. This is consistent with the findings by Zhang 
et al (2010). This can be understandable since 
although unifying feature types effectively 
increases possibility of sharing features, equally, 
this may also increase the proportion of noisy 
features. For example, consider the Wikipedia 
article of ?Horse? (animal), which has a category 
label ?livestock?; and the article ?Famine?, which 
has an outgoing link ?livestock? (in a sentence 
describing diseases that caused decline of livestock 
production). By differentiating the feature types 
?has_category? and ?has_outlink?, the two 
concepts will not be connected even if they both 
have the same word ?livestock? in their feature 
representation. However, using a bag-of-words 
representation where feature types are 
undistinguished, the strength of their relatedness is 
boosted by sharing this word, which may be 
uninteresting in this occasion. 
Compared against state-of-the-art, the 
proposed method has achieved promising results. 
Overall, by harnessing different knowledge sources, 
the method achieves, and in many cases, 
outperforms state-of-the-art. In the general domain, 
it outperforms state-of-the-art on three out of four 
datasets. It is worth noting that all methods based 
on WordNet generally have poor performance on 
the Fin153 and Fin200 datasets (Table 7). Despite 
the heterogeneity in these datasets, this may also 
relate to the quality of the feature space generated 
with WordNet. In fact methods using Wikipedia 
perform better on these datasets. With enriched 
features from both knowledge sources, the 
accuracies are further improved.   
999
In the biomedical domain, the proposed method 
outperforms state-of-the-art on one dataset and 
produces competitive results on others. Note that 
all other methods exploit domain-specific 
ontologies and corpora. The Ped06 best and Ped06 
second methods also depend on a corpus of one 
million documents. These results further confirmed 
the benefits of our method: harnessing knowledge 
from general-purpose knowledge sources of 
limited domain coverage, it is possible to achieve 
results that rival methods based on well-curated 
and specially tailored domain-specific knowledge 
sources. This is an encouraging finding. Although 
there are abundant resources in the biomedical 
domain for this type of tasks, such resources may 
be scarce in other domains and are expensive to 
build. However, the results suggest that the 
proposed method offers a more affordable 
approach that provides reasonable coverage and 
quality, even if individual general knowledge 
sources may be limited in themselves. 
Generality of the method. The proposed 
method represents features extracted from different 
knowledge sources in a generic manner, which 
facilitates cross-source feature enrichment and 
requires generic algorithm computation. As 
discussed in Section 3, semantic evidence of words 
and concepts may be extracted from different 
knowledge sources in different ways, while 
harnessed in the generic model. In contrast, other 
methods using multiple knowledge sources (e.g., 
Han and Zhao, 2010; Tsang and Stevenson, 2010) 
introduce algorithms that are bound to the 
knowledge sources, which may limit their 
adaptability and portability. 
6    Conclusion  
This paper introduced a generic method of 
harnessing different knowledge sources to compute 
semantic relatedness. We have shown empirically 
that different knowledge sources contain 
complementary semantic evidence, which, when 
combined together under a uniform model, can 
improve the accuracy of SR methods. Moreover, 
we have demonstrated its robustness in dealing 
with knowledge sources of different quality and 
coverage. Several remaining issues will be studied 
in the future. First, additional knowledge sources 
will be studied, particularly unstructured corpora 
and domain-specific resources. The experiments 
have shown that although harnessing different 
knowledge sources achieved encouraging results 
on biomedical datasets, they are still far from being 
perfect. While it should be appreciated that the 
results are obtained using only general purpose 
knowledge sources, it would be interesting to 
investigate whether harnessing domain specific 
knowledge sources (where available) further 
improves the performance. Second, different 
methods of concept mapping will be studied. We 
will also design methods for assessing the quality 
of mapping, and analyze their correlations with the 
SR methods. Third, analyses will be carried out to 
uncover the differences between feature 
combination and integration that have led to 
different accuracies. 
Acknowledgments 
Part of this research has been funded under the EC 
7th Framework Program, in the context of the 
SmartProducts project (231204). 
References  
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasca, 
M., Soroa, A. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-
based Approaches. In Proceedings of NAACL?09 
Batet, M., S?nchez, D., Valls, A. 2010. An ontology-
based measure to compute semantic similarity in 
biomedicine. In Journal of Biomedical Informatics, 
44(1), 118-125 
Chen, H., Lin, M., Wei, Y. 2006. Novel association 
measures using web search with double checking. 
Proceedings of COLING?06-ACL?06, pp. 1009-
1016 
Cilibrasi, R., Vitanyi, P. 2007. The Google Similarity 
Distance. In IEEE Transactions on Knowledge and 
Data Engineering. 19(3), 370-383 
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., 
Solan, Z., Wolfman, G., and Ruppin, E. (2002). 
Placing search in context: the concept revisited. In 
ACM Transactions on Information Systems, 20 (1), 
pp. 116 ? 131 
Gabrilovich, E., Markovitch, S. 2007. Computing 
semantic relatedness using Wikipedia-based explicit 
semantic analysis. In proceeding of IJCAI'07 
Gouws, S., Rooyen, G., Engelbrecht, H. 2010. 
Measuring conceptual similarity by spreading 
activation over Wikipedia?s hyperlink structure. 
Proceedings of the 2nd Workshop on The People?s 
Web Meets NLP: Collaboratively Constructed 
Semantic Resources 
1000
Halavais , A. 2008. An Analysis of Topical Coverage of 
Wikipedia. Journal of Computer-Mediated 
Communication, 13(2) 
Han, X., Zhao, J. 2010. Structural semantic relatedness: 
a knowledge-based method to named entity 
disambiguation. In the 48th Annual Meeting of the 
Association for Computational Linguistics. 
Harrington, B. 2010. A semantic network approach to 
measuring relatedness. In Proceedings of COLING? 
10 
Hirst, G., and St-Onge, D. 1998. Lexical chains as 
representation of context for the detection and 
correction malapropisms. In Christiane Fellbaum 
(ed.), WordNet: An Electronic Lexical Database and 
Some of Its Applications, pp. 305?332. Cambridge, 
MA: The MIT Press. 
Holloway, T., Bozicevic, M., B?rner, K. 2007. 
Analyzing and visualizing the semantic coverage of 
Wikipedia and its authors. In Journal of Complexity, 
Special issue on Understanding Complex Systems, 
12(3), 30-40 
Jiang, J. and D. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. 
Proceedings of the International Conference on 
Research in Computational Linguistics, pp. 19-33 
Leacock, C., Chodorow, M. 1998. Combining local 
context and WordNet similarity for word sense 
identification. In C. Fellbaum (Ed.), WordNet. An 
Electronic Lexical Database, Chp. 11, pp. 265-283. 
Lin, D. 1998. An information-theoretic definition of 
similarity. Proceedings of the Fifteenth International 
Conference on Machine Learning, pp. 296-304 
Lord, P., Stevens, R., Brass, A., Goble, C. 2003. 
Investigating semantic similarity measures across 
the Gene Ontology: the relationship between 
sequence and annotation. In Bioinformatics, 19(10), 
pp. 1275?1283 
Matsuo, Y., T. Sakaki., K., Uchiyama, M., Ishizuka. 
2006. Graph-based word clustering using a web 
search engine. In Proceedings of the Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP), pp.542-550 
Miller, G., Charles, W. 1991. Contextual correlates of 
semantic similarity. In Language and Cognitive 
Processes, 6(1): 1-28 
Pan, F., Farrell, R. 2007. Computing semantic similarity 
between skill statements for approximate matching. 
In Proceedings of NAACL-HLT?07, pp. 572-579 
Patwardhan, S., Pedersen, T. 2006. Using WordNet-
based context vectors to estimate the semantic 
relatedness of concepts. Proceedings of the EACL 
2006 Workshop on Making Sense of Sense: 
Bringing Computational Linguistics and 
Psycholinguistics Together 
Pedersen, T., Pakhomov, S., Patwardhan, S., Chute, C. 
2006. Measures of semantic similarity and 
relatedness in the biomedical domain. Journal of 
Biomedical Informatics 40(3), 288-299 
Pekar, V., Staab, S. 2002. Taxonomy learning: factoring 
the structure of a taxonomy into a semantic 
classification decision. Proceedings of COLING?02. 
pp. 786-792 
Pesquita, C., Faria, D., Bastos, H., Falc?o, A., Couto, F. 
(2007). Evaluating GO-based Semantic Similarity 
Measures. ISMB/ECCB 2007 SIG Meeting Program 
Materials, International Society for Computational 
Biology 2007 
Petrakis, E., Varelas, G., Hliaoutakis, A., Raftopoulou, 
P. 2006. Design and evaluation of semantic 
similarity measures for concepts stemming from the 
same or different ontologies. In 4th Workshop on 
Multimedia Semantics (WMS'06), pp. 44-52. 
Pirro, G. 2009. A semantic similarity metric combining 
features and intrinsic information content. In Data 
and Knowledge Engineering, 68(11), pp. 1289-1308 
Pozo A., Pazos F., Valencia, A. 2008. Defining 
functional distances over gene ontology. In BMC 
Bioinformatics 9, pp.50 
Rada, R., Mili, H., Bicknell, E., Blettner, M. 1989. 
Development and application of a metric on 
semantic nets. In IEEE Transactions on Systems, 
Man and Cybernetics 19(1), pp.17-30 
Resnik, P. (1995). Using information content to evaluate 
semantic similarity in a taxonomy. In Proceedings of 
IJCAI-95, pp. 448-453 
Riensche, R., Baddeley, B., Sanfilippo, A., Posse, C., 
Gopalan, B. 2007. XOA: Web-Enabled Cross-
Ontological Analytics. IEEE Congress on Services, 
pp. 99-105 
Rowe, M., Ciravegna, F. 2010. Disambiguating identity 
web references using Web 2.0 data and semantics. 
M Rowe and F Ciravegna. The Journal of Web 
Semantics. 
Rubenstein, H., Goodenough, J. 1965. Contextual 
correlates of synonymy. In Communications of the 
ACM, 8(10):627-633 
Seco, N., and Hayes, T. 2004. An intrinsic information 
content metric for semantic similarity in WordNet. 
In Proceedings of the 16th European conference on 
Artificial Intelligence 
Strube, M., Ponzetto, S. 2006. WikiRelate! Computing 
semantic relatedness using Wikipedia. In 
Proceedings of the 21st national conference on 
Artificial intelligence (AAAI) 
Toral, A., Mu?oz, R. 2006. A Proposal to Automatically 
Build and Maintain Gazetteers for Named Entity 
Recognition by using Wikipedia. In Proceedings of 
Workshop on New Text, ACL?06. 
Tsang, V., Stevenson, S. 2010. A graph-theoretic 
framework for semantic distance. In Journal of 
Computational Linguistics, 36(1). 
1001
Wojtinnek, P., Pulman, S. 2011. Semantic relatedness 
from automatically generated semantic networks. In 
Proceedings of the Ninth International Conference 
on Computational Semantics (IWCS?11) 
Wu, Z. Palmer, M. 1994. Verbs semantics and lexical 
selection. Proceedings of the 32nd annual meeting 
on Association for Computational Linguistics, pp. 
133-138 
Wu, H., Su, Z., Mao, F., Olman, V., Xu, Y. 2005. 
Prediction of functional modules based on 
comparative genome analysis and gene ontology 
application. Nucleic Acids Research, 33, pp. 2822?
2837.  
Yazdani, M., Popescu-Belis, A. 2010. A random walk 
framework to compute textual semantic similarity: a 
unified model for three benchmark tasks. IEEE 
Fourth International Conference on Semantic 
Computing (ICSC), pp. 424-429 
Ye, P., Peyser, B., Pan, X., Boek, J., Spencer, F., Bader, 
J. 2005. Gene function prediction from congruent 
synthetic lethal interactions in yeast. In Molecular 
system biology 
Yeh, E., Ramage, D., Manning, C., Agirre, E., Soroa, A. 
2009. WikiWalk: random walks on Wikipedia for 
semantic relatedness. In Proceedings of the 
TextGraphs-4, Workshop on Graph-based Methods 
for Natural Language Processing, ACL2009 
Zesch, T., and Gurevych, I. 2007. Analysis of the 
Wikipedia category graph for NLP applications. In 
Proceedings of the TextGraphs-2 Workshop 
(NAACL-HLT 2007), pp. 1?8 
Zesch, T., Gurevych, I. 2010. Wisdom of crowds versus 
wisdom of linguists: measuring the semantic 
relatedness of words. In Journal of Natural 
Language Engineering, 16, pp. 25-59 
Zhang, Z., Gentile, A., Xia, L., Iria, J., Chapman, S. 
2010. A random graph walk based approach to 
compute semantic relatedness using knowledge from 
Wikipedia. In Proceedings of LREC?10. 
 
1002
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 289?293,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mining Equivalent Relations from Linked Data 
 
 
Ziqi Zhang1 Anna Lisa Gentile1 Isabelle Augenstein1 
Eva Blomqvist2 Fabio Ciravegna1 
1 Department of Computer Science, 
University of Sheffield, UK 
2 Department of Computer and Information 
Science, Link?ping University, Sweden 
{z.zhang, a.l.gentile, i.augenstein, 
f.ciravegna}@dcs.shef.ac.uk, eva.blomqvist@liu.se
 
 
Abstract 
Linking heterogeneous resources is a major re-
search challenge in the Semantic Web. This 
paper studies the task of mining equivalent re-
lations from Linked Data, which was insuffi-
ciently addressed before. We introduce an un-
supervised method to measure equivalency of 
relation pairs and cluster equivalent relations. 
Early experiments have shown encouraging 
results with an average of 0.75~0.87 precision 
in predicting relation pair equivalency and 
0.78~0.98 precision in relation clustering. 
1 Introduction 
Linked Data defines best practices for exposing, 
sharing, and connecting data on the Semantic 
Web using uniform means such as URIs and 
RDF. It constitutes the conjunction between the 
Web and the Semantic Web, balancing the rich-
ness of semantics offered by Semantic Web with 
the easiness of data publishing. For the last few 
years Linked Open Data has grown to a gigantic 
knowledge base, which, as of 2013, comprised 
31 billion triples in 295 datasets1.  
A major research question concerning Linked 
Data is linking heterogeneous resources, the fact 
that publishers may describe analogous infor-
mation using different vocabulary, or may assign 
different identifiers to the same referents. Among 
such work, many study mappings between ontol-
ogy concepts and data instances (e.g., Isaac et al 
2007; Mi et al, 2009; Le et al, 2010; Duan et al, 
2012). An insufficiently addressed problem is 
linking heterogeneous relations, which is also 
widely found in data and can cause problems in 
information retrieval (Fu et al, 2012). Existing 
work in linking relations typically employ string 
similarity metrics or semantic similarity mea-
                                                 
1 http://lod-cloud.net/state/ 
sures that require a-priori domain knowledge and 
are limited in different ways (Zhong et al, 2002; 
Volz et al, 2009; Han et al, 2011; Zhao and 
Ichise, 2011; Zhao and Ichise, 2012).  
This paper introduces a novel method to dis-
cover equivalent groups of relations for Linked 
Data concepts. It consists of two components: 1) 
a measure of equivalency between pairs of rela-
tions of a concept and 2) a clustering process to 
group equivalent relations. The method is unsu-
pervised; completely data-driven requiring no a-
priori domain knowledge; and also language in-
dependent. Two types of experiments have been 
carried out using two major Linked Data sets: 1) 
evaluating the precision of predicting equivalen-
cy of relation pairs and 2) evaluating the preci-
sion of clustering equivalent relations. Prelimi-
nary results have shown encouraging results as 
the method achieves between 0.75~0.85 preci-
sion in the first set of experiments while 
0.78~0.98 in the latter. 
2 Related Work  
Research on linking heterogeneous ontological 
resources mostly addresses mapping classes (or 
concepts) and instances (Isaac et al 2007; Mi et 
al., 2009; Le et al, 2010; Duan et al, 2012; 
Schopman et al, 2012), typically based on the 
notions of similarity. This is often evaluated by 
string similarity (e.g. string edit distance), se-
mantic similarity (Budanitsky and Hirst, 2006), 
and distributional similarity based on the overlap 
in data usage (Duan et al, 2012; Schopman et 
al., 2012). There have been insufficient studies 
on mapping relations (or properties) across on-
tologies. Typical methods make use of a combi-
nation of string similarity and semantic similarity 
metrics (Zhong et al, 2002; Volz et al, 2009; 
Han et al, 2011; Zhao and Ichise, 2012). While 
string similarity fails to identify equivalent rela-
tions if their lexicalizations are distinct, semantic 
similarity often depends on taxonomic structures 
289
in existing ontologies (Budanitsky and Hirst, 
2006). Unfortunately many Linked Data instanc-
es use relations that are invented arbitrarily or 
originate in rudimentary ontologies (Parundekar 
et al, 2012). Distributional similarity has also 
been used to discover equivalent or similar rela-
tions. Mauge et al (2012) extract product proper-
ties from an e-commerce website and align 
equivalent properties using a supervised maxi-
mum entropy classification method. We study 
linking relations on Linked Data and propose an 
unsupervised method. Fu et al (2012) identify 
similar relations using the overlap of the subjects 
of two relations and the overlap of their objects. 
On the contrary, we aim at identifying strictly 
equivalent relations rather than similarity in gen-
eral. Additionally, the techniques introduced our 
work is also related to work on aligning multilin-
gual Wikipedia resources (Adar et al, 2009; 
Bouma et al, 2009) and semantic relatedness 
(Budanitsky and Hirst, 2006). 
3 Method 
Let t denote a 3-tuple (triple) consisting of a sub-
ject (ts), predicate (tp) and object (to). Linked Da-
ta resources are typed and its type is called class. 
We write type (ts) = c meaning that ts is of class c. 
p denotes a relation and rp is a set of triples 
whose tp=p, i.e., rp={t | tp = p}. 
Given a specific class c, and its pairs of rela-
tions (p, p?) such that rp={t|tp=p, type(ts)=c} and 
rp?={t|tp=p?, type (ts)=c}, we measure the equiv-
alency of p and p? and then cluster equivalent 
relations. The equivalency is calculated locally 
(within same class c) rather than globally (across 
all classes) because two relations can have iden-
tical meaning in specific class context but not 
necessarily so in general. For example, for the 
class Book, the relations dbpp:title and foaf:name 
are used with the same meaning, however for 
Actor, dbpp:title is used interchangeably with 
awards dbpp:awards (e.g., Oscar best actor). 
In practice, given a class c, our method starts 
with retrieving all t from a Linked Data set 
where type(ts)=c, using the universal query lan-
guage SPARQL with any SPARQL data end-
point. This data is then used to measure equiva-
lency for each pair of relations (Section 3.1). The 
equivalence scores are then used to group rela-
tions in equivalent clusters (Section 3.2). 
3.1 Measure of equivalence 
The equivalence for each distinct pair of rela-
tions depends on three components. 
Triple overlap evaluates the degree of over-
lap2 in terms of the usage of relations in triples. 
Let SO(p) be the collection of subject-object 
pairs from rp and SOint the intersection 
)r(SO)r(SO)'p,p(SO 'ppint ??
           [1] 
then the triple overlap TO(p, p?) is calculated as 
}|r|
|)r,r(SO|,|r|
|)r,r(SO|{MAX
'p
'ppint
p
'ppint
        [2] 
Intuitively, if two relations p and p? have a 
large overlap of subject-object pairs in their data 
instances, they are likely to have identical mean-
ing. The MAX function allows addressing infre-
quently used, but still equivalent relations (i.e., 
where the overlap covers most triples of an in-
frequently used relation but only a very small 
proportion of a much more frequently used).  
Subject agreement While triple overlap looks 
at the data in general, subject agreement looks at 
the overlap of subjects of two relations, and the 
degree to which these subjects have overlapping 
objects. Let S(p) return the set of subjects of rela-
tion p, and O(p|s) returns the set of objects of 
relation p whose subjects are s, i.e.: 
}st,pt|t{)s|r(O)s|p(O spop ????
        [3] 
we define: 
)r(S)r(S)'p,p(S 'ppint ??
         [4] 
|)'p,p(S|
otherwise,
|)s|'p(O)s|p(O|if,
int
)'p,p(Ss int
?
?
??
??
0
01
         [5] 
|)'p(S)p(S|/|)'p,p(S| int ???
        [6] 
then the agreement AG(p, p?) is  
????)'p,p(AG            [7] 
Equation [5] counts the number of overlapping 
subjects whose objects have at least one overlap. 
The higher the value of ?, the more the two rela-
tions ?agree? in terms of their shared subjects. 
For each shared subject of p and p? we count 1 if 
they have at least 1 overlapping object and 0 oth-
erwise. This is because both p and p? can be 
1:many relations and a low overlap value could 
mean that one is densely populated while the 
other is not, which does not necessarily mean 
they do not ?agree?. Equation [6] evaluates the 
degree to which two relations share the same set 
of subjects. The agreement AG(p, p?) balances 
the two factors by taking the product. As a result, 
                                                 
2 In this paper overlap is based on ?exact? match. 
290
relations that have high level of agreement will 
have more subjects in common, and higher pro-
portion of shared subjects with shared objects. 
Cardinality ratio is a ratio between cardinali-
ty of the two relations. Cardinality of a relation 
CD(p) is calculated based on data: 
|)r(S|
|r|)p(CD
p
p?
         [8] 
and the cardinality ratio is calculated as 
)}'p(CD),p(CD{MAX
)}'p(CD),p(CD{IN)'p,p(CDR ?
       [9] 
The final equivalency measure integrates all 
the three components to return a value in [0, 2]: 
)'p,p(CDR
)'p,p(AG)'p,p(TO)'p,p(E ??
              [10] 
The measure will favor two relations that have 
similar cardinality.  
3.2 Clustering 
We apply the measure to every pair of relations 
of a concept, and keep those with a non-zero 
equivalence score. The goal of clustering is to 
create groups of equivalent relations based on the 
pair-wise equivalence scores. We use a simple 
rule-based agglomerative clustering algorithm 
for this purpose. First, we rank all relation pairs 
by their equivalence score, then we keep a pair if 
(i) its score and (ii) the number of triples covered 
by each relation are above a certain threshold, 
TminEqvl and TminTP respectively. Each pair forms 
an initial cluster. To merge clusters, given an 
existing cluster c and a new pair (p, p?) where 
either p?c or p??c, the pair is added to c if E(p, 
p?) is close (as a fractional number above the 
threshold TminEqvlRel) to the average scores of all 
connected pairs in c. This preserves the strong 
connectivity in a cluster. This is repeated until no 
merge action is taken. Adjusting these thresholds 
allows balancing between precision and recall. 
4 Experiment Design 
To our knowledge, there is no publically availa-
ble gold standard for relation equivalency using 
Linked Data. We randomly selected 21 concepts 
(Figure 1) from the DBpedia ontology (v3.8): 
Actor, Aircraft, Airline, Airport, Automobile, 
Band, BasketballPlayer, Book, Bridge, Comedian, 
Film, Hospital, Magazine, Museum, Restaurant, 
Scientist, TelevisionShow, TennisPlayer, Theatre, 
University, Writer 
Figure 1. Concepts selected for evaluation. 
We apply our method to each concept to dis-
cover clusters of equivalent relations, using as 
SPARQL endpoint both DBpedia3 and Sindice4 
and report results separately. This is to study 
how the method performs in different conditions: 
on one hand on a smaller and cleaner dataset 
(DBpedia); on the other hand on a larger and 
multi-lingual dataset (Sindice) to also test cross-
lingual capability of our method. We chose rela-
tively low thresholds, i.e. TminEqvl=0.1, TminTP= 
0.01% and TminEqvlRel=0.6, in order to ensure high 
recall without sacrificing much precision.  
Four human annotators manually annotated 
the output for each concept. For this preliminary 
evaluation, we have limited the amount of anno-
tations to a maximum of 100 top scoring pairs of 
relations per concept, resulting in 16~100 pairs 
per concept (avg. 40) for DBpedia experiment 
and 29~100 pairs for Sindice (avg. 91). The an-
notators were asked to rate each edge in each 
cluster with -1 (wrong), 1 (correct) or 0 (cannot 
decide). Pairs with 0 are ignored in the evalua-
tion (about 12% for DBpedia; and 17% for Sin-
dice mainly due to unreadable encoded URLs for 
certain languages). To evaluate cross-lingual 
pairs, we asked annotators to use translation 
tools. Inter-Annotator-Agreement (observed 
IAA) is shown in Table 1. Also using this data, 
we derived a gold standard for clustering based 
on edge connectivity and we evaluate (i) the pre-
cision of top n% (p@n%) ranked equivalent rela-
tion pairs and (ii) the precision of clustering for 
each concept.  
 Mean High Low 
DBpedia 0.79 0.89 0.72 
Sindice 0.75 0.82 0.63 
Table 1. IAA on annotating pair equivalency 
So far the output of 13 concepts has been an-
notated. This dataset 5  contains ?1800 relation 
pairs and is larger than the one by Fu et al 
(2012). Annotation process shows that over 75% 
of relation pairs in the Sindice experiment con-
tain non-English relations and mostly are cross-
lingual. We used this data to report performance, 
although the method has been applied to all the 
21 concepts, and the complete results can be vis-
ualized at our demo website link. Some examples 
are shown in Figure 2.  
                                                 
3 http://dbpedia.org/sparql 
4 http://sparql.sindice.com/ 
5 http://staffwww.dcs.shef.ac.uk/people/Z.Zhang/ re-
sources/paper/acl2013short/web/ 
291
 Figure 2. Examples of visualized clusters 
5 Result and Discussion 
Figure 3 shows p@n% for pair equivalency6 and 
Figure 4 shows clustering precision.  
 
Figure 3. p@n%. The box plots show the ranges of 
precision at each n%; the lines show the average. 
 
Figure 4. Clustering precision  
As it is shown in Figure 2, Linked Data rela-
tions are often heterogeneous. Therefore, finding 
equivalent relations to improve coverage is im-
portant. Results in Figure 3 show that in most 
cases the method identifies equivalent relations 
with high precision. It is effective for both sin-
gle- and cross-language relation pairs. The worst 
performing case for DBpedia is Aircraft (for all 
n%), mostly due to duplicating numeric valued 
objects of different relations (e.g., weight, length, 
capacity). The decreasing precision with respect 
to n% suggests the measure effectively ranks 
correct pairs to the top. This is a useful feature 
from IR point of view. Figure 4 shows that the 
method effectively clusters equivalent relations 
with very high precision: 0.8~0.98 in most cases. 
                                                 
6 Per-concept results are available on our website. 
Overall we believe the results of this early proof-
of-concept are encouraging. As a concrete exam-
ple to compare against Fu et al (2012), for Bas-
ketballPlayer, our method creates separate clus-
ters for relations meaning ?draft team? and ?for-
mer team? because although they are ?similar? 
they are not ?equivalent?. 
We noticed that annotating equivalent rela-
tions is a non-trivial task. Sometimes relations 
and their corresponding schemata (if any) are 
poorly documented and it is impossible to under-
stand the meaning of relations (e.g., due to acro-
nyms) and even very difficult to reason based on 
data. Analyses of the evaluation output show that 
errors are typically found between highly similar 
relations, or whose object values are numeric 
types. In both cases, there is a very high proba-
bility of having a high overlap of subject-object 
pairs between relations. For example, for Air-
craft, the relations dbpp:heightIn and dbpp: 
weight are predicted to be equivalent because 
many instances have the same numeric value for 
the properties. Another example are the Airport 
properties dbpp:runwaySurface, dbpp:r1Surface, 
dbpp:r2Surface etc., which according to the data 
seem to describe the construction material (e.g., 
concrete, asphalt) of airport runways. The rela-
tions are semantically highly similar and the ob-
ject values have a high overlap. A potential solu-
tion to such issues is incorporating ontological 
knowledge if available. For example, if an ontol-
ogy defines the two distinct properties of Airport 
without explicitly defining an ?equivalence? re-
lation between them, they are unlikely to be 
equivalent even if the data suggests the opposite.  
6 Conclusion 
This paper introduced a data-driven, unsuper-
vised and domain and language independent 
method to learn equivalent relations for Linked 
Data concepts. Preliminary experiments show 
encouraging results as it effectively discovers 
equivalent relations in both single- and multi-
lingual settings. In future, we will revise the 
equivalence measure and also experiment with 
clustering algorithms such as (Beeferman et al, 
2000). We will also study the contribution of 
individual components of the measure in such 
task. Large scale comparative evaluations (incl. 
recall) are planned and this work will be extend-
ed to address other tasks such as ontology map-
ping and ontology pattern mining (Nuzzolese et 
al., 2011).  
 
292
Acknowledgement 
Part of this research has been sponsored by the 
EPSRC funded project LODIE: Linked Open 
Data for Information Extraction, EP/J019488/1. 
Additionally, we also thank the reviewers for 
their valuable comments given for this work. 
 
References  
Eytan Adar, Michael Skinner, Daniel Weld. 
2009. Information Arbitrage across Multi-
lingual Wikipedia. Proceedings of the Second 
ACM International Conference on Web 
Search and Data Mining, pp. 94 ? 103. 
Gosse Bouma, Sergio Duarte, Zahurul Islam. 
2009. Cross-lingual Alignment and Comple-
tion of Wikipedia Templates. Proceedings of 
the Third International Workshop on Cross 
Lingual Information Access: Addressing the 
Information Need of Multilingual Societies, 
pp. 61 ? 69   
Doug Beeferman, Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. 
Proceedings of the sixth ACM SIGKDD inter-
national conference on Knowledge discovery 
and data mining, pp. 407-416. 
Alexander Budanitsky and Graeme Hirst. 2006. 
Evaluating WordNet-based Measures of Se-
mantic Distance. Computational Linguistics, 
32(1), pp.13-47. 
Songyun Duan, Achille Fokoue, Oktie Has-
sanzadeh, Anastasios Kementsietsidis, Kavitha 
Srinivas, and Michael J. Ward. 2012. In-
stance-Based Matching of Large Ontologies 
Using Locality-Sensitive Hashing. ISWC 
2012, pp. 46 ? 64 
Linyun Fu, Haofen Wang, Wei Jin, Yong Yu. 
2012. Towards better understanding and uti-
lizing relations in DBpedia. Web Intelligence 
and Agent Systems , Volume 10 (3) 
Andrea Nuzzolese, Aldo Gangemi, Valentina 
Presutti, Paolo Ciancarini. 2011. Encyclopedic 
Knowledge Patterns from Wikipedia Links. 
Proceedings of the 10th International Semantic 
Web Conference, pp. 520-536 
Lushan Han, Tim Finin and Anupam Joshi. 2011. 
GoRelations: An Intuitive Query System for 
DBpedia. Proceedings of the Joint Internation-
al Semantic Technology Conference 
Antoine Isaac, Lourens van der Meij, Stefan 
Schlobach, Shenghui Wang. 2007. An empiri-
cal study of instance-based ontology match-
ing. Proceedings of the 6th International Se-
mantic Web Conference and the 2nd Asian 
conference on Asian Semantic Web Confer-
ence, pp. 253-266 
Ngoc-Thanh Le, Ryutaro Ichise, Hoai-Bac Le. 
2010. Detecting hidden relations in geograph-
ic data. Proceedings of the 4th International 
Conference on Advances in Semantic Pro-
cessing, pp. 61 ? 68 
Karin Mauge, Khash Rohanimanesh, Jean-David 
Ruvini. 2012. Structuring E-Commerce Inven-
tory. Proceedings of ACL2012, pp. 805-814 
Jinhua Mi, Huajun Chen, Bin Lu, Tong Yu, 
Gang Pan. 2009. Deriving similarity graphs 
from open linked data on semantic web. Pro-
ceedings of the 10th IEEE International Con-
ference on Information Reuse and Integration, 
pp. 157?162. 
Rahul Parundekar, Craig Knoblock,  Jos? Luis. 
Ambite. 2012. Discovering Concept Cover-
ings in Ontologies of Linked Data Sources. 
Proceedings of ISWC2012, pp. 427?443. 
Balthasar Schopman, Shenghui Wang, Antoine 
Isaac, Stefan Schlobach. 2012. Instance-Based 
Ontology Matching by Instance Enrichment. 
Journal on Data Semantics, 1(4), pp 219-236 
Julius Volz, Christian Bizer, Martin Gaedke, 
Georgi Kobilarov. 2009. Silk ? A Link Discov-
ery Framework for the Web of Data. Proceed-
ings of the 2nd Workshop on Linked Data on 
the Web 
Lihua Zhao, Ryutaro Ichise. 2011. Mid-ontology 
learning from linked data. Proceedings of the 
Joint International Semantic Technology Con-
ference, pp. 112 ? 127. 
Lihua Zhao, Ryutaro Ichise. 2012. Graph-based 
ontology analysis in the linked open data. Pro-
ceedings of the 8th International Conference 
on Semantic Systems, pp. 56 ? 63 
Jiwei Zhong, Haiping Zhu, Jianming Li and 
Yong Yu. 2002. Conceptual Graph Matching 
for Semantic Search. The 2002 International 
Conference on Computational Science. 
293
