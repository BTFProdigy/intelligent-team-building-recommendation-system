Natural Language and Inference in a Computer Game
Malte Gabsdil and Alexander Koller and Kristina Striegnitz
Dept. of Computational Linguistics
Saarland University, Saarbru?cken, Germany
{gabsdil|koller|kris}@coli.uni-sb.de
Abstract
We present an engine for text adventures ? computer
games with which the player interacts using natu-
ral language. The system employs current meth-
ods from computational linguistics and an efficient
inference system for description logic to make the
interaction more natural. The inference system is
especially useful in the linguistic modules dealing
with reference resolution and generation and we
show how we use it to rank different readings in
the case of referential and syntactic ambiguities. It
turns out that the player?s utterances are naturally
restricted in the game scenario, which simplifies the
language processing task.
1 Introduction
Text adventures are computer games with which
the player interacts via a natural language dialogue.
Texts describe the game world and how it evolves,
and the player can manipulate objects in this game
world by typing in commands; Fig. 1 shows a sam-
ple interaction. Text adventures were very popu-
lar and commercially successful in the eighties, but
have gone out of fashion since then ? mostly be-
cause the parsers were rather limited and forced the
user into very restricted forms of interaction.
We describe an engine for text adventures that
attempts to overcome these limitations by using
current methods from computational linguistics for
processing the natural language input and output,
and a state-of-the-art inference system based on de-
scription logic (DL) to represent the dynamic state
of the game world and what the player knows about
it. The DL prover is used in all language-processing
modules except for parsing and surface realization,
and supports the inferences we need very well.
This shows in particular in the modules for the
resolution and generation of referring expressions.
By keeping track of the true state of the world
and the player?s knowledge in separate knowledge
bases, we can evaluate definite descriptions with re-
spect to what the player knows. In generation, such
inferences allow us to produce smaller while still
sufficiently informative references.
Another interesting aspect which we discuss in
this paper is the treatment of syntactic and referen-
tial ambiguities that come up in understanding input
sentences. Here, too, the player knowledge restricts
the way in which the input should be interpreted and
guides the resolution process. We use inferences
about the player knowledge to rule out inconsistent
analyses and pragmatic heuristics to possibly select
the preferred one.
Players of a text adventure are effectively situ-
ated in a game world and have to accomplish a
specific task, which severely restricts the utterances
they will naturally produce. For example, they will
typically only refer to objects they could ?see? in
the simulated world. This simplifies the language
processing tasks tremendously. The scenario of the
game can be freely specified and tailored to different
applications and levels of complexity. Along with
the modularity of our implementation, this makes
our system an interesting testbed for evaluating
more sophisticated NLP modules, which also have
access to the inference infrastructure. The same rea-
son makes it useful as a teaching tool; in fact, our
implementation was created as an advanced project
for students.
Plan of the paper. We will start with an overview
over the architecture of our system in Section 2.
Section 3 will give a brief introduction to descrip-
tion logics, and then describes the knowledge bases
we use. We will discuss how they interact with the
linguistic modules in Section 4, which focuses on
the resolution and generation of referring expres-
sions. Section 5 deals with the resolution of am-
biguities in the player input. Finally, Section 6 con-
cludes and points to further research.
2 Architecture
The general architecture of the game engine, shown
in Fig. 2, consists of a number of language-
processing modules (drawn as ellipses), which in-
Observation Lounge
This is where the station staff and visitors come to relax. There are a lot of tables
and chairs here, a large observation window, and a plush carpet. In the corner you can
see an AstroCola dispenser. A tube leads up to the station?s main corridor.
> put my galakmid coin into the dispenser
Click.
The dispenser display now reads "Credit = 1.00".
> push diet astrocola button
You hear a rumbling noise in the dispenser, but nothing appears in the tray.
> kick dispenser
A can drops into the tray. Amazing! The oldest trick in the book, and it
actually worked.
Figure 1: An example interaction with a text adventure, taken from (Ledgard, 1999).
Content
Determination
Model
Discourse
Reference
Resolution
Reference
Generation
RealizationParsing
Actions
A-Box: User Knowledge
T-Box
A-Box: World Model
Figure 2: The architecture.
terface with knowledge bases and a discourse model
(drawn as rectangles). There are two separate
knowledge bases, which share a set of common def-
initions: One represents the true state of the world
in a world model, the other keeps track of what the
player knows about the world. Solid arrows indi-
cate the general flow of information, dashed arrows
indicate access to the knowledge bases.
The user?s input is first parsed using an efficient
parser for dependency grammar (Duchier and De-
busmann, 2001). Next, referring expressions are re-
solved to individuals in the game world. The result
is a ground term or a sequence of ground terms that
indicates the action(s) the user wants to take. The
Actions module looks up these actions in a database
(where they are specified in a STRIPS-like format),
checks whether the action?s preconditions are met in
the world, and, if yes, updates the world state with
the effects of the action.
The action can also specify effects on the user?s
knowledge. This information is further enriched
by the Content Determination module; for example,
this module computes detailed descriptions of ob-
jects the player wants to look at. The Reference
Generation module translates the internal names
of individuals into descriptions that can be verbal-
ized. In the last step, an efficient realization mod-
ule (Koller and Striegnitz, 2002) builds the output
sentences according to a TAG grammar. The player
knowledge is updated after Reference Generation
when the content of the game?s response, including
the new information carried e.g. by indefinite NPs,
is fully established.
If an error occurs at any stage, e.g. because a pre-
condition of the action fails, an error message spec-
ifying the reasons for the failure is generated by
using the normal generation track (Content Deter-
mination, Reference Generation, Realization) of the
game.
The system is implemented in the programming
language Mozart (Mozart Consortium, 1999) and
provides an interface to the DL reasoning system
RACER (Haarslev and Mo?ller, 2001), which is used
for mainting and accessing the knowledge bases.
3 The World Model
Now we will look at the way that the state of the
world is represented in the game, which will be
important in the language processing modules de-
scribed in Sections 4 and 5. We will first give a short
overview of description logic (DL) and the theorem
prover we use and then discuss some aspects of the
world model in more detail.
3.1 Description Logic
Description logic (DL) is a family of logics in the
tradition of knowledge representation formalisms
such as KL-ONE (Woods and Schmolze, 1992). DL
is a fragment of first-order logic which only allows
unary and binary predicates (concepts and roles)
and only very restricted quantification. A knowl-
edge base consists of a T-Box, which contains ax-
ioms relating the concepts and roles, and one or
more A-Boxes, which state that individuals belong
to certain concepts, or are related by certain roles.
Theorem provers for description logics support
a range of different reasoning tasks. Among the
most common are consistency checking, subsump-
tion checking, and instance and relation check-
ing. Consistency checks decide whether a combina-
tion of T-Box and A-Box can be satisfied by some
model, subsumption is to decide of two concepts
whether all individuals that belong to one concept
must necessarily belong to another, and instance and
relation checking test whether an individual belongs
to a certain concept and whether a certain relation
holds between a pair of individuals, respectively. In
addition to these basic reasoning tasks, description
logic systems usually also provide some retrieval
functionality which e.g. allows to compute all con-
cepts that a given individual belongs to or all indi-
viduals that belong to a given concept.
There is a wide range of different description log-
ics today which add different extensions to a com-
mon core. Of course, the more expressive these ex-
tensions become, the more complex the reasoning
problems are. ?Traditional? DL systems have con-
centrated on very weak logics with simple reasoning
tasks. In the last few years, however, new systems
such as FaCT (Horrocks et al, 1999) and RACER
(Haarslev and Mo?ller, 2001) have shown that it is
possible to achieve surprisingly good average-case
performance for very expressive (but still decidable)
logics. In this paper, we employ the RACER sys-
tem, mainly because it allows for A-Box inferences.
3.2 The World Model
The T-Box we use in the game specifies the con-
cepts and roles in the world and defines some useful
complex concepts, e.g. the concept of all objects the
player can see. This T-Box is shared by two differ-
ent A-Boxes representing the state of the world and
what the player knows about it respectively.
The player A-Box will typically be a sub-part of
the game A-Box because the player will not have
explored the world completely and will therefore
not have encountered all individuals or know about
all of their properties. Sometimes, however, it may
also be useful to deliberately hide effects of an ac-
tion from the user, e.g. if pushing a button has an
effect in a room that the player cannot see. In this
case, the player A-Box can contain information that
is inconsistent with the world A-Box.
A fragment of the A-Box describing the state of
the world is shown in Fig. 3; Fig. 4 gives a graphical
representation. The T-Box specifies that the world
is partitioned into three parts: rooms, objects, and
players. The individual ?myself? is the only instance
that we ever define of the concept ?player?. Indi-
viduals are connected to their locations (i.e. rooms,
container objects, or players) via the ?has-location?
role; the A-Box also specifies what kind of object
an individual is (e.g. ?apple?) and what properties it
has (?red?). The T-Box then contains axioms such
as ?apple  object?, ?red  colour?, etc., which es-
tablish a taxonomy among concepts.
These definitions allow us to add axioms to the
T-Box which define more complex concepts. One
is the concept ?here?, which contains the room in
which the player currently is ? that is, every indi-
vidual which can be reached over a ?has-location?
role from a player object.
here .= ?has-location?1.player
In this definition, ?has-location?1? is the inverse role
of the role ?has-location?, i.e. it links a and b iff
?has-location? links b and a. Inverse roles are one of
the constructions available in more expressive de-
scription logics. The quantification builds a more
complex concept from a concept and a role: ?R.C
is the concept containing all individuals which are
linked via an R role to some individual in C . In the
example in Fig. 3, ?here? denotes the singleton set
{kitchen}.
Another useful concept is ?accessible?, which
contains all individuals which the player can ma-
nipulate.
accessible .= ?has-location.here unionsq
?has-location.(accessible  open)
All objects in the same room as the player are
accessible; if such an object is an open container,
its contents are also accessible. The T-Box con-
tains axioms that express that some concepts (e.g.
?table?, ?bowl?, and ?player?) contain only ?open?
room(kitchen) player(myself)
table(t1) apple(a1)
apple(a2) worm(w1)
red(a1) green(a2)
bowl(b1) bowl(b2)
has-location(t1, kitchen) has-location(b1, t1)
has-location(b2, kitchen) has-location(a1, b2)
has-location(a2, kitchen) has-detail(a2,w1)
has-location(myself, kitchen) . . .
Figure 3: A fragment of a world A-Box.
objects. This permits access to the player?s inven-
tory. In the simple scenario above, ?accessible? de-
notes the set {myself, t1, a1, a2, b1, b2}. Finally,
we can define the concept ?visible? in a similar way
as ?accessible?. The definition is a bit more com-
plex, including more individuals, and is intended to
denote all individuals that the player can ?see? from
his position in the game world.1
4 Referring Expressions
The interaction between the game and the player re-
volves around performing actions on objects in the
game world and the effects that these actions have
on the objects. This means that the resolution and
generation of referring expressions, which identify
those objects to the user, are central tasks in our ap-
plication.
Our implementation illustrates how useful the
availability of an inference system as provided by
RACER to access the world model is, once such an
infrastructure is available. The inference engine is
complemented by a simple discourse model, which
keeps track of available referents.
4.1 The Discourse Model
Our discourse model (DM) is based on Strube?s
(1998) salience list approach, due to its simplic-
ity. The DM is a data structure that stores an or-
dered list of the most salient discourse entities ac-
cording to their ?information status? and text po-
sition and provides methods for retrieving and in-
serting elements. Following Strube, hearer-old dis-
course entities (which include definites) are ranked
1Remember that ?seeing? in our application does not in-
volve any graphical representations. The player acquires
knowledges about the world only through the textual output
generated by the game engine. This allows us to simplify the
DL modeling of the world because we don?t have to specify
all (e.g. spatial) relations that would implicitly be present in a
picture.
Figure 4: Example Scenario
higher in the DM (i.e. are more available for refer-
ence) than hearer-new discourse entities (including
indefinites). Within these categories, elements are
sorted according to their position in the currently
processed sentence. For example, the ranking of
discourse entities for the sentence take a banana,
the red apple, and the green apple would look as
follows:
[red apple ? green apple]old ? [banana]new
The DM is built incrementally and updated af-
ter each input sentence. Updating removes all dis-
course entities from the DM which are not realized
in the current utterance. That is, there is an assump-
tion that referents mentioned in the previous utter-
ance are much more salient than older ones.
4.2 Resolving Referring Expressions
The task of the resolution module is to map def-
inite and indefinite noun phrases and pronouns to
individuals in the world. This task is simplified in
the adventure setting by the fact that the commu-
nication is situated in a sense: Players will typi-
cally only refer to objects which they can ?see? in
the virtual environment, as modeled by the concept
?visible? above. Furthermore, they should not re-
fer to objects they haven?t seen yet. Hence, we
perform all RACER queries in this section on the
player knowledge A-Box, avoiding unintended am-
biguities when the player?s expression would e.g.
not refer uniquely with respect to the true state of
the world.
The resolution of a definite description means to
find a unique entity which, according to the player?s
knowledge, is visible and matches the description.
To compute such an entity, we construct a DL con-
cept expression corresponding to the description
and then send a query to RACER asking for all in-
stances of this concept. In the case of the apple,
for instance, we would retrieve all instances of the
concept
apple  visible
from the player A-Box. The query concept for the
apple with the worm would be
apple  (?has-detail.worm)  visible.
If this yields only one entity ({a2} for the apple with
the worm for the A-Box in Fig. 3), the reference
has been unambiguous and we are done. It may,
however, also be the case that more than one entity
is returned; e.g. the query for the apple would return
the set {a1,a2}. We will show in the next section
how we deal with this kind of ambiguity. We reject
input sentences with an error message indicating a
failed reference if we cannot resolve an expression
at all, i.e. when no object in the player knowledge
matches the description.
We resolve indefinite NPs, such as an apple, by
querying the player knowledge in the same way as
described above for definites. Unlike in the definite
case, however, we do not require unique reference.
Instead, we assume that the player did not have a
particular object in mind and arbitrarily choose one
of the possible referents. The reply of the game will
automatically inform the player which one was cho-
sen, as a unique definite reference will be generated
(see below).
Pronouns are simply resolved to the most salient
entity in the DM that matches their agreement con-
straints. The restrictions our grammar imposes
on the player input (no embeddings, no reflexive
pronouns) allow us to analyze sentences including
intra-sentential anaphora like take the apple and eat
it. The incremental construction of the DM ensures
that by the time we encounter the pronoun it, the
apple has already been processed and can serve as a
possible antecedent.
4.3 Generating Referring Expressions
The converse task occurs when we generate the
feedback to show to the player: It is necessary to
construct descriptions of individuals in the game
world that enable the player to identify these.
This task is quite simple for objects which are
new to the player. In this case, we generate an indef-
inite NP containing the type and (if it has one) color
of the object, as in the bowl contains a red apple.
We use RACER?s retrieval functionality to extract
this information from the knowledge base.
To refer to an object that the player already has
encountered, we try to construct a definite descrip-
tion that, given the player knowledge, uniquely
identifies this object. For this purpose we use a vari-
ant of Dale and Reiter?s (1995) incremental algo-
rithm, extended to deal with relations between ob-
jects (Dale and Haddock, 1991). The properties of
the target referent are looked at in some predefined
order (e.g. first its type, then its color, its location,
parts it may have, . . .). A property is added to the
description if at least one other object (a distrac-
tor) is excluded from it because it doesn?t share this
property. This is done until the description uniquely
identifies the target referent.
The algorithm uses RACER?s reasoning and re-
trieval functionality to access the relevant informa-
tion about the context, which included e.g. comput-
ing the properties of the target referent and find-
ing the distracting instances. Assuming we want to
refer to entity a1 in the A-Box in Fig. 3 e.g., we
first have to retrieve all concepts and roles of a1
from the player A-Box. This gives us {apple(a1),
red(a1), has-location(a1,b1)}. As we have to have at
least one property specifying the type of a1, we use
RACER?s subsumption checks to extract all those
properties that match this requirement; in this case,
?apple?. Then we retrieve all instances of the con-
cept ?apple? to determine the set of distractors which
is {a1, a2}. Hence, ?apple? alone is not enough to
uniquely identify a1. So, we consider the apple?s
color. Again using subsumption checks, we filter
the colors from the properties of a1 (i.e. ?red?) and
then retrieve all instances belonging to the concept
apple red to check whether and how the set of dis-
tractors gets reduced by adding this property. This
concept has only one member in the example, so we
generate the expression the red apple.
5 Ambiguity Resolution
The other aspect of the game engine which we want
to highlight here is how we deal with referential
and syntactic ambiguity. We handle the former by
a combination of inference and discourse informa-
tion, and the latter by taking psycholinguistically
motivated preferences into account.
5.1 Resolving Referential Ambiguities
When the techniques for reference resolution de-
scribed in the previous section are not able to map
a definite description to a single entity in the player
knowledge, the resolution module returns a set of
possible referents. We then try to narrow this set
down in two steps.
First, we filter out individuals which are com-
pletely unsalient according to the discourse model.
In our (simplified) model, these are all individuals
that haven?t been mentioned in the previous sen-
tence. This heuristic permits the game to deal with
the following dialogue, as the red but not the green
apple is still accessible in the final turn, and is there-
fore chosen as the patient of the ?eat? action.
Game: . . . red apple . . . green apple.
Player: Take the red apple.
Game: You have the red apple.
Player: Eat the apple.
Game: You eat the red apple.
If this narrows down the possible referents to just
one, we are done. Otherwise ? i.e. if several or none
of the referents were mentioned in the previous sen-
tence ?, we check whether the player?s knowledge
rules out some of them. The rationale is that an in-
telligent player would not try to perform an action
on an object on which she knows it cannot be per-
formed.
Assume, by way of example, that the player
knows about the worm in the green apple. This
violates a precondition of the ?eat? action for ap-
ples. Thus if both apples were equally salient, we
would read eat the apple as eat the red apple. We
can test if a combination of referents for the various
referring expressions of a sentence violates precon-
ditions by first instantiating the appropriate action
with these referents. Then we independently add
each instantiated precondition to fresh copies of the
player knowledge A-Box and test them for consis-
tency. If one of the A-Boxes becomes inconsistent,
we conclude that the player knows this precondition
would fail, and conclude that this is not the intended
combination of referents.
If neither of these heuristics manages to pick out
a unique entity, we consider the definite description
to be truly ambiguous and return an error message
to the user, indicating the ambiguity.
5.2 Resolving Syntactic Ambiguities
Another class of ambiguities which we consider are
syntactic ambiguities, especially of PP attachment.
We try to resolve them, too, by taking referential
information into account.
In the simplest case, the referring expressions in
some of the syntactic readings have no possible ref-
erent in the player A-Box at all. If this happens, we
filter these readings out and only continue with the
others (Schuler, 2001). For example, the sentence
unlock the toolbox with the key is ambiguous. In a
scenario where there is a toolbox and a key, but the
key is not attached to the toolbox, resolution fails for
one of the analyses and thereby resolves the syntac-
tic ambiguity.
If more than one syntactic reading survives this
first test, we perform the same computations as
above to filter out possible referents which are either
unsalient or violate the player?s knowledge. Some-
times, only one syntactic reading will have a refer-
ent in this narrower sense; in this case, we are done.
Otherwise, i.e. if more than one syntactic reading
has referents, we remove those readings which are
referentially ambiguous. Consider once more the
example scenario depicted in Fig. 4. The sentence
put the apple in the bowl on the table has two differ-
ent syntactic analyses: In the first, the bowl on the
table is the target of the put action whereas in the
second, in the bowl modifies the apple. Now, note
that in the first reading, we will get two possible ref-
erents for the apple, whereas in the second reading
the apple in the bowl is unique. In cases like this we
pick out the reading which only includes unique ref-
erences (reading 2 in the present example). This ap-
proach assumes that the players are cooperative and
try to refer unambiguously. It is furthermore similar
to what people seem to do. Psycholinguistic eye-
tracking studies (Chambers et al, 2000) indicate
that people prefer interpretations with unambiguous
references: subjects who are faced with scenarios
similar to Fig. 4 and hear the sentence put the ap-
ple in the bowl on the table do not look at the bowl
on the table at all but only at the apple in the bowl
(which is unique) and the table.
At this point, there can still be more than one syn-
tactic reading left; if so, all of these will have unam-
biguous, unique referents. In such a case we cannot
decide which syntactic reading the player meant,
and ask the player to give the game a less ambiguous
command.
6 Conclusion and Outlook
We have described an engine for text adventures
which uses techniques from computational linguis-
tics to make the interaction with the game more nat-
ural. The input is analyzed using a dependency
parser and a simple reference resolution module,
and the output is produced by a small generation
system. Information about the world and about
the player?s knowledge is represented in descrip-
tion logic knowledge bases, and accessed through
a state-of-the-art inference system. Most modules
use the inference component; to illustrate its useful-
ness, we have looked more closely at the resolution
and generation of referring expressions, and at the
resolution of referential and syntactic ambiguities.
Preliminary experiments indicate that the perfor-
mance of our game engine is good enough for flu-
ent gameplay. The constraint based dependency
parser we use for parsing and generation achieves
very good average case runtimes on the grammars
and inputs we use. More interestingly, the infer-
ence system also performs very well. With the cur-
rent knowledge bases, reasoning on the world model
and user knowledge takes 546ms per turn on aver-
age (with a mean of 39 queries per turn). How well
this performance scales to bigger game worlds re-
mains to be seen. One lesson we take from this is
that the recent progress in optimizing inference en-
gines for expressive description logics is beginning
to make them useful for applications.
All the language-processing modules in our sys-
tem are rather simplistic. We can get away with this
because the utterances that players seem to want to
produce in this setting are restricted, e.g. to objects
in the same simulated ?location? as the player. (The
precise extent of this, of course, remains to be eval-
uated.) The result is a system which exceeds tradi-
tional text adventures by far in the flexibility offered
to the user.
Unlike the input, the output that our game gen-
erates is far away from the quality of the com-
mercial text adventures of the eighties, which pro-
duced canned texts, sometimes written by profes-
sional book authors. A possible solution could be to
combine the full generation with a template based
approach, to which the TAG-based generation ap-
proach we take lends itself well. Another problem is
the generation of error messages asking the user to
resolve an ambiguous input. The game should ide-
ally generate and present the player with a choice
of possible (unambiguous) readings. So, the gen-
eration strategy would have to be augmented with
some kind of monitoring, such as the one proposed
by Neumann and van Noord (1994). Finally, we
want to come up with a way of synchronizing the
grammars for parsing and generation, in order to en-
sure that expressions used by the game can always
be used by the player as well.
The system is designed in a way that should make
it reasonably easy to replace our simple modules
by more sophisticated ones. We will shortly make
our adventure engine available over the web, and
want to invite colleagues and students to test their
own language processing modules within our sys-
tem. Generally, we believe that the prototype can
serve as a starting point for an almost unlimited
range of extensions.
References
C.G. Chambers, M.K. Tanenhaus, and J.S. Magnu-
son. 2000. Does real-world knowledge modulate
referential effects on PP-attachment? Evidence
from eye movements in spoken language compre-
hension. In 14th CUNY Conference on Human
Sentence Processing.
R. Dale and N. Haddock. 1991. Generating re-
ferring expressions involving relations. In EACL
?91.
R. Dale and E. Reiter. 1995. Computational inter-
pretations of the gricean maxims in the genera-
tion of referring expressions. Cognitive Science,
18.
D. Duchier and R. Debusmann. 2001. Topological
dependency trees: A constraint-based account of
linear precedence. In ACL ?01.
V. Haarslev and R. Mo?ller. 2001. RACER System
Description. In IJCAR ?01.
I. Horrocks, U. Sattler, and S. Tobies. 1999. Practi-
cal reasoning for expressive description logics. In
H. Ganzinger, D. McAllester, and A. Voronkov,
editors, LPAR?99.
A. Koller and K. Striegnitz. 2002. Generation as
dependency parsing. In ACL ?02.
D. Ledgard. 1999. Space Station. Text adventure,
modelled after a sample transcript of Infocom?s
Planetfall game. http://members.tripod.
com/?infoscripts/planetfa.htm.
Mozart Consortium. 1999. The Mozart Pro-
gramming System web pages. http://www.
mozart-oz.org/.
G. Neumann and G.-J. van Noord. 1994.
Self-monitoring with reversible grammars. In
T. Strzalkowski, editor, Reversible Grammar in
Natural Language Processing.
W. Schuler. 2001. Computational properties of
environment-based disambiguation. In ACL ?01.
M. Strube. 1998. Never Look Back: An Alternative
to Centering. In COLING-ACL ?98.
W. Woods and J. Schmolze. 1992. The KL-ONE
Family. Computer and Mathematics with Appli-
cations, 23(2?5).
Generation as Dependency Parsing
Alexander Koller and Kristina Striegnitz
Dept. of Computational Linguistics, Saarland University
{koller|kris}@coli.uni-sb.de
Abstract
Natural-Language Generation from flat
semantics is an NP-complete problem.
This makes it necessary to develop al-
gorithms that run with reasonable effi-
ciency in practice despite the high worst-
case complexity. We show how to con-
vert TAG generation problems into de-
pendency parsing problems, which is use-
ful because optimizations in recent de-
pendency parsers based on constraint pro-
gramming tackle exactly the combina-
torics that make generation hard. Indeed,
initial experiments display promising run-
times.
1 Introduction
Existing algorithms for realization from a flat input
semantics all have runtimes which are exponential in
the worst case. Several different approaches to im-
proving the runtime in practice have been suggested
in the literature ? e.g. heuristics (Brew, 1992) and
factorizations into smaller exponential subproblems
(Kay, 1996; Carroll et al, 1999). While these solu-
tions achieve some measure of success in making re-
alization efficient, the contrast in efficiency to pars-
ing is striking both in theory and in practice.
The problematic runtimes of generation algo-
rithms are explained by the fact that realization is an
NP-complete problem even using just context-free
grammars, as Brew (1992) showed in the context of
shake-and-bake generation. The first contribution of
our paper is a proof of a stronger NP-completeness
result: If we allow semantic indices in the grammar,
realization is NP-complete even if we fix a single
grammar. Our alternative proof shows clearly that
the combinatorics in generation come from essen-
tially the same sources as in parsing for free word
order languages. It has been noted in the literature
that this problem, too, becomes NP-complete very
easily (Barton et al, 1987).
The main point of this paper is to show how to
encode generation with a variant of tree-adjoining
grammars (TAG) as a parsing problem with depen-
dency grammars (DG). The particular variant of DG
we use, Topological Dependency Grammar (TDG)
(Duchier, 2002; Duchier and Debusmann, 2001),
was developed specifically with efficient parsing for
free word order languages in mind. The mere exis-
tence of this encoding proves TDG?s parsing prob-
lem NP-complete as well, a result which has been
conjectured but never formally shown so far. But it
turns out that the complexities that arise in gener-
ation problems in practice seem to be precisely of
the sort that the TDG parser can handle well. Initial
experiments with generating from the XTAG gram-
mar (XTAG Research Group, 2001) suggest that our
generation system is competitive with state-of-the-
art chart generators, and indeed seems to run in poly-
nomial time in practice.
Next to the attractive runtime behaviour, our ap-
proach to realization is interesting because it may
provide us with a different angle from which to
look for tractable fragments of the general realiza-
tion problem. As we will show, the computation that
takes place in our system is very different from that
in a chart generator, and may be more efficient in
some cases by taking into account global informa-
tion to guide local choices.
Plan of the Paper. We will define the problem we
want to tackle in Section 2, and then show that it is
NP-complete (Section 3). In Section 4, we sketch
the dependency grammar formalism we use. Sec-
tion 5 is the heart of the paper: We show how to
encode TAG generation as TDG parsing, and dis-
cuss some examples and runtimes. We compare our
approach to some others in Section 6, and conclude
and discuss future research in Section 7.
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 17-24.
                         Proceedings of the 40th Annual Meeting of the Association for
2 The Realization Problem
In this paper, we deal with the subtask of natural
language generation known as surface realization:
given a grammar and a semantic representation, the
problem is to find a sentence which is grammatical
according to the grammar and expresses the content
of the semantic representation.
We represent the semantic input as a multiset
(bag) of ground atoms of predicate logic, such
as {buy(e,a,b), name(a,mary) car(b)}. To en-
code syntactic information, we use a tree-adjoining
grammar without feature structures (Joshi and Sch-
abes, 1997). Following Stone and Doran (1997) and
Kay (1996), we enhance this TAG grammar with
a syntax-semantics interface in which nonterminal
nodes of the elementary trees are equipped with in-
dex variables, which can be bound to individuals in
the semantic input. We assume that the root node,
all substitution nodes, and all nodes that admit ad-
junction carry such index variables. We also assign
a semantics to every elementary tree, so that lexi-
cal entries are pairs of the form (?, T ), where ? is
a multiset of semantic atoms, and T is an initial or
auxiliary tree, e.g.
( {buy(x,y,z)},
S:x
NP:y  VP:x
V:x
buys
NP:z 
)
When the lexicon is accessed, x, y, z get bound
to terms occurring in the semantic input, e.g. e, a, b
in our example. Since we furthermore assume that
every index variable that appears in T also appears
in ?, this means that all indices occurring in T get
bound at this stage.
The semantics of a complex tree is the multiset
union of the semantics of the elementary trees in-
volved. Now we say that the realization problem of
a grammar G is to decide for a given input semantics
S and an index i whether there is a derivation tree
which is grammatical according to G, is assigned
the semantics S, and has a root node with index i.
3 NP-Completeness of Realization
This definition is the simplest conceivable formal-
ization of problems occurring in surface realization
as a decision problem: It does not even require us
to compute a single actual realization, just to check
?1 B:i
N:i  E:k
e
B:k 
sem: edge(i,k)
?2 C
eating C 
sem: edge(i,k)
?3 N:i
n
sem: node(i)
?4 B:1
eat C 
sem: start-eating
?5 C
ate
sem: end-eating
Figure 1: The grammar G
ham
.
whether one exists. Every practical generation sys-
tem generating from flat semantics will have to ad-
dress this problem in one form or another.
Now we show that this problem is NP-complete.
A similar result was proved in the context of shake-
and-bake generation by Brew (1992), but he needed
to use the grammar in his encoding, which leaves
the possibility open that for every single grammar
G, there might be a realization algorithm tailored
specifically to G which still runs in polynomial time.
Our result is stronger in that we define a single
grammar G
ham
whose realization problem is NP-
complete in the above sense. Furthermore, we find
that our proof brings out the sources of the complex-
ity more clearly. G
ham
does not permit adjunction,
hence the result also holds for context-free gram-
mars with indices.
1
 
2
 3
It is clear that the problem is in
NP: We can simply guess the ele-
mentary trees we need and how to
combine them, and then check in
polynomial time whether they verbalize the seman-
tics. The NP-hardness proof is by reducing the well-
known HAMILTONIAN-PATH problem to the realiza-
tion problem. HAMILTONIAN-PATH is the problem
of deciding whether a directed graph has a cycle that
visits each node exactly once, e.g. (1,3,2,1) in the
graph shown above.
We will now construct an LTAG grammar G
ham
such that every graph G = (V,E) can be encoded
as a semantic input S for the realization problem of
G
ham
, which can be verbalized if and only if G has
a Hamiltonian cycle. S is defined as follows:
S = {node(i) | i ? V }
? {edge(i, k) | (i, k) ? E}
? {start-eating, end-eating}.
B:1
N:1 
N:1
n
E:3
e
B:3 
B:3
N:3 
N:3
n
E:2
e
B:2 
B:2
N:2 
N:2
n
E:1
e
B:1 
B:1
eat C 
C
eating C 
C
ate
Figure 2: A derivation with G
ham
corresponding to
a Hamiltonian cycle.
The grammar G
ham
is given in Fig. 1; the start
symbol is B, and we want the root to have index 1.
The tree ?
1
models an edge transition from node i
to the node k by consuming the semantic encodings
of this edge and (by way of a substitution of ?
3
) of
the node i. The second substitution node of ?
1
can
be filled either by another ?
1
, in which way a path
through the graph is modelled, or by an ?
4
, in which
case we switch to an ?edge eating mode?. In this
mode, we can arbitrarily consume edges using ?
2
,
and close the tree with ?
5
when we?re done. This
is illustrated in Fig. 2, the tree corresponding to the
cycle in the example graph above.
The Hamiltonian cycle of the graph, if one exists,
is represented in the indices of the B nodes. The list
of these indices is a path in the graph, as the ?
1
trees
model edge transitions; it is a cycle because it starts
in 1 and ends in 1; and it visits each node exactly
once, for we use exactly one ?
1
tree for each node
literal. The edges which weren?t used in the cycle
can be consumed in the edge eating mode.
The main source for the combinatorics of the re-
alization problem is thus the interaction of lexical
ambiguity and the completely free order in the flat
semantics. Once we have chosen between ?
1
and ?
2
in the realization of each edge literal, we have deter-
mined which edges should be part of the prospective
Hamiltonian cycle, and checking whether it really
is one can be done in linear time. If, on the other
hand, the order of the input placed restrictions on
the structure of the derivation tree, we would again
have information that told us when to switch into the
edge eating mode, i.e. which edges should be part
peter likes mary
subj obj
Figure 3: TDG parse tree for ?Peter likes Mary.?
of the cycle. A third source of combinatorics which
does not become so clear in this encoding is the con-
figuration of the elementary trees. Even when we
have committed to the lexical entries, it is conceiv-
able that only one particular way of plugging them
into each other is grammatical.
4 Topological Dependency Grammar
These factors are exactly the same that make depen-
dency parsing for free word order languages diffi-
cult, and it seems worthwhile to see whether op-
timized parsers for dependency grammars can also
contribute to making generation efficient. We now
sketch a dependency formalism which has an effi-
cient parser and then discuss some of the important
properties of this parser. In the next section, we will
see how to employ the parser for generation.
4.1 The Grammar Formalism
The parse trees of topological dependency grammar
(TDG) (Duchier and Debusmann, 2001; Duchier,
2002) are trees whose nodes correspond one-to-one
to the words of the sentence, and whose edges are la-
belled, e.g. with syntactic relations (see Fig. 3). The
trees are unordered, i.e. there is no intrinsic order
among the children of a node. Word order in TDG
is initially completely free, but there is a separate
mechanism to specify constraints on linear prece-
dence. Since completely free order is what we want
for the realization problem, we do not need these
mechanisms and do not go into them here.
The lexicon assigns to each word a set of lexical
entries; in a parse tree, one of these lexical entries
has to be picked for each node. The lexical entry
specifies what labels are allowed on the incoming
edge (the node?s labels) and the outgoing edges (the
node?s valency). Here are some examples:
word labels valency
likes ? {subj, obj, adv?}
Peter {subj, obj} ?
Mary {subj, obj} ?
The lexical entry for ?likes? specifies that the corre-
sponding node does not accept any incoming edges
(and hence must be the root), must have precisely
one subject and one object edge going out, and can
have arbitrarily many outgoing edges with label adv
(indicated by ?). The nodes for ?Peter? and ?Mary?
both require their incoming edge to be labelled with
either subj or obj and neither require nor allow any
outgoing edges.
A well-formed dependency tree for an input sen-
tence is simply a tree with the appropriate nodes,
whose edges obey the labels and valency restric-
tions specified by the lexical entries. So, the tree in
Fig. 3 is well-formed according to our lexicon.
4.2 TDG Parsing
The parsing problem of TDG can be seen as a search
problem: For each node, we must choose a lexi-
cal entry and the correct mother-daughter relations it
participates in. One strength of the TDG approach is
that it is amenable to strong syntactic inferences that
tackle specifically the three sources of complexity
mentioned above.
The parsing algorithm (Duchier, 2002) is stated
in the framework of constraint programming (Koller
and Niehren, 2000), a general approach to coping
with combinatorial problems. Before it explores all
choices that are possible in a certain state of the
search tree (distribution), it first tries to eliminate
some of the choices which definitely cannot lead to a
solution by simple inferences (propagations). ?Sim-
ple? means that propagations take only polynomial
time; the combinatorics is in the distribution steps
alone. That is, it can still happen that a search tree
of exponential size has to be explored, but the time
spent on propagation in each of its node is only poly-
nomial. Strong propagation can reduce the size of
the search tree, and it may even make the whole al-
gorithm run in polynomial time in practice.
The TDG parser translates the parsing prob-
lem into constraints over (variables denoting) fi-
nite sets of integers, as implemented efficiently in
the Mozart programming system (Oz Development
Team, 1999). This translation is complete: Solutions
of the set constraint can be translated back to cor-
rect dependency trees. But for efficiency, the parser
uses additional propagators tailored to the specific
inferences of the dependency problem. For instance,
in the ?Peter likes Mary? example above, one such
propagator could contribute the information that nei-
ther the ?Peter? nor the ?Mary? node can be an adv
child of ?likes?, because neither can accept an adv
edge. Once the choice has been made that ?Peter? is
the subj child of ?likes?, a propagator can contribute
that ?Mary? must be its obj child, as it is the only
possible candidate for the (obligatory) obj child.
Finally, lexical ambiguity is handled by selection
constraints. These constraints restrict which lexical
entry should be picked for a node. When all pos-
sible lexical entries have some information in com-
mon (e.g., that there must be an outgoing subj edge),
this information is automatically lifted to the node
and can be used by the other propagators. Thus it
is sometimes even possible to finish parsing without
committing to single lexical entries for some nodes.
5 Generation as Dependency Parsing
We will now show how TDG parsing can be used to
enumerate all sentences expressing a given input se-
mantics, thereby solving the realization problem in-
troduced in Section 2. We first define the encoding.
Then we give an example and discuss some runtime
results. Finally, we consider a particular restriction
of our encoding and ways of overcoming it.
5.1 The Encoding
Let G be a grammar as described in Section 2;
i.e. lexical entries are of the form (?, T ), where
? is a flat semantics and T is a TAG elementary
tree whose nodes are decorated with semantic in-
dices. We make the following simplifying assump-
tions. First, we assume that the nodes of the elemen-
tary trees of G are not labelled with feature struc-
tures. Next, we assume that whenever we can adjoin
an auxiliary tree at a node, we can adjoin arbitrarily
many trees at this node. The idea of multiple adjunc-
tion is not new (Schabes and Shieber, 1994), but it
is simplified here because we disregard complex ad-
junction constraints. We will discuss these two re-
strictions in the conclusion. Finally, we assume that
every lexical semantics ? has precisely one member;
this restriction will be lifted in Section 5.4.
Now let?s say we want to find the realizations of
the input semantics S = {?
1
, . . . , ?
n
}, using the
grammar G. The input ?sentence? of the parsing
start mary buy car indef red
sub
st NP,
m,1
substS,e,1
substN,c,1
substNP,c,1
adjN,c
Figure 4: Dependency tree for ?Mary buys a red
car.?
problem we construct is the sequence {start} ? S,
where start is a special start symbol. The parse
tree will correspond very closely to a TAG deriva-
tion tree, its nodes standing for the instantiated ele-
mentary trees that are used in the derivation.
To this end, we use two types of edge labels ?
substitution and adjunction labels. An edge with a
substitution label subst
A,i,p
from the node ? to the
node ? (both of which stand for elementary trees)
indicates that ? should be plugged into the p-th sub-
stitution node in ? that has label A and index i. We
write subst(A) for the maximum number of occur-
rences of A as the label of substitution nodes in any
elementary tree of G; this is the maximum value that
p can take.
An edge with an adjunction label adj
A,i
from ? to
? specifies that ? is adjoined at some node within ?
carrying label A and index i and admitting adjunc-
tion. It does not matter for our purposes to which
node in ? ? is adjoined exactly; the choice cannot af-
fect grammaticality because there is no feature uni-
fication involved.
The dependency grammar encodes how an ele-
mentary tree can be used in a TAG derivation by
restricting the labels of the incoming and outgoing
edges via labels and valency requirements in the lex-
icon. Let?s say that T is an elementary tree of G
which has been matched with the input atom ?
r
, in-
stantiating its index variables. Let A be the label
and i the index of the root of T . If T is an auxiliary
tree, it accepts incoming adjunction edges for A and
i, i.e. it gets the labels value {adj
A,i
}. If T is an
initial tree, it will accept arbitrary incoming substi-
tution edges for A and i, i.e. its labels value is
{subst
A,i,p
| 1 ? p ? subst(A)}
In either case, T will require precisely one out-
going substitution edge for each of its substitution
nodes, and it will allow arbitrary numbers of outgo-
ing adjunction edges for each node where we can
adjoin. That is, the valency value is as follows:
{subst
A,i,p
| ex. substitution node N in T s.t. A
is label, i is index of N , and N is
pth substitution node for A:i in T}
? {adj
A,i
? | ex. node with label A, index i
in T which admits adjunction}
We obtain the set of all lexicon entries for the
atom ?
r
by encoding all TAG lexicon entries which
match ?
r
as just specified. The start symbol, start,
gets a special lexicon entry: Its labels entry is the
empty set (i.e. it must be the root of the tree), and its
valency entry is the set {subst
S,k,1
}, where k is the
semantic index with which generation should start.
5.2 An Example
Now let us go through an example to make these def-
initions a bit clearer. Let?s say we want to verbalize
the semantics
{name(m, mary), buy(e,m, c),
car(c), indef(c), red(c)}
The LTAG grammar we use contains the elemen-
tary trees which are used in the tree in Fig. 5, along
with the obvious semantics; we want to generate a
sentence starting with the main event e. The encod-
ing produces the following dependency grammar;
the entries in the ?atom? column are to be read as
abbreviations of the actual atoms in the input seman-
tics.
atom labels valency
start ? {subst
S,e,1
}
buy {subst
S,e,1
} {subst
NP,c,1
, subst
NP,m,1
,
adj
V P,e
?, adj
V,e
?}
mary {subst
NP,m,1
, {adj
NP,1
?, adj
PN,m
?}
subst
NP,m,2
}
indef {subst
NP,c,1
, {adj
NP,c
?}
subst
NP,c,2
}
car {subst
N,c,1
} {adj
N,c
?}
red {adj
N,c
} ?
If we parse the ?sentence?
start mary buy car indef red
with this grammar, leaving the word order com-
pletely open, we obtain precisely one parse tree,
shown in Fig. 4. Reading this parse as a TAG
derivation tree, we can reconstruct the derived tree
in Fig. 5, which indeed produces the string ?Mary
buys a red car?.
S:e
NP:m 
NP:m
PN:m
Mary
VP:e
V:e
buys
NP:c 
NP:c
Detnoad j
a
N:c 
N:c
Adjnoad j
red
N:c
N:c
car
Figure 5: Derived tree for ?Mary buys a red car.?
5.3 Implementation and Experiments
The overall realization algorithm we propose en-
codes the input problem as a DG parsing problem
and then runs the parser described in Section 4.2,
which is freely available over the Web, as a black
box. Because the information lifted to the nodes by
the selection constraints may be strong enough to
compute the parse tree without ever committing to
unique lexical entries, the complete parse may still
contain some lexical ambiguity. This is no problem,
however, because the absence of features guarantees
that every combination of choices will be grammat-
ical. Similarly, a node can have multiple children
over adjunction edges with the same label, and there
may be more than one node in the upper elemen-
tary tree to which the lower tree could be adjoined.
Again, all remaining combinations are guaranteed to
be grammatical.
In order to get an idea of the performance of
our realization algorithm in comparison to the state
of the art, we have tried generating the following
sentences, which are examples from (Carroll et al,
1999):
(1) The manager in that office interviewed a new
consultant from Germany.
(2) Our manager organized an unusual additional
weekly departmental conference.
We have converted the XTAG grammar (XTAG
Research Group, 2001) into our grammar format,
automatically adding indices to the nodes of the el-
ementary trees, removing features, simplifying ad-
junction constraints, and adding artificial lexical se-
mantics that consists of the words at the lexical an-
chors and the indices used in the respective trees.
XTAG typically assigns quite a few elementary trees
to one lemma, and the same lexical semantics can of-
ten be verbalized by more than hundred elementary
trees in the converted grammar. It turns out that the
dependency parser scales very nicely to this degree
of lexical ambiguity: The sentence (1) is generated
in 470 milliseconds (as opposed to Carroll et al?s 1.8
seconds), whereas we generate (2) in about 170 mil-
liseconds (as opposed to 4.3 seconds).1 Although
these numbers are by no means a serious evaluation
of our system?s performance, they do present a first
proof of concept for our approach.
The most encouraging aspect of these results is
that despite the increased lexical ambiguity, the
parser gets by without ever making any wrong
choices, which means that it runs in polynomial
time, on all examples we have tried. This is possible
because on the one hand, the selection constraint au-
tomatically compresses the many different elemen-
tary trees that XTAG assigns to one lemma into very
few classes. On the other hand, the propagation that
rules out impossible edges is so strong that the free
input order does not make the configuration prob-
lem much harder in practice. Finally, our treatment
of modification allows us to multiply out the possi-
ble permutations in a postprocessing step, after the
parser has done the hard work. A particularly strik-
ing example is (2), where the parser gives us a single
solution, which multiplies out to 312 = 13 ? 4! dif-
ferent realizations. (The 13 basic realizations corre-
spond to different syntactic frames for the main verb
in the XTAG grammar, e.g. for topicalized or pas-
sive constructions.)
5.4 More Complex Semantics
So far, we have only considered TAG grammars in
which each elementary tree is assigned a semantics
that contains precisely one atom. However, there
are cases where an elementary tree either has an
empty semantics, or a semantics that contains mul-
tiple atoms. The first case can be avoided by ex-
ploiting TAG?s extended domain of locality, see e.g.
(Gardent and Thater, 2001).
The simplest possible way for dealing with the
second case is to preprocess the input into several
1A newer version of Carroll et al?s system generates (1) in
420 milliseconds (Copestake, p.c.). Our times were measured
on a 700 MHz Pentium-III PC.
different parsing problems. In a first step, we collect
all possible instantiations of LTAG lexical entries
matching subsets of the semantics. Then we con-
struct all partitions of the input semantics in which
each block in the partition is covered by a lexical en-
try, and build a parsing problem in which each block
is one symbol in the input to the parser.
This seems to work quite well in practice, as there
are usually not many possible partitions. In the worst
case, however, this approach produces an exponen-
tial number of parsing problems. Indeed, using a
variant of the grammar from Section 3, it is easy
to show that the problem of deciding whether there
is a partition whose parsing problem can be solved
is NP-complete as well. An alternative approach is
to push the partitioning process into the parser as
well. We expect this will not hurt the runtime all
that much, but the exact effect remains to be seen.
6 Comparison to Other Approaches
The perspective on realization that our system takes
is quite different from previous approaches. In this
section, we relate it to chart generation (Kay, 1996;
Carroll et al, 1999) and to another constraint-based
approach (Gardent and Thater, 2001).
In chart based approaches to realization, the main
idea is to minimize the necessary computation by
reusing partial results that have been computed be-
fore. In the setting of fixed word order parsing, this
brings an immense increase in efficiency. In genera-
tion, however, the NP-completeness manifests itself
in charts of worst-case exponential size. In addition,
it can happen that substructures are built which are
not used in the final realization, especially when pro-
cessing modifications.
By contrast, our system configures nodes into a
dependency tree. It solves a search problem, made
up by choices for mother-daughter relations in the
tree. Propagation, which runs in polynomial time,
has access to global information (illustrated in Sec-
tion 4.2) and can thus rule out impossible mother-
daughter relations efficiently; every propagation step
that takes place actually contributes to zooming in
on the possible realizations. Our system can show
exponential runtimes when the distributions span a
search tree of exponential size.
Gardent and Thater (2001) also propose a con-
straint based approach to generation working with
a variant of TAG. However, the performance of their
system decreases rapidly as the input gets larger
even when when working with a toy grammar. The
main difference between their approach and ours
seems to be that their algorithm tries to construct
a derived tree, while ours builds a derivation tree.
Our parser only has to deal with information that
is essential to solve the combinatorial problem, and
not e.g. with the internal structure of the elementary
trees. The reconstruction of the derived tree, which
is cheap once the derivation tree has been computed,
is delegated to a post-processing step. Working with
derived trees, Gardent and Thater (2001) cannot ig-
nore any information and have to keep track of the
relationships between nodes at points where they are
not relevant.
7 Conclusion
Generation from flat semantics is an NP-complete
problem. In this paper, we have first given an al-
ternative proof for this fact, which works even for
a fixed grammar and makes the connection to the
complexity of free word order parsing clearly visi-
ble. Then we have shown how to translate the re-
alization problem of TAG into parsing problems of
topological dependency grammar, and argued how
the optimizations in the dependency parser ? which
were originally developed for free word order pars-
ing ? help reduce the runtime for the generation sys-
tem. This reduction shows in passing that the pars-
ing problem for TDG is NP-complete as well, which
has been conjectured, but never proved.
The NP-completeness result for the realization
problem explains immediately why all existing com-
plete generation algorithms have exponential run-
times in the worst case. As our proof shows, the
main sources of the combinatorics are the interac-
tion of lexical ambiguity and tree configuration with
the completely unordered nature of the input. Mod-
ification is important and deserves careful treatment
(and indeed, our system deals very gracefully with
it), but it is not as intrinsically important as some
of the literature suggests; our proof gets by without
modification. If we allow the grammar to be part
of the input, we can even modify the proof to show
NP-hardness of the case where semantic atoms can
be verbalized more often than they appear in the in-
put, and of the case where they can be verbalized
less often. The case where every atom can be used
arbitrarily often remains open.
By using techniques from constraint program-
ming, the dependency parser seems to cope rather
well with the combinatorics of generation. Propaga-
tors can rule out impossible local structures on the
grounds of global information, and selection con-
straints greatly alleviate the proliferation of lexical
ambiguity in large TAG grammars by making shared
information available without having to commit to
specific lexical entries. Initial experiments with the
XTAG grammar indicate that we can generate prac-
tical examples in polynomial time, and may be com-
petitive with state-of-the-art realization systems in
terms of raw runtime.
In the future, it will first of all be necessary to lift
the restrictions we have placed on the TAG gram-
mar: So far, the nodes of the elementary trees are
only equipped with nonterminal labels and indices,
not with general feature structures, and we allow
only a restricted form of adjunction constraints. It
should be possible to either encode these construc-
tions directly in the dependency grammar (which al-
lows user-defined features too), or filter out wrong
realizations in a post-processing step. The effect of
such extensions on the runtime remains to be seen.
Finally, we expect that despite the general NP-
completeness, there are restricted generation prob-
lems which can be solved in polynomial time, but
still contain all problems that actually arise for nat-
ural language. The results of this paper open up a
new perspective from which such restrictions can be
sought, especially considering that all the natural-
language examples we tried are indeed processed
in polynomial time. Such a polynomial realiza-
tion algorithm would be the ideal starting point
for algorithms that compute not just any, but the
best possible realization ? a problem which e.g.
Bangalore and Rambow (2000) approximate using
stochastic methods.
Acknowledgments. We are grateful to Tilman
Becker, Chris Brew, Ann Copestake, Ralph Debus-
mann, Gerald Penn, Stefan Thater, and our reviewers
for helpful comments and discussions.
References
Srinivas Bangalore and Owen Rambow. 2000. Using
tags, a tree model, and a language model for genera-
tion. In Proc. of the TAG+5 Workshop, Paris.
G. Edward Barton, Robert C. Berwick, and Eric Sven
Ristad. 1987. Computational Complexity and Natu-
ral Language. MIT Press, Cambridge, Mass.
Chris Brew. 1992. Letting the cat out of the bag: Gen-
eration for Shake-and-Bake MT. In Proceedings of
COLING-92, pages 610?616, Nantes.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An efficient chart generator for
(semi-)lexicalist grammars. In Proceedings of the 7th
European Workshop on NLG, pages 86?95, Toulouse.
Denys Duchier and Ralph Debusmann. 2001. Topolog-
ical dependency trees: A constraint-based account of
linear precedence. In Proceedings of the 39th ACL,
Toulouse, France.
Denys Duchier. 2002. Configuration of labeled trees un-
der lexicalized constraints and principles. Journal of
Language and Computation. To appear.
Claire Gardent and Stefan Thater. 2001. Generating with
a grammar based on tree descriptions: A constraint-
based approach. In Proceedings of the 39th ACL,
Toulouse.
Aravind Joshi and Yves Schabes. 1997. Tree-Adjoining
Grammars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, chapter 2, pages 69?
123. Springer-Verlag, Berlin.
Martin Kay. 1996. Chart generation. In Proceedings of
the 34th Annual Meeting of the ACL, pages 200?204,
Santa Cruz.
Alexander Koller and Joachim Niehren. 2000. Con-
straint programming in computational linguistics. To
appear in Proceedings of LLC8, CSLI Press.
Oz Development Team. 1999. The Mozart Programming
System web pages. http://www.mozart-oz.
org/.
Yves Schabes and Stuart Shieber. 1994. An alterna-
tive conception of tree-adjoining derivation. Compu-
tational Linguistics, 20(1):91?124.
Matthew Stone and Christy Doran. 1997. Sentence plan-
ning as description using tree-adjoining grammar. In
Proceedings of the 35th ACL, pages 198?205.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for english. Technical Report IRCS-01-
03, IRCS, University of Pennsylvania.
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 301?304,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Validating the web-based evaluation of NLG systems
Alexander Koller
Saarland U.
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Donna Byron
Northeastern U.
dbyron@ccs.neu.edu
Justine Cassell
Northwestern U.
justine@northwestern.edu
Robert Dale
Macquarie U.
Robert.Dale@mq.edu.au
Sara Dalzel-Job
U. of Edinburgh
S.Dalzel-Job@sms.ed.ac.uk
Jon Oberlander
U. of Edinburgh
Johanna Moore
U. of Edinburgh
{J.Oberlander|J.Moore}@ed.ac.uk
Abstract
The GIVE Challenge is a recent shared
task in which NLG systems are evaluated
over the Internet. In this paper, we validate
this novel NLG evaluation methodology by
comparing the Internet-based results with
results we collected in a lab experiment.
We find that the results delivered by both
methods are consistent, but the Internet-
based approach offers the statistical power
necessary for more fine-grained evaluations
and is cheaper to carry out.
1 Introduction
Recently, there has been an increased interest in
evaluating and comparing natural language gener-
ation (NLG) systems on shared tasks (Belz, 2009;
Dale and White, 2007; Gatt et al, 2008). However,
this is a notoriously hard problem (Scott and Moore,
2007): Task-based evaluations with human experi-
mental subjects are time-consuming and expensive,
and corpus-based evaluations of NLG systems are
problematic because a mismatch between human-
generated output and system-generated output does
not necessarily mean that the system?s output is
inferior (Belz and Gatt, 2008). This lack of evalua-
tion methods which are both effective and efficient
is a serious obstacle to progress in NLG research.
The GIVE Challenge (Byron et al, 2009) is a
recent shared task which takes a third approach to
NLG evaluation: By connecting NLG systems to
experimental subjects over the Internet, it achieves
a true task-based evaluation at a much lower cost.
Indeed, the first GIVE Challenge acquired data
from over 1100 experimental subjects online. How-
ever, it still remains to be shown that the results
that can be obtained in this way are in fact com-
parable to more established task-based evaluation
efforts, which are based on a carefully selected sub-
ject pool and carried out in a controlled laboratory
environment. By accepting connections from arbi-
trary subjects over the Internet, the evaluator gives
up control over the subjects? behavior, level of lan-
guage proficiency, cooperativeness, etc.; there is
also an issue of whether demographic factors such
as gender might skew the results.
In this paper, we provide the missing link by
repeating the GIVE evaluation in a laboratory en-
vironment and comparing the results. It turns out
that where the two experiments both find a signif-
icant difference between two NLG systems with
respect to a given evaluation measure, they always
agree. However, the Internet-based experiment
finds considerably more such differences, perhaps
because of the higher number of experimental sub-
jects (n = 374 vs. n = 91), and offers other oppor-
tunities for more fine-grained analysis as well. We
take this as an empirical validation of the Internet-
based evaluation of GIVE, and propose that it can
be applied to NLG more generally. Our findings
are in line with studies from psychology that indi-
cate that the results of web-based experiments are
typically consistent with the results of traditional
experiments (Gosling et al, 2004). Nevertheless,
we do find and discuss some effects of the uncon-
trolled subject pool that should be addressed in
future Internet-based NLG challenges.
2 The GIVE Challenge
In the GIVE scenario (Byron et al, 2009), users
try to solve a treasure hunt in a virtual 3D world
that they have not seen before. The computer has
complete information about the virtual world. The
challenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
From the perspective of the users, GIVE con-
sists in playing a 3D game which they start from
a website. The game displays a virtual world and
allows the user to move around in the world and
manipulate objects; it also displays the generated
301
instructions. The first room in each game is a tuto-
rial room in which users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Players
can either finish a game successfully, lose it by
triggering an alarm, or cancel the game at any time.
When a user starts the game, they are randomly
connected to one of the three worlds and one of the
NLG systems. The GIVE-1 Challenge evaluated
five NLG systems, which we abbreviate as A, M,
T, U, and W below. A running GIVE NLG system
has access to the current state of the world and to
an automatically computed plan that tells it what
actions the user should perform to solve the task. It
is notified whenever the user performs some action,
and can generate an instruction and send it to the
client for display at any time.
3 The experiments
The web experiment. For the GIVE-1 challenge,
1143 valid games were collected over the Internet
over the course of three months. These were dis-
tributed over three evaluation worlds (World 1: 374,
World 2: 369, World 3: 400). A game was consid-
ered valid if the game client didn?t crash, the game
wasn?t marked as a test run by the developers, and
the player completed the tutorial.
Of these games, 80% were played by males and
10% by females (the remaining 10% of the partic-
ipants did not specify their gender). The players
were widely distributed over countries: 37% con-
nected from IP addresses in the US, 33% from
Germany, and 17% from China; the rest connected
from 45 further countries. About 34% of the par-
ticipants self-reported as native English speakers,
and 62% specified a language proficiency level of
at least ?expert? (3 on a 5-point scale).
The lab experiment. We repeated the GIVE-1
evaluation in a traditional laboratory setting with
91 participants recruited from a college campus.
In the lab, each participant played the GIVE game
once with each of the five NLG systems. To avoid
learning effects, we only used the first game run
from each subject in the comparison with the web
experiment; as a consequence, subjects were dis-
tributed evenly over the NLG systems. To accom-
modate for the much lower number of participants,
the laboratory experiment only used a single game
world ? World 1, which was known from the online
version to be the easiest world.
Among this group of subjects, 93% self-rated
their English proficiency as ?expert? or better; 81%
were native speakers. In contrast to the online ex-
periment, 31% of participants were male and 65%
were female (4% did not specify their gender).
Results: Objective measures. The GIVE soft-
ware automatically recorded data for five objec-
tive measures: the percentage of successfully com-
pleted games and, for the successfully completed
games, the number of instructions generated by
the NLG system, of actions performed by the user
(such as pushing buttons), of steps taken by the
user (i.e., actions plus movements), and the task
completion time (in seconds).
Fig. 1 shows the results for the objective mea-
sures collected in both experiments. To make the
results comparable, the table for the Internet ex-
periment only includes data for World 1. The task
success rate is only evaluated on games that were
completed successfully or lost, not cancelled, as
laboratory subjects were asked not to cancel. This
brings the number of Internet subjects to 322 for
the success rate, and to 227 (only successful games)
for the other measures.
Task success is the percentage of successfully
completed games; the other measures are reported
as means. The chart assigns systems to groups A
through C or D for each evaluation measure. Sys-
tems in group A are better than systems in group
B, and so on; if two systems have no letter in com-
mon, the difference between them is significant
with p < 0.05. Significance was tested using a ?
2
-
test for task success and ANOVAs for instructions,
steps, actions, and seconds. These were followed
by post hoc tests (pairwise ?
2
and Tukey) to com-
pare the NLG systems pairwise.
Results: Subjective measures. Users were
asked to fill in a questionnaire collecting subjec-
tive ratings of various aspects of the instructions.
For example, users were asked to rate the overall
quality of the direction giving system (on a 7-point
scale), the choice of words and the referring ex-
pressions (on 5-point scales), and they were asked
whether they thought the instructions came at the
right time. Overall, there were twelve subjective
measures (see (Byron et al, 2009)), of which we
only present four typical ones for space reasons.
For each question, the user could choose not to
answer. On the Internet, subjects made consider-
able use of this option: for instance, 32% of users
302
Objective Measures Subjective Measures
task
success
instructions steps actions seconds overall
choice
of words
referring
expressions
timing
A 91% A 83.4 B 99.8 A 9.4 A 123.9 A 4.7 A 4.7 A 4.7 A 81% A
M 76% B 68.1 A 145.1 B 10.0 AB 195.4 BC 3.8 AB 3.8 B 4.0 B 70% ABC
T 85% AB 97.8 C 142.1 B 9.7 AB 174.4 B 4.4 B 4.4 AB 4.3 AB 73% AB
U 93% AB 99.8 C 142.6 B 10.3 B 194.0 BC 4.0 B 4.0 B 4.0 B 51% C
W 24% C 159.7 D 256.0 C 9.6 AB 234.1 C 3.8 AB 3.8 B 4.2 AB 50% BC
A 100% A 78.2 AB 93.4 A 9.9 A 143.9 A 5.7 A 4.7 A 4.8 A 92% A B
M 95% A 66.3 A 141.8 B 10.5 A 211.8 B 5.4 A 3.8 B 4.3 A 95% A B
T 93% A 107.2 CD 134.6 B 9.6 A 205.6 B 4.9 A 4.5 A B 4.4 A 64% A B
U 100% A 88.8 BC 128.8 B 9.8 A 195.1 AB 5.7 A 4.7 A 4.3 A 100% A
W 17% B 134.5 D 213.5 C 10.0 A 252.5 B 5.0 A 4.5 A B 4.0 A 100% B
Figure 1: Objective and selected subjective measures on the web (top) and in the lab (bottom).
didn?t fill in the ?overall evaluation? field of the
questionnaire. In the laboratory experiment, the
subjects were asked to fill in the complete question-
naire and the response rate is close to 100%.
The results for the four selected subjective mea-
sures are summarized in Fig. 1 in the same way as
the objective measures. Also as above, the table
is based only on successfully completed games in
World 1. We will justify this latter choice below.
4 Discussion
The primary question that interests us in a compar-
ative evaluation is which NLG systems performed
significantly better or worse on any given evalua-
tion measure. In the experiments above, we find
that of the 170 possible significant differences (=
17 measures ? 10 pairs of NLG systems), the labo-
ratory experiment only found six that the Internet-
based experiment didn?t find. Conversely, there
are 26 significant differences that only the Internet-
based experiment found. But even more impor-
tantly, all pairwise rankings are consistent across
the two evaluations: Where both systems found a
significant difference between two systems, they al-
ways ranked them in the same order. We conclude
that the Internet experiment provides significance
judgments that are comparable to, and in fact more
precise than, the laboratory experiment.
Nevertheless, there are important differences be-
tween the laboratory and Internet-based results. For
instance, the success rates in the laboratory tend
to be higher, but so are the completion times. We
believe that these differences can be attributed to
the demographic characteristics of the participants.
To substantiate this claim, we looked in some detail
at differences in gender, language proficiency, and
questionnaire response rates.
First, the gender distribution differed greatly be-
Web
games reported mean
success 227 = 61% 93% 4.9
lost 92 = 24% 48% 3.4
cancelled 55 = 15% 16% 3.3
Lab
# games reported mean
success 73 = 80% 100% 5.4
lost 18 = 20% 94% 3.3
cancelled 0 ? ?
Figure 2: Skewed results for ?overall evaluation?.
tween the Internet experiment (10% female) and
the laboratory experiment (65% female). This is
relevant because gender had a significant effect
on task completion time (women took longer) and
on six subjective measures including ?overall eval-
uation? in the laboratory. We speculate that the
difference in task completion time may be related
to well-known gender differences in processing
navigation instructions (Moffat et al, 1998).
Second, the two experiments collected data from
subjects of different language proficiencies. While
93% of the participants in the laboratory experi-
ment self-rated their English proficiency as ?expert?
or better, only 62% of the Internet participants did.
This partially explains the lower task success rates
on the Internet, as Internet subjects with English
proficiencies of 3?5 performed significantly better
on ?task success? than the group with proficiencies
1?2. If we only look at the results of high-English-
proficiency subjects on the Internet, the success
rates for all NLG systems except W rise to at least
86%, and are thus close to the laboratory results.
Finally, the Internet data are skewed by the ten-
dency of unsuccessful participants to not fill in the
questionnaire. Fig. 2 summarizes some data about
the ?overall evaluation? question. Users who didn?t
complete the task successfully tended to judge the
303
systems much lower than successful users, but at
the same time tended not to answer the question
at all. This skew causes the mean subjective judg-
ments across all Internet subjects to be artificially
high. To avoid differences between the laboratory
and the Internet experiment due to this skew, Fig. 1
includes only judgments from successful games.
In summary, we find that while the two experi-
ments made consistent significance judgments, and
the Internet-based evaluation methodology thus
produces meaningful results, the absolute values
they find for the individual evaluation measures
differ due to the demographic characteristics of the
participants in the two studies. This could be taken
as a possible deficit of the Internet-based evalua-
tion. However, we believe that the opposite is true.
In many ways, an online user is in a much more
natural communicative situation than a laboratory
subject who is being discouraged from cancelling
a frustrating task. In addition, every experiment ?
whether in the laboratory or on the Internet ? suf-
fers from some skew in the subject population due
to sampling bias; for instance, one could argue that
an evaluation that is based almost exclusively on na-
tive speakers in universities leads to overly benign
judgments about the quality of NLG systems.
One advantage of the Internet-based approach
to data collection over the laboratory-based one is
that, due to the sheer number of subjects, we can de-
tect such skews and deal with them appropriately.
For instance, we might decide that we are only
interested in the results from proficient English
speakers and ignore the rest of the data; but we
retain the option to run the analysis over all partici-
pants, and to analyze how much each system relies
on the user?s language proficiency. The amount
of data also means that we can obtain much more
fine-grained comparisons between NLG systems.
For instance, the second and third evaluation world
specifically exercised an NLG system?s abilities to
generate referring expressions and navigation in-
structions, respectively, and there were significant
differences in the performance of some systems
across different worlds. Such data, which is highly
valuable for pinpointing specific weaknesses of a
system, would have been prohibitively costly and
time-consuming to collect with laboratory subjects.
5 Conclusion
In this paper, we have argued that carrying out task-
based evaluations of NLG systems over the Internet
is a valid alternative to more traditional laboratory-
based evaluations. Specifically, we have shown
that an Internet-based evaluation of systems in the
GIVE Challenge finds consistent significant differ-
ences as a lab-based evaluation. While the Internet-
based evaluation suffers from certain skews caused
by the lack of control over the subject pool, it does
find more differences than the lab-based evaluation
because much more data is available. The increased
amount of data also makes it possible to compare
the quality of NLG systems across different evalua-
tion worlds and users? language proficiency levels.
We believe that this type of evaluation effort
can be applied to other NLG and dialogue tasks
beyond GIVE. Nevertheless, our results also show
that an Internet-based evaluation risks certain kinds
of skew in the data. It is an interesting question for
the future how this skew can be reduced.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
A. Belz. 2009. That?s nice ... what can you do with it?
Computational Linguistics, 35(1):111?118.
D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2009. Report on the
First NLG Challenge on Generating Instructions in
Virtual Environments (GIVE). In Proceedings of the
12th European Workshop on Natural Language Gen-
eration (Special session on Generation Challenges).
R. Dale and M. White, editors. 2007. Proceedings
of the NSF/SIGGEN Workshop for Shared Tasks and
Comparative Evaluation in NLG, Arlington, VA.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA
challenge 2008: Overview and evaluation results.
In Proceedings of the 5th International Natural
Language Generation Conference (INLG?08), pages
198?206.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
S. Moffat, E. Hampson, and M. Hatzipantelis. 1998.
Navigation in a ?virtual? maze: Sex differences and
correlation with psychometric measures of spatial
ability in humans. Evolution and Human Behavior,
19(2):73?87.
D. Scott and J. Moore. 2007. An NLG evaluation com-
petition? Eight reasons to be cautious. In (Dale and
White, 2007).
304
Proceedings of the 12th European Workshop on Natural Language Generation, pages 165?173,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Report on the First NLG Challenge on
Generating Instructions in Virtual Environments (GIVE)
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the first installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE), a new
shared task for the NLG community. We
motivate the design of the challenge, de-
scribe how we carried it out, and discuss
the results of the system evaluation.
1 Introduction
This paper reports on the methodology and results
of the First Challenge on Generating Instructions
in Virtual Environments (GIVE-1), which we ran
from March 2008 to February 2009. GIVE is a
new shared task for the NLG community. It pro-
vides an end-to-end evaluation methodology for
NLG systems that generate instructions which are
meant to help a user solve a treasure-hunt task in a
virtual 3D world. The most innovative aspect from
an NLG evaluation perspective is that the NLG
system and the user are connected over the Inter-
net. This makes it possible to cheaply collect large
amounts of evaluation data.
Five NLG systems were evaluated in GIVE-
1 over a period of three months from November
2008 to February 2009. During this time, we
collected 1143 games that were played by users
from 48 countries. As far as we know, this makes
GIVE-1 the largest evaluation effort in terms of
experimental subjects ever. We have evaluated the
five systems both on objective measures (success
rate, completion time, etc.) and subjective mea-
sures which were collected by asking the users to
fill in a questionnaire.
GIVE-1 was intended as a pilot experiment in
order to establish the validity of the evaluation
methodology and understand the challenges in-
volved in the instruction-giving task. We believe
that we have achieved these purposes. At the same
time, we provide evaluation results for the five
NLG systems which will help their developers im-
prove them for participation in a future challenge,
GIVE-2. GIVE-2 will retain the successful aspects
of GIVE-1, while refining the task to emphasize
aspects that we found to be challenging. We invite
the ENLG community to participate in designing
GIVE-2.
Plan of the paper. The paper is structured as
follows. In Section 2, we will describe and moti-
vate the GIVE Challenge. In Section 3, we will
then describe the evaluation method and infras-
tructure for the challenge. Section 4 reports on
the evaluation results. Finally, we conclude and
discuss future work in Section 5.
2 The GIVE Challenge
In the GIVE scenario, subjects try to solve a trea-
sure hunt in a virtual 3D world that they have not
seen before. The computer has a complete sym-
bolic representation of the virtual world. The chal-
lenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
Complete maps of the game worlds used in the
evaluation are shown in Figs. 3?5: In these worlds,
players must pick up a trophy, which is in a wall
safe behind a picture. In order to access the tro-
165
Figure 1: What the user sees when playing with
the GIVE Challenge.
phy, they must first push a button to move the pic-
ture to the side, and then push another sequence of
buttons to open the safe. One floor tile is alarmed,
and players lose the game if they step on this tile
without deactivating the alarm first. There are also
a number of distractor buttons which either do
nothing when pressed or set off an alarm. These
distractor buttons are intended to make the game
harder and, more importantly, to require appropri-
ate reference to objects in the game world. Finally,
game worlds contained a number of objects such
as chairs and flowers that did not bear on the task,
but were available for use as landmarks in spatial
descriptions generated by the NLG systems.
2.1 Why a new NLG evaluation paradigm?
The GIVE Challenge addresses a need for a new
evaluation paradigm for natural language gener-
ation (NLG). NLG systems are notoriously hard
to evaluate. On the one hand, simply compar-
ing system outputs to a gold standard using auto-
matic comparison algorithms has limited value be-
cause there can be multiple generated outputs that
are equally good. Finding metrics that account
for this variability and produce results consistent
with human judgments and task performance mea-
sures is difficult (Belz and Gatt, 2008; Stent et
al., 2005; Foster, 2008). Human assessments of
system outputs are preferred, but lab-based eval-
uations that allow human subjects to assess each
aspect of the system?s functionality are expensive
and time-consuming, thereby favoring larger labs
with adequate resources to conduct human sub-
jects studies. Human assessment studies are also
difficult to replicate across sites, so system devel-
opers that are geographically separated find it dif-
ficult to compare different approaches to the same
problem, which in turn leads to an overall diffi-
culty in measuring progress in the field.
The GIVE-1 evaluation was conducted via a
client/server architecture which allows any user
with an Internet connection to provide system
evaluation data. Internet-based studies have been
shown to provide generous amounts of data in
other areas of AI (von Ahn and Dabbish, 2004;
Orkin and Roy, 2007). Our implementation allows
smaller teams to develop a system that will partici-
pate in the challenge, without taking on the burden
of running the human evaluation experiment, and
it provides a direct comparison of all participating
systems on the same evaluation data.
2.2 Why study instruction-giving?
Next to the Internet-based data collection method,
GIVE also differs from other NLG challenges by
its emphasis on generating instructions in a vir-
tual environment and in real time. This focus on
instruction giving is motivated by a growing in-
terest in dialogue-based agents for situated tasks
such as navigation and 3D animations. Due to its
appeal to younger students, the task can also be
used as a pedagogical exercise to stimulate interest
among secondary-school students in the research
challenges found in NLG or Computational Lin-
guistics more broadly.
Embedding the NLG task in a virtual world en-
courages the participating research teams to con-
sider communication in a situated setting. This
makes the NLG task quite different than in other
NLG challenges. For example, experiments have
shown that human instruction givers make the in-
struction follower move to a different location in
order to use a simpler referring expression (RE)
(Stoia et al, 2006). That is, RE generation be-
comes a very different problem than the classi-
cal non-situated Dale & Reiter style RE genera-
tion, which focuses on generating REs that are sin-
gle noun phrases in the context of an unchanging
world.
On the other hand, because the virtual environ-
ments scenario is so open-ended, it ? and specif-
ically the instruction-giving task ? can potentially
be of interest to a wide range of NLG researchers.
This is most obvious for research in sentence plan-
ning (GRE, aggregation, lexical choice) and real-
ization (the real-time nature of the task imposes
high demands on the system?s efficiency). But if
166
extended to two-way dialog, the task can also in-
volve issues of prosody generation (i.e., research
on text/concept-to-speech generation), discourse
generation, and human-robot interaction. Finally,
the game world can be scaled to focus on specific
issues in NLG, such as the generation of REs or
the generation of navigation instructions.
3 Evaluation Method and Logistics
Now we describe the method we applied to obtain
experimental data, and sketch the software infras-
tructure we developed for this purpose.
3.1 Software architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components (shown in Fig. 2):
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide. When a user starts the
client, it connects to the Matchmaker and is ran-
domly assigned an NLG server and a game world.
The client and NLG server then communicate over
the course of one game. At the end of the game,
the client displays a questionnaire to the user, and
the game log and questionnaire data are uploaded
to the Matchmaker and stored in a database. Note
that this division allows the challenge to be con-
ducted without making any assumptions about the
internal structure of an NLG system.
The GIVE software is implemented in Java and
available as an open-source Google Code project.
For more details about the software, see (Koller et
al., 2009).
3.2 Subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the internet.
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 2: The GIVE architecture.
Collecting data from anonymous users over the
Internet presents a variety of issues that a lab-
based experiment does not. An Internet-based
evaluation skews the demographic of the subject
pool toward people who use the Internet, but prob-
ably no more so than if recruiting on a college
campus. More worrisome is that, without a face-
to-face meeting, the researcher has less confidence
in the veracity of self-reported demographic data
collected from the subject. For the purposes of
NLG software, the most important demographic
question is the subject?s fluency in English. Play-
ers of the GIVE 2009 challenge were asked to self-
report their command of English, age, and com-
puter experience. English proficiency did interact
with task completion, which leads us to conclude
that users were honest about their level of English
proficiency. See section 4.4 below for a discus-
sion of this interaction. All-in-all, we feel that the
advantage gained from the large increase in the
size of the subject pool offsets any disadvantage
accrued from the lack of accurate demographic in-
formation.
3.3 Materials
Figs. 3?5 show the layout of the three evaluation
worlds. The worlds were intended to provide vary-
ing levels of difficulty for the direction-giving sys-
tems and to focus on different aspects of the prob-
lem. World 1 is very similar to the development
world that the research teams were given to test
their system on. World 2 was intended to focus
on object descriptions - the world has only one
room which is full of objects and buttons, many of
which cannot be distinguished by simple descrip-
tions. World 3, on the other hand, puts more em-
phasis on navigation directions as the world has
many interconnected rooms and hallways.
The difference between the worlds clearly bears
out in the task completion rates reported below.
167
plant
chair
alarm
lamp
tutorial room
couch
safe
Figure 3: World 1
lamp
plant
chair
alarm
tutorial room
safe
Figure 4: World 2
plant
chair
lamp
safe
tutorial room
alarm
Figure 5: World 3
3.4 Timeline
After the GIVE Challenge was publicized in
March 2008, eight research teams signed up for
participation. We distributed an initial version of
the GIVE software and a development world to
these teams. In the end, four teams submitted
NLG systems. These were connected to a cen-
tral Matchmaker instance that ran for about three
months, from 7 November 2008 to 5 February
2009. During this time, we advertised participa-
tion in the GIVE Challenge to the public in order
to obtain experimental subjects.
3.5 NLG systems
Five NLG systems were evaluated in GIVE-1:
1. one system from the University of Texas at
Austin (?Austin? in the graphics below);
2. one system from Union College in Schenec-
tady, NY (?Union?);
3. one system from the Universidad Com-
plutense de Madrid (?Madrid?);
4. two systems from the University of Twente:
one serious contribution (?Twente?) and one
more playful one (?Warm-Cold?).
Of these systems, ?Austin? can serve as a base-
line: It computes a plan consisting of the actions
the user should take to achieve the goal, and at
each point in the game, it realizes the first step
in this plan as a single instruction. The ?Warm-
Cold? system generates very vague instructions
that only tell the user if they are getting closer
(?warmer?) to their next objective or if they are
moving away from it (?colder?). We included this
system in the evaluation to verify whether the eval-
uation methodology would be able to distinguish
such an obviously suboptimal instruction-giving
strategy from the others.
Detailed descriptions of these systems
as well as each team?s own analysis of
the evaluation results can be found at
http://www.give-challenge.org/
research/give-1.
4 Results
We now report on the results of GIVE-1. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures.
Notice that some of our evaluation measures are
in tension with each other: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?), such as the ?Austin? baseline, will lead the
user to completing the task in a minimum number
of steps; but it will require more instructions than
a system that aggregates these. This is intentional,
and emphasizes both the pilot experiment char-
acter of GIVE-1 and our desire to make GIVE a
friendly comparative challenge rather than a com-
petition with a clear winner.
4.1 Demographics
Over the course of three months, we collected
1143 valid games. A game counted as valid if the
game client didn?t crash, the game wasn?t marked
as a test game by the developers, and the player
completed the tutorial.
Of these games, 80.1% were played by males
and 9.9% by females; a further 10% didn?t specify
their gender. The players were widely distributed
over countries: 37% connected from an IP address
in the US, 33% from an IP address in Germany,
and 17% from China; Canada, the UK, and Aus-
tria also accounted for more than 2% of the partic-
168
037,5
75,0
112,5
150,0
N
o
v
 
7
D
e
c
 
1
J
a
n
 
1
F
e
b
 
1
F
e
b
 
5
# games per day
German
press release
US
press release
posted to
SIGGEN list
covered by
Chinese blog
Figure 6: Histogram of the connections per day.
ipants each, and the remaining 2% of participants
connected from 42 further countries. This imbal-
ance stems from very successful press releases that
were issued in Germany and the US and which
were further picked up by blogs, including one
in China. Nevertheless, over 90% of the partici-
pants who answered this question self-rated their
English proficiency as ?good? or better. About
75% of users connected with a client running on
Windows, with the rest split about evenly among
Linux and Mac OS X.
The effect of the press releases is also plainly
visible if we look at the distribution of the valid
games over the days from November 7 to Febru-
ary 5 (Fig. 6). There are huge peaks at the
very beginning of the evaluation period, coincid-
ing with press releases through Saarland Univer-
sity in Germany and Northwestern University in
the US, which were picked up by science and tech-
nology blogs on the Web. The US peak contains
a smaller peak of connections from China, which
were sparked by coverage in a Chinese blog.
4.2 Objective measures
We then extracted objective and subjective mea-
surements from the valid games. The objective
measures are summarized in Fig. 7. For each sys-
tem and game world, we measured the percent-
age of games which the users completed success-
fully. Furthermore, we counted the numbers of in-
structions the system sent to the user, measured
the time until task completion, and counted the
number of low-level steps executed by the user
(any key press, to either move or manipulate an
object) as well as the number of task-relevant ac-
tions (such as pushing a button to open a door).
? task success (Did the player get the trophy?)
? instructions (Number of instructions pro-
duced by the NLG system.?)
? steps (Number of all player actions.?)
? actions (Number of object manipulation
action.?)
? second (Time in seconds.?)
?
Measured from the end of the tutorial until the
end of the game.
Figure 7: Objective measurements
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
success
40% 71% 35% 73% 18%
A A
B B
C
instructions
83.2 58.3 121.2 80.3 190.0
A
B B
C
D
steps
103.6 124.3 160.9 117.5 307.4
A A
B B
C
D
actions
11.2 8.7 14.3 9.0 14.3
A A
B
C C
seconds
129.3 174.8 207.0 175.2 312.2
A
B B
C
D
Figure 8: Objective measures by system. Task
success is reported as the percentage of suc-
cessfully completed games. The other measures
are reported as the mean number of instruc-
tions/steps/actions/seconds, respectively. Letters
group indistinguishable systems; systems that
don?t share a letter were found to be significantly
different with p < 0.05.
169
To ensure comparability, we only counted success-
fully completed games for all these measures, and
only started counting when the user left the tutorial
room. Crucially, all objective measures were col-
lected completely unobtrusively, without requiring
any action on the user?s part.
Fig. 8 shows the results of these objective mea-
sures. This figure assigns systems to groups A,
B, etc. for each evaluation measure. Systems in
group A are better than systems in group B, etc.;
if two systems don?t share the same letter, the dif-
ference between these two systems is significant
with p < 0.05. Significance was tested using a
?2-test for task success and ANOVAs for instruc-
tions, steps, actions, and seconds. These were fol-
lowed by post-hoc tests (pairwise ?2 and Tukey)
to compare the NLG systems pairwise.
Overall, there is a top group consisting of
the Austin, Madrid, and Union systems: While
Madrid and Union outperform Austin on task suc-
cess (with 70 to 80% of successfully completed
games, depending on the world), Austin signifi-
cantly outperforms all other systems in terms of
task completion time. As expected, the Warm-
Cold system performs significantly worse than all
others in almost all categories. This confirms the
ability of the GIVE evaluation method to distin-
guish between systems of very different qualities.
4.3 Subjective measures
The subjective measures, which were obtained by
asking the users to fill in a questionnaire after each
game, are shown in Fig. 9. Most of the questions
were answered on 5-point Likert scales (?overall?
on a 7-point scale); the ?informativity? and ?tim-
ing? questions had nominal answers. For each
question, the user could choose not to answer.
The results of the subjective measurements are
summarized in Fig. 10, in the same format as
above. We ran ?2-tests for the nominal variables
informativity and timing, and ANOVAs for the
scale data. Again, we used post-hoc pairwise ?2-
and Tukey-tests to compare the NLG systems to
each other one by one.
Here there are fewer significant differences be-
tween different groups than for the objective mea-
sures: For the ?play again? category, there is
no significant difference at all. Nevertheless,
?Austin? is shown to be particularly good at navi-
gation instructions and timing, whereas ?Madrid?
outperforms the rest of the field in ?informativ-
7-point scale items:
overall: What is your overall evaluation of the quality of the
direction-giving system? (very bad 1 . . . 7 very good)
5-point scale items:
task difficulty: How easy or difficult was the task for you to
solve? (very difficult 1 2 3 4 5 very easy)
goal clarity: How easy was it to understand what you were
supposed to do? (very difficult 1 2 3 4 5 very easy)
play again: Would you want to play this game again? (no
way! 1 2 3 4 5 yes please!)
instruction clarity: How clear were the directions? (totally
unclear 1 2 3 4 5 very clear)
instruction helpfulness: How effective were the directions at
helping you complete the task? (not effective 1 2 3 4 5
very effective)
choice of words: How easy to understand was the system?s
choice of wording in its directions to you? (totally un-
clear 1 2 3 4 5 very clear)
referring expressions: How easy was it to pick out which ob-
ject in the world the system was referring to? (very hard
1 2 3 4 5 very easy)
navigation instructions: How easy was it to navigate to a par-
ticular spot, based on the system?s directions? (very
hard 1 2 3 4 5 very easy)
friendliness: How would you rate the friendliness of the sys-
tem? (very unfriendly 1 2 3 4 5 very friendly)
Nominal items:
informativity: Did you feel the amount of information you
were given was: too little / just right / too much
timing: Did the directions come ... too early / just at the right
time / too late
Figure 9: Questionnaire items
ity?. In the overall subjective evaluation, the ear-
lier top group of Austin, Madrid, and Union is
confirmed, although the difference between Union
and Twente is not significant. However, ?Warm-
Cold? again performs significantly worse than all
other systems in most measures. Furthermore, al-
though most systems perform similarly on ?infor-
mativity? and ?timing? in terms of the number of
users who judged them as ?just right?, there are
differences in the tendencies: Twente and Union
tend to be overinformative, whereas Austin and
Warm-Cold tend to be underinformative; Twente
and Union tend to give their instructions too late,
whereas Madrid and Warm-Cold tend to give them
too early.
170
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
difficulty
4.3 4.3 4.0 4.3 3.5
A A A A
B
goal clarity
4.0 3.7 3.9 3.7 3.3
A A A A
B
play again
2.8 2.6 2.4 2.9 2.5
A A A A A
instruction
clarity
4.0 3.6 3.8 3.6 3.0
A A A
B B B
C
instruction
helpfulness
3.8 3.9 3.6 3.7 2.9
A A A A
B
informativity
46% 68% 51% 56% 51%
A
B B B B
overall
4.9 4.9 4.3 4.6 3.6
A A A
B B
C
choice of
words
4.2 3.8 4.1 3.7 3.5
A A
B B
C C C
referring
expressions
3.4 3.9 3.7 3.7 3.5
A A A
B B B B
navigation
instructions
4.6 4.0 4.0 3.7 3.2
A
B B B
C
timing
78% 62% 60% 62% 49%
A
B B B
C C
friendliness
3.4 3.8 3.1 3.6 3.1
A A A
B B B
Figure 10: Subjective measures by system. Infor-
mativity and timing are reported as the percentage
of successfully completed games. The other mea-
sures are reported as the mean rating received by
the players. Letters group indistinguishable sys-
tems; systems that don?t share a letter were found
to be significantly different with p < 0.05.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, there may be other factors which also influ-
ence the outcome of our objective and subjective
measures. We tested the following five factors:
evaluation world, gender, age, computer expertise,
and English proficiency (as reported by the users
on the questionnaire). We found that there is a sig-
nificant difference in task success rate for different
evaluation worlds and between users with different
levels of English proficiency.
The interaction graphs in Figs. 11 and 12 also
suggest that the NLG systems differ in their ro-
bustness with respect to these factors. ?2-tests
that compare the success rate of each system in
the three evaluation worlds show that while the
instructions of Union and Madrid seem to work
equally well in all three worlds, the performance
of the other three systems differs dramatically be-
tween the different worlds. Especially World 2
was challenging for some systems as it required
relational object descriptions, such as the blue but-
ton on the left of another blue button.
The players? English skills also affected the sys-
tems in different ways. While Austin, Madrid and
Warm Cold don?t manage to lead players with only
basic English skills to success as often as other
players, Union?s and Twente?s success rates do not
depend on the players? English skills (?2-tests do
not find significant differences in success rate be-
tween players with different levels of English pro-
ficiency for these two systems). However, if we
remove the players with the lowest level of En-
glish proficiency, language skills do not have an
effect on the task success rate anymore for any of
the systems.
5 Conclusion
In this document, we have described the first in-
stallment of the GIVE Challenge, our experimen-
tal methodology, and the results. Altogether, we
collected 1143 valid games for five NLG systems
over a period of three months. Given that this was
the first time we organized the challenge, that it
was meant as a pilot experiment from the begin-
ning, and that the number of games was sufficient
to get significant differences between systems on
a number of measures, we feel that GIVE-1 was a
success. We are in the process of preparing sev-
eral diagnostic utilities, such as heat maps and a
tool that lets the system developer replay an indi-
171
Figure 11: Effect of the evaluation worlds on the
success rate of the NLG systems.
vidual game, which will help the participants gain
further insight into their NLG systems.
Nevertheless, there are a number of improve-
ments we will make to GIVE for future install-
ments. For one thing, the timing of the challenge
was not optimal: A number of colleagues would
have been interested in participating, but the call
for participation came too late for them to acquire
funding or interest students in time for summer
projects or MSc theses. Secondly, although the
software performed very well in handling thou-
sands of user connections, there were still game-
invalidating issues with the 3D graphics and the
networking code that were individually rare, but
probably cost us several hundred games. These
should be fixed for GIVE-2. At the same time,
we are investigating ways in which the networking
and matchmaking core of GIVE can be factored
out into a separate, challenge-independent system
on which other Internet-based challenges can be
built. Among other things, it would be straightfor-
ward to use the GIVE platform to connect two hu-
man users and observe their dialogue while solv-
ing a problem. Judicious variation of parameters
(such as the familiarity of users or the visibility of
an instruction giving avatar) would allow the con-
struction of new dialogue corpora along such lines.
Finally, GIVE-1 focused on the generation of
navigation instructions and referring expressions,
in a relatively simple world, without giving the
Figure 12: Effect of the players? English skills on
the success rate of the NLG systems.
user a chance to talk back. The high success rate
of some systems in this challenge suggests that
we need to widen the focus for a future GIVE-
2 ? by allowing dialogue, by making the world
more complex (e.g., allowing continuous rather
than discrete movements and turns), by making the
communication multi-modal, etc. Such extensions
would require only rather limited changes to the
GIVE software infrastructure. We plan to come to
a decision about such future directions for GIVE
soon, and are looking forward to many fruitful dis-
cussions about this at ENLG.
Acknowledgments. We are grateful to the par-
ticipants of the 2007 NSF/SIGGEN Workshop on
Shared Tasks and Evaluation in NLG and many
other colleagues for fruitful discussions while we
were designing the GIVE Challenge, and to the
organizers of Generation Challenges 2009 and
ENLG 2009 for their support and the opportunity
to present the results at ENLG. We also thank the
four participating research teams for their contri-
butions and their patience while we were working
out bugs in the GIVE software. The creation of
the GIVE infrastructure was supported in part by
a Small Projects grant from the University of Ed-
inburgh.
172
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. Stoia, D. M. Shockley, D. K. Byron, and E. Fosler-
Lussier. 2006. Noun phrase generation for situated
dialogs. In Proceedings of INLG, Sydney.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
173
Proceedings of the EACL 2009 Demonstrations Session, pages 33?36,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
The Software Architecture for the
First Challenge on Generating Instructions in Virtual Environments
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Kristina Striegnitz
Union College
striegnk@union.edu
Abstract
The GIVE Challenge is a new Internet-
based evaluation effort for natural lan-
guage generation systems. In this paper,
we motivate and describe the software in-
frastructure that we developed to support
this challenge.
1 Introduction
Natural language generation (NLG) systems are
notoriously hard to evaluate. On the one hand,
simply comparing system outputs to a gold stan-
dard is not appropriate because there can be mul-
tiple generated outputs that are equally good, and
finding metrics that account for this variability and
produce results consistent with human judgments
and task performance measures is difficult (Belz
and Gatt, 2008; Stent et al, 2005; Foster, 2008).
On the other hand, lab-based evaluations with hu-
man subjects to assess each aspect of the system?s
functionality are expensive and time-consuming.
These characteristics make it hard to compare dif-
ferent systems and measure progress.
GIVE (?Generating Instructions in Virtual En-
vironments?) (Koller et al, 2007) is a research
challenge for the NLG community designed to
provide a new approach to NLG system evalua-
tion. In the GIVE scenario, users try to solve
a treasure hunt in a virtual 3D world that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual envi-
ronment. The challenge for the NLG system is
to generate, in real time, natural-language instruc-
tions that will guide the users to the successful
completion of their task (see Fig. 1). One cru-
cial advantage of this generation task is that the
NLG system and the user can be physically sepa-
rated. This makes it possible to carry out a task-
based evaluation over the Internet ? an approach
that has been shown to provide generous amounts
Figure 1: The GIVE Challenge.
of data in earlier studies (von Ahn and Dabbish,
2004; Orkin and Roy, 2007).
In this paper, we describe the software archi-
tecture underlying the GIVE Challenge. The soft-
ware connects each player in a 3D game world
with an NLG system over the Internet. It is imple-
mented and open source, and can be a used online
during EACL at www.give-challenge.org.
In Section 2, we give an introduction to the GIVE
evaluation methodology by describing the experi-
ence of a user participating in the evaluation, the
nature of the data we collect, and our scientific
goals. Then we explain the software architecture
behind the scenes and sketch the API that concrete
NLG systems must implement in Section 3. In
Section 4, we present some preliminary evaluation
results, before we conclude in Section 5.
2 Evaluation method
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
33
b2 b3b4 b5
b6
b7
b1
player
b8b9
b10
b11 b14b13b12
safe 
door
b1 opens doorto room 3
b9 moves picture to
b8: part of safe sequencereveal safe
? to win you have to retrieve the trophy from the safe in room 1? use button b9 to move the picture (and get access to the safe)
? if the alarm sounds, the game is over and you have lost
? press buttons b8, b6, b13, b13, b10 (in this order) to open the safe;if a button is pressed in the wrong order, the whole sequence is reset
b14 makes alarm soundb10, b13: part of safe sequence door to room 2b7 opens/closesstepping on this tiletriggers alarm
alarm
room 3
b2 turns off alarm tileb3 opens/closes door to room 2
b6: part of safe sequence
room 1
b5 makes alarm sound
room 2
door
door
lampcouch
chair
flower
pictu
retrophy
Figure 2: The map of a virtual world.
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system.
The map of one of the game worlds is shown in
Fig. 2: In this world, players must pick up a trophy,
which is in a wall safe behind a picture. In order
to access the trophy, they must first push a button
to move the picture to the side, and then push an-
other sequence of buttons to open the safe. One
floor tile is alarmed, and players lose the game
if they step on this tile without deactivating the
alarm first. There are also a number of distrac-
tor buttons which either do nothing when pressed
or set off an alarm. These distractor buttons are in-
tended to make the game harder and, more impor-
tantly, to require appropriate reference to objects
in the game world. Finally, game worlds can con-
tain a number of objects such as chairs and flowers
which are irrelevant for the task, but can be used
as landmarks by a generation system.
Users are asked to fill out a before- and after-
game questionnaire that collects some demo-
graphic data and asks the user to rate various as-
pects of the instructions they received. Every ac-
tion that players take in a game world, and every
instruction that a generation system generates for
them, is recorded in a database. In addition to the
questionnaire data, we are thus able to compute a
number of objective measures such as:
? the percentage of users each system leads to
a successful completion of the task;
? the average time, the average number of in-
structions, and the average number of in-
game actions that this success requires;
? the percentage of generated referring expres-
sions that the user resolves correctly; and
? average reaction times to instructions.
It is important to note that we have designed
the GIVE Challenge not as a competition, but as
a friendly evaluation effort where people try to
learn from each other?s successes. This is reflected
in the evaluation measures above, which are in
tension with one another: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?) will enjoy short reaction times, but it will re-
quire more instructions than a system that aggre-
gates these. To further emphasize this perspective,
we will also provide a number of diagnostic tools,
such as heat maps that show how much time users
spent on each tile, or a playback function which
displays an entire game run in real time.
In summary, the GIVE Challenge is a novel
evaluation effort for NLG systems. It is motivated
by real applications (such as pedestrian navigation
and the generation of task instructions), makes
no assumptions about the internal structure of an
NLG system, and emphasizes the situated genera-
tion of discourse in a simulated physical environ-
ment. The game world is scalable; it can be made
more complex and it can be adapted to focus on
specific issues in natural language generation.
3 Architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components:
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide.
34
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 3: The GIVE architecture.
When a user starts the client, it connects over
the Internet to the Matchmaker. The Matchmaker
then selects a game world and an NLG server at
random, and requests the NLG server to spawn
a new server instance. It then sends the game
world to the client and the server instance and dis-
connects from them, ready to handle new connec-
tions from other clients. The client and the server
instance play one game together: Whenever the
user does something, the client sends a message
about this to the server instance, and the server in-
stance can also send a message back to the client
at any time, which will then be displayed as an in-
struction. When the game ends, the client and the
server instance disconnect from each other. The
server instance sends a log of all game events to
the Matchmaker, and the client sends the ques-
tionnaire results to the Matchmaker; these then are
stored in the database for later analysis.
All of these components are implemented in
Java. This allows the client to be portable across
all major operating systems, and to be started di-
rectly from the website via Java Web Start without
the need for software installation. We felt it was
important to make startup of the client as effort-
less as possible, in order to maximize the num-
ber of users willing to play the game. Unsurpris-
ingly, we had to spend the majority of the pro-
gramming time on the 3D graphics (based on the
free jMonkeyEngine library) and the networking
code. We could have reduced the effort required
for these programming tasks by building upon an
existing virtual 3D world system such as Second
Life. However, we judged that the effort needed to
adapt such a system to our needs would have been
at least as high (in particular, we would have had
to ensure that the user could only move according
to the rules of the GIVE game and to instrument
the virtual world to obtain real-time updates about
events), and the result would have been less exten-
abstract class NlgSystem:
void connectionEstablished();
void connectionDisconnected();
void handleStatusInformation(Position playerPosition,
Orientation playerOrientation,
List?String? visibleObjects);
void handleAction(Atom actionInstance,
List?Formula? updates);
void handleDidNotUnderstand();
void handleMoveTurnAction(Direction direction);
. . .
Figure 4: The interface of an NLG system.
sible to future installments of the challenge.
Since we provided all the 3D, networking, and
database code, the research teams being evaluated
were able to concentrate on the development of
their NLG systems. Our only requirement was
that they implement a concrete subclass of the
class NlgSystem, shown in Fig. 4. This involves
overriding the six abstract callback methods in
this class with concrete implementations in
which the NLG system reacts to specific events.
The methods connectionEstablished
and connectionDisconnected are called
when users enter the game world and when
they disconnect from the game. The method
handleAction gets called whenever the user
performs some physical action, such as pushing a
button, and specifies what changed in the world
due to this action; handleMoveTurnAction
gets called whenever the user moves;
handleDidNotUnderstand gets called
whenever users press the H key to signal that
they didn?t understand the previous instruction;
and handleStatusInformation gets called
once per second and after each user action to
inform the server of the player?s position and
orientation and the visible objects. Ultimately,
each of these method calls gets triggered by a
message that the client sends over the network
in reaction to some event; but this is completely
hidden from the NLG system developer.
The NLG system can use the method send to
send a string to the client to be displayed. It also
has access to various methods querying the state of
the game world and to an interface to an external
planner which can compute a sequence of actions
leading to the goal.
4 First results
For this first installment of the GIVE Challenge,
four research teams from the US, the Netherlands,
35
and Spain provided generation systems, and a
number of other research groups expressed their
interest in participating, but weren?t able to partic-
ipate due to time constraints. Given that this was
the first time we organized this task, we find this
a very encouraging number. All four of the teams
consisted primarily of students who implemented
the NLG systems over the Northern-hemisphere
summer. This is in line with our goal of tak-
ing this first iteration as a ?dry run? in which we
could fine-tune the software, learn about the easy
and hard aspects of the challenge, and validate the
evaluation methodology.
Public involvement in the GIVE Challenge was
launched with a press release in early Novem-
ber 2008; the Matchmaker and the NLG servers
were then kept running until late January 2009.
During this time, online users played over 1100
games, which translates into roughly 75 game runs
for each experimental condition (i.e., five differ-
ent NLG systems paired with three different game
worlds). To our knowledge, this makes GIVE the
largest NLG evaluation effort yet in terms of ex-
perimental subjects.
While we have not yet carried out the detailed
evaluation, the preliminary results look promising:
a casual inspection shows that there are consider-
able differences in task success rate among the dif-
ferent systems.
While there is growing evidence from differ-
ent research areas that the results of Internet-based
evaluations are consistent with more traditional
lab-based experiments (e.g., (Keller et al, 2008;
Gosling et al, 2004)), the issue is not yet set-
tled. Therefore, we are currently conducting a lab-
based evaluation of the GIVE NLG systems, and
will compare those results to the qualitative and
quantitative data provided by the online subjects.
5 Conclusion
In this paper, we have sketched the GIVE Chal-
lenge and the software infrastructure we have de-
veloped for it. The GIVE Challenge is, to the
best of our knowledge, the largest-scale NLG eval-
uation effort with human experimental subjects.
This is made possible by connecting users and
NLG systems over the Internet; we collect eval-
uation data automatically and unobtrusively while
the user simply plays a 3D game. While we will
report on the results of the evaluation in more de-
tail at a later time, first results seem encouraging
in that the performance of different NLG systems
differs considerably.
In the future, we will extend the GIVE Chal-
lenge to harder tasks. Possibilities includ mak-
ing GIVE into a dialogue challenge by allowing
the user to speak as well as act in the world; run-
ning the challenge in a continuous world rather
than a world that only allows discrete movements;
or making it multimodal by allowing the NLG
system to generate arrows or virtual human ges-
tures. All these changes would only require lim-
ited changes to the GIVE software architecture.
However, the exact nature of future directions re-
mains to be discussed with the community.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
F. Keller, S. Gunasekharan, N. Mayo, and M. Corley.
2008. Timing accuracy of web experiments: A case
study using the WebExp software package. Behav-
ior Research Methods, to appear.
A. Koller, J. Moore, B. di Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared task proposal: Instruction giving in virtual
worlds. In M. White and R. Dale, editors, Work-
ing group reports of the Workshop on Shared Tasks
and Comparative Evaluation in Natural Language
Generation. Available at http://www.ling.
ohio-state.edu/nlgeval07/report.html.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
36
Referring Expressions as Formulas of Description Logic
Carlos Areces
INRIA Nancy Grand Est
Nancy, France
areces@loria.fr
Alexander Koller
University of Edinburgh
Edinburgh, UK
a.koller@ed.ac.uk
Kristina Striegnitz
Union College
Schenectady, NY, US
striegnk@union.edu
Abstract
In this paper, we propose to reinterpret the
problem of generating referring expressions
(GRE) as the problem of computing a formula
in a description logic that is only satisfied by
the referent. This view offers a new unifying
perspective under which existing GRE algo-
rithms can be compared. We also show that
by applying existing algorithms for computing
simulation classes in description logic, we can
obtain extremely efficient algorithms for rela-
tional referring expressions without any dan-
ger of running into infinite regress.
1 Introduction
The generation of referring expressions (GRE) is
one of the most active and successful research ar-
eas in natural language generation. Building upon
Dale and Reiter?s work (Dale, 1989; Dale and Reiter,
1995), various researchers have added extensions
such as reference to sets (Stone, 2000), more expres-
sive logical connectives (van Deemter, 2002), and
relational expressions (Dale and Haddock, 1991).
Referring expressions (REs) involving relations,
in particular, have received increasing attention re-
cently; especially in the context of spatial refer-
ring expressions in situated generation (e.g. (Kelle-
her and Kruijff, 2006)), where it seems particularly
natural to use expressions such as ?the book on the
table?. However, the classical algorithm by Dale and
Haddock (1991) was recently shown to be unable
to generate satisfying REs in practice (Viethen and
Dale, 2006). Furthermore, the Dale and Haddock al-
gorithm and most of its successors (such as (Kelle-
her and Kruijff, 2006)) are vulnerable to the prob-
lem of ?infinite regress?, where the algorithm jumps
back and forth between generating descriptions for
two related individuals infinitely, as in ?the book on
the table which supports a book on the table . . . ?.
In this paper, we propose to view GRE as the
problem of computing a formula of description logic
(DL) that denotes exactly the set of individuals that
we want to refer to. This very natural idea has been
mentioned in passing before (Krahmer et al, 2003;
Gardent and Striegnitz, 2007); however, we take it
one step further by proposing DL as an interlingua
for comparing the REs produced by different ap-
proaches to GRE. In this way, we can organize ex-
isting GRE approaches in an expressiveness hierar-
chy. For instance, the classical Dale and Reiter al-
gorithms compute purely conjunctive formulas; van
Deemter (2002) extends this language by adding the
other propositional connectives, whereas Dale and
Haddock (1991) extends it by allowing existential
quantification.
Furthermore, the view of GRE as a problem of
computing DL formulas with a given extension al-
lows us to apply existing algorithms for the lat-
ter problem to obtain efficient algorithms for GRE.
We present algorithms that compute such formulas
for the description logics EL (which allows only
conjunction and existential quantification) andALC
(which also allows negation). These algorithms ef-
fectively compute REs for all individuals in the do-
main at the same time, which allows them to system-
atically avoid the infinite regress problem. The EL
algorithm is capable of generating 67% of the rela-
tional REs in the Viethen and Dale (2006) dataset, in
about 15 milliseconds. The ALC algorithm is even
faster; it computes relational REs for all 100 indi-
viduals in a random model in 140 milliseconds.
The paper is structured as follows. In Section 2,
42
we will first define description logics. We will then
show how to generate REs by computing DL sim-
ilarity sets for ALC and EL in Section 3. In Sec-
tion 4, we evaluate our algorithms and discuss our
results. Section 5 compares our approach to related
research; in particular, it shows how various promi-
nent GRE algorithms fit into the DL framework.
Section 6 concludes and points to future work.
2 Description logics and similarity
In this paper, we will represent referring expres-
sions as formulas of description logic (Baader et al,
2003). In order to make this point, we will now de-
fine the two description logics we will be working
with: ALC and EL.
Formulas (or concepts) ? of ALC are generated
by the following grammar:
?,?? ::= > | p | ?? | ? u ?? | ?R.?
where p is in the set of propositional symbols prop,
and R is in the set of relational symbols rel. EL is
the negation-free fragment of ALC.
Formulas of both ALC and EL are interpreted in
ordinary relational first-order modelsM = (?, || ? ||)
where ? is a non-empty set and || ? || is an interpreta-
tion function such that:
||p|| ? ? for p ? prop
||R|| ? ??? for R ? rel
||??|| = ?? ||?||
||? u ??|| = ||?|| ? ||??||
||?R.?|| = {i | for some i?, (i, i?) ? ||R||
and i? ? ||?||}.
Every formula of a description logic denotes a
set of individuals in the domain; thus we can use
such formulas to describe sets. For instance, in the
model in Fig. 1b, the formula flower denotes the set
{f1, f2}; the formula floweru?in.hat denotes {f2};
and the formula flower u ??in.hat denotes {f1}.
Different description logics differ in the inventory
of logical connectives they allow: While ALC per-
mits negation, EL doesn?t. There are many other
description logics in the literature; some that we
will get back to in Section 5 are CL (EL without
existential quantification, i.e., only conjunctions of
atoms); PL (ALC without existential quantification,
i.e., propositional logic); and ELU (?) (EL plus dis-
junction and atomic negation).
Below, we will use a key notion of formula preser-
vation that we call similarity. For any DL L, we
will say that an individual i is L-similar to i? in a
given modelM if for any formula ? ? L such that
i ? ||?||, we also have i? ? ||?||. Equivalently, there
is no L-formula that holds of i but not of i?. We say
that the L-similarity set of some individual i is the
set of all individuals to which i is L-similar.
Notice that similarity is not necessarily a symmet-
rical relation: For instance, f1 is EL-similar to f2 in
Fig. 1b, but f2 is not EL-similar to f1 (it satisfies the
formula ?in.hat and f1 doesn?t). However, ALC-
similarity is a symmetrical relation because the lan-
guage contains negation; and indeed, f1 is notALC-
similar to f2 either because it satisfies ??in.hat. Be-
causeALC is more expressive than EL, it is possible
for some individual a to be EL-similar but notALC-
similar to some individual b, but not vice versa.
3 Generating referring expressions
Now we apply description logic to GRE. The core
claim of this paper is that it is natural and useful to
view the GRE problem as the problem of computing
a formula of some description logic L whose exten-
sion is a given target set A of individuals.
L-GRE PROBLEM
Input: A modelM and a target set A ? ?.
Output: A formula ? ? L such that ||?|| = A
(if such a formula exists).
In the examples above, it is because flower u
?in.hat denotes exactly {f2} that we can say ?the
flower in the hat? to refer to f2. This perspective pro-
vides a general framework into which many existing
GRE approaches fit: Traditional attribute selection
(Dale and Reiter, 1995) corresponds to building DL
formulas that are conjunctions of atoms; relational
REs as in Dale and Haddock (1991) are formulas of
EL; and so on. We will further pursue the idea of or-
ganizing GRE approaches with respect to the variant
of DL they use in Section 5.
For the rest of this paper, we assume that we are
generating a singular RE, i.e., the target set A will
be a singleton. In this case, we will only be able
to generate a formula that denotes exactly A = {a}
(i.e., a RE that uniquely refers to a) if there is no
43
f1
floor
t
2
table
t
1
table
b
2
c
2
bowl
cup
b
1
bowl
c
1
cup
on
on
on
on
in
in
(a) (b)
r
1
rabbit
r
2
rabbit
r
3
rabbit
r
4
rabbith
1
hat
h
4
hat
h
2
hat
h
3
hat
f
1
flower
f
2
flower
b
1
bathtub
in
in
in
Figure 1: (a) The Dale and Haddock (1991) scenario; (b)
the Stone and Webber (1998) scenario.
other individual b to which a is similar; otherwise,
any formula that is satisfied by a is also satisfied by
b. Conversely, if we know that a is not similar to any
other individual, then there is a formula that is satis-
fied by a and not by anything else; this formula can
serve as a unique singular RE. In other words, we
can reduce the L-GRE problem for a given model
to the problem of computing the L-similarity sets of
this model. Notice that this use of similarity sets can
be seen as a generalization of van Deemter?s (2002)
?satellite sets? to relational descriptions.
In the rest of this section, we will present algo-
rithms that compute the similarity sets of a given
model for ALC and EL, together with characteris-
tic formulas that denote them. In the ALC case,
we adapt a standard algorithm from the literature
for computing simulation classes; we will then fur-
ther adapt this algorithm for EL. In effect, both al-
gorithms compute REs for all individuals in some
model at the same time ? very efficiently and with-
out any danger of infinite regress.
3.1 Computing similarity sets
It can be shown that for ALC, the similarity sets
of a finite model coincide exactly with the simu-
lation classes of this model. Simulation classes
have been studied extensively in the literature (see
e.g., Blackburn et al (2001); Kurtonina and de Ri-
jke (1998)), and there are several efficient algorithms
for computing ALC-simulation classes (Hopcroft,
1971; Paige and Tarjan, 1987; Dovier et al, 2004).
However, these algorithms will only compute the
simulation classes themselves. Here we extend the
Hopcroft (1971) algorithm such that it computes,
along with each set, also a formula that denotes ex-
actly this set. We can then use these formulas as
representations of the referring expressions.
The pseudocode for our ALC algorithm is shown
as Algorithm 1 (with L = ALC) and Algorithm 2.
Given a modelM = (?, || ? ||), the algorithm com-
putes a set RE of ALC formulas such that {||?|| |
? ? RE} is the set of ALC-similarity sets of
M. The algorithm starts with RE = {>} (where
||>|| = ?), and successively refines RE by mak-
ing its elements denote smaller and smaller sets. It
maintains the invariant that at the start and end of ev-
ery iteration, {||?|| | ? ? RE} is always a partition
of ?. The algorithm iterates over all propositional
and relational symbols in prop and rel to construct
new formulas until either all formulas in RE denote
singletons (i.e., there is only one individual that sat-
isfies them), or no progress has been made in the
previous iteration. In each iteration, it calls the pro-
cedure addALC(?, RE ), which intersects ? with any
formula ? ? RE which does not denote a singleton
and which is not equivalent to ? and to ??. In this
case, it replaces ? in RE by ? u ? and ? u ??.
TheALC algorithm computes theALC-similarity
sets of the model in timeO(n3), where n is the num-
ber of individuals in the domain. However, it will
freely introduce negations in the case distinctions,
which can make the resulting formula hard to realize
(see also Section 4.3). This is why we also present
an algorithm for the EL-similarity sets; EL corre-
sponds to positive relational REs, which are gener-
ally much easier to realize.
We obtain the EL algorithm by replacing the call
to addALC in Algorithm 1 by a call to addEL, which
is defined in Algorithm 3. As before, the algo-
rithm maintains a set RE = {?1, . . . , ?n} of for-
mulas (this time of EL) such that ||?1|| ? . . . ?
||?n|| = ?, and which it refines iteratively. However,
where the ALC algorithm maintains the invariant
that ||?1||, . . . , ||?n|| is a partition of ?, we weaken
this invariant to the requirement that there are no
m ? 2 pairwise different indices 1 ? i1, . . . , im ?
n such that ||?i1 || = ||?i2 || ? . . .? ||?im ||. We call ?i1
subsumed if such a decomposition exists.
Because it maintains a weaker invariant, the set
RE may contain more formulas at the same time in
the EL algorithm than in the ALC algorithm. Given
that ? has an exponential number of subsets, there is
a risk that the EL algorithm might have worst-case
44
Algorithm 1: Computing the L-similarity sets
Input: A modelM = (?, || ? ||)
Output: A set RE of formulas such that
{||?|| | ? ? RE} is the set of
L-similarity sets ofM.
RE ? {>}1
for p ? prop do2
addL(p,RE )3
while exists some ? ? RE , |||?|||M > 1 do4
for ? ? RE , R ? rel do5
addL(?R.?,RE )6
if made no changes to RE then7
exit8
exponential runtime (although we are not aware of
such worst-case examples). We leave a more careful
complexity analysis for future work.
We presented both algorithms as first refining RE
according to propositional symbols, and then by re-
lational expressions of increasing depth. But actu-
ally, propositional symbols can be encoded using
new relational symbols (e.g., we could represent that
f1 is a flower in Fig. 1 as a relation labeled flower
from f1 to an additional dummy element d). In this
way, we don?t need to distinguish between proposi-
tions and relations, and any arbitrary preference or-
dering of properties can be used.
3.2 Some examples
Let?s try our algorithms on some examples. We
first run the EL algorithm on the model shown in
Fig. 1a, which is taken from Dale and Haddock
(1991). The algorithm starts with RE = {>}. In
the first loop, it adds the formulas floor, bowl, cup,
and table, and then removes > because it is now
subsumed. Not all of these formulas denote single-
tons; for instance, ||cup|| contains two individuals.
So we iterate over the relations to refine our for-
mulas. After the first iteration over the relations,
we have RE = {floor, bowl u ?on.floor, bowl u
?on.table, cup, table}. Notice that bowl has become
subsumed, but we haven?t distinguished the cups
and tables further.
Now we can use the split between the bowls to
distinguish the cups in the second iteration. The re-
sult of this is RE = {floor, bowlu?on.floor, bowlu
Algorithm 2: addALC(?,RE )
for ? ? RE with |||?||| > 1 do1
if ||? u ?|| 6= ? and ||? u ??|| 6= ? then2
add ? u ? and ? u ?? to RE ;3
remove ? from RE ;4
Algorithm 3: addEL(?, RE )
for ? ? RE with |||?||| > 1 do1
if ? u ? is not subsumed in RE and2
||? u ?|| 6= ? and ||? u ?|| 6= ||?|| then
add ? u ? to RE3
remove subsumed formulas from RE4
?on.table, cup u ?in.(bowl u ?on.floor), cup u
?in.(bowl u ?on.table), table}. At this point, all
formulas except table denote singletons, and further
iterations don?t allow us to refine table; so the al-
gorithm terminates. Each formula with a singleton
extension {a} is a unique description of a; for in-
stance, cup u ?in.(bowl u ?on.table) is only satis-
fied by c2, so we may refer to c2 as ?the cup in the
bowl on the table?. Notice that the algorithm didn?t
focus on any particular individual; it simultaneously
generated REs for all individuals except for the two
tables (which are similar to each other).
The EL algorithm has a harder time with the ex-
ample in Fig. 1b (Stone and Webber, 1998). While
it will correctly identify r1 as ?the rabbit in the hat?
and f2 as ?the flower in the hat?, it will not be able to
compute a RE for f1 because f1 is EL-similar to f2.
Indeed, the algorithm terminates with RE contain-
ing both flower and floweru?in.hat. This is a typical
pattern for asymmetrical cases of similarity in EL: If
there are two formulas ?1 and ?2 in the output set
with ||?1|| ? ||?2||, then there is generally some in-
dividual b ? ||?2|| ? ||?1|| such that all individuals in
||?1|| are similar to b, but not vice versa. By contrast,
theALC algorithm can exploit the greater expressiv-
ity of ALC to split flower into the two new formulas
floweru?in.hat and floweru??in.hat, generating a
unique RE for f1 as well.
4 Discussion
We will now describe two experiments evaluating
the quality of the EL algorithm?s output and the effi-
45
Figure 2: A schematic view of the filing cabinets.
ciency of both of our algorithms, and we discuss the
interface between our algorithms and realization.
4.1 Evaluation: Output quality
To compare the descriptions generated by our al-
gorithm to those humans produce, we use a cor-
pus of human-generated referring expressions col-
lected and made available by Jette Viethen and
Robert Dale.1 They asked human subjects to de-
scribe one of 16 filing cabinet drawers. The draw-
ers had different colors and were arranged in a
four-by-four grid (see Fig. 2). The human subjects
used four non-relational properties (the drawer?s
color, its column and row number, and whether
it is in a corner) and five relational properties
(above, below, next to, left of, right of). Of the 118
referring expressions obtained in the experiment,
only 15 use relations.
Viethen and Dale (2006) describe the data in
more detail and present results of evaluating the Full
Brevity algorithm, the Incremental Algorithm (both
by Dale and Reiter (1995)), and the Relational Al-
gorithm (Dale and Haddock, 1991) on this corpus.
The Incremental Algorithm is dependent on a pre-
defined ordering in which properties are added to
the description. Viethen and Dale, therefore, try all
possible orderings and evaluate what percentage of
descriptions an algorithm can generate with any of
them. The Full Brevity and the Relational Algo-
rithms choose properties based on their discrimina-
tory power and only use the orderings as tie break-
ers. Viethen and Dale found that the Incremental
Algorithm is capable of generating 98 of the 103
non-relational descriptions. However, the Relational
Algorithm was unable to generate even a single one
of the human-generated relational descriptions.
We replicated Viethen and Dale?s experiment for
1http://www.ics.mq.edu.au/?jviethen/drawers
the EL algorithm presented above. In the non-
relational case, our results are the same as theirs for
the Incremental Algorithm: the EL algorithm gener-
ates 98 of the 103 non-relational descriptions, using
four (of the possible) orderings. This is because the
two algorithms perform essentially the same compu-
tations if there are no relations.
When we add relations, our algorithm is able to
generate 10 of the 15 human-produced relational
descriptions correctly (in addition to the 98 non-
relational descriptions). Fig. 3 gives example out-
puts of the EL algorithm for three different order-
ings, which together achieve this coverage. Of the
five human-produced descriptions that the EL algo-
rithm cannot generate, three involve references to
sets (the two blues ones in horizontal sequence/the
two yellow drawers), and two contain so much re-
dundant information that our algorithm cannot re-
produce them: Similarly to the Incremental Algo-
rithm, our algorithm allows for some redundancy,
but stops once it has found a distinguishing descrip-
tion. It does, however, generate other, simpler de-
scriptions for these referents.
4.2 Evaluation: Efficiency
Both the EL and the ALC algorithms took about 15
milliseconds to compute distinguishing formulas for
all 16 individuals in the Viethen and Dale dataset.2
In order to get a more comprehensive picture
of the algorithms? efficiency, we ran them on ran-
dom models with increasing numbers of individu-
als. Each model had random interpretations for ten
different propositional and four relational symbols;
each individual had a 10% chance to be in the exten-
sion of each propositional symbol, and each pair of
individuals had a 10% chance to be related by a re-
lational symbol. The results (averaged over 10 runs
for each model size) are shown in Fig. 4. The EL al-
gorithm takes about 350 ms on average to generate
relational REs for all individuals in the model of size
100, i.e., less than 4 ms on average for each individ-
ual. The ALC algorithm is even faster, at about 140
ms for the model of size 100. As far as we know,
these are by far the fastest published runtimes for
2Runtimes were measured on a MacBook Pro (Intel Core 2
Duo, 2.16 GHz) running Java 1.6 beta. We allowed the Java
VM to warm up, i.e., just-in-time compile all bytecode, before
taking the measurements.
46
id
human-produced description
output of the EL algorithm
2
the orange drawer above the blue drawer
orangeu ?above.blue / orange u ?above.(?below.(orange) u blue) / orange u ?next.(blue) u ?next.(pink)
4
the yellow drawer on the top of the pink one
yellow u ?above.pink / yellow u corner u ?above.pink / yellow u corner u ?above.(?next.(yellow) u pink)
5
? the pink drawer in the fourth column below the yellow one
pink u ?above.orange / pink u ?below.yellow / pink u ?next.(yellow) u ?above.(?next.(yellow) u orange)
6
the yellow drawer on top of the yellow drawer (2?) / ? the drawer after the two blue ones in horizontal sequence
yellow u ?above.yellow / yellow u ?below.pink / yellow u ?next.(blue) u ?next.(pink)
7
the blue drawer below the orange one / ? the blue drawer below the orange drawer in the second column
blueu?above.(blue)u?next.(?above.(orange)ublue) / blueu?below.(orange) / blueu?next.(blue)u?next.(yellow)
10
the blue drawer above the pink drawer (2?)
blueu ?above.(pink) / blue u ?above.(pink) u ?below.(blue) / blue u ?next.(orange) u ?next.(yellow)
11
the yellow drawer next to the orange drawer (2?)
yellow u ?above.orange / yellow u ?below.yellow / yellow u ?next.orange
12
the orange drawer below the pink drawer
orange u ?above.(pink u corner) / orangeu ?below.pink / orange u ?next.yellow
14
? the orange drawer below the two yellow drawers (2?)
orange u ?next.(pink u corner) u ?next.(pink) / orange u ?below.yellow / orange u ?next.(pink u corner)
Figure 3: The relational descriptions from Viethen and Dale (2006), annotated with the drawer id and the outputs of the
EL algorithm using three different orderings. Notice that four descriptions occurred twice in the corpus. Descriptions
that the EL algorithm cannot generate with any ordering are marked by ?. Generated descriptions that match one
produced by humans are in boldface.
any relational GRE algorithm in the literature.
4.3 Interface to realization
Our GRE algorithms do not guarantee that the for-
mula they compute can actually be realized in lan-
guage. For example, none of the formulas our al-
gorithms computed in the Viethen and Dale domain
contained an atom that would commonly be realized
as a noun; the property drawer is never used be-
cause it applies to all individuals in the domain. This
particular problem could easily be worked around
in a post-processing step. However, another prob-
lem arises from the massive use of negation in the
ALC algorithm; it will be hard for any realizer to
find a reasonable way of expressing a formula like
??R.(?Pu?Q) as a smooth noun phrase. Although
we agree with van Deemter (2002) and others that
the careful use of negation and disjunction can im-
prove REs, these connectives must not be overused.
Thus we consider the formulas computed by the EL
algorithm ?safer? with respect to realization.
Of course, we share the problem of interfacing
GRE and realization with every other approach that
separates these two modules, i.e., almost the en-
tire GRE literature (notable exceptions are, e.g., Ho-
racek (1997) and SPUD (Stone and Webber, 1998)).
0
100
200
300
400
10 20 30 40 50 60 70 80 90 100
EL ALC
Figure 4: Average runtimes (in ms) of the two algorithms
on random models with different numbers of individuals.
In principle, we believe that it is a good idea to
handle sentence planning and realization in a single
module; for instance, SPUD can use its awareness
of the syntactic context to generate succinct REs as
in ?take the rabbit from the hat?. We hope that the
ideas we have explored here for efficient and ex-
pressive RE generation can eventually be combined
with recent efficient algorithms for integrated sen-
tence planning and realization, such as in Koller and
Stone (2007).
One problem that arises in our approach is that
47
both algorithms derive some measure of efficiency
from their freedom to build formulas without hav-
ing to respect any linguistic constraints. It seems
straightforward, for instance, to extend Krahmer et
al.?s (2003) approach such that it only considers sub-
graphs that can actually be realized, because their al-
gorithm proceeds by a genuine search for uniquely
identifying subgraphs, and will simply take a differ-
ent branch of the search if some subgraph is useless.
This would be harder in our case. Our algorithms
don?t search in the same way; if we disallow certain
refinements of a partition, we have to allow the al-
gorithms to backtrack and thus jeopardize the worst-
case polynomial runtime. Investigating this inter-
play between efficiency and linguistic constraints is
an interesting avenue for future research.
5 A unified perspective on GRE
Viewing GRE as a problem of generating DL for-
mulas offers a unified perspective: It is the prob-
lem of computing a DL formula with a given exten-
sion. Many existing approaches can be subsumed
under this view; we have summarized this for some
of them in Fig. 5, along with the DL fragment they
use. We already discussed some of these approaches
in Section 3. Furthermore, the non-relational but
negative and disjunctive descriptions generated by
van Deemter (2002) are simply formulas of PL;
and Gardent (2002) generalizes this into generating
formulas of ELU (?), i.e., EL plus disjunction and
atomic negation. The approach presented here fits
well into this landscape, and it completes the pic-
ture by showing how to generate REs inALC, which
combines all connectives used in any of these previ-
ous approaches.
Where our approach breaks new ground is in the
way these formulas are computed: It successively
refines a decomposition of the domain into subsets.
In this way, it is reminiscent of the Incremental Al-
gorithm, which in fact can be seen as a special case
of the EL algorithm. However, unlike Dale and
Haddock (1991) and its successors, such as Kelle-
her and Kruijff (2006), we do not have to take spe-
cial precautions to avoid infinite regress. While Dale
and Haddock?s algorithm attempts to generate a RE
for a single individual, for successive individuals in
the model, our algorithms consider all individuals in
GRE algorithm DL variant
Dale and Reiter (1995) CL
van Deemter (2002) PL
Dale and Haddock (1991) EL
Kelleher and Kruijff (2006) EL
Gardent (2002) ELU (?)
Figure 5: DL variants used by different GRE algorithms.
parallel. It monotonically refines a partition of the
model and never needs to backtrack, and therefore
is always guaranteed to terminate.
Perhaps closest in spirit to our approach is Krah-
mer et al?s graph algorithm (2003), which also com-
putes REs by extending them successively. How-
ever, their subgraphs go beyond the expressive
power of ALC in that they can distinguish between
?the dog that bites a dog? and ?the dog that bites it-
self?. The price they pay for this increase in expres-
sive power is an NP-complete worst-case complex-
ity. Interestingly, Krahmer et al themselves discuss
the possibility of seeing their subgraphs as formu-
las of hybrid logic which are satisfied at the points
where the subgraph can be embedded; and hybrid
logics can be seen as very expressive description
logics (Areces and ten Cate, 2006).
6 Conclusion
In this paper, we have explored the idea of view-
ing the generation of singular REs as the problem
of computing a DL formula with a given extension.
We have shown how such formulas can be computed
efficiently (for ALC and EL) by adapting existing
algorithms from the literature. The EL algorithm
is able to generate 95% of the non-relational and
67% of the relational REs from Viethen and Dale
(2006). Both algorithms are extremely efficient (350
ms and 140 ms respectively to generate relational
REs for all individuals in a random model with 100
individuals); to our knowledge, these are by far the
fastest runtimes for relational GRE reported in the
literature. We have made our implementation avail-
able online at http://code.google.com/p/
crisp-nlg/wiki/DlGre.
Because they compute referring expressions for
all individuals in the domain at once, our algorithms
will perform especially strongly in static settings,
such as the generation of descriptions for museum
48
exhibits, in which the individuals and their proper-
ties don?t change much. However, even in more dy-
namic settings, our algorithms have a chance to out-
perform search algorithms like Dale and Haddock?s
in the average case because they can?t get stuck in
unproductive branches of the search space. Never-
theless, one interesting question for future research
is how to incrementally update simulation classes
when the model changes. Similarly, it would be
interesting to explore how different linguistic con-
straints and attribute orderings can be taken into ac-
count efficiently, how our algorithms could be in-
tegrated with more standard DL T-Box inferences,
and how they can be adapted to use inverse relations
or to compute REs for sets. In exploring these ex-
tensions we will be able to draw on a rich body of
literature that has already considered many variants
of simulation algorithms addressing similar issues.
In experimenting with the Viethen and Dale data,
we found that there is no single ordering that covers
all human-produced descriptions, which seems to be
in contrast to Dale and Reiter?s (1995) assumption
that there is only one ordering for each given do-
main. In fact, it is not even the case that each speaker
consistently uses just one ordering. An interesting
open research question is thus what factors deter-
mine which ordering is used. Unfortunately, both
in the Viethen and Dale dataset and in the TUNA
corpus (van Deemter et al, 2006), only a minor-
ity of referring expressions is relational, maybe be-
cause these domains lend themselves very well to
row/column style propositional REs. We are cur-
rently collecting REs in a domain in which propo-
sitional REs are less preferred.
Acknowledgments. We are grateful to Hector
Geffner (who independently suggested to view GRE as
computation of DL formulas), Kees van Deemter, and
Emiel Krahmer for interesting discussions. We also
thank Jette Viethen and Robert Dale for making their
corpus available, and the reviewers for their comments.
References
C. Areces and B. ten Cate. 2006. Hybrid logics. In
P. Blackburn, F. Wolter, and J. van Benthem, editors,
Handbook of Modal Logics. Elsevier.
F. Baader, D. McGuiness, D. Nardi, and P. Patel-
Schneider, editors. 2003. The Description Logic
Handbook: Theory, implementation and applications.
Cambridge University Press.
P. Blackburn, M. de Rijke, and Y. Venema. 2001. Modal
Logic. Cambridge University Press.
R. Dale and N. Haddock. 1991. Generating referring
expressions involving relations. In Proc. of the 5th
EACL.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19.
R. Dale. 1989. Cooking up referring expressions. In
Proc. of the 27th ACL.
A. Dovier, C. Piazza, and A. Policriti. 2004. An ef-
ficient algorithm for computing bisimulation equiva-
lence. Theoretical Computer Science, 311(1?3).
C. Gardent and K. Striegnitz. 2007. Generating bridg-
ing definite descriptions. In H. Bunt and R. Muskens,
editors, Computing Meaning, Vol. 3. Springer.
C. Gardent. 2002. Generating minimal definite descrip-
tions. In Proc. of the 40th ACL.
J. Hopcroft. 1971. An n log(n) algorithm for minimizing
states in a finite automaton. In Z. Kohave, editor, The-
ory of Machines and computations. Academic Press.
H. Horacek. 1997. An algorithm for generating refer-
ential descriptions with flexible interfaces. In Proc. of
the 35th ACL.
J. Kelleher and G.-J. Kruijff. 2006. Incremental genera-
tion of spatial referring expressions in situated dialog.
In Proc. of COLING/ACL.
A. Koller and M. Stone. 2007. Sentence generation as
planning. In Proc. of the 45th ACL.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1).
N. Kurtonina and M. de Rijke. 1998. Expressiveness
of concept expressions in first-order description logics.
Artificial Intelligence, 107.
R. Paige and R. Tarjan. 1987. Three partition refinement
algorithms. SIAM Journal on Computing, 16(6).
M. Stone and B. Webber. 1998. Textual economy
through close coupling of syntax and semantics. In
Proc. of the 9th INLG workshop.
M. Stone. 2000. On identifying sets. In Proc. of the 1st
INLG.
K. van Deemter, I. van der Sluis, and A. Gatt. 2006.
Building a semantically transparent corpus for the gen-
eration of referring expressions. In Proc. of the 4th
INLG.
K. van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37?52.
J. Viethen and R. Dale. 2006. Algorithms for generating
referring expressions: Do they do what people do? In
Proc. of the 4th INLG.
49
Report on the Second NLG Challenge on
Generating Instructions in Virtual Environments (GIVE-2)
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Andrew Gargett
Saarland University
gargett@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the second installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE-2), a shared
task for the NLG community which took
place in 2009-10. We evaluated seven
NLG systems by connecting them to 1825
users over the Internet, and report the re-
sults of this evaluation in terms of objec-
tive and subjective measures.
1 Introduction
This paper reports on the methodology and results
of the Second Challenge on Generating Instruc-
tions in Virtual Environments (GIVE-2), which
we ran from August 2009 to May 2010. GIVE
is a shared task for the NLG community which
we ran for the first time in 2008-09 (Koller et al,
2010). An NLG system in this task must generate
instructions which guide a human user in solving
a treasure-hunt task in a virtual 3D world, in real
time. For the evaluation, we connect these NLG
systems to users over the Internet, which makes
it possible to collect large amounts of evaluation
data cheaply.
While the GIVE-1 challenge was a success, in
that it evaluated five NLG systems on data from
1143 game runs in the virtual environments, it
was limited in that users could only move and
turn in discrete steps in the virtual environments.
This made the NLG task easier than intended; one
of the best-performing GIVE-1 systems generated
instructions of the form ?move three steps for-
ward?. The primary change in GIVE-2 compared
to GIVE-1 is that users could now move and turn
freely, which makes expressions like ?three steps?
meaningless, and makes it hard to predict the pre-
cise effect of instructing a user to ?turn left?.
We evaluated seven NLG systems from six in-
stitutions in GIVE-2 over a period of three months
from February to May 2010. During this time,
we collected 1825 games that were played by
users from 39 countries, which is an increase of
over 50% over the data we collected in GIVE-
1. We evaluated each system both on objec-
tive measures (success rate, completion time, etc.)
and subjective measures which were collected by
asking the users to fill in a questionnaire. We
completely revised the questionnaire for the sec-
ond challenge, which now consists of relatively
fine-grained questions that can be combined into
more high-level groups for reporting. We also in-
troduced several new objective measures, includ-
ing the point in the game in which users lost
or cancelled, and an experimental ?back-to-base?
task intended to measure how much users learned
about the virtual world while interacting with the
NLG system.
Plan of the paper. The paper is structured as fol-
lows. In Section 2, we describe and motivate the
GIVE-2 Challenge. In section 3, we describe the
evaluation method and infrastructure. Section 4
reports on the evaluation results. Finally, we con-
clude and discuss future work in Section 5.
2 The GIVE Challenge
GIVE-2 is the second installment of the GIVE
Challenge (?Generating Instructions in Virtual En-
vironments?), which we ran for the first time in
2008-09. In the GIVE scenario, subjects try to
solve a treasure hunt in a virtual 3Dworld that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual world.
The challenge for the NLG system is to gener-
ate, in real time, natural-language instructions that
will guide the users to the successful completion
of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
Figure 1: What the user sees when playing with
the GIVE Challenge.
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
In each game world we used in GIVE-2, players
must pick up a trophy, which is in a wall safe be-
hind a picture. In order to access the trophy, they
must first push a button to move the picture to the
side, and then push another sequence of buttons to
open the safe. One floor tile is alarmed, and play-
ers lose the game if they step on this tile without
deactivating the alarm first. There are also a num-
ber of distractor buttons which either do nothing
when pressed or set off an alarm. These distractor
buttons are intended to make the game harder and,
more importantly, to require appropriate reference
to objects in the game world. Finally, game worlds
contained a number of objects such as chairs and
flowers that did not bear on the task, but were
available for use as landmarks in spatial descrip-
tions generated by the NLG systems.
The crucial difference between this task and
the (very similar) GIVE-1 task was that in GIVE-
2, players could move and turn freely in the vir-
tual world. This is in contrast to GIVE-1, where
players could only turn by 90 degree increments,
and jump forward and backward by discrete steps.
This feature of the way the game controls were set
up made it possible for some systems to do very
well in GIVE-1 with only minimal intelligence,
using exclusively instructions such as ?turn right?
and ?move three steps forward?. Such instructions
are unrealistic ? they could not be carried over to
instruction-giving in the real world ?, and our aim
was to make GIVE harder for systems that relied
on them.
3 Method
Following the approach from the GIVE-1 Chal-
lenge (Koller et al, 2010), we connected the NLG
systems to users over the Internet. In each game
run, one user and one NLG system were paired up,
with the system trying to guide the user to success
in a specific game world.
3.1 Software infrastructure
We adapted the GIVE-1 software to the GIVE-2
setting. The GIVE software infrastructure (Koller
et al, 2009a) consists of three different mod-
ules: The client, which is the program which the
user runs on their machine to interact with the
virtual world (see Fig. 1); a collection of NLG
servers, which generate instructions in real-time
and send them to the client; and a matchmaker,
which chooses a random NLG server and virtual
world for each incoming connection from a client
and stores the game results in a database.
The most visible change compared to GIVE-1
was to modify the client so it permitted free move-
ment in the virtual world. This change further ne-
cessitated a number of modifications to the inter-
nal representation of the world. To support the de-
velopment of virtual worlds for GIVE, we changed
the file format for world descriptions to be much
more readable, and provided an automatic tool
for displaying virtual worlds graphically (see the
screenshots in Fig. 2).
3.2 Recruiting subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the Internet
and in traditional newspapers. We further adver-
tised GIVE at the Cebit computer expo as part of
the Saarland University booth. Recruiting anony-
mous experimental subjects over the Internet car-
ries known risks (Gosling et al, 2004), but we
showed in GIVE-1 that the results obtained for
the GIVE Challenge are comparable and more in-
formative than those obtained from a laboratory-
World 1 World 2 World 3
Figure 2: The three GIVE-2 evaluation worlds.
based experiment (Koller et al, 2009b).
We also tried to leverage social networks for re-
cruiting participants by implementing and adver-
tising a Facebook application. Because of a soft-
ware bug, only about 50 participants could be re-
cruited in this way. Thus tapping the true poten-
tial of social networks for recruiting participants
remains a task for the next installment of GIVE.
3.3 Evaluation worlds
Fig. 2 shows the three virtual worlds we used in the
GIVE-2 evaluation. Overall, the worlds were more
difficult than the worlds used in GIVE-1, where
some NLG-systems had success rates around 80%
in some of the worlds. As for GIVE-1, the three
worlds were designed to pose different challenges
to the NLG systems. World 1 was intended to be
more similar to the development world and last
year?s worlds. It did have rooms with more than
one button of the same color, however, these but-
tons were not located close together. World 2 con-
tained several situations which required more so-
phisticated referring expressions, such as rooms
with several buttons of the same color (some of
them close together) and a grid of buttons. Fi-
nally, World 3 was designed to exercise the sys-
tems? navigation instructions: one room contained
a ?maze? of alarm tiles, and another room two
long rows of buttons hidden in ?booths? so that
they were not all visible at the same time.
3.4 Timeline
After the GIVE-2 Challenge was publicized in
June 2009, fifteen researchers and research teams
declared their interest in participating. We dis-
tributed a first version of the software to these
teams in August 2009. In the end, six teams sub-
mitted NLG systems (two more than in GIVE-1);
one team submitted two independent NLG sys-
tems, bringing the total number of NLG systems
up to seven (two more than in GIVE-1). These
were connected to a central matchmaker that ran
for a bit under three months, from 23 February to
17 May 2010.
3.5 NLG systems
Seven NLG systems were evaluated in GIVE-2:
? one system from the Dublin Institute of Tech-
nology (?D? in the discussion below);
? one system from Trinity College Dublin
(?T?);
? one system from the Universidad Com-
plutense de Madrid (?M?);
? one system from the University of Heidelberg
(?H?);
? one system from Saarland University (?S?);
? and two systems from INRIA Grand-Est in
Nancy (?NA? and ?NM?).
Detailed descriptions of these systems as well
as each team?s own analysis of the evalua-
tion results can be found at http://www.
give-challenge.org/research.
4 Results
We now report the results of GIVE-2. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures. The
data for the objective measures are extracted from
the logs of the interactions; whereas the data for
the subjective measures are obtained from a ques-
tionnaire which asked subjects to rate various as-
pects of the NLG system they interacted with.
Notice that some of our evaluation measures are
in tension with each other: For instance, a sys-
tem which gives very low-level instructions may
allow the user to complete the task more quickly
(there is less chance of user errors), but it will re-
quire more instructions than a system that aggre-
gates these. This is intentional, and emphasizes
our desire to make GIVE a friendly comparative
challenge rather than a competition with a clear
winner.
4.1 Demographics
Over the course of three months, we collected
1825 valid games. This is an increase of almost
60% over the number of valid games we collected
in GIVE-1. A game counted as valid if the game
client did not crash, the game was not marked as a
test game by the developers, and the player com-
pleted the tutorial.
Of these games, 79.0% were played by males
and 9.6% by females; a further 11.4% did not
specify their gender. These numbers are compa-
rable to GIVE-1. About 42% of users connected
from an IP address in Germany; 12% from the US,
8% from France, 6% from Great Britain, and the
rest from 35 further countries. About 91% of the
participants who answered the question self-rated
their English language proficiency as ?good? or
better. About 65% of users connected from vari-
ous versions of Windows, the rest were split about
evenly between Linux and MacOS.
4.2 Objective measures
The objective measures are summarize in Fig. 3.
In addition to calculating the percentage of games
users completed successfully when being guided
by the different systems, we measured the time
until task completion, the distance traveled until
task completion, and the number of actions (such
as pushing a button to open a door) executed. Fur-
thermore, we counted howmany instructions users
received from each system, and how many words
these instructions contained on average. All objec-
tive measures were collected completely unobtru-
sively, without requiring any action on the user?s
part. To ensure comparability, we only counted
successfully completed games.
task success: Did the player get the trophy?
duration: Time in seconds from the end of the tu-
torial until the retrieval of the trophy.
distance: Distance traveled (measured in distance
units of the virtual environment).
actions: Number of object manipulation actions.
instructions: Number of instructions produced
by the NLG system.
words per instruction: Average number of
words the NLG system used per instruction.
Figure 3: Objective measures.
Fig. 4 shows the results of these objective mea-
sures. Task success is reported as the percent-
age of successfully completed games. The other
measures are reported as the mean number of sec-
onds/distance units/actions/instructions/words per
instruction, respectively. The figure also assigns
systems to groups A, B, etc. for each evaluation
measure. For example, users interacting with sys-
tems in group A had a higher task success rate,
needed less time, etc. than users interacting with
systems in group B. If two systems do not share
the same letter, the difference between these two
systems is significant with p < 0.05. Significance
was tested using a ?2-test for task success and
ANOVAs for the other objective measures. These
were followed by post-hoc tests (pairwise ?2 and
Tukey) to compare the NLG systems pairwise.
In terms of task success, the systems fall pretty
neatly into four groups. Note that systems D and
T had very low task success rates. That means
that, for these systems, the results for the other ob-
jective measures may not be reliable because they
are based on just a handful of games. Another
aspect in which systems clearly differed is how
many words they used per instruction. Interest-
ingly, the three systems with the best task success
rates also produced the most succinct instructions.
The distinctions between systems in terms of the
other measures is less clear.
4.3 Subjective measures
The subjective measures were obtained from re-
sponses to a questionnaire that was presented to
users after each game. The questionnaire asked
users to rate different statements about the NLG
D H M NA NM S T
task
success
9% 11% 13% 47% 30% 40% 3%
A A
B
C C C
D D
duration
888 470 407 344 435 467 266
A A A A A
B B B B B
C
distance
231 164 126 162 167 150 89
A A A A A A
B B B B B
actions
25 22 17 17 18 17 14
A A A A A A A
instructions
349 209 463 224 244 244 78
A A A A A A
B B
words per
instruction
15 11 16 6 10 6 18
A A
B
C
D
E E
Figure 4: Results for the objective measures.
system using a continuous slider. The slider posi-
tion was translated to a number between -100 and
100. Figs. 7 and 6 show the statements that users
were asked to rate as well as the results. These
results are based on all games, independent of the
success. We report the mean rating for each item,
and, as before, systems that do not share a letter,
were found to be significantly different (p< 0.05).
We used ANOVAs and post-hoc Tukey tests to test
for significance. Note that some items make a pos-
itive statement about the NLG system (e.g., Q1)
and some make a negative statement (e.g., Q2).
For negative statements, we report the reversed
scores, so that in Figs. 7 and 6 greater numbers are
always better, and systems in group A are always
better than systems in group B.
In addition to the items Q1?Q22, the ques-
tionnaire contained a statement about the over-
all instruction quality: ?Overall, the system gave
me good directions.? Furthermore notice that the
other items fall into two categories: items that as-
sess the quality of the instructions (Q1?Q15) and
items that assess the emotional affect of the in-
teraction (Q16?Q22). The ratings in these cate-
D H M NA NM S T
overall
quality
question
-33 -18 -12 36 18 19 -25
A
B B
C C C C
quality
measures
(summed)
-183 -148 -18 373 239 206 -44
A A A
B B B B
emotional
affect
measures
(summed)
-130 -103 -90 20 -5 0 -88
A A A A
B B B B B
C C C C C
Figure 5: Results for item assessing overall in-
struction quality and the aggregated quality and
emotional affect measures.
gories can be aggregated into just two ratings by
summing over them. Fig. 5 shows the results for
the overall question and the aggregated ratings for
quality measures and emotional affect measures.
The three systems with the highest task success
rate get rated highest for overall instruction qual-
ity. The aggregated quality measure also singles
out the same group of three systems.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, some other factors also influence the out-
comes of our objective and subjective measures.
As in GIVE-1, we find that there is a significant
difference in task success rate for different evalua-
tion worlds and between users with different levels
of English proficiency. Fig. 8 illustrates the effect
of the different evaluation worlds on the task suc-
cess rate for different systems, and Fig. 9 shows
the effect that a player?s English skills have on the
task success rate. As in GIVE-1, some systems
seem to be more robust than others with respect to
changes in these factors.
None of the other factors we looked at (gender,
age, and computer expertise) have a significant ef-
fect on the task success rate. With a few excep-
tions the other objective measures were not influ-
enced by these demographic factors either. How-
ever, we do find a significant effect of age on the
time and number of actions a player needs to re-
trieve the trophy: younger players are faster and
need fewer actions. And we find that women travel
a significantly shorter distance than men on their
way to the trophy. Interestingly, we do not find
D H M NA NM S T
Q1: The system used words and phrases
that were easy to understand.
45 26 41 62 54 58 46
A A A A
B B B B
C C C
Q2: I had to re-read instructions to under-
stand what I needed to do.
-26 -9 3 40 8 19 0
A
B B B B
C C C
D D
Q3: The system gave me useful feedback
about my progress.
-17 -30 -31 9 11 -13 -27
A A
B B B B
C C C C
Q4: I was confused about what to do next.
-35 -27 -18 29 9 5 -31
A
B B
C C C C
Q5: I was confused about which direction
to go in.
-32 -20 -16 21 8 3 -25
A A
B B
C C C C
Q6: I had no difficulty with identifying
the objects the system described for me.
-21 -11 -5 18 13 20 -21
A A A
B B
C C C C
Q7: The system gave me a lot of unnec-
essary information.
-22 -9 6 15 10 10 -6
A A A A
B B B B
C C C
D D D
D H M NA NM S T
Q8: The system gave me too much infor-
mation all at once.
-28 -8 9 31 8 21 15
A A A
B B B B
C C
Q9: The system immediately offered help
when I was in trouble.
-15 -13 -13 32 3 -5 -23
A
B B B B B
C C C C
Q10: The system sent instructions too
late.
15 15 9 38 39 14 8
A A
B B B B B
Q11: The system?s instructions were de-
livered too early.
15 5 21 39 12 30 28
A A A
B B B B
C C C C
D D D D
Q12: The system?s instructions were vis-
ible long enough for me to read them.
-67 -21 -19 6 -14 0 -18
A A
B B B
C C C C
D
Q13: The system?s instructions were
clearly worded.
-20 -9 1 32 23 26 6
A A A
B B B
C C C
D D
Q14: The system?s instructions sounded
robotic.
16 -6 8 -4 -1 5 1
A A A A A A
B B B B B B
Q15: The system?s instructions were
repetitive.
-28 -26 -11 -31 -28 -26 -23
A A A A A
B B B B B B
Figure 7: Results for the subjective measures assessing the quality of the instructions.
D H M NA NM S T
Q16: I really wanted to find that trophy.
-10 -13 -9 -11 -8 -7 -12
A A A A A A A
Q17: I lost track of time while solving the
overall task.
-13 -18 -21 -16 -18 -11 -20
A A A A A A A
Q18: I enjoyed solving the overall task.
-21 -23 -20 -8 -4 -5 -21
A A A A A A
B B B B B
Q19: Interacting with the system was re-
ally annoying.
-14 -20 -12 8 -2 -2 -14
A A A
B B B B B
C C C C
Q20: I would recommend this game to a
friend.
-36 -39 -31 -30 -25 -24 -31
A A A A A A A
Q21: The system was very friendly.
0 -1 5 30 20 19 5
A A A
B B B B
C C C C
D D D D
Q22: I felt I could trust the system?s in-
structions.
-21 -6 -3 37 23 21 -13
A A A
B B B B
Figure 6: Results for the subjective measures as-
sessing the emotional affect of the instructions.
Figure 8: Effect of the evaluation worlds on the
success rate of the NLG systems.
Figure 9: Effect of the players? English skills on
the success rate of the NLG systems.
a significant effect of gender on the time players
need to retrieve the trophy as in GIVE-1 (although
the mean duration is somewhat higher for female
than for male players; 481 vs. 438 seconds).
5 Conclusion
In this paper, we have described the setup and re-
sults of the Second GIVE Challenge. Altogether,
we collected 1825 valid games for seven NLG sys-
tems over a period of three months. Given that this
is a 50% increase over GIVE-1, we feel that this
further justifies our basic experimental methodol-
ogy. As we are writing this, we are preparing de-
tailed results and analyses for each participating
team, which we hope will help them understand
and improve the performance of their systems.
The success rate is substantially worse in GIVE-
2 than in GIVE-1. This is probably due to the
Figure 10: Points at which players lose/cancel.
harder task (free movement) explained in Sec-
tion 2 and to the more complex evaluation worlds
(see Section 3.3). It was our intention to make
GIVE-2 more difficult, although we did not antic-
ipate such a dramatic drop in performance. GIVE-
2.5 next year will use the same task as GIVE-2 and
we hope to see an increase in task success as the
participating research teams learn from this year?s
results.
It is also noticeable that players gave mostly
negative ratings in response to statements about
immersion and engagement (Q16-Q20). We dis-
cussed last year how to make the task more engag-
ing on the one hand and how to manage expecta-
tions on the other hand, but none of the suggested
solutions ended up being implemented. It seems
that we need to revisit this issue.
Another indication that the task may not be able
to capture participants is that the vast majority of
cancelled and lost games end in the very begin-
ning. To analyze at what point players lose or give
up, we divide the game into phases demarcated
by manipulations of buttons that belong to the 6-
button safe sequence. Fig. 10 illustrates in which
phase of the game players lose or cancel.
We are currently preparing the GIVE-2.5 Chal-
lenge, which will take place in 2010-11. GIVE-2.5
will be very similar to GIVE-2, so that GIVE-2
systems will be able to participate with only mi-
nor changes. In order to support the development
of GIVE-2.5 systems, we have collected a multi-
lingual corpus of written English and German in-
structions in the GIVE-2 environment (Gargett et
al., 2010). We expect that GIVE-3 will then extend
the GIVE task substantially, perhaps in the direc-
tion of full dialogue or of multimodal interaction.
Acknowledgments. GIVE-2 was only possible
through the support and hard work of a number of
colleagues, especially Konstantina Garoufi (who
handled the website and other publicity-related is-
sues), Ielka van der Sluis (who contributed to the
design of the GIVE-2 questionnaire), and several
student assistants who programmed parts of the
GIVE-2 system. We thank the press offices of
Saarland University, the University of Edinburgh,
and Macquarie University for their helpful press
releases. We also thank the organizers of Gener-
ation Challenges 2010 and INLG 2010 for their
support and the opportunity to present our results,
and the seven participating research teams for their
contributions.
References
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The GIVE-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Malta.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009a. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jo-
hanna Moore, and Jon Oberlander. 2009b. Validat-
ing the web-based evaluation of NLG systems. In
Proceedings of ACL-IJCNLP 2009 (Short Papers),
Singapore.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010. The first challenge on
generating instructions in virtual environments. In
E. Krahmer and M. Theune, editors, Empirical
Methods in Natural Language Generation, volume
5790 of LNCS, pages 337?361. Springer.
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 12?16,
Utica, May 2012. c?2012 Association for Computational Linguistics
Referring in Installments: A Corpus Study of Spoken Object References
in an Interactive Virtual Environment
Kristina Striegnitz?, Hendrik Buschmeier?and Stefan Kopp?
?Computer Science Department, Union College, Schenectady, NY
striegnk@union.edu
?Sociable Agents Group ? CITEC, Bielefeld University, Germany
{hbuschme,skopp}@techfak.uni-bielefeld.de
Abstract
Commonly, the result of referring expression
generation algorithms is a single noun phrase.
In interactive settings with a shared workspace,
however, human dialog partners often split re-
ferring expressions into installments that adapt
to changes in the context and to actions of their
partners. We present a corpus of human?human
interactions in the GIVE-2 setting in which in-
structions are spoken. A first study of object
descriptions in this corpus shows that refer-
ences in installments are quite common in this
scenario and suggests that contextual factors
partly determine their use. We discuss what
new challenges this creates for NLG systems.
1 Introduction
Referring expression generation is classically consid-
ered to be the problem of producing a single noun
phrase that uniquely identifies a referent (Krahmer
and van Deemter, 2012). This approach is well suited
for non-interactive, static contexts, but recently, there
has been increased interest in generation for situated
dialog (Stoia, 2007; Striegnitz et al, 2011).
Most human language use takes place in dynamic
situations, and psycholinguistic research on human?
human dialog has proposed that the production of
referring expressions should rather be seen as a pro-
cess that not only depends on the context and the
choices of the speaker, but also on the reactions of
the addressee. Thus the result is often not a single
noun phrase but a sequence of installments (Clark
and Wilkes-Gibbs, 1986), consisting of multiple utter-
ances which may be interleaved with feedback from
the addressee. In a setting where the dialog partners
have access to a common workspace, they, further-
more, carefully monitor each other?s non-linugistic
actions, which often replace verbal feedback (Clark
and Krych, 2004; Gergle et al, 2004). The following
example from our data illustrates this. A is instructing
B to press a particular button.
(1) A: the blue button
B: [moves and then hesitates]
A: the one you see on your right
B: [starts moving again]
A: press that one
While computational models of this behavior are still
scarce, some first steps have been taken. Stoia (2007)
studies instruction giving in a virtual environment
and finds that references to target objects are often
not made when they first become visible. Instead in-
teraction partners are navigated to a spot from where
an easier description is possible. Garoufi and Koller
(2010) develop a planning-based approach of this be-
havior. But once their system decides to generate a
referring expression, it is delivered in one unit.
Thompson (2009), on the other hand, proposes a
game-theoretic model to predict how noun phrases
are split up into installments. While Thompson did
not specify how the necessary parameters to calculate
the utility of an utterance are derived from the context
and did not implement the model, it provides a good
theoretical basis for an implementation.
The GIVE Challenge is a recent shared task on sit-
uated generation (Striegnitz et al, 2011). In the GIVE
scenario a human user goes on a treasure hunt in a
virtual environment. He or she has to press a series of
buttons that unlock doors and open a safe. The chal-
lenge for the NLG systems is to generate instructions
in real-time to guide the user to the goal. The instruc-
tions are presented to the user as written text, which
12
means that there is less opportunity for interleaving
language and actions than with spoken instructions.
While some systems generate sentence fragments in
certain situations (e.g., not this one when the user
is moving towards the wrong button), instructions
are generally produced as complete sentences and
replaced with a new full sentence when the context
changes (a strategy which would not work for spoken
instructions). Nevertheless, timing issues are a cause
for errors that is cited by several teams who devel-
oped systems for the GIVE challenge, and generating
appropriate feedback has been an important concern
for almost all teams (see the system descriptions in
(Belz et al, 2011)). Unfortunately, no systematic er-
ror analysis has been done for the interactions from
the GIVE challenges. Anecdotally, however, not re-
acting to signs of confusion in the user?s actions at
all or reacting too late seem to be common causes for
problems. Furthermore, we have found that the strat-
egy of replacing instructions with complete sentences
to account for a change in context can lead to con-
fusion because it seems unclear to the user whether
this new instruction is a correction or an elaboration.
In this paper we report on a study of the com-
municative behavior of human dyads in the GIVE
environment where instead of written text instruction
givers use unrestricted spoken language to direct in-
struction followers through the world. We find that
often multiple installments are used to identify a ref-
erent and that the instruction givers are highly respon-
sive to context changes and the instruction followers?
actions. Our goal is to inform the development of a
generation system that generates object descriptions
in installments while taking into account the actions
of its interaction partner.
2 A corpus of spoken instructions in a
virtual environment
Data collection method The setup of this study
was similar to the one used to collect the GIVE-2
corpus of typed instructions (Gargett et al, 2010).
Instruction followers (IFs) used the standard GIVE-2
client to interact with the virtual environment. In-
struction givers (IGs) could observe the followers?
position and actions in the world using an interactive
map, and they were also provided with the same 3D
view into the scene that the IFs saw on their screen.
Differently from the normal GIVE-2 scenario, the
IGs did not type their instructions but gave spoken
instructions, which were audio recorded as well as
streamed to the IFs over the network. A log of the IFs?
position, orientation and actions that was updated ev-
ery 200ms was recorded in a database.
Participants were recruited in pairs on Bielefeld
University?s campus and received a compensation
of six euros each. They were randomly assigned
to the roles of IG and IF and were seated and in-
structed separately. To become familiar with the task,
they switched roles in a first, shorter training world.
These interactions were later used to devise and test
the annotation schemes. They then played two dif-
ferent worlds in their assigned roles. After the first
round, they received a questionnaire assessing the
quality of the interaction; after the second round, they
completed the Santa Barbara sense of direction test
(Hegarty et al, 2006) and answered some questions
about themselves.
Annotations The recorded instructions of the IGs
were transcribed and segmented into utterances (by
identifying speech pauses longer than 300ms) using
Praat (Boersma and Weenink, 2011). We then created
videos showing the IGs? map view as well as the IFs?
scene view and aligned the audio and transcriptions
with them. The data was further annotated by the first
two authors using ELAN (Wittenburg et al, 2006).
Most importantly for this paper, we classified ut-
terances into the following types:
(i) move (MV) ? instruction to turn or to move
(ii) manipulate (MNP) ? instruction to manipulate an object
(e.g., press a button)
(iii) reference (REF) ? utterance referring to an object
(iv) stop ? instruction to stop moving
(v) warning ? telling the user to not do something
(vi) acknowledgment (ACK) ? affirmative feedback
(vii) communication management (CM) ? indicating that the
IG is planning (e.g., uhmm, just a moment, sooo etc.)
(viii) negative acknowledgment ? indicating a mistake on the
player?s part
(ix) other ? anything else
A few utterances which contained both move and
press instructions were further split, but in general
we picked the label that fit best (using the above list
as a precedence order to make a decision if two labels
fit equally well). The inter-annotator agreement for
utterance types was ? = 0.89 (Cohen?s kappa), which
13
is considered to be very good. Since the categories
were of quite different sizes (cf. Table 1), which may
skew the ? statistic, we also calculated the kappa per
category. It was satisfactory for all ?interesting? cate-
gories. The agreement for category REF was ? = 0.77
and the agreement for other was ? = 0.58. The kappa
values for all other categories were 0.84 or greater.
We reviewed all cases with differing annotations and
reached a consensus, which is the basis for all results
presented in this paper. Furthermore, we collapsed
the labels warning, negative acknowledgment and
other which only occurred rarely.
To support a later more in depth analysis, we also
annotated what types of properties are used in object
descriptions, the givenness status of information in
instructions, and whether an utterance is giving pos-
itive or negative feedback on a user action (even if
not explicitly labeled as (negative) acknowledgment).
Finally, information about the IF?s movements and
actions in the world as well as the visible context was
automatically calculated from the GIVE log files and
integrated into the annotation.
Collected data We collected interactions between
eight pairs. Due to failures of the network connection
and some initial problems with the GIVE software,
only four pairs were recorded completely, so that
we currently have data from eight interactions with
four different IGs. We are in the process of collect-
ing additional data in order to achieve a corpus size
that will allow for a more detailed statistical analy-
sis. Furthermore, we are collecting data in English
to be able to make comparisons with the existing
corpus of written instructions in the GIVE world and
to make the data more easily accessible to a wider
audience. The corpus will be made freely available
at http://purl.org/net/sgive-corpus.
Participants were between 20 and 30 years old and
all of them are native German speakers. Two of the
IGs are male and two female; three of the IFs are
female. The mean length of the interactions is 5.24
minutes (SD= 1.86), and the IGs on average use 325
words (SD = 91).
Table 1 gives an overview of the kinds of ut-
terances used by the IGs. While the general pic-
ture is similar for all speakers, there are statisti-
cally significant differences between the frequen-
cies with which different IGs use the utterance types
Table 1: Overall frequency of utterance types.
utterance type count %
MV 334 46.58
MNP 66 9.21
REF 65 9.07
stop 38 5.30
ACK 92 12.83
CM 97 13.53
other 25 3.49
Table 2: Transitional probabilities for utterance types.
M
V
M
N
P
R
E
F
st
op
A
C
K
C
M
ot
he
r
IF pr
es
s
MV .53 .08 .06 .06 .15 .08 .03 .00
MNP .02 .03 .09 .02 .02 .02 .02 .80
REF .00 .33 .19 .02 .14 .00 .02 .30
stop .47 .03 .18 .03 .03 .16 .11 .00
ACK .64 .08 .09 .03 .01 .10 .00 .05
CM .53 .05 .10 .08 .01 .18 .05 .00
other .44 .04 .12 .12 .08 .16 .00 .04
IF press .21 .01 .00 .01 .36 .36 .04 .00
(?2 = 78.82, p ? 0.001). We did not find a signifi-
cant differences (in terms of the utterance types used)
between the two worlds that we used or between the
two rounds that each pair played.
3 How instruction givers describe objects
We now examine how interaction partners establish
what the next target button is. Overall, there are 76
utterance sequences in the data that identify a target
button and lead to the IF pressing that button. We
discuss a selection of seven representative examples.
(2) IG: und dann dr?ckst du den ganz rechten Knopf den
blauen (and then you press the rightmost button the
blue one; MNP)
IF: [goes across the room and does it]
In (2) the IG generates a referring expression iden-
tifying the target and integrates it into an object ma-
nipulation instruction. In our data, 55% of the tar-
get buttons (42 out of 76) get identified in this way
(which fits into the traditional view of referring ex-
pression generation). In all other cases a sequence of
at least two, and in 14% of the cases more than two,
utterances is used.
The transitional probabilities between utterance
types shown in Table 2 suggest what some common
patterns may be. For example, even though move
instructions are so prevalent in our data, they are
uncommon after reference or manipulate utterances.
14
Instead, two thirds of the reference utterances are
followed by object manipulation instruction, another
reference or an acknowledgement. In the remaining
cases, IFs press a button in response to the reference.
(3) IG: vor dir der blaue Knopf (in front of you the blue button;
REF)
IF: [moves across the room toward the button]
IG: drauf dr?cken (press it; MNP)
(4) IG: und auf der rechten Seite sind zwei rote Kn?pfe (and
on the right are two red buttons; REF)
IF: [turns and starts moving towards the buttons]
IG: und den linken davon dr?ckst du (and you press the left
one; MNP)
In (3) and (4) a first reference utterance is followed
by a separate object manipulation utterance. While
in (3) the first reference uniquely identifies the target,
in (4) the first utterance simply directs the player?s
attention to a group of buttons. The second utterance
then picks out the target.
(5) IG: dreh dich nach links etwas (turn left a little; MV)
IF: [turns left] there are two red buttons in front of him
(and some other red buttons to his right)
IG: so, da siehst du zwei rote Schalter (so now you see two
red buttons; REF)
IF: [moves towards buttons]
IG: und den rechten davon dr?ckst du (and you press the
right one; MNP)
IF: [moves closer, but more towards the left one]
IG: rechts (right; REF)
Stoia (2007) observed that IGs use move instruc-
tions to focus the IF?s attention on a particular area.
This is also common in our data. For instance in (5),
the IF is asked to turn to directly face the group of
buttons containing the target. (5) also shows how IGs
monitor their partners? actions and respond to them.
The IF is moving towards the wrong button causing
the IG to repeat part of the previous description.
(6) IG: den blauen Schalter (the blue button; REF)
IF: [moves and then stops]
IG: den du rechts siehst (the one you see on your right;
REF)
IF: [starts moving again]
IG: den dr?cken (press that one; MNP)
Similarly, in (6) the IG produces an elaboration
when the IF stops moving towards the target, indicat-
ing her confusion.
(7) IG: und jetzt rechts an der (and now to the right on the;
REF)
IF: [turns right, is facing the wall with the target button]
IG: ja . . . genau . . . an der Wand den blauen Knopf (yes
. . . right . . . on the wall the blue button; ACK, REF)
IF: [moves towards button]
IG: einmal dr?cken (press once; MNP)
In (7) the IG inserts affirmative feedback when
the IF reacts correctly to a portion of his utterance.
As can be seen in Table 2, reference utterances are
relatively often followed by affirmative feedback.
(8) IF: [enters room, stops, looks around, ends up looking at
the target]
IG: ja genau den gr?nen Knopf neben der Lampe dr?cken
(yes right, press the green button next to the lamp;
MNP)
IGs can also take advantage of IF actions that are
not in direct response to an utterance. This happens
in (8). The IF enters a new room and looks around.
When she looks towards the target, the IG seizes the
opportunity and produces affirmative feedback.
4 Conclusions and future work
We have described a corpus of spoken instructions in
the GIVE scenario which we are currently building
and which we will make available once it is com-
pleted. This corpus differs from other corpora of task-
oriented dialog (specifically, the MapTask corpus
(Anderson et al, 1991), the TRAINS corpus (Hee-
man and Allen, 1995), the Monroe corpus (Stent,
2000)) in that the IG could observe the IF?s actions
in real-time. This led to interactions in which in-
structions are given in installments and linguistic and
non-linguistic actions are interleaved.
This poses interesting new questions for NLG sys-
tems, which we have illustrated by discussing the
patterns of utterance sequences that IGs and IFs use
in our corpus to agree on the objects that need to
be manipulated. In line with results from psycholin-
guistics, we found that the information necessary to
establish a reference is often expressed in multiple
installments and that IGs carefully monitor how their
partners react to their instructions and quickly re-
spond by giving feedback, repeating information or
elaborating on previous utterance when necessary.
The NLG system thus needs to be able to de-
cide when a complete identifying description can
be given in one utterance and when a description in
installments is more effective. Stoia (2007) as well
as Garoufi and Koller (2010) have addressed this
question, but their approaches only make a choice be-
tween generating an instruction to move or a uniquely
identifying referring expression. They do not con-
sider cases in which another type of utterance, for
instance, one that refers to a group of objects or gives
15
an initial ambiguous description, is used to draw the
attention of the IF to a particular area and they do not
generate referring expressions in installments.
The system, furthermore, needs to be able to in-
terpret the IF?s actions and decide when to insert an
acknowledgment, elaboration or correction. It then
has to decide how to formulate this feedback. The
addressee, e.g., needs to be able to distinguish elabo-
rations from corrections. If the feedback was inserted
in the middle of a sentence, if finally has to decide
whether this sentence should be completed and how
the remainder may have to be adapted.
Once we have finished the corpus collection, we
plan to use it to study and address the questions dis-
cussed above. We are planning on building on the
work by Stoia (2007) on using machine learning tech-
niques to develop a model that takes into account var-
ious contextual factors and on the work by Thompson
(2009) on generating references in installments. The
set-up under which the corpus was collected, further-
more, lends itself well to Wizard-of-Oz studies to test
the effectiveness of different interactive strategies for
describing objects.
Acknowledgments This research was supported
by the Deutsche Forschungsgemeinschaft (DFG) in
the Center of Excellence in ?Cognitive Interaction
Technology? (CITEC) and by the Skidmore Union
Network which was funded through an ADVANCE
grant from the National Science Foundation.
References
Anne H. Anderson, Miles Bader, Ellen Gurman Bard, Eliz-
abeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen
Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller,
Catherine Sotillo, Henry S. Thompson, and Regina
Weinert. 1991. The HCRC map task corpus. Lan-
guage and Speech, 34:351?366.
Anja Belz, Albert Gatt, Alexander Koller, and Kristina
Striegnitz, editors. 2011. Proceedings of the Genera-
tion Challenges Session at the 13th European Workshop
on Natural Language Generation, Nancy, France.
Paul Boersma and David Weenink. 2011. Praat: doing
phonetics by computer. Computer program. Retrieved
May 2011, from http://www.praat.org/.
Herbert H. Clark and Meredyth A. Krych. 2004. Speaking
while monitoring addressees for understanding. Jour-
nal of Memory and Language, 50:62?81.
Herbert H Clark and Deanna Wilkes-Gibbs. 1986. Refer-
ring as a collaborative process. Cognition, 22:1?39.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus
of giving instructions in virtual environments. In Pro-
ceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC?10), pages
2401?2406, Valletta, Malta.
Konstantina Garoufi and Alexander Koller. 2010. Au-
tomated planning for situated natural language gener-
ation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1573?1582, Uppsala, Sweden.
Darren Gergle, Robert E. Kraut, and Susan R. Fussell.
2004. Action as language in a shared visual space. In
Proceedings of the 2004 ACM Conference on Computer
Supported Cooperative Work, pages 487?496, Chicago,
IL.
Peter A. Heeman and James Allen. 1995. The Trains 93
dialogues. Technical Report Trains 94-2, Computer Sci-
ence Department, University of Rochester, Rochester,
NY.
Mary Hegarty, Daniel R. Montello, Anthony E. Richard-
son, Toru Ishikawa, and Kristin Lovelace. 2006. Spa-
tial abilities at different scales: Individual differences in
aptitude-test performance and spatial-layout learning.
Intelligence, 34:151?176.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38:173?218.
Amanda Stent. 2000. The Monroe corpus. Technical
Report 728/TN 99-2, Computer Science Department,
University of Rochester, Rochester, NY.
Laura Stoia. 2007. Noun Phrase Generation for Situated
Dialogs. Ph.D. thesis, Graduate School of The Ohio
State University, Columbus, OH.
Kristina Striegnitz, Alexandre Denis, Andrew Gargett,
Konstantina Garoufi, Alexander Koller, and Mari?t The-
une. 2011. Report on the second second challenge on
generating instructions in virtual environments (GIVE-
2.5). In Proceedings of the Generation Challenges
Session at the 13th European Workshop on Natural
Language Generation, pages 270?279, Nancy, France.
Will Thompson. 2009. A Game-Theoretic Model of
Grounding for Referential Communication Tasks. Ph.D.
thesis, Northwestern University, Evanston, IL.
Peter Wittenburg, Hennie Brugman, Albert Russel, Alex
Klassmann, and Han Sloetjes. 2006. ELAN: A pro-
fessional framework for multimodality research. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), pages
1556?1559, Genoa, Italy.
16
